Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 1?12,
Paris, October 2009. c?2009 Association for Computational Linguistics
Parsing Algorithms based on Tree Automata
Andreas Maletti
Departament de Filologies Roma`niques
Universitat Rovira i Virgili, Tarragona, Spain
andreas.maletti@urv.cat
Giorgio Satta
Department of Information Engineering
University of Padua, Italy
satta@dei.unipd.it
Abstract
We investigate several algorithms related
to the parsing problem for weighted au-
tomata, under the assumption that the in-
put is a string rather than a tree. This
assumption is motivated by several natu-
ral language processing applications. We
provide algorithms for the computation of
parse-forests, best tree probability, inside
probability (called partition function), and
prefix probability. Our algorithms are ob-
tained by extending to weighted tree au-
tomata the Bar-Hillel technique, as defined
for context-free grammars.
1 Introduction
Tree automata are finite-state devices that recog-
nize tree languages, that is, sets of trees. There
is a growing interest nowadays in the natural
language parsing community, and especially in
the area of syntax-based machine translation, for
probabilistic tree automata (PTA) viewed as suit-
able representations of grammar models. In fact,
probabilistic tree automata are generatively more
powerful than probabilistic context-free gram-
mars (PCFGs), when we consider the latter as de-
vices that generate tree languages. This difference
can be intuitively understood if we consider that a
computation by a PTA uses hidden states, drawn
from a finite set, that can be used to transfer infor-
mation within the tree structure being recognized.
As an example, in written English we can em-
pirically observe different distributions in the ex-
pansion of so-called noun phrase (NP) nodes, in
the contexts of subject and direct-object positions,
respectively. This can be easily captured using
some states of a PTA that keep a record of the dif-
ferent contexts. In contrast, PCFGs are unable to
model these effects, because NP node expansion
should be independent of the context in the deriva-
tion. This problem for PCFGs is usually solved by
resorting to so-called parental annotations (John-
son, 1998), but this, of course, results in a different
tree language, since these annotations will appear
in the derived tree.
Most of the theoretical work on parsing and es-
timation based on PTA has assumed that the in-
put is a tree (Graehl et al, 2008), in accordance
with the very definition of these devices. How-
ever, both in parsing as well as in machine transla-
tion, the input is most often represented as a string
rather than a tree. When the input is a string, some
trick is applied to map the problem back to the
case of an input tree. As an example in the con-
text of machine translation, assume a probabilistic
tree transducer T as a translation model, and an
input string w to be translated. One can then inter-
mediately construct a tree automaton Mw that rec-
ognizes the set of all possible trees that have w as
yield, with internal nodes from the input alphabet
of T . This automaton Mw is further transformed
into a tree transducer implementing a partial iden-
tity translation, and such a transducer is composed
with T (relation composition). This is usually
called the ?cascaded? approach. Such an approach
can be easily applied also to parsing problems.
In contrast with the cascaded approach above,
which may be rather inefficient, in this paper we
investigate a more direct technique for parsing
strings based on weighted and probabilistic tree
automata. We do this by extending to weighted
tree automata the well-known Bar-Hillel construc-
tion defined for context-free grammars (Bar-Hillel
et al, 1964) and for weighted context-free gram-
mars (Nederhof and Satta, 2003). This provides
an abstract framework under which several pars-
ing algorithms can be directly derived, based on
weighted tree automata. We discuss several appli-
cations of our results, including algorithms for the
computation of parse-forests, best tree probability,
inside probability (called partition function), and
prefix probability.
1
2 Preliminary definitions
Let S be a nonempty set and ? be an associative
binary operation on S. If S contains an element 1
such that 1 ? s = s = s ? 1 for every s ? S, then
(S, ?, 1) is a monoid. A monoid (S, ?, 1) is com-
mutative if the equation s1 ? s2 = s2 ? s1 holds
for every s1, s2 ? S. A commutative semiring
(S,+, ?, 0, 1) is a nonempty set S on which a bi-
nary addition + and a binary multiplication ? have
been defined such that the following conditions are
satisfied:
? (S,+, 0) and (S, ?, 1) are commutative
monoids,
? ? distributes over + from both sides, and
? s ? 0 = 0 = 0 ? s for every s ? S.
A weighted string automaton, abbreviated WSA,
(Schu?tzenberger, 1961; Eilenberg, 1974) is a sys-
tem M = (Q,?,S, I, ?, F ) where
? Q is a finite alphabet of states,
? ? is a finite alphabet of input symbols,
? S = (S,+, ?, 0, 1) is a semiring,
? I : Q? S assigns initial weights,
? ? : Q???Q? S assigns a weight to each
transition, and
? F : Q? S assigns final weights.
We now proceed with the semantics of M . Let
w ? ?? be an input string of length n. For each
integer i with 1 ? i ? n, we write w(i) to denote
the i-th character of w. The set Pos(w) of posi-
tions of w is {i | 0 ? i ? n}. A run of M on w
is a mapping r : Pos(w) ? Q. We denote the set
of all such runs by RunM (w). The weight of a
run r ? RunM (w) is
wtM (r) =
n?
i=1
?(r(i? 1), w(i), r(i)) .
We assume the right-hand side of the above equa-
tion evaluates to 1 in case n = 0. The WSA M
recognizes the mapping M : ?? ? S, which is
defined for every w ? ?? of length n by1
M(w) = ?
r?RunM (w)
I(r(0)) ?wtM (r) ?F (r(n)) .
In order to define weighted tree automata (Bers-
tel and Reutenauer, 1982; E?sik and Kuich, 2003;
Borchardt, 2005), we need to introduce some addi-
tional notation. Let ? be a ranked alphabet, that
1We overload the symbolM to denote both an automaton
and its recognized mapping. However, the intended meaning
will always be clear from the context.
is, an alphabet whose symbols have an associated
arity. We write ?k to denote the set of all k-ary
symbols in ?. We use a special symbol e ? ?0
to syntactically represent the empty string ?. The
set of ?-trees, denoted by T?, is the smallest set
satisfying both of the following conditions
? for every ? ? ?0, the single node labeled ?,
written ?(), is a tree of T?,
? for every ? ? ?k with k ? 1 and for every
t1, . . . , tk ? T?, the tree with a root node la-
beled ? and trees t1, . . . , tk as its k children,
written ?(t1, . . . , tk), belongs to T?.
As a convention, throughout this paper we assume
that ?(t1, . . . , tk) denotes ?() if k = 0. The size
of the tree t ? T?, written |t|, is defined as the
number of occurrences of symbols from ? in t.
Let t = ?(t1, . . . , tk). The yield of t is recur-
sively defined by
yd(t) =
?
??
??
? if ? ? ?0 \ {e}
? if ? = e
yd(t1) ? ? ? yd(tk) otherwise.
The set of positions of t, denoted by Pos(t), is
recursively defined by
Pos(?(t1, . . . , tk)) =
{?} ? {iw | 1 ? i ? k,w ? Pos(ti)} .
Note that |t| = |Pos(t)| and, according to our con-
vention, when k = 0 the above definition provides
Pos(?()) = {?}. We denote the symbol of t at
position w by t(w) and its rank by rkt(w).
A weighted tree automaton (WTA) is a system
M = (Q,?,S, ?, F ) where
? Q is a finite alphabet of states,
? ? is a finite ranked alphabet of input symbols,
? S = (S,+, ?, 0, 1) is a semiring,
? ? is an indexed family (?k)k?N of mappings
?k : ?k ? SQ?Qk , and
? F : Q? S assigns final weights.
In the above definition, Qk is the set of all strings
over Q having length k, with Q0 = {?}. Fur-
ther note that SQ?Qk is the set of all matrices
with elements in S, row index set Q, and column
index set Qk. Correspondingly, we will use the
common matrix notation and write instances of ?
in the form ?k(?)q0,q1???qk . Finally, we assume
q1 ? ? ? qk = ? if k = 0.
We define the semantics also in terms of runs.
Let t ? T?. A run of M on t is a mapping
r : Pos(t)? Q. We denote the set of all such runs
2
by RunM (t). The weight of a run r ? RunM (t)
is
wtM (r) =
?
w?Pos(t)
rkt(w)=k
?k(t(w))r(w),r(w1)???r(wk) .
Note that, according to our convention, the string
r(w1) ? ? ? r(wk) denotes ? when k = 0. The
WTA M recognizes the mapping M : T? ? S,
which is defined by
M(t) = ?
r?RunM (t)
wtM (r) ? F (r(?))
for every t ? T?. We say that t is recognized
by M if M(t) 6= 0.
In our complexity analyses, we use the follow-
ing measures. The size of a transition (p, ?, q) in
(the domain of ? in) a WSA is |p?q| = 3. The size
of a transition in a WTA, viewed as an instance
(?, q0, q1 ? ? ? qk) of some mapping ?k, is defined
as |?q0 ? ? ? qk|, that is, the rank of the input symbol
occurring in the transition plus two. Finally, the
size |M | of an automaton M (WSA or WTA) is
defined as the sum of the sizes of its nonzero tran-
sitions. Note that this does not take into account
the size of the representation of the weights.
3 Binarization
We introduce in this section a specific transfor-
mation of WTA, called binarization, that reduces
the transitions of the automaton to some normal
form in which no more than three states are in-
volved. This transformation maps the set of rec-
ognized trees into a special binary form, in such a
way that the yields of corresponding trees and their
weights are both preserved. We use this transfor-
mation in the next section in order to guarantee
the computational efficiency of the parsing algo-
rithm we develop. The standard ?first-child, next-
sibling? binary encoding for trees (Knuth, 1997)
would eventually result in a transformed WTA of
quadratic size. To obtain instead a linear size
transformation, we introduce a slightly modified
encoding (Ho?gberg et al, 2009, Section 4), which
is inspired by (Carme et al, 2004) and the classical
currying operation.
Let ? be a ranked alphabet and assume a
fresh symbol @ /? ? (corresponding to the ba-
sic list concatenation operator). Moreover, let
? = ?2 ? ?1 ? ?0 be the ranked alphabet such
that ?2 = {@}, ?1 = ?k?1 ?k, and ?0 = ?0. In
?
?
?
?
? ?
? ?
?
?
@
?
?
?
@
? @
?
@
? ?
?
Figure 1: Input tree t and encoded tree enc(t).
words, all the original non-nullary symbols from
? are now unary, @ is binary, and the original
nullary symbols from ? have their rank preserved.
We encode each tree of T? as a tree of T? as fol-
lows:
? enc(?) = ?() for every ? ? ?0,
? enc(?(t)) = ?(enc(t)) for every ? ? ?1 and
t ? T?, and
? for k ? 2, ? ? ?k, and t1, . . . , tk ? T?
enc(?(t1, . . . , tk)) =
?(@(enc(t1), . . .@(enc(tk?1), enc(tk)) ? ? ? )).
An example of the above encoding is illustrated
in Figure 1. Note that |enc(t)| ? O(|t|) for every
t ? T?. Furthermore, t can be easily reconstructed
from enc(t) in linear time.
Definition 1 LetM = (Q,?,S, ?, F ) be a WTA.
The encoded WTA enc(M) is (P,?,S, ??, F ?)
where
P = {[q] | q ? Q} ?
? {[w] |?k(?)q,uw 6= 0, u ? Q?, w ? Q+},
F ?([q]) = F (q) for every q ? Q, and the transi-
tions are constructed as follows:
(i) ??0(?)[q],? = ?0(?)q,? for every ? ? ?0,
(ii) ??1(?)[q],[w] = ?k(?)q,w for every ? ? ?k,
k ? 1, q ? Q, and w ? Qk, and
(iii) ??2(@)[qw],[q][w] = 1 for every [qw] ? P with
|w| ? 1 and q ? Q.
All remaining entries in F ? and ?? are 0. 2
Notice that each transition of enc(M) involves no
more than three states from P . Furthermore, we
have |enc(M)| ? O(|M |). The following result is
rather intuitive (Ho?gberg et al, 2009, Lemma 4.2);
its proof is therefore omitted.
3
Theorem 1 Let M = (Q,?,S, ?, F ) be a WTA,
and let M ? = enc(M). Then M(t) = M ?(enc(t))
for every t ? T?. 2
4 Bar-Hillel construction
The so-called Bar-Hillel construction was pro-
posed in (Bar-Hillel et al, 1964) to show that
the intersection of a context-free language and
a regular language is still a context-free lan-
guage. The proof of the result consisted in an
effective construction of a context-free grammar
Prod(G,N) from a context-free grammar G and
a finite automaton N , such that Prod(G,N) gen-
erates the intersection of the languages generated
by G and N .
It was later recognized that the Bar-Hillel con-
struction constitutes one of the foundations of the
theory of tabular parsing based on context-free
grammars. More precisely, by taking the finite
automaton N to be of some special kind, accept-
ing only a single string, the Bar-Hillel construction
provides a framework under which several well-
known tabular parsing algorithms can easily be de-
rived, that were proposed much later in the litera-
ture.
In this section we extend the Bar-Hillel con-
struction to WTA, with a similar purpose of es-
tablishing an abstract framework under which one
could easily derive parsing algorithms based on
these devices. In order to guarantee computational
efficiency, we avoid here stating the Bar-Hillel
construction for WTA with alphabets of arbitrary
rank. The next result therefore refers to WTA with
alphabet symbols of rank at most 2. These may,
but need not, be automata obtained through the bi-
nary encoding discussed in Section 3.
Definition 2 Let M = (Q,?,S, ?, F ) be a WTA
such that the maximum rank of a symbol in ? is 2,
and let N = (P,?0 \ {e},S, I, ?,G) be a WSA
over the same semiring. We construct the WTA
Prod(M,N) = (P ?Q? P,?,S, ??, F ?)
as follows:
(i) For every ? ? ?2, states p0, p1, p2 ? P , and
states q0, q1, q2 ? Q let
??2(?)(p0,q0,p2),(p0,q1,p1)(p1,q2,p2) = ?2(?)q0,q1q2 .
(ii) For every symbol ? ? ?1, states p0, p1 ? P ,
and states q0, q1 ? Q let
??1(?)(p0,q0,p1),(p0,q1,p1) = ?1(?)q0,q1 .
p0 p2
p0 p1 p1 p2
?
= = =
p0 p1
p0 p1
?
= =
p0 ? p1
?(p0, ?, p1)
p0 e p0
=
Figure 2: Information transport in the first and
third components of the states in our Bar-Hillel
construction.
(iii) For every symbol ? ? ?0, states p0, p1 ? P ,
and q ? Q let
??0(?)(p0,q,p1),? = ?0(?)q,? ? s
where
s =
{
?(p0, ?, p1) if ? 6= e
1 if ? = e and p0 = p1 .
(iv) F ?(p0, q, p1) = I(p0) ?F (q) ?G(p1) for every
p0, p1 ? P and q ? Q.
All remaining entries in ?? are 0. 2
Theorem 2 Let M and N be as in Definition 2,
and let M ? = Prod(M,N). If S is commutative,
thenM ?(t) = M(t) ?N(yd(t)) for every t ? T?.2
PROOF For a state q ? P ? Q ? P , we write qi
to denote its i-th component with i ? {1, 2, 3}.
Let t ? T? and r ? RunM ?(t) be a run of M ?
on t. We call the run r well-formed if for every
w ? Pos(t):
(i) if t(w) = e, then r(w)1 = r(w)3,
(ii) if t(w) /? ?0, then:
(a) r(w)1 = r(w1)1,
(b) r(w rkt(w))3 = r(w)3, and
(c) if rkt(w) = 2, then r(w1)3 = r(w2)1.
Note that no conditions are placed on the second
components of the states in r. We try to illustrate
the conditions in Figure 2.
A standard proof shows that wtM ?(r) = 0 for
all runs r ? RunM ?(t) that are not well-formed.
We now need to map runs of M ? back into ?cor-
responding? runs for M and N . Let us fix some
t ? T? and some well-formed run r ? RunM ?(t).
4
We define the run piM (r) ? RunM (t) by letting
piM (r)(w) = r(w)2,
for every w ? Pos(t). Let {w1, . . . , wn} =
{w? | w? ? Pos(t), t(w?) ? ?0 \ {e}}, with
w1 < ? ? ? < wn according to the lexico-
graphic order on Pos(t). We also define the run
piN (r) ? RunN (yd(t)) by letting
piN (r)(i? 1) = r(wi)1,
for every 1 ? i < n, and
piN (r)(n) = r(wn)3 .
Note that conversely every run of M on t and ev-
ery run of N on yd(t) yield a unique run of M ?
on t.
Now, we claim that
wtM ?(r) = wtM (piM (r)) ? wtN (piN (r))
for every well-formed run r ? RunM ?(t). To
prove the claim, let t = ?(t1, . . . , tk) for some
? ? ?k, k ? 2, and t1, . . . , tk ? T?. Moreover,
for every 1 ? i ? k let ri(w) = r(iw) for every
w ? Pos(ti). Note that ri ? RunM ?(ti) and that
ri is well-formed for every 1 ? i ? k.
For the induction base, let ? ? ?0; we can write
wtM ?(r)
= ??0(?)r(?),?
=
{
?0(?)r(?)2,? ? ?(r(?)1, ?, r(?)3) if ? 6= e
?0(?)r(?)2,? if ? = e
= wtM (piM (r)) ? wtN (piN (r)) .
In the induction step (i.e., k > 0) we have
wtM ?(r)
= ?
w?Pos(t)
rkt(w)=n
??n(t(w))r(w),r(w1)???r(wn)
= ??k(?)r(?),r(1)???r(k) ?
k?
i=1
wtM ?(ri) .
Using the fact that r is well-formed, commutativ-
ity, and the induction hypothesis, we obtain
= ?k(?)r(?)2,r(1)2???r(k)2 ?
?
k?
i=1
(
wtM (piM (ri)) ? wtN (piN (ri))
)
= wtM (pi2(r)) ? wtN (piN (r)) ,
where in the last step we have again used the fact
that r is well-formed. Using the auxiliary state-
ment wtM ?(r) = wtM (piM (r)) ?wtN (piN (r)), the
main proof now is easy.
M ?(t)
= ?
r?RunM? (t)
wtM ?(r) ? F ?(r(?))
= ?
r?RunM? (t)
r well-formed
wtM (piM (r)) ? wtN (piN (r)) ?
? I(r(?)1) ? F (r(?)2) ?G(r(?)3)
=
( ?
r?RunM (t)
wtM (r) ? F (r(?))
)
?
?
( ?
w=yd(t)
r?RunN (w)
I(r(0)) ? wtN (r) ?G(r(|w|))
)
= M(t) ?N(yd(t)) 
Let us analyze now the computational complex-
ity of a possible implementation of the construc-
tion in Definition 2. In step (i), we could restrict
the computation by considering only those transi-
tions inM satisfying ?2(?)q0,q1q2 6= 0, which pro-
vides a number of choices in O(|M |). Combined
with the choices for the states p0, p1, p2 of N ,
this provides O(|M | ? |P |3) non-zero transitions
in Prod(M,N). This is also a bound on the over-
all running time of step (i). Since we additionally
assume that weights can be multiplied in constant
time, it is not difficult to see that all of the remain-
ing steps can be accommodated within such a time
bound. We thus conclude that the construction in
Definition 2 can be implemented to run in time and
space O(|M | ? |P |3).
5 Parsing applications
In this section we discuss several applications of
the construction presented in Definition 2 that are
relevant for parsing based on WTA models.
5.1 Parse forest
Parsing is usually defined as the problem of con-
structing a suitable representation for the set of all
possible parse trees that are assigned to a given in-
put string w by some grammar model. The set of
all such parse trees is called parse forest. The ex-
tension of the Bar-Hillel construction that we have
5
presented in Section 4 can be easily adapted to ob-
tain a parsing algorithm for WTA models. This is
described in what follows.
First, we should represent the input string w in
a WSA that recognizes the language {w}. Such
an automaton has a state set P = {p0, . . . , p|w|}
and transition weights ?(pi?1, w(i), pi) = 1 for
each i with 1 ? i ? |w|. We also set I(p0) = 1
and F (p|w|) = 1. Setting all the weights to 1 for
a WSA N amounts to ignoring the weights, i.e.,
those weights will not contribute in any way when
applying the Bar-Hillel construction.
Assume now that M is our grammar model,
represented as a WTA. The WTA Prod(M,N)
constructed as in Definition 2 is not necessarily
trim, meaning that it might contain transitions
with non-zero weight that are never used in the
recognition. Techniques for eliminating such use-
less transitions are well-known, see for instance
(Ge?cseg and Steinby, 1984, Section II.6), and can
be easily implemented to run in linear time. Once
Prod(M,N) is trim, we have a device that rec-
ognizes all and only those trees that are assigned
by M to the input string w, and the weights of
those trees are preserved, as seen in Theorem 2.
The WTA Prod(M,N) can then be seen as a rep-
resentation of a parse forest for the input string w,
and we conclude that the construction in Defini-
tion 2, combined with some WTA reduction al-
gorithm, represents a parsing algorithm for WTA
models working in cubic time on the length of the
input string and in linear time on the size of the
grammar model.
More interestingly, from the framework devel-
oped in Section 4, one can also design more effi-
cient parsing algorithms based on WTA. Borrow-
ing from standard ideas developed in the litera-
ture for parsing based on context-free grammars,
one can specialize the construction in Definition 2
in such a way that the number of useless transi-
tions generated for Prod(M,N) is considerably
reduced, resulting in a more efficient construction.
This can be done by adopting some search strat-
egy that guides the construction of Prod(M,N)
using knowledge of the input string w as well as
knowledge about the source model M .
As an example, we can apply step (i) only on de-
mand, that is, we process a transition ??2(?)q0,q1q2
in Prod(M,N) only if we have already computed
non-zero transitions of the form ??k1(?1)q1,w1 and
??k2(?2)q2,w2 , for some ?1 ? ?k1 , w1 ? Qk1 and
?2 ? ?k2 , w2 ? Qk2 where Q is the state set
of Prod(M,N). The above amounts to a bottom-
up strategy that is also used in the Cocke-Kasami-
Younger recognition algorithm for context-free
grammars (Younger, 1967).
More sophisticated strategies are also possible.
For instance, one could adopt the Earley strategy
developed for context-free grammar parsing (Ear-
ley, 1970). In this case, parsing is carried out in
a top-down left-to-right fashion, and the binariza-
tion construction of Section 3 is carried out on the
flight. This has the additional advantage that it
would be possible to use WTA models that are not
restricted to the special normal form of Section 3,
still maintaining the cubic time complexity in the
length of the input string. We do not pursue this
idea any further in this paper, since our main goal
here is to outline an abstract framework for pars-
ing based on WTA models.
5.2 Probabilistic tree automata
Let us now look into specific semirings that are
relevant for statistical natural language process-
ing. The semiring of non-negative real numbers
is R?0 = (R?0,+, ?, 0, 1). For the remainder of
the section, let M = (Q,?,R?0, ?, F ) be a WTA
over R?0. M is convergent if
?
t?T?
M(t) < ?.
We say that M is a probabilistic tree automa-
ton (Ellis, 1971; Magidor and Moran, 1970),
or PTA for short, if ?k(?)q,q1???qk ? [0, 1]
and F (q) ? [0, 1], for every ? ? ?k and
q, q1, . . . , qk ? Q. In other words, in a PTA all
weights are in the range [0, 1] and can be inter-
preted as probabilities. For a PTA M we therefore
write pM (r) = wt(r) and pM (t) = M(t), for
each t ? T? and r ? RunM (t).
A PTA is proper if?q?Q F (q) = 1 and
?
???k,k?0,w?Qk
?k(?)q,w = 1
for every q ? Q. Since the set of symbols is finite,
we could have only required that the sum over all
weights as shown with w ? Qk equals 1 for every
q ? Q and ? ? ?k. A simple rescaling would then
be sufficient to arrive at our notion. Furthermore, a
PTA is consistent if ?t?T? pM (t) = 1. If a PTAis consistent, then pM is a probability distribution
over the set T?.
6
The WTAM is unambiguous if for every input
tree t ? T?, there exists at most one r ? RunM (t)
such that r(?) ? F and wtM (r) 6= 0. In other
words, in an unambiguous WTA, there exists at
most one successful run for each input tree. Fi-
nally, M is in final-state normal form if there ex-
ists a state qS ? Q such that
? F (qS) = 1,
? F (q) = 0 for every q ? Q \ {qS}, and
? ?k(?)q,w = 0 if w(i) = qS for some
1 ? i ? k.
We commonly denote the unique final state by qS .
For the following result we refer the reader
to (Droste et al, 2005, Lemma 4.8) and (Bozapa-
lidis, 1999, Lemma 22). The additional properties
mentioned in the items of it are easily seen.
Theorem 3 For every WTA M there exists an
equivalent WTA M ? in final-state normal form.
? If M is convergent (respectively, proper, con-
sistent), then M ? is such, too.
? If M is unambiguous, then M ? is
also unambiguous and for every
t ? T? and r ? RunM (t) we have
wtM ?(r?) = wtM (r) ? F (r(?)) where
r?(?) = qS and r?(w) = r(w) for every
w ? Pos(t) \ {?}. 2
It is not difficult to see that a proper PTA in
final-state normal form is always convergent.
In statistical parsing applications we use gram-
mar models that induce a probability distribution
on the set of parse trees. In these applications,
there is often the need to visit a parse tree with
highest probability, among those in the parse for-
est obtained from the input sentence. This imple-
ments a form of disambiguation, where the most
likely tree under the given model is selected, pre-
tending that it provides the most likely syntactic
analysis of the input string. In our setting, the
above approach reduces to the problem of ?unfold-
ing? a tree from a PTA Prod(M,N), that is as-
signed the highest probability.
In order to find efficient solutions for the above
problem, we make the following two assumptions.
? M is in final-state normal form. By Theo-
rem 3 this can be achieved without loss of
generality.
? M is unambiguous. This restrictive assump-
tion avoids the so-called ?spurious? ambigu-
ity, that would result in several computations
in the model for an individual parse tree.
It is not difficult to see that PTA satisfying these
1: Function BESTPARSE(M)
2: E ? ?
3: repeat
4: A ? {q |?k(?)q,q1???qk > 0, q /? E ,
q1, . . . , qk ? E}
5: for all q ? A do
6: ?(q)? max
???k,k?0
q1,...,qk?E
?k(?)q,q1???qk ?
k?
i=1
?(qi)
7: E ? E ? {argmax
q?A
?(q)}
8: until qS ? E
9: return ?(qS)
Figure 3: Search algorithm for the most probable
parse in an unambiguous PTAM in final-state nor-
mal form.
two properties are still more powerful than the
probabilistic context-free grammar models that are
commonly used in statistical natural language pro-
cessing.
Once more, we borrow from the literature on
parsing for context-free grammars, and adapt a
search algorithm developed by Knuth (1977); see
also (Nederhof, 2003). The basic idea here is
to generalize Dijkstra?s algorithm to compute the
shortest path in a weighted graph. The search al-
gorithm is presented in Figure 3.
The algorithm takes as input a trim PTA M that
recognizes at least one parse tree. We do not im-
pose any bound on the rank of the alphabet sym-
bols forM . Furthermore,M needs not be a proper
PTA. In order to simplify the presentation, we pro-
vide the algorithm in a form that returns the largest
probability assigned to some tree by M .
The algorithm records into the ?(q) variables
the largest probability found so far for a run that
brings M into state q, and stores these states into
an agenda A. States for which ?(q) becomes opti-
mal are popped from A and stored into a set E .
Choices are made on a greedy base. Note that
when a run has been found leading to an optimal
probability ?(q), from our assumption we know
that the associated tree has only one run that ends
up in state q.
Since E is initially empty (line 2), only weights
satisfying ?0(?)q,? > 0 are considered when line 4
is executed for the first time. Later on (line 7)
the largest probability is selected among all those
that can be computed at this time, and the set E is
populated. As a consequence, more states become
7
available in the agenda in the next iteration, and
new transitions can now be considered. The algo-
rithm ends when the largest probability has been
calculated for the unique final state qS .
We now analyze the computational complexity
of the algorithm in Figure 3. The ?repeat-until?
loop runs at most |Q| times. Entirely reprocess-
ing setA at each iteration would be too expensive.
We instead implement A as a priority heap and
maintain a clock for each weight ?k(?)q,q1???qk ,
initially set to k. Whenever a new optimal proba-
bility ?(q) becomes available through E , we decre-
ment the clock associated with each ?k(?)q,q1???qk
by d, in case d > 0 occurrences of q are found
in the string q1 ? ? ? qk. In this way, at each it-
eration of the ?repeat-until? loop, we can con-
sider only those weights ?k(?)q,q1???qk with asso-
ciated clock of zero, compute new values ?(q),
and update the heap. For each ?k(?)q,q1???qk > 0,
all clock updates and the computation of quan-
tity ?k(?)q,q1???qk ?
?k
i=1 ?(qi) (when the associ-
ated clock becomes zero) both take an amount of
time proportional to the length of the transition
itself. The overall time to execute these opera-
tions is therefore linear in |M |. Accounting for
the heap, the algorithm has overall running time
in O(|M |+ |Q| log|Q|).
The algorithm can be easily adapted to return a
tree having probability ?(qS), if we keep a record
of all transitions selected in the computation along
with links from a selected transition and all of the
previously selected transitions that have caused its
selection. If we drop the unambiguity assump-
tion for the PTA, then the problem of comput-
ing the best parse tree becomes NP-hard, through
a reduction from similar problems for finite au-
tomata (Casacuberta and de la Higuera, 2000). In
contrast, the problem of computing the probability
of all parse trees of a string, also called the inside
probability, can be solved in polynomial time in
most practical cases and will be addressed in Sub-
section 5.4.
5.3 Normalization
Consider the WTA Prod(M,N) obtained as in
Definition 2. If N is a WSA encoding an in-
put string w as in Subsection 5.1 and if M is a
proper and consistent PTA, then Prod(M,N) is
a PTA as well. However, in general Prod(M,N)
will not be proper, nor consistent. Properness and
consistency of Prod(M,N) are convenient in all
those applications where a statistical parsing mod-
ule needs to be coupled with other statistical mod-
ules, in such a way that the composition of the
probability spaces still induces a probability dis-
tribution. In this subsection we deal with the more
general problem of how to transform a WTA that
is convergent into a PTA that is proper and con-
sistent. This process is called normalization. The
normalization technique we propose here has been
previously explored, in the context of probabilis-
tic context-free grammars, in (Abney et al, 1999;
Chi, 1999; Nederhof and Satta, 2003).
We start by introducing some new notions. Let
us assume that M is a convergent WTA. For every
q ? Q, we define
wtM (q) =
?
t?T?,r?RunM (t)
r(?)=q
wtM (r) .
Note that quantity wtM (q) equals the sum of the
weights of all trees in T? that would be recognized
by M if we set F (q) = 1 and F (p) = 0 for each
p ? Q \ {q}, that is, if q is the unique final state
of M . It is not difficult to show that, since M is
convergent, the sum in the definition of wtM (q)
converges for each q ? Q. We will show in Sub-
section 5.4 that the quantities wtM (q) can be ap-
proximated to any desired precision.
To simplify the presentation, and without any
loss of generality, throughout this subsection we
assume that our WTA are in final-state normal
form. We can now introduce the normalization
technique.
Definition 3 Let M = (Q,?,R?0, ?, F ) be a
convergent WTA in final-state normal form. We
construct the WTA
Norm(M) =(Q,?,R?0, ??, F ) ,
where for every ? ? ?k, k ? 0, and
q, q1, . . . , qk ? Q
??k(?)q,q1???qk = ?k(?)q,q1???qk ?
? wtM (q1) ? . . . ? wtM (qk)wtM (q) . 2
We now show the claimed property for our
transformation.
Theorem 4 Let M be as in Definition 3, and let
M ? = Norm(M). Then M ? is a proper and
consistent PTA, and for every t ? T? we have
M ?(t) = M(t)wtM (qS) . 2
8
PROOF Clearly, M ? is again in final-state normal
form. An easy derivation shows that
wtM (q) =
?
???k
q1,...,qk?Q
?k(?)q,q1???qk ?
k?
i=1
wtM (qi)
for every q ? Q. Using the previous remark, we
obtain
?
???k,q1,...,qk?Q
??k(?)q,q1???qk
= ?
???k,q1,...,qk?Q
?k(?)q,q1???qk ?
? wtM (q1) ? . . . ? wtM (qk?)wtM (q)
=
?
???k,
q1,...,qk?Q
?k(?)q,q1???qk ?
k?
i=1
wtM (qi)
?
???k,
p1,...,pk?Q
?k(?)q,p1???pk ?
k?
i=1
wtM (pi)
= 1 ,
which proves that M ? is a proper PTA.
Next, we prove an auxiliary statement. Let
t = ?(t1, . . . , tk) for some ? ? ?k, k ? 0, and
t1, . . . , tk ? T?. We claim that
wtM ?(r) = wtM (r)wtM (r(?))
for every r ? RunM (t) = RunM ?(t). For ev-
ery 1 ? i ? k, let ri ? RunM (ti) be such that
ri(w) = r(iw) for every w ? Pos(ti). Then
wtM ?(r) =
?
w?Pos(t)
rkt(w)=n
??n(t(w))r(w),r(w1)???r(wn)
= ??k(?)r(?),r(1)???r(k) ?
k?
i=1
wtM ?(ri)
= ??k(?)r(?),r1(?)???rk(?) ?
k?
i=1
wtM (ri)
wtM (ri(?))
= ?k(?)r(?),r(1)???r(k) ? wtM (r1) ? ? ? ? ? wtM (rk)wtM (r(?))
= wtM (r)wtM (r(?)) .
Consequently,
M ?(t) = ?
r?RunM? (t)
r(?)=qS
wtM ?(r)
= ?
r?RunM (t)
r(?)=qS
wtM (r)
wtM (qS) =
M(t)
wtM (qS)
and
?
t?T?
M ?(t) = ?
t?T?,r?RunM? (t)
r(?)=qS
wtM ?(r)
= ?
t?T?,r?RunM (t)
r(?)=qS
wtM (r)
wtM (qS)
= wtM (qS)wtM (qS) = 1 ,
which prove the main statement and the consis-
tency of M ?, respectively. 
5.4 Probability mass of a state
AssumeM is a convergent WTA. We have defined
quantities wtM (q) for each q ? Q. Note that when
M is a proper PTA in final-state normal form, then
wtM (q) can be seen as the probability mass that
?rests? on state q. When dealing with such PTA,
we use the notation ZM (q) in place of wtM (q),
and call ZM the partition function of M . This
terminology is borrowed from the literature on ex-
ponential or Gibbs probabilistic models.
In the context of probabilistic context-free
grammars, the computation of the partition func-
tion has several applications, including the elim-
ination of epsilon rules (Abney et al, 1999) and
the computation of probabilistic distances between
probability distributions realized by these for-
malisms (Nederhof and Satta, 2008). Besides
what we have seen in Subsection 5.3, we will pro-
vide one more application of partition functions
for the computations of so-called prefix probabil-
ities in Subsection 5.5 We also add that, when
computed on the Bar-Hillel automata of Section 4,
the partition function provides the so-called inside
probabilities of (Graehl et al, 2008) for the given
states and substrings.
Let |Q| = n and let us assume an arbitrary or-
dering q1, . . . , qn for the states in Q. We can then
rewrite the definition of wtM (q) as
wtM (q) =
?
???k,k?0
qi1 ,...,qik?Q
?k(?)q,qi1 ???qik ?
k?
j=1
wtM (qij )
(see proof of Theorem 4). We rename wtM (qi)
with the unknown Xqi , 1 ? i ? n, and derive a
9
system of n nonlinear polynomial equations of the
form
Xqi =
?
???k,k?0
qi1 ,...,qik?Q
?k(?)q,qi1 ???qik ?Xqi1 ? . . . ?Xqik
= fqi(Xq1 , . . . , Xqn) , (1)
for each i with 1 ? i ? n.
Throughout this subsection, we will consider
solutions of the above system in the extended non-
negative real number semiring
R??0 = (R?0 ? {?},+, ?, 0, 1)
with the usual operations extended to ?. We
can write the system in (1) in the compact form
X = F (X), where we represent the unknowns
as a vector X = (Xq1 , . . . , Xqn) and F is a map-
ping of type (R??0)n ? (R??0)n consisting of the
polynomials fqi(X).
We denote the vector (0, . . . , 0) ? (R??0)n as
X0. Let X,X ? ? (R??0)n. We write X ? X ?
if Xqi ? X ?qi for every 1 ? i ? n. Sinceeach polynomial fqi(X) has coefficients repre-
sented by positive real numbers, it is not difficult
to see that, for each X,X ? ? (R??0)n, we have
F (X) ? F (X ?) whenever X0 ? X ? X ?. This
means that F is an order preserving, or monotone,
mapping.
We observe that ((R??0)n,?) is a complete
lattice with least element X0 and greatest el-
ement (?, . . . ,?). Since F is monotone on
a complete lattice, by the Knaster-Tarski theo-
rem (Knaster, 1928; Tarski, 1955) there exists a
least and a greatest fixed-point of F that are solu-
tions ofX = F (X).
The Kleene theorem states that the least fixed-
point solution of X = F (X) can be obtained
by iterating F starting with the least element X0.
In other words, the sequence Xk = F (Xk?1),
k = 1, 2, . . . converges to the least fixed-point so-
lution. Notice that each Xk provides an approxi-
mation for the partition function of M where only
trees of depth not larger than k are considered.
This means that limk??Xk converges to the par-
tition function of M , and the least fixed-point so-
lution is also the sought solution. Thus, we can
approximate wtM (q) with q ? Q to any degree by
iterating F a sufficiently large number of times.
The fixed-point iteration method discussed
above is also well-known in the numerical calcu-
lus literature, and is frequently applied to systems
of nonlinear equations in general, because it can
be easily implemented. When a number of stan-
dard conditions are met, each iteration of the algo-
rithm (corresponding to the value of k above) adds
a fixed number of bits to the precision of the ap-
proximated solution; see (Kelley, 1995) for further
discussion.
Systems of the form X = F (X) where all
fqi(X) are polynomials with nonnegative real co-
efficients are called monotone system of poly-
nomials. Monotone systems of polynomials as-
sociated with proper PTA have been specifically
investigated in (Etessami and Yannakakis, 2005)
and (Kiefer et al, 2007), where worst case results
on exponential rate of convergence are reported
for the fixed-point method.
5.5 Prefix probability
In this subsection we deal with one more applica-
tion of the Bar-Hillel technique presented in Sec-
tion 4. We show how to compute the so-called
prefix probabilities, that is, the probability that a
tree recognized by a PTA generates a string start-
ing with a given prefix. Such probabilities have
several applications in language modeling. As an
example, prefix probabilities can be used to com-
pute the probability distribution on the terminal
symbol that follows a given prefix (under the given
model).
For probabilistic context-free grammars, the
problem of the computation of prefix probabili-
ties has been solved in (Jelinek et al, 1992); see
also (Persoon and Fu, 1975). The approach we
propose here, originally formulated for probabilis-
tic context-free grammars in (Nederhof and Satta,
2003; Nederhof and Satta, 2009), is more abstract
than the previous ones, since it entirely rests on
properties of the Bar-Hillel construction that we
have already proved in Section 4.
Let M = (Q,?,R?0, ?, F ) be a proper
and consistent PTA in final-state normal form,
? = ?0 \ {e}, and let u ? ?+ be some string.
We assume here that M is in the binary form
discussed in Section 3. In addition, we assume
that M has been preprocessed in order to remove
from its recognized trees all of the unary branches
as well as those branches that generate the null
string ?. Although we do not discuss this con-
struction at length in this paper, the result follows
from a transformation casting weighted context-
free grammars into Chomsky Normal Form (Fu
10
and Huang, 1972; Abney et al, 1999).
We define
Pref(M,u) = {t | t ? T?, M(t) > 0,
yd(t) = uv, v ? ??} .
The prefix probability of u underM is defined as
?
t?Pref(M,u)
pM (t) .
Let |u| = n. We define a WSA Nu with state
set P = {p0, . . . , pn} and transition weights
?(pi?1, u(i), pi) = 1 for each i with 1 ? i ? n,
and ?(pn, ?, pn) = 1 for each ? ? ?. We also
set I(p0) = 1 and F (pn) = 1. It is easy to see
that Nu recognizes the language {uv | v ? ??}.
Furthermore, the PTA Mp = Prod(M,Nu) spec-
ified as in Definition 2 recognizes the desired tree
set Pref(M,u), and it preserves the weights of
those trees with respect to M . We therefore con-
clude that ZMp(qS) is the prefix probability of u
under M . Prefix probabilities can then be approx-
imated using the fixed-point iteration method of
Subsection 5.4. Rather than using an approxima-
tion method, we discuss in what follows how the
prefix probabilities can be exactly computed.
Let us consider more closely the product au-
tomaton Mp, assuming that it is trim. Each state
of Mp has the form pi = (pi, q, pj), pi, pj ? P and
q ? Q, with i ? j. We distinguish three, mutually
exclusive cases.
(i) j < n: From our assumption that M (and
thus Mp) does not have unary or ? branches,
it is not difficult to see that all ZMp(pi) can be
exactly computed in time O((j ? i)3).
(ii) i = j = n: We have pi = (pn, q, pn).
Then the equations for ZMp(pi) exactly
mirror the equations for ZM (q), and
ZMp(pi) = ZMp(q). Because M is proper
and consistent, this means that ZMp(pi) = 1.
(iii) i < j = n: A close inspection of Definition 2
reveals that in this case the equations (1) are
all linear, assuming that we have already re-
placed the solutions from (i) and (ii) above
into the system. This is because any weight
?2(?)pi0,pi1pi > 0 in Mp with pi = (pi, q, pn)
and i < n must have (pi1)3 < n. Quanti-
ties ZMp(pi) can then be exactly computed as
the solution of a linear system of equations in
time O(n3).
Putting together all of the observations above,
we obtain that for a proper and consistent PTA that
has been preprocessed, the prefix probability of u
can be computed in cubic time in the length of the
prefix itself.
6 Concluding remarks
In this paper we have extended the Bar-Hillel con-
struction to WTA, closely following the method-
ology proposed in (Nederhof and Satta, 2003) for
weighted context-free grammars. Based on the ob-
tained framework, we have derived several parsing
algorithms for WTA, under the assumption that the
input is a string rather than a tree.
As already remarked in the introduction, WTA
are richer models than weighted context-free
grammar, since the formers use hidden states in
the recognition of trees. This feature makes it
possible to define a product automaton in Defini-
tion 2 that generates exactly those trees of interest
for the input string. In contrast, in the context-
free grammar case the Bar-Hillel technique pro-
vides trees that must be mapped to the tree of in-
terest using some homomorphism. For the same
reason, one cannot directly convert WTA into
weighted context-free grammars and then apply
existing parsing algorithms for the latter formal-
ism, unless the alphabet of nonterminal symbols
is changed. Finally, our main motivation in de-
veloping a framework specifically based on WTA
is that this can be extended to classes of weighted
tree transducers, in order to deal with computa-
tional problems that arise in machine translation
applications. We leave this for future work.
Acknowledgments
The first author has been supported by the Minis-
terio de Educacio?n y Ciencia (MEC) under grant
JDCI-2007-760. The second author has been par-
tially supported by MIUR under project PRIN No.
2007TJNZRE 002.
References
S. Abney, D. McAllester, and F. Pereira. 1999. Relat-
ing probabilistic grammars and automata. In 37th
Annual Meeting of the Association for Computa-
tional Linguistics, Proceedings of the Conference,
pages 542?549, Maryland, USA, June.
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On
formal properties of simple phrase structure gram-
mars. In Y. Bar-Hillel, editor, Language and Infor-
mation: Selected Essays on their Theory and Appli-
cation, chapter 9, pages 116?150. Addison-Wesley,
Reading, Massachusetts.
11
J. Berstel and C. Reutenauer. 1982. Recognizable for-
mal power series on trees. Theoret. Comput. Sci.,
18(2):115?148.
B. Borchardt. 2005. The Theory of Recognizable Tree
Series. Ph.D. thesis, Technische Universita?t Dres-
den.
S. Bozapalidis. 1999. Equational elements in additive
algebras. Theory Comput. Systems, 32(1):1?33.
J. Carme, J. Niehren, and M. Tommasi. 2004. Query-
ing unranked trees with stepwise tree automata. In
Proc. RTA, volume 3091 of LNCS, pages 105?118.
Springer.
F. Casacuberta and C. de la Higuera. 2000. Com-
putational complexity of problems on probabilis-
tic grammars and transducers. In L. Oliveira, edi-
tor, Grammatical Inference: Algorithms and Appli-
cations; 5th International Colloquium, ICGI 2000,
pages 15?24. Springer.
Z. Chi. 1999. Statistical properties of probabilistic
context-free grammars. Computational Linguistics,
25(1):131?160.
M. Droste, C. Pech, and H. Vogler. 2005. A Kleene
theorem for weighted tree automata. Theory Com-
put. Systems, 38(1):1?38.
J. Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 13(2):94?102,
February.
S. Eilenberg. 1974. Automata, Languages, and Ma-
chines, volume 59 of Pure and Applied Math. Aca-
demic Press.
C. A. Ellis. 1971. Probabilistic tree automata. Infor-
mation and Control, 19(5):401?416.
Z. E?sik and W. Kuich. 2003. Formal tree series. J.
Autom. Lang. Combin., 8(2):219?285.
K. Etessami and M. Yannakakis. 2005. Recursive
Markov chains, stochastic grammars, and monotone
systems of nonlinear equations. In 22nd Interna-
tional Symposium on Theoretical Aspects of Com-
puter Science, volume 3404 of Lecture Notes in
Computer Science, pages 340?352, Stuttgart, Ger-
many. Springer-Verlag.
K.S. Fu and T. Huang. 1972. Stochastic grammars and
languages. International Journal of Computer and
Information Sciences, 1(2):135?170.
F. Ge?cseg and M. Steinby. 1984. Tree Automata.
Akade?miai Kiado?, Budapest.
J. Graehl, K. Knight, and J. May. 2008. Training tree
transducers. Comput. Linguist., 34(3):391?427.
J. Ho?gberg, A. Maletti, and H. Vogler. 2009. Bisim-
ulation minimisation of weighted automata on un-
ranked trees. Fundam. Inform. to appear.
F. Jelinek, J.D. Lafferty, and R.L. Mercer. 1992. Basic
methods of probabilistic context free grammars. In
P. Laface and R. De Mori, editors, Speech Recogni-
tion and Understanding ? Recent Advances, Trends
and Applications, pages 345?360. Springer-Verlag.
M. Johnson. 1998. PCFG models of linguistic
tree representations. Computational Linguistics,
24(4):613?632.
C. T. Kelley. 1995. Iterative Methods for Linear and
Nonlinear Equations. Society for Industrial and Ap-
plied Mathematics, Philadelphia, PA.
S. Kiefer, M. Luttenberger, and J. Esparza. 2007. On
the convergence of Newton?s method for monotone
systems of polynomial equations. In Proceedings of
the 39th ACM Symposium on Theory of Computing,
pages 217?266.
B. Knaster. 1928. Un the?ore`me sur les fonctions
d?ensembles. Ann. Soc. Polon. Math., 6:133?134.
D. E. Knuth. 1977. A generalization of Dijkstra?s al-
gorithm. Information Processing Letters, 6(1):1?5,
February.
D. E. Knuth. 1997. Fundamental Algorithms. The Art
of Computer Programming. Addison Wesley, 3rd
edition.
M. Magidor and G. Moran. 1970. Probabilistic tree
automata and context free languages. Israel Journal
of Mathematics, 8(4):340?348.
M.-J. Nederhof and G. Satta. 2003. Probabilistic pars-
ing as intersection. In 8th International Workshop
on Parsing Technologies, pages 137?148, LORIA,
Nancy, France, April.
M.-J. Nederhof and G. Satta. 2008. Computation of
distances for regular and context-free probabilistic
languages. Theoretical Computer Science, 395(2-
3):235?254.
M.-J. Nederhof and G. Satta. 2009. Computing parti-
tion functions of PCFGs. Research on Language &
Computation, 6(2):139?162.
M.-J. Nederhof. 2003. Weighted deductive parsing
and Knuth?s algorithm. Computational Linguistics,
29(1):135?143.
E. Persoon and K.S. Fu. 1975. Sequential classi-
fication of strings generated by SCFG?s. Interna-
tional Journal of Computer and Information Sci-
ences, 4(3):205?217.
M. P. Schu?tzenberger. 1961. On the definition of a
family of automata. Information and Control, 4(2?
3):245?270.
A. Tarski. 1955. A lattice-theoretical fixpoint theorem
and its applications. Pacific J. Math., 5(2):285?309.
D. H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10:189?208.
12
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 808?817,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Composing extended top-down tree transducers?
Aure?lie Lagoutte
E?cole normale supe?rieure de Cachan, De?partement Informatique
alagoutt@dptinfo.ens-cachan.fr
Fabienne Braune and Daniel Quernheim and Andreas Maletti
University of Stuttgart, Institute for Natural Language Processing
{braunefe,daniel,maletti}@ims.uni-stuttgart.de
Abstract
A composition procedure for linear and
nondeleting extended top-down tree trans-
ducers is presented. It is demonstrated that
the new procedure is more widely applica-
ble than the existing methods. In general,
the result of the composition is an extended
top-down tree transducer that is no longer
linear or nondeleting, but in a number of
cases these properties can easily be recov-
ered by a post-processing step.
1 Introduction
Tree-based translation models such as syn-
chronous tree substitution grammars (Eisner,
2003; Shieber, 2004) or multi bottom-up tree
transducers (Lilin, 1978; Engelfriet et al 2009;
Maletti, 2010; Maletti, 2011) are used for sev-
eral aspects of syntax-based machine transla-
tion (Knight and Graehl, 2005). Here we consider
the extended top-down tree transducer (XTOP),
which was studied in (Arnold and Dauchet,
1982; Knight, 2007; Graehl et al 2008; Graehl
et al 2009) and implemented in the toolkit
TIBURON (May and Knight, 2006; May, 2010).
Specifically, we investigate compositions of linear
and nondeleting XTOPs (ln-XTOP). Arnold and
Dauchet (1982) showed that ln-XTOPs compute
a class of transformations that is not closed under
composition, so we cannot compose two arbitrary
ln-XTOPs into a single ln-XTOP. However, we
will show that ln-XTOPs can be composed into a
(not necessarily linear or nondeleting) XTOP. To
illustrate the use of ln-XTOPs in machine transla-
tion, we consider the following English sentence
together with a German reference translation:
? All authors were financially supported by the EMMY
NOETHER project MA / 4959 / 1-1 of the German Research
Foundation (DFG).
RC
PREL
that
C
NP VP
7?
C
NP VP
C
NP VP
VAUX VPART NP
7?
C
NP VP
VAUX NP VPART
Figure 1: Word drop [top] and reordering [bottom].
The newswire reported yesterday that the Serbs have
completed the negotiations.
Gestern [Yesterday] berichtete [reported] die [the]
Nachrichtenagentur [newswire] die [the] Serben
[Serbs] ha?tten [would have] die [the] Verhandlungen
[negotiations] beendet [completed].
The relation between them can be described
(Yamada and Knight, 2001) by three operations:
drop of the relative pronoun, movement of the
participle to end of the clause, and word-to-word
translation. Figure 1 shows the first two oper-
ations, and Figure 2 shows ln-XTOP rules per-
forming them. Let us now informally describe
the execution of an ln-XTOP on the top rule ?
of Figure 2. In general, ln-XTOPs process an in-
put tree from the root towards the leaves using
a set of rules and states. The state p in the left-
hand side of ? controls the particular operation of
Figure 1 [top]. Once the operation has been per-
formed, control is passed to states pNP and pVP,
which use their own rules to process the remain-
ing input subtree governed by the variable below
them (see Figure 2). In the same fashion, an ln-
XTOP containing the bottom rule of Figure 2 re-
orders the English verbal complex.
In this way we model the word drop by an ln-
XTOP M and reordering by an ln-XTOP N . The
syntactic properties of linearity and nondeletion
yield nice algorithmic properties, and the mod-
808
pRC
PREL
that
C
y1 y2
?
C
pNP
y1
pVP
y2
q
C
z1 VP
z2 z3 z4
?
C
qNP
z1
VP
qVA
z2
qVP
z4
qNP
z3
Figure 2: XTOP rules for the operations of Figure 1.
ular approach is desirable for better design and
parametrization of the translation model (May et
al., 2010). Composition allows us to recombine
those parts into one device modeling the whole
translation. In particular, it gives all parts the
chance to vote at the same time. This is especially
important if pruning is used because it might oth-
erwise exclude candidates that score low in one
part but well in others (May et al 2010).
Because ln-XTOP is not closed under compo-
sition, the composition ofM andN might be out-
side ln-XTOP. These cases have been identified
by Arnold and Dauchet (1982) as infinitely ?over-
lapping cuts?, which occur when the right-hand
sides of M and the left-hand sides of N are un-
boundedly overlapping. This can be purely syn-
tactic (for a given ln-XTOP) or semantic (inher-
ent in all ln-XTOPs for a given transformation).
Despite the general impossibility, several strate-
gies have been developed: (i) Extension of the
model (Maletti, 2010; Maletti, 2011), (ii) online
composition (May et al 2010), and (iii) restric-
tion of the model, which we follow. Composi-
tions of subclasses in which the XTOP N has at
most one input symbol in its left-hand sides have
already been studied in (Engelfriet, 1975; Baker,
1979; Maletti and Vogler, 2010). Such compo-
sitions are implemented in the toolkit TIBURON.
However, there are translation tasks in which the
used XTOPs do not fulfill this requirement. Sup-
pose that we simply want to compose the rules of
Figure 2, The bottom rule does not satisfy the re-
quirement that there is at most one input symbol
in the left-hand side.
We will demonstrate how to compose two lin-
ear and nondeleting XTOPs into a single XTOP,
which might however no longer be linear or non-
deleting. However, when the syntactic form of
?(?)
q(1)
x(11)1
?(2)
?(21) q(22)
x(221)2
?(3)
?(31)
p(311)
x(3111)3
?
q
x1
? ?
?
p
x3
Figure 3: Linear normalized tree t ? T?(Q(X)) [left]
and t[?]2 [right] with var(t) = {x1, x2, x3}. The posi-
tions are indicated in t as superscripts. The subtree t|2
is ?(?, q(x2)).
the composed XTOP has only bounded overlap-
ping cuts, post-processing will get rid of them
and restore an ln-XTOP. In the remaining cases,
in which unbounded overlapping is necessary or
occurs in the syntactic form but would not be nec-
essary, we will compute an XTOP. This is still
an improvement on the existing methods that just
fail. Since general XTOPs are implemented in
TIBURON and the new composition covers (essen-
tially) all cases currently possible, our new com-
position procedure could replace the existing one
in TIBURON. Our approach to composition is the
same as in (Engelfriet, 1975; Baker, 1979; Maletti
and Vogler, 2010): We simply parse the right-
hand sides of the XTOP M with the left-hand
sides of the XTOP N . However, to facilitate this
approach we have to adjust the XTOPs M and N
in two pre-processing steps. In a first step we cut
left-hand sides of rules of N into smaller pieces,
which might introduce non-linearity and deletion
into N . In certain cases, this can also intro-
duce finite look-ahead (Engelfriet, 1977; Graehl
et al 2009). To compensate, we expand the rules
of M slightly. Section 4 explains those prepa-
rations. Next, we compose the prepared XTOPs
as usual and obtain a single XTOP computing the
composition of the transformations computed by
M and N (see Section 5). Finally, we apply a
post-processing step to expand rules to reobtain
linearity and nondeletion. Clearly, this cannot be
successful in all cases, but often removes the non-
linearity introduced in the pre-processing step.
2 Preliminaries
Our trees have labels taken from an alphabet ?
of symbols, and in addition, leaves might be
labeled by elements of the countably infinite
809
?x1 ?
?
? ? x2
?
7?
?
? ?
?
? ? x2
?
?[
?
? x3
Figure 4: Substitution where ?(x1) = ?, ?(x2) = x2,
and ?(x3) = ?(?(?, ?, x2)).
set X = {x1, x2, . . . } of formal variables. For-
mally, for every V ? X the set T?(V ) of
?-trees with V -leaves is the smallest set such that
V ? T?(V ) and ?(t1, . . . , tk) ? T?(V ) for all
k ? N, ? ? ?, and t1, . . . , tk ? T?(V ). To avoid
excessive universal quantifications, we drop them
if they are obvious from the context.
For each tree t ? T?(X) we identify nodes by
positions. The root of t has position ? and the po-
sition iw with i ? N and w ? N? addresses the
position w in the i-th direct subtree at the root.
The set of all positions in t is pos(t). We write
t(w) for the label (taken from ? ?X) of t at po-
sition w ? pos(t). Similarly, we use
? t|w to address the subtree of t that is rooted
in position w, and
? t[u]w to represent the tree that is ob-
tained from replacing the subtree t|w at w
by u ? T?(X).
For a given set L ? ? ?X of labels, we let
posL(t) = {w ? pos(t) | t(w) ? L}
be the set of all positions whose label belongs
to L. We also write posl(t) instead of pos{l}(t).
The tree t ? T?(V ) is linear if |posx(t)| ? 1 for
every x ? X . Moreover,
var(t) = {x ? X | posx(t) 6= ?}
collects all variables that occur in t. If the vari-
ables occur in the order x1, x2, . . . in a pre-order
traversal of the tree t, then t is normalized. Given
a finite set Q, we write Q(T ) with T ? T?(X)
for the set {q(t) | q ? Q, t ? T}. We will treat
elements of Q(T ) as special trees of T??Q(X).
The previous notions are illustrated in Figure 3.
A substitution ? is a mapping ? : X ? T?(X).
When applied to a tree t ? T?(X), it will return
the tree t?, which is obtained from t by replacing
all occurrences of x ? X (in parallel) by ?(x).
This can be defined recursively by x? = ?(x) for
all x ? X and ?(t1, . . . , tk)? = ?(t1?, . . . , tk?)
qS
S
x1 VP
x2 x3
?
S?
qV
x2
qNP
x1
qNP
x1
t
qS
S
t1
VP
t2 t3
?
t
S?
qV
t2
qNP
t1
qNP
t1
Figure 5: Rule and its use in a derivation step.
for all ? ? ? and t1, . . . , tk ? T?(X). The effect
of a substitution is displayed in Figure 4. Two
substitutions ?, ?? : X ? T?(X) can be com-
posed to form a substitution ??? : X ? T?(X)
such that ???(x) = ?(x)?? for every x ? X .
Next, we define two notions of compatibility
for trees. Let t, t? ? T?(X) be two trees. If there
exists a substitution ? such that t? = t?, then t? is
an instance of t. Note that this relation is not sym-
metric. A unifier ? for t and t? is a substitution ?
such that t? = t??. The unifier ? is a most gen-
eral unifier (short: mgu) for t and t? if for every
unifier ??? for t and t? there exists a substitution ??
such that ??? = ???. The set mgu(t, t?) is the set of
all mgus for t and t?. Most general unifiers can be
computed efficiently (Robinson, 1965; Martelli
and Montanari, 1982) and all mgus for t and t?
are equal up to a variable renaming.
Example 1. Let t = ?(x1, ?(?(?, ?, x2))) and
t? = ?(?, x3). Then mgu(t, t?) contains ? such
that ?(x1) = ? and ?(x3) = ?(?(?, ?, x2)). Fig-
ure 4 illustrates the unification.
3 The model
The discussed model in this contribution is an
extension of the classical top-down tree trans-
ducer, which was introduced by Rounds (1970)
and Thatcher (1970). The extended top-down
tree transducer with finite look-ahead or just
XTOPF and its variations were studied in (Arnold
and Dauchet, 1982; Knight and Graehl, 2005;
810
qS
S
x1 VP
x2 x3
S?
qV
x2
qNP
x1
qNP
x3
?
qS
S?
x2 x1 x3
S
qNP
x1
VP
qV
x2
qNP
x3
?
Figure 6: Rule [left] and reversed rule [right].
Knight, 2007; Graehl et al 2008; Graehl et
al., 2009). Formally, an extended top-down tree
transducer with finite look-ahead (XTOPF) is a
system M = (Q,?,?, I, R, c) where
? Q is a finite set of states,
? ? and ? are alphabets of input and output
symbols, respectively,
? I ? Q is a set of initial states,
? R is a finite set of (rewrite) rules of the form
` ? r where ` ? Q(T?(X)) is linear and
r ? T?(Q(var(`))), and
? c : R ? X ? T?(X) assigns a look-ahead
restriction to each rule and variable such that
c(?, x) is linear for each ? ? R and x ? X .
The XTOPF M is linear (respectively, nondelet-
ing) if r is linear (respectively, var(r) = var(`))
for every rule ` ? r ? R. It has no look-ahead
(or it is an XTOP) if c(?, x) ? X for all rules
? ? R and x ? X . In this case, we drop the look-
ahead component c from the description. A rule
` ? r ? R is consuming (respectively, produc-
ing) if pos?(`) 6= ? (respectively, pos?(r) 6= ?).
We let Lhs(M) = {l | ?q, r : q(l)? r ? R}.
Let M = (Q,?,?, I, R, c) be an XTOPF. In
order to facilitate composition, we define senten-
tial forms more generally than immediately nec-
essary. Let ?? and ?? be such that ? ? ??
and ? ? ??. To keep the presentation sim-
ple, we assume that Q ? (?? ? ??) = ?. A
sentential form of M (using ?? and ??) is a
tree of SF(M) = T??(Q(T??)). For every
?, ? ? SF(M), we write ? ?M ? if there exist a
positionw ? posQ(?), a rule ? = `? r ? R, and
a substitution ? : X ? T?? such that ?(x) is an in-
stance of c(?, x) for every x ? X and ? = ?[`?]w
and ? = ?[r?]w. If the applicable rules are re-
stricted to a certain subset R? ? R, then we also
write ? ?R? ?. Figure 5 illustrates a derivation
step. The tree transformation computed by M is
?M = {(t, u) ? T? ? T? | ?q ? I : q(t)?
?
M u}
where ??M is the reflexive, transitive closure
of?M . It can easily be verified that the definition
p
C
y1 y2
?
RC
PREL
that
C
pNP
y1
pVP
y2
Figure 7: Top rule of Figure 2 reversed.
of ?M is independent of the choice of ?? and ??.
Moreover, it is known (Graehl et al 2009) that
each XTOPF can be transformed into an equiva-
lent XTOP preserving both linearity and nondele-
tion. However, the notion of XTOPF will be con-
venient in our composition construction. A de-
tailed exposition to XTOPs is presented by Arnold
and Dauchet (1982) and Graehl et al(2009).
A linear and nondeleting XTOP M with
rules R can easily be reversed to obtain
a linear and nondeleting XTOP M?1 with
rules R?1, which computes the inverse transfor-
mation ?M?1 = ?
?1
M , by reversing all its rules.
A (suitable) rule is reversed by exchanging the
locations of the states. More precisely, given
a rule q(l) ? r ? R, we obtain the rule
q(r?) ? l? of R?1, where l? = l? and r? is the
unique tree such that there exists a substitution
? : X ? Q(X) with ?(x) ? Q({x}) for every
x ? X and r = r??. Figure 6 displays a rule
and its corresponding reversed rule. The reversed
form of the XTOP rule modeling the insertion op-
eration in Figure 2 is displayed in Figure 7.
Finally, let us formally define composition.
The XTOP M computes the tree transformation
?M ? T? ? T?. Given another XTOP N that
computes a tree transformation ?N ? T? ? T?,
we might be interested in the tree transforma-
tion computed by the composition of M and N
(i.e., running M first and then N ). Formally, the
composition ?M ; ?N of the tree transformations
?M and ?N is defined by
?M ; ?N = {(s, u) | ?t : (s, t) ? ?M , (t, u) ? ?N}
and we often also use the notion ?composition? for
XTOP with the expectation that the composition
of M and N computes exactly ?M ; ?N .
4 Pre-processing
We want to compose two linear and nondelet-
ing XTOPs M = (P,?,?, IM , RM ) and
811
LHS(M?1) LHS(N)
C
y1 y2
C
z1 VP
z2 z3 z4
Figure 8: Incompatible left-hand sides of Example 3.
N = (Q,?,?, IN , RN ). Before we actually per-
form the composition, we will prepare M and N
in two pre-processing steps. After these two steps,
the composition is very simple. To avoid com-
plications, we assume that (i) all rules of M are
producing and (ii) all rules of N are consuming.
For convenience, we also assume that the XTOPs
M and N only use variables of the disjoint sets
Y ? X and Z ? X , respectively.
4.1 Compatibility
In the existing composition results for subclasses
of XTOPs (Engelfriet, 1975; Baker, 1979; Maletti
and Vogler, 2010) the XTOP N has at most one
input symbol in its left-hand sides. This restric-
tion allows us to match rule applications of N to
positions in the right-hand sides of M . Namely,
for each output symbol in a right-hand side of M ,
we can select a rule of N that can consume that
output symbol. To achieve a similar decompo-
sition strategy in our more general setup, we in-
troduce a compatibility requirement on right-hand
sides of M and left-hand sides of N . Roughly
speaking, we require that the left-hand sides of N
are small enough to completely process right-
hand sides of M . However, a comparison of
left- and right-hand sides is complicated by the
fact that their shape is different (left-hand sides
have a state at the root, whereas right-hand sides
have states in front of the variables). We avoid
these complications by considering reversed rules
of M . Thus, an original right-hand side of M is
now a left-hand side in the reversed rules and thus
has the right format for a comparison. Recall that
Lhs(N) contains all left-hand sides of the rules
of N , in which the state at the root was removed.
Definition 2. The XTOP N is compatible to M
if ?(Y ) ? X for all unifiers ? ? mgu(l1|w, l2)
between a subtree at a ?-labeled position
w ? pos?(l1) in a left-hand side l1 ? Lhs(M
?1)
and a left-hand side l2 ? Lhs(N).
Rule of M?1 Rule of N
?
p1
y1
p2
y2
? ?
p
?
y1 y2
q
?
? ?
z1 z2
?
?
q1
z1
q2
z2
Figure 9: Rules used in Example 5.
Intuitively, for every ?-labeled position w in a
right-hand side r1 of M and any left-hand side l2
of N , we require (ignoring the states) that either
(i) r1|w and l2 are not unifiable or (ii) r1|w is an
instance of l2.
Example 3. The XTOPs for the English-to-
German translation task in the Introduction are
not compatible. This can be observed on the
left-hand side l1 ? Lhs(M?1) of Figure 7
and the left-hand side l2 ? Lhs(N) of Fig-
ure 2[bottom]. These two left-hand sides are il-
lustrated in Figure 8. Between them there is an
mgu such that ?(Y ) 6? X (e.g., ?(y1) = z1 and
?(y2) = VP(z2, z3, z4) is such an mgu).
Theorem 4. There exists an XTOPF N ? that is
equivalent to N and compatible with M .
Proof. We achieve compatibility by cutting of-
fending rules of the XTOP N into smaller pieces.
Unfortunately, both linearity and nondeletion
of N might be lost in the process. We first let
N ? = (Q,?,?, IN , RN , cN ) be the XTOPF such
that cN (?, x) = x for every ? ? RN and x ? X .
If N ? is compatible with M , then we are done.
Otherwise, let l1 ? Lhs(M?1) be a left-hand side,
q(l2) ? r2 ? RN be a rule, and w ? pos?(l1)
be a position such that ?(y) /? X for some
? ? mgu(l1|w, l2) and y ? Y . Let v ? posy(l1|w)
be the unique position of y in l1|w.
Now we have to distinguish two cases: (i) Ei-
ther var(l2|v) = ? and there is no leaf in r2 la-
beled by a symbol from ?. In this case, we have
to introduce deletion and look-ahead into N ?. We
replace the old rule ? = q(l2) ? r2 by the new
rule ?? = q(l2[z]v) ? r2, where z ? X \ var(l2)
is a variable that does not appear in l2. In addition,
we let cN (??, z) = l2|v and cN (??, x) = cN (?, x)
for all x ? X \ {z}.
(ii) Otherwise, let V ? var(l2|v) be a maximal
set such that there exists a minimal (with respect
to the prefix order) position w? ? pos(r2) with
812
Another rule of N
q
?
z1 ?
z2 z3
?
?
q1
z1
q2
z2
q3
z3
Figure 10: Additional rule used in Example 5.
var(r2|w?) ? var(l2|v) and var(r2[?]w?)?V = ?,
where ? ? ? is arbitrary. Let z ? X \ var(l2) be
a fresh variable, q? be a new state of N , and
V ? = var(l2|v) \ V . We replace the rule
? = q(l2)? r2 of RN by
?1 = q(l2[z]v)? trans(r2)[q
?(z)]w?
?2 = q
?(l2|v)? r2|w? .
The look-ahead for z is trivial and other-
wise we simply copy the old look-ahead, so
cN (?1, z) = z and cN (?1, x) = cN (?, x) for all
x ? X \ {z}. Moreover, cN (?2, x) = cN (?, x)
for all x ? X . The mapping ?trans? is given for
t = ?(t1, . . . , tk) and q??(z??) ? Q(Z) by
trans(t) = ?(trans(t1), . . . , trans(tk))
trans(q??(z??)) =
{
?l2|v, q??, v??(z) if z?? ? V ?
q??(z??) otherwise,
where v? = posz??(l2|v).
Finally, we collect all newly generated states
of the form ?l, q, v? in Ql and for every such
state with l = ?(l1, . . . , lk) and v = iw, let
l? = ?(z1, . . . , zk) and
?l, q, v?(l?)?
{
q(zi) if w = ?
?li, q, w?(zi) otherwise
be a new rule of N without look-ahead.
Overall, we run the procedure until N ? is com-
patible with M . The procedure eventually ter-
minates since the left-hand sides of the newly
added rules are always smaller than the replaced
rules. Moreover, each step preserves the seman-
tics of N ?, which completes the proof.
We note that the look-ahead ofN ? after the con-
struction used in the proof of Theorem 4 is either
trivial (i.e., a variable) or a ground tree (i.e., a tree
without variables). Let us illustrate the construc-
tion used in the proof of Theorem 4.
?1 :
q
C
z1 z
?
C
qNP
z1
q?
z
?2 :
q?
VP
z2 z3 z4
?
VP
qVA
z2
qVP
z4
qNP
z3
Figure 11: Rules replacing the rule in Figure 7.
Example 5. Let us consider the rules illustrated
in Figure 9. We might first note that y1 has to
be unified with ?. Since ? does not contain any
variables and the right-hand side of the rule of N
does not contain any non-variable leaves, we are
in case (i) in the proof of Theorem 4. Conse-
quently, the displayed rule of N is replaced by a
variant, in which ? is replaced by a new variable z
with look-ahead ?.
Secondly, with this new rule there is an mgu,
in which y2 is mapped to ?(z1, z2). Clearly, we
are now in case (ii). Furthermore, we can select
the set V = {z1, z2} and position w? = . Cor-
respondingly, the following two new rules for N
replace the old rule:
q(?(z, z?))? q?(z?)
q?(?(z1, z2))? ?(q1(z1), q2(z2)) ,
where the look-ahead for z remains ?.
Figure 10 displays another rule of N . There is
an mgu, in which y2 is mapped to ?(z2, z3). Thus,
we end up in case (ii) again and we can select the
set V = {z2} and position w? = 2. Thus, we
replace the rule of Figure 10 by the new rules
q(?(z1, z))? ?(q1(z1), q
?(z), q3(z)) (?)
q?(?(z2, z3))? q2(z2)
q3(?(z1, z2))? q3(z2) ,
where q3 = ??(z2, z3), q3, 2?.
Let us use the construction in the proof of The-
orem 4 to resolve the incompatibility (see Exam-
ple 3) between the XTOPs presented in the Intro-
duction. Fortunately, the incompatibility can be
resolved easily by cutting the rule of N (see Fig-
ure 7) into the rules of Figure 11. In this example,
linearity and nondeletion are preserved.
813
4.2 Local determinism
After the first pre-processing step, we have the
original linear and nondeleting XTOP M and
an XTOPF N ? = (Q?,?,?, IN , R?N , cN ) that is
equivalent to N and compatible with M . How-
ever, in the first pre-processing step we might
have introduced some non-linear (copying) rules
in N ? (see rule (?) in Example 5), and it is known
that ?nondeterminism [in M ] followed by copy-
ing [inN ?]? is a feature that prevents composition
to work (Engelfriet, 1975; Baker, 1979). How-
ever, our copying is very local and the copies
are only used to project to different subtrees.
Nevertheless, during those projection steps, we
need to make sure that the processing in M pro-
ceeds deterministically. We immediately note that
all but one copy are processed by states of the
form ?l, q, v? ? Ql. These states basically pro-
cess (part of) the tree l and project (with state q)
to the subtree at position v. It is guaranteed that
each such subtree (indicated by v) is reached only
once. Thus, the copying is ?resolved? once the
states of the form ?l, q, v? are left. To keep the
presentation simple, we just add expanded rules
to M such that any rule that can produce a part of
a tree l immediately produces the whole tree. A
similar strategy is used to handle the look-ahead
of N ?. Any right-hand side of a rule of M that
produces part of a left-hand side of a rule of N ?
with look-ahead is expanded to produce the re-
quired look-ahead immediately.
Let L ? T?(Z) be the set of trees l such that
? ?l, q, v? appears as a state of Ql, or
? l = l2? for some ?2 = q(l2) ? r2 ? R?N
of N ? with non-trivial look-ahead (i.e.,
cN (?2, z) /? X for some z ? X), where
?(x) = cN (?2, x) for every x ? X .
To keep the presentation uniform, we assume
that for every l ? L, there exists a state of the
form ?l, q, v? ? Q?. If this is not already the
case, then we can simply add useless states with-
out rules for them. In other words, we assume that
the first case applies to each l ? L.
Next, we add two sets of rules to RM , which
will not change the semantics but prove to be use-
ful in the composition construction. First, for
every tree t ? L, let Rt contain all the rules
p(l) ? r, where p = p(l) ? r is a new state
with p ? P , minimal normalized tree l ? T?(X),
and an instance r ? T?(P (X)) of t such that
q
p
?
y1 y2
?
i
ps
y1
q
?
y2
q?
?
y2
?
i
ps
s?
y1
?
s
i
ps
y1
i
ps

? 
q
?s
?
y1 y2
i
ps
y1
?
q?
?s
?
y1 y2
q
p
y2
?
q
?s,s?/??s,s?
?
y1 y2 y3
i
ps
y1
?
q?
??s,s?
?
y1 y2 y3
?
i
ps?
y2
i
p?
y3
?
q?
?s,s?
?
y1 y2 y3
?
i
ps?
y2
q
?
y3
q?
?
y3
?
Figure 12: Useful rules for the composition M ? ;N ? of
Example 8, where s, s? ? {?, ?} and ? ? P?(z2,z3).
p(l) ??M ? ? ?M ? r for some ? that is not an
instance of t. In other words, we construct each
rule of Rt by applying existing rules of RM in
sequence to generate a (minimal) right-hand side
that is an instance of t. We thus potentially make
the right-hand sides of M bigger by joining sev-
eral existing rules into a single rule. Note that
this affects neither compatibility nor the seman-
tics. In the second step, we add pure ?-rules
that allow us to change the state to one that we
constructed in the previous step. For every new
state p? = p(l) ? r, let base(p?) = p. Then
R?M = RM ? RL ? RE and P
? = P ?
?
t?L Pt
where
RL =
?
t?L
Rt and Pt = {`(?) | `? r ? Rt}
RE = {base(p?)(x1)? p?(x1) | p? ?
?
t?L
Pt} .
Clearly, this does not change the semantics be-
cause each rule of R?M can be simulated by a
chain of rules of RM . Let us now do a full ex-
ample for the pre-processing step. We consider a
nondeterministic variant of the classical example
by Arnold and Dauchet (1982).
Example 6. Let M = (P,?,?, {p}, RM )
be the linear and nondeleting XTOP such that
P = {p, p?, p?}, ? = {?, ?, ?, ?, }, and
RM contains the following rules
p(?(y1, y2))? ?(ps(y1), p(y2)) (?)
814
p(?(y1, y2, y3))? ?(ps(y1), ?(ps?(y2), p(y3)))
p(?(y1, y2, y3))? ?(ps(y1), ?(ps?(y2), p?(y3)))
ps(s
?(y1))? s(ps(y1))
ps()? 
for every s, s? ? {?, ?}. Similarly, we let
N = (Q,?,?, {q}, RN ) be the linear and non-
deleting XTOP such thatQ = {q, i} andRN con-
tains the following rules
q(?(z1, z2))? ?(i(z1), i(z2))
q(?(z1, ?(z2, z3)))? ?(i(z1), i(z2), q(z3)) (?)
i(s(z1))? s(i(z1))
i()? 
for all s ? {?, ?}. It can easily be verified that
M and N meet our requirements. However, N is
not yet compatible with M because an mgu be-
tween rules (?) of M and (?) of N might map y2
to ?(z2, z3). Thus, we decompose (?) into
q(?(z1, z))? ?(i(z1), q(z), q
?(z))
q?(?(z2, z3))? q(z3)
q(?(z1, z2))? i(z1)
where q = ??(z2, z3), i, 1?. This newly obtained
XTOP N ? is compatible with M . In addition, we
only have one special tree ?(z2, z3) that occurs in
states of the form ?l, q, v?. Thus, we need to com-
pute all minimal derivations whose output trees
are instances of ?(z2, z3). This is again simple
since the first three rule schemes ?s, ?s,s? , and
??s,s? of M create such instances, so we simply
create copies of them:
?s(?(y1, y2))? ?(ps(y1), p(y2))
?s,s?(?(y1, y2, y3))? ?(ps(y1), ?(ps?(y2), p(y3)))
??s,s?(?(y1, y2, y3))? ?(ps(y1), ?(ps?(y2), p?(y3)))
for all s, s? ? {?, ?}. These are all the rules
of R?(z2,z3). In addition, we create the following
rules of RE :
p(x1)? ?s(x1) p(x1)? ?s,s?(x1)
p(x1)? ?
?
s,s?(x1)
for all s, s? ? {?, ?}.
Especially after reading the example it might
seem useless to create the rule copies inRl [in Ex-
ample 6 for l = ?(z2, z3)]. However, each such
rule has a distinct state at the root of the left-hand
side, which can be used to trigger only this rule.
In this way, the state selects the next rule to apply,
which yields the desired local determinism.
?q, p?
RC
PREL
that
C
x1 x2
?
C
?qNP, pNP?
x1
?q?, pVP?
x2
Figure 13: Composed rule created from the rule of Fig-
ure 7 and the rules of N ? displayed in Figure 11.
5 Composition
Now we are ready for the actual composition. For
space efficiency reasons we reuse the notations
used in Section 4. Moreover, we identify trees of
T?(Q?(P ?(X))) with trees of T?((Q? ? P ?)(X)).
In other words, when meeting a subtree q(p(x))
with q ? Q?, p ? P ?, and x ? X , then we also
view this equivalently as the tree ?q, p?(x), which
could be part of a rule of our composed XTOP.
However, not all combinations of states will be
allowed in our composed XTOP, so some combi-
nations will never yield valid rules.
Generally, we construct a rule ofM ? ;N ? by ap-
plying a single rule of M ? followed by any num-
ber of pure ?-rules of RE , which can turn states
base(p) into p. Then we apply any number of
rules of N ? and try to obtain a sentential form that
has the required shape of a rule of M ? ;N ?.
Definition 7. Let M ? = (P ?,?,?, IM , R?M ) and
N ? = (Q?,?,?, IN , R?N ) be the XTOPs con-
structed in Section 4, where
?
l?L Pl ? P
? and
?
l?LQl ? Q
?. Let Q?? = Q? \
?
l?LQl. We con-
struct the XTOPM ? ;N ? = (S,?,?, IN?IM , R)
where
S =
?
l?L
(Ql ? Pl) ? (Q
?? ? P ?)
and R contains all normalized rules `? r (of the
required shape) such that
`?M ? ? ?
?
RE ? ?
?
N ? r
for some ?, ? ? T?(Q?(T?(P ?(X)))).
The required rule shape is given by the defi-
nition of an XTOP. Most importantly, we must
have that ` ? S(T?(X)), which we identify
with a certain subset of Q?(P ?(T?(X))), and
r ? T?(S(X)), which similarly corresponds to
a subset of T?(Q?(P ?(X))). The states are sim-
ply combinations of the states of M ? and N ?, of
815
qp
?
y1 ?
y2 y3
?
?
i
ps
y1
i
ps
y2
q
p
y3
Figure 14: Successfully expanded rule from Exam-
ple 9.
which however the combinations of a state q ? Ql
with a state p /? Pl are forbidden. This reflects the
intuition of the previous section. If we entered a
special state of the form ?l, q, v?, then we should
use a corresponding state p ? Pl of M , which
only has rules producing instances of l. We note
that look-ahead of N ? is checked normally in the
derivation process.
Example 8. Now let us illustrate the composition
on Example 6. Let us start with rule (?) of M .
q(p(?(x1, x2)))
?M ? q(?(ps(x1), p(x2)))
?RE q(?(ps(x1), ?s?,s??(x2)))
?N ? ?(i(ps(x1)), q(?s?,s??(x2)), q
?(?s?,s??(x2)))
is a rule of M ? ; N ? for every s, s?, s?? ? {?, ?}.
Note if we had not applied the RE-step, then we
would not have obtained a rule of M ; N (be-
cause we would have obtained the state combina-
tion ?q, p? instead of ?q, ?s?,s???, and ?q, p? is not a
state of M ? ; N ?). Let us also construct a rule for
the state combination ?q, ?s?,s???.
q(?s?,s??(?(x1, x2, x3)))
?M ? q(?(ps?(x1), ?(ps??(x2), p(x3))))
?N ? q
?(ps?(x1))
Finally, let us construct a rule for the state combi-
nation ?q??, ?s?,s???.
q??(?s?,s??(?(x1, x2, x3)))
?M ? q(?(ps?(x1), ?(ps??(x2), p(x3))))
?RE q(?(ps?(x1), ?(ps??(x2), ?s(x3))))
?N ? q(?(ps??(x2), ?s(x3)))
?N ? ?(q
?(ps??(x1)), q(?s(x2)), q
??(?s(x2)))
for every s ? {?, ?}.
After having pre-processed the XTOPs in our
introductory example, the devices M and N ? can
be composed into M ; N ?. One rule of the com-
posed XTOP is illustrated in Figure 13.
q
p
?
y1 ?
y2 y3 y4
?
?
i
ps
y1
i
ps?
y2
?
i
ps??
y3
q
??
y4
q?
??
y4
Figure 15: Expanded rule that remains copying (see
Example 9).
6 Post-processing
Finally, we will compose rules again in an ef-
fort to restore linearity (and nondeletion). Since
the composition of two linear and nondeleting
XTOPs cannot always be computed by a single
XTOP (Arnold and Dauchet, 1982), this method
can fail to return such an XTOP. The presented
method is not a characterization, which means it
might even fail to return a linear and nondelet-
ing XTOP although an equivalent linear and non-
deleting XTOP exists. However, in a significant
number of examples, the recombination succeeds
to rebuild a linear (and nondeleting) XTOP.
Let M ? ;N ? = (S,?,?, I, R) be the composed
XTOP constructed in Section 5. We simply in-
spect each non-linear rule (i.e., each rule with a
non-linear right-hand side) and expand it by all
rule options at the copied variables. Since the
method is pretty standard and variants have al-
ready been used in the pre-processing steps, we
only illustrate it on the rules of Figure 12.
Example 9. The first (top row, left-most) rule of
Figure 12 is non-linear in the variable y2. Thus,
we expand the calls ?q, ??(y2) and ?q?, ??(y2). If
? = ?s for some s ? {?, ?}, then the next rules
are uniquely determined and we obtain the rule
displayed in Figure 14. Here the expansion was
successful and we could delete the original rule
for ? = ?s and replace it by the displayed ex-
panded rule. However, if ? = ??s?,s?? , then we can
also expand the rule to obtain the rule displayed in
Figure 15. It is still copying and we could repeat
the process of expansion here, but we cannot get
rid of all copying rules using this approach (as ex-
pected since there is no linear XTOP computing
the same tree transformation).
816
References
Andre? Arnold and Max Dauchet. 1982. Morphismes
et bimorphismes d?arbres. Theoretical Computer
Science, 20(1):33?93.
Brenda S. Baker. 1979. Composition of top-down
and bottom-up tree transductions. Information and
Control, 41(2):186?213.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proc. ACL,
pages 205?208. Association for Computational Lin-
guistics.
Joost Engelfriet, Eric Lilin, and Andreas Maletti.
2009. Composition and decomposition of extended
multi bottom-up tree transducers. Acta Informatica,
46(8):561?590.
Joost Engelfriet. 1975. Bottom-up and top-down
tree transformations?A comparison. Mathemati-
cal Systems Theory, 9(3):198?231.
Joost Engelfriet. 1977. Top-down tree transducers
with regular look-ahead. Mathematical Systems
Theory, 10(1):289?303.
Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Computational
Linguistics, 34(3):391?427.
Jonathan Graehl, Mark Hopkins, Kevin Knight, and
Andreas Maletti. 2009. The power of extended top-
down tree transducers. SIAM Journal on Comput-
ing, 39(2):410?430.
Kevin Knight and Jonathan Graehl. 2005. An over-
view of probabilistic tree transducers for natural
language processing. In Proc. CICLing, volume
3406 of LNCS, pages 1?24. Springer.
Kevin Knight. 2007. Capturing practical natural
language transformations. Machine Translation,
21(2):121?133.
Eric Lilin. 1978. Une ge?ne?ralisation des transduc-
teurs d?e?tats finis d?arbres: les S-transducteurs.
The`se 3e`me cycle, Universite? de Lille.
Andreas Maletti and Heiko Vogler. 2010. Composi-
tions of top-down tree transducers with ?-rules. In
Proc. FSMNLP, volume 6062 of LNAI, pages 69?
80. Springer.
Andreas Maletti. 2010. Why synchronous tree sub-
stitution grammars? In Proc. HLT-NAACL, pages
876?884. Association for Computational Linguis-
tics.
Andreas Maletti. 2011. An alternative to synchronous
tree substitution grammars. Natural Language En-
gineering, 17(2):221?242.
Alberto Martelli and Ugo Montanari. 1982. An effi-
cient unification algorithm. ACM Transactions on
Programming Languages and Systems, 4(2):258?
282.
Jonathan May and Kevin Knight. 2006. Tiburon: A
weighted tree automata toolkit. In Proc. CIAA, vol-
ume 4094 of LNCS, pages 102?113. Springer.
Jonathan May, Kevin Knight, and Heiko Vogler. 2010.
Efficient inference through cascades of weighted
tree transducers. In Proc. ACL, pages 1058?1066.
Association for Computational Linguistics.
Jonathan May. 2010. Weighted Tree Automata and
Transducers for Syntactic Natural Language Pro-
cessing. Ph.D. thesis, University of Southern Cali-
fornia, Los Angeles.
John Alan Robinson. 1965. A machine-oriented logic
based on the resolution principle. Journal of the
ACM, 12(1):23?41.
William C. Rounds. 1970. Mappings and grammars
on trees. Mathematical Systems Theory, 4(3):257?
287.
Stuart M. Shieber. 2004. Synchronous grammars as
tree transducers. In Proc. TAG+7, pages 88?95.
James W. Thatcher. 1970. Generalized2 sequential
machine maps. Journal of Computer and System
Sciences, 4(4):339?367.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proc. ACL,
pages 523?530. Association for Computational Lin-
guistics.
817
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 876?884,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Why Synchronous Tree Substitution Grammars?
Andreas Maletti
Universitat Rovira i Virgili, Departament de Filologies Roma`niques
Avinguda de Catalunya 35, 43002 Tarragona, Spain
andreas.maletti@urv.cat
Abstract
Synchronous tree substitution grammars are a
translation model that is used in syntax-based
machine translation. They are investigated in
a formal setting and compared to a competi-
tor that is at least as expressive. The competi-
tor is the extended multi bottom-up tree trans-
ducer, which is the bottom-up analogue with
one essential additional feature. This model
has been investigated in theoretical computer
science, but seems widely unknown in natu-
ral language processing. The two models are
compared with respect to standard algorithms
(binarization, regular restriction, composition,
application). Particular attention is paid to the
complexity of the algorithms.
1 Introduction
Every machine translation system uses a transla-
tion model, which is a formal model that describes
the translation process. Either this system is hand-
crafted (in rule-based translation systems) or it is
trained with the help of statistical processes. Brown
et al (1990) discuss automatically trainable transla-
tion models in their seminal paper on the latter ap-
proach. The IBM models of Brown et al (1993) are
string-based in the sense that they base the transla-
tion decision on the words and the surrounding con-
text. In the field of syntax-based machine transla-
tion, the translation models have access to the syntax
(in the form of parse trees) of the sentences. Knight
(2007) presents a good exposition to both fields.
In this paper, we focus on syntax-based transla-
tion models, and in particular, synchronous tree sub-
stitution grammars (STSGs), or the equally pow-
erful (linear and nondeleting) extended (top-down)
tree transducers of Graehl et al (2008). Chiang and
Knight (2006) gives a good introduction to STSGs,
which originate from the syntax-directed transla-
tion schemes of Aho and Ullman (1972) [nowadays
more commonly known as synchronous context-free
grammars]. Roughly speaking, an STSG has rules
in which a nonterminal is replaced by two trees con-
taining terminal and nonterminal symbols. In addi-
tion, the nonterminals in the two trees are linked and
a rule is only applied to linked nonterminals.
Several algorithms for STSGs have been dis-
cussed in the literature. For example, we can
? train them [see Graehl et al (2008)],
? attempt to binarize them using the methods of
(Zhang et al, 2006; Huang et al, 2009; DeNero
et al, 2009b),
? parse them [see DeNero et al (2009a)], or
? attempt to compose them.
However, some important algorithms are partial be-
cause it is known that the construction might not be
possible in general. This is the case, for example,
for binarization and composition.
In the theoretical computer science community,
alternative models have been explored. Such
a model is the multi bottom-up tree transducer
(MBOT) of Arnold and Dauchet (1982) and Lilin
(1981), which essentially is the bottom-up analogue
of STSGs with the additional feature that nontermi-
nals can have an arbitrary rank (the rank of a non-
terminal of an STSG can be considered to be fixed
to 1). This model is even more expressive than
STSGs, but still offers good computational proper-
ties. In this contribution, we will compare STSGs
and MBOTs with respect to some standard algo-
rithms. Generally, MBOTs offer algorithmic ben-
efits over STSG, which can be summarized as fol-
876
lows:
? Every STSG can be transformed into an equiv-
alent MBOT in linear time.
? MBOTs can be fully binarized in linear
time whereas only partial binarizations (or
asynchronous binarizations) are possible for
STSGs.
? The input language of an MBOTM can be reg-
ularly restricted in O(|M | ? |S|3), whereas the
corresponding construction for an STSG M is
in O(|M | ? |S|2 rk(M)+5) where rk(M) is the
maximal number of nonterminals in a rule of
the STSG M .
? MBOTs can be composed, whereas this cannot
be achieved for STSGs.
Overall, we thus conclude that, from an algorith-
mic perspective, it would be beneficial to work with
MBOTs instead of STSGs. However, the full power
of MBOTs should not be tapped because, in gen-
eral, MBOTs have the finite-copying property [see
Engelfriet et al (1980)], which complicates the al-
gorithms for forward and backward application (see
Section 7).
2 Preliminary definitions
An alphabet is a finite set of symbols. Our weighted
devices use real-number weights, but the results
translate easily to the more general setting of com-
mutative semirings [see Golan (1999)]. A weighted
string automaton as in Schu?tzenberger (1961) and
Eilenberg (1974) is a system (S,?, I, ?, F ) where
? S and ? are alphabets of states and input sym-
bols, respectively,
? I, F : S ? R assign initial and final weights,
respectively, and
? ? : S ? ? ? S ? R assigns a weight to each
transition.
Let w = ?1 ? ? ? ?k ? ?? be an input string of
length k. A run on w is r : {0, . . . , k} ? S. The
weight of the run r is wt(r) =
?k
i=1 ?(ri?1, ?i, ri).
The semantics of the automaton A then assigns to w
the weight
A(w) =
?
r run on w
I(r0) ? wt(r) ? F (rk) .
A good introduction to weighted string automata can
be found in Mohri (2009) and Sakarovitch (2009).
To simplify the theoretical discussion, we as-
sume that each symbol that we use in trees has a
fixed rank, which determines the number of chil-
dren of each node with that label. A ranked alpha-
bet ? =
?
k?0 ?k is an alphabet whose symbols
have assigned ranks. The set ?k contains all sym-
bols of rank k. The set T?(V ) of ?-trees indexed
by a set V is the smallest set such that V ? T?(V )
and ?(t1, . . . , tk) ? T?(V ) for every ? ? ?k and
t1, . . . , tk ? T?(V ). The size |t| of the tree t ? T?
is the number of occurrences of symbols from ??V
that appear in t. A context c is a tree of T??{}(V ),
in which the nullary symbol  occurs exactly once.
The set of all such contexts is C?(V ). The tree c[t]
is obtained from c by replacing the symbol  by t.
A weighted synchronous tree substitution gram-
mar (STSG) is a system (N,?,?, I, P ) where
? N is an alphabet of nonterminals,
? ? and ? are ranked alphabets of input and out-
put symbols, respectively,
? I : N ? R assigns initial weights, and
? P is a finite set of productions n : t
a
? u with
n ? N , t ? T?(N), a ? R, and u ? T?(N)
such that
? every n? ? N that occurs in t occurs ex-
actly once in u and vice versa, and
? t /? N or u /? N .
Note that our distinction between nonterminals and
terminals is rather uncommon for STSG [see Chi-
ang (2005)], but improves the generative power. We
chose the symbol ??? because STSG productions
are symmetric. The size |n : t
a
? u| of a produc-
tion is |t| + |u|, and the size |M | of the STSG M is
?
p?P |p|. It is a weighted tree substitution grammar
(TSG) if t = u for all productions n : t
a
? u ? P .
Further, it is in normal form if for every production
n : t
a
? u ? P there exist ? ? ?k, ? ? ?k, and
nonterminals n1, . . . , nk, n?1, . . . , n
?
k ? N such that
t = ?(n1, . . . , nk) and u = ?(n?1, . . . , n
?
k). A de-
tailed exposition to STSGs and STSGs in normal
form (also called synchronous context-free gram-
mars) can be found in Chiang (2005). Further details
on TSGs can be found in Berstel and Reutenauer
(1982) and Fu?lo?p and Vogler (2009).
Equal nonterminals in t and u of a produc-
tion n : t
a
? u ? P are linked. To keep the pre-
sentation simple, we assume that those links are re-
877
SNP1 @
V NP2
?
S
V @
NP1 NP2
S
NP
x1
@
V
x2
NP
x3
?
S
S
x2 @
x1 x3
Figure 1: STSG production (top) and corresponding
MBOT rule (bottom) where @ is an arbitrary symbol that
is introduced during binarization.
membered also in sentential forms. In addition, we
assume that N ? ? = ?. For every c, c? ? C?(N)
and n ? N , let (c[n], c?[n])
a
? (c[t], c?[u]) if
? there is a production n : t
a
? u ? P , and
? the explicit (the ones replacing ) occurrences
of n in c[n] and c?[n] are linked.
Left-most derivations are defined as usual, and the
weight of a derivation D : ?0
a1? ? ? ?
ak? ?k is
wt(D) =
?k
i=1 ai. The weight assigned by the
grammar M to a pair (t, u) ? T? ? T? is
M(t, u) =
?
n?N
I(n) ?
?
D left-most derivation
from (n, n) to (t, u)
wt(D) .
The second restriction on productions ensures that
derivations are of finite length, and thus that the
sums in the definition of M(t, u) are finite.
In the following, we will use syntactic simplifica-
tions such as
? several occurrences of the same nonterminal in
a tree (disambiguated by decoration).
? symbols that are terminals (of ? and ?) and
nonterminals. We will print nonterminals in
italics and terminal symbols upright.
? omission of the nonterminal n (or the weight a)
of a rule n : t
a
? u if the terminal n occurs at
the root of t and u (or a = 1).
? n
a
? t instead of n : t
a
? t if it is a TSG.
A sample STSG production (using those simplifica-
tions) is displayed in Figure 1. Our STSGs are es-
sentially equivalent to the (nondeleting and linear)
extended tree transducers of Graehl et al (2008) and
Maletti et al (2009).
@
V
x2
NP
x3
?
U
x2 x3
S
NP
x1
U
x2 x3
?
U ?
x2 @
x1 x3
U ?
x1 x2
?
S
S
x1 x2
Figure 2: Sample MBOT rules in one-symbol normal
form.
3 Multi bottom-up tree transducers
As indicated in the Introduction, we will compare
STSGs to weighted multi bottom-up tree transduc-
ers, which have been introduced by Arnold and
Dauchet (1982) and Lilin (1981). A more detailed
(and English) presentation can be found in Engel-
friet et al (2009). Let us quickly recall the formal
definition. We use a fixed set X = {x1, x2, . . . }
of (formal) variables. For a ranked alphabet S and
L ? T?(X) we let
S(L) = {s(t1, . . . , tk) | s ? Sk, t1, . . . , tk ? L}
and we treat elements of S(L) like elements
of T??S(X).
Definition 1 A weighted multi bottom-up tree trans-
ducer (MBOT) is a system (S,?,?, F,R) where
? S, ?, and ? are ranked alphabets of states, in-
put symbols, and output symbols, respectively,
? F : S1 ? R assigns final weights, and
? R is a finite set of rules l
a
? r where a ? R,
l ? T?(S(X)), and r ? S(T?(X)) such that
? every x ? X that occurs in l occurs ex-
actly once in r and vice versa, and
? l /? S(X) or r /? S(X).
Roughly speaking, an MBOT is the bottom-up
version of an extended top-down tree transducer, in
which the states can have a rank different from 1. We
chose the symbol ??? because rules have a distin-
guished left- and right-hand side. The size |l
a
? r| of
878
SNP
t1
@
V
t2
NP
t3
?
S
NP
t1
U
t2 t3
?
U ?
t2 @
t1 t3
?
S
S
t2 @
t1 t3
Figure 3: Derivation using the MBOT rules of Fig. 2.
a rule is |l|+ |r|, and the size |M | of an MBOTM is
?
r?R|r|. Again the second condition on the rules
will ensure that derivations will be finite. Let us
continue with the rewrite semantics for the MBOT
(S,?,?, F,R). To simplify the presentation, we
again assume that S ? (? ? ?) = ?. We need
the concept of substitution. Let ? : X ? T? and
t ? T?(X). Then t? is the tree obtained by replac-
ing every occurrence of x ? X in t by ?(x).
Definition 2 Let c ? C?(S(X)) and ? : X ? T?.
Then c[l?]
a
? c[r?] if l
a
? r ? R. The weight of a
derivation D : ?0
a1? ? ? ?
ak? ?k is wt(D) =
?k
i=1 ai.
The weight assigned by the MBOT M to a pair
(t, u) ? T? ? T? is
M(t, u) =
?
s?S1
F (s) ?
?
D left-most derivation
from t to s(u)
wt(D) .
We use the simplifications already mentioned in
the previous section also for MBOTs. Figures
1 and 2 display example rules of an MBOT. The
rules of Figure 2 are applied in a derivation in Fig-
ure 3. The first displayed derivation step uses the
context S(NP(t1),) and any substitution ? such
that ?(x2) = t2 and ?(x3) = t3.
It is argued by Chiang (2005) and Graehl et
al. (2008) that STSGs (and extended tree trans-
ducers) have sufficient power for syntax-based ma-
chine translation. Knight (2007) presents a detailed
overview that also mentions short-comings. Since
our newly proposed device, the MBOT, should be
at least as powerful as STSGs, we quickly demon-
strate how each STSG can be coded as an MBOT.
An STSG production and the corresponding MBOT
rule are displayed in Figure 1. Since the correspon-
dence is rather trivial, we omit a formal definition.
Theorem 3 For every STSG M , an equivalent
MBOT can be constructed in time O(|M |).
4 Binarization
Whenever nondeterminism enters the playfield, bi-
narization becomes an important tool for efficiency
reasons. This is based on the simple, yet powerful
observation that instead of making 5 choices from a
space of n in one instant (represented by n5 rules),
it is more efficient (Wang et al, 2007) to make them
one-by-one (represented by 5n rules). Clearly, this
cannot always be done but positive examples exist in
abundance; e.g., binarization of context-free gram-
mars [see CHOMSKY normal form in Hopcroft and
Ullman (1979)].
Binarization of tree language devices typically
consists of two steps: (i) binarization of the involved
trees (using the auxiliary symbol @) and (ii) adjust-
ment (binarization) of the processing device to work
on (and fully utilize) the binarized trees. If success-
ful, then this leads to binarized derivation trees for
the processing device. In Figure 4 we show the bi-
narization of the trees in an STSG production. An-
other binarization of the rule of Figure 4 is displayed
in Figure 1. The binarization is evident enough, so
we can assume that all trees considered in the fol-
lowing are binarized.
The binarization in Figure 1 is unfortunate be-
cause the obtained production cannot be factor-
ized such that only two nonterminals occur in each
rule. However, the binarization of Figure 4 allows
the factorization into S(U ,NP) ? S(U ,NP) and
U : @(NP ,V )? @(V ,NP), which are fully bina-
rized productions. However, in general, STSGs (or
SCFGs or extended tree transducers) cannot be fully
binarized as shown in Aho and Ullman (1972).
Zhang et al (2006) and Wang et al (2007) show
the benefits of fully binarized STSGs and present a
linear-time algorithm for the binarization of binariz-
able STSGs. We show that those benefits can be
reaped for all STSGs by a simple change of model.
879
SNP1 V NP2
?
S
V NP1 NP2
S
@
NP1 V
NP2 ?
S
@
V NP1
NP2
Figure 4: Binarization of trees in an STSG production.
Top: Original ? Bottom: Binarized trees.
We have already demonstrated that every STSG can
be transformed into an equivalent MBOT in linear
time. Next, we discuss binarization of MBOTs.
An MBOT is in one-symbol normal form if there
is at most one input and at most one output symbol,
but at least one symbol in each rule (see Figure 2).
Raoult (1993) and Engelfriet et al (2009) prove that
every MBOT can be transformed into one-symbol
normal form. The procedure presented there runs in
linear time in the size of the input MBOT. Conse-
quently, we can transform each STSG to an equiv-
alent MBOT in one-symbol normal form in linear
time. Finally, we note that a MBOT in one-symbol
normal form has binarized derivation trees, which
proves that we fully binarized the STSG.
Theorem 4 For every STSG M an equivalent, fully
binarized MBOT can be constructed in O(|M |).
The construction of Engelfriet et al (2009) is il-
lustrated in Figure 2, which shows the rules of an
MBOT in one-symbol normal form. Those rules are
constructed from the unlucky binarization of Fig-
ure 1. In the next section, we show the benefit of the
full binarization on the example of the BAR-HILLEL
construction.
5 Input and output restriction
A standard construction for transformation devices
(and recognition devices alike) is the regular restric-
tion of the input or output language. This con-
struction is used in parsing, integration of a lan-
guage model, and the computation of certain metrics
[see Nederhof and Satta (2003), Nederhof and Satta
(2008), and Satta (2010) for a detailed account]. The
construction is generally known as BAR-HILLEL
construction [see Bar-Hillel et al (1964) for the
original construction on context-free grammars].
STSGs (and extended tree transducers) are sym-
metric, so that input and output can freely be
swapped. Let M be an STSG and A a weighted
string automaton with states S. In the BAR-HILLEL
construction for M and A, the maximal rank rk(M)
of a symbol in the derivation forest ofM enters as an
exponent into the complexityO(|M | ? |S|2 rk(M)+5).
Since full binarization is not possible in general, the
maximal rank cannot be limited to 2. In contrast,
full binarization is possible for MBOTs (with only
linear overhead), so let us investigate whether we
can exploit this in a BAR-HILLEL construction for
MBOTs.
Let M = (S,?,?, F,R) be an MBOT in one-
symbol normal form. The symbols in ? ? ? have
rank at most 2. Moreover, let G = (N,?,?, I, P )
be a TSG in normal form. We want to construct an
MBOT M ? such that M ?(t, u) = M(t, u) ?G(t) for
every t ? T? and u ? T?. In other words, each
input tree should be rescored according to G; in the
unweighted case this yields that the translation ofM
is filtered to the set of input trees accepted by G.
We occasionally write the pair (a, b) in angled
parentheses (??? and ???). In addition, we use the
center line ellipsis ?? ?? (also with decoration) like a
variable (especially for sequences).
Definition 5 The input product Prod(M,G) is the
MBOT Prod(M,G) = (S?N,?,?, F ?, R?) where
? F ?(?s, n?) = F (s) ? I(n) for every s ? S and
n ? N ,
? for every rule s(? ?)
a
? s?(? ??) ? R with
s, s? ? S and every n ? N , there exists a rule
?s, n?(? ?)
a
? ?s?, n?(? ??) ? R? ,
? for every rule ?(s1(? ?1), . . . , sk(? ?k))
a
? s(? ?)
in R with ? ? ?k and s, s1, . . . , sk ? S, and
every production n
b
? ?(n1, . . . , nk) ? P , the
following rule is in R?:
?(?s1, n1?(? ?1), . . . , ?sk, nk?(? ?k))
ab
? ?s, n?(? ?) .
The first type of rule (second item) does not in-
volve an input symbol, and thus the nonterminal
of G is just forwarded to the new state. Since no
step with respect to G is made, only the weight of
the rule of M is charged. The second type of rule
(third item) uses a rule of R with the input symbol ?
880
s1 s3
s1 s2 s2 s3
?
?s1,s3???(?s1,s2?,?s2,s3?)
s1 s2
s1 s2
?
?s1,s2???(?s1,s2?)
s1 ? s2
?(s1, ?, s2)
?s1,s2?
?(s1,?,s2)? ?
Figure 5: Constructing a TSG from a weighted string au-
tomaton.
and a production of P that also contains ?. The rule
and the production are executed in parallel in the re-
sulting rule and its weight is thus the product of the
weights of the original rule and production. Over-
all, this is a classical product construction, which is
similar to other product constructions such as Bor-
chardt (2004). A straightforward proof shows that
M ?(t, u) = M(t, u) ? G(t) for every t ? T? and
u ? T?, which proves the correctness.
Next, let us look at the complexity. The MBOT
Prod(M,G) can be obtained in time O(|M | ? |G|).
Furthermore, it is known [see, for example, Maletti
and Satta (2009)] that for every weighted string au-
tomaton A with states S, we can construct a TSG G
in normal form, which has size O(|?| ? |S|3) and
recognizes each tree of T? with the weight that the
automaton A assigns to its yield. The idea of this
construction is illustrated in Figure 5. Consequently,
our BAR-HILLEL construction has the well-known
complexityO(|M | ? |S|3). This should be compared
to the complexity of the corresponding construction
for an STSG M , which is in O(|M | ? |S|2 rk(M)+5)
where rk(M) is the maximal number of (different)
nonterminals in a production of M . Thus, the STSG
should be transformed into an equivalent MBOT in
one-symbol normal form, which can be achieved
in linear time, and the BAR-HILLEL construction
should be performed on this MBOT.
Since STSGs are symmetric, our approach can
also be applied to the output side of an STSG.
However, it should be noted that we can apply it
only to one side (the input side) of the MBOT. A
construction for the output side of the MBOT can
be defined, but it would suffer from a similarly
high complexity as already presented for STSGs.
More precisely, we expect a complexity of roughly
O(|M | ? |S|2 rk(M)+2) for this construction. The
small gain is due to the one-symbol normal form and
binarization.
6 Composition
Another standard construction for transformations is
(relational) composition. Composition constructs a
translation from a language L to L?? given transla-
tions from L to L? and from L? to L??. Formally,
given transformations M ? : T? ? T? ? R and
M ?? : T??T? ? R, the composition ofM ? andM ??
is a tranformation M ? ;M ?? : T? ? T? ? R with
(M ? ;M ??)(t, v) =
?
u?T?
M ?(t, u) ?M ??(u, v)
for every t ? T? and v ? T?. Mind that the sum-
mation might be infinite, but we will only consider
compositions, in which it is finite.
Unfortunately, Arnold and Dauchet (1982) show
that the composition of two transformations com-
puted by STSGs cannot necessarily be computed by
an STSG. Consequently, there cannot be a general
composition algorithm for STSGs.
Let us consider the problem of composition for
MBOTs. Essentially, we will follow the unweighted
approach of Engelfriet et al (2009) to obtain a com-
position construction, which we present next. Let
M ? = (S?,?,?, F ?, R?) and
M ?? = (S??,?,?, F ??, R??)
be MBOTs in one-symbol normal form. We ex-
tend the rewrite semantics (see Definition 2) to
trees that include symbols foreign to a MBOT. In
other words, we (virtually) extend the input and
output alphabets to contain all used symbols (in
particular also the states of another MBOT). How-
ever, since we do not extend the set of rules, the
MBOT cannot process foreign symbols. Neverthe-
less it can perform rewrite steps on known sym-
bols (or apply rules that do not contain input sym-
bols). We use ?R? and ?R?? for derivation steps
881
s?
s??1
t1 ? ? ? tm
? ? ? s??k
u1 ? ? ? un
?=
s??s??1, . . . , s
??
k?
t1 ? ? ? tm ? ? ? u1 ? ? ? un
Figure 6: Identification in sentential forms.
that exclusively use rules ofR? andR??, respectively.
In addition, we identify s?(s??1(? ?1), . . . , s
??
k(? ?k))
with s??s??1, . . . , s
??
k?(? ?1, . . . , ? ?k) for s
? ? S? and
s??1, . . . , s
??
k ? S
??. This identification is illustrated
in Figure 6.
Definition 6 The MBOT M ? ;M ?? = (S,?,?, F,R)
is such that
? for every s? ? S?k and s
??
1 ? S
??
`1 , . . . , s
??
k ? S
??
`k
we have s??s??1, . . . , s
??
k? ? S`1+???+`k ,
? F (s??s??) = F ?(s?) ? F ??(s??) for every s? ? S?1
and s?? ? S??1 , and
? the rules l
a
? r of R, all of which are such that
the variables in l occur in order (x1, . . . , xk)
from left-to-right, are constructed in 3 ways:
? l a?R? r by a single rule of R?,
? l a?R?? r by a single rule of R??, or
? l
a1?R? ?
a2?R?? r with a = a1 ? a2 and
the applied rule of R? contains an output
symbol.
If a rule l
a
? r can be constructed in several
ways (with exactly weight a), then the weights
of all possibilities are added for the weight of
the new rule.
Intuitively, a single rule ofR? without output sym-
bols is used in the first type (because otherwise
r would have the wrong shape). In the second type, a
single rule of R?? without input symbols is used. Fi-
nally, in the third type, first a rule ofR? that produces
an output symbol of ? is used and then this symbol
is processed by a single rule of R??. Note that every
rule of R? can produce at most one output symbol
and the rules of R?? either process none or one input
symbol due to the assumption that M ? and M ?? are
in one-symbol normal form. We illustrate a rule of
the first in Figure 7.
original rule:
?
q1
x1 x2
q2
x3
a
?
q
x3 x1 x2
constructed rule:
?
q1
p1
x1 x2
p2
x3
q2
p3
x4 x5
a
?
q
p3
x4 x5
p1
x1 x2
p2
x3
Figure 7: Example of a constructed rule of type 1.
The correctness proof of this construction can es-
sentially (i.e., for the unweighted case) be found in
Engelfriet et al (2009). Before we can extend it to
the weighted case, we need to make sure that the
sum in the definition of composition is finite. We
achieve this by requiring that
? for every t ? T? and s ? S?1 there are finitely
many u ? T? such that t
a1? ? ? ?
an? s(u), or
? for every v ? T? and s ? S??1 there are finitely
many u ? T? such that u
a1? ? ? ?
an? s(v).
In other words,M ? may not have cyclic input ?-rules
or M ?? may not have cyclic output ?-rules. Now we
can state the main theorem.
Theorem 7 For all MBOTs M ? and M ?? with the
above restriction the composition M ? ; M ?? of their
transformations can be computed by another MBOT.
This again shows an advantage of MBOTs. The
composition result relies essentially on the one-
symbol normal form (or full binarization), which
can always be achieved for MBOTs, but cannot for
STSGs. Consequently, MBOTs can be composed,
whereas STSGs cannot be composed in general. In-
deed, STSGs in one-symbol normal form, which can
be defined as for MBOTs, can be composed as well,
which shows that the one-symbol normal form is the
key for composition.
Finally, let us discuss the complexity of compo-
sition. Let rk(M ?) be the maximal rank of a state
in S?. Then there are
? O(|M ?| ? |S??|rk(M
?)) rules of type 1,
? O(|M ??| ? |S??|rk(M
?)) rules of type 2, and
882
? O(|M ?| ? |M ??| ? |S??|rk(M
?)) rules of type 3.
Each rule can be constructed in linear time in the size
of the participating rules, so that we obtain a final
complexity ofO(|M ?| ? |M ??| ? |S??|rk(M
?)). Note that
ifM ? is obtained from an STSGM (via Theorem 4),
then rk(M ?) ? rk(M). This shows that binarization
does not avoid the exponent for composition, but at
least enables composition in the general case. More-
over, the complexity could be slightly improved by
the observation that our construction only relies on
(i)M ? having at most one output symbol per rule and
(ii) M ?? having at most one input symbol per rule.
7 Forward and backward application
We might want to apply a transformation not just to
a single tree, but rather to a set of trees, which are,
in some cases, already weighted. In general, the set
of trees is given by a TSG G and we expect the re-
sult to be represented by a TSG as well. Forward
and backward application amount to computing the
image and pre-image of G under the transformation,
respectively. Since STSG are symmetric, we need to
solve only one of the problems if the transformation
is given by an STSG. The other problem can then be
solved by inverting the STSG (exchanging input and
output) and using the method for the solved prob-
lem. We chose to address forward application here.
Forward application can be reduced to the prob-
lem of computing the co-domain (or range) with the
help of a product construction for STSG, which is
similar to the one presented in Definition 5. The co-
domain codM of the tranformation computed by an
STSG M assigns to each t ? T? the weight
codM (t) =
?
u?T?
M(t, u) .
This sum might not be well-defined. However, if
u /? N for all productions n : t
a
? u of the STSG,
then the sum is well-defined and the output-side
TSG (i.e., for every production n : t
a
? u in the
STSG there is a production n
a
? u in the TSG)
computes the co-domain. The restriction ?u /? N?
guarantees that the output side is a TSG. Overall, do-
main, co-domain, and forward and backward appli-
cations (using the product construction) can be com-
puted given such minor new requirements.
Also for transformations computed by MBOTs
we can reduce the problem of forward applica-
tion to the problem of computing the co-domain
with the help of the product construction of Defi-
nition 5. However, the co-domain of an MBOT is
not necessarily representable by a TSG, which is
not due to well-definedness problems but rather the
finite-copying property (Engelfriet et al, 1980) of
MBOTs. This property yields that the co-domain
might not be a regular tree language (or context-free
string language). Consequently, we cannot com-
pute forward or backward applications for arbitrary
MBOT. However, if the MBOT is equivalent to an
STSG (for example, because it was constructed by
the method presented before Theorem 3), then for-
ward and backward application can be computed es-
sentially as for STSG. This can be understood as
a warning. MBOT can efficiently be used (with
computational benefits) as an alternative represen-
tation for transformations computed by STSG (or
compositions of STSG). However, MBOT can also
compute transformations, of which the domain or
range cannot be represented by a TSG. Thus, if we
train MBOT directly and utilize their full expressive
power, then we might not be able to perform forward
and backward application.
In the unweighted case, backward application can
always be computed for MBOT. Moreover, it can be
decided using (E?sik, 1984) whether all forward ap-
plications can be represented by TSGs. However, for
a given specific TSG, it cannot be decided whether
the forward application is representable by a TSG,
which was proved by Fu?lo?p (1994). A subclass
of transformations computable by MBOT (that still
contains all transformations computable by STSG),
which allows all forward and backward applications,
has been identified by Raoult (1993).
Conclusion and acknowledgement
We compared STSGs and MBOTs on several stan-
dard algorithms (binarization, regular restriction,
composition, and application). We prove that
MBOTs offer computational benefits on all men-
tioned algorithms as long as the original transforma-
tion is computable by an STSG.
The author was financially supported by the Min-
isterio de Educacio?n y Ciencia (MEC) grants JDCI-
2007-760 and MTM-2007-63422.
883
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory
of Parsing, Translation, and Compiling. Prentice Hall.
Andre? Arnold and Max Dauchet. 1982. Morphismes
et bimorphismes d?arbres. Theoret. Comput. Sci.,
20(1):33?93.
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On for-
mal properties of simple phrase structure grammars.
In Language and Information: Selected Essays on
their Theory and Application, pages 116?150. Addi-
son Wesley.
Jean Berstel and Christophe Reutenauer. 1982. Recog-
nizable formal power series on trees. Theoret. Com-
put. Sci., 18(2):115?148.
Bjo?rn Borchardt. 2004. A pumping lemma and decid-
ability problems for recognizable tree series. Acta Cy-
bernet., 16(4):509?544.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, John D. Laf-
ferty, Robert L. Mercer, and Paul S. Roossin. 1990. A
statistical approach to machine translation. Computa-
tional Linguistics, 16(2):79?85.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. Mathematics of
statistical machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311.
David Chiang and Kevin Knight. 2006. An introduction
to synchronous grammars. In Proc. ACL tutorial.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. ACL, pages
263?270.
John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein.
2009a. Efficient parsing for transducer grammars. In
Proc. NAACL, pages 227?235.
John DeNero, Adam Pauls, and Dan Klein. 2009b.
Asynchronous binarization for synchronous gram-
mars. In Proc. ACL, pages 141?144.
Samuel Eilenberg. 1974. Automata, Languages, and
Machines. Academic Press.
Joost Engelfriet, Grzegorz Rozenberg, and Giora Slutzki.
1980. Tree transducers, L systems, and two-way ma-
chines. J. Comput. System Sci., 20(2):150?202.
Joost Engelfriet, Eric Lilin, and Andreas Maletti. 2009.
Extended multi bottom-up tree transducers: Composi-
tion and decomposition. Acta Inform., 46(8):561?590.
Zolta?n E?sik. 1984. Decidability results concerning tree
transducers II. Acta Cybernet., 6(3):303?314.
Zolta?n Fu?lo?p and Heiko Vogler. 2009. Weighted tree au-
tomata and tree transducers. In Handbook of Weighted
Automata, chapter IX, pages 313?403. Springer.
Zolta?n Fu?lo?p. 1994. Undecidable properties of determin-
istic top-down tree transducers. Theoret. Comput. Sci.,
134(2):311?328.
Jonathan S. Golan. 1999. Semirings and their Applica-
tions. Kluwer Academic, Dordrecht.
Jonathan Graehl, Kevin Knight, and Jonathan May. 2008.
Training tree transducers. Computational Linguistics,
34(3):391?427.
John E. Hopcroft and Jeffrey D. Ullman. 1979. Intro-
duction to Automata Theory, Languages and Compu-
tation. Addison Wesley.
Liang Huang, Hao Zhang, Daniel Gildea, and Kevin
Knight. 2009. Binarization of synchronous
context-free grammars. Computational Linguistics,
35(4):559?595.
Kevin Knight. 2007. Capturing practical natu-
ral language transformations. Machine Translation,
21(2):121?133.
Eric Lilin. 1981. Proprie?te?s de clo?ture d?une extension
de transducteurs d?arbres de?terministes. In CAAP, vol-
ume 112 of LNCS, pages 280?289. Springer.
Andreas Maletti and Giorgio Satta. 2009. Parsing algo-
rithms based on tree automata. In Proc. IWPT, pages
1?12.
Andreas Maletti, Jonathan Graehl, Mark Hopkins, and
Kevin Knight. 2009. The power of extended top-down
tree transducers. SIAM J. Comput., 39(2):410?430.
Mehryar Mohri. 2009. Weighted automata algorithms.
In Handbook of Weighted Automata, pages 213?254.
Springer.
Mark-Jan Nederhof and Giorgio Satta. 2003. Probabilis-
tic parsing as intersection. In Proc. IWPT, pages 137?
148.
Mark-Jan Nederhof and Giorgio Satta. 2008. Compu-
tation of distances for regular and context-free prob-
abilistic languages. Theoret. Comput. Sci., 395(2?
3):235?254.
Jean-Claude Raoult. 1993. Recursively defined tree
transductions. In Proc. RTA, volume 690 of LNCS,
pages 343?357. Springer.
Jacques Sakarovitch. 2009. Rational and recognisable
power series. In Handbook of Weighted Automata,
chapter IV, pages 105?174. Springer.
Giorgio Satta. 2010. Translation algorithms by means of
language intersection. Manuscript.
Marcel Paul Schu?tzenberger. 1961. On the definition of
a family of automata. Information and Control, 4(2?
3):245?270.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007. Bi-
narizing syntax trees to improve syntax-based machine
translation accuracy. In Proc. EMNLP-CoNLL, pages
746?754.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proc. NAACL-HLT, pages 256?263.
884
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 263?273,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Every sensible extended top-down tree transducer
is a multi bottom-up tree transducer
Andreas Maletti?
Institute for Natural Language Processing, Universit?t Stuttgart
Pfaffenwaldring 5b, 70569 Stuttgart, Germany
andreas.maletti@ims.uni-stuttgart.de
Abstract
A tree transformation is sensible if the size of
each output tree is uniformly bounded by a
linear function in the size of the correspond-
ing input tree. Every sensible tree transfor-
mation computed by an arbitrary weighted ex-
tended top-down tree transducer can also be
computed by a weighted multi bottom-up tree
transducer. This further motivates weighted
multi bottom-up tree transducers as suitable
translation models for syntax-based machine
translation.
1 Introduction
Several different translation models are used in
syntax-based statistical machine translation. Koehn
(2010) presents an introduction to statistical ma-
chine translation, and Knight (2007) presents an
overview of syntax-based statistical machine trans-
lation. The oldest and best-studied tree transfor-
mation device is the top-down tree transducer of
Rounds (1970) and Thatcher (1970). G?cseg and
Steinby (1984) and F?l?p and Vogler (2009) present
the existing results on the unweighted and weighted
model, respectively. Knight (2007) promotes the
use of weighted extended top-down tree transduc-
ers (XTOP), which have also been implemented in
the toolkit TIBURON by May and Knight (2006)
[more detail is reported by May (2010)]. In the con-
text of bimorphisms, Arnold and Dauchet (1976) in-
vestigated XTOP, and Lilin (1978) and Arnold and
Dauchet (1982) investigated multi bottom-up tree
?The author was supported by the German Research Foun-
dation (DFG) grant MA 4959/1-1.
transducers (MBOT) [as k-morphisms]. Recently,
weighted XTOP and MBOT, which are the cen-
tral devices in this contribution, were investigated
by Maletti (2011a) in the context of statistical ma-
chine translation.
Several tree transformation devices are used as
translation models in statistical machine translation.
Chiang (2007) uses synchronous context-free gram-
mars, which force translations to be very similar
as observed by Eisner (2003) and Shieber (2004).
This deficiency is overcome by synchronous tree
substitution grammars, which are state-less linear
and nondeleting XTOP. Recently, Maletti (2010b)
proposed MBOT, and Zhang et al (2008b) and
Sun et al (2009) proposed the even more powerful
synchronous tree-sequence substitution grammars.
Those two models allow certain translation discon-
tinuities, and the former device also offers computa-
tional benefits over linear and nondeleting XTOP as
argued by Maletti (2010b).
The simplicity of XTOP makes them very appeal-
ing as translation models. In 2010 the ATANLP par-
ticipants [workshop at ACL] identified ?copying? as
the most exciting and promising feature of XTOP,
but unrestricted copying can lead to an undesirable
explosion of the size of the translation. According
to Engelfriet and Maneth (2003) a tree transforma-
tion has linear size-increase if the size of each output
tree is linearly bounded by the size of its correspond-
ing input tree. The author believes that this is a very
sensible restriction that intuitively makes sense and
at the same time suitably limits the copying power
of XTOP.
We show that every sensible tree transformation
263
that can be computed by an XTOP can also be com-
puted by an MBOT. For example, linear XTOP (i.e.,
no copying) compute only sensible tree transforma-
tions, and Maletti (2008) shows that for each linear
XTOP there exists an equivalent MBOT. Here, we
do not make any restrictions on the XTOP besides
some sanity conditions (see Section 3). In particu-
lar, we consider copying XTOP. If we accept the re-
striction to linear size-increase tree transformation,
then our main result further motivates MBOT as a
suitable translation model for syntax-based machine
translation because MBOT can implement each rea-
sonable (even copying) XTOP. In addition, our re-
sult allows us to show that each reasonable XTOP
preserves regularity under backward application. As
demonstrated by May et al (2010) backward appli-
cation is the standard application of XTOP in the
machine translation pipeline, and preservation of
regularity is the essential property for several of the
evaluation algorithms of May et al (2010).
2 Notation
We start by introducing our notation for trees, whose
nodes are labeled by elements of an alphabet ? and
a set V . However, only leaves can be labeled by
elements of V . For every set T , we let
?(T ) = {?(t1, . . . , tk) | ? ? ?, t1, . . . , tk ? T} ,
which contains all trees with a ?-labeled root
and direct successors in T . The set T?(V ) of
?-trees with V -leaves is the smallest set T such that
V ? ?(T ) ? T . We use X = {x1, x2, . . . } as a set
of formal variables.
Each node of the tree t ? T?(V ) is identified by
a position p ? N+, which is a sequence of posi-
tive integers. The root is at position ? (the empty
string), and the position ip with i ? N+ and p ? N?+
is the position p in the i-th direct subtree. The
set pos(t) contains all positions of t, and the size
of t is |t| = |pos(t)|. For each p ? pos(t), the label
of t at p is t(p). Given a set L ? ??V of labels, we
let posL(t) = {p ? pos(t) | t(p) ? L} be the posi-
tions with L-labels. We write posl(t) for pos{l}(t)
for each l ? L. Finally, we write t[u]p for the tree
obtained from t by replacing the subtree at position p
by the tree u ? T?(V ).
The following notions refer to the variables X .
The tree t ? T?(V ) [potentially V ? X = ?] is
S?
NP1
PP11
x1112
VP2
VBD21
ran211
RB22
away221
Figure 1: The tree t (with positions indicated as super-
scripts) is linear and var(t) = {x2}. The tree t[He]111 is
the same tree with x2 replaced by ?He?.
linear if every x ? X occurs at most once in t (i.e.,
|posx(t)| ? 1). Moreover,
var(t) = {x ? X | posx(t) 6= ?}
contains the variables that occur in t. A substitu-
tion ? is a mapping ? : X ? T?(V ). When applied
to t, it returns the tree t?, which is obtained from t
by replacing all occurrences of x ? X in t by ?(x).
Our notions for trees are illustrated in Figure 1.
Finally, we present weighted tree grammars
(WTG) as defined by F?l?p and Vogler (2009), who
defined it for arbitrary semirings as weight struc-
tures. In contrast, our weights are always nonneg-
ative reals, which form the semiring (R+,+, ?, 0, 1)
and are used in probabilistic grammars. For each
weight assignment f : T ? R+, we let
supp(f) = {t ? T | f(t) 6= 0} .
WTG offer an efficient representation of weighted
forests (i.e., set of weighted trees), which is even
more efficient than the packed forests of Mi et al
(2008) because they can be minimized efficiently us-
ing an algorithm of Maletti and Quernheim (2011).
In particular, WTG can share more than equivalent
subtrees and can even represent infinite sets of trees.
A WTG is a system G = (Q,?, q0, P,wt) with
? a finite set Q of states (nonterminals),
? an alphabet ? of symbols,
? a starting state q0 ? Q,
? a finite set P of productions q ? r, where
q ? Q and r ? T?(Q) \Q, and
? a mapping wt: P ? R+ that assigns produc-
tion weights.
Without loss of generality, we assume that we can
distinguish states and symbols (i.e., Q ? ? = ?).
For all ?, ? ? T?(Q) and a production ? = q ? r,
264
St1 VP
t2 t3
7?
S
t2 t1 t3
Figure 2: Example rotation. In principle, such rotations
are required in the translation from English to Arabic.
we write ? ??G ? if ? = ?[q]p and ? = ?[r]p, where
p is the lexicographically least element of posQ(?).
The WTG G generates the weighted tree lan-
guage LG : T? ? R+ such that
LG(t) =
?
n?N,?1,...,?n?P
q0?
?1
G ????
?n
G t
wt(?1) ? . . . ? wt(?n)
for every t ? T?. Each such language is regular, and
Reg(?) contains all those languages over the alpha-
bet ?. A thorough introduction to tree languages is
presented by G?cseg and Steinby (1984) and G?c-
seg and Steinby (1997) for the unweighted case and
by F?l?p and Vogler (2009) for the weighted case.
3 Extended top-down tree transducers
We start by introducing the main model of this
contribution. Extended top-down tree transducers
(XTOP) are a generalization of the top-down tree
transducers (TOP) of Rounds (1970) and Thatcher
(1970). XTOP allow rules with several (non-state
and non-variable) symbols in the left-hand side (as
in the rule of Figure 3), whereas a TOP rule contains
exactly one symbol in the left-hand side. Shieber
(2004) and Knight (2007) identified that this exten-
sion is essential for many NLP applications because
without it linear (i.e., non-copying) cannot compute
rotations (see Figure 2). In the form of bimorphisms
XTOP were investigated by Arnold and Dauchet
(1976) and Arnold and Dauchet (1982) in the 1970s,
and Knight (2007) invigorated research.
As demonstrated by Graehl et al (2009) the
most general XTOP model includes copying, dele-
tion, and regular look-ahead in the spirit of En-
gelfriet (1977). More powerful models (such as
synchronous tree-sequence substitution grammars
and multi bottom-up tree transducers) can handle
translation discontinuities naturally as evidenced
by Zhang et al (2008a) and Maletti (2011b), but
q0
S
x1 VP
x2 x3
?
S
qVB
x2
qNP
x1
qNP
x3
Figure 3: Example XTOP rule by Graehl et al (2008).
XTOP need copying and deletion to handle them.
Copying essentially allows an XTOP to translate
certain parts of the input several times and was iden-
tified by the ATANLP 2010 participants as one of the
most interesting and promising features of XTOP.
Currently, the look-ahead feature is not used in ma-
chine translation, but we need it later on in the theo-
retical development.
Given an alphabet Q and a set T , we let
Q[T ] = {q(t) | q ? Q, t ? T},
in which the root always has exactly one succes-
sor from T in contrast to Q(T ). We treat elements
of Q[T?(V )] as special trees of T??Q(V ). More-
over, we let 1??(t) = 1 for every t ? T?. XTOP
with regular look-ahead (XTOPR) were also stud-
ied by Knight and Graehl (2005) and Graehl et al
(2008). Formally, an XTOPR is a system
M = (Q,?,?, q0, R, c,wt)
with
? a finite set Q of states,
? alphabets ? and ? of input and output symbols,
? a starting state q0 ? Q,
? a finite set R of rules of the form ` ? r with
linear ` ? Q[T?(X)] and r ? T?(Q[var(`)]),
? c : R ? X ? Reg(?) assigns a regular look-
ahead to each deleted variable of a rule [i.e.,
c(` ? r, x) = 1?? for all ` ? r ? R and
x ? X \ (var(`) \ var(r))], and
? wt: R? R+ assigns rule weights.
The XTOPR M is linear [respectively, nondeleting]
if r is linear [respectively, var(`) = var(r)] for ev-
ery rule ` ? r ? R. It has no look-ahead (XTOP)
if c(?, x) = 1?? for all ? ? R and x ? X . Figure 3
shows a rule of a linear and nondeleting XTOP.
The look-ahead can be used to restrict rule appli-
cations. It can inspect subtrees that are deleted by a
265
uq0
S
t1
VP
t2 t3
??,.5M
u
S
qVB
t2
qNP
t1
qNP
t3
Figure 4: Rewrite step using rule ? of Figure 3.
rule application, so for each rule ? = ` ? r, we let
del(?) = var(`) \ var(r) be the set of deleted vari-
ables in ?. If we suppose that a variable x ? del(?)
matches to an input subtree t, then the weight of the
look-ahead c(?, x)(t), which we also write c?,x(t),
is applied to the derivation. If it is 0, then this look-
ahead essentially prohibits the application of ?. It is
important that the look-ahead is regular (i.e., there
exists a WTG accepting it). The toolkit TIBURON
by May and Knight (2006) implements XTOP to-
gether with a number of essential operations. Look-
ahead is not implemented in TIBURON, but it can
be simulated using a composition of two XTOP, in
which the first XTOP performs the look-ahead and
marks the results, so that the second XTOP can ac-
cess the look-ahead information.
As for WTG the semantics for the XTOPR
M = (Q,?,?, I, R, c,wt) is presented using
rewriting. Without loss of generality, we again sup-
pose that Q ? (? ??) = ?. Let ?, ? ? T?(Q[T?]),
w ? R+, and ? = ` ? r be a rule of R. We write
? ??,wM ? if there exists a substitution ? : X ? T?
such that
? ? = ?[`?]p,
? ? = ?[r?]p, and
? w = wt(?) ?
?
x?del(?) c?,x(x?),
where p ? posQ(?) is the lexicographically least
Q-labeled position in ?. Figure 4 illustrates a deriva-
tion step.
The XTOPR M computes a weighted tree trans-
formation by applying rewrite steps to the tree q0(t),
where t ? T? is the input tree, until an output
tree u ? T? has been produced. The weight of a
particular derivation is obtained by multiplying the
weights of the rewrite steps. The weight of the trans-
formation from t to u is obtained by summing all
weights of the derivations from q0(t) to u. For-
mally1, the weighted tree transformation computed
by M in state q ? Q is
? qM (t, u) =
?
n?N,?1,...,?n?R
q(t)?
?1,w1
M ????
?n,wn
M u
w1 ? . . . ? wn (1)
for every t ? T? and u ? T?. The XTOPR M
computes the weighted tree transformation ? q0M . Two
XTOPR M and N are equivalent, if ?M = ?N .
The sum (1) can be infinite, which we avoid by
simply requiring that all our XTOPR are produc-
ing, which means that r /? Q[X] for every rule
` ? r ? R.2 In a producing XTOPR each rule ap-
plication produces at least one output symbol, which
limits the number n of rule applications to the size of
the output tree u. A detailed exposition to XTOPR is
presented by Arnold and Dauchet (1982) and Graehl
et al (2009) for the unweighted case and by F?l?p
and Vogler (2009) for the weighted case.
Example 1. LetMex = (Q,?,?, q, R, c,wt) be the
nondeleting XTOP with
? Q = {q},
? ? = {?, ?, ?},
? the two rules
q(?)? ? (?)
q(?(x1))? ?(q(x1), q(x1)) (??)
? trivial look-ahead (i.e., c(?, x) = 1??), and
? wt(?) = 2 and wt(??) = 1.
The XTOPR Mex computes the tree transformation
that turns the input tree ?n(?) into the fully balanced
binary tree u of the same height with weight 2(2
n).
An example derivation is presented in Figure 5.
Unrestricted copying (as in Example 1) yields
very undesirable phenomena and is most likely not
needed in the machine translation task. In fact, it
is almost universally agreed that a translation model
should be ?linear-size increase?, which means that
1There is an additional restriction that is discussed in the
next paragraph.
2This is a convenience requirement. We can use other con-
ditions on the XTOPR or the used weight structures to guarantee
a well-defined semantics.
266
q?
?
?
??
?,1
Mex
?
q
?
?
q
?
?
??
?,1
Mex
?
?
q
?
q
?
q
?
?
??
?,1
Mex
?
?
q
?
q
?
?
q
?
q
?
??,2Mex
?
?
? q
?
?
q
?
q
?
??,2Mex ? ? ? ?
?,2
Mex
?
?
? ?
?
? ?
Figure 5: Example derivation using the XTOP Mex with weight 13 ? 24 = 16.
the size of each output tree should be linearly
bounded in the size of the corresponding input tree
according to Aho and Ullman (1971) and Engelfriet
and Maneth (2003).
Definition 2. A mapping ? : T? ? T? ? R+ is
linear-size increase if there exists an integer n ? N
such that |u| ? n ? |t| for all (t, u) ? supp(?).
An XTOPR M is sensible if ?M is linear-size in-
crease.
?Sensible? is not a syntactic property of an
XTOPR as it does not depend on the actual rules,
but only on its computed weighted tree transforma-
tion. The XTOP Mex of Example 1 is not sensible
because |u| = 2|t| ? 1 for every (t, u) ? ?Mex . In-
tuitively, the number of times that Mex can use the
copying rule ?? is not uniformly bounded.
We need an auxiliary result in the main part.
Let ? : T? ? T? ? R+ be a weighted tree
transformation. We need the weighted tree lan-
guage ??1(u) : T? ? R+ of input trees weighted
by their translation weight to a given output
tree u ? T?. Formally,
(
??1(u)
)
(t) = ?(t, u) for
every t ? T?.
Theorem 3. For every producing XTOPR M and
output tree u? ? T?, the weighted tree lan-
guage ??1M (u
?) is regular.
Proof sketch. We use some properties that are only
defined in the next sections (for proof economy). It
is recommended to skip this proof on the first read-
ing and revisit it later. Maletti (2010a) shows that
we can construct an XTOPR M ? such that
?M ?(t, u) =
{
?M (t, u) if u? = u
0 otherwise
for every t ? T? and u ? T?. This operation is
called ?output product? by Maletti (2010a). The ob-
tained XTOPR M ? is also producing, so we know
that M ? can take at most |u?| rewrite steps to de-
rive u?. Since M ? can only produce the output
tree u?, this also limits the total number of rule appli-
cations in any successful derivation. Consequently,
M ? can only apply a copying rule at most |u?| times,
which shows that M ? is finitely copying (see Def-
inition 8). By Theorem 11 we can implement M ?
by an equivalent MBOT M ?? (i.e., ?M ?? = ?M ? ;
see Section 5), for which we know by Theorem 14
of Maletti (2011a) that ??1M ??(u) = ?
?1
M ? (u) is regu-
lar.
Finally, let us illustrate the overall structure of our
arguments to show that every sensible XTOPR can
be implemented by an equivalent MBOT. We first
normalize the given XTOPR such that the seman-
tic property ?sensible? yields a syntactic property
called ?finitely copying? (see Section 4). In a second
step, we show that each finitely copying XTOPR can
be implemented by an equivalent MBOT (see Sec-
tion 5). Figure 6 illustrates these steps towards our
main result. In the final section, we derive some con-
sequences from our main result (see Section 6).
4 From sensible to finite copying
First, we adjust a normal form of Engelfriet and
Maneth (2003) to our needs. This section bor-
rows heavily from Aho and Ullman (1971) and En-
gelfriet and Maneth (2003), where ?sensible?
(unweighted) deterministic macro tree transduc-
ers (MAC) [see Engelfriet and Vogler (1985)] are
considered. Our setting is simpler on the one hand
because XTOPR do not have context parameters
as MAC, but more difficult on the other hand be-
cause we consider nondeterministic and weighted
transducers.
Intuitively, a sensible XTOPR cannot copy a lot
since the size of each output tree is linearly bounded
in the size of the corresponding input tree. However,
the actual presentation of the XTOPR M might con-
267
sensible XTOPR
sensible proper XTOPR
finitely copying XTOPR
linear and nondeleting MBOT
Figure 6: Overview of the proof steps.
tain rules that allow unbounded copying. This un-
bounded copying might not manifest due to the look-
ahead restrictions or due to the fact that those rules
cannot be used in a successful derivation. The pur-
pose of the normal form is the elimination of those
artifacts. To this end, we eliminate all states (except
the initial state) that can only produce finitely many
outputs. Such a state can simply be replaced by one
of the output trees that it can produce and an ad-
ditional look-ahead that checks whether the current
input tree indeed allows that translation (and inserts
the correct translation weight).
Normalized XTOPR are called ?proper?, and we
define this property next. For the rest of this section,
let M = (Q,?,?, q0, R, c,wt) be the considered
sensible XTOPR. Without loss of generality, we as-
sume that the state q0 does not occur in the right-
hand sides of rules. Moreover, we write ? ??M ? if
there exist nonzero weights w1, . . . , wn ? R+ \ {0}
and rules ?1, . . . , ?n ? R with
? ??1,w1M ? ? ? ?
?n,wn
M ? .
In essence, ? ??M ? means that M can transform ?
into ? (in the unweighted setting).
Definition 4. A state q ? Q is proper if there are in-
finitely many u? ? T? such that there exists a deriva-
tion
q0(t)?
?
M ?[q(s)]p ?
?
M u[u
?]p
where s, t ? T? are input trees, ? ? T?(Q[T?]),
p ? pos(?), and u ? T? is an output tree.
The derivation in Definition 4 is illustrated in Fig-
ure 7. In other words, a proper state is reachable
from the initial state and can transform infinitely
many input trees into infinitely many output trees.
The latter is an immediate consequence of Defini-
tion 4 since each input tree can be transformed into
only finitely many output trees due to sensibility.
The restriction includes the look-ahead (because we
require that the rewrite step weights are nonzero),
which might further restrict the input trees.
Example 5. The state q of the XTOP Mex is proper
because we already demonstrated that it can trans-
form infinitely many input trees into infinitely many
output trees.
The XTOPR M is proper if all its states except
the initial state q0 are proper. Next, we show that
each XTOPR can be transformed into an equivalent
proper XTOPR using a simplified version of the con-
struction of Lemma 5.4 by Engelfriet and Maneth
(2003). Mind that we generally assume that all con-
sidered XTOPR are producing.
Theorem 6. For every XTOPR there exists an equiv-
alent proper XTOPR.
Proof sketch. The construction is iterative. Sup-
pose that M is not yet proper. Then there exists
a state q ? Q, which can produce only finitely
many outputs U . It can be decided whether a state
is proper using Theorem 4.5 of Drewes and Engel-
friet (1998), and in case it is proper, the set U can
also be computed effectively. The cited theorem ap-
plies to unweighted XTOPR, but it can be applied
also in our setting because??M in Definition 4 dis-
regards weights. Now we consider each u ? U in-
dividually. Clearly, (? qM )
?1(u) is regular by The-
orem 3. For each u and each occurrence of q in
the right-hand side of a rule ? ? R of M , we cre-
ate a copy ?? of ?, in which the selected occur-
rence of q(x) is replaced by u and the new look-
ahead is c(??, x) = c(?, x) ? (? qM )
?1(u), which re-
stricts the input tree appropriately and includes the
adjustment of the weights. Since regular weighted
tree languages are closed under HADAMARD prod-
ucts [see F?l?p and Vogler (2009)], the look-ahead
c(?, x) ? (? qM )
?1(u) is again regular.
Essentially, we precompute the action of q as
much as possible, and immediately output one of
the finitely many output trees, check that the input
tree has the required shape using the look-ahead,
and charge the weight for the precomputed trans-
formation again using the look-ahead. This pro-
cess is done for each occurrence, so if a rule con-
tains two occurrences of q, then the process must be
268
q0
t ??M
...
...
q
s
??M
...
...
u?
Figure 7: Illustration of the derivation in Definition 4.
done twice to this rule. In this way, we eventually
purge all occurrences of q from the right-hand sides
of rules of M without changing the computed trans-
formation. Since q 6= q0 and q is now unreachable,
it is useless and can be deleted, which removes one
non-proper state. This process is repeated until all
states except the initial state q0 are proper.
Clearly, the construction of Theorem 6 applied
to a sensible XTOPR M yields a sensible proper
XTOPR M ? since the property ?sensible? refers to
the computed transformation and ?M = ?M ? . Let us
illustrate the construction on a small example.
Example 7. Let ? be the rule displayed in Figure 3,
and let us assume that the state qVB is not proper.
Moreover, suppose that qVB can yield the output
tree u and that we already computed the translation
options that yield u. Let t1, . . . , tn ? T? be those
translation options. Then we create the copy ??
q0(S(x1,VP(x2, x3)))? S(u, qNP(x1), qNP(x3))
of the rule ? with look-ahead c?(??, x) such that
c???,x(t) =
{
c?,x(t) if x 6= x2
? qVBM (t, u) if x = x2 .
In general, there can be infinitely many input
trees ti that translate to a selected output tree u, so
we cannot simply replace the variable in the left-
hand side by all the options for the input tree. This
is the reason why we use the look-ahead because the
set ??1M (u) is a regular weighted tree language.
From now on, we assume that the XTOPR M is
proper. Next, we want to invoke Theorem 7.1 of En-
gelfriet and Maneth (2003) to show that a proper
sensible XTOPR is finitely copying. Engelfriet and
Maneth (2003) present a formal definition of finite
copying, but we only present a high-level descrip-
tion of it.
Definition 8. The XTOPR M is finitely copying if
there is a copying bound n ? N such that no input
subtree is copied more than n times in any derivation
q(t)??M u with q ? Q, t ? T?, and u ? T?.
Example 9. The XTOP of Example 1 is not finitely
copying as the input subtree ? is copied 2n times if
the input tree is ?n(?). Clearly, this shows that there
is no uniform bound on the number of copies.
It is worth noting that the properties ?sensible? and
?finitely copying? are essentially unweighted prop-
erties. They largely disregard the weights and a
weighted XTOPR does have one of those properties
if and only if its associated unweighted XTOPR has
it. We now use this tight connection to lift Theo-
rem 7.1 of Engelfriet and Maneth (2003) from the
unweighted (and deterministic) case to the weighted
(and nondeterministic) case.
Theorem 10. If a proper XTOPR is sensible, then it
is finitely copying.
Proof. Let M be the input XTOPR. Since M is sen-
sible, its associated unweighted XTOPR N , which
is obtained by setting all weights to 1 and comput-
ing in the BOOLEAN semiring, is sensible. Conse-
quently,N is finitely copying by Theorem 7.1 of En-
gelfriet and Maneth (2003). Thus, also M is finitely
copying, which concludes the proof. We remark
that Theorem 7.1 of Engelfriet and Maneth (2003)
only applies to deterministic XTOPR, but the essen-
tial pumping argument, which is Lemma 6.2 of En-
gelfriet and Maneth (2003) also works for nonde-
terministic XTOPR. Essentially, the pumping argu-
ment shows the contraposition. If M is not finitely
copying, then M can copy a certain subtree an arbi-
trarily often. Due to the properness of M , all these
copies have an impact on the output tree, which
yields that its size grows beyond any uniform lin-
ear bound, which in turn demonstrates that M is not
sensible.
269
We showed that each sensible XTOPR can be im-
plemented by a finitely copying XTOPR via the con-
struction of the proper normal form. This approach
actually yields a characterization because finitely
copying XTOPR are trivially sensible by Theo-
rem 4.19 of Engelfriet and Maneth (2003).
5 From finite copying to an MBOT
We complete the argument by showing how to im-
plement a finitely copying XTOPR by a weighted
multi bottom-up tree transducer (MBOT). First, we
recall the MBOT, which was introduced by Arnold
and Dauchet (1982) and Lilin (1978) in the un-
weighted case. Engelfriet et al (2009) give an En-
glish presentation. We present the linear and non-
deleting MBOT of Engelfriet et al (2009).
A weighted multi bottom-up tree transducer is a
system M = (Q,?,?, F,R,wt) with
? an alphabet Q of states,
? alphabets ? and ? of input and output symbols,
? a set F ? Q of final states,
? a finite set R of rules of the form ` ? r where
` ? T?(Q(X)) and r ? Q(T?(X)) are linear
and var(`) = var(r), and
? wt: R? R+ assigning rule weights.
We now use T?(Q(X)) and Q(T?(X)) instead of
T?(Q[X]) and Q[T?(X)], which highlights the dif-
ference between XTOPR and MBOT. First, MBOT
are a bottom-up device, which yields that ? and ?
as well as ` and r exchange their place. More impor-
tantly, MBOT can use states with more than 1 suc-
cessor (e.g, Q(X) instead of Q[X]). An example
rule is displayed in Figure 8.
Let M = (Q,?,?, F,R,wt) be an MBOT such
thatQ?(???) = ?.3 We require that r /? Q(X) for
each rule ` ? r ? R to guarantee finite derivations
and thus a well-defined semantics.4 As before, we
present a rewrite semantics. Let ?, ? ? T?(Q(T?)),
and let ? = ` ? r be a rule. We write ? ??M ?
if there exists a substitution ? : X ? T? such that
? = ?[`?]p and ? = ?[r?]p, where p ? pos(?) be is
the lexicographically least reducible position in ?. A
rewrite step is illustrated in Figure 8.
3This restriction can always be achieved by renaming the
states.
4Again this could have been achieved with the help of other
conditions on the MBOT or the used weight structure.
The weighted tree transformation computed byM
in state q ? Q is
? qM (t, u1 ? ? ?uk) =
?
n?N,?1,...,?n?R
t?
?1
M ????
?n
M q(u1,...,uk)
wt(?1) ? . . . ? wt(?n)
for all t ? T? and u1, . . . , uk ? T?. The semantics
of M is ?M (t, u) =
?
q?F ?
q
M (t, u) for all t ? T?
and u ? T?.
We move to the last step for our main result,
in which we show how to implement each finitely
copying XTOPR by an MBOT using a weighted ver-
sion of the construction in Lemma 15 of Maletti
(2008). The computational benefits (binarization,
composition, efficient parsing, etc.) of MBOT over
XTOPR are described by Maletti (2011a).
Theorem 11. Every finitely copying XTOPR can be
implemented by an MBOT.
Proof sketch. We plan to utilize Theorem 18 of En-
gelfriet et al (2009), which proves the same state-
ment in the unweighted and deterministic case.
Again, the weights are not problematic, but we need
to remove the nondeterminism before we can apply
it. This is achieved by a decomposition into two
XTOPR. The first XTOPR annotates the input tree
with the rules that the second XTOPR is supposed to
use. Thus, the first XTOPR remains nondeterminis-
tic, but the second XTOPR, which simply executes
the annotated rules, is now deterministic. This stan-
dard approach due to Engelfriet (1975) is used in
many similar constructions.
Suppose that n is a copying bound for the input
XTOPR M , which means that no more than n rules
are applied to each input symbol. The first XTOPR
is actually a nondeterministic linear and nondeleting
XTOP that annotates each input tree symbol with ex-
actly n rules of M that are consistent with the state
behavior of M . Moreover, the annotation also pre-
scribes with which of n rules the processing should
continue at each subtree. Since we know all the rules
that will potentially be applied for a certain symbol,
we can make the assignment such that no annotated
rule is used twice in the same derivation. The de-
tails for this construction can be found in Lemma 15
of Maletti (2008).
In this way, we obtain a weighted linear and non-
deleting XTOP M1, which includes the look-ahead,
270
SqNP
x1
VP
qVB
x2
qNP
x3 x4
?
qS
S
x2 x1 x3
x4
t
S
qNP
u1
VP
qVB
u2
qNP
u3 u4
??M
t
qS
S
u2 u1 u3
u4
Figure 8: Example MBOT rule ? [left] and its use in a rewrite step [right].
and an unweighted deterministic XTOP M2. Only
the weight and look-ahead of rules that are actu-
ally executed are applied (e.g., although we anno-
tate n rules at the root symbol, we only execute the
first rule and thus only apply its weight and look-
ahead). The look-ahead of different rules is either
resolved (i.e., pushed to the next rules) or multi-
plied using the HADAMARD product [see F?l?p and
Vogler (2009)], which preserves regularity. This
process is also used by Seemann et al (2012). Now
we can use Theorem 4 of Maletti (2011a) to obtain
an MBOT N1 that is equivalent to M1. Similarly,
we can use Theorem 18 of Engelfriet et al (2009)
to obtain an MBOT N2 that is equivalent to M2.
Since MBOT are closed under composition by The-
orem 23 of Engelfriet et al (2009), we can compose
N1 andN2 to obtain a single MBOTN that is equiv-
alent to M .
Corollary 12. For every sensible producing XTOPR
there exists an equivalent MBOT.
Proof. Theorem 6 shows that there exists an equiva-
lent proper XTOPR, which must be finitely copying
by Theorem 10. This last fact allows us to construct
an equivalent MBOT by Theorem 11.
6 Preservation of regularity
Finally, we present an application of Corollary 12 to
solve an open problem. The translation model is of-
ten used in a backwards manner in a machine trans-
lation system as demonstrated, for example, by May
et al (2010), which means that an output tree is sup-
plied and the corresponding input trees are sought.
This starting output tree is typically the best parse
of the string that we want to translate. However, in-
stead of a single tree, we want to use all parses of
this sentence together with their parse scores. Those
parses form a regular weighted tree language, and
applying them backwards to the translation model
yields another weighted tree language L of corre-
sponding input trees. For an efficient representation
and efficient modification algorithms (such a k-best
extraction) we would like L to be regular. However,
F?l?p et al (2011) demonstrate that the backward
application of a regular weighted tree language to
an XTOPR is not necessarily regular. The counterex-
ample uses a variant of the XTOP of Example 1 and
is thus not sensible. Theorem 14 of Maletti (2011a)
shows that MBOT preserve regularity under back-
ward application.
Corollary 13. Sensible XTOPR preserve regularity
under backward application.
Conclusion
We demonstrated that each sensible XTOPR can be
implemented by an MBOT. The latter formalism of-
fers many computational advantages, so that the au-
thor believes that MBOT should be used instead of
XTOP. We used real number weights, but the author
believes that our results carry over to at least all zero-
sum and zero-divisor free semirings [see Hebisch
and Weinert (1998) and Golan (1999)], which are
semirings such that (i) a+ b = 0 implies a = 0 and
(ii) a ? b = 0 implies 0 ? {a, b}. Whether our results
hold in other semirings (such as the semiring of all
reals where ?1 + 1 = 0) remains an open question.
271
References
Alfred V. Aho and Jeffrey D. Ullman. 1971. Transla-
tions on a context-free grammar. Inform. and Control,
19(5):439?475.
Andr? Arnold and Max Dauchet. 1976. Bi-transductions
de for?ts. In Proc. 3th Int. Coll. Automata, Languages
and Programming, pages 74?86. University of Edin-
burgh.
Andr? Arnold and Max Dauchet. 1982. Morphismes
et bimorphismes d?arbres. Theoret. Comput. Sci.,
20(1):33?93.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Comput. Linguist., 33(2):201?228.
Frank Drewes and Joost Engelfriet. 1998. Decidability
of the finiteness of ranges of tree transductions. In-
form. and Comput., 145(1):1?50.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proc. 41st Ann.
Meeting Association for Computational Linguistics,
pages 205?208. Association for Computational Lin-
guistics.
Joost Engelfriet and Sebastian Maneth. 2003. Macro tree
translations of linear size increase are MSO definable.
SIAM J. Comput., 32(4):950?1006.
Joost Engelfriet and Heiko Vogler. 1985. Macro tree
transducers. J. Comput. System Sci., 31(1):71?146.
Joost Engelfriet, Eric Lilin, and Andreas Maletti. 2009.
Extended multi bottom-up tree transducers ? compo-
sition and decomposition. Acta Inform., 46(8):561?
590.
Joost Engelfriet. 1975. Bottom-up and top-down tree
transformations ? a comparison. Math. Systems The-
ory, 9(3):198?231.
Joost Engelfriet. 1977. Top-down tree transducers
with regular look-ahead. Math. Systems Theory,
10(1):289?303.
Zolt?n F?l?p and Heiko Vogler. 2009. Weighted tree
automata and tree transducers. In Manfred Droste,
Werner Kuich, and Heiko Vogler, editors, Handbook
of Weighted Automata, EATCS Monographs on Theo-
ret. Comput. Sci., chapter 9, pages 313?403. Springer.
Zolt?n F?l?p, Andreas Maletti, and Heiko Vogler. 2011.
Weighted extended tree transducers. Fundam. Inform.,
111(2):163?202.
Ferenc G?cseg and Magnus Steinby. 1984. Tree Au-
tomata. Akad?miai Kiad?, Budapest.
Ferenc G?cseg and Magnus Steinby. 1997. Tree lan-
guages. In Grzegorz Rozenberg and Arto Salomaa,
editors, Handbook of Formal Languages, volume 3,
chapter 1, pages 1?68. Springer.
Jonathan S. Golan. 1999. Semirings and their Applica-
tions. Kluwer Academic, Dordrecht.
Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Comput. Linguist.,
34(3):391?427.
Jonathan Graehl, Mark Hopkins, Kevin Knight, and An-
dreas Maletti. 2009. The power of extended top-down
tree transducers. SIAM J. Comput., 39(2):410?430.
Udo Hebisch and Hanns J. Weinert. 1998. Semirings?
Algebraic Theory and Applications in Computer Sci-
ence. World Scientific.
Kevin Knight and Jonathan Graehl. 2005. An overview
of probabilistic tree transducers for natural language
processing. In Proc. 6th Int. Conf. Computational Lin-
guistics and Intelligent Text Processing, volume 3406
of LNCS, pages 1?24. Springer.
Kevin Knight. 2007. Capturing practical natu-
ral language transformations. Machine Translation,
21(2):121?133.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Eric Lilin. 1978. Une g?n?ralisation des transduc-
teurs d??tats finis d?arbres: les S-transducteurs. Th?se
3?me cycle, Universit? de Lille.
Andreas Maletti and Daniel Quernheim. 2011. Pushing
for weighted tree automata. In Proc. 36th Int. Symp.
Mathematical Foundations of Computer Science, vol-
ume 6907 of LNCS, pages 460?471. Springer.
Andreas Maletti. 2008. Compositions of extended top-
down tree transducers. Inform. and Comput., 206(9?
10):1187?1196.
Andreas Maletti. 2010a. Input and output products
for weighted extended top-down tree transducers. In
Proc. 14th Int. Conf. Developments in Language The-
ory, volume 6224 of LNCS, pages 316?327. Springer.
Andreas Maletti. 2010b. Why synchronous tree substitu-
tion grammars? In Proc. Human Language Technolo-
gies: Conf. North American Chapter of the ACL, pages
876?884. Association for Computational Linguistics.
Andreas Maletti. 2011a. An alternative to synchronous
tree substitution grammars. J. Natur. Lang. Engrg.,
17(2):221?242.
Andreas Maletti. 2011b. How to train your multi bottom-
up tree transducer. In Proc. 49th Ann. Meeting Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 825?834. Association for
Computational Linguistics.
Jonathan May and Kevin Knight. 2006. Tiburon: A
weighted tree automata toolkit. In Proc. 11th Int.
Conf. Implementation and Application of Automata,
volume 4094 of LNCS, pages 102?113. Springer.
Jonathan May, Kevin Knight, and Heiko Vogler. 2010.
Efficient inference through cascades of weighted tree
transducers. In Proc. 48th Ann. Meeting Association
for Computational Linguistics, pages 1058?1066. As-
sociation for Computational Linguistics.
272
Jonathan May. 2010. Weighted Tree Automata and
Transducers for Syntactic Natural Language Process-
ing. Ph.D. thesis, University of Southern California,
Los Angeles.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. 46th Ann. Meeting Associ-
ation for Computational Linguistics, pages 192?199.
Association for Computational Linguistics.
William C. Rounds. 1970. Mappings and grammars on
trees. Math. Systems Theory, 4(3):257?287.
Nina Seemann, Daniel Quernheim, Fabienne Braune, and
Andreas Maletti. 2012. Preservation of recognizabil-
ity for weighted linear extended top-down tree trans-
ducers. In Proc. 2nd Workshop Applications of Tree
Automata in Natural Language Processing, pages 1?
10. Association for Computational Linguistics.
Stuart M. Shieber. 2004. Synchronous grammars as tree
transducers. In Proc. 7th Int. Workshop Tree Adjoining
Grammars and Related Formalisms, pages 88?95.
Jun Sun, Min Zhang, and Chew Lim Tan. 2009. A
non-contiguous tree sequence alignment-based model
for statistical machine translation. In Proc. 47th Ann.
Meeting Association for Computational Linguistics,
pages 914?922. Association for Computational Lin-
guistics.
James W. Thatcher. 1970. Generalized2 sequential ma-
chine maps. J. Comput. System Sci., 4(4):339?367.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008a. A tree se-
quence alignment-based tree-to-tree translation model.
In Proc. 46th Ann. Meeting Association for Compu-
tational Linguistics, pages 559?567. Association for
Computational Linguistics.
Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw, and
Sheng Li. 2008b. Grammar comparison study for
translational equivalence modeling and statistical ma-
chine translation. In Proc. 22nd Int. Conf. Computa-
tional Linguistics, pages 1097?1104. Association for
Computational Linguistics.
273
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1067?1076,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A Tree Transducer Model for Synchronous Tree-Adjoining Grammars
Andreas Maletti
Universitat Rovira i Virgili
Avinguda de Catalunya 25, 43002 Tarragona, Spain.
andreas.maletti@urv.cat
Abstract
A characterization of the expressive power
of synchronous tree-adjoining grammars
(STAGs) in terms of tree transducers (or
equivalently, synchronous tree substitution
grammars) is developed. Essentially, a
STAG corresponds to an extended tree
transducer that uses explicit substitution in
both the input and output. This characteri-
zation allows the easy integration of STAG
into toolkits for extended tree transducers.
Moreover, the applicability of the charac-
terization to several representational and
algorithmic problems is demonstrated.
1 Introduction
Machine translation has seen a multitude of for-
mal translation models. Here we focus on syntax-
based (or tree-based) models. One of the old-
est models is the synchronous context-free gram-
mar (Aho and Ullman, 1972). It is clearly too
weak as a syntax-based model, but found use in
the string-based setting. Top-down tree transduc-
ers (Rounds, 1970; Thatcher, 1970) have been
heavily investigated in the formal language com-
munity (Ge?cseg and Steinby, 1984; Ge?cseg and
Steinby, 1997), but as argued by Shieber (2004)
they are still too weak for syntax-based machine
translation. Instead Shieber (2004) proposes syn-
chronous tree substitution grammars (STSGs) and
develops an equivalent bimorphism (Arnold and
Dauchet, 1982) characterization. This character-
ization eventually led to the rediscovery of ex-
tended tree transducers (Graehl and Knight, 2004;
Knight and Graehl, 2005; Graehl et al, 2008),
which are essentially as powerful as STSG. They
had been studied already by Arnold and Dauchet
(1982) in the form of bimorphisms, but received
little attention until rediscovered.
Shieber (2007) claims that even STSGs might
be too simple to capture naturally occuring transla-
tion phenomena. Instead Shieber (2007) suggests
a yet more powerful mechanism, synchronous
tree-adjoining grammars (STAGs) as introduced
by Shieber and Schabes (1990), that can capture
certain (mildly) context-sensitive features of natu-
ral language. In the tradition of Shieber (2004), a
characterization of the power of STAGs in terms
of bimorphims was developed by Shieber (2006).
The bimorphisms used are rather unconventional
because they consist of a regular tree language and
two embedded tree transducers (instead of two tree
homomorphisms). Such embedded tree transduc-
ers (Shieber, 2006) are particular macro tree trans-
ducers (Courcelle and Franchi-Zannettacci, 1982;
Engelfriet and Vogler, 1985).
In this contribution, we try to unify the pic-
ture even further. We will develop a tree trans-
ducer model that can simulate STAGs. It turns out
that the adjunction operation of an STAG can be
explained easily by explicit substitution. In this
sense, the slogan that an STAG is an STSG with
adjunction, which refers to the syntax, also trans-
lates to the semantics. We prove that any tree
transformation computed by an STAG can also be
computed by an STSG using explicit substitution.
Thus, a simple evaluation procedure that performs
the explicit substitution is all that is needed to sim-
ulate an STAG in a toolkit for STSGs or extended
tree transducers like TIBURON by May and Knight
(2006).
We show that some standard algorithms on
STAG can actually be run on the constructed
STSG, which often is simpler and better under-
stood. Further, it might be easier to develop new
algorithms with the alternative characterization,
which we demonstrate with a product construc-
tion for input restriction in the spirit of Neder-
hof (2009). Finally, we also present a complete
tree transducer model that is as powerful as STAG,
which is an extension of the embedded tree trans-
ducers of Shieber (2006).
1067
2 Notation
We quickly recall some central notions about trees,
tree languages, and tree transformations. For a
more in-depth discussion we refer to Ge?cseg and
Steinby (1984) and Ge?cseg and Steinby (1997). A
finite set ? of labels is an alphabet. The set of all
strings over that alphabet is ?? where ? denotes
the empty string. To simplify the presentation, we
assume an infinite set X = {x1, x2, . . . } of vari-
ables. Those variables are syntactic and represent
only themselves. In particular, they are all differ-
ent. For each k ? 0, we let Xk = {x1, . . . , xk}.
We can also form trees over the alphabet ?. To
allow some more flexibility, we will also allow
leaves from a special set V . Formally, a ?-tree
over V is either:
? a leaf labeled with an element of v ? ? ? V ,
or
? a node that is labeled with an element of ?
with k ? 1 children such that each child is a
?-tree over V itself.1
The set of all ?-trees over V is denoted by T?(V ).
We just write T? for T?(?). The trees in Figure 1
are, for example, elements of T?(Y ) where
? = {S,NP,VP,V,DT,N}
Y = {saw, the} .
We often present trees as terms. A leaf labeled v
is simply written as v. The tree with a root node
labeled ? is written ?(t1, . . . , tk) where t1, . . . , tk
are the term representations of its k children.
A tree language is any subset of T?(V ) for
some alphabet ? and set V . Given another al-
phabet ? and a set Y , a tree transformation is a
relation ? ? T?(V ) ? T?(Y ). In many of our
examples we have V = ? = Y . Occasionally,
we also speak about the translation of a tree trans-
formation ? ? T? ? T?. The translation of ? is
the relation {(yd(t), yd(u)) | (t, u) ? ?} where
yd(t), the yield of t, is the sequence of leaf labels
in a left-to-right tree traversal of t. The yield of the
third tree in Figure 1 is ?the N saw the N?. Note
that the translation is a relation ? ? ? ?? ???.
3 Substitution
A standard operation on (labeled) trees is substitu-
tion, which replaces leaves with a specified label
in one tree by another tree. We write t[u]A for (the
1Note that we do not require the symbols to have a fixed
rank; i.e., a symbol does not determine its number of children.
S
NP VP
V
saw
NP
NP
DT
the
N
S
NP
DT
the
N
VP
V
saw
NP
DT
the
N
t u t[u]NP
Figure 1: A substitution.
result of) the substitution that replaces all leaves
labeled A in the tree t by the tree u. If t ? T?(V )
and u ? T?(Y ), then t[u]A ? T???(V ? Y ). We
often use the variables of X = {x1, x2, . . . } as
substitution points and write t[u1, . . . , uk] instead
of (? ? ? (t[u1]x1) . . . )[uk]xk .
An example substitution is shown in Figure 1.
The figure also illustrates a common problem with
substitution. Occasionally, it is not desirable to re-
place all leaves with a certain label by the same
tree. In the depicted example, we might want
to replace one ?NP? by a different tree, which
cannot be achieved with substitution. Clearly,
this problem is avoided if the source tree t con-
tains only one leaf labeled A. We call a tree A-
proper if it contains exactly one leaf with labelA.2
The subset C?(Xk) ? T?(Xk) contains exactly
those trees of T?(Xk) that are xi-proper for every
1 ? i ? k. For example, the tree t of Figure 1 is
?saw?-proper, and the tree u of Figure 1 is ?the?-
and ?N?-proper.
In this contribution, we will also use substitu-
tion as an explicit operator. The tree t[u]NP in
Figure 1 only shows the result of the substitution.
It cannot be infered from the tree alone, how it
was obtained (if we do not know t and u).3 To
make substitution explicit, we use the special bi-
nary symbols ?[?]A where A is a label. Those sym-
bols will always be used with exactly two chil-
dren (i.e., as binary symbols). Since this prop-
erty can easily be checked by all considered de-
vices, we ignore trees that use those symbols in a
non-binary manner. For every set ? of labels, we
let ? = ? ? {?[?]A | A ? ?} be the extended
set of labels containing also the substition sym-
bols. The substitution of Figure 1 can then be ex-
2A-proper trees are sometimes also called A-context in
the literature.
3This remains true even if we know that the participating
trees t and u are A-proper and the substitution t[u]A replac-
ing leaves labeled A was used. This is due to the fact that, in
general, the root label of u need not coincide with A.
1068
pressed as the tree ?[?]NP(t, u). To obtain t[u]NP
(the right-most tree in Figure 1), we have to evalu-
ate ?[?]NP(t, u). However, we want to replace only
one leaf at a time. Consequently, we restrict the
evaluation of ?[?]A(t, u) such that it applies only to
trees t whose evaluation is A-proper. To enforce
this restriction, we introduce an error signal ?,
which we assume not to occur in any set of la-
bels. Let ? be the set of labels. Then we define
the function ?E : T? ? T? ? {?} by
4
?(t1, . . . , tk)
E = ?(tE1 , . . . , t
E
k )
?[?]A(t, u)
E =
{
tE[uE]A if tE is A-proper
? otherwise
for every k ? 0, ? ? ?, and t, t1, . . . , tk, u ? T?.
5
We generally discard all trees that contain the er-
ror signal ?. Since the devices that we will study
later can also check the required A-properness us-
ing their state behavior, we generally do not dis-
cuss trees with error symbols explicitly.
4 Extended tree transducer
An extended tree transducer is a theoretical model
that computes a tree transformation. Such trans-
ducers have been studied first by Arnold and
Dauchet (1982) in a purely theoretic setting, but
were later applied in, for example, machine trans-
lation (Knight and Graehl, 2005; Knight, 2007;
Graehl et al, 2008; Graehl et al, 2009). Their
popularity in machine translation is due to Shieber
(2004), in which it is shown that extended tree
transducers are essentially (up to a relabeling) as
expressive as synchronous tree substitution gram-
mars (STSG). We refer to Chiang (2006) for an
introduction to synchronous devices.
Let us recall the formal definition. An ex-
tended tree transducer (for short: XTT)6 is a sys-
tem M = (Q,?,?, I, R) where
? Q is a finite set of states,
? ? and ? are alphabets of input and output
symbols, respectively,
? I ? Q is a set of initial states, and
? R is a finite set of rules of the form
(q, l)? (q1 ? ? ? qk, r)
4Formally, we should introduce an evaluation function for
each alphabet ?, but we assume that the alphabet can be in-
fered.
5This evaluation is a special case of a yield-mapping (En-
gelfriet and Vogler, 1985).
6Using the notions of Graehl et al (2009) our extended
tree transducers are linear, nondeleting extended top-down
tree transducers.
qS
S
x1 VP
x2 x3
?
S?
qV
x2
qNP
x1
qNP
x3
qNP
NP
DT
the
N
boy
?
NP
N
atefl
Figure 2: Example rules taken from Graehl et al
(2009). The term representation of the first rule
is (qS,S(x1,VP(x2, x3))) ? (w,S?(x2, x1, x3))
where w = qNPqVqNP.
where k ? 0, l ? C?(Xk), and r ? C?(Xk).
Recall that any tree of C?(Xk) contains each
variable of Xk = {x1, . . . , xk} exactly once. In
graphical representations of a rule
(q, l)? (q1 ? ? ? qk, r) ? R ,
we usually
? add the state q as root node of the left-hand
side7, and
? add the states q1, . . . , qk on top of the nodes
labeled x1, . . . , xk, respectively, in the right-
hand side of the rule.
Some example rules are displayed in Figure 2.
The rules are applied in the expected way (as in
a term-rewrite system). The only additional fea-
ture are the states of Q, which can be used to con-
trol the derivation. A sentential form is a tree that
contains exclusively output symbols towards the
root and remaining parts of the input headed by a
state as leaves. A derivation step starting from ?
then consists in
? selecting a leaf of ? with remaining input
symbols,
? matching the state q and the left-hand side l
of a rule (q, l) ? (q1 ? ? ? qk, r) ? R to the
state and input tree stored in the leaf, thus
matching input subtrees t1, . . . , tk to the vari-
ables x1, . . . , xk,
? replacing all the variables x1, . . . , xk in the
right-hand side r by the matched input sub-
trees q1(t1), . . . , qk(tk) headed by the corre-
sponding state, respectively, and
? replacing the selected leaf in ? by the tree
constructed in the previous item.
The process is illustrated in Figure 3.
Formally, a sentential form of the XTT M is a
tree of SF = T?(Q(T?)) where
Q(T?) = {q(t) | q ? Q, t ? T?} .
7States are thus also special symbols that are exclusively
used as unary symbols.
1069
CqS
S
t1
VP
t2 t3
?
C
S?
qV
t2
qNP
t1
qNP
t3
Figure 3: Illustration of a derivation step of an
XTT using the left rule of Figure 2.
Given ?, ? ? SF, we write ? ? ? if there ex-
ist C ? C?(X1), t1, . . . , tk ? T?, and a rule
(q, l)? (q1 ? ? ? qk, r) ? R such that
? ? = C[q(l[t1, . . . , tk])] and
? ? = C[r[q1(t1), . . . , qk(tk)]].
The tree transformation computed by M is the re-
lation
?M = {(t, u) ? T? ? T? | ?q ? I : q(t)?
? u}
where?? is the reflexive, transitive closure of?.
In other words, the tree t can be transformed into u
if there exists an initial state q such that we can
derive u from q(t) in several derivation steps.
We refer to Arnold and Dauchet (1982), Graehl
et al (2008), and Graehl et al (2009) for a more
detailed exposition to XTT.
5 Synchronous tree-adjoining grammar
XTT are a simple, natural model for tree trans-
formations, however they are not suitably ex-
pressive for all applications in machine transla-
tion (Shieber, 2007). In particular, all tree trans-
formations of XTT have a certain locality condi-
tion, which yields that the input tree and its corre-
sponding translation cannot be separated by an un-
bounded distance. To overcome this problem and
certain dependency problems, Shieber and Sch-
abes (1990) and Shieber (2007) suggest a stronger
model called synchronous tree-adjoining gram-
mar (STAG), which in addition to the substitution
operation of STSG (Chiang, 2005) also has an ad-
joining operation.
Let us recall the model in some detail. A tree-
adjoining grammar essentially is a regular tree
grammar (Ge?cseg and Steinby, 1984; Ge?cseg and
NP
DT
les
N
bonbons
N
N? ADJ
rouges
NP
DT
les
N
N
bonbons
ADJ
rouges
derived
tree
auxiliary
tree
adjunction
Figure 4: Illustration of an adjunction taken from
Nesson et al (2008).
NP
DT
les
?[?]N?
N
N? ADJ
rouges
N
bonbons
Figure 5: Illustration of the adjunction of Figure 4
using explicit substitution.
Steinby, 1997) enhanced with an adjunction oper-
ation. Roughly speaking, an adjunction replaces a
node (not necessarily a leaf) by an auxiliary tree,
which has exactly one distinguished foot node.
The original children of the replaced node will be-
come the children of the foot node after adjunc-
tion. Traditionally, the root label and the label of
the foot node coincide in an auxiliary tree aside
from a star index that marks the foot node. For
example, if the root node of an auxiliary tree is
labeled A, then the foot node is traditionally la-
beled A?. The star index is not reproduced once
adjoined. Formally, the adjunction of the auxil-
iary tree u with root label A (and foot node la-
bel A?) into a tree t = C[A(t1, . . . , tk)] with
C ? C?(X1) and t1, . . . , tk ? T? is
C[u[A(t1, . . . , tk)]A? ] .
Adjunction is illustrated in Figure 4.
We note that adjunction can easily be expressed
using explicit substitution. Essentially, only an ad-
ditional node with the adjoined subtree is added.
The result of the adjunction of Figure 4 using ex-
plicit substitution is displayed in Figure 5.
To simplify the development, we will make
some assumptions on all tree-adjoining grammars
(and synchronous tree-adjoining grammars). A
tree-adjoining grammar (TAG) is a finite set of
initial trees and a finite set of auxiliary trees. Our
1070
ST
c
S
a S
S? a
S
b S
S? b
S
S?
initial
tree
auxiliary
tree
auxiliary
tree
auxiliary
tree
Figure 6: A TAG for the copy string language
{wcw | w ? {a, b}?} taken from Shieber (2006).
TAG do not use substitution, but only adjunction.
A derivation is a chain of trees that starts with an
initial tree and each derived tree is obtained from
the previous one in the chain by adjunction of an
auxiliary tree. As in Shieber (2006) we assume
that all adjunctions are mandatory; i.e., if an aux-
iliary tree can be adjoined, then we need to make
an adjunction. Thus, a derivation starting from an
initial tree to a derived tree is complete if no ad-
junction is possible in the derived tree. Moreover,
we assume that to each node only one adjunction
can be applied. This is easily achieved by label-
ing the root of each adjoined auxiliary tree by a
special marker. Traditionally, the root label A of
an auxiliary tree is replaced by A? once adjoined.
Since we assume that there are no auxiliary trees
with such a root label, no further adjunction is pos-
sible at such nodes. Another effect of this restric-
tion is that the number of operable nodes (i.e., the
nodes to which an adjunction must still be applied)
is known at any given time.8 A full TAG with our
restrictions is shown in Figure 6.
Intuitively, a synchronous tree-adjoining gram-
mar (STAG) is essentially a pair of TAGs. The
synchronization is achieved by pairing the initial
trees and the auxiliary trees. In addition, for each
such pair (t, u) of trees, there exists a bijection be-
tween the operable nodes of t and u. Such nodes in
bijection are linked and the links are preserved in
derivations, in which we now use pairs of trees as
sentential forms. In graphical representations we
often indicate this bijection with integers; i.e., two
nodes marked with the same integer are linked. A
pair of auxiliary trees is then adjoined to linked
nodes (one in each tree of the sentential form) in
the expected manner. We will avoid a formal def-
inition here, but rather present an example STAG
and a derivation with it in Figures 7 and 8. For a
8Without the given restrictions, this number cannot be de-
termined easily because no or several adjunctions can take
place at a certain node.
S1
T
c
?
S1
T
c
S
S1
a S? a
?
S
a S1
S? a
S
S?
?
S
S?
S
S1
b S? b
?
S
b S1
S? b
Figure 7: STAG that computes the translation
{(wcwR, wcw) | w ? {a, b}?} where wR is the
reverse of w.
STAG G we write ?G for the tree transformation
computed by G.
6 Main result
In this section, we will present our main result. Es-
sentially, it states that a STAG is as powerful as a
STSG using explicit substitution. Thus, for every
tree transformation computed by a STAG, there is
an extended tree transducer that computes a repre-
sentation of the tree transformation using explicit
substitution. The converse is also true. For every
extended tree transducer M that uses explicit sub-
stitution, we can construct a STAG that computes
the tree transformation represented by ?M up to
a relabeling (a mapping that consistently replaces
node labels throughout the tree). The additional
relabeling is required because STAGs do not have
states. If we replace the extended tree transducer
by a STSG, then the result holds even without the
relabeling.
Theorem 1 For every STAGG, there exists an ex-
tended tree transducerM such that
?G = {(t
E, uE) | (t, u) ? ?M} .
Conversely, for every extended tree transducerM ,
there exists a STAGG such that the above relation
holds up to a relabeling.
6.1 Proof sketch
The following proof sketch is intended for readers
that are familiar with the literature on embedded
tree transducers, macro tree transducers, and bi-
morphisms. It can safely be skipped because we
will illustrate the relevant construction on our ex-
ample after the proof sketch, which contains the
outline for the correctness.
1071
S1
T
c
?
S1
T
c
S
S1
a S
T
c
a ?
S
a S1
S
T
c
a
S
S
S1
b S
a S
T
c
a
b ?
S
a S
b S1
S
S
T
c
a
b
S
S
S
S1
a S
b S
a S
T
c
a
b
a ?
S
a S
b S
a S1
S
S
S
T
c
a
b
a
Figure 8: An incomplete derivation using the STAG of Figure 7.
Let ? ? T? ? T? be a tree transformation
computed by a STAG. By Shieber (2006) there
exists a regular tree language L ? T? and two
functions e1 : T? ? T? and e2 : T? ? T? such
that ? = {(e1(t), e2(t)) | t ? L}. Moreover,
e1 and e2 can be computed by embedded tree
transducers (Shieber, 2006), which are particu-
lar 1-state, deterministic, total, 1-parameter, lin-
ear, and nondeleting macro tree transducers (Cour-
celle and Franchi-Zannettacci, 1982; Engelfriet
and Vogler, 1985). In fact, the converse is also true
up to a relabeling, which is also shown in Shieber
(2006). The outer part of Figure 9 illustrates these
relations. Finally, we remark that all involved con-
structions are effective.
Using a result of Engelfriet and Vogler (1985),
each embedded tree transducer can be decom-
posed into a top-down tree transducer (Ge?cseg
and Steinby, 1984; Ge?cseg and Steinby, 1997)
and a yield-mapping. In our particular case, the
top-down tree transducers are linear and nondelet-
ing homomorphisms h1 and h2. Linearity and
nondeletion are inherited from the corresponding
properties of the macro tree transducer. The prop-
erties ?1-state?, ?deterministic?, and ?total? of the
macro tree transducer ensure that the obtained top-
down tree transducer is also 1-state, determinis-
tic, and total, which means that it is a homomor-
phism. Finally, the 1-parameter property yields
that the used substitution symbols are binary (as
our substitution symbols ?[?]A). Consequently, the
yield-mapping actually coincides with our evalua-
tion. Again, this decomposition actually is a char-
acterization of embedded tree transducers. Now
the set {(h1(t), h2(t)) | t ? L} can be computed
h1 h2
?E ?E
?M
?
e1 e2
Figure 9: Illustration of the proof sketch.
by an extended tree transducer M due to results
of Shieber (2004) and Maletti (2008). More pre-
cisely, every extended tree transducer computes
such a set, so that also this step is a characteri-
zation. Thus we obtain that ? is an evaluation of a
tree transformation computed by an extended tree
transducer, and moreover, for each extended tree
transducer, the evaluation can be computed (up to
a relabeling) by a STAG. The overall proof struc-
ture is illustrated in Figure 9.
6.2 Example
Let us illustrate one direction (the construction
of the extended tree transducer) on our example
STAG of Figure 7. Essentially, we just prepare all
operable nodes by inserting an explicit substitu-
tion just on top of them. The first subtree of that
substitution will either be a variable (in the left-
hand side of a rule) or a variable headed by a state
(in the right-hand side of a rule). The numbers of
the variables encode the links of the STAG. Two
example rules obtained from the STAG of Figure 7
are presented in Figure 10. Using all XTT rules
constructed for the STAG of Figure 7, we present
1072
qS
?[?]S?
x1 S
T
c
?
?[?]S?
qS
x1
S
T
c
qS
S
?[?]S?
x1 S
a S? a
?
S
a ?[?]S?
qS
x1
S
S? a
Figure 10: Two constructed XTT rules.
a complete derivation of the XTT in Figure 11 that
(up to the final step) matches the derivation of the
STAG in Figure 8. The matching is achieved by
the evaluation ?E introduced in Section 3 (i.e., ap-
plying the evaluation to the derived trees of Fig-
ure 11 yields the corresponding derived trees of
Figure 8.
7 Applications
In this section, we will discuss a few applications
of our main result. Those range from representa-
tional issues to algorithmic problems. Finally, we
also present a tree transducer model that includes
explicit substitution. Such a model might help to
address algorithmic problems because derivation
and evaluation are intertwined in the model and
not separate as in our main result.
7.1 Toolkits
Obviously, our characterization can be applied in
a toolkit for extended tree transducers (or STSG)
such as TIBURON by May and Knight (2006) to
simulate STAG. The existing infrastructure (input-
output, derivation mechanism, etc) for extended
tree transducers can be re-used to run XTTs en-
coding STAGs. The only additional overhead is
the implementation of the evaluation, which is a
straightforward recursive function (as defined in
Section 3). After that any STAG can be simulated
in the existing framework, which allows experi-
ments with STAG and an evaluation of their ex-
pressive power without the need to develop a new
toolkit. It should be remarked that some essential
algorithms that are very sensitive to the input and
output behavior (such as parsing) cannot be sim-
ulated by the corresponding algorithms for STSG.
It remains an open problem whether the close rela-
tionship can also be exploited for such algorithms.
7.2 Algorithms
We already mentioned in the previous section
that some algorithms do not easily translate from
STAG to STSG (or vice versa) with the help of
our characterization. However, many standard al-
gorithms for STAG can easily be derived from
the corresponding algorithms for STSG. The sim-
plest example is the union of two STAG. Instead
of taking the union of two STAG using the clas-
sical construction, we can take the union of the
corresponding XTT (or STSG) that simulate the
STAGs. Their union will simulate the union of the
STAGs. Such properties are especially valuable
when we simulate STAG in toolkits for XTT.
A second standard algorithm that easily trans-
lates is the algorithm computing the n-best deriva-
tions (Huang and Chiang, 2005). Clearly, the n-
best derivation algorithm does not consider a par-
ticular input or output tree. Since the derivations
of the XTT match the derivations of the STAG
(in the former the input and output are encoded
using explicit substitution), the n-best derivations
will coincide. If we are additionally interested in
the input and output trees for those n-best deriva-
tions, then we can simply evaluate the coded input
and output trees returned by n-best derivation al-
gorithm.
Finally, let us consider an algorithm that can be
obtained for STAG by developing it for XTT us-
ing explicit substitution. We will develop a BAR-
HILLEL (Bar-Hillel et al, 1964) construction for
STAG. Thus, given a STAG G and a recognizable
tree language L, we want to construct a STAG G?
such that
?G? = {(t, u) | (t, u) ? ?G, t ? L} .
In other words, we take the tree transformation ?G
but additionally require the input tree to be in L.
Consequently, this operation is also called input
restriction. Since STAG are symmetric, the corre-
sponding output restriction can be obtained in the
same manner. Note that a classical BAR-HILLEL
construction restricting to a regular set of yields
can be obtained easily as a particular input restric-
tion. As in Nederhof (2009) a change of model
is beneficial for the development of such an algo-
rithm, so we will develop an input restriction for
XTT using explicit substitution.
Let M = (Q,?,?, I, R) be an XTT (using ex-
plicit substitution) and G = (N,?, I ?, P ) be a
tree substitution grammar (regular tree grammar)
in normal form that recognizesL (i.e.,L(G) = L).
Let S = {A ? ? | ?[?]A ? ?}. A context is a map-
ping c : S ? N , which remembers a nontermi-
nal of G for each substitution point. Given a rule
1073
qS
?[?]S?
S
?[?]S?
S
?[?]S?
S
?[?]S?
S
S?
S
a S? a
S
b S? b
S
a S? a
S
T
c
?
?[?]S?
qS
S
?[?]S?
S
?[?]S?
S
?[?]S?
S
S?
S
a S? a
S
b S? b
S
a S? a
S
T
c
?
?[?]S?
S
a ?[?]S?
qS
S
?[?]S?
S
?[?]S?
S
S?
S
a S? a
S
b S? b
S
S? a
S
T
c
?
?[?]S?
S
a ?[?]S?
S
b ?[?]S?
qS
S
?[?]S?
S
S?
S
a S? a
S
S? b
S
S? a
S
T
c
?
?[?]S?
S
a ?[?]S?
S
b ?[?]S?
S
a ?[?]S?
qS
S
S?
S
S? a
S
S? b
S
S? a
S
T
c
?
?[?]S?
S
a ?[?]S?
S
b ?[?]S?
S
a ?[?]S?
S
S?
S
S? a
S
S? b
S
S? a
S
T
c
Figure 11: Complete derivation using the constructed XTT rules.
(q, l) ? (q1 ? ? ? qk, r) ? R, a nonterminal p ? N ,
and a context c ? S, we construct new rules cor-
responding to successful parses of l subject to the
following restrictions:
? If l = ?[?]A(l1, l2) for some A ? ?, then se-
lect p? ? N , parse l1 in p with context c?
where c? = c[A 7? p?]9, and parse l2 in p?
with context c.
? If l = A? with A ? ?, then p = c(A).
? Finally, if l = ?(l1, . . . , lk) for some ? ? ?,
then select p ? ?(p1, . . . , pk) ? P is a pro-
duction of G and we parse li with nontermi-
nal pi and context c for each 1 ? i ? k.
7.3 A complete tree transducer model
So far, we have specified a tree transducer model
that requires some additional parsing before it can
be applied. This parsing step has to annotate (and
correspondingly restructure) the input tree by the
adjunction points. This is best illustrated by the
left tree in the last pair of trees in Figure 8. To run
our constructed XTT on the trivially completed
version of this input tree, it has to be transformed
into the first tree of Figure 11, where the adjunc-
tions are now visible. In fact, a second un-parsing
step is required to evaluate the output.
To avoid the first additional parsing step, we
will now modify our tree transducer model such
that this parsing step is part of its semantics. This
shows that it can also be done locally (instead of
globally parsing the whole input tree). In addition,
we arrive at a tree transducer model that exactly
(up to a relabeling) matches the power of STAG,
which can be useful for certain constructions. It is
known that an embedded tree transducer (Shieber,
2006) can handle the mentioned un-parsing step.
An extended embedded tree transducer with
9c? is the same as c except that it maps A to p?.
substitution M = (Q,?,?, I, R) is simply an
embedded tree transducer with extended left-hand
sides (i.e., any number of input symbols is allowed
in the left-hand side) that uses the special sym-
bols ?[?]A in the input. Formally, let
? Q = Q0 ? Q1 be finite where Q0 and Q1
are the set of states that do not and do have a
context parameter, respectively,
? ? and ? be ranked alphabets such that if
?[?]A ? ?, then A,A? ? ?,
? Q?U? be such that
Q?U? = {q?u? | q ? Q1, u ? U} ?
? {q?? | q ? Q0} ,
? I ? Q?T??, and
? R is a finite set of rules l? r such that there
exists k ? 0 with l ? Q?{y}?(C?(Xk)) and
r ? Rhsk where
Rhsk := ?(Rhsk, . . . ,Rhsk) |
| q1?Rhsk?(x) | q0??(x)
with ? ? ?k, q1 ? Q1, q0 ? Q0, and x ? Xk.
Moreover, each variable of l (including y) is
supposed to occur exactly once in r.
We refer to Shieber (2006) for a full description
of embedded tree transducers. As seen from the
syntax, we write the context parameter y of a
state q ? Q1 as q?y?. If q ? Q0, then we also
write q?? or q???. In each right-hand side, such
a context parameter u can contain output symbols
and further calls to input subtrees. The semantics
of extended embedded tree transducers with sub-
stitution deviates slightly from the embedded tree
transducer semantics. Roughly speaking, not its
rules as such, but rather their evaluation are now
applied in a term-rewrite fashion. Let
SF? := ?(SF?, . . . ,SF?) |
| q1?SF
??(t) | q0??(t)
1074
qS??
?[?]S?
x1 S
T
c
?
q???
S
T
c
x1
qS??
S
S
T
c
?
q???
S
T
c
S
S?
Figure 12: Rule and derivation step using the rule
in an extended embedded tree transducer with sub-
stitution where the context parameter (if present)
is displayed as first child.
where ? ? ?k, q1 ? Q1, q0 ? Q0, and t ? T?.
Given ?, ? ? SF?, we write ? ? ? if there exist
C ? C?(X1), t1, . . . , tk ? T?, u ? T??{?}, and
a rule q?u?(l) ? r ? R10 with l ? C?(Xk) such
that
? ? = C[q?u?(l[t1, . . . , tk]E)] and
? ? = C[(r[t1, . . . , tk])[u]y].
Note that the essential difference to the ?stan-
dard? semantics of embedded tree transducers is
the evaluation in the first item. The tree transfor-
mation computed by M is defined as usual. We
illustrate a derivation step in Figure 12, where the
match ?[?]S?(x1, S(T (c)))
E = S(S(T (c))) is suc-
cessful for x1 = S(S?).
Theorem 2 Every STAG can be simulated by an
extended embedded tree transducer with substi-
tution. Moreover, every extended embedded tree
transducer computes a tree transformation that
can be computed by a STAG up to a relabeling.
8 Conclusions
We presented an alternative view on STAG us-
ing tree transducers (or equivalently, STSG). Our
main result shows that the syntactic characteri-
zation of STAG as STSG plus adjunction rules
also carries over to the semantic side. A STAG
tree transformation can also be computed by an
STSG using explicit substitution. In the light
of this result, some standard problems for STAG
can be reduced to the corresponding problems
for STSG. This allows us to re-use existing algo-
rithms for STSG also for STAG. Moreover, exist-
ing STAG algorithms can be related to the corre-
sponding STSG algorithms, which provides fur-
ther evidence of the close relationship between the
two models. We used this relationship to develop a
10Note that u is ? if q ? Q0.
BAR-HILLEL construction for STAG. Finally, we
hope that the alternative characterization is easier
to handle and might provide further insight into
general properties of STAG such as compositions
and preservation of regularity.
Acknowledgements
ANDREAS MALETTI was financially supported
by the Ministerio de Educacio?n y Ciencia (MEC)
grant JDCI-2007-760.
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The The-
ory of Parsing, Translation, and Compiling. Pren-
tice Hall.
Andre? Arnold and Max Dauchet. 1982. Morphismes
et bimorphismes d?arbres. Theoret. Comput. Sci.,
20(1):33?93.
Yehoshua Bar-Hillel, Micha Perles, and Eliyahu
Shamir. 1964. On formal properties of simple
phrase structure grammars. In Yehoshua Bar-Hillel,
editor, Language and Information: Selected Essays
on their Theory and Application, chapter 9, pages
116?150. Addison Wesley.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Proc.
ACL, pages 263?270. Association for Computa-
tional Linguistics.
David Chiang. 2006. An introduction to synchronous
grammars. In Proc. ACL. Association for Computa-
tional Linguistics. Part of a tutorial given with Kevin
Knight.
Bruno Courcelle and Paul Franchi-Zannettacci. 1982.
Attribute grammars and recursive program schemes.
Theoret. Comput. Sci., 17:163?191, 235?257.
Joost Engelfriet and Heiko Vogler. 1985. Macro tree
transducers. J. Comput. System Sci., 31(1):71?146.
Ferenc Ge?cseg and Magnus Steinby. 1984. Tree Au-
tomata. Akade?miai Kiado?, Budapest.
Ferenc Ge?cseg and Magnus Steinby. 1997. Tree lan-
guages. In Handbook of Formal Languages, vol-
ume 3, chapter 1, pages 1?68. Springer.
Jonathan Graehl and Kevin Knight. 2004. Training
tree transducers. In HLT-NAACL, pages 105?112.
Association for Computational Linguistics. See
also (Graehl et al, 2008).
Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Computational
Linguistics, 34(3):391?427.
1075
Jonathan Graehl, Mark Hopkins, Kevin Knight, and
Andreas Maletti. 2009. The power of extended top-
down tree transducers. SIAM Journal on Comput-
ing, 39(2):410?430.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. IWPT, pages 53?64. Association
for Computational Linguistics.
Kevin Knight and Jonathan Graehl. 2005. An over-
view of probabilistic tree transducers for natural lan-
guage processing. In Proc. CICLing, volume 3406
of LNCS, pages 1?24. Springer.
Kevin Knight. 2007. Capturing practical natural
language transformations. Machine Translation,
21(2):121?133.
Andreas Maletti. 2008. Compositions of extended top-
down tree transducers. Inform. and Comput., 206(9?
10):1187?1196.
Jonathan May and Kevin Knight. 2006. TIBURON:
A weighted tree automata toolkit. In Proc. CIAA,
volume 4094 of LNCS, pages 102?113. Springer.
Mark-Jan Nederhof. 2009. Weighted parsing of trees.
In Proc. IWPT, pages 13?24. Association for Com-
putational Linguistics.
Rebecca Nesson, Giorgio Satta, and Stuart M. Shieber.
2008. Optimal k-arization of synchronous tree-
adjoining grammar. In Proc. ACL, pages 604?612.
Association for Computational Linguistics.
William C. Rounds. 1970. Mappings and grammars
on trees. Math. Systems Theory, 4(3):257?287.
Stuart M. Shieber and Yves Schabes. 1990. Syn-
chronous tree-adjoining grammars. In Proc. Com-
putational Linguistics, volume 3, pages 253?258.
Stuart M. Shieber. 2004. Synchronous grammars as
tree transducers. In Proc. TAG+7, pages 88?95.
Stuart M. Shieber. 2006. Unifying synchronous tree
adjoining grammars and tree transducers via bimor-
phisms. In Proc. EACL, pages 377?384. Association
for Computational Linguistics.
Stuart M. Shieber. 2007. Probabilistic synchronous
tree-adjoining grammars for machine translation:
The argument from bilingual dictionaries. In Proc.
Workshop on Syntax and Structure in Statistical
Translation, pages 88?95. Association for Compu-
tational Linguistics.
James W. Thatcher. 1970. Generalized2 sequential
machine maps. J. Comput. System Sci., 4(4):339?
367.
1076
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 825?834,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
How to train your multi bottom-up tree transducer
Andreas Maletti
Universita?t Stuttgart, Institute for Natural Language Processing
Azenbergstra?e 12, 70174 Stuttgart, Germany
andreas.maletti@ims.uni-stuttgart.de
Abstract
The local multi bottom-up tree transducer is
introduced and related to the (non-contiguous)
synchronous tree sequence substitution gram-
mar. It is then shown how to obtain a weighted
local multi bottom-up tree transducer from
a bilingual and biparsed corpus. Finally,
the problem of non-preservation of regular-
ity is addressed. Three properties that ensure
preservation are introduced, and it is discussed
how to adjust the rule extraction process such
that they are automatically fulfilled.
1 Introduction
A (formal) translation model is at the core of ev-
ery machine translation system. Predominantly, sta-
tistical processes are used to instantiate the for-
mal model and derive a specific translation device.
Brown et al (1990) discuss automatically trainable
translation models in their seminal paper. However,
the IBM models of Brown et al (1993) are string-
based in the sense that they base the translation de-
cision on the words and their surrounding context.
Contrary, in the field of syntax-based machine trans-
lation, the translation models have full access to the
syntax of the sentences and can base their decision
on it. A good exposition to both fields is presented
in (Knight, 2007).
In this paper, we deal exclusively with syntax-
based translation models such as synchronous tree
substitution grammars (STSG), multi bottom-up tree
transducers (MBOT), and synchronous tree-sequence
substitution grammars (STSSG). Chiang (2006)
gives a good introduction to STSG, which originate
from the syntax-directed translation schemes of Aho
and Ullman (1972). Roughly speaking, an STSG
has rules in which two linked nonterminals are re-
placed (at the same time) by two corresponding trees
containing terminal and nonterminal symbols. In
addition, the nonterminals in the two replacement
trees are linked, which creates new linked nontermi-
nals to which further rules can be applied. Hence-
forth, we refer to these two trees as input and output
tree. MBOT have been introduced in (Arnold and
Dauchet, 1982; Lilin, 1981) and are slightly more
expressive than STSG. Roughly speaking, they al-
low one replacement input tree and several output
trees in a single rule. This change and the pres-
ence of states yields many algorithmically advanta-
geous properties such as closure under composition,
efficient binarization, and efficient input and output
restriction [see (Maletti, 2010)]. Finally, STSSG,
which have been derived from rational tree rela-
tions (Raoult, 1997), have been discussed by Zhang
et al (2008a), Zhang et al (2008b), and Sun et al
(2009). They are even more expressive than the lo-
cal variant of the multi bottom-up tree transducer
(LMBOT) that we introduce here and can have sev-
eral input and output trees in a single rule.
In this contribution, we restrict MBOT to a form
that is particularly relevant in machine translation.
We drop the general state behavior of MBOT and re-
place it by the common locality tests that are also
present in STSG, STSSG, and STAG (Shieber and
Schabes, 1990; Shieber, 2007). The obtained device
is the local MBOT (LMBOT).
Maletti (2010) argued the algorithmical advan-
tages of MBOT over STSG and proposed MBOT as
an implementation alternative for STSG. In partic-
ular, the training procedure would train STSG; i.e.,
it would not utilize the additional expressive power
825
of MBOT. However, Zhang et al (2008b) and Sun
et al (2009) demonstrate that the additional expres-
sivity gained from non-contiguous rules greatly im-
proves the translation quality. In this contribution
we address this separation and investigate a training
procedure for LMBOT that allows non-contiguous
fragments while preserving the algorithmic advan-
tages of MBOT. To this end, we introduce a rule ex-
traction and weight training method for LMBOT that
is based on the corresponding procedures for STSG
and STSSG. However, general LMBOT can be too
expressive in the sense that they allow translations
that do not preserve regularity. Preservation of reg-
ularity is an important property for efficient repre-
sentations and efficient algorithms [see (May et al,
2010)]. Consequently, we present 3 properties that
ensure that an LMBOT preserves regularity. In addi-
tion, we shortly discuss how these properties could
be enforced in the rule extraction procedure.
2 Notation
The set of nonnegative integers is N. We write [k]
for the set {i | 1 ? i ? k}. We treat functions as
special relations. For every relation R ? A?B and
S ? A, we write
R(S) = {b ? B | ?a ? S : (a, b) ? R}
R?1 = {(b, a) | (a, b) ? R} ,
where R?1 is called the inverse of R.
Given an alphabet ?, the set of all words (or se-
quences) over ? is ??, of which the empty word is ?.
The concatenation of two words u and w is simply
denoted by the juxtaposition uw. The length of a
word w = ?1 ? ? ??k with ?i ? ? for all i ? [k]
is |w| = k. Given 1 ? i ? j ? k, the (i, j)-
span w[i, j] of w is ?i?i+1 ? ? ??j .
The set T? of all ?-trees is the smallest set T
such that ?(t) ? T for all ? ? ? and t ? T ?.
We generally use bold-face characters (like t) for
sequences, and we refer to their elements using sub-
scripts (like ti). Consequently, a tree t consists of
a labeled root node ? followed by a sequence t of
its children. To improve readability we sometimes
write a sequence t1 ? ? ? tk as t1, . . . , tk.
The positions pos(t) ? N? of a tree t = ?(t) are
inductively defined by pos(t) = {?}?pos(t), where
pos(t) =
?
1?i?|t|
{ip | p ? pos(ti)} .
Note that this yields an undesirable difference be-
tween pos(t) and pos(t), but it will always be clear
from the context whether we refer to a single tree or
a sequence. Note that positions are ordered via the
(standard) lexicographic ordering. Let t ? T? and
p ? pos(t). The label of t at position p is t(p), and
the subtree rooted at position p is t|p. Formally, they
are defined by
t(p) =
{
? if p = ?
t(p) otherwise
t(ip) = ti(p)
t|p =
{
t if p = ?
t|p otherwise
t|ip = ti|p
for all t = ?(t) and 1 ? i ? |t|. As demonstrated,
these notions are also used for sequences. A posi-
tion p ? pos(t) is a leaf (in t) if p1 /? pos(t). Given
a subset NT ? ?, we let
?NT(t) = {p ? pos(t) | t(p) ? NT, p leaf in t} .
Later NT will be the set of nonterminals, so that
the elements of ?NT(t) will be the leaf nonterminals
of t. We extend the notion to sequences t by
?NT(t) =
?
1?i?|t|
{ip | p ? ?NT(ti)} .
We also need a substitution that replaces sub-
trees. Let p1, . . . , pn ? pos(t) be pairwise in-
comparable positions and t1, . . . , tn ? T?. Then
t[pi ? ti | 1 ? i ? n] denotes the tree that is ob-
tained from t by replacing (in parallel) the subtrees
at pi by ti for every i ? [k].
Finally, let us recall regular tree languages. A fi-
nite tree automaton M is a tuple (Q,?, ?, F ) such
that Q is a finite set, ? ? Q? ? ? ? Q is a fi-
nite relation, and F ? Q. We extend ? to a map-
ping ? : T? ? 2Q by
?(?(t)) = {q | (q, ?, q) ? ?, ?i ? [ |t| ] : qi ? ?(ti)}
for every ? ? ? and t ? T ??. The finite tree automa-
ton M recognizes the tree language
L(M) = {t ? T? | ?(t) ? F 6= ?} .
A tree language L ? T? is regular if there exists a
finite tree automaton M such that L = L(M).
826
VP
VBD
signed
PP ?
PV
twlY
,
NP-OBJ
NP
DET-NN
AltwqyE
PP
1
S
NP-SBJ VP
?
VP
PV NP-OBJ NP-SBJ1
21
Figure 1: Sample LMBOT rules.
3 The model
In this section, we recall particular multi bottom-
up tree transducers, which have been introduced
by Arnold and Dauchet (1982) and Lilin (1981). A
detailed (and English) presentation of the general
model can be found in Engelfriet et al (2009) and
Maletti (2010). Using the nomenclature of Engel-
friet et al (2009), we recall a variant of linear and
nondeleting extended multi bottom-up tree transduc-
ers (MBOT) here. Occasionally, we will refer to gen-
eral MBOT, which differ from the local variant dis-
cussed here because they have explicit states.
Throughout the article, we assume sets ? and ?
of input and output symbols, respectively. More-
over, let NT ? ? ?? be the set of designated non-
terminal symbols. Finally, we avoid weights in the
formal development to keep it simple. It is straight-
forward to add weights to our model.
Essentially, the model works on pairs ?t,u?
consisting of an input tree t ? T? and a se-
quence u ? T ?? of output trees. Each such pair is
called a pre-translation and the rank rk(?t,u?) the
pre-translation ?t,u? is |u|. In other words, the rank
of a pre-translation equals the number of output trees
stored in it. Given a pre-translation ?t,u? ? T??T k?
and i ? [k], we call ui the ith translation of t. An
alignment for the pre-translation ?t,u? is an injec-
tive mapping ? : ?NT(u) ? ?NT(t) ? N such that
(p, j) ? ?(?NT(u)) for every (p, i) ? ?(?NT(u))
and j ? [i]. In other words, an alignment should re-
quest each translation of a particular subtree at most
once and if it requests the ith translation, then it
should also request all previous translations.
Definition 1 A local multi bottom-up tree trans-
ducer (LMBOT) is a finite setR of rules such that ev-
ery rule, written l ?? r, contains a pre-translation
?l, r? and an alignment ? for it.
The component l is the left-hand side, r is
the right-hand side, and ? is the alignment of a
rule l?? r ? R. The rules of an LMBOT are similar
to the rules of an STSG (synchronous tree substitu-
tion grammar) of Eisner (2003) and Shieber (2004),
but right-hand sides of LMBOT contain a sequence
of trees instead of just a single tree as in an STSG. In
addition, the alignments in an STSG rule are bijec-
tive between leaf nonterminals, whereas our model
permits multiple alignments to a single leaf nonter-
minal in the left-hand side. A model that is even
more powerful than LMBOT is the non-contiguous
version of STSSG (synchronous tree-sequence sub-
stitution grammar) of Zhang et al (2008a), Zhang
et al (2008b), and Sun et al (2009), which al-
lows sequences of trees on both sides of rules [see
also (Raoult, 1997)]. Figure 1 displays sample rules
of an LMBOT using a graphical representation of the
trees and the alignment.
Next, we define the semantics of an LMBOT R.
To avoid difficulties1, we explicitly exclude rules
like l ?? r where l ? NT or r ? NT?; i.e.,
rules where the left- or right-hand side are only
leaf nonterminals. We first define the traditional
bottom-up semantics. Let ? = l ?? r ? R be a
rule and p ? ?NT(l). The p-rank rk(?, p) of ? is
rk(?, p) = |{i ? N | (p, i) ? ?(?NT(r))}|.
Definition 2 The set ?(R) of pre-translations of an
LMBOT R is inductively defined to be the smallest
set such that: If ? = l ?? r ? R is a rule,
?tp,up? ? ?(R) is a pre-translation of R for every
p ? ?NT(l), and
? rk(?, p) = rk(?tp,up?),
? l(p) = tp(?), and
1Actually, difficulties arise only in the weighted setting.
827
PP
IN
for
NP
NNP
Serbia
?
PP
PREP
En
NP
NN-PROP
SrbyA
?
VP
VBD
signed
PP
IN
for
NP
NNP
Serbia
? PV
twlY
,
NP-OBJ
NP
DET-NN
AltwqyE
PP
PREP
En
NP
NN-PROP
SrbyA
?
S
. . . VP
VBD
signed
PP
IN
for
NP
NNP
Serbia
?
VP
PV
twlY
NP-OBJ
NP
DET-NN
AltwqyE
PP
PREP
En
NP
NN-PROP
SrbyA
. . .
?
Figure 2: Top left: (a) Initial pre-translation; Top right: (b) Pre-translation obtained from the left rule of Fig. 1 and (a);
Bottom: (c) Pre-translation obtained from the right rule of Fig. 1 and (b).
? r(p?) = up??(i) with ?(p?) = (p??, i)
for every p? ? ?NT(r), then ?t,u? ? ?(R) where
? t = l[p? tp | p ? ?NT(l)] and
? u = r[p? ? (up??)i | p? ? ??1(p??, i)].
In plain words, each nonterminal leaf p in the
left-hand side of a rule ? can be replaced by the
input tree t of a pre-translation ?t,u? whose root
is labeled by the same nonterminal. In addition,
the rank rk(?, p) of the replaced nonterminal should
match the rank rk(?t,u?) of the pre-translation and
the nonterminals in the right-hand side that are
aligned to p should be replaced by the translation
that the alignment requests, provided that the non-
terminal matches with the root symbol of the re-
quested translation. The main benefit of the bottom-
up semantics is that it works exclusively on pre-
translations. The process is illustrated in Figure 2.
Using the classical bottom-up semantics, we sim-
ply obtain the following theorem by Maletti (2010)
because the MBOT constructed there is in fact an
LMBOT.
Theorem 3 For every STSG, an equivalent LMBOT
can be constructed in linear time, which in turn
yields a particular MBOT in linear time.
Finally, we want to relate LMBOT to the STSSG
of Sun et al (2009). To this end, we also introduce
the top-down semantics for LMBOT. As expected,
both semantics coincide. The top-down semantics is
introduced using rule compositions, which will play
an important rule later on.
Definition 4 The set Rk of k-fold composed rules is
inductively defined as follows:
? R1 = R and
? ` ?? s ? Rk+1 for all ? = l ?? r ? R and
?p = lp ??p rp ? R
k such that
? rk(?, p) = rk(?lp, rp?),
? l(p) = lp(?), and
? r(p?) = rp??(i) with ?(p?) = (p??, i)
for every p ? ?NT(l) and p
? ? ?NT(r) where
? ` = l[p? lp | p ? ?NT(l)],
? s = r[p? ? (rp??)i | p? ? ??1(p??, i)], and
? ?(p?p) = p???p??(ip) for all positions
p? ? ??1(p??, i) and ip ? ?NT(rp??).
The rule closureR?? ofR isR?? =
?
i?1R
i. The
top-down pre-translation of R is
?t(R) = {?l, r ? | l?? r ? R
??, ?NT(l) = ?} .
828
XX
?
X
a X
,
X
a X1
2
X
X
?
X
b X
,
X
b X1
2
X
X
X
?
X
a X
b X
,
X
a X
b X1
2
Figure 3: Composed rule.
The composition of the rules, which is illus-
trated in Figure 3, in the second item of Defini-
tion 4 could also be represented as ?(?1, . . . , ?k)
where ?1, . . . , ?k is an enumeration of the rules
{?p | p ? ?NT(l)} used in the item. The follow-
ing theorem is easy to prove.
Theorem 5 The bottom-up and top-down semantics
coincide; i.e., ?(R) = ?t(R).
Chiang (2005) and Graehl et al (2008) argue that
STSG have sufficient expressive power for syntax-
based machine translation, but Zhang et al (2008a)
show that the additional expressive power of tree-
sequences helps the translation process. This is
mostly due to the fact that smaller (and less specific)
rules can be extracted from bi-parsed word-aligned
training data. A detailed overview that focusses on
STSG is presented by Knight (2007).
Theorem 6 For every LMBOT, an equivalent STSSG
can be constructed in linear time.
4 Rule extraction and training
In this section, we will show how to automatically
obtain an LMBOT from a bi-parsed, word-aligned
parallel corpus. Essentially, the process has two
steps: rule extraction and training. In the rule ex-
traction step, an (unweighted) LMBOT is extracted
from the corpus. The rule weights are then set in the
training procedure.
The two main inspirations for our rule extraction
are the corresponding procedures for STSG (Galley
et al, 2004; Graehl et al, 2008) and for STSSG (Sun
et al, 2009). STSG are always contiguous in both
the left- and right-hand side, which means that they
(completely) cover a single span of input or output
words. On the contrary, STSSG rules can be non-
contiguous on both sides, but the extraction proce-
dure of Sun et al (2009) only extracts rules that are
contiguous on the left- or right-hand side. We can
adjust its 1st phase that extracts rules with (poten-
tially) non-contiguous right-hand sides. The adjust-
ment is necessary because LMBOT rules cannot have
(contiguous) tree sequences in their left-hand sides.
Overall, the rule extraction process is sketched in
Algorithm 1.
Algorithm 1 Rule extraction for LMBOT
Require: word-aligned tree pair (t, u)
Return: LMBOT rules R such that (t, u) ? ?(R)
while there exists a maximal non-leaf node
p ? pos(t) and minimal p1, . . . , pk ? pos(u)
such that t|p and (u|p1 , . . . , u|pk) have a con-
sistent alignment (i.e., no alignments from
within t|p to a leaf outside (u|p1 , . . . , u|pk) and
vice versa)
do
2: add rule ? = t|p ?? (up1 , . . . , upk) to R
with the nonterminal alignments ?
// excise rule ? from (t, u)
4: t? t[p? t(p)]
u? u[pi ? u(pi) | i ? {1, . . . , k}]
6: establish alignments according to position
end while
The requirement that we can only have one in-
put tree in LMBOT rules indeed might cause the ex-
traction of bigger and less useful rules (when com-
pared to the corresponding STSSG rules) as demon-
strated in (Sun et al, 2009). However, the stricter
rule shape preserves the good algorithmic proper-
ties of LMBOT. The more powerful STSSG rules can
cause nonclosure under composition (Raoult, 1997;
Radmacher, 2008) and parsing to be less efficient.
Figure 4 shows an example of biparsed aligned
parallel text. According to the method of Galley et
al. (2004) we can extract the (minimal) STSG rule
displayed in Figure 5. Using the more liberal format
of LMBOT rules, we can decompose the STSG rule of
Figure 5 further into the rules displayed in Figure 1.
The method of Sun et al (2009) would also extract
the rule displayed in Figure 6.
Let us reconsider Figures 1 and 2. Let ?1 be
the top left rule of Figure 2 and ?2 and ?3 be the
829
SNP-SBJ
NML
JJ
Yugoslav
NNP
President
NNP
Voislav
VP
VBD
signed
PP
IN
for
NP
NNP
Serbia
VP
PV
twlY
NP-OBJ
NP
DET-NN
AltwqyE
PP
PREP
En
NP
NN-PROP
SrbyA
NP-SBJ
NP
DET-NN
Alr}ys
DET-ADJ
AlywgwslAfy
NP
NN-PROP
fwyslAf
Figure 4: Biparsed aligned parallel text.
S
NP-SBJ VP
VBD
signed
PP
?
VP
PV
twlY
NP-OBJ
NP
DET-NN
AltwqyE
PP
NP-SBJ
1
1
Figure 5: Minimal STSG rule.
left and right rule of Figure 1, respectively. We
can represent the lower pre-translation of Figure 2
by ?3(? ? ? , ?2(?1)), where ?2(?1) represents the up-
per right pre-translation of Figure 2. If we name
all rules of R, then we can represent each pre-
translation of ?(R) symbolically by a tree contain-
ing rule names. Such trees containing rule names
are often called derivation trees. Overall, we obtain
the following result, for which details can be found
in (Arnold and Dauchet, 1982).
Theorem 7 The setD(R) is a regular tree language
for every LMBOT R, and the set of derivations is also
regular for every MBOT.
VBD
signed
,
IN
for
?
PV
twlY
,
NP
DET-NN
AltwqyE
,
PREP
En
Figure 6: Sample STSSG rule.
Moreover, using the input and output product con-
structions of Maletti (2010) we obtain that even the
set Dt,u(R) of derivations for a specific input tree t
and output tree u is regular. Since Dt,u(R) is reg-
ular, we can compute the inside and outside weight
of each (weighted) rule of R following the method
of Graehl et al (2008). Similarly, we can adjust
the training procedure of Graehl et al (2008), which
yields that we can automatically obtain a weighted
LMBOT from a bi-parsed parallel corpus. Details on
the run-time can be found in (Graehl et al, 2008).
5 Preservation of regularity
Clearly, LMBOT are not symmetric. Although, the
backwards application of an LMBOT preserves regu-
larity, this property does not hold for forward appli-
cation. We will focus on forward application here.
Given a set T of pre-translations and a tree language
830
L ? T?, we let
Tc(L) = {ui | (u1, . . . , uk) ? T (L), i ? [k]} ,
which collects all translations of input trees in L.
We say that T preserves regularity if Tc(L) is regu-
lar for every regular tree language L ? T?. Corre-
spondingly, an LMBOT R preserves regularity if its
set ?(R) of pre-translations preserves regularity.
As mentioned, an LMBOT does not necessarily
preserve regularity. The rules of an LMBOT have
only alignments between the left-hand side (input
tree) and the right-hand side (output tree), which are
also called inter-tree alignments. However, several
alignments to a single nonterminal in the left-hand
side can transitively relate two different nontermi-
nals in the output side and thus simulate an intra-
tree alignment. For example, the right rule of Fig-
ure 1 relates a ?PV? and an ?NP-OBJ? node to a sin-
gle ?VP? node in the left-hand side. This could lead
to an intra-tree alignment (synchronization) between
the ?PV? and ?NP-OBJ? nodes in the right-hand side.
Figure 7 displays the rules R of an LMBOT
that does not preserve regularity. This can easily
be seen on the leaf (word) languages because the
LMBOT can translate the word x to any element
of L = {wcwc | w ? {a, b}?}. Clearly, this word
language L is not context-free. Since the leaf lan-
guage of every regular tree language is context-free
and regular tree languages are closed under inter-
section (needed to single out the translations that
have the symbol Y at the root), this also proves that
?(R)c(T?) is not regular. Since T? is regular, this
proves that the LMBOT does not preserve regularity.
Preservation of regularity is an important property
for a number of translation model manipulations.
For example, the bucket-brigade and the on-the-fly
method for the efficient inference described in (May
et al, 2010) essentially build on it. Moreover, a reg-
ular tree grammar (i.e., a representation of a regular
tree language) is an efficient representation. More
complex representations such as context-free tree
grammars [see, e.g., (Fujiyoshi, 2004)] have worse
algorithmic properties (e.g., more complex parsing
and problematic intersection).
In this section, we investigate three syntactic re-
strictions on the set R of rules that guarantees that
the obtained LMBOT preserves regularity. Then we
shortly discuss how to adjust the rule extraction al-
gorithm, so that the extracted rules automatically
have these property. First, we quickly recall the no-
tion of composed rules from Definition 4 because
it will play an essential role in all three properties.
Figure 3 shows a composition of two rules from Fig-
ure 7. Mind thatR2 might not contain all rules ofR,
but it contains all those without leaf nonterminals.
Definition 8 An LMBOT R is finitely collapsing if
there is n ? N such that ? : ?NT(r)? ?NT(l)?{1}
for every rule l?? r ? Rn.
The following statement follows from a more gen-
eral result of Raoult (1997), which we will introduce
with our second property.
Theorem 9 Every finitely collapsing LMBOT pre-
serves regularity.
Often the simple condition ?finitely collapsing? is
fulfilled after rule extraction. In addition, it is au-
tomatically fulfilled in an LMBOT that was obtained
from an STSG using Theorem 3. It can also be en-
sured in the rule extraction process by introducing
collapsing points for output symbols that can appear
recursively in the corpus. For example, we could en-
force that all extracted rules for clause-level output
symbols (assuming that there is no recursion not in-
volving a clause-level output symbols) should have
only 1 output tree in the right-hand side.
However, ?finitely collapsing? is a rather strict
property. Finitely collapsing LMBOT have only
slightly more expressive power than STSG. In fact,
they could be called STSG with input desynchro-
nization. This is due to the fact that the alignment
in composed rules establishes an injective relation
between leaf nonterminals (as in an STSG), but it
need not be bijective. Consequently, there can be
leaf nonterminals in the left-hand side that have no
aligned leaf nonterminal in the right-hand side. In
this sense, those leaf nonterminals are desynchro-
nized. This feature is illustrated in Figure 8 and
such an LMBOT can compute the transformation
{(t, a) | t ? T?}, which cannot be computed by an
STSG (assuming that T? is suitably rich). Thus STSG
with input desynchronization are more expressive
than STSG, but they still compute a class of trans-
formations that is not closed under composition.
831
Xx
?
X
c
, X
c
X
X
?
X
a X
,
X
a X1
2
X
X
?
X
b X
,
X
b X1
2
Y
X
?
Y
X X1
2
Figure 7: Output subtree synchronization (intra-tree).
X
X X
? a
X
a
?
??
Figure 8: Finitely collapsing LMBOT.
Theorem 10 For every STSG, we can construct an
equivalent finitely collapsing LMBOT in linear time.
Moreover, finitely collapsing LMBOT are strictly
more expressive than STSG.
Next, we investigate a weaker property by Raoult
(1997) that still ensures preservation of regularity.
Definition 11 An LMBOT R has finite synchroniza-
tion if there is n ? N such that for every rule
l ?? r ? Rn and p ? ?NT(l) there exists i ? N
with ??1({p} ? N) ? {iw | w ? N?}.
In plain terms, multiple alignments to a single leaf
nonterminal at p in the left-hand side are allowed,
but all leaf nonterminals of the right-hand side that
are aligned to p must be in the same tree. Clearly,
an LMBOT with finite synchronization is finitely col-
lapsing. Raoult (1997) investigated this restriction
in the context of rational tree relations, which are a
generalization of our LMBOT. Raoult (1997) shows
that finite synchronization can be decided. The next
theorem follows from the results of Raoult (1997).
Theorem 12 Every LMBOT with finite synchroniza-
tion preserves regularity.
MBOT can compute arbitrary compositions of
STSG (Maletti, 2010). However, this no longer re-
mains true for MBOT (or LMBOT) with finite syn-
chronization.2 In Figure 9 we illustrate a transla-
tion that can be computed by a composition of two
STSG, but that cannot be computed by an MBOT
(or LMBOT) with finite synchronization. Intuitively,
when processing the chain of ?X?s of the transforma-
tion depicted in Figure 9, the first and second suc-
2This assumes a straightforward generalization of the ?finite
synchronization? property for MBOT.
Y
X
...
X
Y
t1 t2
t3
?
Z
t1 t2 t3
Figure 9: Transformation that cannot be computed by an
MBOT with finite synchronization.
cessor of the ?Z?-node at the root on the output side
must be aligned to the ?X?-chain. This is necessary
because those two mentioned subtrees must repro-
duce t1 and t2 from the end of the ?X?-chain. We
omit the formal proof here, but obtain the following
statement.
Theorem 13 For every STSG, we can construct an
equivalent LMBOT with finite synchronization in lin-
ear time. LMBOT and MBOT with finite synchroniza-
tion are strictly more expressive than STSG and com-
pute classes that are not closed under composition.
Again, it is straightforward to adjust the rule ex-
traction algorithm by the introduction of synchro-
nization points (for example, for clause level output
symbols). We can simply require that rules extracted
for those selected output symbols fulfill the condi-
tion mentioned in Definition 11.
Finally, we introduce an even weaker version.
Definition 14 An LMBOT R is copy-free if there is
n ? N such that for every rule l ?? r ? Rn and
p ? ?NT(l) we have (i) ?
?1({p} ? N) ? N, or
(ii) ??1({p} ? N) ? {iw | w ? N?} for an i ? N.
Intuitively, a copy-free LMBOT has rules whose
right hand sides may use all leaf nonterminals that
are aligned to a given leaf nonterminal in the left-
hand side directly at the root (of one of the trees
832
XX
...
X
X
?
X
a X
a . . .
X
a X
,
X
a X
a . . .
X
a X
1
2
Figure 10: Composed rule that is not copy-free.
in the right-hand side forest) or group all those leaf
nonterminals in a single tree in the forest. Clearly,
the LMBOT of Figure 7 is not copy-free because the
second rule composes with itself (see Figure 10) to
a rule that does not fulfill the copy-free condition.
Theorem 15 Every copy-free LMBOT preserves
regularity.
Proof sketch: Let n be the integer of Defini-
tion 14. We replace the LMBOT with rules R by the
equivalent LMBOT M with rules Rn. Then all rules
have the form required in Definition 14. Moreover,
let L ? T? be a regular tree language. Then we
can construct the input product of ?(M) with L. In
this way, we obtain an MBOT M ?, whose rules still
fulfill the requirements (adapted for MBOT) of Defi-
nition 14 because the input product does not change
the structure of the rules (it only modifies the state
behavior). Consequently, we only need to show that
the range of the MBOT M ? is regular. This can be
achieved using a decomposition into a relabeling,
which clearly preserves regularity, and a determinis-
tic finite-copying top-down tree transducer (Engel-
friet et al, 1980; Engelfriet, 1982). 2
Figure 11 shows some relevant rules of a copy-
free LMBOT that computes the transformation of
Figure 9. Clearly, copy-free LMBOT are more gen-
eral than LMBOT with finite synchronization, so we
again can obtain copy-free LMBOT from STSG. In
addition, we can adjust the rule extraction process
using synchronization points as for LMBOT with fi-
nite synchronization using the restrictions of Defini-
tion 14.
Theorem 16 For every STSG, we can construct
an equivalent copy-free LMBOT in linear time.
Y
X S
?
Z
S S S
1
2
X
X
?
?
S , S
?
1
2
X
Y
S S
?
?
S , S
?
1
2
Figure 11: Copy-free LMBOT for the transformation
of Figure 9.
Copy-free LMBOT are strictly more expressive than
LMBOT with finite synchronization.
6 Conclusion
We have introduced a simple restriction of multi
bottom-up tree transducers. It abstracts from the
general state behavior of the general model and
only uses the locality tests that are also present in
STSG, STSSG, and STAG. Next, we introduced a
rule extraction procedure and a corresponding rule
weight training procedure for our LMBOT. However,
LMBOT allow translations that do not preserve reg-
ularity, which is an important property for efficient
algorithms. We presented 3 properties that ensure
that regularity is preserved. In addition, we shortly
discussed how these properties could be enforced in
the presented rule extraction procedure.
Acknowledgements
The author gratefully acknowledges the support by
KEVIN KNIGHT, who provided the inspiration and
the data. JONATHAN MAY helped in many fruitful
discussions.
The author was financially supported by
the German Research Foundation (DFG) grant
MA / 4959 / 1-1.
833
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory
of Parsing, Translation, and Compiling. Prentice Hall.
Andre? Arnold and Max Dauchet. 1982. Morphismes
et bimorphismes d?arbres. Theoret. Comput. Sci.,
20(1):33?93.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, John D. Laf-
ferty, Robert L. Mercer, and Paul S. Roossin. 1990. A
statistical approach to machine translation. Computa-
tional Linguistics, 16(2):79?85.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. Mathematics of
statistical machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. ACL, pages
263?270. Association for Computational Linguistics.
David Chiang. 2006. An introduction to synchronous
grammars. In Proc. ACL. Association for Computa-
tional Linguistics. Part of a tutorial given with Kevin
Knight.
Jason Eisner. 2003. Simpler and more general mini-
mization for weighted finite-state automata. In Proc.
NAACL, pages 64?71. Association for Computational
Linguistics.
Joost Engelfriet, Grzegorz Rozenberg, and Giora Slutzki.
1980. Tree transducers, L systems, and two-way ma-
chines. J. Comput. System Sci., 20(2):150?202.
Joost Engelfriet, Eric Lilin, and Andreas Maletti. 2009.
Composition and decomposition of extended multi
bottom-up tree transducers. Acta Inform., 46(8):561?
590.
Joost Engelfriet. 1982. The copying power of one-state
tree transducers. J. Comput. System Sci., 25(3):418?
435.
Akio Fujiyoshi. 2004. Restrictions on monadic context-
free tree grammars. In Proc. CoLing, pages 78?84.
Association for Computational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc.
HLT-NAACL, pages 273?280. Association for Compu-
tational Linguistics.
Jonathan Graehl, Kevin Knight, and Jonathan May. 2008.
Training tree transducers. Computational Linguistics,
34(3):391?427.
Kevin Knight. 2007. Capturing practical natu-
ral language transformations. Machine Translation,
21(2):121?133.
Eric Lilin. 1981. Proprie?te?s de clo?ture d?une ex-
tension de transducteurs d?arbres de?terministes. In
Proc. CAAP, volume 112 of LNCS, pages 280?289.
Springer.
Andreas Maletti. 2010. Why synchronous tree substi-
tution grammars? In Proc. NAACL, pages 876?884.
Association for Computational Linguistics.
Jonathan May, Kevin Knight, and Heiko Vogler. 2010.
Efficient inference through cascades of weighted tree
transducers. In Proc. ACL, pages 1058?1066. Associ-
ation for Computational Linguistics.
Frank G. Radmacher. 2008. An automata theoretic ap-
proach to rational tree relations. In Proc. SOFSEM,
volume 4910 of LNCS, pages 424?435. Springer.
Jean-Claude Raoult. 1997. Rational tree relations. Bull.
Belg. Math. Soc. Simon Stevin, 4(1):149?176.
Stuart M. Shieber and Yves Schabes. 1990. Synchronous
tree-adjoining grammars. In Proc. CoLing, volume 3,
pages 253?258. Association for Computational Lin-
guistics.
Stuart M. Shieber. 2004. Synchronous grammars as tree
transducers. In Proc. TAG+7, pages 88?95, Vancou-
ver, BC, Canada. Simon Fraser University.
Stuart M. Shieber. 2007. Probabilistic synchronous tree-
adjoining grammars for machine translation: The ar-
gument from bilingual dictionaries. In Proc. SSST,
pages 88?95. Association for Computational Linguis-
tics.
Jun Sun, Min Zhang, and Chew Lim Tan. 2009. A non-
contiguous tree sequence alignment-based model for
statistical machine translation. In Proc. ACL, pages
914?922. Association for Computational Linguistics.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008a. A tree se-
quence alignment-based tree-to-tree translation model.
In Proc. ACL, pages 559?567. Association for Compu-
tational Linguistics.
Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw, and
Sheng Li. 2008b. Grammar comparison study for
translational equivalence modeling and statistical ma-
chine translation. In Proc. CoLing, pages 1097?1104.
Association for Computational Linguistics.
834
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 506?515,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Strong Lexicalization of Tree Adjoining Grammars
Andreas Maletti?
IMS, Universita?t Stuttgart
Pfaffenwaldring 5b
70569 Stuttgart, Germany
maletti@ims.uni-stuttgart.de
Joost Engelfriet
LIACS, Leiden University
P.O. Box 9512
2300 RA Leiden, The Netherlands
engelfri@liacs.nl
Abstract
Recently, it was shown (KUHLMANN, SATTA:
Tree-adjoining grammars are not closed un-
der strong lexicalization. Comput. Linguist.,
2012) that finitely ambiguous tree adjoining
grammars cannot be transformed into a nor-
mal form (preserving the generated tree lan-
guage), in which each production contains a
lexical symbol. A more powerful model, the
simple context-free tree grammar, admits such
a normal form. It can be effectively con-
structed and the maximal rank of the non-
terminals only increases by 1. Thus, simple
context-free tree grammars strongly lexicalize
tree adjoining grammars and themselves.
1 Introduction
Tree adjoining grammars [TAG] (Joshi et al, 1969;
Joshi et al, 1975) are a mildly context-sensitive
grammar formalism that can handle certain non-
local dependencies (Kuhlmann and Mohl, 2006),
which occur in several natural languages. A good
overview on TAG, their formal properties, their lin-
guistic motivation, and their applications is pre-
sented by Joshi and Schabes (1992) and Joshi and
Schabes (1997), in which also strong lexicalization
is discussed. In general, lexicalization is the process
of transforming a grammar into an equivalent one
(potentially expressed in another formalism) such
that each production contains a lexical item (or an-
chor). Each production can then be viewed as lex-
ical information on its anchor. It demonstrates a
syntactical construction in which the anchor can oc-
cur. Since a lexical item is a letter of the string
? Financially supported by the German Research Founda-
tion (DFG) grant MA 4959 / 1-1.
alphabet, each production of a lexicalized gram-
mar produces at least one letter of the generated
string. Consequently, lexicalized grammars offer
significant parsing benefits (Schabes et al, 1988)
as the number of applications of productions (i.e.,
derivation steps) is clearly bounded by the length
of the input string. In addition, the lexical items
in the productions guide the production selection in
a derivation, which works especially well in sce-
narios with large alphabets.1 The GREIBACH nor-
mal form (Hopcroft et al, 2001; Blum and Koch,
1999) offers those benefits for context-free gram-
mars [CFG], but it changes the parse trees. Thus,
we distinguish between two notions of equivalence:
Weak equivalence (Bar-Hillel et al, 1960) only re-
quires that the generated string languages coincide,
whereas strong equivalence (Chomsky, 1963) re-
quires that even the generated tree languages coin-
cide. Correspondingly, we obtain weak and strong
lexicalization based on the required equivalence.
The GREIBACH normal form shows that CFG
can weakly lexicalize themselves, but they cannot
strongly lexicalize themselves (Schabes, 1990). It is
a prominent feature of tree adjoining grammars that
they can strongly lexicalize CFG (Schabes, 1990),2
and it was claimed and widely believed that they can
strongly lexicalize themselves. Recently, Kuhlmann
and Satta (2012) proved that TAG actually can-
not strongly lexicalize themselves. In fact, they
prove that TAG cannot even strongly lexicalize the
weaker tree insertion grammars (Schabes and Wa-
ters, 1995). However, TAG can weakly lexicalize
themselves (Fujiyoshi, 2005).
1Chen (2001) presents a detailed account.
2Good algorithmic properties and the good coverage of lin-
guistic phenomena are other prominent features.
506
Simple (i.e., linear and nondeleting) context-free
tree grammars [CFTG] (Rounds, 1969; Rounds,
1970) are a more powerful grammar formalism than
TAG (Mo?nnich, 1997). However, the monadic vari-
ant is strongly equivalent to a slightly extended ver-
sion of TAG, which is called non-strict TAG (Kepser
and Rogers, 2011). A GREIBACH normal form for a
superclass of CFTG (viz., second-order abstract cat-
egorial grammars) was discussed by Kanazawa and
Yoshinaka (2005) and Yoshinaka (2006). In particu-
lar, they also demonstrate that monadic CFTG can
strongly lexicalize regular tree grammars (Ge?cseg
and Steinby, 1984; Ge?cseg and Steinby, 1997).
CFTG are weakly equivalent to the simple macro
grammars of Fischer (1968), which are a notational
variant of the well-nested linear context-free rewrit-
ing systems (LCFRS) of Vijay-Shanker et al (1987)
and the well-nested multiple context-free grammars
(MCFG) of Seki et al (1991).3 Thus, CFTG are
mildly context-sensitive since their generated string
languages are semi-linear and can be parsed in poly-
nomial time (Go?mez-Rodr??guez et al, 2010).
In this contribution, we show that CFTG can
strongly lexicalize TAG and also themselves, thus
answering the second question in the conclusion
of Kuhlmann and Satta (2012). This is achieved
by a series of normalization steps (see Section 4)
and a final lexicalization step (see Section 5), in
which a lexical item is guessed for each produc-
tion that does not already contain one. This item
is then transported in an additional argument until
it is exchanged for the same item in a terminal pro-
duction. The lexicalization is effective and increases
the maximal rank (number of arguments) of the non-
terminals by at most 1. In contrast to a transforma-
tion into GREIBACH normal form, our lexicalization
does not radically change the structure of the deriva-
tions. Overall, our result shows that if we consider
only lexicalization, then CFTG are a more natural
generalization of CFG than TAG.
2 Notation
We write [k] for the set {i ? N | 1 ? i ? k},
where N denotes the set of nonnegative integers. We
use a fixed countably infinite set X = {x1, x2, . . . }
3Kuhlmann (2010), Mo?nnich (2010), and Kanazawa (2009)
discuss well-nestedness.
of (mutually distinguishable) variables, and we let
Xk = {xi | i ? [k]} be the first k variables from X
for every k ? N. As usual, an alphabet ? is a finite
set of symbols, and a ranked alphabet (?, rk) adds a
ranking rk : ? ? N. We let ?k = {? | rk(?) = k}
be the set of k-ary symbols. Moreover, we just
write ? for the ranked alphabet (?, rk).4 We build
trees over the ranked alphabet ? such that the nodes
are labeled by elements of ? and the rank of the node
label determines the number of its children. In addi-
tion, elements of X can label leaves. Formally, the
set T?(X) of ?-trees indexed by X is the smallest
set T such that X ? T and ?(t1, . . . , tk) ? T for all
k ? N, ? ? ?k, and t1, . . . , tk ? T .5
We use positions to address the nodes of a tree. A
position is a sequence of nonnegative integers indi-
cating successively in which subtree the addressed
node is. More precisely, the root is at position ? and
the position ip with i ? N and p ? N? refers to
the position p in the ith direct subtree. Formally, the
set pos(t) ? N? of positions of a tree t ? T?(X) is
defined by pos(x) = {?} for x ? X and
pos(?(t1, . . . , tk)) = {?} ? {ip | i ? [k], p ? pos(ti)}
for all symbols ? ? ?k and t1, . . . , tk ? T?(X).
The positions are indicated as superscripts of the la-
bels in the tree of Figure 1. The subtree of t at posi-
tion p ? pos(t) is denoted by t|p, and the label of t
at position p by t(p). Moreover, t[u]p denotes the
tree obtained from t by replacing the subtree at p by
the tree u ? T?(X). For every label set S ? ?,
we let posS(t) = {p ? pos(t) | t(p) ? S} be
the S-labeled positions of t. For every ? ? ?,
we let pos?(t) = pos{?}(t). The set C?(Xk) con-
tains all trees t of T?(X), in which every x ? Xk
occurs exactly once and posX\Xk(t) = ?. Given
u1, . . . , uk ? T?(X), the first-order substitution
t[u1, . . . , uk] is inductively defined by
xi[u1, . . . , uk] =
{
ui if i ? [k]
xi otherwise
t[u1, . . . , uk] = ?
(
t1[u1, . . . , uk], . . . , tk[u1, . . . , uk]
)
for every i ? N and t = ?(t1, . . . , tk) with ? ? ?k
and t1, . . . , tk ? T?(X). First-order substitution is
illustrated in Figure 1.
4We often decorate a symbol ? with its rank k [e.g. ?(k)].
5We will often drop quantifications like ?for all k ? N?.
507
?[?]
?[1]
?[11] x[12]2
?[2]
x[21]1 ?[22]
[ ?
?
, x1
]
=
?
?
? x1
?
?
?
?
Figure 1: Tree in C?(X2) ? T?(X) with indicated po-
sitions, where ? = {?, ?, ?} with rk(?) = 2, rk(?) = 1,
and rk(?) = 0, and an example first-order substitution.
In first-order substitution we replace leaves (ele-
ments of X), whereas in second-order substitution
we replace an internal node (labeled by a symbol
of ?). Let p ? pos(t) be such that t(p) ? ?k,
and let u ? C?(Xk) be a tree in which the vari-
ables Xk occur exactly once. The second-order sub-
stitution t[p ? u] replaces the subtree at position p
by the tree u into which the children of p are (first-
order) substituted. In essence, u is ?folded? into t at
position p. Formally, t[p? u] = t
[
u[t|1, . . . , t|k]
]
p.
Given P ? pos?(t) with ? ? ?k, we let t[P ? u]
be t[p1 ? u] ? ? ? [pn ? u], where P = {p1, . . . , pn}
and p1 > ? ? ? > pn in the lexicographic order.
Second-order substitution is illustrated in Figure 2.
Ge?cseg and Steinby (1997) present a detailed intro-
duction to trees and tree languages.
3 Context-free tree grammars
In this section, we recall linear and nondeleting
context-free tree grammars [CFTG] (Rounds, 1969;
Rounds, 1970). The property ?linear and nondelet-
ing? is often called ?simple?. The nonterminals of
regular tree grammars only occur at the leaves and
are replaced using first-order substitution. In con-
trast, the nonterminals of a CFTG are ranked sym-
bols, can occur anywhere in a tree, and are replaced
using second-order substitution.6 Consequently, the
nonterminals N of a CFTG form a ranked alpha-
bet. In the left-hand sides of productions we write
A(x1, . . . , xk) for a nonterminal A ? Nk to indi-
cate the variables that hold the direct subtrees of a
particular occurrence of A.
Definition 1. A (simple) context-free tree gram-
mar [CFTG] is a system (N,?, S, P ) such that
? N is a ranked alphabet of nonterminal symbols,
? ? is a ranked alphabet of terminal symbols,7
6see Sections 6 and 15 of (Ge?cseg and Steinby, 1997)
7We assume that ? ?N = ?.
?
? ?
? ?
[
??
?
?
? x2
?
x1 ?
]
=
?
?
? ?
? ?
?
? ?
Figure 2: Example second-order substitution, in which
the boxed symbol ? is replaced.
? S ? N0 is the start nonterminal of rank 0, and
? P is a finite set of productions of the form
A(x1, . . . , xk) ? r, where r ? CN??(Xk)
and A ? Nk.
The components ` and r are called left- and right-
hand side of the production ` ? r in P . We say
that it is an A-production if ` = A(x1, . . . , xk). The
right-hand side is simply a tree using terminal and
nonterminal symbols according to their rank. More-
over, it contains all the variables ofXk exactly once.
Let us illustrate the syntax on an example CFTG. We
use an abstract language for simplicity and clarity.
We use lower-case Greek letters for terminal sym-
bols and upper-case Latin letters for nonterminals.
Example 2. As a running example, we consider the
CFTG Gex = ({S(0), A(2)},?, S, P ) where
? ? = {?(2), ?(0), ?(0)} and
? P contains the productions (see Figure 3):8
S ? A(?, ?) | A(?, ?) | ?(?, ?)
A(x1, x2)? A(?(x1, S), ?(x2, S)) | ?(x1, x2) .
We recall the (term) rewrite semantics (Baader
and Nipkow, 1998) of the CFTG G = (N,?, S, P ).
Since G is simple, the actual rewriting strategy
is irrelevant. The sentential forms of G are sim-
ply SF(G) = TN??(X). This is slightly more gen-
eral than necessary (for the semantics of G), but the
presence of variables in sentential forms will be use-
ful in the next section because it allows us to treat
right-hand sides as sentential forms. In essence in a
rewrite step we just select a nonterminal A ? N and
an A-production ? ? P . Then we replace an occur-
rence of A in the sentential form by the right-hand
side of ? using second-order substitution.
Definition 3. Let ?, ? ? SF(G) be sentential forms.
Given an A-production ? = ` ? r in P and an
8We separate several right-hand sides with ?|?.
508
S ?
A
? ?
S ?
?
? ?
S ?
A
? ?
A
x1 x2
?
A
?
x1 S
?
x2 S
A
x1 x2
?
?
x1 x2
Figure 3: Productions of Example 2.
A-labeled position p ? posA(?) in ?, we write
? ??,pG ?[p ? r]. If there exist ? ? P and
p ? pos(?) such that ? ??,pG ?, then ? ?G ?.
9 The
semantics JGK of G is {t ? T? | S ??G t}, where
??G is the reflexive, transitive closure of?G.
Two CFTGG1 andG2 are (strongly) equivalent if
JG1K = JG2K. In this contribution we are only con-
cerned with strong equivalence (Chomsky, 1963).
Although we recall the string corresponding to a tree
later on (via its yield), we will not investigate weak
equivalence (Bar-Hillel et al, 1960).
Example 4. Reconsider the CFTG Gex of Exam-
ple 2. A derivation to a tree of T? is illustrated in
Figure 4. It demonstrates that the final tree in that
derivation is in the language JGexK generated byGex.
Finally, let us recall the relation between CFTG
and tree adjoining grammars [TAG] (Joshi et al,
1969; Joshi et al, 1975). Joshi et al (1975)
show that TAG are special footed CFTG (Kepser
and Rogers, 2011), which are weakly equivalent
to monadic CFTG, i.e., CFTG whose nonterminals
have rank at most 1 (Mo?nnich, 1997; Fujiyoshi
and Kasai, 2000). Kepser and Rogers (2011) show
the strong equivalence of those CFTG to non-strict
TAG, which are slightly more powerful than tradi-
tional TAG. In general, TAG are a natural formalism
to describe the syntax of natural language.10
4 Normal forms
In this section, we first recall an existing normal
form for CFTG. Then we introduce the property of
finite ambiguity in the spirit of (Schabes, 1990; Joshi
and Schabes, 1992; Kuhlmann and Satta, 2012),
which allows us to normalize our CFTG even fur-
ther. A major tool is a simple production elimination
9For all k ? N and ? ?G ? we note that ? ? CN??(Xk) if
and only if ? ? CN??(Xk).
10XTAG Research Group (2001) wrote a TAG for English.
scheme, which we present in detail. From now on,
let G = (N,?, S, P ) be the considered CFTG.
The CFTG G is start-separated if posS(r) = ?
for every production `? r ? P . In other words, the
start nonterminal S is not allowed in the right-hand
sides of the productions. It is clear that each CFTG
can be transformed into an equivalent start-separated
CFTG. In such a CFTG we call each production of
the form S ? r initial. From now on, we assume,
without loss of generality, that G is start-separated.
Example 5. Let Gex = (N,?, S, P ) be the CFTG
of Example 2. An equivalent start-separated CFTG
is G?ex = ({S
?(0)} ?N,?, S?, P ? {S? ? S}).
We start with the growing normal form of Stamer
and Otto (2007) and Stamer (2009). It requires that
the right-hand side of each non-initial production
contains at least two terminal or nonterminal sym-
bols. In particular, it eliminates projection produc-
tions A(x1) ? x1 and unit productions, in which
the right-hand side has the same shape as the left-
hand side (potentially with a different root symbol
and a different order of the variables).
Definition 6. A production ` ? r is growing if
|posN??(r)| ? 2. The CFTG G is growing if all
of its non-initial productions are growing.
The next theorem is Proposition 2 of (Stamer and
Otto, 2007). Stamer (2009) provides a full proof.
Theorem 7. For every start-separated CFTG there
exists an equivalent start-separated, growing CFTG.
Example 8. Let us transform the CFTG G?ex of Ex-
ample 5 into growing normal form. We obtain the
CFTG G??ex = ({S
?(0), S(0), A(2)},?, S?, P ??) where
P ?? contains S? ? S and for each ? ? {?, ?}
S ? A(?, ?) | ?(?, ?) | ?(?, ?) (1)
A(x1, x2)? A(?(x1, S), ?(x2, S)) (2)
A(x1, x2)? ?(?(x1, S), ?(x2, S)) .
From now on, we assume thatG is growing. Next,
we recall the notion of finite ambiguity from (Sch-
abes, 1990; Joshi and Schabes, 1992; Kuhlmann and
Satta, 2012).11 We distinguish a subset ? ? ?0 of
lexical symbols, which are the symbols that are pre-
served by the yield mapping. The yield of a tree is
11It should not be confused with the notion of ?finite ambigu-
ity? of (Goldstine et al, 1992; Klimann et al, 2004).
509
S ?G
A
? ?
?G
A
?
? S
?
? S
?G
A
?
? A
? ?
?
? S
?G
A
?
? A
? ?
?
? ?
? ?
??G
?
?
? ?
? ?
?
? ?
? ?
Figure 4: Derivation using the CFTG Gex of Example 2. The selected positions are boxed.
a string of lexical symbols. All other symbols are
simply dropped (in a pre-order traversal). Formally,
yd? : T? ? ?
? is such that for all t = ?(t1, . . . , tk)
with ? ? ?k and t1, . . . , tk ? T?
yd?(t) =
{
? yd?(t1) ? ? ? yd?(tk) if ? ? ?
yd?(t1) ? ? ? yd?(tk) otherwise.
Definition 9. The tree language L ? T? has finite
?-ambiguity if {t ? L | yd?(t) = w} is finite for
every w ? ??.
Roughly speaking, we can say that the set L has
finite ?-ambiguity if eachw ? ?? has finitely many
parses in L (where t is a parse of w if yd?(t) = w).
Our example CFTG Gex is such that JGexK has finite
{?, ?}-ambiguity (because ?1 = ?).
In this contribution, we want to (strongly) lexical-
ize CFTG, which means that for each CFTG G such
that JGK has finite ?-ambiguity, we want to con-
struct an equivalent CFTG such that each non-initial
production contains at least one lexical symbol.
This is typically called strong lexicalization (Sch-
abes, 1990; Joshi and Schabes, 1992; Kuhlmann
and Satta, 2012) because we require strong equiva-
lence.12 Let us formalize our lexicalization property.
Definition 10. The production ` ? r is ?-lexical-
ized if pos?(r) 6= ?. The CFTG G is ?-lexicalized
if all its non-initial productions are ?-lexicalized.
Note that the CFTG G??ex of Example 8 is not yet
{?, ?}-lexicalized. We will lexicalize it in the next
section. To do this in general, we need some auxil-
iary normal forms. First, we define our simple pro-
duction elimination scheme, which we will use in
the following. Roughly speaking, a non-initial A-
production such that A does not occur in its right-
hand side can be eliminated fromG by applying it in
12The corresponding notion for weak equivalence is called
weak lexicalization (Joshi and Schabes, 1992).
all possible ways to occurrences in right-hand sides
of the remaining productions.
Definition 11. Let ? = A(x1, . . . , xk) ? r in P
be a non-initial production such that posA(r) = ?.
For every other production ?? = `? ? r? in P and
J ? posA(r
?), let ??J = `
? ? r?[J ? r]. The CFTG
Elim(G, ?) = (N,?, S, P ?) is such that
P ? =
?
??=`??r??P\{?}
{??J | J ? posA(r
?)} .
In particular, ??? = ?
? for every production ??,
so every production besides the eliminated produc-
tion ? is preserved. We obtained the CFTG G??ex of
Example 8 as Elim(G?ex, A(x1, x2) ? ?(x1, x2))
from G?ex of Example 5.
Lemma 12. The CFTG G and G?? = Elim(G, ?)
are equivalent for every non-initial A-production
? = `? r in P such that posA(r) = ?.
Proof. Clearly, every single derivation step of G??
can be simulated by a derivation of G using poten-
tially several steps. Conversely, a derivation of G
can be simulated directly by G?? except for deriva-
tion steps ??,pG using the eliminated production ?.
Since S 6= A, we know that the nonterminal at po-
sition p was generated by another production ??. In
the given derivation of G we examine which non-
terminals in the right-hand side of the instance of ??
were replaced using ?. Let J be the set of positions
corresponding to those nonterminals (thus p ? J).
Then instead of applying ?? and potentially several
times ?, we equivalently apply ??J of G
?
?.
In the next normalization step we use our pro-
duction elimination scheme. The goal is to make
sure that non-initial monic productions (i.e., produc-
tions of which the right-hand side contains at most
one nonterminal) contain at least one lexical sym-
bol. We define the relevant property and then present
510
the construction. A sentential form ? ? SF(G)
is monic if |posN (?)| ? 1. The set of all monic
sentential forms is denoted by SF?1(G). A pro-
duction ` ? r is monic if r is monic. The next
construction is similar to the simultaneous removal
of epsilon-productions A ? ? and unit productions
A ? B for context-free grammars (Hopcroft et al,
2001). Instead of computing the closure under those
productions, we compute a closure under non-?-
lexicalized productions.
Theorem 13. If JGK has finite ?-ambiguity, then
there exists an equivalent CFTG such that all its non-
initial monic productions are ?-lexicalized.
Proof. Without loss of generality, we assume that
G is start-separated and growing by Theorem 7.
Moreover, we assume that each nonterminal is use-
ful. For every A ? N with A 6= S, we compute
all monic sentential forms without a lexical sym-
bol that are reachable from A(x1, . . . , xk), where
k = rk(A). Formally, let
?A = {? ? SF?1(G) | A(x1, . . . , xk)?
+
G? ?} ,
where?+G? is the transitive closure of?G? and the
CFTG G? = (N,?, S, P ?) is such that P ? contains
exactly the non-?-lexicalized productions of P .
The set ?A is finite since only finitely many non-
?-lexicalized productions can be used due to the
finite ?-ambiguity of JGK. Moreover, no senten-
tial form in ?A contains A for the same reason
and the fact that G is growing. We construct the
CFTG G1 = (N,?, S, P ? P1) such that
P1 = {A(x1, . . . , xk)? ? | A ? Nk, ? ? ?A} .
Clearly, G and G1 are equivalent. Next, we elimi-
nate all productions of P1 from G1 using Lemma 12
to obtain an equivalent CFTG G2 with the produc-
tions P2. In the final step, we drop all non-?-
lexicalized monic productions of P2 to obtain the
CFTG G, in which all monic productions are ?-
lexicalized. It is easy to see that G is growing, start-
separated, and equivalent to G2.
The CFTG G??ex only has {?, ?}-lexicalized non-
initial monic productions, so we use a new example.
Example 14. Let ({S(0), A(1), B(1)},?, S, P ) be
the CFTG such that ? = {?(2), ?(0), ?(0)} and
A
x1
?G?
?
? B
x1
?G?
?
? ?
x1 ?
B
x1
?G?
?
x1 ?
Figure 5: The relevant derivations using only productions
that are not ?-lexicalized (see Example 14).
P contains the productions
A(x1)? ?(?,B(x1)) B(x1)? ?(x1, ?) (3)
B(x1)? ?(?,A(x1)) S ? A(?) .
This CFTG Gex2 is start-separated and growing.
Moreover, all its productions are monic, and JGex2K
is finitely ?-ambiguous for the set ? = {?} of
lexical symbols. Then the productions (3) are non-
initial and not ?-lexicalized. So we can run the
construction in the proof of Theorem 13. The rel-
evant derivations using only non-?-lexicalized pro-
ductions are shown in Figure 5. We observe that
|?A| = 2 and |?B| = 1, so we obtain the CFTG
({S(0), B(1)},?, S, P ?), where P ? contains13
S ? ?(?,B(?)) | ?(?, ?(?, ?))
B(x1)? ?(?, ?(?,B(x1)))
B(x1)? ?(?, ?(?, ?(x1, ?))) . (4)
We now do one more normalization step before
we present our lexicalization. We call a production
` ? r terminal if r ? T?(X); i.e., it does not con-
tain nonterminal symbols. Next, we show that for
each CFTG G such that JGK has finite ?-ambiguity
we can require that each non-initial terminal produc-
tion contains at least two occurrences of ?-symbols.
Theorem 15. If JGK has finite ?-ambiguity, then
there exists an equivalent CFTG (N,?, S, P ?) such
that |pos?(r)| ? 2 for all its non-initial terminal
productions `? r ? P ?.
Proof. Without loss of generality, we assume that
G is start-separated and growing by Theorem 7.
Moreover, we assume that each nonterminal is use-
ful and that each of its non-initial monic produc-
tions is ?-lexicalized by Theorem 13. We obtain
the desired CFTG by simply eliminating each non-
initial terminal production ` ? r ? P such that
|pos?(r)| = 1. By Lemma 12 the obtained CFTG
13The nonterminal A became useless, so we just removed it.
511
Ax1 x2
?
A
?
x1 S
?
x2 S
?A,??
x1 x2 x3
?
?A,??
?
x1 S
?
x2 S
x3
?A,??
x1 x2 x3
?
?A,??
?
x1 ?S, ??
?
?
x2 S
x3
Figure 6: Production ? = `? r of (2) [left], a corresponding production ?? of P ? [middle] with right-hand side r?,2,
and a corresponding production of P ??? [right] with right-hand side (r?,2)? (see Theorem 17).
is equivalent to G. The elimination process termi-
nates because a new terminal production can only be
constructed from a monic production and a terminal
production or several terminal productions, but those
combinations already contain two occurrences of ?-
symbols since non-initial monic productions are al-
ready ?-lexicalized.
Example 16. Reconsider the CFTG obtained in Ex-
ample 14. Recall that ? = {?}. Production (4) is
the only non-initial terminal production that violates
the requirement of Theorem 15. We eliminate it and
obtain the CFTG with the productions
S ? ?(?,B(?)) | ?(?, ?(?, ?))
S ? ?(?, ?(?, ?(?, ?(?, ?))))
B(x1)? ?(?, ?(?,B(x1)))
B(x1)? ?(?, ?(?, ?(?, ?(?, ?(x1, ?))))) .
5 Lexicalization
In this section, we present the main lexicalization
step, which lexicalizes non-monic productions. We
assume that JGK has finite ?-ambiguity and is nor-
malized according to the results of Section 4: no
useless nonterminals, start-separated, growing (see
Theorem 7), non-initial monic productions are ?-
lexicalized (see Theorem 13), and non-initial termi-
nal productions contain at least two occurrences of
?-symbols (see Theorem 15).
The basic idea of the construction is that we guess
a lexical symbol for each non-?-lexicalized produc-
tion. The guessed symbol is put into a new param-
eter of a nonterminal. It will be kept in the pa-
rameter until we reach a terminal production, where
we exchange the same lexical symbol by the pa-
rameter. This is the reason why we made sure
that we have two occurrences of lexical symbols in
the terminal productions. After we exchanged one
for a parameter, the resulting terminal production is
still ?-lexicalized. Lexical items that are guessed
for distinct (occurrences of) productions are trans-
ported to distinct (occurrences of) terminal produc-
tions [cf. Section 3 of (Potthoff and Thomas, 1993)
and page 346 of (Hoogeboom and ten Pas, 1997)].
Theorem 17. For every CFTG G such that JGK
has finite ?-ambiguity there exists an equivalent
?-lexicalized CFTG.
Proof. We can assume that G = (N,?, S, P ) has
the properties mentioned before the theorem without
loss of generality. We let N ? = N ?? be a new set
of nonterminals such that rk(?A, ??) = rk(A) + 1
for every A ? N and ? ? ?. Intuitively, ?A, ??
represents the nonterminal A, which has the lexical
symbol ? in its last (new) parameter. This parameter
is handed to the (lexicographically) first nonterminal
in the right-hand side until it is resolved in a termi-
nal production. Formally, for each right-hand side
r ? TN?N ???(X) such that posN (r) 6= ? (i.e., it
contains an original nonterminal), each k ? N, and
each ? ? ?, let r?,k and r? be such that
r?,k = r[?B, ??(r1, . . . , rn, xk+1)]p
r? = r[?B, ??(r1, . . . , rn, ?)]p ,
where p is the lexicographically smallest element
of posN (r) and r|p = B(r1, . . . , rn) with B ? N
and r1, . . . , rn ? TN?N ???(X). For each non-
terminal A-production ? = `? r in P let
?? = ?A, ??(x1, . . . , xk+1)? r?,k ,
where k = rk(A). This construction is illustrated
in Figure 6. Roughly speaking, we select the lexi-
cographically smallest occurrence of a nonterminal
in the right-hand side and pass the lexical symbol ?
in the extra parameter to it. The extra parameter is
used in terminal productions, so let ? = `? r in P
512
S ?
?
? ?
?S, ??
x1
?
?
x1 ?
Figure 7: Original terminal production ? from (1) [left]
and the production ? (see Theorem 17).
be a terminal A-production. Then we define
? = ?A, r(p)?(x1, . . . , xk+1)? r[xk+1]p ,
where p is the lexicographically smallest element
of pos?(r) and k = rk(A). This construction is
illustrated in Figure 7. With these productions we
obtain the CFTG G? = (N ? N ?,?, S, P ), where
P = P ? P ? ? P ?? and
P ? =
?
?=`?r?P
6`=S,posN (r)6=?
{?? | ? ? ?} P
?? =
?
?=`?r?P
6`=S,posN (r)=?
{?} .
It is easy to prove that those new productions man-
age the desired transport of the extra parameter if it
holds the value indicated in the nonterminal.
Finally, we replace each non-initial non-?-lexi-
calized production in G? by new productions that
guess a lexical symbol and add it to the new parame-
ter of the (lexicographically) first nonterminal of N
in the right-hand side. Formally, we let
P nil = {`? r ? P | ` 6= S, pos?(r) = ?}
P ??? = {`? r? | `? r ? P nil, ? ? ?} ,
of which P ??? is added to the productions. Note that
each production ` ? r ? P nil contains at least one
occurrence of a nonterminal ofN (because all monic
productions of G are ?-lexicalized). Now all non-
initial non-?-lexicalized productions from P can be
removed, so we obtain the CFTGG??, which is given
by (N ?N ?,?, S,R) with R = (P ? P ???) \ P nil. It
can be verified that G?? is ?-lexicalized and equiva-
lent to G (using the provided argumentation).
Instead of taking the lexicographically smallest
element of posN (r) or pos?(r) in the previous
proof, we can take any fixed element of that set. In
the definition of P ? we can change posN (r) 6= ?
to |pos?(r)| ? 1, and simultaneously in the defini-
tion of P ?? change posN (r) = ? to |pos?(r)| ? 2.
With the latter changes the guessed lexical item is
only transported until it is resolved in a production
with at least two lexical items.
Example 18. For the last time, we consider the
CFTG G??ex of Example 8. We already illustrated the
parts of the construction of Theorem 17 in Figures
6 and 7. The obtained {?, ?}-lexicalized CFTG has
the following 25 productions for all ?, ?? ? {?, ?}:
S? ? S
S ? A(?, ?) | ?(?, ?) | ?(?, ?)
S?(x1)? A?(?
?, ??, x1) | ?(x1, ?)
S?(x1)? ?(x1, ?)
A(x1, x2)? A?(?(x1, S), ?(x2, S), ?) (5)
A?(x1, x2, x3)? A?(?(x1, S??(?
?)), ?(x2, S), x3)
A(x1, x2)? ?(?(x1, S?(?)), ?(x2, S))
A?(x1, x2, x3)? ?(?(x1, S?(x3)), ?(x2, S??(?
?))) ,
where A? = ?A, ?? and S? = ?S, ??.
If we change the lexicalization construction as
indicated before this example, then all the produc-
tions S?(x1) ? A?(??, ??, x1) are replaced by the
productions S?(x1) ? A(x1, ?). Moreover, the
productions (5) can be replaced by the productions
A(x1, x2) ? A(?(x1, S?(?)), ?(x2, S)), and then
the nonterminalsA? and their productions can be re-
moved, which leaves only 15 productions.
Conclusion
For k ? N, let CFTG(k) be the set of those CFTG
whose nonterminals have rank at most k. Since the
normal form constructions preserve the nonterminal
rank, the proof of Theorem 17 shows that CFTG(k)
are strongly lexicalized by CFTG(k+1). Kepser and
Rogers (2011) show that non-strict TAG are strongly
equivalent to CFTG(1). Hence, non-strict TAG are
strongly lexicalized by CFTG(2).
It follows from Section 6 of Engelfriet et al
(1980) that the classes CFTG(k) with k ? N in-
duce an infinite hierarchy of string languages, but it
remains an open problem whether the rank increase
in our lexicalization construction is necessary.
Go?mez-Rodr??guez et al (2010) show that well-
nested LCFRS of maximal fan-out k can be parsed
in time O(n2k+2), where n is the length of the in-
put string w ? ??. From this result we conclude
that CFTG(k) can be parsed in time O(n2k+4), in
the sense that we can produce a parse tree t that
is generated by the CFTG with yd?(t) = w. It is
not clear yet whether lexicalized CFTG(k) can be
parsed more efficiently in practice.
513
References
Franz Baader and Tobias Nipkow. 1998. Term Rewriting
and All That. Cambridge University Press.
Yehoshua Bar-Hillel, Haim Gaifman, and Eli Shamir.
1960. On categorial and phrase-structure grammars.
Bulletin of the Research Council of Israel, 9F(1):1?16.
Norbert Blum and Robert Koch. 1999. Greibach normal
form transformation revisited. Inform. and Comput.,
150(1):112?118.
John Chen. 2001. Towards Efficient Statistical Parsing
using Lexicalized Grammatical Information. Ph.D.
thesis, University of Delaware, Newark, USA.
Noam Chomsky. 1963. Formal properties of gram-
mar. In R. Duncan Luce, Robert R. Bush, and Eugene
Galanter, editors, Handbook of Mathematical Psychol-
ogy, volume 2, pages 323?418. John Wiley and Sons,
Inc.
Joost Engelfriet, Grzegorz Rozenberg, and Giora Slutzki.
1980. Tree transducers, L systems, and two-way ma-
chines. J. Comput. System Sci., 20(2):150?202.
Michael J. Fischer. 1968. Grammars with macro-like
productions. In Proc. 9th Ann. Symp. Switching and
Automata Theory, pages 131?142. IEEE Computer
Society.
Akio Fujiyoshi. 2005. Epsilon-free grammars and
lexicalized grammars that generate the class of the
mildly context-sensitive languages. In Proc. 7th Int.
Workshop Tree Adjoining Grammar and Related For-
malisms, pages 16?23.
Akio Fujiyoshi and Takumi Kasai. 2000. Spinal-formed
context-free tree grammars. Theory Comput. Syst.,
33(1):59?83.
Ferenc Ge?cseg and Magnus Steinby. 1984. Tree Au-
tomata. Akade?miai Kiado?, Budapest.
Ferenc Ge?cseg and Magnus Steinby. 1997. Tree lan-
guages. In Grzegorz Rozenberg and Arto Salomaa,
editors, Handbook of Formal Languages, volume 3,
chapter 1, pages 1?68. Springer.
Jonathan Goldstine, Hing Leung, and Detlef Wotschke.
1992. On the relation between ambiguity and nonde-
terminism in finite automata. Inform. and Comput.,
100(2):261?270.
Carlos Go?mez-Rodr??guez, Marco Kuhlmann, and Gior-
gio Satta. 2010. Efficient parsing of well-nested lin-
ear context-free rewriting systems. In Proc. Ann. Conf.
North American Chapter of the ACL, pages 276?284.
Association for Computational Linguistics.
Hendrik Jan Hoogeboom and Paulien ten Pas. 1997.
Monadic second-order definable text languages. The-
ory Comput. Syst., 30(4):335?354.
John E. Hopcroft, Rajeev Motwani, and Jeffrey D. Ull-
man. 2001. Introduction to automata theory, lan-
guages, and computation. Addison-Wesley series in
computer science. Addison Wesley, 2nd edition.
Aravind K. Joshi, S. Rao Kosaraju, and H. Yamada.
1969. String adjunct grammars. In Proc. 10th Ann.
Symp. Switching and Automata Theory, pages 245?
262. IEEE Computer Society.
Aravind K. Joshi, Leon S. Levy, and Masako Takahashi.
1975. Tree adjunct grammars. J. Comput. System Sci.,
10(1):136?163.
Aravind K. Joshi and Yves Schabes. 1992. Tree-
adjoining grammars and lexicalized grammars. In
Maurice Nivat and Andreas Podelski, editors, Tree Au-
tomata and Languages. North-Holland.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In Grzegorz Rozenberg and Arto
Salomaa, editors, Beyond Words, volume 3 of Hand-
book of Formal Languages, pages 69?123. Springer.
Makoto Kanazawa. 2009. The convergence of well-
nested mildly context-sensitive grammar formalisms.
Invited talk at the 14th Int. Conf. Formal Gram-
mar. slides available at: research.nii.ac.jp/
?kanazawa.
Makoto Kanazawa and Ryo Yoshinaka. 2005. Lexical-
ization of second-order ACGs. Technical Report NII-
2005-012E, National Institute of Informatics, Tokyo,
Japan.
Stephan Kepser and James Rogers. 2011. The equiv-
alence of tree adjoining grammars and monadic lin-
ear context-free tree grammars. J. Log. Lang. Inf.,
20(3):361?384.
Ines Klimann, Sylvain Lombardy, Jean Mairesse, and
Christophe Prieur. 2004. Deciding unambiguity and
sequentiality from a finitely ambiguous max-plus au-
tomaton. Theoret. Comput. Sci., 327(3):349?373.
Marco Kuhlmann. 2010. Dependency Structures and
Lexicalized Grammars: An Algebraic Approach, vol-
ume 6270 of LNAI. Springer.
Marco Kuhlmann and Mathias Mohl. 2006. Extended
cross-serial dependencies in tree adjoining grammars.
In Proc. 8th Int. Workshop Tree Adjoining Grammars
and Related Formalisms, pages 121?126. ACL.
Marco Kuhlmann and Giorgio Satta. 2012. Tree-
adjoining grammars are not closed under strong lex-
icalization. Comput. Linguist. available at: dx.doi.
org/10.1162/COLI_a_00090.
Uwe Mo?nnich. 1997. Adjunction as substitution: An
algebraic formulation of regular, context-free and tree
adjoining languages. In Proc. 3rd Int. Conf. Formal
Grammar, pages 169?178. Universite? de Provence,
France. available at: arxiv.org/abs/cmp-lg/
9707012v1.
Uwe Mo?nnich. 2010. Well-nested tree languages and at-
tributed tree transducers. In Proc. 10th Int. Conf. Tree
Adjoining Grammars and Related Formalisms. Yale
University. available at: www2.research.att.
com/?srini/TAG+10/papers/uwe.pdf.
514
Andreas Potthoff and Wolfgang Thomas. 1993. Reg-
ular tree languages without unary symbols are star-
free. In Proc. 9th Int. Symp. Fundamentals of Compu-
tation Theory, volume 710 of LNCS, pages 396?405.
Springer.
William C. Rounds. 1969. Context-free grammars on
trees. In Proc. 1st ACM Symp. Theory of Comput.,
pages 143?148. ACM.
William C. Rounds. 1970. Tree-oriented proofs of some
theorems on context-free and indexed languages. In
Proc. 2nd ACM Symp. Theory of Comput., pages 109?
116. ACM.
Yves Schabes. 1990. Mathematical and Computational
Aspects of Lexicalized Grammars. Ph.D. thesis, Uni-
versity of Pennsylvania, Philadelphia, USA.
Yves Schabes, Anne Abeille?, and Aravind K. Joshi.
1988. Parsing strategies with ?lexicalized? grammars:
Application to tree adjoining grammars. In Proc. 12th
Int. Conf. Computational Linguistics, pages 578?583.
John von Neumann Society for Computing Sciences,
Budapest.
Yves Schabes and Richard C. Waters. 1995. Tree in-
sertion grammar: A cubic-time parsable formalism
that lexicalizes context-free grammars without chang-
ing the trees produced. Comput. Linguist., 21(4):479?
513.
Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, and
Tadao Kasami. 1991. On multiple context-free gram-
mars. Theoret. Comput. Sci., 88(2):191?229.
Heiko Stamer. 2009. Restarting Tree Automata: Formal
Properties and Possible Variations. Ph.D. thesis, Uni-
versity of Kassel, Germany.
Heiko Stamer and Friedrich Otto. 2007. Restarting tree
automata and linear context-free tree languages. In
Proc. 2nd Int. Conf. Algebraic Informatics, volume
4728 of LNCS, pages 275?289. Springer.
K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi.
1987. Characterizing structural descriptions produced
by various grammatical formalisms. In Proc. 25th
Ann. Meeting of the Association for Computational
Linguistics, pages 104?111. Association for Compu-
tational Linguistics.
XTAG Research Group. 2001. A lexicalized tree adjoin-
ing grammar for English. Technical Report IRCS-01-
03, University of Pennsylvania, Philadelphia, USA.
Ryo Yoshinaka. 2006. Extensions and Restrictions of
Abstract Categorial Grammars. Ph.D. thesis, Univer-
sity of Tokyo.
515
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 811?821,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Shallow Local Multi Bottom-up Tree Transducers
in Statistical Machine Translation
Fabienne Braune and Nina Seemann and Daniel Quernheim and Andreas Maletti
Institute for Natural Language Processing, University of Stuttgart
Pfaffenwaldring 5b, 70569 Stuttgart, Germany
{braunefe,seemanna,daniel,maletti}@ims.uni-stuttgart.de
Abstract
We present a new translation model in-
tegrating the shallow local multi bottom-
up tree transducer. We perform a large-
scale empirical evaluation of our obtained
system, which demonstrates that we sig-
nificantly beat a realistic tree-to-tree base-
line on the WMT 2009 English?German
translation task. As an additional contribu-
tion we make the developed software and
complete tool-chain publicly available for
further experimentation.
1 Introduction
Besides phrase-based machine translation sys-
tems (Koehn et al, 2003), syntax-based systems
have become widely used because of their abil-
ity to handle non-local reordering. Those systems
use synchronous context-free grammars (Chi-
ang, 2007), synchronous tree substitution gram-
mars (Eisner, 2003) or even more powerful for-
malisms like synchronous tree-sequence substitu-
tion grammars (Sun et al, 2009). However, those
systems use linguistic syntactic annotation at dif-
ferent levels. For example, the systems proposed
by Wu (1997) and Chiang (2007) use no linguis-
tic information and are syntactic in a structural
sense only. Huang et al (2006) and Liu et al
(2006) use syntactic annotations on the source lan-
guage side and show significant improvements in
translation quality. Using syntax exclusively on
the target language side has also been success-
fully tried by Galley et al (2004) and Galley et
al. (2006). Nowadays, open-source toolkits such
as Moses (Koehn et al, 2007) offer syntax-based
components (Hoang et al, 2009), which allow
experiments without expert knowledge. The im-
provements observed for systems using syntactic
annotation on either the source or the target lan-
guage side naturally led to experiments with mod-
els that use syntactic annotations on both sides.
However, as noted by Lavie et al (2008), Liu et
al. (2009), and Chiang (2010), the integration of
syntactic information on both sides tends to de-
crease translation quality because the systems be-
come too restrictive. Several strategies such as
(i) using parse forests instead of single parses (Liu
et al, 2009) or (ii) soft syntactic constraints (Chi-
ang, 2010) have been developed to alleviate this
problem. Another successful approach has been
to switch to more powerful formalisms, which al-
low the extraction of more general rules. A par-
ticularly powerful model is the non-contiguous
version of synchronous tree-sequence substitu-
tion grammars (STSSG) of Zhang et al (2008a),
Zhang et al (2008b), and Sun et al (2009),
which allows sequences of trees on both sides of
the rules [see also (Raoult, 1997)]. The multi
bottom-up tree transducer (MBOT) of Arnold and
Dauchet (1982) and Lilin (1978) offers a middle
ground between traditional syntax-based models
and STSSG. Roughly speaking, an MBOT is an
STSSG, in which all the discontinuities must oc-
cur on the target language side (Maletti, 2011).
This restriction yields many algorithmic advan-
tages over both the traditional models as well as
STSSG as demonstrated by Maletti (2010). For-
mally, they are expressive enough to express all
sensible translations (Maletti, 2012)1. Figure 2
displays sample rules of the MBOT variant, called
`MBOT, that we use (in a graphical representation
of the trees and the alignment).
In this contribution, we report on our novel sta-
tistical machine translation system that uses an
`MBOT-based translation model. The theoreti-
cal foundations of `MBOT and their integration
into our translation model are presented in Sec-
tions 2 and 3. In order to empirically evaluate the
`MBOT model, we implemented a machine trans-
1A translation is sensible if it is of linear size increase
and can be computed by some (potentially copying) top-down
tree transducer.
811
S?
NP1
JJ11
Official111
NNS12
forecasts121
VP2
VBD21
predicted211
NP22
QP221
RB2211
just22111
CD2212
322121
NN222
%2221
Figure 1: Example tree t with indicated positions.
We have t(21) = VBD and t|221 is the subtree
marked in red.
lation system that we are going to make available
to the public. We implemented `MBOT inside
the syntax-based component of the Moses open
source toolkit. Section 4 presents the most im-
portant algorithms of our `MBOT decoder. We
evaluate our new system on the WMT 2009 shared
translation task English ? German. The trans-
lation quality is automatically measured using
BLEU scores, and we confirm the findings by pro-
viding linguistic evidence (see Section 5). Note
that in contrast to several previous approaches, we
perform large scale experiments by training sys-
tems with approx. 1.5 million parallel sentences.
2 Theoretical Model
In this section, we present the theoretical genera-
tive model used in our approach to syntax-based
machine translation. Essentially, it is the local
multi bottom-up tree transducer of Maletti (2011)
with the restriction that all rules must be shallow,
which means that the left-hand side of each rule
has height at most 2 (see Figure 2 for shallow
rules and Figure 4 for rules including non-shallow
rules). The rules extracted from the training exam-
ple of Figure 3 are displayed in Figure 4. Those
extracted rules are forcibly made shallow by re-
moving internal nodes. The application of those
rules is illustrated in Figures 5 and 6.
For those that want to understand the inner
workings, we recall the principal model in full de-
tail in the rest of this section. Since we utilize syn-
tactic parse trees, let us introduce trees first. Given
an alphabet ? of labels, the set T? of all ?-trees is
the smallest set T such that ?(t1, . . . , tk) ? T for
all ? ? ?, integer k ? 0, and t1, . . . , tk ? T . In-
tuitively, a tree t consists of a labeled root node ?
followed by a sequence t1, . . . , tk of its children.
A tree t ? T? is shallow if t = ?(t1, . . . , tk) with
? ? ? and t1, . . . , tk ? ?.
NP
QP NN ?
( PP
von AP NN
)
S
NP VBD NP ?
( S
NP VAFIN PP VVPP
)
Figure 2: Sample `MBOT rules.
To address a node inside a tree, we use its po-
sition, which is a word consisting of positive in-
tegers. Roughly speaking, the root of a tree is
addressed with the position ? (the empty word).
The position iw with i ? N addresses the po-
sition w in the ith direct child of the root. In
this way, each node in the tree is assigned a
unique position. We illustrate this notion in Fig-
ure 1. Formally, the positions pos(t) ? N? of
a tree t = ?(t1, . . . , tk) are inductively defined
by pos(t) = {?} ? pos(k)(t1, . . . , tk), where
pos(k)(t1, . . . , tk) =
?
1?i?k
{iw | w ? pos(ti)} .
Let t ? T? and w ? pos(t). The label of t at
position w is t(w), and the subtree rooted at posi-
tion w is t|w. These notions are also illustrated in
Figure 1. A position w ? pos(t) is a leaf (in t) if
w1 /? pos(t). In other words, leaves do not have
any children. Given a subset N ? ?, we let
leafN (t) = {w ? pos(t) | t(w) ? N, w leaf in t}
be the set of all leaves labeled by elements of N .
When N is the set of nonterminals, we call them
leaf nonterminals. We extend this notion to se-
quences t1, . . . , tk ? T? by
leaf(k)N (t1, . . . , tk) =
?
1?i?k
{iw | w ? leafN (ti)}.
Let w1, . . . , wn ? pos(t) be (pairwise prefix-
incomparable) positions and t1, . . . , tn ? T?.
Then t[wi ? ti]1?i?n denotes the tree that is ob-
tained from t by replacing (in parallel) the subtrees
at wi by ti for every 1 ? i ? n.
Now we are ready to introduce our model,
which is a minor variation of the local multi
bottom-up tree transducer of Maletti (2011). Let
? and ? be the input and output symbols, respec-
tively, and let N ? ? ?? be the set of nontermi-
nal symbols. Essentially, the model works on pairs
?t, (u1, . . . , uk)? consisting of an input tree t ? T?
812
and a sequence u1, . . . , uk ? T? of output trees.
Such pairs are pre-translations of rank k. The pre-
translation ?t, (u1, . . . , uk)? is shallow if all trees
t, u1, . . . , uk in it are shallow.
Together with a pre-translation we typically
have to store an alignment. Given a pre-translation
?t, (u1, . . . , uk)? of rank k and 1 ? i ? k,
we call ui the ith translation of t. An align-
ment for this pre-translation is an injective map-
ping ? : leaf(k)N (u1, . . . , uk)? leafN (t)?N such
that if (w, j) ? ran(?), then also (w, i) ? ran(?)
for all 1 ? j ? i.2 In other words, if an alignment
requests the ith translation, then it should also re-
quest all previous translations.
Definition 1 A shallow local multi bottom-up tree
transducer (`MBOT) is a finite set R of rules to-
gether with a mapping c : R ? R such that every
rule, written t ?? (u1, . . . , uk), contains a shal-
low pre-translation ?t, (u1, . . . , uk)? and an align-
ment ? for it.
The components t, (u1, . . . , uk), ?, and c(?)
are called the left-hand side, the right-hand
side, the alignment, and the weight of the
rule ? = t ?? (u1, . . . , uk). Figure 2 shows two
example `MBOT rules (without weights). Overall,
the rules of an `MBOT are similar to the rules of
an SCFG (synchronous context-free grammar), but
our right-hand sides contain a sequence of trees
instead of just a single tree. In addition, the align-
ments in an SCFG rule are bijective between leaf
nonterminals, whereas our model permits multi-
ple alignments to a single leaf nonterminal in the
left-hand side (see Figure 2).
Our `MBOT rules are obtained automatically
from data like that in Figure 3. Thus, we (word)
align the bilingual text and parse it in both the
source and the target language. In this manner
we obtain sentence pairs like the one shown in
Figure 3. To these sentence pairs we apply the
rule extraction method of Maletti (2011). The
rules extracted from the sentence pair of Figure 3
are shown in Figure 4. Note that these rules
are not necessarily shallow (the last two rules are
not). Thus, we post-process the extracted rules
and make them shallow. The shallow rules corre-
sponding to the non-shallow rules of Figure 4 are
shown in Figure 2.
Next, we define how to combine rules to form
derivations. In contrast to most other models, we
2ran(f) for a mapping f : A? B denotes the range of f ,
which is {f(a) | a ? A}.
S
NP
JJ
Official
NNS
forecasts
VP
VBD
predicted
NP
QP
RB
just
CD
3
NN
%
S
NP
ADJA
Offizielle
NN
Prognosen
VAFIN
sind
VP
PP
APPR
von
AP
ADV
nur
CARD
3
NN
%
VVPP
ausgegangen
Figure 3: Aligned parsed sentences.
only introduce a derivation semantics that does
not collapse multiple derivations for the same
input-output pair.3 We need one final notion.
Let ? = t ?? (u1, . . . , uk) be a rule and
w ? leafN (t) be a leaf nonterminal (occurrence)
in the left-hand side. The w-rank rk(?, w) of the
rule ? is
rk(?, w) = max {i ? N | (w, i) ? ran(?)} .
For example, for the lower rule ? in Figure 2 we
have rk(?, 1) = 1, rk(?, 2) = 2, and rk(?, 3) = 1.
Definition 2 The set ?(R, c) of weighted pre-
translations of an `MBOT (R, c) is the smallest
set T subject to the following restriction: If there
exist
? a rule ? = t?? (u1, . . . , uk) ? R,
? a weighted pre-translation
?tw, cw, (uw1 , . . . , uwkw)? ? T
for every w ? leafN (t) with
? rk(?, w) = kw,4
? t(w) = tw(?),5 and
? for every iw? ? leaf(k)N (u1, . . . , uk),6
ui(w?) = uvj (?) with ?(iw?) = (v, j),
then ?t?, c?, (u?1, . . . , u?k)? ? T is a weighted pre-
translation, where
? t? = t[w ? tw | w ? leafN (t)],
3A standard semantics is presented, for example,
in (Maletti, 2011).
4If w has n alignments, then the pre-translation selected
for it has to have suitably many output trees.
5The labels have to coincide for the input tree.
6Also the labels for the output trees have to coincide.
813
JJ
Official ?
( ADJA
Offizielle
) NNS
forecasts ?
( NN
Prognosen
) VBD
predicted ?
( VAFIN
sind ,
VVPP
ausgegangen
) RB
just ?
( ADV
nur
) CD
3 ?
( CARD
3
) NN
% ?
( NN
%
)
NP
JJ NNS ?
( NP
ADJA NN
) QP
RB CD ?
( AP
ADV CARD
) NP
QP NN ?
( PPAPPR
von
AP NN )
S
NP VP
VBD NP
? (
S
NP VAFIN VP
PP VVPP
)
Figure 4: Extracted (even non-shallow) rules. We obtain our rules by making those rules shallow.
? c? = c(?) ??w?leafN (t) cw, and
? u?i = ui[iw? ? uvj | ?(iw?) = (v, j)] for
every 1 ? i ? k.
Rules that do not contain any nonterminal
leaves are automatically weighted pre-translations
with their associated rule weight. Otherwise, each
nonterminal leaf w in the left-hand side of a rule ?
must be replaced by the input tree tw of a pre-
translation ?tw, cw, (uw1 , . . . , uwkw)?, whose root islabeled by the same nonterminal. In addition, the
rank rk(?, w) of the replaced nonterminal should
match the number kw of components in the se-
lected weighted pre-translation. Finally, the non-
terminals in the right-hand side that are aligned
to w should be replaced by the translation that the
alignment requests, provided that the nontermi-
nal matches with the root symbol of the requested
translation. The weight of the new pre-translation
is obtained simply by multiplying the rule weight
and the weights of the selected weighted pre-
translations. The overall process is illustrated in
Figures 5 and 6.
3 Translation Model
Given a source language sentence e, our transla-
tion model aims to find the best corresponding tar-
get language translation g?;7 i.e.,
g? = arg maxg p(g|e) .
We estimate the probability p(g|e) through a log-
linear combination of component models with pa-
rameters ?m scored on the pre-translations ?t, (u)?
such that the leaves of t concatenated read e.8
p(g|e) ?
7?
m=1
hm
(
?t, (u)?
)?m
Our model uses the following features
hm(?t, (u1, . . . , uk)?) for a general pre-translation
? = ?t, (u1, . . . , uk)?:
7Our main translation direction is English to German.
8Actually, t must embed in the parse tree of e; see Sec-
tion 4.
(1) The forward translation weight using the rule
weights as described in Section 2
(2) The indirect translation weight using the rule
weights as described in Section 2
(3) Lexical translation weight source? target
(4) Lexical translation weight target? source
(5) Target side language model
(6) Number of words in the target sentences
(7) Number of rules used in the pre-translation
(8) Number of target side sequences; here k times
the number of sequences used in the pre-
translations that constructed ? (gap penalty)
The rule weights required for (1) are relative
frequencies normalized over all rules with the
same left-hand side. In the same fashion the rule
weights required for (2) are relative frequencies
normalized over all rules with the same right-
hand side. Additionally, rules that were extracted
at most 10 times are discounted by multiplying
the rule weight by 10?2. The lexical weights
for (2) and (3) are obtained by multiplying the
word translationsw(gi|ej) [respectively,w(ej |gi)]
of lexically aligned words (gi, ej) accross (possi-
bly discontiguous) target side sequences.9 When-
ever a source word ej is aligned to multiple target
words, we average over the word translations.10
h3(?t, (u1, . . . , uk)?)
=
?
lexical item
e occurs in t
average {w(g|e) | g aligned to e}
The computation of the language model esti-
mates for (6) is adapted to score partial transla-
tions consisting of discontiguous units. We ex-
plain the details in Section 4. Finally, the count c
of target sequences obtained in (7) is actually used
as a score 1001?c. This discourages rules with
many target sequences.
9The lexical alignments are different from the alignments
used with a pre-translation.
10If the word ej has no alignment to a target word, then
it is assumed to be aligned to a special NULL word and this
alignment is scored.
814
Combining a rule with pre-translations:
NP
JJ NNS ?
( NP
ADJA NN
)
JJ
Official ?
( ADJA
Offizielle
) NNS
forecasts ?
( NN
Prognosen
)
Obtained new pre-translation:
NP
JJ
Official
NNS
forecasts
?
( NPADJA
Offizielle
NN
Prognosen
)
Figure 5: Simple rule application.
Combining a rule with pre-translations:
S
NP VBD NP ?
( S
NP VAFIN PP VVPP
)
NP
JJ
Official
NNS
forecasts
? (
NP
ADJA
Offizielle
NN
Prognosen
) VBD
predicted ?
( VAFIN
sind ,
VVPP
ausgegangen
)
NP
QP
RB
just
CD
3
NN
% ?
(
PP
von AP
ADV
nur
CARD
3
NN
%
)
Obtained new pre-translation:
S
NP
JJ
Official
NNS
forecasts
VBD
predicted
NP
QP
RB
just
CD
3
NN
%
?
(
S
NP
ADJA
Offizielle
NN
Prognosen
VAFIN
sind
PP
von AP
ADV
nur
CARD
3
NN
%
VVPP
ausgegangen
)
Figure 6: Complex rule application.
S
NP VAFIN PP VVPP
Offizielle Prognosen ( sind , ausgegangen ) von nur 3 %
Figure 7: Illustration of LM scoring.
815
4 Decoding
We implemented our model in the syntax-based
component of the Moses open-source toolkit
by Koehn et al (2007) and Hoang et al (2009).
The standard Moses syntax-based decoder only
handles SCFG rules; i.e, rules with contiguous
components on the source and the target lan-
guage side. Roughly speaking, SCFG rules are
`MBOT rules with exactly one output tree. We
thus had to extend the system to support our
`MBOT rules, in which arbitrarily many output
trees are allowed.
The standard Moses syntax-based decoder uses
a CYK+ chart parsing algorithm, in which each
source sentence is parsed and contiguous spans are
processed in a bottom-up fashion. A rule is appli-
cable11 if the left-hand side of it matches the non-
terminal assigned to the full span by the parser and
the (non-)terminal assigned to each subspan.12 In
order to speed up the decoding, cube pruning (Chi-
ang, 2007) is applied to each chart cell in order
to select the most likely hypotheses for subspans.
The language model (LM) scoring is directly in-
tegrated into the cube pruning algorithm. Thus,
LM estimates are available for all considered hy-
potheses. To accommodate `MBOT rules, we had
to modify the Moses syntax-based decoder in sev-
eral ways. First, the rule representation itself is ad-
justed to allow sequences of shallow output trees
on the target side. Naturally, we also had to ad-
just hypothesis expansion and, most importantly,
language model scoring inside the cube pruning
algorithm. An overview of the modified pruning
procedure is given in Algorithm 1.
The most important modifications are hidden
in lines 5 and 8. The expansion in Line 5 in-
volves matching all nonterminal leaves in the rule
as defined in Definition 2, which includes match-
ing all leaf nonterminals in all (discontiguous) out-
put trees. Because the output trees can remain
discontiguous after hypothesis creation, LM scor-
ing has to be done individually over all output
trees. Algorithm 2 describes our LM scoring in
detail. In it we use k strings w1, . . . , wk to col-
lect the lexical information from the k output com-
11Note that our notion of applicable rules differs from the
default in Moses.
12Theoretically, this allows that the decoder ignores unary
parser nonterminals, which could also disappear when we
make our rules shallow; e.g., the parse tree left in the pre-
translation of Figure 5 can be matched by a rule with left-
hand side NP(Official, forecasts).
Algorithm 1 Cube pruning with `MBOT rules
Data structures:
- r[i, j]: list of rules matching span e[i . . . j]
- h[i, j]: hypotheses covering span e[i . . . j]
- c[i, j]: cube of hypotheses covering span e[i . . . j]
1: for all `MBOT rules ? covering span e[i . . . j] do
2: Insert ? into r[i, j]
3: Sort r[i, j]
4: for all (l?? r) ? r[i, j] do
5: Create h[i, j] by expanding all nonterminals in l with
best scoring hypotheses for subspans
6: Add h[i, j] to c[i, j]
7: for all hypotheses h ? c[i, j] do
8: Estimate LM score for h // see Algorithm 2
9: Estimate remaining feature scores
10: Sort c[i, j]
11: Retrieve first ? elements from c[i, j] // we use ? = 103
ponents (u1, . . . , uk) of a rule. These strings can
later be rearranged in any order, so we LM-score
all of them separately. Roughly speaking, we ob-
tain wi by traversing ui depth-first left-to-right.
If we meet a lexical element (terminal), then we
add it to the end of wi. On the other hand, if we
meet a nonterminal, then we have to consult the
best pre-translation ? ? = ?t?, (u?1, . . . , u?k?)?, which
will contribute the subtree at this position. Sup-
pose that u?j will be substituted into the nontermi-
nal in question. Then we first LM-score the pre-
translation ? ? to obtain the string w?j correspond-
ing to u?j . This string w?j is then appended to wi.
Once all the strings are built, we score them using
our 4-gram LM. The overall LM score for the pre-
translation is obtained by multiplying the scores
for w1, . . . , wk. Clearly, this treats w1, . . . , wk as
k separate strings, although they eventually will
be combined into a single string. Whenever such
a concatenation happens, our LM scoring will au-
tomatically compute n-gram LM scores based on
the concatenation, which in particular means that
the LM scores get more accurate for larger spans.
Finally, in the final rule only one component is al-
lowed, which yields that the LM indeed scores the
complete output sentence.
Figure 7 illustrates our LM scoring for a pre-
translation involving a rule with two (discontigu-
ous) target sequences (the construction of the pre-
translation is illustrated in Figure 6). When pro-
cessing the rule rooted at S, an LM estimate is
computed by expanding all nonterminal leaves. In
our case, these are NP, VAFIN, PP, and VVPP.
However, the nodes VAFIN and VVPP are assem-
bled from a (discontiguous) tree sequence. This
means that those units have been considered as in-
816
Algorithm 2 LM scoring
Data structures:
- (u1, . . . , uk): right-hand side of a rule
- (w1, . . . , wk): k strings all initially empty
1: score = 1
2: for all 1 ? i ? k do
3: for all leaves ` in ui (in lexicographic order) do
4: if ` is a terminal then
5: Append ` to wi
6: else
7: LM score the best hypothesis for the subspan
8: Expand wi by the corresponding w?j
9: score = score ? LM(wi)
dependent until now. So far, the LM scorer could
only score their associated unigrams. However,
we also have their associated strings w?1 and w?2,
which can now be used. Since VAFIN and VVPP
now become parts of a single tree, we can perform
LM scoring normally. Assembling the string we
obtain
Offizielle Prognosen sind von nur 3 %
ausgegangen
which is scored by the LM. Thus, we first score
the 4-grams ?Offizielle Prognosen sind von?, then
?Prognosen sind von nur?, etc.
5 Experiments
5.1 Setup
The baseline system for our experiments is the
syntax-based component of the Moses open-
source toolkit of Koehn et al (2007) and Hoang
et al (2009). We use linguistic syntactic anno-
tation on both the source and the target language
side (tree-to-tree). Our contrastive system is the
`MBOT-based translation system presented here.
We provide the system with a set of SCFG as well
as `MBOT rules. We do not impose any maximal
span restriction on either system.
The compared systems are evaluated on the
English-to-German13 news translation task of
WMT 2009 (Callison-Burch et al, 2009). For
both systems, the used training data is from the
4th version of the Europarl Corpus (Koehn, 2005)
and the News Commentary corpus. Both trans-
lation models were trained with approximately
1.5 million bilingual sentences after length-ratio
filtering. The word alignments were generated
by GIZA++ (Och and Ney, 2003) with the grow-
diag-final-and heuristic (Koehn et al, 2005). The
13Note that our `MBOT-based system can be applied to any
language pair as it involves no language-specific engineering.
System BLEU
Baseline 12.60
`MBOT ?13.06
Moses t-to-s 12.72
Table 1: Evaluation results. The starred results
are statistically significant improvements over the
Baseline (at confidence p < 0.05).
English side of the bilingual data was parsed us-
ing the Charniak parser of Charniak and John-
son (2005), and the German side was parsed us-
ing BitPar (Schmid, 2004) without the function
and morphological annotations. Our German 4-
gram language model was trained on the Ger-
man sentences in the training data augmented
by the Stuttgart SdeWaC corpus (Web-as-Corpus
Consortium, 2008), whose generation is detailed
in (Baroni et al, 2009). The weights ?m in the
log-linear model were trained using minimum er-
ror rate training (Och, 2003) with the News 2009
development set. Both systems use glue-rules,
which allow them to concatenate partial transla-
tions without performing any reordering.
5.2 Results
We measured the overall translation quality with
the help of 4-gram BLEU (Papineni et al, 2002),
which was computed on tokenized and lower-
cased data for both systems. The results of our
evaluation are reported in Table 1. For com-
parison, we also report the results obtained by
a system that utilizes parses only on the source
side (Moses tree-to-string) with its standard fea-
tures.
We can observe from Table 1 that our `MBOT-
based system outperforms the baseline. We ob-
tain a BLEU score of 13.06, which is a gain of
0.46 BLEU points over the baseline. This im-
provement is statistically significant at confidence
p < 0.05, which we computed using the pairwise
bootstrap resampling technique of Koehn (2004).
Our system is also better than the Moses tree-to-
string system. However this improvement (0.34)
is not statistically significant. In the next section,
we confirm the result of the automatic evaluation
through a manual examination of some transla-
tions generated by our system and the baseline.
In Table 2, we report the number of `MBOT
rules used by our system when decoding the test
set. By lex we denote rules containing only lexical
817
lex non-term total
contiguous 23,175 18,355 41,530
discontiguous 315 2,516 2,831
Table 2: Number of rules used in decoding test
(lex: only lexical items; non-term: at least one
nonterminal).
2-dis 3-dis 4-dis
2,480 323 28
Table 3: Number of k-discontiguous rules.
items. The label non-term stands for rules contain-
ing at least one leaf nonterminal. The results show
that approx. 6% of all rules used by our `MBOT-
system have discontiguous target sides. Further-
more, the reported numbers show that the system
also uses rules in which lexical items are com-
bined with nonterminals. Finally, Table 3 presents
the number of rules with k target side components
used during decoding.
5.3 Linguistic Analysis
In this section we present linguistic evidence sup-
porting the fact that the `MBOT-based system sig-
nificantly outperforms the baseline. All exam-
ples are taken from the translation of the test set
used for automatic evaluation. We show that when
our system generates better translations, this is di-
rectly related to the use of `MBOT rules.
Figures 8 and 9 show the ability of our system to
correctly reorder multiple segments in the source
sentence where the baseline translates those seg-
ments sequentially. An analysis of the generated
derivations shows that our system produces the
correct translation by taking advantage of rules
with discontiguous units on target language side.
The rules used in the presented derivations are dis-
played in Figures 10 and 11. In the first example
(Figure 8), we begin by translating ?((smuggle)VB
(eight projectiles)NP (into the kingdom)PP)VP? into
the discontiguous sequence composed of (i) ?(acht
geschosse)NP? ; (ii) ?(in das ko?nigreich)PP? and
(iii) ?(schmuggeln)VP?. In a second step we as-
semble all sequences in a rule with contiguous tar-
get language side and, at the same time, insert the
word ?(zu)PTKZU? between ?(in das ko?nigreich)PP?
and ?(schmuggeln)VP?.
The second example (Figure 9) illustrates a
more complex reordering. First, we trans-
VP
VB NP PP
?
( NP
NP
, PP
PP
, VVINF
VVINF
)
S
TO VP
?
( VP
NP PP PTKZU VVINF
)
Figure 10: Used `MBOT rules for verbal reorder-
ing
VP
ADV commented on NP
?
( NP
NP
, ADV
ADV
, VPP
kommentiert
)
VP
VBZ VP
?
( NP
NP
, VAFIN
VAFIN
, ADV
ADV
, VPP
VPP
)
TOP
NP VP
?
( TOP
NP VAFIN NP ADV VVPP
)
Figure 11: Used `MBOT rules for verbal reorder-
ing
late ?((again)ADV commented on (the problem
of global warming)NP)VP? into the discontigu-
ous sequence composed of (i) ?(das problem
der globalen erwa?rmung)NP?; (ii) ?(wieder)ADV?
and (iii) ?(kommentiert)VPP?. In a second step,
we translate the auxiliary ?(has)VBZ? by in-
serting ?(hat)VAFIN? into the sequence. We
thus obtain, for the input segment ?((has)VBZ
(again)ADV commented on (the problem of global
warming)NP)VP?, the sequence (i) ?(das problem
der globalen erwa?rmung)NP?; (ii) ?(hat)VAFIN?;
(iii) ?(wieder)ADV?; (iv) ?(kommentiert)VVPP?. In
a last step, the constituent ?(president va?clav
klaus)NP? is inserted between the discontiguous
units ?(hat)VAFIN? and ?(wieder)ADV? to form the
contiguous sequence ?((das problem der glob-
alen erwa?rmung)NP (hat)VAFIN (pra?sident va?clav
klaus)NP (wieder)ADV (kommentiert)VVPP)TOP?.
Figures 12 and 13 show examples where our
system generates complex words in the target
language out of a simple source language word.
Again, an analysis of the generated derivation
shows that `MBOT takes advantage of rules hav-
ing several target side components. Examples of
such rules are given in Figure 14. Through its
ability to use these discontiguous rules, our sys-
tem correctly translates into reflexive or particle
verbs such as ?konzentriert sich? (for the English
?focuses?) or ?besteht darauf ? (for the English
?insist?). Another phenomenon well handled by
our system are relative pronouns. Pronouns such
as ?that? or ?whose? are systematically translated
818
. . . geplant hatten 8 geschosse in das ko?nigreich zu schmuggeln
. . . had planned to smuggle 8 projectiles into the kingdom
. . . vorhatten zu schmuggeln 8 geschosse in das ko?nigreich
Figure 8: Verbal Reordering (top: our system, bottom: baseline)
das problem der globalen erwa?rmung hat pra?sident va?clav klaus wieder kommentiert
president va?clav klaus has again commented on the problem of global warming
pra?sident va?clav klaus hat wieder kommentiert das problem der globalen erwa?rmung
Figure 9: Verbal Reordering (top: our system, bottom: baseline)
. . . die serbische delegation bestand darauf , dass jede entscheidung . . .
. . . the serbian delegation insisted that every decision . . .
. . . die serbische delegation bestand , jede entscheidung . . .
Figure 12: Relative Clause (top: our system, bot-
tom: baseline)
. . . die roadmap von bali , konzentriert sich auf die bemu?hungen . . .
. . . the bali roadmap that focuses on efforts . . .
. . . die bali roadmap , konzentriert auf bemu?hungen . . .
Figure 13: Reflexive Pronoun (top: our system,
bottom: baseline)
into both both, ?,? and ?dass? or ?,? and ?deren?
(Figure 12).
6 Conclusion and Future Work
We demonstrated that our `MBOT-based machine
translation system beats a standard tree-to-tree
system (Moses tree-to-tree) on the WMT 2009
translation task English ? German. To achieve
this we implemented the formal model as de-
scribed in Section 2 inside the Moses machine
translation toolkit. Several modifications were
necessary to obtain a working system. We publicly
release all our developed software and our com-
plete tool-chain to allow independent experiments
and evaluation. This includes our `MBOT decoder
IN
that
?
( $,
,
, KOUS
dass
) VBZ
focuses
?
( VVFIN
konzentriert
, PRF
sich
)
Figure 14: `MBOT rules generating a relative
clause/reflexive pronoun
presented in Section 4 and a separate C++ module
that we use for rule extraction (see Section 3).
Besides the automatic evaluation, we also per-
formed a small manual analysis of obtained trans-
lations and show-cased some examples (see Sec-
tion 5.3). We argue that our `MBOT approach can
adequately handle discontiguous phrases, which
occur frequently in German. Other languages that
exhibit such phenomena include Czech, Dutch,
Russian, and Polish. Thus, we hope that our sys-
tem can also successfully be applied for other lan-
guage pairs, which we plan to pursue as well.
In other future work, we want to investigate
full backwards application of `MBOT rules, which
would be more suitable for the converse transla-
tion direction German? English. The current in-
dependent LM scoring of components has some
negative side-effects that we plan to circumvent
with the use of lazy LM scoring.
Acknowledgement
The authors thank Alexander Fraser for his ongo-
ing support and advice. All authors were finan-
cially supported by the German Research Founda-
tion (DFG) grant MA 4959 / 1-1.
819
References
Andre? Arnold and Max Dauchet. 1982. Morphismes
et bimorphismes d?arbres. Theoret. Comput. Sci.,
20(1):33?93.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky Wide
Web: A collection of very large linguistically pro-
cessed web-crawled corpora. Language Resources
and Evaluation, 43(3):209?226.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proc. 4th Workshop on Statistical Machine Trans-
lation, pages 1?28.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proc. 43rd ACL, pages 173?180.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computat. Linguist., 33(2):201?228.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proc. 48th ACL, pages 1443?
1452.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proc. 41st
ACL, pages 205?208.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. HLT-NAACL, pages 273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve Deneefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
44th ACL, pages 961?968.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A unified framework for phrase-based, hierarchical,
and syntax-based statistical machine translation. In
Proc. 6th Int. Workshop Spoken Language Transla-
tion, pages 152?159.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proc. 7th Conf. Association
for Machine Translation of the Americas, pages 66?
73.
Philip Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL, pages 127?133.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT Speech Translation Evaluation.
In Proc. 2nd Int. Workshop Spoken Language Trans-
lation.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. ACL, pages 177?180.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP,
pages 388?395.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proc. 10th Ma-
chine Translation Summit, pages 79?86.
Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed parallel
corpora. In Proc. 2nd ACL Workshop on Syntax and
Structure in Statistical Translation, pages 87?95.
Eric Lilin. 1978. Une ge?ne?ralisation des transducteurs
d?e?tats finis d?arbres: les S-transducteurs. The`se
3e`me cycle, Universite? de Lille.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. 44th ACL, pages 609?616.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Proc.
47th ACL, pages 558?566.
Andreas Maletti. 2010. Why synchronous tree sub-
stitution grammars? In Proc. HLT-NAACL, pages
876?884.
Andreas Maletti. 2011. How to train your multi
bottom-up tree transducer. In Proc. 49th ACL, pages
825?834.
Andreas Maletti. 2012. Every sensible extended top-
down tree transducer is a multi bottom-up tree trans-
ducer. In Proc. HLT-NAACL, pages 263?273.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computat. Linguist., 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proc. 41st ACL,
pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. 40th
ACL, pages 311?318.
Jean-Claude Raoult. 1997. Rational tree relations.
Bull. Belg. Math. Soc. Simon Stevin, 4(1):149?176.
Helmut Schmid. 2004. Efficient parsing of highly am-
biguous context-free grammars with bit vectors. In
Proc. 20th COLING, pages 162?168.
820
Jun Sun, Min Zhang, and Chew Lim Tan. 2009. A non-
contiguous tree sequence alignment-based model for
statistical machine translation. In Proc. 47th ACL,
pages 914?922.
Web-as-Corpus Consortium. 2008. SDeWaC ? a 0.88
billion word corpus for german. Website: http:
//wacky.sslmit.unibo.it/doku.php.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computat. Linguist., 23(3):377?403.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008a. A tree
sequence alignment-based tree-to-tree translation
model. In Proc. 46th ACL, pages 559?567.
Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw, and
Sheng Li. 2008b. Grammar comparison study
for translational equivalence modeling and statis-
tical machine translation. In Proc. 22nd Inter-
national Conference on Computational Linguistics,
pages 1097?1104.
821
Proceedings of the 2010 Workshop on Applications of Tree Automata in Natural Language Processing, ACL 2010, pages 1?9,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Preservation of Recognizability for
Synchronous Tree Substitution Grammars
Zolta?n Fu?lo?p
Department of Computer Science
University of Szeged
Szeged, Hungary
Andreas Maletti
Departament de Filologies Roma`niques
Universitat Rovira i Virgili
Tarragona, Spain
Heiko Vogler
Faculty of Computer Science
Technische Universita?t Dresden
Dresden, Germany
Abstract
We consider synchronous tree substitution
grammars (STSG). With the help of a
characterization of the expressive power
of STSG in terms of weighted tree bimor-
phisms, we show that both the forward and
the backward application of an STSG pre-
serve recognizability of weighted tree lan-
guages in all reasonable cases. As a con-
sequence, both the domain and the range
of an STSG without chain rules are recog-
nizable weighted tree languages.
1 Introduction
The syntax-based approach to statistical machine
translation (Yamada and Knight, 2001) becomes
more and more competitive in machine transla-
tion, which is a subfield of natural language pro-
cessing (NLP). In this approach the full parse trees
of the involved sentences are available to the trans-
lation model, which can base its decisions on this
rich structure. In the competing phrase-based ap-
proach (Koehn et al, 2003) the translation model
only has access to the linear sentence structure.
There are two major classes of syntax-based
translation models: tree transducers and synchro-
nous grammars. Examples in the former class
are the top-down tree transducer (Rounds, 1970;
Thatcher, 1970), the extended top-down tree trans-
ducer (Arnold and Dauchet, 1982; Galley et al,
2004; Knight and Graehl, 2005; Graehl et al,
2008; Maletti et al, 2009), and the extended
multi bottom-up tree transducer (Lilin, 1981; En-
gelfriet et al, 2009; Maletti, 2010). The lat-
ter class contains the syntax-directed transduc-
tions of Lewis II and Stearns (1968), the gen-
eralized syntax-directed transductions (Aho and
Ullman, 1969), the synchronous tree substitu-
tion grammar (STSG) by Schabes (1990) and the
synchronous tree adjoining grammar (STAG) by
Abeille? et al (1990) and Shieber and Schabes
(1990). The first bridge between those two classes
were established in (Martin and Vere, 1970). Fur-
ther comparisons can be found in (Shieber, 2004)
for STSG and in (Shieber, 2006) for STAG.
One of the main challenges in NLP is the am-
biguity that is inherent in natural languages. For
instance, the sentence ?I saw the man with the
telescope? has several different meanings. Some
of them can be distinguished by the parse tree,
so that probabilistic parsers (Nederhof and Satta,
2006) for natural languages can (partially) achieve
the disambiguation. Such a parser returns a set
of parse trees for each input sentence, and in
addition, each returned parse tree is assigned a
likelihood. Thus, the result can be seen as a
mapping from parse trees to probabilities where
the impossible parses are assigned the probabil-
ity 0. Such mappings are called weighted tree lan-
guages, of which some can be finitely represented
by weighted regular tree grammars (Alexandrakis
and Bozapalidis, 1987). Those weighted tree
languages are recognizable and there exist algo-
rithms (Huang and Chiang, 2005) that efficiently
extract the k-best parse trees (i.e., those with the
highest probability) for further processing.
In this paper we consider synchronized tree sub-
stitution grammars (STSG). To overcome a techni-
cal difficulty we add (grammar) nonterminals to
them. Since an STSG often uses the nontermi-
nals of a context-free grammar as terminal sym-
bols (i.e., its derived trees contain both termi-
nal and nonterminal symbols of the context-free
grammar), we call the newly added (grammar)
nonterminals of the STSG states. Substitution does
no longer take place at synchronized nonterminals
(of the context-free grammar) but at synchronized
states (one for the input and one for the output
side). The states themselves will not appear in the
final derived trees, which yields that it is sufficient
to assume that only identical states are synchro-
1
nized. Under those conventions a rule of an STSG
has the form q ? (s, t, V, a) where q is a state,
a ? R?0 is the rule weight, s is an input tree that
can contain states at the leaves, and t is an output
tree that can also contain states. Finally, the syn-
chronization is defined by V , which is a bijection
between the state-labeled leaves of s and t. We
require that V only relates identical states.
The rules of an STSG are applied in a step-wise
manner. Here we use a derivation relation to define
the semantics of an STSG. It can be understood as
the synchronization of the derivation relations of
two regular tree grammars (Ge?cseg and Steinby,
1984; Ge?cseg and Steinby, 1997) where the syn-
chronization is done on nonterminals (or states) in
the spirit of syntax-directed transductions (Lewis
II and Stearns, 1968). Thus each sentential form
is a pair of (nonterminal-) connected trees.
An STSG G computes a mapping ?G , called
its weighted tree transformation, that assigns a
weight to each pair of input and output trees,
where both the input and output tree may not con-
tain any state. This transformation is obtained as
follows: We start with two copies of the initial
state that are synchronized. Given a connected tree
pair (?, ?), we can apply the rule q ? (s, t, V, a)
to each pair of synchronized states q. Such an ap-
plication replaces the selected state q in ? by s and
the corresponding state q in ? by t. All the re-
maining synchronized states and the synchronized
states of V remain synchronized. The result is
a new connected tree pair. This step charges the
weight a. The weights of successive applications
(or steps) are multiplied to obtain the weight of the
derivation. The weighted tree transformation ?G
assigns to each pair of trees the sum of all weights
of derivations that derive that pair.
Shieber (2004) showed that for every classical
unweighted STSG there exists an equivalent bi-
morphism (Arnold and Dauchet, 1982). The con-
verse result only holds up to deterministic rela-
belings (Ge?cseg and Steinby, 1984; Ge?cseg and
Steinby, 1997), which remove the state informa-
tion from the input and output tree. It is this dif-
ference that motivates us to add states to STSG.
We generalize the result of Shieber (2004) and
prove that every weighted tree transformation that
is computable by an STSG can also be computed
by a weighted bimorphism and vice versa.
Given an STSG and a recognizable weighted
tree language ? of input trees, we investigate un-
der which conditions the weighted tree language
obtained by applying G to ? is again recognizable.
In other words, we investigate under which condi-
tions the forward application of G preserves rec-
ognizability. The same question is investigated for
backward application, which is the corresponding
operation given a recognizable weighted tree lan-
guage of output trees. Since STSG are symmet-
ric (i.e., input and output can be exchanged), the
results for backward application can be obtained
easily from the results for forward application.
Our main result is that forward application pre-
serves recognizability if the STSG G is output-
productive, which means that each rule of G con-
tains at least one output symbol that is not a state.
Dually, backward application preserves recogniz-
ability if G is input-productive, which is the anal-
ogous property for the input side. In fact, those re-
sults hold for weights taken from an arbitrary com-
mutative semiring (Hebisch and Weinert, 1998;
Golan, 1999), but we present the results only for
probabilities.
2 Preliminary definitions
In this contribution we will work with ranked
trees. Each symbol that occurs in such a tree
has a fixed rank that determines the number of
children of nodes with this label. Formally, let
? be a ranked alphabet, which is a finite set ?
together with a mapping rk? : ? ? N that asso-
ciates a rank rk?(?) with every ? ? ?. We let
?k = {? ? ? | rk?(?) = k} be the set contain-
ing all symbols in ? that have rank k. A ?-tree
indexed by a set Q is a tree with nodes labeled by
elements of ? ? Q, where the nodes labeled by
some ? ? ? have exactly rk?(?) children and the
nodes with labels ofQ have no children. Formally,
the set T?(Q) of (term representations of) ?-trees
indexed by a set Q is the smallest set T such that
? Q ? T and
? ?(t1, . . . , tk) ? T for every ? ? ?k and
t1, . . . , tk ? T .
We generally write ? instead of ?() for all ? ? ?0.
We frequently work with the set pos(t) of po-
sitions of a ?-tree t, which is defined as fol-
lows. If t ? Q, then pos(t) = {?}, and if
t = ?(t1, . . . , tk), then
pos(t) = {?} ? {iw | 1 ? i ? k,w ? pos(ti)} .
Thus, each position is a finite (possibly empty) se-
quence of natural numbers. Clearly, each position
2
designates a node of the tree, and vice versa. Thus
we identify nodes with positions. As usual, a leaf
is a node that has no children. The set of all leaves
of t is denoted by lv(t). Clearly, lv(t) ? pos(t).
The label of a position w ? pos(t) is denoted
by t(w). Moreover, for every A ? ? ? Q, let
posA(t) = {w ? pos(t) | t(w) ? A} and
lvA(t) = posA(t) ? lv(t) be the sets of po-
sitions and leaves that are labeled with an ele-
ment of A, respectively. Let t ? T?(Q) and
w1, . . . , wk ? lvQ(t) be k (pairwise) different
leaves. We write t[w1 ? t1, . . . , wk ? tk] or just
t[wi ? ti | 1 ? i ? k] with t1, . . . , tk ? T?(Q)
for the tree obtained from t by replacing, for every
1 ? i ? k, the leaf wi with the tree ti.
For the rest of this paper, let ? and ? be two
arbitrary ranked alphabets. To avoid consistency
issues, we assume that a symbol ? that occurs in
both ? and ? has the same rank in ? and ?; i.e.,
rk?(?) = rk?(?). A deterministic relabeling is
a mapping r : ? ? ? such that r(?) ? ?k for
every ? ? ?k. For a tree s ? T?, the relabeled
tree r(s) ? T? is such that pos(r(s)) = pos(s)
and
(
r(s)
)
(w) = r(s(w)) for every w ? pos(s).
The class of tree transformations computed by de-
terministic relabelings is denoted by dREL.
A tree language (over ?) is a subset of T?. Cor-
respondingly, a weighted tree language (over ?)
is a mapping ? : T? ? R?0. A weighted tree
transformation (over ? and ?) is a mapping
? : T? ? T? ? R?0. Its inverse is the weighted
tree transformation ??1 : T??T? ? R?0, which
is defined by ??1(t, s) = ?(s, t) for every t ? T?
and s ? T?.
3 Synchronous tree substitution
grammars with states
Let Q be a finite set of states with a distinguished
initial state qS ? Q. A connected tree pair is a
tuple (s, t, V, a) where s ? T?(Q), t ? T?(Q),
and a ? R?0. Moreover, V : lvQ(s) ? lvQ(t) is
a bijective mapping such that s(u) = t(v) for ev-
ery (u, v) ? V . We will often identify V with its
graph. Intuitively, a connected tree pair (s, t, V, a)
is a pair of trees (s, t) with a weight a such that
each node labeled by a state in s has a correspond-
ing node in t, and vice versa. Such a connected
tree pair (s, t, V, a) is input-productive and output-
productive if s /? Q and t /? Q, respectively. Let
Conn denote the set of all connected tree pairs that
use the index setQ. Moreover, let Connp ? Conn
contain all connected tree pairs that are input- or
output-productive.
A synchronous tree substitution grammar G
(with states) over ?, ?, and Q (for short: STSG),
is a finite set of rules of the form q ? (s, t, V, a)
where q ? Q and (s, t, V, a) ? Connp. We call
a rule q ? (s, t, V, a) a q-rule, of which q and
(s, t, V, a) are the left-hand and right-hand side,
respectively, and a is its weight. The STSG G is
input-productive (respectively, output-productive)
if each of its rules is so. To simplify the following
development, we assume (without loss of general-
ity) that two different q-rules differ on more than
just their weight.1
To make sure that we do not account essentially
the same derivation twice, we have to use a deter-
ministic derivation mode. Since the choice is im-
material, we use the leftmost derivation mode for
the output component t of a connected tree pair
(s, t, V, a). For every (s, t, V, a) ? Conn such
that V 6= ?, the leftmost output position is the
pair (w,w?) ? V , where w? is the leftmost (i.e.,
the lexicographically smallest) position of lvQ(t).
Next we define derivations. The derivation re-
lation induced by G is the binary relation ?G
over Conn such that
? = (s1, t1, V1, a1)?G (s2, t2, V2, a2) = ?
if and only if the leftmost output position of ? is
(w,w?) ? V1 and there exists a rule
s1(w)? (s, t, V, a) ? G
such that
? s2 = s1[w ? s] and t2 = t1[w? ? t],
? V2 = (V1 \ {(w,w?)}) ? V ? where
V ? = {(ww1, w?w2) | (w1, w2) ? V }, and
? a2 = a1 ? a.
A sequence D = (?1, . . . , ?n) ? Connn is a
derivation of (s, t, V, a) ? Conn from q ? Q if
? ?1 = (q, q, {(?, ?)}, 1),
? ?n = (s, t, V, a), and
? ?i ?G ?i+1 for every 1 ? i ? n? 1.
The set of all such derivations is denoted by
DqG(s, t, V, a).
For every q ? Q, s ? T?(Q), t ? T?(Q), and
bijection V : lvQ(s)? lvQ(t), let
? qG(s, t, V ) =
?
a?R?0,D?D
q
G(s,t,V,a)
a .
1Formally, q ? (s, t, V, a) ? G and q ? (s, t, V, b) ? G
implies a = b.
3
o 1? o ?G,lo
?
e o
6?1
?
?
o e
?G,lo
?
e ?
18?1
?
?
? e
?G,lo
?
?
o o
? 36?1?
?
? ?
o o
?G,lo
?
?
o ?
? 108?1?
?
? ?
? o
?G,lo
?
?
? ?
? 324?1?
?
? ?
? ?
Figure 1: Example derivation with the STSG G of Example 1.
Finally, the weighted tree transformation com-
puted by G is the weighted tree transformation
?G : T? ? T? ? R?0 with ?G(s, t) = ?
qS
G (s, t, ?)
for every s ? T? and t ? T?. As usual, we
call two STSG equivalent if they compute the same
weighted tree transformation. We observe that
every STSG is essentially a linear, nondeleting
weighted extended top-down (or bottom-up) tree
transducer (Arnold and Dauchet, 1982; Graehl et
al., 2008; Engelfriet et al, 2009) without (both-
sided) epsilon rules, and vice versa.
Example 1. Let us consider the STSG G over
? = ? = {?, ?} and Q = {e, o} where qS = o,
rk(?) = 2, and rk(?) = 0. The STSG G consists
of the following rules where V = {(1, 2), (2, 1)}
and id = {(1, 1), (2, 2)}:
o? (?(o, e), ?(e, o), V, 1/3) (?1)
o? (?(e, o), ?(o, e), V, 1/6) (?2)
o? (?(e, o), ?(e, o), id, 1/6) (?3)
o? (?, ?, ?, 1/3) (?4)
e? (?(e, e), ?(e, e), V, 1/2) (?5)
e? (?(o, o), ?(o, o), V, 1/2) (?6)
Figure 1 shows a derivation induced by G. It can
easily be checked that ?G(s, t) = 16?3?2?3?3 where
s = ?(?(?, ?), ?) and t = ?(?, ?(?, ?)). More-
over, ?G(s, s) = ?G(s, t). If ?
q
G(s, t, ?) 6= 0 with
q ? {e, o}, then s and t have the same number
of ?-labeled leaves. This number is odd if q = o,
otherwise it is even. Moreover, at every position
w ? pos(s), the left and right subtrees s1 and s2
are interchanged in s and t (due to V in the rules
?1, ?2, ?5, ?6) except if s1 and s2 contain an even
and odd number, respectively, of ?-labeled leaves.
In the latter case, the subtrees can be interchanged
or left unchanged (both with probability 1/6).
4 Recognizable weighted tree languages
Next, we recall weighted regular tree grammars
(Alexandrakis and Bozapalidis, 1987). To keep
the presentation simple, we identify WRTG with
particular STSG, in which the input and the out-
put components are identical. More precisely, a
weighted regular tree grammar over ? and Q (for
short: WRTG) is an STSG G over ?, ?, and Q
where each rule has the form q ? (s, s, id, a)
where id is the suitable (partial) identity mapping.
It follows that s /? Q, which yields that we do not
have chain rules. In the rest of this paper, we will
specify a rule q ? (s, s, id, a) of a WRTG sim-
ply by q
a
? s. For every q ? Q, we define the
weighted tree language ?qG : T?(Q) ? R?0 gen-
erated by G from q by ?qG(s) = ?
q
G(s, s, idlvQ(s))
for every s ? T?(Q), where idlvQ(s) is the iden-
tity on lvQ(s). Moreover, the weighted tree lan-
guage ?G : T? ? R?0 generated by G is defined
by ?G(s) = ?
qS
G (s) for every s ? T?.
A weighted tree language ? : T? ? R?0 is
recognizable if there exists a WRTG G such that
? = ?G . We note that our notion of recognizabil-
ity coincides with the classical one (Alexandrakis
and Bozapalidis, 1987; Fu?lo?p and Vogler, 2009).
Example 2. We consider the WRTGK over the in-
put alphabet ? = {?, ?} and P = {p, q} with
qS = q, rk(?) = 2, and rk(?) = 0. The WRTG K
contains the following rules:
q
0.4
? ?(p, ?) q
0.6
? ? p
1
? ?(?, q) (?1??3)
Let s ? T? be such that ?K(s) 6= 0. Then s is a
thin tree with zig-zag shape; i.e., there exists n ? 1
such that pos(s) contains exactly the positions:
? (12)i for every 0 ? i ? bn?12 c, and
? (12)i1, (12)i2, and (12)i11 for every integer
0 ? i ? bn?32 c.
The integer n can be understood as the length of
a derivation that derives s from q. Some example
4
??
?
? ?
?
?
?
? ?
?
? ?
?
?
weight: 0.6 weight: 0.24 weight: 0.096
Figure 2: Example trees and their weight in ?G
where G is the WRTG of Example 2.
trees with their weights are displayed in Figure 2.
Proposition 3. For every WRTG G there is an
equivalent WRTG G? in normal form, in which the
right-hand side of every rule contains exactly one
symbol of ?.
Proof. We can obtain the statement by a trivial ex-
tension to the weighted case of the approach used
in Lemma II.3.4 of (Ge?cseg and Steinby, 1984)
and Section 6 of (Ge?cseg and Steinby, 1997).
5 STSG and weighted bimorphisms
In this section, we characterize the expressive
power of STSG in terms of weighted bimorphisms.
This will provide a conceptually clear pattern for
the construction in our main result (see Theo-
rem 6) concerning the closure of recognizable
weighted tree languages under forward and back-
ward application. For this we first recall tree ho-
momorphisms. Let ? and ? be two ranked al-
phabets. Moreover, let h : ? ? T? ? (N?)?
be a mapping such that h(?) = (s, u1, . . . , uk)
for every ? ? ?k where s ? T? and all leaves
u1, . . . , uk ? lv(s) are pairwise different. The
mapping h induces the (linear and complete) tree
homomorphism h? : T? ? T?, which is defined by
h?(?(d1, . . . , dk)) = s[u1 ? d?1, . . . , uk ? d?k]
for every ? ? ?k and d1, . . . , dk ? T? with
h(?) = (s, u1, . . . , uk) and d?i = h?(di) for ev-
ery 1 ? i ? k. Moreover, every (linear and
complete) tree homomorphism is induced in this
way. In the rest of this paper we will not distin-
guish between h and h? and simply write h instead
of h?. The homomorphism h is order-preserving
if u1 < ? ? ? < uk for every ? ? ?k where
h(?) = (s, u1, . . . , uk). Finally, we note that
every ? ? dREL can be computed by a order-
preserving tree homomorphism.
A weighted bimorphism B over ? and ? con-
sists of a WRTG K over ? and P and two tree ho-
T? R?0
T? ? T?
(hin, hout)
?K
?B
Figure 3: Illustration of the semantics of the bi-
morphism B.
momorphisms
hin : T? ? T? and hout : T? ? T? .
The bimorphism B computes the weighted tree
transformation ?B : T? ? T? ? R?0 with
?B(s, t) =
?
d?h?1in (s)?h
?1
out(t)
?K(d)
for every s ? T? and t ? T?.
Without loss of generality, we assume that ev-
ery bimorphism B is presented by an WRTG K in
normal form and an order-preserving output ho-
momorphism hout. Next, we prepare the relation
between STSG and weighted bimorphisms. Let
G be an STSG over ?, ?, and Q. Moreover, let
B be a weighted bimorphism over ? and ? con-
sisting of (i) K over ? and P in normal form,
(ii) hin, and (iii) order-preserving hout. We say
that G and B are related if Q = P and there
is a bijection ? : G ? K such that, for every
rule ? ? G with ? = (q ? (s, t, V, a)) and
?(?) = (p
a
? ?(p1, . . . , pk)) we have
? p = q,
? hin(?) = (s, u1, . . . , uk),
? hout(?) = (t, v1, . . . , vk),
? V = {(u1, v1), . . . , (uk, vk)}, and
? s(ui) = pi = t(vi) for every 1 ? i ? k.
Let G and B be related. The following three easy
statements can be used to prove that G and B are
equivalent:
1. For every derivation D ? DqG(s, t, ?, a) with
q ? Q, s ? T?, t ? T?, a ? R?0, there exists
d ? T? and a derivation D? ? D
q
K(d, d, ?, a)
such that hin(d) = s and hout(d) = t.
2. For every d ? T? and D? ? D
q
K(d, d, ?, a)
with q ? Q and a ? R?0, there exists a
derivation D ? DqG(hin(d), hout(d), ?, a).
3. The mentioned correspondence on deriva-
tions is a bijection.
Given an STSG G, we can easily construct a
weighted bimorphism B such that G and B are re-
lated, and vice versa. Hence, STSG and weighted
5
bimorphisms are equally expressive, which gener-
alizes the corresponding characterization result in
the unweighted case by Shieber (2004), which we
will state after the introduction of STSG?.
Classical synchronous tree substitution gram-
mars (STSG?) do not have states. An STSG? can
be seen as an STSG by considering every substitu-
tion site (i.e., each pair of synchronised nontermi-
nals) as a state.2 We illustrate this by means of an
example here. Let us consider the STSG? G with
the following rules:
? (S(?,B?), S(D?, ?)) with weight 0.2
? (B(?,B?), D(?,D?)) with weight 0.3
? (B(?), D(?)) with weight 0.4.
The substitution sites are marked with ?. Any
rule with root A can be applied to a substitution
site A?. An equivalent STSG G? has the rules:
?S, S? ? (S(?, ?B,D?), S(?B,D?, ?), V, 0.2)
?B,D? ? (B(?, ?B,D?), D(?, ?B,D?), V ?, 0.3)
?B,D? ? (B(?), D(?), ?, 0.4) ,
where V = {(2, 1)} and V ? = {(2, 2)}. It is easy
to see that G and G? are equivalent.
Let ? = {?, ??, ???, ?, ?} where ?, ??, ??? ? ?1
and ?, ? ? ?0 (and ?? 6= ??? and ? 6= ?). We write
?m(t) with t ? T? for the tree ?(? ? ? ?(t) ? ? ? ) con-
taining m occurrences of ? above t. STSG? have a
certain locality property, which yields that STSG?
cannot compute transformations like
?(s, t) =
?
??
??
1 if s = ??(?m(?)) = t
or s = ???(?m(?)) = t
0 otherwise
for every s, t ? T?. The non-local feature is the
correspondence between the symbols ?? and ? (in
the first alternative) and the symbols ??? and ? (in
the second alternative). An STSG that computes ?
is presented in Figure 4.
Theorem 4. Let ? be a weighted tree transforma-
tion. Then the following are equivalent.
1. ? is computable by an STSG.
2. ? is computable by a weighted bimorphism.
3. There exists a STSG? G and deterministic re-
labelings r1 and r2 such that
?(s, t) =
?
s??r?11 (s),t
??r?12 (t)
?G(s
?, t?) .
2To avoid a severe expressivity restriction, several initial
states are allowed for an STSG?.
The inverse of an STSG computable weighted
tree transformation can be computed by an STSG.
Formally, the inverse of the STSG G is the STSG
G?1 = {(t, s, V ?1, a) | (s, t, V, a) ? G}
where V ?1 is the inverse of V . Then ?G?1 = ?
?1
G .
6 Forward and backward application
Let us start this section with the definition of the
concepts of forward and backward application of a
weighted tree transformation ? : T? ? T? ? R?0
to weighted tree languages ? : T? ? R?0 and
? : T? ? R?0. We will give general definitions
first and deal with the potentially infinite sums
later. The forward application of ? to ? is the
weighted tree language ?(?) : T? ? R?0, which
is defined for every t ? T? by
(
?(?)
)
(t) =
?
s?T?
?(s) ? ?(s, t) . (1)
Dually, the backward application of ? to ? is
the weighted tree language ??1(?) : T? ? R?0,
which is defined for every s ? T? by
(
??1(?)
)
(s) =
?
t?T?
?(s, t) ? ?(t) . (2)
In general, the sums in Equations (1) and (2) can
be infinite. Let us recall the important property
that makes them finite in our theorems.
Proposition 5. For every input-productive (resp.,
output-productive) STSG G and every tree s ? T?
(resp., t ? T?), there exist only finitely many
trees t ? T? (respectively, s ? T?) such that
?G(s, t) 6= 0.
Proof sketch. If G is input-productive, then each
derivation step creates at least one input symbol.
Consequently, any derivation for the input tree s
can contain at most as many steps as there are
nodes (or positions) in s. Clearly, there are only
finitely many such derivations, which proves the
statement. Dually, we can obtain the statement for
output-productive STSG.
In the following, we will consider forward ap-
plications ?G(?) where G is an output-productive
STSG and ? is recognizable, which yields that (1)
is well-defined by Proposition 5. Similarly, we
consider backward applications ??1G (?) where G
is input-productive and ? is recognizable, which
again yields that (2) is well-defined by Proposi-
tion 5. The question is whether ?G(?) and ?
?1
G (?)
6
q0 ?
??
q1
1
?
??
q1
q0 ?
???
q2
1
?
???
q2
q1 ?
?
q1
1
?
?
q1
q2 ?
?
q2
1
?
?
q2
q1 ? ?
1
? ?
q2 ? ?
1
? ?
Figure 4: STSG computing the weighted tree transformation ? with initial state q0.
are again recognizable. To avoid confusion, we
occasionally use angled parentheses as in ?p, q?
instead of standard parentheses as in (p, q). More-
over, for ease of presentation, we identify the ini-
tial state qS with ?qS, qS?.
Theorem 6. Let G be an STSG over ?, ?, and Q.
Moreover, let ? : T? ? R?0 and ? : T? ? R?0
be recognizable weighted tree languages.
1. If G is output-productive, then ?G(?) is rec-
ognizable.
2. If G is input-productive, then ??1G (?) is rec-
ognizable.
Proof. For the first item, let K be a WRTG over
? and P such that ? = ?K. Without loss of gen-
erality, we suppose that K is in normal form.
Intuitively, we take each rule q ? (s, t, V, a)
of G and run the WRTG K with every start state p
on the input side s of the rule. In this way, we
obtain a weight b. The WRTG will reach the state
leaves of s in certain states, which we then trans-
fer to the linked states in t to obtain t?. Finally, we
remove the input side and obtain a rule ?p, q?
ab
? t?
for the WRTG L that represents the forward ap-
plication. We note that the same rule of L might
be constructed several times. If this happens, then
we replace the several copies by one rule whose
weight is the sum of the weights of all copies.
As already mentioned the initial state is ?qS, qS?.
Clearly, this approach is inspired (and made rea-
sonable) by the bimorphism characterization. We
can take the HADAMARD product of the WRTG of
the bimorphism with the inverse image of ?K un-
der its input homomorphism. Then we can simply
project to the output side. Our construction per-
forms those three steps at once. The whole process
is illustrated in Figure 5.
Formally, we construct the WRTG L over ? and
P?Qwith the following rules. Let p ? P , q ? Q,
and t? ? T?(P ? Q). Then ?p, q?
c
? t? is a rule
in L?, where
c =
?
(q?(s,t,V,a))?G
V={(u1,v1),...,(uk,vk)}
p1,...,pk?P
t?=t[vi??pi,t(vi)?|1?i?k]
b=?pK(s[ui?pi|1?i?k])
ab .
This might create infinitely many rules in L?, but
clearly only finitely many will have a weight dif-
ferent from 0. Thus, we can obtain the finite rule
set L by removing all rules with weight 0.
The main statement to prove is the following:
for every t ? T?(Q) with lvQ(t) = {v1, . . . , vk},
p, p1, . . . , pk ? P , and q ? Q
?
s?T?(Q)
u1,...,uk?lvQ(s)
?pK(s
?) ? ? qG(s, t, V ) = ?
?p,q?
L (t
?) ,
where
? V = {(u1, v1), . . . , (uk, vk)},
? s? = s[ui ? pi | 1 ? i ? k], and
? t? = t[vi ? ?pi, t(vi)? | 1 ? i ? k].
In particular, for t ? T? we obtain
?
s?T?
?pK(s) ? ?
q
G(s, t, ?) = ?
?p,q?
L (t) ,
which yields
(
?G(?K)
)
(t) =
?
s?T?
?K(s) ? ?G(s, t)
=
?
s?T?
?qSK (s) ? ?
qS
G (s, t, ?)
= ??qS,qS?L (t) = ?L(t) .
In the second item G is input-productive. Then
G?1 is output-productive and ??1G (?) = ?G?1(?).
Hence the first statement proves that ??1G (?) is
recognizable.
Example 7. As an illustration of the construction
in Theorem 6, let us apply the STSG G of Exam-
ple 1 to the WRTG K over ? and P = {p, qS, q?}
and the following rules:
qS
2
5? ?(p, q?) qS
3
5? ?
p
1
? ?(q?, qS) q?
1
? ? .
In fact, K is in normal form and is equivalent to
the WRTG of Example 2. Using the construction
in the proof of Theorem 6 we obtain the WRTG L
over ? and P ?Q with Q = {e, o}. We will only
7
o
?
?
?
o o
? 136?
?
? ?
o o
?q, o?
?
?
?
?q?, o? ?q, o?
? 136 ? 25?
?
? ?
o o
?q, o?
1
36 ?
2
5????
?
? ?
?q, o? ?q?, o?
Figure 5: Illustration of the construction in the proof of Theorem 6 using the WRTG K of Example 7:
some example rule (left), run of K on the input side of the rule (middle), and resulting rule (right).
q1
1
15??
?
q2 q3
q1
1
15??
?
q3 q2
q1
1
5?? ?
q2
1
3?? ? q3
1
5??
?
q1 q2
Figure 6: WRTG constructed in Example 7. We
renamed the states and calculated the weights.
show rules of L that contribute to ?L. To the right
of each rule we indicate from which state ofK and
which rule of G the rule was constructed.
?qS, o?
1
6 ?
2
5?? ?(?q?, o?, ?p, e?) qS, ?2
?qS, o?
1
6 ?
2
5?? ?(?p, e?, ?q?, o?) qS, ?3
?qS, o?
1
3 ?
3
5?? ? qS, ?4
?q?, o?
1
3 ?1?? ? q?, ?4
?p, e?
1
2 ?
2
5?? ?(?qS, o?, ?q?, o?) p, ?6
The initial state ofL is ?qS, o?. It is easy to see that
every t ? T? such that ?L(t) 6= 0 is thin, which
means that |pos(t) ? Nn| ? 2 for every n ? N.
7 Domain and range
Finally, let us consider the domain and range of a
weighted tree transformation ? : T??T? ? R?0.
Again, we first give general definitions and deal
with the infinite sums that might occur in them
later. The domain dom(?) of ? and the range
range(?) of ? are defined by
(
dom(?)
)
(s) =
?
u?T?
?(s, u) (3)
(
range(?)
)
(t) =
?
u?T?
?(u, t) (4)
for every s ? T? and t ? T?. Obviously,
the domain dom(?) is the range range(??1) of
the inverse of ? . Moreover, we can express the
domain dom(?) of ? as the backward applica-
tion ??1(1) where 1 is the weighted tree language
that assigns the weight 1 to each tree. Note that 1
is recognizable for every ranked alphabet.
We note that the sums in Equations (3) and (4)
might be infinite, but for input-productive (re-
spectively, output-productive) STSG G the do-
main dom(?G) (respectively, the range range(?G))
are well-defined by Proposition 5. Using those ob-
servations and Theorem 6 we can obtain the fol-
lowing statement.
Corollary 8. Let G be an STSG. If G is input-
productive, then dom(?G) is recognizable. More-
over, if G is output-productive, then range(?G) is
recognizable.
Proof. These statements follow directly from The-
orem 6 with the help of the observation that
dom(?G) = ?
?1
G (1) and range(?G) = ?G(1).
Conclusion
We showed that every output-productive STSG
preserves recognizability under forward applica-
tion. Dually, every input-productive STSG pre-
serves recognizability under backward applica-
tion. We presented direct and effective construc-
tions for these operations. Special cases of those
constructions can be used to compute the domain
of an input-productive STSG and the range of an
output-productive STSG. Finally, we presented a
characterization of the power of STSG in terms of
weighted bimorphisms.
Acknowledgements
ZOLTA?N FU?LO?P and HEIKO VOGLER were finan-
cially supported by the TA?MOP-4.2.2/08/1/2008-
0008 program of the Hungarian National Devel-
opment Agency. ANDREAS MALETTI was finan-
cially supported by the Ministerio de Educacio?n y
Ciencia (MEC) grant JDCI-2007-760.
8
References
Anne Abeille?, Yves Schabes, and Aravind K. Joshi.
1990. Using lexicalized TAGs for machine trans-
lation. In Proc. 13th CoLing, volume 3, pages 1?6.
University of Helsinki, Finland.
Alfred V. Aho and Jeffrey D. Ullman. 1969. Transla-
tions on a context-free grammar. In Proc. 1st STOC,
pages 93?112. ACM.
Athanasios Alexandrakis and Symeon Bozapalidis.
1987. Weighted grammars and Kleene?s theorem.
Inf. Process. Lett., 24(1):1?4.
Andre? Arnold and Max Dauchet. 1982. Morphismes
et bimorphismes d?arbres. Theoret. Comput. Sci.,
20(1):33?93.
Joost Engelfriet, Eric Lilin, and Andreas Maletti.
2009. Extended multi bottom-up tree transducers
? composition and decomposition. Acta Inform.,
46(8):561?590.
Zolta?n Fu?lo?p and Heiko Vogler. 2009. Weighted tree
automata and tree transducers. In Manfred Droste,
Werner Kuich, and Heiko Vogler, editors, Handbook
of Weighted Automata, chapter 9, pages 313?403.
Springer.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. HLT-NAACL 2004, pages 273?280. ACL.
Ferenc Ge?cseg and Magnus Steinby. 1984. Tree Au-
tomata. Akade?miai Kiado?, Budapest, Hungary.
Ferenc Ge?cseg and Magnus Steinby. 1997. Tree lan-
guages. In Grzegorz Rozenberg and Arto Salomaa,
editors, Handbook of Formal Languages, chapter 1,
pages 1?68. Springer.
Jonathan S. Golan. 1999. Semirings and their Appli-
cations. Kluwer Academic.
Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Computational
Linguistics, 34(3):391?427.
Udo Hebisch and Hanns J. Weinert. 1998. Semirings
? Algebraic Theory and Applications in Computer
Science. World Scientific.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. 9th IWPT, pages 53?64. ACL.
Kevin Knight and Jonathan Graehl. 2005. An
overview of probabilistic tree transducers for natural
language processing. In Proc. 6th CICLing, volume
3406 of LNCS, pages 1?24. Springer.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL 2003, pages 48?54. ACL.
Philip M. Lewis II and Richard Edwin Stearns. 1968.
Syntax-directed transductions. J. ACM, 15(3):465?
488.
Eric Lilin. 1981. Proprie?te?s de clo?ture d?une extension
de transducteurs d?arbres de?terministes. In Proc.
6th CAAP, volume 112 of LNCS, pages 280?289.
Springer.
Andreas Maletti, Jonathan Graehl, Mark Hopkins,
and Kevin Knight. 2009. The power of ex-
tended top-down tree transducers. SIAM J. Comput.,
39(2):410?430.
Andreas Maletti. 2010. Why synchronous tree substi-
tution grammars? In Proc. HLT-NAACL 2010. ACL.
to appear.
David F. Martin and Steven A. Vere. 1970. On syntax-
directed transduction and tree transducers. In Proc.
2nd STOC, pages 129?135. ACM.
Mark-Jan Nederhof and Giorgio Satta. 2006. Proba-
bilistic parsing strategies. J. ACM, 53(3):406?436.
William C. Rounds. 1970. Mappings and grammars
on trees. Math. Systems Theory, 4(3):257?287.
Yves Schabes. 1990. Mathematical and computa-
tional aspects of lexicalized grammars. Ph.D. thesis,
University of Pennsylvania.
Stuart M. Shieber and Yves Schabes. 1990. Syn-
chronous tree-adjoining grammars. In Proc. 13th
CoLing, pages 253?258. ACL.
Stuart M. Shieber. 2004. Synchronous grammars as
tree transducers. In Proc. TAG+7, pages 88?95. Si-
mon Fraser University.
Stuart M. Shieber. 2006. Unifying synchronous tree
adjoining grammars and tree transducers via bimor-
phisms. In Proc. 11th EACL, pages 377?384. ACL.
James W. Thatcher. 1970. Generalized2 sequential
machine maps. J. Comput. System Sci., 4(4):339?
367.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proc. 39th
ACL, pages 523?530. ACL.
9
Proceedings of the 2010 Workshop on Applications of Tree Automata in Natural Language Processing, ACL 2010, pages 19?27,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Parsing and Translation Algorithms
Based on Weighted Extended Tree Transducers
Andreas Maletti?
Departament de Filologies Roma`niques
Universitat Rovira i Virgili
Tarragona, Spain
Giorgio Satta
Department of Information Engineering
University of Padua
Padua, Italy
Abstract
This paper proposes a uniform frame-
work for the development of parsing and
translation algorithms for weighted ex-
tended (top-down) tree transducers and in-
put strings. The asymptotic time complex-
ity of these algorithms can be improved
in practice by exploiting an algorithm for
rule factorization in the above transducers.
1 Introduction
In the field of statistical machine translation, con-
siderable interest has recently been shown for
translation models based on weighted tree trans-
ducers. In this paper we consider the so-called
weighted extended (top-down) tree transducers
(WXTTs for short). WXTTs have been proposed
by Graehl and Knight (2004) and Knight (2007)
and are rooted in similar devices introduced ear-
lier in the formal language literature (Arnold and
Dauchet, 1982).
WXTTs have enough expressivity to represent
hierarchical syntactic analyses for natural lan-
guage sentences and can directly model most of
the elementary operations that rule the process
of translation between natural languages (Knight,
2007). Furthermore, the use of weights and in-
ternal states allows the encoding of statistical pa-
rameters that have recently been shown to be ex-
tremely useful in discriminating likely translations
from less plausible ones.
For an WXTT M , the parsing problem is tradi-
tionally defined for a pair of trees t and u and re-
quires as output some representation of the set of
all computations ofM that map t into u. Similarly,
the translation problem forM is defined for an in-
put tree t and requires as output some representa-
tion of the set of all computations of M mapping t
?Financially supported by the Ministerio de Educacio?n y
Ciencia (MEC) grant JDCI-2007-760.
into any other tree. When we deal with natural
language processing applications, however, pars-
ing and translation are most often represented on
the basis of input strings rather than trees. Some
tricks are then applied to map the problem back
to the case of input trees. As an example in the
context of machine translation, let w be some in-
put string to be translated. One can intermediately
construct a tree automaton Mw that recognizes the
set of all possible trees that have w as yield with
internal nodes from the input alphabet of M . This
automaton Mw is further transformed into a tree
transducer implementing a partial identity trans-
lation. This transducer is then composed with M
(relational composition) to obtain a transducer that
represents all translations of w. This is usually
called the ?cascaded? approach.
In contrast with the cascaded approach above,
which may be rather inefficient, we investigate a
more direct technique for both parsing and transla-
tion of strings based on WXTTs. We do this by ex-
tending to WXTTs the well-known BAR-HILLEL
construction defined for context-free grammars
(Bar-Hillel et al, 1964) and for weighted context-
free grammars (Nederhof and Satta, 2003). We
then derive computational complexity results for
parsing and translation of input strings on the ba-
sis of WXTTs. Finally, we develop a novel fac-
torization algorithm for WXTTs that, in practical
applications, can reduce the asymptotic complex-
ity for such problems.
2 Preliminary definitions
Let ? be an associative binary operation on a set S.
If S contains an element 1 such that 1?s = s = s?1
for every s ? S, then (S, ?, 1) is a monoid. Such
a monoid (S, ?, 1) is commutative if the identity
s1 ?s2 = s2 ?s1 holds for all s1, s2 ? S. A commu-
tative semiring (S,+, ?, 0, 1) is an algebraic struc-
ture such that:
? (S,+, 0) and (S, ?, 1) are commutative
19
monoids,
? ? distributes over + (from both sides), and
? s ? 0 = 0 = 0 ? s for every s ? S.
From now on, let (S,+, ?, 0, 1) be a com-
mutative semiring. An alphabet is a finite
set of symbols. A weighted string automa-
ton [WSA] (Schu?tzenberger, 1961; Eilenberg,
1974) is a system N = (P,?, J, ?, F ) where
? P and ? are alphabets of states and input
symbols, respectively,
? J, F : P ? S assign initial and final weights,
respectively, and
? ? : P ? ?? P ? S assigns a weight to each
transition.
The transition weight mapping ? can be under-
stood as square matrices ?(?, ?, ?) ? SP?P for ev-
ery ? ? ?. The WSA N is deterministic if
? J(p) 6= 0 for at most one p ? P and
? for every p ? P and ? ? ? there exists at
most one p? ? P such that ?(p, ?, p?) 6= 0.
We now proceed with the semantics of N . We
will define the initial algebra semantics here; al-
ternative, equivalent definitions of the semantics
exist (Sakarovitch, 2009). Let w ? ?? be an in-
put string, ? ? ?, and p, p? ? P be two states.
We extend ? to a mapping h? : P ? ?? ? P ? S
recursively as follows:
h?(p, ?, p
?) =
{
1 if p = p?
0 otherwise
h?(p, ?w, p
?) =
?
p???P
?(p, ?, p??) ? h?(p
??, w, p?) .
Consequently,
h?(p, uw, p
?) =
?
p???P
h?(p, u, p
??) ? h?(p
??, w, p?)
for all p, p? ? P and u,w ? ??. Then the matrix
h?(?, ?1 ? ? ? ?k, ?) equals ?(?, ?1, ?) ? . . . ? ?(?, ?k, ?).
Thus, if the semiring operations can be performed
in constant time and access to ?(p, ?, q) is in con-
stant time for every p, q ? P , then for every
w ? ?? we can compute the matrix h?(?, w, ?) in
time O(|w| ? |P |3) because it can be computed by
|w| ? 1 matrix multiplications.
The WSA N computes the map N : ?? ? S,
which is defined for every w ? ?? by1
N(w) =
?
p,p??P
J(p) ? h?(p, w, p
?) ? F (p?) .
1We overload the symbol N to denote both the WSA and
its recognized mapping. However, the intended meaning will
always be clear from the context.
Since we will also consider individual runs,
let us recall the run semantics as well. Let
w = ?1 ? ? ? ?k ? ?? be an input string of length k.
Then any mapping r : [0, k] ? P is a run of N
on w, where [0, k] denotes the set of integers be-
tween (inclusive) 0 and k. A run can be under-
stood as a vector of states and thus we some-
times write ri instead of r(i). The weight of
such a run r, denoted by wtN (r), is defined by
wtN (r) =
?k
i=1 ?(ri?1, ?i, ri). Then
h?(p, w, p
?) =
?
r : [0,k]?P
r0=p,rk=p?
wtN (r)
for every p, p? ? P and w ? ??.
3 Weighted extended tree transducers
Next, we move to tree languages, for which we
need to introduce some additional notation. Let
? be a ranked alphabet, that is, an alphabet
whose symbols have a unique associated arity. We
write ?k to denote the set of all k-ary symbols
in ?. We use the special nullary symbol e ? ?0 to
syntactically represent the empty string ?. The set
of ?-trees indexed by a set V , denoted by T?(V ),
is the smallest set satisfying both of the following
conditions:
? for every v ? V , the single node labeled v,
written v, is a tree of T?(V ),
? for every ? ? ?k and t1, . . . , tk ? T?(V ),
the tree with a root node labeled ? and
trees t1, . . . , tk as its k children, written
?(t1, . . . , tk), belongs to T?(V ).
Throughout this paper we sometimes write ?() as
just ?. In the following, let t ? T?(V ). The set
of positions Pos(t) ? N? of a tree t ? T?(V ) is
recursively defined as follows:
Pos(v) = {?}
Pos(t) = {?} ? {iw | 1 ? i ? k,w ? Pos(ti)}
for every v ? V , ? ? ?k, and t1, . . . , tk ? T?(V )
where t = ?(t1, . . . , tk). The label of t at posi-
tion w ? Pos(t) is denoted by t(w). The size of
the tree t ? T? is defined as |t| = |Pos(t)|. For
every w ? Pos(t) the subtree of t that is rooted
at w is denoted by subt(w); i.e.,
subt(?) = t
sub?(t1,...,tk)(iw) = subti(w)
20
for every ? ? ?k, t1, . . . , tk ? T?(V ), 1 ? i ? k,
and w ? Pos(ti). Finally, the set of vari-
ables var(t) is given by
var(t) = {v ? V | ?w ? Pos(t) : t(w) = v} .
If for every v ? var(t) there exists exactly one
w ? Pos(t) such that t(w) = v, then t is linear.
We use the fixed sets X = {xi | i ? 1} and
Y = {yi,j | 1 ? i < j} of formal variables
and the subsets Xk = {xi | 1 ? i ? k} and
Yk = {yi,j | 1 ? i < j ? k} for every k ? 0.
Note thatX0 = ?. For everyH ? ?0?X?Y , the
H-yield of t is recursively defined by ydH(t) = t
if t ? H \ {e}, ydH(t) = ydH(t1) ? ? ? ydH(tk) if
t = ?(t1, . . . , tk) with ? ? ?k and k ? 1, and
ydH(t) = ? otherwise. If H = ?0 ?X ? Y , then
we also omit the index and just write yd(t).
Let l ? T?(V ) and ? : V ? T?(V ). Then
l? denotes the result obtained from l by replacing
every occurrence of v ? V by ?(v). The k-fold
application is denoted by l?k. If l?k = l?k+1 for
some k ? 0, then we denote l?k by l??. In addi-
tion, if V = Xk, then we write l[?(x1), . . . , ?(xk)]
instead of l?. We write C?(Xk) for the subset
of those trees of T?(Xk) such that every vari-
able of x ? Xk occurs exactly once in it. Given
t ? T?(X), we write dec(t) for the set
{
(l, t1, . . . , tk)
?
?
?
l ? C?(Xk), l[t1, . . . , tk] = t,
t1, . . . , tk ? T?(X)
}
A (linear and nondeleting) weighted extended
(top-down) tree transducer [WXTT] (Arnold and
Dauchet, 1975; Arnold and Dauchet, 1976; Lilin,
1981; Arnold and Dauchet, 1982; Maletti et al,
2009) is a system M = (Q,?,?, I, R) where
? Q is an alphabet of states,
? ? and ? are ranked alphabets of input and
output symbols, respectively,
? I : Q? S assigns initial weights, and
? R is a finite set of rules of the form
(q, l)
s
? (q1 ? ? ? qk, r) with q, q1, . . . , qk ? Q,
l ? C?(Xk) and r ? C?(Xk), and s ? S
such that {l, r} 6? X .
Let us discuss the final restriction imposed on
the rules of a WXTT. Essentially, it disallows rules
of the form (q, x1)
s
? (q?, x1) with q, q? ? Q and
s ? S. Such pure epsilon rules only change the
state and charge a cost. However, they can yield
infinite derivations (and with it infinite products
and sums) and are not needed in our applications.
The WXTT M is standard if ydX(r) = x1 ? ? ?xk
for every (q, l)
s
? (q1 ? ? ? qk, r) ? R. This restric-
tion enforces that the order of the variables is fixed
on the right-hand side r, but since the order is ar-
bitrary in the left-hand side l (and the names of the
variables are inconsequential), it can be achieved
easily without loss of generality. If there are sev-
eral rules that differ only in the naming of the vari-
ables, then their weights should be added to obtain
a single standard rule. To keep the presentation
simple, we also construct nonstandard WXTTs in
the sequel. However, we implicitly assume that
those are converted into standard WXTTs.
The semantics of a standard WXTT is in-
spired by the initial-algebra semantics for classi-
cal weighted top-down and bottom-up tree trans-
ducers (Fu?lo?p and Vogler, 2009) [also called top-
down and bottom-up tree series transducers by En-
gelfriet et al (2002)]. Note that our semantics
is equivalent to the classical term rewriting se-
mantics, which is presented by Graehl and Knight
(2004) and Graehl et al (2008), for example. In
fact, we will present an equivalent semantics based
on runs later. Let M = (Q,?,?, I, R) be a
WXTT. We present a definition that is more gen-
eral than immediately necessary, but the general-
ization will be useful later on. For every n ? N,
p1, . . . , pn ? Q, and L ? R, we define the
mapping hp1???pnL : T?(Xn) ? T?(Xn) ? S
Q by
hp1???pnL (xi, xi)pi = 1 for every 1 ? i ? n and
hp1???pnL (t, u)q
=
?
(l,t1,...,tk)?dec(t)
(r,u1,...,uk)?dec(u)
(q,l)
s
?(q1???qk,r)?L
s ?
k?
i=1
hp1???pnL (ti, ui)qi (1)
for all remaining t ? T?(Xn), u ? T?(Xn), and
q ? Q. Note that for each nonzero summand in (1)
one of the decompositions dec(t) and dec(u) must
be proper (i.e., either l /? X or r /? X). This
immediately yields that the sum is finite and the
recursion well-defined. The transformation com-
puted by M , also denoted by M , is the map-
ping M : T? ? T? ? S, which is defined by
M(t, u) =
?
q?Q I(q)?hR(t, u)q for every t ? T?
and u ? T?.
Let us also introduce a run semantics for the
WXTT (Q,?,?, I, R). The rank of a rule
? = (q, l)
s
? (q1 ? ? ? qk, r) ? R, denoted by rk(?),
is rk(?) = k. This turns R into a ranked alphabet.
The input state of ? is in(?) = q, the ith output
state is outi(?) = qi for every 1 ? i ? k, and
21
the weight of ? is wt(?) = s. A tree r ? TR(X)
is called run if in(r(wi)) = outi(r(w)) for every
wi ? Pos(r) and 1 ? i ? rk(r(w)) such that
r(wi) ? R. The weight of a run r ? TR(X) is
wt(r) =
?
w?Pos(r),r(w)?R
wt(r(w)) .
The evaluation mappings pi1 : TR(X) ? T?(X)
and pi2 : TR(X) ? T?(X) are defined for every
x ? X , ? = (q, l)
s
? (q1 ? ? ? qk, r) ? R, and
r1, . . . , rk ? TR(X) by pi1(x) = x, pi2(x) = x,
and
pi1(?(r1, . . . , rk)) = l[pi1(r1), . . . , pi1(rk)]
pi2(?(r1, . . . , rk)) = r[pi2(r1), . . . , pi2(rk)] .
We obtain the weighted tree transformation for ev-
ery t ? T? and u ? T? as follows2
M(t, u) =
?
run r?TR
t=pi1(r),u=pi2(r)
I(in(r(?))) ? wt(r) .
This approach is also called the bimorphism ap-
proach (Arnold and Dauchet, 1982) to tree trans-
formations.
4 Input and output restrictions of WXTT
In this section we will discuss the BAR-HILLEL
construction for the input and the output part of a
WXTT M . This construction essentially restricts
the input or output of the WXTT M to the string
language recognized by a WSA N . Contrary to
(direct or inverse) application, this construction
is supposed to yield another WXTT. More pre-
cisely, the constructed WXTT should assign to
each translation (t, u) the weight assigned to it
by M multiplied by the weight assigned by N
to the yield of t (or u if the output is restricted).
Since our WXTTs are symmetric, we will actu-
ally only need one construction. Let us quickly
establish the mentioned symmetry statement. Es-
sentially we just have to exchange left- and right-
hand sides and redistribute the states in those left-
and right-hand sides accordingly.
From now on, let M = (Q,?,?, I, R) be a
WXTT.
Theorem 1. There exists a WXTT M ? such that
M ?(u, t) = M(t, u) for every t ? T? and u ? T?.
2We immediately also use M for the run semantics be-
cause the two semantics trivially coincide.
Proof. Let M ? = (Q,?,?, I, R?) be the WXTT
such that
R? = {(q, r)
s
? (w, l) | (q, l)
s
? (w, r) ? R} .
It should be clear that M ?(u, t) = M(t, u) for ev-
ery t ? T? and u ? T?.
With the symmetry established, we now only
need to present the BAR-HILLEL construction for
either the input or output side. Without loss of
generality, let us assume that M is standard. We
then choose the output side here because the order
of variables is fixed in it. Note that we sometimes
use the angled parentheses ??? and ??? instead of
parentheses for clarity.
Definition 2. Let N = (P,?, J, ?, F ) be a WSA
with ? = ?0 \ {e}. We construct the output prod-
uct Prod(M,N) = (P?Q?P,?,?, I ?, R?) such
that
? I ?(?p, q, p??) = J(p) ? I(q) ? F (p?) for every
p, p? ? P and q ? Q,
? for every rule (q, l)
s
? (q1 ? ? ? qk, r) ? R and
every p0, . . . , pk, p?0, . . . , p
?
k ? P , let
(q?, l)
s?s0?...?sk?????? (q?1 ? ? ? q
?
k, r) ? R
?
where
? q? = ?p0, q, p?k?,
? q?i = ?p
?
i?1, qi, pi? for every 1 ? i ? k,
? yd(r) = w0x1w1 ? ? ?wk?1xkwk with
w0, . . . , wk ? ??, and
? si = h?(pi, wi, p?i) for every 0 ? i ? k.
Let ? = (q, l)
s
? (q1 ? ? ? qk, r) ? R. The
size of ? is |?| = |l| + |r|. The size and
rank of the WXTT M are |M | =
?
??R|?|
and rk(M) = max??R rk(?), respectively. Fi-
nally, the maximal output yield length of M , de-
noted by len(M), is the maximal length of yd(r)
for all rules (q, l)
s
? (q1 ? ? ? qk, r) ? R.
The size and rank of Prod(M,N) are in
O(|M | ? |P |2 rk(M)+2) and rk(M), respec-
tively. We can compute Prod(M,N) in time
O(|R| ? len(M) ? |P |2 rk(M)+5). If N is de-
terministic, then the size of Prod(M,N) is
in O(|M | ? |P |rk(M)+1) and the required time is
inO(|R| ? len(M) ? |P |rk(M)+1). Next, let us prove
that our BAR-HILLEL construction is actually cor-
rect.
Theorem 3. Let M and N be as in Defini-
tion 2, and let M ? = Prod(M,N). Then
M ?(t, u) = M(t, u) ?N(yd(u)) for every t ? T?
and u ? T?.
22
Proof. Let M ? = (Q?,?,?, I ?, R?). First, a sim-
ple proof shows that
hR?(t, u)?p,q,p?? = hR(t, u)q ? h?(p, yd(u), p
?)
for every t ? T?, u ? T?, q ? Q, and p, p? ? P .
Now we can prove the main statement as follows:
M ?(t, u)
=
?
q??Q?
I ?(q?) ? hR?(t, u)q?
=
?
p,p??P
q?Q
I ?(?p, q, p??) ? hR(t, u)q ? h?(p, yd(u), p
?)
= M(t, u) ?N(yd(u))
for every t ? T? and u ? T?.
Note that the typical property of many BAR-
HILLEL constructions, namely that a run of M
and a run of N uniquely determine a run
of Prod(M,N) and vice versa, does not hold for
our construction. In fact, a run of M and a run
of N uniquely determine a run of Prod(M,N),
but the converse does not hold. We could modify
the construction to enable this property at the ex-
pense of an exponential increase in the number of
states of Prod(M,N). However, since those re-
lations are important for our applications, we ex-
plore the relation between runs in some detail here.
To simplify the discussion, we assume, without
loss of generality, that M is standard and s = s?
for every two rules (q, l)
s
? (w, r) ? R and
(q, l)
s?
? (w, r) ? R. Moreover, we assume the
symbols of Definition 2. For every r? ? TR?(X),
we let base(r?) denote the run obtained from r? by
replacing each symbol
(q?, l)
s?s0?...?sk?????? (q?1 ? ? ? q
?
k, r)
by just (q, l)
s
? (q1 ? ? ? qk, r) ? R. Thus, we re-
place a rule (which is a symbol) of R? by the un-
derlying rule of R. We start with a general lemma,
which we believe to be self-evident.
Lemma 4. Let r? ? TR? and n = |yd(pi2(r?))|.
Then wtM ?(r?) = wtM (base(r?))?
?
r?R?? wtN (r)
where R?? is a nonempty subset of
{r : [0, n]? P | in(r?(?)) = ?r0, q, rn?}.
Let us assume that N is trim (i.e., all states are
reachable and co-reachable) and unambiguous. In
this case, for every ?1 ? ? ? ?k ? ?? and p, p? ? P
there is at most one successful run r : [0, k] ? P
such that
? ?(ri?1, ?i, ri) 6= 0 for every 1 ? i ? k, and
? r0 = p and rk = p?.
This immediately yields the following corollary.
Corollary 5 (of Lemma 4). Let N be trim and
unambiguous. For every r? ? TR? we have
wtM ?(r
?) = wtM (base(r
?)) ? wtN (r)
for some r : [0, n]? P with n = |yd(pi2(r?))|.
We now turn to applications of the product con-
struction. We first consider the translation prob-
lem for an input string w and a WXTTM . We can
represent w as a trim and unambiguous WSA Nw
that recognizes the language {w} with weight
of 1 on each transition (which amounts to ignor-
ing the weight contribution of Nw). Then the in-
put product transducer Mw = Prod(Nw,M) pro-
vides a compact representation of the set of all
computations of M that translate the string w.
From Corollary 5 we have that the weights of
these computations are also preserved. Thus,
Mw(T? ? T?) =
?
(t,u)?T??T?
Mw(t, u) is the
weight of the set of string translations of w.
As usual in natural language processing ap-
plications, we can exploit appropriate semirings
and compute several useful statistical parameters
through Mw(T? ? T?), as for instance the high-
est weight of a computation, the inside probabil-
ity and the rule expectations; see (Li and Eisner,
2009) for further discussion.
One could also construct in linear time the range
tree automaton for Mw, which can be interpreted
as a parsing forest with all the weighted trees as-
signed to translations of w under M . If we fur-
ther assume thatM is unambiguous, thenMw will
also have this property, and we can apply standard
techniques to extract from Mw the highest score
computation. In machine translation applications,
the unambiguity assumption is usually met, and
avoids the so-called ?spurious? ambiguity, that is,
having several computations for an individual pair
of trees.
The parsing problem for input strings w and u
can be treated in a similar way, by restricting M
both to the left and to the right.
5 Rule factorization
As already discussed, the time complexity of the
product construction is an exponential function
of the rank of the transducer. Unfortunately,
it is not possible in the general case to cast a
23
WXTT into a normal form such that the rank is
bounded by some constant. This is also expected
from the fact that the translation problem for sub-
classes of WXTTs such as synchronous context-
free grammars is NP-hard (Satta and Peserico,
2005). Nonetheless, there are cases in which a
rank reduction is possible, which might result in
an improvement of the asymptotical run-time of
our construction.
Following the above line, we present here a
linear time algorithm for reducing the rank of a
WXTT under certain conditions. Similar algo-
rithms for tree-based transformation devices have
been discussed in the literature. Nesson et al
(2008) consider synchronous tree adjoining gram-
mars; their algorithm is conceptually very sim-
ilar to ours, but computationally more demand-
ing due to the treatment of adjunction. Follow-
ing that work, we also demand here that the new
WXTT ?preserves? the recursive structure of the
input WXTT, as formalized below. Galley et al
(2004) algorithm also behaves in linear time, but
deals with the different problem of tree to string
translation. Rank reduction algorithms for string-
based translation devices have also been discussed
by Zhang et al (2006) and Gildea et al (2006).
Recall that M = (Q,?,?, I, R) is a standard
WXTT. Let M ? = (Q?,?,?, I ?, R?) be a WXTT
with Q ? Q?.3 Then M ? is a structure-preserving
factorization of M if
? I ?(q) = I(q) for every q ? Q and I ?(q) = 0
otherwise, and
? hp1???pnR? (t, u)q = h
p1???pn
R (t, u)q for every
q, p1, . . . , pn ? Q, t ? T?(Xn), and
u ? T?(Xn).
In particular, we have hR?(t, u)q = hR(t, u)q for
n = 0. Consequently, M ? and M are equivalent
because
M ?(t, u) =
?
q?Q?
I ?(q) ? hR?(t, u)q
=
?
q?Q
I(q) ? hR(t, u)q = M(t, u) .
Note that the relation ?is structure-preserving fac-
torization of? is reflexive and transitive, and thus, a
pre-order. Moreover, in a ring (actually, additively
cancellative semirings are sufficient) it is also anti-
symmetric, and consequently, a partial order.
3Actually, an injective mapping Q ? Q? would be suffi-
cient, but since the naming of the states is arbitrary, we im-
mediately identify according to the injective mapping.
Informally, a structure-preserving factorization
ofM consists in a set of new rules that can be com-
posed to provide the original rules and preserve
their weights. We develop an algorithm for finding
a structure-preserving factorization by decompos-
ing each rule as much as possible. The algorithm
can then be iterated for all the rules in the WXTT.
The idea underlying our algorithm is very simple.
Let ? = (q, l)
s
? (q1 ? ? ? qk, r) ? R be an origi-
nal rule. We look for subtrees l? and r? of l and r,
respectively, such that var(l?) = var(r?). The con-
dition that var(l?) = var(r?) is derived from the
fact that hq1???qkR (l
?, r?)q = 0 if var(l?) 6= var(r?).
We then split ? into two new rules by ?excis-
ing? subtrees l? and r? from l and r, respectively.
In the remaining trees the ?excised? trees are re-
placed with some fresh variable. The tricky part
is the efficient computation of the pairs (wl, wr),
since in the worst case the number of such pairs
is in ?(|l| ? |r|), and na??ve testing of the condition
var(l?) = var(r?) takes time O(rk(?)).
Let us start with the formal development. Recall
the doubly-indexed set Y = {yi,j | 1 ? i < j}.
Intuitively speaking, the variable yi,j will
represent the set {xi, . . . , xj}. With this
intuition in mind, we define the mapping
vars : T?(X ? Y )? N3? as follows:
vars(xi) = (i, i, 1)
vars(yi,j) = (i, j, j ? i+ 1)
and vars(?(t1, . . . , tk)) is
(
k
min
`=1
vars(t`)1,
k
max
`=1
vars(t`)2,
k?
`=1
vars(t`)3)
for every i, j ? N with i < j, ? ? ?k, and
t1, . . . , tk ? T?(X ? Y ). Clearly, vars(t) can
be computed in time O(|t|), which also in-
cludes the computation of vars(u) for every sub-
tree u of t. In addition, vars(t)3 = |var(t)|
for all linear t ? T?(X). Finally, if
t ? T?(X), then vars(t)1 and vars(t)2 are the
minimal and maximal index i ? N such that
xi ? var(t), respectively (they are ? and 0,
respectively, if var(t) = ?). For better read-
ability, we use minvar(t) and maxvar(t) for
vars(t)1 and vars(t)2, respectively.
Let ? = (q, l)
s
? (q1 ? ? ? qk, r) ? R be an origi-
nal rule. In the following, we will use minvar(t),
maxvar(t), and |var(t)| freely for all subtrees t
of l and r and assume that they are precomputed,
24
which can be done in time O(|?|). Moreover, we
will freely use the test ?var(t) = var(u)? for sub-
trees t and u of l and r, respectively. This test can
be performed in constant time [disregarding the
time needed to precompute vars(t) and vars(u)]
by the equivalent test
? minvar(t) = minvar(u),
? maxvar(t) = maxvar(u),
? |var(t)| = maxvar(t)?minvar(t) + 1, and
? |var(u)| = maxvar(u)?minvar(u) + 1.
Our factorization algorithm is presented in Al-
gorithm 1. Its first two parameters hold the left-
and right-hand side (l, r), which are to be decom-
posed. The third and fourth parameter should ini-
tially be x1. To simplify the algorithm, we assume
that it is only called with left- and right-hand sides
that (i) contain the same variables and (ii) contain
at least two variables. These conditions are en-
sured by the algorithm for the recursive calls. The
algorithm returns a decomposition of (l, r) in the
form of a set D ? T?(X ? Y ) ? T?(X ? Y )
such that var(l?) = var(r?) for every (l?, r?) ? D.
Moreover, all such l? and r? are linear. Finally, the
pairs in D can be composed (by means of point-
wise substitution at the variables of Y ) to form the
original pair (l, r).
Before we move on to formal properties of Al-
gorithm 1, let us illustrate its execution on an ex-
ample.
Example 6. We work with the left-hand side
l = ?(x1, ?(x3, x2)) and the right-hand side
r = ?(?(x1, ?(?(x2, x3)))). Then |var(l)| ? 2
and var(l) = var(r). Let us trace the call
DECOMPOSE(l, r, x1, x1). The condition in line 1
is clearly false, so we proceed with line 3. The
condition is true for i = 1, so we continue with
DECOMPOSE(l, ?(x1, ?(?(x2, x3))), x1, ?(x1)).
This time neither the condition in line 1 nor the
condition in line 3 are true. In line 6, j is set to 1
and we initialize r?1 = x1 and r
?
2 = ?(?(x2, x3)).
Moreover, the array h is initialized to h(1) = 1,
h(2) = 2, and h(3) = 2. Now let us discuss the
main loop starting in line 12 in more detail. First,
we consider i = 1. Since l1 = x1, the condition in
line 13 is fulfilled and we set l?1 = x1 and proceed
with the next iteration (i = 2). This time the condi-
tion of line 13 is false because l2 = ?(x3, x2) and
var(l2) = var(rh(2)) = var(r2) = {x2, x3}. Con-
sequently, j is set to 2 and l?2 = r
?
2 = y2,3. Next,
DECOMPOSE(?(x3, x2), ?(?(x2, x3)), x1, x1) is
processed. Let us suppose that it generates the
set D. Then we return
D ? {(?(x1, y2,3), ?(?(x1, y2,3)))} .
Finally, let us quickly discuss how the set D
is obtained. Since the condition in line 3 is
true, we have to evaluate the recursive call
DECOMPOSE(?(x3, x2), ?(x2, x3), x1, ?(x1)).
Now, j = 2, h(2) = 1, and h(3) = 2.
Moreover, r?1 = x2 and r
?
2 = x3. In the
main loop starting in line 12, the condition of
line 13 is always fulfilled, which yields that
l?1 = x3 and l
?
2 = x2. Thus, we return
{(?(x3, x2), ?(?(x2, x3)))}, which is exactly the
input because decomposition completely failed.
Thus, the overall decomposition of l and r is
{(?(x1, y2,3), ?(?(x1, y2,3))),
(?(x3, x2), ?(?(x2, x3)))} ,
which, when the second pair is substituted (point-
wise) for y2,3 in the first pair, yields exactly (l, r).
Informally, the rules are obtained as follows
fromD. If all variables occur in a pair (l?, r?) ? D,
then the left-hand side is assigned to the original
input state. Furthermore, for every variable yi,j we
introduce a new fresh state qi,j whereas the vari-
able xi is associated to qi. In this way, we deter-
mine the states in the right-hand side.
Formally, let ? = (q, l)
s
? (q1 ? ? ? qk, r)
be the original rule and D be the result of
DECOMPOSE(l, r, x1, x1) of Algorithm 1. In ad-
dition, for every 1 ? i < j ? k, let q?,i,j be a new
state such that q?,1,k = q. Let
Q?? = {q, q1, . . . , qk} ? {q?,i,j | 1 ? i < j ? k} .
Then for every (l?, r?) ? D we obtain the rule
(q?,minvar(r?),maxvar(r?), l
?)
s?
? (p1 ? ? ? pn, r
?)
where ydX?Y (r
?) = z1 ? ? ? zn,
s? =
{
s if vars(r?)3 = k
1 otherwise
q?` =
{
qj if z` = xj
q?,i,j if z` = yi,j
for every 1 ? ` ? n. The rules obtained in this
fashion are collected in R??.
4 The WXTT dec(M)
is dec(M) = (Q?,?,?, I ?, R?) where
4Those rules need to be normalized to obtain a standard
WXTT.
25
Algorithm 1 DECOMPOSE(l, r, l?, r?) computing the decomposition of linear l ? T?(Xk) and
r ? T?(Xk) with var(l) = var(r) and |var(l)| ? 2.
if l = ?(l1, . . . , lm) and there exists i ? N is such that var(li) = var(l) then
2: return DECOMPOSE(li, r, l?[?(l1, . . . , li?1, x1, li+1, . . . , lm)], r?[x1])
if r = ?(r1, . . . , rn) and there exists i ? N is such that var(ri) = var(r) then
4: return DECOMPOSE(l, ri, l?[x1], r?[?(r1, . . . , ri?1, x1, ri+1, . . . , rn)])
let l = ?(l1, . . . , lm) and r = ?(r1, . . . , rn)
6: j = minvar(r)
for all 1 ? i ? n do
8: r?i = ri
while j ? maxvar(ri) do
10: h(j) = i; j = j + 1
D = ?
12: for all 1 ? i ? m do
if |var(li)| ? 1 or var(li) 6= var(rh(minvar(li))) then
14: l?i = li
else
16: j = h(minvar(li))
l?i = r
?
j = yminvar(li),maxvar(li)
18: D = D ? DECOMPOSE(li, rj , x1, x1)
return D ? {(l?[?(l?1, . . . , l
?
m)], r
?[?(r?1, . . . , r
?
n)])}
? Q? = Q ?
?
??R,rk(?)?2Q
?
?,
? I ?(q) = I(q) for every q ? Q and I ?(q) = 0
otherwise, and
? R? is
{? ? R | rk(?) < 2} ?
?
??R,rk(?)?2
R?? .
To measure the success of the factorization, we
introduce the following notion. The degree of M ,
denoted by deg(M), is the minimal rank of all
structure-preserving factorizations M ? of M ; i.e.,
deg(M) = min
M ? a structure-preserving
factorization of M
rk(M ?) .
Then the goal of this section is the efficient com-
putation of a structure-preserving factorizationM ?
of M such that rk(M ?) = deg(M).
Theorem 7. The WXTT dec(M) is a structure-
preserving factorization of M such that
rk(dec(M)) = deg(M). Moreover, dec(M) can
be computed in time O(|M |).
Proof. Let us only discuss the run-time complex-
ity shortly. Clearly, DECOMPOSE(l, r, x1, x1)
should be called once for each rule
(q, l)
s
? (q1 ? ? ? qk, r) ? R. In lines 1?4 the
structure of l and r is inspected and the prop-
erties var(li) = var(l) and var(ri) = var(r)
are tested in constant time. Mind that we pre-
computed vars(l) and vars(r), which can be
done in linear time in the size of the rule. Then
each subtree ri is considered in lines 7?10 in
constant time. Finally, we consider all direct input
subtrees li in lines 12?18. The tests involving
the variables are all performed in constant time
due to the preprocessing step that computes
vars(l) and vars(r). Moreover, at most one
recursive call to DECOMPOSE is generated for
each input subtree ti. So if we implement the
union in lines 18 and 19 by a constant-time
operation (such as list concatenation, which can
be done since it is trivially a disjoint union), then
we obtain the linear time-complexity.
6 Concluding remarks
In this paper we have shown how to restrict com-
putations of WXTTs to given input and output
WSA, and have discussed the relevance of this
technique for parsing and translation applications
over input strings, resulting in the computation of
translation forests and other statistical parameters
of interest. We have also shown how to factorize
transducer rules, resulting in an asymptotic reduc-
tion in the complexity for these algorithms.
In machine translation applications transduc-
ers usually have very large sets of rules. One
should then specialize the restriction construction
in such a way that the number of useless rules
for Prod(Nw,M) is considerably reduced, result-
ing in a more efficient construction. This can be
achieved by grounding the construction of the new
rules by means of specialized strategies, as usually
done for parsing based on context-free grammars;
see for instance the parsing algorithms by Younger
(1967) or by Earley (1970).
26
References
Andre? Arnold and Max Dauchet. 1975. Transductions
inversibles de fore?ts. The`se 3e`me cycle M. Dauchet,
Universite? de Lille.
Andre? Arnold and Max Dauchet. 1976. Bi-
transductions de fore?ts. In ICALP, pages 74?86. Ed-
inburgh University Press.
Andre? Arnold and Max Dauchet. 1982. Morphismes
et bimorphismes d?arbres. Theoret. Comput. Sci.,
20(1):33?93.
Yehoshua Bar-Hillel, Micha Perles, and Eliyahu
Shamir. 1964. On formal properties of simple
phrase structure grammars. In Yehoshua Bar-Hillel,
editor, Language and Information: Selected Essays
on their Theory and Application, chapter 9, pages
116?150. Addison Wesley.
Jay Earley. 1970. An efficient context-free parsing al-
gorithm. Commun. ACM, 13(2):94?102.
Samuel Eilenberg. 1974. Automata, Languages, and
Machines, volume 59 of Pure and Applied Math.
Academic Press.
Joost Engelfriet, Zolta?n Fu?lo?p, and Heiko Vogler.
2002. Bottom-up and top-down tree series transfor-
mations. J. Autom. Lang. Combin., 7(1):11?70.
Zolta?n Fu?lo?p and Heiko Vogler. 2009. Weighted tree
automata and tree transducers. In Manfred Droste,
Werner Kuich, and Heiko Vogler, editors, Hand-
book of Weighted Automata, EATCS Monographs on
Theoret. Comput. Sci., chapter IX, pages 313?403.
Springer.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. HLT-NAACL, pages 273?280. Association
for Computational Linguistics.
Daniel Gildea, Giorgio Satta, and Hao Zhang. 2006.
Factoring synchronous grammars by sorting. In
Proc. CoLing/ACL, pages 279?286. Association for
Computational Linguistics.
Jonathan Graehl and Kevin Knight. 2004. Training
tree transducers. In HLT-NAACL, pages 105?112.
Association for Computational Linguistics. See
also (Graehl et al, 2008).
Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Computational
Linguistics, 34(3):391?427.
Kevin Knight. 2007. Capturing practical natural
language transformations. Machine Translation,
21(2):121?133.
Zhifei Li and Jason Eisner. 2009. First- and second-
order expectation semirings with applications to
minimum-risk training on translation forests. In
Proc. EMNLP, pages 40?51. Association for Com-
putational Linguistics.
Eric Lilin. 1981. Proprie?te?s de clo?ture d?une extension
de transducteurs d?arbres de?terministes. In CAAP,
volume 112 of LNCS, pages 280?289. Springer.
Andreas Maletti, Jonathan Graehl, Mark Hopkins,
and Kevin Knight. 2009. The power of ex-
tended top-down tree transducers. SIAM J. Comput.,
39(2):410?430.
Mark-Jan Nederhof and Giorgio Satta. 2003. Prob-
abilistic parsing as intersection. In Proc. IWPT,
pages 137?148. Association for Computational Lin-
guistics.
Rebecca Nesson, Giorgio Satta, and Stuart M. Shieber.
2008. Optimal k-arization of synchronous tree-
adjoining grammar. In Proc. ACL, pages 604?612.
Association for Computational Linguistics.
Jacques Sakarovitch. 2009. Rational and recognisable
power series. In Manfred Droste, Werner Kuich, and
Heiko Vogler, editors, Handbook of Weighted Au-
tomata, EATCS Monographs on Theoret. Comput.
Sci., chapter IV, pages 105?174. Springer.
Giorgio Satta and Enoch Peserico. 2005. Some
computational complexity results for synchronous
context-free grammars. In Proc. HLT-EMNLP,
pages 803?810. Association for Computational Lin-
guistics.
Marcel Paul Schu?tzenberger. 1961. On the definition
of a family of automata. Information and Control,
4(2?3):245?270.
Daniel H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Inform. Control,
10(2):189?208.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Proc. HLT-NAACL, pages 256?
263. Association for Computational Linguistics.
27
Proc. EACL 2012 Workshop on Applications of Tree Automata Techniques in Natural Language Processing, pages 1?10,
Avignon, France, April 24 2012. c?2012 Association for Computational Linguistics
Preservation of Recognizability for
Weighted Linear Extended Top-Down Tree Transducers?
Nina Seemann and Daniel Quernheim and Fabienne Braune and Andreas Maletti
University of Stuttgart, Institute for Natural Language Processing
{seemanna,daniel,braunefe,maletti}@ims.uni-stuttgart.de
Abstract
An open question in [FU?LO?P, MALETTI,
VOGLER: Weighted extended tree trans-
ducers. Fundamenta Informaticae 111(2),
2011] asks whether weighted linear ex-
tended tree transducers preserve recogniz-
ability in countably complete commuta-
tive semirings. In this contribution, the
question is answered positively, which is
achieved with a construction that utilizes
inside weights. Due to the completeness
of the semiring, the inside weights always
exist, but the construction is only effective
if they can be effectively determined. It is
demonstrated how to achieve this in a num-
ber of important cases.
1 Introduction
Syntax-based statistical machine translation
(Knight, 2007) created renewed interest in tree
automata and tree transducer theory (Fu?lo?p
and Vogler, 2009). In particular, it sparked
research on extended top-down tree transduc-
ers (Graehl et al, 2009), which are top-down
tree transducers (Rounds, 1970; Thatcher, 1970)
in which the left-hand sides can contain several
(or no) input symbols. A recent contribution
by Fu?lo?p et al (2011) investigates the theoretical
properties of weighted extended tree transduc-
ers over countably complete and commutative
semirings (Hebisch and Weinert, 1998; Golan,
1999). Such semirings permit sums of countably
many summands, which still obey the usual
associativity, commutativity, and distributivity
laws. We will use the same class of semirings.
? All authors were financially supported by the EMMY
NOETHER project MA / 4959 / 1-1 of the German Research
Foundation (DFG).
Input? Parser ? TM ? LM ? Output
Figure 1: Syntax-based machine translation pipeline.
Extended top-down tree transducers are used as
translation models (TM) in syntax-based machine
translation. In the standard pipeline (see Figure 1;
LM is short for language model) the translation
model is applied to the parses of the input sen-
tence, which can be represented as a recogniz-
able weighted forest (Fu?lo?p and Vogler, 2009).
In practice, only the best or the n-best parses are
used, but in principle, we can use the recogniz-
able weighted forest of all parses. In either case,
the translation model transforms the input trees
into a weighted forest of translated output trees.
A class of transducers preserves recognizability
if for every transducer of the class and each rec-
ognizable weighted forest, this weighted forest
of translated output trees is again recognizable.
Fu?lo?p et al (2011) investigates which extended
top-down tree transducers preserve recognizabil-
ity under forward (i.e., the setting previously de-
scribed) and backward application (i.e., the set-
ting, in which we start with the output trees and
apply the inverse of the translation model), but the
question remained open for forward application
of weighted linear extended top-down tree trans-
ducers [see Table 1 for an overview of the exist-
ing results for forward application due to Engel-
friet (1975) in the unweighted case and Fu?lo?p et
al. (2010) and Fu?lo?p et al (2011) for the weighted
case]. In conclusion, Fu?lo?p et al (2011) ask: ?Are
there a commutative semiring S that is count-
ably complete wrt.
?
, a linear wxttM [weighted
extended top-down tree transducer with regular
look-ahead; see Section 4], and a recognizable
1
model preserves regularity
unweighted
ln-XTOP 3
l-XTOP 3
l-XTOPR 3
XTOP 7
weighted
ln-XTOP 3
l-XTOP 3
l-XTOPR 3
XTOP 7
Table 1: Overview of the known results due to Engel-
friet (1975) and Fu?lo?p et al (2011) and our results in
boxes.
weighted tree language ? such that M(?) [for-
ward application] is not recognizable? Or even
harder, are there S and M with the same prop-
erties such that M(1?) [1? is the weighted forest
in which each tree has weight 1] is not recogniz-
able??
In this contribution, we thus investigate preser-
vation of recognizability (under forward applica-
tion) for linear extended top-down tree transduc-
ers with regular look-ahead (Engelfriet, 1977),
which are equivalent to linear weighted extended
tree transducers by Fu?lo?p et al (2011). We show
that they always preserve recognizability, thus
confirming the implicit hypothesis of Fu?lo?p et al
(2011). The essential tool for our construction is
the inside weight (Lari and Young, 1990; Graehl
et al, 2008) of the states of the weighted tree
grammar (Alexandrakis and Bozapalidis, 1987)
representing the parses. The inside weight of a
state q is the sum of all weights of trees accepted
in this state. In our main construction (see Sec-
tion 5) we first compose the input weighted tree
grammar with the transducer (input restriction).
This is particularly simple since we just abuse
the look-ahead of the initial rules. In a second
step, we normalize the obtained transducer, which
yields the standard product construction typically
used for input restriction. Finally, we project to
the output by basically eliminating the left-hand
sides. In this step, the inside weights of states
belonging to deleted subtrees are multiplied to
the production weight. Due to the completeness
of the semiring, the inside weights always ex-
ist, but the infinite sums have to be computed ef-
fectively for the final step of the construction to
be effective. This problem is addressed in Sec-
tion 6, where we show several methods to effec-
tively compute or approximate the inside weights
for all states of a weighted tree grammar.
2 Notation
Our weights will be taken from a commuta-
tive semiring (A,+, ?, 0, 1), which is an algebraic
structure of two commutative monoids (A,+, 0)
and (A, ?, 1) such that ? distributes over + and
0 ? a = 0 for all a ? A. An infinitary sum opera-
tion
?
is a family (
?
I)I where I is a countable
index set and
?
I : A
I ? A. Given f : I ? A,
we write
?
i?I f(i) instead of
?
I f . The semi-
ring together with the infinitary sum operation
?
is countably complete (Eilenberg, 1974; Hebisch
and Weinert, 1998; Golan, 1999; Karner, 2004) if
for all countable sets I and ai ? A with i ? I
?
?
i?I ai = am + an if I = {m,n},
?
?
i?I ai =
?
j?J
(?
i?Ij
ai
)
if I =
?
j?J Ij
for countable sets J and Ij with j ? J such
that Ij ? Ij? = ? for all different j, j? ? J ,
and
? a ?
(?
i?I ai
)
=
?
i?I(a ? ai) for all a ? A.
For such a semiring, we let a? =
?
i?N a
i for
every a ? A. In the following, we assume that
(A,+, ?, 0, 1) is a commutative semiring that is
countably complete with respect to
?
.
Our trees have node labels taken from an al-
phabet ? and leaves might also be labeled by el-
ements of a set V . Given a set T , we write ?(T )
for the set
{?(t1, . . . , tk) | k ? N, ? ? ?, t1, . . . , tk ? T} .
The set T?(V ) of ?-trees with V -leaves is defined
as the smallest set T such that V ? ?(T ) ? T .
We write T? for T?(?). For each tree t ? T?(V )
we identify nodes by positions. The root of t has
position ? and the position iw with i ? N and
w ? N? addresses the position w in the i-th di-
rect subtree at the root. The set of all positions
in t is pos(t). We write t(w) for the label (taken
from ? ? V ) of t at position w ? pos(t). Sim-
ilarly, we use t|w to address the subtree of t that
is rooted in position w, and t[u]w to represent the
tree that is obtained from replacing the subtree t|w
at w by u ? T?(V ). For a given set L ? ? ? V
of labels, we let
posL(t) = {w ? pos(t) | t(w) ? L}
2
be the set of all positions whose label belongs
to L. We also write posl(t) instead of pos{l}(t).
We often use the set X = {x1, x2, . . . } of vari-
ables and its finite subsets Xk = {x1, . . . , xk}
for every k ? N to label leaves. Let V
be a set potentially containing some variables
of X . The tree t ? T?(V ) is linear if
|posx(t)| ? 1 for every x ? X . Moreover,
var(t) = {x ? X | posx(t) 6= ?} collects all
variables that occur in t. Given a finite set Q and
T ? T?(V ), we let
Q[T ] = {q(t) | q ? Q, t ? T} .
We will treat elements ofQ[T ] (in which elements
ofQ are always used as unary symbols) as special
trees of T??Q(V ). A substitution ? is a mapping
? : X ? T?(V ). When applied to t ? T?(V ),
it returns the tree t?, which is obtained from t
by replacing all occurrences of x ? X (in par-
allel) by ?(x). This can be defined recursively
by x? = ?(x) for all x ? X , v? = v for all
v ? V \X , and ?(t1, . . . , tk)? = ?(t1?, . . . , tk?)
for all ? ? ? and t1, . . . , tk ? T?(V ).
3 Weighted Tree Grammars
In this section, we will recall weighted tree
grammars (Alexandrakis and Bozapalidis, 1987)
[see (Fu?lo?p and Vogler, 2009) for a modern treat-
ment and a complete historical account]. In gen-
eral, weighted tree grammars (WTGs) offer an ef-
ficient representation of weighted forests, which
are sets of trees such that each individual tree
is equipped with a weight. The representation
is even more efficient than packed forests (Mi et
al., 2008) and moreover can represent an infinite
number of weighted trees. To avoid confusion
between the nonterminals of a parser, which pro-
duces the forests considered here, and our WTGs,
we will refer to the nonterminals of our WTG as
states.
Definition 1. A weighted tree grammar (WTG) is
a system (Q,?, q0, P ) where
? Q is a finite set of states (nonterminals),
? ? is the alphabet of symbols,
? q0 ? Q is the starting state, and
? P is a finite set of productions q
a
? t, where
q ? Q, a ? A, and t ? T?(Q).
Example 2. We illustrate our notation on the
WTG Gex = (Q,?, qs, P ) where
? Q = {qs, qnp, qprp, qn, qadj},
? ? contains ?S?, ?NP?, ?VP?, ?PP?, ?DT?,
?NN?, ?N?, ?VBD?, ?PRP?, ?ADJ?, ?man?,
?hill?, ?telescope?, ?laughs?, ?the?, ?on?,
?with?, ?old?, and ?young?, and
? P contains the productions
qs
1.0
? S(qnp,VP(VBD(laughs))) (?1)
qnp
0.4
? NP(qnp,PP(qprp, qnp))
qnp
0.6
? NP(DT(the), qn) (?2)
qprp
0.5
? PRP(on)
qprp
0.5
? PRP(with)
qn
0.3
? N(qadj , qn)
qn
0.3
? NN(man) (?3)
qn
0.2
? NN(hill)
qn
0.2
? NN(telescope)
qadj
0.5
? ADJ(old)
qadj
0.5
? ADJ(young)
It produces a weighted forest representing sen-
tences about young and old men with telescopes
on hills.
In the following, let G = (Q,?, q0, P ) be a
WTG. For every production ? = q
a
? t in P , we
let wtG(?) = a. The semantics of G is defined
with the help of derivations. Let ? ? T?(Q) be
a sentential form, and let w ? posQ(?) be such
that w is the lexicographically smallest Q-labeled
position in ?. Then ? ??G ?[t]w if ?(w) = q. For
a sequence ?1, . . . , ?n ? P of productions, we
let wtG(?1 ? ? ? ?n) =
?n
i=1 wtG(?i). For every
q ? Q and t ? T?(Q), we let
wtG(q, t) =
?
?1,...,?n?P
q?
?1
G ????
?n
G t
wtG(?1 ? ? ? ?n) .
The WTG G computes the weighted forest
LG : T? ? A such that LG(t) = wtG(q0, t) for
every t ? T?. Two WTGs are equivalent if they
compute the same weighted forest. Since produc-
tions of weight 0 are useless, we often omit them.
Example 3. For the WTG Gex of Example 2 we
display a derivation with weight 0.18 for the sen-
tence ?the man laughs? in Figure 2.
The notion of inside weights (Lari and Young,
1990) is well-established, and Maletti and Satta
3
qs ??1G
S
qnp VP
VBD
laughs
??2G
S
NP
DT
the
qn
VP
VBD
laughs
??3G
S
NP
DT
the
NN
man
VP
VBD
laughs
Figure 2: Derivation with weight 1.0 ? 0.6 ? 0.3.
(2009) consider them for WTGs. Let us recall the
definition.
Definition 4. The inside weight of state q ? Q is
inG(q) =
?
t?T?
wtG(q, t) .
In Section 6 we demonstrate how to compute
inside weights. Finally, let us introduce WTGs in
normal form. The WTG G is in normal form if
t ? ?(Q) for all its productions q
a
? t in P . The
following theorem was proven by Alexandrakis
and Bozapalidis (1987) as Proposition 1.2.
Theorem 5. For every WTG there exists an
equivalent WTG in normal form.
Example 6. The WTG Gex of Example 2 is not
normalized. To illustrate the normalization step,
we show the normalization of the production ?2,
which is replaced by the following three produc-
tions:
qnp
0.6
? NP(qdt, qn) qdt
1.0
? DT(qt)
qt
1.0
? the .
4 Weighted linear extended tree
transducers
The model discussed in this contribution is an ex-
tension of the classical top-down tree transducer,
which was introduced by Rounds (1970) and
Thatcher (1970). Here we consider a weighted
and extended variant that additionally has regular
look-ahead. The weighted top-down tree trans-
ducer is discussed in (Fu?lo?p and Vogler, 2009),
and extended top-down tree transducers were
studied in (Arnold and Dauchet, 1982; Knight and
Graehl, 2005; Knight, 2007; Graehl et al, 2008;
Graehl et al, 2009). The combination (weighted
extended top-down tree transducer) was recently
investigated by Fu?lo?p et al (2011), who also con-
sidered (weighted) regular look-ahead, which was
first introduced by Engelfriet (1977) in the un-
weighted setting.
Definition 7. A linear extended top-down
tree transducer with full regular look-ahead
(l-XTOPRf ) is a system (S,?,?, s0, G,R) where
? S is a finite set of states,
? ? and ? are alphabets of input and output
symbols, respectively,
? s0 ? S is an initial state,
? G = (Q,?, q0, P ) is a WTG, and
? R is a finite set of weighted rules of the form
`
a
?? r where
? a ? A is the rule weight,
? ` ? S[T?(X)] is the linear left-hand
side,
? ? : var(`)? Q is the look-ahead, and
? r ? T?(S[var(`)]) is the linear right-
hand side.
In the following, let M = (S,?,?, s0, G,R)
be an l-XTOPRf . We assume that the WTG G
contains a state > such that wtG(>, t) = 1 for
every t ? T?. In essence, this state represents
the trivial look-ahead. If ?(x) = > for every
rule `
a
?? r ? R and x ? var(r) (respectively,
x ? var(`)), then M is an l-XTOPR (respectively,
l-XTOP). l-XTOPR and l-XTOP coincide exactly
with the models of Fu?lo?p et al (2011), and in the
latter model we drop the look-ahead component ?
and the WTG G completely.
Example 8. The rules of our running example
l-XTOP Mex (over the input and output alpha-
bet ?, which is also used by the WTG Gex of Ex-
ample 2) are displayed in Figure 3.
Next, we present the semantics. Without loss
of generality, we assume that we can distin-
guish states from input and output symbols (i.e.,
S ? (? ? ?) = ?). A sentential form of M is a
tree of SF(M) = T?(Q[T?]). Let ? = `
a
?? r be
a rule of R. Moreover, let ?, ? ? SF(M) be sen-
tential forms and w ? N? be the lexicographically
smallest position in posQ(?). We write ?
b
?M,? ?
if there exists a substitution ? : X ? T? such that
? ? = ?[`?]w,
? ? = ?[r?]w, and
? b = a ?
?
x?var(`) wtG(?(x), ?(x)).
4
s0
S
NP
x1 x2
VP
x3
? 0.6
S
NP
s1
x1
s2
x2
VP
s3
x3
?
?
? 0.4
S
s1
x1
VP
s3
x3
s2
N
ADJ
x1
x2
? 0.7
N
ADJ
s5
x1
s2
x2
?
?
? 0.3
s2
x2
s1
NP
x1 x2
? 0.5
NP
s1
x1
s2
x2
?
?
? 0.5
s1
x1
s1
DT
the
? 1.0
DT
the
s3
VBD
laughs
? 1.0
VBD
laughs
s2
PP
x1 x2
? 1.0
PP
s4
x1
s1
x2
s2
NN
man /
hill /
telescope
? 1.0
NN
man /
hill /
telescope
s4
PRP
on /
with
? 1.0
PRP
on /
with
Figure 3: Example rules of an l-XTOP. We collapsed rules with the same left-hand side as well as several lexical
items to save space.
s0
S
NP
NP
DT
the
NN
man
PP
PRP
on
NP
DT
the
NN
hill
VP
VBD
laughs
0.4?M
S
s1
NP
DT
the
NN
man
VP
s3
VBD
laughs
0.5?M
S
NP
s1
DT
the
s2
NN
man
VP
s3
VBD
laughs
??M
S
NP
DT
the
NN
man
VP
VBD
laughs
Figure 4: Derivation with weight 0.4 ? 0.5 ? 1.0 (rules omitted).
The tree transformation ?M computed byM is de-
fined by
?M (t, u) =
?
?1,...,?n?R
s0(t)
a1?M,?1 ???
an?M,?nu
a1 ? . . . ? an
for every t ? T? and u ? T?.
Example 9. A sequence of derivation steps of the
l-XTOP Mex is illustrated in Figure 4. The trans-
formation it computes is capable of deleting the
PP child of every NP-node with probability 0.4 as
well as deleting the ADJ child of every N-node
with probability 0.3.
A detailed exposition to unweighted l-XTOPR
is presented by Arnold and Dauchet (1982) and
Graehl et al (2009).
5 The construction
In this section, we present the main construction
of this contribution, in which we will construct a
WTG for the forward application of another WTG
via an l-XTOPR. Let us first introduce the main
notions. Let L : T? ? A be a weighted forest
and ? : T??T? ? A be a weighted tree transfor-
mation. Then the forward application of L via ?
yields the weighted forest ?(L) : T? ? A such
that (?(L))(u) =
?
t?T?
L(t) ? ?(t, u) for ev-
ery u ? T?. In other words, to compute the
weight of u in ?(L), we consider all input trees t
and multiply their weight in L with their trans-
lation weight to u. The sum of all those prod-
ucts yields the weight for u in ?(L). In the par-
ticular setting considered in this contribution, the
weighted forest L is computed by a WTG and the
weighted tree transformation ? is computed by an
l-XTOPR. The question is whether the resulting
weighted forest ?(L) can be computed by a WTG.
Our approach to answer this question con-
sists of three steps: (i) composition, (ii) nor-
malization, and (iii) range projection, which
we address in separate sections. Our input is
5
qs ?
S
qnp qvp
?
S
NP
qnp qpp
qvp ?2
S
NP
qnp qpp
VP
VBD
qv
qs ?
S
qnp qvp
?
S
NP
qdt qn
qvp ?2
S
NP
qdt qn
VP
VBD
qv
Figure 5: Two derivations (without production and
grammar decoration) with weight 0.4 [top] and
0.6 [bottom] of the normalized version of the
WTG Gex (see Example 10).
the WTG G? = (Q?,?, q?0, P
?), which com-
putes the weighted forest L = LG? , and
the l-XTOPR M = (S,?,?, s0, G,R) with
G = (Q,?, q0, P ), which computes the weighted
tree transformation ? = ?M . Without loss of gen-
erality, we suppose thatG andG? contain a special
state > such that wtG(>, t) = wtG?(>, t) = 1
for all t ? T?. Moreover, we assume that the
WTG G? is in normal form. Finally, we assume
that s0 is separated, which means that the initial
state of M does not occur in any right-hand side.
Our example l-XTOP Mex has this property. All
these restrictions can be assumed without loss of
generality. Finally, for every state s ? S, we let
Rs = {`
a
?? r ? R | `(?) = s} .
5.1 Composition
We combine the WTG G? and the l-XTOPR M
into a single l-XTOPRf M
? that computes
?M ?(t, u) = LG?(t) ? ?M (t, u) = L(t) ? ?(t, u)
for every t ? T? and u ? T?. To this end, we
construct
M ? = (S,?,?, s0, G?G
?, (R \Rs0) ?R
?)
such that G ? G? is the classical product WTG
[see Proposition 5.1 of (Berstel and Reutenauer,
1982)] and for every rule `
a
?? r in Rs0 and
? : var(`)? Q?, the rule
`
a?wtG? (q
?
0,`?)??????????? r
is in R?, where ??(x) = ??(x), ?(x)? for every
x ? var(`).
Example 10. Let us illustrate the construction on
the WTG Gex of Example 2 and the l-XTOP Mex
of Example 8. According to our assumptions,
Gex should first be normalized (see Theorem 5).
We have two rules in Rs0 and they have the same
left-hand side `. It can be determined easily that
wtG?ex(qs, `?) 6= 0 only if
? ?(x1)?(x2)?(x3) = qnpqppqv or
? ?(x1)?(x2)?(x3) = qdtqnqv.
Figure 5 shows the two corresponding derivations
and their weights. Thus, the s0-rules are replaced
by the 4 rules displayed in Figure 6.
Theorem 11. For every t ? T? and u ? T?, we
have ?M ?(t, u) = L(t) ? ?(t, u).
Proof. We prove an intermediate property for
each derivation of M . Let
s0(t)
b1?M,?1 ? ? ?
bn?M,?n u
be a derivation of M . Let ?1 = `
a1?? r be the
first rule, which trivially must be in Rs0 . Then for
every ? : var(`)? Q?, there exists a derivation
s0(t)
c1?M ?,??1 ?2
b2?M ?,?2 ? ? ?
bn?M ?,?n u
in M ? such that
c1 = b1?wtG?(q
?
0, `?)?
?
x?var(`)
wtG?(?(x), ?
?(x)) ,
where ?? : var(`) ? T? is such that t = `??.
Since we sum over all such derivations and
?
? : var(`)?Q?
wtG?(q
?
0, `?) ?
?
x?var(`)
wtG?(?(x), ?
?(x))
= wtG?(q
?
0, t) = LG?(t)
by a straightforward extension of Lemma 4.1.8
of (Borchardt, 2005), we obtain that the deriva-
tions in M ? sum to LG?(t) ? b1 ? . . . ? bn as desired.
The main property follows trivially from the in-
termediate result.
5.2 Normalization
Currently, the weights of the input WTG are
only on the initial rules and in its look-ahead.
Next, we use essentially the same method as
in the previous section to remove the look-
ahead from all variables that are not deleted.
Let M ? = (S,?,?, s0, G ? G?, R) be the
l-XTOPRf constructed in the previous section and
6
s0
S
NP
x1 x2
VP
x3
??
0.6 ? c
S
NP
s1
x1
s2
x2
VP
s3
x3
?
?
?
0.4 ? c
S
s1
x1
VP
s3
x3
Figure 6: 4 new l-XTOPRf rules, where ? and c are
either (i) ?(x1)?(x2)?(x3) = qnpqppqv and c = 0.4
or (ii) ?(x1)?(x2)?(x3) = qdtqnqv and c = 0.6 (see
Example 10).
s0
S
NP
x1 x2
VP
x3
??
0.4 ? 0.4
S
?s1, qnp?
x1
VP
?s3, qv?
x3
?
?
?
0.4 ? 0.6
S
?s1, qdt?
x1
VP
?s3, qv?
x3
Figure 7: New l-XTOPR rules, where ?(x2) = qpp
[left] and ?(x2) = qn [right] (see Figure 6).
? = `
a
?? r ? R be a rule with ?(x) = ?>, q??
for some q? ? Q? \ {>} and x ? var(r). Note
that ?(x) = ?>, q?? for some q? ? Q? for all
x ? var(r) since M is an l-XTOPR. Then we
construct the l-XTOPRf M
??
(S ? S ?Q?,?,?, s0, G?G
?, (R \ {?}) ?R?)
such that R? contains the rule `
a
??? r?, where
??(x?) =
{
?>,>? if x = x?
?(x?) otherwise
for all x? ? var(`) and r? is obtained from r by re-
placing the subtree s(x) with s ? S by ?s, q??(x).
Additionally, for every rule `??
a??
???? r?? in Rs and
? : var(`??)? Q?, the rule
`??
a???wtG? (q
?,`???)
?????????????? r
??
is in R?, where ????(x) = ????(x), ?(x)? for ev-
ery x ? var(`). This procedure is iterated until
we obtain an l-XTOPR M ??. Clearly, the iteration
must terminate since we do not change the rule
shape, which yields that the size of the potential
rule set is bounded.
Theorem 12. The l-XTOPR M ?? and the
l-XTOPRf M
? are equivalent.
?s2, qn?
N
ADJ
x1
x2
??
0.32 ? 0.5
?s2, qn?
x2
?s1, qnp?
NP
x1 x2
???|???
0.5 ? 0.4
?s1, qnp?
x1
?
?
?
0.5 ? 0.6
?s1, qdt?
x1
Figure 8: New l-XTOPR rules, where ?(x1) is either
qold or qyoung , ??(x2) = qpp, and ???(x2) = qn.
Proof. It can be proved that the l-XTOPRf con-
structed after each iteration is equivalent to its
input l-XTOPRf in the same fashion as in Theo-
rem 11 with the only difference that the rule re-
placement now occurs anywhere in the derivation
(not necessarily at the beginning) and potentially
several times. Consequently, the finally obtained
l-XTOPR M ?? is equivalent to M ?.
Example 13. Let us reconsider the l-XTOPRf con-
structed in the previous section and apply the nor-
malization step. The interesting rules (i.e., those
rules l
a
?? r where var(r) 6= var(l)) are dis-
played in Figures 7 and 8.
5.3 Range projection
We now have an l-XTOPR M ?? with rules R??
computing ?M ??(t, u) = L(t) ? ?(t, u). In the fi-
nal step, we simply disregard the input and project
to the output. Formally, we want to construct a
WTG G?? such that
LG??(u) =
?
t?T?
?M ??(t, u) =
?
t?T?
L(t) ? ?(t, u)
for every u ? T?. Let us suppose that G is the
WTG inside M ??. Recall that the inside weight of
state q ? Q is
inG(q) =
?
t?T?
wtG(q, t) .
We construct the WTG
G?? = (S ? S ?Q?,?, s0, P
??)
such that `(?)
c
? r? is in P ?? for every rule
`
a
?? r ? R??, where
c = a ?
?
x?var(`)\var(r)
inG(?(x))
and r? is obtained from r by removing the vari-
ables of X . If the same production is constructed
from several rules, then we add the weights. Note
that the WTG G?? can be effectively computed if
inG(q) is computable for every state q.
7
qs qprp
qnp qn qadj
Figure 9: Dependency graph of the WTG Gex.
Theorem 14. For every u ? T?, we have
LG??(u) =
?
t?T?
L(t) ? ?(t, u) = (?(L))(u) .
Example 15. The WTG productions for the rules
of Figures 7 and 8 are
s0
0.4?0.4
? S(?s1, qnp?,VP(?s3, qv?))
s0
0.4?0.6
? S(?s1, qdt?,VP(?s3, qv?))
?s2, qn?
0.3?0.3
? ?s2, qn?
?s1, qnp?
0.5?0.4
? ?s1, qnp?
?s1, qnp?
0.5?0.6
? ?s1, qdt? .
Note that all inside weights are 1 in our exam-
ple. The first production uses the inside weight
of qpp, whereas the second production uses the in-
side weight of qn. Note that the third production
can be constructed twice.
6 Computation of inside weights
In this section, we address how to effectively com-
pute the inside weight for every state. If the WTG
G = (Q,?, q0, P ) permits only finitely many
derivations, then for every q ? Q, the inside
weight inG(q) can be computed according to Def-
inition 4 because wtG(q, t) = 0 for almost all
t ? T?. If P contains (useful) recursive rules,
then this approach does not work anymore. Our
WTG Gex of Example 2 has the following two re-
cursive rules:
qnp
0.4
? NP(qnp,PP(qprp, qnp)) (?4)
qn
0.3
? N(qadj , qn) . (?5)
The dependency graph of Gex, which is shown in
Figure 9, has cycles, which yields that Gex per-
mits infinitely many derivations. Due to the com-
pleteness of the semiring, even the infinite sum of
Definition 4 is well-defined, but we still have to
compute it. We will present two simple methods
to achieve this: (a) an analytic method and (b) an
approximation in the next sections.
6.1 Analytic computation
In simple cases we can compute the inside weight
using the stars a?, which we defined in Section 2.
Let us first list some interesting countably com-
plete semirings for NLP applications and their
corresponding stars.
? Probabilities: (R??0,+, ?, 0, 1) where R
?
?0
contains all nonnegative real numbers
and ?, which is bigger than every real
number. For every a ? R??0 we have
a? =
{
1
1?a if 0 ? a < 1
? otherwise
? VITERBI: ([0, 1],max, ?, 0, 1) where [0, 1] is
the (inclusive) interval of real numbers be-
tween 0 and 1. For every 0 ? a ? 1 we have
a? = 1.
? Tropical: (R??0,min,+,?, 0) where
a? = 0 for every a ? R??0.
? Tree unification: (2T?(X1),?,unionsq, ?, {x1})
where 2T?(X1) = {L | L ? T?(X1)} and
unionsq is unification (where different occurrences
of x1 can be replaced differently) extended
to sets as usual. For every L ? T?(Xk) we
have L? = {x1} ? (L unionsq L).
We can always try to develop a regular expres-
sion (Fu?lo?p and Vogler, 2009) for the weighted
forest recognized by a certain state, in which we
then can drop the actual trees and only compute
with the weights. This is particularly easy if our
WTG has only left- or right-recursive productions
because in this case we obtain classical regular
expressions (for strings). Let us consider produc-
tion ?5. It is right-recursive. On the string level,
we obtain the following unweighted regular ex-
pression for the string language generated by qn:
L(qadj)
?(man | hill | telescope)
where L(qadj) = {old, young} is the set of strings
generated by qadj . Correspondingly, we can de-
rive the inside weight by replacing the generated
string with the weights used to derive them. For
example, the production ?5, which generates the
state qadj , has weight 0.3. We obtain the expres-
sion
inG(qn) = (0.3 ? inG(qadj))
? ? (0.3 + 0.2 + 0.2) .
8
Example 16. If we calculate in the probability
semiring and inG(qadj) = 1, then
inG(qn) =
1
1? 0.3
? (0.3 + 0.2 + 0.2) = 1 ,
as expected (since our productions induce a prob-
ability distribution on all trees generated from
each state).
Example 17. If we calculate in the tropical semi-
ring, then we obtain
inG(qn) = min(0.3, 0.2, 0.2) = 0.2 .
It should be stressed that this method only
allows us to compute inG(q) in very simple
cases (e.g., WTG containing only left- or right-
recursive productions). The production ?4 has
a more complicated recursion, so this simple
method cannot be used for our full example WTG.
However, for extremal semirings the inside
weight always coincides with a particular deriva-
tion. Let us also recall this result. The semiring is
extremal if a+ a? ? {a, a?} for all a, a? ? A. The
VITERBI and the tropical semiring are extremal.
Recall that
inG(q) =
?
t?T?
wtG(q, t)
=
?
t?T?
?
?1,...,?n?P
q?
?1
G ????
?n
G t
wtG(?1 ? ? ? ?n) ,
which yields that inG(q) coincides with the
derivation weight wtG(?1 ? ? ? ?n) of some deriva-
tion q ??1G ? ? ? ?
?n
G t for some t ? T?. In
the VITERBI semiring this is the highest scor-
ing derivation and in the tropical semiring it is
the lowest scoring derivation (mind that in the
VITERBI semiring the production weights are
multiplied in a derivation, whereas they are added
in the tropical semiring). There are efficient algo-
rithms (Viterbi, 1967) that compute those deriva-
tions and their weights.
6.2 Numerical Approximation
Next, we show how to obtain a numerical ap-
proximation of the inside weights (up to any
desired precision) in the probability semiring,
which is the most important of all semirings
discussed here. A similar approach was used
by Stolcke (1995) for context-free grammars. To
keep the presentation simple, let us suppose that
G = (Q,?, q0, P ) is in normal form (see The-
orem 5). The method works just as well in the
general case.
We first observe an important property of the
inside weights. For every state q ? Q
inG(q) =
?
q
a
??(q1,...,qn)?P
a ? inG(q1) ? . . . ? inG(qn) ,
which can trivially be understood as a system of
equations (where each inG(q) with q ? Q is a
variable). Since there is one such equation for
each variable inG(q) with q ? Q, we have a
system of |Q| non-linear polynomial equations in
|Q| variables.
Several methods to solve non-linear systems of
equations are known in the numerical calculus lit-
erature. For example, the NEWTON-RAPHSON
method allows us to iteratively compute the roots
of any differentiable real-valued function, which
can be used to solve our system of equations be-
cause we can compute the JACOBI matrix for our
system of equations easily. Given a good starting
point, the NEWTON-RAPHSON method assures
quadratic convergence to a root. A good start-
ing point can be obtained, for example, by bisec-
tion (Corliss, 1977). Another popular root-finding
approximation is described by Brent (1973).
Example 18. For the WTG of Example 2 we ob-
tain the following system of equations:
inG(qs) = 1.0 ? inG(qnp)
inG(qnp) = 0.4 ? inG(qnp) ? inG(qprp) ? inG(qnp)
+ 0.6 ? inG(qn)
inG(qn) = 0.3 ? inG(qadj) ? inG(qn)
+ 0.3 + 0.2 + 0.2
inG(qadj) = 0.5 + 0.5
inG(qprp) = 0.5 + 0.5 .
Together with inG(qn) = 1, which we already
calculated in Example 16, the only interesting
value is
inG(qs) = inG(qnp) = 0.4 ? inG(qnp)
2 + 0.6 ,
which yields the roots inG(qnp) = 1 and
inG(qnp) = 1.5. The former is the desired solu-
tion. As before, this is the expected solution.
9
References
Athanasios Alexandrakis and Symeon Bozapalidis.
1987. Weighted grammars and Kleene?s theorem.
Inf. Process. Lett., 24(1):1?4.
Andre? Arnold and Max Dauchet. 1982. Morphismes
et bimorphismes d?arbres. Theoret. Comput. Sci.,
20(1):33?93.
Jean Berstel and Christophe Reutenauer. 1982. Rec-
ognizable formal power series on trees. Theoret.
Comput. Sci., 18(2):115?148.
Bjo?rn Borchardt. 2005. The Theory of Recognizable
Tree Series. Ph.D. thesis, Technische Universita?t
Dresden.
Richard P. Brent. 1973. Algorithms for Minimization
without Derivatives. Series in Automatic Computa-
tion. Prentice Hall, Englewood Cliffs, NJ, USA.
George Corliss. 1977. Which root does the bisection
algorithm find? SIAM Review, 19(2):325?327.
Samuel Eilenberg. 1974. Automata, Languages, and
Machines ? Volume A, volume 59 of Pure and Ap-
plied Math. Academic Press.
Joost Engelfriet. 1975. Bottom-up and top-down tree
transformations ? a comparison. Math. Systems
Theory, 9(3):198?231.
Joost Engelfriet. 1977. Top-down tree transducers
with regular look-ahead. Math. Systems Theory,
10(1):289?303.
Zolta?n Fu?lo?p and Heiko Vogler. 2009. Weighted tree
automata and tree transducers. In Manfred Droste,
Werner Kuich, and Heiko Vogler, editors, Hand-
book of Weighted Automata, EATCS Monographs
on Theoret. Comput. Sci., chapter 9, pages 313?
403. Springer.
Zolta?n Fu?lo?p, Andreas Maletti, and Heiko Vogler.
2010. Preservation of recognizability for syn-
chronous tree substitution grammars. In Proc. 1st
Workshop Applications of Tree Automata in Natu-
ral Language Processing, pages 1?9. Association
for Computational Linguistics.
Zolta?n Fu?lo?p, Andreas Maletti, and Heiko Vogler.
2011. Weighted extended tree transducers. Fun-
dam. Inform., 111(2):163?202.
Jonathan S. Golan. 1999. Semirings and their Appli-
cations. Kluwer Academic, Dordrecht.
Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Comput. Linguist.,
34(3):391?427.
Jonathan Graehl, Mark Hopkins, Kevin Knight, and
Andreas Maletti. 2009. The power of extended
top-down tree transducers. SIAM J. Comput.,
39(2):410?430.
Udo Hebisch and Hanns J. Weinert. 1998. Semirings
? Algebraic Theory and Applications in Computer
Science. World Scientific.
Georg Karner. 2004. Continuous monoids and semi-
rings. Theoret. Comput. Sci., 318(3):355?372.
Kevin Knight and Jonathan Graehl. 2005. An over-
view of probabilistic tree transducers for natural
language processing. In Proc. 6th Int. Conf. Com-
putational Linguistics and Intelligent Text Process-
ing, volume 3406 of LNCS, pages 1?24. Springer.
Kevin Knight. 2007. Capturing practical natural
language transformations. Machine Translation,
21(2):121?133.
Karim Lari and Steve J. Young. 1990. The esti-
mation of stochastic context-free grammars using
the inside-outside algorithm. Computer Speech and
Language, 4(1):35?56.
Andreas Maletti and Giorgio Satta. 2009. Parsing al-
gorithms based on tree automata. In Proc. 11th Int.
Workshop Parsing Technologies, pages 1?12. Asso-
ciation for Computational Linguistics.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. 46th Ann. Meeting of
the ACL, pages 192?199. Association for Computa-
tional Linguistics.
William C. Rounds. 1970. Mappings and grammars
on trees. Math. Systems Theory, 4(3):257?287.
Andreas Stolcke. 1995. An efficient probabilistic
context-free parsing algorithm that computes prefix
probabilities. Comput. Linguist., 21(2):165?201.
James W. Thatcher. 1970. Generalized2 sequential
machine maps. J. Comput. System Sci., 4(4):339?
367.
Andrew J. Viterbi. 1967. Error bounds for convo-
lutional codes and an asymptotically optimum de-
coding algorithm. IEEE Trans. Inform. Theory,
13(2):260?269.
10
