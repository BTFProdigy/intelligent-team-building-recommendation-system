Coling 2010: Poster Volume, pages 739?747,
Beijing, August 2010
Visually and Phonologically Similar Characters in Incorrect 
Simplified Chinese Words 
Chao-Lin Liu? Min-Hua Lai? Yi-Hsuan Chuang? Chia-Ying Lee?
???Department of Computer Science; ??Center for Mind, Brain, and Learning 
National Chengchi University 
?Institute of Linguistics, Academia Sinica 
{?chaolin, ?g9523, ?g9804}@cs.nccu.edu.tw, ?chiaying@gate.sinica.edu.tw 
Abstract
Visually and phonologically similar cha-
racters are major contributing factors for 
errors in Chinese text. By defining ap-
propriate similarity measures that consid-
er extended Cangjie codes, we can identi-
fy visually similar characters within a 
fraction of a second. Relying on the pro-
nunciation information noted for individ-
ual characters in Chinese lexicons, we 
can compute a list of characters that are 
phonologically similar to a given charac-
ter. We collected 621 incorrect Chinese 
words reported on the Internet, and ana-
lyzed the causes of these errors. 83% of 
these errors were related to phonological 
similarity, and 48% of them were related 
to visual similarity between the involved 
characters. Generating the lists of phono-
logically and visually similar characters, 
our programs were able to contain more 
than 90% of the incorrect characters in 
the reported errors. 
1 Introduction 
In this paper, we report the experience of our 
studying the errors in simplified Chinese words. 
Chinese words consist of individual characters. 
Some words contain just one character, but most 
words comprise two or more characters. For in-
stance, ??? (mai4)1 has just one character, and 
???? (yu3 yan2) is formed by two characters. 
Two most common causes for writing or typing 
incorrect Chinese words are due to visual and 
phonological similarity between the correct and 
1 We show simplified Chinese characters followed by 
their Hanyu pinyin. The digit that follows the symbols 
for the sound is the tone for the character. 
the incorrect characters. For instance, one might 
use ??? (hwa2) in the place of ???(hwa4)  in 
?????? (ke1 hwa4 xing2 xiang4) partially 
because of phonological similarity; one might 
replace ??? (zhuo2) in ?????? (xin1 lao2 
li4 zhuo2) with ??? (chu4) partially due to visu-
al similarity. (We do not claim that the visual or 
phonological similarity alone can explain the 
observed errors.) 
Similar characters are important for under-
standing the errors in both traditional and simpli-
fied Chinese. Liu et al (2009a-c) applied tech-
niques for manipulating correctness of Chinese 
words to computer assisted test-item generation. 
Research in psycholinguistics has shown that the 
number of neighbor characters influences the 
timing of activating the mental lexicon during the 
process of understanding Chinese text (Kuo et al 
2004; Lee et al 2006).  Having a way to compute 
and find similar characters will facilitate the 
process of finding neighbor words, so can be in-
strumental for related studies in psycholinguistics. 
Algorithms for optical character recognition for 
Chinese and for recognizing written Chinese try 
to guess the input characters based on sets of 
confusing sets (Fan et al 1995; Liu et al, 2004). 
The confusing sets happen to be hand-crafted 
clusters of visually similar characters. 
It is relatively easy to judge whether two cha-
racters have similar pronunciations based on 
their records in a given Chinese lexicon. We will 
discuss more related issues shortly.  
To determine whether two characters are vi-
sually similar is not as easy. Image processing 
techniques may be useful but is not perfectly 
feasible, given that there are more than fifty 
thousand Chinese characters (HanDict, 2010) 
and that many of them are similar to each other 
in special ways.  Liu et al (2008) extend the 
Cangjie codes (Cangjie, 2010; Chu, 2010) to en-
code the layouts and details about traditional 
739
Chinese characters for computing visually simi-
lar characters. Evidence observed in psycholin-
guistic studies offers a cognition-based support 
for the design of Liu et al?s approach (Yeh and 
Li, 2002). In addition, the proposed method 
proves to be effective in capturing incorrect tra-
ditional Chinese words (Liu et al, 2009a-c). 
In this paper, we work on the errors in simpli-
fied Chinese words by extending the Cangjie 
codes for simplified Chinese. We obtain two lists 
of incorrect words that were reported on the In-
ternet, analyze the major reasons that contribute 
to the observed errors, and evaluate how the new 
Cangjie codes help us spot the incorrect charac-
ters. Results of our analysis show that phonolog-
ical and visual similarities contribute similar por-
tions of errors in simplified and traditional Chi-
nese. Experimental results also show that, we can 
catch more than 90% of the reported errors. 
We go over some issues about phonological 
similarity in Section 2, elaborate how we extend 
and apply Cangjie codes for simplified Chinese 
in Section 3, present details about our experi-
ments and observations in Section 4, and discuss 
some technical issues in Section 5.  
2 Phonologically Similar Characters 
The pronunciation of a Chinese character in-
volves a sound, which consists of the nucleus and 
an optional onset, and a tone. In Mandarin Chi-
nese, there are four tones. (Some researchers in-
clude the fifth tone.) 
In our work, we consider four categories of 
phonological similarity between two characters: 
same sound and same tone (SS), same sound and 
different tone (SD), similar sound and same tone 
(MS), and similar sound and different tone (MD).  
We rely on the information provided in a lex-
icon (Dict, 2010) to determine whether two cha-
racters have the same sound or the same tone. 
The judgment of whether two characters have 
similar sound should consider the language expe-
rience of an individual. One who live in the 
southern and one who live in the northern China 
may have quite different perceptions of ?similar? 
sound. In this work, we resort to the confusion 
sets observed in a psycholinguistic study con-
ducted at the Academic Sinica. 
Some Chinese characters are heteronyms. Let 
C1 and C2 be two characters that have multiple 
pronunciations. If C1 and C2 share one of their 
pronunciations, we consider that C1 and C2 be-
long to the SS category. This principle applies 
when we consider phonological similarity in oth-
er categories. 
One challenge in defining similarity between 
characters is that the pronunciations of a charac-
ter can depend on its context. The most common 
example of tone sandhi in Chinese (Chen, 2000) 
is that the first third-tone character in words 
formed by two adjacent third-tone characters will 
be pronounced in the second tone. At present, we 
ignore the influences of context when determin-
ing whether two characters are phonologically 
similar.  
Although we have confined our definition of 
phonological similarity to the context of the 
Mandarin Chinese, it is important to note the in-
fluence of sublanguages within the Chinese lan-
guage family will affect the perception of phono-
logical similarity. Sublanguages used in different 
areas in China, e.g., Shanghai, Min, and Canton 
share the same written forms with the Mandarin 
Chinese, but have quite different though related 
pronunciation systems. Hence, people living in 
different areas in China may perceive phonologi-
cal similarity in very different ways. The study in 
this direction is beyond the scope of the current 
study.  
3 Visually Similar Characters 
Figure 1 shows four groups of visually similar 
characters. Characters in group 1 and group 2 
differ subtly at the stroke level. Characters in 
group 3 share the components on their right sides. 
The shared component of the characters in group 
4 appears at different places within the characters. 
Radicals are used in Chinese dictionaries to 
organize characters, so are useful for finding vi-
sually similar characters. The characters in group 
1 and group 2 belong to the radicals ??? and ? ?,
respectively. Notice that, although the radical for 
group 2 is clear, the radical for group 1 is not 
obvious because ??? is not a standalone compo-
nent.
However, the shared components might not be 
the radicals of characters. The shared compo-
nents in groups 3 and 4 are not the radicals. In 
Figure 1. Examples of visually similar characters
740
many cases, radicals are semantic components of 
Chinese characters. In groups 3 and 4, the shared 
components carry information about the pronun-
ciations of the characters. Hence, those charac-
ters are listed under different radicals, though 
they do look similar in some ways.  
Hence, a mechanism other than just relying on 
information about characters in typical lexicons 
is necessary, and we will use the extended Cang-
jie codes for finding visually similar characters. 
3.1 Cangjie Codes for Simplified Chinese 
Table 1 shows the Cangjie codes for the 13 
characters listed in Figure 1 and five other 
characters. The ?ID? column shows the 
identification number for the characters, and we 
will refer to the ith character by ci, where i is the 
ID. The ?CC? column shows the Chinese 
characters, and the ?Cangjie? column shows the 
Cangjie codes. Each symbol in the Cangjie codes 
corresponds to a key on the keyboard, e.g. ???
and ? ? ? collocate with ?W? and ?L?, 
respectively. Information about the complete 
correspondence is available on the Wikipedia2.
Using the Cangjie codes saves us from using 
image processing methods to determine the de-
grees of similarity between characters. Take the 
Cangjie codes for the characters in group 2 (c5, c6,
and c7) for example. It is possible to find that the 
characters share a common component, based on 
the shared substrings of the Cangjie codes, i.e., 
????.  Using the common substring  (shown in 
black bold) of the Cangjie codes, we may also 
find the shared component ??? for characters in 
group 3 (c10, c11, and c12), the shared component 
??? in c13 and c14, the shared component ??? in 
c15 and c16, and the shared component ? ? in c16
and c17.
 Despite the perceivable advantages, these 
original Cangjie codes are not good enough. In 
order to maintain efficiency in inputting Chinese 
characters, the Cangjie codes have been limited 
to no more than five keys. Thus, users of the 
Cangjie input method must familiarize them-
selves with the principles for simplifying the 
Cangjie codes. While the simplified codes help 
the input efficiency, they also introduce difficul-
ties and ambiguities when we compare the Cang-
2en.wikipedia.org/wiki/Cangjie_input_method#Keyboard_la
yout ; last visited on 22 April 2010. 
jie codes for computing similar characters. The 
prefix ???? in c16 and c17 can represent ? ?, 
??? (e.g., c8), and ??? (e.g., c9). Characters 
whose Cangjie codes include ???? may con-
tain any of these three components, but they do 
not really look alike. 
Therefore, we augment the original Cangjie 
codes by using the complete Cangjie codes and 
annotate each Chinese character with a layout 
identification that encodes the overall contours of 
the characters. This is how Liu and his col-
leagues (2008) did for the Cangjie codes for tra-
ditional Chinese characters, and we employ a 
similar exploration for the simplified Chinese. 
3.2 Augmenting the Cangjie Codes 
Figure 2 shows the twelve possible layouts that 
are considered for the Cangjie codes for 
simplified Chinese characters. Some of the 
layouts contain smaller areas, and the rectangles 
show a subarea within a character. The smaller 
areas are assigned IDs between one and three. 
Notice that, to maintain read-ability of the 
figures, not all IDs for subareas are shown in 
Figure 2. An example character is provided 
below each layout. From left to right and from 
top to bottom, each layout is assigned an 
identification number from 1 to 12. For example, 
the layout ID of ??? is 8. ??? has two parts, i.e., 
??? and ???.
Researchers have come up with other ways to 
ID CC Cangjie ID CC Cangjie 
1 ? ?! 10 ?! ????!
2 ? ??! 11 ?! ???!
3 ? ??! 12 ?! ???!
4 ? ???! 13 ?! ???!
5 ? ????! 14 ?! ????!
6 ? ????! 15 ?! ????!
7 ? ???! 16 ?! ????!
8 ? ???? 17 ?! ?????!
9 ? ???? 18 ?! ?????!
Table 1. Examples of Cangjie codes 
Figure 2. Layouts of Chinese characters 
741
decompose individual Chinese characters. The 
Chinese Document Lab at the Academia Sinica 
proposed a system with 13 operators for describ-
ing the relationships among components in Chi-
nese characters (CDL, 2010). Lee (2010b) pro-
pose more than 30 possible layouts.  
The layout of a character affects how people 
perceive visual similarity between characters. 
For instance, c16 in Table 1 is more similar to c17
than to c18, although they share ? ?. We rely on 
the expertise in Cangjie codes reported in (Lee, 
2010a) to split the codes into parts. 
Table 2 shows the extended codes for some 
characters listed in Table 1. The ?ID? column 
provides links between the characters listed in 
both Table 1 and Table 2. The ?CC? column 
shows the Chinese characters. The ?LID? column 
shows the identifications for the layouts of the 
characters. The columns with headings ?P1?, 
?P2?, and ?P3? show the extended Cangjie codes, 
where ?Pi? shows the ith part of the Cangjie 
codes, as indicated in Figure 2. 
We decide the extended codes for the parts 
with the help of computer programs and subjec-
tive judgments. Starting from the original Cang-
jie codes, we can compute the most frequent sub-
strings just like we can compute the frequencies 
of n-grams in corpora (cf. Jurafsky and Martin, 
2009). Computing the most common substrings 
in the original codes is not a complex task be-
cause the longest original Cangjie codes contain 
just five symbols.   
Often, the frequent substrings are simplified 
codes for popular components in Chinese charac-
ters, e.g., ? ? and ? ?. The original codes for ? ?
and ? ? are ????? and ?????, but they are 
often simplified to ???? and ????, respec-
tively.  When simplified, ? ? have the same 
Cangjie code with ???, and ? ? have the same 
Cangjie code with ??? and ???.
After finding the frequent substrings, we veri-
fy whether these frequent substrings are simpli-
fied codes for meaningful components. For mea-
ningful components, we replace the simplified 
codes with complete codes. For instance the 
Cangjie codes for ??? and ??? are extended to 
include ??? in Table 2, where we indicate the 
extended keys that did not belong to the original 
Cangjie codes in boldface and with a surrounding 
box. Most of the non-meaningful frequent sub-
strings have two keys: one is the last key of a 
part, and the other is the first key of another part. 
They were by observed by coincidence. 
Although most of the examples provided in 
Table 2 indicate that we expand only the first 
part of the Cangjie codes, it is absolutely possible 
that the other parts, i.e., P2 and P3, may need to 
be extended too. c19 shows such an example. 
Replacing simplified codes with complete 
codes not only help us avoid incorrect matches 
but also help us find matches that would be 
missed due to simplification of Cangjie codes. 
Using just the original Cangjie codes in Table 1, 
it is not easy to determine that c18 (???) in Table 
1 shares a component (? ?) with c16 and c17 (???
and ???). In contrast, there is a chance to find 
the similarity with the extended Cangjie codes in 
Table 2, given that all of the three Cangjie codes 
include ?????.
We can see an application of the LIDs, using 
???, ??? and ??? as an example. Consider the 
case that we want to determine which of ???
and ??? is more similar to ???. Their extended 
Cangjie codes will indicate that ??? is the an-
swer to this question for two reasons. First, ???
and ??? belong to the same type of layout; and, 
second, the shared components reside at the same 
area in ??? and ???.
3.3 Similarity Measures 
The main differences between the original and 
the extended Cangjie codes are the degrees of 
details about the structures of the Chinese cha-
racters. By recovering the details that were ig-
nored in the original codes, our programs will be 
ID CC LID P1 P2 P3
5 ? 2 ???! ??! !
6 ? 2 ???! ??! !
7 ? 2 ???! ?! !
10 ? 10 ??! ?! ?!
11 ? 10 ?! ?! ?!
12 ? 10 ?! ?! ?!
13 ? 5 ?! ??! !
14 ? 9 ?! ?! ??!
15 ? 2 ???! ??! !
16 ? 2 ???! ??! !
17 ? 2 ???! ???! !
18 ? 3 ???! ??! ?!
19 ? 4 ?! ???! ??!
Table 2. Examples of extended Cangjie codes 
742
better equipped to find the similarity between 
characters.  
In the current study, we experiment with three 
different scoring methods to measure the visual 
similarity between two characters based on their 
extended Cangjie codes. Two of these methods 
had been tried by Liu and his colleagues? study 
for traditional Chinese characters (Liu et al, 
2009b-c). The first method, denoted SC1, con-
siders the total number of matched keys in the 
matched parts (without considering their part 
IDs). Let ci denote the i
th character listed in Table 
2. We have SC1(c15, c16) = 2 because of the 
matched ????. Analogously, we have SC1(c19,
c16) = 2.  
The second method, denoted SC2, includes 
the score of SC1 and considers the following 
conditions: (1) add one point if the matched parts 
locate at the same place in the characters and (2) 
if the first condition is met, an extra point will be 
added if the characters belong to the same layout.  
Hence, we have SC2(c15, c16) =SC1(c15,
c16)+1+1=4 because (1) the matched ???? lo-
cate at P2 in both characters and (2) c15 and c16
belong to the same layout. Assuming that c16 be-
longs to layout 5, than SC2(c15, c16) would be-
come 3. In contrast, we have SC2(c19, c16)=2. No 
extra weights for the matching ???? because it 
locates at different parts in the characters. The 
extra weight considers the spatial influences of 
the matched parts on the perception of similarity. 
While splitting the extended Cangjie codes in-
to parts allows us to tell that c15 is more similar 
to c16 than to c19, it also creates a new barrier in 
computing similarity scores. An example of this 
problem is that SC2(c17, c18)=0. This is because 
that ????? at P1 in c17 can match neither ??
?? at P2 nor ??? at P3 in c18.
To alleviate this problem, we consider SC3 
which computes the similarity in three steps. 
First, we concatenate the parts of a Cangjie code 
for a character. Then, we compute the longest 
common subsequence (LCS) (cf. Cormen et al, 
2009) of the concatenated codes of the two cha-
racters being compared, and compute a Dice?s 
coefficient (cf. Croft et al, 2010) as the similari-
ty. Let X and Y denote the concatenated, ex-
tended Cangjie codes for two characters, and let 
Z be the LCS of X and Y. The similarity is de-
fined by the following equation.  
S
YX
Z
DiceLCS stringoflength theisS where,
2

u
   (1) 
We compute another Dice?s coefficient be-
tween X and Y. The formula is the similar to (1), 
except that we set Z to the longest common con-
secutive subsequence. We call this score 
LCCSDice . Notice that LCSLCCS DiceDice d ,
1dLCCSDice , and 1dLCSDice  . Finally, SC3 of two 
characters is the sum of their SC2, LCCSDiceu10 ,
and LCSDiceu5 . We multiply the Dice?s coeffi-
cients with constants to make them as influential 
as the SC2 component in SC3. The constants 
were not scientifically chosen, but were selected 
heuristically. 
4 Error Analysis and Evaluation 
We evaluate the effectiveness of using the pho-
nologically and visually similar characters to 
captures errors in simplified Chinese words with 
two lists of reported errors that were collected 
from the Internet.  
4.1 Data Sources 
We need two types of data for the experiments. 
The information about the pronunciation and 
structures of the Chinese characters help us gen-
erate lists of similar characters. We also need 
reported errors so that we can evaluate whether 
the similar characters catch the reported errors. 
A lexicon that provides the pronunciation in-
formation about Chinese characters and a data-
base that contains the extended Cangjie codes are 
necessary for our programs to generate lists of 
characters that are phonologically and visually 
similar to a given character. 
It is not difficult to acquire lexicons that show 
standard pronunciations for Chinese characters. 
As we stated in Section 2, the main problem is 
that it is not easy to predict how people in differ-
ent areas in China actually pronounce the charac-
ters. Hence, we can only rely on the standards 
that are recorded in lexicons.  
With the procedure reported in Section 3.2, we 
built a database of extended Cangjie codes for 
the simplified Chinese. The database was de-
signed to contain 5401 common characters in the 
BIG5 encoding, which was originally designed 
for the traditional Chinese. After converting the 
traditional Chinese characters to the simplified 
counterparts, the database contained only 5170
743
different characters. 
We searched the Internet for reported errors 
that were collected in real-world scenarios, and 
obtained two lists of errors. The first list3 came 
from the entrance examinations for senior high 
schools in China, and the second list4 contained 
errors observed at senior high schools in China. 
We used 160 and 524 errors from the first and 
the second lists, respectively, and we refer to the 
combined list as the Ilist. An item of reported 
error contained two parts: the correct word and 
the mistaken character, both of which will be 
used in our experiments. 
4.2 Preliminary Data Analysis 
Since our programs can compare the similarity 
only between characters that are included in our 
lexicon, we have to exclude some reported errors 
from the Ilist. As a result, we used only 621 er-
rors in this section.  
Two native speakers subjectively classified the 
causes of these errors into three categories based 
on whether the errors were related to phonologi-
cal similarity, visual similarity, or neither. Since 
the annotators did not always agree on their clas-
sifications, the final results have five interesting 
categories: ?P?, ?V?, ?N?, ?D?, and ?B? in Table 
3. P and V indicate that the annotators agreed on 
the types of errors to be related to phonological 
and visual similarity, respectively. N indicates 
that the annotators believed that the errors were 
not due to phonological or visual similarity. D 
indicates that the annotators believed that the 
errors were due to phonological or visual similar-
ity, but they did not have a consensus. B indi-
cates the intersection of P and V.  
Table 3 shows the percentages of errors in 
these categories. To get 100% from the table, we 
can add up P, V, N, and D, and subtract B from 
the total. In reality there are errors of type N, and 
Liu and his colleagues (2009b) reported this type 
of errors. Errors in this category happened to be 
missing in the Ilist. Based on our and Liu?s ob-
3www.0668edu.com/soft/4/12/95/2008/2008091357140.htm
 ; last visited on 22 April 2010. 
4 gaozhong.kt5u.com/soft/2/38018.html; last visited on 22 
April 2010. 
servations, the percentages of phonological and 
visual similarities contribute to the errors in sim-
plified and traditional Chinese words with simi-
lar percentages.  
4.3 Experimental Procedure 
We design and employ the ICCEval procedure 
for the evaluation task. 
At step 1, given the correct word and the cor-
rect character to be intentionally replaced with 
incorrect characters, we created a list of charac-
ters based on the selection criterion. We may 
choose to evaluate phonologically or visually 
similar characters. For a given character, ICCEv-
al can generate characters that are in the SS, SD, 
MS, and MD categories for phonologically simi-
lar characters (cf. Section 2). For visually similar 
characters, ICCEval can select characters based 
on SC1, SC2, and SC3 (cf. Section 3.3). In addi-
tion, ICCEval can generate a list of characters 
that belong to the same radical and have the same 
number of strokes with the correct character. In 
the experimental results, we refer to this type of 
similar characters as RS.
At step 2, for a correct word that people origi-
nally wanted to write, we replaced the correct 
character with an incorrect character with the 
characters that were generated at step 1, submit-
ted the incorrect word to Google AJAX Search 
 P V N D B 
Ilist 83.1 48.3 0 3.7 35.1
Table 3. Percentages of types of errors
Procedure ICCEval
Input:
ccr: the correct character; cwd:
the correct word; crit: the selec-
tion criterion; num: number of re-
quested characters; rnk: the cri-
terion to rank the incorrect 
words;
Output: a list of ranked candidates 
for ccr 
Steps:
1. Generate a list, L, of charac-
ters for ccr with the specified 
criterion, crit. When using SC1, 
SC2, or SC3 to select visually 
similar characters, at most num
characters will be selected. 
2. For each c in L, replace ccr in 
cwd with c, submit the resulting 
incorrect word to Google, and 
record the ENOP. 
3. Rank the list of incorrect words 
generated at step 2, using the 
criterion specified by rnk.
4. Return the ranked list. 
744
API, and extracted the estimated numbers of 
pages (ENOP) 5  that contained the incorrect 
words. In an ordinary interaction with Google, an 
ENOP can be retrieved from the search results, 
and it typically follows the string ?Results 1-
10 of about? on the upper part of the browser 
window. Using the AJAX API, we just have to 
parse the returned results with a simple method.  
Larger ENOPs for incorrect words suggest 
that these words are incorrect words that people 
frequently used on their web pages. Hence, we 
ranked the similar characters based on their 
ENOPs at step 3, and return the list. 
Since the reported errors contained informa-
tion about the incorrect ways to write the correct 
words, we could check whether the real incorrect 
characters were among the similar characters that 
our programs generated at step 1 (inclusion tests). 
We could also check whether the actual incorrect 
characters were ranked higher in the ranked lists 
(ranking tests). 
Take the word ?????? as an example. In 
the collected data, it is reported that people wrote 
this word as ??????, i.e., the second charac-
ter was incorrect. Hoping to capture the error, 
ICCEval generated a list of possible substitutions 
for ??? at step 1. Depending on the categories 
of sources of errors, ICCEval generated a list of 
characters. When aiming to test the effectiveness 
of visually similar characters, we could ask IC-
CEval to apply SC3 to generate a list of alterna-
tives for ???, possibly including ???, ???, 
???, and other candidates. At step 2, we created 
and submitted query strings ??????, ???
???, and ?????? to obtain the ENOPs for 
the candidates. If the ENOPs were, respectively, 
410000, 26100, and 7940, these candidates 
would be returned in the order of ???, ???, and 
???. As a result, the returned list contained the 
actual incorrect character ???, and placed ???
on top of the ranked list. 
Notice that we considered the contexts in 
which the incorrect characters appeared to rank. 
We did not rank the incorrect characters with just 
the unigrams. In addition, although this running 
example shows that we ranked the characters 
directly with the ENOPs, we also ranked the list 
5According to (Croft et al, 2010), the ENOPs may not re-
flect the actual number of pages on the Internet. 
of alternatives with pointwise mutual information: 
 
)Pr()Pr(
)Pr(
,
XC
XC
XCPMI
u
?
 ,                 (2) 
where X is the candidate character to replace the 
correct character and C is the correct word ex-
cluding the correct character to be replaced. To 
compute the score of replacing ??? with ??? in 
??????, X = ???, C=??????, and (C?X)
is ??????. (?!denotes a character to be re-
placed.) PMI is a common tool for judging collo-
cations in natural language processing. (cf. Ju-
rafsky and Martin, 2009). 
It would demand very much computation ef-
fort to find Pr(C). Fortunately, we do not have to 
consider Pr(C) because it is a common denomi-
nator for all incorrect characters. Let X1 and X2
be two competing candidates for the correct cha-
racter. We can ignore Pr(C) because of the fol-
lowing relationship. 
   
)Pr(
)Pr(
)Pr(
)Pr(
,,
2
2
1
1
21 X
XC
X
XC
XCPMIXCPMI
?
t
?
?t
Hence, X1 prevails if  1, XCscore  is larger. 
 
)Pr(
)Pr(
,
X
XC
XCscore
?                    (3) 
In our work, we approximate the probabilities 
used in (3) by the corresponding frequencies that 
we can collect through Google, similar to the 
methods that we used to collect the ENOPs. 
4.4 Experimental Results: Inclusion Tests 
We ran ICCEval with 621 errors in the Ilist. The 
experiments were conducted for all categories of 
phonological and visual similarity. When using 
SS, SD, MS, MD, and RS as the selection crite-
rion, we did not limit the number of candidate 
characters. When using SC1, SC2, and SC3 as 
the criterion, we limited the number candidates 
to be no more than 30. We consider only words 
that the native speakers have consensus over the 
causes of errors. Hence, we dropped those 3.7% 
of words in Table 3, and had just 598 errors. The 
ENOPs were obtained during March and April 
2010. 
Table 4 shows the chances that the lists, gen-
SS SD MS MD Phone
Ilist 82.6 29.3 1.7 1.6 97.3 
SC1 SC2 SC3 RS Visual
Ilist 78.3 71.0 87.7 1.3 90.0 
Table 4. Chances of the recommended list con-
tains the incorrect character
745
erated with different crit at step 1, contained the 
incorrect character in the reported errors. In the 
Ilist, there were 516 and 3006  errors that were 
related to phonological and visual similarity, re-
spectively. Using the characters generated with 
the SS criterion, we captured 426 out of 516 
phone-related errors, so we showed 426/516 = 
82.6% in the table. 
Results in Table 4 show that we captured 
phone-related errors more effectively than visual-
ly-similar errors. With a simple method, we can 
compute the union of the characters that were 
generated with the SS, SD, MS, and MD criteria. 
This integrated list suggested how well we cap-
tured the errors that were related to phones, and 
we show its effectiveness under ?Phone?. Simi-
larly, we integrated the lists generated by SC1, 
SC2, SC3, and RS to explore the effectiveness of 
finding errors that are related to visual similarity, 
and the result is shown under ?Visual?. 
4.5 Experimental Results: Ranking Tests 
To put the generated characters into work, we 
wish to put the actual incorrect character high in 
the ranked list. This will help the efficiency in 
supporting computer assisted test-item writing. 
Having short lists that contain relatively more 
confusing characters may facilitate the data prep-
aration for psycholinguistic studies. 
At step 3, we ranked the candidate characters 
by forming incorrect words with other characters 
in the correct words as the context and submitted 
the words to Google for ENOPs. The results of 
ranking, shown in Table 5, indicate that we may 
just offer the leading five candidates to cover the 
actual incorrect characters in almost all cases.  
The ?Total? column shows the total number of 
errors that were captured by the selection crite-
rion. The column ?Ri? shows the percentage of 
all errors, due to phonological or visual similarity, 
that were re-created and ranked ith at step 3 in 
ICCEVAL. The row headings show the selection 
criteria that were used in the experiments. For 
instance, using SS as the criterion, 70.3% of ac-
tual phone-related errors were rank first, 7.4% of 
the phone-related errors were ranked second, etc. 
If we recommended only 5 leading incorrect cha-
6The sum of 516 and 300 is larger than 598 because 
some of the characters are similar both phonologically 
and visually.
racters only with SS, we would have captured the 
actual incorrect characters that were phone re-
lated 81.6% (the sum of R1 to R5) of the time. 
For errors that were related to visual similarity, 
recommending the top five candidates with SC3 
would capture the actual incorrect characters 
87.1% of the time. Since we do not show the 
complete distributions, the sums over the rows 
are not 100%. In the current experiments, the 
worst rank was 21. 
We also used PMI to rank the incorrect words. 
Due to page limits, we cannot show complete 
details about the results. The observed distribu-
tions in ranks were not very different from those 
shown in Table 5. 
5 Discussion 
Compared with Liu et al?s analysis (2009b-c) 
for the traditional Chinese, the proportions of 
errors related to phonological factors are almost 
the same, both at about 80%. The proportion of 
errors related to visual factors varied, but the av-
erages in both studies were about 48%. A larger 
scale of study is needed for how traditional and 
simplified characters affect the distributions of 
errors. Results shown in Table 4 suggest that it is 
relatively easy to capture errors related to visual 
factors in simplified Chinese. Although we can-
not elaborate, we note that Cangjie codes are not 
good for comparing characters that have few 
strokes, e.g., c1 to c4 in Table 1. In these cases, 
the coding method for Wubihua input method 
(Wubihua, 2010) should be applied. 
Acknowledgement 
This research was supported in part by the research 
contract NSC-97-2221-E-004-007-MY2 from the Na-
tional Science Council of Taiwan. We thank the ano-
nymous reviewers for constructive comments. Al-
though we are not able to respond to all the comments 
Total R1 R2 R3 R4 R5
SS 426 70.3 7.4 2.9 0.4 0.6
SD 151 25.6 2.7 0.6 0.0 0.4
MS 9 1.4 0.4 0.0 0.0 0.0
MD 8 1.6 0.0 0.0 0.0 0.0
SC1 235 61.3 10.3 4.3 2.0 0.3
SC2 213 53.7 11.0 3.7 2.3 0.3
SC3 263 66.7 12.7 5.7 1.7 0.3
RS 4 1.3 0.0 0.0 0.0 0.0
Table 5. Ranking the candidates 
746
in this paper, we have done so in an extended version 
of this paper. 
References 
Cangjie. 2010. Last visited on 22 April 2010: 
en.wikipedia.org/wiki/Cangjie_input_method. 
CDL. 2010. Chinese document laboratory, Academia 
Sinica. Last visited on 22 April, 2010; 
cdp.sinica.edu.tw/cdphanzi/. (in Chinese) 
Chen, Matthew. Y. 2000. Tone Sandhi: Patterns 
across Chinese Dialects, (Cambridge Studies in 
Linguistics 92). Cambridge University Press. 
Chu, Bong-Foo. 2010. Handbook of the Fifth Genera-
tion of the Cangjie Input Method. last visited on 22 
April 2010: www.cbflabs.com/book/5cjbook/. (in Chi-
nese) 
Cormen, Thomas H., Charles E. Leiserson, Ronald L. 
Rivest, and Clifford Stein. 2009. Introduction to 
Algorithms, third edition. MIT Press. 
Croft, W. Bruce, Donald Metzler, and Trevor Stroh-
man, 2010. Search Engines: Information Retrieval 
in Practice, Pearson. 
Dict. 2010. Last visited on 22 April 2010,
www.cns11643.gov.tw/AIDB/welcome.do 
Fan, Kuo-Chin, Chang-Keng Lin, and Kuo-Sen Chou. 
1995. Confusion set recognition of on-line Chinese 
characters by artificial intelligence technique. Pat-
tern Recognition, 28(3):303?313. 
HanDict. 2010. Last visit on 22 April 2010, 
www.zdic.net/appendix/f19.htm. 
Jurafsky, Daniel and James H. Martin. 2009. Speech 
and Language Processing, second edition, Pearson. 
Kuo, Wen-Jui, Tzu-Chen Yeh, Jun-Ren Lee, Li-Fen 
Chen, Po-Lei Lee, Shyan-Shiou Chen, Low-Tone 
Ho, Daisy L. Hung, Ovid J.-L. Tzeng, and Jen-
Chuen Hsieh. 2004. Orthographic and phonological 
processing of Chinese characters: An fMRI study. 
NeuroImage, 21(4):1721?1731. 
Lee, Chia-Ying, Jie-Li Tsai, Hsu-Wen Huang, Daisy 
L. Hung, Ovid J.-L. Tzeng. 2006. The temporal 
signatures of semantic and phonological activations 
for Chinese sublexical processing: An even-related 
potential study. Brain Research, 1121(1):150-159. 
Lee, Hsiang. 2010a. Cangjie Input Methods in 30 
Days 2. Foruto. Last visited on 22 April 2010:  in-
put.foruto.com/cccls/cjzd.html. 
Lee, Mu. 2010b. A quantitative study of the formation 
of Chinese characters. Last visited on 22 April 
2010: chinese.exponode.com/0_1.htm. (in Chinese) 
Liu, Chao-Lin, and Jen-Hsiang Lin. 2008. Using 
structural information for identifying similar Chi-
nese characters. Proc. of the 46th Annual Meeting 
of the Association for Computational Linguistics,
short papers, 93?96.
Liu, Chao-Lin, Kan-Wen Tien, Yi-Hsuan Chuang, 
Chih-Bin Huang, and Juei-Yu Weng. 2009a. Two 
applications of lexical information to computer-
assisted item authoring for elementary Chinese. 
Proc. of the 22nd Int?l Conf. on Industrial En-
gineering & Other Applications of Applied Intel-
ligent Systems, 470?480. 
Liu, Chao-Lin, Kan-Wen Tien, Min-Hua Lai, Yi-
Hsuan Chuang, and Shih-Hung Wu. 2009b. Cap-
turing errors in written Chinese words. Proc. of the 
47th Annual Meeting of the Association for Compu-
tational Linguistics, short papers, 25?28. 
Liu, Chao-Lin, Kan-Wen Tien, Min-Hua Lai, Yi-
Hsuan Chuang, and Shih-Hung Wu. 2009c. Phono-
logical and logographic influences on errors in 
written Chinese words. Proc. of the 7th Workshop 
on Asian Language Resources, the 47th Annual 
Meeting of the ACL, 84?91. 
Liu, Cheng-Lin, Stefan Jaeger, and Masaki Nakagawa. 
2004. Online recognition of Chinese characters: 
The state-of-the-art. IEEE Transaction on Pattern 
Analysis and Machine Intelligence, 26(2):198?213. 
Wubihua. 2010. Last visited on 22 April 2010: 
en.wikipedia.org/wiki/Wubihua_method. 
Yeh, Su-Ling, and Jing-Ling Li. 2002. Role of struc-
ture and component in judgments of visual simi-
larity of Chinese Characters. Journal of Expe-
rimental Psychology: Human Perception and Per-
formance, 28(4):933?947. 
747
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 182?192,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Joint Learning of Phonetic Units and Word Pronunciations for ASR
Chia-ying Lee, Yu Zhang, James Glass
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
{chiaying,yzhang87,jrg}@csail.mit.edu
Abstract
The creation of a pronunciation lexicon re-
mains the most inefficient process in develop-
ing an Automatic Speech Recognizer (ASR).
In this paper, we propose an unsupervised
alternative ? requiring no language-specific
knowledge ? to the conventional manual ap-
proach for creating pronunciation dictionar-
ies. We present a hierarchical Bayesian model,
which jointly discovers the phonetic inven-
tory and the Letter-to-Sound (L2S) mapping
rules in a language using only transcribed
data. When tested on a corpus of spontaneous
queries, the results demonstrate the superior-
ity of the proposed joint learning scheme over
its sequential counterpart, in which the la-
tent phonetic inventory and L2S mappings are
learned separately. Furthermore, the recogniz-
ers built with the automatically induced lexi-
con consistently outperform grapheme-based
recognizers and even approach the perfor-
mance of recognition systems trained using
conventional supervised procedures.
1 Introduction
Modern automatic speech recognizers require a few
essential ingredients such as a signal representation
of the speech signal, a search component, and typ-
ically a set of stochastic models that capture 1) the
acoustic realizations of the basic sounds of a lan-
guage, for example, phonemes, 2) the realization of
words in terms of these sounds, and 3) how words
are combined in spoken language. When creating
a speech recognizer for a new language the usual
requirements are: first, a large speech corpus with
word-level annotations; second, a pronunciation dic-
tionary that essentially defines a phonetic inventory
for the language as well as word-level pronuncia-
tions, and third, optional additional text data that
can be used to train the language model. Given
these data and some decision about the signal rep-
resentation, e.g., centi-second Mel-Frequency Cep-
stral Coefficients (MFCCs) (Davis and Mermelstein,
1980) with various derivatives, as well as the nature
of the acoustic and language model such as 3-state
HMMs and n-grams, iterative training methods can
be used to effectively learn the model parameters for
the acoustic and language models. Although the de-
tails of the components have changed through the
years, this basic ASR formulation was well estab-
lished by the late 1980?s, and has not really changed
much since then.
One of the interesting aspects of this formulation
is the inherent dependence on the dictionary, which
defines both the phonetic inventory of a language,
and the pronunciations of all the words in the vo-
cabulary. The dictionary is arguably the cornerstone
of a speech recognizer as it provides the essential
transduction from sounds to words. Unfortunately,
the dependency on this resource is a significant im-
pediment to the creation of speech recognizers for
new languages, since they are typically created by
experts, whereas annotated corpora can be relatively
more easily created by native speakers of a language.
The existence of an expert-derived dictionary in
the midst of stochastic speech recognition models is
somewhat ironic, and it is natural to ask why it con-
tinues to receive special status after all these years.
Why can we not learn the inventory of sounds of a
language and associated word pronunciations auto-
matically, much as we learn our acoustic model pa-
rameters? If successful, we would move one step
forward towards breaking the language barrier that
182
limits us from having speech recognizers for all lan-
guages of the world, instead of the less than 2% that
currently exist.
In this paper, we investigate the problem of infer-
ring a pronunciation lexicon from an annotated cor-
pus without exploiting any language-specific knowl-
edge. We formulate our approach as a hierarchi-
cal Bayesian model, which jointly discovers the
acoustic inventory and the latent encoding scheme
between the letters and the sounds of a language.
We evaluate the quality of the induced lexicon and
acoustic model through a series of speech recogni-
tion experiments on a conversational weather query
corpus (Zue et al, 2000). The results demonstrate
that our model consistently generates close perfor-
mance to recognizers that are trained with expert-
defined phonetic inventory and lexicon. Compared
to grapheme-based recognizers, our model is capa-
ble of improving the Word Error Rates (WERs) by
at least 15.3%. Finally, the joint learning framework
proposed in this paper is proven to be much more
effective than modeling the acoustic units and the
letter-to-sound mappings separately, as shown in a
45% WER deduction our model achieves compared
to a sequential approach.
2 Related Work
Various algorithms for learning sub-word based pro-
nunciations were proposed in (Lee et al, 1988;
Fukada et al, 1996; Bacchiani and Ostendorf, 1999;
Paliwal, 1990). In these previous approaches, spo-
ken samples of a word are gathered, and usually
only one single pronunciation for the word is de-
rived based on the acoustic evidence observed in the
spoken samples. The major difference between our
work and these previous works is that our model
learns word pronunciations in the context of letter
sequences. More specifically, our model learns letter
pronunciations first and then concatenates the pro-
nunciation of each letter in a word to form the word
pronunciation. The advantage of our approach is
that pronunciation knowledge learned for a particu-
lar letter in some arbitrary word can subsequently be
used to help learn the letter?s pronunciation in other
words. This property allows our model to potentially
learn better pronunciations for less frequent words.
The more recent work by Garcia and Gish (2006)
and Siu et al (2013) has made extensive use
of self-organizing units for keyword spotting and
other tasks for languages with limited linguistic
resources. Others who have more recently ex-
plored the unsupervised space include (Varadarajan
et al, 2008; Jansen and Church, 2011; Lee and
Glass, 2012). The latter work introduced a non-
parametric Bayesian inference procedure for auto-
matically learning acoustic units that is most similar
to our current work except that our model also infers
word pronunciations simultaneously.
The concept of creating a speech recognizer for
a language with only orthographically annotated
speech data has also been explored previously by
means of graphemes. This approach has been shown
to be effective for alphabetic languages with rela-
tively straightforward grapheme to phoneme trans-
formations and does not require any unsupervised
learning of units or pronunciations (Killer et al,
2003; Stu?ker and Schultz, 2004). As we explain in
later sections, grapheme-based systems can actually
be regarded as a special case of our model; therefore,
we expect our model to have greater flexibilities for
capturing pronunciation rules of graphemes.
3 Model
The goal of our model is to induce a word pronunci-
ation lexicon from spoken utterances and their cor-
responding word transcriptions. No other language-
specific knowledge is assumed to be available, in-
cluding the phonetic inventory of the language. To
achieve the goal, our model needs to solve the fol-
lowing two tasks:
? Discover the phonetic inventory.
? Reveal the latent mapping between the letters
and the discovered phonetic units.
We propose a hierarchical Bayesian model for
jointly discovering the two latent structures from
an annotated speech corpus. Before presenting our
model, we first describe the key latent and observed
variables of the problem.
Letter (lmi ) We use l
m
i to denote the i
th let-
ter observed in the word transcription of the
mth training sample. To be sure, a train-
ing sample involves a speech utterance and its
183
corresponding text transcription. The letter se-
quence composed of lmi and its context, namely
lmi??, ? ? ? , l
m
i?1, l
m
i , l
m
i+1, ? ? ? , l
m
i+?, is denoted as ~l
m
i,?.
Although lmi is referred to as a letter in this paper,
it can represent any character observed in the text
data, including space and symbols indicating sen-
tence boundaries. The set of unique characters ob-
served in the data set is denoted as G. For notation
simplicity, we use L? to denote the set of letter se-
quences of length 2? + 1 that appear in the dataset
and use ~l? to denote the elements in L?. Finally,
P(~l?) is used to represent the parent of ~l?, which is
a substring of ~l? with the first and the last characters
truncated.
Number of Mapped Acoustic Units (nmi ) Each
letter lmi in the transcriptions is assumed to be
mapped to a certain number of phonetic units. For
example, the letter x in the word fox is mapped to
2 phonetic units /k/ and /s/, while the letter e in the
word lake is mapped to 0 phonetic units. We denote
this number as nmi and limit its value to be 0, 1 or 2
in our model. The value of nmi is always unobserved
and needs to be inferred by the our model.
Identity of the Acoustic Unit (cmi,p) For each pho-
netic unit that lmi maps to, we use c
m
i,p, for 1 ? p ?
nmi , to denote the identity of the phonetic unit. Note
that the phonetic inventory that describes the data
set is unknown to our model, and the identities of
the phonetic units are associated with the acoustic
units discovered automatically by our model.
Speech Feature xmt The observed speech data in
our problem are converted to a series of 25 ms 13-
dimensional MFCCs (Davis and Mermelstein, 1980)
and their first- and second-order time derivatives at
a 10 ms analysis rate. We use xmt ? R
39 to denote
the tth feature frame of the mth utterance.
3.1 Generative Process
We present the generative process for a single train-
ing sample (i.e., a speech utterance and its corre-
sponding text transcription); to keep notation sim-
ple, we discard the index variable m in this section.
For each li in the transcription, the model gener-
ates ni, given ~li,?, from the 3-dimensional categori-
cal distribution ?~li,?(ni). Note that for every unique
~li,? letter sequence, there is an associated ?~li,?(ni)
lj 
  1?  p ? ni 
?0 
ci, p 
?0 
K
?c 
di,p  
? 
1 ? i ? Lm 
ni 
xt 
1 ? m ? M 
?l2,n,p 
? ? 
?l,n,p 
G ?{(n,p) | 0 ? n ? 2, 1 ? p ? n} 
?l1,n,p 
G ?G 
G ?G 
?1 
?2 
i-2 ? j ? i+2 
Figure 1: The graphical representation of the pro-
posed hierarchical Bayesian model. The shaded cir-
cle denotes the observed text and speech data, and
the squares denote the hyperparameters of the priors
in our model. See Sec. 3 for a detailed explanation
of the generative process of our model.
distribution, which captures the fact that the number
of phonetic units a letter maps to may depend on its
context. In our model, we impose a Dirichlet distri-
bution prior Dir(?) on ?~li,?(ni).
If ni = 0, li is not mapped to any acoustic units
and the generative process stops for li; otherwise,
for 1 ? p ? ni, the model generates ci,p from:
ci,p ? pi~li,?,ni,p (1)
where pi~li,?,ni,p is a K-dimensional categorical dis-
tribution, whose outcomes correspond to the pho-
netic units discovered by the model from the given
speech data. Eq. 1 shows that for each combination
of~li,?, ni and p, there is an unique categorical distri-
bution. An important property of these categorical
distributions is that they are coupled together such
that their outcomes point to a consistent set of pho-
netic units. In order to enforce the coupling, we con-
struct pi~li,?,ni,p through a hierarchical process.
? ? Dir(?) (2)
pi~li,?,ni,p ? Dir(???) for ? = 0 (3)
pi~li,?,ni,p ? Dir(??pi~li,??1,ni,p) for ? ? 1 (4)
184
To interpret Eq. 2 to Eq. 4, we envision that
the observed speech data are generated by a K-
component mixture model, of which the components
correspond to the phonetic units in the language. As
a result, ? in Eq. 2 can be viewed as the mixture
weight over the components, which indicates how
likely we are to observe each acoustic unit in the
data overall. By adopting this point of view, we
can also regard the mapping between li and the pho-
netic units as a mixture model, and pili,ni,p
1 repre-
sents how probable li is mapped to each phonetic
unit given ni and p. We apply a Dirichlet distribu-
tion prior parametrized by ?0? to pili,ni,p as shown
in Eq. 3. With this parameterization, the mean of
pili,ni,p is the global mixture weight ?, and ?0 con-
trols how similar pili,ni,p is to the mean. More specif-
ically, for large ?0  K, the Dirichlet distribution
is highly peaked around the mean; on the contrary,
for ?0  K, the mean lies in a valley. The parame-
ters of a Dirichlet distribution can also be viewed as
pseudo-counts for each category. Eq. 4 shows that
the prior for pi~li,?,ni,p is seeded by pseudo-counts
that are proportional to the mapping weights over
the phonetic units of li in a shorter context. In other
words, the mapping distribution of li in a shorter
context can be thought of as a back-off distribution
of li?s mapping weights in a longer context.
Each component of the K-dimensional mixture
model is linked to a 3-state Hidden Markov Model
(HMM). These K HMMs are used to model the
phonetic units in the language (Jelinek, 1976). The
emission probability of each HMM state is modeled
by a diagonal Gaussian Mixture Model (GMM). We
use ?c to represent the set of parameters that define
the cth HMM, which includes the state transition
probability and the GMM parameters of each state
emission distribution. The conjugate prior of ?c is
denoted as H(?0)2.
Finally, to finish the generative process, for each
ci,p we use the corresponding HMM ?ci,p to gen-
erate the observed speech data xt, and the genera-
tive process of the HMM determines the duration,
1An abbreviation of pi~li,0,ni,p
2H(?0) includes a Dirichlet prior for the transition probabil-
ity of each state, and a Dirichlet prior for each mixture weight
of the three GMMs, and a normal-Gamma distribution for the
mean and precision of each Gaussian mixture in the 3-state
HMM.
di,p, of the speech segment. The complete genera-
tive model, with ? set to 2, is depicted in Fig. 1; M
is the total number of transcribed utterances in the
corpus, and Lm is the number of letters in utterance
m. The shaded circles denote the observed data, and
the squares denote the hyperparameters of the priors
used in our model. Lastly, the unshaded circles de-
note the latent variables of our model, for which we
derive inference algorithms in the next section.
4 Inference
We employ Gibbs sampling (Gelman et al, 2004) to
approximate the posterior distribution of the latent
variables in our model. In the following sections, we
first present a message-passing algorithm for block-
sampling ni and ci,p, and then describe how we
leverage acoustic cues to accelerate the computa-
tion of the message-passing algorithm. Note that the
block-sampling algorithm for ni and ci,p can be par-
allelized across utterances. Finally, we briefly dis-
cuss the inference procedures for ?~l? , pi~l?,n,p, ?, ?c.
4.1 Block-sampling ni and ci,p
To understand the message-passing algorithm in this
study, it is helpful to think of our model as a sim-
plified Hidden Semi-Markov Model (HSMM), in
which the letters represent the states and the speech
features are the observations. However, unlike in
a regular HSMM, where the state sequence is hid-
den, in our case, the state sequence is fixed to be the
given letter sequence. With this point of view, we
can modify the message-passing algorithms of Mur-
phy (2002) and Johnson and Willsky (2013) to com-
pute the posterior information required for block-
sampling ni and ci,p.
Let L(xt) be a function that returns the index
of the letter from which xt is generated; also, let
Ft = 1 be a tag indicating that a new phone segment
starts at t+ 1. Given the constraint that 0 ? ni ? 2,
for 0 ? i ? Lm and 0 ? t ? Tm, the backwards
messages Bt(i) and B?t (i) for the m
th training sam-
ple can be defined and computed as in Eq. 5 and
Eq. 7. Note that for clarity we discard the index vari-
able m in the derivation of the algorithm.
185
Bt(i) , p(xt+1:T |L(xt) = i, Ft = 1)
=
min{L,i+1+U}?
j=i+1
B?t (j)
j?1?
k=i+1
p(nk = 0|~li,?)
=
min{L,i+1+U}?
j=i+1
B?t (j)
j?1?
k=i+1
?~li,?(0) (5)
B?t (i) , p(xt+1:T |L(xt+1) = i, Ft = 1)
=
T?t?
d=1
p(xt+1:t+d|~li,?)Bt+d(i) (6)
=
T?t?
d=1
{
K?
ci,1=1
?~li,?(1)pi~li,?,1,1(ci,1)p(xt+1:t+d|?ci,1)
+
d?1?
v=1
K?
ci,1
K?
ci,2
?~li,?(2)pi~li,?,2,1(ci,1)pi~li,?,2,2(ci,2)
? p(xt+1:t+v|?ci,1)p(xt+v+1:t+d|?ci,2)}Bt+d(i)
(7)
We use xt1:t2 to denote the segment consisting of
xt1 , ? ? ? , xt2 . Our inference algorithm only allows
up to U letters to emit 0 acoustic units in a row. The
value of U is set to 2 for our experiments. Bt(i)
represents the total probability of all possible align-
ments between xt+1:T and li+1:L. B?t (i) contains
the probability of all the alignments between xt+1:T
and li+1:L that map xt+1 to li particularly. This
alignment constraint between xt+1 and li is explic-
itly shown in the first term of Eq. 6, which represents
how likely the speech segment xt+1:t+d is generated
by li given li?s context. This likelihood is simply
the marginal probability of p(xt+1:t+d, ni, ci,p|~li,?)
with ni and ci,p integrated out, which can be ex-
panded and computed as shown in the last three rows
of Eq. 7. The index v specifies where the phone
boundary is between the two acoustic units that li
is aligned with when ni = 2. Eq. 8 to Eq. 10 are
the boundary conditions of the message passing al-
gorithm. B0(0) carries the total probably of all pos-
sible alignments between l1:L and x1:T . Eq. 9 spec-
ifies that at most U letters at the end of an sentence
can be left unaligned with any speech features, while
Eq. 10 indicates that all of the speech features in an
utterance must be assigned to a letter.
Algorithm 1 Block-sample ni and ci,p from Bt(i)
and B?t (i)
1: i? 0
2: t? 0
3: while i < L ? t < T do
4: nexti ? SampleFromBt(i)
5: if nexti > i+ 1 then
6: for k = i+ 1 to k = nexti ? 1 do
7: nk ? 0
8: end for
9: end if
10: d, ni, ?ci,p?, v ? SampleFromB?t (nexti)
11: t? t+ d
12: i? nexti
13: end while
B0(0) =
min{L,U+1}?
j=1
B?0(j)
j?1?
k=1
?~li,?(0) (8)
BT (i) ,
?
??
??
1 if i = L
?L
j=i+1 ?~li,?(0) if L? U ? i < L
0 if i < L? U
(9)
Bt(L) ,
{
1 if t = T
0 otherwise
(10)
Given Bt(i) and B?t (i), ni and ci,p for each letter
in the utterance can be sampled using Alg. 1. The
SampleFromBt(i) function in line 4 returns a ran-
dom sample from the relative probability distribu-
tion composed by entries of the summation in Eq. 5.
Line 5 to line 9 check whether li (and maybe li+1)
is mapped to zero phonetic units. nexti points to
the letter that needs to be aligned with 1 or 2 phone
segments starting from xt. The number of phonetic
units that lnexti maps to and the identities of the
units are sampled in SampleFromB?t (i). This sub-
routine generates a tuple of d, ni, ?ci,p? as well as
v (if ni = 2) from all the entries of the summation
shown in Eq. 73.
3We use ?ci,p? to denote that ?ci,p?may consist of two num-
bers, ci,1 and ci,2, when ni = 2.
186
4.2 Heuristic Phone Boundary Elimination
The variables d and v in Eq. 7 enumerate through
every frame index in a sentence, treating each fea-
ture frame as a potential boundary between acous-
tic units. However, it is possible to exploit acoustic
cues to avoid checking feature frames that are un-
likely to be phonetic boundaries. We follow the pre-
segmentation method described in Glass (2003) to
skip roughly 80% of the feature frames and greatly
speed up the computation of B?t (i).
Another heuristic applied to our algorithm to re-
duce the search space for d and v is based on the
observation that the average duration of phonetic
units is usually no longer than 300 ms. Therefore,
when computing B?t (i), we only consider speech
segments that are shorter than 300 ms to avoid align-
ing letters to speech segments that are too long to be
phonetic units.
4.3 Sampling ?~l? , pi~l?,ni,p, ? and ?c
Sampling ?~l? To compute the posterior distribu-
tion of ?~l? , we count how many times
~l? is mapped
to 0, 1 and 2 phonetic units from nmi . More specifi-
cally, we define N~l?(j) for 0 ? j ? 2 as follows:
N~l?(j) =
M?
m=1
Lm?
i=1
?(nmi , j)?(~l
m
i,?,~l?)
where we use ?(?) to denote the discrete Kronecker
delta. With N~l? , we can simply sample a new value
for ?~l? from the following distribution:
?~l? ? Dir(? +N~l?)
Sampling pi~l?,n,p and ? The posterior distribu-
tions of pi~l?,n,p and ? are constructed recursively due
to the hierarchical structure imposed on pi~l?,n,p and
?. We start with gathering counts for updating the
pi variables at the lowest level, i.e., pi~l2,n,p given that
? is set to 2 in our model implementation, and then
sample pseudo-counts for the pi variables at higher
hierarchies as well as ?. With the pseudo-counts, a
new ? can be generated, which allows pi~l?,n,p to be
re-sampled sequentially.
More specifically, we define C~l2,n,p(k) to be the
number of times that ~l2 is mapped to n units and
the unit in position p is the kth phonetic unit. This
value can be counted from the current values of cmi,p
as follows.
C~l2,n,p(k) =
M?
m=1
Lm?
i=1
?(~li,2,~l2)?(n
m
i , n)?(c
m
i,p, k)
To derive the posterior distribution of pi~l1,n,p an-
alytically, we need to sample pseudo-counts C~l1,n,p,
which is defined as follows.
C~l1,n,p(k) =
?
~l2?U~l1
C~l2,n,p
(k)
?
i=1
I[?i <
?2pi~l1,n,p(k)
i+ ?2pi~l1,n,p(k)
]
(11)
We use U~l1 = {
~l2|P(~l2) = ~l1} to denote the set of
~l2 whose parent is~l1 and ?i to represent random vari-
ables sampled from a uniform distribution between
0 and 1. Eq. 11 can be applied recursively to com-
pute C~l0,n,p(k) and C ,n,p(k), the pseudo-counts that
are applied to the conjugate priors of pi~l0,n,p and ?.
With the pseudo-count variables computed, new val-
ues for ? and pi~l?,n,p can be sampled sequentially as
shown in Eq. 12 to Eq. 14.
? ? Dir(? + C ,n,p) (12)
pi~l?,n,p ? Dir(??? + C~l?,n,p) for ? = 0 (13)
pi~l?,n,p ? Dir(??pi~l??1,n,p + C~l?,n,p) for ? ? 1
(14)
5 Experimental Setup
To test the effectiveness of our model for joint learn-
ing phonetic units and word pronunciations from an
annotated speech corpus, we construct speech rec-
ognizers out of the training results of our model.
The performance of the recognizers is evaluated and
compared against three baselines: first, a grapheme-
based speech recognizer; second, a recognizer built
by using an expert-crafted lexicon, which is referred
to as an expert lexicon in the rest of the paper for
simplicity; and third, a recognizer built by discover-
ing the phonetic units and L2S pronunciation rules
sequentially without using a lexicon. In this section,
we provide a detailed description of the experimen-
tal setup.
187
? ? ?0 ?1 ?2 ?0 ? K
?0.1?3 ?10?100 1 0.1 0.2 * 2 100
Table 1: The values of the hyperparameters of our
model. We use ?a?D to denote aD-dimensional vec-
tor with all entries being a. *We follow the proce-
dure reported in (Lee and Glass, 2012) to set up the
HMM prior ?0.
5.1 Dataset
All the speech recognition experiments reported in
this paper are performed on a weather query dataset,
which consists of narrow-band, conversational tele-
phone speech (Zue et al, 2000). We follow the ex-
perimental setup of McGraw et al (2013) and split
the corpus into a training set of 87,351 utterances, a
dev set of 1,179 utterances and a test set of 3,497 ut-
terances. A subset of 10,000 utterances is randomly
selected from the training set. We use this subset of
data for training our model to demonstrate that our
model is able to discover the phonetic composition
and the pronunciation rules of a language even from
just a few hours of data.
5.2 Building a Recognizer from Our Model
The values of the hyperparameters of our model are
listed in Table 1. We run the inference procedure de-
scribed in Sec. 4 for 10,000 times on the randomly
selected 10,000 utterances. The samples of ?~l? and
pi~l?n,p from the last iteration are used to decode n
m
i
and cmi,p for each sentence in the entire training set by
following the block-sampling algorithm described
in Sec. 4.1. Since cmi,p is the phonetic mapping of
lmi , by concatenating the phonetic mapping of ev-
ery letter in a word, we can obtain a pronunciation
of the word represented in the labels of discovered
phonetic units. For example, assume that word w
appears in sentence m and consists of l3l4l5 (the
sentence index m is ignored for simplicity). Also,
assume that after decoding, n3 = 1, n4 = 2 and
n5 = 1. A pronunciation ofw is then encoded by the
sequence of phonetic labels c3,1c4,1c4,2c5,1. By re-
peating this process for each word in every sentence
for the training set, a list of word pronunciations can
be compiled and used as a stochastic lexicon to build
a speech recognizer.
In theory, the HMMs inferred by our model can be
directly used as the acoustic model of a monophone
speech recognizer. However, if we regard the ci,p
labels of each utterance as the phone transcription
of the sentence, then a new acoustic model can be
easily re-trained on the entire data set. More conve-
niently, the phone boundaries corresponding to the
ci,p labels are the by-products of the block-sampling
algorithm, which are indicated by the values of d and
v in line 10 of Alg. 1 and can be easily saved during
the sampling procedure. Since these data are readily
available, we re-build a context-independent model
on the entire data set. In this new acoustic model,
a 3-state HMM is used to model each phonetic unit,
and the emission probability of each state is modeled
by a 32-mixture GMM.
Finally, a trigram language model is built by using
the word transcriptions in the full training set. This
language model is utilized in all speech recogni-
tion experiments reported in this paper. Finite State
Transducers (FSTs) are used to build all the recog-
nizers used in this study. With the language model,
the lexicon and the context-independent acoustic
model constructed by the methods described in this
section, we can build a speech recognizer from
the learning output of the proposed model without
the need of a pre-defined phone inventory and any
expert-crafted lexicons.
5.2.1 Pronunciation Mixture Model Retraining
McGraw et al (2013) presented the Pronuncia-
tion Mixture Model (PMM) for composing stochas-
tic lexicons that outperform pronunciation dictionar-
ies created by experts. Although the PMM frame-
work was designed to incorporate and augment ex-
pert lexicons, we found that it can be adapted to pol-
ish the pronunciation list generated by our model.
In particular, the training procedure for PMMs in-
cludes three steps. First, train a L2S model from
a manually specified expert-pronunciation lexicon;
second, generate a list of pronunciations for each
word in the dataset using the L2S model; and finally,
use an acoustic model to re-weight the pronuncia-
tions based on the acoustic scores of the spoken ex-
amples of each word.
To adapt this procedure for our purposes, we sim-
ply plug in the word pronunciations and the acous-
tic model generated by our model. Once we ob-
tain the re-weighted lexicon, we re-generate forced
188
phone alignments and retrain the acoustic model,
which can be utilized to repeat the PMM lexicon re-
weighting procedure. For our experiments, we it-
erate through this model refining process until the
recognition performance converges.
5.2.2 Triphone Model
Conventionally, to train a context-dependent
acoustic model, a list of questions based on the
linguistic properties of phonetic units is required
for growing decision tree classifiers (Young et al,
1994). However, such language-specific knowledge
is not available for our training framework; there-
fore, our strategy is to compile a question list that
treats each phonetic unit as a unique linguistic class.
In other words, our approach to training a context-
dependent acoustic model for the automatically dis-
covered units is to let the decision trees grow fully
based on acoustic evidence.
5.3 Baselines
We compare the recognizers trained by following
the procedures described in Sec. 5.2 against three
baselines. The first baseline is a grapheme-based
speech recognizer. We follow the procedure de-
scribed in Killer et al (2003) and train a 3-state
HMM for each grapheme, which we refer to as the
monophone grapheme model. Furthermore, we cre-
ate a singleton question set (Killer et al, 2003), in
which each grapheme is listed as a question, to train
a triphone grapheme model. Note that to enforce
better initial alignments between the graphemes and
the speech data, we use a pre-trained acoustic model
to identify the non-speech segments at the beginning
and the end of each utterance before starting training
the monophone grapheme model.
Our model jointly discovers the phonetic inven-
tory and the L2S mapping rules from a set of tran-
scribed data. An alternative of our approach is to
learn the two latent structures sequentially. We fol-
low the training procedure of Lee and Glass (2012)
to learn a set of acoustic models from the speech
data and use these acoustic models to generate a
phone transcription for each utterance. The phone
transcriptions along with the corresponding word
transcriptions are fed as inputs to the L2S model
proposed in Bisani and Ney (2008). A stochastic
lexicon can be learned by applying the L2S model
unit(%) Monophone
Our model 17.0
Oracle 13.8
Grapheme 32.7
Sequential model 31.4
Table 2: Word error rates generated by the four
monophone recognizers described in Sec. 5.2 and
Sec. 5.3 on the weather query corpus.
and the discovered acoustic models to PMM. This
two-stage approach for training a speech recognizer
without an expert lexicon is referred to as the se-
quential model in this paper.
Finally, we compare our system against a rec-
ognizer trained from an oracle recognition system.
We build the oracle recognizer on the same weather
query corpus by following the procedure presented
in McGraw et al (2013). This oracle recognizer is
then applied to generate forced-aligned phone tran-
scriptions for the training utterances, from which
we can build both monophone and triphone acous-
tic models. The expert-crafted lexicon used in the
oracle recognizer is also used in this baseline. Note
that for training the triphone model, we compose a
singleton question list (Killer et al, 2003) that has
every expert-defined phonetic unit as a question. We
use this singleton question list instead of a more so-
phisticated one to ensure that this baseline and our
system differ only in the acoustic model and the lex-
icon used to generate the initial phone transcriptions.
We call this baseline the oracle baseline.
6 Results and Analysis
6.1 Monophone Systems
Table 2 shows the WERs produced by the four
monophone recognizers described in Sec. 5.2 and
Sec. 5.3. It can be seen that our model outper-
forms the grapheme and the sequential model base-
lines significantly while approaching the perfor-
mance of the supervised oracle baseline. The im-
provement over the sequential baseline demonstrates
the strength of the proposed joint learning frame-
work. More specifically, unlike the sequential base-
line, in which the acoustic units are discovered in-
dependently from the text data, our model is able to
exploit the L2S mapping constraints provided by the
word transcriptions to cluster speech segments.
189
By comparing our model to the grapheme base-
line, we can see the advantage of modeling the
pronunciations of a letter using a mixture model,
especially for a language like English which has
many pronunciation irregularities. However, even
for languages with straightforward pronunciation
rules, the concept of modeling letter pronunciations
using mixture models still applies. The main dif-
ference is that the mixture weights for letters of
languages with simple pronunciation rules will be
sparser and spikier. In other words, in theory, our
model should always perform comparable to, if not
better than, grapheme recognizers.
Last but not least, the recognizer trained with the
automatically induced lexicon performs similarly to
the recognizer initialized by an oracle recognition
system, which demonstrates the effectiveness of the
proposed model for discovering the phonetic inven-
tory and a pronunciation lexicon from an annotated
corpus. In the next section, we provide some in-
sights into the quality of the learned lexicon and
into what could have caused the performance gap
between our model and the conventionally trained
recognizer.
6.2 Pronunciation Entropy
The major difference between the recognizer that is
trained by using our model and the recognizer that
is seeded by an oracle recognition system is that
the former uses an automatically discovered lexicon,
while the latter exploits an expert-defined pronun-
ciation dictionary. In order to quantify, as well as
to gain insights into, the difference between these
two lexicons, we define the average pronunciation
entropy, H? , of a lexicon as follows.
H? ?
?1
|V |
?
w?V
?
b?B(w)
p(b) log p(b) (15)
where V denotes the vocabulary of a lexicon, B(w)
represents the set of pronunciations of a word w
and p(b) stands for the weight of a certain pronun-
ciation b. Intuitively, we can regard H? as an in-
dicator of how much pronunciation variation that
each word in a lexicon has on average. Table 3
shows that the H? values of the lexicon induced by
our model and the expert-defined lexicon as well as
Our model PMM iterations
(Discovered lexicon) 0 1 2
H? (bit) 4.58 3.47 3.03
WER (%) 17.0 16.6 15.9
Oracle PMM iterations
(Expert lexicon) 0 1 2
H? (bit) 0.69 0.90 0.92
WER (%) 13.8 12.8 12.4
Table 3: The upper-half of the table shows the aver-
age pronunciation entropies, H? , of the lexicons in-
duced by our model and refined by PMM as well
as the WERs of the monophone recognizers built
with the corresponding lexicons for the weather
query corpus. The definition of H? can be found in
Sec. 6.2. The first row of the lower-half of the ta-
ble lists the average pronunciation entropies, H? , of
the expert-defined lexicon and the lexicons gener-
ated and weighted by the L2P-PMM framework de-
scribed in McGraw et al (2013). The second row of
the lower-half of the table shows the WERs of the
recognizers that are trained with the expert-lexicon
and its PMM-refined versions.
their respective PMM-refined versions4. In Table 3,
we can see that the automatically-discovered lexi-
con and its PMM-reweighted versions have much
higher H? values than their expert-defined counter-
parts. These higher H? values imply that the lexicon
induced by our model contains more pronunciation
variation than the expert-defined lexicon. Therefore,
the lattices constructed during the decoding process
for our recognizer tend to be larger than those con-
structed for the oracle baseline, which explains the
performance gap between the two systems in Table 2
and Table 3.
As shown in Table 3, even though the lexicon
induced by our model is noisier than the expert-
defined dictionary, the PMM retraining framework
consistently refines the induced lexicon and im-
proves the performance of the recognizers5. To the
best of our knowledge, we are the first to apply
PMM to lexicons that are created by a fully unsu-
4We build the PMM-refined version of the expert-defined
lexicon by following the L2P-PMM framework described
in McGraw et al (2013).
5The recognition results all converge in 2 ? 3 PMM retrain-
ing iterations.
190
pronunciations
pronunciation probabilities
Our model 1 PMM 2 PMM
93 56 87 39 19 0.125 - -
93 56 61 87 73 99 0.125 - -
11 56 61 87 73 99 0.125 0.400 0.419
93 20 75 87 17 27 52 0.125 0.125 0.124
55 93 56 61 87 73 84 19 0.125 0.220 0.210
93 26 61 87 49 0.125 0.128 0.140
63 83 86 87 73 53 19 0.125 - -
93 26 61 87 61 0.125 0.127 0.107
Table 4: Pronunciation lists of the word Burma pro-
duced by our model and refined by PMM after 1 and
2 iterations.
pervised method. Therefore, in this paper, we pro-
vide further analysis on how PMM helps enhance
the performance of our model.
We compare the pronunciation lists for the word
Burma generated by our model and refined itera-
tively by PMM in Table 4. The first column of Ta-
ble 4 shows all the pronunciations of Burma dis-
covered by our model, to which our model assigns
equal probabilities to create a stochastic list6. As
demonstrated in the third and the fourth columns of
Table 4, the PMM framework is able to iteratively
re-distribute the pronunciation weights and filter out
less-likely pronunciations, which effectively reduces
both the size and the entropy of the stochastic lexi-
con generated by our model. The benefits of using
the PMM to refine the induced lexicon are twofold.
First, the search space constructed during the recog-
nition decoding process with the refined lexicon is
more constrained, which is the main reason why the
PMM is capable of improving the performance of
the monophone recognizer that is trained with the
output of our model. Secondly, and more impor-
tantly, the refined lexicon can greatly reduce the size
of the FST built for the triphone recognizer of our
model. These two observations illustrate why the
PMM framework can be an useful tool for enhancing
the lexicon discovered automatically by our model.
6.3 Triphone Systems
The best monophone systems of the grapheme base-
line, the oracle baseline and our model are used to
6It is also possible to assign probabilities proportional to the
decoding scores of the word tokens.
Unit(%) Triphone
Our model 13.4
Oracle 10.0
Grapheme 15.7
Table 5: Word error rates of the triphone recogniz-
ers. The triphone recognizers are all built by us-
ing the phone transcriptions generated by their best
monohpone system. For the oracle initialized base-
line and for our model, the PMM-refined lexicons
are used to build the triphone recognizers.
generate forced-aligned phone transcriptions, which
are used to train the triphone models described in
Sec. 5.2.2 and Sec. 5.3. Table 5 shows the WERs
of the triphone recognition systems. Note that if a
more conventional question list, for example, a list
that contains rules to classify phones into different
broad classes, is used to build the oracle triphone
system, the WER can be reduced to 6.5%. However,
as mentioned earlier, in order to gain insights into
the quality of the induced lexicon and the discovered
phonetic set, we compare our model against an ora-
cle triphone system that is built by using a singleton
question set.
By comparing Table 2 and Table 5, we can see
that the grapheme triphone improves by a large mar-
gin compared to its monophone counterpart, which
is consistent with the results reported in (Killer et
al., 2003). However, even though the grapheme
baseline achieves a great performance gain with
context-dependent acoustic models, the recognizer
trained using the lexicon learned by our model and
subsequently refined by PMM still outperforms the
grapheme baseline. The consistently better perfor-
mance our model achieves over the grapheme base-
line demonstrates the strength of modeling the pro-
nunciation of each letter with a mixture model that
is presented in this paper.
Last but not least, by comparing Table 2 and
Table 5, it can be seen that the relative perfor-
mance gain achieved by our model is similar to
that obtained by the oracle baseline. Both Table 2
and Table 5 show that even without exploiting any
language-specific knowledge during training, our
recognizer is able to perform comparably with the
recognizer trained using an expert lexicon. The abil-
ity of our model to obtain such similar performance
191
further supports the effectiveness of the joint learn-
ing framework proposed in this paper for discover-
ing the phonetic inventory and the word pronuncia-
tions from simply an annotated speech corpus.
7 Conclusion
We present a hierarchical Bayesian model for si-
multaneously discovering acoustic units and learn-
ing word pronunciations from transcribed spoken ut-
terances. Both monophone and triphone recogniz-
ers can be built on the discovered acoustic units and
the inferred lexicon. The recognizers trained with
the proposed unsupervised method consistently out-
performs grapheme-based recognizers and approach
the performance of recognizers trained with expert-
defined lexicons. In the future, we plan to apply this
technology to develop ASRs for more languages.
Acknowledgements
The authors would like to thank Ian McGraw and
Ekapol Chuangsuwanich for their advice on the
PMM and recognition experiments presented in this
paper. Thanks to the anonymous reviewers for help-
ful comments. Finally, the authors would like to
thank Stephen Shum for proofreading and editing
the early drafts of this paper.
References
Michiel Bacchiani and Mari Ostendorf. 1999. Joint lexi-
con, acoustic unit inventory and model design. Speech
Communication, 29:99 ? 114.
Maximilian Bisani and Hermann Ney. 2008. Joint-
sequence models for grapheme-to-phoneme conver-
sion. Speech Communication, 50(5):434?451, May.
Steven B. Davis and Paul Mermelstein. 1980. Com-
parison of parametric representations for monosyllabic
word recognition in continuously spoken sentences.
IEEE Trans. on Acoustics, Speech, and Signal Pro-
cessing, 28(4):357?366.
Toshiaki Fukada, Michiel Bacchiani, Kuldip Paliwal, and
Yoshinori Sagisaka. 1996. Speech recognition based
on acoustically derived segment units. In Proceedings
of ICSLP, pages 1077 ? 1080.
Alvin Garcia and Herbert Gish. 2006. Keyword spotting
of arbitrary words using minimal speech resources. In
Proceedings of ICASSP, pages 949?952.
Andrew Gelman, John B. Carlin, Hal S. Stern, and Don-
ald B. Rubin. 2004. Bayesian Data Analysis. Texts
in Statistical Science. Chapman & Hall/CRC, second
edition.
James Glass. 2003. A probabilistic framework for
segment-based speech recognition. Computer Speech
and Language, 17:137 ? 152.
Aren Jansen and Kenneth Church. 2011. Towards un-
supervised training of speaker independent acoustic
models. In Proceedings of INTERSPEECH, pages
1693 ? 1696.
Frederick Jelinek. 1976. Continuous speech recogni-
tion by statistical methods. Proceedings of the IEEE,
64:532 ? 556.
Matthew J. Johnson and Alan S. Willsky. 2013. Bayesian
nonparametric hidden semi-markov models. Journal
of Machine Learning Research, 14:673?701, February.
Mirjam Killer, Sebastian Stu?ker, and Tanja Schultz.
2003. Grapheme based speech recognition. In Pro-
ceeding of the Eurospeech, pages 3141?3144.
Chia-ying Lee and James Glass. 2012. A nonparamet-
ric Bayesian approach to acoustic model discovery. In
Proceedings of ACL, pages 40?49.
Chin-Hui Lee, Frank Soong, and Biing-Hwang Juang.
1988. A segment model based approach to speech
recognition. In Proceedings of ICASSP, pages 501?
504.
Ian McGraw, Ibrahim Badr, and James Glass. 2013.
Learning lexicons from speech using a pronunciation
mixture model. IEEE Trans. on Speech and Audio
Processing, 21(2):357?366.
Kevin P. Murphy. 2002. Hidden semi-Markov mod-
els (hsmms). Technical report, University of British
Columbia.
Kuldip Paliwal. 1990. Lexicon-building methods for an
acoustic sub-word based speech recognizer. In Pro-
ceedings of ICASSP, pages 729?732.
Man-hung Siu, Herbert Gish, Arthur Chan, William
Belfield, and Steve Lowe. 2013. Unsupervised train-
ing of an HMM-based self-organizing unit recgonizer
with applications to topic classification and keyword
discovery. Computer, Speech, and Language.
Sebastian Stu?ker and Tanja Schultz. 2004. A grapheme
based speech recognition system for Russian. In Pro-
ceedings of the 9th Conference Speech and Computer.
Balakrishnan Varadarajan, Sanjeev Khudanpur, and Em-
manuel Dupoux. 2008. Unsupervised learning of
acoustic sub-word units. In Proceedings of ACL-08:
HLT, Short Papers, pages 165?168.
Steve J. Young, J.J. Odell, and Philip C. Woodland. 1994.
Tree-based state tying for high accuracy acoustic mod-
elling. In Proceedings of HLT, pages 307?312.
Victor Zue, Stephanie Seneff, James Glass, Joseph Po-
lifroni, Christine Pao, Timothy J. Hazen, and Lee Het-
herington. 2000. Jupiter: A telephone-based con-
versational interface for weather information. IEEE
Trans. on Speech and Audio Processing, 8:85?96.
192
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 40?49,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Nonparametric Bayesian Approach to Acoustic Model Discovery
Chia-ying Lee and James Glass
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
{chiaying,jrg}@csail.mit.edu
Abstract
We investigate the problem of acoustic mod-
eling in which prior language-specific knowl-
edge and transcribed data are unavailable. We
present an unsupervised model that simultane-
ously segments the speech, discovers a proper
set of sub-word units (e.g., phones) and learns
a Hidden Markov Model (HMM) for each in-
duced acoustic unit. Our approach is formu-
lated as a Dirichlet process mixture model in
which each mixture is an HMM that repre-
sents a sub-word unit. We apply our model
to the TIMIT corpus, and the results demon-
strate that our model discovers sub-word units
that are highly correlated with English phones
and also produces better segmentation than the
state-of-the-art unsupervised baseline. We test
the quality of the learned acoustic models on a
spoken term detection task. Compared to the
baselines, our model improves the relative pre-
cision of top hits by at least 22.1% and outper-
forms a language-mismatched acoustic model.
1 Introduction
Acoustic models are an indispensable component
of speech recognizers. However, the standard pro-
cess of training acoustic models is expensive, and
requires not only language-specific knowledge, e.g.,
the phone set of the language, a pronunciation dic-
tionary, but also a large amount of transcribed data.
Unfortunately, these necessary data are only avail-
able for a very small number of languages in the
world. Therefore, a procedure for training acous-
tic models without annotated data would not only
be a breakthrough from the traditional approach, but
would also allow us to build speech recognizers for
any language efficiently.
In this paper, we investigate the problem of unsu-
pervised acoustic modeling with only spoken utter-
ances as training data. As suggested in Garcia and
Gish (2006), unsupervised acoustic modeling can
be broken down to three sub-tasks: segmentation,
clustering segments, and modeling the sound pattern
of each cluster. In previous work, the three sub-
problems were often approached sequentially and
independently in which initial steps are not related to
later ones (Lee et al, 1988; Garcia and Gish, 2006;
Chan and Lee, 2011). For example, the speech data
was usually segmented regardless of the clustering
results and the learned acoustic models.
In contrast to the previous methods, we approach
the problem by modeling the three sub-problems as
well as the unknown set of sub-word units as la-
tent variables in one nonparametric Bayesian model.
More specifically, we formulate a Dirichlet pro-
cess mixture model where each mixture is a Hid-
den Markov Model (HMM) used to model a sub-
word unit and to generate observed segments of that
unit. Our model seeks the set of sub-word units,
segmentation, clustering and HMMs that best repre-
sent the observed data through an iterative inference
process. We implement the inference process using
Gibbs sampling.
We test the effectiveness of our model on the
TIMIT database (Garofolo et al, 1993). Our model
shows its ability to discover sub-word units that are
highly correlated with standard English phones and
to capture acoustic context information. For the seg-
mentation task, our model outperforms the state-of-
40
the-art unsupervised method and improves the rel-
ative F-score by 18.8 points (Dusan and Rabiner,
2006). Finally, we test the quality of the learned
acoustic models through a keyword spotting task.
Compared to the state-of-the-art unsupervised meth-
ods (Zhang and Glass, 2009; Zhang et al, 2012),
our model yields a relative improvement in precision
of top hits by at least 22.1% with only some degra-
dation in equal error rate (EER), and outperforms
a language-mismatched acoustic model trained with
supervised data.
2 Related Work
Unsupervised Sub-word Modeling We follow
the general guideline used in (Lee et al, 1988; Gar-
cia and Gish, 2006; Chan and Lee, 2011) and ap-
proach the problem of unsupervised acoustic mod-
eling by solving three sub-problems of the task:
segmentation, clustering and modeling each cluster.
The key difference, however, is that our model does
not assume independence among the three aspects of
the problem, which allows our model to refine its so-
lution to one sub-problem by exploiting what it has
learned about other parts of the problem. Second,
unlike (Lee et al, 1988; Garcia and Gish, 2006) in
which the number of sub-word units to be learned is
assumed to be known, our model learns the proper
size from the training data directly.
Instead of segmenting utterances, the authors
of (Varadarajan et al, 2008) trained a single state
HMM using all data at first, and then iteratively
split the HMM states based on objective functions.
This method achieved high performance in a phone
recognition task using a label-to-phone transducer
trained from some transcriptions. However, the per-
formance seemed to rely on the quality of the trans-
ducer. For our work, we assume no transcriptions
are available and measure the quality of the learned
acoustic units via a spoken query detection task as
in Jansen and Church (2011).
Jansen and Church (2011) approached the task of
unsupervised acoustic modeling by first discovering
repetitive patterns in the data, and then learned a
whole-word HMM for each found pattern, where the
state number of each HMM depends on the average
length of the pattern. The states of the whole-word
HMMs were then collapsed and used to represent
acoustic units. Instead of discovering repetitive pat-
terns first, our model is able to learn from any given
data.
Unsupervised Speech Segmentation One goal
of our model is to segment speech data into
small sub-word (e.g., phone) segments. Most un-
supervised speech segmentation methods rely on
acoustic change for hypothesizing phone bound-
aries (Scharenborg et al, 2010; Qiao et al, 2008;
Dusan and Rabiner, 2006; Estevan et al, 2007).
Even though the overall approaches differ, these al-
gorithms are all one-stage and bottom-up segmenta-
tion methods (Scharenborg et al, 2010). Our model
does not make a single one-stage decision; instead, it
infers the segmentation through an iterative process
and exploits the learned sub-word models to guide
its hypotheses on phone boundaries.
Bayesian Model for Segmentation Our model is
inspired by previous applications of nonparametric
Bayesian models to segmentation problems in NLP
and speaker diarization (Goldwater, 2009; Fox et al,
2011); particularly, we adapt the inference method
used in (Goldwater, 2009) to our segmentation task.
Our problem is, in principle, similar to the word seg-
mentation problem discussed in (Goldwater, 2009).
The main difference, however, is that our model
is under the continuous real value domain, and the
problem of (Goldwater, 2009) is under the discrete
symbolic domain. For the domain our problem is ap-
plied to, our model has to include more latent vari-
ables and is more complex.
3 Problem Formulation
The goal of our model, given a set of spoken utter-
ances, is to jointly learn the following:
? Segmentation: To find the phonetic boundaries
within each utterance.
? Nonparametric clustering: To find a proper set
of clusters and group acoustically similar seg-
ments into the same cluster.
? Sub-word modeling: To learn a HMM to model
each sub-word acoustic unit.
We model the three sub-tasks as latent variables
in our approach. In this section, we describe the ob-
served data, latent variables, and auxiliary variables
41
? 
x
2
i
? 
x
3
i
? 
x
4
i
? 
x
5
i
? 
x
6
i
? 
x
7
i
? 
x
8
i
? 
x
9
i
? 
x
10
i
? 
x
11
i
? 
x
1
i
b a n a n a 
? 
(x
t
i
)
? 
(t) 1 2 3 4 5 6 7 8 9 10 11 
? 
(b
t
i
)
? 
(g
q
i
)
? 
g
0
i
? 
g
1
i
? 
g
2
i
? 
g
3
i
? 
g
4
i
? 
g
5
i
? 
g
6
i
? 
(p
j ,k
i
)
? 
p
1,1
i
? 
p
2,4
i
? 
p
5,6
i
? 
p
7,8
i
? 
p
9,9
i
? 
p
10,11
i
? 
(c
j ,k
i
)
? 
c
1,1
i
? 
c
2,4
i
? 
c
5,6
i
? 
c
7,8
i
? 
c
9,9
i
? 
c
10,11
i
? 
(?
c
)
? 
?
1
? 
?
2
? 
?
3
? 
?
4
? 
?
3
? 
?
2
? 
(s
t
i
) 1 1 2 3 1 3 1 3 1 1 3 
Frame index Speech feature Boundary variable Boundary index Segment 
Cluster label 
HMM 
Hidden state 
[b] [ax] [n] [ae] [n] [ax] Pronunciation 
1 0 0 1 0 1 0 1 1  0 1 
Duration 
? 
(d
j,k
i
) 1 3 2 2 1 2 
1 1 6 8 3 7 5 2 8 2 8 Mixture ID 
Figure 1: An example of the observed data and hidden
variables of the problem for the word banana. See Sec-
tion 3 for a detailed explanation.
of the problem and show an example in Fig. 1. In
the next section, we show the generative process our
model uses to generate the observed data.
Speech Feature (xit) The only observed data for
our problem are a set of spoken utterances, which are
converted to a series of 25 ms 13-dimensional Mel-
Frequency Cepstral Coefficients (MFCCs) (Davis
and Mermelstein, 1980) and their first- and second-
order time derivatives at a 10 ms analysis rate. We
use xit ? R
39 to denote the tth feature frame of the
ith utterance. Fig. 1 illustrates how the speech signal
of a single word utterance banana is converted to a
sequence of feature vectors xi1 to x
i
11.
Boundary (bit) We use a binary variable b
i
t to in-
dicate whether a phone boundary exists between xit
and xit+1. If our model hypothesizes x
i
t to be the last
frame of a sub-word unit, which is called a boundary
frame in this paper, bit is assigned with value 1; or 0
otherwise. Fig. 1 shows an example of the boundary
variables where the values correspond to the true an-
swers. We use an auxiliary variable giq to denote the
index of the qth boundary frame in utterance i. To
make the derivation of posterior distributions easier
in Section 5, we define gi0 to be the beginning of
an utterance, and Li to be the number of boundary
frames in an utterance. For the example shown in
Fig. 1, Li is equal to 6.
Segment (pij,k) We define a segment to be com-
posed of feature vectors between two boundary
frames. We use pij,k to denote a segment that con-
sists of xij , x
i
j+1 ? ? ?x
i
k and d
i
j,k to denote the length
of pij,k. See Fig. 1 for more examples.
Cluster Label (cij,k) We use c
i
j,k to specify the
cluster label of pij,k. We assume segment p
i
j,k is gen-
erated by the sub-word HMM with label cij,k.
HMM (?c) In our model, each HMM has three
emission states, which correspond to the beginning,
middle and end of a sub-word unit (Jelinek, 1976).
A traversal of each HMM must start from the first
state, and only left-to-right transitions are allowed
even though we allow skipping of the middle and
the last state for segments shorter than three frames.
The emission probability of each state is modeled by
a diagonal Gaussian Mixture Model (GMM) with 8
mixtures. We use ?c to represent the set of param-
eters that define the cth HMM, which includes state
transition probability aj,kc , and the GMM parameters
of each state emission probability. We use wmc,s ? R,
?mc,s ? R
39 and ?mc,s ? R
39 to denote the weight,
mean vector and the diagonal of the inverse covari-
ance matrix of the mth mixture in the GMM for the
sth state in the cth HMM.
Hidden State (sit) Since we assume the observed
data are generated by HMMs, each feature vector,
xit, has an associated hidden state index. We denote
the hidden state of xit as s
i
t.
Mixture ID (mit) Similarly, each feature vector is
assumed to be emitted by the state GMM it belongs
to. We use mit to identify the Gaussian mixture that
generates xit.
4 Model
We aim to discover and model a set of sub-word
units that represent the spoken data. If we think of
utterances as sequences of repeated sub-word units,
then in order to find the sub-words, we need a model
that concentrates probability on highly frequent pat-
terns while still preserving probability for previously
unseen ones. Dirichlet processes are particulary
suitable for our goal. Therefore, we construct our
model as a Dirichlet Process (DP) mixture model,
of which the components are HMMs that are used
42
parameter of Bernoulli distribution 
? 
?
b
? 
?
? 
?
0
concentration parameter of DP base distribution of DP 
? 
? prior distribution for cluster labels 
? 
b
t
boundary variable 
? 
d
j ,k duration of a segment 
? 
c
j,k
cluster label 
? 
?
c
HMM parameters 
? 
s
t
hidden state 
? 
m
t
Gaussian mixture id 
? 
x
t
observed feature vector deterministic relation 
? 
?
? 
T
? 
?
? 
d
j ,k
? 
?
? 
?
b
? 
?
0
? 
c
j,k
? 
s
t
? 
j,k = g
q
+1,g
q+1
? 
x
t
? 
d
j ,k
? 
m
t
? 
b
t
? 
?
c
? 
0 ? q < L
? 
T
total number of  observed features frames 
? 
L
total number of  segments determined by  
? 
b
t
? 
g
q
the index of the       boundary variable with value 1 
? 
q
th
Figure 2: The graphical model for our approach. The shaded circle denotes the observed feature vectors, and the
squares denote the hyperparameters of the priors used in our model. The dotted arrows indicate deterministic relations.
Note that the Markov chain structure over the st variables is not shown here due to limited space.
to model sub-word units. We assume each spoken
segment is generated by one of the clusters in this
DP mixture model. Here, we describe the genera-
tive process our model uses to generate the observed
utterances and present the corresponding graphical
model. For clarity, we assume that the values of
the boundary variables bit are given in the genera-
tive process. In the next section, we explain how to
infer their values.
Let pi
giq+1,g
i
q+1
for 0 ? q ? Li ? 1 be the seg-
ments of the ith utterance. Our model assumes each
segment is generated as follows:
1. Choose a cluster label ci
giq+1,g
i
q+1
for pi
giq+1,g
i
q+1
.
This cluster label can be either an existing la-
bel or a new one. Note that the cluster label
determines which HMM is used to generate the
segment.
2. Given the cluster label, choose a hidden state
for each feature vector xit in the segment.
3. For each xit, based on its hidden state, choose a
mixture from the GMM of the chosen state.
4. Use the chosen Gaussian mixture to generate
the observed feature vector xit.
The generative process indicates that our model
ignores utterance boundaries and views the entire
data as concatenated spoken segments. Given this
viewpoint, we discard the utterance index, i, of all
variables in the rest of the paper.
The graphical model representing this generative
process is shown in Fig. 2, where the shaded circle
denotes the observed feature vectors, and the squares
denote the hyperparameters of the priors used in our
model. Specifically, we use a Bernoulli distribution
as the prior of the boundary variables and impose
a Dirichlet process prior on the cluster labels and
the HMM parameters. The dotted arrows represent
deterministic relations. For example, the boundary
variables deterministically construct the duration of
each segment, d, which in turn sets the number of
feature vectors that should be generated for a seg-
ment. In the next section, we show how to infer the
value of each of the latent variables in Fig. 21.
5 Inference
We employ Gibbs sampling (Gelman et al, 2004)
to approximate the posterior distribution of the hid-
den variables in our model. To apply Gibbs sam-
pling to our problem, we need to derive the condi-
tional posterior distributions of each hidden variable
of the model. In the following sections, we first de-
rive the sampling equations for each hidden variable
and then describe how we incorporate acoustic cues
to reduce the sampling load at the end.
1Note that the value of pi is irrelevant to our problem; there-
fore, it is integrated out in the inference process
43
5.1 Sampling Equations
Here we present the sampling equations for each
hidden variable defined in Section 3. We use
P (?| ? ? ? ) to denote a conditional posterior probabil-
ity given observed data, all the other variables, and
hyperparameters for the model.
Cluster Label (cj,k) Let C be the set of distinctive
label values in c?j,k, which represents all the cluster
labels except cj,k. The conditional posterior proba-
bility of cj,k for c ? C is:
P (cj,k = c| ? ? ? ) ? P (cj,k = c|c?j,k; ?)P (pj,k|?c)
=
n(c)
N ? 1 + ?
P (pj,k|?c) (1)
where ? is a parameter of the DP prior. The first line
of Eq. 1 follows Bayes? rule. The first term is the
conditional prior, which is a result of the DP prior
imposed on the cluster labels 2. The second term is
the conditional likelihood, which reflects how likely
the segment pj,k is generated by HMMc. We use n(c)
to represent the number of cluster labels in c?j,k tak-
ing the value c and N to represent the total number
of segments in current segmentation.
In addition to existing cluster labels, cj,k can also
take a new cluster label, which corresponds to a new
sub-word unit. The corresponding conditional pos-
terior probability is:
P (cj,k 6= c, c ? C| ? ? ? ) ?
?
N ? 1 + ?
?
?
P (pj,k|?) d?
(2)
To deal with the integral in Eq. 2, we follow the
suggestions in (Rasmussen, 2000; Neal, 2000). We
sample an HMM from the prior and compute the
likelihood of the segment given the new HMM to
approximate the integral.
Finally, by normalizing Eq. 1 and Eq. 2, the Gibbs
sampler can draw a new value for cj,k by sampling
from the normalized distribution.
Hidden State (st) To enforce the assumption that
a traversal of an HMM must start from the first state
and end at the last state3, we do not sample hidden
state indices for the first and the last frame of a seg-
ment. For each of the remaining feature vectors in
2See (Neal, 2000) for an overview on Dirichlet process mix-
ture models and the inference methods.
3If a segment has only 1 frame, we assign the first state to it.
a segment pj,k, we sample a hidden state index ac-
cording to the conditional posterior probability:
P (st = s| ? ? ? ) ?
P (st = s|st?1)P (xt|?cj,k , st = s)P (st+1|st = s)
= ast?1,scj,k P (xt|?cj,k , st = s)a
s,st+1
cj,k (3)
where the first term and the third term are the condi-
tional prior ? the transition probability of the HMM
that pj,k belongs to. The second term is the like-
lihood of xt being emitted by state s of HMMcj,k .
Note for initialization, st is sampled from the first
prior term in Eq. 3.
Mixture ID (mt) For each feature vector in a seg-
ment, given the cluster label cj,k and the hidden state
index st, the derivation of the conditional posterior
probability of its mixture ID is straightforward:
P (mt = m| ? ? ? )
? P (mt = m|?cj,k , st)P (xt|?cj,k , st,mt = m)
= wmcj,k,stP (xt|?
m
cj,k,st , ?
m
cj,k,st) (4)
where 1 ? m ? 8. The conditional posterior con-
sists of two terms: 1) the mixing weight of the mth
Gaussian in the state GMM indexed by cj,k and st
and 2) the likelihood of xt given the Gaussian mix-
ture. The sampler draws a value for mt from the
normalized distribution of Eq. 4.
HMM Parameters (?c) Each ?c consists of two
sets of variables that define an HMM: the state emis-
sion probabilities wmc,s, ?
m
c,s, ?
m
c,s and the state transi-
tion probabilities aj,kc . In the following, we derive
the conditional posteriors of these variables.
Mixture Weight wmc,s: We use wc,s = {w
m
c,s|1 ?
m ? 8} to denote the mixing weights of the Gaus-
sian mixtures of state s of HMM c. We choose a
symmetric Dirichlet distribution with a positive hy-
perparameter ? as its prior. The conditional poste-
rior probability of wc,s is:
P (wc,s| ? ? ? ) ? P (wc,s;?)P (mc,s|wc,s)
? Dir(wc,s;?)Mul(mc,s;wc,s)
? Dir(wc,s;?
?) (5)
where mc,s is the set of mixture IDs of feature vec-
tors that belong to state s of HMM c. The mth entry
of ?? is ? +
?
mt?mc,s ?(mt,m), where we use ?(?)
44
P (pl,t, pt+1,r|c?,?) = P (pl,t|c?,?)P (pt+1,r|c?, cl,t,?)
=
[
?
c?C
n(c)
N? + ?
P (pl,t|?c) +
?
N? + ?
?
?
P (pl,t|?) d?
]
?
[
?
c?C
n(c) + ?(cl,t, c)
N? + 1 + ?
P (pt+1,r|?c) +
?
N? + 1 + ?
?
?
P (pt+1,r|?) d?
]
P (pl,r|c?,?) =
?
c?C
n(c)
N? + ?
P (pl,r|?c) +
?
N? + ?
?
?
P (pl,r|?) d?
Figure 3: The full derivation of the relative conditional posterior probabilities of a boundary variable.
to denote the discrete Kronecker delta. The last line
of Eq. 5 comes from the fact that Dirichlet distribu-
tions are a conjugate prior for multinomial distribu-
tions. This property allows us to derive the update
rule analytically.
Gaussian Mixture ?mc,s, ?
m
c,s: We assume the di-
mensions in the feature space are independent. This
assumption allows us to derive the conditional pos-
terior probability for a single-dimensional Gaussian
and generalize the results to other dimensions.
Let the dth entry of ?mc,s and ?
m
c,s be ?
m,d
c,s and
?m,dc,s . The conjugate prior we use for the two vari-
ables is a normal-Gamma distribution with hyperpa-
rameters ?0, ?0, ?0 and ?0 (Murphy, 2007).
P (?m,dc,s , ?
m,d
c,s |?0, ?0, ?0, ?0)
= N(?m,dc,s |?0, (?0?
m,d
c,s )
?1)Ga(?m,dc,s |?0, ?0)
By tracking the dth dimension of feature vectors
x ? {xt|mt = m, st = s, cj,k = c, xt ? pj,k}, we
can derive the conditional posterior distribution of
?m,dc,s and ?
m,d
c,s analytically following the procedures
shown in (Murphy, 2007). Due to limited space,
we encourage interested readers to find more details
in (Murphy, 2007).
Transition Probabilities aj,kc : We represent the
transition probabilities at state j in HMM c using ajc.
If we view ajc as mixing weights for states reachable
from state j, we can simply apply the update rule
derived for the mixing weights of Gaussian mixtures
shown in Eq. 5 to ajc. Assume we use a symmetric
Dirichlet distribution with a positive hyperparameter
? as the prior, the conditional posterior for ajc is:
P (ajc| ? ? ? ) ? Dir(a
j
c; ?
?)
where the kth entry of ?? is ? + nj,kc , the number
of occurrences of the state transition pair (j, k) in
segments that belong to HMM c.
Boundary Variable (bt) To derive the conditional
posterior probability for bt, we introduce two vari-
ables:
l = (argmax
gq
gq < t) + 1
r = argmin
gq
t < gq
where l is the index of the closest turned-on bound-
ary variable that precedes bt plus 1, while r is the in-
dex of the closest turned-on boundary variable that
follows bt. Note that because g0 and gL are defined,
l and r always exist for any bt.
Note that the value of bt only affects segmentation
between xl and xr. If bt is turned on, the sampler hy-
pothesizes two segments pl,t and pt+1,r between xl
and xr. Otherwise, only one segment pl,r is hypoth-
esized. Since the segmentation on the rest of the data
remains the same no matter what value bt takes, the
conditional posterior probability of bt is:
P (bt = 1| ? ? ? ) ? P (pl,t, pt+1,r|c?,?) (6)
P (bt = 0| ? ? ? ) ? P (pl,r|c?,?) (7)
where we assume that the prior probabilities for
bt = 1 and bt = 0 are equal; c? is the set of cluster
labels of all segments except those between xl and
xr ; and ? indicates the set of HMMs that have as-
sociated segments. Our Gibbs sampler hypothesizes
bt?s value by sampling from the normalized distribu-
tion of Eq. 6 and Eq. 7. The full derivations of Eq. 6
and Eq. 7 are shown in Fig. 3.
Note that in Fig. 3, N? is the total number of seg-
ments in the data except those between xl and xr.
45
For bt = 1, to account the fact that when the model
generates pt+1,r, pl,t is already generated and owns
a cluster label, we sample a cluster label for pl,t that
is reflected in the Kronecker delta function. To han-
dle the integral in Fig. 3, we sample one HMM from
the prior and compute the likelihood using the new
HMM to approximate the integral as suggested in
(Rasmussen, 2000; Neal, 2000).
5.2 Heuristic Boundary Elimination
To reduce the inference load on the boundary vari-
ables bt, we exploit acoustic cues in the feature space
to eliminate bt?s that are unlikely to be phonetic
boundaries. We follow the pre-segmentation method
described in Glass (2003) to achieve the goal. For
the rest of the boundary variables that are proposed
by the heuristic algorithm, we randomly initialize
their values and proceed with the sampling process
described above.
6 Experimental Setup
To the best of our knowledge, there are no stan-
dard corpora for evaluating unsupervised methods
for acoustic modeling. However, numerous related
studies have reported performance on the TIMIT
corpus (Dusan and Rabiner, 2006; Estevan et al,
2007; Qiao et al, 2008; Zhang and Glass, 2009;
Zhang et al, 2012), which creates a set of strong
baselines for us to compare against. Therefore, the
TIMIT corpus is chosen as the evaluation set for
our model. In this section, we describe the methods
used to measure the performance of our model on
the following three tasks: sub-word acoustic model-
ing, segmentation and nonparametric clustering.
Unsupervised Segmentation We compare the
phonetic boundaries proposed by our model to the
manual labels provided in the TIMIT dataset. We
follow the suggestion of (Scharenborg et al, 2010)
and use a 20-ms tolerance window to compute re-
call, precision rates and F-score of the segmentation
our model proposed for TIMIT?s training set. We
compare our model against the state-of-the-art un-
supervised and semi-supervised segmentation meth-
ods that were also evaluated on the TIMIT training
set (Dusan and Rabiner, 2006; Qiao et al, 2008).
Nonparametric Clustering Our model automat-
ically groups speech segments into different clus-
ters. One question we are interested in answering
is whether these learned clusters correlate to En-
glish phones. To answer the question, we develop
a method to map cluster labels to the phone set in
a dataset. We align each cluster label in an utter-
ance to the phone(s) it overlaps with in time by
using the boundaries proposed by our model and
the manually-labeled ones. When a cluster label
overlaps with more than one phone, we align it
to the phone with the largest overlap.4 We com-
pile the alignment results for 3696 training utter-
ances5 and present a confusion matrix between the
learned cluster labels and the 48 phonetic units used
in TIMIT (Lee and Hon, 1989).
Sub-word Acoustic Modeling Finally, and most
importantly, we need to gauge the quality of the
learned sub-word acoustic models. In previous
work, Varadarajan et al (2008) and Garcia and
Gish (2006) tested their models on a phone recog-
nition task and a term detection task respectively.
These two tasks are fair measuring methods, but per-
formance on these tasks depends not only on the
learned acoustic models, but also other components
such as the label-to-phone transducer in (Varadara-
jan et al, 2008) and the graphone model in (Garcia
and Gish, 2006). To reduce performance dependen-
cies on components other than the acoustic model,
we turn to the task of spoken term detection, which
is also the measuring method used in (Jansen and
Church, 2011).
We compare our unsupervised acoustic model
with three supervised ones: 1) an English triphone
model, 2) an English monophone model and 3) a
Thai monophone model. The first two were trained
on TIMIT, while the Thai monophone model was
trained with 32 hour clean read Thai speech from
the LOTUS corpus (Kasuriya et al, 2003). All
of the three models, as well as ours, used three-
state HMMs to model phonetic units. To conduct
spoken term detection experiments on the TIMIT
dataset, we computed a posteriorgram representa-
tion for both training and test feature frames over the
4Except when a cluster label is mapped to /vcl/ /b/, /vcl/ /g/
and /vcl/ /d/, where the duration of the release /b/, /g/, /d/ is
almost always shorter than the closure /vcl/. In this case, we
align the cluster label to both the closure and the release.
5The TIMIT training set excluding the sa-type subset.
46
? ?b ? ? ?0 ?0 ?0 ?0
1 0.5 3 3 ?d 5 3 3/?d
Table 1: The values of the hyperparameters of our model,
where ?d and ?d are the dth entry of the mean and the
diagonal of the inverse covariance matrix of training data.
HMM states for each of the four models. Ten key-
words were randomly selected for the task. For ev-
ery keyword, spoken examples were extracted from
the training set and were searched for in the test set
using segmental dynamic time warping (Zhang and
Glass, 2009).
In addition to the supervised acoustic models,
we also compare our model against the state-of-
the-art unsupervised methods for this task (Zhang
and Glass, 2009; Zhang et al, 2012). Zhang and
Glass (2009) trained a GMM with 50 components
to decode posteriorgrams for the feature frames, and
Zhang et al (2012) used a deep Boltzmann machine
(DBM) trained with pseudo phone labels generated
from an unsupervised GMM to produce a posteri-
orgram representation. The evaluation metrics they
used were: 1) P@N, the average precision of the top
N hits, where N is the number of occurrences of each
keyword in the test set; 2) EER: the average equal er-
ror rate at which the false acceptance rate is equal to
the false rejection rate. We also report experimental
results using the P@N and EER metrics.
Hyperparameters and Training Iterations The
values of the hyperparameters of our model are
shown in Table 1, where ?d and ?d are the dth en-
try of the mean and the diagonal of the inverse co-
variance matrix computed from training data. We
pick these values to impose weak priors on our
model.6 We run our sampler for 20,000 iterations,
after which the evaluation metrics for our model all
converged. In Section 7, we report the performance
of our model using the sample from the last iteration.
7 Results
Fig. 4 shows a confusion matrix of the 48 phones
used in TIMIT and the sub-word units learned from
3696 TIMIT utterances. Each circle represents a
mapping pair for a cluster label and an English
phone. The confusion matrix demonstrates a strong
6In the future, we plan to extend the model and infer the
values of these hyperparameters from data directly.
0510
152025
303540
455055
606570
758085
9095100
105110115
120
iy ix ih ey eh y ae ay aw aa ao ah ax uh uw ow oy w l el er r m n en ng z s zh sh ch jh hh v f dh th d b dx g vcl t p k cl epi sil 
 
Figure 4: A confusion matrix of the learned cluster labels
from the TIMIT training set excluding the sa type utter-
ances and the 48 phones used in TIMIT. Note that for
clarity, we show only pairs that occurred more than 200
times in the alignment results. The average co-occurrence
frequency of the mapping pairs in this figure is 431.
correlation between the cluster labels and individ-
ual English phones. For example, clusters 19, 20
and 21 are mapped exclusively to the vowel /ae/. A
more careful examination on the alignment results
shows that the three clusters are mapped to the same
vowel in a different acoustic context. For example,
cluster 19 is mapped to /ae/ followed by stop conso-
nants, while cluster 20 corresponds to /ae/ followed
by nasal consonants. This context-dependent rela-
tionship is also observed in other English phones
and their corresponding sets of clusters. Fig. 4 also
shows that a cluster may be mapped to multiple En-
glish phones. For instance, clusters 85 and 89 are
mapped to more than one phone; nevertheless, a
closer look reveals that these clusters are mapped to
/n/, /d/ and /b/, which are sounds with a similar place
of articulation (i.e. labial and dental). These corre-
lations indicate that our model is able to discover the
phonetic composition of a set of speech data without
any language-specific knowledge.
The performance of the four acoustic models on
the spoken term detection task is presented in Ta-
ble 2. The English triphone model achieves the best
P@N and EER results and performs slightly bet-
ter than the English monophone model, which indi-
cates a correlation between the quality of an acous-
tic model and its performance on the spoken term
detection task. Although our unsupervised model
does not perform as well as the supervised English
47
unit(%) P@N EER
English triphone 75.9 11.7
English monophone 74.0 11.8
Thai monophone 56.6 14.9
Our model 63.0 16.9
Table 2: The performance of our model and three super-
vised acoustic models on the spoken term detection task.
acoustic models, it generates a comparable EER and
a more accurate detection performance for top hits
than the Thai monophone model. This indicates that
even without supervision, our model captures and
learns the acoustic characteristics of a language au-
tomatically and is able to produce an acoustic model
that outperforms a language-mismatched acoustic
model trained with high supervision.
Table 3 shows that our model improves P@N by
a large margin and generates only a slightly worse
EER than the GMM baseline on the spoken term
detection task. At the end of the training process,
our model induced 169 HMMs, which were used to
compute posteriorgrams. This seems unfair at first
glance because Zhang and Glass (2009) only used
50 Gaussians for decoding, and the better result of
our model could be a natural outcome of the higher
complexity of our model. However, Zhang and
Glass (2009) pointed out that using more Gaussian
mixtures for their model did not improve their model
performance. This indicates that the key reason for
the improvement is our joint modeling method in-
stead of simply the higher complexity of our model.
Compared to the DBM baseline, our model pro-
duces a higher EER; however, it improves the rel-
ative detection precision of top hits by 24.3%. As
indicated in (Zhang et al, 2012), the hierarchical
structure of DBM allows the model to provide a
descent posterior representation of phonetic units.
Even though our model only contains simple HMMs
and Gaussians, it still achieves a comparable, if not
better, performance as the DBM baseline. This
demonstrates that even with just a simple model
structure, the proposed learning algorithm is able
to acquire rich phonetic knowledge from data and
generate a fine posterior representation for phonetic
units.
Table 4 summarizes the segmentation perfor-
mance of the baselines, our model and the heuristic
unit(%) P@N EER
GMM (Zhang and Glass, 2009) 52.5 16.4
DBM (Zhang et al, 2012) 51.1 14.7
Our model 63.0 16.9
Table 3: The performance of our model and the GMM
and DBM baselines on the spoken term detection task.
unit(%) Recall Precision F-score
Dusan (2006) 75.2 66.8 70.8
Qiao et al (2008)* 77.5 76.3 76.9
Our model 76.2 76.4 76.3
Pre-seg 87.0 50.6 64.0
Table 4: The segmentation performance of the baselines,
our model and the heuristic pre-segmentation on TIMIT
training set. *The number of phone boundaries in each
utterance was assumed to be known in this model.
pre-segmentation (pre-seg) method. The language-
independent pre-seg method is suitable for seeding
our model. It eliminates most unlikely boundaries
while retaining about 87% true boundaries. Even
though this indicates that at best our model only
recalls 87% of the true boundaries, the pre-seg re-
duces the search space significantly. In addition,
it also allows the model to capture proper phone
durations, which compensates the fact that we do
not include any explicit duration modeling mecha-
nisms in our approach. In the best semi-supervised
baseline model (Qiao et al, 2008), the number of
phone boundaries in an utterance was assumed to
be known. Although our model does not incorpo-
rate this information, it still achieves a very close
F-score. When compared to the baseline in which
the number of phone boundaries in each utterance
was also unknown (Dusan and Rabiner, 2006), our
model outperforms in both recall and precision, im-
proving the relative F-score by 18.8%. The key dif-
ference between the two baselines and our method
is that our model does not treat segmentation as a
stand-alone problem; instead, it jointly learns seg-
mentation, clustering and acoustic units from data.
The improvement on the segmentation task shown
by our model further supports the strength of the
joint learning scheme proposed in this paper.
8 Conclusion
We present a Bayesian unsupervised approach to the
problem of acoustic modeling. Without any prior
48
knowledge, this method is able to discover phonetic
units that are closely related to English phones, im-
prove upon state-of-the-art unsupervised segmenta-
tion method and generate more precise spoken term
detection performance on the TIMIT dataset. In the
future, we plan to explore phonological context and
use more flexible topological structures to model
acoustic units within our framework.
Acknowledgements
The authors would like to thank Hung-an Chang and
Ekapol Chuangsuwanich for training the English
and Thai acoustic models. Thanks to Matthew John-
son, Ramesh Sridharan, Finale Doshi, S.R.K. Brana-
van, the MIT Spoken Language Systems group and
the anonymous reviewers for helpful comments.
References
Chun-An Chan and Lin-Shan Lee. 2011. Unsupervised
hidden Markov modeling of spoken queries for spo-
ken term detection without speech recognition. In Pro-
ceedings of INTERSPEECH, pages 2141 ? 2144.
Steven B. Davis and Paul Mermelstein. 1980. Com-
parison of parametric representations for monosyllabic
word recognition in continuously spoken sentences.
IEEE Trans. on Acoustics, Speech, and Signal Pro-
cessing, 28(4):357?366.
Sorin Dusan and Lawrence Rabiner. 2006. On the re-
lation between maximum spectral transition positions
and phone boundaries. In Proceedings of INTER-
SPEECH, pages 1317 ? 1320.
Yago Pereiro Estevan, Vincent Wan, and Odette Scharen-
borg. 2007. Finding maximum margin segments in
speech. In Proceedings of ICASSP, pages 937 ? 940.
Emily Fox, Erik B. Sudderth, Michael I. Jordan, and
Alan S. Willsky. 2011. A sticky HDP-HMM with
application to speaker diarization. Annals of Applied
Statistics.
Alvin Garcia and Herbert Gish. 2006. Keyword spotting
of arbitrary words using minimal speech resources. In
Proceedings of ICASSP, pages 949?952.
John S. Garofolo, Lori F. Lamel, William M. Fisher,
Jonathan G. Fiscus, David S. Pallet, Nancy L.
Dahlgren, and Victor Zue. 1993. Timit acoustic-
phonetic continuous speech corpus.
Andrew Gelman, John B. Carlin, Hal S. Stern, and Don-
ald B. Rubin. 2004. Bayesian Data Analysis. Texts
in Statistical Science. Chapman & Hall/CRC, second
edition.
James Glass. 2003. A probabilistic framework for
segment-based speech recognition. Computer Speech
and Language, 17:137 ? 152.
Sharon Goldwater. 2009. A Bayesian framework for
word segmentation: exploring the effects of context.
Cognition, 112:21?54.
Aren Jansen and Kenneth Church. 2011. Towards un-
supervised training of speaker independent acoustic
models. In Proceedings of INTERSPEECH, pages
1693 ? 1696.
Frederick Jelinek. 1976. Continuous speech recogni-
tion by statistical methods. Proceedings of the IEEE,
64:532 ? 556.
Sawit Kasuriya, Virach Sornlertlamvanich, Patcharika
Cotsomrong, Supphanat Kanokphara, and Nattanun
Thatphithakkul. 2003. Thai speech corpus for Thai
speech recognition. In Proceedings of Oriental CO-
COSDA, pages 54?61.
Kai-Fu Lee and Hsiao-Wuen Hon. 1989. Speaker-
independent phone recognition using hidden Markov
models. IEEE Trans. on Acoustics, Speech, and Sig-
nal Processing, 37:1641 ? 1648.
Chin-Hui Lee, Frank Soong, and Biing-Hwang Juang.
1988. A segment model based approach to speech
recognition. In Proceedings of ICASSP, pages 501?
504.
Kevin P. Murphy. 2007. Conjugate Bayesian analysis of
the Gaussian distribution. Technical report, University
of British Columbia.
Radford M. Neal. 2000. Markov chain sampling meth-
ods for Dirichlet process mixture models. Journal
of Computational and Graphical Statistics, 9(2):249?
265.
Yu Qiao, Naoya Shimomura, and Nobuaki Minematsu.
2008. Unsupervised optimal phoeme segmentation:
Objectives, algorithms and comparisons. In Proceed-
ings of ICASSP, pages 3989 ? 3992.
Carl Edward Rasmussen. 2000. The infinite Gaussian
mixture model. In Advances in Neural Information
Processing Systems, 12:554?560.
Odette Scharenborg, Vincent Wan, and Mirjam Ernestus.
2010. Unsupervised speech segmentation: An analy-
sis of the hypothesized phone boundaries. Journal of
the Acoustical Society of America, 127:1084?1095.
Balakrishnan Varadarajan, Sanjeev Khudanpur, and Em-
manuel Dupoux. 2008. Unsupervised learning of
acoustic sub-word units. In Proceedings of ACL-08:
HLT, Short Papers, pages 165?168.
Yaodong Zhang and James Glass. 2009. Unsuper-
vised spoken keyword spotting via segmental DTW
on Gaussian posteriorgrams. In Proceedings of ASRU,
pages 398 ? 403.
Yaodong Zhang, Ruslan Salakhutdinov, Hung-An Chang,
and James Glass. 2012. Resource configurable spoken
query detection using deep Boltzmann machines. In
Proceedings of ICASSP, pages 5161?5164.
49
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1?6,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Applications of GPC Rules and Character Structures in Games for 
Learning Chinese Characters 
?Wei-Jie Huang ?Chia-Ru Chou ?Yu-Lin Tzeng ?Chia-Ying Lee ?Chao-Lin Liu 
??National Chengchi University, Taiwan ???Academia Sinica, Taiwan 
?chaolin@nccu.edu.tw, ?chiaying@gate.sinica.edu.tw 
Abstract 
We demonstrate applications of psycholin-
guistic and sublexical information for learn-
ing Chinese characters. The knowledge 
about the grapheme-phoneme conversion 
(GPC) rules of languages has been shown to 
be highly correlated to the ability of reading 
alphabetic languages and Chinese. We build 
and will demo a game platform for 
strengthening the association of phonologi-
cal components in Chinese characters with 
the pronunciations of the characters. Results 
of a preliminary evaluation of our games 
indicated significant improvement in learn-
ers? response times in Chinese naming 
tasks. In addition, we construct a Web-
based open system for teachers to prepare 
their own games to best meet their teaching 
goals. Techniques for decomposing Chinese 
characters and for comparing the similarity 
between Chinese characters were employed 
to recommend lists of Chinese characters 
for authoring the games. Evaluation of the 
authoring environment with 20 subjects 
showed that our system made the authoring 
of games more effective and efficient. 
1 Introduction 
Learning to read and write Chinese characters is a 
challenging task for learners of Chinese. To read 
everyday news articles, one needs to learn thou-
sands of Chinese characters. The official agents in 
Taiwan and China, respectively, chose 5401 and 
3755 characters as important basic characters in 
national standards. Consequently, the general pub-
lic has gained the impression that it is not easy to 
read Chinese articles, because each of these thou-
sands of characters is written in different ways. 
Teachers adopt various strategies to help learn-
ers to memorize Chinese characters. An instructor 
at the University of Michigan made up stories 
based on decomposed characters to help students 
remember their formations  (Tao, 2007). Some take 
linguistics-based approaches. Pictogram is a major 
formation of Chinese characters, and radicals carry 
partial semantic information about Chinese charac-
ters. Hence, one may use radicals as hints to link 
the meanings and writings of Chinese characters. 
For instance, ???(he2, river) [Note: Chinese char-
acters will be followed by their pronunciations, 
denoted in Hanyu pinyin, and, when necessary, an 
English translation.], ???(hai3, sea), and 
???(yang2, ocean) are related to huge water sys-
tems, so they share the semantic radical, ?, which 
is a pictogram for ?water? in Chinese. Applying 
the concepts of pictograms, researchers designed 
games, e.g.,  (Lan et al, 2009) and animations, e.g., 
(Lu, 2011) for learning Chinese characters. 
The aforementioned approaches and designs 
mainly employ visual stimuli in activities. We re-
port exploration of using the combination of audio 
and visual stimuli. In addition to pictograms, more 
than 80% of Chinese characters are phono-
semantic characters (PSCs, henceforth)  (Ho and 
Bryant, 1997). A PSC consists of a phonological 
component (PC, henceforth) and a semantic com-
ponent. Typically, the semantic components are the 
radicals of PSCs. For instance, ???(du2), 
???(du2), ??? (du2), ???(du2) contain different 
radicals, but they share the same phonological 
components, ???(mai4), on their right sides. Due 
to the shared PC, these four characters are pro-
nounced in exactly the same way. If a learner can 
learn and apply this rule, one may guess and read 
???(du2) correctly easily. 
In the above example, ??? is a normal Chinese 
character, but not all Chinese PCs are standalone 
characters. The characters ???(jian3), ??? 
(jian3), and ???(jian3) share their PCs on their 
right sides, but that PC is not a standalone Chinese 
character. In addition, when a PC is a standalone 
character, it might not indicate its own or similar 
pronunciation when it serves as a PC in the hosting 
character, e.g., ??? and ??? are pronounced as 
/mai4/ and /du2/, respectively. In contrast, the pro-
nunciations of ???, ???, ???, and ??? are 
/tao2/. 
Pronunciations of specific substrings in words of 
alphabetic languages are governed by grapheme-
phoneme conversion (GPC) rules, though not all 
languages have very strict GPC rules. The GPC 
rules in English are not as strict as those in Finish 
1
 (Ziegler and Goswami, 2005), for instance. The 
substring ?ean? are pronounced consistently in 
?bean?, ?clean?, and ?dean,? but the substring ?ch? 
does not have a consistent pronunciation in 
?school?, ?chase?, and ?machine.? PCs in Chinese 
do not follow strict GPC rules either, but they re-
main to be good agents for learning to read. 
Despite the differences among phoneme systems 
and among the degrees of strictness of the GPC 
rules in different languages, ample psycholinguis-
tic evidences have shown that phonological aware-
ness is a crucial factor in predicting students? read-
ing ability, e.g.,  (Siok and Fletcher, 2001). Moreo-
ver, the ability to detect and apply phonological 
consistency in GPCs, including the roles of PCs in 
PSCs in Chinese, plays an instrumental role in 
learners? competence in reading Chinese. Phono-
logical consistency is an important concept for 
learners of various alphabetic languages  (Jared et 
al., 1990; Ziegler and Goswami, 2005) and of Chi-
nese, e.g., (Lee et al, 2005), and is important for 
both young readers  (Ho and Bryant, 1997; Lee, 
2009) and adult readers  (Lin and Collins, 2012). 
This demonstration is unique on two aspects: (1) 
students play games that are designed to strengthen 
the association between Chinese PCs and the pro-
nunciations of hosting characters and (2) teachers 
compile the games with tools that are supported by 
sublexical information in Chinese. The games aim 
at implicitly informing players of the Chinese GPC 
rules, mimicking the process of how infants would 
apply statistical learning  (Saffran et al, 1996). We 
evaluated the effectiveness of the game platform 
with 116 students between grade 1 and grade 6 in 
Taiwan, and found that the students made progress 
in the Chinese naming tasks. 
As we will show, it is not trivial to author games 
for learning a GPC rule to meet individualized 
teaching goals. For this reason, techniques reported 
in a previous ACL conference for decomposing 
and comparing Chinese characters were employed 
to assist the preparation of games (Liu et al, 2011). 
Results of our evaluation showed that the author-
ing tool facilitates the authoring process, improv-
ing both efficiency and effectiveness. 
We describe the learning games in Section 2, 
and report the evaluation results of the games in 
Section 3. The authoring tool is presented in Sec-
tion 4, and its evaluation is discussed in Section 5. 
We provide some concluding remarks in Section 6. 
2 The Learning Games 
A game platform should include several functional 
components such 
as the manage-
ment of players? 
accounts and the 
maintenance of 
players? learning 
profiles. Yet, due 
to the page limits, 
we focus on the 
parts that are 
most relevant to the demonstration. 
Figure 1 shows a screenshot when a player is 
playing the game. This is a game of ?whac-a-
mole? style. The target PC appears in the upper 
middle of the window (???(li3) in this example), 
and a character and an accompanying monster (one 
at a time) will pop up randomly from any of the six 
holes on the ground. The player will hear the pro-
nunciation of the character (i.e., ???(li3)), such 
that the player receives both audio and visual stim-
uli during a game. Players? task is to hit the mon-
sters for the characters that contain the shown PC. 
The box at the upper left corner shows the current 
credit (i.e., 3120) of the player. The player?s credit 
will be increased or decreased if s/he hits a correct 
or an incorrect character, respectively. If the player 
does not hit, the credit will remain the same. Play-
ers are ranked, in the Hall of Fame, according to 
their total credits to provide an incentive for them 
to play the game after school. 
In Figure 1, the player has to hit the monster be-
fore the monster disappears to get the credit. If the 
player does not act in time, the credit will not 
change. 
On ordinary computers, the player manipulates 
the mouse to hit the monster. On multi-touch tablet 
computers, the play can just touch the monsters 
with fingers. Both systems will be demoed. 
2.1 Challenging Levels 
At the time of logging into the game, players can 
choose two parameters: (1) class level: lower class 
(i.e., grades 1 and 2), middle class (i.e., grades 3 
and 4), or upper class (i.e., grades 5 and 6) and (2) 
speed level: the duration between the monsters? 
popping up and going down. The characters for 
lower, middle, and upper classes vary in terms of 
frequency and complexity of the characters. A stu-
dent can choose the upper class only if s/he is in 
the upper class or if s/he has gathered sufficient 
credits. There are three different speeds for the 
monsters to appear and hide: 2, 3, and 5 seconds. 
Choosing different combinations of these two pa-
Figure 1. The learning game 
2
rameters affect how the credits are added or de-
ducted when the players hit the monsters correctly 
or incorrectly, respectively. Table 1 shows the in-
crements of credits for different settings. The num-
bers on the leftmost column are speed levels. 
2.2  Feedback Information 
After finishing a 
game, the player 
receives feed-
back about the 
correct and in-
correct actions 
that were taken 
during the game. 
Figure 2 shows 
such an example. 
The feedback informs the players what characters 
were correctly hit (???(mai2), ???(li3), 
???(li3), and ???(li3)), incorrectly hit 
(???(ting2) and ???(show4)), and should have 
been hit (???(li2)). When the player moves mouse 
over these characters, a sample Chinese word that 
shows how the character is used in daily lives will 
show up in a vertical box near the middle (i.e., 
????(li3 mian4)). 
The main purpose of providing the feedback in-
formation is to allow players a chance to reflect on 
what s/he had done during the game, thereby 
strengthening the learning effects. 
On the upper right hand side of Figure 2 are four 
tabs for more functions. Clicking on the top tab 
(???) will take the player to the next game. In 
the next game, the focus will switch to a different 
PC. The selection of the next PC is random in the 
current system, but we plan to make the switching 
from a game to another adaptive to the students? 
performance in future systems. Clicking on the 
second tab (???) will see the player list in the 
Hall of Fame, clicking on the third tab 
(?????) will return to the main menu, and 
clicking on the fourth (???) will lead to games 
for extra credits. We have extended our games to 
lead students to learning Chinese words from char-
acters, and details will be illustrated during the 
demo. 
2.3 Behind the Scene 
The data structure of a game is simple. When com-
piling a game, a teacher selects the PC for the 
game, and prepares six characters that contain the 
PC (to be referred as an In-list henceforth) and 
four characters as distracter characters that do not 
contain the PC (to be referred as an Out-list hence-
forth). The simplest internal form of a game looks 
like {target PC= ???, In-list= ????????, 
Out-list= ?????? }. We can convert this struc-
ture into a game easily. Through this simple struc-
ture, teachers choose the PCs to teach with charac-
ter combinations of different challenging levels. 
During the process of playing, our system ran-
domly selects one character from the list of 10 
characters. In a game, 10 characters will be pre-
sented to the player. 
3 Preliminary Evaluation and Analysis 
The game platform was evaluated with 116 stu-
dents, and was found to shorten students? response 
times in Chinese naming tasks. 
3.1 Procedure and Participants 
The evaluation was conducted at an elementary 
school in Taipei, Taiwan, during the winter break 
between late January and the end of February 
2011. The lunar new year of 2011 happened to be 
within this period. 
Students were divided into an experimental 
group and a control group. We taught students of 
the experimental group and showed them how to 
play the games in class hours before the break be-
gan. The experimental group had one month of 
time to play the games, but there were no rules 
asking the participants how much time they must 
spend on the games. Instead, they were told that 
they would be rewarded if they were ranked high 
in the Hall of Fame. Table 2 shows the numbers of 
participants and their actual class levels. 
As we explained in Section 2.1, a player could 
choose the class level before the game begins. 
Hence, for example, it is possible for a lower class 
player to play the games designed for middle or 
even upper class levels to increase their credits 
faster. However, if the player is not competent, the 
credits may be deducted faster as well. In the eval-
uation, 20 PCs were used in the games for each 
class level in Table 1. 
Pretests and posttests were administered with the 
standardized (1) Chinese Character Recognition 
Figure 2. Feedback information
 Lower Middle Upper 
Experimental 11 23 24 
Control 11 23 24 
Table 2. Number of participants 
 Lower Middle Upper 
5 10 20 30 
3 15 25 35 
2 20 30 40 
Table 1.Credits for challenging levels
3
Test (CCRT) and (2) Rapid Automatized Naming 
Task (RAN). In CCRT, participants needed to 
write the pronunciations in Jhuyin, which is a pho-
netic system used in Taiwan, for 200 Chinese 
characters. The number of correctly written 
Jhuyins for the characters was recorded. In RAN, 
participants read 20 Chinese characters as fast as 
they could, and their speeds and accuracies were 
recorded. 
3.2 Results and Analysis 
Table 3 shows the statistics for the control group. 
After the one month evaluation period, the perfor-
mance of the control group did not change signifi-
cantly, except participants in the upper class. This 
subgroup improved their speeds in RAN. (Statisti-
cally significant numbers are highlighted.) 
Table 4 shows the statistics for the experimental 
group. After the evaluation period, the speeds in 
RAN of all class levels improved significantly. 
The correct rates in RAN of the control group 
did not improve or fall, though not statistically sig-
nificant. In contrast, the correct rates in RAN of 
the experimental group improved, but the im-
provement was not statistically significant either. 
The statistics for the CCRT tests were not statis-
tically significant. The only exception is that the 
middle class in the experimental group achieved 
better CCRT results. We were disappointed in the 
falling of the performance in CCRT of the lower 
class, though the change was not significant. The 
lower class students were very young, so we con-
jectured that it was harder for them to remember 
the writing of Jhuyin symbols after the winter 
break. Hence, after the evaluation, we strengthened 
the feedback by adding Jhuyin information. In Fig-
ure 2, the Jhuyin information is now added beside 
the sample Chinese words, i.e., ???? (li3 mian4). 
4 An Open Authoring Tool for the Games 
Our game platform has attracted the attention of 
teachers of several elementary schools. To meet 
the teaching goals of teacher in different areas, we 
have to allow the teachers to compile their own 
games for their needs. 
The data structure for a game, as we explained 
in Section  2.3, is not complex. A teacher needs to 
determine the PC to be taught first, then s/he must 
choose an In-list and an Out-list. In the current im-
plementation, we choose to have six characters in 
the In-list and four characters in the Out-list. We 
allow repeated characters when the qualified char-
acters are not enough. 
This authoring process is far less trivial as it 
might seem to be. In a previous evaluation, even 
native speakers of Chinese found it challenging to 
list many qualified characters out of the sky. Be-
cause PCs are not radicals, ordinary dictionaries 
would not help very much. For instance, ??? 
(mai2), ???(li2), ???(li3), and ???(li3) belong 
to different radicals and have different pronuncia-
tions, so there is no simple way to find them at just 
one place. 
Identifying characters for the In-list of a PC is 
not easy, and finding the characters for the Out-list 
is even more challenging. In Figure 1, ??? (li3) is 
the PC to teach in the game. Without considering 
the characters in In-list for the game, we might 
believe that ??? (jia3) and ??? (cheng2) look 
equally similar to ???, so both are good distract-
ers. If, assuming that ???(li3) is in the In-list, 
??? (jia3) will be a better distracter than ??? 
(cheng2) for the Out-list, because ??? and ??? 
are more similar in appearance. By contrast, if we 
have ??? in the In-list, we may prefer to having 
??? (cheng2) than having ??? in the Out-list. 
Namely, given a PC to teach and a selected In-
list, the ?quality? of the Out-list is dependent on 
the characters in In-list. Out-lists of high quality 
influence the challenging levels of the games, and 
will become a crucial ingredient when we make the 
games adaptive to players? competence. 
4.1 PC Selection 
Control Group 
 Class Pretests Posttests p-value 
CCRT 
(charac-
ters) 
Lower 59 61 .292 
Middle 80 83 .186 
Upper 117 120 .268 
RAN 
Correct 
Rate 
Lower 83% 79% .341 
Middle 59% 64% .107 
Upper 89% 89% 1.00 
RAN 
Speed 
(second) 
Lower 23.1 20.6 .149 
Middle 24.3 20.2 .131 
Upper 15.7 14.1 .026 
Table 3. Results for control group
Experimental Group 
 Class Pretests Posttests p-value 
CCRT 
(charac-
ters) 
Lower 64 61 .226 
Middle 91 104 .001 
Upper 122 124 .52 
RAN 
Correct 
Rate 
Lower 73% 76% .574
Middle 70% 75% .171 
Upper 89% 91% .279 
RAN 
Speed 
(second) 
Lower 21.5 16.9 .012 
Middle 24.6 19.0 .001 
Upper 16.9 14.7 <0.001 
Table 4. Results for experimental group 
4
In a realistic teaching situation, a teacher will be 
teaching new characters and would like to provide 
students games that are related to the structures of 
the new characters. Hence, it is most convenient 
for the teachers that our tool decomposes a given 
character and recommends the PC in the character. 
For instance, given ???, we show the teacher that 
we could compile a game for ???. This is achiev-
able using the techniques that we illustrate in the 
next subsection. 
4.2 Character Recommendation 
Given a selected PC, a teacher has to prepare the 
In-list and Out-list for the game. Extending the 
techniques we reported in  (Liu et al, 2011), we 
decompose every Chinese character into a se-
quence of detailed Cangjie codes, which allows us 
to infer the PC contained in a character and to infer 
the similarity between two Chinese characters. 
For instance, the internal codes for ???, ???, 
???, and ??? are, respectively, ?WG?, 
?MGWG?, ?LWG?, and ?MGWL?. The English 
letters denote the basic elements of Chinese char-
acters. For instance, ?WG? stands for ????, 
which are the upper and the lower parts of ???, 
?WL? stands for ????, which could be used to 
rebuild ??? in a sense. By comparing the internal 
codes of Chinese characters, it is possible to find 
that (1) ??? and ??? include ??? and that (2) 
??? and ??? are visually similar based on the 
overlapping codes. 
For the example problem that we showed in 
Figures 1 and 2, we may apply an extended proce-
dure of  (Liu et al, 2011) to find an In-list for ???: 
???????????. This list includes more 
characters than most native speakers can produce 
for ??? within a short period. Similar to what we 
reported previously, it is not easy to find a perfect 
list of characters. More specifically, it was relative-
ly easy to achieve high recall rates, but the preci-
sion rates varied among different PCs. However, 
with a good scoring function to rank the characters, 
it is not hard to achieve quality recommendations 
by placing the characters that actually contain the 
target PCs on top of the recommendation. 
Given that ??? is the target PC and the above 
In-list, we can recommend characters that look like 
the correct characters, e.g., ????? for ???, 
????? for ???, ????? for ???, 
?????? for ??? , and ???? for  ???. 
We employed similar techniques to recommend 
characters for In-lists and Out-lists. The database 
that contains information about the decomposed 
Chinese charac-
ters was the 
same, but we 
utilized different 
object functions 
in selecting and 
ranking the 
characters.  We 
considered all 
elements in a 
character to rec-
ommend charac-
ters for In-lists, but focused on the inclusion of 
target PCs in the decomposed characters to rec-
ommend characters for Out-lists. Again our rec-
ommendations for the Out-lists were not perfect, 
and different ranking functions affect the perceived 
usefulness of the authoring tools.  
Figure 3 shows the step to choose characters in 
the Out-list for characters in the In-list. In this ex-
ample, six characters for the In-list for the PC ? ? 
had been chosen, and were listed near the top: 
????????.  Teachers can find characters 
that are similar to these six correct characters in 
separate pull-down lists. The screenshot shows the 
operation to choose a character that is similar to 
??? (yao2) from the pull-down list. The selected 
character would be added into the Out-list. 
4.3 Game Management 
We allow teachers to apply for accounts and pre-
pare the games based on their own teaching goals. 
However, we cannot describe this management 
subsystem for page limits. 
5 Evaluation of the Authoring Tool 
We evaluated how well our tools can help teachers 
with 20 native speakers. 
5.1 Participants and Procedure 
We recruited 20 native speakers of Chinese: nine 
of them are undergraduates, and the rest are gradu-
ate students. Eight are studying some engineering 
fields, and the rest are in liberal arts or business. 
The subjects were equally split into two groups. 
The control group used only paper and pens to au-
thor the games, and the experimental group would 
use our authoring tools. We informed and showed 
the experimental group how to use our tool, and 
members of the experimental group must follow an 
illustration to create a sample game before the 
evaluation began. 
Every subject must author 5 games, each for a 
Figure 3. Selecting a character for an Out-list 
5
different PC. A game needed 6 characters in the In-
list and 4 characters in the Out-list. Every evalua-
tor had up to 15 minutes to finish all tasks. 
The games authored by the evaluators were 
judged by psycholinguists who have experience in 
teaching. The highest possible scores for the In-list 
and the Out-list were both 30 for a game. 
5.2 Gains in Efficiency and Effectiveness 
Table 5 shows the results of the evaluation. The 
experimental group outperformed the control 
group in both the quality of the games and in the 
time spent on the authoring task. The differences 
are clearly statistically significant. 
Table 6 shows the scores for the In-list and Out-
list achieved by the control and the experimental 
groups. Using the authoring tools helped the evalu-
ators to achieved significantly higher scores for the 
Out-list. Indeed, it is not easy to find characters 
that (1) are similar to the characters in the In-list 
and (2) cannot contain the target PC. 
Due to the page limits, we could not present the 
complete authoring system, but hope to have the 
chance to show it during the demonstration. 
6 Concluding Remarks 
We reported a game for strengthening the associa-
tion of the phonetic components and the pronun-
ciations of Chinese characters. Experimental re-
sults indicated that playing the games helped stu-
dents shorten the response times in naming tasks. 
To make our platform more useable, we built an 
authoring tool so that teachers could prepare games 
that meet specific teaching goals. Evaluation of the 
tool with college and graduate students showed 
that our system offered an efficient and effective 
environment for this authoring task. 
Currently, players of our games still have to 
choose challenge levels. In the near future, we 
wish to make the game adaptive to players? compe-
tence by adopting more advanced techniques, in-
cluding the introduction of ?consistency values? 
 (Jared et al, 1990). Evidence shows that foreign 
students did not take advantage of the GPC rules in 
Chinese to learn Chinese characters  (Shen, 2005). 
Hence, it should be interesting to evaluate our sys-
tem with foreign students to see whether our ap-
proach remains effective. 
Acknowledgement 
We thank the partial support of NSC-100-2221-E-004-014 
and NSC-98-2517-S-004-001-MY3 projects of the Nation-
al Science Council, Taiwan. We appreciate reviewers? 
invaluable comments, which we will respond in an ex-
tended version of this paper. 
References  
C. S.-H. Ho and P. Bryant. 1997. Phonological skills are im-
portant in learning to read Chinese, Developmental Psy-
chology, 33(6), 946?951. 
D. Jared, K. McRae, and M. S. Seidenberg. 1990. The basis of 
consistency effects in word naming, J. of Memory & Lan-
guage, 29(6), 687?715. 
Y.-J. Lan, Y.-T. Sung, C.-Y. Wu, R.-L. Wang, and K.-E. 
Chang. 2009. A cognitive interactive approach to Chinese 
characters learning: System design and development, Proc. 
of the Int?l Conf. on Edutainment, 559?564. 
C.-Y. Lee. 2009. The cognitive and neural basis for learning to 
reading Chinese, J. of Basic Education, 18(2), 63?85. 
C.-Y. Lee, J.-L. Tsai, E. C.-I Su, O. J.-L. Tzeng, and D.-L. 
Hung. 2005. Consistency, regularity, and frequency effects 
in naming Chinese characters, Language and Linguistics, 
6(1), 75?107. 
C.-H. Lin and P. Collins. 2012. The effects of L1 and ortho-
graphic regularity and consistency in naming Chinese char-
acters. Reading and Writing. 
C.-L. Liu, M.-H. Lai, K.-W. Tien, Y.-H. Chuang, S.-H. Wu, 
and C.-Y. Lee. 2011. Visually and phonologically similar 
characters in incorrect Chinese words: Analyses, identifica-
tion, and applications, ACM Trans. on Asian Language In-
formation Processing, 10(2), 10:1?39. 
M.-T. P. Lu. 2011. The Effect of Instructional Embodiment 
Designs on Chinese Language Learning: The Use of Em-
bodied Animation for Beginning Learners of Chinese 
Characters, Ph.D. Diss., Columbia University, USA. 
J. R. Saffran, R. N. Aslin, and E. L. Newport. 1996. Statistical 
learning by 8-month-old infants, Science, 274(5294), 
1926?1928. 
H. H. Shen. 2005. An investigation of Chinese-character 
learning strategies among non-native speakers of Chinese, 
System, 33, 49?68. 
W.T. Siok and P. Fletcher. 2001. The role of phonological 
awareness and visual-orthographic skills in Chinese read-
ing acquisition, Developmental Psychology, 37(6), 886?
899. 
H. Tao. 2007. Stories for 130 Chinese characters, textbook 
used at the University of Michigan, USA. 
J. C. Ziegler and U. Goswami. 2005. Reading acquisition, 
developmental dyslexia, and skilled reading across lan-
guages: A psycholinguistic grain size theory, Psychological 
Bulletin, 131(1), 3?29. 
 Avg. scores 
(In-list and Out-list) 
Avg. time 
Control 16.8 15 min 
Experimental 52.8 7.1 min 
p-value < 0.0001 < 0.0001 
Table 5. Improved effectiveness and efficiency 
 Avg. scores 
In-list Out-list 
Control 15.9 1 
Experimental 29.9 22.9 
Table 6. Detailed scores for the average scores
6
Proceedings of the NAACL HLT 2010 First Workshop on Computational Neurolinguistics, pages 45?51,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Hemispheric processing of Chinese polysemy in the disyllabic verb/ noun 
compounds: an event-related potential study 

 
Chih-ying Huang 
Institute of Linguistics  
128, Sec. 2, Academia Road, Taipei,  
Taiwan, R.O.C  
evelynhg@alumni.nccu.edu.tw 
 
Chia-ying Lee 
Institute of Linguistics  
128, Sec. 2, Academia Road, Taipei,  
Taiwan, R.O.C 
chiaying@gate.sinica.edu.tw 
 
 
Abstract 
Through the application of Chinese WordNet, 
the current study used the manipulation of 
visual field and the number of senses of the 
first character in Chinese disyllabic com-
pounds to investigate the representation and 
the hemispheric processing of related senses 
in nouns and verbs. In the previous study, 
Huang et al (2009) have found the ERP evi-
dence to indicate single entry representation 
for Chinese polysemy in the left hemisphere; 
however, in the right hemisphere, they found 
sense inhibition which may be due to (1) the 
nature of hemispheric processing in dealing 
with semantic ambiguity or (2) the semantic 
activation from the separate-entry representa-
tion for senses. To clarify these possibilities, 
the study used the word class judgment task 
with the attempt to push subjects in a deeper 
level of lexical processing. The results re-
vealed sense facilitation effect in the RH and 
suggested that in a deeper level, the RH had 
more possibility to observe the sense facilita-
tion due to different efficiency of cerebral 
hemispheres. 
1 Introduction 
1.1 Homonymy vs. polysemy  
Lexical ambiguity is very common in language. 
Linguistically, homonymy and polysemy are tradi-
tionally distinguished as two types of ambiguity. 
Early behavioral studies on semantic ambiguity 
obtained ambiguity advantage effects (e.g., Ru-
benstein et al, 1970; Jastrzembski, 1981, Millis & 
Button, 1989) in lexical decisions in which ambi-
guous words yielded faster reaction time than un-
ambiguous words. However, the same results were 
not replicated in some other studies (e.g., Bo-
rowsky & Masson, 1996; Azuma & Van Orden, 
1997). More recent psycholinguistic studies found 
that the so-called ambiguity advantage effects were 
in fact resulted from the activation of words having 
related senses rather than that of words having un-
related meanings (e.g., Rodd et al, 2002; Beretta et 
al., 2005; Pylkk?nen et al, 2006). These studies 
were generally in agreement with the linguistic 
assumption in that homonymy and polysemy might 
be represented differently in the mental lexicon. 
1.2 Hemispheric processing of semantic am-
biguity  
The issue of hemispheric processing in combina-
tion with lexical ambiguity have been widely stu-
died (e.g., Burgess & Simpson, 1988; Beeman & 
Chiarello, 1998; Faust & Lavidor, 2003) and sug-
gested that both cerebral hemispheres process word 
meanings in complementary ways. For example, 
Faust and Lavidor (2003) demonstrated that the 
LH benefited most from semantically congruent 
primes related to dominant meaning of ambiguous 
targets while the RH benefited most from semanti-
cally mixed primes. The overall pattern of priming 
was also suggestive of dissociation in the hemis-
pheric meaning retrieval, with the LH engaging in 
fine semantic coding that focused on a single 
meaning interpretation, and the RH engaging in 
coarser semantic coding where multiple alternate 
meanings were activated. Alternatively, Federmei-
45
er and Kutas (1999) offered electrophysiological 
data in a sentence comprehension task to present 
another explanation in hemispheric language 
processing. They suggested that while both hemis-
pheres involved in lexical resolution, they played 
different roles with the LH being ?predictive?, the 
RH being ?integrative?, to complement each other.  
    Pylkk?nen et al, (2006) were the first to focus 
on the investigation of how different but related 
senses were psychologically represented in the 
mental lexicon. Their MEG data suggested the sin-
gle-entry representation for related senses in the 
LH whereas they showed the sense inhibition in 
the RH and interpreted it as a potential sense com-
petition effect. In Chinese, Huang et al (2009) 
demonstrated similar patterns in their ERP data in 
which there was sense facilitation in the LH and 
sense inhibition in the RH. Nevertheless, the ques-
tion concerning the representation of related senses 
in the RH still left unresolved. Early studies on 
Chinese ambiguity such as Lin (1999) obtained 
ambiguity advantage but the calculation of ?senses? 
1included related and unrelated meanings and the 
effect was not reliable enough.  
1.3 Ambiguity in Chinese disyllabic com-
pounds 
In Chinese words recognition process, the issue of 
lexical ambiguity involves the composition of con-
stituent characters and how they contribute to the 
whole word reading. Chinese words differ from 
English in at least two aspects. First, about 80% of 
Chinese words are composed of two characters 
(Huang et al, 2006). Second, unlike the words in 
English, which every word is composed of letters 
corresponding to phonemes, Chinese words consist 
of characters corresponding to morphemes. In oth-
er words, each character in Chinese has its mor-
pheme(s) when they are embedded in two-
character compounds. Therefore, before we look 
into the lexical ambiguity of two-character words 
                                                          
1 The definition of ?sense? in Lin (1999) is different from the 
?sense advantage effect? demonstrated by Rodd et al (2002). 
Lin argued that ?meaning? in past research is used as a general 
term to refer to any kind of linguistic meaning. He claimed 
that, based upon Ahrens (1999) and Ahrens et al (1998), it is 
better to use ?sense? and ?facets? as a measure index. Though 
the ?number of senses? Lin used is a little different from the 
?number of meanings? used by Azuma and Van Orden (1997), 
it is regarded that Lin still did not solve the unreliable findings 
of ambiguity advantage effect.  
as lexical items, we should investigate the sense 
representation of its subcomponent, the representa-
tion of its single character within two-character 
compounds.  
In the circumstance which every character in 
the disyllabic compounds may contribute to word 
recognition, there still exists disparity between the 
roles of the first and second character. In light of 
the studies on the neighborhood size effect, word 
recognition process will be influenced by the com-
position of letters or characters. In English, faci-
litative neighborhood size effects and inhibitory 
neighborhood size effects were robust findings in 
low frequency words (e.g. Andrews, 1989, 1992; 
Grainger and Jacobs, 1996). In the Chinese neigh-
borhood size study (Huang et al, 2006) and eye 
movement study (Tsai et al, 2006), it was sug-
gested that the neighborhood size of the first cha-
racter constituent played a more important role in 
lexical processing than did neighborhood size of 
the second character constituent. Based on the as-
sumption that the first character will play a key 
role in whole word reading, the study primarily 
manipulated the number of senses of the first cha-
racter and attempted to reveal the hypotheses of 
sense representation in the context provided by the 
second character. 
The question left in Huang et al (2009) was 
that whether the sense inhibition in the RH was 
due to the nature of hemispheric processing in 
dealing with semantic ambiguity or the semantic 
activation from the separate-entry representation 
for senses. Considering the sense inhibition in the 
N400 of the ERP component, the pattern in their 
data was that words having many senses were 
more negative than those having few senses. That 
is, there existed competition when the first charac-
ters of the targets had many related senses. Never-
theless, based on the single entry assumption for 
related senses, we assumed sense facilitation for 
the representation of senses.  
In Huang et al (2009), they required subjects 
to make word/ non-word lexical decision, but sub-
jects might make their judgments based on percep-
tual familiarity rather than the involvement of 
lexical access. Previous studies on probabilistic 
phonotactics (Vitevitch and Luce, 1998) or on 
Chinese semantic combinability (Cheng, 2006) 
have demonstrated opposing effects in early and 
late levels of word processing. In order to clarify 
the results in Huang et al (2009), we designed the 
46
word class judgment task to deepen the difficulty 
of the experimental procedure. 
2 The experiment  
By changing the depth of the task, the goal of the 
experiment was to find out if, under the assump-
tion of single entry representation for senses, there 
was a chance to discover the sense facilitation in 
the RH. Suppose the representation of Chinese 
senses had single entry, words having more senses 
should be less negative than few senses in the 
N400 because of the benefits of semantic activa-
tion. On the contrary, if there were multiple entries 
for senses in the RH, words of more senses should 
be more negative than few senses and displayed 
semantic competition and inhibition. 
2.1 Participants  
38 college students (18 to28 years of age, mean 
age 22.39) took part in the experiment (male, right-
handedness). Written consent was obtained from 
all participants. The study was approved by the 
Taiwan governmental ethics committee.  
2.2 Materials 
120 Chinese disyllabic compounds, counterba-
lanced with word class (noun/ verb), were divided 
into four subsets according to visual field (LVF/ 
RVF) and NOS of the first character (few/ many 
senses). Few-sense words were those whose first 
character senses were from 1 to 3 (mean 1.97) 
whereas many-sense words were those whose first 
character senses were over 6 (mean 11.38). Possi-
ble confounding factors such as word frequency, 
NS1, NS2 were controlled.  
The number of senses in the current study was 
collected from the Chinese WordNet, a lexical cor-
pus of Mandarin Chinese and established by Aca-
demia Sinica in Taiwan. The corpus attempts to 
build an up-to-date Chinese lexical network and 
provides complete information of Chinese word 
senses.  
In Chinese, there exists controversy over the 
distinction of verbs and nouns. To avoid this prob-
lem, the resolutions included: (1) to label the word 
class according to the system established in Aca-
demia Sinica balanced corpus of modern Chinese 
and (2) to give pilot pretests to another group of 
people to exclude these possibly confused choices. 
These subjects were asked to use their language 
intuition to write down their word-class judgments 
in a paper sheet containing 120 targets.  
 
Table1. Examples of the stimuli  
No. of 
senses 
Word 
class 
RVF LVF 
Few Noun  ?? 
?a smiling 
face? 
?? 
?a hair pin? 
Few  Verb  ?? 
?to guess a 
riddle? 
?? 
?to take 
medicine? 
Many  Noun  ?? 
?first prize? 
?? 
?green tea? 
Many  Verb  ?? 
?to stoop? 
?? 
?to ex-
change? 
2.3 Procedure 
Each trial began with a white cross presented cen-
trally for 500 ms. Presentation of the target words 
appeared on the screen for 150 ms. The disyllabic 
compound targets were vertically arranged in the 
left or right visual hemifield with inner edge two 
degrees of visual angle from fixation. Presentation 
of numbers from 1 to 9 appeared pseudorandomly 
in the center of the screen in order to control par-
ticipants? eyesight. At the end of each trial, a capi-
tal B was presented in the center to allow eye 
blinking for 1500 ms. Participants were asked not 
to blink their eyes until the appearance to the capi-
tal B to minimize the interference of eye move-
ment. 
Participants were instructed to judge whether 
the compound presented was a noun or a verb. For 
odd-number subjects, they were asked to press the 
response box with both of their index fingers when 
the targets were verbs and with both of their mid-
dle fingers when the targets were nouns. For even-
number subjects, the instruction was the opposite. 
To control the central fixation of eyes, numbers 
from 1 to 9 also appeared pseudorandomly. Odd-
number subjects should press the response box 
with both of their index fingers when number 6 to 
9 was presented centrally on the screen and with 
both of their middle fingers when number 1 to 4 
was on the screen. For even-number subjects, in-
struction reversed. Response time and event-
related potentials data were both collected during 
the process.  
47
 
Figure1. Timing diagram of the experimental procedure 
2.4 Event-related potential recording  
The electroencephalogram was recorded from 64 
electrodes embedded in an electro-cap(QuickCap, 
Neuromedical Supplies, Sterling, Texas, USA), 
referenced to the left and right mastoid, M1, M2 
respectively. Positions of all the electrodes were 
arranged according to the international ten-twenty 
system. The electroencephalogram was conti-
nuously recorded and digitized at a rate of 500 Hz. 
The signal was amplified by SYNAMPS2 (Neu-
roscan Inc., El Paso, Texas, USA) with the band-
pass set at 0.5?100 Hz. Blinks and eye movements 
were monitored via electrodes placed on the infra-
orbital ridges of the left eye (VEOG) and the outer 
canthus left and right electrode (HEOG). A ground 
electrode was placed on the forehead anterior to 
the FZ electrode. Electrode impedance was kept 
below 5 kohms.   
2.5 ERP components  
In the analyses of the ERP waveforms elicited by 
every stimulus in each condition, there were typi-
cally composed of a negative-going peak at around 
100ms (N1), a positive-going peak at around 
200ms (P200), a negative-going peak maximizing 
at around 400 ms (N400) over central and parietal 
electrode sites. Among these, N 170 was regarded 
as the early index for visual detection in word 
processing. In the current study, N170 was used to 
examine the manipulation of visual field. N400 
was characterized as an index sensitive to lan-
guage-related processing and was generally consi-
dered in response to violations of semantic 
expectations (Kutas and Hillyard, 1980). With the 
presentation of a semantically inappropriate or in-
congruent word, a large N400 activity would be 
elicited. In Huang et al (2009), the 400 in the RH 
was regarded as sense competition because words 
with many senses elicit more negativity at around 
400 ms.  
3 Results  
Behavioral accuracy below 70 percent and ERPs 
accepted trials below 16 were excluded from 
ANOVA analyses. Data from 28 of participants 
were used in the following behavioral and ERP 
analyses. 
3.1 Behavioral data 
A 2?2 (number of senses ? visual field) analysis of 
variance (ANOVA) was performed on correct RTs 
and accuracy. For RTs, no significant main effect 
of number of senses (F (1, 27) =0.5, p=.48) and 
interaction (F (1, 27) =1.33, p=.26) was observed. 
A main effect of visual field reached marginally 
significance (F (1, 27) = 3.38, p=.077). Stimuli 
presented to RVF/ LH had the tendency to produce 
shorter response time than those presented to LVF/ 
RH. For accuracy, not any main effect or interac-
tion was obtained. 
3.2 ERP data 
Temporal time windows of interest were N170 
(150-180 ms) and N400 (350-500 ms). The mean 
amplitude of each time window from selected elec-
trodes served as dependent measures in a repeated 
measures analysis of variance (ANOVA).  
3.2.1 N170 (150-180 ms) 
The mean amplitude of N170 was analyzed by 
ANOVA with factors of visual field (LVF/RVF), 
number of senses, and electrodes (P3/P4, P5/P6, 
P7/P8, PO5/ PO6). We obtained a significant visu-
al field ? electrodes interaction F (7,189) =45.34, 
p<.001. Post-hoc comparison indicated that visual 
field simple main effects reached statistical signi-
ficance in all electrodes (p?s<.001). In electrodes 
on the left, P3, P5, P7, PO5, right visual field pres-
entation elicited much greater negativity than left 
visual presentation and vice versa in electrodes on 
the right, P4, P6, P8, and PO8.  
3.2.2 N400 (350-500 ms) 
Mean amplitudes of all conditions were measured 
from 350 to 500ms and subjected to ANOVA with 
48
factors of visual field, the number of senses, elec-
trodes, hemispheres. The midline analysis revealed 
marginal significance of two way interaction be-
tween the number of senses and visual field (F (1, 
27) =3.83, p=.06). In the lateral analysis, there was 
marginal significance of visual field by number of 
senses interaction (F (1, 27) = 3.18, p=.086) and a 
marginally significant 4-way interaction of visual 
field, number of senses, electrodes and hemis-
pheres (F (4, 108) =2.53, p=.072). Post-hoc com-
parisons showed that in the LVF/ RH few senses 
tended to be more negative than many senses 
(p<.05) while in the RVF/ LH, few and many 
senses did not reveal any difference (p=.73).  
 
Figure 2?Grand averaged ERPs at CPZ in the RVF/LH. 
 
Figure 3?Grand averaged ERPs at CPZ in the LVF/RH 
4 Discussion  
In the behavioral data, no significant main effect of 
the number of senses and interaction was observed. 
Nevertheless, the ERP data demonstrated that there 
was marginal significance of two-way interaction 
(visual field ? number of senses) and a marginally 
significant 4-way interaction. Post-hoc comparison 
showed that there were significant sense facilita-
tion effects in the RH and no effect in the LH. ERP 
waveforms showed that words of few senses eli-
cited more negativity than words of many senses 
around 400 ms in the RH, but the two conditions 
did not differ from each other in the LH. 
The marginality of statistical significance led 
to the speculation in that the word category effect 
might dilute the sense effect in the experiment. 
Many studies, in general, suggested that the neural 
systems for lexical processing of nouns and verbs 
were anatomically distinct. For example, in child-
ren?s lexical development, the acquisition of nouns 
seems to be earlier and easier than that of verbs 
(Gentner, 1982). In aphasic findings, case studies 
indicated that patients with lesions located in left 
anterior and middle temporal lobe, outside so 
called language areas, had difficulty in the produc-
tion of nouns whereas patients with lesions areas in 
left frontal premotor cortex had difficulty in the 
production of verbs (Damasio & Damasio, 1992; 
Damasio et al, 1993). Evidence from event-related 
potentials also disclosed electrocortical differences 
between nouns and verbs over widespread cortical 
areas (Pulverm?ller et al, 1999). Therefore, verbs 
were assumed to elicit stronger electrocortical ac-
tivity around primary frontal, prefrontal areas as-
sociated with motor, premotor functions. Nouns, 
associated with concrete and well-imaginable 
meanings related to visual modality, were assumed 
to elicit larger electrocortical activity around visual 
cortices. 
There was also evidence indicating that the 
conclusions were oversimplified. For example, 
Tyler et al (2001, PET) found no significant action 
differences for nouns and verbs in lexical decision 
and semantic categorization task. Similarly, in an 
fMRI Chinese study, Li et al (2004) pointed out 
that nouns and verbs were found to activate a wide 
range of overlapping brain areas and suggested 
distributed networks for either word class. One 
recent Chinese study on concreteness also showed 
similar distribution over the scalp for both nouns 
and verbs (Tsai et al, 2008).  
The study was not meant to resolve the con-
troversy of neural representations for nous and 
verbs. Instead, from the marginal significance of 
the data in the experiment, we speculated that the 
word class effect may influence the results, which 
led to the failure to reach significance in overall 
data. Therefore, we reanalyzed the data with the 
addition word class as one within-subject factor. 
4.1 Re-analyses 
49
To further examine the sense effect in nouns and 
verbs condition, separate analyses of ANOVA 
were carried out according to different word 
classes. 
4.2 Behavioral data 
A 2?2?2 (number of senses ? visual field ? word 
class) analysis of variance (ANOVA) was per-
formed on correct RTs and accuracy. For RTs, re-
sults showed marginally significant effects for 
visual field (F (1, 27) = 3.38, p=.077) and word 
class (F (1, 27) = 2.97, p=.096) and for number of 
senses ? word class interaction (F (1, 27) = 2.94, 
p=.098). Stimuli presented to the RVF/ LH tended 
to responded more quickly than to the LVF/ RH. 
Stimuli of nouns had shorter response time than 
stimuli of verbs. For accuracy analysis, nouns had 
significant higher accuracy than verbs (word class 
(F (1, 27) =5.41, p<.05). 
4.3 ERP data 
The grand mean ERPs elicited by few and many 
senses in RVF/ LH and LVF/ RH were presented 
in nouns and verbs separately.  
4.3.1 Nouns  
In the midline, there was a marginally significant 
number of senses ? electrodes interaction (F (4, 
108) = 2.8, p<.08). Lateral analyses indicated that 
there was a significant visual field ? number of 
senses ? electrode interaction (F (1, 27) = 3.65, 
p<.05). Planned comparison showed that only 
when stimuli presented to the LVF/ RH, few 
senses were more negative in C, CP, P (p?s <.05 to 
<.01).  
4.3.2 Verbs 
In the midline analysis, there was no significant 
main effect of senses or interaction. In the lateral 
analyses, there were significant interactions of vis-
ual field ?  number of senses (F (1, 27) =4.69, 
p<.05) and visual field ? number of senses ? elec-
trodes ? hemispheres (F (4, 108) = 4.23, p<.01). 
Planned comparisons of four way interaction 
showed that when presented to LVF/ RH, few 
senses were more negative in F3, C3, CP3 and FC4 
(p?s<.05 to <.01) whereas when presented to the 
RVF/ LH, there was no difference between few 
and many senses.  
5 Discussion  
The purpose of additional analyses of sense effects 
in nouns and verbs was to examine clearer effects 
of senses without the confounding of the word 
class factor. The separate analyses for nouns and 
verbs both showed significant sense effects in the 
lateral sites. Furthermore, planned comparison of 
the senses demonstrated disparate distributions for 
nouns and verbs respectively. To be more specific, 
the sense effects for nouns were located in central-
to-parietal areas of brain, whereas these effects for 
verbs primarily showed up in frontal, central, cen-
tral-parietal electrodes on the left. The re-analyses 
of ERP data showed that the differences of distri-
bution from either word category diluted the sense 
effect observed in the first analysis; therefore, the 
data was only marginally significant in the original 
analyses. Besides, though the current study was not 
meant to resolve the representations for different 
word categories, the additional results seemed to 
support the distinct neural representations for 
nouns and verbs, since each word class had its dis-
tribution for the sense effects. Certainly, further 
evidence of Chinese word class was required to 
approve the statement since there was also evi-
dence suggesting distributed network for Chinese 
lexical processing (e.g. Li et al, 2004).  
According to previous studies, different levels 
of processing in perception of words would lead to 
opposing results (e.g. Vitevitch and Luce, 1998; 
Cheng, 2006). Suppose the results were derived 
from the single entry representation of senses, the 
sense effect should be observed in the RH in the 
experiment since the depth of the task was changed. 
In other words, when subjects were undergoing a 
deeper level of lexical processing, the relatedness 
of senses might have been early processed in the 
LH due to the engagement in fine semantic 
processing; on the other hand, the sense effect 
might appear in the RH because its capacity al-
lowed alternate meanings to maintain. Hence, in a 
deeper level of task, which slowed down the se-
mantic processing, the facilitative sense effect was 
observed in the RH.   
Overall, we suggested that the representation 
of Chinese senses be single entry and obtained the 
sense facilitation effects in LVF/ RH in which few 
50
senses were more negative than many senses both 
in nouns and verbs. We assumed that the results 
also provided empirical evidence indicating that 
the construction of Chinese WordNet has psycho-
logical validity.  
6 Conclusions 
The study attempted to find out whether the repre-
sentation of senses in the RH was single-or sepa-
rate-entry. When the depth of task was changed, 
the RH advantage for the processing of semantical-
ly related senses was observed. The finding was 
consistent with recent studies on the representation 
of polysemy (e.g. Beretta et al 2005; Pylkk?nen et 
al. 2006, Rodd et al, 2002).  
Acknowledgments 
This work was supported by grants from Taiwan Na-
tional Science Council (NSC 94-2411-H-001-068, 95-
2413-H-010-002?and NSC 96-2628-H-001-058) and the 
theme project of brain, cognition and behavioral science 
from Academia Sinica, Taiwan (AS-93-TP-C05). The 
authors would like to give gratitude to Jie-Li Tsai, Hsu-
wen Huang, Chun-Hsien Hsu and Wen-Hsuan Chan, 
Chiaju Chou who gave technical support, valuable sug-
gestions and comments on the early draft of this paper. 
References  
Academia Sinica balanced corpus (version 3). (1998). 
Academia Sinica, Taipei, Taiwan. 
Azuma, T. & Van Orden, G. C. (1997). Why SAFE Is 
Better Than FAST: The Relatedness of a 
Word's Meanings Affects Lexical Decision 
Times. Journal of Memory and Language, 
36(4), 484-504. 
Beretta, A., Fiorentino, R., & Poeppel, D. (2005). The 
effects of homonymy and polysemy on lexical 
access: an MEG study. Cognitive Brain Re-
search, 24(1), 57-65. 
Burgess, C., & Simpson, G. B. (1988). Cerebral hemis-
pheric mechanisms in the retrieval of ambi-
guous word meanings. Brain Lang, 33(1), 86-
103. 
Damasio A. R. & Daniel, T. (1993). Nouns and verbs 
are retrieved with differently distributed neural 
systems. Paper presented at the Proceedings of 
the National Academy of Science. 
Faust, M., & Lavidor, M. (2003). Semantically conver-
gent and semantically divergent priming in the 
cerebral hemispheres: lexical decision and se-
mantic judgment. Cognitive Brain Research, 
17(3), 585-597. 
Federmeier, K. D. & Kutas, M. (1999). Right words and 
left words: electrophysiological evidence for 
hemispheric differences in meaning processing. 
Cognitive Brain Research, 8(3), 373-392.  
Huang, C-Y, Huang, H-W, Tsai, J-L, Huang, C-C & 
Lee, C-Y (2009, October). Number of senses 
effects of Chinese disyllabic compounds in two 
hemispheres. Poster presented at the 13th In-
ternational Conference on the Processing of 
East Asian Languages, Beijing Normal Univer-
sity, Beijing, China.  
Huang, H. W., Lee, C. Y., Tsai, J. L., Lee, C. L., Hung, 
D. L., & Tzeng, O. J. (2006). Orthographic 
neighborhood effects in reading Chinese two-
character words. Neuroreport, 17(10), 1061-
1065. 
Kutas, M., & Hillyard, S. A. (1980). Reading senseless 
sentences: brain potentials reflect semantic in-
congruity. Science, 207(4427), 203. 
Lyons, J. (1977). Semantics. Cambridge, England: 
Cambridge University Press. 
Pulvermuller, F., Lutzenberger, W., & Preissl, H. (1999). 
Nouns and Verbs in the Intact Brain: Evidence 
from Event-related Potentials and High-
frequency Cortical Responses. Cerebral Cortex, 
9(5), 497-506. 
Pylkk?nen, L., Llin?s, R., & Murphy, G. L. (2006). The 
representation  of Polysemy: MEG Evidence. 
Journal of Cognitive Neuroscience, 18(1), 97-
109. 
Rodd, J., Gaskell, G., & Marslen-Wilson, W. (2002). 
Making Sense of Semantic Ambiguity: Seman-
tic Competition in Lexical Access. Journal of 
Memory and Language, 46(2), 245-266. 
Rubenstein, H., Garfield, L., & Millikan, J. A. (1970). 
Homographic entries in the internal lexicon. 
Journal of Verbal Learning and Verbal Beha-
vior, 9(5), 487?494. 
Vitevitch, M. S., & Luce, P. A. (1998). When Words 
Compete: Levels of Processing in Perception 
of Spoken Words. Psychological Science, 9(4), 
325-329. 
51
