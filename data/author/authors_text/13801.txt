Coling 2010: Poster Volume, pages 972?978,
Beijing, August 2010
Improving Name Origin Recognition with Context Features and
Unlabelled Data
Vladimir Pervouchine, Min Zhang, Ming Liu and Haizhou Li
Institute for Infocomm Research, A-STAR
vpervouchine@gmail.com,{mzhang,mliu,hli}@i2r.a-star.edu.sg
Abstract
We demonstrate the use of context fea-
tures, namely, names of places, and un-
labelled data for the detection of per-
sonal name language of origin.
While some early work used either
rule-based methods or n-gram statisti-
cal models to determine the name lan-
guage of origin, we use the discrimi-
native classification maximum entropy
model and view the task as a classifica-
tion task. We perform bootstrapping of
the learning using list of names out of
context but with known origin and then
using expectation-maximisation algo-
rithm to further train the model on
a large corpus of names of unknown
origin but with context features. Us-
ing a relatively small unlabelled cor-
pus we improve the accuracy of name
origin recognition for names written
in Chinese from 82.7% to 85.8%, a
significant reduction in the error rate.
The improvement in F -score for infre-
quent Japanese names is even greater:
from 77.4% without context features to
82.8% with context features.
1 Introduction
Transliteration is a process of rewriting a
word from a source language to a target lan-
guage in a different writing system using the
word?s phonological equivalent. Many techni-
cal terms and proper nouns, such as personal
names, names of places and organisations are
transliterated during translation of a text from
one language to another. A process reverse
to the transliteration, which is recovering a
word in its native language from its translit-
eration in a foreign language, is called back-
transliteration (Knight and Graehl, 1998). In
many natural language processing (NLP) tasks
such as machine translation and cross-lingual
information retrieval, transliteration is an im-
portant component.
Name origin refers to the language of ori-
gin of a name. For example, the origin of En-
glish name ?Smith? and its Chinese transliter-
ation ???? (Shi-Mi-Si)? is English, while
both ?Tokyo? and ??? (Dong-Jing)? are of
Japanese origin.
For machine transliteration the name origins
dictate the way we re-write a foreign name.
For example, given a name written in Chi-
nese for which we do not have a translation
in an English-Chinese dictionary, we first have
to decide whether the name is of Chinese,
Japanese, Korean, English or another origin.
Then we follow the transliteration rules im-
plied by the origin of the name. Although
all English personal names are rendered in
26 letters, they may come from different ro-
manization systems. Each romanisation sys-
972
tem has its own rewriting rules. English name
?Smith? could be directly transliterated into
Chinese as ???? (Shi-Mi-Si)? since it fol-
lows the English phonetic rules, while the Chi-
nese translation of Japanese name ?Koizumi?
becomes ??? (Xiao-Quan)? following the
Japanese phonetic rules. The name origins
are equally important in back-transliteration.
Li et al (2007b) demonstrated that incorpo-
rating name origin recognition (NOR) into a
transliteration system greatly improves the per-
formance of personal name transliteration. Be-
sides multilingual processing, the name origin
also provides useful semantic information (re-
gional and language information) for common
NLP tasks, such as co-reference resolution and
name entity recognition.
Unfortunately, not much attention has been
given to name origin recognition (NOR) so far
in the literature. In this paper, we are inter-
ested in recognition of the origins of names
written in Chinese, which names can be of
three origins: Chinese, Japanese or English,
where ?English? is a rather broad category that
includes other West European and American
names written natively in Latin script.
Unlike previous work (Qu and Grefenstette,
2004; Li et al, 2007a; Li et al, 2007b),
where NOR was formulated with a genera-
tive model, we follow the approach of Zhang
et al (2008) and regard the NOR task as a
classification problem, using a discriminative
learning algorithm for classification. Further-
more, in the training data with names labelled
with their origin is rather limited, whereas
there is vast data from news articles that con-
tains many personal names without any labels
of their origins. In this research we propose
a method to harness the power of the unla-
belled noisy news data by bootstrapping the
learning process with labelled data and then
using the personal name context in the unla-
belled data to improve the NOR model. We
achieve that by using the maximum entropy
model and the expectation-maximisation train-
ing, and demonstrate that our method can sig-
nificantly improve the accuracy of NOR com-
pared to the baseline model trained only from
the labelled data.
The rest of the paper is organised as follows:
in Section 2 we review the previous research.
In Section 3 we present our approach, and in
Section 4 we describe our experimental setup,
the data used and the evaluation method. We
conclude in Section 5.
2 Related research
Most the research up to date focuses primar-
ily on recognition of origin of names written
in Latin script, called English NOR (ENOR),
although the same methods can be extended to
names in Chinese script (CNOR). We notice
that there are two informative clues that used
in previous work in ENOR. One is the lexi-
cal structure of a romanisation system, for ex-
ample, Hanyu Pinyin, Mandarin Wade-Giles,
Japanese Hepbrun or Korean Yale, each has
a finite set of syllable inventory (Li et al,
2007a). Another is the phonetic and phono-
tactic structure of a language, such as phonetic
composition, syllable structure. For example,
English has unique consonant clusters such
as ?str? and ?ks? which Chinese, Japanese
and Korean (CJK) do not have. Consider-
ing the NOR solutions by the use of these
two clues, we can roughly group them into
two categories: rule-based methods (for solu-
tions based on lexical structures) and statisti-
cal methods (for solutions based on phonotac-
tic structures).
Rule-based method Kuo et al (2007) pro-
posed using a rule-based method to recog-
nise different romanisation system for Chinese
only. The left-to-right longest match-based
lexical segmentation was used to parse a test
word. The romanisation system is confirmed
973
if it gives rise to a successful parse of the test
word. This kind of approach (Qu and Grefen-
stette, 2004) is suitable for romanisation sys-
tems that have a finite set of discriminative
syllable inventory, such as Pinyin for Chinese
Mandarin. For the general tasks of identifying
the language origin and romanisation system,
rule based approach sounds less attractive be-
cause not all languages have a finite set of dis-
criminative syllable inventory.
N-gram statistics methods
N-gram sum method Qu and Grefenstette
(2004) proposed a NOR identifier us-
ing a trigram language model (Cavnar
and Trenkle, 1994) to distinguish per-
sonal names of three language origins,
namely Chinese, Japanese and English.
In their work the training set includes
11,416 Chinese, 83,295 Japanese and
88,000 English name entries. How-
ever, the trigram is defined as the joint
probability p(cici?1ci?2) rather than the
commonly used conditional probability
p(ci|ci?1ci?2). Therefore it is basically
a substring unigram probability. For ori-
gin recognition of Japanese names, this
method works well with an accuracy of
92%. However, for English and Chinese,
the results are far behind with a reported
accuracy of 87% and 70% respectively.
N-gram perplexity method Li et al (2007a)
proposed a method of NOR using n-gram
character perplexity PPc to identify the
origin of names written in Latin script.
Using bigrams, the perplexity is defined
as
PPc = 2
?1
Nc
?Nc
i=1 log p(ci|ci?1)
whereNc is the total number of characters
in a given name, ci is the i-th character
in the name and p(ci|ci?1) is the bigram
probability learned from a list of names
of the same origin. Therefore, PPc can
be used to measure how well a new name
fits the model learned from the training
set of names. The origin is assigned ac-
cording to the model that gives the lowest
perplexity value. Li et al (2007a) demon-
strated that using PPc gives much better
performance than with the substring uni-
gram method.
Classification method Zhang et al (2008)
proposed using a discriminative classification
approach and extract features from the names.
They use Maximum Entropy (MaxEnt) model
and a number of features based on n-grams,
character positions, word length as well as
some rule-based phonetic features. They per-
formed both ENOR and CNOR and demon-
strated that their method indeed leads to better
performance in name origin recognition then
the n-gram statistics method. They attribute
that to the fact their model incorporates more
robust features than the n-gram statistics based
models.
In this paper we too follow the discriminat-
ing classification approach, but we add fea-
tures based on the context of a personal name.
These features require the original text with the
names to be available. Our approach closely
models the real-life situation when large cor-
pora of articles with personal names is read-
ily available in the Web, yet the origins of the
names are unknown.
3 Model and training methods
3.1 Maximum entropy model for NOR
The principle of maximum entropy is that
given a collection of facts we should choose
a model that is consistent with all the facts but
otherwise as uniform as possible (Berger et al,
1996). maximum entropy model (MaxEnt) is
known to easily combine diverse features and
974
has been used widely in natural language pro-
cessing research. Given an observation x the
probability of outcome label ci, i = 1 . . . N
given x is given by
p(ci|x) =
1
Z exp
?
?
n?
j=1
?jfj (x, ci)
?
? (1)
where N is the number of the outcome labels,
which is the number of name origins in our
case, n is the number of features, fj are the
feature functions and ?j are the model param-
eters. Each parameter corresponds to exactly
one feature and can be viewed as a ?weight?
for the corresponding feature. Z is the normal-
isation factor given by
Z =
N?
i=1
p(ci|x) (2)
In the problem at hand x is a personal name
and all the features are binary. The features,
also known as contextual predicates, are in the
form
fi(x, c) =
{
1 if c = ci and cp(x) = true
0 otherwise
(3)
where cp is the contextual predicate that maps
a pair (ci, x) to {true, false}.
In our experiments we use Zhang?s maxi-
mum entropy library1.
3.2 Initial training with labelled data and
n-gram features
For the initial training of MaxEnt model we
use labelled data: personal names of Chinese,
Japanese or English origin written in Chinese.
The origin of each name is known. Following
paper by Zhang et al (2008) and their findings
1http://homepages.inf.ed.ac.uk/lzhang10/
maxent toolkit.html
regarding the contribution value of each fea-
ture that they studied, we extract unigram, po-
sitional unigram and word length features. For
example, Chinese name ????? has the fol-
lowing features:
??? (?,0) (?,1) (?,2) 3
We restrict the n-gram features to unigram
only to avoid the data sparseness, because our
data contains a number of Chinese surnames
and given names, which have a length of one
or two characters.
3.3 Further training with unlabelled data
and context features
For further training of MaxEnt model we use
unlabelled data collected from news articles.
The name origin is not known but each per-
sonal name is in a context and is often sur-
rounded by names of places that may give a
hint about the personal name origin. For each
personal name we extract all names of places
in the same paragraph and use them as fea-
tures. If a place name is repeated many times
in the same paragraph we only include it once
in the feature list.
For example, paragraph containing passage
?The U.S. President Barack Obama ...? will
result in two personal names ?Barack? and
?Obama? having ?U.S.? as their context fea-
ture. Due to the diversity of place names we
also attempt to map the names of the places
into the country names. In this case, features
like ?U.S.?, ?USA?, ?America? are manually
substituted with ?USA?. In our experiments we
also try to narrow the place name extraction
to windows of different sizes surrounding the
personal name. The rationale here is that the
closer a place name is to the personal name,
the more likely it has a connection to the ori-
gin of the personal name.
In summary, our algorithm includes two
steps.
975
First, we use the boostrap data and n-gram,
positional n-gram and name length features to
do the initial training (the 0-th iteration) of
MaxEnt model with L-BFGS method (Byrd et
al., 1995). After that we use the model to as-
sign origin labels to names of the training set
of the unlabelled data.
Next, we use both the bootstrap data and
the training set of the unlabelled data, labelled
in the previous step, and add the context fea-
tures to the already used n-gram, positional n-
gram and name length features. Since there is
no context available for the bootstrap data, the
context features for it are missing, which can
be handled by the MaxEnt model. We perform
the Expectation-Maximisation (EM) iterations
by using the mixed data to train the i-th itera-
tion of the MaxEnt model, then use the model
to re-label the training set of the unlabelled
data and repeat the training of the model for
the (i + 1)-st iteration. We stop the iterations
when the ratio of patterns that change the ori-
gin labels becomes less than 0.01%.
4 Experiments
4.1 Corpora
The corpora consists of two datasets. One
dataset, called the ?bootstrap data?, is a set of
Chinese, Japanese and English names written
in Chinese following the respective translitera-
tion rules according to the name origins. The
names are a mixture of full names, first (given)
names and surnames. Table 1 shows the num-
ber of names of each origin. This is the la-
belled data; the origin of each name is known.
The data is used to start the MaxEnt model
training.
The second dataset, called the ?unlabelled
data?, is Chinese, Japanese and English per-
sonal names written in Chinese, which have
been extracted from the news articles col-
lected over 6 months from Xinhua news web-
site. The articles have been processed by an
Origin Number of names
Chinese 52,342
Japanese 26,171
English 26,171
Table 1: Number of names of each origin in
the bootstrap dataset.
automatic part-of-speech (POS) tagger, after
which personal names and names of places
have been manually identified (the latter for
extracting the context features). Normally the
first (given) name and surnames are identi-
fied as two separate personal names. The data
is split into a training set of 27,882 names
with unknown origin and a testing set of 1,476
names whose origin was manually assigned.
We split data in such a way that there is no
overlap between patterns in the training and
testing sets, although there may be overlap be-
tween names. For example, if a name may
be present in both training and testing sets but
in a different context, making the two names
two distinct patterns. The number of names
of each origin in the testing set is shown in
Table 2. As seen from the table, the number
Origin Number of names
Chinese 738
Japanese 369
English 422
Table 2: Number of names of each origin in
the testing dataset.
of Chinese names exceeds the number of En-
glish or Japanese names. This is an expected
consequence of using articles from a Chinese
news agency because many of the articles are
reporting on local affairs. We have manually
removed a number of Chinese name patterns
from the testing set, since the original percent-
age of Chinese names in the articles is about
83%.
976
4.2 Evaluation method
Following Zhang et al (2008) to make
our results comparable to theirs, we eval-
uate our system using precision Po, recall
Ro and F -score Fo for each origin o ?
{?Chinese ?? ?Japanese ?? ?English ??}. Let
the number of correctly recognised names of
a given origin o be ko, and the total number of
names recognised as being of origin o be mo,
while the actual number of names of origin o
be no. Then the precision, recall and F -score
are given as:
Po =
ko
mo
Ro =
ko
no
Fo =
2? Po ?Ro
Po +Ro
We also report the overall accuracy of the sys-
tem (or, rather the overall recall), which is the
ratio of the total number of correctly recog-
nised names to the number of all names:
Acc = kChinese + kJapanese + kEnglishnChinese + nJapanese + nEnglish
4.3 Results
After each iteration of our MaxEnt-based EM
algorithm, we record the number of patterns in
the training set that changed their origin labels,
as well as calculate the precision, recall and
F -score for each origin as well as the overall
accuracy. The results are reported in Tables 3
and 4, where for the sake of brevity the origin
subscripts are ?C?, ?J? and ?W? for Chinese,
Japanese and English name origin respectively.
Compared to the 0-th iteration there is an
significant improvement in accuracy, particu-
larly in recognition of Japanese names, which
are relatively infrequent compared to Chinese
and English ones in the unlabelled training
data. This clearly shows the effectiveness of
our proposed method.
Iteration PC PJ PW RC RJ RW0 0.887 0.724 0.857 0.823 0.911 0.761
1 0.914 0.736 0.875 0.823 0.968 0.775
2 0.910 0.736 0.874 0.823 0.968 0.767
3 0.914 0.737 0.874 0.824 0.973 0.767
4 0.913 0.742 0.875 0.825 0.968 0.778
Table 3: Results of running EM iterations,
original names of the places are kept.
Iteration Acc FC FJ FW
0 0.829 0.854 0.807 0.806
1 0.847 0.866 0.836 0.822
2 0.845 0.864 0.836 0.817
3 0.847 0.867 0.839 0.817
4 0.849 0.867 0.840 0.824
Table 4: Results of running EM iterations,
original names of the places are kept.
5 Conclusions
We propose extension of MaxEnt model for
NOR task by using two types of data for train-
ing: origin-labelled names alone and origin-
unlabelled names in their context surrounding.
We show how to apply a simple EM method to
make use of the contextual words as features,
and improve the NOR accuracy from 82.9%
to 84.9% overall, while for rare names such
as Japanese the effect of using unlabelled data
with context features is even greater.
The purpose of this research is to demon-
strate how the unlabelled data can be used. In
the future we hope to investigate the use of
other context features, as well as to study the
effect of data size on the NOR accuracy im-
provement.
The feature of names? places normally ex-
hibit great variation: one country name may be
spelled in many different ways, and often there
are names of cities etc that surround personal
names. We will explore to normalise names
of places by substituting each name with name
of the country where the place is in the future
work.
977
References
[Berger et al1996] Berger, A., Stephen A.
Della Pietra, and V. Della Pietra. 1996. A
maximum entropy approach to natural lan-
guage processing. Computational Linguistics,
22(1):39?71.
[Byrd et al1995] Byrd, R. H., P. Lu, and J. Nocedal.
1995. A limited memory algorithm for bound
constrained optimization. SIAM Journal of Sci-
entific and Statistical Computing, 16(5):1190?
1208.
[Cavnar and Trenkle1994] Cavnar, William B. and
John M. Trenkle. 1994. Ngram based text cat-
egorization. In Proc. 3rd Annual Symposium on
Document Analysis and Information Retrieval,
pages 275?282.
[Knight and Graehl1998] Knight, Kevin and
Jonathan Graehl. 1998. Machine translitera-
tion. Computational Linguistics, 24(4).
[Kuo et al2007] Kuo, Jin-Shea, Haizhou Li, and
Ying-Kuei Yang. 2007. A phonetic similarity
model for automatic extraction of transliteration
pairs. ACM Transactions on Asian Language In-
formation Processing, 6(2).
[Li et al2007a] Li, Haizhou, Shuanhu Bai, and Jin-
Shea Kuo. 2007a. Transliteration. In Advances
in Chinese Spoken Language Processing, chap-
ter 15, pages 341?364. World Scientific.
[Li et al2007b] Li, Haizhou, Khe Chai Sim, Jin-
Shea Kuo, and Minghui Dong. 2007b. Semantic
transliteration of personal names. In Proc. 45th
Annual Meeting of the ACL, pages 120?127.
[Qu and Grefenstette2004] Qu, Yan and Gregory
Grefenstette. 2004. Finding ideographic rep-
resentations of Japanese names written in Latin
script via language identification and corpus val-
idation. In Proc. 42nd ACL Annual Meeting,
pages 183?190, Barcelona, Spain.
[Zhang et al2008] Zhang, Min, Chengjie Sun,
Haizhou Li, Aiti Aw, Chew Lim Tan, and Xi-
aolong Wang. 2008. Name origin recognition
using maximum entropy model and diverse fea-
tures. In Proc. 3rd Int?l Conf. NLP, pages 56?63.
978
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 843?847,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Multimodal DBN for Predicting High-Quality Answers in cQA portals
Haifeng Hu, Bingquan Liu, Baoxun Wang, Ming Liu, Xiaolong Wang
School of Computer Science and Technology
Harbin Institute of Technology, China
{hfhu, liubq, bxwang, mliu, wangxl}@insun.hit.edu.cn
Abstract
In this paper, we address the problem for
predicting cQA answer quality as a clas-
sification task. We propose a multimodal
deep belief nets based approach that op-
erates in two stages: First, the joint rep-
resentation is learned by taking both tex-
tual and non-textual features into a deep
learning network. Then, the joint repre-
sentation learned by the network is used
as input features for a linear classifier. Ex-
tensive experimental results conducted on
two cQA datasets demonstrate the effec-
tiveness of our proposed approach.
1 Introduction
Predicting the quality of answers in communi-
ty based Question Answering (cQA) portals is a
challenging task. One straightforward approach
is to use textual features as a text classification
task (Agichtein et al, 2008). However, due to
the word over-sparsity and inherent noise of user-
generated content, the classical bag-of-words rep-
resentation, is not appropriate to estimate the qual-
ity of short texts (Huang et al, 2011). Another typ-
ical approach is to leverage non-textual features to
automatically identify high quality answers (Jeon
et al, 2006; Zhou et al, 2012). However, in this
way, the mining of meaningful textual features
usually tends to be ignored.
Intuitively, combining both textual and non-
textual information extracted from answers is
helpful to improve the performance for predict-
ing the answer quality. However, textual and non-
textual features usually have different kinds of rep-
resentations and the correlations between them are
highly non-linear. Previous study (Ngiam et al,
2011) has shown that it is hard for a shallow model
to discover the correlations over multiple sources.
To this end, a deep learning approach, called
multimodal deep belief nets (mDBN), is intro-
duced to address the above problems to predict the
answer quality. The approach includes two stages:
feature learning and supervised training. In the
former stage, a specially designed deep network is
given to learn the unified representation using both
textual and non-textual information. In the latter
stage, the outputs of the network are then used as
inputs for a linear classifier to make prediction.
The rest of this paper is organized as follows:
The related work is surveyed in Section 2. Then,
the proposed approach and experimental results
are presented in Section 3 and Section 4 respec-
tively. Finally, conclusions and future directions
are drawn in Section 5.
2 Related Work
The typical way to predict the answer quality is
exploring various features and employing machine
learning methods. For example, Jeon et al (2006)
have proposed a framework to predict the qual-
ity of answers by incorporating non-textual fea-
tures into a maximum entropy model. Subsequent-
ly, Agichtein et al (2008) and Bian et al (2009)
both leverage a larger range of features to find high
quality answers. The deep research on evaluating
answer quality has been taken by Shah and Pomer-
antz (2010) using the logistic regression model.
We borrow some of their ideas in this paper.
In deep learning field, extensive studies have
been done by Hinton and his co-workers (Hin-
ton et al, 2006; Hinton and Salakhutdinov, 2006;
Salakhutdinov and Hinton, 2009), who initial-
ly propose the deep belief nets (DBN). Wang
et.al (2010; 2011) firstly apply the DBNs to model
semantic relevance for qa pairs in social communi-
ties. Meanwhile, the feature learning for disparate
sources has also been the hot research topic. Lee
et al (2009) demonstrate that the hidden represen-
tations computed by a convolutional DBN make
excellent features for visual recognition.
843
3 Approach
We consider the problem of high-quality answer
prediction as a classification task. Figure 1 sum-
marizes the framework of our proposed approach.
First, textual features and non-textual features ex-
TextualFeatures Non-textualFeaturesCQAArchives
ClassifierFusion Representation
FeatureLearning  Supervised Training
High-qualityAnswers
Figure 1: Framework of our proposed approach.
tracted from cQA portals are used to train two DB-
N models to learn the high-level representation-
s independently for answers. The two high-level
representations learned by the deep architectures
are then joined together to train a RBM model.
Finally, a linear classifier is trained with the final
shared representation as input to make prediction.
In this section, a deep network for the cQA an-
swer quality prediction is presented. Textual and
non-textual features are typically characterized by
distinct statistical properties and the correlations
between them are highly non-linear. It is very dif-
ficult for a shallow model to discover these corre-
lations and form an informative unified represen-
tation. Our motivation of proposing the mDBN is
to tackle these problems using an unified represen-
tation to enhance the classification performance.
3.1 The Restricted Boltzmann Machines
The basic building block of our feature leaning
component is the Restricted Boltzmann Machine
(RBM). The classical RBM is a two-layer undi-
rected graphical model with stochastic visible u-
nits v and stochastic hidden units h.The visible
layer and the hidden layer are fully connected to
the units in the other layer by a symmetric matrix
w. The classical RBM has been used effectively in
modeling distributions over binary-value data. As
for real-value inputs, the gaussian RBM (Bengio
et al, 2007) can be employed. Different from the
former, the hypothesis for the visible unit in the
gaussian RBM is the normal distribution.
3.2 Feature Learning
The illustration of the feature learning model is
given by Figure 2. Basically, the model consists
of two parts.
In the bottom part (i.e., V -H1, H1-H2), each
data modality is modeled by a two-layer DBN sep-
arately. For clarity, we take the textual modality
as an example to illustrate the construction of the
mDBN in this part. Given a textual input vector v,
the visible layer generates the hidden vector h, by
p(hj = 1|v) = ?(cj +
?
iwijvi).
Then the conditional distribution of v given h, is
p(vi = 1|h) = ?(bi +
?
j wijhj).
where ?(x) = (1 + e?x)?1 denotes the logistic
function. The parameters are updated by perform-
ing gradient ascent using Contrastive Divergence
(CD) algorithm (Hinton, 2002).
After learning the RBMs in the bottom layer,
we treat the activation probabilities of its hidden
units driven by the inputs, as the training data for
training a new layer. The construction procedures
for the non-textual modality are similar to the tex-
tual one, except that we use the gaussian RBM to
model the real-value inputs in the bottom layer.
Finally, we combine the two models by adding
an additional layer, H3, on the top of them to form
the mDBN. The training method is also similar to
the bottom?s, but the input vector is the concatena-
tion of the mapped textual vector and the mapped
non-textual vector.
Figure 2: mDBN for Feature Learning
It should be noted in the network, the bottom
part is essential to form the joint representation
because the correlations between the textual and
non-textual features are highly non-linear. It is
hard for a RBM directly combining the two dis-
parate sources to learn their correlations.
3.3 Supervised Training and Classification
After the above steps, a deep network for feature
learning between textual and non-textual data is
established. Classifiers, either support vector ma-
chine (SVM) or logistic regression (LR), can then
be trained with the unified representation (Ngiam
844
et al, 2011; Srivastava and Salakhutdinov, 2012).
Specifically, the LR classifier is used to make the
final prediction in our experiments since it keeps
to deliver the best performance.
3.4 Basic Features
Textual Features: The textual features ex-
tract from 1,500 most frequent words in the train-
ing dataset after standard preprocessing steps,
namely word segmentation, stopwords removal
and stemming1. As a result, each answer is repre-
sented as a vector containing 1,500 distinct terms
weighted by binary scheme.
Non-textual Features: Referring to
the previous work (Jeon et al, 2006; Shah and
Pomerantz, 2010), we adopt some features used
in theirs and also explore three additional features
marked by ? sign. The complete list is described
in Table 1.
Features Type
Length of question title (description) Integer
Length of answer Integer
Number of unique words for the answer ? Integer
Ratio of the qa length ? Float
Answer?s relative position ? Integer
Number of answers for the question Integer
Number of comments for the question Integer
Number of questions asked by asker (answerer) Integer
Number of questions resolved by asker (answerer) Integer
Asker?s (Answerer?s) total points Integer
Asker?s (Answerer?s) level Integer
Asker?s (Answerer?s) total stars Integer
Asker?s (Answerer?s) best answer ratio Float
Table 1: Summary of non-textual features.
4 Experiments
4.1 Experiment Setup
Datasets: We carry out experiments on two
datasets. One dataset comes from Baidu Zhi-
dao2, which contains 33,740 resolved questions
crawled by us from the ?travel? category. The oth-
er dataset is built by Chen and Nayak (2008) from
Yahoo! Answers3. We refer to these two dataset-
s as ZHIDAO and YAHOO respectively and ran-
domly sample 10,000 questions from each to form
our experimental datasets. According to the us-
er name, we have crawled all the user profile web
pages for non-textual feature collection. To allevi-
ate unnecessary noise, we only select those ques-
tions with number of answers no less than 3 (one
1The stemming step is only used in English corpus.
2http://zhidao.baidu.com
3http://answers.yahoo.com
best answer among them), and those answers at
least have 10 tokens. The statistics on the datasets
used for experiments are summarized in Table 2.
Statistics Items YAHOO ZHIDAO
# of questions 6841 5368
# of answers 74485 22435
# of answers per question 10.9 4.1
# of users 28812 12734
Table 2: Statistics on experimental datasets.
Baselines and Evaluation Metrics: We com-
pare against the following methods as our base-
lines. (1) Logistic Regression (LR): We imple-
ment the approach used by Shah and Pomer-
antz (2010) with textual features LR-T, non-
textual features LR-N and their simple combina-
tion LR-C. (2) DBN: Similar to the mDBN, the
outputs of the last hidden layer by the DBN are
used as inputs for LR model. Based on the fea-
ture sets, we have DBN-T for textual features and
DBN-N for non-textual features.
Since we mainly focus on the high quality an-
swers, the precision, recall and f1 for positive class
and the overall accuracy for both classes are em-
ployed as our evaluation metrics.
Model Architecture and Training Details: To
create the mDBN architecture, we use the classi-
cal RBM with 1500 visible units followed by 2
hidden layers with 1000 and 800 units respective-
ly for the textual branch, and the gaussian RBM
with 20 visible units followed by 2 hidden layers
with 100 and 200 units respectively for the non-
textual branch. On the joint layer of the network,
the layer contains 1000 real-value units.
Each RBM is trained using 1-step CD algorith-
m. During the training stage, a small weight-cost
of 0.0002 is used, and the learning rate for textu-
al data modal is 0.05 while the non-textual data is
0.001. We also adopt a monument of 0.5 for the
first five epochs and 0.9 for the rest epochs. In
addition, all non-textual data vectors are normal-
ized to have zero mean and unit standard variance.
More details for training the deep architecture can
be found in Hinton (2012).
4.2 Results and Analysis
In the first experiment, we compare the perfor-
mance of mDBN with different methods. To make
a fare comparison, we use the liblinear toolkit4 for
logistic regression model with L2 regularization
and randomly select 70% QA pairs as training data
4available at http://www.csie.ntu.edu.tw/ cjlin/liblinear
845
and the rest 30% as testing data. Table 3 and Ta-
ble 4 summarize the average results of the 5 round
experiments on YAHOO and ZHIDAO respectively.
Methods P R F1 Accu.
LR-T 0.374 0.558 0.448 0.542
LR-N 0.524 0.614 0.566 0.686
LR-C 0.493 0.557 0.523 0.662
DBN-T 0.496 0.571 0.531 0.663
DBN-N 0.505 0.578 0.539 0.670
mDBN 0.534 0.631 0.579 0.694
Table 3: Comparing results on YAHOO
It is promising to see that the proposed mDBN
method notably outperforms almost all the other
methods on both datasets over all the metrics as
expected, except for the recall on ZHIDAO. The
main reason for the improvements is that the joint
representation learned by mDBN is able to com-
plement each modality perfectly. In addition, the
mDBN can extract stronger representation through
modeling semantic relationship between textual
and non-textual information, which can effectively
help distinguish more complicated answers from
high quality to low quality.
Methods P R F1 Accu.
LR-T 0.380 0.540 0.446 0.553
LR-N 0.523 0.735 0.611 0.688
LR-C 0.537 0.695 0.606 0.698
DBN-T 0.527 0.730 0.612 0.692
DBN-N 0.539 0.760 0.631 0.703
mDBN 0.590 0.755 0.662 0.743
Table 4: Comparing results on ZHIDAO
The classification performance of the textu-
al features are worse on average compared with
non-textual features, even when the feature learn-
ing strategy is employed. More interestingly, we
find the simple combinations of textual and non-
textual features don?t improve the classification
results compared with using non-textual features
alone.We conjecture that there are mainly three
reasons for the phenomena: First, this is due to the
fact that user-generated content is inherently noisy
with low word frequency, resulting in the sparsity
of employing textual feature. Second, non-textual
features (e.g., answer length) usually own strongly
statistical properties and feature sparsity problem
can be better relieved to some extent. Finally, s-
ince correlations between the textual features and
non-textual features are highly non-linear, con-
catenating these features simply sometimes can
submerge classification performance. In contrast,
mDBN enjoys the advantage of the shared repre-
sentation between textual features and non-textual
features using the deep learning architecture.
We also note that neither the mDBN nor other
approaches perform very well in predicting answer
quality across the two datasets. The best precision
on ZHIDAO and YAHOO are respectively 59.0%
and 53.4%, which means that there are nearly half
of the high quality answers not effectively identi-
fied. One of the possible reason is that the quali-
ty of the corpora influences the result significant-
ly. As shown in Table 2, each question on aver-
age receives more than 4 answers on ZHIDAO and
more than 10 on YAHOO. Therefore, it is possi-
ble that there are several answers with high quali-
ty to the same question. Selecting only one as the
high quality answer is relatively difficult for our
humans, not to mention for the models.
100 500 1000 2000 5000
# iterations
0.50
0.55
0.60
0.65
0.70
0.75
0.80
Precision Recall F1 Accuracy
Figure 3: Influences of iterations for mDBN
In the second experiment, we intend to exam-
ine the performance of mDBN with different num-
ber of iterations. Figure 3 depicts the metrics on
ZHIDAO when the iteration number is varied from
100 to 5000. From the result, the first observa-
tion is that increasing the number of iterations the
performance of mDBN can improve significant-
ly, obtaining the best results for iteration of 1000.
This clearly shows the representation power of the
mDBN again. However, after a large number of it-
erations (large than 1000), the mDBN has a detri-
mental performance. This may be explained by
with large number of iterations, the deep learning
architecture is easier to be overfitting. The similar
trend is also observed on YAHOO.
5 Conclusions and Future work
In this paper, we have provided a new perspec-
tive to predict the cQA answer quality: learning
an informative unified representation between tex-
tual and non-textual features instead of concate-
nating them simply. Specifically, we have pro-
posed a multimodal deep learning framework to
846
form the unified representation. We compare this
with the basic features both in isolation and in
combination. Experimental results have demon-
strated that our proposed approach can capture the
complementarity between textual and non-textual
features, which is helpful to improve the perfor-
mance for cQA answer quality prediction.
For the future work, we plan to explore more se-
mantic analysis to approach the issue for short tex-
t quality evaluation. Additionally, more research
will be taken to put forward other approaches for
learning multimodal representation.
Acknowledgments
The authors are grateful to the anonymous re-
viewers for their constructive comments. Spe-
cial thanks to Chengjie Sun and Deyuan Zhang
for insightful suggestions. This work is supported
by National Natural Science Foundation of China
(NSFC) via grant 61272383 and 61100094.
References
E. Agichtein, C. Castillo, D. Donato, A. Gionis, and
G. Mishne. 2008. Finding high-quality content in
social media. In Proceedings of the internation-
al conference on Web search and web data mining,
pages 183?194. ACM.
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and
Hugo Larochelle. 2007. Greedy layer-wise training
of deep networks. In Advances in Neural Informa-
tion Processing Systems, pages 153?160.
Jiang Bian, Yandong Liu, Ding Zhou, Eugene
Agichtein, and Hongyuan Zha. 2009. Learning to
recognize reliable users and content in social media
with coupled mutual reinforcement. In Proceedings
of the 18th international conference on World wide
web, pages 51?60. ACM.
L. Chen and R. Nayak. 2008. Expertise analysis in a
question answer portal for author ranking. In Inter-
national Conference on Web Intelligence and Intel-
ligent Agent Technology, volume 1, pages 134?140.
G.E. Hinton and R.R. Salakhutdinov. 2006. Reduc-
ing the dimensionality of data with neural networks.
Science, 313(5786):504?507.
G.E. Hinton, S. Osindero, and Y.W. Teh. 2006. A fast
learning algorithm for deep belief nets. Neural com-
putation, 18(7):1527?1554.
G.E. Hinton. 2002. Training products of experts by
minimizing contrastive divergence. Neural compu-
tation, 14(8):1771?1800.
G.E. Hinton. 2012. A practical guide to training re-
stricted boltzmann machines. Lecture Notes in Com-
puter Science, pages 599?619.
Minlie Huang, Yi Yang, and Xiaoyan Zhu. 2011.
Quality-biased ranking of short texts in microblog-
ging services. In Proceedings of the 5th Internation-
al Joint Conference on Natural Language Process-
ing, pages 373?382.
J. Jeon, W.B. Croft, J.H. Lee, and S. Park. 2006. A
framework to predict the quality of answers with
non-textual features. In Proceedings of the 29th an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval,
pages 228?235. ACM.
H. Lee, R. Grosse, R. Ranganath, and A.Y. Ng. 2009.
Convolutional deep belief networks for scalable un-
supervised learning of hierarchical representation-
s. In Proceedings of the 26th Annual International
Conference on Machine Learning, pages 609?616.
J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A.Y.
Ng. 2011. Multimodal deep learning. In Proceed-
ings of the 28th International Conference on Ma-
chine Learning (ICML), pages 689?696.
R. Salakhutdinov and G.E. Hinton. 2009. Deep boltz-
mann machines. In Proceedings of the internation-
al conference on artificial intelligence and statistics,
volume 5, pages 448?455.
C. Shah and J. Pomerantz. 2010. Evaluating and pre-
dicting answer quality in community qa. In Pro-
ceeding of the 33rd international ACM SIGIR con-
ference on Research and development in information
retrieval, pages 411?418.
N. Srivastava and R. Salakhutdinov. 2012. Multi-
modal learning with deep boltzmann machines. In
Advances in Neural Information Processing System-
s, pages 2231?2239.
B. Wang, X. Wang, C. Sun, B. Liu, and L. Sun. 2010.
Modeling semantic relevance for question-answer
pairs in web social communities. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 1230?1238. ACL.
B. Wang, B. Liu, X. Wang, C. Sun, and D. Zhang.
2011. Deep learning approaches to semantic rele-
vance modeling for chinese question-answer pairs.
ACM Transactions on Asian Language Information
Processing, 10(4):21:1?21:16.
Z.M. Zhou, M. Lan, Z.Y. Niu, and Y. Lu. 2012. Ex-
ploiting user profile information for answer ranking
in cqa. In Proceedings of the 21st international con-
ference on World Wide Web, pages 767?774. ACM.
847
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 67?72,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
PAL: A Chatterbot System for Answering Domain-specific Questions 
Yuanchao Liu1 Ming Liu1 Xiaolong Wang1 Limin Wang2 Jingjing Li1 
 
1 School of Computer Science and Technology, Harbin Institute of Technology, 
Harbin, China 
2. School of public health, Harbin Medical University, Harbin, China 
{lyc,mliu,wangxl,jjl}@insun.hit.edu.cn, wanglimin2008@163.com 
 
Abstract 
In this paper, we propose PAL, a prototype 
chatterbot for answering non-obstructive 
psychological domain-specific questions. This 
system focuses on providing primary 
suggestions or helping people relieve pressure 
by extracting knowledge from online forums, 
based on which the chatterbot system is 
constructed. The strategies used by PAL, 
including semantic-extension-based question 
matching, solution management with personal 
information consideration, and XML-based 
knowledge pattern construction, are described 
and discussed. We also conduct a primary test 
for the feasibility of our system. 
1 Introduction 
A wide variety of chatterbots and 
question-and-answer (Q&A) systems have been 
proposed over the past decades, each with 
strengths that make them appropriate for 
particular applications. With numerous advances 
in information construction, people increasingly 
aim to communicate with computers using natural 
language. For example, chatterbots in some 
e-commerce Web sites can interact with 
customers and provide help similar to a real-life 
secretary (DeeAnna Merz Nagel, 2011; Yvette 
Col?n, 2011). 
  In this paper, we propose PAL (Psychologist of 
Artificial Language), a chatterbot system for 
answering non-obstructive psychological 
questions. Non-obstructive questions refer to 
problems on family, human relationships, 
marriage, life pressure, learning, work and so on. 
In these cases, we expect the chatterbot to play an 
active role by providing tutoring, solution, 
support, advice, or even sympathy depending on 
the help needed by its users.  
  The difference of PAL from existing 
chatterbots lies not only in the specific research 
focus of this paper but also in the strategies we 
designed, such as P-XML templates for storing a 
knowledge base, comprehensive question 
matching method by considering both index and 
semantic similarities, and solution management 
by considering personal information. In the 
following sections, we will briefly discuss related 
work and then introduce our system and its main 
features. 
2 Related Work 
A number of research work on chatterbots 
(Rafael E. Banchs, Haizhou Li, 2012; Ai Ti Aw 
and Lian Hau Lee, 2012), Q&A systems (Shilin 
Ding, Gao Cong, Chin-Yew Lin, 2008; Leila 
Kosseim, 2008; Tiphaine Dalmas, 2007), and 
related natural language understanding 
technologies have recently been conducted 
(Walid S. Saba, 2007; Jing dong, 2009). Several 
studies on the application of natural language 
processing technologies for non-obstructive 
psychological Q&A systems have also been 
published (Hai-hu Shi, 2005).  
Several online psychology counselling Web 
sites with service provided by human experts have 
also been established recently (DeeAnna Merz 
Nagel, 2011; Yvette Col?n, 2011). For these Web 
sites, when the visitors ask similar questions, the 
expert may provide the same or very similar 
answers repeatedly. Based on this observation and 
consideration, we collected a large number of 
counselling Q&A pairs to extract common 
knowledge for the construction of a chatterbot 
system. Advances in automatic language analysis 
and processing are used as the bases for the 
emergence of a complex, task-oriented chatterbot 
system. 
67
3 Basic Framework of PAL 
A running screenshot of PAL is shown in Figure 
1, and its basic system structure is demonstrated 
in Figure 2. As shown in Figure 2, the basic 
principles of PAL are as follows: 
1) All interactions between system and users are 
scheduled by control logic; 
2) When the user inputs a question, the system 
will search through its knowledge base for 
the matching entry, and then 
3) The system will respond with an appropriate 
answer by analysing both the matched entry 
and the dialogue history. 
Figure 1. Running Screenshot of PAL 
 
Lexicon analysis
 &extracting 
features
Knowledge
 base
Dialog control 
logic
XML knowledge Engine
(Running in background)
User
Index 
generation
Semantic 
extension
Keyword 
extraction
Response
Question
Answer 
generation
Crawing Q&A pairs 
from on-line forums
 
Solution 
management
Dialog history 
analysis
 Figure 2. Basic Framework of PAL 
4 Conversation Control Strategy of PAL 
The Q&A process of the PAL system is 
coordinated by control logic to communicate with 
users effectively. The basic control logic strategy 
is shown in Figure 3.  
  
Figure 3. Basic Control Logic of PAL 
68
As shown in Figure 3, the initial state is set to 
welcome mode, and the system can select a 
sentence from the ?sign on? list, which will then 
provide a response. When users enter a question, 
the system will conduct the necessary analysis. 
The system?s knowledge base is indexed by 
Clucene1 beforehand. Thus, the knowledge index 
will be used to search the matched records quickly. 
If the system can find the matched patterns 
directly and the answer is suitable for the current 
user, one answer will be randomly selected to 
generate the response. Historical information and 
personal information will be analysed when 
necessary. We mainly adopted the method of 
ELIZA2
5 Knowledge Construction and 
Question Matching Method 
, which is an open-source program, to 
consider the historical information. A ?not found? 
response list is also set to deal with situations 
when no suitable answers can be identified. Both 
system utterance and user input will be pushed 
into the stack as historical information. Given that 
user questions are at times very simple, the 
combination with historical input may also be 
required to determine its meaning. This step can 
also avoid the duplication of utterances. 
We design P-XML to store the knowledge base 
for PAL, as shown in Figure 4. The knowledge 
base for PAL is mainly derived from the Q&A 
pairs in the BAIDU ZHIDAO community3
<?xml version="1.0" encoding="GB2312"?> 
. One 
question usually has many corresponding 
answers. 
<domain name="*"> <qapair speaker="*">        
<zhidao_question_title>*</zhidao_question_t
itle> 
<zhidao_question_content>*</zhidao_question
_content><zhidao_other_answer 
intersection_number="4">* 
<entity_and_problemword>*</entity_and_probl
emword> <peopleword>*</peopleword>          
</zhidao_other_answer>    
<title_extension>*</title_extension>   
</qapair> 
? 
</domain> 
Figure 4. The Structure of P-XML 
                                                          
1 http://sourceforge.net/projects/clucene/ 
2 http://www.codeforge.cn/article/191554 
3 http://Zhidao.baidu.com 
 
An effective method of capturing the user?s 
meaning accurately is to create an extension for 
questions in the knowledge base. In this paper, the 
extension is primarily a synonym expansion of the 
keywords of questions, with CILIN (Wanxiang 
Che, 2010) as extension knowledge source.  
The questions are indexed by Clucene to 
improve the retrieval efficiency of the search for a 
matched entry in the knowledge base. During the 
knowledge base searching step, both the index of 
the original form and the extension form of the 
problem are used to find the most possible 
matched record for the user?s question, as shown 
in algorithm 1. Algorithm 1 is used to examine the 
similarity between user input and the record 
returned by Clucene, including traditional and 
extension similarities.   
Algorithm 1. Problem-matching method 
Begin  
1) User inputs question Q; 
2) Search from the index of original questions and 
obtain the returned record set RS1; 
3) For the highest ranked record R1 in RS1, 
a) compute the similarity sim1 between 
question R1 and Q; 
b) compute the extension similarity sim2 
between the question extensions of R1 and 
Q;  
4) If sim1 is greater than the threshold value T1 or 
sim2 is greater than the threshold value T2, go to 
the solution management stage and obtain the 
answers of R1, and then find the candidate 
answer using algorithm 2; 
5) Otherwise, a ?not found? prompt is given.  
End 
6 Response Management Method 
 One question usually has many corresponding 
answers in the knowledge base, and these 
answers differ in explanation quality. Thus, the 
basic strategy employed by solution management 
is to select a reliable answer from the matched 
record as response, as shown in algorithm 2. 
Personalised information includes name entity, 
gender, marital status and age information. PAL 
maintains some heuristics rules to help recognize 
such information. Based on these rules, if one 
answer contains personal information, it will be 
selected as the candidate answer only when the 
personal information is consistent with that of the 
current user. Very concise answers that do not 
69
contain personal information can generally be 
selected as a candidate answer. 
 
Algorithm 2.  Answer-selection method 
Begin 
1) User inputs one question Q; 
2) The system extracts the speaker role S and 
personal information from Q; 
3) Use Q as query to conduct information retrieval 
from the index and knowledge base and obtain 
the top matched record set R; 
4) For each matched question Q? in R, test the 
following conditions: 
a) (condition 1) extract the speaker role S? 
in Q?, and examine if S? is equal to S; 
b) (condition 2) extract personal 
information in Q?, and examine if they 
are equal to that of in Q? 
c) For each answer A? of Q? 
i. If no personal information is found 
in A?, A? will be pushed into 
response list; 
ii. If personal information is contained 
in A? and if both conditions 1 and 2 
are true, A? will be pushed into 
response list; 
d) End for 
5) End for 
End 
7 Experiments 
For the current implementation of PAL, the size of 
the knowledge base is approximately 1.2G and 
contains six different topics: ?Husband and 
wife?, ?Family relations?, ?Love affairs?, 
?Adolescence?, ?Feeling and Mood?, and 
?Mental tutors?. Dialogue data collection used in 
PAL is mainly crawled from 
http://zhidao.baidu.com, which is one of the 
largest Chinese online communities. The 
criterion for choosing these six categories is also 
because they are the main topics in BAIDU 
communities about psychological problems. 
Some information on the knowledge base is 
given in Table 1, in which ?Percent of questions 
matched? denotes the number of similar 
questions found when 100 open questions are 
input (we suppose that if the similarity threshold 
is bigger than 0.5, then a similar question will be 
deemed as ?hit? in the knowledge base). 
In 7.1, we examine the feasibility of using 
downloaded dialogue collection for constructing 
the knowledge base. Some dialogue examples are 
given in 7.2.  
 
Domain Avg. ques. 
 length 
Num. of unique 
 Terms in ques. 
Avg. ans. 
 length 
Num. of unique 
terms in ans. 
Percent of questions 
matched (similarity threshold: 0.5) 
Size(MB) 
QS1 58.69 11571 64.13 27312 25 125 
QS2 54.96 10918 64.92 25185 24 292 
QS3 59.66 13530 49.52 13664 15 53 
QS4 42.41 8607 47.11 23492 22 224 
QS5 63.57 11915 48.86 26860 26 276 
QS6 31.82 10009 98.55 20896 25 216 
Table 1. Information of the knowledge base 
 
7.1 System Performance Evaluation 
Additional questions and their corresponding 
answers beyond the knowledge base are also used 
as a test set to evaluate system performance. 
Concretely, suppose question Q has |A| answers in 
the test set. Q is then input into the system. 
Suppose the system output is O, we examine if 
one best answer exists among |A| answers that are 
very similar to O (the similarity is greater than 
threshold T3). If yes, we then assume that one 
suitable answer has been found. In this way, 
precision can be calculated as the number of 
questions that have very similar answers in the 
system divided by the number of all input 
questions.  
The performance evaluation results are shown 
in Figure 5. The horizon axis denotes the 
similarity threshold (T1 for sim1 and T2 for sim2) 
between a user?s input and the questions in the 
knowledge base. Sim1 is the original similarity, 
whereas sim2 is the semantic extension similarity. 
Different thresholds were used (0.5 to 0.9). The 
similarity threshold T3 denotes the similarity 
70
between the answer in the test set and system 
output O. From Figures 5 (A) and (B), different 
T3 values were used (0.5 to 0.8).  
Some observations can be made from Figure 5. 
The average system precision is approximately 
0.5, and the range is from 0.2 to 0.9. Basically, 
when T3 is bigger, the system?s performance 
tends to decrease because a high T3 value denotes 
a strict evaluation standard. Performance also 
differs between different areas, such that D4, D5 
and D6 outperform than D1, D2 and D3.  
When only index is used and both sim1 and 
sim2 are below the corresponding threshold T1 or 
T2, the system can still return record set RS2, but 
the returned answer may be inconsistent with 
user?s question. Thus, incorporating semantic 
search shown in algorithm 1 is necessary. 
  
  
?A?(Sim. Thres. T3=0.5) ?B?(Sim. Thres. T3=0.6) 
  
(C) (Sim. Thres. T3=0.7) (D) (Sim. Thres. T3=0.8) 
Figure 5. Precision of PAL with different similarity thresholds T3 (The X axis denotes different thresholds 
for sim1 (T1) and sim2 (T2). For example, 0.5(sim1) indicates that the threshold for sim1 is 0.5, whereas 
0.5(sim2) suggests that the threshold for sim2 is 0.5. The Y axis stands for the precision value of different 
T1, T2 and T3 values for different areas). 
7.2?Some Interaction Examples 
Table 2 provides an interaction example of PAL in 
English. As users input their questions, the system 
can output the corresponding responses. The entire 
dialogue history can also be displayed. Basically, 
when users describe their troubles, PAL can 
provide some explanation or even try to console the 
user.  
Compared to other chatterbots like Eliza, PAL 
is domain-specific and may provide help to some 
extent. Many different answers can usually be 
identified. Thus, the user could input the same 
question many times, with the system providing 
suggestions from different angles. PAL can also 
output voice by using Xiao-kun and Microsoft 
Anna as the text-to-speech (TTS) engine. The 
speed, volume and echo properties of the voice can 
be adjusted.  
 
Dialog for Figure 1 (in English) 
PAL> Hello, you are welcome to communicate with 
me! You can input from the left side and press enter to 
get answer! 
I> How do I deal with the children who like to 
complain very frequently?  
PAL>Very simple. Don't? let him experience the 
benefits of complaining very frequently. 
I> Why do parents give children so much pressure  
PAL> Children have pressure indeed; but parents also 
have a lot of pressure.  
I> Why am I unable to control my emotions, and often 
quarrel with my parents?  
PAL.> Now you may be in the stage of treason. And 
have your own uncertain plan for the future, want to be 
yourself, and do not want to be disturbed. So when 
parents criticize, you will be unhappy. 
I> How to solve the gap problem?  
PAL.> Communication, and listening . 
Table 2. Example of an interaction with PAL (in English) 
 
71
8 Conclusions 
In this paper, we introduce a simple chatterbot for 
answering non-obstructive psychological 
questions. The knowledge behind the chatterbot is 
learned from the Q&A pairs derived from an 
online forum using several extraction strategies. 
The historical and personal information from the 
dialogues are also incorporated to output an 
appropriate answer. 
For future work, we expect to add more features 
to PAL, e.g., enabling the system to ask questions 
actively and further improving P-XML to form 
richer patterns for storing Q&A knowledge. 
Another interesting aspect would be to add speech 
input as well as TTS and to transform PAL into a 
mobile platform for widespread use.  
Acknowledgments 
This research was supported by the project of The 
National High Technology Research and 
Development Program (863 program) of PR China 
under a research Grant No.2007AA01Z172?
Youth Funds of China social & humanity science 
(10YJCZH099), and Key Laboratory Opening 
Funding of China MOE?MS Key Laboratory of 
Natural Language Processing and Speech 
(HIT.KLOF.2009022). 
References  
Ai Ti Aw and Lian Hau Lee. Personalized 
Normalization for a Multilingual Chat System. 
Proceedings of the 50th Annual Meeting of the 
Association for Computational Linguistics, Jeju, 
Republic of Korea, 8-14 July 2012, pages 31?36, 
DeeAnna Merz Nagel, Kate Anthony. Text-based 
Online Counseling Chat. Online Counseling 
(Second Edition), 2011, Pages 169-182 
Hai-hu Shi, Yan Feng, LI Dong-mei, HU Ying-fei. 
Research on on-line psychology consultation expert 
system based on man-machine interaction technique. 
Computer Engineering and Design. 2005, 
26(12):3307-3309 
Jing dong. Research of sentiment model based on 
HMM and its application in psychological 
consulting expert system. Master?s thesis. Capital 
normal university (china), 2009. 
Leila Kosseim, Jamileh Yousefi. Improving the 
performance of question answering with 
semantically equivalent answer patterns. Data & 
Knowledge Engineering, 2008, 66(1):53-67 
Rafael E. Banchs, Haizhou Li. IRIS: a Chat-oriented 
Dialogue System based on the Vector Space Model. 
Proceedings of the 50th Annual Meeting of the 
Association for Computational Linguistics, Jeju, 
Republic of Korea, 8-14 July 2012. pages 37?42 
Shilin Ding, Gao Cong, Chin-Yew Lin, Xiaoyan Zhu. 
Using Conditional Random Fields to Extract 
Contexts and Answers of Questions from Online 
Forums. Proceedings of 2008 Association for 
Computational Linguistics, Columbus, Ohio, 
USA, June 2008. pages 710?718 
Tiphaine Dalmas, Bonnie Webber. Answer comparison 
in automated question answering. Journal of 
Applied Logic, Volume 5, Issue 1, March 2007, 
Pages 104-120 
Walid S. Saba. Language, logic and ontology: 
Uncovering the structure of commonsense 
knowledge. International Journal of 
Human-Computer Studies, Volume 65, Issue 7, 
July 2007, Pages 610-623 
Wanxiang Che, Zhenghua Li, Ting Liu. LTP: A 
Chinese Language Technology Platform. In 
Proceedings of the Coling 
2010:Demonstrations. August 2010, pp13-16, 
Beijing, China. 
Yvette Col?n, Stephanie Stern. Counseling Groups 
Online: Theory and Framework. Online 
Counseling (Second Edition), 2011, Pages 
183-202. 
 
 
72
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1?9,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Whitepaper of NEWS 2012 Shared Task on Machine Transliteration?
Min Zhang?, Haizhou Li?, Ming Liu?, A Kumaran?
?Institute for Infocomm Research, A*STAR, Singapore 138632
{mzhang,hli,mliu}@i2r.a-star.edu.sg
?Multilingual Systems Research, Microsoft Research India
A.Kumaran@microsoft.com
Abstract
Transliteration is defined as phonetic
translation of names across languages.
Transliteration of Named Entities (NEs)
is necessary in many applications, such
as machine translation, corpus alignment,
cross-language IR, information extraction
and automatic lexicon acquisition. All
such systems call for high-performance
transliteration, which is the focus of
shared task in the NEWS 2012 workshop.
The objective of the shared task is to pro-
mote machine transliteration research by
providing a common benchmarking plat-
form for the community to evaluate the
state-of-the-art technologies.
1 Task Description
The task is to develop machine transliteration sys-
tem in one or more of the specified language pairs
being considered for the task. Each language pair
consists of a source and a target language. The
training and development data sets released for
each language pair are to be used for developing
a transliteration system in whatever way that the
participants find appropriate. At the evaluation
time, a test set of source names only would be
released, on which the participants are expected
to produce a ranked list of transliteration candi-
dates in another language (i.e. n-best translitera-
tions), and this will be evaluated using common
metrics. For every language pair the participants
must submit at least one run that uses only the
data provided by the NEWS workshop organisers
in a given language pair (designated as ?standard?
run, primary submission). Users may submit more
?stanrard? runs. They may also submit several
?non-standard? runs for each language pair that
?http://translit.i2r.a-star.edu.sg/news2012/
use other data than those provided by the NEWS
2012 workshop; such runs would be evaluated and
reported separately.
2 Important Dates
Research paper submission deadline 25 March 2012
Shared task
Registration opens 18 Jan 2012
Registration closes 11 Mar 2012
Training/Development data release 20 Jan 2012
Test data release 12 Mar 2012
Results Submission Due 16 Mar 2012
Results Announcement 20 Mar 2012
Task (short) Papers Due 25 Mar 2012
For all submissions
Acceptance Notification 20 April 2012
Camera-Ready Copy Deadline 30 April 2012
Workshop Date 12/13/14 July 2012
3 Participation
1. Registration (18 Jan 2012)
(a) NEWS Shared Task opens for registra-
tion.
(b) Prospective participants are to register to
the NEWS Workshop homepage.
2. Training & Development Data (20 Jan 2012)
(a) Registered participants are to obtain
training and development data from the
Shared Task organiser and/or the desig-
nated copyright owners of databases.
(b) All registered participants are required
to participate in the evaluation of at least
one language pair, submit the results and
a short paper and attend the workshop at
ACL 2012.
3. Test data (12 March 2012)1
(a) The test data would be released on 12
March 2012, and the participants have a
maximum of 5 days to submit their re-
sults in the expected format.
(b) One ?standard? run must be submit-
ted from every group on a given lan-
guage pair. Additional ?standard? runs
may be submitted, up to 4 ?standard?
runs in total. However, the partici-
pants must indicate one of the submit-
ted ?standard? runs as the ?primary sub-
mission?. The primary submission will
be used for the performance summary.
In addition to the ?standard? runs, more
?non-standard? runs may be submitted.
In total, maximum 8 runs (up to 4 ?stan-
dard? runs plus up to 4 ?non-standard?
runs) can be submitted from each group
on a registered language pair. The defi-
nition of ?standard? and ?non-standard?
runs is in Section 5.
(c) Any runs that are ?non-standard? must
be tagged as such.
(d) The test set is a list of names in source
language only. Every group will pro-
duce and submit a ranked list of translit-
eration candidates in another language
for each given name in the test set.
Please note that this shared task is a
?transliteration generation? task, i.e.,
given a name in a source language one
is supposed to generate one or more
transliterations in a target language. It
is not the task of ?transliteration discov-
ery?, i.e., given a name in the source lan-
guage and a set of names in the target
language evaluate how to find the ap-
propriate names from the target set that
are transliterations of the given source
name.
4. Results (20 March 2012)
(a) On 20 March 2012, the evaluation re-
sults would be announced and will be
made available on the Workshop web-
site.
(b) Note that only the scores (in respective
metrics) of the participating systems on
each language pairs would be published,
and no explicit ranking of the participat-
ing systems would be published.
(c) Note that this is a shared evaluation task
and not a competition; the results are
meant to be used to evaluate systems on
common data set with common metrics,
and not to rank the participating sys-
tems. While the participants can cite the
performance of their systems (scores on
metrics) from the workshop report, they
should not use any ranking information
in their publications.
(d) Furthermore, all participants should
agree not to reveal identities of other
participants in any of their publications
unless you get permission from the other
respective participants. By default, all
participants remain anonymous in pub-
lished results, unless they indicate oth-
erwise at the time of uploading their re-
sults. Note that the results of all systems
will be published, but the identities of
those participants that choose not to dis-
close their identity to other participants
will be masked. As a result, in this case,
your organisation name will still appear
in the web site as one of participants, but
it will not be linked explicitly to your re-
sults.
5. Short Papers on Task (25 March 2012)
(a) Each submitting site is required to sub-
mit a 4-page system paper (short paper)
for its submissions, including their ap-
proach, data used and the results on ei-
ther test set or development set or by n-
fold cross validation on training set.
(b) The review of the system papers will be
done to improve paper quality and read-
ability and make sure the authors? ideas
and methods can be understood by the
workshop participants. We are aiming
at accepting all system papers, and se-
lected ones will be presented orally in
the NEWS 2012 workshop.
(c) All registered participants are required
to register and attend the workshop to
introduce your work.
(d) All paper submission and review will be
managed electronically through https://
www.softconf.com/acl2012/news2012/.2
4 Language Pairs
The tasks are to transliterate personal names or
place names from a source to a target language as
summarised in Table 1. NEWS 2012 Shared Task
offers 14 evaluation subtasks, among them ChEn
and ThEn are the back-transliteration of EnCh and
EnTh tasks respectively. NEWS 2012 releases
training, development and testing data for each of
the language pairs. NEWS 2012 continues all lan-
guage pairs that were evaluated in NEWS 2011. In
such cases, the training and development data in
the release of NEWS 2012 are the same as those
in NEWS 2011. However, the test data in NEWS
2012 are entirely new.
Please note that in order to have an accurate
study of the research progress of machine transla-
tion technology, different from previous practice,
the test/reference sets of NEWS 2011 are not re-
leased to the research community. Instead, we
use the test sets of NEWS 2011 as progress test
sets in NEWS 2012. NEWS 2012 participants are
requested to submit results on the NEWS 2012
progress test sets (i.e., NEWS 2011 test sets). By
doing so, we would like to do comparison studies
by comparing the NEWS 2012 and NEWS 2011
results on the progress test sets. We hope that we
can have some insightful research findings in the
progress studies.
The names given in the training sets for Chi-
nese, Japanese, Korean, Thai and Persian lan-
guages are Western names and their respective
transliterations; the Japanese Name (in English)
? Japanese Kanji data set consists only of native
Japanese names; the Arabic data set consists only
of native Arabic names. The Indic data set (Hindi,
Tamil, Kannada, Bangla) consists of a mix of In-
dian and Western names.
Examples of transliteration:
English ? Chinese
Timothy ???
English ? Japanese Katakana
Harrington ??????
English ? Korean Hangul
Bennett ? ??
Japanese name in English ? Japanese Kanji
Akihiro ???
English ? Hindi
San Francisco ? ????????????????
English ? Tamil
London ? ??????
English ? Kannada
Tokyo ? ??????
Arabic ? Arabic name in English
? 
Khalid
????
5 Standard Databases
Training Data (Parallel)
Paired names between source and target lan-
guages; size 7K ? 37K.
Training Data is used for training a basic
transliteration system.
Development Data (Parallel)
Paired names between source and target lan-
guages; size 1K ? 2.8K.
Development Data is in addition to the Train-
ing data, which is used for system fine-tuning
of parameters in case of need. Participants
are allowed to use it as part of training data.
Testing Data
Source names only; size 1K ? 2K.
This is a held-out set, which would be used
for evaluating the quality of the translitera-
tions.
Progress Testing Data
Source names only; size 0.6K ? 2.6K.
This is the NEWS 2011 test set, it is held-out
for progress study.
1. Participants will need to obtain licenses from
the respective copyright owners and/or agree
to the terms and conditions of use that are
given on the downloading website (Li et al,
2004; MSRI, 2010; CJKI, 2010). NEWS
2011 will provide the contact details of each
individual database. The data would be pro-
vided in Unicode UTF-8 encoding, in XML
format; the results are expected to be sub-
mitted in UTF-8 encoding in XML format.
The XML formats details are available in Ap-
pendix A.
2. The data are provided in 3 sets as described
above.
3. Name pairs are distributed as-is, as provided
by the respective creators.3
Name origin Source script Target script Data Owner Data Size Task IDTrain Dev Progress Test 2012 Test
Western English Chinese Institute for Infocomm Research 37K 2.8K 2K 1K EnCh
Western Chinese English Institute for Infocomm Research 28K 2.7K 2.2K 1K ChEn
Western English Korean Hangul CJK Institute 7K 1K 609 1K EnKo
Western English Japanese Katakana CJK Institute 26K 2K 1.8K 1K EnJa
Japanese English Japanese Kanji CJK Institute 10K 2K 571 1K JnJk
Arabic Arabic English CJK Institute 27K 2.5K 2.6K 1K ArEn
Mixed English Hindi Microsoft Research India 12K 1K 1K 1K EnHi
Mixed English Tamil Microsoft Research India 10K 1K 1K 1K EnTa
Mixed English Kannada Microsoft Research India 10K 1K 1K 1K EnKa
Mixed English Bangla Microsoft Research India 13K 1K 1K 1K EnBa
Western English Thai NECTEC 27K 2K 2K 1K EnTh
Western Thai English NECTEC 25K 2K 1.9K 1K ThEn
Western English Persian Sarvnaz Karimi / RMIT 10K 2K 2K 1K EnPe
Western English Hebrew Microsoft Research India 9.5K 1K 1K 1K EnHe
Table 1: Source and target languages for the shared task on transliteration.
(a) While the databases are mostly man-
ually checked, there may be still in-
consistency (that is, non-standard usage,
region-specific usage, errors, etc.) or in-
completeness (that is, not all right varia-
tions may be covered).
(b) The participants may use any method to
further clean up the data provided.
i. If they are cleaned up manually, we
appeal that such data be provided
back to the organisers for redistri-
bution to all the participating groups
in that language pair; such sharing
benefits all participants, and further
ensures that the evaluation provides
normalisation with respect to data
quality.
ii. If automatic cleanup were used,
such cleanup would be considered a
part of the system fielded, and hence
not required to be shared with all
participants.
4. Standard Runs We expect that the partici-
pants to use only the data (parallel names)
provided by the Shared Task for translitera-
tion task for a ?standard? run to ensure a fair
evaluation. One such run (using only the data
provided by the shared task) is mandatory for
all participants for a given language pair that
they participate in.
5. Non-standard Runs If more data (either par-
allel names data or monolingual data) were
used, then all such runs using extra data must
be marked as ?non-standard?. For such ?non-
standard? runs, it is required to disclose the
size and characteristics of the data used in the
system paper.
6. A participant may submit a maximum of 8
runs for a given language pair (including the
mandatory 1 ?standard? run marked as ?pri-
mary submission?).
6 Paper Format
Paper submissions to NEWS 2012 should follow
the ACL 2012 paper submission policy, includ-
ing paper format, blind review policy and title and
author format convention. Full papers (research
paper) are in two-column format without exceed-
ing eight (8) pages of content plus two (2) extra
page for references and short papers (task paper)
are also in two-column format without exceeding
four (4) pages content plus two (2) extra page for
references. Submission must conform to the offi-
cial ACL 2012 style guidelines. For details, please
refer to the ACL 2012 website2.
7 Evaluation Metrics
We plan to measure the quality of the translitera-
tion task using the following 4 metrics. We accept
up to 10 output candidates in a ranked list for each
input entry.
Since a given source name may have multiple
correct target transliterations, all these alternatives
are treated equally in the evaluation. That is, any
of these alternatives are considered as a correct
transliteration, and the first correct transliteration
in the ranked list is accepted as a correct hit.
2http://www.ACL2012.org/4
The following notation is further assumed:
N : Total number of names (source
words) in the test set
ni : Number of reference transliterations
for i-th name in the test set (ni ? 1)
ri,j : j-th reference transliteration for i-th
name in the test set
ci,k : k-th candidate transliteration (system
output) for i-th name in the test set
(1 ? k ? 10)
Ki : Number of candidate transliterations
produced by a transliteration system
1. Word Accuracy in Top-1 (ACC) Also
known as Word Error Rate, it measures correct-
ness of the first transliteration candidate in the can-
didate list produced by a transliteration system.
ACC = 1 means that all top candidates are cor-
rect transliterations i.e. they match one of the ref-
erences, and ACC = 0 means that none of the top
candidates are correct.
ACC =
1
N
N?
i=1
{
1 if ? ri,j : ri,j = ci,1;
0 otherwise
}
(1)
2. Fuzziness in Top-1 (Mean F-score) The
mean F-score measures how different, on average,
the top transliteration candidate is from its closest
reference. F-score for each source word is a func-
tion of Precision and Recall and equals 1 when the
top candidate matches one of the references, and
0 when there are no common characters between
the candidate and any of the references.
Precision and Recall are calculated based on the
length of the Longest Common Subsequence be-
tween a candidate and a reference:
LCS(c, r) =
1
2
(|c|+ |r| ? ED(c, r)) (2)
where ED is the edit distance and |x| is the length
of x. For example, the longest common subse-
quence between ?abcd? and ?afcde? is ?acd? and
its length is 3. The best matching reference, that
is, the reference for which the edit distance has
the minimum, is taken for calculation. If the best
matching reference is given by
ri,m = argmin
j
(ED(ci,1, ri,j)) (3)
then Recall, Precision and F-score for i-th word
are calculated as
Ri =
LCS(ci,1, ri,m)
|ri,m|
(4)
Pi =
LCS(ci,1, ri,m)
|ci,1|
(5)
Fi = 2
Ri ? Pi
Ri + Pi
(6)
? The length is computed in distinct Unicode
characters.
? No distinction is made on different character
types of a language (e.g., vowel vs. conso-
nants vs. combining diereses etc.)
3. Mean Reciprocal Rank (MRR) Measures
traditional MRR for any right answer produced by
the system, from among the candidates. 1/MRR
tells approximately the average rank of the correct
transliteration. MRR closer to 1 implies that the
correct answer is mostly produced close to the top
of the n-best lists.
RRi =
{
minj 1j if ?ri,j , ci,k : ri,j = ci,k;
0 otherwise
}
(7)
MRR =
1
N
N?
i=1
RRi (8)
4. MAPref Measures tightly the precision in the
n-best candidates for i-th source name, for which
reference transliterations are available. If all of
the references are produced, then the MAP is 1.
Let?s denote the number of correct candidates for
the i-th source word in k-best list as num(i, k).
MAPref is then given by
MAPref =
1
N
N?
i
1
ni
(
ni?
k=1
num(i, k)
)
(9)
8 Contact Us
If you have any questions about this share task and
the database, please email to
Mr. Ming Liu
Institute for Infocomm Research (I2R),
A*STAR
1 Fusionopolis Way
#08-05 South Tower, Connexis
Singapore 138632
mliu@i2r.a-star.edu.sg5
Dr. Min Zhang
Institute for Infocomm Research (I2R),
A*STAR
1 Fusionopolis Way
#08-05 South Tower, Connexis
Singapore 138632
mzhang@i2r.a-star.edu.sg
References
[CJKI2010] CJKI. 2010. CJK Institute.
http://www.cjk.org/.
[Li et al2004] Haizhou Li, Min Zhang, and Jian Su.
2004. A joint source-channel model for machine
transliteration. In Proc. 42nd ACL Annual Meeting,
pages 159?166, Barcelona, Spain.
[MSRI2010] MSRI. 2010. Microsoft Research India.
http://research.microsoft.com/india.
6
A Training/Development Data
? File Naming Conventions:
NEWS12 train XXYY nnnn.xml
NEWS12 dev XXYY nnnn.xml
NEWS12 test XXYY nnnn.xml
NEWS11 test XXYY nnnn.xml
(progress test sets)
? XX: Source Language
? YY: Target Language
? nnnn: size of parallel/monolingual
names (?25K?, ?10000?, etc)
? File formats:
All data will be made available in XML for-
mats (Figure 1).
? Data Encoding Formats:
The data will be in Unicode UTF-8 encod-
ing files without byte-order mark, and in the
XML format specified.
B Submission of Results
? File Naming Conventions:
You can give your files any name you like.
During submission online you will need to
indicate whether this submission belongs to
a ?standard? or ?non-standard? run, and if it
is a ?standard? run, whether it is the primary
submission.
? File formats:
All data will be made available in XML for-
mats (Figure 2).
? Data Encoding Formats:
The results are expected to be submitted in
UTF-8 encoded files without byte-order mark
only, and in the XML format specified.
7
<?xml version="1.0" encoding="UTF-8"?>
<TransliterationCorpus
CorpusID = "NEWS2012-Train-EnHi-25K"
SourceLang = "English"
TargetLang = "Hindi"
CorpusType = "Train|Dev"
CorpusSize = "25000"
CorpusFormat = "UTF8">
<Name ID=fl1fl>
<SourceName>eeeeee1</SourceName>
<TargetName ID="1">hhhhhh1_1</TargetName>
<TargetName ID="2">hhhhhh1_2</TargetName>
...
<TargetName ID="n">hhhhhh1_n</TargetName>
</Name>
<Name ID=fl2fl>
<SourceName>eeeeee2</SourceName>
<TargetName ID="1">hhhhhh2_1</TargetName>
<TargetName ID="2">hhhhhh2_2</TargetName>
...
<TargetName ID="m">hhhhhh2_m</TargetName>
</Name>
...
<!-- rest of the names to follow -->
...
</TransliterationCorpus>
Figure 1: File: NEWS2012 Train EnHi 25K.xml
8
<?xml version="1.0" encoding="UTF-8"?>
<TransliterationTaskResults
SourceLang = "English"
TargetLang = "Hindi"
GroupID = "Trans University"
RunID = "1"
RunType = "Standard"
Comments = "HMM Run with params: alpha=0.8 beta=1.25">
<Name ID="1">
<SourceName>eeeeee1</SourceName>
<TargetName ID="1">hhhhhh11</TargetName>
<TargetName ID="2">hhhhhh12</TargetName>
<TargetName ID="3">hhhhhh13</TargetName>
...
<TargetName ID="10">hhhhhh110</TargetName>
<!-- Participants to provide their
top 10 candidate transliterations -->
</Name>
<Name ID="2">
<SourceName>eeeeee2</SourceName>
<TargetName ID="1">hhhhhh21</TargetName>
<TargetName ID="2">hhhhhh22</TargetName>
<TargetName ID="3">hhhhhh23</TargetName>
...
<TargetName ID="10">hhhhhh110</TargetName>
<!-- Participants to provide their
top 10 candidate transliterations -->
</Name>
...
<!-- All names in test corpus to follow -->
...
</TransliterationTaskResults>
Figure 2: Example file: NEWS2012 EnHi TUniv 01 StdRunHMMBased.xml
9
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 10?20,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Report of NEWS 2012 Machine Transliteration Shared Task
Min Zhang?, Haizhou Li?, A Kumaran? and Ming Liu ?
?Institute for Infocomm Research, A*STAR, Singapore 138632
{mzhang,hli,mliu}@i2r.a-star.edu.sg
?Multilingual Systems Research, Microsoft Research India
A.Kumaran@microsoft.com
Abstract
This report documents the Machine
Transliteration Shared Task conducted as
a part of the Named Entities Workshop
(NEWS 2012), an ACL 2012 workshop.
The shared task features machine translit-
eration of proper names from English to
11 languages and from 3 languages to
English. In total, 14 tasks are provided.
7 teams participated in the evaluations.
Finally, 57 standard and 1 non-standard
runs are submitted, where diverse translit-
eration methodologies are explored and
reported on the evaluation data. We report
the results with 4 performance metrics.
We believe that the shared task has
successfully achieved its objective by pro-
viding a common benchmarking platform
for the research community to evaluate the
state-of-the-art technologies that benefit
the future research and development.
1 Introduction
Names play a significant role in many Natural
Language Processing (NLP) and Information Re-
trieval (IR) systems. They are important in Cross
Lingual Information Retrieval (CLIR) and Ma-
chine Translation (MT) as the system performance
has been shown to positively correlate with the
correct conversion of names between the lan-
guages in several studies (Demner-Fushman and
Oard, 2002; Mandl and Womser-Hacker, 2005;
Hermjakob et al, 2008; Udupa et al, 2009). The
traditional source for name equivalence, the bilin-
gual dictionaries ? whether handcrafted or sta-
tistical ? offer only limited support because new
names always emerge.
All of the above point to the critical need for ro-
bust Machine Transliteration technology and sys-
tems. Much research effort has been made to ad-
dress the transliteration issue in the research com-
munity (Knight and Graehl, 1998; Meng et al,
2001; Li et al, 2004; Zelenko and Aone, 2006;
Sproat et al, 2006; Sherif and Kondrak, 2007;
Hermjakob et al, 2008; Al-Onaizan and Knight,
2002; Goldwasser and Roth, 2008; Goldberg and
Elhadad, 2008; Klementiev and Roth, 2006; Oh
and Choi, 2002; Virga and Khudanpur, 2003; Wan
and Verspoor, 1998; Kang and Choi, 2000; Gao
et al, 2004; Zelenko and Aone, 2006; Li et al,
2009b; Li et al, 2009a). These previous work
fall into three categories, i.e., grapheme-based,
phoneme-based and hybrid methods. Grapheme-
based method (Li et al, 2004) treats translitera-
tion as a direct orthographic mapping and only
uses orthography-related features while phoneme-
based method (Knight and Graehl, 1998) makes
use of phonetic correspondence to generate the
transliteration. Hybrid method refers to the com-
bination of several different models or knowledge
sources to support the transliteration generation.
The first machine transliteration shared task (Li
et al, 2009b; Li et al, 2009a) was held in NEWS
2009 at ACL-IJCNLP 2009. It was the first time
to provide common benchmarking data in diverse
language pairs for evaluation of state-of-the-art
techniques. While the focus of the 2009 shared
task was on establishing the quality metrics and
on baselining the transliteration quality based on
those metrics, the 2010 shared task (Li et al,
2010a; Li et al, 2010b) expanded the scope of
the transliteration generation task to about a dozen
languages, and explored the quality depending on
the direction of transliteration, between the lan-
guages. In NEWS 2011 (Zhang et al, 2011a;
Zhang et al, 2011b), we significantly increased
the hand-crafted parallel named entities corpora to
include 14 different language pairs from 11 lan-
guage families, and made them available as the
common dataset for the shared task. NEWS 2012
was a continued effort of NEWS 2011, NEWS10
2010 and NEWS 2009.
The rest of the report is organised as follows.
Section 2 outlines the machine transliteration task
and the corpora used and Section 3 discusses the
metrics chosen for evaluation, along with the ratio-
nale for choosing them. Sections 4 and 5 present
the participation in the shared task and the results
with their analysis, respectively. Section 6 con-
cludes the report.
2 Transliteration Shared Task
In this section, we outline the definition and the
description of the shared task.
2.1 ?Transliteration?: A definition
There exists several terms that are used inter-
changeably in the contemporary research litera-
ture for the conversion of names between two
languages, such as, transliteration, transcription,
and sometimes Romanisation, especially if Latin
scripts are used for target strings (Halpern, 2007).
Our aim is not only at capturing the name con-
version process from a source to a target lan-
guage, but also at its practical utility for down-
stream applications, such as CLIR and MT. There-
fore, we adopted the same definition of translit-
eration as during the NEWS 2009 workshop (Li
et al, 2009a) to narrow down ?transliteration? to
three specific requirements for the task, as fol-
lows:?Transliteration is the conversion of a given
name in the source language (a text string in the
source writing system or orthography) to a name
in the target language (another text string in the
target writing system or orthography), such that
the target language name is: (i) phonemically
equivalent to the source name (ii) conforms to the
phonology of the target language and (iii) matches
the user intuition of the equivalent of the source
language name in the target language, consider-
ing the culture and orthographic character usage
in the target language.?
Following NEWS 2011, in NEWS 2012, we
still keep the three back-transliteration tasks. We
define back-transliteration as a process of restor-
ing transliterated words to their original lan-
guages. For example, NEWS 2012 offers the tasks
to convert western names written in Chinese and
Thai into their original English spellings, and ro-
manized Japanese names into their original Kanji
writings.
2.2 Shared Task Description
Following the tradition of NEWS workshop se-
ries, the shared task at NEWS 2012 is specified
as development of machine transliteration systems
in one or more of the specified language pairs.
Each language pair of the shared task consists of a
source and a target language, implicitly specifying
the transliteration direction. Training and develop-
ment data in each of the language pairs have been
made available to all registered participants for de-
veloping a transliteration system for that specific
language pair using any approach that they find
appropriate.
At the evaluation time, a standard hand-crafted
test set consisting of between 500 and 3,000
source names (approximately 5-10% of the train-
ing data size) have been released, on which the
participants are required to produce a ranked list
of transliteration candidates in the target language
for each source name. The system output is
tested against a reference set (which may include
multiple correct transliterations for some source
names), and the performance of a system is cap-
tured in multiple metrics (defined in Section 3),
each designed to capture a specific performance
dimension.
For every language pair each participant is re-
quired to submit at least one run (designated as a
?standard? run) that uses only the data provided by
the NEWS workshop organisers in that language
pair, and no other data or linguistic resources. This
standard run ensures parity between systems and
enables meaningful comparison of performance
of various algorithmic approaches in a given lan-
guage pair. Participants are allowed to submit
more ?standard? runs, up to 4 in total. If more than
one ?standard? runs is submitted, it is required to
name one of them as a ?primary? run, which is
used to compare results across different systems.
In addition, up to 4 ?non-standard? runs could be
submitted for every language pair using either data
beyond that provided by the shared task organisers
or linguistic resources in a specific language, or
both. This essentially may enable any participant
to demonstrate the limits of performance of their
system in a given language pair.
The shared task timelines provide adequate time
for development, testing (more than 1 month after
the release of the training data) and the final re-
sult submission (4 days after the release of the test
data).11
2.3 Shared Task Corpora
We considered two specific constraints in select-
ing languages for the shared task: language diver-
sity and data availability. To make the shared task
interesting and to attract wider participation, it is
important to ensure a reasonable variety among
the languages in terms of linguistic diversity, or-
thography and geography. Clearly, the ability of
procuring and distributing a reasonably large (ap-
proximately 10K paired names for training and
testing together) hand-crafted corpora consisting
primarily of paired names is critical for this pro-
cess. At the end of the planning stage and after
discussion with the data providers, we have cho-
sen the set of 14 tasks shown in Table 1 (Li et al,
2004; Kumaran and Kellner, 2007; MSRI, 2009;
CJKI, 2010).
NEWS 2012 leverages on the success of NEWS
2011 by utilizing the training set of NEWS 2011 as
the training data of NEWS 2012 and the dev data
of NEWS 2011 as the dev data of NEWS 2012.
NEWS 2012 provides entirely new test data across
all 14 tasks for evaluation.
The names given in the training sets for Chi-
nese, Japanese, Korean, Thai, Persian and Hebrew
languages are Western names and their respective
transliterations; the Japanese Name (in English)
? Japanese Kanji data set consists only of native
Japanese names; the Arabic data set consists only
of native Arabic names. The Indic data set (Hindi,
Tamil, Kannada, Bangla) consists of a mix of In-
dian and Western names.
For all of the tasks chosen, we have been
able to procure paired names data between the
source and the target scripts and were able to
make them available to the participants. For
some language pairs, such as English-Chinese and
English-Thai, there are both transliteration and
back-transliteration tasks. Most of the task are just
one-way transliteration, although Indian data sets
contained mixture of names of both Indian and
Western origins. The language of origin of the
names for each task is indicated in the first column
of Table 1.
Finally, it should be noted here that the corpora
procured and released for NEWS 2012 represent
perhaps the most diverse and largest corpora to be
used for any common transliteration tasks today.
3 Evaluation Metrics and Rationale
The participants have been asked to submit results
of up to four standard and four non-standard runs.
One standard run must be named as the primary
submission and is used for the performance sum-
mary. Each run contains a ranked list of up to
10 candidate transliterations for each source name.
The submitted results are compared to the ground
truth (reference transliterations) using 4 evalua-
tion metrics capturing different aspects of translit-
eration performance. The same as the NEWS
2011, we have dropped two MAP metrics used
in NEWS 2009 because they don?t offer additional
information to MAPref . Since a name may have
multiple correct transliterations, all these alterna-
tives are treated equally in the evaluation, that is,
any of these alternatives is considered as a correct
transliteration, and all candidates matching any of
the reference transliterations are accepted as cor-
rect ones.
The following notation is further assumed:
N : Total number of names (source
words) in the test set
ni : Number of reference transliterations
for i-th name in the test set (ni ? 1)
ri,j : j-th reference transliteration for i-th
name in the test set
ci,k : k-th candidate transliteration (system
output) for i-th name in the test set
(1 ? k ? 10)
Ki : Number of candidate transliterations
produced by a transliteration system
3.1 Word Accuracy in Top-1 (ACC)
Also known as Word Error Rate, it measures cor-
rectness of the first transliteration candidate in the
candidate list produced by a transliteration system.
ACC = 1 means that all top candidates are cor-
rect transliterations i.e. they match one of the ref-
erences, and ACC = 0 means that none of the top
candidates are correct.
ACC =
1
N
N?
i=1
{
1 if ?ri,j : ri,j = ci,1;
0 otherwise
}
(1)
3.2 Fuzziness in Top-1 (Mean F-score)
The mean F-score measures how different, on av-
erage, the top transliteration candidate is from its
closest reference. F-score for each source word12
Name origin Source script Target script Data Owner Data Size Task IDTrain Dev Test
Western English Chinese Institute for Infocomm Research 37K 2.8K 2K 1K EnCh
Western Chinese English Institute for Infocomm Research 28K 2.7K 2.2K 1K ChEn
Western English Korean Hangul CJK Institute 7K 1K 609 1K EnKo
Western English Japanese Katakana CJK Institute 26K 2K 1.8K 1K EnJa
Japanese English Japanese Kanji CJK Institute 10K 2K 571 1K JnJk
Arabic Arabic English CJK Institute 27K 2.5K 2.6K 1K ArEn
Mixed English Hindi Microsoft Research India 12K 1K 1K 1K EnHi
Mixed English Tamil Microsoft Research India 10K 1K 1K 1K EnTa
Mixed English Kannada Microsoft Research India 10K 1K 1K 1K EnKa
Mixed English Bangla Microsoft Research India 13K 1K 1K 1K EnBa
Western English Thai NECTEC 27K 2K 2K 1K EnTh
Western Thai English NECTEC 25K 2K 1.9K 1K ThEn
Western English Persian Sarvnaz Karimi / RMIT 10K 2K 2K 1K EnPe
Western English Hebrew Microsoft Research India 9.5K 1K 1K 1K EnHe
Table 1: Source and target languages for the shared task on transliteration.
is a function of Precision and Recall and equals 1
when the top candidate matches one of the refer-
ences, and 0 when there are no common characters
between the candidate and any of the references.
Precision and Recall are calculated based on
the length of the Longest Common Subsequence
(LCS) between a candidate and a reference:
LCS(c, r) =
1
2
(|c|+ |r| ? ED(c, r)) (2)
where ED is the edit distance and |x| is the length
of x. For example, the longest common subse-
quence between ?abcd? and ?afcde? is ?acd? and
its length is 3. The best matching reference, that
is, the reference for which the edit distance has
the minimum, is taken for calculation. If the best
matching reference is given by
ri,m = argmin
j
(ED(ci,1, ri,j)) (3)
then Recall, Precision and F-score for i-th word
are calculated as
Ri =
LCS(ci,1, ri,m)
|ri,m|
(4)
Pi =
LCS(ci,1, ri,m)
|ci,1|
(5)
Fi = 2
Ri ? Pi
Ri + Pi
(6)
? The length is computed in distinct Unicode
characters.
? No distinction is made on different character
types of a language (e.g., vowel vs. conso-
nants vs. combining diereses etc.)
3.3 Mean Reciprocal Rank (MRR)
Measures traditional MRR for any right answer
produced by the system, from among the candi-
dates. 1/MRR tells approximately the average
rank of the correct transliteration. MRR closer to 1
implies that the correct answer is mostly produced
close to the top of the n-best lists.
RRi =
{
minj 1j if ?ri,j , ci,k : ri,j = ci,k;
0 otherwise
}
(7)
MRR =
1
N
N?
i=1
RRi (8)
3.4 MAPref
Measures tightly the precision in the n-best can-
didates for i-th source name, for which reference
transliterations are available. If all of the refer-
ences are produced, then the MAP is 1. Let?s de-
note the number of correct candidates for the i-th
source word in k-best list as num(i, k). MAPref
is then given by
MAPref =
1
N
N?
i
1
ni
(
ni?
k=1
num(i, k)
)
(9)
4 Participation in Shared Task
7 teams submitted their transliteration results. Ta-
ble 3 shows the details of registration tasks. Teams
are required to submit at least one standard run for
every task they participated in. In total, we re-
ceive 57 standard and 1 non-standard runs. Table 2
shows the number of standard and non-standard
runs submitted for each task. It is clear that the
most ?popular? task is the transliteration from En-
glish to Chinese being attempted by 7 participants.13
English to
Chinese
Chinese to
English
English to
Thai
Thai to En-
glish
English to
Hindi
English to
Tamil
English to
Kannada
Language pair code EnCh ChEn EnTh ThEn EnHi EnTa EnKa
Standard runs 14 5 2 2 2 2 2
Non-standard runs 0 0 0 0 0 0 0
English to
Japanese
Katakana
English
to Korean
Hangul
English to
Japanese
Kanji
Arabic to
English
English to
Bengali
(Bangla)
English to
Persian
English to
Hebrew
Language pair code EnJa EnKo JnJk ArEn EnBa EnPe EnHe
Standard runs 3 4 4 5 4 4 4
Non-standard runs 0 1 0 0 0 0 0
Table 2: Number of runs submitted for each task. Number of participants coincides with the number of
standard runs submitted.
Team
ID
Organisation EnCh ChEn EnTh ThEn EnHi EnTa EnKa EnJa EnKo JnJk ArEn EnBa EnPe EnHe
1 University of Alberta x
2 NICT x x x x x x x x x x x x x x
3 MIT@Lab of HIT x
4 IASL, Academia
Sinica
x
5 Yahoo Japan Corpora-
tion
x x x x x x x x
6 Yuan Ze University x
7 CMU x x x x x x x x x x x x x x
Table 3: Participation of teams in different tasks.
5 Task Results and Analysis
5.1 Standard runs
All the results are presented numerically in Ta-
bles 4?17, for all evaluation metrics. These are the
official evaluation results published for this edition
of the transliteration shared task.
The methodologies used in the ten submitted
system papers are summarized as follows. Similar
to their NEWS 2011 system, Finch et al (2012)
employ non-Parametric Bayesian method to co-
segment bilingual named entities for model train-
ing and report very good performance. This sys-
tem is based on phrase-based statistical machine
transliteration (SMT) (Finch and Sumita, 2008),
an approach initially developed for machine trans-
lation (Koehn et al, 2003), where the SMT sys-
tem?s log-linear model is augmented with a set of
features specifically suited to the task of translit-
eration. In particular, the model utilizes a fea-
ture based on a joint source-channel model, and
a feature based on a maximum entropy model that
predicts target grapheme sequences using the local
context of graphemes and grapheme sequences in
both source and target languages. Different from
their NEWS 2011 system, in order to solve the
data sparseness issue, they use two RNN-based
LM to project the grapheme set onto a smaller hid-
den representation: one for the target grapheme se-
quence and the other for the sequence of grapheme
sequence pair used to generate the target.
Zhang et al (2012) also use the statistical
phrase-based SMT framework. They propose the
fine-grained English segmentation algorithm and
other new features and achieve very good perfor-
mance. Wu et al (2012) uses m2m-aligner and
DirecTL-p decoder and two re-ranking methods:
co-occurrence at web corpus and JLIS-Reranking
method based on the features from alignment re-
sults. They report very good performance at
English-Korean tasks. Okuno (2012) studies the
mpaligner (an improvement of m2m-aligner) and14
shows that mpaligner is more effective than m2m-
aligner. They also find that de-romanization is cru-
cial to JnJk task and mora is the best alignment
unit for EnJa task. Ammar et al (2012) use CRF
as the basic model but with two innovations: a
training objective that optimizes toward any of a
set of possible correct labels (i.e., multiple refer-
ences) and a k-best reranking with non-local fea-
tures. Their results on ArEn show that the two
features are very effective in accuracy improve-
ment. Kondrak et al (2012) study the language-
specific adaptations in the context of two language
pairs: English to Chinese (Pinyin representation)
and Arabic to English (letter mapping). They con-
clude that Pinyin representation is useful while let-
ter mapping is less effective. Kuo et al (2012) ex-
plore two-stage CRF for Enligsh-to-Chinese task
and show that the two-stage CRF outperform tra-
ditional one-stage CRF.
5.2 Non-standard runs
For the non-standard runs, we pose no restrictions
on the use of data or other linguistic resources.
The purpose of non-standard runs is to see how
best personal name transliteration can be, for a
given language pair. In NEWS 2012, only one
non-standard run (Wu et al, 2012) was submitted.
Their reported web-based re-validation method is
very effective.
6 Conclusions and Future Plans
The Machine Transliteration Shared Task in
NEWS 2012 shows that the community has a con-
tinued interest in this area. This report summa-
rizes the results of the shared task. Again, we
are pleased to report a comprehensive calibra-
tion and baselining of machine transliteration ap-
proaches as most state-of-the-art machine translit-
eration techniques are represented in the shared
task.
In addition to the most popular techniques such
as Phrase-Based Machine Transliteration (Koehn
et al, 2003), CRF, re-ranking, DirecTL-p de-
coder, Non-Parametric Bayesian Co-segmentation
(Finch et al, 2011), and Multi-to-Multi Joint
Source Channel Model (Chen et al, 2011) in the
NEWS 2011, we are delighted to see that sev-
eral new techniques have been proposed and ex-
plored with promising results reported, including
RNN-based LM (Finch et al, 2012), English Seg-
mentation algorithm (Zhang et al, 2012), JLIS-
reranking method (Wu et al, 2012), improved
m2m-aligner (Okuno, 2012), multiple reference-
optimized CRF (Ammar et al, 2012), language
dependent adaptation (Kondrak et al, 2012) and
two-stage CRF (Kuo et al, 2012). As the stan-
dard runs are limited by the use of corpus, most of
the systems are implemented under the direct or-
thographic mapping (DOM) framework (Li et al,
2004). While the standard runs allow us to con-
duct meaningful comparison across different al-
gorithms, we recognise that the non-standard runs
open up more opportunities for exploiting a vari-
ety of additional linguistic corpora.
Encouraged by the success of the NEWS work-
shop series, we would like to continue this event
in the future conference to promote the machine
transliteration research and development.
Acknowledgements
The organisers of the NEWS 2012 Shared Task
would like to thank the Institute for Infocomm
Research (Singapore), Microsoft Research In-
dia, CJK Institute (Japan), National Electronics
and Computer Technology Center (Thailand) and
Sarvnaz Karim / RMIT for providing the corpora
and technical support. Without those, the Shared
Task would not be possible. We thank those par-
ticipants who identified errors in the data and sent
us the errata. We also want to thank the members
of programme committee for their invaluable com-
ments that improve the quality of the shared task
papers. Finally, we wish to thank all the partici-
pants for their active participation that have made
this first machine transliteration shared task a com-
prehensive one.
15
References
Yaser Al-Onaizan and Kevin Knight. 2002. Machine
transliteration of names in arabic text. In Proc.
ACL-2002Workshop: Computational Apporaches to
Semitic Languages, Philadelphia, PA, USA.
Waleed Ammar, Chris Dyer, and Noah Smith. 2012.
Transliteration by sequence labeling with lattice en-
codings and reranking. In Proc. Named Entities
Workshop at ACL 2012.
Yu Chen, Rui Wang, and Yi Zhang. 2011. Statisti-
cal machine transliteration with multi-to-multi joint
source channel model. In Proc. Named Entities
Workshop at IJCNLP 2011.
CJKI. 2010. CJK Institute. http://www.cjk.org/.
D. Demner-Fushman and D. W. Oard. 2002. The ef-
fect of bilingual term list size on dictionary-based
cross-language information retrieval. In Proc. 36-th
Hawaii Int?l. Conf. System Sciences, volume 4, page
108.2.
Andrew Finch and Eiichiro Sumita. 2008. Phrase-
based machine transliteration. In Proc. 3rd Int?l.
Joint Conf NLP, volume 1, Hyderabad, India, Jan-
uary.
Andrew Finch, Paul Dixon, and Eiichiro Sumita. 2011.
Integrating models derived from non-parametric
bayesian co-segmentation into a statistical machine
transliteration system. In Proc. Named Entities
Workshop at IJCNLP 2011.
Andrew Finch, Paul Dixon, and Eiichiro Sumita. 2012.
Rescoring a phrase-based machine transliteration
systemwith recurrent neural network language mod-
els. In Proc. Named Entities Workshop at ACL 2012.
Wei Gao, Kam-Fai Wong, and Wai Lam. 2004.
Phoneme-based transliteration of foreign names for
OOV problem. In Proc. IJCNLP, pages 374?381,
Sanya, Hainan, China.
Yoav Goldberg andMichael Elhadad. 2008. Identifica-
tion of transliterated foreign words in Hebrew script.
In Proc. CICLing, volume LNCS 4919, pages 466?
477.
Dan Goldwasser and Dan Roth. 2008. Translitera-
tion as constrained optimization. In Proc. EMNLP,
pages 353?362.
Jack Halpern. 2007. The challenges and pitfalls
of Arabic romanization and arabization. In Proc.
Workshop on Comp. Approaches to Arabic Script-
based Lang.
Ulf Hermjakob, Kevin Knight, and Hal Daume?. 2008.
Name translation in statistical machine translation:
Learning when to transliterate. In Proc. ACL,
Columbus, OH, USA, June.
Byung-Ju Kang and Key-Sun Choi. 2000.
English-Korean automatic transliteration/back-
transliteration system and character alignment. In
Proc. ACL, pages 17?18, Hong Kong.
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discov-
ery from multilingual comparable corpora. In Proc.
21st Int?l Conf Computational Linguistics and 44th
Annual Meeting of ACL, pages 817?824, Sydney,
Australia, July.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4).
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. HLT-NAACL.
Grzegorz Kondrak, Xingkai Li, and Mohammad
Salameh. 2012. Transliteration experiments on chi-
nese and arabic. In Proc. Named Entities Workshop
at ACL 2012.
A Kumaran and T. Kellner. 2007. A generic frame-
work for machine transliteration. In Proc. SIGIR,
pages 721?722.
Chan-Hung Kuo, Shih-Hung Liu, Mike Tian-Jian
Jiang, Cheng-Wei Lee, and Wen-Lian Hsu. 2012.
Cost-benefit analysis of two-stage conditional
random fields based english-to-chinese machine
transliteration. In Proc. Named Entities Workshop
at ACL 2012.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source-channel model for machine transliteration.
In Proc. 42nd ACL Annual Meeting, pages 159?166,
Barcelona, Spain.
Haizhou Li, A Kumaran, Vladimir Pervouchine, and
Min Zhang. 2009a. Report of NEWS 2009 machine
transliteration shared task. In Proc. Named Entities
Workshop at ACL 2009.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir
Pervouchine. 2009b. ACL-IJCNLP 2009 Named
Entities Workshop ? Shared Task on Translitera-
tion. In Proc. Named Entities Workshop at ACL
2009.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir
Pervouchine. 2010a. Report of news 2010 translit-
eration generation shared task. In Proc. Named En-
tities Workshop at ACL 2010.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir
Pervouchine. 2010b. Whitepaper of news 2010
shared task on transliteration generation. In Proc.
Named Entities Workshop at ACL 2010.
T. Mandl and C. Womser-Hacker. 2005. The effect of
named entities on effectiveness in cross-language in-
formation retrieval evaluation. In Proc. ACM Symp.
Applied Comp., pages 1059?1064.16
Helen M. Meng, Wai-Kit Lo, Berlin Chen, and Karen
Tang. 2001. Generate phonetic cognates to han-
dle name entities in English-Chinese cross-language
spoken document retrieval. In Proc. ASRU.
MSRI. 2009. Microsoft Research India.
http://research.microsoft.com/india.
Jong-Hoon Oh and Key-Sun Choi. 2002. An English-
Korean transliteration model using pronunciation
and contextual rules. In Proc. COLING 2002,
Taipei, Taiwan.
Yoh Okuno. 2012. Applying mpaligner to machine
transliteration with japanese-specific heuristics. In
Proc. Named Entities Workshop at ACL 2012.
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
based transliteration. In Proc. 45th Annual Meeting
of the ACL, pages 944?951, Prague, Czech Repub-
lic, June.
Richard Sproat, Tao Tao, and ChengXiang Zhai. 2006.
Named entity transliteration with comparable cor-
pora. In Proc. 21st Int?l Conf Computational Lin-
guistics and 44th Annual Meeting of ACL, pages 73?
80, Sydney, Australia.
Raghavendra Udupa, K. Saravanan, Anton Bakalov,
and Abhijit Bhole. 2009. ?They are out there, if
you know where to look?: Mining transliterations
of OOV query terms for cross-language informa-
tion retrieval. In LNCS: Advances in Information
Retrieval, volume 5478, pages 437?448. Springer
Berlin / Heidelberg.
Paola Virga and Sanjeev Khudanpur. 2003. Translit-
eration of proper names in cross-lingual information
retrieval. In Proc. ACL MLNER, Sapporo, Japan.
Stephen Wan and Cornelia Maria Verspoor. 1998. Au-
tomatic English-Chinese name transliteration for de-
velopment of multilingual resources. In Proc. COL-
ING, pages 1352?1356.
Chun-Kai Wu, Yu-Chun Wang, and Richard Tzong-
Han Tsai. 2012. English-korean named entity
transliteration using substring alignment and re-
ranking methods. In Proc. Named Entities Work-
shop at ACL 2012.
Dmitry Zelenko and Chinatsu Aone. 2006. Discrimi-
native methods for transliteration. In Proc. EMNLP,
pages 612?617, Sydney, Australia, July.
Min Zhang, A Kumaran, and Haizhou Li. 2011a.
Whitepaper of news 2011 shared task on machine
transliteration. In Proc. Named Entities Workshop
at IJCNLP 2011.
Min Zhang, Haizhou Li, A Kumaran, and Ming Liu.
2011b. Report of news 2011 machine transliteration
shared task. In Proc. Named Entities Workshop at
IJCNLP 2011.
Chunyue Zhang, Tingting Li, and Tiejun Zhao. 2012.
Syllable-based machine transliteration with extra
phrase features. In Proc. Named Entities Workshop
at ACL 2012.
17
Team ID ACC F -score MRR MAPref Organisation
Primary runs
3 0.330357 0.66898 0.413062 0.320285 MIT@Lab of HIT
1 0.325397 0.67228 0.418079 0.316296 University of Alberta
2 0.310516 0.66585 0.44664 0.307788 NICT
4 0.310516 0.662467 0.37696 0.299266 IASL, Academia Sinica
5 0.300595 0.655091 0.376025 0.292252 Yahoo Japan Corporation
7 0.031746 0.430698 0.055574 0.030265 CMU
Non-primary standard runs
3 0.330357 0.676232 0.407755 0.3191 MIT@Lab of HIT
1 0.325397 0.673053 0.409452 0.316055 University of Alberta
1 0.324405 0.668165 0.424517 0.316248 University of Alberta
3 0.31746 0.666551 0.399476 0.308187 MIT@Lab of HIT
4 0.298611 0.658836 0.362263 0.288725 IASL, Academia Sinica
5 0.298611 0.656974 0.357481 0.289373 Yahoo Japan Corporation
4 0.294643 0.651988 0.357495 0.284274 IASL, Academia Sinica
4 0.290675 0.653565 0.370733 0.282545 IASL, Academia Sinica
Table 4: Runs submitted for English to Chinese task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.20314 0.736058 0.308801 0.199569 NICT
3 0.176644 0.701791 0.257324 0.172991 MIT@Lab of HIT
7 0.030422 0.489705 0.048211 0.03004 CMU
5 0.012758 0.258962 0.017354 0.012758 Yahoo Japan Corporation
Non-primary standard runs
5 0.007851 0.258013 0.012163 0.007851 Yahoo Japan Corporation
Table 5: Runs submitted for Chinese to English back-transliteration task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.122168 0.746824 0.183318 0.122168 NICT
7 0.000809 0.288585 0.001883 0.000809 CMU
Table 6: Runs submitted for English to Thai task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.139968 0.765534 0.21551 0.139968 NICT
7 0 0.417451 0.000566 0 CMU
Table 7: Runs submitted for Thai to English back-transliteration task.
18
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.668 0.923347 0.73795 0.661278 NICT
7 0.048 0.645666 0.087842 0.048528 CMU
Table 8: Runs submitted for English to Hindi task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.592 0.908444 0.67881 0.5915 NICT
7 0.052 0.638029 0.083728 0.052 CMU
Table 9: Runs submitted for English to Tamil task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.546 0.900557 0.640534 0.545361 NICT
7 0.116 0.737857 0.180234 0.11625 CMU
Table 10: Runs submitted for English to Kannada task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.400774 0.810109 0.522758 0.397386 NICT
5 0.362052 0.802701 0.468973 0.35939 Yahoo Japan Corporation
7 0 0.147441 0.00038 0 CMU
Table 11: Runs submitted for English to Japanese Katakana task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
6 0.398095 0.731212 0.398095 0.396905 Yuan Ze University
2 0.38381 0.721247 0.464553 0.383095 NICT
5 0.334286 0.687794 0.411264 0.334048 Yahoo Japan Corporation
7 0 0 0.00019 0 CMU
Non-standard runs
6 0.458095 0.756755 0.484048 0.458095 Yuan Ze University
Table 12: Runs submitted for English to Korean task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.513242 0.693184 0.598304 0.418708 NICT
5 0.512329 0.693029 0.581803 0.400505 Yahoo Japan Corporation
7 0 0 0 0 CMU
Non-primary standard runs
5 0.511416 0.691131 0.580485 0.402127 Yahoo Japan Corporation
Table 13: Runs submitted for English to Japanese Kanji back-transliteration task.19
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.588235 0.929787 0.709003 0.506991 NICT
7 0.58391 0.925292 0.694338 0.367162 CMU
1 0.583045 0.932959 0.670457 0.42041 University of Alberta
Non-primary standard runs
7 0.57699 0.93025 0.678898 0.330353 CMU
7 0.573529 0.925306 0.675125 0.328782 CMU
Table 14: Runs submitted for Arabic to English task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
2 0.46 0.891476 0.582944 0.458417 NICT
5 0.404 0.882395 0.514541 0.402917 Yahoo Japan Corporation
7 0.178 0.783893 0.248674 0.177139 CMU
Non-primary standard runs
5 0.398 0.880286 0.510148 0.396528 Yahoo Japan Corporation
Table 15: Runs submitted for English to Bengali (Bangla) task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
5 0.658349 0.940642 0.761223 0.639873 Yahoo Japan Corporation
2 0.65547 0.941044 0.773843 0.642663 NICT
7 0.18618 0.803002 0.311881 0.184961 CMU
Non-primary standard runs
5 0.054702 0.627335 0.082754 0.054367 Yahoo Japan Corporation
Table 16: Runs submitted for English to Persian task.
Team ID ACC F -score MRR MAPref Organisation
Primary runs
5 0.190909 0.808491 0.253575 0.19 Yahoo Japan Corporation
2 0.153636 0.787254 0.228649 0.152727 NICT
7 0.097273 0.759444 0.130955 0.096818 CMU
Non-primary standard runs
5 0.165455 0.803019 0.241948 0.164545 Yahoo Japan Corporation
Table 17: Runs submitted for English to Hebrew task.
20
