c? 2003 Association for Computational Linguistics
Anaphora and Discourse Structure
Bonnie Webber? Matthew Stone?
Edinburgh University Rutgers University
Aravind Joshi? Alistair Knott?
University of Pennsylvania University of Otago
We argue in this article that many common adverbial phrases generally taken to signal a discourse
relation between syntactically connected units within discourse structure instead work anaphor-
ically to contribute relational meaning, with only indirect dependence on discourse structure.
This allows a simpler discourse structure to provide scaffolding for compositional semantics and
reveals multiple ways in which the relational meaning conveyed by adverbial connectives can
interact with that associated with discourse structure. We conclude by sketching out a lexicalized
grammar for discourse that facilitates discourse interpretation as a product of compositional rules,
anaphor resolution, and inference.
1. Introduction
It is a truism that a text means more than the sum of its component sentences. One
source of additional meaning are relations taken to hold between adjacent sentences
?syntactically? connected within a larger discourse structure. It has been very difficult,
however, to say what discourse relations there are, either theoretically (Mann and
Thompson 1988; Kehler 2002; Asher and Lascarides 2003) or empirically (Knott 1996).
Knott?s empirical attempt to identify and characterize cue phrases as evidence
for discourse relations illustrates some of the difficulties. Knott used the following
theory-neutral test to identify cue phrases: For a potential cue phrase ? in naturally
occurring text, consider in isolation the clause in which it appears. If the clause ap-
pears incomplete without an adjacent left context, whereas it appears complete if ? is
removed, then ? is a cue phrase. Knott?s test produced a nonexhaustive list of about
two hundred different phrases from 226 pages of text. He then attempted to charac-
terize the discourse relation(s) conveyed by each phrase by identifying when (always,
sometimes, never) one phrase could substitute for another in a way that preserved
meaning. He showed how these substitution patterns could be a consequence of a set
of semantic features and their values. Roughly speaking, one cue phrase could always
substitute for another if it had the same set of features and values, sometimes do so if
it was less specific than the other in terms of its feature values, and never do so if their
values conflicted for one or more features.
? School of Informatics, University of Edinburgh, 2 Buccleuch Place, Edinburgh, EH8 9LW, UK. E-mail:
bonnie@inf.ed.ac.uk.
? Department of Computer Science, Rutgers Universtiy, 110 Frelinghuysen Road, Piscataway, NJ
08854-8019. E-mail: mdstone@cs.rutgers.edu.
? Department of Computer & Information Science, University of Pennsylvania, 200 South 33rd Street,
Philadelphia, PA 19104-6389. E-mail: joshi@linc.cis.upenn.edu.
? Department of Computer Science, University of Otago, P.O. Box 56, DUNEDIN 9015, New Zealand.
E-mail: alik@cs.otago.ac.nz.
546
Computational Linguistics Volume 29, Number 4
By assuming that cue phrases contribute meaning in a uniform way, Knott was
led to a set of surprisingly complex directed acyclic graphs relating cue phrases in
terms of features and their values, each graph loosely corresponding to some family of
discourse relations. But what if the relational meaning conveyed by cue phrases could
in fact interact with discourse meaning in multiple ways? Then Knott?s substitution
patterns among cue phrases may have reflected these complex interactions, as well as
the meanings of individual cue phrases themselves.
This article argues that cue phrases do depend on another mechanism for convey-
ing extrasentential meaning?specifically, anaphora. One early hint that adverbial cue
phrases (called here discourse connectives) might be anaphoric can be found in an
ACL workshop paper in which Janyce Wiebe (1993) used the following example to
question the adequacy of tree structures for discourse:
(1) a. The car was finally coming toward him.
b. He [Chee] finished his diagnostic tests,
c. feeling relief.
d. But then the car started to turn right.
The problem Wiebe noted was that the discourse connectives but and then appear to
link clause (1d) to two different things: then to clause (1b) in a sequence relation (i.e.,
the car?s starting to turn right being the next relevant event after Chee?s finishing his
tests) and but to a grouping of clauses (1a) and (1c) (i.e., reporting a contrast between,
on the one hand, Chee?s attitude toward the car coming toward him and his feeling
of relief and, on the other hand, his seeing the car turning right). (Wiebe doesn?t give
a name to the relation she posits between (1d) and the grouping of (1a) and (1c), but
it appears to be some form of contrast.)
If these relations are taken to be the basis for discourse structure, some possible
discourse structures for this example are given in Figure 1. Such structures might seem
advantageous in allowing the semantics of the example to be computed directly by
compositional rules and defeasible inference. However, both structures are directed
acyclic graphs (DAGs), with acyclicity the only constraint on what nodes can be con-
nected. Viewed syntactically, arbitrary DAGs are completely unconstrained systems.
They substantially complicate interpretive rules for discourse, in order for those rules
to account for the relative scope of unrelated operators and the contribution of syn-
tactic nodes with arbitrarily many parents.1
We are not committed to trees as the limiting case of discourse structure. For
example, we agree, by and large, with the analysis that Bateman (1999) gives of
(2) (vi) The first to do that were the German jewellers, (vii) in particular Klaus
Burie. (viii) And Morris followed very quickly after, (ix) using a lacquetry
technique to make the brooch, (x) and using acrylics, (xi) and exploring
the use of colour, (xii) and colour is another thing that was new at that
time.
1 A reviewer has suggested an alternative analysis of (1) in which clause (1a) is elaborated by clause
(1b), which is in turn elaborated by (1c), and clause (1d) stands in both a sequence relation and a
contrast relation to the segment as a whole. Although this might address Wiebe?s problem, the result is
still a DAG, and such a fix will not address the additional examples we present in section 2, in which a
purely structural account still requires DAGs with crossing arcs.
547
Webber et al Anaphora and Discourse Structure
b
seq
d
contrast
c
cb
elaboration
elaboration
a seq
a d
seq contrast
(i)
(ii)
Figure 1
Possible discourse structure for example (1). Each root and internal node is labeled by the type
of relation that Wiebe takes to hold between the daughters of that node. (i) uses an n-ary
branching sequence relation, whereas in (ii), sequence is binary branching.
(ix)(vi)
succession manner
(viii)
Figure 2
Simple multiparent structure.
in which clause (ix) stands in a manner relation with clause (viii), which in turn stands
in a succession (i.e., sequence) relation with clause (vi). This is illustrated in Figure 2,
which shows a DAG (rather than a tree), but without crossing dependencies.
So it is the cost of moving to arbitrary DAGs for discourse structure that we feel is
too great to be taken lightly. This is what has led us to look for another explanation for
these and other examples of apparent complex and crossing dependencies in discourse.
The position we argue for in this article, is that whereas adjacency and explicit
conjunction (coordinating conjunctions such as and, or, so, and but; subordinating con-
junctions such as although, whereas, and when) imply discourse relations between (the
interpretation of) adjacent or conjoined discourse units, discourse adverbials such as
then, otherwise, nevertheless, and instead are anaphors, signaling a relation between the
interpretation of their matrix clause and an entity in or derived from the discourse
context. This position has four advantages:
1. Understanding discourse adverbials as anaphors recognizes their
behavioral similarity to the pronouns and definite noun phrases (NPs)
that are the bread and butter of previous work on anaphora. This is
discussed in section 2.
2. By understanding and exploring the full range of phenomena for which
an anaphoric account is appropriate, we can better characterize anaphors
and devise more accurate algorithms for resolving them. This is explored
in section 3.
3. Any theory of discourse must still provide an account of how a sequence
of adjacent discourse units (clauses, sentences, and the larger units that
they can comprise) means more than just the sum of its component
548
Computational Linguistics Volume 29, Number 4
units. This is a goal that researchers have been pursuing for some time,
using both compositional rules and defeasible inference to determine
these additional aspects of meaning (Asher and Lascarides 1999; Gardent
1997; Hobbs et al 1993; Kehler 2002; Polanyi and van den Berg 1996;
Scha and Polanyi 1988; Schilder 1997a, 1997b; van den Berg 1996) If that
portion of discourse semantics that can be handled by mechanisms
already needed for resolving other forms of anaphora and deixis is
factored out, there is less need to stretch and possibly distort
compositional rules and defeasible inference to handle everything.2
Moreover, recognizing the possibility of two separate relations (one
derived anaphorically and one associated with adjacency and/or a
structural connective) admits additional richness to discourse semantics.
Both points are discussed further in section 4.
4. Understanding discourse adverbials as anaphors allows us to see more
clearly how a lexicalized approach to the computation of clausal syntax
and semantics extends naturally to the computation of discourse syntax
and semantics, providing a single syntactic and semantic matrix with
which to associate speaker intentions and other aspects of pragmatics
(section 5.)
The account we provide here is meant to be compatible with current approaches
to discourse semantics such as DRT (Kamp and Reyle 1993; van Eijck and Kamp
1997), dynamic semantics (Stokhof and Groenendijk 1999), and even SDRT (Asher
1993; Asher and Lascarides 2003), understood as a representational scheme rather
than an interpretive mechanism. It is also meant to be compatible with more detailed
analyses of the meaning and use of individual discourse adverbials, such as Jayes
and Rossari (1998a, 1998b) and Traugott, (1995, 1997). It provides what we believe to
be a more coherent account of how discourse meaning is computed, rather than an
alternative account of what that meaning is or what speaker intentions it is being used
to achieve.
2. Discourse Adverbials as Anaphors
2.1 Discourse Adverbials Do Not Behave like Structural Connectives
We take the building blocks of the most basic level of discourse structure to be explicit
structural connectives between adjacent discourse units (i.e., coordinating and subor-
dinating conjunctions and ?paired? conjunctions such as not only . . . but also, and on the
one hand . . . on the other (hand) and inferred relations between adjacent discourse units
(in the absense of an explicit structural connective). Here, adjacency is what triggers
the inference. Consider the following example:
(3) You shouldn?t trust John. He never returns what he borrows.
Adjacency leads the hearer to hypothesize that a discourse relation of something like
explanation holds between the two clauses. Placing the subordinate conjunction (struc-
tural connective) because between the two clauses provides more evidence for this rela-
2 There is an analogous situation at the sentence level, where the relationship between syntactic structure
and compositional semantics is simplified by factoring away intersentential anaphoric relations. Here
the factorization is so obvious that one does not even think about any other possibility.
549
Webber et al Anaphora and Discourse Structure
tion. Our goal in this section is to convince the reader that many discourse adverbials,
including then, also, otherwise, nevertheless, and instead, do not behave in this way.
Structural connectives and discourse adverbials do have one thing in common:
Like verbs, they can both be seen as heading a predicate-argument construction; unlike
verbs, their arguments are independent clauses. For example, both the subordinate
conjunction after and the adverbial then (in its temporal sense) can be seen as binary
predicates (e.g., sequence) whose arguments are clausally derived events, with the
earlier event in first position and the succeeding event in second.
But that is the only thing that discourse adverbials and structural connectives have
in common. As we have pointed out in earlier papers (Webber, Knott, and Joshi 2001;
Webber et al, 1999a, 1999b), structural connectives have two relevant properties: (1)
they admit stretching of predicate-argument dependencies; and (2) they do not admit
crossing of those dependencies. This is most obvious in the case of preposed subor-
dinate conjunctions (example (4)) or ?paired? coordinate conjunctions (example (5)).
With such connectives, the initial predicate signals that its two arguments will follow.
(4) Although John is generous, he is hard to find.
(5) On the one hand, Fred likes beans. On the other hand, he?s allergic to them.
Like verbs, structural connectives allow the distance between the predicate and its
arguments to be ?stretched? over embedded material, without loss of the dependency
between them. For the verb like and an object argument apples, such stretching without
loss of dependency is illustrated in example (6b).
(6) a. Apples John likes.
b. Apples Bill thinks he heard Fred say John likes.
That this also happens with structural connectives and their arguments is illustrated
in example (7) (in which the first clause of example (4) is elaborated by another pre-
posed subordinate-main clause construction embedded within it) and in example (8)
(in which the first conjunct of example (5) is elaborated by another paired-conjunction
construction embedded within it). Possible discourse structures for these examples are
given in Figure 3.
(7) a. Although John is very generous?
b. if you need some money,
c. you only have to ask him for it?
d. he?s very hard to find.
(8) a. On the one hand, Fred likes beans.
b. Not only does he eat them for dinner.
c. But he also eats them for breakfast and snacks.
d. On the other hand, he?s allergic to them.
But, as already noted, structural connectives do not admit crossing of predicate-
argument dependencies. If we admit crossing dependencies in examples (7) and (8),
we get
(9) a. Although John is very generous?
b. if you need some money?
550
Computational Linguistics Volume 29, Number 4
(i) (ii)
a
b
d
contrast[one/other]
elaboration
comparison[not only/but also]
a
b
d
elaboration
concession[although]
cc
condition[if]
Figure 3
Discourse structures associated with (i) example (7) and (ii) Example (8).
a cb
concession[although] condition[if]
elaboration
a b
elaboration
contrast[one/other] comparison[not only...]
(i) (ii)
dcd
Figure 4
(Impossible) discourse structures that would have to be associated with (i) Example (9) and
(ii) example (10).
c. he?s very hard to find?
d. you only have to ask him for it.
(10) a. On the one hand, Fred likes beans.
b. Not only does he eat them for dinner.
c. On the other hand, he?s allergic to them.
d. But he also eats them for breakfast and snacks.
Possible discourse structures for these (impossible) discourses are given in Figure 4.
Even if the reader finds no problem with these crossed versions, they clearly do not
mean the same thing as their uncrossed counterparts: In (10), but now appears to link
(10d) with (10c), conveying that despite being allergic to beans, Fred eats them for
breakfast and snacks. And although this might be inferred from (8), it is certainly not
conveyed directly. As a consequence, we stipulate that structural connectives do not
admit crossing of their predicate-argument dependencies.3
That is not all. Since we take the basic level of discourse structure to be a conse-
quence of (1) relations associated with explicit structural connectives and (2) relations
3 A reviewer has asked how much stretching is possible in discourse without losing its thread or having
to rephrase later material in light of the intervening material. One could ask a similar question about
the apparently unbounded dependencies of sentence-level syntax, which inattentive speakers are prone
to lose track of and ?fracture.? Neither question seems answerable on theoretical grounds alone, with
both demanding substantial amounts of empirical data from both written and spoken discourse. The
point we are trying to make is simply that there is a difference in discourse between any amount of
stretching and even the smallest amount of crossing.
551
Webber et al Anaphora and Discourse Structure
whose defeasible inference is triggered by adjacency, we stipulate that discourse struc-
ture itself does not admit crossing structural dependencies. (In this sense, discourse structure
may be truly simpler than sentence structure. To verify this, one might examine the
discourse structure of languages such as Dutch that allow crossing dependencies in
sentence-level syntax. Initial cursory examination does not give any evidence of cross-
ing dependencies in Dutch discourse.)
If we now consider the corresponding properties of discourse adverbials, we see
that they do admit crossing of predicate-argument dependencies, as shown in exam-
ples (11)?(13).
(11) a. John loves Barolo.
b. So he ordered three cases of the ?97.
c. But he had to cancel the order
d. because then he discovered he was broke.
(12) a. High heels are fine for going to the theater.
b. But wear comfortable shoes
c. if instead you plan to go to the zoo.
(13) a. Because Fred is ill
b. you will have to stay home
c. whereas otherwise the two of you could have gone to the zoo.
Consider first the discourse adverbial then in clause (11d). For it to get its first
argument from (11b) (i.e., the event that the discovery in (d) is ?after?), it must cross
the structural connection between clauses (c) and (d) associated with because). This
crossing dependency is illustrated in Figure 5(i). Now consider the discourse adverbial
instead) in clause (12c). For it to get its first argument from (12a) (i.e., going to the zoo is
an alternative to going to the theater), it must cross the structural connection between
clauses (12b) and (12c) associated with if. This crossing dependency is illustrated in
Figure 5(ii). Example (13) is its mirror image: For the discourse adverbial otherwise in
(13c) to get its first argument from (13a) (i.e., alternatives to the state/condition of
Fred being ill), it must cross the structural connection associated with because. This is
illustrated in Figure 5(iii).
Crossing dependencies are not unusual in discourse when one considers anaphora
(e.g., pronouns and definite NPs), as for example in
b
conseq[so]
contrast[but]
dc
seq[then]
b
explanation[because]
conditional[if]
a
contrast[but]
alt[instead]
a
c ba
contrast[whereas]
c
explanation?[because]
alt[otherwise]
(i) (ii) (iii)
Figure 5
Discourse structures for examples (11)?(13). Structural dependencies are indicated by solid
lines and dependencies associate with discourse adverbials are indicated by broken lines.
(explanation? is the inverse of explanation?i.e., with its arguments in reverse order. Such
relations are used to maintain the given linear order of clauses.)
552
Computational Linguistics Volume 29, Number 4
(14) Every mani tells every womanj hei meets that shej reminds himi of hisi
mother.
(15) Suei drives an Alfa Romeo. Shei drives too fast. Maryj races heri on week-
ends. Shej often beats heri. (Strube 1998)
This suggests that in examples (11)?(13), the relationship between the discourse ad-
verbial and its (initial) argument from the previous discourse might usefully be taken
to be anaphoric as well.4
2.2 Discourse Adverbials Do Behave like Anaphors
There is additional evidence to suggest that otherwise, then, and other discourse adver-
bials are anaphors. First, anaphors in the form of definite and demonstrative NPs can
take implicit material as their referents. For example, in
(16) Stack five blocks on top of one another. Now close your eyes and try
knocking {the tower, this tower} over with your nose.
both NPs refer to the structure which is the implicit result of the block stacking.
(Further discussion of such examples can be found in Isard [1975]; Dale [1992]; and
Webber and Baldwin [1992].) The same is true of discourse adverbials. In
(17) Do you want an apple? Otherwise you can have a pear.
the situation in which you can have a pear is one in which you don?t want an apple?
that is, one in which your answer to the question is ?no.? But this answer isn?t there
structurally: It is only inferred. Although it appears natural to resolve an anaphor to an
inferred entity, it would be much more difficult to establish such links through purely
structural connections: To do so would require complex transformations that introduce
invisible elements into discourse syntax with no deeper motivation. For example, in
(17), we would need a rule that takes a discourse unit consisting solely of a yes/no
question P? and replaces it with a complex segment consisting of P? and the clause
it is possible that P, with the two related by something like elaboration. Then and only
then could we account for the interpretation of the subsequent otherwise structurally,
by a syntactic link to the covert material (i.e., to the possibility that P holds, which
otherwise introduces an alterative to).
Second, discourse adverbials have a wider range of options with respect to their
initial argument than do structural connectives (i.e., coordinating and subordinating
conjunctions). The latter are constrained to linking a discourse unit on the right frontier
of the evolving discourse (i.e., the clause, sentence and larger discourse units to its
immediate left). Discourse adverbials are not so constrained. To see this, consider the
following example:
4 We are aware that crossing examples such as (11)?(13) are rare in naturally occurring discourse. We
believe that this is because they are only possible when, as here, strong constraints from the discourse
adverbial and from context prevent the adverbial from relating to the closest (leftmost) eventuality or
an eventuality coerced from that one. But rarity doesn?t necessarily mean ill-formedness or marginality,
as readers can see for themselves if they use Google to search the Web for strings such as because then, if
instead, and whereas otherwise, and consider (1) whether the hundreds, even thousands, of texts in which
these strings occur are ill-formed, and (2) what then, instead, and otherwise are relating in these texts.
One must look at rare events if one is studying complex linguistic phenomena in detail. Thus it is not
the case that only common things in language are real or worth further study.
553
Webber et al Anaphora and Discourse Structure
(18) If the light is red, stop. Otherwise you?ll get a ticket.
(If you do something other than stop, you?ll get a ticket.)
This can be paraphrased using the conjunction or:
If the light is red, stop, or you?ll get a ticket.
Here or links its right argument to a unit on the right frontier of the evolving discourse?
in this case, the clause stop on its immediate left. Now consider the related example
(19) If the light is red, stop. Otherwise go straight on.
(If the light is not red, go straight on.)
This cannot be paraphrased with or, as in
(20) If the light is red, stop, or go straight on.
even though both stop and If the light is red, stop are on the right frontier of the evolving
discourse structure. This is because otherwise is accessing something else, so that (20)
means something quite different from either (18) or (19). What otherwise is accessing,
which or cannot, is the interpretation of the condition alone.5 Thus discourse adver-
bials, like other anaphors, have access to material that is not available to structural
connectives.
Finally, discourse adverbials, like other anaphors, may require semantic represen-
tations in which their arguments are bound variables ranging over discourse entities.
That is, whereas it might be possible to represent Although P, Q using a binary modal
operator
(21) although(p, q)
where formulas p and q translate the sentences P and Q that although combines, we
cannot represent P . . .Nevertheless, Q this way. We need something more like
(22) p ? nevertheless(e, q)
The motivation for the variable e in this representation is that discourse adverbials,
like pronouns, can appear intrasententially in an analog of donkey sentences. Donkey
sentences such as example (23) are a special kind of bound-variable reading:
(23) Every farmer who owns a donkey feeds it rutabagas.
In donkey sentences, anaphors are interpreted as covarying with their antecedents:
The it that is being fed in (23) varies with the farmer who feeds it. These anaphors,
however, appear in a structural and interpretive environment in which a direct syn-
tactic relationship between anaphor and antecedent is normally impossible, so they
cannot be a reflex of true binding in the syntax-semantics interface. Rather, donkey
sentences show that discourse semantics has to provide variables to translate pronouns,
5 This was independently pointed out by several people when this work was presented at ESSLLI?01 in
Helsinki in August 2001. The authors would like to thank Natalia Modjeska, Lauri Karttunen, Mark
Steedman, Robin Cooper, and David Traum for bringing it to their attention.
554
Computational Linguistics Volume 29, Number 4
and that discourse mechanisms must interpret these variables as bound?even though
the pronouns appear ?free? by syntactic criteria.
Thus, it is significant that discourse adverbials can appear in their own version of
donkey sentences, as in
(24) a. Anyone who has developed innovative new software has then had to
hire a lawyer to protect his/her interests. (i.e., after developing innovative
new software)
b. Several people who have developed innovative new software have
nevertheless failed to profit from it. (i.e., despite having developed innovative
new software)
c. Every person selling ?The Big Issue? might otherwise be asking for
spare change. (i.e., if he/she weren?t selling ?The Big Issue?)
The examples in (24) involve binding in the interpretation of discourse adverbials.
In (24a), the temporal use of then locates each hiring event after the corresponding
software development. Likewise in (24b), the adversative use of nevertheless signals
each developer?s unexpected failure to turn the corresponding profit. And in (24c),
otherwise envisions each person?s begging if that person weren?t selling ?The Big Issue?.
Such bound interpretations require variables in the semantic representations and
alternative values for them in some model?hence the representation given in (22).
Indeed, it is clear that the binding here has to be the discourse kind, not the syntac-
tic kind, for the same reason as in (23), although we cannot imagine anyone arguing
otherwise, since discourse adverbials have always been treated as elements of dis-
course interpretation. So the variables must be the discourse variables usually used to
translate other kinds of discourse anaphors.6
These arguments have been directed at the behavioral similarity between discourse
adverbials and what we normally take to be discourse anaphors. But this isn?t the only
reason to recognize discourse adverbials as anaphors: In the next section, we suggest
a framework for anaphora that is broad enough to include discourse adverbials as
well as definite and demonstrative pronouns and NPs, along with other discourse
phenomena that have been argued to be anaphoric, such as VP ellipsis (Hardt 1992,
1999; Kehler 2002), tense (Partee 1984; Webber 1988) and modality (Kibble 1995; Frank
and Kamp 1997; Stone and Hardt 1999).
3. A Framework for Anaphora
Here we show how only a single extension to a general framework for discourse
anaphora is needed to cover discourse adverbials. The general framework is presented
in Section 3.1, and the extension in Section 3.2.
3.1 Discourse Referents and Anaphor Interpretation
The simplest discourse anaphors are coreferential: definite pronouns and definite NPs
that denote one (or more) discourse referents in focus within the current discourse
6 Although rhetorical structure theory (RST) (Mann and Thompson 1998) was developed as an account
of the relation between adjacent units within a text, Marcu?s guide to RST annotation (Marcu 1999) has
added an ?embedded? version of each RST relation in order to handle examples such as (24). Although
this importantly recognizes that material in an embedded clause can bear a semantic relation to its
matrix clause, it does not contribute to understanding the nature of the phenomenon.
555
Webber et al Anaphora and Discourse Structure
context. (Under coreference we include split reference, in which a plural anaphor such
as the companies denotes all the separately mentioned companies in focus within the
discourse context.) Much has been written about the factors affecting what discourse
referents are taken to be in focus. For a recent review by Andrew Kehler, see chap-
ter 18 of Jurafsky and Martin (2000). For the effect of different types of quantifiers on
discourse referents and focus, see Kibble (1995).
Somewhat more complex than coreference is indirect anaphora (Hellman and
Fraurud 1996) (also called partial anaphora [Luperfoy 1992], textual ellipsis [Hahn,
Markert, and Strube 1996], associative anaphora [Cosse 1996] bridging anaphora
[Clark 1975; Clark and Marshall 1981; Not, Tovena, and Zancanaro 1999], and in-
ferrables [Prince 1992]), in which the anaphor (usually a definite NP) denotes a dis-
course referent associated with one (or more) discourse referents in the current discourse
context; for example,
(25) Myra darted to a phone and picked up the receiver.
Here the receiver denotes the receiver associated with (by virtue of being part of) the
already-mentioned phone Myra darted to.
Coreference and indirect anaphora can be uniformly modeled by saying that the
discourse referent e? denoted by an anaphoric expression ? is either equal to or asso-
ciated with an existing discourse referent er, that is, e?=er or e? ?assoc(er). But coref-
erence and associative anaphora do not exhaust the space of constructs that derive
all or part of their sense from the discourse context and are thus anaphoric. Consider
?other NPs? (Bierner 2001a; Bierner and Webber 2000; Modjeska 2001, 2002), as in:
(26) Sue grabbed one phone, as Tom darted to the other phone.
Although ?other NPs? are clearly anaphoric, should the referent of the other phone
(e?)?the phone other than the one Sue grabbed (er)?simply be considered a case of
e? ? assoc(er)? Here are two reasons why they should not.
First, in all cases of associative anaphora discussed in the literature, possible as-
sociations have depended only on the antecedent er and not on the anaphor. For
example, only antecedents that have parts participate in whole-part associations (e.g.,
phone ? receiver). Only antecedents with functional schemata participate in schema-
based associations (e.g., lock ? key). In (26), the relationship between e?, the referent
of the other phone, and its antecedent, er, depends in part on the anaphor, and not just
on the antecedent?in particular, on the presence of the word other.
Second, we also have examples such as
(27) Sue lifted the receiver as Tom darted to the other phone.7
in which the referent of the other phone (e?) is the phone other than the phone associated
with the receiver that Sue lifted. Together, these two points argue for a third possibility,
in which an anaphoric element can convey a specific function f? that is idiosyncratic
to the anaphor, which may be applied to either er or an associate of er. The result of
that application is e?. For want of a better name, we will call these lexically specified
anaphors.
Other lexically specified anaphors include noun phrases headed by other (exam-
ple (28)), NPs with such but no postmodifying as phrase (example (29)), comparative
7 Modjeska (2001) discovered such examples in the British National Corpus.
556
Computational Linguistics Volume 29, Number 4
NPs with no postmodifying than phrase (example (30)), and the pronoun elsewhere
(example (31)) (Bierner 2001b)
(28) Some dogs are constantly on the move. Others lie around until you call
them.
(29) I saw a 2kg lobster in the fish store yesterday. The fishmonger said it takes
about five years to grow to such a size.
(30) Terriers are very nervous. Larger dogs tend to have calmer dispositions.
(31) I don?t like sitting in this room. Can we move elsewhere?
To summarize the situation with anaphors so far, we have coreference when e?=er,
indirect anaphora when e? ?assoc(er), and lexically specified anaphora when e?=f?(ei)
where ei = er or ei ?assoc(er).
3.2 Discourse Adverbials as Lexical Anaphors
There is nothing in this generalized approach to discourse anaphora that requires that
the source of er be an NP, or that the anaphor be a pronoun or NP. For example, the
antecedent er of a singular demonstrative pronoun (in English, this or that) is often
an eventuality that derives from a clause, a sentence, or a larger unit in the recent
discourse (Asher 1993; Byron 2002; Eckert and Strube 2000; Webber 1991). We will
show that this is the case with discourse adverbials as well.
The extension we make to the general framework presented above in order to
include discourse adverbials as discourse anaphors is to allow more general functions
f? to be associated with lexically specified anaphors. In particular, for the discourse
adverbials considered in this article, the function associated with an adverbial maps its
anaphoric argument?an eventuality derived from the current discourse context?to
a function that applies to the interpretation of the adverbial?s matrix clause (itself an
eventuality). The result is a binary relation that holds between the two eventualities
and is added to the discourse context. For example, in
(32) John loves Barolo. So he ordered three cases of the ?97. But he had to
cancel the order because he then discovered he was broke.
then, roughly speaking, contributes the fact that its matrix clause event (John?s find-
ing he was broke) is after the anaphorically derived event of his ordering the wine.8
Similarly, in
(33) John didn?t have enough money to buy a mango. Instead, he bought a
guava.
instead contributes the fact that its matrix clause event (buying a guava) is an alternative
to the anaphorically derived event of buying a mango. The relation between the two
sentences is something like result, as in So instead, he bought a guava.
8 Words and phrases that function as discourse adverbials usually have other roles as well: For example,
otherwise also serves as an adjectival modifier, as in I was otherwise occupied with grading exams. This
overloading of closed-class lexico-syntactic items is not unusual in English, and any ambiguities that
arise must be handled as part of the normal ambiguity resolution process.
557
Webber et al Anaphora and Discourse Structure
Note that our only concern here is with the compositional and anaphoric mech-
anisms by which adverbials contribute meaning. For detailed analysis of the lexical
semantics of adverbials (but no attention to mechanism), the reader is referred to Jayes
and Rossari (1998a, 1998b, Lagerwerf (1998), Traugott (1995, 1997), and others.
Formally, we represent the function that a discourse adverbial ? contributes as a
?-expression involving a binary relation R? that is idiosyncratic to ?, one of whose
arguments (represented here by the variable EV) is resolved anaphorically:
?x . R?(x, EV)
R? gets its other argument compositionally, when this ?-expression is applied to ??s
matrix clause S interpreted as an eventuality ?, that is,
[?x . R?(x, EV)]? ? R?(?, EV)
The result of both function application and resolving EV to some eventuality ei derived
from the discourse context either directly or by association is the proposition R?(?, ei),
one of whose arguments (ei) has been supplied by the discourse context and the other
(?) compositionally from syntax.
Note that this is a formal model, meant to have no implications for how pro-
cessing takes place. We have not tried at this stage to instantiate our view of how
discourse adverbials are resolved in the context of (simultaneous) sentence-level and
discourse-level processing. Our basic view is that resolution is initiated when the dis-
course adverbial (?) is encountered. As ??s matrix clause S is incrementally parsed and
interpreted, producing eventuality ?, the resolution process polls the discourse context
and either finds an appropriate eventuality ei (or creates one by a bridging inference,
as illustrated in the next section) such that R?(?, ei) makes sense with respect to the
discourse so far. As is the case with resolving a discourse deictic (Asher 1993; Byron
2002; Eckert and Strube 2000; Webber 1991) this resolution process would use syn-
tactic and semantic constraints that it accumulates as the incremental sentence-level
parser/interpreter processes S. As with discourse deixis, this is best seen as a con-
straint satisfaction problem that involves finding or deriving an eventuality from the
current discourse context that meets the constraints of the adverbial with respect to the
eventuality interpretation of the matrix clause. (Examples of this are given throughout
the rest of the article.)
3.3 A Logical Form for Eventualities
Before using this generalized view of anaphora to show what discourse adverbials
contribute to discourse and how they interact with discourse relations that arise from
adjacency or explicit discourse connectives, we briefly describe how we represent
clausal interpretations in logical form (LF).
Essentially, we follow Hobbs (1985) in using a rich ontology and a representation
scheme that makes explicit all the individuals and abstract objects (i.e., propositions,
facts/beliefs, and eventualities) (Asher 1993) involved in the LF interpretation of an
utterance. We do so because we want to make intuitions about individuals, eventual-
ities, lexical meaning, and anaphora as clear as possible. But certainly, other forms of
representation are possible.
In this LF representation scheme, each clause and each relation between clauses
is indexed by the label of its associated abstract object. So, for example, the LF inter-
pretation of the sentence
(34) John left because Mary left.
558
Computational Linguistics Volume 29, Number 4
would be written
e1:left(j) ? john(j) ? e2:left(m) ? mary(m) ? e3:because(e1,e2)
where the first argument of the asymmetric binary predicate because is the consequent
and the second is the eventuality leading to this consequent. Thus when because occurs
sentence-medially, as in the above example, the eventuality arguments are in the same
order as their corresponding clauses occur in the text. When because occurs sentence-
initially (as in Because Mary left, John did), the interpretation of the second clause (John
[left]) will appear as the first argument and the interpretation of the first clause (Mary
left) will appear as the second.9
The set of available discourse referents includes both individuals like j and m, and
also abstract objects like e1 and e2. We then represent resolved anaphors by reusing
these discourse referents. So, for example, the LF interpretation of the follow-on sen-
tence
(35) This upset Sue.
would be written
e4:upset(DPRO, s) ? sue(s)
where DPRO is the anaphoric variable contributed by the demonstrative pronoun this.
Since the subject of upset could be either the eventuality of John?s leaving or the fact
that he left because Mary left, DPRO could be resolved to either e1 or e3, that is,
a. e4:upset(e1, s) ? sue(s)
b. e4:upset(e3, s) ? sue(s)
depending on whether one took Sue to have been upset by (1) John?s leaving or (2)
that he left because Mary left.
3.4 The Contribution of Discourse Adverbials to Discourse Semantics
Here we step through some examples of discourse adverbials and demonstrate how
they make their semantic contribution to the discourse context. We start with exam-
ple (32), repeated here as (36):
(36) a. John loves Barolo.
b. So he ordered three cases of the ?97.
c. But he had to cancel the order
d. because he then discovered he was broke.
9 We are not claiming to give a detailed semantics of discourse connectives except insofar as they may
affect how discourse adverbials are resolved. Thus, for example, we are not bothering to distinguish
among different senses of because (epistemic vs. nonepistemic), while (temporal vs. concessive), since
(temporal vs. causal), etc. Of course, these distinctions are important to discourse interpretation, but
they are independent of and orthogonal to the points made in this article. Similarly, Asher (1993)
argues that a simple ontology of eventualities is too coarse-grained, and that discourse representations
need to distinguish different kinds of abstract objects, including actions, propositions, and facts as well
as eventualities. Different discourse connectives will require different kinds of abstract objects as
arguments. This distinction is also orthogonal to the points made in this article, because we can
understand these abstract referents to be associates of the corresponding Hobbsian eventualities and
leave the appropriate choice to the lexical semantics of discourse connectives. Byron (2002) advocates a
similar approach to resolving discourse anaphora.
559
Webber et al Anaphora and Discourse Structure
Using the above LF representation scheme and our notation from Section 3.2, namely,
? ? = the anaphoric expression (here, the discourse adverbial)
? R? = the relation name linked with ?
? S = the matrix clause/sentence containing ?
? ? = the interpretation of S as an abstract object
and ignoring, for now, the conjunction because (discussed in section 4), the relevant
elements of (36d) can be represented as:
? = then
R? = after
S = he [John] discovered he was broke
? = e4:find(j,e5), where e5:broke(j)
This means that the unresolved interpretation of (36d) is
[?x . R?(x,EV)]? ? [?x . after(x,EV)]e4 ? after(e4, EV)
The anaphoric argument EV is resolved to the eventuality e2, derived from (36b)?
e2:order(j, c1).
after(e4,EV) ? after(e4,e2)
That is, the eventuality of John?s finding he was broke is after that of John?s ordering
three cases of the ?97 Barolo. The resulting proposition after(e4,e2) would be given its
own index, e6, and added to the discourse context.
When then is understood temporally, as it is above, as opposed to logically, it
requires a culminated eventuality from the discourse context as its first argument
(which Vendler (1967) calls an achievement or an accomplishment). The ordering
event in (36b) is such a Vendlerian accomplishment. In example (37), though, there
is no culminated eventuality in the discourse context for then), to take as its first
argument.
(37) a. Go west on Lancaster Avenue.
b. Then turn right on County Line.
How does (37b) get its interpretation?
As with (36d), the relevant elements of (37b) can be represented as
? = then
R? = after
S = turn right on County Line
? = e3:turn-right(you, county line)
and the unresolved interpretation of (37b) is thus
[? x . after(x, EV)]e3 ? after(e3, EV)
560
Computational Linguistics Volume 29, Number 4
As for resolving EV, in a well-known article, Moens and Steedman (1988) discuss
several ways in which an eventuality of one type (e.g., a process) can be coerced into
an eventuality of another type (e.g., an accomplishment, which Moens and Steedman
call a culminated process). In this case, the matrix argument of then (the eventuality of
turning right on County Line) can be used to coerce the process eventuality in (37b) into
a culminated process of going west on Lancaster Avenue until County Line. We treat this
coercion as a type of associative or bridging inference, as in the examples discussed
in section 3.1. That is,
e2 = culmination(e1)?assoc(e1), where e1:go-west(you, lancaster ave)
Taking this e2 as the anaphoric argument EV of then yields the proposition
after(e3, e2)
That is, the eventuality of turning right onto County Line is after that of going west
on Lancaster Avenue to County Line. This proposition would be indexed and added
to the discourse context.
It is important to stress here that the level of representation we are concerned
with is essentially an LF for discourse. Any reasoning that might then have to be done
on the content of LFs might then require making explicit the different modal and
temporal contexts involved, their accessibility relations, the status of abstract objects
as facts, propositions or eventualities, and so on. But as our goal here is primarily
to capture the mechanism by means of which discourse adverbials are involved in
discourse structure and discourse semantics, we will continue to assume for as long
as possible that an LF representation will suffice.
Now it may appear as if there is no difference between treating adverbials as
anaphors and treating them as structural connectives, especially in cases like (37) in
which the antecedent comes from the immediately left-adjacent context, and in which
the only obvious semantic relation between the adjacent sentences appears to be the
one expressed by the discourse adverbial. (Of course, there may also be a separate
intentional relation between the two sentences [Moore and Pollack 1992], independent
of the relation conveyed by the discourse adverbial.)
One must distinguish, however, between whether a theory allows a distinction
to be made and whether that distinction needs to be made in a particular case. It
is clear that there are many examples in which the two approaches (i.e., a purely
structural treatment of all connectives, versus one that treats adverbials as linking into
the discourse context anaphorically) appear to make the same prediction. We have
already, however, demonstrated cases in which a purely structural account makes the
wrong prediction, and in the next section, we will demonstrate the additional power
of an account that allows for two relations between an adverbial?s matrix clause or
sentence and the previous discourse: one arising from the anaphoric connection and
the other inferred from adjacency or conveyed explicitly by a structural connective.
Before closing this section, we want to step through examples (19)?(20), repeated
here as examples (38)?(39).
(38) If the light is red, stop. Otherwise you?ll get a ticket.
(39) If the light is red, stop. Otherwise go straight on.
561
Webber et al Anaphora and Discourse Structure
Roughly speaking, otherwise conveys that the complement of its anaphorically derived
argument serves as the condition under which the interpretation of its structural ar-
gument holds. (This complement must be with respect to some contextually relevant
set.)10
If we represent a conditional relation between two eventualities with the asym-
metric relation if(e1,e2), where e1 is derived from the antecedent and e2 from the conse-
quent, and we approximate a single contextually relevant alternative e2 to an eventu-
ality e1 using a symmetric complement relation, complement(e1, e2), then we can represent
the interpretation of otherwise as
? x . if(VE, x), where complement(VE, EV)
where variable EV is resolved anaphorically to an eventuality in the current discourse
context that admits a complement. That is, otherwise requires a contextually relevant
complement to its antecedent and asserts that if that complement holds, the argument
to the ?-expression will as well. The resulting ?-expression applies to the interpretation
of the matrix clause of otherwise, resulting in the conditional?s being added to the
discourse context:
[?x . if(VE,x)] ? ? if(VE,?), where complement(VE,EV)
Here the relevant elements of (38b) and (39b) can be represented as
? = otherwise
R? = if
S38 = you get a ticket
?38 = e3, where e3:get ticket(you)
S39 = go straight on
?39 = e3? , where e3? :go straight(you)
The unresolved interpretations of (38b) and (39b) are thus:
[?x . if(VE38,x)] e3 ? if(VE38,e3), where complement(VE38,EV38)
[?x . if(VE39,x)] e3? ? if(VE39,e3? ), where complement(VE39,EV39)
As we showed in section 2.2, different ways of resolving the anaphoric argument lead
to different interpretations. In (38), the anaphoric argument is resolved to e2:stop(you),
10 Kruijff-Korbayova? and Webber (2001a) demonstrate that the information structure of sentences in the
previous discourse (theme-rheme partitioning, as well as focus within theme and within rheme
[Steedman 2000a]) can influence what eventualities er are available for resolving the anaphorically
derived argument of otherwise. This then correctly predicts different interpretations for ?otherwise? in
(i) and (ii):
(i) Q. How should I transport the dog?
A. You should carry the dog. Otherwise you might get hurt.
(ii) Q. What should I carry?
A. You should carry the dog. Otherwise you might get hurt.
In both (i) and (ii), the questions constrain the theme/rheme partition of the answer. Small capitals
represent focus within the rheme. In (i), the otherwise clause will be interpreted as warning the hearer
(H) that H might get hurt if he/she transports the dog in some way other than carrying it (e.g., H might
get tangled up in its lead). In (ii), the otherwise clause warns H that he/she might get hurt if what she
is carrying is not the dog (e.g., H might be walking past fanatical members of the Royal Kennel Club).
562
Computational Linguistics Volume 29, Number 4
whereas in (39), it is resolved to e1:red(light1). Thus the resulting interpretations of
(38b) and (39b) are, respectively,
if(e4,e3), where complement(e2,e4) and e2:stop(you)
(If you do something other than stop, you?ll get a ticket.)
if(e4? , e3? ), where complement(e1,e4? ) and e1:red(light)
(If the light is not red, go straight on.)
We have not been specific about how the anaphoric argument of otherwise (or
of any other discourse adverbial) is resolved, other than having it treated as a con-
straint satisfaction problem. This is the subject of current and future work, exploring
the empirical properties of resolution algorithms with data drawn from appropriately
annotated corpora and from psycholinguistic studies of human discourse interpreta-
tion. To this end, Creswell et al (2002) report on a preliminary annotation study of
discourse adverbials and the location and type of their antecedents. This initial ef-
fort involves nine discourse adverbials?three each from the classes of concessive,
result, and reinforcing (additive) conjuncts given in Quirk et al (1972). Meanwhile,
Venditti et al (2002) present a preliminary report on the use of a constraint satisfac-
tion model of interpretation, crucially combining anaphoric and structural reasoning
about discourse relations, to predict subjects? on-line interpretation of discourses in-
volving stressed pronouns. In addition, two proposals have recently been submitted to
construct a larger and more extensively annotated corpus, covering more adverbials,
based on what we have learned from this initial effort. This more extensive study
would be an adequate basis for developing resolution algorithms.11
3.5 Summary
In this section, we have presented a general framework for anaphora with the follow-
ing features:
? Anaphors can access one or more discourse referents or entities
associated with them through bridging inferences. These are sufficient
for interpreting anaphoric pronouns, definite NPs and demonstrative
NPs, allowing entities to be evoked by NPs or by clauses. In the case of
clauses, this may be on an as-needed basis, as in Eckert and Strube
(2000).
? A type of anaphor ? that we call lexically specified can also contribute
additional meaning through a function f? that is idiosyncratic to ?, that
can be applied to either an existing discourse referent or an entity
associated with it through a bridging inference. In the case of the
premodifier other, f? applied to its argument produces contextually
11 With respect to how many discourse adverbials there are, Quirk et al (1972) discuss 60 conjunctions
and discourse adverbials under the overall heading time relations and 123 under the overall heading
conjuncts. Some entries appear under several headings, so that the total number of conjunctions and
discourse adverbials they present is closer to 160. In another enumeration of discourse adverbials,
Forbes and Webber (2002) start with all annotations of sentence-level adverbials in the Penn Treebank,
then filter them systematically to determine which draw part of their meaning from the preceding
discourse and how they do so. What we understand from both of these studies is that there are fewer
than 200 adverbials to be considered, many of which are minor variations of one another (in contrast, by
contrast, by way of contrast, in comparison, by comparison, by way of comparison that are unlikely to differ in
their anaphoric properties, and some of which, such as contrariwise, hitherto, and to cap it all, will occur
only rarely in a corpus of modern English.
563
Webber et al Anaphora and Discourse Structure
relevant alternatives to that argument. In the case of the premodifier
such, it yields a set of entities that are similar to its argument in a
contextually relevant way.
? Discourse adverbials are lexically specified anaphors whose meaning
function f? is a ?-expression involving a binary relation R? that is
idiosyncratic to ?, one of whose arguments is resolved anaphorically and
the other is provided compositionally, when the ?-expression is applied
to ??s matrix clause interpreted as an eventuality ?.
In the next section, we move on to consider how the presence of both a semantic rela-
tion associated with a discourse adverbial and a semantic relation associated with the
adjacency of two clauses or a structural connective between them allows for interesting
interactions between the two.
4. Patterns of Anaphoric Relations and Structural/Inferred Relations
Prior to the current work, researchers have treated both explicit structural connec-
tives (coordinating and subordinating conjunctions, and ?paired? conjunctions) and
discourse adverbials simply as evidence for a particular structural relation holding
between adjacent units. For example, Kehler (2002) takes but as evidence of a contrast
relation between adjacent units, in general as evidence of a generalization relation,
in other words as evidence of an elaboration relation, therefore as evidence of a result
relation, because as evidence of an explanation relation, and even though as evidence
of a denial of preventer relation (Kehler 2002, Section 2.1). Here Kehler has probably
correctly identified the type of relation that holds between elements, but not which
elements it holds between.
In one respect, we follow previous researchers, in that we accept that when clauses,
sentences, or larger discourse units are placed adjacent to one another, listeners infer a
relation between the two, and that the structural connective (coordinate or subordinate
conjunction) gives evidence for the relation that is intended to hold between them.
Because we take discourse adverbials to contribute meaning through an anaphoric
connection with the previous discourse, however, this means that there may be two
relations on offer and opens up the possibility that the relation contributed by the
discourse adverbial can interact in more than one way with the relation conveyed
by a structural connective or inferred through adjacency. Below we show that this
prediction is correct.
We start from the idea that, in the absence of an explicit structural connective, de-
feasible inference correlates with structural attachment of adjacent discourse segments
in discourse structure, relating their interpretations. The most basic relation is that the
following segment in some way describes the same object or eventuality as the one it
abuts (elaboration). But evidence in the segments can lead (via defeasible inference) to
a more specific relation, such as one of the resemblance relations (e.g., parallel, contrast,
exemplification, generalisation), or cause-effect relations (result, explanation, violated expecta-
tion), or contiguity relations (narration) described in Hobbs (1990) and Kehler (2002). If
nothing more specific can be inferred, the relation will remain simply elaboration. What
explicit structural connectives can do is convey relations that are not easy to convey
by defeasible inference (e.g., if, conveying condition, and or, conveying disjunction) or
provide nondefeasible evidence for an inferrable relation (e.g., yet, so, and because).
Discourse adverbials can interact with structural connectives, with adjacency-
triggered defeasible inference, and with each other. To describe the ways in which we
564
Computational Linguistics Volume 29, Number 4
have so far observed discourse adverbials to interact with relations conveyed struc-
turally, we extend the notation used in the previous section:
? ? = discourse adverbial
? R? = the name of the relation associated with ?
? S = the matrix clause/sentence of ?
? ? = the logical form (LF) interpretation of S
adding the following:
? D = the discourse unit that is left-adjacent to S, to which a relationship
holds by either inference or a structural connective
? ? = the LF interpretation of D
? R = the name of the relation that holds with ?
Although ? is one argument of R, we show below that the other argument of R may
be one of at least two different abstract objects.
Case 1: ? separately serves as an argument to both R? and R. This is the case that
holds in example (36) (repeated here):
(36) a. John loves Barolo.
b. So he ordered three cases of the ?97.
c. But he had to cancel the order
d. because he then discovered he was broke.
We have already seen that the interpretation of the clause in (36d) following because
involves
R? = after
? = e4:discover(j,e5), where e5:broke(j)
[?x . after(x,EV)]e4 ? after(e4, EV)
where EV is resolved to e2:order(j, c1), and the proposition after(e4, e2) is added to the
discourse context?that is, John?s discovering he was broke is after his ordering the
wine.
Now consider the explanation relation R associated with because in (36d). It relates
e4, John?s finding he was broke, to the intepretation of (36c), e3:cancel(j,o1)?that is,
explanation(e4,e3). Clause 36d thus adds both explanation(e4,e3) and after(e4, e2) to the
discourse. Although these two propositions share an argument (e4), they are neverthe-
less distinct.12
12 Because eventuality e4, John?s finding he was broke, both explains the canceling and follows the ordering, it
follows that the canceling is after the ordering.
565
Webber et al Anaphora and Discourse Structure
Case 2: R?(?, ei) is an argument of R. In case 1, it is the interpretation of the ad-
verbial?s matrix clause ? that serves as one argument to the discourse relation R. In
contrast, in case 2, that argument is filled by the relation contributed by the discourse
adverbial (itself an abstract object available for subsequent reference). In both cases,
the other argument to R is ?.
One configuration in which case 2 holds is with the discourse adverbial otherwise.
Recall from section 3.4 that the interpretation of otherwise involves a conditional relation
between the complement of its anaphoric argument and the interpretation ? of its
matrix clause:
[?x . if(VE,x)] ? ? if(VE,?), where complement(VE,EV)
With variable EV resolved to an eventuality in the discourse context, it is the resulting
relation (viewed as an abstract object) that serves as one argument to R, with ? serving
as the other. We can see this most clearly by considering variants of examples (38) and
(39) that contain an explicit connective between the clauses. In (38), the conjunction
because is made explicit (example (40)), and in (39), the connective is simply and or but
(example (41)).
(40) If the light is red, stop, because otherwise you?ll get a ticket.
R? = if
?38 = e3:get ticket(you)
(41) If the light is red, stop, and/but otherwise go straight on.
R? = if
?39 = e3? :go straight(you)
In the case of (40), resolving otherwise contributes the relation
e6: if(e4,e3), where complement(e4,e2) and e2:stop(you)
(If you do something other than stop, you?ll get a ticket.)
At the level of LF, the abstract object e6 that is associated with the conditional relation
serves as one argument to the explanation relation contributed by because, with e2 being
the other. That is, because and otherwise together end up contributing explanation(e2,e6)
(i.e., your needing to stop is explained by the fact that if you do something other than
stop, you?ll get a ticket).
In the case of (41), resolving otherwise contributes the relation
e6? :if(e4? , e3? ), where complement(e4? ,e1) and e1:red(light)
(If the light is not red, go straight on.)
What is the discourse relation to which otherwise contributes this abstract object e6??
Whether the connective is and or but, both its conjuncts describe (elaborate) alternative
specializations of the same situation e0 introduced earlier in the discourse (e.g., e0 could
be associated with the first sentence of Go another mile and you?ll get to a bridge. If the
light is red, stop. Otherwise go straight on.) If the connective is and, what is added to
context might simply be elaboration(e6? ,e0). (Note that without otherwise, the relation
elaboration(e5,e0) would have been added to context, where e5 is the abstract object
associated with the interpretation of If the light is red, stop.) If the connective is but, then
one might also possibly add contrast(e6? ,e5)?that is, the situation that (if the light is
566
Computational Linguistics Volume 29, Number 4
red) you should stop is in contrast to the situation that if the light is not red, you
should go straight on.13
As is clear from the original pair of examples (38) and (39), interpretations can
arise through adjacency-triggered inference that are similar to those that arise with an
explicit connective. In either case, the above treatment demonstrates that there is no
need for a separate otherwise relation, as proposed in rhetorical structure theory (Mann
and Thompson 1988). We are not, however, entirely clear at this point when case 1
holds and when case 2 does. A more careful analysis is clearly required.
Case 3: R? is parasitic on R. Case 3 appears to apply with discourse adverbials such
as for example and for instance. The interpretation of such adverbials appears to be
parasitic on the relation associated with a structural connective or discourse adverbial
to their left, or on an inferred relation triggered by adjacency. The way to understand
this is to first consider intraclausal for example, where it follows the verb, as in
(42) Q. What does this box contain?
A. It contains, for example, some hematite.
The interpretation of for example here involves abstracting the meaning of its matrix
structure with respect to the material to its right, then making an assertion with respect
to this abstraction. That is, if the LF contributed by the matrix clause of (42A) is,
roughly,
i. contain(box1,hematite1)
then the LF resulting from the addition of for example can be written either with set
notation (as in (ii)), taking an entity to exemplify a set, or with ?-notation (as in (iii)),
taking an entity to exemplify a property:
ii. exemplify(hematite1, {X | contain(box1,X)})
iii. exemplify(hematite1, ?X . contain(box1,X))
Both express the fact that hematite is an example of what is contained in the box.14 Since
one can derive (i) logically from either (ii) or (iii), one might choose to retain only (ii) or
(iii) and derive (i) if and when it is needed. In the remainder of the article, we use the
? notation given in (iii). Note that from the perspective of compositional semantics, for
example resembles a quantifier, in that the scope of its interpretation is not isomorphic
to its syntactic position. Thus producing an interpretation for for example will require
techniques similar to those that have long been used in interpreting quantifiers (Woods,
1978; Barwise and Cooper 1981). We take this up again in section 5.
If we look at the comparable situation in discourse, such as (43)?(44), where for
example occurs to the right of a discourse connective, it can also be seen as abstracting
13 A much finer-grained treatment of the semantics of otherwise in terms of context-update potential is
given in Kruijff-Korbayova? and Webber (2001b). Here we are just concerned with its interaction with
structural connectives and adjacency-triggered relations.
14 The material to the right of for example can be any kind of constituent, including such strange ones as
John gave, for example, a flower to a nurse.
Here, a flower to a nurse would be an example of the set of object-recipient pairs within John?s givings.
Such nonstandard constituents are also found with coordination, which was one motivation for
combinatory categorial grammar (Steedman 1996). This just illustrates another case in which such
nonstandard constituents are needed.
567
Webber et al Anaphora and Discourse Structure
the interpretation of its discourse-level matrix structure, with respect to the material
to its right:
(43) John just broke his arm. So, for example, he can?t cycle to work now.
(44) You shouldn?t trust John because, for example, he never returns what he
borrows.
In (43), the connective so leads to
result(?,?)
being added to the discourse, where ? is the interpretation of John can?t cycle to work
now, and ? is the interpretation of John just broke his arm. For example then abstracts this
relation with respect to the material to its right (i.e., ?), thereby contributing
exemplify(?, ?X . result(X, ?))
That is, John can?t cycle to work is an example of what results from John?s breaking his
arm. Similarly, because in (44) leads to
explanation(?,?)
being added to the discourse, where ? is the interpretation of he never returns what he
borrows, ? is the interpretation of you shouldn?t trust John, and for example adds
exemplify(?, ?X . explanation(X,?))
that is, that ? is an example of the reasons for not trusting John.
For example interacts with discourse adverbials in the same way:
(45) Shall we go to the Lincoln Memorial? Then, for example, we can go to the
White House.
(46) As a money manager and a grass-roots environmentalist, I was very dis-
appointed to read in the premiere issue of Garbage that The Wall Street
Journal uses 220,000 metric tons of newsprint each year, but that only 1.4%
of it comes from recycled paper. By contrast, the Los Angeles Times, for ex-
ample, uses 83% recycled paper. [WSJ, from Penn Treebank /02/wsj-0269]
In example (45), the resolved discourse adverbial then leads to after(?,?) being added
to the discourse context, where ? is the interpretation of we can go to the White House, ?
is the interpretation of we shall go to the Lincoln Memorial, and for example adds
exemplify(?, ?X . after(X,?))
that is, that ? is an example of the events that [can] follow going to the Lincoln
Memorial. (As already noted, we are being fairly fast and loose regarding tense and
modality, in the interests of focusing on the types of interactions.)
568
Computational Linguistics Volume 29, Number 4
In example (46), the resolved discourse anaphor by contrast contributes contrast(?,?),
where ? is the interpretation of the Los Angeles Times?s using 83% recycled paper and ? is
the intepretation of only 1.4% of it [newsprint used by the WSJ] comes from recycled paper.
For example then contributes
exemplify(?, ?X . contrast(X,?))
that is, that ? is one example of contrasts with the WSJ?s minimal use of recycled
paper.
What occurs with discourse connectives and adverbials can also occur with rela-
tions added through adjacency-triggered defeasible inference, as in
(47) You shouldn?t trust John. For example, he never returns what he borrows.
explanation(?,?)
exemplify(?, ?X . explanation(?,X))
Here, as in (44), the relation provided by adjacency-triggered inference is R = explana-
tion, which is then used by for example.
But what about the many cases in which only exemplify seems present, as in
(48) In some respects they [hypertext books] are clearly superior to normal
books, for example they have database cross-referencing facilities ordinary
volumes lack. (British National Corpus, CBX 1087)
(49) He [James Bellows] and his successor, Mary Anne Dolan, restored respect
for the editorial product, and though in recent years the paper had been
limping along on limited resources, its accomplishments were notable. For
example, the Herald consistently beat its much-larger rival on disclosures
about Los Angeles Mayor Tom Bradley?s financial dealings.
There are at least two explanations: One is that for example simply provides direct
nondefeasible evidence for exemplify, which is the only relation that holds. The other
explanation follows the same pattern as the examples given above, but with no further
relation than elaboration(?,?). That is, we understand in (48) that having database cross-
referencing facilities elaborates the respects in which hypertext books are superior to
normal books, whereas in (49), we understand that the Herald?s [newspaper] consistently
beating its much-larger rival elaborates the claim that its accomplishments were notable. This
elaboration relation is then abstracted (in response to for example) to produce:
exemplify(?, ?X . elaboration(X, ?))
that is, that this is one example of many possible elaborations. Because this is more
specific than elaboration and seems to mean the same as exemplify(?,?), one might
simply take it to be the only relation that holds. Given that so many naturally occuring
instances of for example occur with elaboration, it is probably useful to persist with the
above shorthand. But it shouldn?t obscure the regular pattern that appears to hold.
Before going on to case 4, we should comment on an ambiguity associated with for
example. When for example occurs after an NP, a PP, or a clause that can be interpreted
as a general concept or a set, it can contribute a relation between the general concept
or set and an instance, rather than being parasitic on another relation. For example,
in:
569
Webber et al Anaphora and Discourse Structure
(50) In the case of the managed funds they will be denominated in a leading
currency, for example US dollar, . . . (BNC CBX 1590)
for example relates the general concept denoted by a leading currency to a specific in-
stance, U.S. dollars. (In British English, the BNC shows that most such examples occur
with such as?i.e., in the construction such as for example. This paraphrase does not work
with the predicate-abstracting for example that is of primary concern here, as in exam-
ple (42).)
But for example occurring after an NP, a PP, or a clause can, alternatively, contribute
a more subtle parasitic relationship to the previous clause, as in
(51) All the children are ill, so Andrew, for example, can?t help out in the shop.
This differs from both (43) and (50). That is, one cannot paraphrase (51) as (52) as in
(43) where for example follows so:
(52) All the children are ill, so for example Andrew can?t help out in the shop.
Example (52) simply specifies an example consequence of all the children being ill, as
does
(53) All the children are ill, so for example one of us has to be at home at all
times.
In contrast, (51) specifies an example consequence for Andrew, as one of the children.
Support for this comes from the fact that in (52), Andrew doesn?t have to be one of
the children: He could be their nanny or child minder, now stuck with dealing with a
lot of sick kids. But (51) is not felicitous if Andrew is not one of the children.
We suspect here the involvement of information structure (Steedman 2000a):
Whereas the interpretation conveyed by for example is parasitic on the adjacency rela-
tion (result in example (51)), its position after the NP Andrew in (51) may indicate a
contrastive theme with respect to the previous clause, according to which Andrew in
contrast to the other children suffers this particular consequence. But more work needs
to be done on this to gain a full understanding of what is going on.
Case 4: R? is a defeasible rule that incorporates R. Case 4 occurs with discourse
adverbials that carry the same presupposition as the discourse connectives although
and the concessive sense of while (Lagerwerf 1998). Case 4 shares one feature with
case 1, in that the discourse relation R conveyed by a structural connective or inferred
from adjacency holds between ? (the interpretation of the adverbial?s matrix clause)
and ? (the interpretation of the left-adjacent discourse unit). Where it differs is that
the result is then incorporated into the presupposition of the discourse adverbial. This
presupposition, according to Lagerwerf (1998), has the nature of a presupposed (or
conventionally implicated) defeasible rule that fails to hold in the current situation.
He gives as an example
(54) Although Greta Garbo was called the yardstick of beauty, she never mar-
ried.
This asserts both that Greta Garbo was called the yardstick of beauty and that she
never married. The first implies that Greta Garbo was beautiful. The example also
570
Computational Linguistics Volume 29, Number 4
presupposes that, in general, if a woman is beautiful, she will marry. If such a pre-
supposition can be accommodated, it will simply be added to the discourse context.
If not, the hearer will find the utterance confusing or possibly even insulting.
We argue here that the same thing happens with the discourse adverbials never-
theless and though. The difference is that, with discourse adverbials, the antecedent to
the rule derives anaphorically from the previous discourse, whereas the consequent
derives from the adverbial?s matrix clause. (With the conjunctions although and con-
cessive while, both arguments are provided structurally.)
We first illustrate case 4 with two examples in which nevertheless occurs in the
main clause of a sentence containing a preposed subordinate clause. The subordinate
conjunction helps clarify the relation between the clauses that forms the basis for the
presupposed defeasible rule. After these, we give a further example in which the
relation between the adjacent clauses comes through inference.
(55) While John is discussing politics, he is nevertheless thinking about his fish.
In (55), the conjunction while conveys a temporal relation R between the two clauses
it connects:
during(e2, e1), where e1:discuss(john,politics) and e2:think about(john,fish)
What nevertheless contributes to (55) is a defeasible rule based on this relation, which
we will write informally as
during(X,E) ? E:discuss(Y,politics)) > ?X:think about(Y,fish))
Normally, whatever one does during the time one is discussing politics, it is
not thinking about one?s fish.
This rule uses Asher and Morreau?s (1991) defeasible implication operator (>) and
abstracts over the individual (John), which seems appropriate for the general statement
conveyed by the present tense of the utterance.
Similarly, in
(56) Even after John has had three glasses of wine, he is nevertheless able to
solve difficult math problems.
the conjunction after contributes a relation between the two clauses it connects:
after(e2, e1), where e1:drink(john,wine) and e2:solve(john,hard problems)
What nevertheless contributes to this example is a defeasible rule that we will again
write informally as
after(X,E) ? E:drink(Y,wine)) > ?X:solve(Y,hard problems))
Normally, whatever one is able to do after one has had three glasses of wine, it
is not solving difficult algebra problems.
571
Webber et al Anaphora and Discourse Structure
Again, we have abstracted over the individual, as the presupposed defeasible rule
associated with the present-tense sentence appears to be more general than a statement
about a particular individual.15
On the other hand, in the following example illustrating a presupposed defeasi-
ble rule and a discourse relation associated with adjacency, it seems possible for the
presupposed defeasible rule to be about John himself:
(57) John is discussing politics. Nevertheless, he is thinking about his fish.
Here the discourse relation between the two clauses, each of which denotes a specific
event, is
during(e2, e1), where e1:discuss(john,politics) and e2:think about(john,fish)
(Note that our LF representation isn?t sufficiently rich to express the difference between
(55) and (57).) What nevertheless contributes here is the presupposed defeasible rule
during(X,e1) > ?X = e2
Normally what occurs during John?s discussing politics is not John?s thinking
about his fish.
Lagerwerf (1998) does not discuss how specific or general will be the presup-
posed defeasible rule that is accommodated or what factors affect the choice. Kruijff-
Korbayova? and Webber (2001a) also punt on the question, when considering the effect
of information structure on what presupposed defeasible rule is associated with al-
though. Again, this seems to be a topic for future work.
Summary
We have indicated four ways in which we have found the relation associated with a
discourse adverbial to interact with a relation R triggered by adjacency or conveyed
by structural connectives or, in some cases, by another relational anaphor:
1. ? separately serves as an argument to both R? and R.
2. R?(?, ei) is an argument of R.
3. R? is parasitic on R.
4. R? is a defeasible rule that incorporates R.
We do not know whether this list is exhaustive or whether a discourse adverbial
always behaves the same way vis-a`-vis other relations. Moreover, in the process of
setting down the four cases we discuss, we have identified several problems that we
have not addressed, on which further work is needed. Still, we hope that we have
convinced the reader of our main thesis: that by recognizing discourse adverbials
as doing something different from simply signaling the discourse relation between
adjacent discourse units and by considering their contribution as relations in their own
right, one can begin to characterize different ways in which anaphoric and structural
relations may themselves interact.
15 We speculate that the reason examples such as (55) and (56) sound more natural with the focus particle
even applied to the subordinate clause is that even conveys an even greater likelihood that the
defeasible rule holds, so nevertheless emphasizes its failure to do so.
572
Computational Linguistics Volume 29, Number 4
5. Lexicalized Grammar for Discourse Syntax and Semantics
The question we consider in this section is how the treatment we have presented of
discourse adverbials and structural connectives can be incorporated into a general
approach to discourse interpretation. There are three possible ways.
The first possibility is simply to incorporate our treatment of adverbials and con-
nectives into a sentence-level grammar, since such grammars already cover the syntax
of sentence-level conjunction (both coordinate and subordinate) and the syntax of
adverbials of all types. The problem with this approach is that sentence-level gram-
mars, whether phrasal or lexicalized, stop at explicit sentence-level conjunction and do
not provide any mechanism for forming the meaning of multiclausal units that cross
sentence-level punctuation. Moreover, as we have already shown in section 3, the
interpretation of discourse adverbials can interact with the implicit relation between
adjacent sentences, as well as with an explicitly signaled relation, so that a syntax and
compositional semantics that stops at the sentence will not provide all the structures
and associated semantics needed to build the structures and interpretations of interest.
The second possibility is to have a completely different approach to discourse-
level syntax and semantics than to sentence-level syntax and semantics, combining
(for example) a definite clause grammar with rhetorical structure theory. But as we
and others have already noted, this requires discourse semantics reaching further and
further into sentence-level syntax and semantics to handle relations between main and
embedded clauses, and between embedded clauses themselves, as in example (58).
(58) If they?re drunk and they?re meant to be on parade and you go to their
room and they?re lying in a pool of piss, then you lock them up for a day.
(The Independent, June 17, 1997)
Thus it becomes harder and harder to distinguish the scope of discourse-level syntax
and semantics from that at the sentence-level.
The third possibility is to recognize the overlapping scope and similar mechanisms
and simply extend a sentence-level grammar and its associated semantic mechanisms
to discourse. The additional responsibilities of the grammer would be to account for
the formation of larger units of discourse from smaller units; the projection of the
interpretation of smaller discourse units onto the interpretation of the larger discourse
units they participate in; and the effect of discourse unit interpretation on the evolv-
ing discourse model. There are two styles of grammar one could use for this: (1) a
phrase structure grammar (PSG) extended to discourse, as in Figure 6, or (2) a lexi-
calized grammar that extends to discourse, a sentence-level lexicalized grammar such
as tree-adjoining grammar (Joshi, 1987; XTAG-Group 2001) or combinatory categorial
grammar (CCG) (Steedman 1996, 2000b).
Whereas Polanyi and van den Berg (1996) extend a PSG to discourse, we argue
for extending a lexicalized grammar, even though TAG and CCG are weakly context-
sensitive (CS) and the power needed for a discourse grammar with no crossing de-
pendencies is only context-free (section 2.1). Our argument is based on our desire to
use a discourse grammar in natural language generation (NLG). It is well-known that
context-free PSGs (CF PSGs) set up a complex search space for NLG. A discourse
grammar specified in terms of phrase structure rules such as those shown in Figure 6
doesn?t provide sufficient guidance when reversed for use in generating discourse.
For example, one might end up having to guess randomly how many sentences and
connectives one had, in what order, before being able to fill in the sentences and con-
nectives with any content. More generally, trying to generate exactly a given semantics
573
Webber et al Anaphora and Discourse Structure
Seg := SPunct Seg | Seg SPunct | SPunct |
on the one hand Seg on the other hand Seg |
not only Seg but also Seg
SPunct := S Punctuation
Punctuation := . | ; | : | ? | !
S := S Coord S | S Subord S | Subord S S | Sadv S |
NP Sadv VP | S Sadv | . . .
Coord := and | or | but | so
Subord := although | after | because | before | ...
Sadv := DAdv | SimpleAdv
DAdv := instead | otherwise | for example | meanwhile | ...
SimpleAdv := yesterday | today | surprisingly | hopefully | ...
Figure 6
PS rules for a discourse grammar.
when semantics underspecifies syntactic dependency (as discourse semantics must, on
our account) is known to be intractable (Koller and Striegnitz 2002). An effective so-
lution is to generate semantics and syntax simultaneously, which is straightforward
with a lexicalized grammar (Stone et al 2001).
Given the importance of various types of inference in discourse understanding,
there is a second argument for using a lexicalized discourse grammar that derives from
the role of implicature in discourse. Gricean reasoning about implicatures requires a
hearer be able to infer the meaningful alternatives that a speaker had in composing a
sentence. With lexicalization, these alternatives can be given by a grammar, allowing
the hearer, for example, to ask sensible questions like ?Why did the speaker say ?in-
stead? here instead of nothing at all?? and draw implicatures from this. A CF PSG, on
the other hand, might suggest questions like ?Why did the speaker say two sentences
rather than one here?? which seem empirically not to lead to any real implicatures.
(On the contrast between choices, which seem to lead to implicatures, and mere alter-
native linguistic formulations, which do not seem to, see, for example, Dale and Reiter
[1995] and Levison [2000].)
In several previous papers (Webber, Knott, and Joshi, 2001; Webber et al, 1999a,
1999b), we described how our approach fits into the framework of tree-adjoining gram-
mar. This led to the initial version of a discourse parser (Forbes et al 2001) in which
the same parser that builds trees for individual clauses using clause-level LTAG trees
then combines them using discourse-level LTAG trees. Here we simply outline the
grammar, called DLTAG (section 5.1), then show how it supports the approach to
structural and anaphoric discourse connectives presented earlier (section 5.2).
(Of course, one still needs to account for how speakers realize their intentions
through text and how what is achieved through a single unit of text contributes to
what a speaker hopes to achieve through any larger unit in which it is embedded.
Preliminary accounts are given in Grosz and Sidner [1990] and Moser and Moore
[1996]. Given the complex relation between individual sentences and speaker inten-
tions, however, it is unlikely that the relation between multisentence discourse and
speaker intentions can be modeled in a straightforward way similar to the basically
monotonic compositional process that we have discussed in this article for discourse
semantics.)
574
Computational Linguistics Volume 29, Number 4
Dc
DcDc
subconj
(a)
Dc
Dc Dc
subconj
(b)
?:subconj_mid ?: subconj_pre
Figure 7
Initial trees for a subordinate conjunction: (a) postposed; (b) preposed. Dc stands for discourse
clause, ? indicates a substitution site, and subconj stands for the particular subordinate
conjunction that anchors the tree.
5.1 DLTAG and Discourse Syntax
A lexicalized TAG begins with the notion of a lexical anchor, which can have one
or more associated tree structures. For example, the verb likes anchors one tree corre-
sponding to John likes apples, another corresponding to the topicalized Apples John likes,
a third corresponding to the passive Apples are liked by John, and others as well. That
is, there is a tree for each minimal syntactic construction in which likes can appear, all
sharing the same predicate-argument structure. This syntactic/semantic encapsulation
is possible because of the extended domain of locality of LTAG.
A lexicalized TAG contains two kinds of elementary trees: initial trees that reflect
basic functor-argument dependencies and auxiliary trees that introduce recursion and
allow elementary trees to be modified and/or elaborated. Unlike the wide variety of
trees needed at the clause level, we have found that extending a lexicalized TAG to
discourse requires only a few elementary tree structures, possibly because clause-level
syntax exploits structural variation in ways that discourse doesn?t.
5.1.1 Initial Trees. DLTAG has initial trees associated with subordinate conjunctions,
with parallel constructions, and with some coordinate conjuctions. We describe each
in turn.
In the large LTAG developed by the XTAG project (XTAG-Group 2001) subordi-
nate clauses are seen as adjuncts to sentences or verb phrases (i.e., as auxiliary trees)
because they are outside the domain of locality of the verb. In DLTAG, however, it
is predicates on clausal arguments (such as coordinate and subordinate conjunctions)
that define the domain of locality. Thus, at this level, these predicates anchor initial
trees into which clauses substitute as arguments. Figure 7 shows the initial trees for (a)
postposed subordinate clauses and (b) preposed subordinate clauses.16 At both leaves
and root is a discourse clause (Dc): a clause or a structure composed of discourse
clauses.
One reason for taking something to be an initial tree is that its local dependencies
can be stretched long distance. At the sentence level, the dependency between apples
and likes in Apples John likes is localized in all the trees for likes. This dependency can
be stretched long distance, as in Apples, Bill thinks John may like. In discourse, as we
noted in section 2, local dependencies can be stretched long distance as well, as in
(59) a. Although John is generous, he?s hard to find.
16 Although in an earlier paper (Webber and Joshi 1998), we discuss reasons for taking the lexical anchors
of the initial trees in Figures 7 and 8 to be feature structures, following the analysis in Knott (1996) and
Knott and Mellish (1996), here we just take them to be specific lexical items.
575
Webber et al Anaphora and Discourse Structure
Dc
On the
one hand
On the
other
Dc Dc
?:contrast
Figure 8
An initial tree for parallel constructions. This particular tree is for a contrastive construction
anchored by on the one hand and on the other hand.
b. Although John is generous?for example, he gives money to anyone
who asks him for it?he?s hard to find.
(60) a. On the one hand, John is generous. On the other hand, he?s hard to
find.
b. On the one hand, John is generous. For example, suppose you needed
some money: You?d only have to ask him for it. On the other hand,
he?s hard to find.
Thus DLTAG also contains initial trees for parallel constructions as in (60). Such an
initial tree is shown in Figure 8. Like some initial trees in XTAG (XTAG-Group 2001),
such trees can have a pair of anchors. Since there are different ways in which dis-
course units can be parallel, we assume a different initial tree for contrast (on the one
hand. . . on the other (hand). . . ), disjunction (either. . . or. . . ), addition (not only. . . but also. . . ),
and concession (admittedly. . . but. . . ).
Finally, there are initial trees for structural connectives between adjacent sentences
or clauses that convey a particular relation between the connected units. One clear
example is so, conveying result. Its initial tree is shown in Figure 9. We will have a
better sense of what other connectives to treat as structural as a result of annotation
efforts of the sort described in Creswell et al (2002).17
5.1.2 Auxiliary Trees. DLTAG uses auxiliary trees in two ways: (1) for discourse units
that continue a description in some way, and (2) for discourse adverbials. Again we
describe each in turn.
17 For example, one might also have initial trees for marked uses of and and or that have a specific
meaning beyond simple conjunction or disjunction, as in
(61) a. Throw another spit ball and you?ll regret it.
b. Eat your spinach or you won?t get dessert.
These differ from the more frequent, simple coordinate uses of and and or in that the second conjunct
in these marked cases bears a discourse relation to the first conjunct (result in both (61a) and (61b)).
With simple coordinate uses of and and or, all conjuncts (disjuncts) bear the same relation to the same
immediately left-adjacent discourse unit. For example, in (62), each conjunct is a separate explanation
for not trusting John, wheras in (63), each disjunct conveys an alternative result of John?s good fortune:
(62) You shouldn?t trust John. He never returns what he borrows, and he bad-mouths his associates
behind their backs.
(63) John just won the lottery. So he will quit his job, or he will at least stop working overtime.
For simple coordinate uses of and and or, we have auxiliary trees (section 5.1.2).
576
Computational Linguistics Volume 29, Number 4
Dc
DcDc
?:so
so
Figure 9
Initial tree for coordinate conjunction so.
Dc
Dc Dc
?
.
Dc
Dc Dc
? and ?
S
S
then
(a) (b) (c)
?: punct1 ?: and ?: then
Figure 10
Auxiliary trees for basic elaboration. These particular trees are anchored by (a) the
punctuation mark ?period? and (b) and. The symbol ? indicates the foot node of the auxiliary
tree, which has the same label as its root. (c) Auxiliary tree for the discourse adverbial then.
?: punct1
?: punct1
3
?2
*
.
T1
T2
T1 T2
.
?1
0
Figure 11
TAG derivation of example (64).
First, auxiliary trees anchored by punctuation (e.g., period, comma, semicolon.) (Fig-
ure 10a) or by simple coordination (Figure 10b) are used to provide further description
of a situation or of one or more entities (objects, events, situations, states, etc.) within
the situation.18 The additional information is conveyed by the discourse clause that fills
its substitution site. Such auxiliary trees are used in the derivation of simple discourses
such as
(64) a. John went to the zoo.
b. He took his cell phone with him.
Figure 11 shows the DLTAG derivation of example (64), starting from LTAG deriva-
tions of the individual sentences.19 To the left of the horizontal arrow are the elemen-
tary trees to be combined: T1 stands for the LTAG tree for clause (64a), T2 for clause
18 The latter use of an auxiliary tree is related to dominant topic chaining in Scha and Polanyi (1988) and
entity chains in Knott et al (2001).
19 We comment on left-to-right incremental construction of DLTAG structures in parallel with
sentence-level LTAG structures at the end of Section 5.2.
577
Webber et al Anaphora and Discourse Structure
(64b), and ?:punct1 for the auxiliary tree assocated with the period after (64a). In the
derivation, the foot node of ?:punct1 is adjoined to the root of T1 and its substitution
site filled by T2, resulting in the tree to the right of the horizontal arrow. (A standard
way of indicating TAG derivations is shown under the horizontal arrow, where bro-
ken lines indicate adjunction and solid lines, substitution. Each line is labeled with the
address of the argument at which the operation occurs. ?1 is the derivation tree for
T1 and ?2, the derivation tree for T2.)
The other auxiliary trees used in the lexicalized discourse grammar are those for
discourse adverbials, which are simply auxiliary trees in a sentence-level LTAG (XTAG-
Group 2001), but with an interpretation that projects up to the discourse level. An
example is shown in Figure 10c. Adjoining such an adverbial to a clausal/sentential
structure contributes to how information conveyed by that structure relates to the
previous discourse.
There is some lexical ambiguity in this grammar, but no more than serious con-
sideration of adverbials and conjunctions demands. First, as already noted, discourse
adverbials have other uses that may not be anaphoric (65a?b) and may not be clausal
(65a?c):
(65) a. John ate an apple instead of a pear.
b. In contrast with Sue, Fred was tired.
c. Mary was otherwise occupied.
Second, many of the adverbials found in second position in parallel constructions
(e.g., on the other hand, at the same time, nevertheless) can also serve as simple adverbial
discourse connectives on their own. In the first case, they will be one of the two anchors
of an initial tree (Figure 8), and in the second, they will anchor a simple auxiliary tree
(Figure 10c). These lexical ambiguities correlate with structural ambiguity.
5.2 Example Derivations
It should be clear by now that our approach aims to explain discourse semantics in
terms of a product of the same three interpretive mechanisms that operate within
clause-level semantics:
? compositional rules on syntactic structure (here, discourse structure)
? anaphor resolution
? inference triggered by adjacency and structural connection
For the compositional part of semantics in DLTAG (in particular, computing interpre-
tations on derivation trees), we follow Joshi and Vijay-Shanker (2001). Roughly, they
compute interpretations on the derivation tree using a bottom-up procedure. At each
level, function application is used to assemble the interpretation of the tree from the
interpretation of its root node and its subtrees. Where multiple subtrees have function
types, the interpretation procedure is potentially nondeterministic: The resulting am-
biguities in interpretation may be admitted as genuine, or they may be eliminated by a
lexical specification. Multicomponent TAG tree sets are used to provide an appropriate
compositional treatment for quantifiers, which we borrow for interpreting for example
(examples (66c?d)).
In showing how DLTAG and an interpretative process on its derivations operate,
we must, of necessity, gloss over how inference triggered by adjacency or associated
with a structural connective provides the intended relation between adjacent discourse
578
Computational Linguistics Volume 29, Number 4
units: It may be a matter simply of statistical inference, as in Marcu and Echihabi
(2002), or of more complex inference, as in Hobbs et al (1993). As we noted, our view
is that there are three mechanisms at work in discourse semantics, just as there are in
clause-level semantics: Inference isn?t the only process involved. Thus the focus of our
presentation here is on how compositional rules and anaphor resolution (which itself
often appears to require inference) operate together with inference to yield discourse
semantics.
We start with previous examples (44) (here (66c)) and (47) (here (66d)) and two
somewhat simpler variants (66a?b):
(66) a. You shouldn?t trust John because he never returns what he borrows.
b. You shouldn?t trust John. He never returns what he borrows.
c. You shouldn?t trust John because, for example, he never returns what
he borrows.
d. You shouldn?t trust John. For example, he never returns what he bor-
rows.
This allows us to show how (66a?b) and (66c?d) receive similar interpretations, despite
having somewhat different derivations, and how the discourse adverbial for example
contributes both syntactically and semantically to those interpretations.
We let T1 stand for the LTAG parse tree for you shouldn?t trust John, ?1, for its
derivation tree, and interp(T1), for the eventuality associated with its interpretation.
Similarly, we let T2 stand for the LTAG parse tree for he never returns what he bor-
rows, ?2, for its derivation tree, and interp(T2), for the eventuality associated with its
interpretation.
Example (66a) involves an initial tree (?:because-mid) anchored by because (Fig-
ure 12). Its derived tree comes from T1 substituting at the left-hand substitution
site of ?:because-mid (index 1) and T2 at its right-hand substitution site (index 3).
Compositional interpretation of the resulting derivation tree yields explanation(interp
(T2),interp(T1)). (A more precise interpretation would distinguish between the direct
and epistemic causality senses of because, but the derivation would proceed in the
same way.)
In contrast with (66a), example (66b) employs an auxiliary tree (?:punct1) anchored
by a period (Figure 13). Its derived tree comes from T2 substituting at the right-hand
substitution site (index 3) of ?:punct1, and ?:punct1 adjoining at the root of T1 (index 0).
Compositional interpretation of the derivation tree yields merely that T2 continues the
description of the situation associated with T1, that is, elaboration(interp(T2),interp(T1)).
Further inference triggered by adjacency and structural connection leads to a con-
?:because_mid
?:because_mid
31
T2
T1
?1 ?2because
because
T1 T2
Figure 12
Derivation of example (66a). The derivation tree is shown below the arrow, and the derived
tree, to its right. (Node labels Dc have been omitted for simplicity.)
579
Webber et al Anaphora and Discourse Structure
?: punct1
?: punct1
*
T2 . .
T1
T1 T2
3
?2
?1
0
Figure 13
Derivation of example (66b).
D
D  *
cD{ c
c
}
for-ex2
?: for-ex1
0
3
?:?1
0
because_mid
?2
?:
1
T2
T1
?: for-ex1
because_mid?:T1
T2 because
?: for-ex2
because
for example
?
for example
Figure 14
Derivation of example (66c).
clusion of causality between them, that is, explanation(interp(T2),interp(T1)), but this
conclusion is defeasible because it can be denied without a contradiction: for example,
(67) You shouldn?t trust John. He never returns what he borrows. But that?s
not why you shouldn?t trust him.
Example (66c) differs from (66a) in containing for example in its second clause. As
noted earlier, for example resembles a quantifier with respect to its semantics, as its
interpretation takes wider scope than would be explained by its syntactic position. We
handle this in the same way that quantifiers are handled in Joshi and Vijay-Shanker
(2001) by associating with for example a two-element TAG tree set (Figure 14). Both
trees in the tree set participate in the derivation: The auxiliary tree ?:for ex1 adjoins
at the root of T2, whereas the auxiliary tree ?:for ex2 adjoins at the root of the higher
discourse unit. Since we saw from example (66a) that the interpretation of this higher
discourse unit is explanation(interp(T2),interp(T1)), the interpretation associated with
the adjoined ?:for ex2 node both embeds and abstracts this interpretation, yielding
exemplification(interp(T2), ?X . explanation(X,interp(T1))
That is, John?s never returning what he borrows is one instance of a set of explanations.
Similarly, example (66d) differs from (66b) in containing for example in its second
sentence (Figure 15). As in example (66b), an inferred relation is triggered between
the interpretations of T2 and T1, namely, explanation(interp(T2),interp(T1)). Then, as
a result of ?:for ex1 adjoining at T2 and ?:for ex2 adjoining at the root of the higher
580
Computational Linguistics Volume 29, Number 4
D
D
* .
.
D* }{ for-ex1?: for-ex2
0
punct1
?23
0
?:
?1
?:
0
T1
T2
?: for-ex2
?:
punct1
for example
T2
T1
for example ?
for-ex1?:
Figure 15
Derivation of example (66d).
discourse unit, for example again contributes the interpretation
exemplification(interp(T2), ?X . explanation(X,interp(T1))
Thus (66c) and (66d) differ only in the derivation of the interpretation that for example
then abstracts over.
The next example we will walk through is example (11) (repeated here as exam-
ple (68)):
(68) John loves Barolo. So he ordered three cases of the ?97. But he had to
cancel the order because then he discovered he was broke.
As shown in Figure 16, this example involves two initial trees (?:so, ?:because mid) for
the structural connectives so and because; an auxiliary tree for the structural connective
but (?:but), since but functions as a simple conjunction to continue the description of
the situation under discussion; an auxiliary tree (?:then) for the discourse adverbial
then; and initial trees for the four individual clauses T1?T4. As can be seen from the
derivation tree, T1 and T2 substitute into ?:so as its first and third arguments, and ?:but
root-adjoins to the result. The substitution argument of ?:but is filled by ?:because mid,
with T3 and T4 substituted in as its first and third arguments, and ?:then is root-
adjoined to T4. The interpretation contributed by then, after its anaphoric argument is
resolved to interp(T2), is
?4: after(interp(T4), interp(T2))
The interpretations derived compositionally from the structural connectives so, because,
and but are
?1: result(interp(T2), interp(T1))
?2: explanation(interp(T4), interp(S3))
?3: elaboration(?2,?1)
Further inference may then refine elaboration to contrast, based on how but is being
used.
Finally, we want to point out one more way in which texts that seem to be close
paraphrases get their interpretations in different ways. Consider the two texts in ex-
ample (69):
581
Webber et al Anaphora and Discourse Structure
*
so
?:then
*then
because
?:because_mid
?3
T1 ?1
?4T3
T2
T4
?: but
?: so
?: but
?:because_mid
?:
?2
3
1 3
0
?3 ?4
then
?1 ?2
31 0
because
thenT3
T4
T2
so
T1
but
?:
but
so
Figure 16
Derivation of example (68).
(69) a. You should eliminate part 2 before part 3 because part 2 is more sus-
ceptible to damage.
b. You should eliminate part 2 before part 3. This is because part 2 is
more susceptible to damage.
Example (69b) is a simpler version of an example in Moser and Moore (1995), in which
This is because is treated as an unanalyzed cue phrase, no different from because in (69a).
We show here that this isn?t necessary: One can analyze (69b) using compositional
semantics and anaphor resolution and achieve the same results.
First consider (69a). Given the interpretations of its two component clauses, its
overall interpretation follows in the same way as (66a), shown in Figure 12. Now
consider (69b) and the derivation shown in Figure 17. Here the initial tree ?:because mid
T1
T2
TB
?:because_mid
because
?: punct1
*
.
.
because
T2 TB
?: punct1
?:because_mid
31
?2 ??
?1
0
3
T1
Figure 17
Derivation of example (69b).
582
Computational Linguistics Volume 29, Number 4
has its two arguments filled by T2, the TAG analysis of this is and TB, the TAG analysis
of part 2 is more susceptible to damage. The overall derived tree for (69b) comes from
?:punct1 root-adjoining to T1 (the TAG analysis of You should eliminate part 2 before
part 3), with the subsitution site of ?:punct1 filled by the ?:because mid derivation.
The compositional interpretation of the derivation tree yields the interpretation of the
?:because mid tree (i1) as an elaboration of the interpretation of T1:
i1: explanation(interp(TB),interp(T2))
i2: elaboration(i1,interp(T1))
But this is not all. The pronoun this in T2 is resolved anaphorically to the nearest con-
sistent eventuality (Eckert and Strube 2000; Byron 2002) which in this case is interp(T1).
Taking this as the interpretation of T2 and substituting, we get
i1: explanation(interp(TB),interp(T1))
i2: elaboration(i1,interp(T1))
Notice that i1 is also the interpretation of (69a). To this, i2 adds the somewhat redun-
dant information that i1 serves to elaborate the advice in T1. Thus (69a) and (69b)
receive similar interpretations but by different means. This treatment has the added
advantage that one does not have to treat This is not because as a separate cue phrase.
Rather, negation simply produces
i1: ?explanation(interp(TB),interp(T1))
i2: elaboration(i1,interp(T1))
That is, T1 is elaborated by a denial of a (possible) explanation. Presumably, the text
would go on to provide the actual explanation.
Finally, we want to comment on the Holy Grail of discourse parsing: a realistic
model that is computed in parallel with incremental sentence-level parsing. Neither
the analyses given in this section nor the discourse parsing described in Forbes et
al. (2001) is done in a left-to-right incremental fashion, in parallel with incremental
left-to-right sentence-level parsing.
What would an integrated incremental method of sentence-discourse processing
require? At minimum, we believe it would involve:
? A left-to-right parser that would simultaneously compute increments to
sentence-level syntactic structure, sentence-level semantics,
discourse-level syntactic structure, and discourse-level semantics.
Increments to the latter two would occur only at clause boundaries and
with discourse adverbials and structural connectives.
? An incremental anaphor resolution mechanism, similar to that in Strube
(1998), but extended both to deictic pronouns, as in Eckert and Strube
(2000) and Byron (2002) and to the anaphoric argument of discourse
adverbials.
? Incremental computation of discourse structure in terms of elaboration
relations and further nondefeasible reasoning to more specific relations,
where possible.
A left-to-right parser that simultaneously produces sentence-level syntactic and
semantic analyses already exists for combinatory categorial grammar (Steedman 1996,
583
Webber et al Anaphora and Discourse Structure
2000b; Hockenmaier, Bierner, and Baldridge, forthcoming), and it would seem straight-
forward to extend such a parser to computing discourse-level syntax and semantics
as well. Similarly, it seems straightforward to produce an incremental version of any
of the current generation of anaphor resolution mechanisms, extended to deictic pro-
nouns, although current approaches attempt to resolve this and that only with the
interpretation of a single clause, not with that of any larger discourse unit. As these
approaches are also not very accurate as yet, incremental anaphor resolution awaits
improvements to anaphor resolution in general. Moreover, as we better understand
the specific anaphoric properties of discourse adverbials through empirical analysis
such as Creswell et al (2000), such anaphor resolution mechanisms can be extended
to include them as well.
As for building discourse structure incrementally in parallel with syntactic struc-
ture, there is no working prototype yet that will do what is needed. But we have no
doubt that as psycholinguistics and computation together develop a better understand-
ing of incremental semantic processing, researchers? desire for a working prototype
will eventually result in the development of one.
6. Conclusion
In this article, we have argued that discourse adverbials make an anaphoric, rather than
a structural, connection with the previous discourse (section 2), and we have provided
a general view of anaphora in which it makes sense to talk of discourse adverbials as
being anaphoric (section 3). We have then shown that this view of discourse adverbials
allows us to characterize a range of ways in which the relation contributed by a
discourse adverbial can interact with the relation conveyed by a structural connective
or inferred through adjacency (section 4), and then illustrated how discourse syntax
and semantics can be treated as an extension of sentence-level syntax and semantics,
using a lexicalized discourse grammar (section 5).
We are clearly not the first to have proposed a grammatical treatment of low-level
aspects of discourse semantics (Asher and Lascarides 1999; Gardent 1997; Polanyi and
van den Berg 1996; Scha and Polanyi 1988; Schilder 1997a, 1997b; van den Berg 1996),
but we are the first to have recognized that the key to avoiding problems of maintain-
ing a compositional semantics for discourse lies in recognizing discourse adverbials as
anaphors and not trying to shoehorn everything into a single class of discourse connec-
tives. Although we are not yet able to propose a solution to the problem of correctly
resolving discourse adverbials or a way of achieving the Holy Grail of computing
discourse syntax and semantics in parallel with incremental sentence processing, the
proposed approach does simplify issues of discourse structure and discourse semantics
in ways that have not before been possible.
Acknowledgments
The authors would like to thank Kate
Forbes, Katja Markert, Natalia Modjeska,
Rashmi Prasad, Eleni Miltsakaki, Cassandra
Creswell, Mark Steedman, members of the
University of Edinburgh Dialogue Systems
Group, and participants at ESSLLI?01 for
helpful criticism as the ideas in the article
were being developed. We would also like
to thank our three anonymous reviewers.
We believe that in addressing their
criticisms and suggestions, both the article?s
arguments and its presentation have
become clearer. This work has been funded
in part by EPSRC grant GR/M75129
(Webber), NSF grant CISE CDA 9818322
(Stone), and NSF grants NSF-STC SBR
8920230 and NSF-EIA02-24417 (Joshi).
References
Asher, Nicholas. 1993. Reference to Abstract
Objects in Discourse. Kluwer, Boston.
Asher, Nicholas and Alex Lascarides. 1999.
The semantics and pragmatics of
584
Computational Linguistics Volume 29, Number 4
presupposition. Journal of Semantics,
15(3):239?300.
Asher, Nicholas and Alex Lascarides. 2003.
Logics of Conversation. Cambridge
University Press, Cambridge, England.
Asher, Nicholas and Michael Morreau. 1991.
Commonsense entailment. In Proceedings
of the Ninth International Joint Conference on
Artificial Intelligence IJCAI?91, pages
387?392, Sydney, Australia.
Barwise, Jon and Robin Cooper. 1981.
Generalized quantifiers and natural
language. Linguistics and Philosophy,
4:159?219.
Bateman, John. 1999. The dynamics of
?surfacing?: An initial exploration. In
Proceedings of International Workshop on
Levels of Representation in Discourse
(LORID?99), pages 127?133, Edinburgh.
Bierner, Gann. 2001a. Alternative phrases
and natural language information
retrieval. In Proceedings of the 39th Annual
Conference of the Association for
Computational Linguistics, Toulouse,
France, July.
Bierner, Gann. 2001b. Alternative Phrases:
Theoretical Analysis and Practical Application.
Ph.D. thesis, University of Edinburgh.
Bierner, Gann and Bonnie Webber. 2000.
Inference through alternative set
semantics. Journal of Language and
Computation, 1(2):259?274.
Byron, Donna. 2002. Resolving pronominal
reference to abstract entities. In
Proceedings of the 40th Annual Meeting,
Association for Computational Linguistics,
pages 80?87, University of Pennsylvania.
Clark, Herbert. 1975. Bridging. In
Proceedings of Theoretical Issues in Natural
Language Processing (TINLAP-1), pages
169?174, Cambridge, MA.
Clark, Herbert and Catherine Marshall.
1981. Definite reference and mutual
knowledge. In Aravind Joshi, Bonnie
Webber, and Ivan Sag, editors, Elements of
Discourse Understanding. Cambridge
University Press, Cambridge, England,
pages 10?63.
Cosse, Michel. 1996. Indefinite associative
anaphora in French. In Proceedings of the
IndiAna Workshop on Indirect Anaphora,
University of Lancaster, Lancaster,
England.
Creswell, Cassandre, Kate Forbes, Eleni
Miltsakaki, Rashmi Prasad, Aravind Joshi,
and Bonnie Webber. 2002. The discourse
anaphoric properties of connectives. In
Proceedings of the Discourse Anaphora and
Anaphor Resolution Colloquium, Lisbon,
Portugal.
Dale, Robert. 1992. Generating Referring
Expressions. MIT Press, Cambridge, MA.
Dale, Robert and Ehud Reiter. 1995.
Computational interpretations of the
Gricean maxims in the generation of
referring expressions. Cognitive Science,
18:233?263.
Eckert, Miriam and Michael Strube. 2000.
Synchronising units and anaphora
resolution. Journal of Semantics, 17:51?89.
Forbes, Katherine, Eleni Miltsakaki, Rashmi
Prasad, Anoop Sarkar, Aravind Joshi, and
Bonnie Webber. 2001. D-LTAG system:
Discourse parsing with a lexicalized
tree-adjoining grammar. In ESSLLI?2001
Workshop on Information Structure, Discourse
Structure and Discourse Semantics, Helsinki,
Finland.
Forbes, Kate and Bonnie Webber. 2002. A
semantic account of adverbials as
discourse connectives. In Proceedings of
Third SIGDial Workshop, pages 27?36,
Philadelphia, PA.
Frank, Anette and Hans Kamp. 1997. On
context dependence in modal
constructions. In SALT-97, Stanford, CA.
Gardent, Claire. 1997. Discourse tree
adjoining grammars. Claus Report no. 89,
University of the Saarland, Saarbru?cken,
Germany.
Grosz, Barbara and Candace Sidner. 1990.
Plans for discourse. In Philip Cohen, Jerry
Morgan, and Martha Pollack, editors,
Intentions in Communication. MIT Press,
Cambridge, MA, pages 417?444.
Hahn, Udo, Katja Markert, and Michael
Strube. 1996. A conceptual reasoning
approach to textual ellipsis. In Proceedings
of the 12th European Conference on Artificial
Intelligence, pages 572?576, Budapest,
Hungary.
Hardt, Dan. 1992. VP ellipsis and contextual
interpretation. In Proceedings of
International Conference on Computational
Linguistics(COLING-92), pages 303?309,
Nantes.
Hardt, Dan. 1999. Dynamic interpretation of
verb phrase ellipsis. Linguistics and
Philosophy, 22:187?221.
Hellman, Christina and Kari Fraurud. 1996.
Proceedings of the IndiAna Workshop on
Indirect Anaphora. University of Lancaster,
Lancaster, England.
Hobbs, Jerry. 1985. Ontological promiscuity.
In Proceedings of the 23rd Annual Meeting of
the Association for Computational Linguistics,
pages 61?69, Palo Alto, CA. Morgan
Kaufmann.
Hobbs, Jerry. 1990. Literature and Cognition.
Volume 21 of CSLI Lecture Notes. Center
for the Study of Language and
Information, Stanford, CA.
585
Webber et al Anaphora and Discourse Structure
Hobbs, Jerry, Mark Stickel, Paul Martin, and
Douglas Edwards. 1993. Interpretation as
abduction. Artificial Intelligence,
63(1?2):69?142.
Hockenmaier, Julia, Gann Bierner, and Jason
Baldridge. Forthcoming. Providing
robustness for a CCG system. Journal of
Language and Computation.
Isard, Stephen. 1975. Changing the context.
In Edward Keenan, editor, Formal
Semantics of Natural Language. Cambridge
University Press, Cambridge, England,
pages 287?296.
Jayez, Jacques and Corinne Rossari. 1998a.
Pragmatic connectives as predicates. In
Patrick Saint-Dizier, editor, Predicative
Structures in Natural Language and Lexical
Knowledge Bases. Kluwer Academic,
Dordrecht, the Netherlands, pages
306?340.
Jayez, Jacques and Corinne Rossari. 1998b.
The semantics of pragmatic connectives
in TAG: The French donc example. In
Anne Abeille? and Owen Rambow, editors,
Proceedings of the TAG+4 Conference. CSLI
Publications, Stanford, CA.
Joshi, Aravind. 1987. An introduction to tree
adjoining grammar. In Alexis
Manaster-Ramer, editor, Mathematics of
Language. John Benjamins, Amsterdam,
pages 87?114.
Joshi, Aravind and K. Vijay-Shanker. 2001.
Compositional semantics with lexicalized
tree-adjoining grammar (LTAG): How
much underspecification is necessary? In
Harry Bunt, Reinhard Muskens, and Elias
Thijsse, editors, Computing Meaning,
Volume 2, Kluwer, Dordrecht, the
Netherlands, pages 147?163.
Jurafsky, Dan and James Martin. 2000.
Speech and Language Processing.
Prentice-Hall, Englewood Cliffs, NJ.
Kamp, Hans and Uwe Reyle. 1993. From
Discourse to Logic. Kluwer, Dordrecht, the
Netherlands.
Kehler, Andrew. 2002. Coherence, Reference
and the Theory of Grammar. CSLI
Publications, Stanford, CA.
Kibble, Rodger. 1995. Modal
insubordination. In Empirical Issues in
Formal Syntax and Semantics, Selected Papers
from the Colloque de Syntaxe et de Se?mantique
de Paris, pages 317?332.
Knott, Alistair. 1996. A Data-Driven
Methodology for Motivating a Set of Coherence
Relations. Ph.D. thesis, Department of
Artificial Intelligence, University of
Edinburgh.
Knott, Alistair and Chris Mellish. 1996. A
feature-based account of the relations
signalled by sentence and clause
connectives. Language and Speech,
39(2?3):143?183.
Knott, Alistair, Jon Oberlander, Mick
O?Donnell, and Chris Mellish. 2001.
Beyond elaboration: The interaction of
relations and focus in coherent text. In
T. Sanders, J. Schilperoord, and
W. Spooren, editors, Text Representation:
Linguistic and Psycholinguistic Aspects. John
Benjamins, Amsterdam, pages 181?196.
Koller, Alexander and Kristina Striegnitz.
2002. Generation as dependency parsing.
In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics,
pages 17?24, Philadelphia, PA.
Kruijff-Korbayova?, Ivana and Bonnie
Webber. 2001a. Concession, implicature
and alternative sets. In Fourth International
Workshop on Computational Semantics,
Tilburg, the Netherlands.
Kruijff-Korbayova?, Ivana and Bonnie
Webber. 2001b. Information structure and
the semantics of ?otherwise.? In
ESSLLI?2001 Workshop on Information
Structure, Discourse Structure and Discourse
Semantics, pages 61?78, Helsinki, Finland.
Lagerwerf, Luuk. 1998. Causal Connectives
Have Presuppositions. Holland Academic
Graphics, The Hague, the Netherlands.
Levinson, Stephen. 2000. Presumptive
Meanings: The Theory of Generalized
Conversational Implicature. MIT Press,
Cambridge, MA.
Luperfoy, Susann. 1992. The representation
of multimodal user interface dialogues
using discourse pegs. In Proceedings of the
30th Annual Meeting of the Association for
Computational Linguistics (ACL), pages
22?31, University of Delaware, Newark.
Mann, William and Sandra Thompson.
1988. Rhetorical structure theory: Toward
a functional theory of text organization.
Text, 8(3):243?281.
Marcu, Daniel. 1999. Instructions for
manually annotating the discourse
structure of texts. Available from
http://www.isi.edu/?marcu.
Marcu, Daniel and Abdessamad Echihabi.
2002. An unsupervised approach to
recognizing discourse relations. In
Proceedings of the 40th Annual Meeting,
Association for Computational Linguistics,
pages 368?375, University of
Pennsylvania, Philadelphia.
Modjeska, Natalia Nygren. 2001. Towards a
resolution of comparative anaphora: A
corpus study of ??other.? In PAPACOL,
Italy.
Modjeska, Natalia Nygren. 2002. Lexical
and grammatical role constraints in
resolving other-anaphora. In Proceedings of
586
Computational Linguistics Volume 29, Number 4
the Discourse Anaphora and Anaphor
Resolution Colloquium, Lisbon, Portugal.
Moens, Marc and Mark Steedman. 1988.
Temporal ontology and temporal
reference. Computational Linguistics,
14(1):15?28.
Moore, Johanna and Martha Pollack. 1992.
A problem for RST: The need for
multi-level discouse analysis.
Computational Linguistics, 18(4):537?544.
Moser, Megan and Johanna Moore. 1995.
Investigating cue selection and placement
in tutorial discourse. In Proceedings of the
33rd Annual Meeting, Association for
Computational Linguistics, pages 130?135,
MIT, Cambridge, MA.
Moser, Megan and Johanna Moore. 1996.
Toward a synthesis of two accounts of
discourse structure. Computational
Linguistics, 22(3):409?419.
Not, Elena, Lucia Tovena, and Massimo
Zancanaro. 1999. Positing and resolving
bridging anaphora in deverbal NPs. In
ACL?99 Workshop on the Relationship between
Discourse/Dialogue Structure and Reference,
College Park, MD.
Partee, Barbara. 1984. Nominal and
temporal anaphora. Linguistics and
Philosophy, 7(3):287?324.
Polanyi, Livia and Martin H. van den Berg.
1996. Discourse structure and discourse
interpretation. In P. Dekker and
M. Stokhof, editors, Proceedings of the Tenth
Amsterdam Colloquium, pages 113?131,
University of Amsterdam.
Prince, Ellen. 1992. The ZPG letter: Subjects,
definiteness and information-status. In
Susan Thompson and William Mann,
editors, Discourse Description: Diverse
Analyses of a Fundraising Text. John
Benjamins, Amsterdam, pages 295?325.
Quirk, Randolph, Sidney Greenbaum,
Geoffrey Leech, and Jan Svartik. 1972. A
Grammar of Contemporary English.
Longman, Harlow, England.
Scha, Remko and Livia Polanyi. 1988. An
augmented context free grammar for
discourse. In Proceedings of the 12th
International Conference on Computational
Linguistics (COLING?88), pages 573?577,
Budapest, Hungary, August.
Schilder, Frank. 1997a. Towards a theory of
discourse processing: Flashback sequences
described by D-trees. In Proceedings of the
Formal Grammar Conference (ESSLLI?97),
Aix-en-Provence, France, August.
Schilder, Frank. 1997b. Tree discourse
grammar, or how to get attached to a
discourse. In Proceedings of the Second
International Workshop on Computational
Semantics, Tilburg, the Netherlands,
January.
Steedman, Mark. 1996. Surface Structure and
Interpretation. Volume 30 of Linguistic
Inquiry Monograph, 5, MIT Press,
Cambridge, MA.
Steedman, Mark. 2000a. Information
structure and the syntax-phonology
interface. Linguistic Inquiry, 34:649?689.
Steedman, Mark. 2000b. The Syntactic
Process. MIT Press, Cambridge, MA.
Stokhof, Martin and Jeroen Groenendijk.
1999. Dynamic semantics. In Robert
Wilson and Frank Keil, editors, MIT
Encyclopedia of Cognitive Science. MIT Press.
Cambridge, MA, pages 247?249
Stone, Matthew, Christine Doran, Bonnie
Webber, Tonia Bleam, and Martha Palmer.
2001. Microplanning from communicative
intentions: Sentence planning using
descriptions (SPUD). Technical Report no.
RUCCS TR68, Department of Cognitive
Science, Rutgers University, New
Brunswick, NJ.
Stone, Matthew and Daniel Hardt. 1999.
Dynamic discourse referents for tense and
modals. In Harry Bunt, editor,
Computational Semantics. Kluwer,
Dordrecht, the Netherlands, pages
287?299.
Strube, Michael. 1998. Never look back: An
alternative to centering. In Proceedings,
COLING/ACL?98, pages 1251?1257,
Montreal, Quebec, Canada.
Traugott, Elizabeth. 1995. The role of the
development of discourse markers in a
theory of grammaticalization. Paper
presented at ICHL XII, Manchester,
England. Revised version of (1997)
available at http://www.stanford.
edu/traugott/ect-papersonline.html.
Traugott, Elizabeth. 1997. The discourse
connective after all: A historical
pragmatic account. Paper presented at
ICL, Paris. Available at http://www.
stanford.edu/traugott/ect-
papersonline.html.
van den Berg, Martin H. 1996. Discourse
grammar and dynamic logic. In P. Dekker
and M. Stokhof, editors, Proceedings of the
Tenth Amsterdam Colloquium, pages 93?111,
ILLC/Department of Philosophy,
University of Amsterdam.
van Eijck, Jan and Hans Kamp. 1997.
Representing discourse in context. In Jan
van Benthem and Alice ter Meulen,
editors, Handbook of Logic and Language.
Elsevier Science B.V., Amsterdam, pages
181?237.
Venditti, Jennifer J., Matthew Stone,
Preetham Nanda, and Paul Tepper. 2002.
Discourse constraints on the
587
Webber et al Anaphora and Discourse Structure
interpretation of nuclear-accented
pronouns. In Proceedings of Symposium on
Speech Prosody, Aix-en-Provence, France.
Available at http://www.lpl.
univ-aix.fr/sp2002/papers.htm.
Vendler, Zeno. 1967. Linguistics in Philosophy.
Cornell University Press, Ithaca, NY.
Webber, Bonnie. 1988. Tense as discourse
anaphor. Computational Linguistics,
14(2):61?73.
Webber, Bonnie. 1991. Structure and
ostension in the interpretation of
discourse deixis. Language and Cognitive
Processes, 6(2):107?135.
Webber, Bonnie and Breck Baldwin. 1992.
Accommodating context change. In
Proceedings of the 30th Annual Meeting of the
Association for Computational Linguistics
(ACL), pages 96?103, University of
Delaware, Newark.
Webber, Bonnie and Aravind Joshi. 1998.
Anchoring a lexicalized tree-adjoining
grammar for discourse. In COLING/ACL
Workshop on Discourse Relations and
Discourse Markers, pages 86?92, Montreal,
Quebec, Canada.
Webber, Bonnie, Alistair Knott, and Aravind
Joshi. 2001. Multiple discourse
connectives in a lexicalized grammar for
discourse. In Harry Bunt, Reinhard
Muskens, and Elias Thijsse, editors,
Computing Meaning, volume 2. Kluwer,
Dordrecht, the Netherlands, pages
229?249.
Webber, Bonnie, Alistair Knott, Matthew
Stone, and Aravind Joshi. 1999a.
Discourse relations: A structural and
presuppositional account using lexicalised
TAG. In Proceedings of the 36th Annual
Meeting of the Association for Computational
Linguistics, pages 41?48, College Park,
MD.
Webber, Bonnie, Alistair Knott, Matthew
Stone, and Aravind Joshi. 1999b. What
are little trees made of: A structural and
presuppositional account using lexicalised
TAG. In Proceedings of International
Workshop on Levels of Representation in
Discourse (LORID?99), pages 151?156,
Edinburgh.
Wiebe, Janyce. 1993. Issues in linguistic
segmentation. In Workshop on Intentionality
and Structure in Discourse Relations,
Association for Computational Linguistics,
pages 148?151, Ohio State University.
Woods, William. 1978. Semantics and
quantification in natural language
question answering. In Marshall C. Yovits,
editor, Advances in Computers, volume 17.
Academic Press, New York, pages 1?87.
XTAG-Group. 2001. A lexicalized tree
adjoining grammar for English. Technical
Report no. IRCS 01-03, University
of Pennsylvania, Philadelphia. Available at
ftp://ftp.cis.upenn.edu/pub/ircs/technical-
reports/01-03.
Optimising text quality in generation from relational databases 
Michael  O 'Donne l l t  (micko@dai .ed .ac .uk) ,  
A l i s ta i r  Knott:~ (a l i k@hermes .o tago .ac .nz ) ,  
Jon  Ober lander ,  ( jon@cogsc i .ed .ac .uk) ,  
Chr i s  Me l l i sh t (chr i sm@dai .ed .ac .uk)  
, D iv is ion  of  In fo rmat ics ,  Un ivers i ty  of  Ed inburgh .  
. . . . .  ~.:D.eparl~me~t.nf: Compulzer?c ience~ ~Otago Univers ity:  
Abst rac t  
This paper outlines a text generation system suited 
to a large class of information sources, relational 
databases. We focus on one aspect of the problem: 
the additional information which needs to be spe- 
cified to produce reasonable text quality when gen- 
erating from relational databases. We outline how 
databases need to be prepared, and then describe 
various types of domain semantics which can be used 
to improve text qualify. 
1 In t roduct ion  
As the problems of how we generate text are gradu- 
ally solved, a new problem is gaining prominence 
- where do we obtain the information which feeds 
the generation. Many domain models for existing 
generation systems are hand-crafted for the specific 
system. Other systems take advantage of existing 
information sources. 
A good information source for text generation 
resides in the vast number of relational databases 
which are in use around tile world. These resources 
have usually been provided for some reason other 
than text generation, such as inventory manage- 
ment, accounting, etc. However, given that the in- 
formation is on hand, it can be of value to conuect 
these databases to text generation facilities. 
The benefits include natural anguage access to in- 
formation which is usually accessed in tabular form, 
which can be difficult to interpret. Natural Lan- 
guage descriptions are easier to read, can be tailored 
to user types, and can be expressed in different lan- 
guages if properly represented. 
This paper outlines the domain specification lan- 
guage for the ILEX text g~neration system, (for 
Intelligent Labelling Explorer). 1
ILEX is a tool for ?dynamic browsing of database- 
defined information: it allows a user to browse 
through the information in a database using hyper- 
1Earlier ILEX papers have been based on Ilex 2.0, which 
was relatively domain-dependent.  This  paper is based around 
version 3.0 of ILEX, a re-draft to make the system domain- 
independent, and domain acquisition far easier. The ILEX 
project was supported by EPSRC grant GR/K53321.  
text. ILEX generates descriptions of database ob- 
jects on the fly, taking into account he user's con- 
text of browsing. Figure 1 shows the ILEX web in- 
terface, as applied to a museum domain, in this case 
the Twentieth Century Jewellery exhibition at the 
the National Museum of Scotland. 2 The links to 
related database objects are also automatically gen- 
erated. ILEX has been applied to other domains, in- 
cluding personnel (Nowson, 1999), and a sales cata- 
logue for computer systems and peripherals (Ander- 
son and Bradshaw, 1998). 
One of the advantages of using NLG for database 
browsing is that the system can keep track of what 
has already been said about objects, and not repeat 
that information on later pages. Appropriate refer- 
ring expressions can also be selected on the basis 
of the discourse history. The object descriptions can 
be tailored to the informational interests of the user. 
See Knott et al (1997) and Mellish et al (1998) for 
more information on these aspects of ILEX. 
In section 2, we consider some systems related to 
the ILEX system. Section 3 describes the form of 
relational database that ILEX accepts as input. Sec- 
tion 4 outlines what additional information - domain 
semantics - needs to be provided for coherent ext 
production from the database, while section 5 de- 
scribes additional information which can be provided 
to improve the quality of the text produced. 
2 Re la ted  Work  
It should be clear that the task we are discussing is 
very distinct from the task of response generation in 
a natural language interface to a database (e.g., see 
Androutsopoulos et al (1995)). ' In such systtems, 
the role of text planning is quite simple or absent, 
usually dealing with single sentences, or in the most 
? ? complex systems;~ a:single:sentence ,answer ~with an 
additional clause or two of supporting information. 
ILEX is not a query response generation system, 
it is an object description system. It composes a full 
text, at whatever size, with the goal of making that 
text a coherent discourse. 
2The authors thank the museum for making their database 
available: 
133 
Sflver.A~nd Ename . :  
!- S.~.v~ t !~s ,  w i~ blu~-~e~.~i/e.1 . ! 
' ) . :~v{ .  ,EX :~- - : :  . . . . . . . . . . . . . .  )' . . . . . . .  " t ' ': i:: ' :  :!:i 
? lessie-I~-X~g.,l~. Place of,. ; 
? This Jewel !s apel'l.d~mat-neckla~ ililitwaS . . 
I madebZ,aSa~h de:a,S~caUed-Jesae M , 
l<: gin,g:ilt~bnedlhe f~mrRemStn:tht~:case.,::_: , '  
:: ? / lowers reseri~A a~ai~t  i t - I t  is tn ~e Arts :~ud,  
. Crafts:style and was made t1~ :lgfl~ It has an . . . .  
elaborate aesign; specifically It h~ floral mows.  
: :;::anlllustrat~too, In fact., shg did qttite, a' l~ of,-: : 
" differei~tl~rpes of creative Wark;/cwdleiTls ? : i:. 
:. ; :  :i'; ~:t~n Arts. amt Craft#Style . ),.::.:):i:.: 2,} :,i.: 'i:' :" 
:'::' ; :-'? ;~.~,~t,~I,t~,,/~l.a~.~_~': ~" ; : "  : : ; " : " ,  -'; 7.; ...:. 
."  ' .:,L'~n Ai'ts aiid Crafts:s~lgne~iil~e -: :.::'..': " i 
J 
.; ... (; . 
Figure 1: Browsing Object Descriptions 
In this regard, ILEX should be more fruit- 
fully compared with text generation systems such 
as GOSSIP (Carcagno and Iordanskaja, 1993), 
PEBA (Milosavljevie, 1997; Milosavljevic, 1999), or 
POWER (Dale et al, 1998), systems which build an 
extended text fl'om an underlying database. 
ILEX 3.0 has been developed to be domain in- 
dependent, to handle relational databases from any 
domain, as long as the information is provided in the 
required format. The first two of the systems above 
are single domain systems. T:he third, POWER,  is 
an extension of PEBA to handle a new domain. It 
is not clear however whether the resulting system is 
.. itself domain-dependent or not. 
This last system is perhaps the best comparison 
for the ILEX system, since it also generates de- 
scriptions of museum objects from an underlying 
database. In that paper, the main focus is on the 
problem of extracting out usable information from 
badly structured databases (as often provided by 
museulns), and on generating texts using only only 
this information (plus some linguistic knowledge). 
The present paper differs from this approach by as- 
suming that information is already available in a nor- 
malised relational database. We observe, as do Dale 
et al (1998), that texts generated from this inform- 
ation alone are quite poor in quality. We go one 
step further by examining what additional informa- 
tion can be provided to improve the quality of the 
text to a reasonable l vel. 
The ILEX system has been implemented to be 
flexible in regards to the available domain inform- 
ation. With a bare minimum, the system provides 
poor quality texts, but as the domain developer ex- 
.tends-the domain semantics, the quahty of.texts im- 
proves, up to a point where users sometimes nfistake 
ILEX-generated texts for human-authored texts. 
3 The Structure of a Relational 
Database 
Databases vary widely in form, so we have assumed 
a fairly" standard relational database format. 
134 
3.1 Entity Files 
:.The database consists of .a number:.:of ~ntity files, 
each file providing the records for a different entity 
type. Each record (row) in the entity file defines a 
unique entity. The columns define attributes of the 
entities. In a museum domain, we might have an 
entity file for museum artifacts, another for people 
involved with the artifacts (designers, owners, etc.), 
another for locations, etc. See figure 2 for a sample 
entity file for the Jewellery domain. Given the wide 
.range of database formats..a~vailable, !LEX ~sumes 
a tab-delimited format for database files. 
ILEX imposes two requirements on the entity files 
it uses: 
1. Single field key: while relational databases of- 
ten use multiple attributes to form a unique key 
(e.g., name and birthdate), ILEX requires that 
each entity have a unique identifier in a single 
attribute. This identifier must be under a field 
labelled ID. 
2. Typing of entities: ILEX depends trongly on a 
type system. We require that each entity record 
provides a type for the entity in a field labelled 
Class. 
Some other attribute labels are reserved by the 
system, allowing ILEX to deal intelligently with 
them, including Name, Short-Name and Gender. 
3.2 L ink Fi les 
In some cases, an entity will have multiple fillers of 
an attribute, for instance, a jewellery piece may be 
made of any number of materials. Entity files, with 
fixed record structure, cannot handle such eases. 
The standard approach in relational databases i to 
provide a link file for each case where multiple fillers 
are possible. A link file consists of two columns only, 
one identifying the entity, the other identifying the 
filler (the name of the attribute is provided in the 
first line of the file, see figure 3). 
We are aware that the above specification repres- 
ents an impoverished view of relational databases. 
Many relational databases provide far more than 
simple entity and link files. However, by no means 
all relational databases provide more than this, so 
we have adopted the lowest common denominator. 
Most relational databases can be exported in a form 
which meets our requirements. 
3.3 Terminology 
In the following discussion, we will use the following 
terminology: 
* Predicate: each column of an entity file defines 
a predicate. Class, Designer and Date are thus 
predicates introduced in figure 2. Each link file 
also defines a predicate. 
? Record: each row of an entity table provides the 
attributes o f  a: single.,entity.: The row is termed 
a record in database terminology. 
? Fact: each entry in a record defines what we 
call a fact about that entity, a A fact consists o f  
three parts: its predicate name, and two argu- 
ments, being the entity of the record, and the 
filler of the slot. 
? ARC1: the first argument of a fact, the entity 
the  fact is about. 
. ARC2: the second argument of a fact, the filler 
of the attribute for the entity. 
4 Spec i fy ing  the  Semant ics  o f  the  
Database  
A database itself says nothing about the nature of 
the contents of each field in the database. It might 
be a name, a date, a price, etc. Similarly for the 
field label: the field label names a relation between 
the entity represented by the record and the entity 
represented by the filler. However, without further 
specification, we do not know what this relationship 
entails, apart from the label itself, e.g., 'Designer'. 
Before we can begin to process a database intelli- 
gently, we need to define the 'semantics' of the data- 
base. This section will outline how this is done in the 
ILEX case. There has been some work on automatic 
acquisition of database semantics, uch as in the con- 
struction of taxonomies of domain entity types (see 
Dale et al (1998) for instance). However, it is diffi- 
cult to perform this process reliably and in a domain- 
independent manner, so we have not attempted to 
in this case. The specification of domain semantics 
is still a manual process which has to be undertaken 
to link a database to the text generator. 
To use a database for generation, additional in- 
formation of several kinds needs to be provided: 
1. Taxonomic organisation: supplying of types for 
each database ntity, and organisation of these 
types into taxonomies; 
2. Taxonomic lexification: specif~'ing how each do- 
main type is lexified; 
3. Data type off attribute fillers: telling the system 
to expect the filler of a record slot to be an 
entity-id, a string, a date, etc. 
4. Domain type specification:specifying What do- 
main type the slot filler can be assumed to be. 
Each of these aspects of domain specification will 
be briefly described below. 
3Excepting the first column, which provides the entity-id 
for tile record. 
135 
 Class brooch -necklace necklace Designer KingO1 "KingO1 ChanelO1 Style J___190~ A-rt-Deco : ~_~_~ Art-Noveux London Paris 
L_ 
Sponsor 
Liberty01 
Figure 2: A Sample from an Entity file 
\ [ ~ .  Material 
Figure 3: A Sample from a Link file 
(def-basic-type 
:domain jewellery-domain 
:head jewellery 
:mn-link 3D-PHYS-0BJECT) 
(def-taxonomy 
:type jewellery 
:subtypes (neck-jewellery wrist-jewellery 
pin-jewellery pendant buckle 
earring earring-pair finger-ring 
ringset watch button dress-clip 
hat-pin)) 
Figure 4: Defining Taxonomic Knowledge 
4.1 Taxonomic  Organ isat ion  
ILEX requires that the entities of the domain are or- 
ganised under a domain taxonomy. The user defines 
a basic type (e.g., jewellery), and then defines the 
sub-types of the basic-type, and perhaps further sub- 
classification. Figure 4 shows the lisp forms defining 
a basic type in the jewellery domain, and the sub- 
classification of this type. The basic type is also 
mapped onto a type (or set of types) in the concept 
ontology used for sentence generation, a version of 
Penman's Upper Model (Bateman, 1990). This al- 
lows the sentence generator to reason about the ob- 
jects it expresses. 
Taxonomic organisation is important for several 
reasons, including among others: 
1. Expressing Entities: each type can be related to 
lexical i tems'to use,to-express that  type (e.g., 
linking the type brooch to a the lexical item for 
"brooch". If no lexical item is defined for a type, 
a lexical item associated with some super-type 
can be used instead. Other aspects of the ex- 
pression of entities may depend on the concep- 
tual type, for instance pronominalisation, deixis 
(e.g., mass or count entities), etc. 
2. Supporting Inferences and Generalisations: 
ILEX allows the user to assert generalisations 
about types, e.g., that Arts and Crafts jewellery 
tends to be made using enamel (see section 5.4). 
The type hierarchy is used to check whether a 
particular generalisation is appropriate for any 
given instance. 
The earlier version of ILEX, Ilex2.0, allowed the 
full representational power of the Systemic formal- 
ism for representing domain taxonomies, including 
cross-classification, and multiple inheritance (both 
disjunctive and conjunctive). However, our exper- 
iences with non-linguists trying to define domain 
models showed us that the more scope for expres- 
sion, the more direction was needed. We thus sim- 
plified the formalism, by requiring taxonomies to be 
simple, with no cross-classification r multiple inher- 
itance. We felt that the minor loss of expressivity 
was well balanced by the gain in simplicity for do- 
main developers. 
4.2 Type Lexi f icat ion 
To express each database ntity, it is essential to be 
able to map from its defined type, to a noun to use 
in a referring expression, e.g., this brooch. 
Ilex comes with a basic lexicon already provided. 
covering the commonly occurring words. Each entry 
defines the svntactic and morphological information 
required for sentence generation. For these items, 
the domain developer needs to provide a simpl e map- 
ping from domain type to lexical item, for instance, 
the following lisp form specifies that the domain type 
location should be lexified by the lexical item whose 
id is location=noun: 
(lexify location location-noun) 
For those lexical items not already defined, the do- 
main developer needs to provide in addition lexical 
item definitions for the nouns expressing the types 
in their domain. A typical entry has the form shown 
in figure 5. 
136 
(def-lexical-item 
:name professor-noun 
:spelling "professor" 
:grammatical-features (common-noun count-noun) 
) 
Figure 5: A Sample Lexical item Specification 
. . . .  (defobject-structurejewellery- " ..... 
:class :generic-type 
:subclass :generic-type 
:designer :entity-id 
:style :entity-id 
:material :generic-type 
:date :date 
:place :string 
:dimension :dimension) 
Figure 6: Specifying Field Semantics 
(def-predicateClass 
:expression (:verb be-verb) 
) 
Figure 8: Simple Fact Expression 
4.3 Data Type of Slot Fillers 
Each field in a database record contains a string of 
characters. It is not clear whether this string is an 
identifier for another domain entity, a string (e.g., 
someone's urname), a date, a number, a type in 
the type hierarchy, etc. 
ILEX requires, for each entity file, a statement as 
to how the field fillers should be interpreted. See 
figure 6 for an example. 
Some special filler types have been provided to 
facilitate the import of structured ata types. This 
includes both :date and :dimension in the current 
example. Special code has been written to convert 
the fillers of these slots into ILEX objects. Other 
special filler types are being added as needed. 
4.4 Domain  Type  o f  Slot Fi l lers 
The def-predicate form allows the domain developer 
to state what type the fillers of a particular field 
should be. This not only allows for type checking, 
but also allows the type of an entity to be inferred 
if not otherwise provided. For instance, by assert- 
ing that fillers of the Place field should of type city, 
the system can infer that "London" is a city even if 
London itself has no database record. See figure 7. 
(def-predicate Place 
:argl jewellery 
:arg2 city 
) 
Figure 7: Speci~'ing Predicate Fillers 
4.5 Summary  
..... '.:~With:just chisvmuch-semantics~specified,. ILEX e-an 
generate very poor texts, but texts which convey 
the content of the database records. In the next 
section, we will outline the extensions to the domain 
semantics which are needed to improve the quality 
of the text produced by ILEX. 
5 Extending Domain Semantics for 
Improved Text Quality 
So far we have discussed only the simplest level of 
domain semantics, which allows a fairly direct ex- 
pression of domain information. ILEX allows the 
domain developer to provide additional domain se- 
mantics to improve the quality of the text. 
5.1 Expression of Facts 
Unless told otherwise, ILEX will express each fact in 
a simple regular form, such as The designer of this 
brooch is Jessie M. King, using a template form4: 
The <predicate> of <entity-expression> 
is <filler-expression>. 
However, a text consisting solely of clauses of this 
form is unnatural, and depends on the predicate la- 
bel being appropriate to the task (labels like given-by 
will produce nonsense sentences). 
To produce better text, ILEX can be told how 
to express facts. The domain developer can provide 
an optional slot to the &f-predicate form as shown 
in figure 8. The expression specification first of all 
defines which verb to use in the expression. By de- 
fault, the ARG1 element is mapped onto the Sub- 
ject, and the ARG2 onto the Object. Default val- 
ues are assumed for tense, modality, polarity, voice. 
finiteness, quantification, etc., unless otherwise spe- 
cified. So, using the above expression specification, 
the Class fact of a jewel would be expressed by a 
clause like: This item is a brooch. 
To .produce less .standard expressions, we need to 
modify some of the defaults. A more complex ex- 
pression specification is shown in figure 9, which 
would result in the expression such as: For further 
information, see Liberty Style Guide No. 326: 
4ILEX3.0  borrowed this use of a default  express ion tem- 
p late  from the POWER system (Dale et al, 1998). In previ-  
ous vers ions of ILEX,  all facts were expressed by full NLG as 
exp la ined below. 
137 
(def-predicate Bib-Note 
:argl jewellery 
:expression ( 
:adjunctl "for further information" 
:mood imperative 
:verb see-verb 
:voice active) 
Figure 9: More Complex Fact Expression 
The expression form is used to construct a par- 
tial syntactic specification, which is then completed 
using the sentence generation module of the WAG 
sentence generator (O'Donnell, 1996). 
With the level of domain semantics pecified so 
far, ILEX is able to produce texts such as the two be- 
low, which provides an initial page describing data- 
base entity BUNDY01, and then a subsequent page 
when more information was requested (this from the 
Personnel domain (Nowson, 1999)): 
o Page  1: Alan Bundy is located in room F1, 
which is in South Bridge. He lectures a course 
called Advanced Automated Reasoning and is in 
the Institute for Representation and Reasoning. 
He is the Head of Division and is a professor. 
* Page  2: As already mentioned, Alan Bundy lec- 
tures Advanced Automated Reasoning. AAR is 
lectured to MSc and AI4. 
This expression specification form has been de- 
signed to limit the linguistic skills needed for domain 
developers working with the system. Given that the 
domain developers may be museum staff, not com- 
putational linguists, this is necessary. The notation 
however allows for a wide range of linguistic expres- 
sions if the full range of parameters are used. 
5.2 User  Adapt ion  
To enable the system to adapt its content to the 
type of user, the domain developers can associate 
information with each predicate indicating the sys- 
tem's view of the predicate's interest, importance, 
etc., to the user. This information is added to the 
d@predicate form, as shown in figure 10. 
The user annotations allowed by ILEX include: 
1. Interest: how interesting does the system judge 
the information to be to the user; 
2. Importance: how important is it to the system 
that the user reads the information; 
3. Assimilation: to what degree does the system 
judge the user to already know the infornlation: 
.<def~predicate Designer 
. o .  
:importance ((expert lO)(default 6)(child 5)) 
:interest ((expert lO)(default 6)(child 4)) 
:assimilation ((expert O)(default O)(child 0)) 
:assim-rate ((expert l)(default l)(child 0.5)) 
) 
Figure 10: Specifying User Parameters 
4. Assimilation Rate: How quickly does the sys- 
tem believe the user will absorb the information 
when presented (is one presentation enough?). 
This information influences what content will be 
expressed to a particular user, and in what or- 
der (more relevant on earlier pages). Information 
already assimilated will not be delivered, except 
when relevant for other purposes (e.g., when refer- 
ring to the entity). If no annotations are provided, 
no user customisation will occur. 
The values in ILEX's user models have been set 
intuitively by the implementers. While ideally these 
values would be derived through user studies, our 
purpose was purely to test the adaptive mechanism, 
and demonstrate that it works. We .leave the devel- 
opment of real user models for later work. 
ILEX has opted out of using adaptive user model- 
ling, whereby the user model attributes are adapted 
as a result of observed user choices in the web inter- 
face. We leave this for future research. 
5.3 Compar i sons  
When describing an object, it seems sometimes use- 
ful to compare it to similar articles already seen. 
With small addition to the domain specification, 
ILEX can compare items (an extension by Maria Mi- 
losavljevic), as demonstrated in the following text: 
This item is also a brooch. Like the previ- 
ous item, it was designed by King. How- 
ever, it differs from the previous item in 
that it is made of gold and enamel, while 
the previous brooch was made of silver and 
enamel. 
For ILEX to properly compare two entities, it 
needs to Mmw how the various.attributes of the en- 
tity can be compared (nominal, ordinal, scalar, etc.). 
Again, information can be added to the d@predicate 
for each predicate to define its scale of comparabil- 
ity. See Milosavljevic (1997) and (1999) for more de- 
tail. Figure 11 shows the additions for the Designer 
predicate. Comparisons introduce several RST re- 
lations to the text structure, including rst-contrast, 
rst-similarity and rst-whereas. 
138 
(def-predicate Designer 
:variation (string i) 
:scale nominal 
) 
Figure lh Specifying Predicate Comparability 
(def-defeasible-rule 
? :qv ($jewel jewellery) ....... 
:lhs (some ($X (style $jewel $X)) 
(arts-and-crafts SX))) 
:rhs (some ($X (made-of Sjewel SX)) 
(enamel SX))) 
Figure 12: Specifying Generalisations 
5 . 4  G e n e r a l i s a t i o n s  
We found it useful to allow facts about general types 
of entities to be asserted, for instance, that Arts and 
Crafts jewellery tend to be made of enamel. These 
generalisations can then be used to improve the qual- 
ity of text, producing object descriptions as in the 
following: 
This brooch is in the Arts and Crafts style. 
Arts and Crafts jewels tend to be made of 
enamel. However, this one is not. 
These generalisations are defined using defeasible 
implication - similar to the usual implication, but 
working in terms of few, many, or most rather than 
all or none. They are entered in a form derived 
from first order predicate calculus, for instance, see 
figure 12 which specifies that most Arts and Crafts 
jewellery uses enamel. 
ILEX find each instance which matches the gen- 
eral type (in this case, instances of type jewellery 
which have Arts and Crafts in the Style role). If 
the fact about the generic object has a correspond- 
ing fact on the instantial object, an exemplification 
relation is asserted between the facts. Otherwise, 
a ?concession relation is asserted. See Knott et al 
(1997) for more details on this procedure. 
6 Summary  
While observing people trying to convert an earlier 
ILEX system to a new domain, we noted the diffi- 
culty they had. To avoid these problems, we under- 
took to re-implement the domain specification as- 
pects of ILEX to simplify the task. 
Towards this end, we have followed a number of 
steps. Firstly, we reconstructed ILEX to be domain 
- Taxonomies 
- Lexification of Types 
- Filler Domain Type Information 
- Filler Data Type Information 
OBLIGATORY 
- Predicate Expression 
- Comparison Information 
- Generalisations 
- User Annotations 
OPTIONAL 
Figure 13: Obligatory and Optional Steps in Domain 
Specification 
independent, with all domain information defined in 
declarative resource files. This means that domain 
developers do not have to deal with code. 
Secondly, we built into ILEX the ability to import 
entity definitions directly from a relational database 
(although with some restrictions as to its form). 
A database by itself does not provide enough in- 
formation to produce text. Domain semantics is re- 
quired. We have provided a system of incremental 
specification of this semantics which allows a domain 
developer to hook up adynamic hypertext interface 
to a relational database quickly, although producing 
poor quality text. Minimally, the system requires 
a domain taxonomy, information on lexification of 
types, and specification of the data type of each re- 
cord field. 
Additional effort can then improve the quality of 
text up to a quite reasonable l vel. The additional 
information can include: specification of predicate 
expression, and specifications supporting comparis- 
ons, user adaption, and generalisations. 
Figure 13 summarises the obligatory and optional 
steps in domain specification in ILEX. 
Simplifying the domain specification task is a ne- 
cessity as text generation systems move outside of 
research labs and into the real world, where the 
domain developer may not be a computational lin- 
guist, but a museum curator, personnel officer or 
wine salesman. ~ have tried to take a step towards 
making their task easier. 
Re ferences  
Gail Anderson and Tim Bradshaw. 1998. ILEX: 
The intelligent labelling explorer: Experience of 
Building a Demonstrator for the Workstation Do- 
main. Internal Report, Artificial Intelligence Ap- 
plications tnstitute,University of Edinburgh. 
I. Androutsopoulos, G.D. Ritchie, and P. Thanisch. 
1995. Natural language interfaces to databases - 
an introduction. Natural Language Engineering, 1
(1):29-81. 
John Bateman. 1990. Upper modeling: organiz- 
ing knowledge for natural language processing. 
In Proceedings of the Fifth International Work- 
139 
shop on Natural Language Generation, Pitts- 
burgh, June. 
Denis Carcagno and Lidija Iordanskaja. 1993. Con- 
tent determination a d text structuring: two in- 
terrelated processes. In Helmut Horocek and Mi- 
chael Zock, editors, New Concepts in Natural Lan- 
guage Generation, Communication i Artificial 
Intelligence Series, pages 10 - 26. Pinter: London. 
Robert Dale, Stephen J Green, Maria Milosavljevic, 
CEcile Paris, Cornelia Verspoor, and Sandra Wil- 
liams. 1998. The realities of generating natural 
language from databases. In "Proceedings of the 
11th Australian Joint Conference on Artificial In- 
telligence, Brisbane, Australia, 13-17 July. 
Alistair Knott, Michael O'Donnell, Jon Oberlander, 
and Chris Mellish. 1997. Defeasible rules in con- 
tent selection and text structuring. In Proceedings 
of the 6th European Workshop on Natural Lan- 
guage Generation, Gerhard-Mercator University, 
Duisburg, Germany, March 24 - 26. 
Chris Mellish, Mick O'Donnell, Jon Oberlander, and 
Alistair Knott. 1998. An architecture for oppor- 
tunistic text generation. In Proceedings of the 
Ninth International Workshop on Natural Lan- 
guage Generation, Niagara-on-the-Lake, Ontario, 
Canada. 
Maria Milosavljevic. 1997. Augmenting the user's 
knowledge via comparison. In Proceedings of the 
6th International Conference on User Modelling, 
pages 119-130, Sardinia, 2-5 June. 
Maria Milosavljevic. 1999. Maximising the Co- 
herence of Descriptions via Comparison. Ph.D. 
thesis, Macquarie University, Sydney, Australia. 
Scott Nowson. 1999. Acquiring ILEX for a Per- 
sonnel Domain. Honours Thesis, Artificial Intel- 
ligence, University of Edinburgh. 
Michael O'Donnell. 1996. Input specification i the 
wag sentence generation system. In Proceedings of 
the 8th International Workshop on Natural Lan- 
guage Generation, Herstmonceux Castle, UK, 13- 
15 June. 
140 - ' 
Demonstrat ion of ILEX 3.0 
Michae l  O 'Donne l l t  (micko@dai .ed.ac .uk) ,  
A l is ta i r  Knott:~ (a l ik@hermes.otago.ac .nz) ,  
Jon  Ober lander t  ( jon@cogsci .ed.ac.uk) ,  
Chr is  Mel l isht (chr ism@dai .ed.ac .uk)  
t Divis ion o f  Informat ics ,  Un ivers i ty  of  Ed inburgh .  
:~ Depar tment  of  Computer  Science ~ Otago  University.  
Abst rac t  
We will demonstrate the ILEX system, a system 
which dynamically generates descriptions of data- 
base objects for the web, adapting the description to 
the discourse context and user type. Among other 
improvements in version 3, the system now gener- 
ates from relational databases, and this demonstra- 
tion will focus on this ability. We will also show how 
incremental extensions to the domain semantics im- 
prove the quality of the text produced. 
1 In t roduct ion  
ILEX is a tool for dynamic browsing of database- 
defined information: it allows a user to browse 
through the information in a database using hyper- 
text. ILEX generates descriptions of a database ob- 
ject on the fly, taking into account he user's con- 
text of browsing. For more information on ILEX, 
see Knott et al (1997) and Mellish et al (1998). 
The demonstration will consist of generating a
series of texts, in each case adding in additional com- 
ponents of the domain semantics. This short paper 
should be read in conjunction with the full paper 
elsewhere in this volume. 
2 Generat ing  f rom Bare  Data  
We start initially with a relational database, as 
defined by a set of tab-delimited database files, plus 
some minimal semantics. As discussed in the paper, 
we use assume a relational database to consist of two 
types of files: 
1. Entity Files: each of which provides data for 
a particular entity type. Each row (or record) 
defines the attributes of a different entity. See 
figure 1. 
2. Link Files: where a particular attribute may 
have multiple fillers, we use link files to define 
the entity-entity relations. See figure 2. 
To generate from these files, the dolnain-editor 
needs to provide two additional resources: 
1. Data-type specification for each entity-file, a 
specification of what data-type the values in the 
~ Material 
silver 
enamel 
gold 
Figure 2: A Sample from a Link file 
. 
3. 
column are, e.g., string, entity-id, domain type, 
etc. 
Domain Taxonomy: detailing the taxonomic or- 
ganisation of the various classes of the entities. 
Mapping Domain taxonomy onto Upper Model: 
ILEX uses an Upper Model (a domain- 
independent semantic taxonomy, see Bateman 
(1990)), which supports the grammatical ex- 
pression of entities, e.g., selection of pronoun, 
differentiation between mass and count entities, 
between things and qualities, etc. We require 
that the basic types in the domain taxonomy 
are mapped onto the upper model, to allow the 
entities to be grammaticalised and lexicalised 
appropriately. 
With just this semantics, we can generate texts, 
although impoverished texts, such as: 
The class of J-997 is necklace. It's de- 
signer is Jessie M. King. It's date is 1905. 
Several tricks are needed to generate without a 
specified omain semantics: 
Use of standard clause templates: lacking any 
knowledge of how different attributes are to be 
expressed, the system-can only generate ach 
attribute using a standard template structures, 
such as the X of Y is Z or It's X is Z. The 
attribute names, e.g., Designer, Style, etc. can 
be assumed to work as the lexical head of the 
Subject. This ploy sometimes goes wrong, but 
in general works. (this approach borrowed from 
Dale et al (1998)). 
257 
ID Class 
J-997 brooch 
J~998: :neddace 
J-999 i necklace 
etc. I 
Designer Date Style Place Sponsor 
King01 11905 Art-Deco London Liberty01 
King01 '19116 - Art-Deco "London 
Chanel01 1910 Art-Noveau Paris 
Figure 1: A Sample from an Entity file 
* Referring to Entities: there are a number of 
strategies open for referring to entities. If the 
Name attribute.is.supplied-(a:defined- attribute 
within the ILEX system), then the system can 
use this for referring. Lacking a name, it is pos- 
sible for the system to form nominal references 
using the Class attr ibute of the entity (all en- 
tities in ILEX databases are required to have 
this attribute provided). We could thus gener- 
ate indefinite references such as a brooch as first 
mentions, and on subsequent mentions, gener- 
ate forms such as the brooch or the brooch whose 
designer is Jessie M. King. Without specifica- 
tion of which entities should be considered part 
of the general knowledge of the reader, we must 
assume all entities are initially unknown. 
* Fact Annotations: ILEX was designed to work 
with various extra information known about 
facts, such as the assumed level of interest to the 
current reader model, the importance of the fact 
to the system's educational agenda, and the as- 
sumed assimilation of the information (how well 
does the system believe the reader to already 
understand it). See the main paper for more 
details. 
Lacking this information, the system assumes 
an average value for interest and importance, 
and a 0 value for assimilation (totally un- 
known). 
With only default values, the system cannot 
customise the text to the particular user. It may 
provide information already well known by the 
user, and thus risking boring them. Also, there 
can be no selection of information to ensure that 
the more interesting and important information 
is provided on earlier pages (the reader may not 
bother to look at later pages). 
Other information (defeasible rules), which allows 
us to organise the material into complex rhetorical 
structure, is also missing. 
So, these tricks allow us to generate simple texts, 
consisting of a list of template-formatted clauses. 
3 Add ing  Express ion  in fo rmat ion  
In the next step, we will add in information about 
how the various attributes hould be expressed. This 
includes three main resources: 
1. Syntactic expression of attributes: for each at- 
tribute, we provide a specification of how the 
......... ~. ~.~ribu:te~should~be~-expressed. syntactically. 
2. Lexicalisation of domain types: by providing 
a lexicon, which maps domain types to lexical 
items, we avoid problems of using the domain 
type itself as the spelling. The lexical inform- 
ation allows correct generation of inflectional 
forms (e.g., of the plural for nouns, comparative 
or superlative forms for adjectives). 
3. Restrictive modifiers for referring expressions: 
In choosing restrictive modifiers for forming re- 
ferring expressions, ome facts work better than 
others. For instance, the brooch designed by 
King is more likely to refer adequately than the 
brooch which was 3 inches long. ILEX allows 
the user to state the preferential order for choos- 
ing restrictive modifiers. 
The addition of these resources will result in im- 
proved expression within the clauses, but not af- 
fect the text structure itself, which are still a list 
of clauses in random order. 
4 Add ing  User  Annotat ions  
In the next step, we add in the user model, which 
provides, for each attribute type, predicted user in- 
terest, importance for the system, and expected user 
assimilation. 
Using these values, ILEX can start to organise 
the text, placing important/interesting i formation 
on earlier pages, and avoiding information already 
known by tile user. 
5 Add ing  Defeas ib le  Ru les ,  S tor ies  
As a final step, we add in various resources which 
improve the texture of the text. 
o Defeasible Rules: ILEX allows the assertion 
of generalisations like most Art Deco jewels 
use enamel. These rules allow the genera- 
tion of complex rhetorical structures which in- 
dude Generalisation, Exemplification and Con- 
cession. The use of these relations improves tim 
quality of the text generated. 
* Stories: much of the information obtainable 
about tile domain is in natural language. Of- 
ten, the information is specific to a particular 
258 
entity, and as such, it would be a waste of time 
to reduce the in.formation i to ILEX's Pred-Arg 
knowledge structure, just to regenerate he text. 
Because of this, ILEX allows the association 
of canned text with a database ntity (e.g., J- 
999), or type of entity (e.g., jewels designed for 
Liberty). The text can then be included in the 
text when the entity or type of entity is men- 
tioned. 
The intermixing of generated and canned text 
improves the qual i ty of generated texts by 
providing more variety of structures, and al- 
lowing anecdotes, which would be difficult to 
model in terms of the knowledge representation 
system. 
6 Conc lus ion  
By showing incremental addition of domain spe- 
cification within the ILEX system, we have demon- 
strated that it is a system which can function with 
varying degrees of information. This allows domain 
developers to rapidly prototype a working system, 
after which they can concentrate on improving the 
quality of text in the directions they favour. 
Re ferences  
John Bateman. 1990. Upper modeling: organiz- 
ing knowledge for natural language processing. 
In Proceedings of the Fifth International Work- 
shop on Natural Language Generation, Pitts- 
burgh, June. 
Robert Dale, Stephen J Green, Maria Milosavljevic, 
CEcile Paris, Cornelia Verspoor, and Sandra Wil- 
liams. 1998. The realities of generating natural 
language from databases. In Proceedings of the 
11th Australian Joint Conference on Artificial In- 
telligence, Brisbane, Australia, 13-17 July. 
Alistair Knott, Michael O'Donnell, Jon Oberlander, 
and Chris Mellish. 1997. Defeasible rules in con- 
tent selection and text structuring. In Proceedings 
of the 6th European Workshop on Natural Lan- 
9uage Generation, Gerhard-Mercator University, 
Duisburg, Germany, March 24 - 26. 
Chris Mellish, Mick O'Donnell, Jon Oberlander, and 
Alistair Knott. 1998. An architecture for oppor- 
tunistic text generation. In Proceedings of the 
Ninth International Workshop on Natural Lan- 
guage Generation, Niagara-on-the-Lake, Ontario, 
Canada. 
259 
Speaker-independent context update rules for dialogue management
Samson de Jager, Nick Wright, Alistair Knott
Dept. of Computer Science
University of Otago
sdejager/nwright/alik@cs.otago.ac.nz
Abstract
This paper describes a dialogue manage-
ment system in which an attempt is made
to factor out a declarative theory of con-
text updates in dialogue from a procedural
theory of generating and interpreting ut-
terances in dialogue.
1 Background: declarative and procedural
resources for text processing
In computational linguistics, a very useful distinc-
tion can be drawn between declarative models of
language, which specify what constitutes a ?correct?
or ?proper? sentence or discourse, and procedural
models, which specify how a proper sentence or dis-
course can be interpreted or generated. This distinc-
tion is virtually ubiquitous in computational mod-
els of sentence structure and sentence processing.
In these models, a sentence grammar is a declar-
ative construct, and a sentence parser or sentence
generator consults the grammar in a systematic way
at each iteration in order to interpret or create sen-
tences. The idea of systematicity is very important.
For instance, in a chart parser the process of sentence
parsing is broken down into a sequence of procedu-
ral operations each of which has exactly the same
general form: a search of the set of grammar rules,
and the creation of a new chart edge if the search
is successful. In fact, the benefits of thinking of a
parser as a procedural module consulting a declar-
ative grammatical resource are most clearly seen in
the fact that the procedural component can be ex-
pressed systematically in this way.
The declarative/procedural distinction is also in-
creasingly common in computational treatments of
extended monologues. There are several overtly
declarative theories of the structure of such texts
(many of them stemming from the work of Mann
and Thompson (1988) and Grosz and Sidner
(1986)), and several models of text generation and
text interpretation which make reference to these
declarative theories (see e.g. Hovy (1993) and
Marcu (2000) for a summary of generation and in-
terpretation methods respectively). Again, the most
attractive feature of the declarative/procedural dis-
tinction is that the procedural algorithms envisaged
are very systematic.
In models of dialogue structure, a clean separa-
tion between declarative and procedural models has
proved more elusive. By analogy with the cases of
sentences and monologic discourse just described,
what is required is a declarative model of ?well-
formed dialogue?, together with procedural mecha-
nisms that consult this model in a systematic way to
produce and interpret contributions to a dialogue.
To begin with, what is a declarative theory of di-
alogue structure? In this paper, we will assume a
theoretical perspective in which dialogue moves are
represented as context-update operations (see e.g.
Traum et al(1999)). An utterance in a dialogue is
understood as a function which (when defined) takes
the current dialogue context and outputs a new dia-
logue context that constitutes the input to the next
utterance in the dialogue. The declarative theory of
dialogue coherence will therefore be a theory about
the legal ways in which the dialogue context can be
utterance 
interpretation
Dialogue
context
utterance
processing
utterance
processing
interlocutor (I)
utterance processing rules
A?s utterance
I?s utterance
utterance
generation
A B
A B
A C
Process A consults data from B
Process A writes data in B
Process A hands control to process C
Figure 1: A model of dialogue management factoring out a declarative component of ?utterance processing?.
updated.1 The question we focus on is: how can we
formulate a model of dialogue processing that makes
reference in a systematic way to this declarative the-
ory?
The difficulty is that ?dialogue processing? is not
a uniform activity. There are two very different pro-
cesses involved: one is utterance interpretation, and
the other is utterance generation. An agent par-
ticipating in a dialogue runs these two processes
roughly in alternation. If an algorithm for dialogue
processing is to be systematic in the way discussed
above, we expect it to consult the declarative theory
of context updates in the same way when process-
ing each utterance, whether it is being generated or
interpreted. But is this really possible? The pro-
cess of generating an utterance seems very different
from that of interpreting one. Perhaps most obvi-
ously, when an agent is generating an utterance, it
needs to work out what the content of this utterance
is, while when the agent is interpreting an utterance,
this content is handed to it on a plate, and it simply
has to work out how it is to be incorporated into the
current discourse context.
2 Dialogue management using
speaker-independent resources
This paper describes the dialogue management
scheme implemented in a dialogue system called
Te Kaitito2 (Knott et al, 2002; de Jager et al,
1Note that there is nothing procedural in the notion of an
update; it is just a way of characterising the meaning of an ut-
terance in a dialogue.
2Te Kaitito is a bilingual system, which supports dialogues
in either English or Ma?ori. Te Kaitito means ?the extempore
speaker? or ?the improviser?.
2002). In this scheme, an attempt is made to fac-
tor out a speaker-independent component of ut-
terance processing which is the same whether the
utterance is being generated or interpreted from a
speaker-dependent component of utterance pro-
cessing which is completely different for utterance
generation and utterance interpretation. Both of
these types of processing are thought of as opera-
tions which perform updates on the dialogue con-
text. The basic idea is that a speaker-dependent op-
eration is responsible for adding new information
about an utterance and its semantic content to the
dialogue context, while a speaker-independent oper-
ation consults a declarative model of dialogue struc-
ture to further process this information in the light of
what is already in the dialogue context. Imagine an
agent A participating in a dialogue with an interlocu-
tor I . When A is generating an utterance, we pro-
pose that A performs two completely separate opera-
tions. Firstly there is a speaker-dependent operation
of utterance generation, which is performed using
a collection of very procedural and very domain-
specific resources, including A?s current plans and
knowledge of the world. As a consequence of this
operation, a new utterance representation is added
to the dialogue context, and (as a side-effect) A
actually makes an utterance. Secondly, a speaker-
independent operation of utterance processing oc-
curs, in which a set of utterance processing rules
are consulted, and various further changes are made
to the dialogue context. Imagine now that A inter-
prets a response utterance coming from I . There
are again two separate operations. Firstly there is a
speaker-dependent operation of utterance interpre-
tation, involving sentence parsing, syntactic and se-
mantic disambiguation and so on. As a consequence
of this operation, again a new utterance represen-
tation is added to the dialogue context. Secondly,
another speaker-independent operation of utterance
processing occurs, in which exactly the same set of
utterance processing rules are consulted, and further
context updates are made. This picture of dialogue
processing is summarised in Figure 1. In the remain-
der of the paper, this picture will be picture will be
examined in more detail.
3 A DRT-based model of utterances and
dialogue context
The semantics of sentences in Te Kaitito are rep-
resented using Discourse Representation Structures
(DRSs: see (Kamp and Reyle, 1993)), both for sen-
tence generation and sentence interpretation. The
main extension of classical DRT is to incorporate a
treatment of presuppositions along the lines of that
given in van der Sandt (1992). In this treatment,
a sentence is modelled as an assertion DRS and a
set of presupposition DRSs, each of which spec-
ifies information which must be found in the dia-
logue context before the sentence?s assertion can be
further processed. Our system is a modification of
van der Sandt?s, in that presuppositions are used to
model the context-dependence of questions and an-
swers as well as phenomena like definite descrip-
tions and anaphora (see de Jager et al (2002) for
some motivation for this idea, and see Section 5 for
some examples).
The dialogue context in Te Kaitito is also repre-
sented as a DRS, with additional extensions roughly
along the lines of Poesio and Traum (1998). The
main extension is the idea of a discourse unit, which
is a sub-DRS gathering all of the semantic informa-
tion expressed in a single utterance by one of the
dialogue participants, the whole of which is treated
as a first-class object to which other predicates can
apply (e.g. asserts(speaker, U1)). This idea of
discourse units is also adopted in the MIDAS sys-
tem (Bos and Gabsdil, 2000). The other extension
of classical DRT (also following Poesio and Traum)
is that the dialogue context is modelled as a pair
of DRSs: a common ground DRS containing facts
which are mutually believed, and a stack DRS con-
taining ungrounded information (questions which
have not been fully answered, assertions which have
not been acknowledged, and so on). The stack DRS
is actually a sub-DRS of the common ground DRS
in our implementation, capturing the idea that refer-
ents within the common ground are available within
the stack DRS, but not vice versa.
Our model differs from that of Poesio and Traum
in one fairly small respect, because we also envis-
age a role for discourse units in text planning. One
of the functions of a dialogue in our system is to al-
low a user to author a knowledge base of facts that
serves as input to a text planning system (Knott and
Wright, 2003). A text planner needs to be able to
partition its knowledge base into ?utterance-sized?
sets of facts, which can then be structured into larger
texts in various ways. It is very useful if the system
can remember how the user performed these parti-
tions during the authoring dialogue. The discourse
units of Poesio and Traum seem almost tailor-made
for this purpose. However, not all of the predicates
inside a discourse unit should be retained in these
cached utterances. Some predicates are only present
in a sentence because they feature in referring ex-
pressions; since referring expressions will be differ-
ent depending on the context in which they are pro-
duced, we need a separate context-specific routine
to add these predicates to the semantics of an utter-
ance when a sentence is to be produced. See Knott
and Wright (2003) for more details about how these
cached utterances are obtained and used.
An example of a dialogue context is given in Fig-
ure 2. This is how the context would appear after the
house(X2)
U1,X1,X2
U1:
in(X1,X2)
Common ground DRS
Stack DRS
cat(X1)
Figure 2: A simple dialogue context
interlocutor I has uttered the sentence A cat was in
a house, and the agent A has successfully added this
information to the common ground. Note first that
the stack DRS is empty. Note also that the predicates
cat(X1) and house(X2) have been moved from the
utterance unit U1 into the top level of the common
ground, while the actual predication associated with
this sentence stays in the utterance. This ?unloading?
of material into the top-level DRS is superficially
similar to the operation performed in MIDAS sys-
tem for grounding an utterance. But in our case its
function is purely to do with the text-planning appli-
cation, and has nothing to do with making referents
accessible. We assume all material inside utterance
units is accessible as if it were in the outer DRS.
4 Utterance processing rules
In the top-level control loop of the dialogue man-
ager described in Section 2, when an utterance is
interpreted or generated (using speaker-specific pro-
cesses), a new utterance representation is added
to the stack DRS. An utterance representation con-
sists of a discourse unit as described in Section 3,
plus two kinds of predicate about the utterance rep-
resented by this unit: firstly a predicate specifying
what dialogue act it performs, and secondly predi-
cates specifying which variables it contains. After
this comes the speaker-independent operation of ut-
terance processing introduced in Section 2.
Utterance processing is modelled as a cyclical
process of applying context update rules, in the kind
of way envisaged in the MIDAS system (Bos and
Gabsdil, 2000). An utterance processing rule con-
tains a condition to look for in the dialogue context,
and an action, which is an update operation on the
context. At each cycle, the set of rules is searched,
and the first rule whose condition matches has its
action executed. The cycle is repeated until no rules
trigger.
There is no reason a priori why speaker-
independent context update operations need be ap-
plied cyclically by a rule interpreter. But there are
several reasons why one might want to use a cycli-
cal scheme. For Bos and Gabsdil (2000), the pri-
mary reason is that updating the context involves a
great deal of reasoning, and can in fact be construed
as a kind of reasoning. Reasoning is naturally mod-
elled as the cyclical application of rules of inference,
and hence a cyclical framework for updates seems
quite natural. In our system, however, the dialogue
manager does not invoke a general-purpose theorem
prover.3 We use a cyclical scheme for several sepa-
3While this is of course limiting for a practical system, we
rate reasons.
Firstly, there are dialogue moves which decom-
pose naturally into sequences of smaller moves,
some of which can also be made individually. For
instance, an acknowledgement can be given explic-
itly (using an utterance like Okay), or implicitly, by
making a new forward-looking utterance. It makes
sense to model the implicit acknowledgement in this
latter case using the same rule used to model the ex-
plicit one. To do this requires a cyclical scheme for
making context updates.
Secondly, there are dialogue moves which decom-
pose into sub-moves which have specific verbal and
nonverbal reflexes. For instance, when a speaker is
processing an incoming question utterance, she first
of all has to recognise that it is a question. When
this happens, the agent might furrow her brow, or
utter a filler like Hmm!, and only after some time ac-
tually respond to the question. Such actions are con-
veniently modelled as overt side-effects of the appli-
cation of cyclical context-update rules.4
Perhaps most importantly, the cyclical applica-
tion of context-update rules has the same kind of
systematicity as is found in the operation of a sen-
tence parser or a sentence generator. In Section 1 it
was argued that the benefits of a separation between
declarative and procedural resources are largely due
to the systematic iterative or procedural recursive al-
gorithms which this separation permits. Our sug-
gestion is to think of the set of utterance-processing
rules as a declarative theory of ?legal dialogue con-
text updates?, and to think of the rule interpreter cy-
cling on these rules as the dialogue equivalent of a
sentence generation algorithm.
5 A simple worked example
In this section, we will give some examples of the
utterance processing rules used in Te Kaitito?s dia-
logue manager, and explain how they are used. The
examples relate to the very simple dialogue given in
Figure 3. After the first two utterances, the dialogue
context will be the one which was shown in Figure 2.
feel that architectures for dialogue management can be studied
relatively independently from issues to do with general-purpose
reasoning techniques.
4Te Kaitito has in fact been used as the back-end of an an-
imated conversational agent which performs a few simple non-
verbal strategies of this kind. For details see (King et al, 2003).
We take up the story when the agent?s interlocutor I
1 I A cat was in a house
2 A Okay
3 I The cat barked
4 A Okay
Figure 3: An example dialogue
makes the second assertion. After this utterance has
been interpreted, the dialogue context is as shown in
Figure 4. Basically, a new sentence DRS has arrived
house(X2)
U1,X1,X2
U1:
in(X1,X2)
Common ground DRS
Stack DRS
U2:
U2
barked(x1) cat(x1)
x1
var?inside(x1, U2)
cat(X1)
assertion?act(U2)
Figure 4: Context after interpretation of Utterance 3
in the stack DRS. Note that the sentence contains a
presupposition DRS, which is given in dashed lines.
Note also the predicates specifying the dialogue act
performed by the utterance (which in this case is de-
rived from the syntax of the sentence), and detailing
which variables are mentioned in it.
Speaker-independent utterance processing rules
now fire. The first rule to fire in this case is a rule
which attempts to resolve the presuppositions of the
utterance. (Recall that presupposition resolution is
considered part of dialogue management rather than
simple sentence interpretation; again see de Jager et
al. (2002) for details.) After presupposition reso-
lution, the context is as shown in Figure 5; the pre-
supposition box disappears, and the variable x1 is
bound to X1. In fact, no further utterance process-
ing rules will fire in this cycle.
A speaker-specific process of response generation
is now invoked. In this case, the process specifies
that an assertion whose presuppositions have been
fully resolved should be acknowledged, and conse-
quently an acknowledgement is given to the speaker.
A semantic representation of this acknowledgement
Stack DRS
U2:
U2
barked(X1)
assertion?act(U2)
var?inside(X1, U2)
house(X2)
U1,X1,X2
U1:
in(X1,X2)
Common ground DRS
cat(X1)
Figure 5: Context after processing of Utterance 3
sentence is added to the stack DRS, as shown in
Figure 6. Notice that an acknowledgement is rep-
resented semantically as presupposing a forward-
looking dialogue act. (In Te Kaitito?s type hierarchy,
an assertion is one such act.)
Stack DRS
U2:
U2
var?inside(x1, U2)
barked(X1)
assertion?act(U2)
U3
U3:
forward?act(x1)
x1
acknowledgement?act(U3)
house(X2)
U1,X1,X2
U1:
in(X1,X2)
Common ground DRS
cat(X1)
Figure 6: Context after generation of Utterance 4
The agent has now uttered the acknowledgement;
it remains for it to bring its dialogue context up
to date, using the same utterance processing rules
as are used when processing the interlocutor?s ut-
terance. The first rule is presupposition resolution
again. Crucially, the same presupposition resolu-
tion routine is invoked now as was invoked when the
interlocutor?s assertion was being processed. After
this routine, the presupposed forward-looking dia-
logue act is bound to the assertion utterance (which
in Te Kaitito?s type hierarchy is a type of forward-
looking act), the presupposition box is removed, and
an explicit statement of the relationship between the
assertion U2 and the acknowledgement U3 is added.
The dialogue context is now as shown in Figure 7.
house(X2)
U1,X1,X2
U1:
in(X1,X2)
Common ground DRS
cat(X1) Stack DRS
U2:
U2
barked(X1)
assertion?act(U2)
U3
U3:
var?inside(X1, U2)
acknowl?ment?of(U3, U2)
Figure 7: Context after processing Utterance 4 (1)
Finally, a second utterance-processing rule re-
moves the acknowledgement altogether and trans-
fers the assertion U2 to the common ground, as
shown in Figure 8.
U2:
barked(X1)
house(X2)
U1:
in(X1,X2)
Common ground DRS
cat(X1)
U1,X1,X2, U2
Stack DRS
Figure 8: Context after processing Utterance 4 (2)
6 Discussion
The important thing about the example just worked
through is that the utterance-processing rules which
operate after each utterance is added to the stack
would work in just the same way if the agent had
been making an assertion and the interlocutor had
been acknowledging it. The Te Kaitito system is
able to generate assertions as well as interpret them,
and is able to interpret acknowledgements as well
as generate them. For instance, if the user asks a
question, the system has to answer it with an as-
sertive statement, and it has to be able to interpret an
acknowledgement of this answer (whether the user
gives this explicitly, or implicitly).
An example dialogue to illustrate this alternative
pattern is given in Figure 9. During the course of
5 I Which cat barked?
6 A It was the blue cat.
7 I Okay.
Figure 9: Dialogue with an acknowledgement by I
this dialogue, the utterance processing rules which
update the dialogue context after the system?s asser-
tion (Utterance 6) are the same as those which up-
date the dialogue context after the assertion by the
user in Utterance 3 of Figure 3. And the utterance
processing rules which update the context after the
user?s acknowledgement (Utterance 7) are the same
as the rules which update the context after the sys-
tem?s acknowledgement of the user?s assertion in
Utterance 4 of Figure 3. The process of generating
an assertion is quite different from that of interpret-
ing one, as emphasised in Section 1, and the same
goes for the processes of interpreting and generating
an acknowledgement, but what this system attempts
to do is to factor out the components of these two
operations which are the same, and which just con-
cern how the semantic specification of an incoming
utterance can be incorporated into the current dia-
logue context. To the extent that this is possible,
an attractive separation can be achieved between a
declarative (rule-based) theory of context updates in
dialogue and a procedural theory of utterance inter-
pretation and generation.
7 Acknowledgements
This work was funded by University of Otago Re-
search Grant MFHB10, and by the NZ Founda-
tion for Research in Science & Technology grant
UOOX02.
References
J Bos and M Gabsdil. 2000. First-order inference and the
interpretation of questions and answers. In Proceed-
ings of Gotalog 2000. Fourth Workshop on the Seman-
tics and Pragmatics of Dialogue., pages 43?50.
S de Jager, A Knott, and I Bayard. 2002. A DRT-based
framework for presuppositions in dialogue manage-
ment. In Proceedings of the 6th workshop on the se-
mantics and pragmatics of dialogue (EDILOG 2002),
Edinburgh.
B J Grosz and C L Sidner. 1986. Attention, intentions,
and the structure of discourse. Computational Lin-
guistics, pages 175?203.
E Hovy. 1993. Automated discourse generation using
discourse structure relations. Artificial Intelligence,
63:341?385.
H Kamp and U Reyle. 1993. From discourse to logic.
Kluwer Academic Publishers, Dordrecht.
S King, A Knott, and B McCane. 2003. Language-
driven nonverbal communication in a bilingual con-
versational agent. In Proceedings of the 16th Interna-
tional Conference on Computer Animation and Social
Agents (CASA).
A Knott and N Wright. 2003. A dialogue-based knowl-
edge authoring system for text generation. In AAAI
Spring Symposium on Natural Language Generation
in Spoken and Written Dialogue, Stanford, CA.
A Knott, I Bayard, S de Jager, and N Wright. 2002. An
architecture for bilingual and bidirectional nlp. In Pro-
ceedings of the 2nd Australasian Natural Language
Processing Workshop (ANLP 2002).
W C Mann and S A Thompson. 1988. Rhetorical
structure theory: A theory of text organization. Text,
8(3):243?281.
D Marcu. 2000. The Theory and Practice of Discourse
Parsing and Summarization. MIT Press, Cambridge,
MA.
M Poesio and D Traum. 1998. Towards an axiomati-
zation of dialogue acts. In J Hulstijn and A Nijholt,
editors, Proceedings of the Twente Workshop on the
Formal Semantics and Pragmatics of Dialogues (13th
Twente Workshop on Language Technology), pages
207?222.
D Traum, Bos J, R Cooper, S Larsson, I Lewin, C Mathe-
son, and M Poesio. 1999. A model of dialogue moves
and information state revision. TRINDI project deliv-
erable.
R Van der Sandt. 1992. Presupposition projection as
anaphora resolution. Journal of Semantics, 9:333?
377.
