Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1137?1146,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Generating Aspect-oriented Multi-Document Summarization with
Event-aspect model
Peng Li1 and Yinglin Wang1 and Wei Gao2and Jing Jiang3
1 Department of Computer Science and Engineering, Shanghai Jiao Tong University
2 Department of Systems Engineering and Engineering Management, Chinese University of Hong Kong
3 School of Information Systems, Singapore Management University
{lipeng, ylwang@sjtu.edu.cn} {wgao@se.cuhk.edu.hk} {jingjiang@smu.edu.sg}
Abstract
In this paper, we propose a novel approach to
automatic generation of aspect-oriented sum-
maries from multiple documents. We first de-
velop an event-aspect LDA model to cluster
sentences into aspects. We then use extend-
ed LexRank algorithm to rank the sentences
in each cluster. We use Integer Linear Pro-
gramming for sentence selection. Key features
of our method include automatic grouping of
semantically related sentences and sentence
ranking based on extension of random walk
model. Also, we implement a new sentence
compression algorithm which use dependency
tree instead of parser tree. We compare our
method with four baseline methods. Quantita-
tive evaluation based on Rouge metric demon-
strates the effectiveness and advantages of our
method.
1 Introduction
In recent years, there has been much interest in
the task of multi-document summarization. In this
paper, we study the task of automatically generat-
ing aspect-oriented summaries from multiple docu-
ments. The goal of aspect-oriented summarization
is to present the most important content to the us-
er in a condensed form and a well-organized struc-
ture to satisfy the user?s needs. A summary should
follow a readable structure and cover all the aspect-
s users are interested in. For example, a summary
about natural disasters should include aspects about
what happened, when/where it happened, reasons,
damages, rescue efforts, etc. and these aspects may
be scattered in multiple articles written by different
news agencies. Our goal is to automatically collect
aspects and construct summaries from multiple doc-
uments.
Aspect-oriented summarization can be used in
many scenarios. First of all, it can be used to gener-
ateWikipedia-like summary articles, especially used
to generate introduction sections that summarizes
the subject of articles before the table of contents
and other elaborate sections. Second, opinionat-
ed text often contains multiple viewpoints about an
issue generated by different people. Summarizing
these multiple opinions can help people easily di-
gest them. Furthermore, combined with search en-
gines and question&answering systems, we can bet-
ter organize the summary content based on aspects
to improve user experience.
Despite its usefulness, the problem of modeling
domain specific aspects for multi-document summa-
rization has not been well studied. The most relevant
work is by (Haghighi and Vanderwende, 2009) on
exploring content models for multi-document sum-
marization. They proposed a HIERSUM model for
finding the subtopics or aspects which are combined
by using KL-divergence criterion for selecting rel-
evant sentences. They introduced a general con-
tent distribution and several specific content distri-
butions to discover the topic and aspects for a s-
ingle document collection. However, the aspects
may be shared not only across documents in a sin-
gle collection, but also across documents in different
topic-related collections. Their model is conceptual-
ly inadequate for simultaneously summarizing mul-
tiple topic-related document collections. Further-
more, their sentence selection method based on KL-
divergence cannot prevent redundancy across differ-
ent aspects.
In this paper, we study how to overcome these
1137
limitations. We hypothesize that comparatively
summarizing topics across similar collections can
improve the effectiveness of aspect-oriented multi-
document summarization. We propose a novel
extraction-based approach which consists of four
main steps listed below:
Sentence Clustering: Our goal in this step is to
automatically identify the different aspects and clus-
ter sentences into aspects (See Section 2). We sub-
stantially extend the entity-aspect model in (Li et al,
2010) for generating general sentence clusters.
Sentence Ranking: In this step, we use an exten-
sion of LexRank algorithm proposed by (Paul et al,
2010) to score representative sentences in each clus-
ter (See Section 3).
Sentence Compression: In this step, we aim to
improve the linguistic quality of the summaries by
simplifying the sentence expressions. We prune sen-
tences using grammatical relations defined on de-
pendency trees for recognizing important clauses
and removing redundant subtrees (See Section 4).
Sentence Selection: Finally, we select one com-
pressed version of the sentences from each aspec-
t cluster. We use Integer Linear Programming
(ILP) algorithm, which optimizes a global objective
function, for sentence selection (McDonald, 2007;
Gillick and Favre, 2009; Sauper and Barzilay, 2009)
(See Section 5).
We evaluate our method using TAC2010 Guided
Summarization task data sets1 (Section 6). Our eval-
uation shows that our method obtains better ROUGE
recall score compared with four baseline methods,
and it also achieve reasonably high-quality aspec-
t clusters in terms of purity.
2 Sentence Clustering
In this step, our goal is to discover event aspects con-
tained in a document set and cluster sentences in-
to aspects. Here we substantially extend the entity-
aspect model in Li et al (2010) and refer to it as
event-aspect model. The main difference between
our event-aspect model and entity-aspect model is
that we introduce an additional layer of event topics
and the separation of general and specific aspects.
1http://www.nist.gov/tac/2010/
Summarization/
Our extension is based upon the following ob-
servations. For example, specific events like
?Columbine Massacre? and ?Malaysia Resort Ab-
duction? can be related to the ?Attack? topic. Each
event consists of multiple articles written by dif-
ferent news agencies. Interesting aspects may in-
clude ?what happened, when, where, perpetrators,
reasons, who affected, damages and countermea-
sures,? etc2. We compared the ?Columbine Mas-
sacre? and ?Malaysia Resort Abduction? data set-
s and found 5 different kinds of words in the text:
(1) stop words that occur frequently in any docu-
ment collection; (2) general content words describ-
ing ?damages? or ?countermeasures? aspect of at-
tacks; (3) specific content words describing ?what
happened?, ?who affected? or ?where? aspect of the
concrete event; (4) background words describing the
general topic of ?Attack?; (5) words that are local to
a single document and do not appear across different
documents. Table 1 shows four sentences related to
two major aspects. We found that the entity-aspect
model does not have enough capacity to cluster sen-
tences into aspects (See Section 6). So we introduce
additional layer to improve the effectiveness of sen-
tence clustering. We also found that their one aspect
per sentence assumption is not very strong in this
scenario. Although a sentence may belong to a sin-
gle general aspect, it still contains multiple specific
aspect words like second sentence in Table 1. There-
fore, We assume that each sentence belongs to both
a general aspect and a specific aspect.
2.1 Event-Aspect Model
Stop words can be ignored by LDA model because
they can be easily identified using a standard stop
word list. Suppose that for a given event topic, there
are in total C specific events for which we need to
simultaneously generate summaries. We can assume
four kinds of unigram language models (i.e. multi-
nomial word distributions). For each event topic,
there is a background model ?B that generates words
commonly used in all documents, and there are AG
general aspect models ?ga (1 ? ga ? AG), where
AG is the number of general aspects. For each spe-
cific event in a topic, there are AS specific aspect
2http://www.nist.gov/tac/2010/
Summarization/Guided-Summ.2010.guidelines.
html
1138
countermeasures
Police/GA are/S close/B to/S identifying/GA someone/B responsible/GA
for/S the/S attack/B .
Investigators/GA do/S not/S know/B how/S many/S suspects/SA
they/S are/S looking/B for/S, but/S reported/B progress/B toward/S
identifying/GA one/S of/S the/S bombers/SA .
what happened, when, where
During/S the/S morning/SA rush/D hour/D on/S July/SA 7/SA terrorists/B
exploded/SA bombs/SA on/S three/D London/SA subway/D trains/SA and/S a/S
double-decker/D bus/SA .
Four/D coordinated/B bombings/SA struck/B central/B London/SA on/SA
July/SA 7/SA, three/D in/S subway/D cars/SA and/S one/D on/S a/S bus/SA .
Table 1: Four sentences on ?COUNTERMEASURES? and ?What, When, Where? aspects from the ?Attack? topic. S:
stop word. B: background word. GA: general aspect word. SA: specific aspect word. D: document word.
models ?sa (1 ? sa ? AS), where AS is the num-
ber of specific aspects, and also there are D doc-
ument models ?d (1 ? d ? D), where D is the
number of documents in this collection. We assume
that these word distributions have a uniform Dirich-
let prior with parameter ?.
We introduce a level distribution ? that control-
s whether we choose a word from ?ga or ?sa. ?
is sampled from Beta(?0, ?1) distribution. We also
introduce an aspect distribution ? that controls how
often a general or a specific aspect occurs in the col-
lection, where ? is sampled from another Dirichlet
prior with parameter ?. There is also a multinomi-
al distribution ? that controls in each sentence how
often we encounter a background word, a document
word, or an aspect word. ? has a Dirichlet prior with
parameter ?.
Let Sd denote the number of sentences in docu-
ment d, Nd,s denote the number of words (after stop
word removal) in sentence s of document d, and
wd,s,n denote the n?th word in this sentence. We
introduce hidden variables zgad,s and zsad,s to indicatethat a sentence s of document d belongs to which
general or specific aspects . We introduce hidden
variables yd,s,n for each word to indicate whether a
word is generated from the background model, the
document model, or the aspect model. We also intro-
duce hidden variables ld,s,n to indicate whether the
n?th word in sentence s of document d is generated
from the general aspect model. Figure 1 describes
the process of generating the whole document col-
lection. The plate notation of the model is shown in
Figure 2. Note that the values of ?0, ?1, ?1, ?2, ?
and ? are fixed. The number of general and specific
aspects AG and AS are also empirically set.
Given a document collection, i.e. the set of all
wd,s,n, our goal is to find the most likely assignmen-
t of zgad,s, zsad,s, yd,s,n and ld,s,n that maximizes dis-tribution p(z,y, l|w;?, ?, ?, ?), where z, y, l and w
represent the set of all z, y, l andw variables, respec-
tively. With the assignment, sentences are naturally
clustered into aspects, and words are labeled as ei-
ther a background word, a document word, a general
aspect word or a specific aspect word.
Inference can be done with Gibbs sampling,
which is commonly used in LDA models (Griffiths
and Steyvers, 2004).
In our experiments, we set ?1 = 5, ?2 = 3,
? = 0.01, ? = 20, ?1 = 10 and ?2 = 10. We
run 100 burn-in iterations through all documents in
a collection to stabilize the distribution of z and y
before collecting samples. We take 10 samples with
a gap of 10 iterations between two samples, and av-
erage over these 10 samples to get the estimation for
the parameters.
After estimating all the distributions, we can find
the values of each zgad,s and zsad,s that gives us sen-tences clustered into general and specific aspects.
3 Sentence Ranking
In this step, we want to order the clustered sen-
tences so that the representative sentences can be
ranked higher in each aspect. Inspired by Paul et
al. (2010), we use an extended LexRank algorithm
to obtain top ranked sentences. LexRank (Erkan and
Radev, 2004) algorithm defines a random walk mod-
1139
1. Draw ?1 ? Dir(?1), ?2 ? Dir(?2), ? ? Dir(?)
Draw ? ? Beta(?0, ?1)
2. For each event topic, there is a background model
?B, and there are general aspect ga, where 1 ?
ga ? AG
(a) draw ?B ? Dir(?)
(b) draw ?ga ? Dir(?)
3. For each document collection, there are specific
aspect sa, where 1 ? sa ? AS
(a) draw ?sa ? Dir(?)
4. For each document d = 1, . . . , D,
(a) draw ?d ? Dir(?)
(b) for each sentence s = 1, . . . , Sd
i. draw zga ? Multi(?1)
ii. draw zsa ? Multi(?2)
iii. for each word n = 1, . . . , Nd,s
A. draw ld,s,n ? Binomial(?)
B. draw yd,s,n ? Multi(?)
C. drawwd,s,n ? Multi(?B) if yd,s,n =
1, wd,s,n ? Multi(?d) if yd,s,n = 2,
wd,s,n ? Multi(?z
sa
d,s) if yd,s,n =
3 and ld,s,n = 1 or wd,s,n ?
Multi(?z
ga
d,s) if yd,s,n = 3 and
ld,s,n = 0
Figure 1: The document generation process.
el on top of a graph that represents sentences to be
summarized as nodes and their similarities as edges.
The LexRank score of a sentence gives the expected
probability that a random walk will visit that sen-
tence in the long run. A variant is called continu-
ous LexRank improved LexRank by making use of
the strength of the similarity links. The continuous
LexRank score can be computed using the following
formula:
L(u) = dN + (1 ? d)
?
v?adj[u]
p(u|v)L(v)
whereL(u) is the LexRank value of sentence u,N is
the total number of nodes in the graph, d is a damp-
ing factor for the convergence of the method, and
p(u|v) is the jumping probability between sentence
u and its neighboring sentence v. p(u|v) is defined
using content similarity function sim(u, v) between
two sentences:
T
yd? d
SD sd ,wSA?
?
C
?
pi ?
?
gaz saz
l
?
GA
B?
1? 2?
1? 2?
Figure 2: The event-aspect model.
p(u|v) = sim(u, v)?
z?adj[v] sim(z, v)
The major extension is to modify this jumping
probability so as to favor visiting representative sen-
tences. More specifically, we scale sim(u, v) by the
likelihood that the two sentences represent the same
general aspect ga or specific aspect sa:
sim?(u, v) = sim(u, v)[
AG?
ga=1
P (ga|u)P (ga|v)
+
AS?
sa=1
P (sa|u)P (sa|v)]
where the value P (ga|u) and P (sa|u) can be
computed by our event-aspect model. We define
sim(u, v) as the tf ? idf weighted cosine similar-
ity between two sentences.
We found that sentence ranking is better con-
ducted before the compression because the pre-
compressed sentences are more informative and the
similarity function in LexRank can be better off with
the complete information.
4 Sentence Compression
It has been shown that sentence compression can
improve linguistic quality of summaries (Zajic et
al., 2007; Gillick et al, 2010). Commonly used
?Syntactic parse and trim? approach may produce
poor compression results. For example, given the
sentence ?We have friends whose children go to
Columbine, the freshman said?, the procedure tries
to remove the clause ?the freshman said? from the
parse tree by using the ?SBAR? label to locate the
1140
clause, and will result in ?whose children go to
Columbine?, which is not adequate. Furthermore,
some important temporal modifier, numeric modifier
and clausal complement need to be retained because
they reflect content aspects of the summary. There-
fore, we propose the ?dependency parse and trim?
approach, which prunes sentences based on depen-
dency tree representations, using English grammati-
cal relations to recognize clauses and remove redun-
dant structures. Table 2 shows two examples by re-
moving redundant auxiliary clauses. Below is the
sentence compression procedure:
1. Select possible subtree root nodes using gram-
matical relations, such as clausal complement,
complementizer, or parataxis 3.
2. Decide which subtree root node can be the root
of clause. If this root contains maximum num-
ber of child nodes and the collection of all child
edges include object or auxiliary relations, it is
selected as the root node.
3. Remove redundant modifiers such as adverbial-
s, relative clause modifiers and abbreviations,
participials and infinitive modifiers.
4. Traverse the subtrees and generate all possible
compression alternatives using the subtree root
node, then keep the top two longest sub sen-
tences.
5. Drop the sub sentences shorter than 5 words.
5 Sentence Selection
After sentence pruning, we prepare for the final
event summary generation process. In this step, we
select one compressed version of the sentence from
each aspect cluster. To avoid redundancy between
aspects, we use Integer Linear Programming to opti-
mize a global objective function for sentence selec-
tion. Inspired by (Sauper and Barzilay, 2009), we
formulate the optimization problem based on sen-
tence ranking information. More specifically, we
3The parataxis relation is a relation between the main verb
of a clause and other sentential elements, such as a sentential
parenthetical, colon, or semicolon
Original Compressed
When rescue workers
arrived, they said, on-
ly one of his limbs was
visible.
When rescue workers
arrived, only one of his
limbs was visible.
Two days earlier, a
massacre by two s-
tudents at Columbine
High, whose teams are
called the Rebels, left
15 people dead and
dozens wounded.
Two days earlier, a
massacre by two stu-
dents at Columbine
High, left 15 peo-
ple dead and dozens
wounded.
Table 2: Example compressed sentences.
would like to select exactly one compressed sen-
tence which receives the highest possible ranking s-
core from each aspect cluster subject to a series of
constraints, such as redundancy and length. We em-
ployed lp solver 4, an efficient mixed integer pro-
gramming solver using the Branch-and-Bound algo-
rithm to select sentences.
Assume that there are in total K aspects in an
event topic. For each aspect j, there are in total R
ranked sentences. The variables Sjl is a binary indi-
cator of the sentence. That is, Sjl= 1 if the sentence
is included in the final summary, and Sjl = 0 other-
wise. l is the ranked position of the sentence in this
aspect cluster.
Objective Function
Top ranked sentences are the most relevant corre-
sponding to the related aspects which we want to in-
clude in the final summary. Thus we try to minimize
the ranks of the sentences to improve the overall re-
sponsiveness.
min(
K?
j=1
Rj?
l=1
l ? Sjl)
Exclusivity Constraints
To prevent redundancy in each aspect, we just
choose one sentence from each general or specific
aspect cluster. The constraint is formulated as fol-
lows:
Rj?
l=1
Sjl = 1 ?j ? {1 . . .K}
4http://lpsolve.sourceforge.net/5.5/
1141
Redundancy Constraints
We also want to prevent redundancy across differ-
ent aspects. If sentence-similarity sim(sjl, sj?l?) be-
tween sentence sjl and sj?l? is above 0.5, then we
drop the pair and choose one sentence ranked higher
from the pair otherwise. This constraint is formulat-
ed as follows:
(Sjl + Sj?l?) ? sim(sjl, sj?l?) ? 0.5
?j, j? ? {1 . . .K}?l ? {1 . . . Rj}?l? ? {1 . . . Rj?}
Length Constraints
We add this constraint to ensure that the length of
the final summary is limited to L words.
K?
j=1
Rj?
l=1
lenjl ? Sjl ? L
where lenjl is the length of Sjl.
6 Evaluation
In order to systematically evaluate our method, we
want to check (1) whether the whole system is effec-
tive, which means to quantitatively evaluate summa-
ry quality, and (2) whether individual components
like clustering and compression algorithms are use-
ful.
6.1 Data
We use TAC2010 Summarization task data set for
the summary content evaluation. This data set pro-
vides 46 events. Each event falls into a predefined
event topic. Each specific event includes an even-
t statement and 20 relevant newswire articles which
have been divided into 2 sets: Document Set A and
Document Set B. Each document set has 10 docu-
ments, and all the documents in Set A chronologi-
cally precede the documents in Set B. We just use
document Set A for our task. Assessors wrote mod-
el summaries for each event, so we can compare
our automatic generated summaries with the model
summaries. We combine topic related data sets to-
gether, then these data sets simultaneously annotated
by our Event-aspect model. After labeling process,
we run sentence ranking, compression and selection
module to get final aspect-oriented summarizations.
6.2 Quality of summary
We use the ROUGE (Lin and Hovy, 2003) metric for
measuring the summarization system performance.
Ideally, a summarization criterion should be more
recall oriented. So the average recall of ROUGE-
1, ROUGE-2, ROUGE-SU4, ROUGE-W-1.2 and
ROUGE-L were computed by running ROUGE-
1.5.5 with stemming but no removal of stop word-
s. We compare our method with the following four
baseline methods.
Baseline 1
In this baseline, we try to compare different sen-
tence clustering algorithms in the multi-document
summarization scenario. First, we use CLUTO 5 to
do K-means clustering. Then we try entity-aspect
model proposed by Li et al (2010) to do sentence
clustering. Entity-aspect model is similar with ?HI-
ERSUM? content model proposed by Haghighi and
Vanderwende (2009). We use the same ranking,
compression, and selection components to generate
aspect-oriented summaries for comparison.
Baseline 2
In this baseline, we compare our method with
traditional ranking and selection summary genera-
tion framework (Erkan and Radev, 2004; Nenkova
and Vanderwende, 2005) to show that our sentence
clustering component is necessary in aspect-oriented
summarization system. Also we want check whether
sentence ranking combined with greedy based sen-
tence selection can prevent redundancy effective-
ly. We follow LexRank based sentence ranking
combined with greedy sentence selection methods.
We implement two greedy algorithms (Zhang et al,
2008; Paul et al, 2010). One is to select the top
ranked sentence simultaneously by removing 10 re-
dundant neighbor sentences from the sentence sim-
ilarity graph if the summary length is less then 100
words. This is repeated until the graph cannot be
partitioned. The similarity graph building threshold
is 0.3, damping factor is 0.2 and error tolerance for
Power Method in LexRank is 0.1. The other is to se-
lect top ranked sentences as long as the redundancy
score (similarity) between a candidate sentence and
5http://glaros.dtc.umn.edu/gkhome/cluto/
cluto/overview
1142
current summary is under 0.5. This is repeated until
the summary reaches a 100 word length limit.
Baseline 3
In this baseline, we compare our ILP based sen-
tence selection with KL-divergence based sentence
selection. The KL-divergence formula we use is be-
low,
KL(PS ||QD) =
?
w
P (w) log P (w)Q(w)
where P (S) is the empirical unigram distribution of
the candidate summary S, and Q(D) is the unigram
distribution of document collection D. We only re-
placed our selection method with the KL-divergence
selection method. Other parts are the same. After
ranking sentences for each aspect, we add the sen-
tence with the highest ranking score from each as-
pect sentence cluster as long as the KL-divergence
between candidate and current summary does not
decrease. This is repeated until the summary reach-
es a 100 word length limit. To our knowledge, this
is the first work to directly compare Integer Lin-
ear Programming based sentence selection with KL-
divergence based sentence selection in summariza-
tion generation framework.
Baseline 4
In this baseline, we directly compare our method
with ?HIERSUM? proposed by (Haghighi and Van-
derwende, 2009). As in Baseline 1, we use entity-
aspect model to approximate ?HIERSUM? mod-
el. We replace unigram distribution of P (w) in
KL-divergence with learned distribution estimated
by ?HIERSUM? model. The KL-divergence based
greedy sentence selection algorithm is similar to
Baseline 3.
For fair comparison, Baselines 1, 2, 3 and 4 use
the same sentence compression algorithm and have
the summary length no more then 100 words. In
Table 3, we show the average ROUGE recall of 46
summaries generated by our method and four base-
line methods. We can see that our method gives
better Rouge recall measures then the four baseline
methods. For BL-1, we can see that LDA-based sen-
tence clustering is better then k-means. For BL-2,
we can see that traditional ranking plus greedy selec-
tion summary generation framework is not suitable
for the aspect-oriented summarization task. More
specifically, greedy-based sentence selection can not
prevent redundancy effectively. BL-3 evaluation re-
sults showed that ILP-based sentence selection is
better then KL-divergence selection in terms of pre-
venting redundancy across different aspects. The
measurement performance between BL-3 and BL-
4 is close. They use the same KL-divergence based
sentence selection, but topic model they use are d-
ifferent, and also BL-3 has a sentence ranking pro-
cess. The Rouge recall of our method is better than
BL-4. It is expected because our event-aspect mod-
el can better find the aspects and also prove that
our LexRank based sentence ranking combined with
ILP-based sentence selection can prevent redundan-
cy.
Due to TAC2010 summarization community just
compute ROUGE-2 and ROUGE-SU4 metrics for
participants, our ROUGE-2 metric ranked 11 out
of 23, ROUGE-SU4 metric ranked 12 out of 23.
They use MEAD6 as their baseline approach. The
ROUGE-2 score of our approach achieve 0.06508
higher than MEAD?s 0.05929. The ROUGE-SU4 s-
core of our approach achieve 0.10146 higher than
MEAD?s 0.09112. Many systems that get high-
er performances leverage domain knowledge bases
like Wikipedia or training data, but we didn?t. The
advantage of our method is that we generate sum-
maries with totally unsupervised framework and this
approach is domain adaptive.
6.3 Quality of aspect-oriented sentence clusters
To judge the quality of the aspect-oriented sentence
clusters, we ask the human judges to group the
ground truth sentences based on the aspect related-
ness in each event topic. We then compute the pu-
rity of the automatically generated clusters against
the human judged clusters. The results are shown
in Table 4. In our experiments, we set the number
of general aspect clusters AG is 5 and specific as-
pect clusters AS is 3. We can see from Table 4 that
our generated aspect clusters can achieve reasonably
good performance.
6http://www.summarization.com/mead/
1143
Rouge Average Recall
Method ROUGE-1 ROUGE-2 ROUGE-SU4 ROUGE-W-1.2 ROUGE-L
BL-1 k-means 0.21895 0.03689 0.06644 0.06683 0.19208
entity-aspect 0.26082 0.05082 0.08286 0.08055 0.22976
BL-2 greedy 1 0.27802 0.04872 0.08302 0.08488 0.24426
greedy 2 0.27898 0.04723 0.08275 0.08500 0.24430
BL-3 KL-Div 0.29286 0.05369 0.09117 0.08827 0.25100
BL-4 HIERSUM 0.28736 0.05502 0.08932 0.08923 0.25285
Without compression 0.30563 0.05983 0.09513 0.09468 0.25487
Our Method 0.32641 0.06508 0.10146 0.09998 0.28610
Table 3: ROUGE evaluation results on TAC2010 Summarization data sets
Category A Purity
Accidents and Natural Disasters 7 0.613
Attacks 8 0.658
Health and Safety 5 0.724
Endangered Resources 4 0.716
Investigations and Trials 6 0.669
Table 4: The true numbers of aspects as judged by the
human annotator (A), and the purity of the clusters.
Category Average Score
Accidents and Natural Disasters 2.4
Attacks 2.3
Health and Safety 2.6
Endangered Resources 2.5
Investigations and Trials 2.4
Table 5: The average score of each event topic.
6.4 Quality of sentence compression
To judge the quality of the dependency tree based
sentence compression algorithm, we ask the human
judges to choose 20 sentences from each event top-
ic then score them. The judges follow 3-point scale
to score each compressed sentence: 1 means poor,
2 means barely acceptable, and 3 means good. We
then compute the average scores. The results are
shown in Table 5. To evaluate the effectiveness of
sentence compression component, we conduct the
system without sentence compression component,
then compare it with our system. In Table 3, we
can see that sentence compression can improve the
system performance.
7 Related Work
Our event-aspect model is related to a number of
previous extensions of LDA models. Chemudugun-
ta et al (2007) proposed to introduce a background
topic and document-specific topics. Our background
and document language models are similar to theirs.
However, they still treat documents as bags of words
rather then sets of sentences as in our models. Titov
and McDonald (2008) exploited the idea that a short
paragraph within a document is likely to be about
the same aspect. The way we separate words in-
to stop words, background words, document word-
s and aspect words bears similarity to that used
in (Daume? III and Marcu, 2006; Haghighi and Van-
derwende, 2009). Paul and Girju (2010) proposed a
topic-aspect model for simultaneously finding topic-
s and aspects. The most related extension is entity-
aspect model proposed by Li et al (2010). The main
difference between event-aspect model and entity-
aspect model is our model further consider aspect
granularity and add a layer to model topic-related
events.
Filippova and Strube (2008) proposed a depen-
dency tree based sentence compression algorithm.
Their approach need a large corpus to build language
model for compression, whereas we prune depen-
dency tree using grammatical rules.
Paul et al (2010) proposed to modify LexRank
algorithm using their topic-aspect model. But their
task is to summarize contrastive viewpoints in opin-
ionated text. Furthermore, they use a simple greedy
approach for constructing summary.
McDonald (2007) proposed to use Integer Linear
Programming framework in multi-document sum-
1144
marization. And Sauper and Barzilay (2009) use in-
teger linear programming framework to automatical-
ly generate Wikipedia articles. There is a fundamen-
tal difference between their method and ours. They
used trained perceptron algorithm for ranking ex-
cerpts, whereas we give an extended LexRank with
integer linear programming to optimize sentence se-
lection for our aspect-oriented multi-document sum-
marization.
8 Conclusions and Future Work
In this paper, we study the task of automatically
generating aspect-oriented summary from multiple
documents. We proposed an event-aspect model
that can automatically cluster sentences into aspect-
s. We then use an extension of the LexRank algo-
rithm to rank sentences. We took advantage of the
output generated by the event-aspect model to mod-
ify jumping probabilities so as to favor visiting rep-
resentative sentence. We also proposed dependen-
cy tree compression algorithm to prune sentence for
improving linguistic quality of the summaries. Fi-
nally we use Integer Linear Programming Frame-
work to select aspect relevant sentences. We con-
ducted quantitative evaluation using standard test
data sets. We found that our method gave overal-
l better ROUGE scores than four baseline methods,
and the new sentence clustering and compression al-
gorithm are robust.
There are a number of directions we plan to pur-
sue in the future in order to improve our method.
First, we can possibly apply more linguistic knowl-
edge to improve the quality of sentence compres-
sion. Currently the sentence compression algorith-
m may generate meaningless subtrees. It is rela-
tively hard to decide which clause is redundant in
terms of summarization. Second, we may explore
more domain knowledge to improve the quality of
aspect-oriented summaries. For example, we know
that the ?who-affected? aspect is related to person,
and ?when, where? are related to Time and Location.
we can import Name Entity Recognition to anno-
tate these phrases and then help locate relevant sen-
tences. Third, we want to extend our event-aspect
model to simultaneously find topics and aspects.
Acknowledgments
This work was supported by the National Nat-
ural Science Foundation of China (NSFC No.
60773088), the National High-tech R&D Program
of China (863 Program No. 2009AA04Z106), and
the Key Program of Basic Research of Shanghai
Municipal S&T Commission (No. 08JC1411700).
References
Chaitanya Chemudugunta, Padhraic Smyth, and Mark
Steyvers. 2007. Modeling general and specific aspects
of documents with a probabilistic topic model. In Ad-
vances in Neural Information Processing Systems 19,
pages 241?248.
Hal. Daume? III and Daniel. Marcu. 2006. Bayesian
query-focused summarization. In Proceedings of the
21st International Conference on Computational Lin-
guistics and the 44th annual meeting of the Associ-
ation for Computational Linguistics, pages 305?312.
Association for Computational Linguistics.
Gu?nes. Erkan and Dragomir Radev. 2004. LexRank:
Graph-based lexical centrality as salience in text sum-
marization. Journal of Artificial Intelligence Re-
search, 22(1):457?479.
K. Filippova and M. Strube. 2008. Dependency
tree based sentence compression. In Proceedings of
the Fifth International Natural Language Generation
Conference, pages 25?32. Association for Computa-
tional Linguistics.
Dan Gillick and Benoit Favre. 2009. A scalable global
model for summarization. In Proceedings of the Work-
shop on Integer Linear Programming for Natural Lan-
gauge Processing, pages 10?18.
Dan Gillick, Benoit Favre, D. Hakkani-Tur, B. Bohnet,
Y. Liu, and S. Xie. 2010. The icsi/utd summarization
system at tac 2009. In Proceedings of the Second Text
Analysis Conference, Gaithersburg, Maryland, USA:
National Institute of Standards and Technology.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National A-
cademy of Sciences of the United States of America,
101(Suppl. 1):5228?5235.
A. Haghighi and L. Vanderwende. 2009. Exploring con-
tent models for multi-document summarization. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics on
ZZZ, pages 362?370. Association for Computational
Linguistics.
1145
Peng Li, Jing Jiang, and Yinglin Wang. 2010. Gen-
erating templates of entity summaries with an entity-
aspect model and pattern mining. In Proceedings of
the Joint Conference of the 48th Annual Meeting of the
ACL. Association for Computational Linguistics.
C.Y. Lin and E. Hovy. 2003. Automatic evaluation
of summaries using n-gram co-occurrence statistics.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 71?78. Association for Computation-
al Linguistics.
RyanMcDonald. 2007. A study of global inference algo-
rithms in multi-document summarization. Advances in
Information Retrieval, pages 557?564.
A. Nenkova and L. Vanderwende. 2005. The impact
of frequency on summarization. Microsoft Research,
Redmond, Washington, Tech. Rep. MSR-TR-2005-101.
Michael J. Paul and Roxana Girju. 2010. A two-
dimensional topic-aspect model for discovering multi-
faceted topics. In In AAAI-2010: Twenty-Fourth Con-
ference on Artificial Intelligence.
Michael J. Paul, ChengXiang Zhai, and Roxana Girju.
2010. Summarizing contrastive viewpoints in opin-
ionated text. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ?10, pages 66?76, Morristown, NJ, USA.
Association for Computational Linguistics.
Christina Sauper and Regina Barzilay. 2009. Automati-
cally generating wikipedia articles: A structure-aware
approach. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the AFNLP, pages 208?216, Suntec, Singa-
pore, August. Association for Computational Linguis-
tics.
Ivan Titov and Ryan McDonald. 2008. Modeling online
reviews with multi-grain topic models. In Proceeding
of the 17th International Conference on World Wide
Web, pages 111?120.
D. Zajic, B.J. Dorr, J. Lin, and R. Schwartz. 2007. Multi-
candidate reduction: Sentence compression as a tool
for document summarization tasks. Information Pro-
cessing & Management, 43(6):1549?1570.
Jin. Zhang, Xueqi. Cheng, and Hongbo. Xu. 2008. GSP-
Summary: a graph-based sub-topic partition algorith-
m for summarization. In Proceedings of the 4th Asi-
a information retrieval conference on Information re-
trieval technology, pages 321?334. Springer-Verlag.
1146
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 640?649,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Generating Templates of Entity Summaries
with an Entity-Aspect Model and Pattern Mining
Peng Li1 and Jing Jiang2 and Yinglin Wang1
1Department of Computer Science and Engineering, Shanghai Jiao Tong University
2School of Information Systems, Singapore Management University
{lipeng,ylwang}@sjtu.edu.cn jingjiang@smu.edu.sg
Abstract
In this paper, we propose a novel approach
to automatic generation of summary tem-
plates from given collections of summary
articles. This kind of summary templates
can be useful in various applications. We
first develop an entity-aspect LDA model
to simultaneously cluster both sentences
and words into aspects. We then apply fre-
quent subtree pattern mining on the depen-
dency parse trees of the clustered and la-
beled sentences to discover sentence pat-
terns that well represent the aspects. Key
features of our method include automatic
grouping of semantically related sentence
patterns and automatic identification of
template slots that need to be filled in. We
apply our method on five Wikipedia entity
categories and compare our method with
two baseline methods. Both quantitative
evaluation based on human judgment and
qualitative comparison demonstrate the ef-
fectiveness and advantages of our method.
1 Introduction
In this paper, we study the task of automatically
generating templates for entity summaries. An en-
tity summary is a short document that gives the
most important facts about an entity. In Wikipedia,
for instance, most articles have an introduction
section that summarizes the subject entity before
the table of contents and other elaborate sections.
These introduction sections are examples of en-
tity summaries we consider. Summaries of enti-
ties from the same category usually share some
common structure. For example, biographies of
physicists usually contain facts about the national-
ity, educational background, affiliation and major
contributions of the physicist, whereas introduc-
tions of companies usually list information such
as the industry, founder and headquarter of the
company. Our goal is to automatically construct
a summary template that outlines the most salient
types of facts for an entity category, given a col-
lection of entity summaries from this category.
Such kind of summary templates can be very
useful in many applications. First of all, they
can uncover the underlying structures of summary
articles and help better organize the information
units, much in the same way as infoboxes do in
Wikipedia. In fact, automatic template genera-
tion provides a solution to induction of infobox
structures, which are still highly incomplete in
Wikipedia (Wu and Weld, 2007). A template
can also serve as a starting point for human edi-
tors to create new summary articles. Furthermore,
with summary templates, we can potentially ap-
ply information retrieval and extraction techniques
to construct summaries for new entities automati-
cally on the fly, improving the user experience for
search engine and question answering systems.
Despite its usefulness, the problem has not been
well studied. The most relevant work is by Fila-
tova et al (2006) on automatic creation of domain
templates, where the defintion of a domain is sim-
ilar to our notion of an entity category. Filatova
et al (2006) first identify the important verbs for
a domain using corpus statistics, and then find fre-
quent parse tree patterns from sentences contain-
ing these verbs to construct a domain template.
There are two major limitations of their approach.
First, the focus on verbs restricts the template pat-
terns that can be found. Second, redundant or
related patterns using different verbs to express
the same or similar facts cannot be grouped to-
gether. For example, ?won X award? and ?re-
ceived X prize? are considered two different pat-
terns by this approach. We propose a method that
can overcome these two limitations. Automatic
template generation is also related to a number of
other problems that have been studied before, in-
640
cluding unsupervised IE pattern discovery (Sudo
et al, 2003; Shinyama and Sekine, 2006; Sekine,
2006; Yan et al, 2009) and automatic generation
of Wikipedia articles (Sauper and Barzilay, 2009).
We discuss the differences of our work from exist-
ing related work in Section 6.
In this paper we propose a novel approach to
the task of automatically generating entity sum-
mary templates. We first develop an entity-aspect
model that extends standard LDA to identify clus-
ters of words that can represent different aspects
of facts that are salient in a given summary col-
lection (Section 3). For example, the words ?re-
ceived,? ?award,? ?won? and ?Nobel? may be
clustered together from biographies of physicists
to represent one aspect, even though they may ap-
pear in different sentences from different biogra-
phies. Simultaneously, the entity-aspect model
separates words in each sentence into background
words, document words and aspect words, and
sentences likely about the same aspect are natu-
rally clustered together. After this aspect identi-
fication step, we mine frequent subtree patterns
from the dependency parse trees of the clustered
sentences (Section 4). Different from previous
work, we leverage the word labels assigned by the
entity-aspect model to prune the patterns and to
locate template slots to be filled in.
We evaluate our method on five entity cate-
gories using Wikipedia articles (Section 5). Be-
cause the task is new and thus there is no stan-
dard evaluation criteria, we conduct both quanti-
tative evaluation using our own human judgment
and qualitative comparison. Our evaluation shows
that our method can obtain better sentence patterns
in terms of f1 measure compared with two baseline
methods, and it can also achieve reasonably good
quality of aspect clusters in terms of purity. Com-
pared with standard LDA and K-means sentence
clustering, the aspects identified by our method are
also more meaningful.
2 The Task
Given a collection of entity summaries from the
same entity category, our task is to automatically
construct a summary template that outlines the
most important information one should include in
a summary for this entity category. For example,
given a collection of biographies of physicists, ide-
ally the summary template should indicate that im-
portant facts about a physicist include his/her ed-
Aspect Pattern
ENT received his phd from ? university
1 ENT studied ? under ?
ENT earned his ? in physics from university of
?
ENT was awarded the medal in ?
2 ENT won the ? award
ENT received the nobel prize in physics in ?
ENT was ? director
3 ENT was the head of ?
ENT worked for ?
ENT made contributions to ?
4 ENT is best known for work on ?
ENT is noted for ?
Table 1: Examples of some good template patterns
and their aspects generated by our method.
ucational background, affiliation, major contribu-
tions, awards received, etc.
However, it is not clear what is the best repre-
sentation of such templates. Should a template
comprise a list of subtopic labels (e.g. ?educa-
tion? and ?affiliation?) or a set of explicit ques-
tions? Here we define a template format based on
the usage of the templates as well as our obser-
vations from Wikipedia entity summaries. First,
since we expect that the templates can be used by
human editors for creating new summaries, we use
sentence patterns that are human readable as basic
units of the templates. For example, we may have
a sentence pattern ?ENT graduated from ? Uni-
versity? for the entity category ?physicist,? where
ENT is a placeholder for the entity that the sum-
mary is about, and ??? is a slot to be filled in. Sec-
ond, we observe that information about entities of
the same category can be grouped into subtopics.
For example, the sentences ?Bohr is a Nobel lau-
reate? and ?Einstein received the Nobel Prize? are
paraphrases of the same type of facts, while the
sentences ?Taub earned his doctorate at Prince-
ton University? and ?he graduated from MIT? are
slightly different but both describe a person?s ed-
ucational background. Therefore, it makes sense
to group sentence patterns based on the subtopics
they pertain to. Here we call these subtopics the
aspects of a summary template.
Formally, we define a summary template to be a
set of sentence patterns grouped into aspects. Each
sentence pattern has a placeholder for the entity to
be summarized and possibly one or more template
slots to be filled in. Table 1 shows some sentence
patterns our method has generated for the ?physi-
cist? category.
641
2.1 Overview of Our Method
Our automatic template generation method con-
sists of two steps:
Aspect Identification: In this step, our goal is
to automatically identify the different aspects or
subtopics of the given summary collection. We si-
multaneously cluster sentences and words into as-
pects, using an entity-aspect model extended from
the standard LDA model that is widely used in
text mining (Blei et al, 2003). The output of this
step are sentences clustered into aspects, with each
word labeled as a stop word, a background word,
a document word or an aspect word.
Sentence Pattern Generation: In this step, we
generate human-readable sentence patterns to rep-
resent each aspect. We use frequent subtree pat-
tern mining to find the most representative sen-
tence structures for each aspect. The fixed struc-
ture of a sentence pattern consists of aspect words,
background words and stop words, while docu-
ment words become template slots whose values
can vary from summary to summary.
3 Aspect Identification
At the aspect identification step, our goal is to dis-
cover the most salient aspects or subtopics con-
tained in a summary collection. Here we propose
a principled method based on a modified LDA
model to simultaneously cluster both sentences
and words to discover aspects.
We first make the following observation. In en-
tity summaries such as the introduction sections
of Wikipedia articles, most sentences are talk-
ing about a single fact of the entity. If we look
closely, there are a few different kinds of words in
these sentences. First of all, there are stop words
that occur frequently in any document collection.
Second, for a given entity category, some words
are generally used in all aspects of the collection.
Third, some words are clearly associated with the
aspects of the sentences they occur in. And finally,
there are also words that are document or entity
specific. For example, in Table 2 we show two
sentences related to the ?affiliation? aspect from
the ?physicist? summary collection. Stop words
such as ?is? and ?the? are labeled with ?S.? The
word ?physics? can be regarded as a background
word for this collection. ?Professor? and ?univer-
sity? are clearly related to the ?affiliation? aspect.
Finally words such as ?Modena? and ?Chicago?
are specifically associated with the subject enti-
ties being discussed, that is, they are specific to
the summary documents.
To capture background words and document-
specific words, Chemudugunta et al (2007)
proposed to introduce a background topic and
document-specific topics. Here we borrow their
idea and also include a background topic as well
as document-specific topics. To discover aspects
that are local to one or a few adjacent sentences but
may occur in many documents, Titov and McDon-
ald (2008) proposed a multi-grain topic model,
which relies on word co-occurrences within short
paragraphs rather than documents in order to dis-
cover aspects. Inspired by their model, we rely
on word co-occurrences within single sentences to
identify aspects.
3.1 Entity-Aspect Model
We now formally present our entity-aspect model.
First, we assume that stop words can be identified
using a standard stop word list. We then assume
that for a given entity category there are three
kinds of unigram language models (i.e. multino-
mial word distributions). There is a background
model ?B that generates words commonly used
in all documents and all aspects. There are D
document models ?d (1 ? d ? D), where D
is the number of documents in the given sum-
mary collection, and there are A aspect models ?a
(1 ? a ? A), where A is the number of aspects.
We assume that these word distributions have a
uniform Dirichlet prior with parameter ?.
Since not all aspects are discussed equally fre-
quently, we assume that there is a global aspect
distribution ? that controls how often each aspect
occurs in the collection. ? is sampled from another
Dirichlet prior with parameter ?. There is also a
multinomial distribution pi that controls in each
sentence how often we encounter a background
word, a document word, or an aspect word. pi has
a Dirichlet prior with parameter ?.
Let Sd denote the number of sentences in doc-
ument d, Nd,s denote the number of words (after
stop word removal) in sentence s of document d,
and wd,s,n denote the n?th word in this sentence.
We introduce hidden variables zd,s for each sen-
tence to indicate the aspect a sentence belongs to.
We also introduce hidden variables yd,s,n for each
word to indicate whether a word is generated from
the background model, the document model, or
the aspect model. Figure 1 shows the process of
642
Venturi/D is/S a/S professor/A of/S physics/B at/S the/S University/A of/S
Modena/D ./S
He/S was/S a/S professor/A of/S physics/B at/S the/S University/A of/S
Chicago/D until/S 1982/D ./S
Table 2: Two sentences on ?affiliation? from the ?physicist? entity category. S: stop word. B: background
word. A: aspect word. D: document word.
1. Draw ? ? Dir(?), ?B ? Dir(?), pi ? Dir(?)
2. For each aspect a = 1, . . . , A,
(a) draw ?a ? Dir(?)
3. For each document d = 1, . . . , D,
(a) draw ?d ? Dir(?)
(b) for each sentence s = 1, . . . , Sd
i. draw zd,s ? Multi(?)
ii. for each word n = 1, . . . , Nd,s
A. draw yd,s,n ? Multi(pi)
B. draw wd,s,n ? Multi(?B) if yd,s,n = 1,
wd,s,n ? Multi(?d) if yd,s,n = 2, or
wd,s,n ? Multi(?zd,s) if yd,s,n = 3
Figure 1: The document generation process.
y z
?pi
? ?
?
? A
dSD sdN ,
B?
?
w
Figure 2: The entity-aspect model.
generating the whole document collection. The
plate notation of the model is shown in Figure 2.
Note that the values of ?, ? and ? are fixed. The
number of aspects A is also manually set.
3.2 Inference
Given a summary collection, i.e. the set of all
wd,s,n, our goal is to find the most likely assign-
ment of zd,s and yd,s,n, that is, the assignment that
maximizes p(z,y|w;?, ?, ?), where z, y and w rep-
resent the set of all z, y and w variables, respec-
tively. With the assignment, sentences are natu-
rally clustered into aspects, and words are labeled
as either a background word, a document word, or
an aspect word.
We approximate p(y, z|w;?, ?, ?) by
p(y,z|w; ??B, {??d}Dd=1, {??a}Aa=1, ??, p?i), where ??B,
{??d}Dd=1, {??a}Aa=1, ?? and p?i are estimated using
Gibbs sampling, which is commonly used for
inference for LDA models (Griffiths and Steyvers,
2004). Due to space limit, we give the formulas
for the Gibbs sampler below without derivation.
First, given sentence s in document d, we sam-
ple a value for zd,s given the values of all other z
and y variables using the following formula:
p(zd,s = a|z?{d,s},y,w)
? C
A
(a) + ?
CA(?) +A?
?
?V
v=1
?E(v)
i=0 (Ca(v) + i+ ?)?E(?)
i=0 (Ca(?) + i+ V ?)
.
In the formula above, z?{d,s} is the current aspect
assignment of all sentences excluding the current
sentence. CA(a) is the number of sentences assigned
to aspect a, and CA(?) is the total number of sen-
tences. V is the vocabulary size. Ca(v) is the num-
ber of times word v has been assigned to aspect
a. Ca(?) is the total number of words assigned to
aspect a. All the counts above exclude the current
sentence. E(v) is the number of times word v oc-
curs in the current sentence and is assigned to be
an aspect word, as indicated by y, and E(?) is the
total number of words in the current sentence that
are assigned to be an aspect word.
We then sample a value for yd,s,n for each word
in the current sentence using the following formu-
las:
p(yd,s,n = 1|z,y?{d,s,n}) ?
Cpi(1) + ?
Cpi(?) + 3?
?
CB(wd,s,n) + ?
CB(?) + V ?
,
p(yd,s,n = 2|z,y?{d,s,n}) ?
Cpi(2) + ?
Cpi(?) + 3?
?
Cd(wd,s,n) + ?
Cd(?) + V ?
,
p(yd,s,n = 3|z,y?{d,s,n}) ?
Cpi(3) + ?
Cpi(?) + 3?
?
Ca(wd,s,n) + ?
Ca(?) + V ?
.
In the formulas above, y?{d,s,n} is the set of all y
variables excluding yd,s,n. Cpi(1), Cpi(2) and Cpi(3) are
the numbers of words assigned to be a background
word, a document word, or an aspect word, respec-
tively, and Cpi(?) is the total number of words. CB
and Cd are counters similar to Ca but are for the
background model and the document models. In
all these counts, the current word is excluded.
With one Gibbs sample, we can make the fol-
lowing estimation:
643
??Bv =
CB(v) + ?
CB(?) + V ?
, ??dv =
Cd(v) + ?
Cd(?) + V ?
, ??av =
Ca(v) + ?
Ca(?) + V ?
,
??a =
CA(a) + ?
CA(?) +A?
, p?it =
Cpi(t) + ?
Cpi(?) + 3?
(1 ? t ? 3).
Here the counts include all sentences and all
words.
In our experiments, we set ? = 5, ? = 0.01 and
? = 20. We run 100 burn-in iterations through all
documents in a collection to stabilize the distri-
bution of z and y before collecting samples. We
found that empirically 100 burn-in iterations were
sufficient for our data set. We take 10 samples with
a gap of 10 iterations between two samples, and
average over these 10 samples to get the estima-
tion for the parameters.
After estimating ??B, {??d}Dd=1, {??a}Aa=1, ?? and p?i,
we find the values of each zd,s and yd,s,n that max-
imize p(y, z|w; ??B, {??d}Dd=1, {??a}Aa=1, ??, p?i). This as-
signment, together with the standard stop word list
we use, gives us sentences clustered into A as-
pects, where each word is labeled as either a stop
word, a background word, a document word or an
aspect word.
3.3 Comparison with Other Models
A major difference of our entity-aspect model
from standard LDA model is that we assume each
sentence belongs to a single aspect while in LDA
words in the same sentence can be assigned to
different topics. Our one-aspect-per-sentence as-
sumption is important because our goal is to clus-
ter sentences into aspects so that we can mine
common sentence patterns for each aspect.
To cluster sentences, we could have used a
straightforward solution similar to document clus-
tering, where sentences are represented as feature
vectors using the vector space model, and a stan-
dard clustering algorithm such as K-means can
be applied to group sentences together. However,
there are some potential problems with directly ap-
plying this typical document clustering method.
First, unlike documents, sentences are short, and
the number of words in a sentence that imply its
aspect is even smaller. Besides, we do not know
the aspect-related words in advance. As a result,
the cosine similarity between two sentences may
not reflect whether they are about the same aspect.
We can perform heuristic term weighting, but the
method becomes less robust. Second, after sen-
tence clustering, we may still want to identify the
the aspect words in each sentence, which are use-
ful in the next pattern mining step. Directly taking
the most frequent words from each sentence clus-
ter as aspect words may not work well even af-
ter stop word removal, because there can be back-
ground words commonly used in all aspects.
4 Sentence Pattern Generation
At the pattern generation step, we want to iden-
tify human-readable sentence patterns that best
represent each cluster. Following the basic idea
from (Filatova et al, 2006), we start with the parse
trees of sentences in each cluster, and apply a
frequent subtree pattern mining algorithm to find
sentence structures that have occurred at least K
times in the cluster. Here we use dependency parse
trees.
However, different from (Filatova et al, 2006),
the word labels (S, B, D and A) assigned by the
entity-aspect model give us some advantages. In-
tuitively, a representative sentence pattern for an
aspect should contain at least one aspect word. On
the other hand, document words are entity-specific
and therefore should not appear in the generic tem-
plate patterns; instead, they correspond to tem-
plate slots that need to be filled in. Furthermore,
since we work on entity summaries, in each sen-
tence there is usually a word or phrase that refers
to the subject entity, and we should have a place-
holder for the subject entity in each pattern.
Based on the intuitions above, we have the fol-
lowing sentence pattern generation process.
1. Locate subject entities: In each sentence, we
want to locate the word or phrase that refers to the
subject entity. For example, in a biography, usu-
ally a pronoun ?he? or ?she? is used to refer to
the subject person. We use the following heuristic
to locate the subject entities: For each summary
document, we first find the top 3 frequent base
noun phrases that are subjects of sentences. For
example, in a company introduction, the phrase
?the company? is probably used frequently as a
sentence subject. Then for each sentence, we first
look for the title of the Wikipedia article. If it oc-
curs, it is tagged as the subject entity. Otherwise,
we check whether one of the top 3 subject base
noun phrases occurs, and if so, it is tagged as the
subject entity. Otherwise, we tag the subject of the
sentence as the subject entity. Finally, for the iden-
tified subject entity word or phrase, we replace the
label assigned by the entity-aspect model with a
644
professor_A
is_SENT a_S
physics_B university_A
?the_S
nsubj
cop
det
prep_of
det
prep_at
prep_of
Figure 3: An example labeled dependency parse
tree.
new label E.
2. Generate labeled parse trees: We parse each
sentence using the Stanford Parser1. After parsing,
for each sentence we obtain a dependency parse
tree where each node is a single word and each
edge is labeled with a dependency relation. Each
word is also labeled with one of {E, S, B, D,
A}. We replace words labeled with E by a place-
holder ENT, and replace words labeled with D by
a question mark to indicate that these correspond
to template slots. For the other words, we attach
their labels to the tree nodes. Figure 3 shows an
example labeled dependency parse tree.
3. Mine frequent subtree patterns: For the set
of parse trees in each cluster, we use FREQT2, a
software that implements the frequent subtree pat-
tern mining algorithm proposed in (Zaki, 2002), to
find all subtrees with a minimum support of K.
4. Prune patterns: We remove subtree patterns
found by FREQT that do not contain ENT or any
aspect word. We also remove small patterns that
are contained in some other larger pattern in the
same cluster.
5. Covert subtree patterns to sentence patterns:
The remaining patterns are still represented as sub-
trees. To covert them back to human-readable sen-
tence patterns, we map each pattern back to one of
the sentences that contain the pattern to order the
tree nodes according to their original order in the
sentence.
In the end, for each summary collection, we ob-
tain A clusters of sentence patterns, where each
cluster presumably corresponds to a single aspect
or subtopic.
1http://nlp.stanford.edu/software/
lex-parser.shtml
2http://chasen.org/?taku/software/
freqt/
Category D S Sd
min max avg
US Actress 407 1721 1 21 4
Physicist 697 4238 1 49 6
US CEO 179 1040 1 24 5
US Company 375 2477 1 36 6
Restaurant 152 1195 1 37 7
Table 3: The number of documents (D), total
number of sentences (S) and minimum, maximum
and average numbers of sentences per document
(Sd) of the data set.
5 Evaluation
Because we study a non-standard task, there is no
existing annotated data set. We therefore created a
small data set and made our own human judgment
for quantitative evaluation purpose.
5.1 Data
We downloaded five collections of Wikipedia ar-
ticles from different entity categories. We took
only the introduction sections of each article (be-
fore the tables of contents) as entity summaries.
Some statistics of the data set are given in Table 3.
5.2 Quantitative Evaluation
To quantitatively evaluate the summary templates,
we want to check (1) whether our sentence pat-
terns are meaningful and can represent the corre-
sponding entity categories well, and (2) whether
semantically related sentence patterns are grouped
into the same aspect. It is hard to evaluate both
together. We therefore separate these two criteria.
5.2.1 Quality of sentence patterns
To judge the quality of sentence patterns without
looking at aspect clusters, ideally we want to com-
pute the precision and recall of our patterns, that
is, the percentage of our sentence patterns that are
meaningful, and the percentage of true meaningful
sentence patterns of each category that our method
can capture. The former is relatively easy to obtain
because we can ask humans to judge the quality of
our patterns. The latter is much harder to com-
pute because we need human judges to find the set
of true sentence patterns for each entity category,
which can be very subjective.
We adopt the following pooling strategy bor-
rowed from information retrieval. Assume we
want to compare a number of methods that each
can generate a set of sentence patterns from a sum-
mary collection. We take the union of these sets
645
of patterns generated by the different methods and
order them randomly. We then ask a human judge
to decide whether each sentence pattern is mean-
ingful for the given category. We can then treat
the set of meaningful sentence patterns found by
the human judge this way as the ground truth, and
precision and recall of each method can be com-
puted. If our goal is only to compare the different
methods, this pooling strategy should suffice.
We compare our method with the following two
baseline methods.
Baseline 1: In this baseline, we use the same
subtree pattern mining algorithm to find sentence
patterns from each summary collection. We also
locate the subject entities and replace them with
ENT. However, we do not have aspect words or
document words in this case. Therefore we do not
prune any pattern except to merge small patterns
with the large ones that contain them. The pat-
terns generated by this method do not have tem-
plate slots.
Baseline 2: In the second baseline, we apply a
verb-based pruning on the patterns generated by
the first baseline, similar to (Filatova et al, 2006).
We first find the top-20 verbs using the scoring
function below that is taken from (Filatova et al,
2006), and then prune patterns that do not contain
any of the top-20 verbs.
s(vi) = N(vi)?
vj?V N(vj)
? M(vi)D ,
where N(vi) is the frequency of verb vi in the
collection, V is the set of all verbs, D is the total
number of documents in the collection, and M(vi)
is the number of documents in the collection that
contains vi.
In Table 4, we show the precision, recall and f1
of the sentence patterns generated by our method
and the two baseline methods for the five cate-
gories. For our method, we set the support of
the subtree patterns K to 2, that is, each pattern
has occurred in at least two sentences in the cor-
responding aspect cluster. For the two baseline
methods, because sentences are not clustered, we
use a larger support K of 3; otherwise, we find
that there can be too many patterns. We can see
that overall our method gives better f1 measures
than the two baseline methods for most categories.
Our method achieves a good balance between pre-
cision and recall. For BL-1, the precision is high
but recall is low. Intuitively BL-1 should have a
higher recall than our method because our method
Category B Purity
US Actress 4 0.626
Physicist 6 0.714
US CEO 4 0.674
US Company 4 0.614
Restaurant 3 0.587
Table 5: The true numbers of aspects as judged
by the human annotator (B), and the purity of the
clusters.
does more pattern pruning than BL-1 using aspect
words. Here it is not the case mainly because we
used a higher frequency threshold (K = 3) to se-
lect frequent patterns in BL-1, giving overall fewer
patterns than in our method. For BL-2, the preci-
sion is higher than BL-1 but recall is lower. It is
expected because the patterns of BL-2 is a subset
of that of BL-1.
There are some advantages of our method that
are not reflected in Table 4. First, many of our pat-
terns contain template slots, which make the pat-
tern more meaningful. In contrast the baseline pat-
terns do not contain template slots. Because the
human judge did not give preference over patterns
with slots, both ?ENT won the award? and ?ENT
won the ? award? were judged to be meaningful
without any distinction, although the former one
generated by our method is more meaningful. Sec-
ond, compared with BL-2, our method can obtain
patterns that do not contain a non-auxiliary verb,
such as ?ENT was ? director.?
5.2.2 Quality of aspect clusters
We also want to judge the quality of the aspect
clusters. To do so, we ask the human judge to
group the ground truth sentence patterns of each
category based on semantic relatedness. We then
compute the purity of the automatically generated
clusters against the human judged clusters using
purity. The results are shown in Table 5. In our
experiments, we set the number of clusters A used
in the entity-aspect model to be 10. We can see
from Table 5 that our generated aspect clusters can
achieve reasonably good performance.
5.3 Qualitative evaluation
We also conducted qualitative comparison be-
tween our entity-aspect model and standard LDA
model as well as a K-means sentence clustering
method. In Table 6, we show the top 5 fre-
quent words of three sample aspects as found by
our method, standard LDA, and K-means. Note
that although we try to align the aspects, there is
646
Category
Method US Actress Physicist US CEO US Company Restaurant
BL-1 precision 0.714 0.695 0.778 0.622 0.706
recall 0.545 0.300 0.367 0.425 0.361
f1 0.618 0.419 0.499 0.505 0.478
BL-2 precision 0.845 0.767 0.829 0.809 1.000
recall 0.260 0.096 0.127 0.167 0.188
f1 0.397 0.17 0.220 0.276 0.316
Ours precision 0.544 0.607 0.586 0.450 0.560
recall 0.710 0.785 0.712 0.618 0.701
f1 0.616 0.684 0.643 0.520 0.624
Table 4: Quality of sentence patterns in terms of precision, recall and f1.
Method Sample Aspects
1 2 3
Our university prize academy
entity- received nobel sciences
aspect ph.d. physics member
model college awarded national
degree medal society
Standard physics nobel physics
LDA american prize institute
professor physicist research
received awarded member
university john sciences
K-means physics physicist physics
university american academy
institute physics sciences
work university university
research nobel new
Table 6: Comparison of the top 5 words of three
sample aspects using different methods.
no correspondence between clusters numbered the
same but generated by different methods.
We can see that our method gives very mean-
ingful aspect clusters. Standard LDA also gives
meaningful words, but background words such
as ?physics? and ?physicist? are mixed with as-
pect words. Entity-specific words such as ?john?
also appear mixed with aspect words. K-means
clusters are much less meaningful, with too many
background words mixed with aspect words.
6 Related Work
The most related existing work is on domain tem-
plate generation by Filatova et al (2006). There
are several differences between our work and
theirs. First, their template patterns must contain a
non-auxiliary verb whereas ours do not have this
restriction. Second, their verb-centered patterns
are independent of each other, whereas we group
semantically related patterns into aspects, giving
more meaningful templates. Third, in their work,
named entities, numbers and general nouns are
treated as template slots. In our method, we ap-
ply the entity-aspect model to automatically iden-
tify words that are document-specific, and treat
these words as template slots, which can be poten-
tially more robust as we do not rely on the quality
of named entity recognition. Last but not least,
their documents are event-centered while ours are
entity-centered. Therefore we can use heuristics to
anchor our patterns on the subject entities.
Sauper and Barzilay (2009) proposed a frame-
work to learn to automatically generate Wikipedia
articles. There is a fundamental difference be-
tween their task and ours. The articles they gen-
erate are long, comprehensive documents consist-
ing of several sections on different subtopics of
the subject entity, and they focus on learning the
topical structures from complete Wikipedia arti-
cles. We focus on learning sentence patterns of the
short, concise introduction sections of Wikipedia
articles.
Our entity-aspect model is related to a num-
ber of previous extensions of LDA models.
Chemudugunta et al (2007) proposed to intro-
duce a background topic and document-specific
topics. Our background and document language
models are similar to theirs. However, they still
treat documents as bags of words rather than sets
of sentences as in our model. Titov and McDon-
ald (2008) exploited the idea that a short paragraph
within a document is likely to be about the same
aspect. Our one-aspect-per-sentence assumption
is a stricter than theirs, but it is required in our
model for the purpose of mining sentence patterns.
The way we separate words into stop words, back-
ground words, document words and aspect words
bears similarity to that used in (Daume? III and
Marcu, 2006; Haghighi and Vanderwende, 2009),
but their task is multi-document summarization
while ours is to induce summary templates.
647
7 Conclusions and Future Work
In this paper, we studied the task of automati-
cally generating templates for entity summaries.
We proposed an entity-aspect model that can auto-
matically cluster sentences and words into aspects.
The model also labels words in sentences as either
a stop word, a background word, a document word
or an aspect word. We then applied frequent sub-
tree pattern mining to generate sentence patterns
that can represent the aspects. We took advan-
tage of the labels generated by the entity-aspect
model to prune patterns and to locate template
slots. We conducted both quantitative and qualita-
tive evaluation using five collections of Wikipedia
entity summaries. We found that our method gave
overall better template patterns than two baseline
methods, and the aspect clusters generated by our
method are reasonably good.
There are a number of directions we plan to pur-
sue in the future in order to improve our method.
First, we can possibly apply linguistic knowledge
to improve the quality of sentence patterns. Cur-
rently the method may generate similar sentence
patterns that differ only slightly, e.g. change of a
preposition. Also, the sentence patterns may not
form complete, meaningful sentences. For exam-
ple, a sentence pattern may contain an adjective
but not the noun it modifies. We plan to study
how to use linguistic knowledge to guide the con-
struction of sentence patterns and make them more
meaningful. Second, we have not quantitatively
evaluated the quality of the template slots, because
our judgment is only at the whole sentence pattern
level. We plan to get more human judges and more
rigorously judge the relevance and usefulness of
both the sentence patterns and the template slots.
It is also possible to introduce certain rules or con-
straints to selectively form template slots rather
than treating all words labeled with D as template
slots.
Acknowledgments
This work was done during Peng Li?s visit to the
Singapore Management University. This work
was partially supported by the National High-tech
Research and Development Project of China (863)
under the grant number 2009AA04Z106 and the
National Science Foundation of China (NSFC) un-
der the grant number 60773088. We thank the
anonymous reviewers for their helpful comments.
References
David Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Chaitanya Chemudugunta, Padhraic Smyth, and Mark
Steyvers. 2007. Modeling general and specific as-
pects of documents with a probabilistic topic model.
In Advances in Neural Information Processing Sys-
tems 19, pages 241?248.
Hal Daume? III and Daniel Marcu. 2006. Bayesian
query-focused summarization. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 305?312.
Elena Filatova, Vasileios Hatzivassiloglou, and Kath-
leen McKeown. 2006. Automatic creation of do-
main templates. In Proceedings of 21st Interna-
tional Conference on Computational Linguistics and
the 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 207?214.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of Amer-
ica, 101(Suppl. 1):5228?5235.
Aria Haghighi and Lucy Vanderwende. 2009. Explor-
ing content models for multi-document summariza-
tion. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 362?370.
Christina Sauper and Regina Barzilay. 2009. Automat-
ically generating Wikipedia articles: A structure-
aware approach. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 208?216.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In Proceedings of 21st International Confer-
ence on Computational Linguistics and the 44th An-
nual Meeting of the Association for Computational
Linguistics, pages 731?738.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
tion discovery. In Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 304?311.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representa-
tion model for automatic IE pattern acquisition. In
Proceedings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 224?
231.
Ivan Titov and Ryan McDonald. 2008. Modeling
online reviews with multi-grain topic models. In
648
Proceeding of the 17th International Conference on
World Wide Web, pages 111?120.
Fei Wu and Daniel S. Weld. 2007. Autonomously se-
mantifying Wikipedia. In Proceedings of the 16th
ACM Conference on Information and Knowledge
Management, pages 41?50.
Yulan Yan, Naoaki Okazaki, Yutaka Matsuo, Zhenglu
Yang, and Mitsuru Ishizuka. 2009. Unsupervised
relation extraction by mining Wikipedia texts using
information from the Web. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
1021?1029.
Mohammed J. Zaki. 2002. Efficiently mining fre-
quent trees in a forest. In Proceedings of the 8th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 71?80.
649
