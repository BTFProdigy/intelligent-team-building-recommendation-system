Syntax annotation for the GENIA corpus  
Yuka Tateisi1 Akane Yakushiji2 Tomoko Ohta1 Jun?ichi Tsujii2,3,1
1 CREST, Japan Science and Technology Agency 
4-1-8, Honcho, Kawaguchi-shi, Saitama 332-0012 Japan 
2 Department of Computer Science, University of Tokyo 
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan 
3 School of Informatics, University of Manchester 
POBox 88, Sackville St, MANCHESTER M60 1QD, UK 
{yucca,akane,okap,tsujii}@is.s.u-tokyo.ac.jp 
 
Abstract 
Linguistically annotated corpus based 
on texts in biomedical domain has been 
constructed to tune natural language 
processing (NLP) tools for bio-
textmining. As the focus of information 
extraction is shifting from "nominal" 
information such as named entity to 
"verbal" information such as function 
and interaction of substances, applica-
tion of parsers has become one of the 
key technologies and thus the corpus 
annotated for syntactic structure of sen-
tences is in demand. A subset of the 
GENIA corpus consisting of 500 
MEDLINE abstracts has been anno-
tated for syntactic structure in an XML-
based format based on Penn Treebank 
II (PTB) scheme. Inter-annotator 
agreement test indicated that the writ-
ing style rather than the contents of the 
research abstracts is the source of the 
difficulty in tree annotation, and that 
annotation can be stably done by lin-
guists without much knowledge of bi-
ology with appropriate guidelines 
regarding to linguistic phenomena par-
ticular to scientific texts. 
1 Introduction 
Research and development for information ex-
traction from biomedical literature (bio-
textmining) has been rapidly advancing due to 
demands caused by information overload in the 
genome-related field. Natural language process-
ing (NLP) techniques have been regarded as 
useful for this purpose. Now that focus of in-
formation extraction is shifting from extraction 
of ?nominal? information such as named entity 
to ?verbal? information such as relations of enti-
ties including events and functions, syntactic 
analysis is an important issue of NLP applica-
tion in biomedical domain. In extraction of rela-
tion, the roles of entities participating in the 
relation must be identified along with the verb 
that represents the relation itself. In text analysis, 
this corresponds to identifying the subjects, ob-
jects, and other arguments of the verb. 
Though rule-based relation information ex-
traction systems using surface pattern matching 
and/or shallow parsing can achieve high-
precision (e.g. Koike et al, 2004) in a particular 
target domain, they tend to suffer from low re-
call due to the wide variation of the surface ex-
pression that describe a relation between a verb 
and its arguments. In addition,  the portability of 
such systems is low because the system has to 
be re-equipped with different set of rules when 
different kind of relation is to be extracted. One 
solution to this problem is using deep parsers 
which can abstract the syntactic variation of a 
relation between a verb and its arguments repre-
sented in the text, and constructing extraction 
rule on the abstract predicate-argument structure. 
To do so, wide-coverage and high-precision 
parsers are required. 
While basic NLP techniques are relatively 
general and portable from domain to domain, 
customization and tuning are inevitable, espe-
cially in order to apply the techniques effec-
tively to highly specialized literatures such as 
research papers and abstracts. As recent ad-
vances in NLP technology depend on machine-
learning techniques, annotated corpora from 
which system can acquire rules (including 
grammar rules, lexicon, etc.) are indispensable 
220
resources for customizing general-purpose NLP 
tools. In bio-textmining, for example, training 
on part-of-speech (POS)-annotated GENIA cor-
pus was reported to improve the accuracy of 
JunK tagger (English POS tagger) (Kazama et 
al., 2001) from  83.5% to 98.1% on MEDLINE 
abstracts (Tateisi and Tsujii, 2004), and the 
FraMed corpus (Wermter and Hahn, 2004) was 
used to train TnT tagger on German (Brants, 
2000) to improve its accuracy from 95.7% to 
98% on clinical reports and other biomedical 
texts. Corpus annotated for syntactic structures 
is expected to play a similar role in tuning pars-
ers to biomedical domain, i.e., similar improve-
ment on the performance of parsers is expected 
by using domain-specific treebank as a resource 
for learning. For this purpose, we construct 
GENA Treebank (GTB), a treebank on research 
abstracts in biomedical domain. 
2 Outline of the Corpus 
The base text of GTB is that of the GENIA cor-
pus constructed at University of Tokyo (Kim et 
al., 2003), which is a collection of research ab-
stracts selected from the search results of 
MEDLINE database with keywords (MeSH 
terms) human, blood cells and transcription fac-
tors. In the GENIA corpus, the abstracts are en-
coded in an XML scheme where each abstract is 
numbered with MEDLINE UID and contains 
title and abstract. The text  of title and abstract is 
segmented into sentences in which biological 
terms are annotated with their semantic classes. 
The GENIA corpus is also annotated for part-of-
speech (POS) (Tateisi and Tsujii, 2004), and 
coreference is also annotated in a part of the 
GENIA corpus by MedCo project at Institute for 
Infocomm Research, Singapore (Yang et al 
2004).  
GTB is the addition of syntactic information 
to the GENIA corpus. By annotating various 
linguistic information on a same set of text, the 
GENIA corpus will be a resource not only for 
individual purpose such as named entity extrac-
tion or training parsers but also for integrated 
systems such as information extraction using 
deep linguistic analysis. Similar attempt of con-
structing integrated corpora is being done in 
University of Pennsylvania, where a corpus of 
MEDLINE abstracts in CYP450 and oncology 
domains where annotated for named entities, 
POS, and tree structure of sentences (Kulick et 
al, 2004).  
2.1 Annotation Scheme 
The annotation scheme basically follows the 
Penn Treebank II (PTB) scheme (Beis et al 
1995), encoded in XML. A non-null constituent 
is marked as an element, with its syntactic cate-
gory (which may be combined with its function 
tags indicating grammatical roles such as  -SBJ, 
-PRD, and -ADV) used as tags. A null constitu-
ent is marked as a childless element whose tag 
corresponds to its categories. Other function tags 
are encoded as attributes. Figure 1 shows an ex-
ample of annotated sentence in XML, and the 
corresponding PTB notation. The label ?S? 
means ?sentence?, ?NP? noun phrase, ?PP? 
prepositional phrase, and ?VP? verb phrase.  
The label ?NP-SBJ? means that the element is 
an NP that serves as the subject of the sentence. 
A null element, the trace of the object of ?stud-
ied? moved by passivization, is denoted by 
? <NP NULL="NONE" ref="i55"/>? in XML 
and ?*-55? in PTB notation. The number ?55? 
which refers to the identifier of the moved ele-
ment, is denoted by ?id? and ?ref? attributes in 
XML, and is denoted as a part of a label in PTB. 
In addition to changing the encoding, we 
made some modifications to the scheme. First, 
analysis within the noun phrase is simplified. 
Second, semantic division of adverbial phrases 
such as ??TMP? (time) and ??MNR? (manner) 
are not used: adverbial constituents other than 
?ADVP? (adverbial phrases) or ?PP? used ad-
verbially are marked with ?ADV tags but not 
with semantic tags. Third, a coordination struc-
ture is explicitly marked with the attribute 
SYN=?COOD? whereas in the original PTB 
scheme it is not marked as such.   
 In our GTB scheme, ?NX? (head of a com-
plex noun phrase) and ?NAC? (a certain kind of 
nominal modifier within a noun phrase) of the 
PTB scheme are not used. A noun phrase is gen-
erally left unstructured. This is mainly in order 
to simplify the process of annotation. In case of 
biomedical abstracts, long noun phrases often 
involve multi-word technical terms whose syn-
tactic structure is difficult to determine without 
deep domain knowledge. However, the structure 
of noun phrases are usually independent of the 
structure outside the phrase, so that it would be 
221
easier to analyze the phrases involving such 
terms independently (e.g. by biologists) and 
later merge the two analysis together. Thus we 
have decided that we leave noun phrases un-
structured in GTB annotation unless their analy-
sis is necessary for determining the structure 
outside the phrase. One of the exception is the 
cases that involves coordination where it is nec-
essary to explicitly mark up the coordinated 
constituents. 
In addition, we have added special attributes 
?TXTERR?, ?UNSURE?,  and ?COMMENT? 
for later inspection. The ?TXTERR? is used 
when the annotator suspects that there is a 
grammatical error in the original text; the 
?UNSURE? attribute is used when the annotator 
is not confident; and the ?COMMENT? is used 
for free comments (e.g. reason of using 
?UNSURE?) by the annotator.  
2.2   Annotation Process 
The sentences in the titles and abstracts of the 
base text of GENIA corpus are annotated manu-
ally using an XML editor used for the Global 
Document Annotation project (Hasida 2000). 
Although the sentence boundaries were adopted 
from the corpus, the tree structure annotation 
was done independently of POS- and term- an-
notation already done on the GENIA corpus. 
The annotator was a Japanese non-biologist who 
has previously involved in the POS annotation 
of the GENIA corpus and accustomed to the 
style of research abstracts in English. Manually 
annotated abstracts are automatically converted 
to the PTB format, merged with the POS annota-
tion of the GENIA corpus (version 3.02). 
3 Annotation Results 
So far, 500 abstracts are annotated and con-
verted to the merged PTB format. In the merg-
ing process, we found several annotation errors. 
The 500 abstracts with correction of these errors 
are made publicly available as ?The GENIA 
Treebank Beta Version? (GTB-beta).   
For further clean-up, we also tried to parse 
the corpus by the Enju parser (Miyao and Tsujii 
2004), and identify the error of the corpus by 
investigating into the parse errors. Enju is an 
HPSG parser that can be trained with PTB-type 
corpora which is reported to have 87% accuracy 
on Wall Street Journal portion of Penn Treebank 
corpus. Currently the accuracy of the parser 
drops down to 82% on GTB-beta, and although 
proper quantitative analysis is yet to be done, it 
was found that the mismatches between labels of 
the treebank and the GENIA POS corpus (e.g. 
an ?ing form labeled as noun in the POS corpus 
and as the head of a verb phrase in the tree cor-
pus) are a major source of parse error. The cor-
rection is complicated because several errors in 
the GENIA POS corpus were found in this 
cleaning-up process. When the cleaning-up 
process is done, we will make the corpus pub-
licly available as the proper release. 
<S><PP>In <NP>the present paper </NP></PP>, 
<NP-SBJ id="i55"><NP>the binding 
</NP><PP>of <NP>a [125I]-labeled aldosterone 
derivative </NP></PP><PP>to <NP><NP>plasma 
membrane rich fractions </NP><PP>of HML 
</PP></NP></PP></NP-SBJ><VP>was 
<VP>studied <NP NULL="NONE" 
ref="i55"/></VP> 
</VP>.</S> 
 
4 Inter-Annotator Agreement 
We have also checked inter-annotator agreement. 
Although the PTB scheme is popular among 
natural language processing society, applicabil-
ity of the scheme to highly specialized text such 
as research abstract is yet to be discussed. Espe-
cially, when the annotation is done by linguists, 
lack of domain knowledge might decrease the 
stability and accuracy of annotation. 
A small part of the base text set (10 ab-
stracts) was annotated by another annotator. The 
10 abstracts were chosen randomly, had 6 to 17 
sentences per abstract (total 108 sentences). The 
new annotator had a similar background as the 
first annotator that she is a Japanese non-
biologist who has experiences in translation of 
(S (PP In (NP the present paper)), (NP-SBJ-55 (NP 
the binding) (PP of (NP a [125I]-labeled aldosterone 
derivative)) (PP to (NP (NP plasma membrane rich 
fractions) (PP of HML)))) (VP was (VP studied *-
55)).) 
Figure 1. The sentence ?In the present paper, the binding of 
a [125I]-labeled aldosterone derivative to plasma mem-
brane rich fractions of HML was studied? annotated in 
XML and PTB formats.  
222
technical documents in English and in corpus 
annotation of  English texts. 
The two results were examined manually, 
and there were 131 disagreements. Almost every 
sentence had at least one disagreement. We have 
made the ?gold standard? from the two sets of 
abstracts by resolving the disagreements, and the 
accuracy of the annotators against this gold 
standard were 96.7% for the first annotator and 
97.4% for the second annotator. 
 Of the disagreement, the most prominent 
were the cases involving coordination, espe-
cially the ones with ellipsis. For example, one 
annotator annotated the phrase ?IL-1- and IL-18-
mediated function? as in Figure 2a, the other 
annotated as Figure 2b.  
 Such problem is addressed in the PTB 
guideline and both formats are allowed as alter-
natives. As coordination with ellipsis occurs 
rather frequently in research abstracts, this kind 
of phenomena has higher effect on decrease of 
the agreement rate than in Penn Treebank. Of 
the 131 disagreements, 25 were on this type of 
coordination. 
Another source of disagreement is the at-
tachment of modifiers such as prepositional 
phrases and pronominal adjectives. However, 
most are ?benign ambiguity? where the differ-
ence of the structure does not affect on interpre-
tation, such as ?high expression of STAT in 
monocytes? where the prepositional phrase ?in 
monocytes? can attach to ?expression? or 
?STAT? without much difference in meaning, 
and ?is augmented when the sensitizing tumor is 
a genetically modified variant? where the wh-
clause can attach to ?is augmented? or ?aug-
mented? without changing the meaning. The 
PTB guideline states that the modifier should be 
attached at the higher level in the former case 
and at the lower case in the latter. In the annota-
tion results, one annotator consistently attached 
the modifiers in both cases at the higher level, 
and the other consistently at the lower level, in-
dicating that the problem is in understanding the 
scheme rather than understanding the sentence. 
Only 15 cases were true ambiguities that needed 
knowledge of biology to solve, in which 5 in-
volved coordination (e.g., the scope of ?various? 
in ?various T cell lines and peripheral blood 
cells?) .  
 Although the number was small, there were 
disagreements on how to annotate a mathemati-
cal formula such as ?n=2? embedded in the sen-
tence, since mathematical formulae were outside 
the scope of the original PTB scheme. One an-
notator annotated this kind of phrase consis-
tently as a phrase with ?=? as an adjective, the 
other annotated as phrase with ?=? as a verb. 
There were 6 such cases. Another disagreement 
particular to abstracts is a treatment of labeled 
sentences. There were 8 sentences in two ab-
stracts where there is a label like ?Background:?.  
One annotator included the colon (?:?) in the la-
bel, while the other did not. Yet another is that 
one regarded the phrase ?Author et al as coor-
dination, and the other regarded ?et al as a 
modifier.   
<NP SYN="COOD"> 
<NP><ADJP>IL-1- <ADJP NULL="QSTN"/></ADJP> 
         <NP NULL="RNR" ref="i20"/></NP> 
and  
<NP>IL-18-mediated <NP NULL="RNR" ref="i20"/></NP> 
<NP id="i20">function </NP> 
 Other disagreements are more general type 
such as regarding ?-ed? form of a verb as an ad-
jective or a participle, miscellaneous errors such 
as omission of a subtype of label (such as ?-
PRD? or ?-SBJ) or the position of <PRN> tags 
<NP> 
<ADJP SYN="COOD"> 
  <ADJP>IL-1- <ADJP NULL="QSTN"/></ADJP> 
  and  
  <ADJP>IL-18-mediated </ADJP></ADJP> 
function  
</NP> 
    NP    
       
  ADJP   Function  
       
ADJP     and ADJP    
         
IL-1  *   IL-18 mediated   
Figure 2a. Annotation of a coordinated phrase by the first 
annotator. A* denotes a null constituent. 
</NP> 
        NP     
       
 NP And NP   
         
    ADJP  *20 IL-18 meidiated NP  
          
IL-1 *      function20
Figure 2b. Annotation of the same phrase as in Figure 2a 
by the second annotator.  A * denotes a null constituent 
and ?20? denotes coindexing. 
223
with regards to ?,? for the inserted phrase, or the 
errors which look like just ?careless?. Such dis-
agreements and mistakes are at least partially 
eliminated when reliable taggers and parsers are 
available for preprocessing 
5 Discussion 
The result of the inter-annotator agreement 
test indicates that the writing style rather than 
the contents of the research abstracts is the 
source of the difficulty in tree annotation. Con-
trary to the expectation that the lack of domain 
knowledge causes a problem in annotation on 
attachments of modifiers, the number of cases 
where annotation of modifier attachment needs 
domain knowledge is small. This indicates that 
linguists can annotate most of syntactic structure 
without an expert level of domain knowledge.  
A major source of difficulty is coordination, 
especially the ones involving ellipsis. Coordina-
tion is reported to be difficult phenomena in an-
notation of different levels in the GENIA corpus 
(Tateisi and Tsujii, 2004), (Kim et al, 2003). In 
addition to the fact that this is the major source 
of inter-annotator agreement, the annotator often 
commented the coordinated structure as ?unsure?. 
The problem of coordination can be divided into 
two with different nature: one is that the annota-
tion policy is still not well-established for the 
coordination involving ellipsis, and the other is 
an ambiguity when the coordinated phrase has 
modifiers.  
Syntax annotation of coordination with ellip-
sis is difficult in general but the more so in an-
notation of abstracts than in the case of general 
texts, because in abstracts authors tend to pack 
information in limited number of words. The 
PTB guideline dedicates a long section for this 
phenomena and allows alternatives in annotation, 
but there are still cases which are not well-
covered by the scheme. For example, in addition 
to the disagreement, the phrase illustrated in 
Figure 2a and Figure 2b shows another problem 
of the annotation scheme. Both annotators fail to 
indicate that it is ?mediated? that was to be after 
?IL-1? because there is no mechanism of 
coindexing a null element with a part of a token.  
This problem of ellipsis can frequently occur 
in research abstracts, and it can be argued that 
the tokenization criteria must be changed for 
texts in biomedical domain (Yamamoto and Sa-
tou, 2004) so that such fragment as ?IL-18? and 
?mediated? in ?IL-18-ediated? should be regarede 
as separate tokens. The Pennsylvania biology 
corpus (Kulick et al, 2004) partially solves this 
problem by separating a token where two or 
more subtokens are connected with hyphens, but 
in the cases where a shared part of the word is 
not separated by a hyphen (e.g. ?metric? of ?ste-
reo- and isometric alleles?) the word including 
the part is left uncut. The current GTB follows 
the GENIA corpus that it retains the tokeniza-
tion criteria of the original Penn Treebank, but 
this must be reconsidered in future. 
 For analysis of coordination with ellipsis, if 
the information on full forms is available, one 
strategy would be to leave the inside structure of 
coordination unannotated in the treebank corpus 
(and in the phase of text analysis the structure is 
not established in the phase of parsing but with a 
different mechanism) and later merge it with the 
coordination structure annotation. The GENIA 
term corpus annotates the full form of a techni-
cal term whose part is omitted in the surface as 
an attribute of the ?<cons>? element indicating a 
technical term (Kim et al, 2003). In the above-
mentioned Pennsylvania corpus, a similar 
mechanism (?chaining?) is used for recovering 
the full form of named entities. However, in 
both corpora, no such information is available 
outside the terms/entities.  
The cases where scope of modification in 
coordinated phrases is problematic are few but 
they are more difficult in abstracts than in gen-
eral texts because the resolution of ambiguity 
needs domain knowledge. If term/entity annota-
tion is already done, that information can help 
resolve this type of ambiguity, but again the 
problem is that outside the terms/entities such 
information is not available. It would be practi-
cal to have the structure flat but specially 
marked when the tree annotators are unsure and 
have a domain expert resolve the ambiguity, as 
the sentences that needs such intervention seems 
few. Some cases of ambiguity in modifier at-
tachment (which do not involve coordination) 
can be solved with similar process. 
We believe that other type of disagreements 
can be solved with supplementing criteria for 
linguistic phenomena not well-covered by the 
scheme, and annotator training. Automatic pre-
processing by POS taggers and parsers can also 
help increase the consistent annotation. 
224
6 Conclusion 
A subset of the GENIA corpus is annotated 
for syntactic (tree) structure. Inter-annotator 
agreement test indicated that the annotation can 
be done stably by linguists without much 
knowledge in biology, provided that proper 
guideline is established for linguistic phenomena 
particular to scientific research abstracts. We 
have made the 500-abstract corpus in both XML 
and PTB formats and made it publicly available 
as ?the GENIA Treebank beta version? (GTB-
beta). We are in further cleaning up process of 
the 500-abstract set, and at the same time, initial 
annotation of the remaining abstracts is being 
done, so that the full GENIA set of 2000 ab-
stracts will be annotated with tree structure.  
For parsers to be useful for information ex-
traction, they have to establish a map between 
syntactic structure and more semantic predicate-
argument structure, and between the linguistic 
predicate-argument structures to the factual rela-
tion to be extracted. Annotation of various in-
formation on a same set of text can help 
establish these maps. For the factual relations, 
we are annotating relations between proteins and 
genes in cooperation with a group of biologists. 
For predicate-argument annotation, we are in-
vestigating the use of the parse results of the 
Enju parser. 
Acknowledgments 
The authors are grateful to annotators and col-
leagues that helped the construction of the cor-
pus. This work is partially supported by Grant-
in-Aid for Scientific Research on Priority Area 
C ?Genome Information Science? from the Min-
istry of Education, Culture, Sports, Science and 
Technology of Japan. 
References 
 Brants,T.(2000). TnT: a statistical part-of-speech 
tagger, Proceedings of the sixth conference on Ap-
plied natural language processing, pp.224-231, 
Morgan Kaufmann Publishers Inc.  
Beis.A., Ferguson,M., Katz,K., and Mac-
Intire,R.(1995). Bracketing Guidelines for Tree-
bank II Style: Penn Treebank Project, University 
of Pennsylvania 
Hasida, K. (2000). GDA: Annotated Document as 
Intelligent Content.  Proceedings of 
COLING?2000 Workshop on Semantic Annotation 
and Intelligent Content. 
Kazama,J., Miyao,Y., and Tsujii,J.(2001) A Maxi-
mum Entropy Tagger with Unsupervised Hidden 
Markov Models, Proceedings of the Sixth Natural 
Language Processing Pacific Rim Symposium, pp. 
333-340.  
Kim,J-D, Ohta,T., Tateisi,Y. and Tsujii,J. (2003). 
GENIA corpus - a semantically annotated corpus 
for bio-textmining. Bioinformatics. 19(suppl. 1). 
pp. i180-i182. Oxford University Press.  
Koike,A., Niwa,Y., and Takagi,T. (2004) Automatic 
extraction of gene/protein biological functions 
from biomedical text. Bioinformatics, Advanced 
Access published on October 27, 2004; 
doi:10.1093/bioinformatics/bti084.Oxford Univer-
sity Press. 
Kulick,S., Bies,A., Liberman,M., Mandel,M., 
McDonald,R., Palmer,M., Schein,A., Ungar,L., 
Winters,S. and White,P. (2004)  Integrated Anno-
tation for Biomedical Information Extraction. 
BioLINK 2004: Linking Biological Literature, On-
tologies, and Databases, pp. 61-68.Association for 
Computational Linguistics. 
Miyao,Y. and Tsujii,J. (2004a). Deep Linguistic 
Analysis for the Accurate Identification of Predi-
cate-Argument Relations. Proceedings of 
COLING 2004. pp. 1392-1397. 
Tateisi,Y. and Tsujii,J. (2004). Part-of-Speech Anno-
tation of Biology Research Abstracts. Proceedings 
of the 4th International Conference on Language 
Resource and Evaluation (LREC2004). IV. pp. 
1267-1270, European Language Resources Asso-
ciation. 
Wermter, J. and Hahn, U. (2004). An annotated Ger-
man-language medical text corpus. GMDS 2004 
meeting, 
http://www.egms.de/en/meetings/gmds2004/04gm
ds168.shtml. 
Yamamoto,K., and Satou,K (2004). Low-level Text 
Processing for Life Science, Proceedings of the 
SIG meeting on Natural Language Processing, In-
formation Processing Society of Japan, IPSJ-
SIGNL-159 (In Japanese). 
 Yang,XF., Zhou,GD., Su,J., and Tan.,CL (2004). 
Improving Noun Phrase Coreference Resolution 
by Matching Strings. Proceedings of 1st Interna-
tional Joint Conference on Natural Language 
Processing (IJCNLP'2004), pp226-233.
 
225
A Debug Tool for Practical Grammar Development
Akane Yakushiji? Yuka Tateisi?? Yusuke Miyao?
?Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
?CREST, JST (Japan Science and Technology Corporation)
Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 JAPAN
{akane,yucca,yusuke,yoshinag,tsujii}@is.s.u-tokyo.ac.jp
Naoki Yoshinaga? Jun?ichi Tsujii??
Abstract
We have developed willex, a tool that
helps grammar developers to work effi-
ciently by using annotated corpora and
recording parsing errors. Willex has two
major new functions. First, it decreases
ambiguity of the parsing results by com-
paring them to an annotated corpus and
removing wrong partial results both au-
tomatically and manually. Second, willex
accumulates parsing errors as data for the
developers to clarify the defects of the
grammar statistically. We applied willex
to a large-scale HPSG-style grammar as
an example.
1 Introduction
There is an increasing need for syntactical parsers
for practical usages, such as information extrac-
tion. For example, Yakushiji et al (2001) extracted
argument structures from biomedical papers using
a parser based on XHPSG (Tateisi et al, 1998),
which is a large-scale HPSG. Although large-scale
and general-purpose grammars have been devel-
oped, they have a problem of limited coverage.
The limits are derived from deficiencies of gram-
mars themselves. For example, XHPSG cannot treat
coordinations of verbs (ex. ?Molybdate slowed but
did not prevent the conversion.?) nor reduced rel-
atives (ex. ?Rb mutants derived from patients with
retinoblastoma.?). Finding these grammar defects
and modifying them require tremendous human ef-
fort.
Hence, we have developed willex that helps to im-
prove the general-purpose grammars. Willex has two
major functions. First, it reduces a human workload
to improve the general-purpose grammar through
using language intuition encoded in syntactically
tagged corpora in XML format. Second, it records
data of grammar defects to allow developers to have
a whole picture of parsing errors found in the target
corpora to save debugging time and effort by priori-
tizing them.
2 What Is the Ideal Grammar Debugging?
There are already other grammar developing tools,
such as a grammar writer of XTAG (Paroubek et al,
1992), ALEP (Schmidt et al, 1996), ConTroll (Go?tz
and Meurers, 1997), a tool by Nara Institute of Sci-
ence and Technology (Miyata et al, 1999), and [incr
tsdb()] (Oepen et al, 2002). But these tools have
following problems; they largely depend on human
debuggers? language intuition, they do not help users
to handle large amount of parsing results effectively,
and they let human debuggers correct the bugs one
after another manually and locally.
To cope with these shortcomings, willex proposes
an alternative method for more efficient debugging
process.
The workflow of the conventional grammar devel-
oping tools and willex are different in the following
ways. With the conventional tools, human debug-
gers must check each sentence to find out grammar
defects and modify them one by one. On the other
hand, with willex human debuggers check sentences
that are tagged with syntactical structure, one by
one, find grammar defects, and record them, while
willex collects the whole grammar defect records.
Then human debuggers modify the found grammar
defects. This process allows human debuggers to
make priority over defects that appear more fre-
quently in the corpora, or defects that are more crit-
ical for purposes of syntactical parsing. Indeed, it
is possible for human debuggers using the conven-
tional tools to collect and modify the defects but
willex saves the trouble of human debuggers to col-
lect defects to modify them more efficiently.
3 Functions of willex
To create the new debugging tool, we have extended
will (Imai et al, 1998). Will is a browser of parsing
results of grammars based on feature structures. Will
and willex are implemented in JAVA.
3.1 Using XML Tagged Corpora
Willex uses sentence boundaries, word chunking,
and POSs/labels encoded in XML tagged corpora.
First, with the information of sentence boundaries
and word chunking, ambiguity of sentences is re-
duced, and ambiguity at parsing phase is also re-
duced. A parser connected to willex is assumed to
produce only results consistent with the information.
An example is shown in Figure 1 (<su> is a senten-
tial tag and <np> is a tag for noun phrases).
I  saw  a girl  with a telescope
I  saw  a girl  with a telescope


<su> I saw <np> a girl with a telescope </np></su>
Figure 1: An example of pa sing results along with
word chunking
Next, willex compares POSs/labels encoded in
XML tags and parsing results, and deletes improper
parsing trees. Therefore, it reduces numbers of par-
tial parsing trees, which appear in the way of parsing
and should be checked by human debuggers. In ad-
dition, human debuggers can delete partial parsing
trees manually later. Figure 2 shows a concrete ex-
ample. (NP and S are labels for noun and sentential
phrases respectively.)
POS/label from Tagged Corpus
POSs/labels from Partial Results
<NP> A cat </NP> knows everything
A      cat
D      N N       V
A      cat
NP S  
Figure 2: An example of deletion by using
POSs/labels
3.2 Output of Grammar Defects
Willex has a function to output information of gram-
mar defects into a file in order to collect the de-
fects data and treat them statistically. In addition,
we can save a log of debugging experiences which
show what grammar defects are found.
An example of an output file is shown in Table
1. It includes sentence numbers, word ranges in
which parsing failed, and comments input by a hu-
man debugger. For example, the first row of the ta-
ble means that the sentence #0 has coordinations of
verb phrases at position #3?#12, which cannot be
parsed. ?OK? in the second row means the sen-
tence is parsed correctly (i.e., no grammar defects
are found in the sentence). The third row means that
the word #4 of the sentence #2 has no proper lexical
entry.
The word ranges are specified by human debug-
gers using a GUI, which shows parsing results in
CKY tables and parse trees. The comments are input
by human debuggers in a natural language or chosen
from the list of previous comments. A postprocess-
ing module of willex sorts the error data by the com-
ments to help statistical analysis.
Table 1: An example of file output
Sentence # Word # comment
0 3?12 V-V coordination
1 ? OK
2 4 no lexical entry
4 Experiments and Discussion
We have applied willex to rental-XTAG, an HPSG-
style grammar converted from the XTAG English
grammar (The XTAG Research Group, 2001) by a
grammar conversion (Yoshinaga and Miyao, 2001).1
The corpus used is MEDLINE abstracts with tags
based on a slightly modified version of GDA-
DTD2 (Hasida, 2003). The corpus is ?partially
parsed?; the attachments of prepositional phrases are
annotated manually.
The tags do not always specify the correct struc-
tures based on rental-XTAG (i.e., the grammar as-
sumed by tags is different from rental-XTAG), so we
prepared a POS/label conversion table. We can use
tagged corpora based on various grammars different
from the grammar that the parser is assuming by us-
ing POS/label conversion tables.
We investigated 208 sentences (average 24.2
words) from 26 abstracts. 73 sentences were parsed
successfully and got correct results. Thus the cover-
age was 35.1%.
4.1 Qualitative Evaluation
Willex received three major positive feedbacks from
a user; first, the function of restricting partial results
was helpful, as it allows human debuggers to check
fewer results, second, the function to delete incorrect
partial results manually was useful, because there
are some cases that tags do not specify POSs/labels,
and third, human debuggers could use the record-
ing function to make notes to analyze them carefully
later.
However, willex also received some negative eval-
uations; the process of locating the cause of pars-
ing failure in a sentence was found to be a bit trou-
blesome. Also, willex loses its accuracy if the hu-
man debuggers themselves have trouble understand-
ing the correct syntactical structure of a sentence.3
1Since XTAG and rental-XTAG generate equivalent parse
results for the same input, debugging rental-XTAG means de-
bugging XTAG itself.
2GDA has no tags which specify prepositional phrases, so
we add <prep> and <prepp>.
3Thus, we divided the process of identifying grammar de-
fects to two steps. First, a non-expert roughly classifies pars-
ing errors and records temporary memorandums. Then, the
non-expert shows typical examples of sentences in each class
to experts and identifies grammar defects based on experts? in-
ference. Here, we can make use of the recording function of
We found from these evaluations that the func-
tions of willex can be used effectively, though more
automation is needed.
4.2 Quantitative Evaluation
Figure 3 shows the decrease in partial parsing trees
caused by using the tagged corpus. (Data of 10 sen-
tences among the 208 sentences are shown.) The
graph shows that human workload was reduced by
using the tagged corpus.
0
5000
10000
15000
20000
25000
30000
35000
10 15 20 25 30 35 40
n
u
m
b
e
r
 
o
f
 
p
a
r
t
i
a
l
 
r
e
s
u
l
t
s
length of a sentence (number of words)
without any info.with chunk info.with chunk and POS/label info.
Figure 3: Examples of numbers of partial results
4.3 Defects of rental-XTAG
Table 2 shows the defects of rental-XTAG which are
found by using willex.
Table 2: The defects of rental-XTAG
the defects of rental-XTAG #
no lexical entry 62
cannot handle reduced relative 35
cannot handle V-V coordination 22
Adjective does not post-modify NP 9
cannot parse ?, but not? 4
cannot handle objective to-infinitive 3
?, which ...? does not post-modify NP 3
cannot handle reduced as-relative clause 2
cannot parse ?greater than?(?>?) 2
misc. 17
From this table, it is inferred that (1) lack of lexi-
cal entries, (2) inability to parse reduced relative and
willex.
(3) inability to parse coordinations of verbs are seri-
ous problems of rental-XTAG.
4.4 Conflicts Between the Modified GDA and
rental-XTAG
Conflicts between rental-XTAG and the grammar on
which the modified GDA based cause parsing fail-
ures. Statistics of the conflicts is shown in Table 3.
Table 3: Conflicts between the modified GDA and
rental-XTAG
modified GDA rental-XTAG #
adjectival phrase verbal phrase 36
bracketing except ?,? 10
bracketing of ?,? 8
treatment of omitted words 2
misc. 5
These conflicts cannot be resolved by a simple
POS/label conversion table. One resolution is insert-
ing a preprocess module that deletes and moves tags
which cause conflicts.
We do not consider these conflicts as grammar de-
fects but the difference of grammars to be absorbed
in the conversion phase.
5 Conclusion and Future Work
We developed a debug tool, willex, which uses XML
tagged corpora and outputs information of grammar
defects. By using tagged corpora, willex succeeded
to reduce human workload. And by recording gram-
mar defects, it provides debugging environment with
a bigger perspective. But there remains a prob-
lem that a simple POS/label conversion table is not
enough to resolve conflicts of a debugged grammar
and a grammar assumed by tags. The tool should
support to handle the complicated conflicts.
In the future, we will try to modify willex to infer
causes of parsing errors (semi-)automatically. It is
difficult to find a point of parsing failure automati-
cally, because subsentences that have no correspon-
dent partial results are not always the failed point.
Hence, we will expand willex to find the longest
subsentences that are parsed successfully. Words,
POS/labels and features of the subsentences can be
clues to infer the causes of parsing errors.
References
Thilo Go?tz and Walt Detmar Meurers. 1997. The Con-
Troll system as large grammar development platform.
In Proc. of Workshop on Computational Environments
for Grammar Development and Linguistic Engineer-
ing, pages 38?45.
Koiti Hasida. 2003. Global docu-
ment annotation (GDA). available in
http://www.i-content.org/GDA/.
Hisao Imai, Yusuke Miyao, and Jun?ichi Tsujii. 1998.
GUI for an HPSG parser. In Information Processing
Society of Japan SIG Notes NL-127, pages 173?178,
September. In Japanese.
Takashi Miyata, Kazuma Takaoka, and Yuji Mat-
sumoto. 1999. Implementation of GUI debugger for
unification-based grammar. In Information Process-
ing Society of Japan SIG Notes NL-129, pages 87?94,
January. In Japanese.
Stephan Oepen, Emily M. Bender, Uli Callmeier, Dan
Flickinger, and Melanie Siegel. 2002. Parallel dis-
tributed grammar engineering for practical applica-
tions. In Proc. of the Workshop on Grammar Engi-
neering and Evaluation, pages 15?21.
Patrick Paroubek, Yves Schabes, and Aravind K. Joshi.
1992. XTAG ? a graphical workbench for developing
Tree-Adjoining grammars. In Proc. of the 3rd Confer-
ence on Applied Natural Language Processing, pages
216?223.
Paul Schmidt, Axel Theofilidis, Sibylle Rieder, and
Thierry Declerck. 1996. Lean formalisms, linguis-
tic theory, and applications. Grammar development in
ALEP. In Proc. of COLING ?96, volume 1, pages
286?291.
Yuka Tateisi, Kentaro Torisawa, Yusuke Miyao, and
Jun?ichi Tsujii. 1998. Translating the XTAG english
grammar to HPSG. In Proc. of TAG+4 workshop,
pages 172?175.
The XTAG Research Group. 2001. A Lex-
icalized Tree Adjoining Grammar for English.
Technical Report IRCS Research Report 01-03,
IRCS, University of Pennsylvania. available in
http://www.cis.upenn.edu/?xtag/.
Akane Yakushiji, Yuka Tateisi, Yusuke Miyao, and
Jun?ichi Tsujii. 2001. Event extraction from biomedi-
cal papers using a full parser. In Pacific Symposium on
Biocomputing 2001, pages 408?419, January.
Naoki Yoshinaga and Yusuke Miyao. 2001. Grammar
conversion from LTAG to HPSG. In Proc. of the sixth
ESSLLI Student Session, pages 309?324.
Finding Anchor Verbs for Biomedical IE
Using Predicate-Argument Structures
Akane YAKUSHIJI? Yuka TATEISI?? Yusuke MIYAO?
?Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
?CREST, JST (Japan Science and Technology Agency)
Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 JAPAN
{akane,yucca,yusuke,tsujii}@is.s.u-tokyo.ac.jp
Jun?ichi TSUJII??
Abstract
For biomedical information extraction, most sys-
tems use syntactic patterns on verbs (anchor verbs)
and their arguments. Anchor verbs can be se-
lected by focusing on their arguments. We propose
to use predicate-argument structures (PASs), which
are outputs of a full parser, to obtain verbs and their
arguments. In this paper, we evaluated PAS method
by comparing it to a method using part of speech
(POSs) pattern matching. POS patterns produced
larger results with incorrect arguments, and the re-
sults will cause adverse effects on a phase selecting
appropriate verbs.
1 Introduction
Research in molecular-biology field is discovering
enormous amount of new facts, and thus there is
an increasing need for information extraction (IE)
technology to support database building and to find
novel knowledge in online journals.
To implement IE systems, we need to construct
extraction rules, i.e., rules to extract desired infor-
mation from processed resource. One subtask of the
construction is defining a set of anchor verbs, which
express realization of desired information in natural
language text.
In this paper, we propose a novel method of
finding anchor verbs: extracting anchor verbs from
predicate-argument structures (PASs) obtained by
full parsing. We here discuss only finding anchor
verbs, although our final purpose is construction
of extraction rules. Most anchor verbs take topi-
cal nouns, i.e., nouns describing target entities for
IE, as their arguments. Thus verbs which take top-
ical nouns can be candidates for anchor verbs. Our
method collects anchor verb candidates by choosing
PASs whose arguments are topical nouns. Then, se-
mantically inappropriate verbs are filtered out. We
leave this filtering phase as a future work, and dis-
cuss the acquisition of candidates. We have also in-
vestigated difference in verbs and their arguments
extracted by naive POS patterns and PAS method.
When anchor verbs are found based on whether
their arguments are topical nouns, like in (Hatzivas-
siloglou and Weng, 2002), it is important to obtain
correct arguments. Thus, in this paper, we set our
goal to obtain anchor verb candidates and their cor-
rect arguments.
2 Background
There are some works on acquiring extraction rules
automatically. Sudo et al (2003) acquired subtrees
derived from dependency trees as extraction rules
for IE in general domains. One problem of their sys-
tem is that dependency trees cannot treat non-local
dependencies, and thus rules acquired from the con-
structions are partial. Hatzivassiloglou and Weng
(2002) used frequency of collocation of verbs and
topical nouns and verb occurrence rates in several
domains to obtain anchor verbs for biological inter-
action. They used only POSs and word positions
to detect relations between verbs and topical nouns.
Their performance was 87.5% precision and 82.4%
recall. One of the reasons of errors they reported is
failures to detect verb-noun relations.
To avoid these problems, we decided to use PASs
obtained by full parsing to get precise relations be-
tween verbs and their arguments. The obtained pre-
cise relations will improve precision. In addition,
PASs obtained by full parsing can treat non-local
dependencies, thus recall will also be improved.
The sentence below is an example which sup-
ports advantage of full parsing. A gerund ?activat-
ing? takes a non-local semantic subject ?IL-4?. In
full parsing based on Head-Driven Phrase Structure
Grammar (HPSG) (Sag and Wasow, 1999), the sub-
ject of the whole sentence and the semantic subject
of ?activating? are shared, and thus we can extract
the subject of ?activating?.
IL-4 may mediate its biological effects by activat-
ing a tyrosine-phosphorylated DNA binding pro-
tein.
interacts
ARG1 it
1 with
MODIFY
ARG1      regions
2
1
of
MODIFY
ARG1 molecules
2
,
,
(a) (b) (c)
It interacts with non-polymorphic regions of major his-
tocompatibility complex class II molecules.
Figure 1: PAS examples
with
MODIFY
interacts
ARG1 it
ARG1 regions
Core verb
serves
ARG1      IL-5
1
ARG2
to
ARG1
ARG2
stimulate
ARG1
ARG2 binding
1
Core verb
1
Figure 2: Core verbs of PASs
3 Anchor Verb Finding by PASs
By using PASs, we extract candidates for anchor
verbs from a sentence in the following steps:
1. Obtain all PASs of a sentence by a full
parser. The PASs correspond not only to verbal
phrases but also other phrases such as preposi-
tional phrases.
2. Select PASs which take one or more topical
nouns as arguments.
3. From the selected PASs in Step 2, select PASs
which include one or more verbs.
4. Extract a core verb, which is the innermost ver-
bal predicate, from each of the chosen PASs.
In Step 1, we use a probabilistic HPSG parser
developed by Miyao et al (2003), (2004). PASs
obtained by the parser are illustrated in Figure 1.1
Bold words are predicates. Arguments of the predi-
cates are described in ARGn (n = 1, 2, . . .). MOD-
IFY denotes the modified PAS. Numbers in squares
denote shared structures. Examples of core verbs
are illustrated in Figure 2. We regard all arguments
in a PAS are arguments of the core verb.
Extraction of candidates for anchor verbs from
the sentence in Figure 1 is as follows. Here, ?re-
gions? and ?molecules? are topical nouns.
In Step 1, we obtain all the PASs, (a), (b) and (c),
in Figure 1.
1Here, named entities are regarded as chunked, and thus
internal structures of noun phrases are not illustrated.
Next, in Step 2, we check each argument of (a),
(b) and (c). (a) is discarded because it does not have
a topical noun argument.2 (b) is selected because
ARG1 ?regions? is a topical noun. Similarly, (c) is
selected because of ARG1 ?molecules?.
And then, in Step 3, we check each POS of a
predicate included in (b) and (c). (b) is selected be-
cause it has the verb ?interacts? in 1 which shares
the structure with (a). (c) is discarded because it
includes no verbs.
Finally, in Step 4, we extract a core verb from (b).
(b) includes 1 asMODIFY, and the predicate of 1
is the verb, ?interacts?. So we extract it.
4 Experiments
We investigated the verbs and their arguments ex-
tracted by PAS method and POS pattern matching,
which is less expressive in analyzing sentence struc-
tures but would be more robust.
For topical nouns and POSs, we used the GENIA
corpus (Kim et al, 2003), a corpus of annotated ab-
stracts taken from National Library of Medicine?s
MEDLINE database. We defined topical nouns as
the names tagged as protein, peptide, amino acid,
DNA, RNA, or nucleic acid. We chose PASs which
take one or more topical nouns as an argument or
arguments, and substrings matched by POS patterns
which include topical nouns. All names tagged in
the corpus were replaced by their head nouns in
order to reduce complexity of sentences and thus
reduce the task of the parser and the POS pattern
matcher.
4.1 Implementation of PAS method
We implemented PAS method on LiLFeS, a
unification-based programming system for typed
feature structures (Makino et al, 1998; Miyao et al,
2000).
The selection in Step 2 described in Section 3
is realized by matching PASs with nine PAS tem-
plates. Four of the templates are illustrated in Fig-
ure 3.
4.2 POS Pattern Method
We constructed a POS pattern matcher with a par-
tial verb chunking function according to (Hatzivas-
siloglou and Weng, 2002). Because the original
matcher has problems in recall (its verb group de-
tector has low coverage) and precision (it does not
consider other words to detect relations between
verb groups and topical nouns), we implemented
2(a) may be selected if the anaphora (?it?) is resolved. But
we regard anaphora resolving is too hard task as a subprocess
of finding anchor verbs.
*any*
ARG1 N1
N1 = topical noun
*any*
ARG1 N1
ARG2 N2
N1 = topical noun
or N2 = topical noun
? ?
*any*
MODIFY *any*
ARG1 N1
N1 = topical noun
*any*
MODIFY *any*
ARG1 N1
ARG2 N2
N1 = topical noun
or N2 = topical noun
Figure 3: PAS templates
N ? V G ? N
N ? V G
V G ? N
N : is a topical noun
V G: is a verb group which is accepted by a finite state
machine described in (Hatzivassiloglou andWeng, 2002)
or one of {VB, VBD, VBG, VBN, VBP, VBZ}
?: is 0?4 tokens which do not include {FW, NN, NNS,
NNP, NNPS, PRP, VBG, WP, *}
(Parts in Bold letters are added to the patterns of Hatzi-
vassiloglou and Weng (2002).)
Figure 4: POS patterns
our POS pattern matcher as a modified version of
one in (Hatzivassiloglou and Weng, 2002).
Figure 4 shows patterns in our experiment. The
last verb of V G is extracted if all of Ns are topical
nouns. Non-topical nouns are disregarded. Adding
candidates for verb groups raises recall of obtained
relations of verbs and their arguments. Restriction
on intervening tokens to non-nouns raises the preci-
sion, although it decreases the recall.
4.3 Experiment 1
We extracted last verbs of POS patterns and core
verbs of PASs with their arguments from 100 ab-
stracts (976 sentences) of the GENIA corpus. We
took up not the verbs only but tuples of the verbs
and their arguments (VAs), in order to estimate ef-
fect of the arguments on semantical filtering.
Results
The numbers of VAs extracted from the 100 ab-
stracts using POS patterns and PASs are shown in
Table 1. (Total ? VAs of verbs not extracted by the
other method) are not the same, because more than
one VA can be extracted on a verb in a sentence.
POS patterns method extracted more VAs, although
POS patterns PASs
Total 1127 766
VAs of verbs
not extracted 478 105
by the other
Table 1: Numbers of VAs extracted from the 100
abstracts
Appropriate Inappropriate Total
Correct 43 12 55
Incorrect 20 23 43
Total 63 35 98
Table 2: Numbers of VAs extracted by POS patterns
(in detail)
their correctness is not considered.
4.4 Experiment 2
For the first 10 abstracts (92 sentences), we man-
ually investigated whether extracted VAs are syn-
tactically or semantically correct. The investigation
was based on two criteria: ?appropriateness? based
on whether the extracted verb can be used for an an-
chor verb and ?correctness? based on whether the
syntactical analysis is correct, i.e., whether the ar-
guments were extracted correctly.
Based on human judgment, the verbs that rep-
resent interactions, events, and properties were se-
lected as semantically appropriate for anchor verbs,
and the others were treated as inappropriate. For ex-
ample, ?identified? in ?We identified ZEBRA pro-
tein.? is not appropriate and discarded.
We did not consider non-topical noun arguments
for POS pattern method, whereas we considered
them for PAS method. Thus decision on correctness
is stricter for PAS method.
Results
The manual investigation results on extracted
VAs from the 10 abstracts using POS patterns and
PASs are shown in Table 2 and 3 respectively.
POS patterns extracted more (98) VAs than PASs
(75), but many of the increment were from incor-
rect POS pattern matching. By POS patterns, 43
VAs (44%) were extracted based on incorrect anal-
ysis. On the other hand, by PASs, 20 VAs (27%)
were extracted incorrectly. Thus the ratio of VAs
extracted by syntactically correct analysis is larger
on PAS method.
POS pattern method extracted 38 VAs of verbs
not extracted by PAS method and 7 of them are cor-
rect. For PAS method, correspondent numbers are
Appropriate Inappropriate Total
Correct 44 11 55
Incorrect 14 6 20
Total 58 17 75
Table 3: Numbers of VAs extracted by PASs (in de-
tail)
11 and 4 respectively. Thus the increments tend to
be caused by incorrect analysis, and the tendency is
greater in POS pattern method.
Since not all of verbs that take topical nouns are
appropriate for anchor verbs, automatic filtering is
required. In the filtering phase that we leave as a
future work, we can use semantical classes and fre-
quencies of arguments of the verbs. The results with
syntactically incorrect arguments will cause adverse
effect on filtering because they express incorrect re-
lationship between verbs and arguments. Since the
numbers of extracted VAs after excluding the ones
with incorrect arguments are the same (55) between
PAS and POS pattern methods, it can be concluded
that the precision of PAS method is higher. Al-
though there are few (7) correct VAs which were
extracted by POS pattern method but not by PAS
method, we expect the number of such verbs can be
reduced using a larger corpus.
Examples of appropriate VAs extracted by only
one method are as follows: (A) is correct and (B)
incorrect, extracted by only POS pattern method,
and (C) is correct and (D) incorrect, extracted by
only PAS method. Bold words are extracted verbs
or predicates and italic words their extracted argu-
ments.
(A) This delay is associated with down-regulation
of many erythroid cell-specific genes, including
alpha- and beta-globin, band 3, band 4.1, and . . . .
(B) . . . show that several elements in the . . . region of
the IL-2R alpha gene contribute to IL-1 respon-
siveness, . . . .
(C) The CD4 coreceptor interacts with non-
polymorphic regions of . . . molecules on
non-polymorphic cells and contributes to T cell
activation.
(D) Whereas activation of the HIV-1 enhancer follow-
ing T-cell stimulation is mediated largely through
binding of the . . . factor NF-kappa B to two adja-
cent kappa B sites in . . . .
5 Conclusions
We have proposed a method of extracting anchor
verbs as elements of extraction rules for IE by us-
ing PASs obtained by full parsing. To compare
our method with more naive and robust methods,
we have extracted verbs and their arguments using
POS patterns and PASs. POS pattern method could
obtain more candidate verbs for anchor verbs, but
many of them were extracted with incorrect argu-
ments by incorrect matching. A later filtering pro-
cess benefits by precise relations between verbs and
their arguments which PASs obtained. The short-
coming of PAS method is expected to be reduced by
using a larger corpus, because verbs to extract will
appear many times in many forms. One of the future
works is to extend PAS method to handle events in
nominalized forms.
Acknowledgements
This work was partially supported by Grant-in-
Aid for Scientific Research on Priority Areas (C)
?Genome Information Science? from the Ministry
of Education, Culture, Sports, Science and Technol-
ogy of Japan.
References
Vasileios Hatzivassiloglou and Wubin Weng. 2002.
Learning anchor verbs for biological interaction
patterns from published text articles. Interna-
tional Journal of Medical Informatics, 67:19?32.
Jin-Dong Kim, Tomoko Ohta, Yuka Teteisi, and
Jun?ichi Tsujii. 2003. GENIA corpus ? a se-
mantically annotated corpus for bio-textmining.
Bioinformatics, 19(suppl. 1):i180?i182.
Takaki Makino, Minoru Yoshida, Kentaro Tori-
sawa, and Jun-ichi Tsujii. 1998. LiLFeS ? to-
wards a practical HPSG parser. In Proceedings
of COLING-ACL?98.
Yusuke Miyao, Takaki Makino, Kentaro Torisawa,
and Jun-ichi Tsujii. 2000. The LiLFeS abstract
machine and its evaluation with the LinGO gram-
mar. Natural Language Engineering, 6(1):47 ?
61.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi
Tsujii. 2003. Probabilistic modeling of argument
structures including non-local dependencies. In
Proceedings of RANLP 2003, pages 285?291.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi
Tsujii. 2004. Corpus-oriented grammar develop-
ment for acquiring a Head-driven Phrase Struc-
ture Grammar from the Penn Treebank. In Pro-
ceedings of IJCNLP-04.
Ivan A. Sag and Thomas Wasow. 1999. Syntactic
Theory. CSLI publications.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern represen-
tation model for automatic IE pattern acquisition.
In Proceedings of ACL 2003, pages 224?231.
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 17?20,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An intelligent search engine and GUI-based efficient MEDLINE search
tool based on deep syntactic parsing
Tomoko Ohta
Yoshimasa Tsuruoka??
Jumpei Takeuchi
Jin-Dong Kim
Yusuke Miyao
Akane Yakushiji?
Kazuhiro Yoshida
Yuka Tateisi?
Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
{okap, yusuke, ninomi, tsuruoka, akane, kmasuda, tj jug,
kyoshida, harasan, jdkim, yucca, tsujii}@is.s.u-tokyo.ac.jp
Takashi Ninomiya?
Katsuya Masuda
Tadayoshi Hara
Jun?ichi Tsujii
Abstract
We present a practical HPSG parser for
English, an intelligent search engine to re-
trieve MEDLINE abstracts that represent
biomedical events and an efficient MED-
LINE search tool helping users to find in-
formation about biomedical entities such
as genes, proteins, and the interactions be-
tween them.
1 Introduction
Recently, biomedical researchers have been fac-
ing the vast repository of research papers, e.g.
MEDLINE. These researchers are eager to search
biomedical correlations such as protein-protein or
gene-disease associations. The use of natural lan-
guage processing technology is expected to re-
duce their burden, and various attempts of infor-
mation extraction using NLP has been being made
(Blaschke and Valencia, 2002; Hao et al, 2005;
Chun et al, 2006). However, the framework of
traditional information retrieval (IR) has difficulty
with the accurate retrieval of such relational con-
cepts. This is because relational concepts are
essentially determined by semantic relations of
words, and keyword-based IR techniques are in-
sufficient to describe such relations precisely.
This paper proposes a practical HPSG parser
for English, Enju, an intelligent search engine for
the accurate retrieval of relational concepts from
?Current Affiliation:
?School of Informatics, University of Manchester
?Knowledge Research Center, Fujitsu Laboratories LTD.
?Faculty of Informatics, Kogakuin University
?Information Technology Center, University of Tokyo
F-Score
GENIA treebank Penn Treebank
HPSG-PTB 85.10% 87.16%
HPSG-GENIA 86.87% 86.81%
Table 1: Performance for Penn Treebank and the
GENIA corpus
MEDLINE, MEDIE, and a GUI-based efficient
MEDLINE search tool, Info-PubMed.
2 Enju: An English HPSG Parser
We developed an English HPSG parser, Enju 1
(Miyao and Tsujii, 2005; Hara et al, 2005; Ni-
nomiya et al, 2005). Table 1 shows the perfor-
mance. The F-score in the table was accuracy
of the predicate-argument relations output by the
parser. A predicate-argument relation is defined
as a tuple ??,wh, a, wa?, where ? is the predi-
cate type (e.g., adjective, intransitive verb), wh
is the head word of the predicate, a is the argu-
ment label (MOD, ARG1, ..., ARG4), and wa is
the head word of the argument. Precision/recall
is the ratio of tuples correctly identified by the
parser. The lexicon of the grammar was extracted
from Sections 02-21 of Penn Treebank (39,832
sentences). In the table, ?HPSG-PTB? means that
the statistical model was trained on Penn Tree-
bank. ?HPSG-GENIA? means that the statistical
model was trained on both Penn Treebank and GE-
NIA treebank as described in (Hara et al, 2005).
The GENIA treebank (Tateisi et al, 2005) consists
of 500 abstracts (4,446 sentences) extracted from
MEDLINE.
Figure 1 shows a part of the parse tree and fea-
1http://www-tsujii.is.s.u-tokyo.ac.jp/enju/
17
Figure 1: Snapshot of Enju
ture structure for the sentence ?NASA officials
vowed to land Discovery early Tuesday at one
of three locations after weather conditions forced
them to scrub Monday?s scheduled return.?
3 MEDIE: a search engine for
MEDLINE
Figure 2 shows the top page of the MEDIE. ME-
DIE is an intelligent search engine for the accu-
rate retrieval of relational concepts from MED-
LINE 2 (Miyao et al, 2006). Prior to retrieval, all
sentences are annotated with predicate argument
structures and ontological identifiers by applying
Enju and a term recognizer.
3.1 Automatically Annotated Corpus
First, we applied a POS analyzer and then Enju.
The POS analyzer and HPSG parser are trained
by using the GENIA corpus (Tsuruoka et al,
2005; Hara et al, 2005), which comprises around
2,000 MEDLINE abstracts annotated with POS
and Penn Treebank style syntactic parse trees
(Tateisi et al, 2005). The HPSG parser generates
parse trees in a stand-off format that can be con-
verted to XML by combining it with the original
text.
We also annotated technical terms of genes and
diseases in our developed corpus. Technical terms
are annotated simply by exact matching of dictio-
2http://www-tsujii.is.s.u-tokyo.ac.jp/medie/
nary entries and the terms separated by space, tab,
period, comma, hat, colon, semi-colon, brackets,
square brackets and slash in MEDLINE.
The entire dictionary was generated by apply-
ing the automatic generation method of name vari-
ations (Tsuruoka and Tsujii, 2004) to the GENA
dictionary for the gene names (Koike and Takagi,
2004) and the UMLS (Unified Medical Language
System) meta-thesaurus for the disease names
(Lindberg et al, 1993). It was generated by ap-
plying the name-variation generation method, and
we obtained 4,467,855 entries of a gene and dis-
ease dictionary.
3.2 Functions of MEDIE
MEDIE provides three types of search, seman-
tic search, keyword search, GCL search. GCL
search provides us the most fundamental and pow-
erful functions in which users can specify the
boolean relations, linear order relation and struc-
tural relations with variables. Trained users can
enjoy all functions in MEDIE by the GCL search,
but it is not easy for general users to write ap-
propriate queries for the parsed corpus. The se-
mantic search enables us to specify an event verb
with its subject and object easily. MEDIE auto-
matically generates the GCL query from the se-
mantic query, and runs the GCL search. Figure 3
shows the output of semantic search for the query
?What disease does dystrophin cause??. This ex-
ample will give us the most intuitive understand-
ings of the proximal and structural retrieval with a
richly annotated parsed corpus. MEDIE retrieves
sentences which include event verbs of ?cause?
and noun ?dystrophin? such that ?dystrophin? is the
subject of the event verbs. The event verb and its
subject and object are highlighted with designated
colors. As seen in the figure, small sentences in
relative clauses, passive forms or coordination are
retrieved. As the objects of the event verbs are
highlighted, we can easily see what disease dys-
trophin caused. As the target corpus is already
annotated with diseases entities, MEDIE can ef-
ficiently retrieve the disease expressions.
4 Info-PubMed: a GUI-based
MEDLINE search tool
Info-PubMed is a MEDLINE search tool with
GUI, helping users to find information about
biomedical entities such as genes, proteins, and
18
Figure 2: Snapshot of MEDIE: top page?
Figure 3: Snapshot of MEDIE: ?What disease does
dystrophin cause??
the interactions between them 3.
Info-PubMed provides information from MED-
LINE on protein-protein interactions. Given the
name of a gene or protein, it shows a list of the
names of other genes/proteins which co-occur in
sentences from MEDLINE, along with the fre-
quency of co-occurrence.
Co-occurrence of two proteins/genes in the
same sentence does not always imply that they in-
teract. For more accurate extraction of sentences
that indicate interactions, it is necessary to iden-
tify relations between the two substances. We
adopted PASs derived by Enju and constructed ex-
traction patterns on specific verbs and their argu-
ments based on the derived PASs (Yakusiji, 2006).
Figure 4: Snapshot of Info-PubMed (1)
Figure 5: Snapshot of Info-PubMed (2)
Figure 6: Snapshot of Info-PubMed (3)
4.1 Functions of Info-PubMed
In the ?Gene Searcher? window, enter the name
of a gene or protein that you are interested in.
For example, if you are interested in Raf1, type
?raf1? in the ?Gene Searcher? (Figure 4). You
will see a list of genes whose description in our
dictionary contains ?raf1? (Figure 5). Then, drag
3http://www-tsujii.is.s.u-tokyo.ac.jp/info-pubmed/
19
one of the GeneBoxes from the ?Gene Searcher?
to the ?Interaction Viewer.? You will see a list
of genes/proteins which co-occur in the same
sentences, along with co-occurrence frequency.
The GeneBox in the leftmost column is the one
you have moved to ?Interaction Viewer.? The
GeneBoxes in the second column correspond to
gene/proteins which co-occur in the same sen-
tences, followed by the boxes in the third column,
InteractionBoxes.
Drag an InteractionBox to ?ContentViewer? to
see the content of the box (Figure 6). An In-
teractionBox is a set of SentenceBoxes. A Sen-
tenceBox corresponds to a sentence in MEDLINE
in which the two gene/proteins co-occur. A Sen-
tenceBox indicates whether the co-occurrence in
the sentence is direct evidence of interaction or
not. If it is judged as direct evidence of interac-
tion, it is indicated as Interaction. Otherwise, it is
indicated as Co-occurrence.
5 Conclusion
We presented an English HPSG parser, Enju, a
search engine for relational concepts from MED-
LINE, MEDIE, and a GUI-based MEDLINE
search tool, Info-PubMed.
MEDIE and Info-PubMed demonstrate how the
results of deep parsing can be used for intelligent
text mining and semantic information retrieval in
the biomedical domain.
6 Acknowledgment
This work was partially supported by Grant-in-Aid
for Scientific Research on Priority Areas ?Sys-
tems Genomics? (MEXT, Japan) and Solution-
Oriented Research for Science and Technology
(JST, Japan).
References
C. Blaschke and A. Valencia. 2002. The frame-based
module of the SUISEKI information extraction sys-
tem. IEEE Intelligent Systems, 17(2):14?20.
Y. Hao, X. Zhu, M. Huang, and M. Li. 2005. Dis-
covering patterns to extract protein-protein interac-
tions from the literature: Part II. Bioinformatics,
21(15):3294?3300.
H.-W. Chun, Y. Tsuruoka, J.-D. Kim, R. Shiba, N. Na-
gata, T. Hishiki, and J. Tsujii. 2006. Extraction
of gene-disease relations from MedLine using do-
main dictionaries and machine learning. In Proc.
PSB 2006, pages 4?15.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilis-
tic disambiguation models for wide-coverage HPSG
parsing. In Proc. of ACL?05, pages 83?90.
Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2005. Adapting a probabilistic disambiguation
model of an HPSG parser to a new domain. In Proc.
of IJCNLP 2005.
Takashi Ninomiya, Yoshimasa Tsuruoka, Yusuke
Miyao, and Jun?ichi Tsujii. 2005. Efficacy of beam
thresholding, unification filtering and hybrid parsing
in probabilistic HPSG parsing. In Proc. of IWPT
2005, pages 103?114.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and
Jun?ichi Tsujii. 2005. Syntax Annotation for the
GENIA corpus. In Proc. of the IJCNLP 2005, Com-
panion volume, pp. 222?227.
Yusuke Miyao, Tomoko Ohta, Katsuya Masuda, Yoshi-
masa Tsuruoka, Kazuhiro Yoshida, Takashi Ni-
nomiya and Jun?ichi Tsujii. 2006. Semantic Re-
trieval for the Accurate Identification of Relational
Concepts in Massive Textbases. In Proc. of ACL ?06,
to appear.
Yoshimasa Tsuruoka, Yuka Tateisi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun?ichi Tsujii. 2005. Part-of-speech tagger for
biomedical text. In Proc. of the 10th Panhellenic
Conference on Informatics.
Y. Tsuruoka and J. Tsujii. 2004. Improving the per-
formance of dictionary-based approaches in protein
name recognition. Journal of Biomedical Informat-
ics, 37(6):461?470.
Asako Koike and Toshihisa Takagi. 2004.
Gene/protein/family name recognition in biomed-
ical literature. In Proc. of HLT-NAACL 2004
Workshop: Biolink 2004, pages 9?16.
D.A. Lindberg, B.L. Humphreys, and A.T. McCray.
1993. The unified medical language system. Meth-
ods in Inf. Med., 32(4):281?291.
Akane Yakushiji. 2006. Relation Information Extrac-
tion Using Deep Syntactic Analysis. Ph.D. Thesis,
University of Tokyo.
20
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 284?292,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatic Construction of Predicate-argument Structure Patterns
for Biomedical Information Extraction
Akane Yakushiji? ? Yusuke Miyao? Tomoko Ohta?
?Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
? School of Informatics, University of Manchester
POBox 88, Sackville St, MANCHESTER M60 1QD, UK
{akane, yusuke, okap, yucca, tsujii}@is.s.u-tokyo.ac.jp
Yuka Tateisi? ? Jun?ichi Tsujii? ?
Abstract
This paper presents a method of automat-
ically constructing information extraction
patterns on predicate-argument structures
(PASs) obtained by full parsing from a
smaller training corpus. Because PASs
represent generalized structures for syn-
tactical variants, patterns on PASs are ex-
pected to be more generalized than those
on surface words. In addition, patterns
are divided into components to improve
recall and we introduce a Support Vector
Machine to learn a prediction model using
pattern matching results. In this paper, we
present experimental results and analyze
them on how well protein-protein interac-
tions were extracted from MEDLINE ab-
stracts. The results demonstrated that our
method improved accuracy compared to a
machine learning approach using surface
word/part-of-speech patterns.
1 Introduction
One primitive approach to Information Extrac-
tion (IE) is to manually craft numerous extrac-
tion patterns for particular applications and this
is presently one of the main streams of biomedi-
cal IE (Blaschke and Valencia, 2002; Koike et al,
2003). Although such IE attempts have demon-
strated near-practical performance, the same sets
of patterns cannot be applied to different kinds of
information. A real-world task requires several
kinds of IE, thus manually engineering extraction
Current Affiliation:
? FUJITSU LABORATORIES LTD.
? Faculty of Informatics, Kogakuin University
patterns, which is tedious and time-consuming
process, is not really practical.
Techniques based on machine learning (Zhou et
al., 2005; Hao et al, 2005; Bunescu and Mooney,
2006) are expected to alleviate this problem in
manually crafted IE. However, in most cases, the
cost of manually crafting patterns is simply trans-
ferred to that for constructing a large amount of
training data, which requires tedious amount of
manual labor to annotate text.
To systematically reduce the necessary amount
of training data, we divided the task of construct-
ing extraction patterns into a subtask that general
natural language processing techniques can solve
and a subtask that has specific properties accord-
ing to the information to be extracted. The former
subtask is of full parsing (i.e. recognizing syntactic
structures of sentences), and the latter subtask is of
constructing specific extraction patterns (i.e. find-
ing clue words to extract information) based on the
obtained syntactic structures.
We adopted full parsing from various levels
of parsing, because we believe that it offers the
best utility to generalize sentences into normal-
ized syntactic relations. We also divided patterns
into components to improve recall and we intro-
duced machine learning with a Support Vector
Machine (SVM) to learn a prediction model us-
ing the matching results of extraction patterns. As
an actual IE task, we extracted pairs of interacting
protein names from biomedical text.
2 Full Parsing
2.1 Necessity for Full Parsing
A technique that many previous approaches have
used is shallow parsing (Koike et al, 2003; Yao
et al, 2004; Zhou et al, 2005). Their assertion is
284
Distance Count (%) Sum (%)
?1 54 5.0 5.0
0 8 0.7 5.7
1 170 15.7 21.4
2?5 337 31.1 52.5
6?10 267 24.6 77.1
11? 248 22.9 100.0
Distance ?1 means protein word has been annotated as in-
teracting with itself (e.g. ?actin polymerization?). Distance 0
means words of the interacting proteins are directly next to
one another. Multi-word protein names are concatenated as
long as they do not cross tags to annotate proteins.
Table 1: Distance between Interacting Proteins
that shallow parsers are more robust and would be
sufficient for IE. However, their claims that shal-
low parsers are sufficient, or that full parsers do
not contribute to application tasks, have not been
fully proved by experimental results.
Zhou et al (2005) argued that most informa-
tion useful for IE derived from full parsing was
shallow. However, they only used dependency
trees and paths on full parse trees in their experi-
ment. Such structures did not include information
of semantic subjects/objects, which full parsing
can recognize. Additionally, most relations they
extracted from the ACE corpus (Linguistic Data
Consortium, 2005) on broadcasts and newswires
were within very short word-distance (70% where
two entities are embedded in each other or sep-
arated by at most one word), and therefore shal-
low information was beneficial. However, Table 1
shows that the word distance is long between in-
teracting protein names annotated on the AImed
corpus (Bunescu and Mooney, 2004), and we have
to treat long-distance relations for information like
protein-protein interactions.
Full parsing is more effective for acquiring gen-
eralized data from long-length words than shallow
parsing. The sentences at left in Figure 1 exem-
plify the advantages of full parsing. The gerund
?activating? in the last sentence takes a non-local
semantic subject ?ENTITY1?, and shallow parsing
cannot recognize this relation because ?ENTITY1?
and ?activating? are in different phrases. Full pars-
ing, on the other hand, can identify both the sub-
ject of the whole sentence and the semantic subject
of ?activating? have been shared.
2.2 Predicate-argument Structures
We applied Enju (Tsujii Laboratory, 2005a) as
a full parser which outputs predicate-argument
structures (PASs). PASs are well normalized
forms that represent syntactic relations. Enju
is based on Head-driven Phrase Structure Gram-
mar (Sag and Wasow, 1999), and it has been
trained on the Penn Treebank (PTB) (Marcus et
al., 1994) and a biomedical corpus, the GENIA
Treebank (GTB) (Tsujii Laboratory, 2005b). We
used a part-of-speech (POS) tagger trained on the
GENIA corpus (Tsujii Laboratory, 2005b) as a
preprocessor for Enju. On predicate-argument re-
lations, Enju achieved 88.0% precision and 87.2%
recall on PTB, and 87.1% precision and 85.4% re-
call on GTB.
The illustration at right in Figure 1 is a PAS
example, which represents the relation between
?activate?, ?ENTITY1? and ?ENTITY2? of all sen-
tences to the left. The predicate and its argu-
ments are words converted to their base forms,
augmented by their POSs. The arrows denote
the connections from predicates to their arguments
and the types of arguments are indicated as arrow
labels, i.e., ARGn (n = 1, 2, . . .), MOD. For ex-
ample, the semantic subject of a transitive verb is
ARG1 and the semantic object is ARG2.
What is important here is, thanks to the strong
normalization of syntactic variations, that we can
expect that the construction algorithm for extract-
ing patterns that works on PASs will need a much
smaller training corpus than those working on
surface-word sequences. Furthermore, because of
the reduced diversity of surface-word sequences at
the PAS level, any IE system at this level should
demonstrate improved recall.
3 Related Work
Sudo et al (2003), Culotta and Sorensen (2004)
and Bunescu and Mooney (2005) acquired sub-
structures derived from dependency trees as ex-
traction patterns for IE in general domains. Their
approaches were similar to our approach using
PASs derived from full parsing. However, one
problem with their systems is that they could
not treat non-local dependencies such as seman-
tic subjects of gerund constructions (discussed in
Section 2), and thus rules acquired from the con-
structions were partial.
Bunescu and Mooney (2006) also learned ex-
traction patterns for protein-protein interactions
by SVM with a generalized subsequence kernel.
Their patterns are sequences of words, POSs, en-
tity types, etc., and they heuristically restricted
length and word positions of the patterns. Al-
285
ENTITY1 recognizes and activates ENTITY2.
ENTITY2 activated by ENTITY1 are not well characterized.
The herpesvirus encodes a functional ENTITY1 that activates human ENTITY2.
ENTITY1 can functionally cooperate to synergistically activate ENTITY2.
The ENTITY1 plays key roles by activating ENTITY2.
ENTITY1/NN
activate/VB ENTITY2/NN
ARG1 ARG2
Figure 1: Syntactical Variations of ?activate?
though they achieved about 60% precision and
about 40% recall, these heuristic restrictions could
not be guaranteed to be applied to other IE tasks.
Hao et al (2005) learned extraction patterns
for protein-protein interactions as sequences of
words, POSs, entity tags and gaps by dynamic
programming, and reduced/merged them using a
minimum description length-based algorithm. Al-
though they achieved 79.8% precision and 59.5%
recall, sentences in their test corpus have too
many positive instances and some of the pat-
terns they claimed to have been successfully con-
structed went against linguistic or biomedical in-
tuition. (e.g. ?ENTITY1 and interacts with EN-
TITY2? should be replaced by a more general pat-
tern because they aimed to reduce the number of
patterns.)
4 Method
We automatically construct patterns to extract
protein-protein interactions from an annotated
training corpus. The corpus needs to be tagged to
denote which protein words are interacting pairs.
We follow five steps in constructing extraction
patterns from the training corpus. (1) Sentences
in the training corpus are parsed into PASs and
we extract raw patterns from the PASs. (2) We
divide the raw patterns to generate both combi-
nation and fragmental patterns. Because obtained
patterns include inappropriate ones (wrongly gen-
erated or too general), (3) we apply both kinds of
patterns to PASs of sentences in the training cor-
pus, (4) calculate the scores for matching results
of combination patterns, and (5) make a prediction
model with SVM using these matching results and
scores.
We extract pairs of interacting proteins from a
target text in the actual IE phase, in three steps.
(1) Sentences in the target corpus are parsed into
PASs. (2) We apply both kinds of extraction pat-
terns to these PASs and (3) calculate scores for
combination pattern matching. (4) We use the pre-
diction model to predict interacting pairs.
ENTITY1
ENTITY2
CD4/NN protein/NN
interact/VB
with/IN polymorphic/JJ
region/NN
of/INMHCII/NN
MOD ARG1 ARG1 ARG2 ARG1 ARG2
ARG1
Parsing Result
Raw Pattern
CD4 protein interacts with polymorphic regions of MHCII .
ENTITY1
ENTITY2
Sentence in Training Corpus
protein/NN
interact/VB
with/IN
region/NN
of/IN
MOD ARG1 ARG1 ARG2 ARG1 ARG2
(1) (2) (3) (4) (5) (6)
p
0
p
1
p
2
p
3
p
4
p
5
p
6
ENTITY2/NN
ENTITY1/NN
Figure 2: Extraction of Raw Pattern
4.1 Full Parsing and Extraction of Raw
Patterns
As the first step in both the construction phase and
application phase of extraction patterns, we parse
sentences into PASs using Enju.1 We label all
PASs of the protein names as protein PASs.
After parsing, we extract the smallest set of
PASs, which connect words that denote interact-
ing proteins, and make it a raw pattern. We take
the same method to extract and refine raw patterns
as Yakushiji et al (2005). Connecting means we
can trace predicate-argument relations from one
protein word to the other in an interacting pair.
The procedure to obtain a raw pattern (p0, . . . , pn)
is as follows:
predicate(p): PASs that have p as their argument
argument(p): PASs that p has as its arguments
1. pi = p0 is the PAS of a word correspondent
to one of interacting proteins, and we obtain
candidates of the raw pattern as follows:
1-1. If pi is of the word of the other interact-
ing protein, (p0, . . . , pi) is a candidate
of the raw pattern.
1-2. If not, make pattern candidates
for each pi+1 ? predicate(pi) ?
argument(pi) ? {p0, . . . , pi} by
returning to 1-1.
2. Select the pattern candidate of the smallest
set as the raw pattern.
1Before parsing, we concatenate each multi-word protein
name into the one word as long as the concatenation does not
cross name boundaries.
286
3. Substitute variables (ENTITY1, ENTITY2) for
the predicates of PASs correspondent to the
interacting proteins.
The lower part of Figure 2 shows an example
of the extraction of a raw pattern. ?CD4? and
?MHCII? are words representing interacting pro-
teins. First, we set the PAS of ?CD4? as p0.
argument(p0) includes the PAS of ?protein?, and
we set it as p1 (in other words, tracing the arrow
(1)). Next, predicate(p1) includes the PAS of ?in-
teract? (tracing the arrow (2) back), so we set it
as p2. We continue similarly until we reach the
PAS of ?MHCII? (p6). The result of the extracted
raw pattern is the set of p0, . . . , p6 with substitut-
ing variables ENTITY1 and ENTITY2 for ?CD4?
and ?MHCII?.
There are some cases where an extracted raw
pattern is not appropriate and we need to re-
fine it. One case is when unnecessary coordi-
nations/parentheses are included in the pattern,
e.g. two interactions are described in a combined
representation (?ENTITY1 binds this protein and
ENTITY2?). Another is when two interacting pro-
teins are connected directly by a conjunction or
only one protein participates in an interaction. In
such cases, we refine patterns by unfolding of co-
ordinations/parentheses and extension of patterns,
respectively. We have omitted detailed explana-
tions because of space limitations. The details are
described in the work of Yakushiji et al (2005).
4.2 Division of Patterns
Division for generating combination patterns is
based on observation of Yakushiji et al (2005) that
there are many cases where combinations of verbs
and certain nouns form IE patterns. In the work
of Yakushiji et al (2005), we divided only patterns
that include only one verb. We have extended the
division process to also treat nominal patterns or
patterns that include more than one verb.
Combination patterns are not appropriate for
utilizing individual word information because they
are always used in rather strictly combined ways.
Therefore we have newly introduced fragmental
patterns which consist of independent PASs from
raw patterns, in order to use individual word infor-
mation for higher recall.
4.2.1 Division for Generating Combination
Patterns
Raw patterns are divided into some compo-
nents and the components are combined to con-
ENTITY1/NN protein/NN interact/VBwith/IN region/NN
of/IN
ENTITY2/NN
MOD ARG1 ARG1 ARG2 ARG1 ARG2
*/VBwith/IN
ARG1ARG2
*/NN
ENTITY/NN
protein/NN
MOD
region/NN of/INENTITY/NN
ARG1
ARG2
interact/VB
ARG1
=
*/NN
*/VB
ARG1
*/NN
=
$X
$X
Main
Prep
Entity
Entity
Entity
MainEntity Main
Main
Entity
Raw Pattern
Combination Pattern
Figure 3: Division of Raw Pattern into Combina-
tion Pattern Components (Entity-Main-Entity)
struct combination patterns according to types of
the division. There are three types of division of
raw patterns for generating combination patterns.
These are:
(a) Two-entity Division
(a-1) Entity-Main-Entity Division
(a-2) Main-Entity-Entity Division
(b) Single-entity Division, and
(c) No Division (Naive Patterns).
Most raw patterns, where entities are at both
ends of the patterns, are divided into Entity-Main-
Entity. Main-Entity-Entity are for the cases where
there are PASs other than entities at the ends of
the patterns (e.g. ?interaction between ENTITY1
and ENTITY2?). Single-entity is a special Main-
Entity-Entity for interactions with only one partic-
ipant (e.g. ?ENTITY1 dimerization?).
There is an example of Entity-Main-Entity divi-
sion in Figure 3. First, the main component from
the raw pattern is the syntactic head PAS of the
raw pattern. If the raw pattern corresponds to a
sentence, the syntactic head PAS is the PAS of the
main verb. We underspecify the arguments of the
main component, to enable them to unify with the
PASs of any words with the same POSs. Next, if
there are PASs of prepositions connecting to the
main component, they become prep components.
If there is no PAS of a preposition next to the main
component on the connecting link from the main
component to an entity, we make the pseudo PAS
of a null preposition the prep component. The left
prep component ($X) in Figure 3 is a pseudo PAS
of a null preposition. We also underspecify the ar-
guments of prep components. Finally, the remain-
ing two parts, which are typically noun phrases, of
the raw pattern become entity components. PASs
287
corresponding to the entities of the original pair
are labeled as only unifiable with the entities of
other pairs.
Main-Entity-Entity division is similar, except
we distinguish only one prep component as a
double-prep component and the PAS of the coor-
dinate conjunction between entities becomes the
coord component. Single-entity division is simi-
lar to Main-Entity-Entity division and the differ-
ence is that single-entity division produces no co-
ord and one entity component. Naive patterns are
patterns without division, where no division can be
applied (e.g. ?ENTITY1/NN in/IN complexes/NN
with/IN ENTITY2/NN?).
All PASs on boundaries of components are la-
beled to determine which PAS on a boundary of
another component can be unified. Labels are rep-
resented by subscriptions in Figure 3. These re-
strictions on component connection are used in the
step of constructing combination patterns.
Constructing combination patterns by combin-
ing components is equal to reconstructing orig-
inal raw patterns with the original combination
of components, or constructing new raw patterns
with new combinations of components. For exam-
ple, an Entity-Main-Entity pattern is constructed
by combination of any main, any two prep and any
two entity components. Actually, this construction
process by combination is executed in the pattern
matching step. That is, we do not off-line con-
struct all possible combination patterns from the
components and only construct the combination
patterns that are able to match the target.
4.2.2 Division for Generating Fragmental
Patterns
A raw pattern is splitted into individual PASs
and each PAS becomes a fragmental pattern. We
also prepare underspecified patterns where one or
more of the arguments of the original are under-
specified, i.e., are able to match any words of
the same POSs and the same label of protein/not-
protein. We underspecify the PASs of entities in
fragmental patterns to enable them to unify with
any PASs with the same POSs and a protein la-
bel, although in combination patterns we retain the
PASs of entities as only unifiable with entities of
pairs. This is because fragmental patterns are de-
signed to be less strict than combination patterns.
4.3 Pattern Matching
Matching of combination patterns is executed as
a process to match and combine combination pat-
tern components according to their division types
(Entity-Main-Entity, Main-Entity-Entity, Single-
entity and No Division). Fragmental matching is
matching all fragmental patterns to PASs derived
from sentences.
4.4 Scoring for Combination Matching
We next calculate the score of each combination
matching to estimate the adequacy of the combina-
tion of components. This is because new combina-
tion of components may form inadequate patterns.
(e.g. ?ENTITY1 be ENTITY2? can be formed of
components from ?ENTITY1 be ENTITY2 recep-
tor?.) Scores are derived from the results of com-
bination matching to the source training corpus.
We apply the combination patterns to the train-
ing corpus, and count pairs of True Positives (TP)
and False Positives (FP). The scores are calculated
basically by the following formula:
Score = TP/(TP + FP ) + ? ? TP
This formula is based on the precision of the pat-
tern on the training corpus, i.e., an estimated pre-
cision on a test corpus. ? works for smoothing,
that is, to accept only patterns of large TP when
FP = 0. ? is set as 0.01 empirically. The formula
is similar to the Apriori algorithm (Agrawal and
Srikant, 1995) that learns association rules from a
database. The first term corresponds to the confi-
dence of the algorithm, and the second term corre-
sponds to the support.
For patterns where TP = FP = 0, which
are not matched to PASs in the training corpus
(i.e., newly produced by combinations of com-
ponents), we estimates TP ? and FP ? by using
the confidence of the main and entity compo-
nents. This is because main and entity components
tend to contain pattern meanings, whereas prep,
double-prep and coord components are rather
functional. The formulas to calculate the scores
for all cases are:
Score =
8
>
>
>
<
>
>
>
:
TP/(TP + FP ) + ? ? TP
(TP + FP ?= 0)
TP ?/(TP ? + FP ?)
(TP = FP = 0, TP ? + FP ? ?= 0)
0 (TP = FP = TP ? = FP ? = 0)
288
Combination Pattern
(1) Combination of components in combination
matching
(2) Main component in combination matching
(3) Entity components in combination matching
(4) Score for combination matching (SCORE)
Fragmental Pattern
(5) Matched fragmental patterns
(6) Number of PASs of example that are not matched
in fragmental matching
Raw Pattern
(7) Length of raw pattern derived from example
Table 2: Features for SVM Learning of Prediction
Model
TP ? =
8
>
<
>
:
TP ?main + TP ?entity1(+TP ?entity2)
(for Two-entity, Single-entity)
0 (for Naive)
FP ? = (similar to TP ? but TP ?x is replaced by FP ?x)
TP ?main =
8
>
>
>
>
>
<
>
>
>
>
>
:
TPmain:two/(TPmain:two + FPmain:two)
 
TPmain:two + FPmain:two ?= 0,
for Two-entity
!
TPmain:single/(TPmain:single + FPmain:single)
 
TPmain:single + FPmain:single ?= 0,
for Single-entity
!
0 (other cases)
TP ?entityi =
8
>
<
>
:
TPentityi/(TPentityi + FPentityi)
?
TPentityi + FPentityi ?= 0
?
0 (other cases)
FP ?x =
?
similar to TP ?x but TP ?y in the
numerators is replaced by FP ?y
?
? TP : number of TPs by the combination of components
? TPmain:two: sum of TPs by two-entity combinations
that include the same main component
? TPmain:single: sum of TPs by single-entity combina-
tions that include the same main component
? TPentityi: sum of TPs by combinations that include
the same entity component which is not the straight en-
tity component
? FPx: similar to TPx but TP is replaced by FP
The entity component ?ENTITY/NN?, which
only consists of the PAS of an entity, adds no infor-
mation to combinations of components. We call
this component a straight entity component and
exclude its effect from the scores.
4.5 Construction of Prediction Model
We use an SVM to learn a prediction model to de-
termine whether a new protein pair is interacting.
We used SV M light (Joachims, 1999) with an rbf
kernel, which is known as the best kernel for most
tasks. The prediction model is based on the fea-
tures of Table 2.
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Prec
ision
Recall
ALLSCOREERK
Figure 4: Results of IE Experiment
ENTITY1
FGF-2/NN
bind well to FGFR1 but
interact/VB with/IN
poorly/RB
ENTITY2
KGFR/NN
ARG1 ARG1 ARG2
Figure 5: Example Demonstrating Advantages of
Full Parsing
5 Results and Discussion
5.1 Experimental Results on the AImed
Corpus
To evaluate extraction patterns automatically con-
structed with our method, we used the AImed cor-
pus, which consists of 225 MEDLINE (U.S. Na-
tional Library of Medicine, 2006) abstracts (1969
sentences) annotated with protein names and
protein-protein interactions, for the training/test
corpora. We used tags for the protein names given.
We measured the accuracy of the IE task using
the same criterion as Bunescu andMooney (2006),
who used an SVM to construct extraction patterns
on word/POS/type sequences from the AImed cor-
pus. That is, an extracted interaction from an ab-
stract is correct if the proteins are tagged as inter-
acting with each other somewhere in that abstract
(document-level measure).
Figure 4 plots our 10-fold cross validation and
the results of Bunescu and Mooney (2006). The
line ALL represents results when we used all fea-
tures for SVM learning. The line SCORE repre-
sents results when we extracted pairs with higher
combination matching scores than various thresh-
old values. And the line ERK represents results
by Bunescu and Mooney (2006).
The line ALL obtained our best overall F-
measure 57.3%, with 71.8% precision and 48.4%
recall. Our method was significantly better than
Bunescu and Mooney (2006) for precision be-
289
tween 50% and 80%. It also needs to be noted
that SCORE, which did not use SVM learning
and only used the combination patterns, achieved
performance comparable to that by Bunescu and
Mooney (2006) for the precision range from 50%
to 80%. And for this range, introducing the frag-
mental patterns with SVM learning raised the re-
call. This range of precision is practical for the
IE task, because precision is more important than
recall for significant interactions that tend to be
described in many abstracts (as shown by the
next experiment), and too-low recall accompa-
nying too-high precision requires an excessively
large source text.
Figure 5 shows the advantage of introducing
full parsing. ?FGF-2? and ?KGFR? is an interact-
ing protein pair. The pattern ?ENTITY1 interact
with ENTITY2? based on PASs successfully ex-
tracts this pair. However, it is difficult to extract
this pair with patterns based on surface words, be-
cause there are 5 words between ?FGF-2? and ?in-
teract?.
5.2 Experimental Results on Abstracts of
MEDLINE
We also conducted an experiment to extract in-
teracting protein pairs from a large amount of
biomedical text, i.e. about 14 million titles and
8 million abstracts in MEDLINE. We constructed
combination patterns from all 225 abstracts of the
AImed corpus, and calculated a threshold value
of combination scores that produced about 70%
precision and 30% recall on the training corpus.
We extracted protein pairs with higher combi-
nation scores than the threshold value. We ex-
cluded single-protein interactions to reduce time
consumption and we used a protein name recog-
nizer in this experiment2.
We compared the extracted pairs with a man-
ually curated database, Reactome (Joshi-Tope et
al., 2005), which published 16,564 human pro-
tein interaction pairs as pairs of Entrez Gene
IDs (U.S. National Library of Medicine, 2006).
We converted our extracted protein pairs into pairs
of Entrez Gene IDs by the protein name recog-
nizer.3 Because there may be pairs missed by Re-
2Because protein names were recognized after the pars-
ing, multi-word protein names were not concatenated.
3Although the same protein names are used for humans
and other species, these are considered to be human proteins
without checking the context. This is a fair assumption be-
cause Reactome itself infers human interaction events from
experiments on model organisms such as mice.
Total 89
Parsing Error/Failure 35
(Related to coordinations) (14)
Lack of Combination Pattern Component 33
Requiring Anaphora Resolution 9
Error in Prediction Model 8
Requiring Attributive Adjectives 5
Others 10
More than one cause can occur in one error, thus the sum of
all causes is larger than the total number of False Negatives.
Table 3: Causes of Error for FNs
actome or pairs that our processed text did not in-
clude, we excluded extracted pairs of IDs that are
not included in Reactome and excluded Reactome
pairs of IDs that do not co-occur in the sentences
of our processed text.
After this postprocessing, we found that we had
extracted 7775 human protein pairs. Of them, 155
pairs were also included in Reactome ([a] pseudo
TPs) and 7620 pairs were not included in Reac-
tome ([b] pseudo FPs). 947 pairs of Reactome
were not extracted by our system ([c] pseudo False
Negatives (FNs)). However, these results included
pairs that Reactome missed or those that only co-
occurred and were not interacting pairs in the text.
There may also have been errors with ID assign-
ment.
To determine such cases, a biologist investi-
gated 100 pairs randomly selected from pairs of
pseudo TPs, FPs and FNs retaining their ratio of
numbers. She also checked correctness of the as-
signed IDs. 2 pairs were selected from pseudo
TPs, 88 pairs were from pseudo FPs and 10 pairs
were from pseudo FNs. The biologist found that
57 pairs were actual TPs (2 pairs of pseudo TPs
and 55 pairs of pseudo FPs) and 32 pairs were ac-
tual FPs of the pseudo FPs. Thus, the precision
was 64.0% in this sample set. Furthermore, even
if we assume that all pseudo FNs are actual FNs,
the recall can be estimated by actual TPs / (actual
TPs + pseudo FNs) ? 100 = 83.8%.
These results mean that the recall of an IE sys-
tem for interacting proteins is improved for a large
amount of text even if it is low for a small corpus.
Thus, this justifies our assertion that a high degree
of precision in the low-recall range is important.
5.3 Error Analysis
Tables 3 and 4 list causes of error for FNs/FPs on
a test set of the AImed corpus using the predic-
tion model with the best F-measure with all the
290
Total 35
Requiring Attributive Adjectives 13
Corpus Error 11
Error in Prediction Model 5
Requiring Negation Words 2
Parsing Error 1
Others 3
Table 4: Causes of Error for FPs
features. Different to Subsection 5.1, we individ-
ually checked each occurring pair of interacting
proteins. The biggest problems were parsing er-
ror/failure, lack of necessary patterns and learning
of inappropriate patterns.
5.3.1 Parsing Error
As listed in Table 3, 14 (40%) of the 35 pars-
ing errors/failures were related to coordinations.
Many of these were caused by differences in the
characteristics of the PTB/GTB, the training cor-
pora for Enju, and the AImed Corpus. For ex-
ample, Enju failed to obtain the correct structure
for ?the ENTITY1 / ENTITY1 complex? because
words in the PTB/GTB are not segmented with
?/? and Enju could not be trained on such a case.
One method to solve this problem is to avoid seg-
menting words with ?/? and introducing extraction
patterns based on surface characters, such as ?EN-
TITY1/ENTITY2 complex?.
Parsing errors are intrinsic problems to IE meth-
ods using parsing. However, from Table 3, we can
conclude that the key to gaining better accuracy
is refining of the method with which the PAS pat-
terns are constructed (there were 46 related FNs)
rather than improving parsing (there were 35 FNs).
5.3.2 Lack of Necessary Patterns and
Learning of Inappropriate Patterns
There are two different reasons causing the
problems with the lack of necessary patterns and
the learning of inappropriate patterns: (1) the
training corpus was not sufficiently large to sat-
urate IE accuracy and (2) our method of pattern
construction was too limited.
Effect of Training Corpus Size To investigate
whether the training corpus was large enough to
maximize IE accuracy, we conducted experiments
on training corpora of various sizes. Figure 6 plots
graphs of F-measures by SCORE and Figure 7
plots the number of combination patterns on train-
ing corpora of various sizes. From Figures 6 and 7,
the training corpus (207 abstracts at a maximum)
 0.35
 0.4
 0.45
 0.5
 0.55
 0  50  100  150  200
F-m
eas
ure
 by
 SC
OR
E
Training Corpus Size (Number of Abstracts)
Figure 6: Effect of Training Corpus Size (1)
 0
 100
 200
 300
 400
 500
 600
 0  50  100  150  200
Nu
mb
er
Training Corpus Size (Number of Abstracts)
Raw Patterns (before division)Main ComponentEntity ComponentOther ComponentNaive Pattern
Figure 7: Effect of Training Corpus Size (2)
is not large enough. Thus increasing corpus size
will further improve IE accuracy.
Limitation of the Present Pattern Construc-
tion The limitations with our pattern construc-
tion method are revealed by the fact that we
could not achieve a high precision like Bunescu
and Mooney (2006) within the high-recall range.
Compared to theirs, one of our problems is that our
method could not handle attributives. One exam-
ple is ?binding property of ENTITY1 to ENTITY2?.
We could not obtain ?binding? because the small-
est set of PASs connecting ?ENTITY1? and ?EN-
TITY2? includes only the PASs of ?property?, ?of?
and ?to?. To handle these attributives, we need dis-
tinguish necessary attributives from those that are
general4 by semantic analysis or bootstrapping.
Another approach to improve our method is to
include local information in sentences, such as
surface words between protein names. Zhao and
Grishman (2005) reported that adding local infor-
mation to deep syntactic information improved IE
results. This approach is also applicable to IE in
other domains, where related entities are in a short
4Consider the case where a source sentence for a pattern is
?ENTITY1 is an important homodimeric protein.? (?homod-
imeric? represents that two molecules of ?ENTITY1? interact
with each other.)
291
distance like the work of Zhou et al (2005).
6 Conclusion
We proposed the use of PASs to construct pat-
terns as extraction rules, utilizing their ability to
abstract syntactical variants with the same rela-
tion. In addition, we divided the patterns for gen-
eralization, and used matching results for SVM
learning. In experiments on extracting of protein-
protein interactions, we obtained 71.8% precision
and 48.4% recall on a small corpus and 64.0% pre-
cision and 83.8% recall estimated on a large text,
which demonstrated the obvious advantages of our
method.
Acknowledgement
This work was partially supported by Grant-in-Aid
for Scientific Research on Priority Areas ?Sys-
tems Genomics? (MEXT, Japan) and Solution-
Oriented Research for Science and Technology
(JST, Japan).
References
R. Agrawal and R Srikant. 1995. Mining Sequential
Patterns. In Proc. the 11th International Conference
on Data Engineering, pages 3?14.
Christian Blaschke and Alfonso Valencia. 2002. The
Frame-Based Module of the SUISEKI Informa-
tion Extraction System. IEEE Intelligent Systems,
17(2):14?20.
Razvan Bunescu and Raymond J. Mooney. 2004.
Collective information extraction with relational
markov networks. In Proc. ACL?04, pages 439?446.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
Shortest Path Dependency Kernel for Relation Ex-
traction. In Proc. HLT/EMNLP 2005, pages 724?
731.
Razvan Bunescu and Raymond Mooney. 2006. Subse-
quence kernels for relation extraction. In Advances
in Neural Information Processing Systems 18, pages
171?178. MIT Press.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proc. ACL?04,
pages 423?429.
Yu Hao, Xiaoyan Zhu, Minlie Huang, and Ming
Li. 2005. Discovering patterns to extract protein-
protein interactions from the literature: Part II.
Bioinformatics, 21(15):3294?3300.
Thorsten Joachims. 1999. Making Large-Scale SVM
Learning Practical. In Advances in Kernel Methods
? Support Vector Learning. MIT-Press.
G Joshi-Tope, M Gillespie, I Vastrik, P D?Eustachio,
E Schmidt, B de Bono, B Jassal, GR Gopinath,
GR Wu, L Matthews, S Lewis, E Birney, and Stein
L. 2005. Reactome: a knowledgebase of biologi-
cal pathways. Nucleic Acids Research, 33(Database
Issue):D428?D432.
Asako Koike, Yoshiyuki Kobayashi, and Toshihisa
Takagi. 2003. Kinase Pathway Database: An
Integrated Protein-Kinase and NLP-Based Protein-
Interaction Resource. Genome Research, 13:1231?
1243.
Linguistic Data Consortium. 2005. ACE Program.
http://projects.ldc.upenn.edu/ace/.
Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn Treebank: Annotating
predicate argument structure. In Proc. AAI ?94.
Ivan A. Sag and Thomas Wasow. 1999. Syntactic The-
ory. CSLI publications.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representa-
tion model for automatic IE pattern acquisition. In
Proc. ACL 2003, pages 224?231.
Tsujii Laboratory. 2005a. Enju - A practical HPSG
parser. http://www-tsujii.is.s.u-tokyo.ac.jp/enju/.
Tsujii Laboratory. 2005b. GENIA Project.
http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/.
U.S. National Library of Medicine. 2006. PubMed.
http://www.pubmed.gov.
Akane Yakushiji, Yusuke Miyao, Yuka Tateisi, and
Jun?ichi Tsujii. 2005. Biomedical information ex-
traction with predicate-argument structure patterns.
In Proc. SMBM 2005, pages 60?69.
Daming Yao, Jingbo Wang, Yanmei Lu, Nathan No-
ble, Huandong Sun, Xiaoyan Zhu, Nan Lin, Don-
ald G. Payan, Ming Li, and Kunbin Qu. 2004. Path-
wayFinder: Paving The Way Towards Automatic
Pathway Extraction. In Bioinformatics 2004: Proc.
the 2nd APBC, volume 29 of CRPIT, pages 53?62.
Shubin Zhao and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In Proc. ACL?05, pages 419?426.
GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Proc. ACL?05, pages 427?434.
292
