Large-Scale Induction and Evaluation of
Lexical Resources from the Penn-II and
Penn-III Treebanks
Ruth O?Donovan?
Dublin City University
Michael Burke??
Dublin City University
Aoife Cahill?
Dublin City University
Josef van Genabith??
Dublin City University
Andy Way??
Dublin City University
We present a methodology for extracting subcategorization frames based on an automatic
lexical-functional grammar (LFG) f-structure annotation algorithm for the Penn-II and
Penn-III Treebanks. We extract syntactic-function-based subcategorization frames (LFG
semantic forms) and traditional CFG category-based subcategorization frames as well as
mixed function/category-based frames, with or without preposition information for obliques
and particle information for particle verbs. Our approach associates probabilities with frames
conditional on the lemma, distinguishes between active and passive frames, and fully
reflects the effects of long-distance dependencies in the source data structures. In contrast
to many other approaches, ours does not predefine the subcategorization frame types extracted,
learning them instead from the source data. Including particles and prepositions, we extract
21,005 lemma frame types for 4,362 verb lemmas, with a total of 577 frame types and an
average of 4.8 frame types per verb. We present a large-scale evaluation of the complete
set of forms extracted against the full COMLEX resource. To our knowledge, this is
the largest and most complete evaluation of subcategorization frames acquired automatically
for English.
1. Introduction
In modern syntactic theories (e.g., lexical-functional grammar [LFG] [Kaplan and
Bresnan 1982; Bresnan 2001; Dalrymple 2001], head-driven phrase structure gram-
mar [HPSG] [Pollard and Sag 1994], tree-adjoining grammar [TAG] [Joshi 1988], and
combinatory categorial grammar [CCG] [Ades and Steedman 1982]), the lexicon is
the central repository for much morphological, syntactic, and semantic information.
? National Centre for Language Technology, School of Computing, Dublin City University, Glasnevin,
Dublin 9, Ireland. E-mail: {rodonovan,mburke,acahill,josef,away}@computing.dcu.ie.
? Centre for Advanced Studies, IBM, Dublin, Ireland.
Submission received: 19 March 2004; revised submission received: 18 December 2004; accepted for
publication: 2 March 2005.
? 2005 Association for Computational Linguistics
Computational Linguistics Volume 31, Number 3
Extensive lexical resources, therefore, are crucial in the construction of wide-coverage
computational systems based on such theories.
One important type of lexical information is the subcategorization requirements
of an entry (i.e., the arguments a predicate must take in order to form a grammatical
construction). Lexicons, including subcategorization details, were traditionally pro-
duced by hand. However, as the manual construction of lexical resources is time con-
suming, error prone, expensive, and rarely ever complete, it is often the case that the
limitations of NLP systems based on lexicalized approaches are due to bottlenecks in
the lexicon component. In addition, subcategorization requirements may vary across
linguistic domain or genre (Carroll and Rooth 1998). Manning (1993) argues that, aside
from missing domain-specific complementation trends, dictionaries produced by hand
will tend to lag behind real language use because of their static nature. Given these
facts, research on automating acquisition of dictionaries for lexically based NLP sys-
tems is a particularly important issue.
Aside from the extraction of theory-neutral subcategorization lexicons, there has
also been work in the automatic construction of lexical resources which comply
with the principles of particular linguistic theories such as LTAG, CCG, and HPSG
(Chen and Vijay-Shanker 2000; Xia 1999; Hockenmaier, Bierner, and Baldridge 2004;
Nakanishi, Miyao, and Tsujii 2004). In this article we present an approach to auto-
mating the process of lexical acquisition for LFG (i.e., grammatical-function-based sys-
tems). However, our approach also generalizes to CFG category-based approaches. In
LFG, subcategorization requirements are enforced through semantic forms specifying
which grammatical functions are required by a particular predicate. Our approach is
based on earlier work on LFG semantic form extraction (van Genabith, Sadler, and
Way 1999) and recent progress in automatically annotating the Penn-II and Penn-III
Treebanks with LFG f-structures (Cahill et al 2002; Cahill, McCarthy, et al 2004). Our
technique requires a treebank annotated with LFG functional schemata. In the early
approach of van Genabith, Sadler, and Way (1999), this was provided by manually
annotating the rules extracted from the publicly available subset of the AP Treebank to
automatically produce corresponding f-structures. If the f-structures are of high qual-
ity, reliable LFG semantic forms can be generated quite simply by recursively reading
off the subcategorizable grammatical functions for each local PRED value at each level of
embedding in the f-structures. The work reported in van Genabith, Sadler, and Way
(1999) was small scale (100 trees) and proof of concept and required considerable
manual annotation work. It did not associate frames with probabilities, discriminate
between frames for active and passive constructions, properly reflect the effects of
long-distance dependencies (LDDs), or include CFG category information. In this
article we show how the extraction process can be scaled to the complete Wall
Street Journal (WSJ) section of the Penn-II Treebank, with about one million words
in 50,000 sentences, based on the automatic LFG f-structure annotation algorithm
described in Cahill et al (2002) and Cahill, McCarthy, et al (2004). More recently
we have extended the extraction approach to the larger, domain-diverse Penn-III
Treebank. Aside from the parsed WSJ section, this version of the treebank contains
parses for a subsection of the Brown corpus (almost 385,000 words in 24,000 trees)
taken from a variety of text genres.1 In addition to extracting grammatical-function-
1 For the remainder of this work, when we refer to the Penn-II Treebank, we mean the parse-annotated WSJ,
and when we refer to the Penn-III Treebank, we mean the parse-annotated WSJ and Brown corpus
combined.
330
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
based subcategorization frames, we also include the syntactic categories of the predicate
and its subcategorized arguments, as well as additional details such as the prepositions
required by obliques and particles accompanying particle verbs. Our method discrim-
inates between active and passive frames, properly reflects LDDs in the source data
structures, assigns conditional probabilities to the semantic forms associated with each
predicate, and does not predefine the subcategorization frames extracted.
In Section 2 of this article, we briefly outline LFG, presenting typical lexical entries
and the encoding of subcategorization information. Section 3 reviews related work in
the area of automatic subcategorization frame extraction. Our methodology and its
implementation are presented in Section 4. In Section 5 we present results from the
extraction process. We evaluate the complete induced lexicon against the COMLEX
resource (Grishman, MacLeod, and Meyers 1994) and present the results in Section 6.
To our knowledge, this is by far the largest and most complete evaluation of subcat-
egorization frames automatically acquired for English. In Section 7, we examine the
coverage of our lexicon in regard to unseen data and the rate at which new lexical
entries are learned. Finally, in Section 8 we conclude and give suggestions for future
work.
2. Subcategorization in LFG
Lexical functional grammar (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple
2001) is a member of the family of constraint-based grammars. It posits minimally
two levels of syntactic representation:2 c(onstituent)-structure encodes details of sur-
face syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic
information about predicate?argument?modifier relations and certain morphosyntactic
properties such as tense, aspect, and case. C-structure takes the form of phrase structure
trees and is defined in terms of CFG rules and lexical entries. F-structure is pro-
duced from functional annotations on the nodes of the c-structure and implemented
in terms of recursive feature structures (attribute?value matrices). This is exemplified
by the analysis of the string The inquiry soon focused on the judge (wsj 0267 72) using
the grammar in Figure 1, which results in the annotated c-structure and f-structure in
Figure 2.
The value of the PRED attribute in an f-structure is a semantic form ??gf1, gf2, . . . ,
gfn?, where ? is a lemma and gf a grammatical function. The semantic form provides
an argument list ?gf1,gf2, . . . ,gfn? specifying the governable grammatical functions (or
arguments) required by the predicate to form a grammatical construction. In Figure 1
the verb FOCUS requires a subject and an oblique object introduced by the preposition
on: FOCUS?(? SUBJ)(? OBLon)?. The argument list can be empty, as in the PRED value
for judge in Figure 1. According to Dalrymple (2001), LFG assumes the following uni-
versally available inventory of grammatical functions: SUBJ(ect), OBJ(ect), OBJ?, COMP,
XCOMP, OBL(ique)?, ADJ(unct), XADJ. OBJ? and OBL? represent families of grammatical
functions indexed by their semantic role, represented by the theta subscript. This list
of grammatical functions is divided into governable (subcategorizable) grammatical
functions (arguments) and nongovernable (nonsubcategorizable) grammatical func-
tions (modifiers/adjuncts), as summarized in Table 1.
2 LFGs may also involve morphological and semantic levels of representation.
331
Computational Linguistics Volume 31, Number 3
Figure 1
Sample LFG rules and lexical entries.
A number of languages allow the possibility of object functions in addition to the
primary OBJ, such as the second or indirect object in English. Oblique arguments are
realized as prepositional phrases in English. COMP, XCOMP, and XADJ are all clausal
functions which differ in the way in which they are controlled. A COMP is a closed
function which contains its own internal SUBJ:
The judge thinks [COMP that it will resume].
XCOMP and XADJ are open functions not requiring an internal SUBJ. The subject is
instead specified externally in the matrix phrase:
The judge wants [XCOMP to open an inquiry].
While many linguistic theories state subcategorization requirements in terms
of phrase structure (CFG categories), Dalrymple (2001) questions the viability and
universality of such an approach because of the variety of ways in which grammatical
functions may be realized at the language-specific constituent structure level. LFG
argues that subcategorization requirements are best stated at the f-structure level,
in functional rather than phrasal terms. This is because of the assumption that
abstract grammatical functions are primitive concepts as opposed to derivatives
332
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Figure 2
C- and f-structures for Penn Treebank sentence wsj 0267 72, The inquiry soon focused on the judge.
of phrase structural position. In LFG, the subcategorization requirements of a
particular predicate are expressed by its semantic form: FOCUS?(? SUBJ)(? OBLon)? in
Figure 1.
The subcategorization requirements expressed by semantic forms are enforced at
f-structure level through completeness and coherence well-formedness conditions on
f-structure (Kaplan and Bresnan 1982):
An f-structure is locally complete iff it contains all the governable grammatical
functions that its predicate governs. An f-structure is complete iff it and all its
subsidiary f-structures are locally complete. An f-structure is locally coherent iff
all the governable grammatical functions that it contains are governed by a
local predicate. An f-structure is coherent iff it and all its subsidiary f-structures
are locally coherent. (page 211)
Consider again the f-structure in Figure 2. The semantic form associated with
the verb focus is FOCUS?(? SUBJ)(? OBLon)?. The f-structure is locally complete, as it
contains the SUBJ and an OBL with the preposition on specified by the semantic
form. The f-structure also satisfies the coherence condition, as it does not contain
any governable grammatical functions other than the SUBJ and OBL required by the
local PRED.
333
Computational Linguistics Volume 31, Number 3
Table 1
Governable and nongovernable grammatical functions in LFG.
Governable GFs Nongovernable GFs
SUBJ ADJ
OBJ XADJ
XCOMP
COMP
OBJ?
OBL?
Because of the specific form of the LFG lexicon, our extraction approach differs in
interesting ways from that of previous lexical extraction experiments. This contrast is
made evident in Sections 3 and 4.
3. Related Work
The encoding of verb subcategorization properties is an essential step in the
construction of computational lexicons for tasks such as parsing, generation, and
machine translation. Creating such a resource by hand is time consuming and error
prone, requires considerable linguistic expertise, and is rarely if ever complete. In
addition, a hand-crafted lexicon cannot be easily adapted to specific domains or
account for linguistic change. Accordingly, many researchers have attempted to
construct lexicons automatically, especially for English. In this section, we discuss
approaches to CFG-based subcategorization frame extraction as well as attempts to
induce lexical resources which comply with specific linguistic theories or express
information in terms of more abstract predicate-argument relations. The evaluation of
these approaches is discussed in greater detail in Section 6, in which we compare our
results with those reported elsewhere in the literature.
We will divide more-general approaches to subcategorization frame acquisition
into two groups: those which extract information from raw text and those which
use preparsed and hand-corrected treebank data as their input. Typically in the
approaches based on raw text, a number of subcategorization patterns are predefined,
a set of verb subcategorization frame associations are hypothesized from the data,
and statistical methods are applied to reliably select hypotheses for the final lexicon.
Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators
of six predefined subcategorization frames. The frames do not include details of specific
prepositions. Brent used hypothesis testing on binomial frequency data to statistically
filter the induced frames. Ushioda et al (1993) run a finite-state NP parser on a
POS-tagged corpus to calculate the relative frequency of the same six subcategoriza-
tion verb classes. The experiment is limited by the fact that all prepositional phrases
are treated as adjuncts. Ushioda et al (1993) employ an additional statistical method
based on log-linear models and Bayes? theorem to filter the extra noise introduced by
the parser and were the first to induce relative frequencies for the extracted frames.
Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw
text through a stochastic tagger and a finite-state parser (which includes a set of
simple rules for subcategorization frame recognition) in order to extract verbs and
the constituents with which they co-occur. He assumes 19 different subcategorization
334
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
frame definitions, and the extracted frames include details of specific prepositions.
The extracted frames are noisy as a result of parser errors and so are filtered using
the binomial hypothesis theory (BHT), following Brent (1993). Applying his technique
to approximately four million words of New York Times newswire, Manning acquired
4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames
per verb. Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames,
obtained by manually merging the classes exemplified in the COMLEX (MacLeod,
Grishman, and Meyers 1994) and ANLT (Boguraev et al 1987) dictionaries and adding
around 30 frames found by manual inspection. The frames incorporate control informa-
tion and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a
priori information about the probabilities of subcategorization frame membership and
use it to filter the induced frames. Recent work by Korhonen (2002) on the filtering
phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining
more accurate back-off estimates for hypothesis selection. Carroll and Rooth (1998)
use a handwritten head-lexicalized, context-free grammar and a text corpus to
compute the probability of particular subcategorization patterns. The approach is
iterative with the aim of estimating the distribution of subcategorization frames
associated with a particular predicate. They perform a mapping between their frames
and those of the OALD, resulting in 15 frame types. These do not contain details of
specific prepositions.
More recently, a number of researchers have applied similar techniques to auto-
matically derive lexical resources for languages other than English. Schulte im Walde
(2002a, 2002b) uses a head-lexicalized probabilistic context-free grammar similar to
that of Caroll and Rooth (1998) to extract subcategorization frames from a large
German newspaper corpus from the 1990s. She predefines 38 distinct frame types,
which contain maximally three arguments each and are made up of a combination
of the following: nominative, dative, and accusative noun phrases; reflexive pro-
nouns; prepositional phrases; expletive es; subordinated nonfinite clauses; subordinated
finite clauses; and copula constructions. The frames may optionally contain details of
particular prepositional use. Unsupervised training is performed on a large German
newspaper corpus, and the resulting probabilistic grammar establishes the relevance of
different frame types to a specific lexical head. Because of computing time constraints,
Schulte im Walde limits sentence length for grammar training and parsing. Sentences
of length between 5 and 10 words were used to bootstrap the lexicalized grammar
model. For lexicalized training, sentences of length between 5 and 13 words were
used. The result is a subcategorization lexicon for over 14,000 German verbs. The
extensive evaluation carried out by Schulte im Walde will be discussed in greater detail
in Section 6.
Approaches using treebank-based data as a source for subcategorization infor-
mation, such as ours, do not predefine the frames to be extracted but rather learn them
from the data. Kinyon and Prolo (2002) describe a simple tool which uses fine-grained
rules to identify the arguments of verb occurrences in the Penn-II Treebank. This is
made possible by manual examination of more than 150 different sequences of syntactic
and functional tags in the treebank. Each of these sequences was categorized as a
modifier or argument. Arguments were then mapped to traditional syntactic functions.
For example, the tag sequence NP-SBJ denotes a mandatory argument, and its syntactic
function is subject. In general, argumenthood was preferred over adjuncthoood. As
Kinyon and Prolo (2002) does not include an evaluation, currently it is impossible to
say how effective their technique is. Sarkar and Zeman (2000) present an approach to
learn previously unknown frames for Czech from the Prague Dependency Bank (Hajic
335
Computational Linguistics Volume 31, Number 3
1998). Czech is a language with a freer word order than English and so configurational
information cannot be relied upon. In a dependency tree, the set of all dependents
of the verb make up a so-called observed frame, whereas a subcategorization frame
contains a subset of the dependents in the observed frame. Finding subcategorization
frames involves filtering adjuncts from the observed frame. This is achieved using three
different hypothesis tests: BHT, log-likelihood ratio, and t-score. The system learns 137
subcategorization frames from 19,126 sentences for 914 verbs (those which occurred
five times or more). Marinov and Hemming (2004) present preliminary work on the
automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank
(Simov, Popova, and Osenova 2002). In a similar way to that of Sarkar and Zeman
(2000), Marinov and Hemming?s system collects both arguments and adjuncts. It then
uses the binomial log-likelihood ratio to filter incorrect frames. The BulTreebank trees
are annotated with HPSG-typed feature structure information and thus contain more
detail than the dependency trees. The work done for Bulgarian is small-scale, however,
as Marinov and Hemming are working with a preliminary version of the treebank with
580 sentences.
Work has been carried out on the extraction of formalism-specific lexical resources
from the Penn-II Treebank, in particular TAG, CCG, and HPSG. As these formalisms are
fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component,
the extraction of a lexicon essentially amounts to the creation of a grammar. Chen and
Vijay-Shanker (2000) explore a number of related approaches to the extraction of a
lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical
model for parsing. The extraction procedure utilizes a head percolation table as intro-
duced by Magerman (1995) in combination with a variation of Collins?s (1997) approach
to the differentiation between complement and adjunct. This results in the construction
of a set of lexically anchored elementary trees which make up the TAG in question.
The number of frame types extracted (i.e., an elementary tree without a specific lexical
anchor) ranged from 2,366 to 8,996. Xia (1999) also presents a similar method for
the extraction of a TAG from the Penn Treebank. The extraction procedure consists
of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and
extended based on the approaches of Magerman (1994) and Collins (1997). Then the
elementary trees are read off in a quite straightforward manner. Finally any invalid
elementary trees produced as a result of annotation errors in the treebank are filtered out
using linguistic heuristics. The number of frame types extracted by Xia (1999) ranged
from 3,014 to 6,099.
Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic
extraction of a large syntactic CCG lexicon from the Penn-II Treebank. For each tree, the
algorithm annotates the nodes with CCG categories in a top-down recursive manner.
The first step is to label each node as either a head, complement, or adjunct based
on the approaches of Magerman (1994) and Collins (1997). Each node is subsequently
assigned the relevant category based on its constituent type and surface configuration.
The algorithm handles ?like? coordination and exploits the traces used in the treebank
in order to interpret LDDs. Unlike our approach, those of Xia (1999) and Hockenmaier,
Bierner, and Baldridge (2004) include a substantial initial correction and clean-up of the
Penn-II trees.
Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii (2004)
describe a methodology for acquiring an English HPSG from the Penn-II Treebank.
Manually defined heuristics are used to automatically annotate each tree in the treebank
with partially specified HPSG derivation trees: Head/argument/modifier distinctions
are made for each node in the tree based on Magerman (1994) and Collins (1997);
336
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
the whole tree is then converted to a binary tree; heuristics are applied to deal with
phenomena such as LDDs and coordination and to correct some errors in the tree-
bank, and finally an HPSG category is assigned to each node in the tree in accordance
with its CFG category. In the next phase of the process (externalization), HPSG lexical
entries are automatically extracted from the annotated trees through the application of
?inverse schemata.?
4. Methodology
The first step in the application of our methodology is the production of a tree-
bank annotated with LFG f-structure information. F-structures are attribute?value
structures which represent abstract syntactic information, approximating to ba-
sic predicate?argument?modifier structures. Most of the early work on automatic
f-structure annotation (e.g., van Genabith, Way, and Sadler 1999; Frank 2000; Sadler,
van Genabith, and Way 2000) was applied only to small data sets (fewer than 200
sentences) and was largely proof of concept. However, more recent work (Cahill et al
2002; Cahill, McCarthy, et al 2004) has presented efforts in evolving and scaling up
annotation techniques to the Penn-II Treebank (Marcus et al 1994), containing more
than 1,000,000 words and 49,000 sentences.
We utilize the automatic annotation algorithm of Cahill et al (2002) and Cahill,
McCarthy, et al (2004) to derive a version of Penn-II in which each node in each
tree is annotated with LFG functional annotations in the form of attribute-value struc-
ture equations. The algorithm uses categorial, configurational, local head, and Penn-II
functional and trace information. The annotation procedure is dependent on locating
the head daughter, for which an amended version of Magerman (1994) is used. The
head is annotated with the LFG equation ?=?. Linguistic generalizations are provided
over the left (the prefix) and the right (suffix) context of the head for each syntactic
category occurring as the mother nodes of such heads. To give a simple example, the
rightmost NP to the left of a VP head under an S is likely to be the subject of the sen-
tence (? SUBJ =?), while the leftmost NP to the right of the V head of a VP is most
probably the verb?s object (? OBJ =?). Cahill, McCarthy, et al (2004) provide four
classes of annotation principles: one for noncoordinate configurations, one for coor-
dinate configurations, one for traces (long-distance dependencies), and a final ?catch
all and clean up? phase.
The satisfactory treatment of long-distance dependencies by the annotation algo-
rithm is imperative for the extraction of accurate semantic forms. The Penn Treebank
employs a rich arsenal of traces and empty productions (nodes which do not realize
any lexical material) to coindex displaced material with the position where it should
be interpreted semantically. The algorithm of Cahill, McCarthy, et al (2004) translates
the traces into corresponding reentrancies in the f-structure representation by treating
null constituents as full nodes and recording the traces in terms of index=i f-structure
annotations (Figure 3). Passive movement is captured and expressed at f-structure level
using a passive:+ annotation. Once a treebank tree is annotated with feature structure
equations by the annotation algorithm, the equations are collected, and a constraint
solver produces an f-structure.
In order to ensure the quality of the semantic forms extracted by our method, we
must first ensure the quality of the f-structure annotations. The results of two different
evaluations of the automatically generated f-structures are presented in Table 2. Both
use the evaluation software and triple encoding presented in Crouch et al (2002). The
first of these is against the DCU 105, a gold-standard set of 105 hand-coded f-structures
337
Computational Linguistics Volume 31, Number 3
Figure 3
Use of reentrancy between TOPIC and COMP to capture long-distance dependency in Penn
Treebank sentence wsj 0008 2, Until Congress acts, the government hasn?t any authority to issue new
debt obligations of any kind, the Treasury said.
from Section 23 of the Penn Treebank as described in Cahill, McCarthy, et al (2004). For
the full set of annotations they achieve precision of over 96.5% and recall of over 96.6%.
There is, however, a risk of overfitting when evaluation is limited to a gold standard
of this size. More recently, Burke, Cahill, et al (2004a) carried out an evaluation of the
automatic annotation algorithm against the publicly available PARC 700 Dependency
Bank (King et al 2003), a set of 700 randomly selected sentences from Section 23
which have been parsed, converted to dependency format, and manually corrected
and extended by human validators. They report precision of over 88.5% and recall of
over 86% (Table 2). The PARC 700 Dependency Bank differs substantially from both
the DCU 105 f-structure bank and the automatically generated f-structures in regard to
Table 2
Results of f-structure evaluation.
DCU 105 PARC 700
Precision 96.52% 88.57%
Recall 96.62% 86.10%
F-score 96.57% 87.32%
338
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
the style of linguistic analysis, feature nomenclature, and feature geometry. Some, but
not all, of these differences are captured by automatic conversion software. A detailed
discussion of the issues inherent in this process and a full analysis of results is presented
in Burke, Cahill, et al (2004a). Results broken down by grammatical function for the
DCU 105 evaluation are presented in Table 3. OBL (prepositional phrase) arguments are
traditionally difficult to annotate reliably. The results show, however, that with respect
to obliques, the annotation algorithm, while slightly conservative (recall of 82%), is very
accurate: 96% of the time it annotates an oblique, the annotation is correct.
A high-quality set of f-structures having been produced, the semantic form ex-
traction methodology is applied. This is based on and substantially extends both the
granularity and coverage of an idea in van Genabith, Sadler, and Way (1999):
For each f-structure generated, for each level of embedding we determine the local
PRED value and collect the subcategorisable grammatical functions present at that level
of embedding. (page 72)
Consider the automatically generated f-structure in Figure 4 for tree wsj 0003 22
in the Penn-II and Penn-III Treebanks. It is crucial to note that in the automatically
generated f-structures the value of the PRED feature is a lemma and not a semantic
form. Exploiting the information contained in the f-structure and applying the
method described above, we recursively extract the following nonempty semantic
forms: impose([subj, obj, obl:on]), in([obj]), of([obj]), and on([obj]). In effect,
in both the approach of van Genabith, Sadler, and Way (1999) and our approach,
semantic forms are reverse-engineered from automatically generated f-structures
for treebank trees. The automatically induced semantic forms contain the following
subcategorizable syntactic functions:
SUBJ OBJ OBJ2 OBLprep OBL2 COMP XCOMP PART
PART is not a syntactic function in the strict sense, but we decided to capture the
relevant co-occurrence patterns of verbs and particles in the semantic forms. Just as
Table 3
Precision and recall on automatically generated f-structures by feature against the DCU 105.
Feature Precision Recall F-score
ADJUNCT 892/968 = 92 892/950 = 94 93
COMP 88/92 = 96 88/102 = 86 91
COORD 153/184 = 83 153/167 = 92 87
DET 265/267 = 99 265/269 = 99 99
OBJ 442/459 = 96 442/461 = 96 96
OBL 50/52 = 96 50/61 = 82 88
OBLAG 12/12 = 100 12/12 = 100 100
PASSIVE 76/79 = 96 76/80 = 95 96
RELMOD 46/48 = 96 46/50 = 92 94
SUBJ 396/412 = 96 396/414 = 96 96
TOPIC 13/13 = 100 13/13 = 100 100
TOPICREL 46/49 = 94 46/52 = 88 91
XCOMP 145/153 = 95 145/146 = 99 97
339
Computational Linguistics Volume 31, Number 3
Figure 4
Automatically generated f-structure and extracted semantic forms for the Penn-II Treebank
string wsj 0003 22, In July, the Environmental Protection Agency imposed a gradual ban on virtually
all uses of asbestos.
OBLprep includes the prepositional head of the PP, PART includes the actual particle
which occurs, for example, add([subj, obj, part:up]).
In the work presented here, we substantially extend and scale the approach of
van Genabith, Sadler, and Way (1999) in regard to coverage, granularity, and eval-
uation. First, we scale the approach to the full WSJ section of the Penn-II Treebank
and the parsed Brown corpus section of Penn-III, with a combined total of approx-
imately 75,000 trees. Van Genabith, Sadler, and Way (1999) was proof of concept on
100 trees. Second, in contrast to the approach of van Genabith, Sadler, and Way (1999)
(and many other approaches), our approach fully reflects long-distance dependencies,
indicated in terms of traces in the Penn-II and Penn-III Treebanks and correspond-
ing reentrancies at f-structure. Third, in addition to abstract syntactic-function-
based subcategorization frames, we also compute frames for syntactic function?CFG
category pairs, for both the verbal heads and their arguments, and also generate
340
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Table 4
Conflation of Penn Treebank tags.
Conflated Category Penn Treebank Category
JJ JJ
JJR
JJS
N NN
NNS
NNP
NNPS
PRP
RB RB
RBR
RBS
V VB
VBD
VBG
VBN
VBP
VBZ
MD
pure CFG-based subcategorization frames. Fourth, in contrast to the approach of
van Genabith, Sadler, and Way (1999) (and many other approaches), our method differ-
entiates between frames for active and passive constructions. Fifth, in contrast to that of
van Genabith, Sadler, and Way (1999), our method associates conditional probabilities
with frames. Sixth, we evaluate the complete set of semantic forms extracted (not
just a selection) against the manually constructed COMLEX (MacLeod, Grishman, and
Meyers 1994) resource.
In order to capture CFG-based categorial information, we add a CAT feature to
the f-structures automatically generated from the Penn-II and Penn-III Treebanks. Its
value is the syntactic category of the lexical item whose lemma gives rise to the PRED
value at that particular level of embedding. This makes it possible to classify words
and their semantic forms based on their syntactic category and reduces the risk of
inaccurate assignment of subcategorization frame frequencies due to POS ambiguity,
distinguishing, for example, between the nominal and verbal occurrences of the lemma
fight. With this, the output for the verb impose in Figure 4 is impose(v,[subj, obj,
obl:on]). For some of our experiments, we conflate the different verbal (and other)
tags used in the Penn Treebanks to a single verbal marker (Table 4). As a further
extension, the extraction procedure reads off the syntactic category of the head of
each of the subcategorized syntactic functions: impose(v,[subj(n),obj(n),obl:on]).3
In this way, our methodology is able to produce surface syntactic as well as abstract
functional subcategorization details. Dalrymple (2001) argues that there are cases,
albeit exceptional ones, in which constraints on syntactic category are an issue in
subcategorization. In contrast to much of the work reviewed in Section 3, which limits
itself to the extraction of surface syntactic subcategorization details, our system can
provide this information as well as details of grammatical function.
3 We do not associate syntactic categories with OBLs as they are always PPs.
341
Computational Linguistics Volume 31, Number 3
Another way in which we develop and extend the basic extraction algorithm
is to deal with passive voice and its effect on subcategorization behavior. Consider
Figure 5: Not taking into account that the example sentence is a passive construction,
the extraction algorithm extracts outlaw([subj]). This is incorrect, as outlaw is a tran-
sitive verb and therefore requires both a subject and an object to form a gram-
matical sentence in the active voice. To cope with this problem, the extraction al-
gorithm uses the feature-value pair passive:+, which appears in the f-structure at
the level of embedding of the verb in question, to mark that predicate as occurring
in the passive: outlaw([subj],p). The annotation algorithm?s accuracy in recognizing
passive constructions is reflected by the f-score of 96% reported in Table 3 for the
PASSIVE feature.
The syntactic functions COMP and XCOMP refer to clausal complements with
different predicate control patterns as described in Section 2. However, as it stands,
neither of these functions betrays anything about the syntactic nature of the constructs
in question. Many lexicons, both automatically acquired and manually created, are
more fine grained in their approaches to subcategorized clausal arguments, differ-
entiating, for example, between a that-clause and a to + infinitive clause (Ushioda
et al 1993). With only a slight modification, our system, along with the details
provided by the automatically generated f-structures, allows us to extract frames
with an equivalent level of detail. For example, to identify a that-clause, we use
Figure 5
Automatically generated f-structure for the Penn-II Treebank string wsj 0003 23. By 1997, almost
all remaining uses of cancer-causing asbestos will be outlawed.
342
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Table 5
Semantic forms for the verb accept.
Semantic form Occurrences Conditional probability
accept([subj, obj]) 122 0.813
accept ([subj]) 11 0.073
accept([subj, comp]) 5 0.033
accept([subj, obl:as]) 3 0.020
accept([subj, obj, obl:as]) 3 0.020
accept([subj, obj, obl:from]) 3 0.020
accept([subj, obj, obl:at]) 1 0.007
accept([subj, obj, obl:for]) 1 0.007
accept([subj, obj, xcomp]) 1 0.007
the feature-value pair that:+ at f-structure level to read off the following subcate-
gorization frame for the verb add: add([subj,comp(that)]). Using the feature-value pair
to inf:+, we can identify to + infinitive clauses, resulting in the following frame for
the verb want: want([subj,xcomp(to inf)]). We can also derive control information about
open complements. In Figure 5, the reentrant XCOMP subject is identical to the subject
of will in the matrix clause, which allows us to induce information about the nature
of the external control of the XCOMP (i.e., whether it is subject or object control).
In order to estimate the likelihood of the co-occurrence of a predicate with a partic-
ular argument list, we compute conditional probabilities for subcategorization frames
based on the number of token occurrences in the corpus:
P (ArgList|?) = count(??ArgList?)?n
i=1 count(??ArgListi?)
where ArgList1... ArgListn are the possible argument lists which can occur for ?. Be-
cause of variations in verbal subcategorization across domains, probabilities are also
useful for predicting the way in which verbs behave in certain contexts. In Section 6,
we use the conditional probabilities to filter possible error judgments by our system.
Tables 5?7 show, with varying levels of analysis, the attested semantic forms for the
verb accept with their associated conditional probabilities. The effect of differentiating
between the active and passive occurrences of verbs can be seen in the different con-
ditional probabilities associated with the intransitive frame ([subj]) of the verb accept
(shown in boldface type) in Tables 5 and 6.4 Table 7 shows the joint grammatical-
function/syntactic-category-based subcategorization frames.
5. Results
We extract semantic forms for 4,362 verb lemmas from Penn-III. Table 8 shows the
number of distinct semantic form types (i.e., lemma and argument list combination)
4 Given these, it is possible to condition frames on both lemma (?) and voice (v: active/passive):
P (ArgList|?, v) = count(??ArgList, v?)?n
i=1 count(??ArgListi, v?)
343
Computational Linguistics Volume 31, Number 3
Table 6
Semantic forms for the verb accept marked with p for passive use.
Semantic form Occurrences Conditional probability
accept([subj, obj]) 122 0.813
accept ([subj],p) 9 0.060
accept([subj, comp]) 5 0.033
accept([subj, obl:as],p) 3 0.020
accept([subj, obj, obl:as]) 3 0.020
accept([subj, obj, obl:from]) 3 0.020
accept ([subj]) 2 0.013
accept([subj, obj, obl:at]) 1 0.007
accept([subj, obj, obl:for]) 1 0.007
accept([subj, obj, xcomp]) 1 0.007
Table 7
Semantic forms for the verb accept including syntactic category for each grammatical function.
Semantic form Occurrences Conditional probability
accept([subj(n), obj(n)]) 116 0.773
accept([subj(n)]) 11 0.073
accept([subj(n), comp(that)]) 4 0.027
accept([subj(n), obj(n), obl:from]) 3 0.020
accept([subj(n), obl:as]) 3 0.020
Other 13 0.087
extracted. Discriminating obliques by associated preposition and recording particle
information, the algorithm finds a total of 21,005 semantic form types, 16,000 occurring
in active voice and 5,005 in passive voice. When the obliques are parameterized for
prepositions and particles are included for particle verbs, we find an average of 4.82
semantic form types per verb. Without the inclusion of details for individual preposi-
tions or particles, there was an average of 3.45 semantic form types per verb. Unlike
many of the researchers whose work is reviewed in Section 3, we do not predefine the
frames extracted by our system. Table 9 shows the numbers of distinct frame types
extracted from Penn-II, ignoring PRED values.5 We provide two columns of statistics,
one in which all oblique (PP) arguments are condensed into one OBL function and
all particle arguments are condensed into part, and the other in which we differen-
tiate among obl:to (e.g., give), obl:on (e.g., rely), obl:for (e.g., compensate), etc., and
likewise for particles. Collapsing obliques and particles into simple functions, we extract
38 frame types. Discriminating particles and obliques by preposition, we extract 577
frame types. Table 10 shows the same results for Penn-III, with 50 simple frame types
and 1,084 types when parameterized for prepositions and particles. We also show the
result of applying absolute thresholding techniques to the semantic forms induced.
Applying an absolute threshold of five occurrences, we still generate 162 frame types
5 To recap, if two verbs have the same subcategorization requirements (e.g., give([subj, obj, obj2]),
send([subj, obj, obj2])), then that frame [subj, obj, obj2] is counted only once.
344
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Table 8
Number of semantic form types for Penn-III.
Without prepositions and particles With prepositions and particles
Semantic form types 15,166 21,005
Active 11,038 16,000
Passive 4,128 5,005
Table 9
Number of frame types for verbs for Penn-II.
Without prepositions With prepositions
and particles and particles
Number of frame types 38 577
Number of singletons 1 243
Number occurring twice 1 84
Number occurring five or fewer times 7 415
Number occurring more than five times 31 162
from Penn-II and 221 from Penn-III. Briscoe and Carroll (1997), by comparison, employ
163 distinct predefined frames.
6. Evaluation
Most of the previous approaches discussed in Section 3 have been evaluated to
different degrees. In general, a small number of frequently occurring verbs is selected,
and the subcategorization frames extracted for these verbs (from some quantity of
unseen test data) are compared to a gold standard. The gold standard is either manually
custom-made based on the test data or adapted from an existing external resource
such as the OALD (Hornby 1980) or COMLEX (MacLeod, Grishman, and Meyers
1994). There are advantages and disadvantages to both types of gold standard. While
it is time-consuming to manually construct a custom-made standard, the resulting
standard has the advantage of containing only the subcategorization frames exhibited
in the test data. Using an existing externally produced resource is quicker, but the gold
Table 10
Number of frame types for verbs for Penn-III.
Without prepositions With prepositions
and particles and particles
Number of frame types 50 1,084
Number of singletons 6 544
Number occurring twice 2 147
Number occurring five or fewer times 12 863
Number occurring more than five times 38 221
345
Computational Linguistics Volume 31, Number 3
standard may contain many more frames than those which occur in the data from which
the test lexicon is induced or, indeed, may omit relevant correct frames contained in
the data. As a result, systems generally score better against custom-made, manually
established gold standards.
Carroll and Rooth (1998) achieve an F-score of 77% against the OALD when they
evaluate a selection of 100 verbs with absolute frequency of greater than 500 each.
Their system recognizes 15 frames, and these do not contain details of subcategorized-
for prepositions. Still, to date this is the largest number of verbs used in any of the
evaluations of the systems for English described in Section 3. Sarkar and Zeman (2000)
evaluate 914 Czech verbs against a custom-made gold standard and record a token
recall of 88%. However, their evaluation does not examine the extracted subcatego-
rization frames but rather the argument?adjunct distinctions posited by their sys-
tem. The largest lexical evaluation we know of is that of Schulte im Walde (2002b)
for German. She evaluates 3,000 German verbs with a token frequency between
10 and 2,000 against the Duden (Dudenredaktion 2001). We will refer to this work
and the methods and results presented by Schulte im Walde again in Sections 6.2
and 6.3.
We carried out a large-scale evaluation of our automatically induced lexicon (2,993
active verb lemmas for Penn-II and 3,529 for Penn-III, as well as 1,422 passive verb
lemmas from Penn-II) against the COMLEX resource. To our knowledge this is the most
extensive evaluation ever carried out for English lexical extraction. We conducted a
number of experiments on the subcategorization frames extracted from Penn-II and
Penn-III which are described and discussed in Sections 6.2, 6.3, and 6.4. Finding a
common format for the gold standard and induced lexical entries is a nontrivial task.
To ensure that we did not bias the evaluation in favor of either resource, we carried
out two different mappings for the frames from Penn-II and Penn-III: COMLEX-LFG
Mapping I and COMLEX-LFG Mapping II. For each mapping we carried out six basic
experiments (and two additional ones for COMLEX-LFG Mapping II) for the active
subcategorization frames extracted. Within each experiment, the following factors were
varied: level of prepositional phrase detail, level of particle detail, relative threshold
(1% or 5%), and incorporation of an expanded set of directional prepositions. Using
the second mapping we also evaluated the automatically extracted passive frames and
experimented with absolute thresholds. Direct comparison of subcategorization frame
acquisition systems is difficult because of variations in the number of frames extracted,
the number of test verbs, the gold standards used, the size of the test data, and the
level of detail in the subcategorization frames (e.g., whether they are parameterized
for specific prepositions). Therefore, in order to establish a baseline against which to
compare our results, following Schulte in Walde (2002b), we assigned the two most
frequent frame types (transitive and intransitive) by default to each verb and compared
this ?artificial? lexicon to the gold standard. The section concludes with a full discussion
of the reported results.
6.1 COMLEX
We evaluate our induced semantic forms against COMLEX (MacLeod, Grishman, and
Meyers 1994), a computational machine-readable lexicon containing syntactic infor-
mation for approximately 38,000 English headwords. Its creators paid particular
attention to the encoding of more detailed subcategorization information than is avail-
able in either the OALD or the LDOCE (Proctor 1978), both for verbs and for nouns
346
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Figure 6
Intersection between active-verb lemma types in COMLEX and the Penn-II-induced lexicon.
and adjectives which take complements (Grishman, MacLeod, and Meyers 1994). By
choosing to evaluate against COMLEX, we set our sights high: Our extracted semantic
forms are fine-grained, and COMLEX is considerably more detailed than the OALD
or LDOCE used for earlier evaluations. While our system can generate semantic forms
for any lemma (regardless of part of speech) which induces a PRED value, we have
thus far evaluated the automatic generation of subcategorization frames for verbs
only. COMLEX defines 138 distinct verb frame types without the inclusion of specific
prepositions or particles.
As COMLEX contains information other than subcategorization details, it was
necessary for us to extract the subcategorization frames associated with each verbal
lexicon entry. The following is a sample entry for the verb reimburse:
(VERB :ORTH ?reimburse? :SUBC ((NP-NP)
(NP-PP :PVAL (?for?))
(NP)))
Each entry is organized as a nested set of typed feature-value lists. The first symbol
(i.e., VERB) gives the part of speech. The value of the :ORTH feature is the base form
of the verb. Any entry with irregular morphological behavior will also include the
features :PLURAL, :PAST, and so on, with the relevant values. All verbs have a :SUBC
feature, and for our purposes, this is the most interesting feature. In the case of the
example above, the subcategorization values specify that reimburse can occur with two
object noun phrases (NP-NP), an object noun phrase followed by a prepositional phrase
headed by for (NP-PP :PVAL (?for?)) or just an object noun phrase (NP). (Note that the
details of the subject are not included in COMLEX frames.) What makes the COMLEX
resource particularly suitable for our evaluation is that each of the complement types
(NP-NP, NP-PP, and NP) which make up the value of the :SUBC feature is associated with
a formal frame definition which looks like the following:
(vp-frame np-np :cs ((np 2)(np 3))
:gs (:subject 1 :obj 2 :obj2 3)
:ex ?she asked him his name?)
The value of the :cs feature is the constituent structure of the subcategorization
frame, which lists the syntactic CF-PSG constituents in sequence (omitting the sub-
ject, again). The value of the :gs feature is the grammatical structure which indicates
the functional role played by each of the CF-PSG constituents. The elements of the
347
Computational Linguistics Volume 31, Number 3
Figure 7
Intersection between active-verb lemma types in COMLEX and the Penn-III-induced lexicon.
constituent structure are indexed, and these indices are referenced in the :gs field.
The index 1 always refers to the surface subject of the verb. This mapping between
constituent structure and functional structure makes the information contained in
COMLEX particularly suitable as an evaluation standard for the LFG semantic forms
which we induce.
We present the evaluation for the verbs which occur in an active context in the
treebank. COMLEX does not provide passive frames. For Penn-II, there are 2,993
verb lemmas (used actively) that both resources have in common. 2,669 verb lemmas
appear in COMLEX but not in the induced lexicon, and 416 verb lemmas (used actively)
appear in the induced lexicon but not in COMLEX (Figure 6). For Penn-III, COMLEX
and the induced lexicon share 3,529 verb lemmas (used actively). This is shown in
Figure 7. 6
6.2 COMLEX-LFG Mapping I and Penn-II
In order to carry out the evaluation, we have to find a common format for the expression
of subcategorization information between our induced LFG-style subcategorization
frames and those contained in COMLEX. The following are the common syntactic
functions: SUBJ, OBJ, OBJi, COMP, and PART. Unlike our system, COMLEX does not
distinguish an OBL from an OBJi, so we converted all the obliques in the induced frames
to OBJi. As in COMLEX, the value of i depends on the number of objects/obliques
already present in the semantic form. COMLEX does not differentiate between COMPs
and XCOMPs as our system does (control information is expressed in a different way:
see Section 6.3), so we conflate our two LFG categories to that of COMP. The process is
summarized in Table 11.
The manually constructed COMLEX entries provide a gold standard against which
we evaluate the automatically induced frames. We calculate the number of true pos-
itives (tps) (where our semantic forms and those from COMLEX are the same), the
number of false negatives ( fns) (those frames which appeared in COMLEX but were not
produced by our system), and the number of false positives ( fps) (those frames
6 Given these figures, one might begin to wonder about the value of automatic induction. First, COMLEX
does not rank frames by probabilities, which are essential in disambiguation. Second, the coverage of
COMLEX is not complete: 518 lemmas ?discovered? by the induction experiment are not listed in
COMLEX; see the error analysis in Section 6.5.
348
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Table 11
Mapping I: Merging of COMLEX and LFG syntactic functions.
Our syntactic functions COMLEX syntactic functions Merged function
SUBJ Subject SUBJ
OBJ Object OBJ
OBJ2 Obj2 OBJi
OBL Obj3
OBL2 Obj4
COMP Comp COMP
XCOMP
PART Part PART
produced by our system which do not appear in COMLEX). We calculate precision,
recall, and F-score using the following standard equations:
recall =
tp
tp + fn
precision =
tp
tp + fp
f-score =
2 ? recall ? precision
recall + precision
We use the frequencies associated with each of our semantic forms in order to set
a relative threshold to filter the selection of semantic forms. For a threshold of 1% we
disregard any semantic forms with a conditional probability (i.e., given a lemma) of
less than or equal to 0.01. As some verbs occur less frequently than others, we think it
is important to use a relative rather than absolute threshold (as in Carroll and Rooth
[1998], for instance) in this way. We carried out the evaluation in a similar way to
Schulte im Walde?s (2002b) for German, the only experiment comparable in scale to
ours. Despite the obvious differences in approach and language, this allows us to make
some tentative comparisons between our respective results. The statistics shown in
Table 12 give the results of three different experiments with the relative threshold set
to 1%. As for all the results tables, the baseline statistics (simply assigning the most
frequent frames, in this case transitive and intransitive, to each lemma by default) are
in each case shown in the left column, and the results achieved by our induced lexicon
are presented in the right column. Distinguishing between complement and adjunct
prepositional phrases is a notoriously difficult aspect of automatic subcategorization
frame acquisition. For this reason, following the evaluation setup in Schulte im Walde
(2002b), the three experiments vary with respect to the amount of prepositional infor-
mation contained in the subcategorization frames.
Experiment 1. Here we excluded subcategorized prepositional-phrase arguments en-
tirely from the comparison. In a manner similar to that of Schulte im Walde (2002b), any
349
Computational Linguistics Volume 31, Number 3
Table 12
Results of Penn-II evaluation of active frames against COMLEX (relative threshold of 1%).
Precision Recall F-score
Mapping I Baseline Induced Baseline Induced Baseline Induced
Experiment 1 66.1% 75.2% 65.8% 69.1% 66.0% 72.0%
Experiment 2 71.5% 65.5% 64.3% 63.1% 67.7% 64.3%
Experiment 3 64.7% 71.8% 11.9% 16.8% 20.1% 27.3%
frames containing an OBL were mapped to the same frame type minus that argument.
For example, the frame [subj,obl:for] becomes [subj]. Using a relative threshold of
1% (Table 12), our results (precision of 75.2%, recall of 69.1%, and F-score of 72.0%)
are remarkably similar to those of Schulte im Walde (2002b), who reports precision of
74.53%, recall of 69.74%, and an f-score of 72.05%.
Experiment 2. Here we include subcategorized prepositional phrase arguments but
only in their simplest form; that is, they were not parameterized for particular prepo-
sitions. For example, the frame [subj,obl:for] is rewritten as [subj,obl]. Using a
relative threshold of 1% (Table 12), our results (precision of 65.5%, recall of 63.1%, and
F-score of 64.3%) compare favorably to those of Schulte im Walde (2002b), who recorded
precision of 60.76%, recall of 63.91%, and an F-score of 62.30%.
Experiment 3. Here we used semantic forms which contain details of specific prepo-
sitions for any subcategorized prepositional phrase (e.g., [subj,obl:for]). Using a rela-
tive threshold of 1% (Table 12), our precision figure (71.8%) is quite high (in comparison
to 65.52% as recorded by Schulte im Walde [2002b]). However our recall (16.8%) is very
low (compared to the 50.83% that Schulte im Walde [2002b] reports). Consequently our
F-score (27.3%) is also low (Schulte im Walde [2002b] records an F-score of 57.24%). The
reason for this is discussed in Section 6.2.1.
The statistics in Table 13 are the result of the second experiment, in which the
relative threshold was increased to 5%. The effect of such an increase is obvious in
that precision goes up (by as much as 5%) for each of the three evaluations while
recall goes down (by as much as 5.5%). This is to be expected, as a greater threshold
means that there are fewer semantic forms associated with each verb in the induced
lexicon, but they are more likely to be correct because of their greater frequency of
occurrence. The conditional probabilities we associate with each semantic form together
with thresholding can be used to customize the induced lexicon to the task for which
it is required, that is, whether a very precise lexicon is preferred to one with broader
Table 13
Results of Penn-II evaluation of active frames against COMLEX (relative threshold of 5%).
Precision Recall F-score
Mapping I Baseline Induced Baseline Induced Baseline Induced
Experiment 1 66.1% 80.2% 65.8% 63.6% 66.0% 70.9%
Experiment 2 71.5% 69.6% 64.3% 56.9% 67.7% 62.7%
Experiment 3 64.7% 76.7% 11.9% 13.9% 20.1% 23.5%
350
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
coverage. In Tables 12 and 13, the baseline is exceeded in all experiments with the
exception of Experiment 2. This can be attributed to Mapping I, in which OBLi becomes
OBJi (Table 11). Experiment 2 includes obliques without the specific preposition, mean-
ing that in this mapping, the frame [subj,obj:with] becomes [subj,obj]. Therefore,
the transitive baseline frame scores better than it should against the gold standard. A
more fine-grained LFG-COMLEX mapping in which this effect disappears is presented
in Section 6.3.
6.2.1 Directional Prepositions. Our recall statistic was particularly low in the case of
evaluation using details of prepositions (Experiment 3, Tables 12 and 13). This can be
accounted for by the fact that the creators of COMLEX have chosen to err on the side
of overgeneration in regard to the list of prepositions which may occur with a verb and
a subcategorization frame containing a prepositional phrase. This is particularly true
of directional prepositions. For COMLEX, a list of 31 directional prepositions (Table 14)
was prepared and assigned in its entirety by default to any verb which can potentially
appear with any directional preposition in order to save time and avoid the risk of
missing prepositions. Grishman, MacLeod, and Meyers (1994) acknowledge that this
can lead to a preposition list which is ?a little rich? for a particular verb, but this is
the approach they have chosen to take. In a subsequent experiment, we incorporated
this list of directional prepositions by default into our semantic form induction process
in the same way as the creators of COMLEX have done. Table 15 shows that doing
so results in a significant improvement in the recall statistic (45.1%), as would be
expected, with the new statistic being almost three times as good as the result re-
ported in Table 12 for Experiment 3 (16.8%). There is also an improvement in the
precision figure (from 71.8% to 86.9%). This is due to a substantial increase in the
number of true positives (from 5,612 to 14,675) compared with a stationary false pos-
itive figure (2,205 in both cases). The f-score increases from 27.3% to 59.4%.
6.3 COMLEX-LFG Mapping II and Penn-II
The COMLEX-LFG Mapping I presented above establishes a ?least common denomi-
nator? for the COMLEX and our LFG-inspired resources. More-fine-grained mappings
are possible: in order to ensure that the mapping from our semantic forms to the
COMLEX frames did not oversimplify the information in the automatically extracted
subcategorization frames, we conducted a further set of experiments in which we
converted the information in the COMLEX entries to the format of our extracted
semantic forms. We explicitly differentiated between OBLs and OBJs by automatically
Table 14
COMLEX directional prepositions.
about across along around
behind below beneath between
beyond by down from
in inside into off
on onto out out of
outside over past through
throughout to toward toward
up up to via
351
Computational Linguistics Volume 31, Number 3
Table 15
Penn-II evaluation of active frames against COMLEX using p-dir list (relative threshold of 1%).
Mapping I Precision Recall F-score
Experiment 3 86.9% 45.1% 59.4%
deducing whether a COMLEX OBJi was coindexed with an NP or a PP. Furthermore, as
can be seen in the following example, COMLEX frame definitions contain details of the
control patterns of sentential complements, encoded using the :features attribute. This
allows for automatic discrimination between COMPs and XCOMPs.
(vp-frame to-inf-sc :cs (vp 2 :mood to-infinitive :subject 1)
:features (:control subject)
:gs (:subject 1 :comp 2)
:ex ?I wanted to come?)
The mapping is summarized in Table 16. The results of the subsequent evaluation are
presented in Tables 17 and 18. We have added Experiments 2a and 3a. These are the
same as Experiments 2 and 3, except that they additionally include the specific particle
with each PART function. While the recall figures in Tables 17 and 18 are slightly lower
than those in Tables 12 and 13, changing the mapping in this way results in an increase
in precision in each case (by as much as 11.6%). The results of the lexical evaluation
are consistently better than the baseline, in some cases by almost 16% (Experiment 2,
threshold 5%). Notice that in contrast to Tables 12 and 13, in the more-fine-grained
COMLEX-LFG Mapping II presented here, all experiments exceed the baseline.
6.3.1 Directional Prepositions. The recall figures for Experiments 3 and 3a in Table 17
(24.0% and 21.5%) and Table 18 (19.7% and 17.4%) drop in a similar fashion to the results
seen in Tables 12 and 13. For this reason, we again incorporated the list of 31 directional
prepositions (Table 14) by default and reran Experiments 3 and 3a for a threshold of
1%. The results are presented in Table 19. The effect was as expected: The recall scores
for the two experiments increased to 40.8% and 35.4% (from 24.0% and 22.5%), and the
F-scores increased to 54.4% and 49.7% (from 35.9% and 33.0%).
6.3.2 Passive Evaluation. Table 20 presents the results of evaluating the extracted pas-
sive semantic forms for 1,422 verb lemmas shared by the induced lexicon and COMLEX.
Table 16
Mapping II: Merging of COMLEX and LFG syntactic functions.
Our syntactic functions COMLEX syntactic functions Merged function
SUBJ Subject SUBJ
OBJ Object OBJ
OBJ2 Obj2 OBJ2
OBL Obj3 OBL
OBL2 Obj4 OBL2
COMP Comp COMP
XCOMP Comp XCOMP
PART Part PART
352
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Table 17
Results of Penn-II evaluation of active frames against COMLEX (relative threshold of 1%).
Precision Recall F-score
Mapping II Baseline Induced Baseline Induced Baseline Induced
Experiment 1 72.1% 79.0% 58.5% 59.6% 64.6% 68.0%
Experiment 2 65.2% 77.1% 37.4% 50.4% 47.5% 61.0%
Experiment 2a 65.2% 76.4% 32.7% 44.5% 43.6% 56.3%
Experiment 3 65.2% 75.9% 15.2% 24.0% 24.7% 35.9%
Experiment 3a 65.2% 71.0% 13.6% 21.5% 22.5% 33.0%
Table 18
Results of Penn-II evaluation of active frames against COMLEX (relative threshold of 5%).
Precision Recall F-score
Mapping II Baseline Induced Baseline Induced Baseline Induced
Experiment 1 72.1% 83.5% 58.5% 54.7% 64.6% 66.1%
Experiment 2 65.2% 81.4% 37.4% 44.8% 47.5% 57.8%
Experiment 2a 65.2% 80.9% 32.7% 39.0% 43.6% 52.6%
Experiment 3 65.2% 75.9% 15.2% 19.7% 24.7% 31.3%
Experiment 3a 65.2% 75.5% 13.6% 17.4% 22.5% 28.3%
We applied lexical-redundancy rules (Kaplan and Bresnan 1982) to automatically con-
vert the active COMLEX frames to their passive counterparts: For example, subjects are
demoted to optional by oblique agents, and direct objects become subjects. The resulting
precision was very high (from 72.3% to 80.2%), and there was the expected drop in recall
when prepositional details were included (from 54.7% to 29.3%).
Table 19
Penn-II evaluation of active frames against COMLEX using p-dir list (relative threshold of 1%).
Mapping II Precision Recall F-score
Experiment 3 81.7% 40.8% 54.4%
Experiment 3a 83.1% 35.4% 49.7%
Table 20
Results of Penn-II evaluation of passive frames (relative threshold of 1%).
Passive Precision Recall F-score
Experiment 2 80.2% 54.7% 65.1%
Experiment 2a 79.7% 46.2% 58.5%
Experiment 3 72.6% 33.4% 45.8%
Experiment 3a 72.3% 29.3% 41.7%
353
Computational Linguistics Volume 31, Number 3
6.3.3 Absolute Thresholds. Many of the previous approaches discussed in Section 3 use
a limited number of verbs for evaluation, based on the verbs? absolute frequency in the
corpus. We carried out a similar experiment. Table 21 shows the results of Experiment
2 for all verbs, for the verb lemmas with an absolute frequency greater than 100, and
for verbs with a frequency greater than 200. The use of an absolute threshold results
in an increase in precision (from 77.1% to 82.3% and 81.7%), an increase in recall (from
50.4% to 60.8% to 58.7%), and an overall increase in F-score (from 61.0% to 69.9%
and 68.4%).
6.4 Penn-III (Mapping-II)
Recently we have applied our methodology to the Penn-III Treebank, a more balanced
corpus resource with a number of text genres. Penn-III consists of the WSJ section from
Penn-II as well as a parse-annotated subset of the Brown corpus. The Brown corpus
comprises 24,242 trees compiled from a variety of text genres including popular lore,
general fiction, science fiction, mystery and detective fiction, and humor. It has been
shown (Roland and Jurafsky 1998) that the subcategorization tendencies of verbs vary
across linguistic domains. Our aim, therefore, is to increase the scope of the induced
lexicon not only in terms of the verb lemmas for which there are entries, but also in
terms of the frames with which they co-occur. The f-structure annotation algorithm was
extended with only minor amendments to cover the parsed Brown corpus. The most
important of these was the way in which we distinguish between oblique and adjunct.
We noted in Section 4 that our method of assigning an oblique annotation in Penn-II
was precise, albeit conservative. Because of a change of annotation policy in Penn-III,
the -CLR tag (indicating a close relationship between a PP and the local syntactic head),
information which we had previously exploited, is no longer used. For Penn-III the
algorithm annotates all PPs which do not carry a Penn adverbial functional tag (such
as -TMP or -LOC) and occur as the sisters of the verbal head of a VP as obliques.
In addition, the algorithm annotates as obliques PPs associated with -PUT (locative
complements of the verb put) or -DTV (second object in ditransitives) tags.
When evaluating the application of the lexical extraction system on Penn-III, we
carried out two sets of experiments, identical in each case to those described for Penn-II
in Section 6.3, including the use of relative (1% and 5%) rather than absolute thresholds.
For the first set of experiments we evaluated the lexicon induced from the parse-
annotated Brown corpus only. This evaluation was performed for 2,713 active-verb
lemmas using the more fine-grained Mapping-II. Tables 22 and 23 show that the results
generally exceed the baseline, in some cases by almost 10%, similar to those recorded
for Penn-II (Tables 17 and 18). While the precision is slightly lower than that re-
ported for the experiments in Tables 17 and 18, in particular for Experiments 2, 2a, 3,
Table 21
Penn-II evaluation of active frames against COMLEX using absolute thresholds (Experiment 2).
Threshold Precision Recall F-score
All 77.1% 50.4% 61.0%
Threshold 100 82.3% 60.8% 69.9%
Threshold 200 81.7% 58.7% 68.4%
354
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Table 22
Results of Penn-III active frames (Brown Corpus only) COMLEX comparison (relative threshold
of 1%).
Precision Recall F-Score
Mapping II Baseline Induced Baseline Induced Baseline Induced
Experiment 1 73.2% 79.2% 60.1% 60.0% 66.0% 68.2%
Experiment 2 66.0% 70.5% 37.5% 50.5% 47.8% 58.9%
Experiment 2a 66.0% 71.3% 32.7% 44.5% 43.7% 54.8%
Experiment 3 66.0% 64.3% 15.2% 23.1% 24.8% 34.0%
Experiment 3a 66.0% 64.1% 13.5% 20.7% 22.4% 31.3%
and 3a, in which details of obliques are included, the recall in each of these experi-
ments is slightly higher than that recorded for Penn-II. We conjecture that the main
reason for this is that the amended approach to the annotation of obliques is slightly
less precise and conservative than the largely -CLR-tag-driven approach taken for
Penn-II. Consequently we record an increase in recall and a drop in precision. This
trend is repeated in the second set of experiments. In this instance, we combined the
lexicon extracted from the WSJ with that extracted from the parse-annotated Brown
corpus, and evaluated the resulting resource for 3,529 active-verb lemmas. The results
are shown in Tables 24 and 25. The results compare very positively against the baseline.
The precision scores are lower (by between 1.5% and 9.7%) than those reported for
Penn-II (Tables 17 and 18). There has however been a significant increase in recall (up to
8.7%) and an overall increase in F-score (by up to 4.4%).
6.5 Error Analysis and Discussion
The work presented in this section highlights a number of issues associated with the
evaluation of automatically induced subcategorization frames against an existing exter-
nal gold standard, in this case COMLEX. While this evaluation approach is arguably
less labor-intensive than the manual construction of a custom-made gold standard,
it does introduce a number of difficulties into the evaluation procedure. It is a
nontrivial task to convert both the gold standard and the induced resource to a common
Table 23
Results of Penn-III active frames (Brown corpus only) COMLEX comparison (relative threshold
of 5%).
Precision Recall F-score
Mapping II Baseline Induced Baseline Induced Baseline Induced
Experiment 1 73.2% 82.7% 60.1% 56.4% 66.0% 67.0%
Experiment 2 66.0% 74.6% 37.5% 46.1% 47.8% 57.0%
Experiment 2a 66.0% 76.0% 32.7% 40.0% 43.7% 52.4%
Experiment 3 66.0% 69.2% 15.2% 18.7% 24.8% 29.5%
Experiment 3a 66.0% 69.0% 13.5% 16.6% 22.4% 26.7%
355
Computational Linguistics Volume 31, Number 3
Table 24
Results of Penn-III active frames (Brown and WSJ) COMLEX comparison (relative threshold of
1%).
Precision Recall F-score
Mapping II Baseline Induced Baseline Induced Baseline Induced
Experiment 1 71.2% 77.4% 62.9% 66.2% 66.8% 71.4%
Experiment 2 64.5% 70.4% 40.0% 58.0% 49.3% 63.6%
Experiment 2a 64.5% 71.5% 35.1% 51.9% 45.5% 60.2%
Experiment 3 64.5% 66.2% 17.0% 27.4% 26.8% 38.8%
Experiment 3a 64.5% 66.0% 15.1% 24.8% 24.5% 36.0%
format in order to facilitate evaluation. In addition, as our results show, the choice
of common format and mapping to it can affect the results. In COMLEX-LFG Map-
ping I (Section 6.2), we found that mapping from the induced lexicon to COMLEX
resulted in higher recall scores than those achieved when we (effectively) reversed the
mapping (COMLEX-LFG Mapping II [Section 6.3]). The first mapping is essentially a
conflation of our more fine-grained LFG grammatical functions with the more generic
COMLEX functions, while the second mapping tries to maintain as many distinctions
as possible.
Another drawback to using an existing external gold standard such as COMLEX
to evaluate an automatically induced subcategorization lexicon is that the resources
are not necessarily constructed from the same source data. As noted above, it is well doc-
umented (Roland and Jurafsky 1998) that subcategorization frames (and their frequen-
cies) vary across domains. We have extracted frames from two sources (the WSJ and the
Brown corpus), whereas COMLEX was built using examples from the San Jose Mercury
News, the Brown corpus, several literary works from the Library of America, scientific
abstracts from the U.S. Department of Energy, and the WSJ. For this reason, it is likely
to contain a greater variety of subcategorization frames than our induced lexicon. It is
also possible that because of human error, COMLEX contains subcategorization frames
the validity of which are in doubt, for example, the overgeneration of subcategorized-for
directional prepositional phrases. This is because the aim of the COMLEX project was to
construct as complete a set of subcategorization frames as possible, even for infrequent
verbs. Lexicographers were allowed to extrapolate from the citations found, a procedure
Table 25
Results of Penn-III active frames (Brown and WSJ) COMLEX comparison (relative threshold of
5%).
Precision Recall F-score
Mapping II Baseline Induced Baseline Induced Baseline Induced
Experiment 1 71.2% 82.0% 62.9% 61.0% 66.8% 69.9%
Experiment 2 64.5% 74.3% 40.0% 53.5% 49.3% 62.2%
Experiment 2a 64.5% 76.4% 35.1% 45.1% 45.5% 56.7%
Experiment 3 64.5% 71.1% 17.0% 21.5% 26.8% 33.0%
Experiment 3a 64.5% 70.8% 15.1% 19.2% 24.5% 30.2%
356
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
which is bound to be less certain than the assignment of frames based entirely on exist-
ing examples. As a generalization, Briscoe (2001) notes that lexicons such as COMLEX
tend to demonstrate high precision but low recall. Briscoe and Carroll (1997) report
on manually analyzing an open-class vocabulary of 35,000 head words for predicate
subcategorization information and comparing the results against the subcategorization
details in COMLEX. Precision was quite high (95%), but recall was low (84%). This has
an effect on both the precision and recall scores of our system against COMLEX. In order
to ascertain the effect of using COMLEX as a gold standard for our induced lexicon,
we carried out some more-detailed error analysis, the results of which are summarized
in Table 26. We randomly selected 80 false negatives (fn) and 80 false positives (fp)
across a range of active frame types containing prepositional and particle detail taken
from Penn-III and manually examined them in order to classify them as ?correct? or
?incorrect.? Of the 80 fps, 33 were manually judged to be legitimate subcategorization
frames. For example, as Table 26 shows, there are a number of correct transitive verbs
([subj,obj]) in our automatically induced lexicon which are not included in COMLEX.
This examination was also useful in highlighting to us the frame types on which
the lexical extraction procedure was performing poorly, in our case, those containing
XCOMPs and those containing OBJ2S. Out of 80 fns, 14 were judged to be incorrect when
manually examined. These can be broken down as follows: one intransitive frame, three
ditransitive frames, three frames containing a COMP, and seven frames containing an
oblique were found to be invalid.
7. Lexical Accession Rates
In addition to evaluating the quality of our extracted semantic forms, we also examined
the rate at which they are induced. This can be expressed as a measure of the coverage
of the induced lexicon on new data. Following Hockenmaier, Bierner, and Baldridge
(2002), Xia (1999), and Miyao, Ninomiya, and Tsujii (2004), we extract a reference
lexicon from Sections 02?21 of the WSJ. We then compare this to a test lexicon from
Section 23. Table 27 shows the results of the evaluation of the coverage of an induced
lexicon for verbs only. There is a corresponding semantic form in the reference lexicon
for 89.89% of the verbs in Section 23. 10.11% of the entries in the test lexicon did not
appear in the reference lexicon. Within this group, we can distinguish between known
words, which have an entry in the reference lexicon, and unknown words, which do
not exist at all in the reference lexicon. In the same way we make the distinction
Table 26
Error analysis.
Frame type COMLEX: False negatives Induced: False positives
Correct Incorrect Correct Incorrect
[subj] 9 1 4 6
[subj, obj] 10 0 9 1
[subj, obj, obj2] 7 3 1 9
[.., xcomp, ..] 10 0 1 10
[.., comp, ..] 7 3 4 5
[.., obl, ..] 23 7 14 16
357
Computational Linguistics Volume 31, Number 3
Table 27
Coverage of induced lexicon (WSJ 02?21) on unseen data (WSJ 23) (verbs only).
Entries also in reference lexicon 89.89%
Entries not in reference lexicon 10.11%
Known words 7.85%
Known words, known frames 7.85%
Known words, unknown frames 0
Unknown words 2.32%
Unknown words, known frames 2.32%
Unknown words, unknown frames 0
between known frames and unknown frames. There are, therefore, four different cases
in which an entry may not appear in the reference lexicon. Table 27 shows that the
most common case is that of known verbs occurring with a different, although known,
subcategorization frame (7.85%).
The rate of accession may also be represented graphically. In Charniak (1996) and
Krotov et al (1998), it was observed that treebank grammars (CFGs extracted from
treebanks) are very large and grow with the size of the treebank. We were interested in
discovering whether the acquisition of lexical material from the same data displayed a
similar propensity. Figure 8 graphs the rate of induction of semantic form and CFG rule
types from Penn-III (the WSJ and parse-annotated Brown corpus combined). Because
of the variation in the size of sections between the Brown and the WSJ, we plotted
accession against word count. The first part of the graph (up to 1,004,414 words)
Figure 8
Comparison of accession rates for semantic form and CFG rule types for Penn-III (nonempty
frames) (WSJ followed by Brown).
358
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
represents the rate of accession from the WSJ, and the final 384,646 words are those
of the Brown corpus. The seven curves represent the following: The acquisition of
semantic form types (nonempty) for all syntactic categories with and without specific
preposition and particle information, the acquisition of semantic form types (non-
empty) for all verbs with and without specific preposition and particle information,
the number of lemmas associated with the extract semantic forms, and the acqui-
sition of CFG rule types. The curve representing the growth in the overall size of
the lexicon is similar in shape to that of the PCFG, while the rate of increase in
the number of verbal semantic forms (particularly when obliques and particles are
excluded) appears to slow more quickly. Figure 8 shows the effect of domain di-
versity from the Brown section in terms of increased growth rates for 1e+06 words
upward. Figure 9 depicts the same information, this time extracted from the Brown
section first followed by the WSJ. The curves are different, but similar trends are
represented. This time the effects of domain diversity for the Brown section are
discernible by comparing the absolute accession rate for the 0.4e+06 mark between
Figures 8 and 9.
Figure 10 shows the result when we abstract away from semantic forms (verb
frame combinations) to subcategorization frames and plot their rate of acces-
sion. The graph represents the growth rate of frame types for Penn-III (WSJ fol-
lowed by Brown and Brown followed by WSJ). The curve rises sharply initially
but gradually levels, practically flattening out, despite the increase in the number
of words. This reflects the information about Section 23 in Table 27, where we demon-
strate that although new verb frame combinations occur, all of the frame types in
Section 23 have been seen by the lexical extraction program in previous sections.
Figure 9
Comparison of accession rates for semantic form and CFG rule types for Penn-III (nonempty
frames) (Brown followed by WSJ).
359
Computational Linguistics Volume 31, Number 3
Figure 10
Accession rates for frame types (without prepositions and particles) for Penn-III.
Figure 11 shows that including information about prepositions and particles in the
frames results in an accession rate which continues to grow, albeit ever more slowly,
with the increase in size of the extraction data. This emphasizes the advantage of our
approach, which extracts frames containing such information without the limitation
of predefinition.
Figure 11
Accession rates for frame types for Penn-III.
360
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
8. Conclusions and Further Work
We have presented an algorithm for the extraction of semantic forms (or subcatego-
rization frames) from the Penn-II and Penn-III Treebanks, automatically annotated with
LFG f-structures. In contrast to many other approaches, ours does not predefine the sub-
categorization frames we extract. We have applied the algorithm to the WSJ sections of
Penn-II (50,000 trees) (O?Donovan et al 2004) and to the parse-annotated Brown corpus
of Penn-III (almost 25,000 additional trees). We extract syntactic-function-based subcat-
egorization frames (LFG semantic forms) and traditional CFG category-based frames, as
well as mixed-function-category-based frames. Unlike many other approaches to sub-
categorization frame extraction, our system properly reflects the effects of long-distance
dependencies. Also unlike many approaches, our method distinguishes between active
and passive frames. Finally, our system associates conditional probabilities with the
frames we extract. Making the distinction between the behavior of verbs in active and
passive contexts is particularly important for the accurate assignment of probabilities
to semantic forms. We carried out an extensive evaluation of the complete induced
lexicon against the full COMLEX resource. To our knowledge, this is the most extensive
qualitative evaluation of subcategorization extraction in English. The only evaluation of
a similar scale is that carried out by Schulte im Walde (2002b) for German. The results
reported here for Penn-II compare favorably against the baseline and, in fact, are an
improvement on those reported in O?Donovan et al (2004). The results for the larger,
more domain-diverse Penn-III lexicon are very encouraging, in some cases almost 15%
above the baseline. We believe our semantic forms are fine-grained, and by choosing
to evaluate against COMLEX, we set our sights high: COMLEX is considerably more
detailed than the OALD or LDOCE used for other earlier evaluations. Our error analysis
also revealed some interesting issues associated with using an external standard such as
COMLEX. In the future, we hope to evaluate the automatic annotations and extracted
lexicon against Propbank (Kingsbury and Palmer 2002).
Apart from the related approach of Miyao, Ninomiya, and Tsujii (2004), which
does not distinguish between argument and adjunct prepositional phrases, our
treebank and automatic f-structure annotation-based architecture for the automatic
acquisition of detailed subcategorization frames is quite unlike any of the architec-
tures presented in the literature. Subcategorization frames are reverse-engineered and
almost a byproduct of the automatic f-structure annotation algorithm. It is important
to realize that the induction of lexical resources is part of a larger project on the
acquisition of wide-coverage, robust, probabilistic, deep unification grammar resources
from treebanks Burke, Cahill, et al (2004b). We are already using the extracted seman-
tic forms in parsing new text with robust, wide-coverage probabilistic LFG grammar
approximations automatically acquired from the f-structure-annotated Penn-II tree-
bank, specifically in the resolution of LDDs, as described in Cahill, Burke, et al (2004).
We hope to be able to apply our lexical acquisition methodology beyond existing
parse-annotated corpora (Penn-II and Penn-III): New text is parsed by our probabilistic
LFG approximations into f-structures from which we can then extract further seman-
tic forms. The work reported here is part of the core components for bootstrapping
this approach.
In the shorter term, we intend to make the extracted subcategorization lexicons from
Penn-II and Penn-III available as a downloadable public-domain research resource.
We have also applied our more general unification grammar acquisition meth-
odology to the TIGER Treebank (Brants et al 2002) and Penn Chinese Treebank
(Xue, Chiou, and Palmer 2002), extracting wide-coverage, probabilistic LFG grammar
361
Computational Linguistics Volume 31, Number 3
approximations and lexical resources for German (Cahill et al 2003) and Chinese
(Burke, Lam, et al 2004). The lexical resources, however, have not yet been evaluated.
This, and much else, has to await further research.
Acknowledgments
The research reported here is partially
supported by Enterprise Ireland Basic
Research Grant SC/2001/186, an IRCSET
PhD fellowship award, and an IBM PhD
fellowship award. We are particularly
grateful to our anonymous reviewers, whose
insightful comments have helped to improve
this article considerably.
References
Ades, Anthony and Mark Steedman. 1982.
On the order of words. Linguistics and
Philosophy, 4(4):517? 558.
Boguraev, Branimir, Edward Briscoe,
John Carroll, David Carter, and
Claire Grover. 1987. The derivation of
a grammatically indexed lexicon from
the Longman Dictionary of Contemporary
English. In Proceedings of the 25th
Annual Meeting of the Association of
Computational Linguistics, pages 193?200,
Stanford, CA.
Brants, Sabine, Stefanie Dipper, Silvia
Hansen, Wolfgang Lezius, and George
Smith. 2002. The TIGER Treebank. In
Proceedings of the Workshop on Treebanks
and Linguistic Theories, Sozopol, Bulgaria.
Brent, Michael. 1993. From grammar to
lexicon: Unsupervised learning of lexical
syntax. Computational Linguistics,
19(2):203?222.
Bresnan, Joan. 2001. Lexical-Functional Syntax.
Blackwell, Oxford.
Briscoe, Edward. 2001. From dictionary to
corpus to self-organizing dictionary:
Learning valency associations in the face
of variation and change. In Proceedings of
Corpus Linguistics 2001, Lancaster, UK.
Briscoe, Edward and John Carroll. 1997.
Automatic extraction of subcategorization
from corpora. In Proceedings of the Fifth
ACL Conference on Applied Natural
Language Processing, pages 356?363,
Washington, DC.
Burke, Michael, Aoife Cahill, Ruth
O?Donovan, Josef van Genabith, and
Andy Way. 2004a. Evaluation of an
automatic annotation algorithm against
the PARC 700 Dependency Bank. In
Proceedings of the Ninth International
Conference on LFG, pages 101?121,
Christchurch, New Zealand.
Burke, Michael, Aoife Cahill, Ruth
O?Donovan, Josef van Genabith, and
Andy Way. 2004b. Treebank-based
acquisition of wide-coverage, probabilistic
LFG resources: Project overview, results
and evaluation. In Proceedings of the
Workshop ?Beyond Shallow Analyses?
Formalisms and Statistical Modelling
for Deep Analyses? at the First International
Joint Conference on Natural Language
Processing (IJCNLP-04), Hainan
Island, China.
Burke, Michael, Olivia Lam, Rowena
Chan, Aoife Cahill, Ruth O?Donovan,
Adams Bodomo, Josef van Genabith,
and Andy Way. 2004. Treebank-based
acquisition of a Chinese lexical-functional
grammar. In Proceedings of the 18th
Pacific Asia Conference on Language,
Information and Computation,
pages 161?172, Tokyo.
Cahill, Aoife, Michael Burke, Ruth
O?Donovan, Josef van Genabith, and Andy
Way. 2004. Long-distance dependency
resolution in automatically acquired
wide-coverage PCFG-based LFG
approximations. In Proceedings
of the 42nd Annual Meeting of the
Association of Computational Linguistics,
pages 320?327, Barcelona.
Cahill, Aoife, Martin Forst, Mairead
McCarthy, Ruth O?Donovan, Christian
Rohrer, Josef van Genabith, and Andy
Way. 2003. Treebank-based multilingual
unification-grammar development. In
Proceedings of the Workshop on Ideas and
Strategies for Multilingual Grammar
Development at the 15th ESS-LLI,
pages 17?24, Vienna.
Cahill, Aoife, Mairead McCarthy,
Michael Burke, Ruth O?Donovan,
Josef van Genabith, and Andy Way.
2004. Evaluating automatic F-structure
annotation for the Penn-II Treebank.
Journal of Research on Language and
Computation, 2(4):523?547.
Cahill, Aoife, Mairead McCarthy, Josef van
Genabith, and Andy Way. 2002. Parsing
text with a PCFG derived from Penn-II
with an automatic F-structure annotation
procedure. In Proceedings of the Seventh
International Conference on LFG, edited by
Miriam Butt and Tracy Holloway King.
CSLI Publications, Stanford, CA,
pages 76?95.
362
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Carroll, Glenn and Mats Rooth. 1998. Valence
induction with a head-lexicalised PCFG. In
Proceedings of the Third Conference on
Empirical Methods in Natural Language
Processing, pages 36?45,
Granada, Spain.
Charniak, Eugene. 1996. Tree-bank
grammars. In AAAI-96: Proceedings of the
Thirteenth National Conference on Artificial
Intelligence. MIT Press, Cambridge, MA,
pages 1031?1036.
Chen, John and K. Vijay-Shanker. 2000.
Automated extraction of TAGs from the
Penn Treebank. In Proceedings of the 38th
Annual Meeting of the Association of
Computational Linguistics, pages 65?76,
Hong Kong.
Collins, Michael. 1997. Three generative
lexicalised models for statistical parsing. In
Proceedings of the 35th Annual Meeting of the
Association for Computational Linguistics,
pages 16?23, Madrid.
Crouch, Richard, Ron Kaplan, Tracy King,
and Stefan Riezler. 2002. A comparison
of evaluation metrics for a broad coverage
parser. In Proceedings of Workshop
?Beyond PARSEVAL? at Third International
Conference on Language Resources and
Evaluation, Las Palmas, Spain.
Dalrymple, Mary. 2001. Lexical Functional
Grammar. Volume 34 of Syntax and
Semantics. Academic Press, New York.
Dowty, David. 1982. Grammatical relations
and Montague grammar. In Pauline
Jacobson and Geoffrey Pullum, editors,
The Nature of Syntactic Representation.
Reidel, Dordrecht, The Netherlands,
pages 79?130.
Dudenredaktion, editor. 2001. DUDEN?Das
Stilworterbuch. [DUDEN?The Style
Dictionary]. Number 2 in Duden in zwo?lf
Banden [Duden in Twelve Volumes].
Dudenverlag, Mannheim, Germany.
Eckle, Judith. 1999. Linguistic Knowledge for
Automatic Lexicon Acquisition from German
Text Corpora. Ph.D. thesis, University of
Stuttgart, Germany.
Frank, Anette. 2000. Automatic F-structure
annotation of treebank trees. In Proceedings
of the Fifth International Conference on LFG,
Berkeley, CA, edited by Miriam Butt
and Tracy Holloway King. CSLI,
pages 139?160.
Grishman, Ralph, Catherine MacLeod, and
Adam Meyers. 1994. COMLEX syntax:
Building a computational lexicon. In
Proceedings of the 15th International
Conference on Computational Linguistics,
pages 268?272, Kyoto.
Hajic, Jan. 1998. Building a syntactically
annotated corpus: The Prague
Dependency Treebank. In Issues in Valency
and Meaning, edited by Eva Hajicova.
Karolinum, Prague, Czech Republic,
pages 106?132.
Hindle, Donald and Mats Rooth. 1993.
Ambiguity and lexical relations.
Computational Linguistics, 19(1):103?120.
Hockenmaier, Julia, Gann Bierner, and Jason
Baldridge. 2004. Extending the coverage of
a CCG system. Journal of Language and
Computation, 2(2):165?208.
Hornby, Albert, editor. 1980. Oxford Advanced
Learner?s Dictionary of Current English.
Oxford University Press, Oxford, UK.
Joshi, Aravind. 1988. Tree adjoining
grammars. In David Dowty, Lauri
Karttunen, and Arnold Zwicky, editors,
Natural Language Parsing. Cambridge
University Press, Cambridge,
pages 206?250.
Kaplan, Ronald and Joan Bresnan. 1982.
Lexical functional grammar: A formal
system for grammatical representation. In
Joan Bresnan, editor, The Mental
Representation of Grammatical Relations. MIT
Press, Cambridge, MA, pages 173?281.
King, Tracy Holloway, Richard Crouch,
Stefan Riezler, Mary Dalrymple, and
Ronald Kaplan. 2003. The PARC 700
Dependency Bank. In Proceedings of the
Fourth International Workshop on
Linguistically Interpreted Corpora, Budapest.
Kingsbury, Paul and Martha Palmer. 2002.
From Treebank to PropBank. In Proceedings
of the Third International Conference on
Language Resources and Evaluation
(LREC-2002), Las Palmas, Spain.
Kinyon, Alexandra and Carlos Prolo. 2002.
Identifying verb arguments and their
syntactic function in the Penn Treebank. In
Proceedings of the Third LREC Conference,
pages 1982?1987, Las Palmas, Spain.
Korhonen, Anna. 2002. Subcategorization
acquisition. As Technical Report
UCAM-CL-TR-530, Computer Laboratory,
University of Cambridge, UK.
Krotov, Alexander, Mark Hepple, Robert
Gaizauskas, and Yorick Wilks. 1998.
Compacting the Penn Treebank grammar.
In Proceedings of the 36th Annual Meeting of
the Association for Computational Linguistics
and 17th International Conference on
Computational Linguistics, pages 669?703,
Montreal.
Levin, Beth. 1993. English Verb Classes and
Alternations. University of Chicago Press,
Chicago.
363
Computational Linguistics Volume 31, Number 3
MacLeod, Catherine, Ralph Grishman, and
Adam Meyers. 1994. The Comlex Syntax
Project: The first year. In Proceedings of the
ARPA Workshop on Human Language
Technology, pages 669?703, Princeton.
Magerman, David. 1994. Natural Language
Parsing as Statistical Pattern Recognition.
Ph.D. thesis, Stanford University,
Stanford, CA.
Magerman, David. 1995. Statistical decision
tree models for parsing. In Proceedings of
the 33rd Annual Meeting for the Association
of Computational Linguistics, pages 276?283,
Cambridge, MA.
Manning, Christopher. 1993. Automatic
acquisition of a large subcategorisation
dictionary from corpora. In Proceedings of
the 31st Annual Meeting of the Association for
Computational Linguistics, pages 235?242,
Columbus, OH.
Marcus, Mitchell, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Mark
Ferguson, Karen Katz, and Britta
Schasberger. 1994. The Penn Treebank:
Annotating predicate argument structure.
In Proceedings of the ARPA Human Language
Technology Workshop, Princeton.
Marinov, Svetoslav and Cecilia Hemming.
2004. Automatic Extraction of
Subcategorization Frames from the
Bulgarian Tree Bank. Unpublished
manuscript, Graduate School of Language
Technology, Go?teborg, Sweden.
Meyers, Adam, Catherine MacLeod, and
Ralph Grishman. 1996. Standardization of
the complement/adjunct distinction.
In Proceedings of the Seventh
EURALEX International Conference,
Go?teborg, Sweden.
Miyao, Yusuke, Takashi Ninomiya, and
Jun?ichi Tsujii. 2004. Corpus-oriented
grammar development for acquiring a
head-driven phrase structure grammar
from the Penn Treebank. In Proceedings of
the First International Joint Conference on
Natural Language Processing (IJCNLP-04),
pages 390?398, Hainan Island, China.
Nakanishi, Hiroko, Yusuke Miyao, and
Jun?ichi Tsujii. 2004. Using inverse
lexical rules to acquire a wide-coverage
lexicalized grammar. In Proceedings
of the Workshop ?Beyond Shallow
Analyses?Formalisms and Statistical
Modelling for Deep Analyses? at the First
International Joint Conference on Natural
Language Processing (IJCNLP-04), Hainan
Island, China.
O?Donovan, Ruth, Michael Burke,
Aoife Cahill, Josef van Genabith, and
Andy Way. 2004. Large-scale induction
and evaluation of lexical resources from
the Penn-II Treebank. In Proceedings
of the 42nd Annual Meeting of the
Association of Computational Linguistics,
pages 368?375, Barcelona.
Pollard, Carl and Ivan Sag. 1994.
Head-Driven Phrase Structure Grammar.
University of Chicago Press, Chicago.
Proctor, Paul, editor. 1978. Longman
Dictionary of Contemporary English.
Longman, London.
Roland, Douglas and Daniel Jurafsky.
1998. How verb subcategorization
frequencies are affected by corpus
choice. In Proceedings of the 36th
Annual Meeting of the Association
for Computational Linguistics and
17th International Conference on
Computational Linguistics,
pages 1117?1121, Montreal.
Sadler, Louisa, Josef van Genabith,
and Andy Way. 2000. Automatic
F-structure annotation from the
AP Treebank. In Proceedings of the
Fifth International Conference on LFG,
Berkeley, CA, edited by Miriam
Butt and Tracy Holloway King. CSLI,
pages 226?243.
Sarkar, Anoop and Daniel Zeman. 2000.
Automatic extraction of subcategorization
frames for Czech. In Proceedings of the 19th
International Conference on Computational
Linguistics, pages 691?697, Saarbru?cken,
Germany.
Schulte im Walde, Sabine. 2002a. A
subcategorisation lexicon for German
verbs induced from a lexicalised PCFG. In
Proceedings of the Third LREC Conference,
pages 1351?1357, Las Palmas, Spain.
Schulte im Walde, Sabine. 2002b. Evaluating
verb subcategorisation frames learned by a
German statistical grammar against
manual definitions in the Duden
Dictionary. In Proceedings of the 10th
EURALEX International Congress,
pages 187?197, Copenhagen.
Simov, Kiril, Gergana Popova, and Petya
Osenova. 2002. HPSG-based syntactic
treebank of Bulgarian (BulTreeBank). In
Andrew Wilson, Paul Rayson, and Tony
McEnery, editors, A Rainbow of Corpora:
Corpus Linguistics and the Languages of the
World. Lincon-Europa, Munich,
pages 135?142.
Ushioda, Akira, David Evans, Ted Gibson,
and Alex Waibel. 1993. The Automatic
acquisition of frequencies of verb
subcategorization frames from tagged
364
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
corpora. In SIGLEX ACL Workshop on the
Acquisition of Lexical Knowledge from Text,
pages 95?106, Columbus, OH.
van Genabith, Josef, Louisa Sadler, and
Andy Way. 1999. Data-driven compilation
of LFG semantic forms. In EACL-99
Workshop on Linguistically Interpreted
Corpora (LINC-99), pages 69?76, Bergen,
Norway.
van Genabith, Josef, Andy Way, and Louisa
Sadler. 1999. Semi-automatic generation of
F-structures from Treebanks. In
Proceedings of the Fourth International
Conference on Lexical-Functional Grammar,
Manchester, UK. Available at
http://cslipublications.stanford.edu/.
Wauschkuhn, Oliver. 1999. Automatische
Extraktion von Verbvalenzen aus deutschen
Textkorpora [Automatic Extraction of Verb
Valence from German Text Corpora]. PhD
thesis, University of Stuttgart, Germany.
Xia, Fei. 1999. Extracting tree adjoining
grammars from bracketed corpora.
In Fifth Natural Language Processing
Pacific Rim Symposium (NLPRS-99),
Beijing, China.
Xue, Nianwen, Fu-Dong Chiou, and Martha
Palmer. 2002. Building a large-scale
annotated Chinese corpus. In Proceedings
of the 19th International Conference on
Computational Linguistics (COLING 2002),
Taipei, Taiwan.
365

Long-Distance Dependency Resolution in Automatically Acquired
Wide-Coverage PCFG-Based LFG Approximations
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef van Genabith, Andy Way
National Centre for Language Technology and School of Computing,
Dublin City University, Dublin, Ireland
{acahill,mburke,rodonovan,josef,away}@computing.dcu.ie
Abstract
This paper shows how finite approximations of
long distance dependency (LDD) resolution can be
obtained automatically for wide-coverage, robust,
probabilistic Lexical-Functional Grammar (LFG)
resources acquired from treebanks. We extract LFG
subcategorisation frames and paths linking LDD
reentrancies from f-structures generated automati-
cally for the Penn-II treebank trees and use them
in an LDD resolution algorithm to parse new text.
Unlike (Collins, 1999; Johnson, 2002), in our ap-
proach resolution of LDDs is done at f-structure
(attribute-value structure representations of basic
predicate-argument or dependency structure) with-
out empty productions, traces and coindexation in
CFG parse trees. Currently our best automatically
induced grammars achieve 80.97% f-score for f-
structures parsing section 23 of the WSJ part of the
Penn-II treebank and evaluating against the DCU
1051 and 80.24% against the PARC 700 Depen-
dency Bank (King et al, 2003), performing at the
same or a slightly better level than state-of-the-art
hand-crafted grammars (Kaplan et al, 2004).
1 Introduction
The determination of syntactic structure is an im-
portant step in natural language processing as syn-
tactic structure strongly determines semantic inter-
pretation in the form of predicate-argument struc-
ture, dependency relations or logical form. For a
substantial number of linguistic phenomena such
as topicalisation, wh-movement in relative clauses
and interrogative sentences, however, there is an im-
portant difference between the location of the (sur-
face) realisation of linguistic material and the loca-
tion where this material should be interpreted se-
mantically. Resolution of such long-distance de-
pendencies (LDDs) is therefore crucial in the de-
termination of accurate predicate-argument struc-
1Manually constructed f-structures for 105 randomly se-
lected trees from Section 23 of the WSJ section of the Penn-II
Treebank
ture, deep dependency relations and the construc-
tion of proper meaning representations such as log-
ical forms (Johnson, 2002).
Modern unification/constraint-based grammars
such as LFG or HPSG capture deep linguistic infor-
mation including LDDs, predicate-argument struc-
ture, or logical form. Manually scaling rich uni-
fication grammars to naturally occurring free text,
however, is extremely time-consuming, expensive
and requires considerable linguistic and computa-
tional expertise. Few hand-crafted, deep unification
grammars have in fact achieved the coverage and
robustness required to parse a corpus of say the size
and complexity of the Penn treebank: (Riezler et
al., 2002) show how a deep, carefully hand-crafted
LFG is successfully scaled to parse the Penn-II tree-
bank (Marcus et al, 1994) with discriminative (log-
linear) parameter estimation techniques.
The last 20 years have seen continuously increas-
ing efforts in the construction of parse-annotated
corpora. Substantial treebanks2 are now available
for many languages (including English, Japanese,
Chinese, German, French, Czech, Turkish), others
are currently under construction (Arabic, Bulgarian)
or near completion (Spanish, Catalan). Treebanks
have been enormously influential in the develop-
ment of robust, state-of-the-art parsing technology:
grammars (or grammatical information) automat-
ically extracted from treebank resources provide
the backbone of many state-of-the-art probabilis-
tic parsing approaches (Charniak, 1996; Collins,
1999; Charniak, 1999; Hockenmaier, 2003; Klein
and Manning, 2003). Such approaches are attrac-
tive as they achieve robustness, coverage and per-
formance while incurring very low grammar devel-
opment cost. However, with few notable exceptions
(e.g. Collins? Model 3, (Johnson, 2002), (Hocken-
maier, 2003) ), treebank-based probabilistic parsers
return fairly simple ?surfacey? CFG trees, with-
out deep syntactic or semantic information. The
grammars used by such systems are sometimes re-
2Or dependency banks.
ferred to as ?half? (or ?shallow?) grammars (John-
son, 2002), i.e. they do not resolve LDDs but inter-
pret linguistic material purely locally where it oc-
curs in the tree.
Recently (Cahill et al, 2002) showed how
wide-coverage, probabilistic unification grammar
resources can be acquired automatically from f-
structure-annotated treebanks. Many second gen-
eration treebanks provide a certain amount of
deep syntactic or dependency information (e.g. in
the form of Penn-II functional tags and traces)
supporting the computation of representations of
deep linguistic information. Exploiting this in-
formation (Cahill et al, 2002) implement an
automatic LFG f-structure annotation algorithm
that associates nodes in treebank trees with f-
structure annotations in the form of attribute-value
structure equations representing abstract predicate-
argument structure/dependency relations. From the
f-structure annotated treebank they automatically
extract wide-coverage, robust, PCFG-based LFG
approximations that parse new text into trees and
f-structure representations.
The LFG approximations of (Cahill et al, 2002),
however, are only ?half? grammars, i.e. like most
of their probabilistic CFG cousins (Charniak, 1996;
Johnson, 1999; Klein and Manning, 2003) they do
not resolve LDDs but interpret linguistic material
purely locally where it occurs in the tree.
In this paper we show how finite approxima-
tions of long distance dependency resolution can be
obtained automatically for wide-coverage, robust,
probabilistic LFG resources automatically acquired
from treebanks. We extract LFG subcategorisation
frames and paths linking LDD reentrancies from
f-structures generated automatically for the Penn-
II treebank trees and use them in an LDD resolu-
tion algorithm to parse new text. Unlike (Collins,
1999; Johnson, 2002), in our approach LDDs are
resolved on the level of f-structure representation,
rather than in terms of empty productions and co-
indexation on parse trees. Currently we achieve f-
structure/dependency f-scores of 80.24 and 80.97
for parsing section 23 of the WSJ part of the Penn-
II treebank, evaluating against the PARC 700 and
DCU 105 respectively.
The paper is structured as follows: we give a
brief introduction to LFG. We outline the automatic
f-structure annotation algorithm, PCFG-based LFG
grammar approximations and parsing architectures
of (Cahill et al, 2002). We present our subcategori-
sation frame extraction and introduce the treebank-
based acquisition of finite approximations of LFG
functional uncertainty equations in terms of LDD
paths. We present the f-structure LDD resolution
algorithm, provide results and extensive evaluation.
We compare our method with previous work. Fi-
nally, we conclude.
2 Lexical Functional Grammar (LFG)
Lexical-Functional Grammar (Kaplan and Bres-
nan, 1982; Dalrymple, 2001) minimally involves
two levels of syntactic representation:3 c-structure
and f-structure. C(onstituent)-structure represents
the grouping of words and phrases into larger
constituents and is realised in terms of a CF-
PSG grammar. F(unctional)-structure represents
abstract syntactic functions such as SUBJ(ect),
OBJ(ect), OBL(ique), closed and open clausal
COMP/XCOMP(lement), ADJ(unct), APP(osition)
etc. and is implemented in terms of recursive feature
structures (attribute-value matrices). C-structure
captures surface grammatical configurations, f-
structure encodes abstract syntactic information
approximating to predicate-argument/dependency
structure or simple logical form (van Genabith
and Crouch, 1996). C- and f-structures are re-
lated in terms of functional annotations (constraints,
attribute-value equations) on c-structure rules (cf.
Figure 1).
S
NP VP
U.N. V NP
signs treaty
[
SUBJ
[
PRED U.N.
]
PRED sign
OBJ
[
PRED treaty
]
]
S ? NP VP
?SUBJ=? ?=?
VP ? V NP
?=? ?OBJ=?
NP ? U.N V ? signs
?PRED=U.N. ?PRED=sign
Figure 1: Simple LFG C- and F-Structure
Uparrows point to the f-structure associated with the
mother node, downarrows to that of the local node.
The equations are collected with arrows instanti-
ated to unique tree node identifiers, and a constraint
solver generates an f-structure.
3 Automatic F-Structure Annotation
The Penn-II treebank employs CFG trees with addi-
tional ?functional? node annotations (such as -LOC,
-TMP, -SBJ, -LGS, . . . ) as well as traces and coin-
dexation (to indicate LDDs) as basic data structures.
The f-structure annotation algorithm of (Cahill et
3LFGs may also involve morphological and semantic levels
of representation.
al., 2002) exploits configurational, categorial, Penn-
II ?functional?, local head and trace information
to annotate nodes with LFG feature-structure equa-
tions. A slightly adapted version of (Magerman,
1994)?s scheme automatically head-lexicalises the
Penn-II trees. This partitions local subtrees of depth
one (corresponding to CFG rules) into left and right
contexts (relative to head). The annotation algo-
rithm is modular with four components (Figure 2):
left-right (L-R) annotation principles (e.g. leftmost
NP to right of V head of VP type rule is likely to be
an object etc.); coordination annotation principles
(separating these out simplifies other components
of the algorithm); traces (translates traces and coin-
dexation in trees into corresponding reentrancies in
f-structure ( 1 in Figure 3)); catch all and clean-up.
Lexical information is provided via macros for POS
tag classes.
L/R Context ? Coordination ? Traces ? Catch-All
Figure 2: Annotation Algorithm
The f-structure annotations are passed to a con-
straint solver to produce f-structures. Annotation
is evaluated in terms of coverage and quality, sum-
marised in Table 1. Coverage is near complete with
99.82% of the 48K Penn-II sentences receiving a
single, connected f-structure. Annotation quality is
measured in terms of precision and recall (P&R)
against the DCU 105. The algorithm achieves an
F-score of 96.57% for full f-structures and 94.3%
for preds-only f-structures.4
S
S-TPC- 1
NP
U.N.
VP
V
signs
NP
treaty
NP
Det
the
N
headline
VP
V
said
S
T- 1
?
?
?
?
?
?
?
TOPIC
[
SUBJ
[
PRED U.N.
]
PRED sign
OBJ
[
PRED treaty
]
]
1
SUBJ
[
SPEC the
PRED headline
]
PRED say
COMP 1
?
?
?
?
?
?
?
Figure 3: Penn-II style tree with LDD trace and cor-
responding reentrancy in f-structure
4Full f-structures measure all attribute-value pairs includ-
ing?minor? features such as person, number etc. The stricter
preds-only captures only paths ending in PRED:VALUE.
# frags # sent percent
0 85 0.176
1 48337 99.820
2 2 0.004
all preds
P 96.52 94.45
R 96.63 94.16
Table 1: F-structure annotation results for DCU 105
4 PCFG-Based LFG Approximations
Based on these resources (Cahill et al, 2002) de-
veloped two parsing architectures. Both generate
PCFG-based approximations of LFG grammars.
In the pipeline architecture a standard PCFG is
extracted from the ?raw? treebank to parse unseen
text. The resulting parse-trees are then annotated by
the automatic f-structure annotation algorithm and
resolved into f-structures.
In the integrated architecture the treebank
is first annotated with f-structure equations.
An annotated PCFG is then extracted where
each non-terminal symbol in the grammar
has been augmented with LFG f-equations:
NP[?OBJ=?] ? DT[?SPEC=?] NN[?=?] . Nodes
followed by annotations are treated as a monadic
category for grammar extraction and parsing.
Post-parsing, equations are collected from parse
trees and resolved into f-structures.
Both architectures parse raw text into ?proto? f-
structures with LDDs unresolved resulting in in-
complete argument structures as in Figure 4.
S
S
NP
U.N.
VP
V
signs
NP
treaty
NP
Det
the
N
headline
VP
V
said
?
?
?
?
?
TOPIC
[
SUBJ
[
PRED U.N.
]
PRED sign
OBJ
[
PRED treaty
]
]
SUBJ
[
SPEC the
PRED headline
]
PRED say
?
?
?
?
?
Figure 4: Shallow-Parser Output with Unresolved
LDD and Incomplete Argument Structure (cf. Fig-
ure 3)
5 LDDs and LFG FU-Equations
Theoretically, LDDs can span unbounded amounts
of intervening linguistic material as in
[U.N. signs treaty]1 the paper claimed . . . a source said []1.
In LFG, LDDs are resolved at the f-structure level,
obviating the need for empty productions and traces
in trees (Dalrymple, 2001), using functional uncer-
tainty (FU) equations. FUs are regular expressions
specifying paths in f-structure between a source
(where linguistic material is encountered) and a tar-
get (where linguistic material is interpreted seman-
tically). To account for the fronted sentential con-
stituents in Figures 3 and 4, an FU equation of the
form ? TOPIC = ? COMP* COMP would be required.
The equation states that the value of the TOPIC at-
tribute is token identical with the value of the final
COMP argument along a path through the immedi-
ately enclosing f-structure along zero or more COMP
attributes. This FU equation is annotated to the top-
icalised sentential constituent in the relevant CFG
rules as follows
S ? S NP VP
?TOPIC=? ?SUBJ=? ?=?
?TOPIC=?COMP*COMP
and generates the LDD-resolved proper f-structure
in Figure 3 for the traceless tree in Figure 4, as re-
quired.
In addition to FU equations, subcategorisation in-
formation is a crucial ingredient in LFG?s account
of LDDs. As an example, for a topicalised con-
stituent to be resolved as the argument of a local
predicate as specified by the FU equation, the local
predicate must (i) subcategorise for the argument in
question and (ii) the argument in question must not
be already filled. Subcategorisation requirements
are provided lexically in terms of semantic forms
(subcat lists) and coherence and completeness con-
ditions (all GFs specified must be present, and no
others may be present) on f-structure representa-
tions. Semantic forms specify which grammatical
functions (GFs) a predicate requires locally. For our
example in Figures 3 and 4, the relevant lexical en-
tries are:
V ? said ?PRED=say?? SUBJ, ? COMP?
V ? signs ?PRED=sign?? SUBJ, ? OBJ?
FU equations and subcategorisation requirements
together ensure that LDDs can only be resolved at
suitable f-structure locations.
6 Acquiring Lexical and LDD Resources
In order to model the LFG account of LDD resolu-
tion we require subcat frames (i.e. semantic forms)
and LDD resolution paths through f-structure. Tra-
ditionally, such resources were handcoded. Here we
show how they can be acquired from f-structure an-
notated treebank resources.
LFG distinguishes between governable (argu-
ments) and nongovernable (adjuncts) grammati-
cal functions (GFs). If the automatic f-structure
annotation algorithm outlined in Section 3 gen-
erates high quality f-structures, reliable seman-
tic forms can be extracted (reverse-engineered):
for each f-structure generated, for each level of
embedding we determine the local PRED value
and collect the governable, i.e. subcategoris-
able grammatical functions present at that level
of embedding. For the proper f-structure in
Figure 3 we obtain sign([subj,obj]) and
say([subj,comp]). We extract frames from
the full WSJ section of the Penn-II Treebank with
48K trees. Unlike many other approaches, our ex-
traction process does not predefine frames, fully
reflects LDDs in the source data-structures (cf.
Figure 3), discriminates between active and pas-
sive frames, computes GF, GF:CFG category pair-
as well as CFG category-based subcategorisation
frames and associates conditional probabilities with
frames. Given a lemma l and an argument list s, the
probability of s given l is estimated as:
P(s|l) := count(l, s)?n
i=1 count(l, si)
Table 2 summarises the results. We extract 3586
verb lemmas and 10969 unique verbal semantic
form types (lemma followed by non-empty argu-
ment list). Including prepositions associated with
the subcategorised OBLs and particles, this number
goes up to 14348. The number of unique frame
types (without lemma) is 38 without specific prepo-
sitions and particles, 577 with. F-structure anno-
tations allow us to distinguish passive and active
frames. Table 3 shows the most frequent seman-
tic forms for accept. Passive frames are marked
p. We carried out a comprehensive evaluation of
the automatically acquired verbal semantic forms
against the COMLEX Resource (Macleod et al,
1994) for the 2992 active verb lemmas that both re-
sources have in common. We report on the evalu-
ation of GF-based frames for the full frames with
complete prepositional and particle infomation. We
use relative conditional probability thresholds (1%
and 5%) to filter the selection of semantic forms
(Table 4). (O?Donovan et al, 2004) provide a more
detailed description of the extraction and evaluation
of semantic forms.
Without Prep/Part With Prep/Part
Lemmas 3586 3586
Sem. Forms 10969 14348
Frame Types 38 577
Active Frame Types 38 548
Passive Frame Types 21 177
Table 2: Verb Results
Semantic Form Occurrences Prob.
accept([obj,subj]) 122 0.813
accept([subj],p) 9 0.060
accept([comp,subj]) 5 0.033
accept([subj,obl:as],p) 3 0.020
accept([obj,subj,obl:as]) 3 0.020
accept([obj,subj,obl:from]) 3 0.020
accept([subj]) 2 0.013
accept([obj,subj,obl:at]) 1 0.007
accept([obj,subj,obl:for]) 1 0.007
accept([obj,subj,xcomp]) 1 0.007
Table 3: Semantic forms for the verb accept.
Threshold 1% Threshold 5%
P R F-Score P R F-Score
Exp. 73.7% 22.1% 34.0% 78.0% 18.3% 29.6%
Table 4: COMLEX Comparison
We further acquire finite approximations of FU-
equations. by extracting paths between co-indexed
material occurring in the automatically generated f-
structures from sections 02-21 of the Penn-II tree-
bank. We extract 26 unique TOPIC, 60 TOPIC-REL
and 13 FOCUS path types (with a total of 14,911 to-
ken occurrences), each with an associated probabil-
ity. We distinguish between two types of TOPIC-
REL paths, those that occur in wh-less constructions,
and all other types (c.f Table 5). Given a path p and
an LDD type t (either TOPIC, TOPIC-REL or FO-
CUS), the probability of p given t is estimated as:
P(p|t) := count(t, p)?n
i=1 count(t, pi)
In order to get a first measure of how well the ap-
proximation models the data, we compute the path
types in section 23 not covered by those extracted
from 02-21: 23/(02-21). There are 3 such path types
(Table 6), each occuring exactly once. Given that
the total number of path tokens in section 23 is 949,
the finite approximation extracted from 02-23 cov-
ers 99.69% of all LDD paths in section 23.
7 Resolving LDDs in F-Structure
Given a set of semantic forms s with probabilities
P(s|l) (where l is a lemma), a set of paths p with
P(p|t) (where t is either TOPIC, TOPIC-REL or FO-
CUS) and an f-structure f , the core of the algorithm
to resolve LDDs recursively traverses f to:
find TOPIC|TOPIC-REL|FOCUS:g pair; retrieve
TOPIC|TOPIC-REL|FOCUS paths; for each path p
with GF1 : . . . : GFn : GF, traverse f along GF1 : . . . :
GFn to sub-f-structure h; retrieve local PRED:l;
add GF:g to h iff
? GF is not present at h
wh-less TOPIC-REL # wh-less TOPIC-REL #
subj 5692 adjunct 1314
xcomp:adjunct 610 obj 364
xcomp:obj 291 xcomp:xcomp:adjunct 96
comp:subj 76 xcomp:subj 67
Table 5: Most frequent wh-less TOPIC-REL paths
02?21 23 23 /(02?21)
TOPIC 26 7 2
FOCUS 13 4 0
TOPIC-REL 60 22 1
Table 6: Number of path types extracted
? h together with GF is locally complete and co-
herent with respect to a semantic form s for l
rank resolution by P(s|l) ? P(p|t)
The algorithm supports multiple, interacting TOPIC,
TOPIC-REL and FOCUS LDDs. We use P(s|l) ?
P(p|t) to rank a solution, depending on how likely
the PRED takes semantic frame s, and how likely
the TOPIC, FOCUS or TOPIC-REL is resolved using
path p. The algorithm also supports resolution of
LDDs where no overt linguistic material introduces
a source TOPIC-REL function (e.g. in reduced rela-
tive clause constructions). We distinguish between
passive and active constructions, using the relevant
semantic frame type when resolving LDDs.
8 Experiments and Evaluation
We ran experiments with grammars in both the
pipeline and the integrated parsing architectures.
The first grammar is a basic PCFG, while A-PCFG
includes the f-structure annotations. We apply a
parent transformation to each grammar (Johnson,
1999) to give P-PCFG and PA-PCFG. We train
on sections 02-21 (grammar, lexical extraction and
LDD paths) of the Penn-II Treebank and test on sec-
tion 23. The only pre-processing of the trees that we
do is to remove empty nodes, and remove all Penn-
II functional tags in the integrated model. We evalu-
ate the parse trees using evalb. Following (Riezler et
al., 2002), we convert f-structures into dependency
triple format. Using their software we evaluate the
f-structure parser output against:
1. The DCU 105 (Cahill et al, 2002)
2. The full 2,416 f-structures automatically gen-
erated by the f-structure annotation algorithm
for the original Penn-II trees, in a CCG-style
(Hockenmaier, 2003) evaluation experiment
Pipeline Integrated
PCFG P-PCFG A-PCFG PA-PCFG
2416 Section 23 trees
# Parses 2416 2416 2416 2414
Lab. F-Score 75.83 80.80 79.17 81.32
Unlab. F-Score 78.28 82.70 81.49 83.28
DCU 105 F-Strs
All GFs F-Score (before LDD resolution) 79.82 79.24 81.12 81.20
All GFs F-Score (after LDD resolution) 83.79 84.59 86.30 87.04
Preds only F-Score (before LDD resolution) 70.00 71.57 73.45 74.61
Preds only F-Score (after LDD resolution) 73.78 77.43 78.76 80.97
2416 F-Strs
All GFs F-Score (before LDD resolution) 81.98 81.49 83.32 82.78
All GFs F-Score (after LDD resolution) 84.16 84.37 86.45 86.00
Preds only F-Score (before LDD resolution) 72.00 73.23 75.22 75.10
Preds only F-Score (after LDD resolution) 74.07 76.12 78.36 78.40
PARC 700 Dependency Bank
Subset of GFs following (Kaplan et al, 2004) 77.86 80.24 77.68 78.60
Table 7: Parser Evaluation
3. A subset of 560 dependency structures of the
PARC 700 Dependency Bank following (Ka-
plan et al, 2004)
The results are given in Table 7. The parent-
transformed grammars perform best in both archi-
tectures. In all cases, there is a marked improve-
ment (2.07-6.36%) in the f-structures after LDD res-
olution. We achieve between 73.78% and 80.97%
preds-only and 83.79% to 87.04% all GFs f-score,
depending on gold-standard. We achieve between
77.68% and 80.24% against the PARC 700 follow-
ing the experiments in (Kaplan et al, 2004). For
details on how we map the f-structures produced
by our parsers to a format similar to that of the
PARC 700 Dependency Bank, see (Burke et al,
2004). Table 8 shows the evaluation result broken
down by individual GF (preds-only) for the inte-
grated model PA-PCFG against the DCU 105. In
order to measure how many of the LDD reentran-
cies in the gold-standard f-structures are captured
correctly by our parsers, we developed evaluation
software for f-structure LDD reentrancies (similar
to Johnson?s (2002) evaluation to capture traces and
their antecedents in trees). Table 9 shows the results
with the integrated model achieving more than 76%
correct LDD reentrancies.
9 Related Work
(Collins, 1999)?s Model 3 is limited to wh-traces
in relative clauses (it doesn?t treat topicalisation,
focus etc.). Johnson?s (2002) work is closest to
ours in spirit. Like our approach he provides a fi-
nite approximation of LDDs. Unlike our approach,
however, he works with tree fragments in a post-
processing approach to add empty nodes and their
DEP. PRECISION RECALL F-SCORE
adjunct 717/903 = 79 717/947 = 76 78
app 14/15 = 93 14/19 = 74 82
comp 35/43 = 81 35/65 = 54 65
coord 109/143 = 76 109/161 = 68 72
det 253/264 = 96 253/269 = 94 95
focus 1/1 = 100 1/1 = 100 100
obj 387/445 = 87 387/461 = 84 85
obj2 0/1 = 0 0/2 = 0 0
obl 27/56 = 48 27/61 = 44 46
obl2 1/3 = 33 1/2 = 50 40
obl ag 5/11 = 45 5/12 = 42 43
poss 69/73 = 95 69/81 = 85 90
quant 40/55 = 73 40/52 = 77 75
relmod 26/38 = 68 26/50 = 52 59
subj 330/361 = 91 330/414 = 80 85
topic 12/12 = 100 12/13 = 92 96
topicrel 35/42 = 83 35/52 = 67 74
xcomp 139/160 = 87 139/146 = 95 91
OVERALL 83.78 78.35 80.97
Table 8: Preds-only results of PA-PCFG against the
DCU 105
antecedents to parse trees, while we present an ap-
proach to LDD resolution on the level of f-structure.
It seems that the f-structure-based approach is more
abstract (99 LDD path types against approximately
9,000 tree-fragment types in (Johnson, 2002)) and
fine-grained in its use of lexical information (sub-
cat frames). In contrast to Johnson?s approach, our
LDD resolution algorithm is not biased. It com-
putes all possible complete resolutions and order-
ranks them using LDD path and subcat frame prob-
abilities. It is difficult to provide a satisfactory com-
parison between the two methods, but we have car-
ried out an experiment that compares them at the
f-structure level. We take the output of Charniak?s
Pipeline Integrated
PCFG P-PCFG A-PCFG PA-PCFG
TOPIC
Precision (11/14) (12/13) (12/13) (12/12)
Recall (11/13) (12/13) (12/13) (12/13)
F-Score 0.81 0.92 0.92 0.96
FOCUS
Precision (0/1) (0/1) (0/1) (0/1)
Recall (0/1) (0/1) (0/1) (0/1)
F-Score 0 0 0 0
TOPIC-REL
Precision (20/34) (27/36) (34/42) (34/42)
Recall (20/52) (27/52) (34/52) (34/52)
F-Score 0.47 0.613 0.72 0.72
OVERALL 0.54 0.67 0.75 0.76
Table 9: LDD Evaluation on the DCU 105
Charniak -LDD res. +LDD res. (Johnson, 2002)
All GFs 80.86 86.65 85.16
Preds Only 74.63 80.97 79.75
Table 10: Comparison at f-structure level of LDD
resolution to (Johnson, 2002) on the DCU 105
parser (Charniak, 1999) and, using the pipeline
f-structure annotation model, evaluate against the
DCU 105, both before and after LDD resolution.
Using the software described in (Johnson, 2002) we
add empty nodes to the output of Charniak?s parser,
pass these trees to our automatic annotation algo-
rithm and evaluate against the DCU 105. The re-
sults are given in Table 10. Our method of resolv-
ing LDDs at f-structure level results in a preds-only
f-score of 80.97%. Using (Johnson, 2002)?s method
of adding empty nodes to the parse-trees results in
an f-score of 79.75%.
(Hockenmaier, 2003) provides CCG-based mod-
els of LDDs. Some of these involve extensive clean-
up of the underlying Penn-II treebank resource prior
to grammar extraction. In contrast, in our approach
we leave the treebank as is and only add (but never
correct) annotations. Earlier HPSG work (Tateisi
et al, 1998) is based on independently constructed
hand-crafted XTAG resources. In contrast, we ac-
quire our resources from treebanks and achieve sub-
stantially wider coverage.
Our approach provides wide-coverage, robust,
and ? with the addition of LDD resolution ? ?deep?
or ?full?, PCFG-based LFG approximations. Cru-
cially, we do not claim to provide fully adequate sta-
tistical models. It is well known (Abney, 1997) that
PCFG-type approximations to unification grammars
can yield inconsistent probability models due to
loss of probability mass: the parser successfully re-
turns the highest ranked parse tree but the constraint
solver cannot resolve the f-equations (generated in
the pipeline or ?hidden? in the integrated model)
and the probability mass associated with that tree is
lost. This case, however, is surprisingly rare for our
grammars: only 0.0018% (85 out of 48424) of the
original Penn-II trees (without FRAGs) fail to pro-
duce an f-structure due to inconsistent annotations
(Table 1), and for parsing section 23 with the in-
tegrated model (A-PCFG), only 9 sentences do not
receive a parse because no f-structure can be gen-
erated for the highest ranked tree (0.4%). Parsing
with the pipeline model, all sentences receive one
complete f-structure. Research on adequate prob-
ability models for unification grammars is impor-
tant. (Miyao et al, 2003) present a Penn-II tree-
bank based HPSG with log-linear probability mod-
els. They achieve coverage of 50.2% on section
23, as against 99% in our approach. (Riezler et
al., 2002; Kaplan et al, 2004) describe how a care-
fully hand-crafted LFG is scaled to the full Penn-II
treebank with log-linear based probability models.
They achieve 79% coverage (full parse) and 21%
fragement/skimmed parses. By the same measure,
full parse coverage is around 99% for our automat-
ically acquired PCFG-based LFG approximations.
Against the PARC 700, the hand-crafted LFG gram-
mar reported in (Kaplan et al, 2004) achieves an f-
score of 79.6%. For the same experiment, our best
automatically-induced grammar achieves an f-score
of 80.24%.
10 Conclusions
We presented and extensively evaluated a finite
approximation of LDD resolution in automati-
cally constructed, wide-coverage, robust, PCFG-
based LFG approximations, effectively turning the
?half?(or ?shallow?)-grammars presented in (Cahill
et al, 2002) into ?full? or ?deep? grammars. In
our approach, LDDs are resolved in f-structure, not
trees. The method achieves a preds-only f-score
of 80.97% for f-structures with the PA-PCFG in
the integrated architecture against the DCU 105
and 78.4% against the 2,416 automatically gener-
ated f-structures for the original Penn-II treebank
trees. Evaluating against the PARC 700 Depen-
dency Bank, the P-PCFG achieves an f-score of
80.24%, an overall improvement of approximately
0.6% on the result reported for the best hand-crafted
grammars in (Kaplan et al, 2004).
Acknowledgements
This research was funded by Enterprise Ireland Ba-
sic Research Grant SC/2001/186 and IRCSET.
References
S. Abney. 1997. Stochastic attribute-value gram-
mars. Computational Linguistics, 23(4):597?
618.
M. Burke, A. Cahill, R. O?Donovan, J. van Gen-
abith, and A. Way 2004. The Evaluation of
an Automatic Annotation Algorithm against the
PARC 700 Dependency Bank. In Proceedings
of the Ninth International Conference on LFG,
Christchurch, New Zealand (to appear).
A. Cahill, M. McCarthy, J. van Genabith, and A.
Way. 2002. Parsing with PCFGs and Auto-
matic F-Structure Annotation. In Miriam Butt
and Tracy Holloway King, editors, Proceedings
of the Seventh International Conference on LFG,
pages 76?95. CSLI Publications, Stanford, CA.
E. Charniak. 1996. Tree-Bank Grammars. In
AAAI/IAAI, Vol. 2, pages 1031?1036.
E. Charniak. 1999. A Maximum-Entropy-Inspired
Parser. Technical Report CS-99-12, Brown Uni-
versity, Providence, RI.
M. Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Uni-
versity of Pennsylvania, Philadelphia, PA.
M. Dalrymple. 2001. Lexical-Functional Gram-
mar. San Diego, CA; London Academic Press.
J. Hockenmaier. 2003. Parsing with Generative
models of Predicate-Argument Structure. In Pro-
ceedings of the 41st Annual Conference of the
Association for Computational Linguistics, pages
359?366, Sapporo, Japan.
M. Johnson. 1999. PCFG models of linguistic
tree representations. Computational Linguistics,
24(4):613?632.
M. Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their
antecedents. In Proceedings of the 40th Annual
Meeting of the Association for Computational
Linguistics, pages 136?143, Philadelphia, PA.
R. Kaplan and J. Bresnan. 1982. Lexical Func-
tional Grammar, a Formal System for Grammat-
ical Representation. In The Mental Representa-
tion of Grammatical Relations, pages 173?281.
MIT Press, Cambridge, MA.
R. Kaplan, S. Riezler, T. H. King, J. T. Maxwell,
A. Vasserman, and R. Crouch. 2004. Speed and
accuracy in shallow and deep stochastic parsing.
In Proceedings of the Human Language Tech-
nology Conference and the 4th Annual Meeting
of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 97?
104, Boston, MA.
T.H. King, R. Crouch, S. Riezler, M. Dalrymple,
and R. Kaplan. 2003. The PARC700 dependency
bank. In Proceedings of the EACL03: 4th Inter-
national Workshop on Linguistically Interpreted
Corpora (LINC-03), pages 1?8, Budapest.
D. Klein and C. Manning. 2003. Accurate Unlexi-
calized Parsing. In Proceedings of the 41st An-
nual Meeting of the Association for Computa-
tional Linguistics (ACL?02), pages 423?430, Sap-
poro, Japan.
C. Macleod, A. Meyers, and R. Grishman. 1994.
The COMLEX Syntax Project: The First Year.
In Proceedings of the ARPA Workshop on Human
Language Technology, pages 669-703, Princeton,
NJ.
D. Magerman. 1994. Natural Language Parsing as
Statistical Pattern Recognition. PhD thesis, Stan-
ford University, CA.
M. Marcus, G. Kim, M.A. Marcinkiewicz, R. Mac-
Intyre, A. Bies, M. Ferguson, K. Katz, and B.
Schasberger. 1994. The Penn Treebank: Anno-
tating Predicate Argument Structure. In Proceed-
ings of the ARPA Workshop on Human Language
Technology, pages 110?115, Princeton, NJ.
Y. Miyao, T. Ninomiya, and J. Tsujii. 2003. Proba-
bilistic modeling of argument structures includ-
ing non-local dependencies. In Proceedings of
the Conference on Recent Advances in Natural
Language Processing (RANLP), pages 285?291,
Borovets, Bulgaria.
R. O?Donovan, M. Burke, A. Cahill, J. van Gen-
abith, and A. Way. 2004. Large-Scale Induc-
tion and Evaluation of Lexical Resources from
the Penn-II Treebank. In Proceedings of the 42nd
Annual Conference of the Association for Com-
putational Linguistics (ACL-04), Barcelona.
S. Riezler, T.H. King, R. Kaplan, R. Crouch,
J. T. Maxwell III, and M. Johnson. 2002. Pars-
ing the Wall Street Journal using a Lexical-
Functional Grammar and Discriminative Estima-
tion Techniques. In Proceedings of the 40th An-
nual Conference of the Association for Compu-
tational Linguistics (ACL-02), pages 271?278,
Philadelphia, PA.
Y. Tateisi, K. Torisawa, Y. Miyao, and J. Tsujii.
1998. Translating the XTAG English Grammar
to HPSG. In 4th International Workshop on Tree
Adjoining Grammars and Related Frameworks,
Philadelphia, PA, pages 172?175.
J. van Genabith and R. Crouch. 1996. Direct
and Underspecified Interpretations of LFG f-
Structures. In Proceedings of the 16th Interna-
tional Conference on Computational Linguistics
(COLING), pages 262?267, Copenhagen.
Large-Scale Induction and Evaluation of Lexical Resources from the
Penn-II Treebank
Ruth O?Donovan, Michael Burke, Aoife Cahill, Josef van Genabith, Andy Way
National Centre for Language Technology and School of Computing
Dublin City University
Glasnevin
Dublin 9
Ireland
{rodonovan,mburke,acahill,josef,away}@computing.dcu.ie
Abstract
In this paper we present a methodology for ex-
tracting subcategorisation frames based on an
automatic LFG f-structure annotation algorithm
for the Penn-II Treebank. We extract abstract
syntactic function-based subcategorisation frames
(LFG semantic forms), traditional CFG category-
based subcategorisation frames as well as mixed
function/category-based frames, with or without
preposition information for obliques and particle in-
formation for particle verbs. Our approach does
not predefine frames, associates probabilities with
frames conditional on the lemma, distinguishes be-
tween active and passive frames, and fully reflects
the effects of long-distance dependencies in the
source data structures. We extract 3586 verb lem-
mas, 14348 semantic form types (an average of 4
per lemma) with 577 frame types. We present a
large-scale evaluation of the complete set of forms
extracted against the full COMLEX resource.
1 Introduction
Lexical resources are crucial in the construction
of wide-coverage computational systems based on
modern syntactic theories (e.g. LFG, HPSG, CCG,
LTAG etc.). However, as manual construction of
such lexical resources is time-consuming, error-
prone, expensive and rarely ever complete, it is of-
ten the case that limitations of NLP systems based
on lexicalised approaches are due to bottlenecks in
the lexicon component.
Given this, research on automating lexical acqui-
sition for lexically-based NLP systems is a partic-
ularly important issue. In this paper we present an
approach to automating subcategorisation frame ac-
quisition for LFG (Kaplan and Bresnan, 1982) i.e.
grammatical function-based systems. LFG has two
levels of structural representation: c(onstituent)-
structure, and f(unctional)-structure. LFG differ-
entiates between governable (argument) and non-
governable (adjunct) grammatical functions. Sub-
categorisation requirements are enforced through
semantic forms specifying the governable grammat-
ical functions required by a particular predicate (e.g.
FOCUS?(? SUBJ)(? OBLon)?). Our approach is
based on earlier work on LFG semantic form extrac-
tion (van Genabith et al, 1999) and recent progress
in automatically annotating the Penn-II treebank
with LFG f-structures (Cahill et al, 2004b). De-
pending on the quality of the f-structures, reliable
LFG semantic forms can then be generated quite
simply by recursively reading off the subcategoris-
able grammatical functions for each local pred
value at each level of embedding in the f-structures.
The work reported in (van Genabith et al, 1999)
was small scale (100 trees), proof of concept and
required considerable manual annotation work. In
this paper we show how the extraction process can
be scaled to the complete Wall Street Journal (WSJ)
section of the Penn-II treebank, with about 1 mil-
lion words in 50,000 sentences, based on the au-
tomatic LFG f-structure annotation algorithm de-
scribed in (Cahill et al, 2004b). In addition to ex-
tracting grammatical function-based subcategorisa-
tion frames, we also include the syntactic categories
of the predicate and its subcategorised arguments,
as well as additional details such as the prepositions
required by obliques, and particles accompanying
particle verbs. Our method does not predefine the
frames to be extracted. In contrast to many other
approaches, it discriminates between active and pas-
sive frames, properly reflects long distance depen-
dencies and assigns conditional probabilities to the
semantic forms associated with each predicate.
Section 2 reviews related work in the area of
automatic subcategorisation frame extraction. Our
methodology and its implementation are presented
in Section 3. Section 4 presents the results of our
lexical extraction. In Section 5 we evaluate the
complete extracted lexicon against the COMLEX
resource (MacLeod et al, 1994). To our knowl-
edge, this is the largest evaluation of subcategorisa-
tion frames for English. In Section 6, we conclude
and give suggestions for future work.
2 Related Work
Creating a (subcategorisation) lexicon by hand is
time-consuming, error-prone, requires considerable
linguistic expertise and is rarely, if ever, complete.
In addition, a system incorporating a manually con-
structed lexicon cannot easily be adapted to specific
domains. Accordingly, many researchers have at-
tempted to construct lexicons automatically, espe-
cially for English.
(Brent, 1993) relies on local morphosyntactic
cues (such as the -ing suffix, except where such a
word follows a determiner or a preposition other
than to) in the untagged Brown Corpus as proba-
bilistic indicators of six different predefined subcat-
egorisation frames. The frames do not include de-
tails of specific prepositions. (Manning, 1993) ob-
serves that Brent?s recognition technique is a ?rather
simplistic and inadequate approach to verb detec-
tion, with a very high error rate?. Manning feeds
the output from a stochastic tagger into a finite state
parser, and applies statistical filtering to the parsing
results. He predefines 19 different subcategorisation
frames, including details of prepositions. Applying
this technique to approx. 4 million words of New
York Times newswire, Manning acquires 4900 sub-
categorisation frames for 3104 verbs, an average of
1.6 per verb. (Ushioda et al, 1993) run a finite state
NP parser on a POS-tagged corpus to calculate the
relative frequency of just six subcategorisation verb
classes. In addition, all prepositional phrases are
treated as adjuncts. For 1565 tokens of 33 selected
verbs, they report an accuracy rate of 83%.
(Briscoe and Carroll, 1997) observe that in the
work of (Brent, 1993), (Manning, 1993) and (Ush-
ioda et al, 1993), ?the maximum number of distinct
subcategorization classes recognized is sixteen, and
only Ushioda et al attempt to derive relative subcat-
egorization frequency for individual predicates?. In
contrast, the system of (Briscoe and Carroll, 1997)
distinguishes 163 verbal subcategorisation classes
by means of a statistical shallow parser, a classifier
of subcategorisation classes, and a priori estimates
of the probability that any verb will be a member
of those classes. More recent work by Korhonen
(2002) on the filtering phase of this approach has
improved results. Korhonen experiments with the
use of linguistic verb classes for obtaining more ac-
curate back-off estimates for use in hypothesis se-
lection. Using this extended approach, the average
results for 45 semantically classified test verbs eval-
uated against hand judgements are precision 87.1%
and recall 71.2%. By comparison, the average re-
sults for 30 verbs not classified semantically are pre-
cision 78.2% and recall 58.7%.
Carroll and Rooth (1998) use a hand-written
head-lexicalised context-free grammar and a text
corpus to compute the probability of particular sub-
categorisation scenarios. The extracted frames do
not contain details of prepositions.
More recently, a number of researchers have
applied similar techniques to derive resources for
other languages, especially German. One of these,
(Schulte im Walde, 2002), induces a computational
subcategorisation lexicon for over 14,000 German
verbs. Using sentences of limited length, she ex-
tracts 38 distinct frame types, which contain max-
imally three arguments each. The frames may op-
tionally contain details of particular prepositional
use. Her evaluation on over 3000 frequently occur-
ring verbs against the German dictionary Duden -
Das Stilwo?rterbuch is similar in scale to ours and is
discussed further in Section 5.
There has also been some work on extracting
subcategorisation details from the Penn Treebank.
(Kinyon and Prolo, 2002) introduce a tool which
uses fine-grained rules to identify the arguments,
including optional arguments, of each verb occur-
rence in the Penn Treebank, along with their syn-
tactic functions. They manually examined the 150+
possible sequences of tags, both functional and cat-
egorial, in Penn-II and determined whether the se-
quence in question denoted a modifier, argument or
optional argument. Arguments were then mapped
to traditional syntactic functions. As they do not in-
clude an evaluation, currently it is impossible to say
how effective this technique is.
(Xia et al, 2000) and (Chen and Vijay-Shanker,
2000) extract lexicalised TAGs from the Penn Tree-
bank. Both techniques implement variations on
the approaches of (Magerman, 1994) and (Collins,
1997) for the purpose of differentiating between
complement and adjunct. In the case of (Xia et al,
2000), invalid elementary trees produced as a result
of annotation errors in the treebank are filtered out
using linguistic heuristics.
(Hockenmaier et al, 2002) outline a method for
the automatic extraction of a large syntactic CCG
lexicon from Penn-II. For each tree, the algorithm
annotates the nodes with CCG categories in a top-
down recursive manner. In order to examine the
coverage of the extracted lexicon in a manner simi-
lar to (Xia et al, 2000), (Hockenmaier et al, 2002)
compared the reference lexicon acquired from Sec-
tions 02-21 with a test lexicon extracted from Sec-
tion 23 of the WSJ. It was found that the reference
CCG lexicon contained 95.09% of the entries in the
test lexicon, while 94.03% of the entries in the test
TAG lexicon also occurred in the reference lexicon.
Both approaches involve extensive correction and
clean-up of the treebank prior to lexical extraction.
3 Our Methodology
The first step in the application of our methodology
is the production of a treebank annotated with LFG
f-structure information. F-structures are feature
structures which represent abstract syntactic infor-
mation, approximating to basic predicate-argument-
modifier structures. We utilise the automatic anno-
tation algorithm of (Cahill et al, 2004b) to derive
a version of Penn-II where each node in each tree
is annotated with an LFG functional annotation (i.e.
an attribute value structure equation). Trees are tra-
versed top-down, and annotation is driven by cate-
gorial, basic configurational, trace and Penn-II func-
tional tag information in local subtrees of mostly
depth one (i.e. CFG rules). The annotation proce-
dure is dependent on locating the head daughter, for
which the scheme of (Magerman, 1994) with some
changes and amendments is used. The head is anno-
tated with the LFG equation ?=?. Linguistic gen-
eralisations are provided over the left (the prefix)
and the right (suffix) context of the head for each
syntactic category occurring as the mother node of
such heads. To give a simple example, the rightmost
NP to the left of a VP head under an S is likely to
be its subject (? SUBJ =?), while the leftmost NP
to the right of the V head of a VP is most proba-
bly its object (? OBJ =?). (Cahill et al, 2004b)
provide four sets of annotation principles, one for
non-coordinate configurations, one for coordinate
configurations, one for traces (long distance depen-
dencies) and a final ?catch all and clean up? phase.
Distinguishing between argument and adjunct is an
inherent step in the automatic assignment of func-
tional annotations.
The satisfactory treatment of long distance de-
pendencies by the annotation algorithm is impera-
tive for the extraction of accurate semantic forms.
The Penn Treebank employs a rich arsenal of traces
and empty productions (nodes which do not re-
alise any lexical material) to co-index displaced ma-
terial with the position where it should be inter-
preted semantically. The algorithm of (Cahill et
al., 2004b) translates the traces into corresponding
re-entrancies in the f-structure representation (Fig-
ure 1). Passive movement is also captured and ex-
pressed at f-structure level using a passive:+ an-
notation. Once a treebank tree is annotated with
feature structure equations by the annotation algo-
rithm, the equations are collected and passed to a
constraint solver which produces the f-structures.
In order to ensure the quality of the seman-
S
S-TPC- 1
NP
U.N.
VP
V
signs
NP
treaty
NP
Det
the
N
headline
VP
V
said
S
T- 1
?
?
?
?
?
?
?
TOPIC
[
SUBJ
[
PRED U.N.
]
PRED sign
OBJ
[
PRED treaty
]
]
1
SUBJ
[
SPEC the
PRED headline
]
PRED say
COMP 1
?
?
?
?
?
?
?
Figure 1: Penn-II style tree with long distance depen-
dency trace and corresponding reentrancy in f-structure
tic forms extracted by our method, we must first
ensure the quality of the f-structure annotations.
(Cahill et al, 2004b) measure annotation quality
in terms of precision and recall against manually
constructed, gold-standard f-structures for 105 ran-
domly selected trees from section 23 of the WSJ
section of Penn-II. The algorithm currently achieves
an F-score of 96.3% for complete f-structures and
93.6% for preds-only f-structures.1
Our semantic form extraction methodology is
based on the procedure of (van Genabith et al,
1999): For each f-structure generated, for each
level of embedding we determine the local PRED
value and collect the subcategorisable grammat-
ical functions present at that level of embed-
ding. Consider the f-structure in Figure 1. From
this we recursively extract the following non-
empty semantic forms: say([subj,comp]),
sign([subj,obj]). In effect, in both (van
Genabith et al, 1999) and our approach seman-
tic forms are reverse engineered from automatically
generated f-structures for treebank trees. We ex-
tract the following subcategorisable syntactic func-
tions: SUBJ, OBJ, OBJ2, OBLprep, OBL2prep, COMP,
XCOMP and PART. Adjuncts (e.g. ADJ, APP etc)
are not included in the semantic forms. PART
is not a syntactic function in the strict sense but
we capture the relevant co-occurrence patterns of
verbs and particles in the semantic forms. Just
as OBL includes the prepositional head of the PP,
PART includes the actual particle which occurs e.g.
add([subj,obj,part:up]).
In the work presented here we substantially ex-
tend the approach of (van Genabith et al, 1999) as
1Preds-only measures only paths ending in PRED:VALUE so
features such as number, person etc are not included.
regards coverage, granularity and evaluation: First,
we scale the approach of (van Genabith et al, 1999)
which was proof of concept on 100 trees to the full
WSJ section of the Penn-II Treebank. Second, our
approach fully reflects long distance dependencies,
indicated in terms of traces in the Penn-II Tree-
bank and corresponding re-entrancies at f-structure.
Third, in addition to abstract syntactic function-
based subcategorisation frames we compute frames
for syntactic function-CFG category pairs, both for
the verbal heads and their arguments and also gen-
erate pure CFG-based subcat frames. Fourth, our
method differentiates between frames captured for
active or passive constructions. Fifth, our method
associates conditional probabilities with frames.
In contrast to much of the work reviewed in the
previous section, our system is able to produce sur-
face syntactic as well as abstract functional subcat-
egorisation details. To incorporate CFG details into
the extracted semantic forms, we add an extra fea-
ture to the generated f-structures, the value of which
is the syntactic category of the pred at each level
of embedding. Exploiting this information, the ex-
tracted semantic form for the verb sign looks as fol-
lows: sign(v,[subj(np),obj(np)]).
We have also extended the algorithm to deal with
passive voice and its effect on subcategorisation be-
haviour. Consider Figure 2: not taking voice into
account, the algorithm extracts an intransitive frame
outlaw([subj]) for the transitive outlaw. To
correct this, the extraction algorithm uses the fea-
ture value pair passive:+, which appears in the
f-structure at the level of embedding of the verb in
question, to mark that predicate as occurring in the
passive: outlaw([subj],p).
In order to estimate the likelihood of the cooc-
currence of a predicate with a particular argument
list, we compute conditional probabilities for sub-
categorisation frames based on the number of token
occurrences in the corpus. Given a lemma l and an
argument list s, the probability of s given l is esti-
mated as:
P(s|l) := count(l, s)?n
i=1 count(l, si)
We use thresholding to filter possible error judge-
ments by our system. Table 1 shows the attested
semantic forms for the verb accept with their as-
sociated conditional probabilities. Note that were
the distinction between active and passive not taken
into account, the intransitive occurrence of accept
would have been assigned an unmerited probability.
subj : spec : quant : pred : all
adjunct : 2 : pred : almost
adjunct : 3 : pred : remain
participle : pres
4 : obj : adjunct : 5 : pred : cancer-causing
pers : 3
pred : asbestos
num : sg
pform : of
pers : 3
pred : use
num : pl
passive : +
adjunct : 1 : obj : pred : 1997
pform : by
xcomp : subj : spec: quant : pred : all
adjunct : 2 : pred : almost
...
...
passive : +
xcomp : subj : spec: quant : pred : all
adjunct : 2 : pred : almost
...
...
passive : +
pred : outlaw
tense : past
pred : be
pred : will
modal : +
Figure 2: Automatically generated f-structure
for the string wsj 0003 23?By 1997, almost
all remaining uses of cancer-causing
asbestos will be outlawed.?
Semantic Form Frequency Probability
accept([subj,obj]) 122 0.813
- accept([subj],p) 9 0.060
accept([subj,comp]) 5 0.033
- accept([subj,obl:as],p) 3 0.020
accept([subj,obj,obl:as]) 3 0.020
accept([subj,obj,obl:from]) 3 0.020
- accept([subj]) 2 0.013
accept([subj,obj,obl:at]) 1 0.007
accept([subj,obj,obl:for]) 1 0.007
accept([subj,obj,xcomp]) 1 0.007
Table 1: Semantic Forms for the verb accept marked
with p for passive use.
4 Results
We extract non-empty semantic forms2 for 3586
verb lemmas and 10969 unique verbal semantic
form types (lemma followed by non-empty argu-
ment list). Including prepositions associated with
the OBLs and particles, this number rises to 14348,
an average of 4.0 per lemma (Table 2). The num-
ber of unique frame types (without lemma) is 38
without specific prepositions and particles, 577 with
(Table 3). F-structure annotations allow us to distin-
guish passive and active frames.
5 COMLEX Evaluation
We evaluated our induced (verbal) semantic forms
against COMLEX (MacLeod et al, 1994). COM-
2Frames with at least one subcategorised grammatical func-
tion.
Without Prep/Part With Prep/Part
Sem. Form Types 10969 14348
Active 8516 11367
Passive 2453 2981
Table 2: Number of Semantic Form Types
Without Prep/Part With Prep/Part
# Frame Types 38 577
# Singletons 1 243
# Twice Occurring 1 84
# Occurring max. 5 7 415
# Occurring > 5 31 162
Table 3: Number of Distinct Frames for Verbs (not in-
cluding syntactic category for grammatical function)
LEX defines 138 distinct verb frame types without
the inclusion of specific prepositions or particles.
The following is a sample entry for the verb
reimburse:
(VERB :ORTH ?reimburse? :SUBC ((NP-NP)
(NP-PP :PVAL (?for?))
(NP)))
Each verb has a :SUBC feature, specifying
its subcategorisation behaviour. For example,
reimburse can occur with two noun phrases
(NP-NP), a noun phrase and a prepositional phrase
headed by ?for? (NP-PP :PVAL (?for?)) or a single
noun phrase (NP). Note that the details of the subject
noun phrase are not included in COMLEX frames.
Each of the complement types which make up the
value of the :SUBC feature is associated with a for-
mal frame definition which looks as follows:
(vp-frame np-np :cs ((np 2)(np 3))
:gs (:subject 1 :obj 2 :obj2 3)
:ex ?she asked him his name?)
The value of the :cs feature is the constituent struc-
ture of the subcategorisation frame, which lists the
syntactic CF-PSG constituents in sequence. The
value of the :gs feature is the grammatical struc-
ture which indicates the functional role played by
each of the CF-PSG constituents. The elements of
the constituent structure are indexed, and referenced
in the :gs field. This mapping between constituent
structure and functional structure makes the infor-
mation contained in COMLEX suitable as an eval-
uation standard for the LFG semantic forms which
we induce.
5.1 COMLEX-LFG Mapping
We devised a common format for our induced se-
mantic forms and those contained in COMLEX.
This is summarised in Table 4. COMLEX does
not distinguish between obliques and objects so we
converted Obji to OBLi as required. In addition,
COMLEX does not explicitly differentiate between
COMPs and XCOMPs, but does encode control in-
formation for any Comps which occur, thus allow-
ing us to deduce the distinction automatically. The
manually constructed COMLEX entries provided us
with a gold standard against which we evaluated the
automatically induced frames for the 2992 (active)
verbs that both resources have in common.
LFG COMLEX Merged
SUBJ Subject SUBJ
OBJ Object OBJ
OBJ2 Obj2 OBJ2
OBL Obj3 OBL
OBL2 Obj4 OBL2
COMP Comp COMP
XCOMP Comp XCOMP
PART Part PART
Table 4: COMLEX and LFG Syntactic Functions
We use the computed conditional probabilities to set
a threshold to filter the selection of semantic forms.
As some verbs occur less frequently than others we
felt it was important to use a relative rather than ab-
solute threshold. For a threshold of 1%, we disre-
gard any frames with a conditional probability of
less than or equal to 0.01. We carried out the evalu-
ation in a similar way to (Schulte im Walde, 2002).
The scale of our evaluation is comparable to hers.
This allows us to make tentative comparisons be-
tween our respective results. The figures shown in
Table 5 are the results of three different kinds of
evaluation with the threshold set to 1% and 5%. The
effect of the threshold increase is obvious in that
Precision goes up for each of the experiments while
Recall goes down.
For Exp 1, we excluded prepositional phrases en-
tirely from the comparison, i.e. assumed that PPs
were adjunct material (e.g. [subj,obl:for] becomes
[subj]). Our results are better for Precision than for
Recall compared to Schulte im Walde (op cit.), who
reports Precision of 74.53%, Recall of 69.74% and
an F-score of 72.05%.
Exp 2 includes prepositional phrases but not
parameterised for particular prepositions (e.g.
[subj,obl:for] becomes [subj,obl]). While our fig-
ures for Recall are again lower, our results for
Precision are considerably higher than those of
Schulte im Walde (op cit.) who recorded Preci-
sion of 60.76%, Recall of 63.91% and an F-score
of 62.30%.
For Exp. 3, we used semantic forms which con-
tained details of specific prepositions for any sub-
categorised prepositional phrase. Our Precision fig-
ures are again high (in comparison to 65.52% as
recorded by (Schulte im Walde, 2002)). However,
Threshold 1% Threshold 5%
P R F-Score P R F-Score
Exp. 1 79.0% 59.6% 68.0% 83.5% 54.7% 66.1%
Exp. 2 77.1% 50.4% 61.0% 81.4% 44.8% 57.8%
Exp. 2a 76.4% 44.5% 56.3% 80.9% 39.0% 52.6%
Exp. 3 73.7% 22.1% 34.0% 78.0% 18.3% 29.6%
Exp. 3a 73.3% 19.9% 31.3% 77.6% 16.2% 26.8%
Table 5: COMLEX Comparison
our Recall is very low (compared to the 50.83% that
Schulte im Walde (op cit.) reports). Consequently
our F-score is also low (Schulte im Walde (op cit.)
records an F-score of 57.24%). Experiments 2a and
3a are similar to Experiments 2 and 3 respectively
except they include the specific particle associated
with each PART.
5.1.1 Directional Prepositions
There are a number of possible reasons for our
low recall scores for Experiment 3 in Table 5. It
is a well-documented fact (Briscoe and Carroll,
1997) that subcategorisation frames (and their fre-
quencies) vary across domains. We have extracted
frames from one domain (the WSJ) whereas COM-
LEX was built using examples from the San Jose
Mercury News, the Brown Corpus, several literary
works from the Library of America, scientific ab-
stracts from the U.S. Department of Energy, and
the WSJ. For this reason, it is likely to contain
a greater variety of subcategorisation frames than
our induced lexicon. It is also possible that due
to human error COMLEX contains subcategorisa-
tion frames, the validity of which may be in doubt.
This is due to the fact that the aim of the COMLEX
project was to construct as complete a set of subcat-
egorisation frames as possible, even for infrequent
verbs. Lexicographers were allowed to extrapo-
late from the citations found, a procedure which
is bound to be less certain than the assignment of
frames based entirely on existing examples. Our re-
call figure was particularly low in the case of eval-
uation using details of prepositions (Experiment 3).
This can be accounted for by the fact that COMLEX
errs on the side of overgeneration when it comes to
preposition assignment. This is particularly true of
directional prepositions, a list of 31 of which has
been prepared and is assigned in its entirety by de-
fault to any verb which can potentially appear with
any directional preposition. In a subsequent exper-
iment, we incorporate this list of directional prepo-
sitions by default into our semantic form induction
process in the same way as the creators of COM-
LEX have done. Table 6 shows the results of this
experiment. As expected there is a significant im-
Precision Recall F-Score
Experiment 3 81.7% 40.8% 54.4%
Experiment 3a 83.1% 35.4% 49.7%
Table 6: COMLEX Comparison using p-dir(Threshold
of 1%)
Passive Precision Recall F-Score
Experiment 2 80.2% 54.7% 65.1%
Experiment 2a 79.7% 46.2% 58.5%
Experiment 3 72.6% 33.4% 45.8%
Experiment 3a 72.3% 29.3% 41.7%
Table 7: Passive evaluation (Threshold of 1%)
provement in the recall figure, being almost double
the figures reported in Table 5 for Experiments 3
and 3a.
5.1.2 Passive Evaluation
Table 7 presents the results of our evaluation of
the passive semantic forms we extract. It was
carried out for 1422 verbs which occur with pas-
sive frames and are shared by the induced lexicon
and COMLEX. As COMLEX does not provide ex-
plicit passive entries, we applied Lexical Redun-
dancy Rules (Kaplan and Bresnan, 1982) to auto-
matically convert the active COMLEX frames to
their passive counterparts. For example, the COM-
LEX entry see([subj,obj]) is converted to
see([subj]). The resulting precision is very
high, a slight increase on that for the active frames.
The recall score drops for passive frames (from
54.7% to 29.3%) in a similar way to that for active
frames when prepositional details are included.
5.2 Lexical Accession Rates
As well as evaluating the quality of our extracted
semantic forms, we also examine the rate at which
they are induced. (Charniak, 1996) and (Krotov et
al., 1998) observed that treebank grammars (CFGs
extracted from treebanks) are very large and grow
with the size of the treebank. We were interested in
discovering whether the acquisition of lexical mate-
rial on the same data displays a similar propensity.
Figure 3 displays the accession rates for the seman-
tic forms induced by our method for sections 0?24
of the WSJ section of the Penn-II treebank. When
we do not distinguish semantic forms by category,
all semantic forms together with those for verbs dis-
play smaller accession rates than for the PCFG.
We also examined the coverage of our system in
a similar way to (Hockenmaier et al, 2002). We ex-
tracted a verb-only reference lexicon from Sections
02-21 of the WSJ and subsequently compared this
to a test lexicon constructed in the same way from
 0
 5000
 10000
 15000
 20000
 25000
 0  5  10  15  20  25
N
o.
 o
f S
Fs
/R
ul
es
WSJ Section
All SF Frames
All Verbs
All SF Frames, no category
All Verbs, no category
PCFG
Figure 3: Accession Rates for Semantic Forms and CFG
Rules
Entries also in reference lexicon: 89.89%
Entries not in reference lexicon: 10.11%
Known words: 7.85%
- Known words, known frames: 7.85%
- Known words, unknown frames: -
Unknown words: 2.32%
- Unknown words, known frames: 2.32%
- Unknown words, unknown frames: -
Table 8: Coverage of induced lexicon on unseen
data (Verbs Only)
Section 23. Table 8 shows the results of this ex-
periment. 89.89% of the entries in the test lexicon
appeared in the reference lexicon.
6 Conclusions
We have presented an algorithm and its implementa-
tion for the extraction of semantic forms or subcate-
gorisation frames from the Penn-II Treebank, auto-
matically annotated with LFG f-structures. We have
substantially extended an earlier approach by (van
Genabith et al, 1999). The original approach was
small-scale and ?proof of concept?. We have scaled
our approach to the entire WSJ Sections of Penn-
II (50,000 trees). Our approach does not predefine
the subcategorisation frames we extract as many
other approaches do. We extract abstract syntac-
tic function-based subcategorisation frames (LFG
semantic forms), traditional CFG category-based
frames as well as mixed function-category based
frames. Unlike many other approaches to subcate-
gorisation frame extraction, our system properly re-
flects the effects of long distance dependencies and
distinguishes between active and passive frames.
Finally our system associates conditional probabil-
ities with the frames we extract. We carried out an
extensive evaluation of the complete induced lexi-
con (not just a sample) against the full COMLEX
resource. To our knowledge, this is the most exten-
sive qualitative evaluation of subcategorisation ex-
traction in English. The only evaluation of a similar
scale is that carried out by (Schulte im Walde, 2002)
for German. Our results compare well with hers.
We believe our semantic forms are fine-grained and
by choosing to evaluate against COMLEX we set
our sights high: COMLEX is considerably more
detailed than the OALD or LDOCE used for other
evaluations.
Currently work is under way to extend the cov-
erage of our acquired lexicons by applying our
methodology to the Penn-III treebank, a more bal-
anced corpus resource with a number of text gen-
res (in addition to the WSJ sections). It is impor-
tant to realise that the induction of lexical resources
is part of a larger project on the acquisition of
wide-coverage, robust, probabilistic, deep unifica-
tion grammar resources from treebanks. We are al-
ready using the extracted semantic forms in parsing
new text with robust, wide-coverage PCFG-based
LFG grammar approximations automatically ac-
quired from the f-structure annotated Penn-II tree-
bank (Cahill et al, 2004a). We hope to be able to
apply our lexical acquisition methodology beyond
existing parse-annotated corpora (Penn-II and Penn-
III): new text is parsed by our PCFG-based LFG ap-
proximations into f-structures from which we can
then extract further semantic forms. The work re-
ported here is part of the core component for boot-
strapping this approach.
As the extraction algorithm we presented derives
semantic forms at f-structure level, it is easily ap-
plied to other, even typologically different, lan-
guages. We have successfully ported our automatic
annotation algorithm to the TIGER Treebank, de-
spite German being a less configurational language
than English, and extracted wide-coverage, proba-
bilistic LFG grammar approximations and lexical
resources for German (Cahill et al, 2003). Cur-
rently, we are migrating the technique to Spanish,
which has freer word order than English and less
morphological marking than German. Preliminary
results have been very encouraging.
7 Acknowledgements
The research reported here is supported by Enter-
prise Ireland Basic Research Grant SC/2001/186
and an IRCSET PhD fellowship award.
References
M. Brent. 1993. From Grammar to Lexicon: Unsu-
pervised Learning of Lexical Syntax. Computa-
tional Linguistics, 19(2):203?222.
E. Briscoe and J. Carroll. 1997. Automatic Extrac-
tion of Subcategorization from Corpora. In Pro-
ceedings of the 5th ACL Conference on Applied
Natural Language Processing, pages 356?363,
Washington, DC.
A. Cahill, M. Forst, M. McCarthy, R. O?Donovan,
C. Rohrer, J. van Genabith, and A. Way.
2003. Treebank-Based Multilingual Unification-
Grammar Development. In Proceedings of the
Workshop on Ideas and Strategies for Multilin-
gual Grammar Development at the 15th ESSLLI,
pages 17?24, Vienna, Austria.
A. Cahill, M. Burke, R. O?Donovan, J. van Gen-
abith, and A. Way. 2004a. Long-Distance De-
pendency Resolution in Automatically Acquired
Wide-Coverage PCFG-Based LFG Approxima-
tions. In Proceedings of the 42nd Annual Con-
ference of the Association for Computational Lin-
guistics (ACL-04), Barcelona, Spain.
A. Cahill, M. McCarthy, M. Burke, R. O?Donovan,
J. van Genabith, and A. Way. 2004b. Evaluating
Automatic F-Structure Annotation for the Penn-
II Treebank. Journal of Research on Language
and Computation.
G. Carroll and M. Rooth. 1998. Valence Induc-
tion with a Head-Lexicalised PCFG. In Proceed-
ings of the 3rd Conference on Empirical Meth-
ods in Natural Language Processing, pages 36?
45, Granada, Spain.
E. Charniak. 1996. Tree-bank Grammars. In AAAI-
96: Proceedings of the Thirteenth National Con-
ference on Artificial Intelligence, MIT Press,
pages 1031?1036, Cambridge, MA.
J. Chen and K. Vijay-Shanker. 2000. Automated
Extraction of TAGs from the Penn Treebank. In
Proceedings of the 38th Annual Meeting of the
Association of Computational Linguistics, pages
65?76, Hong Kong.
M. Collins. 1997. Three generative lexicalised
models for statistical parsing. In Proceedings of
the 35th Annual Meeting of the Association for
Computational Linguistics, pages 16?23.
J. Hockenmaier, G. Bierner, and J. Baldridge. 2002.
Extending the Coverage of a CCG System. Jour-
nal of Language and Computation, (2).
R. Kaplan and J. Bresnan. 1982. Lexical Func-
tional Grammar: A Formal System for Gram-
matical Representation. In Joan Bresnan, editor,
The Mental Representation of Grammatical Re-
lations, pages 206?250. MIT Press, Cambridge,
MA, Mannheim, 8th Edition.
A. Kinyon and C. Prolo. 2002. Identifying Verb Ar-
guments and their Syntactic Function in the Penn
Treebank. In Proceedings of the 3rd LREC Con-
ference, pages 1982?1987, Las Palmas, Spain.
A. Korhonen. 2002. Subcategorization Acquisition.
PhD thesis published as Techical Report UCAM-
CL-TR-530, Computer Laboratory, University of
Cambridge, UK.
A. Krotov, M. Hepple, R. Gaizauskas, and Y. Wilks.
1998. Compacting the Penn Treebank Grammar.
In Proceedings of COLING-ACL?98, pages 669?
703, Montreal, Canada.
C. MacLeod, R. Grishman, and A. Meyers. 1994.
The Comlex Syntax Project: The First Year. In
Proceedings of the ARPA Workshop on Human
Language Technology, pages 669?703, Prince-
ton, NJ.
D. Magerman. 1994. Natural Language Parsing
as Statistical Pattern Recognition. PhD Thesis,
Stanford University, CA.
C. Manning. 1993. Automatic Acquisition of a
Large Subcategorisation Dictionary from Cor-
pora. In Proceedings of the 31st Annual Meeting
of the Association for Computational Linguistics,
pages 235?242, Columbus, OH.
S. Schulte im Walde. 2002. Evaluating Verb Sub-
categorisation Frames learned by a German Sta-
tistical Grammar against Manual Definitions in
the Duden Dictionary. In Proceedings of the 10th
EURALEX International Congress, pages 187?
197, Copenhagen, Denmark.
A. Ushioda, D. Evans, T. Gibson, and A. Waibel.
1993. The Automatic Acquisition of Frequencies
of Verb Subcategorization Frames from Tagged
Corpora. In SIGLEX ACL Workshop on the Ac-
quisition of Lexical Knowledge from Text, pages
95?106, Columbus, OH.
J. van Genabith, A. Way, and L. Sadler. 1999. Data-
driven Compilation of LFG Semantic Forms. In
EACL-99 Workshop on Linguistically Interpreted
Corpora, pages 69?76, Bergen, Norway.
F. Xia, M. Palmer, and A. Joshi. 2000. A Uniform
Method of Grammar Extraction and its Applica-
tions. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP-2000), pages 53?62, Hong Kong.
Wide-Coverage Deep Statistical Parsing
Using Automatic Dependency
Structure Annotation
Aoife Cahill?
Dublin City University
Michael Burke??,?
Dublin City University
IBM Center for Advanced Studies
Ruth O?Donovan??
Dublin City University
Stefan Riezler?
Palo Alto Research Center
Josef van Genabith??,?
Dublin City University
IBM Center for Advanced Studies
Andy Way??,?
Dublin City University
IBM Center for Advanced Studies
A number of researchers have recently conducted experiments comparing ?deep? hand-crafted
wide-coverage with ?shallow? treebank- and machine-learning-based parsers at the level of
dependencies, using simple and automatic methods to convert tree output generated by the
shallow parsers into dependencies. In this article, we revisit such experiments, this time using
sophisticated automatic LFG f-structure annotation methodologies with surprising results. We
compare various PCFG and history-based parsers to find a baseline parsing system that fits
best into our automatic dependency structure annotation technique. This combined system of
syntactic parser and dependency structure annotation is compared to two hand-crafted, deep
constraint-based parsers, RASP and XLE. We evaluate using dependency-based gold standards
? Now at the Institut fu?r Maschinelle Sprachverarbeitung, Universita?t Stuttgart, Germany. E-mail: aoife.
cahill@ims.uni-stuttgart.de.
?? National Centre for Language Technology, Dublin City University, Dublin 9, Ireland.
? IBM Dublin Center for Advanced Studies (CAS), Dublin 15, Ireland.
? Now at Google Inc., Mountain View, CA.
Submission received: 24 August 2005; revised submission received: 20 March 2007; accepted for publication:
2 June 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 1
and use the Approximate Randomization Test to test the statistical significance of the results.
Our experiments show that machine-learning-based shallow grammars augmented with so-
phisticated automatic dependency annotation technology outperform hand-crafted, deep, wide-
coverage constraint grammars. Currently our best system achieves an f-score of 82.73% against
the PARC 700 Dependency Bank, a statistically significant improvement of 2.18% over the
most recent results of 80.55% for the hand-crafted LFG grammar and XLE parsing system
and an f-score of 80.23% against the CBS 500 Dependency Bank, a statistically significant
3.66% improvement over the 76.57% achieved by the hand-crafted RASP grammar and parsing
system.
1. Introduction
Wide-coverage parsers are often evaluated against gold-standard CFG trees (e.g.,
Penn-II WSJ Section 23 trees) reporting traditional PARSEVALmetrics (Black et al 1991)
of labeled and unlabeled bracketing precision, recall and f-score measures, number of
crossing brackets, complete matches, and so forth. Although tree-based parser evalua-
tion provides valuable insights into the performance of grammars and parsing systems,
it is subject to a number of (related) drawbacks:
1. Bracketed trees do not always provide NLP applications with enough
information to carry out the required tasks: Many applications involve
a deeper analysis of the input in the form of semantically motivated
information such as deep dependency relations, predicate?argument
structures, or simple logical forms.
2. A number of alternative, but equally valid tree representations can
potentially be given for the same input. To give just a few examples: In
English, VPs containing modals and auxiliaries can be analyzed using
(predominantly) binary branching rules (Penn-II [Marcus et al 1994]), or
employ flatter analyses where modals and auxiliaries are sisters of the
main verb (AP treebank [Leech and Garside 1991]), or indeed do without
a designated VP constituent at all (SUSANNE [Sampson 1995]). Treebank
bracketing guidelines can use ?traditional? CFG categories such as S, NP,
and so on (Penn-II) or a maximal projection-inspired analysis with IPs
and DPs (Chinese Penn Treebank [Xue et al 2004]).
3. Because a tree-based gold standard for parser evaluation must adopt a
particular style of linguistic analysis (reflected in the geometry and
nomenclature of the nodes in the trees), evaluation of statistical parsers
and grammars that are derived from particular treebank resources (as
well as hand-crafted grammars/parsers) can suffer unduly if the gold
standard deviates systematically from the (possibly) equally valid style
of linguistic analysis provided by the parser.
Problems such as these have motivated research on more abstract, dependency-
based parser evaluation (e.g., Lin 1995; Carroll, Briscoe, and Sanfilippo 1998; Carroll
et al 2002; Clark and Hockenmaier 2002; King et al 2003; Preiss 2003; Kaplan et al
2004; Miyao and Tsujii 2004). Dependency-based linguistic representations are ap-
proximations of abstract predicate-argument-adjunct (or more basic head-dependent)
82
Cahill et al Statistical Parsing Using Automatic Dependency Structures
structures, providing a more normalized representation abstracting away from the
particulars of surface realization or CFG-tree representation, which enables meaningful
cross-parser evaluation.
A related contrast holds between shallow and deep grammars and parsers.1 In
addition to defining a language (as a set of strings), deep grammars relate strings to in-
formation/meaning, often in the form of predicate?argument structure, dependency re-
lations,2 or logical forms. By contrast, a shallow grammar simply defines a language and
may associate syntactic (e.g., CFG tree) representations with strings. Natural languages
do not always interpret linguistic material locally where the material is encountered
in the string (or tree). In order to obtain accurate and complete predicate?argument,
dependency, or logical form representations, a hallmark of deep grammars is that they
usually involve a long-distance dependency (LDD) resolution mechanism.
Traditionally, deep grammars are hand-crafted (cf. the ALVEY Natural Language
Tools [Briscoe et al 1987], the Core Language Engine [Alshawi and Pulman 1992], the
Alpino Dutch dependency parser [Bouma, van Noord, andMalouf 2000], the Xerox Lin-
guistic Environment [Butt et al 2002], the RASP dependency parser [Carroll and Briscoe
2002] and the LinGO English Resource Grammar [Flickinger 2000; Baldwin et al 2004]).
Wide-coverage, deep-grammar development, particularly in rich formalisms such as
LFG (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001) and HPSG (Pollard
and Sag 1994), is knowledge-intensive, time-consuming and expensive, constituting
an instance of the (in-)famous ?knowledge acquisition bottleneck? familiar from other
areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars
(Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al 2002)
have, in fact, been successfully scaled to unrestricted input.
The last 15 years have seen extensive efforts on treebank-based automatic gram-
mar acquisition using a variety of machine-learning techniques (e.g., Gaizauskas 1995;
Charniak 1996; Collins 1999; Johnson 1999; Charniak 2000; Bikel 2002; Bod 2003; Klein
and Manning 2003). These grammars are wide-coverage and robust and in contrast
to manual grammar development, machine-learning-based grammar acquisition in-
curs relatively low development cost. With few notable exceptions,3 however, these
treebank-induced wide-coverage grammars are shallow: They usually do not attempt
to resolve LDDs nor do they associate strings with meaning representations.
Over the last few years, addressing the knowledge acquisition bottleneck in deep
constraint-based grammar development, a growing body of research has emerged to au-
tomatically acquire wide-coverage deep grammars from treebank resources (TAG [Xia
1999], CCG [Hockenmaier and Steedman 2002], HPSG [Miyao, Ninomiya, and Tsujii
2003], LFG [Cahill et al 2002b, 2004]). To a first approximation, these approaches can
be classified as ?conversion?- or ?annotation?-based. TAG-based approaches convert
1 Our use of the terms ?shallow? and ?deep? parsers/grammars follows Kaplan et al (2004) where
a ?shallow parser? does not relate strings to meaning representations. This deviates from a more
common use of the terms where, for example, a ?shallow parser? refers to (often finite-state-based)
parsers (or chunkers) that may produce partial bracketings of input strings.
2 By dependency relations we mean deep, fine-grained, labeled dependencies that encode long-distance
dependencies and passive information, for example. These differ from the types of unlabeled
dependency relations in other work such as (McDonald and Pereira 2006).
3 Both Collins Model 3 (1999) and Johnson (2002) output CFG tree representations with traces. Collins
Model 3 performs LDD resolution for wh-relative clause constructions, Johnson (2002) for a wide range
of LDD phenomena in a post-processing approach based on Penn-II tree fragments linking displaced
material with where it is to be interpreted semantically. The work of Dienes and Dubey (2003) and
Levy and Manning (2004) is similar to that of Johnson (2002), recovering empty categories on top of
CFG-based parsers. None of them map strings into dependencies.
83
Computational Linguistics Volume 34, Number 1
treebank trees into (lexicalized) elementary or adjunct trees. CCG-based approaches
convert trees into CCG derivations fromwhich CCG categories can be extracted. HPSG-
and LFG-based grammar induction methods automatically annotate treebank trees
with (typed) attribute-value structure information for the extraction of constraint-based
grammars and lexical resources.
Two recent papers (Preiss 2003; Kaplan et al 2004) have started tying together
the research strands just sketched: They use dependency-based parser evaluation to
compare wide-coverage parsing systems using hand-crafted, deep, constraint-based
grammars with systems based on a simple version of treebank-based deep grammar
acquisition technology in the conversion paradigm. In the experiments, tree output
generated by Collins?s Model 1, 2, and 3 (1999) and Charniak?s (2000) parsers, for
example, are automatically translated into dependency structures and evaluated against
gold-standard dependency banks.
Preiss (2003) uses the grammatical relations and the CBS 500 Dependency Bank
described in Carroll, Briscoe, and Sanfilippo (1998) to compare a number of parsing
systems (Briscoe and Carroll 1993; Collins?s 1997 models 1 and 2; and Charniak 2000)
using a simple version of the conversion-based deep grammar acquisition process (i.e.,
reading off grammatical relations fromCFG parse trees produced by the treebank-based
shallow parsers). The article also reports on a task-based evaluation experiment to rank
the parsers using the grammatical relations as input to an anaphora resolution system.
Preiss concluded that parser ranking using grammatical relations reflected the absolute
ranking (between treebank-induced parsers) using traditional tree-based metrics, but
that the difference between the performance of the parsing algorithms narrowed when
they carried out the anaphora resolution task. Her results show that the hand-crafted
deep unification parser (Briscoe and Carroll 1993) outperforms the machine-learned
parsers (Collins 1997; Charniak 2000) on the f-score derived from weighted precision
and recall on grammatical relations.4 Kaplan et al (2004) compare their deep, hand-
crafted, LFG-based XLE parsing system (Riezler et al 2002) with Collins?s (1999) model
3 using a simple conversion-based approach, capturing dependencies from the tree
output of the machine-learned parser, and evaluating both parsers against the PARC
700 Dependency Bank (King et al 2003). They conclude that the hand-crafted, deep
grammar outperforms the state-of-the-art treebank-based shallow parser on the level of
dependency representation, at the price of a small decrease in parsing speed.
Both Preiss (2003) and Kaplan et al (2004) emphasize that they use rather basic
versions of the conversion-based deep grammar acquisition technology outlined herein.
In this article we revisit the experiments carried out by Preiss and Kaplan et al, this time
using the sophisticated and fine-grained treebank- and annotation-based, deep, probabilis-
tic LFG grammar acquisitionmethodology developed in Cahill et al (2002b), Cahill et al
(2004), O?Donovan et al (2004), and Burke (2006) with a number of surprising results:
1. Evaluating against the PARC 700 Dependency Bank (King et al 2003)
using a retrained version of Bikel?s (2002) parser, the best automatically
induced, deep LFG resources achieve an f-score of 82.73%. This is an
improvement of 3.13 percentage points over the previously best published
results established by Kaplan et al (2004) who use a hand-crafted,
wide-coverage, deep LFG and the XLE parsing system. This is also a
4 The numbers given are difficult to compare as the results for the Briscoe and Carroll (1993) parser were
captured for a richer set of grammatical relations than those for Collins (1997) and Charniak (2000).
84
Cahill et al Statistical Parsing Using Automatic Dependency Structures
statistically significant improvement of 2.18 percentage points over the
most recent improved results presented in this article for the XLE system.
2. Evaluating against the Carroll, Briscoe, and Sanfilippo (1998) CBS 500
gold-standard dependency bank using a retrained version of Bikel?s (2002)
parser, the best Penn-II treebank-based, automatically acquired, deep LFG
resources achieve an f-score of 80.23%. This is a statistically significant
improvement of 3.66 percentage points over Carroll and Briscoe (2002),
who use a hand-crafted, wide-coverage, deep, unification grammar and
the RASP parsing system.
Evaluation results on a reannotated version (Briscoe and Carroll 2006) of the PARC
700 Dependency Bank were recently published in Clark and Curran (2007), reporting
f-scores of 81.9% for the CCG parser, and 76.3% for RASP. As Briscoe and Carroll
(2006) point out, these evaluations are not directly comparable with the Kaplan et al
(2004) style evaluation against the original PARC 700 Dependency Bank, because the
annotation schemes are different.
The article is structured as follows: In Section 2, we outline the automatic LFG
f-structure annotation algorithm and the pipeline parsing architecture of Cahill et al
(2002b), Cahill et al (2004), and Burke (2006). In Section 3, we present our experiment
design. In Section 4, using the DCU 105 Dependency Bank as our development set, we
evaluate a number of treebank-induced LFG parsing systems against the automatically
generated Penn-II WSJ Section 22 Dependency Bank test set. We use the Approximate
Randomization Test (Noreen 1989) to test for statistical significance and choose the best
parsing system for the evaluations against the wide-coverage, hand-crafted RASP and
LFG grammars of Carroll and Briscoe (2002) and Kaplan et al (2004) using the CBS
500 and PARC 700 Dependency Banks in Section 5. In Section 6, we discuss results and
issues raised by our methodology, outline related and future research and conclude in
Section 7.
2. Methodology
In this section, we briefly outline LFG and present our automatic f-structure annotation
algorithm and parsing architecture. The parsing architecture enables us to integrate
PCFG- and history-based parsers, which allows us to compare these parsers at the level
of dependency structures, rather than just trees.
2.1 Lexical Functional Grammar
Lexical Functional Grammar (LFG) (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple
2001) is a constraint-based theory of grammar. It (minimally) posits two levels of
representation, c(onstituent)-structure and f(unctional)-structure. C-structure is rep-
resented by context-free phrase-structure trees, and captures surface grammatical
configurations such as word order. The nodes in the trees are annotated with functional
equations (attribute-value structure constraints, for example (?OBJ)=?) which are
resolved (in the case of well-formed strings) to produce an f-structure. F-structures
are recursive attribute-value matrices, representing abstract syntactic functions, which
85
Computational Linguistics Volume 34, Number 1
Figure 1
C- and f-structures for the sentence U.N. signs treaty.
approximate to basic predicate-argument-adjunct structures or dependency relations.5
Figure 1 shows the c- and f-structures for the string U.N. signs treaty. Each node in the
c-structure is annotated with f-structure equations, for example (? SUBJ)= ?. The
uparrows (?) point to the f-structure associated with the mother node, downarrows
(?) to that of the local node. In a complete parse tree, these ? and ? meta variables are
instantiated to unique tree node identifiers and a set of constraints (a set of terms in an
equality logic) is generated which (if satisfiable) generates an f-structure.
2.2 Automatic F-Structure Annotation Algorithm
Deep grammars can be induced from treebank resources if the treebank encodes
enough information to support the derivation of deep grammatical information, such
as predicate?argument structures, deep dependency relations, or logical forms. Many
second generation treebanks such as Penn-II provide information to support the compi-
lation of meaning representations, for example in the form of traces relating displaced
linguistic material to where it should be interpreted semantically. The f-structure anno-
tation algorithm exploits configurational and categorial information, as well as traces
and the Penn-II functional tag annotations (Table 1) to automatically associate Penn-II
CFG trees with LFG f-structure information.
Given a tree, such as the Penn-II-style tree in Figure 2, the algorithm will traverse
the tree and deterministically add f-structure equations to the phrasal and leaf nodes
of the tree, resulting in an f-structure annotated version of the tree. The annotations are
then collected and passed on to a constraint solver which generates an f-structure (if the
constraints are satisfiable). We use a simple graph-unification-based constraint solver
(Eisele and Do?rre 1986), extended to handle path, set-valued, disjunctive, and existential
constraints. Given parser output without Penn-II style annotations and traces, the same
algorithm is used to assign annotations to each node in the tree, whereas a separate
module is applied at the level of f-structure to resolve any long-distance dependencies
(see Section 2.3).
5 van Genabith and Crouch (1996, 1997) provide translations between f-structures, Quasi-Logical Forms
(QLFs), and Underspecified Discourse Representation Structures (UDRSs).
86
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 1
A complete list of the Penn-II functional labels.
Tag Description
Form/function discrepancies
-ADV clausal and NP adverbials
-NOM non NPs that function as NPs
Grammatical role
-DTV dative
-LGS logical subjects in passives
-PRD non VP predicates
-PUT locative complement of put
-SBJ surface subject
-TPC topicalized and fronted constituents
-VOC vocatives
Adverbials
-BNF benefactive
-DIR direction and trajectory
-EXT extent
-LOC location
-MNR manner
-PRP purpose and reason
-TMP temporal phrases
Miscellaneous
-CLR closely related to verb
-CLF true clefts
-HLN headlines and datelines
-TTL titles
The f-structure annotation algorithm is described in detail in Cahill et al (2002a),
McCarthy (2003), Cahill et al (2004), and Burke (2006). In brief, the algorithm is modular
with four components (Figure 3), taking Penn-II trees as input and automatically adding
LFG f-structure equations to each node in the tree.
Lexical Information. Lexical information is generated automatically by macros for each
of the POS classes in Penn-II. To give a simple example, third-person plural noun
Penn-II POS-word sequences of the form NNS word are automatically associated with
the equations (?PRED) = word?, (?NUM) = pl and (?PERS) = 3rd, where word? is the
lemmatized word.
Left?Right Context Annotation. The Left?Right context annotation component identifies
the heads of Penn-II trees using a modified version of the head finding rules of
Magerman (1994). This partitions each local subtree (of depth one) into a local head, a
left context (left sisters), and a right context (right sisters). The contexts together with
information about the local mother and daughter categories and (if present) Penn-II
87
Computational Linguistics Volume 34, Number 1
Figure 2
Trees for the sentence U.N. signs treaty, the headline said before and after automatic f-structure
annotation, with the f-structure automatically produced.
Figure 3
F-structure annotation algorithm modules.
88
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 2
Sample from an NP Annotation matrix.
Left context Head Right context
DT: (?SPEC DET)=? NN, NNS, NNP, NNPS, NP: RRC, SBAR: (?RELMOD)=?
CD: (?SPEC QUANT)=? ?=? PP: ??(?ADJUNCT)
ADJP, JJ, NN, NNP: ??(?ADJUNCT) NP: ??(?APP)
functional tag labels (Table 1) are used by the f-structure annotation algorithm. For each
Penn-II mother (i.e., phrasal) category an Annotation matrix expresses generalizations
about how to annotate immediate daughters dominated by the mother category relative
to their location in relation to the local head. To give a (much simplified) example,
the head finding rules for NPs state that the rightmost nominal (NN, NNS, NNP, . . . )
not preceded by a comma or ?-?6 is likely to be the local head. The Annotation ma-
trix for NPs states (inter alia) that heads are annotated ?=?, that DTs (determiners)
to the left of the head are annotated (? SPEC DET) = ?, NPs to the right of the head as
??(? APP) (appositions). Table 2 provides a sample extract from the NP Annotation
matrix. Figure 4 provides an example of the application of the NP and PP Annotation
matrices to a simple tree.
For each phrasal category, Annotation matrices are constructed by inspecting the
most frequent Penn-II rule types expanding the category such that the token occurrences
of these rule types cover more than 85% of all occurrences of expansions of that category
in Penn-II. For NP rules, for example, this means that we analyze the most frequent
102 rule types expanding NP, rather than the complete set of more than 6,500 Penn-II
NP rule types, in order to populate the NP Annotation matrix. Annotation matrices
generalize to unseen rule types as, in the case of NPs, these may also feature DTs to
the left of the local head and NPs to the right and similarly for rule types expanding
other categories.
Coordination. In order to support the modularity, maintainability, and extendability of
the annotation algorithm, the Left?Right Annotation matrices apply only to local trees
of depth one, which do not feature coordination. This keeps the statement of Annotation
matrices perspicuous and compact. The Penn-II treatment of coordination is (inten-
tionally) flat. The annotation algorithm has modules for like- and unlike-constituent
coordination. Coordinated constituents are elements of a COORD set and annotated ??
(? COORD). The Coordination module reuses the Left?Right context Annotation ma-
trices to annotate any remaining nodes in a local subtree containing a coordinating
conjunction. Figure 5 provides a VP-coordination example (with right-node-raising).
Catch-All and Clean-Up. The Catch-All and Clean-Up module provides defaults to cap-
ture remaining unannotated nodes (Catch-All) and corrects (Clean-Up) overgeneraliza-
tions resulting from the application of the Left?Right context Annotation matrices. The
Left?Right Annotation matrices are allowed a certain amount of overgeneralization as
this facilitates the perspicuous statement of generalizations and a separate statement of
exceptions, supporting the modularity and maintainability of the annotation algorithm.
PPs under VPs are a case in point. The VP Annotation matrix analyses PPs to the right
of the local VP head as adjuncts: ? ? (?ADJUNCT). The Catch-All and Clean-Up module
6 If the rightmost nominal is preceded by a comma or ?-?, it is likely to be an apposition to the head.
89
Computational Linguistics Volume 34, Number 1
Figure 4
Automatically annotated Penn-II tree (fragment) and f-structure (simplified) for Gerry Purdy,
director of marketing.
uses Penn-II functional tag (Table 1) information (if present), for example -CLR (closely
related to local head), to replace the original adjunct analysis by an oblique argument
analysis: (?OBL)=?. An example of this is provided by the PP-CLR in the left VP-conjunct
in Figure 5. In other cases, argument?adjunct distinctions are encoded configurationally
in Penn-II (without the use of -CLR tags). To give a simple example, the NP Anno-
tation matrix indiscriminately associates SBARs to the right of the local head with
(? RELMOD) = ?. However, some of these SBARs are actually arguments of the local
NP head and, unlike SBAR relative clauses which are Chomsky-adjoined to NP (i.e.,
relative clauses are daughters of an NP mother and sisters of a phrasal NP head), SBAR
arguments are sisters of non-phrasal NP heads.7 In such cases, the Catch-All and Clean-
Up module rewrites the original relative clause analysis into the correct complement
argument analysis (?COMP)=?. Figure 6 shows the COMP f-structure analyses for an
example NP containing an internal SBAR argument (rather than relative clause) node.
Traces. The Traces module translates traces and coindexed material in Penn-II trees
representing long-distance dependencies into corresponding reentrancies at f-structure.
Penn-II provides a rich arsenal of trace types to relate ?displaced? material to where it
7 Structural information of this kind is not encoded in the Annotation matrices; compare Table 2.
90
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Figure 5
Automatically annotated Penn-II tree (fragment) and resulting f-structure for asked for and
received refunds.
should be interpreted semantically. The f-structure annotation algorithm coverswh- and
wh-less relative clause constructions, interrogatives, control and raising constructions,
right-node-raising, and general ICH (interpret constituent here) traces. Figure 5 gives an
example that shows the interplay between coordination, right-node-raising traces and
the corresponding automatically generated reentrancies at f-structure.
2.3 Parsing Architecture
The pipeline parsing architecture of Cahill et al (2004) and Cahill (2004) for parsing raw
text into LFG f-structures is shown in Figure 7. In this model, PCFGs or history-based
lexicalized parsers are extracted from the unannotated treebank and used to parse raw
text into trees. The resulting parse trees are then passed to the automatic f-structure
annotation algorithm to generate f-structures.8
Compared to full Penn-II treebank trees, the output of standard probabilistic
parsers is impoverished: Parsers do not normally output Penn-II functional tag an-
notations (Table 1) nor do they indicate/resolve long-distance dependencies, recorded
8 In the integratedmodel (Cahill et al 2004; Cahill 2004), we extract f-structure annotated PCFGs
(A-PCFGs) from the f-structure annotated treebank, where each non-terminal symbol in the grammar
has been augmented with LFG functional equations, such as NP[?OBJ=?] ? DT[?SPEC=?] NN[?=?].
We treat a non-terminal symbol followed by annotations as a monadic category for grammar extraction
and parsing. Parsing with A-PCFGs results in annotated parse trees, from which an f-structure can be
generated. In this article we only use the pipeline parsing architecture.
91
Computational Linguistics Volume 34, Number 1
Figure 6
Automatically annotated Penn-II tree (fragment) and f-structure for signs that managers
expect declines.
in terms of a fine-grained system of empty productions (traces) and coindexation in
the full Penn-II treebank trees. The f-structure annotation algorithm, as described in
Section 2.2, makes use of Penn-II functional tag information (if present) and relies on
traces and coindexation to capture LDDs in terms of corresponding reentrancies at
f-structure.
Penn-II functional labels are used by the annotation algorithm to discriminate
between adjuncts and (oblique) arguments. PP-sisters to a head verb are analyzed as
arguments iff they are labeled -CLR, -PUT, -DTV or -BNF, for example. Conversely,
functional labels (e.g., -TMP) are also used to analyze certain NPs as adjuncts, and
-LGS labels help to identify logical subjects in passive constructions. In the absence of
functional labels, the annotation algorithm will default to decisions based on simple
structural, configurational, and CFG-category information (and, for example, conserva-
tively analyze a PP sister to a head verb as an adjunct, rather than as an argument).
In Sections 3 and 4 we present a number of treebank-based parsers (in particular the
PCFGs and a version of Bikel?s history-based, lexicalized generative parser) trained to
output CFG categories with Penn-II functional tags. We achieve this through a simple
92
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Figure 7
Treebank-based LFG parsing architecture.
masking and un-masking operation where functional tags are joined with their local
CFG category label to form a new (larger) set of (monadic) CFG category labels (e.g.,
PP-CLR goes to PP CLR) for training and parsing (for Bikel, the parser head-finding rules
are also adjusted to the expanded set of categories). After parsing, the Penn-II functional
tags are unmasked and available to the f-structure annotation algorithm.
The Traces component in the f-structure annotation algorithm (Figure 3) translates
LDDs represented in terms of traces and coindexation in the original Penn-II treebank
trees into corresponding reentrancies at f-structure. Most probabilistic treebank-based
parsers, however, do not indicate/resolve LDDs, and the Traces component of the an-
notation algorithm does not apply. Initially, the f-structures produced for parser output
trees in the architecture in Figure 7 are therefore LDD-unresolved: They are incomplete
(or proto) f-structures, where displaced material (e.g., the values of FOCUS, TOPIC, and
TOPICREL attributes [wh- and wh-less relative clauses, topicalization, and interrogative
constructions] at f-structure) is not yet linked to the appropriate argument grammati-
cal functions (or elements of adjunct sets) for the governing local PRED. A dedicated
LDD Resolution component in the architecture in Figure 7 turns parser output proto-
f-structures into fully LDD-resolved proper f-structures, without traces and coindexa-
tion in parse trees.
Consider the following fragment of a proper Penn-II treebank tree (Figure 8), where
the LDD between the WHNP in the relative clause and the embedded direct object
position of the verb reward is indicated in terms of the trace *T*-3 and its coindexation
with the antecedent WHNP-3. Note further that the control relation between the subject
of the verbs wanted and reward is similarly expressed in terms of traces (*T*-2) and
coindexation (NP-SBJ-2). From the treebank tree, the f-structure annotation algorithm
is able to derive a fully resolved f-structure where the LDD and the control relation are
captured in terms of corresponding reentrancies (Figure 9).
93
Computational Linguistics Volume 34, Number 1
Figure 8
Penn-II treebank tree with LDD indicated in terms of traces (empty productions) and
coindexation and f-structure annotations generated by the annotation algorithm.
Now consider the corresponding ?impoverished? (but otherwise correct) parser
output tree (Figure 10) for the same string: The parser output does not explicitly record
the control relation nor the LDD.
Given this parser output tree, prior to the LDD resolution component in the parsing
architecture (Figure 7), the f-structure annotation algorithmwould initially construct the
partial (proto-) f-structure in Figure 11, where the LDD indicated by the TOPICREL func-
tion is unresolved (i.e., the value of TOPICREL is not coindexedwith the OBJ grammatical
function of the embedded verb reward). The control relation (shared subject between
the two verbs in the relative clause) is in fact captured by the annotation algorithm in
terms of a default annotation (? SUBJ) = (? SUBJ) on sole argument VPs to the right of
head verbs (as often, even in the full Penn-II treebank trees, control relations are not
consistently captured through explicit argument traces).
In LFG, LDD resolution operates at the level of f-structure, using functional un-
certainty equations (regular expressions over paths in f-structure [Kaplan and Zaenen
1989] relating f-structure components in different parts of an f-structure), obviating
traces and coindexation in c-structure trees. For the example in Figure 10, a functional
uncertainty equation of the form (?TOPICREL) = (?[COMP|XCOMP]? [SUBJ|OBJ]) would
be associated with the WHNP daughter node of the SBAR relative clause. The equation
states that the value of the TOPICREL attribute is token-identical (re-entrant) with the
value of a SUBJ or OBJ function, reached through a path along any number (including
94
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Figure 9
Fully LDD-resolved f-structure.
Figure 10
Impoverished parser output tree: LDDs not captured.
zero) of COMP or XCOMP attributes. This equation, together with subcategorization
frames (LFG semantic forms) for the local PREDs and the usual LFG completeness and
coherence conditions, resolve the partial proto-f-structure in Figure 11 into the fully
LDD-resolved proper f-structure in Figure 9.
95
Computational Linguistics Volume 34, Number 1
Figure 11
Proto-f-structure: LDDs not captured.
Following Cahill et al (2004), in our parsing architecture (Figure 7) we model
LFG LDD resolution using automatically induced finite approximations of functional-
uncertainty equations and subcategorization frames from the f-structure-annotated
Penn-II treebank (O?Donovan et al 2004) in an LDD resolution component. From the
fully LDD-resolved f-structures from the Penn-II training section treebank trees we
learn probabilistic LDD resolution paths (reentrancies in f-structure), conditional on
LDD type (Table 3), and subcategorization frames, conditional on lemma (and voice)
(Table 4). Table 3 lists the eight most probable TOPICREL paths (out of a total of 37
TOPICREL paths acquired). The totality of these paths constitutes a finite subset of the
reference language definde by the full functional uncertainty equation (?TOPICREL) =
(?[COMP|XCOMP]? [SUBJ|OBJ]). Given an unresolved LDD type (such as TOPICREL in
the parser output for the relative clause example in Figure 11), admissible LDD res-
olutions assert a reentrancy between the value of the LDD trigger (here, TOPICREL)
and a grammatical function (or adjunct set element) of an embedded local predicate,
subject to the conditions that (i) the local predicate can be reached from the LDD trigger
using the LDD path; (ii) the grammatical function terminates the LDD path; (iii) the
grammatical function is not already present (at the relevant level of embedding in
the local f-structure); and (vi) the local predicate subcategorizes for the grammatical
function in question.9 Solutions satisfying (i)?(iv) are ranked using the product of
LDD path and subcategorization frame probabilities and the highest ranked solution
(possibly involving multiple interacting LDDs for a single f-structure) is returned by
the algorithm (for details and comparison against alternative LDD resolution methods,
see Cahill et al 2004).10
For our example (Figure 11), the highest ranked LDD resolution is for LDD path
(?TOPICREL) = (? XCOMP OBJ) and the local subcat frame REWARD?? SUBJ, ? OBJ?. This
9 Conditions (i)?(iv) are suitably adapted for LDD resolutions terminating in adjunct sets.
10 In our experiments we do not use the limited LDD resolution for wh-phrases provided by Collins?s Model
3 parser as better results are achieved using the purely f-structure-based LDD resolution as shown in
Cahill et al (2004).
96
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 3
Most frequent wh-TOPICREL paths.
wh-TOPICREL Probability wh-TOPICREL Probability
subj .7583 xcomp .0830
obj .0458 xcomp:obj .0338
xcomp:xcomp .0168 xcomp:subj .0109
comp .0097 comp:subj .0073
Table 4
Most frequent semantic forms for active and passive (p) occurrences of the verb want and
reward.
Semantic form Probability
want([subj,xcomp]) .6208
want([subj,obj]) .2496
want([subj,obj,xcomp]) .1008
want([subj]) .0096
want([subj,obj,obl]) .0048
want([subj,obj,part]),p) .5000
want([subj,obl]),p) .1667
want([subj,part]),p) .1667
want([subj]),p) .1667
reward([subj,obj]) .8000
reward([subj,obj,obl]) .2000
reward([subj]),p) 1.0000
(together with the subject control equation described previously) turns the parser-
output proto-f-structure (in Figure 11) into the fully LDD resolved f-structure in
(Figure 9).
The full pipeline parsing architecture with the LDD resolution (rather than the
Traces component for LDD resolved Penn-II treebank trees) component (and the LDD
path and subcategorization frame extraction) is given in Figure 7.
The pipeline architecture supports flexible integration of treebank-based PCFGs
or state-of-the-art, history-based, and lexicalized parsers (Collins 1999; Charniak 2000;
Bikel 2002) and enables dependency-based evaluation of such parsers.
3. Experiment Design
In our experiments we compare four history-based parsers for integration into the
pipeline parsing architecture described in Section 2.3:
 Collins?s 1999 Models 311
 Charniak?s 2000 maximum-entropy inspired parser12
11 Downloaded from ftp://ftp.cis.upenn.edu/pub/mcollins/PARSER.tar.gz.
12 Downloaded from ftp://ftp.cs.brown.edu/pub/nlparser/.
97
Computational Linguistics Volume 34, Number 1
 Bikel?s 2002 emulation of Collins Model 213
 a retrained version of Bikel?s (2002) parser which retains Penn-II functional
tags
Input for Collins?s and Bikel?s parsers was pre-tagged using the MXPOST POS tag-
ger (Ratnaparkhi 1996). Charniak?s parser provides its own POS tagger. The combined
system of best history-based parser and automatic f-structure annotation is compared
to two probabilistic parsing systems based on hand-crafted, wide-coverage, constraint-
based, deep grammars:
 the RASP parsing system (Carroll and Briscoe 2002)
 the XLE parsing system (Riezler et al 2002; Kaplan et al 2004)
Both hand-crafted grammars perform their own POS tagging, resolve LDDs, and
associate strings with dependency relations (in the form of grammatical relations or
LFG f-structures).
We evaluate the parsers against a number of gold-standard dependency banks.
We use the DCU 105 Dependency Bank (Cahill et al 2002a) as our development set
for the treebank-based LFG parsers. We use the f-structure annotation algorithm to
automatically generate a gold-standard test set from the original Section 22 treebank
trees (the f-structure annotated WSJ Section 22 Dependency Bank) to choose the best
treebank-based LFG parsing systems for the PARC 700 and CBS 500 experiments.
Following the experimental setup in Kaplan et al (2004), we use the Penn-II Section 23-
based PARC 700 Dependency Bank (King et al 2003) to evaluate the treebank-induced
LFG resources against the hand-crafted XLE grammar and parsing system of Riezler
et al (2002) and Kaplan et al Following Preiss (2003), we use the SUSANNE Based CBS
500 Dependency Bank (Carroll, Briscoe, and Sanfilippo 1998) to evaluate the treebank-
induced LFG resources against the hand-crafted RASP grammar and parsing system
(Carroll and Briscoe 2002) as well as against the XLE system (Riezler et al 2002).
For each gold standard, our experiment design is as follows: We parse automati-
cally tagged input14 sentences with the treebank- and machine-learning-based parsers
trained on WSJ Sections 02?21 in the pipeline architecture, pass the resulting parse
trees to our automatic f-structure annotation algorithm, collect the f-structure equations,
pass them to a constraint-solver which generates an f-structure, resolve long-distance
dependencies at f-structure following Cahill et al (2004) and convert the resulting LDD-
resolved f-structures into dependency representations using the formats and software
of Crouch et al (2002) (for the DCU 105, PARC 700, and WSJ Section 22 evaluations)
and the formats and software of Carroll, Briscoe, and Sanfilippo (1998) (for the CBS
500 evaluation). In the experiments we did not use any additional annotations such as
-A (for argument) that can be generated by some of the history-based parsers (Collins
1999) as the f-structure annotation algorithm is designed for Penn-II trees (which do
not contain such annotations). We also did not use the limited LDD resolution for wh-
relative clauses provided by Collins?s Model 3 as better results are achieved by LDD
13 This was developed at the University of Pennsylvania by Dan Bikel and is freely available to download
from http://www.cis.upenn.edu/?dbikel/software.html.
14 Tags were automatically assigned either by the parsers themselves or by the MXPOST tagger
(Ratnaparkhi 1996).
98
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 5
Results of tree-based evaluation on all sentences WSJ section 23, Penn-II.
Parser Labeled
f-score (%)
PCFG 73.03
Parent-PCFG 78.05
Collins M3 88.33
Charniak 89.73
Bikel 88.32
Bikel+Tags 87.53
resolution on f-structure (Cahill et al 2004). A complete set of parameter settings for the
parsers is provided in the Appendix.
In order to evaluate the treebank-induced LFG resources against the PARC 700
and the CBS 500 dependency banks, a certain amount of automatic mapping is re-
quired to account for systematic differences in linguistic analysis, feature geometry,
and nomenclature at the level of dependencies. This is discussed in Sections 5.1 and
5.2. Throughout, we use the Approximate Randomization Test (Noreen 1989) to test the
statistical significance of the results.
4. Choosing a Treebank-Based LFG Parsing System
In this section, we choose the best treebank-based LFG parsing system for the compar-
isons with the hand-crafted XLE and RASP resources in Section 5. We use the DCU
105 Dependency Bank as our development set and carry out comparative evaluation
and statistical significance testing on the larger, automatically generated WSJ Section 22
Dependency Bank as a test set. The system based on Bikel?s (2002) parser retrained to
retain Penn-II functional tags (Table 1) achieves overall best results.
4.1 Tree-Based Evaluation against WSJ Section 23
For reference, we include the traditional CFG-tree-based comparison for treebank-
induced parsers. The parsers are trained on Sections 02 to 21 of the Penn-II Treebank and
tested on Section 23. The published results15 on these experiments for the history-based
parsers are given in Table 5. We also include figures for a PCFG and a Parent-PCFG (a
PCFG which has undergone the parent transformation [Johnson 1999]). These PCFGs
are induced following standard treebank preprocessing steps, including elimination of
empty nodes, but following Cahill et al (2004), they do include Penn-II functional tags
(Table 1), as these tags contain valuable information for the automatic f-structure anno-
tation algorithm (Section 2.2). These tags are removed for the tree-based evaluation.
The results show that the history-based parsers produce considerably better trees
than the more basic PCFGs (with and without parent transformations). Charniak?s
(2000) parser scores best with an f-score of 89.73% on all sentences in Section 23. The
15 Where there were no published results available for Section 23, we calculated them using the
downloadable versions of the parsers.
99
Computational Linguistics Volume 34, Number 1
vanilla PCFG achieves the lowest f-score of 73.03%, a difference of 16.7 percentage
points. The hand-crafted XLE and RASP grammars achieve around 80% coverage
(measured in terms of complete spanning parse) on Section 23 and use a variety of
(longest) fragments combining techniques to generate dependency representations for
the remaining 20% of Section 23 strings. By contrast, the treebank-induced PCFGs and
history-based parsers all achieve coverage of over 99.9%. Given that the history-based
parsers score considerably better than PCFGs on trees, we would also expect them to
produce dependency structures of substantially higher quality.
4.2 Using DCU 105 as a Development Set
The DCU 105 (Cahill et al 2002a) is a hand-crafted gold-standard dependency bank
for 105 sentences, randomly chosen from Section 23 of the Penn-II Treebank.16 This is a
relatively small gold standard, initially developed to evaluate the automatic f-structure
annotation algorithm. We parse the 105 tagged sentences into LFG f-structures with
each of the treebank-induced parsers in the pipeline parsing and f-structure annotation
architecture. The f-structures of the gold standard and the f-structures returned by the
parsing systems are converted into dependency triples following Crouch et al (2002)
and Riezler et al (2002) and we also use their software for evaluation. The following
dependency triples are produced by the f-structure in Figure 1:
subj(sign?0,U.N.?1)
obj(sign?0,treaty?2)
num(U.N.?1,sg)
pers(U.N.?1,3)
num(treaty?2,sg)
pers(treaty?3,3)
tense(sign?0,present)
We evaluate preds-only f-structures (i.e., where paths in f-structures end in a PRED
value: the predicate-argument-adjunct structure skeleton) and all grammatical func-
tions (GFs) including number, tense, person, and so on. The results are given in Table 6.
With one main exception, Tables 5 and 6 confirm the general expectation that
the better the trees produced by the parsers, the better the f-structures automatically
generated for those trees. The exception is Bikel+Tags. The automatic f-structure an-
notation algorithm will exploit Penn-II functional tag information if present to generate
appropriate f-structure equations (see Section 2.2). It will default to possibly less reliable
configurational and categorial information if Penn-II tags are not present in the trees.
In order to test whether the retention of Penn-II functional labels in the history-
based parser output will improve LFG f-structure-based dependency results, we use
Bikel?s (2002) training software,17 and retrain the parser on a version of the Penn-II
treebank (Sections 02 to 21) with the Penn-II functional tag labels (Table 1) annotated
in such a way that the resulting history-based parser will retain them (Section 2.3). The
retrained parser (Bikel+Tags) then produces CFG-trees with Penn-II functional labels
and these are used by the f-structure annotation algorithm. We evaluate the f-structure
dependencies against the DCU 105 (Table 6) and achieve an f-score of 82.92% preds-only
16 It is publicly available for download from: http://nclt.computing.dcu.ie/gold105.txt.
17 We use Bikel?s software rather than Charniak?s for this experiment as the former proved more stable
during the retraining phase.
100
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 6
Treebank-induced parsers: results of dependency-based evaluation against DCU 105.
Parser Preds only All GFs
f-score (%) f-score (%)
PCFG 70.24 79.90
Parent-PCFG 75.84 83.58
Collins M3 77.84 85.08
Charniak 79.61 85.66
Bikel 79.39 86.56
Bikel+Tags 82.92 88.30
Table 7
Treebank induced parsers: breakdown by dependency relation of preds-only evaluation against
DCU 105.
Dep. Percent of total F-score (%)
Parent-PCFG Collins M3 Charniak Bikel Bikel+Tags
ADJUNCT 33.73 71 72 76 73 79
APP 0.68 61 0 55 70 65
COMP 2.31 60 61 66 61 73
COORD 5.73 64 73 77 67 76
DET 9.58 91 93 96 96 96
FOCUS 0.04 100 100 0 100 100
OBJ 16.42 82 84 86 85 90
OBJ2 0.07 80 57 50 57 50
OBL 2.17 58 24 27 23 63
OBL2 0.07 50 0 0 0 67
OBL AG 0.43 40 96 96 92 92
POSS 2.88 80 83 82 82 79
QUANT 1.85 70 67 69 70 70
RELMOD 1.78 50 78 67 78 73
SUBJ 14.74 80 81 83 85 85
TOPIC 0.46 85 87 96 96 89
TOPICREL 1.85 61 80 80 79 74
XCOMP 5.20 90 92 79 93 93
and 88.3% all GFs. A detailed breakdown by dependency is given in Table 7. The system
based on the retrained parser is nowmuch better able to identify oblique arguments and
overall preds-only accuracy has improved by 3.53% over the original Bikel experiment
and 3.31% over Charniak?s parser, even though Charniak?s parser performs more than
2% better on the tree-based scores in Table 5 and even though the retrained parser drops
0.79% against the original Bikel parser on the tree-based scores.18
Inspection of the results broken down by grammatical function (Table 7) for the
preds-only evaluation against the DCU 105 shows that just over one third of all depen-
dency triples in the gold standard are adjuncts. SUBJ(ects) and OBJ(ects) together make
up a further 30%.
18 The figures suggest that retraining Charniak?s parser to retain Penn-II functional tags is likely to produce
even better dependency scores than those achieved by Bikel?s retrained parser.
101
Computational Linguistics Volume 34, Number 1
Table 7 shows that the treebank-based LFG system using Collins?s Models 3 is
unable to identify APP(osition). This is due to Collins?s treatment of punctuation and
the fact that punctuation is often required to reliably identify apposition.19 None of
the original history-based parsers produced trees which enabled the annotation algo-
rithm to identify second oblique dependencies (OBL2), and they generally performed
considerably worse than Parent-PCFG when identifying OBL(ique) dependencies. This
is because the automatic f-structure annotation algorithm is cautious to the point of
undergeneralization when identifying oblique arguments. In many cases, the algorithm
relies on the presence of, for example, a -CLR Penn-II functional label (indicating that the
phrase is closely related to the verb), and the history-based (Collins M3, Charniak, and
Bikel) parsers do not produce these labels, whereas Parent-PCFG (as well as PCFG) are
trained to retain Penn-II functional labels. Parent-PCFG, by contrast, performs poorly
for oblique agents (OBL AG, agentive by-phrases in passive constructions), whereas the
history-based parsers are able to identify these with considerable accuracy. This is be-
cause Parent-PCFG often erroneously finds oblique agents, even when the preposition
is not by, as it never has enough context in which to distinguish by prepositional phrases
from other PPs. The history-based parsers produce trees from which the automatic
f-structure annotation algorithm can better identify RELMOD and TOPICREL dependen-
cies than Parent-PCFG. This, in turn, leads to improved long distance dependency
resolution which improves overall accuracy.
The DCU 105 development set is too small to support reliable statistical significance
testing of the performance ranking of the six treebank-based LFG parsing systems. In
order to carry out significance testing to select the best treebank-based LFG parsing
system for comparative evaluation against the hand-crafted deep XLE and RASP re-
sources, we move to a larger dependency-based evaluation data set: the gold-standard
dependency bank automatically generated fromWSJ Section 22.
4.3 Evaluation against WSJ Section 22 Dependencies
In an experimental setup similar to that of Hockenmaier and Steedman (2002),20 we
evaluate each parser against a large automatically generated gold standard. The gold-
standard dependency bank is automatically generated by annotating the original 1,700
treebank trees from WSJ Section 22 of the Penn-II Treebank with our f-structure an-
notation algorithm. We then evaluate the f-structures generated from the tree output
of the six parsers trained on Sections 02 to 21 resulting from parsing the Section 22
strings against the automatically produced f-structures for the original Section 22 Penn-II
treebank trees. The results are given in Table 8.
Compared to Table 6 for the DCU 105 gold standard, most scores are up, particularly
so for the history-based parsers. This trend is possibly due to the fact that the WSJ
19 The annotation algorithm relies on Penn-II-style punctuation patterns where an NP apposition follows a
nominal head separated by a comma ([NP [NP Bush ] , [NP the president ] ]), all three sisters of the same
mother node, while the trees produced by Collins?s parser attach the comma low in the tree ([NP [NP
Bush,] [NP the president ] ]). Although it would be trivial to carry out a tree transformation on the Collins
output to raise the punctuation to the expected level, we have not done this here.
20 This corresponds to experiments where the original Penn-II Section 23 treebank trees are automatically
converted into CCG derivations, which are then used as a gold standard to evaluate the CCG parser
trained on Sections 02?21. A similar methodology is used for the evaluation of treebank-based HPSG
resources (Miyao, Ninomiya, and Tsujii 2003) where Penn-II treebank trees are automatically annotated
with HPSG typed-feature structure information.
102
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 8
Results of dependency-based evaluation against the automatically generated gold standard for
WSJ Section 22.
Parser Preds only All GFs
f-score (%) f-score (%)
PCFG 70.76 80.44
Parent-PCFG 74.92 83.04
Collins M3 79.30 86.00
Charniak 81.35 86.96
Bikel 81.40 87.00
Bikel+Tags 83.06 87.63
Section 22 gold standard is generated automatically from the original ?perfect? Penn-II
treebank trees using the automatic f-structure annotation algorithm, whereas the DCU
105 has been created manually without regard as to whether or not the f-structure
annotation algorithm could ever generate the f-structures, even given the ?perfect?
trees.
The LFG system based on Bikel?s retrained parser achieves the highest f-score of
83.06% preds-only and 87.63% all GFs. Parent-PCFG achieves an f-score of 74.92%
preds-only and 83.04% all GFs. Table 9 provides a breakdown by feature of the preds-
only evaluation.
Table 9 shows that, once again, the automatic f-structure annotation algorithm is
not able to identify any cases of apposition from the output of Collins?s Model 3 parser.
Apart from Bikel?s retrained parser, none of the history-based parsers are able to identify
Table 9
Breakdown by dependency of results of preds-only evaluation against the automatically
generated Section 22 gold standard.
Dep. Percent of total F-score (%)
Parent-PCFG Collins M3 Charniak Bikel Bikel+Tags
ADJUNCT 33.77 70 75 78 78 80
APP 0.74 61 0 77 77 71
COMP 1.35 60 72 70 70 80
COORD 5.11 74 78 82 82 81
DET 10.72 88 91 92 92 91
FOCUS 0.02 27 67 88 88 71
OBJ 16.17 80 85 87 87 88
OBJ2 0.07 15 30 32 32 71
OBL 1.92 50 19 21 21 73
OBL2 0.07 47 3 3 3 69
OBL AG 0.31 50 90 89 89 85
POSS 2.47 86 91 91 91 91
QUANT 2.12 89 89 93 93 92
RELMOD 1.84 51 71 72 72 69
SUBJ 15.45 75 80 81 81 82
TOPIC 0.44 81 85 84 84 76
TOPICREL 1.82 61 72 74 75 69
XCOMP 5.62 81 87 81 81 88
103
Computational Linguistics Volume 34, Number 1
Figure 12
Approximate Randomization Test for statistical significance testing.
OBJ2, OBL or OBL2 dependencies very well, although Parent-PCFG is able to produce
trees from which it is easier to identify obliques (OBL), because of the Penn-II functional
-CLR label. The automatic annotation algorithm is unable to identify RELMOD depen-
dencies satisfactorily from the trees produced by parsing with Parent-PCFG, although
the history-based parsers score reasonably well for this function. Whereas Charniak?s
parser is able to identify some dependencies better than Bikel?s retrained parser, overall
the system based on Bikel?s retrained parser performs better when evaluating against
the dependencies in WSJ Section 22.
In order to determine whether the results are statistically significant, we use the Ap-
proximate Randomization Test (Noreen 1989).21 This test is an example of a computer-
intensive statistical hypothesis test. Such tests are designed to assess result differences
with respect to a test statistic in cases where the sampling distribution of the test statistic
is unknown. Comparative evaluations of outputs of parsing systems according to test
statistics, such as differences in f-score, are examples of this situation. The test statistics
are computed by accumulating certain count variables over the sentences in the test
set. In the case of f-score, variable tuples consisting of the number of dependency-
relations in the parse for the system translation, the number of dependency-relations
in the parse for the reference translation, and the number of matching dependency-
relations between system and reference parse, are accumulated over the test set.
Under the null hypothesis, the compared systems are not different, thus any vari-
able tuple produced by one of the systems could just as likely have been produced by
the other system. So shuffling the variable tuples between the two systems with equal
probability, and recomputing the test statistic, creates an approximate distribution of
the test statistic under the null hypothesis. For a test set of S sentences there are 2S
different ways to shuffle the variable tuples between the two systems. Approximate
randomization produces shuffles by random assignments instead of evaluating all 2S
possible assignments. Significance levels are computed as the percentage of trials where
the pseudo statistic, that is the test statistic computed on the shuffled data, is greater
than or equal to the actual statistic, that is the test statistic computed on the test data. A
sketch of an algorithm for approximate randomization testing is given in Figure 12.
21 Applications of this test to natural language processing problems can be found in Chinchor et al (1993)
and Yeh (2000).
104
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 10
Comparing parsers evaluated against Section 22 dependencies (preds-only): p-values for
approximate randomization test for 10,000,000 randomizations.
PCFG Parent-PCFG Collins M3 Charniak Bikel Bikel+Tags
PCFG - - - - - -
Parent-PCFG <.0001 - - - - -
Collins M3 <.0001 <.0001 - - - -
Charniak <.0001 <.0001 <.0001 - - -
Bikel <.0001 <.0001 <.0001 .0003 - -
Bikel+Tags <.0001 <.0001 <.0001 <.0001 <.0001 -
Table 10 gives the p-values (the smallest fixed level at which the null hypothesis can
be rejected) for comparing each parser against all of the other parsers. We test for sig-
nificance at the 95% level. Because we are doing a pairwise comparison of six systems,
giving 15 comparisons, the p-value needs to be below .0034 for there to be a significant
difference at the 95% level.22 For each parser, the values in the row corresponding to
that parser represent the p-values for those parsers that achieve a lower f-score than
that parser. This shows that the system based on Bikel?s retrained parser is significantly
better than those based on the other parsers with a statistical significance of >95%. For
the XLE and RASP comparisons, we will use the f-structure-annotation algorithm and
Bikel retrained-based LFG system.
5. Cross-Formalism Comparison of Treebank-Induced and Hand-Crafted Grammars
From the experiments in Section 4, we choose the treebank-based LFG system using
the retrained version of Bikel?s parser (which retains Penn-II functional tag labels) to
compare against parsing systems using deep, hand-crafted, constraint-based grammars
at the level of dependencies. We report on two experiments. In the first experiment
(Section 5.1), we evaluate the f-structure annotation algorithm and Bikel retrained
parser-based LFG system against the hand-crafted, wide-coverage LFG and XLE pars-
ing system (Riezler et al 2002; Kaplan et al 2004) on the PARC 700 Dependency Bank
(King et al 2003). In the second experiment (Section 5.2), we evaluate against the hand-
crafted, wide-coverage unification grammar and RASP parsing system of Carroll and
Briscoe (2002) on the CBS 500 Dependency Bank (Carroll, Briscoe, and Sanfilippo 1998).
5.1 Evaluation against PARC 700
The PARC 700 Dependency Bank (King et al 2003) provides dependency relations
(including LDD relations) for 700 sentences randomly selected from WSJ Section 23 of
the Penn-II Treebank. In order to evaluate the parsers, we follow the experimental setup
of Kaplan et al (2004) with a split of 560 dependency structures for the test set and 140
for the development set. The set of features (Table 12, later in this article) evaluated
in the experiment form a proper superset of preds-only, but a proper subset of all
22 Based on Cohen (1995, p. 190): ?e ? 1 ? (1 ? ?c )m, where m is the number of pairwise comparisons, ?e is
the experiment-wise error, and ?c is the per-comparison error.
105
Computational Linguistics Volume 34, Number 1
Figure 13
PARC 700 conversion software.
grammatical functions (preds-only ? PARC ? all GFs). This feature set was selected in
Kaplan et al because the features carry important semantic information. There are sys-
tematic differences between the PARC 700 dependencies and the f-structures generated
in our approach as regards feature geometry, feature nomenclature, and the treatment
of named entities. In order to evaluate against the PARC 700 test set, we automatically
map the f-structures produced by our parsers to a format similar to that of the PARC
700 Dependency Bank. This is done with conversion software in a post-processing stage
on the f-structure annotated trees (Figure 13).
The conversion software is developed on the 140-sentence development set of the
PARC 700, except for the Multi-Word Expressions section. Following the experimental
setup of Kaplan et al (2004), we mark up multi-word expression predicates based on
the gold-standard PARC 700 Dependency Bank.
Multi-Word Expressions The f-structure annotation algorithm analyzes the internal
structure of all noun phrases fully. In Figure 14, for example, BT is analyzed as
an ADJUNCT modifier of the head securities, whereas PARC 700 analyzes this and
other (more complex) named entities as multi-word expression predicates. The
conversion software transforms the output of the f-structure annotation algorithm
into the multi-word expression predicate format.
Feature Geometry In constructions such as Figure 2, the f-structure annotation algo-
rithm analyzes say as the main PRED with what is said as the value of a COMP
argument. In the PARC 700, these constructions are analyzed in such a way that
what is said/reported provides the top level f-structure whereas other material
(who reported, etc.) is analyzed in terms of ADJUNCTs modifying the top level
f-structure. A further systematic structural divergence is provided by the analysis
Figure 14
Named entity and OBL AG feature geometry mapping.
106
Cahill et al Statistical Parsing Using Automatic Dependency Structures
of passive oblique agent constructions (Figure 14): The f-structure annotation
algorithm generates a complex internal analysis of the oblique agent PP, whereas
the PARC analysis encodes a flat representation. The conversion software adjusts
the output of the f-structure annotation algorithm to the PARC-style encoding of
linguistic information.
Feature Nomenclature There are a number of systematic differences between feature
names used by the automatic annotation algorithm and PARC 700: For example,
DET is DET FORM in the PARC 700, COORD is CONJ, FOCUS is FOCUS INT. Nomen-
clature differences are treated in terms of a simple relabeling by the conversion
software.
Additional Features A number of features in the PARC 700 are not produced by the au-
tomatic annotation algorithm. These include: AQUANT for adjectival quantifiers,
MOD for NP-internal modifiers, and STMT TYPE for statement type (declarative,
interrogative, etc.). Additional features (and their values) are automatically gen-
erated by the mapping software, using categorial, configurational, and already
produced f-structure annotation information, extending the original annotation
algorithm.
XCOMP Flattening The automatic annotation algorithm treats both auxiliary and
modal verb constructions in terms of hierarchically cascading XCOMPs, whereas
in PARC 700 the temporal and aspectual information expressed by auxiliary verbs
is represented in terms of a flat analysis and features (Figure 15). The conversion
software automatically flattens the f-structures produced by the automatic anno-
tation algorithm into the PARC-style encoding.
For full details of the mapping, see Burke et al (2004).
In our parsing experiments, we used the most up-to-date version of the hand-
crafted, wide-coverage, deep LFG resources and XLE parsing system with improved
results over those reported in Kaplan et al (2004): This latest version achieves 80.55%
f-score, a 0.95 percentage point improvement on the previous 79.6%. The XLE parsing
system combines a large-scale, hand-crafted LFG for English and a statistical disam-
biguation component to choose the most likely analysis among those returned by
the symbolic parser. The statistical component is a log-linear model trained on 10,000
partially labeled structures from the WSJ. The results of the parsing experiments are
presented in Table 11. We also include a figure for the upper bound of each system.23
Using Bikel?s retrained parser, the treebank-based LFG system achieves an f-score of
82.73%, and the hand-crafted grammar and XLE-based system achieves an f-score
of 80.55%. The approximate randomization test produced a p-value of .0054 for this
pairwise comparison, showing that this result difference is statistically significant at
the 95% level. Evaluation results on a reannotated version (Briscoe and Carroll 2006) of
the PARC 700 Dependency Bank were recently published in Clark and Curran (2007),
reporting f-scores of 81.9% for the CCG parser, and 76.3% for RASP. As Briscoe and
23 The upper bound for the treebank-based LFG system is determined by taking the original Penn-II WSJ
Section 23 trees corresponding to the PARC 700 strings, automatically annotating them with the
f-structure annotation algorithm, and evaluating the f-structures against the PARC 700 dependencies. The
upper bound for the XLE system is determined by selecting the XLE parse that scores best against the
PARC 700 dependencies for each of the PARC 700 strings. It is interesting to note that the upper bound
for the treebank-based system is only 1.18 percentage points higher than that for the XLE system. Apart
from the two different methods for establishing the upper bounds, this is most likely due to the fact that
the mapping required for evaluating the treebank-based LFG system against PARC 700 is lossy (cf. the
discussion in Section 6).
107
Computational Linguistics Volume 34, Number 1
Figure 15
DCU 105 and PARC 700 analyses for the sentence Unlike 1987, interest rates have been falling
this year.
Carroll point out, these evaluations are not directly comparable with the Kaplan et al
(2004) style evaluation against the original PARC 700 Dependency Bank, because the
annotation schemes are different. Kaplan et al and our experiments use a fine-grained
feature set of 34 features (Table 12), while the Briscoe and Carroll scheme uses 17
features.
A breakdown by dependency relation for each system is given in Table 12. The
treebank-induced grammar system can better identify DET FORM, SUBORD FORM, and
Table 11
Results of evaluation against the PARC 700 Dependency Bank following the experimental setup
of Kaplan et al (2004).
Bikel+Tags XLE p-Value
F-score 82.73 80.55 .0054
Upper bound 86.83 85.65 -
108
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 12
Breakdown by dependency relation of results of evaluation against PARC 700.
Dep. Percent of deps. F-score (%)
Bikel+Tags XLE
ADEGREE 6.17 80 82
ADJUNCT 14.32 68 66
AQUANT 0.06 78 61
COMP 1.23 80 74
CONJ 2.64 73 69
COORD FORM 1.20 83 90
DET FORM 4.61 97 91
FOCUS 0.02 0 36
MOD 2.74 74 67
NUM 19.82 91 89
NUMBER 1.42 89 83
NUMBER TYPE 2.10 94 86
OBJ 8.92 87 78
OBJ THETA 0.05 43 31
OBL 0.83 55 69
OBL AG 0.22 82 76
OBL COMPAR 0.07 38 56
PASSIVE 1.14 80 88
PCASE 0.25 79 68
PERF 0.41 89 90
POSS 0.98 88 80
PRECOORD FORM 0.03 0 91
PROG 0.97 89 81
PRON FORM 2.54 92 94
PRON INT 0.03 0 33
PRON REL 0.57 74 72
PROPER 3.56 83 93
PRT FORM 0.22 80 41
QUANT 0.34 77 80
STMT TYPE 5.23 87 80
SUBJ 8.51 78 78
SUBORD FORM 0.93 47 42
TENSE 5.02 95 90
TOPIC REL 0.57 56 73
XCOMP 2.29 80 78
PRT FORM dependencies24 and achieves higher f-scores for OBJ and POSS. However, the
hand-crafted parsing system can better identify FOCUS, OBL(ique) arguments, PRECO-
ORD FORM and TOPICREL relations.
5.2 Evaluation against CBS 500
We also compare the hand-crafted, deep, probabilistic unification grammar-based RASP
parsing system of Carroll and Briscoe (2002) to our treebank- and retrained Bikel
24 DET FORM, SUBORD FORM, and PRT FORM (and in general X FORM) dependencies record (semantically
relevant) surface forms in f-structure for X-type closed class categories.
109
Computational Linguistics Volume 34, Number 1
Figure 16
CBS 500 conversion software.
parser-based LFG system. The RASP parsing system is a domain-independent, robust
statistical parsing system for English, based on a hand-written, feature-based unification
grammar. A probabilistic parse selection model conditioned on the structural parse
context, degree of support for a subanalysis in the parse forest, and lexical informa-
tion (when available) chooses the most likely parses. For this experiment, we evaluate
against the CBS 500,25 developed by Carroll, Briscoe, and Sanfilippo (1998) in order to
evaluate a precursor of the RASP parsing resources. The CBS 500 contains dependency
structures (including some long distance dependencies26) for 500 sentences chosen at
random from the SUSANNE corpus (Sampson 1995), but subject to the constraint that
they are parsable by the parser in Carroll, Briscoe, and Sanfilippo. Aswith the PARC 700,
there are systematic differences between the f-structures produced by our methodology
and the dependency structures of the CBS 500. In order to be able to evaluate against
the CBS 500, we automatically map our f-structures into a format similar to theirs. We
did not split the data into a heldout and a test set when developing the mapping, so
that a comparison could be made with other systems that report evaluations against
the CBS 500. The following CBS 500-style grammatical relations are produced from the
f-structure in Figure 1:
(ncsubj sign U.N.)
(dobj sign treaty)
Some mapping is carried out (as in the evaluation against the PARC 700) on the f-
structure annotated trees, and the remaining mapping is carried out on the f-structures
(Figure 16). As with the PARC 700 mapping, all mappings are carried out automatically.
The following phenomena were dealt with on the f-structure annotated trees:
Auxiliary verbs (xcomp flattening) XCOMPS were flattened to promote the main verb
to the top level, while maintaining a list of auxiliary and modal verbs and their
relation to one another.
Treatment of topicalized sentences The predicate of the topicalized sentence became
the main predicate and any other top level material became an adjunct.
Multi-word expressions Multi-word expressions (such as according to) were not
marked up in the parser input, but captured in the annotated trees and the
annotations adjusted accordingly.
Treatment of the verbs be and become Our automatic annotation algorithm does not
treat the verbs be and become differently from any other verbs when they are used
transitively. This analysis conflicted with the CBS 500 analysis, so was changed to
match theirs.
25 This was downloaded from http://www.informatics.susx.ac.uk/research/nlp/carroll/greval.html.
26 The long distance dependencies include passive, wh-less relative clauses, control verbs, and so forth.
110
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 13
Results of dependency evaluation against the CBS 500 (Carroll, Briscoe, and Sanfillipo 1998).
Bikel+Tags RASP p-Value
F-score 80.23 76.57 <.0001
The following are the main mappings carried out on the f-structures:
Encoding of Passive We treat passive as a feature in our automatic f-structure annota-
tion algorithm, whereas the CBS 500 triples encode this information indirectly.
Objects of Prepositional Phrases No dependency was generated for these objects, as
there was no corresponding dependency in the CBS 500 analyses.
Nomenclature Differences There were some trivial mappings to account for differ-
ences in nomenclature, for example OBL in our analyses became IOBJ in the
mapped dependencies.
Encoding of wh-less relative clauses These are encoded by means of reentrancies in
f-structure, but were encoded in a more indirect way in the mapped dependencies
to match the CBS 500 annotation format.
To carry out the experiments, we POS-tagged the tokenized CBS 500 sentences with
the MXPOST tagger (Ratnaparkhi 1996) and parsed the tag sequences with our Penn-II
and Bikel retrained-based LFG system. We use the evaluation software of Carroll,
Briscoe, and Sanfilippo (1998)27 to evaluate the grammatical relations produced by each
parser. The results are given in Table 13.
Our LFG system based on Bikel?s retrained parser achieves an f-score of 80.23%,
whereas the hand-crafted RASP grammar and parser achieves an f-score of 76.57%.
Crouch et al (2002) report that their XLE system achieves an f-score of 76.1% for the
same experiment. A detailed breakdown by dependency is given in Table 14. The
LFG system based on Bikel?s retrained parser is able to better identify MOD(ifier) de-
pendency relations, ARG MOD (the relation between a head and a semantic argument
which is syntactically realized as a modifier, for example by-phrases), IOBJ (indirect
object) and AUXiliary relations. RASP is able to better identify XSUBJ (clausal subjects
controlled from without), CSUBJ (clausal subjects), and COMP (clausal complement)
relations. Again we use the Approximate Randomization Test to test the parsing results
for statistical significance. The p-value for the test comparing our system using Bikel?s
retrained parser against RASP is <.0001. The treebank-based LFG system using Bikel?s
retrained parser is significantly better than the hand-crafted, deep, unification grammar-
based RASP parsing system with a statistical significance of >95%.
6. Discussion and Related Work
At the moment, we can only speculate as to why our treebank-based LFG resources
outperform the hand-crafted XLE and RASP grammars.
In Section 4, we observed that the treebank-induced LFG resources have consid-
erably wider coverage (>99.9% measured in terms of complete spanning parse) than
27 This was downloaded from http://www.informatics.susx.ac.uk/research/nlp/carroll/greval.html.
111
Computational Linguistics Volume 34, Number 1
Table 14
Breakdown by grammatical relation for results of evaluation against CBS 500.
Dep. Percent of deps. F-score (%)
Bikel+Tags RASP
DEPENDANT 100.00 80.23 76.57
MOD 48.96 80.30 75.29
NCMOD 30.42 85.37 72.98
XMOD 1.60 70.05 55.88
CMOD 2.61 75.60 53.08
DETMOD 14.06 95.85 91.97
ARG MOD 0.51 80.00 64.52
ARG 43.70 78.28 77.57
SUBJ 13.10 79.84 83.57
NCSUBJ 12.99 87.84 84.32
XSUBJ 0.06 0.00 88.89
CSUBJ 0.04 0.00 22.22
SUBJ OR DOBJ 18.22 81.21 83.84
COMP 12.38 76.73 71.87
OBJ 7.34 76.05 69.53
DOBJ 5.11 84.55 84.57
OBJ2 0.25 48.00 43.84
IOBJ 1.98 59.04 47.60
CLAUSAL 5.04 77.74 75.37
XCOMP 4.03 80.00 84.11
CCOMP 1.01 69.61 75.14
AUX 4.76 94.94 88.27
CONJ 2.06 68.84 69.09
the hand-crafted grammars (?80% for XLE and RASP grammars on unseen treebank
text). Both XLE and RASP use a number of (largest) fragment-combining techniques
to achieve full coverage. If coverage is a significant component in the performance
difference observed between the hand-crafted and treebank-induced resources, then
it is reasonable to expect that the performance difference is more pronounced with
increasing sentence length (with shorter sentences being simpler and more likely to
be within the coverage of the hand-crafted grammars). In other words, we expect the
hand-crafted, deep, precision grammars to do better on shorter sentences (more likely to
be within their coverage), whereas the treebank-induced grammars should show better
performance on longer strings (less likely to be within the coverage of the hand-crafted
grammars).
In order to test this hypothesis, we carried out a number of experiments:
First, we plotted the sentence length distribution for the 560 PARC 700 test sen-
tences and the 500 CBS 500 sentences (Figures 17 and 18). Both gold standards are
approximately normally distributed, with the CBS 500 distribution possibly showing
the effects of being chosen subject to the constraint that the strings are parsable by the
parser in Carroll, Briscoe, and Sanfilippo (1998). For each case we use the mean, ?, and
two standard deviations, 2?, to the left and right of themean to exclude sentence lengths
not supported by sufficient observations: For PARC 700, ? = 23.27, ?? 2? = 2.17, and
?+ 2? = 44.36; for CBS 500, ? = 17.27, ?? 2? = 1.59, and ?+ 2? = 32.96. Both the
PARC 700 and the CB 500 distributions are positively skewed. For the PARC 700, ?? 2?
is actually outside the observed data range, whereas for CB 500, ?? 2? almost coincides
112
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Figure 17
Distribution of sentence frequency by sentence length in the PARC 700 test set with Bezier
interpolation. Vertical lines mark two standard deviations from the mean.
with the left border. It is therefore useful to further constrain the ?2? range by a
sentence count threshold of ? 5.28 This results in a sentence length range of 4?41 for
PARC 700 and 4?32 for CBS 500.
Second, in order to test whether fragment parses increase with sentence length, we
plotted the percentage of fragment parses over sentence length for the XLE parses of the
560-sentence test set of the PARC 700 (we did not do this for the CBS 500 as its strings
are selected to be fully parsable by RASP). Figure 19 shows that the number of fragment
parses tends to increase with sentence length.
Third, we plotted the average dependency f-score for the hand-crafted and the
treebank-induced resources against sentence lengths. Figure 20 shows the results for
PARC 700, Figure 21 for CBS 500.
In both cases, contrary to our (perhaps somewhat naive) assumption, the graphs
show that the treebank-induced resources outperform the hand-crafted resources
within (most of) the 4?41 and 4?32 sentence length bounds, with the results for the very
short and the very long strings outside those bounds not being supported by sufficient
data points.
In the parsing literature, results are often also provided for strings with lengths
?40. Below we give those results and statistical significance testing for the PARC 700
and CBS 500 (Tables 15 and 16). The results show that the Bikel retrained?based LFG
system achieves a higher dependency f-score on sentences of length ?40 than on all
sentences, whereas the XLE system achieves a slightly lower score on sentences of
length ?40. The Bikel-retrained system achieves an f-score of 83.18%, a statistically
28 Note that because the distributions in Figures 17 and 18 are Bezier interpolated, the constraint does not
guarantee that every sentence length within the range occurs five or more times.
113
Computational Linguistics Volume 34, Number 1
Figure 18
Distribution of sentence frequency by sentence length in the CB 500 test set with Bezier
interpolation. Vertical lines mark two standard deviations from the mean.
Figure 19
Percentage of fragment sentences for XLE parsing system per sentence length with Bezier
interpolation.
significant improvement of 2.67 percentage points over the XLE system on sentences of
length?40. Against the CBS 500, Bikel?s retrained system achieves a weighted f-score of
82.58%, a statistically significant improvement of 3.87 percentage points over the RASP
system which achieves a weighted f-score of 78.81% on sentences of length ?40.
114
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Figure 20
Average f-score by sentence length for PARC 700 test set with Bezier interpolation.
Figure 21
Average f-score by sentence length for CB 500 test set with Bezier interpolation.
Finally, we test whether the strict preds-only dependency evaluation has an ef-
fect on the results for the PARC 700 experiments. Recall that following Kaplan et al
(2004) for the PARC 700 evaluation we used a set of semantically relevant grammatical
functions that is a superset of preds-only and a subset of all-GFs. A preds-only based
evaluation is ?stricter? and tends to produce lower scores as it directly reflects the effects
115
Computational Linguistics Volume 34, Number 1
Table 15
Evaluation and significance testing of sentences length ?40 against the PARC 700.
All sentence lengths Length ?40
f-score f-score
Bikel + Tags 82.73 83.18
XLE 80.55 80.51
p-value .0054 .0010
Table 16
Evaluation and significance testing of sentences length ?40 against the CBS 500.
All sentence lengths Length ?40
f-score f-score
Bikel + Tags 80.23 82.58
RASP 76.57 78.81
p-value <.0001 <.0001
Table 17
Preds-only evaluation against the PARC 700 Dependency Bank.
All GFs Preds only
f-score f-score
Bikel + Tags 82.73 77.40
XLE 80.55 74.31
of predicate?argument/adjunct misattachments in the resulting dependency represen-
tations (while local functions such as NUM(ber), for example, can score properly even
if the local predicate is misattached). Table 1729 below gives the results for preds-only
evaluation30 on the PARC 700 for all sentence lengths. The results show that the Bikel-
retrained treebank-based LFG resource achieves an f-score of 77.40%, 5.33 percentage
points lower than the score for all the PARC dependencies. The XLE system achieves
an f-score of 74.31%, 6.24 percentage points lower than the score for all the PARC
dependencies and a 3.09 percentage point drop against the results obtained by the
Bikel-retrained treebank-based LFG resources. The performance of the Bikel retrained?
based LFG system suffers less than the XLE system when preds-only dependencies are
evaluated.
It is important to note that in our f-structure annotation algorithm and treebank-
based LFG parsing architectures, we do not claim to provide fully adequate statistical
models. It is well known (Abney 1997) that PCFG- or history-based parser approxima-
tions to general constraint-based grammars can yield inconsistent probability models
29 We do not include a p-value here as the breakdown by function per sentence was not available to us for
the XLE data.
30 The dependency relations we include in preds-only evaluation are: ADJUNCT, AQUANT, COMP, CONJ,
COORD FORM, DET FORM, FOCUS INT, MOD, NUMBER, OBJ, OBJ THETA, OBL, OBL AG, OBL COMPAR,
POSS, PRON INT, PRON REL, PRT FORM, QUANT, SUBJ, SUBORD FORM, TOPIC REL, XCOMP.
116
Cahill et al Statistical Parsing Using Automatic Dependency Structures
due to loss of probability mass: The parser successfully returns the highest ranked
parse tree but the constraint solver cannot resolve the f-structure equations and the
probability mass associated with that tree is lost. Research on adequate probability
models for constraint-based grammars is important (Bouma, van Noord, and Malouf
2000; Miyao and Tsujii 2002; Riezler et al 2002; Clark and Curran 2004). In this context,
it is interesting to compare parser performance against upper bounds. For the PARC
700 evaluation, the upper bound for the XLE-based resources is 85.65%, against 86.83%
for the treebank-based LFG resources. XLE-based parsing currently achieves 94.05%
(f-score 80.55%) of its upper bound using a discriminative disambiguation method,
whereas the treebank-based LFG resource achieves 95.28% (f-score 82.73%) of its upper
bound.
Although this seems to indicate that the two disambiguationmodels achieve similar
results, the figures are actually very difficult to compare. In the case of the XLE, the
upper bound is established by unpacking all parses for a string and scoring the best
match against the gold standard (rather than letting the probability model select a
parse). By contrast, in the case of the treebank-based LFG resources, we use the original
?perfect? Penn-II treebank trees (rather than the trees produced by the parser), auto-
matically annotate those trees with the f-structure annotation algorithm, and score the
results against the PARC 700 (it is not feasible to generate all parses for a string, there
are simply too many for treebank-induced resources). The upper bound computed in
this fashion for the treebank-based LFG resource (86.83%) is relatively low. The main
reason is that the automatic mapping required to relate the f-structures generated by the
treebank-based LFG resources to the PARC 700 dependencies is lossy. This is indicated
by comparing the upper bound for the treebank-based LFG resources for the PARC 700
against the upper bound for the DCU 105 gold standard, where little or no mapping
(apart from the feature-structure to dependency-triple conversion) is required: Scoring
the f-structure annotations for the original treebank trees results in 86.83% against PARC
700 versus 96.80% against DCU 105.
Our discussion shows how delicate it can be to compare parsing systems and
their disambiguation models. Ultimately what is required is an evaluation strategy that
separates out and clearly distinguishes between the grammar, parsing algorithm, and
disambiguation model and is capable of assessing different combinations of these core
components. Of course, this will not always be possible andmoving towards it is part of
a much more extended research agenda, well beyond the scope of the research reported
in the present article. Our approach, and previous approaches, evaluate systems at
the highest level of granularity, that of the complete package: the combined grammar-
parser-disambiguation model. The results show that machine-learning-based resources
can outperform deep, state-of-the-art hand-crafted resources with respect to the quality
of dependencies generated.
Treebank-based, deep and wide-coverage constraint-based grammar acquisition
has become an important research topic: Starting with the seminal TAG-based work
of Xia (1999), there are now also HPSG-, CCG- and LFG-based approaches. Miyao,
Ninomiya, and Tsujii (2003) and Tsuruoka, Miyao, and Tsujii (2004) present re-
search on inducing Penn-II treebank-based HPSGs with log-linear probability models.
Hockenmaier and Steedman (2002) and Hockenmaier (2003) provide treebank-induced
CCG-basedmodels including LDD resolution. It would be interesting to conduct a com-
parative evaluation involving treebank-based HPSG, CCG, and LFG resources against
the PARC 700 and CBS 500 Dependency Banks to enable more comprehensive compar-
ison of machine-learning-based with hand-crafted, deep, wide-coverage resources such
as those used in the XLE or RASP parsing systems.
117
Computational Linguistics Volume 34, Number 1
Gildea and Hockenmaier (2003) and Miyao and Tsujii (2004) present PropBank
(Kingsbury, Palmer, andMarcus 2002)-based evaluations of their automatically induced
CCG and HPSG resources. PropBank-based evaluations provide valuable information
to rank parsing systems. Currently, however, PropBank-based evaluations are some-
what partial: They only represent (and hence score) verbal arguments and disregard a
raft of other semantically important dependencies (e.g., temporal and aspectual infor-
mation, dependencies within nominal clusters, etc.) as captured in the PARC 700 or CBS
500 Dependency Banks.31
7. Conclusions
Parser comparison is a non-trivial and time-consuming exercise. We extensively eval-
uated four machine-learning-based shallow parsers and two hand-crafted, wide-
coverage deep probabilistic parsers involving four gold-standard dependency banks,
using the Approximate Randomization Test (Noreen 1989) to test for statistical signifi-
cance. We used a sophisticated method for automatically producing deep dependency
relations from Penn-II-style CFG-trees (Cahill et al 2002b, 2004) to compare shallow
parser output at the level of dependency relation and revisit experiments carried out by
Preiss (2003) and Kaplan et al (2004).
Our main findings are twofold:
1. Using our CFG-tree-to-dependency annotation technology, together with treebank- and
machine-learning-based parsers, we can outperform hand-crafted, wide-coverage, deep,
probabilistic, constraint-based grammars and parsers.
This result is surprising for two reasons. First, it is established against two externally-
provided dependency banks (the PARC 700 and the CBS 500 gold standards), originally
designed using the hand-crafted, wide-coverage, probabilistic grammars for the XLE
(Riezler et al 2002) and RASP (Carroll and Briscoe 2002) parsing systems to evaluate
those systems. What is more, the SUSANNE corpus-based CBS 500 constitutes an
instance of domain variation for the Penn-II-trained LFG resources, likely to adversely
affect scores. Second, the treebank- and machine-learning-based LFG resources require
automatic mapping to relate f-structure output of the treebank-based parsing systems
to the representation format in the PARC 700 and CBS 500 Dependency Banks. These
mappings are partial and lossy: That is, they do not cover all of the systematic dif-
ferences between f-structure and dependency bank representations and introduce a
certain amount of error in what they are designed to capture, that is they both over-
and undergeneralize, again adversely affecting scores. Improvements of the mappings
should lead to a further improvement in the dependency scores.
2. Parser evaluation at the level of dependency representation still requires non-trivial
mappings between different dependency representation formats.
31 In a sense, PropBank does not yet provide a single agreed upon gold standard: Role information is
provided indirectly and an evaluation gold-standard has to be computed from this. In doing so, choices
have to be made as regards the representation of shared arguments, the analysis of coordinate structures,
and so forth, and it is not clear that the same choices are currently made for evaluations carried out by
different research groups.
118
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Earlier we criticized tree-based parser evaluation on the grounds that equally valid
different tree-typologies can be associated with strings, and identified this as a major
obstacle to fair and unbiased parser evaluation. Dependency-based parser evaluation,
as it turns out, is not entirely free of this criticism either: There are significant systematic
differences between the PARC 700 dependency and the CBS 500 dependency represen-
tations; there are significant systematic differences between the LFG f-structures gener-
ated by the hand-crafted, wide-coverage grammars of Riezler et al (2002) and Kaplan
et al (2004) and those of the treebank-induced and f-structure annotation algorithm
based resources of Cahill et al (2004). These differences require careful implementation
of mappings if parsers are not to be unduly penalized for systematic and motivated
differences at the level of dependency representation. By and large, these differences
are, however, less pronounced than differences on CFG tree representations, making
dependency-based parser evaluation a worthwhile and rewarding exercise.
Summarizing our results, we find that against the DCU 105 development set, the
treebank- and f-structure annotation algorithm-based LFG system using Bikel?s parser
retrained to retain Penn-II functional tag labels performs best, achieving f-scores of
82.92% preds-only and 88.3% all grammatical functions. Against the automatically
generated WSJ Section 22 Dependency Bank, the system using Bikel?s retrained parser
achieves the highest results, achieving f-scores of 83.06% preds-only and 87.861% all
GFs. This is statistically significantly better than all other parsers. In order to evaluate
against the PARC 700 and CBS 500 gold standards, we automaticallymap the dependen-
cies produced by our treebank-based LFG system into a format compatible with the gold
standards. Against the PARC 700 Dependency Bank, the treebank-based LFG system
using Bikel?s retrained parser achieves an f-score of 82.73%, a statistically significant
improvement of 2.18% against the most up-to-date results of the hand-crafted XLE-
based parsing resources. Against the CBS 500, the treebank-based LFG system using
Bikel?s retrained parser achieved the highest f-score of 80.23%, a statistically significant
improvement of 3.66 percentage points on the highest previously published results
for the same experiment with the hand-crafted RASP resources in Carroll and Briscoe
(2002).
Appendix A. Parser Parameter Settings
This section provides a complete list of the parameter settings used for each of the
parsers described in this article.
Parser Parameters
Collins Model 3 We used the Collins parser with its default settings of a
beam size of 10,000 and where the values of the following
flags are set to 1: punctuation-flag, distaflag, distvflag,
and npbflag. Input was pre-tagged using the MXPOST
POS tagger (Ratnaparkhi 1996). We parse the input file
with all three models and use the scripts provided to
merge the outputs into the final parser output file. Note
that this file has been cleaned of all -A functional tags and
trace nodes.
Charniak We used the parser dated August 2005 and ran the
parser using the data provided in the download on pre-
tokenized sentences of length ?200. Input was automati-
cally tagged by the parser.
119
Computational Linguistics Volume 34, Number 1
Bikel Emulation of We used version 0.9.9b of the parser trained on the file
Collins Model 2 of observed events made available on the downloads
page. We used the collins.properties file and a maximum
heap size of 1,500 MB. Input was pre-tagged using the
MXPOST POS tagger (Ratnaparkhi 1996).
Bikel + Functional Tags We used version 0.9.9b of the parser trained on a version
of Sections 02?21 of the Penn-II treebank where functional
tags were not ignored by the parser. We updated the de-
fault head-finding rules to deal with the new categories.
We also trained on all sentences from the training set
(the default collins.properties file is set to ignore trees of
more than 500 tokens). Input was pre-tagged using the
MXPOST POS tagger (Ratnaparkhi 1996) and the maxi-
mum heap size was 1,500 MB.
RASP Weused a file of parser output provided through personal
communication with John Carroll. (Tagging is carried out
automatically by the parser.)
XLE We used a file of parser output provided by the Natural
Language Theory and Technology group at the Palo Alto
Research Center. (Tagging is carried out automatically by
the parser.)
Acknowledgments
We are grateful to our anonymous reviewers
whose insightful comments have improved
the article significantly. We would like to
thank John Carroll for discussion and help
with reproducing the RASP parsing results;
Michael Collins, Eugene Charniak, Dan
Bikel, and Helmut Schmid for making their
parsing engines available; and Ron Kaplan
and the team at PARC for discussion,
feedback, and support. Part of the research
presented here has been supported by
Science Foundation Ireland grants
04/BR/CS0370 and 04/IN/I527, Enterprise
Ireland Basic Research Grant SC/2001/0186,
an Irish Research Council for Science,
Engineering and Technology Ph.D.
studentship, an IBM Ph.D. studentship and
support from IBM?s Centre for Advanced
Studies (CAS) in Dublin.
References
Abney, Stephen. 1997. Stochastic
attribute-value grammars. Computational
Linguistics, 23(4):597?618.
Alshawi, Hiyan and Stephen Pulman, 1992.
Ellipsis, Comparatives, and Generation,
chapter 13. The MIT Press, Cambridge,
MA.
Baldwin, Timothy, Emily Bender, Dan
Flickinger, Ara Kim, and Stephan Oepen.
2004. Road-testing the English Resource
Grammar over the British National
Corpus. In Proceedings of the Fourth
International Conference on Language
Resources and Evaluation (LREC 2004),
pages 2047?2050, Lisbon, Portugal.
Bikel, Dan. 2002. Design of a multi-lingual,
parallel-processing statistical parsing
engine. In Proceedings of HLT 2002,
pages 24?27, San Diego, CA.
Black, Ezra, Steven Abney, Dan Flickenger,
Claudia Gdaniec, Ralph Grishman, Philip
Harrison, Donald Hindle, Robert Ingria,
Fred Jelineck, Judith Klavans, Mark
Liberman, Mitchell Marcus, Salim Roukos,
Beatrice Santorini, and Tomek
Strzalkowski. 1991. A procedure for
quantitatively comparing the syntactic
coverage of english grammars. In
Proceedings of the Speech and Natural
Language Workshop, pages 306?311, Pacific
Grove, CA.
Bod, Rens. 2003. An efficient implementation
of a new DOP model. In Proceedings of the
Tenth Conference of the European Chapter of
the Association for Computational Linguistics
(EACL?03), pages 19?26, Budapest,
Hungary.
Bouma, Gosse, Gertjan van Noord, and
Robert Malouf. 2000. Alpino:
Wide-coverage computational analysis of
dutch. In Walter Daelemans, Khalil
Sima?an, Jorn Veenstra, and Jakub Zavrel,
editors, Computational Linguistics in The
Netherlands 2000. Rodopi, Amsterdam,
pages 45?59.
120
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Bresnan, Joan. 2001. Lexical-Functional Syntax.
Blackwell, Oxford, England.
Briscoe, Edward and John Carroll. 1993.
Generalized probabilistic LR parsing of
natural language (corpora) with
unification-based grammars. Computational
Linguistics, 19(1):25?59.
Briscoe, Ted and John Carroll. 2006.
Evaluating the accuracy of an
unlexicalized statistical parser on the
PARC DepBank. In Proceedings of the
COLING/ACL 2006 Main Conference Poster
Sessions, pages 41?48, Sydney, Australia.
Briscoe, Edward, Claire Grover, Bran
Boguraev, and John Carroll. 1987. A
formalism and environment for the
development of a large grammar of
English. In Proceedings of the 10th
International Joint Conference on AI,
pages 703?708, Milan, Italy.
Burke, Michael. 2006. Automatic Treebank
Annotation for the Acquisition of LFG
Resources. Ph.D. thesis, School of
Computing, Dublin City University,
Dublin, Ireland.
Burke, Michael, Aoife Cahill, Ruth
O?Donovan, Josef van Genabith, and Andy
Way. 2004. Evaluation of an automatic
annotation algorithm against the PARC
700 Dependency Bank. In Proceedings of the
Ninth International Conference on LFG,
pages 101?121, Christchurch, New Zealand.
Butt, Miriam, Helge Dyvik, Tracy Holloway
King, Hiroshi Masuichi, and Christian
Rohrer. 2002. The Parallel Grammar
Project. In Proceedings of COLING 2002,
Workshop on Grammar Engineering and
Evaluation, pages 1?7, Taipei, Taiwan.
Cahill, Aoife. 2004. Parsing with Automatically
Acquired, Wide-Coverage, Robust,
Probabilistic LFG Approximations. Ph.D.
thesis, School of Computing, Dublin City
University, Dublin, Ireland.
Cahill, Aoife, Michael Burke, Ruth
O?Donovan, Josef van Genabith, and Andy
Way. 2004. Long-distance dependency
resolution in automatically acquired
wide-coverage PCFG-based LFG
approximations. In Proceedings of the 42nd
Annual Meeting of the Association for
Computational Linguistics, pages 320?327,
Barcelona, Spain.
Cahill, Aoife, Maire?ad McCarthy, Josef van
Genabith, and Andy Way. 2002a.
Automatic annotation of the Penn
Treebank with LFG f-structure
information. In Proceedings of the LREC
Workshop on Linguistic Knowledge
Acquisition and Representation: Bootstrapping
Annotated Language Data, pages 8?15, Las
Palmas, Canary Islands, Spain.
Cahill, Aoife, Maire?ad McCarthy, Josef van
Genabith, and Andy Way. 2002b. Parsing
with PCFGs and automatic f-structure
annotation. In Proceedings of the Seventh
International Conference on LFG,
pages 76?95, Palo Alto, CA.
Carroll, John and Edward Briscoe. 2002.
High precision extraction of grammatical
relations. In Proceedings of the 19th
International Conference on Computational
Linguistics (COLING), pages 134?140,
Taipei, Taiwan.
Carroll, John, Edward Briscoe, and Antonio
Sanfilippo. 1998. Parser evaluation: A
survey and new proposal. In Proceedings of
the International Conference on Language
Resources and Evaluation, pages 447?454,
Granada, Spain.
Carroll, John, Anette Frank, Dekang Lin,
Detlef Prescher, and Hans Uszkoreit,
editors. 2002. HLT Workhop: ?Beyond
PARSEVAL ? Towards improved evaluation
measures for parsing systems?, Las Palmas,
Canary Islands, Spain.
Charniak, Eugene. 1996. Tree-bank
grammars. In Proceedings of the
Thirteenth National Conference on
Artificial Intelligence, pages 1031?1036,
Menlo Park, CA.
Charniak, Eugene. 2000. A maximum
entropy inspired parser. In Proceedings
of the First Annual Meeting of the North
American Chapter of the Association for
Computational Linguistics (NAACL 2000),
pages 132?139, Seattle, WA.
Chinchor, Nancy, Lynette Hirschman, and
David D. Lewis. 1993. Evaluating message
understanding systems: An analysis of the
Third Message Understanding Conference
(MUC-3). Computational Linguistics,
19(3):409?449.
Clark, Stephen and James Curran. 2004.
Parsing the WSJ using CCG and log-linear
models. In Proceedings of the 42nd Annual
Meeting of the Association for Computational
Linguistics (ACL-04), pages 104?111,
Barcelona, Spain.
Clark, Stephen and James Curran. 2007.
Formalism-independent parser evaluation
with CCG and DepBank. In Proceedings of
the 45th Annual Meeting of the Association
for Computational Linguistics (ACL 2007),
pages 248?255, Prague, Czech Republic
http://www.aclweb-org/anthology/P/
P07/P07-1032.
Clark, Stephen and Julia Hockenmaier. 2002.
Evaluating a wide-coverage CCG parser.
121
Computational Linguistics Volume 34, Number 1
In Proceedings of the LREC 2002 Beyond
Parseval Workshop, pages 60?66, Las
Palmas, Canary Islands, Spain.
Cohen, Paul R. 1995. Empirical Methods for
Artificial Intelligence. The MIT Press,
Cambridge, MA.
Collins, Michael. 1997. Three generative,
lexicalized models for statistical parsing.
In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics,
pages 16?23, Madrid, Spain.
Collins, Michael. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania,
Philadelphia, PA.
Crouch, Richard, Ron Kaplan, Tracy Holloway
King, and Stefan Riezler. 2002. A
comparison of evaluation metrics for a
broad coverage parser. In Proceedings of
the LREC Workshop: Beyond PARSEVAL?
Towards Improved Evaluation Measures for
Parsing Systems, pages 67?74, Las Palmas,
Canary Islands, Spain.
Dalrymple, Mary. 2001. Lexical-Functional
Grammar. London, Academic Press.
Dienes, Pe?ter and Amit Dubey. 2003.
Antecedent recovery: Experiments with a
trace tagger. In Proceedings of the 2003
Conference on Empirical Methods in Natural
Language Processing, pages 33?40, Sapporo,
Japan.
Eisele, Andreas and Jochen Do?rre. 1986. A
lexical functional grammar system in
Prolog. In Proceedings of the 11th International
Conference on Computational Linguistics
(COLING 1986), pages 551?553, Bonn.
Flickinger, Dan. 2000. On building a more
efficient grammar by exploiting types.
Natural Language Engineering, 6(1):15?28.
Gaizauskas, Rob. 1995. Investigations into
the grammar underlying the Penn
Treebank II. Research Memorandum
CS-95-25, Department of Computer
Science, Univeristy of Sheffield, UK.
Gildea, Daniel and Julia Hockenmaier.
2003. Identifying semantic roles using
combinatory categorial grammar.
In Proceedings of the 2003 Conference
on Empirical Methods in Natural
Language Processing, pages 57?64,
Sapporo, Japan.
Hockenmaier, Julia. 2003. Parsing with
generative models of predicate?argument
structure. In Proceedings of the 41st Annual
Conference of the Association for
Computational Linguistics, pages 359?366,
Sapporo, Japan.
Hockenmaier, Julia and Mark Steedman.
2002. Generative models for statistical
parsing with combinatory categorial
grammar. In Proceedings of the 40th
Annual Meeting of the Association for
Computational Linguistics, pages 335?342,
Philadelphia, PA.
Johnson, Mark. 1999. PCFG models of
linguistic tree representations.
Computational Linguistics, 24(4):613?632.
Johnson, Mark. 2002. A simple
pattern-matching algorithm for
recovering empty nodes and their
antecedents. In Proceedings of the 40th
Annual Meeting of the Association for
Computational Linguistics, pages 136?143,
Philadelphia, PA.
Kaplan, Ron and Joan Bresnan. 1982.
Lexical functional grammar, a formal
system for grammatical representation.
In Joan Bresnan, editor, The Mental
Representation of Grammatical Relations.
MIT Press, Cambridge, MA,
pages 173?281.
Kaplan, Ron, Stefan Riezler, Tracy Holloway
King, John T. Maxwell, Alexander
Vasserman, and Richard Crouch. 2004.
Speed and accuracy in shallow and deep
stochastic parsing. In Proceedings of the
Human Language Technology Conference and
the 4th Annual Meeting of the North
American Chapter of the Association for
Computational Linguistics (HLT-NAACL?04),
pages 97?104, Boston, MA.
Kaplan, Ronald and Annie Zaenen. 1989.
Long-distance dependencies, constituent
structure and functional uncertainty. In
Mark Baltin and Anthony Kroch, editors,
Alternative Conceptions of Phrase Structure,
pages 17?42, University of Chicago Press,
Chicago.
King, Tracy Holloway, Richard Crouch,
Stefan Riezler, Mary Dalrymple, and Ron
Kaplan. 2003. The PARC 700 dependency
bank. In Proceedings of the EACL03: 4th
International Workshop on Linguistically
Interpreted Corpora (LINC-03), pages 1?8,
Budapest, Hungary.
Kingsbury, Paul, Martha Palmer, and
Mitch Marcus. 2002. Adding semantic
annotation to the Penn TreeBank. In
Proceedings of the Human Language
Technology Conference, pages 252?256,
San Diego, CA.
Klein, Dan and Christopher D. Manning.
2003. Accurate unlexicalized parsing. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics,
pages 423?430, Sapporo, Japan.
Leech, Geoffrey and Roger Garside. 1991.
Running a grammar factory: On the
122
Cahill et al Statistical Parsing Using Automatic Dependency Structures
compilation of parsed corpora, or
?treebanks?. In Stig Johansson and
Anna-Brita Stenstro?m, editors, English
Computer Corpora: Selected Papers. Mouton
de Gruyter, Berlin, pages 15?32.
Levy, Roger and Christopher D. Manning.
2004. Deep dependencies from context-free
statistical parsers: Correcting the surface
dependency approximation. In Proceedings
of the 42nd Annual Meeting of the Association
for Computational Linguistics (ACL 2004),
pages 328?335, Barcelona, Spain.
Lin, Dekang. 1995. A dependency-based
method for evaluating broad-coverage
parsers. In Proceedings of the International
Joint Conference on AI, pages 1420?1427,
Montre?al, Canada.
Magerman, David. 1994. Natural Language
Parsing as Statistical Pattern Recognition.
Ph.D. thesis, Department of Computer
Science, Stanford University, CA.
Marcus, Mitchell, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre,
Ann Bies, Mark Ferguson, Karen Katz,
and Britta Schasberger. 1994. The
Penn Treebank: Annotating predicate
argument structure. In Proceedings
of the ARPA Workshop on Human
Language Technology, pages 110?115,
Princeton, NJ.
McCarthy, Maire?ad. 2003. Design and
Evaluation of the Linguistic Basis of an
Automatic F-Structure Annotation Algorithm
for the Penn-II Treebank. Master?s thesis,
School of Computing, Dublin City
University, Dublin, Ireland.
McDonald, Ryan and Fernando Pereira. 2006.
Online learning of approximate
dependency parsing algorithms. In
Proceedings of the 11th Conference of the
European Chapter of the Association for
Computational Linguistics, pages 81?88,
Trento, Italy.
Miyao, Yusuke, Takashi Ninomiya, and
Jun?ichi Tsujii. 2003. Probabilistic modeling
of argument structures including non-local
dependencies. In Proceedings of the
Conference on Recent Advances in Natural
Language Processing (RANLP),
pages 285?291, Borovets, Bulgaria.
Miyao, Yusuke and Jun?ichi Tsujii. 2002.
Maximum entropy estimation for feature
forests. In Proceedings of Human Language
Technology Conference (HLT 2002),
pages 292?297, San Diego, CA.
Miyao, Yusuke and Jun?ichi Tsujii.
2004. Deep linguistic analysis
for the accurate identification of
predicate?argument relations. In
Proceedings of the 18th International
Conference on Computational Linguistics
(COLING 2004), pages 1392?1397,
Geneva, Switzerland.
Noreen, Eric W. 1989. Computer Intensive
Methods for Testing Hypotheses: An
Introduction. Wiley, New York.
O?Donovan, Ruth, Michael Burke, Aoife
Cahill, Josef van Genabith, and Andy
Way. 2004. Large-scale induction and
evaluation of lexical resources from the
Penn-II treebank. In Proceedings of the 42nd
Annual Meeting of the Association for
Computational Linguistics, pages 368?375,
Barcelona, Spain.
Pollard, Carl and Ivan Sag. 1994. Head-driven
Phrase Structure Grammar. CSLI
Publications, Stanford, CA.
Preiss, Judita. 2003. Using grammatical
relations to compare parsers. In Proceedings
of the Tenth Conference of the European
Chapter of the Association for Computational
Linguistics (EACL?03), pages 291?298,
Budapest, Hungary.
Ratnaparkhi, Adwait. 1996. A maximum
entropy part-of-speech tagger. In
Proceedings of the Empirical Methods in
Natural Language Processing Conference,
pages 133?142, Philadelphia, PA.
Riezler, Stefan, Tracy King, Ronald Kaplan,
Richard Crouch, John T. Maxwell,
and Mark Johnson. 2002. Parsing
theWall Street Journal using a
lexical-functional grammar and
discriminative estimation techniques.
In Proceedings of the 40th Annual
Conference of the Association for
Computational Linguistics (ACL-02),
pages 271?278, Philadelphia, PA.
Sampson, Geoffrey. 1995. English for the
Computer: The SUSANNE Corpus and
Analytic Scheme. Clarendon Press,
Oxford, England.
Tsuruoka, Yoshimasa, Yusuke Miyao,
and Jun?ichi Tsujii. 2004. Towards
efficient probabilistic HPSG parsing:
Integrating semantic and syntactic
preference to guide the parsing.
In Proceedings of IJCNLP-04 Workshop:
Beyond shallow analyses?Formalisms
and statistical modeling for deep
analyses, Hainan Island, China. [No
page numbers].
van Genabith, Josef and Richard Crouch.
1996. Direct and underspecified
interpretations of LFG f-structures. In 16th
International Conference on Computational
Linguistics (COLING 96), pages 262?267,
Copenhagen, Denmark.
123
Computational Linguistics Volume 34, Number 1
van Genabith, Josef and Richard
Crouch. 1997. On interpreting
f-structures as UDRSs. In Proceedings
of ACL-EACL-97, pages 402?409,
Madrid, Spain.
Xia, Fei. 1999. Extracting tree adjoining
grammars from bracketed corpora. In
Proceedings of the 5th Natural Language
Processing Pacific Rim Symposium
(NLPRS-99), pages 398?403, Beijing,
China.
Xue, Nianwen, Fei Xia, Fu-Dong Chiou, and
Martha Palmer. 2004. The Penn Chinese
treebank: Phrase structure annotation of a
large corpus. Natural Language Engineering,
10(4):1?30.
Yeh, Alexander. 2000. More accurate tests
for the statistical significance of result
differences. In Proceedings of the 18th
International Conference on Computational
Linguistics (COLING, 2000), pages 947?953,
Saarbru?cken, Germany.
124
