Binding Constraints as Instructions of Binding Machines 
Ant6nio Branco 
I)ept. of ll~fi)rmatics, University of Lisbon 
Faculdade de Ciancias de Lisboa 
Campo Grande, 1700 Lisboa, Portugal 
Antonio .B ranco@di .  fc.ul .p t  
Abstract 
Binding constraints have resisted to be fully 
integrated into the course of grammatical 
processing despite its practical relevance 
and cross-linguistic generality. The ultimate 
root for this is to be found in the exponential 
"overgenerate & filter" procedure of the 
mainstream rationale for their satisfaction. 
In this paper we design an alternative 
approach based on the view that nominals 
are binding machines. 
Introduction 
Binding constraints are an important set of filters 
in the process of anaphor resolution'. As they 
delimit the relative positioning of anaphors and 
their possible antecedents in the grammatical 
geometry, these constraints are of crucial 
importance for restricting the search space for 
antecedent candidates and enhancing the 
performance of resolvers. From an empirical 
perspective, they stem from quite robust 
generalizations and exhibit a universal character. 
given their parameterized validity across natural 
langnages. From a conceptual point of view, in 
turn, the relations anaong binding constraints 
involve non-trivial symmetry, which lends them 
a moduhtr nature. Accordingly, they have 
typically been taken as one of the most 
intriguing and robust gramnaar modules. 
I See Annex for examples and the definition of 
binding constraints. 
Ill contrast to this, however, the formal and 
computational handling of binding constraints 
has presented non-negligible resistance when it 
comes to their integration into the representation 
and processing of grammatical knowledge. 
In its mainstream formulation, the methodology 
for verifying the compliance of grammatical 
representations with binding constraints requires 
a series of extra grammatical parsing steps 
(Chomsky, 81). More recently, prominent 
unification-based frameworks either require 
special purpose extensions of the description 
formalism for a partial handling of these 
constraints (LFG: Dalrymple, 93), or offer no 
integration yet for them into the grammatical 
setup (HPSG: Pollard and Sag, 94.. Backofen et 
al., 96). 
Our primary aim in this paper is to bridge this 
gap between the gram,natical nature of binding 
constraints and their full integration into 
grammar processing. In Section 1, we review 
previous steps towards this goal proposed in the 
literature. Building on these contributions in 
Section 2, we introduce the rationale of a new 
methodology for the verification of binding 
constraints, in Section 3, in the light of this new 
approach, we show how these constraints are 
fully integrated into g,ammar and the drawbacks 
of current methodology are overcome. 
1 The Cohldexation Paradigm 
The specification of binding constraints have 
greatly evolved in the htst three decades. The 
device of coindexation for marking anaphoric 
104 
links has, however, remained quite stable. This 
stems from the fact this device is at the heart of 
the mainstrealn lethodology for verifying these 
constraints, a methodology whose basics were 
proposed in (Chomsky, 80, 81) and have been 
adopted since then in its different variants. 
L1 Post -g rammat ica l l  overgerierat ior~ 
and fi ltering 
This methodology can be outlined as in Fig. 1. 
After the grammatical parsirig of a seriterice 
with n NPs has been completed: 
(i) iteration:repeat (ii)-(iii) uritil all 
possible different assignments of n indices 
(tokens) have been exhausted; 
(ii) indexat ion:generate a tree by assigning 
indices to its NPs; 
( i l l ) f i l ter ing:store the arniotated tree if the 
indexation of NPs respects binding 
constraints, otherwi'~e dc:letc it. 
Fig. 1 ? ('/wm,vk),'.v al:;()i?ithm 
As noted as early as in (Correa, g/'), thi:~ 
approach is grossly inefficient. Later Fong, 90? 
showed that its complexity is of exponential 
order. Moreover, this methodology disregards 
any concern with interfacing grarnmar with 
systems for reference processing. The input for 
such systems wil l  riot l)c a grammatical 
representatiori to lie refined vis4-vis the 
heuristics for anaphor esohition, but a forest of 
differently labeled trees that have to be 
internally searched and compared with each 
other by anaphor esolvers. 
Besides the efficiency issue, this methodology 
implies the conceptual awkwardne.,;s of having a 
<2 tnodnle of ~rammar that is not made operative 
~,ranallaatical processing, but as an during the <2 
extra-grammatical dd-on. Correa, 88, p.123~ 
observed that although the integration of binding 
constraints "into rules which rnay be used to 
derive structure that a l ready  satisfies the 
\[constraints\] is not a straightforward task", that 
should be the path to follow, a point also 
strongly stressed in subsequent elaboration o11 
this isstie by Merle, 93. 
1.2 Packag inganaphor ic  ambigu i ty  
A first proposal for enhancing integration of 
binding cosntraints into grammar is due to 
Correa, 88. Simplifying some details, the 
proposed algorithm can be outlined as in Fig.2. 
Start fl'om the top of the tree with two empty 
stacks A and B where indices will be 
collected, respectively ocal c-commanding 
indices and non-local c-commanding indices. 
While walking down a tree where every NP 
l-ias a distinct index (type): 
When an NP is found: 
(i) copy:leave a copy of A (if it is an 
anaphor) or B (if it is a pronoun) at the NP 
node; 
(ii) a,vsign:take the first index i of the stack 
copied into the NP node., take the NP iridex j, 
and annotate the NP with j=i; 
(iii) collect:add NP index j to A. 
When a local domain border is crossed: 
(iv) reset:reset B to A m:B. 
t,'i,g-. 2 -, Cor/ea's a&orithm 
This algorithm was given two different 
implementations, one by Correa, 88, the other by 
Ingria and Stallard, 89. Further elaboration by 
Giorgi et al, 90, and Pianesi, 91, led to a 
restatement of this algorithm using formal 
language techniques. 
The do-it-while-parsing approach of Correa's 
implelnenlation has the advantage of discarding 
)OSW a special-purpose \[ .' erammatical module for 
binding. That iml~lementation, however, tulns 
out to be del)cndent on a top-down parsing 
strategy. On the other hand, lngria and Stallard's 
implementation has the advantage of being 
indelmridenl of the parsing strategy adopted. 
105 
This was done however at the cost of still 
requiring a special purpose postgrammatical 
parsing module for binding. 
Besides the issue of bringing binding into 
grammar, it is worth noticing that this evolution 
inside the coindexation paradigm presented a
significant improvement and yet a clear 
drawback. On the one hand, if one disregards 
step (ii) (a disguised recency heuristic spuriously 
mixed with binding constraints) and considers 
the result of verifying binding constraints to be 
the assignment to an NP of the set of indices of 
its antecedent candidates, then it is possible to 
discard the proliferation of indexed trees as a 
way to express anaphoric ambiguity. 
On the other hand, the algorithm is 
acknowledged not to be able to cope with 
constraints possibly involving non-local 
anaphoric links. Principle C, backwards 
anaphora or cross-over cases were not accounted 
for (Correa, 88, p.127, Ingria and Stallard, 89, 
p.268). Moreover, as stack B only contains 
indices of the non-local c-commanders 2, but not 
all indices in the tree except those of the local 
c-commanders, Principle B is also not correctly 
accounted for. 
1.3 Packaging non-locality 
Other contributions to improve the coindexation 
method are due to Dalrymple, 93 and 
Johnson, 95. Instead of being directed to 
packaging ambiguity as the one above, they 
have in common being concerned with 
packaging non-locality. 
1.3.1 Nodes as mirrors o f  trees 
Johnson's algorithm is embodied in Prolog code. 
Abstracting away from details associated to that 
format, it gets the outline in Fig.3. 
2 C-command is a configurational version of the 
command relation where x c-commands y iff the first 
branching node that dominates x dominates y.
Although this outline renders the algorithm in a 
bottom up fashion, Johnson ingeniously 
developed an implementation that is independent 
of the parsing strategy resorting to delaying 
mechanisms. Crucially, in spite of its post 
grammat ica l  f lavor, l ikewise Correa's 
implementation, this algorithm does not require 
postgrammatical processing. 
These results were obtained with some 
(i) Repeat (ii) until all NP~ (l<_i_<n) in the 
tree have been used as starting points; 
(ii) Walk up the tree from NPi and 
repeat (iii) until the top node of the tree is 
reached; 
(iii.i) When other locally c-commanding 
NPj is found: 
(iii.i.i) if NPi is a short-distance 
reflexive, annotate NPi with i=j; 
(iii.i.ii) if NPi is a non-reflexive, 
annotate NP~ with i:~j; 
(iii.ii) When other non-locally 
c-commanding NP 3 is found: if NP~ is a 
non-pronoun, annotate NP~ with i~j. 
Fig. 3 - .lohnson's algorithm 
accessory devices: Each node in the tree is 
"conceptualized asa pair consisting of a tree and 
a vertex in that tree" (p.62). Consequently, the 
whole tree where a given NP appears is locally 
accessible to be "walked up" since its replica is 
present at the pair (Category, Tree), which is the 
NP node itself. 
This algorithm improves the coindexation 
methodology in terms of efficiency as it does not 
resort to free indexation. Note, however, that the 
anaphoric ambiguity of pronouns and 
nonprououns i not captured (Principles B and 
C) since grammatical coindexation of pronouns 
or nonpronouns with their possible antecedents 
is dismissed. Only reflexives and their 
antecedents end up coindexed, while the index 
of a pronoun is only made "unequal" with the 
106 
indices of its (non-grammatical) locally 
c-commanding antecedents. Nevertheless, even 
dispensing with free indexation and restricting 
the representation of anaphoric ambiguity to 
reflexives, this approach does not get rid of the 
proliferation of trees: For a given reflexive, each 
corresponding tree/coindexation represents a
different antecedent candidate. 
1.3.2 Equations with regular expressions 
The I_,FG/I)alrymple, 93, account of binding 
resorts to a different approach to generalize over 
the eventual non-locality of anaphoric links. It 
uses lexical "inside-.out equations", a 
special-purpose extension of the description 
formalism which may include regular' 
expressions (as in (3) below for long-distance 
reflexives): 
(1) John/introduced Billj to himself//)-. 
kimse(\[! ((()Bi-c;<,,,i "1" ) SUB J) o. = 1"c~ or 
((()13L(;<,~u "1" ) ()B\])c ~ = ~'o 
(2) *John introduced Bill/to hirrli. 
him: ((()BI<<,:,I I" ) ()BJ)o. ~ "i" o. 
(3) Zhangsani yiwei \[LMi yiwei\[...z_~i_i/j/k/...\]...\] 
Zkangsani thouglu \[Li.v(i tlumgkt f .. kimi/j,d../...J 
z(ji: ((OBJ * J" ) S\[JBJ)<, = 1" o 
The right-hand side of the equation stands for 
the sernantic representation (c~) of tile 
functional-strncture ('\]') of the anaphor. The left 
hand side stands for the semantics of the 
antecedent: In (3) the Chinese long-distance 
reflexive is an Object in a functional--structure 
where one of the upstairs Subjects may be the 
antecedent. 
Although initial scepticism about the tractability 
of these equations was dissipated by Kaphm and 
Maxwell, 88, the survey by Backofen et al, 96, 
reports that no iml~lemented I,FG grammar was 
known to handle binding. To a significant extent 
this bears on tile fact that many different 
equations have to be defined for every anaphor: 
Each equation specify concrete grammatical 
functions for the anaphor and its potential 
antecedent, but either the anaphor or the 
antecedents may occur with one of a range of 
several grammatical functions (see a n-finimal 
ex,'lnlple ill (1)). Moreover, it is not defined how 
non-lexical NPs (e.g. anaphoric definite 
descriptions, ruled by Principle C) may be 
assigned the respective equation. 
However these difficulties turn out to be solved, 
the LFG variant of the coindexation 
methodology presents the same type of problems 
of Johnson's approach. The proliferation of 
representations is not avoided: The ambiguity of 
reflexives may end up represented by several 
different grammatical representations. These 
representations correspond to the satisfaction of 
the different equations involving different 
grammatical functions, as in (1), and possibly 
result also from the several existential 
interpretations of functional tmcertainty in the 
case of long-distance reflexives, as in (3). 
Likewise, the ambiguity of pronouns is omitted 
in the single functional-structure resulting from 
the universal interpretation f negative quations 
associated with these anaphoric expressions. 
Moreover, the positive equations for reflexives 
do not require identity of indices between 
anaphorically related expressions, but instead 
impose identity of semantic representations, this 
way incorrectly enforcing any type of anaphora 
(bound, bridging, e-type, "donkey", etc.) to the 
sole modality of coreference. 
2 The  Concept  o f  B ind ing  Mach ine  
Being partially successful in overcoming 
problems of tile original post-gramnmtical 
"overgenerate & filter" methodology, each of tile 
contributions mentioned above brought to tile 
fore essential dimensions of binding that have to 
be concomitantly acconnted for. Accordingly, an 
alternative methodology for binding constraints 
107 
verification should find a way to harmonize all 
these dimensions: lexicalization, anaphoric 
ambiguity, packaging and non-local context 
packaging. 
Given these hints, a breakthrough depends now 
on changing some entrenched primitives 
under ly ing the concept ion of binding 
constraints. These constraints have been 
basically taken as syntactic well 
conditions: "\[they\] capture the distribution of 
pronouns and reflexives" (Reinhart and 
Reuland, 93, p.657). In line with Gawron and 
Peters, 90, however, we take them as conditions 
on semantic interpretation, as they delimit 
non-local aspects of meaning composition. 
In what follows, we set up a semantics-driven 
methodology for verifying binding constraints, 
organized under the rationale that an NP is a 
binding machine: (i) it reads a representation of
the context; (ii) updates its own semantics given 
this context and its own anaphoric potential (in 
accordance with its binding constraint, if it is a 
non-quantificational NP); (iii) and makes a 
contribution to the context, against which other 
NPs are interpreted. This rationale is in line with 
the insights of Johnson and Klein,90 concerning 
the processing of the semantics of nominals, and 
also the spirit (but by no means the letter) of the 
dynamic semantics framework (Chierchia, 95). 
The output of a nominal n as a binding machine 
is simply the incrementing of the context with a 
copy of its reference marker (Kamp and 
Reyle, 93). The internal state of the machine 
after its operation is a compacted representation 
of the anaphoric potential of n, if any, under the 
form of the set of the reference markers of the 
grammatically admissible antecedents of n - -  
this internal state results fiom the binding 
constraint, lexically associated to n, being 
applied to the input. The input is a representation 
of the relevant aspects of the context under the 
form of a set of three lists of reference markers, 
A ,  Z and U ,  from which the internal 
state/semantics of anaphors can be computed. 
Taking n and its subcategorizing predicator p, A 
is the list with the reference markers of the 
complements of p ordered according to their 
relative obliqueness; Z includes the elements of 
A plus the reference markers of the upstairs 
predicators directly or indirectly selecting the 
domaiu of p, observing the multiclausal 
obliqueness hierarchy; and U is the list of all 
reference markers in the discourse context. 
Given this setup, the verification of binding 
constraints consists in a few simple steps. If n is 
a short-distance r flexive, A' is associated to its 
semantic representation, where A' contains the 
reference markers of the o-commanders of n in 
A. If n is a long-distance reflexive, its semantic 
representation i cludes Z', such that Z' contains 
the o-commanders of n in Z. If n is a pronoun, 
the set B=U\(A'u{refin,,}) is coded into its 
representation. Finally if n is a nonpronoun, its 
semantics keeps a copy of C=US(Z'u{ refin,,}). 
3 An HPSG exercise 
This methodology can be easily accommodated 
in a unification-based framework such as HPSG. 
We designed an extension to the UDRT 
component for HPSG of Frank and Reyle, 95. 
This component is encoded as the CONT(ENT) 
value, which is enhanced now with feature 
ANAPH(ORA). On a par with this extension, also 
the NONLOC(AL) value is extended with the new 
feature BIND(ING), with subfeatures LIST-A, 
LIST-Z, LIST-U and LIST-protoU. 
The SYNSEM value of a pronoun is as follows: 
p ,  MAX 
LS LI,-MIN ~V~\] 
SU.ORD { } 
J\[l ,ABEl, LOC\[CONT / C?NDs \[LARG-R ~\ ]}  
\[.A.NAPIt \[-REFM \[JJ \[ANTEC non - Ioc -  ocom(lW\], VVI,~j-) 
-LIST- A 
NLOC I BIND LIST- Z IJST- U 
LIST- protoU (\[~\])j 
108 
The relational constraint ~on-loc-ocomm takes 
(in first argnnlent) all markers in the context. 
~iven in LISI'-U wflue, and remove l'rom them 
both the local o-commanders (included in 
second argunrent) of tlie t)ronoun and the 
pronoun itself (in third argument). 
Under the conception of nominals as binding 
machines, LISTSA, LIST-Z and LIST-U stand for 
the input, ANTEC(EDFNTS) encodes the internal 
state, and REF(ERENCE)M(ARKF.R) encodes the 
output. The SYNSEM of other anaphors, ruled by 
Principles A, C or Z, art quite similar to the one 
above. The major difference lies in the relational 
constraints in ANTEC value, which encode the 
corresponding binding constraint 3. 
Turning now to the lists with reference markers, 
we handle them by means of a new HPSG 
principle, the Binding I)omains Principle. This 
principle consists of three clauses constraining 
signs and their values with respect o these lists. 
I)ue to space limitations a, we illustrate this 
Principle with its ('lause 1, for LIST-U and 
LIST-protoU: 
Binding Domain~ Principle, Clause I
(i) in every sign, LlST-protoW value is identical 
to the concatenation f LlST-protoU values of its 
daughters; 
(ii) in a sign of sort discourse, I,IST-protoU 
and LIST-U wflues are token-identical; 
(iii) in a non-NP sign, LIST-U wflue is 
token-identical to each LIST-U value of its 
daughters; 
(iv) in an NP sign k: 
(iv.i) in Spec-daughter, LIST-U value is the 
result of removing the elements of LIST-e\ value 
of Head-daughter from the I,IST-U value ot' k; 
3 Binding constraints fin non-lexical nominals are 
lexically stated in their determiners. 
4 Binding constraints are fttlly integrated in a 
computational lIPS(; gramma,, documented in 
(Branco, 99). 
(iv.ii) in Head-daughter, LIST-U value is the 
result of removing the value of REFM of 
Spec-daughter from the IJST-U value of k. 
The HPSG ontology was extended with the sort 
d iscourse  corresponding to sequences of 
sentential signs. Subclause (iv) above is meant 
to avoid what is known as i-within-i effect. 
Conclusion 
In this paper we designed an alternative to the 
mainstream postgrammatical "overgenerate & 
filter" methodology for the verification of 
binding constraints. Our semantics-driven 
methodology is based on the conception of NPs 
as binding machines. We showed how this 
innovation helped to integrate binding 
constraints into grammar epresentation and 
processing and to avoid the intractability implied 
by the mainstream ethodology. 
Acknowledgements  
I am grateful to Hans Uszkoreit for patient 
advice and criticism. My thanks go also to Mark 
Johnson for helpful discussion. 
Annex 
Recent results (Xue et al, 94, Branco and 
Marrafa, 98) indicate that there are four binding 
constraints: 
Principle A.' 
A locally o-commanded short-distance r flexive 
must be locally o-bound. 
Lee i thinks \[Ma.~ i saw himselj,i/j/. 
Principle Z: 
An o-commanded long-distance r llexive must 
be o-bound. 
Zhang.valq zhidao /Lis'{/ remvei \[gqH~gwttk 
:ui .vihmm z!/i/.//kl/ 
Zhangsan k ow \[Lisi think \[Wangwu most like sell'I\] 
Zhangsani knows that Lis!/ thinks that \Vangwu k likes 
himi,j/lmnself k most (Xue et al, 94) 
1 09 
Principle B: 
A pronoun must be locally o-free. 
Leei thinl~v \[Ma.x). saw himi/*j\]\]. 
Principle C: 
A nonpronoun must be o-free. 
\[Kimi'sJJ'iend \]j tkinks \[Lee saw Kimi/*j\]. 
These constraints are defined on the basis of 
some auxil iary notions. The notion of local 
domain involves the partition of sentences and 
associated grammatical geometry into two zones 
of greater or less structural proximity with 
respect o the anaphor. O-command is a partial 
order under which, in a clause, Subjects 
o-command Direct Objects, Direct Objects 
o -command Indirect Objects, and so on, 
following the usual obliqueness hierarchy of 
grammat ica l  funct ions,  being that in a 
multiclausal sentence, the upstairs arguments 
o-command the embedded arguments, etc. The 
notion of o-binding is such that x o-binds y iff x 
o..commands y and x and 3' are coindexed, where 
coindexation is meant to represent anaphoric 
links. 
References 
Backofen, R., T. Becker, J. Calder, 3. Capstick, 
L. Dini, J. D6rre, G. Erbach, D. Estival, 
S. Manandhar, A. Mineur, G. van Noord, S. Oepen, 
1t. Uszkoreit. 1996. Final Report of the EAGLES 
Formalisms Working Group. Luxemburg: EC. 
Branco, A., and P. Marrafa. 1998. Long-distance 
Reflexives and the Binding Square of Opposition. 
G. Webelhuth, J. Koenig and A, Kathol. eds. 
Lexical and Constructiomll Aspects of Linguistic 
Explamaion. Stanford: CSLI Press, 163-177. 
Branco, A.. 1999. Reference ProcessMg and its 
Ulliversal ConstraMts. Doctoral disse,tation. 
Lisbon: University of Lisbon. 
Chierchia, G.. 1995. Dylmmics of Mealling. 
Chicago:Univ. of Chicago Press. 
Chomsky, N.. 1980. On Binding. Linguistic lnquily, 
1-46. 
Chomsky, N.. 1981. Lectures on GovermneJlt and 
Binditlg. Dordrecht: Foris. 
Cor,ea, N.. 1988. A Binding Rule for 
Government.-binding Parsing. COLING'8& 
123-129. 
Dah'ymple, M.. 1993. The Sy~tax of Amtphoric 
Binding. Stanford: CSLI Press. 
Erbach, G.. 1995. ProFIT, Prolog with Features, 
Inheritance and Templates. EACL'95, 180-187. 
Fong, S.. 1990. Free Indexation. ACL'90, 105-110. 
Frank, A., and U. Reyle. 1995. Principle--Based 
Semantics for HPSG. EACL'95, 9-16. 
Giorgi, A., F. Pianesi and G. Satta. 1990. A 
Computational Approach to Binding Theory. 
COLING'90, 1-6. 
Ingria, R., and D. Stallard. 1989. A Computational 
Mechanism for Pronominal Reference. ACL'89, 
262-271. 
Johnson, M.. 1995. Constraint-based Natural 
Language Parsing. Barcelona: 7th ESSLI. Course 
Notes. 
Johnson, M., and E. Klein. 1990. Discourse, 
Anaphora nd Parsing. ACL'90, 197-302. 
Kamp, H., and U. Reyle. 1993. From Discourse to 
Logic. Dordrecht: Kluwer. 
Kaplan, R., and J. Maxwell. 1988. An Algorithm for 
Functional Uncertainty. COLING'88, 297-302. 
Merlo, P.. 1993. For an Incremental Computation of 
Intra-sentential Coreference. IJCAI'93, 1216-1221. 
Pianesi. F.. 1991. Indexing and Referential 
Dependencies within Binding Theory. EACL'93, 
39-44. 
Pollard, C., and I. Sag. 1994. Head-Driven Phrase 
Structure Grammar. Stanford: CSLI Press. 
Reinhart, T. and E, Reuland. 1993. Reflexivity. 
Linguistic lnquio,, 657-720. 
Xue, P., C. Pollard, and I. Sag. 1994. A New 
Perspective o,1 Chinese Ziji. WCCFLI3. Stanford: 
CSLI Press. 
110 
A Suite of Shallow Processing Tools for Portuguese:
LX-Suite
Anto?nio Branco
Department of Informatics
University of Lisbon
ahb@di.fc.ul.pt
Joa?o Ricardo Silva
Department of Informatics
University of Lisbon
jsilva@di.fc.ul.pt
Abstract
In this paper we present LX-Suite, a set
of tools for the shallow processing of Por-
tuguese. This suite comprises several
modules, namely: a sentence chunker, a
tokenizer, a POS tagger, featurizers and
lemmatizers.
1 Introduction
The purpose of this paper is to present LX-Suite,
a set of tools for the shallow processing of Por-
tuguese, developed under the TagShare1 project by
the NLX Group.2
The tools included in this suite are a sentence
chunker; a tokenizer; a POS tagger; a nominal fea-
turizer; a nominal lemmatizer; and a verbal featur-
izer and lemmatizer.
These tools were implemented as autonomous
modules. This option allows to easily replace any
of the modules by an updated version or even by a
third-party tool. It also allows to use any of these
tools separately, outside the pipeline of the suite.
The evaluation results mentioned in the next
sections have been obtained using an accurately
hand-tagged 280, 000 token corpus composed of
newspaper articles and short novels.
2 Sentence chunker
The sentence chunker is a finite state automaton
(FSA), where the state transitions are triggered
by specified character sequences in the input, and
the emitted symbols correspond to sentence (<s>)
and paragraph (<p>) boundaries. Within this
setup, a transition rule could define, for example,
1http://tagshare.di.fc.ul.pt
2NLX?Natural Language and Speech Group, at the De-
partment of Informatics of the University of Lisbon, Faculty
of Sciences: http://nlx.di.fc.ul.pt
that a period, when followed by a space and a cap-
ital letter, marks a sentence boundary:
?? ? ?. A? ? ?? ? ?? ? ?.</s><s>A? ? ??
Being a rule-based chunker, it was tailored to
handle orthographic conventions that are specific
to Portuguese, in particular those governing dia-
log excerpts. This allowed the tool to reach a very
good performance, with values of 99.95% for re-
call and 99.92% for precision.3
3 Tokenizer
Tokenization is, for the most part, a simple task,
as the whitespace character is used to mark most
token boundaries. Most of other cases are also
rather simple: Punctuation symbols are separated
from words, contracted forms are expanded and cl-
itics in enclisis or mesoclisis position are detached
from verbs. It is worth noting that the first ele-
ment of an expanded contraction is marked with
a symbol (+) indicating that, originally, that token
occurred as part of a contraction:4
um, dois ?|um|,|dois|
da ?|de+|a|
viu-o ?|viu|-o|
In what concerns Portuguese, the non-trivial as-
pects of tokenization are found in the handling of
ambiguous strings that, depending on their POS
tag, may or may not be considered a contrac-
tion. For example, the word deste can be tok-
enized as the single token |deste| if it occurs
as a verb (Eng.: [you] gave) or as the two tokens
|de+|este| if it occurs as a contraction (Eng.:
of this).
3For more details, see (Branco and Silva, 2004).
4In these examples the | symbol will be used to mark
token boundaries more clearly.
179
It is worth noting that this problem is not a mi-
nor issue, as these strings amount to 2% of the cor-
pus that was used and any tokenization error will
have a considerable negative influence on the sub-
sequent steps of processing, such as POS tagging.
To resolve the issue of ambiguous strings, a
two-stage tokenization strategy is used, where the
ambiguous strings are not immediately tokenized.
Instead, the decision counts on the contribution of
the POS tagger: The tagger must first be trained
on a version of the corpus where the ambiguous
strings are not tokenized, and are tagged with a
composite tag when occurring as a contraction (for
example P+DEM for a contraction of a preposition
and a demonstrative). The tagger then runs over
the text and assigns a simple or a composite tag to
the ambiguous strings. A second pass with the to-
kenizer then looks for occurrences of tokens with
a composite tag and splits them:
deste/V?|deste/V|
deste/P+DEM?|de+/P|este/DEM|
This approach allowed us to successfully re-
solve 99.4% of the ambiguous strings. This is a
much better value than the baseline 78.20% ob-
tained by always considering that the ambiguous
strings are a contraction.5
4 POS tagger
For the POS tagging task we used Brant?s TnT tag-
ger (Brants, 2000), a very efficient statistical tag-
ger based on Hidden Markov Models.
For training, we used 90% of a 280, 000 token
corpus, accurately hand-tagged with a tagset of ca.
60 tags, with inflectional feature values left aside.
Evaluation showed an accuracy of 96.87% for
this tool, obtained by averaging 10 test runs over
different 10% contiguous portions of the corpus
that were not used for training.
The POS tagger we developed is currently the
fastest tagger for the Portuguese language, and it
is in line with state-of-the-art taggers for other lan-
guages, as discussed in (Branco and Silva, 2004).
5 Nominal featurizer
This tool assigns feature value tags for inflection
(Gender and Number) and degree (Diminutive,
Superlative and Comparative) to words from nom-
inal morphosyntactic categories.
5For further details see (Branco and Silva, 2003).
Such tagging is typically done by a POS tagger,
by using a tagset where the base POS tags have
been extended with feature values. However, this
increase in the number of tags leads to a lower tag-
ging accuracy due to the data-sparseness problem.
With our tool, we explored what could be gained
by having a dedicated tool for the task of nominal
featurization.
We tried several approaches to nominal featur-
ization. Here we report on the rule-based approach
which is the one that better highlights the difficul-
ties in this task.
For this tool, we built on morphological regular-
ities and used a set of rules that, depending on the
word termination, assign default feature values to
words. Naturally, these rules were supplemented
by a list of exceptions, which was collected by us-
ing an machine readable dictionary (MRD) that al-
lowed us to search words by termination.
Nevertheless, this procedure is still not enough
to assign a feature value to every token. The
most direct reason is due to the so-called invari-
ant words, which are lexically ambiguous with re-
spect to feature values. For example, the Common
Noun ermita (Eng.: hermit) can be masculine or
feminine, depending on the occurrence. By simply
using termination rules supplemented with excep-
tions, such words will always be tagged with un-
derspecified feature values:6
ermita/?S
To handle such cases the featurizer makes use of
feature propagation. With this mechanism, words
from closed classes, for which we know their fea-
ture values, propagate their values to the words
from open classes following them. These words,
in turn, propagate those features to other words:
o/MS ermita/MS humilde/MS
Eng.: the-MS humble-MS hermit-MS
but
a/FS ermita/FS humilde/FS
Eng.: the-FS humble-FS hermit-FS
Special care must be taken to avoid that feature
propagation reaches outside NP boundaries. For
this purpose, some sequences of POS categories
block feature propagation. In the example below,
a PP inside an NP context, azul (an ?invariant?
6Values: M:masculine, F:feminine, S:singular, P:plural
and ?:undefined.
180
adjective) might agree with faca or with the pre-
ceding word, ac?o. To prevent mistakes, propaga-
tion from ac?o to azul should be blocked.
faca/FS de ac?o/MS azul/FS
Eng.: blue (steel knife)
or
faca/FS de ac?o/MS azul/MS
Eng.: (blue steel) knife
For the sake of comparability with other pos-
sible similar tools, we evaluated the featurizer
only over Adjectives and Common Nouns: It has
95.05% recall (leaving ca. 5% of the tokens with
underspecified tags) and 99.05% precision.7
6 Nominal lemmatizer
Nominal lemmatization consists in assigning to
Adjectives and Common Nouns a normalized
form, typically the masculine singular if available.
Our approach uses a list of transformation rules
that helps changing the termination of the words.
For example, one states that any word ending in
ta should have that ending transformed into to:
gata ([female] cat)
? gato ([male] cat)
There are, however, exceptions that must be ac-
counted for. The word porta, for example, is a
feminine common noun, and its lemma is porta:
porta (door, feminine common noun)
? porta
Relevant exceptions like the one above were
collected by resorting to a MRD that allowed to
search words on the basis of their termination. Be-
ing that dictionaries only list lemmas (and not in-
flected forms), it is possible to search for words
with terminations matching the termination of in-
flected words (for example, words ending in ta).
Any word found by the search can thus be consid-
ered as an exception.
A major difficulty in this task lies in the list-
ing of exceptions when non-inflectional affixes are
taken into account. As an example, lets con-
sider again the word porta. This word is an
exception to the rule that transforms ta into to.
As expected, this word can occur prefixed, as
in superporta. Therefore, this derived word
7For a much more extensive analysis, including a compar-
ison with other approaches, see (Branco and Silva, 2005a).
should also appear in the list of exceptions to pre-
vent it from being lemmatized into superporto
by the rule. However, proceeding like this for ev-
ery possible prefix leads to an explosion in the
number of exceptions. To avoid this, a mechanism
was used that progressively strips prefixes from
words while checking the resulting word forms
against the list of exceptions:
supergata
-----gata (apply rule)
? supergato
but
superporta
-----porta (exception)
? superporta
A similar problem arises when tackling words
with suffixes. For instance, the suffix -zinho
and its inflected forms (-zinha, -zinhos and
-zinhas) are used as diminutives. These suf-
fixes should be removed by the lemmatization pro-
cess. However, there are exceptions, such as the
word vizinho (Eng.: neighbor) which is not a
diminutive. This word has to be listed as an excep-
tion, together with its inflected forms (vizinha,
vizinhos and vizinhas), which again leads
to a great increase in the number of exceptions. To
avoid this, only vizinho is explicitly listed as an
exception and the inflected forms of the diminu-
tive are progressively undone while looking for an
exception:
vizinhas (feminine plural)
vizinha (feminine singular)
vizinho (exception)
? vizinho
To ensure that exceptions will not be over-
looked, when both these mechanisms work in par-
allel one must follow all possible paths of affix re-
moval. An heuristic chooses the lemma as being
the result found in the least number of steps.8
To illustrate this, consider the word antena
(Eng.: antenna). Figure 1 shows the paths fol-
lowed by the lemmatization algorithm when it is
faced with antenazinha (Eng.: [small] an-
tenna). Both ante- and -zinha are possible
affixes. In a first step, two search branches are
opened, the first where ante- is removed and
the second where -zinha is transformed into
8This can be seen as following a rationale similar to Karls-
son?s (1990) local disambiguation procedure.
181
antenazinha
nazinha antenazinho
nazinho nazinho antena
na
no
na
no
0
1
2
3
4
Figure 1: Lemmatization of antenazinha
-zinho. The search proceeds under each branch
until no transformation is possible, or an exception
has been found. The end result is the ?leaf node?
with the shortest depth which, in this example, is
antena (an exception).
This branching might seem to lead to a great
performance penalty, but only a few words have
affixes, and most of them have only one, in which
case there is no branching at all.
This tool evaluates to an accuracy of 94.75%.9
7 Verbal featurizer and lemmatizer
To each verbal token, this tool assigns the corre-
sponding lemma and tag with feature values for
Mood, Tense, Person and Number.
The tool uses a list of rules that, depending on
the termination of the word, assign all possible
lemma-feature pairs. The word diria, for exam-
ple, is assigned the following lemma-feature pairs:
diria
? ?dizer,Cond-1ps?
? ?dizer,Cond-3ps?
? ?diriar,PresInd-3ps?
? ?diriar,ImpAfirm-2ps?
Currently, this tool does not attempt to disam-
biguate among the proposed lemma-feature pairs.
So, each verbal token will be tagged with all its
possible lemma-feature pairs.
The tool was evaluated over a list with ca.
800, 000 verbal forms. It achieves 100% preci-
sion, but at 50% recall, as half of those forms
are ambiguous and receive more than one lemma-
feature pair.
9For further details, see (Branco and Silva, 2005b).
8 Final Remarks
So far, LX-Suite has mostly been used in-house
for projects being developed by the NLX Group.
It is being used in the GramaXing project,
where a computational core grammar for deep lin-
guistic processing of Portuguese is being devel-
oped under the Delphin initiative.10
In collaboration with CLUL,11 and under the
TagShare project, LX-Suite is being used to help
in the building of a corpus of 1 million accurately
hand-tagged tokens, by providing an initial, high-
quality tagging which is then manually corrected.
It is also used for the QueXting project, whose
aim is to make available a question answering sys-
tem on the Portuguese Web.
There is an on-line demo of LX-Suite located
at http://lxsuite.di.fc.ul.pt. This
on-line version of the suite is a partial demo,
as it currently only includes the modules up to
the POS tagger. By the end of the TagShare
project (mid-2006), all the other modules de-
scribed in this paper are planned to have been
included. Additionally, the verbal featurizer and
lemmatizer can be tested as a standalone tool at
http://lxlemmatizer.di.fc.ul.pt.
Future work will be focused on extending the
suite with new tools, such as a named-entity rec-
ognizer and a phrase chunker.
References
Branco, Anto?nio and Joa?o Ricardo Silva. 2003. Con-
tractions: breaking the tokenization-tagging circu-
larity. LNAI 2721. pp. 167?170.
Branco, Anto?nio and Joa?o Ricardo Silva. 2004. Evalu-
ating Solutions for the Rapid Development of State-
of-the-Art POS Taggers for Portuguese. In Proc. of
the 4th LREC. pp. 507?510.
Branco, Anto?nio and Joa?o Ricardo Silva. 2005a. Ded-
icated Nominal Featurization in Portuguese. ms.
Branco, Anto?nio and Joa?o Ricardo Silva. 2005b. Nom-
inal Lemmatization with Minimal Word List. ms.
Brants, Thorsten. 2000. TnT - A Statistical Part-of-
Speech Tagger. In Proc. of the 6th ANLP.
Karlsson, Fred 1990. Constraint Grammar as a
Framework for Parsing Running Text. In Proc. of
the 13th COLING.
10http://www.delph-in.net
11Linguistics Center of the University of Lisbon
182
Binding Machines
Anto?nio Branco
University of Lisbon
Binding constraints form one of the most robust modules of grammatical knowledge. Despite their
crosslinguistic generality and practical relevance for anaphor resolution, they have resisted full
integration into grammar processing. The ultimate reason for this is to be found in the original
exhaustive coindexation rationale for their specification and verification. As an alternative, we
propose an approach which, while permitting a unification-based specification of binding con-
straints, allows for a verification methodology that helps to overcome previous drawbacks. This
alternative approach is based on the rationale that anaphoric nominals can be viewed as binding
machines.
1. Introduction
Since the so-called integrative approach to anaphor resolution was developed in the
late 1980s (Carbonell and Brown 1988; Rich and LuperFoy 1988; Asher and Wada
1989), and its practical viability extensively tested (e.g., Lappin and Leass 1994; Mitkov
1997, 1998), it has been common wisdom that factors determining the antecedents
of anaphors divide into filters and preferences. The former exclude impossible an-
tecedents and help to circumscribe the set of antecedent candidates; the latter help to
pick the most likely candidate, which will be proposed as the antecedent.
Binding constraints are a significant subset of such filters. As they delimit the rel-
ative positioning of anaphors and their possible antecedents in grammatical geometry,
these constraints are crucial to restricting the search space for antecedents and enhanc-
ing the performance of anaphor resolvers.1 From an empirical perspective, they stem
from quite robust generalizations and exhibit a universal character, given their param-
eterized validity across natural languages. From a conceptual point of view, in turn, the
relations among binding constraints involve nontrivial symmetry, which lends them a
modular nature. Accordingly, they have been considered one of the most robust and
intriguing grammar submodules, usually referred to as binding theory. However, in
contrast to this, the formal and computational handling of binding constraints has
presented considerable resistance.
Anaphor resolution typically builds on many sources of information?among them,
information about the grammatical structure of the sentence?so that the different fil-
ters and preferences may be used. Consequently, it must in general be regarded as a
postgrammatical process, in the sense that it is completed after sentences are parsed.
Binding constraints, as a subset of the filters for anaphor resolution, are a special case
 Department of Informatics, Faculdade de Cie^ncias de Lisboa, Campo Grande, 1700 Lisboa, Portugal.
E-mail: Antonio.Branco@di.fc.ul.pt.
1 See the Appendix for a specification of binding constraints. We adhere to the following terminological
convention: anaphors divide into reflexives and nonreflexives; reflexives form a class that includes
short-distance (ruled by Constraint A; e.g., himself ) and long-distance reflexives (Constraint Z; e.g.,
Chinese ziji); nonreflexives include pronouns (Constraint B; e.g., he) and nonpronouns (Constraint C;
e.g., the student).
c? 2002 Association for Computational Linguistics
Computational Linguistics Volume 28, Number 1
in this respect. Given that they form a submodule of grammar, they are specified on a
par with other grammatical submodules and constraints, and they are thus expected
to be integrated already into the processing of grammar. Nevertheless, this integration
cannot be considered to have been adequately achieved.
As we will discuss at length, the original methodology for verifying the compliance
of grammatical representations with binding constraints requires extragrammatical
processing steps delivering a forest of indexed trees to anaphor resolvers (Chomsky
1981). More recently, constraint-based grammatical frameworks either require special-
purpose extensions of the description formalism, though ensuring only a partial han-
dling of these constraints, as in Lexical-Functional Grammar (LFG; Dalrymple 1993), or
do not offer a solution yet to integrate them into grammar, as in Head-Driven Phrase
Structure Grammar (HPSG; Pollard and Sag 1994).2
Our primary goal here is thus to bridge the gap between the grammatical nature
of binding constraints and their full integration into grammar processing. In particular,
we aim at achieving this in such a way that a lean interface between grammar and
reference processing emerges.
In Section 2, we first underline the distinction, seldom taken into account, between
specification and verification of binding constraints. We then review advances pro-
posed in the literature concerning the completion of the verification task. We observe
that three major lines of progress can be identified: packing of anaphoric ambiguity,
packing of nonlocal context, and lexicalization of binding constraints.
Building on these contributions, in Section 3 we argue that the remaining step
forward is to harmonize these different advances. We suggest that a more accurate,
semantics-driven comprehension of the nature of binding constraints is a relevant
move toward this harmonization. On the basis of this revision, we introduce a method-
ology for verifying these constraints, which rests on the new concept of binding ma-
chine, to be defined.
In Section 4, in the light of this new methodology, we show how binding con-
straints can be given a unification-based specification and can be fully integrated into
grammar.
In Section 5, we present an illustrative example and discuss in detail how binding
constraints and reference-processing systems are coordinated, and how the previously
identified drawbacks are overcome.
2. Advances in the Verification Task
In recent decades, great strides have been made toward an empirically adequate spec-
ification of binding constraints, this being an important research issue in theoretical
linguistics. Many aspects of this issue?a parameterizable definition of local domain,
the existence of a fourth constraint for long-distance reflexives, the possible subject-
orientedness of some anaphors, and the degree of universality of binding constraints,
to name just a few?have come under intense scrutiny.
In contrast, the verification task has been studied much less extensively. Even
though important problems also remain to be solved in this more applied dimension
2 The fragment of grammar developed and extensively discussed in Pollard and Sag (1994) is formally
specified in its Appendix with the HPSG unification-based description language. Binding constraints
escape such encoding. While noting that these constraints have yet to be accommodated in HPSG
grammars, Bredenkamp?s (1996) and Backofen et al?s (1996) subsequent elaboration of this issue
implies that some kind of essential limitation of the unification-based formalism might have been
reached, a suggestion we seek to contradict here.
2
Branco Binding Machines
of the so-called binding theory, the issue of determining whether a given grammatical
representation complies with binding constraints has not attracted similar attention.
In this section, we briefly review major advances reported in resolving this issue.
2.1 Exhaustive Coindexing for Filtering
The first formulation of a verification procedure, based on exhaustive coindexation,
dates back to Chomsky (1980, Appendix; 1981, Section 3.2.3). The basics of this ap-
proach can be outlined as follows:
After the grammatical parsing of a sentence with n NPs has been completed,
for every parse tree t:
a. Indexation: Generate a new, annotated tree by assigning indices to the NPs
in t.
b. Filtering: Store this annotated tree if the indexation of NPs respects binding
constraints; otherwise, delete it.
c. Iteration: Repeat (a)?(b) until all type-different assignments of n possibly
different indices have been exhausted.
As discussed in Correa (1988), this procedure is grossly inefficient: its complex-
ity was shown in Fong (1990) to be of exponential order. Moreover, this approach
is conceptually awkward, given that a submodule of the grammar, the set of bind-
ing constraints, is not operative during grammatical processing, but functions as an
extragrammatical add-on.3
This proposal also disregards the need to interface grammar with systems for ref-
erence processing. The input for such systems will not be a grammatical representation
to be refined vis-a`-vis the preferences for anaphor resolution, but a forest of differ-
ently labeled trees that have to be internally searched and compared with each other
by anaphor resolvers.
2.2 Packing Anaphoric Ambiguity
A first proposal for improving the exhaustive coindexation-driven methodology is due
to Correa (1988), whose goal was to enhance the integration of binding constraints into
grammar and obtain a tractable verification procedure.
Simplifying some details, the proposed algorithm can be outlined as follows:
Let t be a constituency tree where every NP has a type-distinct index. Start from
the top node of t with two empty stacks, A and B, where indices will be collected,
respectively local c-commanding4 indices and nonlocal c-commanding indices,
while descending the tree. When an NPj is found:
a. Copy: Leave a copy of A (if NPj is a short-distance reflexive) or B (if it is a
pronoun) at the NPj.
3 Correa (1988, page 123) observes that although the integration of binding constraints ?into rules which
may be used to derive structure that already satisfies the [constraints] is not a straightforward task,?
that should be the path to follow, a point also strongly stressed in subsequent elaboration on this issue
by Merlo (1993).
4 C-command is a configurational version of the command relation where x c-commands y iff the first
branching node that dominates x dominates y (Barker and Pullum 1990).
3
Computational Linguistics Volume 28, Number 1
b. Assign: Take the first index i of the stack copied into the NPj node, and
annotate NPj with j = i.
c. Collect: Add index j to A in each sister node of NPj.
When a local domain border is crossed:
d. Reset: Reset B to A [ B.
This algorithm has been given two different implementations, one by Correa
(1988), the other by Ingria and Stallard (1989). Further elaboration by Giorgi, Pianesi,
and Satta (1990) and Pianesi (1991) offers a variant in terms of formal language tech-
niques, where the stack copied into pronouns contains the antecedent candidates ex-
cluded by Principle B.
The ?do-it-while-parsing? approach of Correa?s implementation has the advantage
of discarding a special-purpose postgrammatical module for binding. Nevertheless,
this solution turns out to be dependent on a top-down parsing strategy. On the other
hand, while Ingria and Stallard?s implementation is independent of the parsing strat-
egy adopted, its independence comes at the cost of still requiring a special-purpose
postgrammatical parsing module for binding.
Besides incorporating binding theory into grammar, Correa?s development inside
the coindexation-driven methodology presents other significant improvements. If one
disregards step (b)?a disguised recency preference mixed with binding constraints?
and considers the result of verifying these constraints to be the assignment to an NP
of the set of indices of its grammatically admissible antecedents, then it is possible
to discard the proliferation of indexed trees as a way to express anaphoric ambigu-
ity. Moreover, this packing of anaphoric ambiguity provides for a neat interface with
anaphor resolvers, whose preferences will then pick the most likely antecedent candi-
date from the relevant stack of indices.
These advances permit a verification procedure of tractable complexity (Correa
1988, page 127; Giorgi, Pianesi, and Satta 1990, page 5). This results crucially from
the move toward the lexicalization of the constraining effect of binding principles, a
solution also adopted in subsequent proposals by other authors, as we will discuss
below. The binding constraint of each anaphor is now enforced independently of how
the surrounding anaphors happen to be resolved. This implies that there is no need
to anticipate all the different resolutions for every relevant anaphor with a process of
exhaustive coindexation. It also implies that cases of undesired transitive anaphoricity
are handled by other filters during the anaphor resolution process.5
However, these positive results regarding the verification task seem to be obtained
at the cost of some negative consequences regarding the specification task and em-
pirical adequacy. The above algorithm is acknowledged not to be able to cope with
5 Consider the sentence John said that he shaved him. Ignoring how other anaphors are resolved, in the light
of Binding Constraint B, he can take John as its antecedent, as empirically replicated in other minimally
different examples such as Johni said that he shaved Peter; likewise, him can take John as its antecedent. A
point worth noting is that, if he actually ends up resolved against John, the latter cannot be the
antecedent of him, and vice versa. This specific resolution of he and him, out of the many possible
resolutions, blocks two anaphoric links that would otherwise have been admissible. It induces a
contingent violation of binding constraint B due to an accidental, transitive anaphoric relationship
between he and him.
This issue is not discussed in Correa (1988), since this paper is strictly focused on syntax and
binding. See footnote 13 below for a suggestion on how this issue may be handled in a grammatical
framework integrating syntactic and semantic representations.
4
Branco Binding Machines
constraints involving nonlocal dependencies. It does not account for Principle C, and
it only partially accommodates the anaphoric potential of anaphors complying with
Principle B. As Stack B only contains indices of the nonlocal c-commanders?rather
than all indices except those of the local c-commanders?the algorithm does not cor-
rectly account for the constraining effect of Principle B. Also this approach does not
account for backward anaphora or crossover cases (Correa 1988, page 127; Ingria and
Stallard 1989, page 268).6
2.3 Packing Nonlocality
Other improvements in the task of verifying binding constraints are due to Dalrymple
(1993) and Johnson (1995). Instead of being concerned with packing ambiguity, they
are concerned with packing nonlocality.
2.3.1 Trees in Nodes of Trees. Johnson?s (1995) algorithm is embodied in Prolog code.
Abstracting away from details associated with that format, it can be outlined as fol-
lows:
Let t be a constituency tree where every NP has a type-distinct index. For
every NPi in t, traverse the tree from NPi upward until the top node is reached.
When a locally c-commanding NPj is found:
a. Annotate NPi with i = j if NPi is a short-distance reflexive.
b. Annotate NPi with i 6= j if NPi is a nonreflexive.
When a nonlocally c-commanding NPj is found:
c. Annotate NPi with i 6= j if NPi is a nonpronoun.
Although this outline renders the algorithm in a bottom-up fashion, Johnson inge-
niously develops an implementation of it that is independent of the parsing strategy by
resorting to delaying mechanisms. Consequently, despite its postgrammatical flavor,
this implementation does not require postgrammatical processing, thus incorporating
the task of binding constraint verification into grammar processing.
These results are obtained with some auxiliary devices. Each node in the tree is
?conceptualized as a pair consisting of a tree and a vertex in that tree? (Johnson 1995,
page 62). Consequently, the whole tree where a given NP appears is locally accessible
to be ?walked up? since its replica is present at the pair (Category, Tree), which is the
NP node itself.
This algorithm makes the verification of binding constraints more efficient because
it does not resort to exhaustive indexation. However, it does so at the cost of highly
complicating the grammatical representation, since the tree is replicated at each one
of its nodes.
While avoiding exhaustive indexation, this approach does not fully eliminate the
proliferation of trees. For a given ambiguous reflexive, with more than one admissible
6 See the Appendix for the notion of locality and local domain and other auxiliary notions in the
definition of binding constraints.
Backward anaphora occurs in cases where the anaphor is resolved against an antecedent that
occurs linearly after the anaphor, as in If hei is around, Peteri will do it.
An example of so-called crossover cases is the ungrammatical construction *Whoi did Peter think shei
saw? or *Peteri, hei said you like, where the fronted phrase is meant to be the antecedent of some pronoun
c-commanding the position from which this phrase is displaced.
5
Computational Linguistics Volume 28, Number 1
antecedent, each antecedent candidate corresponds to a different coindexation and,
consequently, to a different tree. That is what generally happens with long-distance
reflexives, whose antecedents can be found in any of the binding domains induced
by the local or by the upward predicators, but it may also happen with short-distance
ones, as in (2) below.
As to the interface with reference processing, problems arise with reflexives and
nonreflexives, though of different nature. Reflexives, if ambiguous, give rise to prolif-
eration of trees, thus requiring comparison between trees during subsequent anaphor
resolution.
As to nonreflexives?pronouns and nonpronouns?their analysis does not give
rise to proliferation of trees, but the representation of their ambiguity is not fully
made explicit in the grammatical representation of the sentence being parsed. This
is so because they end up associated with negative information, that is, information
about what NPs cannot be their antecedents. The index of a pronoun is made unequal
with the indices of its local c-commanders; it is not made equal with the indices
of its grammatically admissible antecedents. The same holds for nonpronouns with
respect to their c-commanders. Consequently, in this case, the task of determining
the antecedent candidates that satisfy the relevant binding constraint of nonreflexives
remains to be completed after grammatical processing is finished. This will involve
some postgrammatical rescanning of the parse tree generated for extracting the indices
that do not enter in the inequalities obtained during the parsing.
Finally, like Correa?s approach, Johnson?s does not account for backward anaphora,
as only surface c-commanders are visible to the tree-climbing procedure.
2.3.2 Equations with Regular Expressions. The basic LFG account of binding, set
forth by Dalrymple (1993), adopts a different approach to generalize over the possible
nonlocality of intrasentential anaphoric dependencies. This approach makes crucial
use of a special-purpose extension of the LFG description formalism, the so-called
binding equations, which are lexically associated with anaphors. Building on Kaplan
and Maxwell?s (1988) proposal concerning functional uncertainty, binding equations
are designed to encode the uncertainty concerning the long-distance path between the
positions of the anaphor and its permissible antecedent in the grammatical structure.7
Given that uncertainty concerning long-distance dependencies involves a (possibly
infinite) disjunction of possibilities, the basic idea is to encode such a disjunction in
finite terms by the use of regular expressions over feature structures. An example of
a binding equation encoding functional uncertainty is given in (1), preceded by an
example with the corresponding long-distance subject-oriented reflexive, Chinese ziji.
(1) Zhangsani yiwei [Lisij yiwei [: : : zijii=j=:::: : : ]].
Zhangsani thought [Lisij thought [: : :himi=j=:::: : : ]]
ziji: ((COMP* OBJ ") SUBJ) = " 
The right-hand side of the equation stands for the semantic representation (??) of the
anaphor (?"?), while the left-hand side stands for the semantic representation of the
antecedent. The description of the antecedent indicates that the long-distance reflexive
is an object and that this object is constrained to be part of a feature structure where
7 Koenig (1999) introduces a device in HPSG description language for stating inside-out constraints. This
would help in developing an HPSG emulation of the LFG approach for the verification of binding
constraints.
6
Branco Binding Machines
its antecedent may be one of the possibly many upward subjects. The Kleene operator
?*? allows abbreviation of the set of paths consisting of zero or more occurrences
of COMP?corresponding to possible successive clausal embeddings?followed by one
occurrence of OBJ.
While regular expressions may be used in binding equations, such expressions are
not necessary if the grammatical relation between the anaphor and its admissible an-
tecedents does not involve a long-distance dependency. That is the case in (2), which
displays the binding equation for the short-distance reflexive himself. Given that both
the subject and the object are admissible antecedents for the reflexive, in the binding
equation the use of the attribute GF, which stands for any grammatical function, un-
derspecifies the grammatical functions of the admissible antecedents (Dalrymple 1993,
Section 4.4.2).
(2) Johni described Billj to himselfi=j.
himself: ((OBLGoal ") GF) =" 
Binding equations may also express negative constraints, as in (3), where the se-
mantic representation of the pronoun is constrained to be different from that of its
local coarguments.
(3) Johni described Billj to himi=j.
him: ((OBLGoal ") GF) 6=" 
As noted in Dalrymple (1993, Section 3.3), a few aspects of this approach for bind-
ing need to be fully worked out. For instance, the positive equations for reflexives
do not require identity of indices of anaphorically related expressions, but instead
impose identity of semantic representations. Without further elaboration, this will in-
correctly enforce any type of anaphoric link (coreference, bound, bridging, e-type, etc.)
to the sole mode of coreference. Another important issue is the account of nonlexical
anaphoric NPs: it is not clear how this type of NP (e.g., anaphoric definite descriptions,
ruled by Principle C) may be assigned the corresponding binding equation.
However these difficulties turn out to be resolved, the LFG approach for binding,
though building on a different strategy for handling nonlocality, presents the same
sort of problems as Johnson?s proposal.
The interfacing of grammar with reference-processing systems is problematic since
the proliferation of representations is not avoided. Constructions with reflexives, if
these are ambiguous, end up associated with several grammatical representations.
In the case of long-distance reflexives, as exemplified in (1), these representations
result from the possibly many solutions for the functional uncertainty encoded by the
regular expression in the binding equation. In the case of short-distance reflexives, as
exemplified in (2), they result from the different solutions for the unification of the
different grammatical functions of the admissible antecedents with the attribute GF in
the binding equation.
Likewise, the anaphoric capacity of pronouns and nonpronouns, typically ambigu-
ous, is not explicitly captured in the final grammatical representation. These anaphors
are lexically associated with negative equations, and for this type of equation there
is only one possible solution, namely, the grammatical structure where the semantic
representation of the anaphor is not identical to the semantic representations of any
of the phrases complying with the description of the antecedent in the left-hand side
7
Computational Linguistics Volume 28, Number 1
of the equation (Dalrymple 1993, Section 4.1.5). Therefore, for these anaphors the fi-
nal grammatical representation provides no information about what their admissible
antecedents are according to the relevant binding constraints.
3. A Semantics-Driven Approach
The contributions assessed above share a common point of departure with regard
to the verification algorithm first proposed by Chomsky (1981), each addressing and
solving some of its more significant drawbacks. The common move toward the lexical-
ization of binding constraints represents an important shift in the verification strategy:
verifying binding constraints is not a matter of inspecting final grammatical represen-
tations, but instead a matter of some local operation triggered by information lexically
associated with anaphors about their anaphoric class. This move has allowed bind-
ing constraint verification to be incorporated into grammar processing and permitted
tractable verification procedures.
From the discussion in the previous section, it follows also that these contribu-
tions have been partially successful in overcoming other problems of the verification
methodology based on exhaustive coindexation. Though partially successful, they have
brought to the fore important dimensions of binding that have to be concomitantly
accounted for. Accordingly, an alternative method for the verification of binding con-
straints has to find a way to harmonize all those different dimensions?lexicalization,
anaphoric ambiguity packing, and nonlocal context packing?while providing ade-
quate empirical coverage and neatly interfacing grammar with reference processing.
Against this background, a breakthrough depends, in our view, on reconsidering
some primitives underlying the conception of binding constraints. In the previous
section, we made a clear distinction between specification and verification of binding
constraints, so that the latter task could be isolated and better assessed. We will argue
now that further progress on the verification task depends on bridging this distinction
and possibly changing the way the specification of binding constraints is understood.
3.1 Patterns in the Semantics of Anaphors
Binding constraints have generally been viewed as well-formedness conditions on
syntactic representations, thus belonging to the realm of syntax. In line with Gawron
and Peters (1990), however, we think these constraints should rather be understood as
conditions on semantic representations, since they primarily delimit (nonlocal) aspects
of semantic composition, rather than aspects of syntactic composition.8
Like other types of constraints on semantic composition, binding constraints im-
pose conditions on the interpretation of certain expressions?anaphors, in the present
case?based on syntactic geometry. However, this cannot be viewed as implying that
they express grammaticality requirements. By replacing a pronoun with a reflexive
in a given sentence, for instance, we do not turn a grammatical construction into an
ungrammatical one, even if we assign to the reflexive the antecedent appropriately se-
8 As implied by the title of this section, and as will become clear in the following discussion, this does
not mean that we are claiming that binding theory can be built without any reference to syntactic
constructs.
In the argument in the following paragraphs, we are assuming a notion of semantic composition
not in its strict sense, as used for example in Montague Grammar, but in the broader sense that the
intermediate semantic representations of the expressions are composed from other representations, as
used in Discourse Representation Theory (DRT). Note that reformulations of frameworks like DRT can
be worked out that result in a semantic system adhering to strict compositionality; see Janssen (1997,
Section 4.4) for references and a thorough discussion of this issue.
8
Branco Binding Machines
lected for the pronoun. In that case, we are simply asking the hearer to try to assign to
that sentence a meaning it cannot express?just as if we were to ask whether someone
could interpret The red book is on the white table as describing a situation where a white
book is on a red table.
In this example, given how they happen to be syntactically related, the semantic
values of red and table cannot be composed in such a way that the sentence could be
used to describe a situation concerning a red table, rather than a white table. Likewise,
in the sentence John thinks Peter shaved him, given how they happen to be syntactically
related, the semantic values of Peter and him cannot be composed in such a way that
this sentence could be used to describe a situation where John thinks that Peter shaved
himself (i.e., Peter), rather than a situation where John thinks that Peter shaved other
people (e.g., Paul, Bill, or John himself). The difference between these two cases is that
in the former, the composition of the semantic contributions of white and table (for the
interpretation of the NP white table) is constrained by local syntactic geometry, while
in the latter, the composition of the semantic contributions of John and him (for the
interpretation of the NP him) is constrained by nonlocal syntactic geometry.
This discussion leads us to consider that, semantically, an anaphor should be spec-
ified in the lexicon as a function whose argument is a suitable representation of the
context?providing a semantic representation of the NPs available in the discourse
vicinity?and its value is the set of the grammatically admissible antecedents for that
anaphor. This rationale is in line with other approaches to the meaning of anaphors
that, building in other sorts of arguments or research concerns, understand it also as
a projection from some relevant representation of contexts to entities.9 But given the
specific focus of the present study, what should be noted is that, all in all, there will
be four such functions available to be lexically associated with anaphors, each corre-
sponding to one of the four different classes of anaphors, in accordance with the four
binding constraints A, B, C, and Z.10
3.2 Binding Machines
Given these considerations, we can show that this conceptual shift to a semantics-
driven approach for the verification of binding constraints provides an adequate basis
for harmonizing the advances put forward in the literature and discussed above.
To make this alternative rationale for binding perspicuous, we suggest envisioning
an anaphoric NP as a binding machine, which operates by receiving an input, changing
its internal state, and returning an output. More specifically, an anaphoric NP can be
9 See, among others, Gawron and Peters (1990), Lappin and Francez (1994), and the discussion in
Jacobson (1999).
Adopting Lo?bner?s (1987) duality criterion for quantification in natural language, and the formal
tools he developed for the analysis of phase quantification, we showed in Branco (2000) that the four
binding constraints can be seen as the effect of four binding quantifiers. These phase quantifiers can be
viewed as being expressed by the nominals of the four binding classes, and they quantify over the
reference markers organized in the obliqueness order.
A full-fledged account of the empirical support and justification for these results, and of their
implications, is beyond the scope of this article. For an abridged presentation of the core argument, see
Branco (1998).
10 As there are different grammatical frameworks, binding constraints have been specified under different
versions. Some differences between versions are due just to this fact that binding constraints are
supposed to be accommodated into different grammatical frameworks; some other differences,
however, are real differences of specification in the sense that different variants may not have the same
empirical coverage or be aimed at predicting the same (un)grammatical constructions. In the
Appendix, we present a common and fairly well empirically tested version of binding theory given the
current state of the art in this area, a version presently adopted in the HPSG framework. For an
alternative, see for example Reinhart and Reuland (1993).
9
Computational Linguistics Volume 28, Number 1
viewed as a binding machine that (1) takes a representation of its context; (2) updates
its own semantic value in response both to its context and to its intrinsic anaphoric
potential (i.e., in accordance with its binding constraint); and (3) contributes to the
makeup of the context, which the other binding machines read as input (i.e., against
which the other anaphoric NPs are interpreted).11
The output of an anaphoric nominal n viewed as a binding machine is simply the
incrementing of the context with a copy of its reference marker.12
The internal state of the machine after its operation is a representation of the con-
textualized anaphoric capacity of n under the form of the set of reference markers
of the grammatically admissible antecedents of n. This internal state results when the
binding constraint associated with n is applied to the input, and it is the interface point
between grammar and reference processing. This set of reference markers collects the
antecedent candidates, and its elements are submitted to other filters and preferences
by the anaphor resolvers so that one of them ends up being chosen as the antecedent.
The input, in turn, is a representation of the aspects of the context relevant to
help circumscribe the anaphoric potential of nominal anaphors. It is coded under the
form of three lists of reference markers, A, Z, and U. In list A, the reference markers
of the local o-commanders of n are ordered according to their relative grammatical
obliqueness; Z includes the o-commanders of n, possibly observing a multiclausal
obliqueness hierarchy; and U is the list of all reference markers in the discourse context,
including those not linguistically introduced.
Given this setup, the contribution of binding constraints in circumscribing the an-
aphoric potential of nominals is explicitly acknowledged. The particular contextualized
instantiation of that potential and the verification of binding constraints coincide and
consist of a few simple steps. If n is a short-distance reflexive, its internal state is set
up as A0, where A0 contains the reference markers of the o-commanders of n in A.
If n is a long-distance reflexive, its semantic representation includes Z0, such that Z0
contains the o-commanders of n in Z. If n is a pronoun, B = U n (A0 [ [r-markn]) is
encoded into its representation, where r-markn is the reference marker of n. Finally, if
n is a nonpronoun, its updated semantics keeps a copy of C = U n (Z0 [ [r-markn]).
Besides adhering to an empirically grounded conception of binding constraints,
this approach embodies, and harmonizes, the crucial contributions of previous pro-
posals concerning the verification of these constraints. It assumes the lexicalization
of binding constraints. Concomitantly, it builds on specific strategies for the packing
of anaphoric ambiguity (viz., list of reference markers) and nonlocal context (viz., set
of lists of reference markers). Moreover, it achieves this while avoiding the above-
mentioned problems related to the proliferation of grammatical representations and
to the interfacing of grammar with reference processing, as well as the problems of
ensuring complete empirical coverage.
What remains to be discussed is whether, given this new format for the verifica-
tion of binding constraints, they can still be specified and integrated into grammar
processing with currently affordable formal and computational tools.
4. A Unification-Based Specification Exercise
This new approach to binding constraints can be integrated into grammar easily and
in a principled manner. In what follows, we outline how these constraints can be
specified and handled in a unification-based grammatical framework such as HPSG.
11 This rationale is in line with the insights of Johnson and Klein (1990) concerning the processing of the
semantics of nominals.
12 See Kamp and Reyle (1993) for the notion of reference marker.
10
Branco Binding Machines
As a proposal for that integration, we designed an extension to the Underspecified
Discourse Representation Theory (UDRT) semantics component for HPSG developed
by Frank and Reyle (1995). This component is encoded as the value of the feature
CONT(ENT), which is now extended with the feature ANAPH(ORA); see (4). This new fea-
ture keeps information about the anaphoric potential of the corresponding nominal n:
its subfeature ANTEC(EDENTS) keeps a record of how that potential is updated when the
anaphor enters a grammatical construction; and its subfeature R(EFERENCE)-MARK(ER)
indicates the reference marker of n, to be contributed to the context.
Similarly, and still assuming Pollard and Sag?s (1994) feature geometry as a starting
point, the NONLOC value is also extended with a new feature, BIND(ING), with subfea-
tures LIST-A, LIST-Z, and LIST-U. These lists provide a specification of the relevant context
and correspond to the lists A, Z, and U above. Subfeature LIST-LU is a fourth, auxiliary
list for encoding the contribution of local context to the global, nonlocal context.
The SYNSEM value of a pronoun, for instance, can now be designed as shown in (4).
(4)
2
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
4
loc jcont
2
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
4
ls
"
l-max 1
l-min 1
#
subord fg
conds
8
<
:
"
label 1
arg-r 2
#
9
=
;
anaph
2
4
r-mark 2
antec 5 principleB

4 , 3 , 2

3
5
3
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
5
nonloc jbind
2
6
6
6
4
list-a 3
list-z list
list-u 4
list-lu 2
3
7
7
7
5
3
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
5
Given this feature structure, the binding constraint associated with pronouns is
specified as the relational constraint principleB. This relational constraint returns list
B as the value of ANTEC. It is defined to take (in the first argument) all markers in
the discourse context, given in LIST-U value, and remove from them both the local
o-commanders of the pronoun (included in the second argument) and the marker
corresponding to the pronoun (in the third argument).
The SYNSEMs of other anaphors, ruled by Principles A, C, and Z, are similar to the
one above.13 The only difference lies in the relational constraint in the ANTEC value,
which encodes the appropriate binding constraint and returns the updated anaphoric
potential under the form of list A0, C, or Z0, respectively, as discussed in the previous
section.
Turning to the specification of the context (i.e., the values of LIST-A, LIST-Z, LIST-U,
and LIST-LU), this is handled by means of a new HPSG principle, which can be termed
the Binding Domains Principle. This principle consists of three clauses constraining
13 Binding constraints for nonlexical anaphoric nominals are lexically stated in the corresponding
determiners.
A constraint for pronominal anaphoric transitivity may also be introduced at the lexical
representation of pronouns, by including in the CONDS value in (4) Discourse Representation
Structure conditions expressing that
8ra, rb(( 2 = anaphra ^ rb = anaphra) ) ([rb] [ 5 = 5 )).
11
Computational Linguistics Volume 28, Number 1
signs and their values with respect to these lists of reference markers. Due to space
limitations, we illustrate this principle simply by stating Clause I, which constrains
LIST-U and LIST-LU.14
(5) Binding Domains Principle, Clause I
a. In every sign, the LIST-LU value is identical to the concatenation
of the LIST-LU values of its daughters.
b. In a sign of sort discourse, the LIST-LU and LIST-U values are token
identical.
c. In a non-NP sign, the LIST-U value is token identical to each
LIST-U value of its daughters.
d. In an NP sign k,
i. In Spec-daughter, the LIST-U value is the result of
removing the elements of the LIST-A value of
Head-daughter from the LIST-U value of k;
ii. In Head-daughter, the LIST-U value is the result of
removing the value of R-MARK of Spec-daughter from the
LIST-U value of k.
LIST-LU collects, up to the outermost sign of sort discourse, all the markers contributed
by the different NPs for the context. At this sign, they are passed to LIST-U, by means
of which they are propagated to every NP. The HPSG ontology was extended with the
sort discourse, which corresponds to sequences of sentential signs and at whose signs
reference markers from the nonlinguistic context may be introduced in the semantic
representation.15 Subclause (d) is meant to avoid what is known in the literature as
the i-within-i effect.
5. Example and Discussion
The above unification-based specification of binding constraints, while ensuring their
integration into grammar, allows the binding module to be suitably hooked up
with systems of reference processing. Feature ANTEC is the interface point between
them.
14 Clauses II and III constrain LIST-A and LIST-Z, respectively. Roughly, Clause II ensures that the LIST-A
value is passed from the lexical head to its successive projections, and also from the head-daughters to
their arguments. Note that exemption occurs when principleA( 1 , 2 ) is the empty list, in which case the
reflexive should find its antecedent outside any binding constraint (Pollard and Sag 1994,
Chapter 6).
Clause III ensures that, at the top node of the grammatical representation, LIST-Z is set up as the
LIST-A value of that node, and that LIST-Z is successively incremented at the suitable downstairs nodes
by appending its value with the LIST-A value of those nodes.
At the lexical entry of a predicator, LIST-A is defined as the concatenation of the R-MARK values of
its subcategorized arguments specified in the ARG-S value.
For a detailed specification of the Binding Domains Principle, see Branco (2000).
15 Reference markers can be introduced linguistically, by the utterance of the corresponding expressions,
or nonlinguistically, by means of their cognitive availability in the context of the discourse. Theories of
natural language semantics can be used to represent these two types of reference markers.
Nevertheless, only a global theory encompassing natural language and cognition seems to be able to
pursue the ambitious goal of providing an integrated account of how both types of markers, and not
only those linguistically evoked, are introduced into semantic representation.
12
Branco Binding Machines
We are following a distinction between the notions of anaphor resolution and ref-
erence processing commonly assumed in the literature. Anaphor resolution is seen as
being concerned with the task of identifying the antecedents of anaphors. It is there-
fore part of a reference-processing system, whose overall goal, in turn, is to determine
the interpretation of the anaphors. This involves determining the appropriate semantic
type of the anaphoric link between an anaphor and its antecedent (coreference, bridg-
ing, e-type, bound anaphora, etc.) and providing a suitable semantic representation
for this link.
Being the interface point between grammatical representation and reference pro-
cessing, the list value of the feature ANTEC has just to be reduced by anaphor resolvers,
given the relevant preferences and filters other than binding constraints, until the most
likely antecedent is isolated. It is thus a process concerning selection in a list, rather
than search in a set of indexed trees.
As to reference processing in general, the specification suggested in the previous
section provides a suitable framework for the correct representation of the semantically
different types of anaphoric links, the range of options not being restricted to corefer-
ence only. After the anaphor has been resolved, the reference marker of the anaphor
and the reference marker selected as the antecedent can be related in accordance with
the mode of anaphora determined by the reference-processing system.
This semantic relation between anaphorically related reference markers can be
represented simply as another DRS condition in the CONDS value. This makes possible
a mainstream DRT representation for the resolved anaphoric link, thus building on
the substantial number of already worked out solutions available in the literature for
DRT-based semantic representation of anaphora.16
This specification of binding theory for HPSG was tested with a computational
implementation using ProFIT (Erbach 1995). In this implementation, the relational
constraints corresponding to binding principles were straightforwardly encoded by
means of Prolog predicates associated to the lexical clauses of anaphoric expressions,
and defined in terms of simple auxiliary predicates ensuring the component operations
of list appending, list difference, and so on. It is worth noting that some of these
predicates have arguments?for example, the LIST-U value, whose value is computed
when the whole relevant grammatical representation is built up. This is a consequence
of packing nonlocal information in such lists. As in Johnson?s approach, it requires
that some delaying device be used, which in this computational grammar was done
by resorting to the Prolog built-in predicate freeze/2.
For the sake of the example, consider the following multiclausal sentence from
Portuguese displaying backward anaphora between a topicalized reflexive and a pro-
noun:
(6) De si pro?prio, cada estudante disse que ele gosta.
of him self every student said that he likes
?Himself, every student said that he likes.?
An abridged version of the grammatical representation produced by the imple-
mented grammar for this sentence is presented in Figure 1, where the feature structures
below the tree correspond to partial grammatical representations of the constituents
16 See Kamp and Reyle (1993) for a comprehensive rendering of DRT, and Branco (2000, Chapter 5) for an
overview concerning the semantic representation of different modes of anaphora.
13
Computational Linguistics Volume 28, Number 1
h
e
 e
ve
ry
 s
tu
d
en
t
h
im
se
lf
...
|A
N
A
P
H
O
R
A
R
E
FM
A
R
K
A
N
T
E
C
...
|
B
IN
D
IN
G
L
IS
T
_A
L
IS
T
_Z
L
IS
T
_U
L
IS
T
_L
U
24 4
15
24
7
54
39
2
24
39
2
54
24
39
2
41
5
24
24
7
54
39
2
24
,
,
,
, ,
,
,
,
,
,
? ?? ?
? ?? ?
? ?? ? ? ? ? ?
? ?? ? ? ? ? ?
? ?? ? ? ? ? ? ? ? ?
? ?? ? ? ? ? ? ? ? ?
...
|A
N
A
P
H
O
R
A
R
E
FM
A
R
K
V
A
R
...
|
B
IN
D
IN
G
L
IS
T
_A
L
IS
T
_Z
L
IS
T
_U
L
IS
T
_L
U
24
7
54
54 54 41
5
24
24
7
54
39
2
24
7
54
? ??
? ??
? ?? ? ? ? ? ?
? ?? ? ? ? ? ?
? ?? ? ? ? ? ? ? ? ?
? ?? ? ? ? ? ? ? ? ?
,
,
,
,
,
...
|A
N
A
P
H
O
R
A
R
E
FM
A
R
K
A
N
T
E
C
...
|
B
IN
D
IN
G
L
IS
T
_A
L
IS
T
_Z
L
IS
T
_U
L
IS
T
_L
U
39
2
24
24
39
2
54
24
39
2
41
5
24
24
7
54
39
2
39
2
? ??
? ??
? ?? ? ? ? ? ?
? ?? ? ? ? ? ?
? ?? ? ? ? ? ? ? ? ?
? ?? ? ? ? ? ? ? ? ?
, ,
,
,
,
,
,
sa
id
th
at
 
c
t
x
...
C
O
N
T
|C
O
N
D
S
A
R
G
R
...
|
B
IN
D
IN
G
L
IS
T
_A
L
IS
T
_Z
L
IS
T
_U
L
IS
T
_L
U
...
... ,
,
,
,
41
5
41
5
24
24
7
54
39
2
41
5
[
]
? ?? ? ? ? ? ?
? ?? ? ? ? ? ?
? ?? ? ? ? ? ? ? ?
? ?? ? ? ? ? ? ? ?
li
ke
s
t
ra
c
e
L
IS
T
_A
L
IS
T
_Z
L
IS
T
_U
L
IS
T
_L
U
24
39
2
54
24
39
2
41
5
24
24
7
54
39
2
, ,
,
,
,
,
,
? ?? ? ? ? ?
? ?? ? ? ? ?
L
IS
T
_A
L
IS
T
_Z
L
IS
T
_U
L
IS
T
_L
U
24
39
2
54
24
39
2
41
5
24
24
7
54
39
2
39
2
, ,
,
,
,
,
,
? ?? ? ? ? ? ?
? ?? ? ? ? ? ?
L
IS
T
_A
L
IS
T
_Z
L
IS
T
_U
L
IS
T
_L
U
54 54 41
5
24
24
7
54
39
2
24
24
7
54
,
,
,
,
,
,
? ?? ? ? ? ? ?
? ?? ? ? ? ? ?
L
IS
T
_A
L
IS
T
_Z
L
IS
T
_U
L
IS
T
_L
U
54 54 41
5
24
24
7
54
39
2
24
24
7
54
39
2
,
,
,
,
,
,
,
? ?? ? ? ? ? ?
? ?? ? ? ? ? ?
L
IS
T
_A
L
IS
T
_Z
L
IS
T
_U
L
IS
T
_L
U
41
5
24
24
7
54
39
2
41
5
24
24
7
54
39
2
,
,
,
,
,
,
,
,
? ?? ? ? ? ? ?
? ?? ? ? ? ? ?
Figure 1
Abridged grammatical representation for the example sentence (9).
14
Branco Binding Machines
in the leaves of the tree, while the ones above the tree correspond to partial represen-
tations of some nonterminal nodes.
First, consider LIST-Z. In the outer nodes of the matrix clause, due to the effect of
the Binding Domains Principle, Clause III, the LIST-Z value is obtained from the value
of LIST-A, with which it is token identical, comprising the list with a single element
h 54 i. In the nodes of the embedded clause, the LIST-Z value is the concatenation of that
upper LIST-Z value and the LIST-A value h 24 , 392 i in the embedded clause, from which
the list h 54 , 24 , 392 i is the result. LIST-A values are obtained from the representation of
the subcategorization frames of the verbal predicators.
Next, consider LIST-LU. Reading upward, note that at each higher level in the
constituency representation, the list gets longer; by the effect of the Binding Domains
Principle, Clause I, the LIST-LU value at a given node gathers the reference markers of
the nodes dominated by it. At the discourse top node, LIST-LU includes all the reference
markers of the NPs in the example, the list h 415 , 24 , 247 , 54 , 392 i. The Binding Domains
Principle, Clause I, also ensures that this list of all reference markers is passed to the
LIST-U value of the top node and that it is then percolated down to all relevant nodes
of the grammatical representation.
Taking a closer look at the NPs, it is easy to check that every phrase contributes to
the global anaphoric potential of its linguistic context by passing the tag of its reference
marker into its own LIST-LU. In the case of the quantificational NP every student, two
tags are passed, corresponding to the REFMARK value, providing for e-type anaphora,
and the VAR value, providing for bound anaphora interpretations. And in the case of
the ctx node, to illustrate how the nonlinguistic context may be taken into account
in the linguistic representation, the reference marker h 415 i is obtained from the set of
semantic conditions that conventionally may capture the nonlinguistic context.
On the other hand, the context also contributes to establishing the anaphoric po-
tential of each NP. This is ensured by the different clauses of the Binding Domains
Principle, which enforce the presence of suitable values of LIST-A, LIST-Z, and LIST-U at
the different nodes.
Finally, token identity is ensured between the ANTEC value and the outcome of the
different relational constraints that are lexically associated with each NP and express
binding constraints. The value of ANTEC is a list that, at this stage of anaphor resolution,
records the grammatically admissible antecedents of the corresponding anaphor only
in the light of binding constraints.
6. Conclusions
Departing from the coindexation-driven approach for encoding anaphoric dependen-
cies in grammatical representations, we have proposed an alternative methodology
where binding constraints are viewed as contributing to circumscribing their contex-
tually determined semantic value. This semantics-driven approach allows a principled
integration of binding constraints into grammar that supports both a specification for-
mat and a verification methodology free from previous difficulties. Importantly, it also
permits a neat interface between the grammatical module of binding and systems of
reference processing.
Appendix
In this article, we consider the version of binding constraints formulated within Head-
Driven Phrase Structure Grammar (Pollard and Sag 1994, Chapter 6). Recent devel-
opments indicate that there are four binding constraints (Xue, Pollard, and Sag 1994;
15
Computational Linguistics Volume 28, Number 1
Branco and Marrafa 1999). Here, the definition of each binding constraint is followed
by an illustrative example.
(7) Principle A
A locally o-commanded short-distance reflexive must be locally o-bound.
Leei thinks [Maxj saw himselfi=j].
(8) Principle Z
An o-commanded long-distance reflexive must be o-bound.
[O amigo do Ruii]j acha que o Pedrok gosta dele pro?prioi=j=k.
[the friend of the Rui] thinks that the Pedro likes of he PRO?PRIO
?[Rui?s friend]j thinks that Pedrok likes himj/himselfk.? (Portuguese)
(9) Principle B
A pronoun must be locally o-free.
Leei thinks [Maxj saw himi=j].
(10) Principle C
A nonpronoun must be o-free.
[Kimi?s friend]j thinks [Lee saw Kimi=j].
These constraints are defined on the basis of some auxiliary notions.
The notion of local domain involves the partition of sentences and associated gram-
matical geometry into two zones of greater or lesser proximity with respect to the
anaphor. The exact definition of the boundary separating the local from the nonlocal
domain may vary from language to language. Typically, the local domain tends to
correspond to the structure in the grammatical representation that is affected by the
selectional capacity and requirements of a predicator.
O-command is a partial order under which, in a clause, the subject o-commands the
direct object, the direct object o-commands the indirect object, and so on, following the
usual obliqueness hierarchy of grammatical functions, while in a multiclausal sentence,
the upward arguments o-command the successively embedded arguments.
The notion of o-binding is such that x o-binds y iff x o-commands y and x and y
are coindexed, where coindexation is meant to represent anaphoric links.
Acknowledgments
I am grateful to Hans Uszkoreit for advice
and helpful discussion, and to Mark
Johnson for clarifying criticisms. I am solely
responsible for remaining errors.
The results presented here were obtained
while I was on leave at the Language
Technology Group of the DFKI-German
Research Center on Artificial Intelligence,
Saarbru?cken, Germany, whose hospitality
and enthusiastic atmosphere I was very
fortunate to enjoy and I hereby gratefully
acknowledge.
References
Asher, Nicholas and Hajime Wada. 1989. A
computational account of syntactic,
semantic and discourse principles for
anaphora resolution. Journal of Semantics,
6:309?344.
Backofen, Rolf, Tilman Becker, Jo Calder,
Joanne Capstick, Luca Dini, Jochen Do?rre,
16
Branco Binding Machines
Gregor Erbach, Dominique Estival, Suresh
Manandhar, Anne-Marie Mineur, Gertjan
van Noord, Stephan Oepen, and Hans
Uszkoreit. 1996. Final report of the
EAGLES Formalisms Working Group.
Technical Report, EAGLES-Expert
Advisory Group on Language
Engineering Standards, Luxemburg.
Barker, Chris and Geoffrey K. Pullum. 1990.
A theory of command relations.
Linguistics and Philosophy, 13:1?34.
Branco, Anto?nio. 1998. The logical structure
of binding. In Proceedings of the 36th Annual
Meeting of the Association for Computational
Linguistics and 17th International Conference
on Computational Linguistics
(ACL/COLING?98), pages 181?187.
Branco, Anto?nio. 2000. Reference Processing
and Its Universal Constraints. Edic?o?es
Colibri, Lisbon.
Branco, Anto?nio and Palmira Marrafa. 1999.
Long-distance reflexives and the binding
square of opposition. In Gert Webelhuth,
Jean-Pierre Koenig, and Andreas Kathol,
editors, Lexical and Constructional Aspects of
Linguistic Explanation. CSLI Publications,
Stanford, CA, pages 163?177.
Bredenkamp, Andrew. 1996. Towards a
Binding Theory for Head-Driven Phrase
Structure Grammar. Ph.D. thesis,
University of Essex.
Carbonell, Jaime and Ralf Brown. 1988.
Anaphora resolution: A multi-strategy
approach. In Proceedings of the 12th
International Conference on Computational
Linguistics (COLING?88), pages 96?101.
Chomsky, Noam. 1980. On binding.
Linguistic Inquiry, 11:1?46.
Chomsky, Noam. 1981. Lectures on
Government and Binding. Foris, Dordrecht.
Correa, Nelson. 1988. A binding rule for
government-binding parsing. In
Proceedings of the 12th International
Conference on Computational Linguistics
(COLING?88), pages 123?129.
Dalrymple, Mary. 1993. The Syntax of
Anaphoric Binding. CSLI Publications,
Stanford, CA.
Erbach, Gregor. 1995. Prolog with features,
inheritance and templates. In Proceedings
of the 7th Conference of the European Chapter
of the Association for Computational
Linguistics (EACL?95), pages 180?187.
Fong, Sandiway. 1990. Free indexation:
Combinatorial analysis and a
compositional algorithm. In Proceedings of
the 28th Annual Meeting of the Association for
Computational Linguistics (ACL?90),
pages 105?110.
Frank, Anette and Uwe Reyle. 1995.
Principle based semantics for HPSG. In
Proceedings of the 7th Conference of the
European Chapter of the Association for
Computational Linguistics (EACL?95),
pages 9?16.
Gawron, Jean Mark and Stanley Peters.
1990. Anaphora and Quantification in
Situation Semantics. CSLI Publications,
Stanford, CA.
Giorgi, Alessandra, Fabio Pianesi, and
Giorgio Satta. 1990. A computational
approach to binding theory. In Proceedings
of the 13th International Conference on
Computational Linguistics (COLING?90),
pages 1?6.
Ingria, Robert and David Stallard. 1989. A
computational mechanism for pronominal
reference. In Proceedings of the 27th Annual
Meeting of the Association for Computational
Linguistics (ACL?89), pages 262?271.
Jacobson, Pauline. 1999. Binding without
pronouns. In Formal Grammar Conference
1999: Symposium on Grammatical Resources
and Grammatical Inference. Abstract.
Janssen, Theo. 1997. Compositionality. In
Johan van Benthem and Alice ter Meulen,
editors, Handbook of Logic and Language.
Elsevier, Amsterdam, pages 417?474.
Johnson, Mark. 1995. Constraint-based
natural language parsing. Course notes,
7th European Summer School in Logic,
Language and Information, Barcelona.
Johnson, Mark and Ewan Klein. 1990.
Discourse, anaphora and parsing. In
Proceedings of the 28th Annual Meeting of the
Association for Computational Linguistics
(ACL?90), pages 669?675.
Kamp, Hans and Uwe Reyle. 1993. From
Discourse to Logic: Introduction to
Modeltheoretic Semantics of Natural
Language, Formal Logic and Discourse
Representation Theory. Kluwer, Dordrecht.
Kaplan, Ronald and John Maxwell. 1988. An
algorithm for functional uncertainty. In
Proceedings of the 12th International
Conference on Computational Linguistics
(COLING?88), pages 297?302.
Koenig, Jean-Pierre. 1999. Inside-out
constraints and description languages for
HPSG. In Gert Webelhuth, Jean-Pierre
Koenig, and Andreas Kathol, editors,
Lexical and Constructional Aspects of
Linguistic Explanation. CSLI Publications,
Stanford, CA, pages 265?280.
Lappin, Shalom and Nissim Francez. 1994.
E-type pronouns, I-sums, and donkey
anaphora. Linguistics and Philosophy,
17:391?428.
Lappin, Shalom and Herbert Leass. 1994.
An algorithm for pronominal anaphora
resolution. Computational Linguistics,
20:535?561.
17
Computational Linguistics Volume 28, Number 1
Lo?bner, Sebastian. 1987. Quantification as a
major module of natural language
semantics. In Jeroen Groenendijk, Dick de
Jong, and Martin Stokhof, editors, Studies
in DRT and the Theory of Generalized
Quantifiers. Foris, Dordrecht, pages 53?85.
Merlo, Paola. 1993. For an incremental
computation of intra-sentential
coreference. In Proceedings of the
International Joint Conference on Artificial
Intelligence (IJCAI?93), pages 1216?1221.
Mitkov, Ruslan. 1997. Factors in anaphora
resolution: They are not the only things
that matter. In Proceedings of the
ACL/EACL?97 Workshop on Operational
Factors in Practical, Robust Anaphora
Resolution, pages 36?51.
Mitkov, Ruslan. 1998. Robust pronoun
resolution with limited knowledge. In
Proceedings of the 36th Annual Meeting of the
Association for Computational Linguistics and
17th International Conference on
Computational Linguistics
(ACL/COLING?98), pages 869?875.
Pianesi, Fabio. 1991. Indexing and referential
dependencies within binding theory: A
computational framework. In Proceedings
of the 5th Conference of the European Chapter
of the Association for Computational
Linguistics (EACL?91), pages 39?44.
Pollard, Carl and Ivan Sag. 1994.
Head-Driven Phrase Structure Grammar.
University of Chicago Press, Chicago.
Reinhart, Tanya and Eric Reuland. 1993.
Reflexivity. Linguistic Inquiry, 24:657?720.
Rich, Elaine and Susann LuperFoy. 1988. An
architecture for anaphora resolution. In
Proceedings of the 2nd Conference on Applied
Natural Language Processing (ANLP?88),
pages 18?24.
Xue, Ping, Carl Pollard, and Ivan Sag. 1994.
A new perspective on Chinese ziji. In
Proceedings of the 12th West Coast Conference
on Formal Linguistics (WCCFL?94). CSLI
Publications, Stanford, CA.
18
Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 5?8,
Suntec, Singapore, 3 August 2009.
c
?2009 ACL and AFNLP
LX-Center: a center of online linguistic services
Ant
?
onio Branco, Francisco Costa, Eduardo Ferreira, Pedro Martins,
Filipe Nunes, Jo
?
ao Silva and Sara Silveira
University of Lisbon
Department of Informatics
{antonio.branco, fcosta, eferreira, pedro.martins,
fnunes, jsilva, sara.silveira}@di.fc.ul.pt
Abstract
This is a paper supporting the demonstra-
tion of the LX-Center at ACL-IJCNLP-09.
LX-Center is a web center of online lin-
guistic services aimed at both demonstrat-
ing a range of language technology tools
and at fostering the education, research
and development in natural language sci-
ence and technology.
1 Introduction
This paper is aimed at supporting the demonstra-
tion of a web center of online linguistic services.
These services demonstrate language technology
tools for the Portuguese language and are made
available to foster the education, research and de-
velopment in natural language science and tech-
nology.
This paper adheres to the common format de-
fined for demo proposals: the next Section 2
presents an extended abstract of the technical con-
tent to be demonstrated; Section 3 provides a
script outline of the demo presentation; and the
last Section 4 describes the hardware and internet
requirements expected to be provided by the local
organizer.
2 Extended abstract
The LX-Center is a web center of online linguis-
tic services for the Portuguese language located at
http://lxcenter.di.fc.ul.pt. This is
a freely available center targeted at human users. It
has a counterpart in terms of a webservice for soft-
ware agents, the LXService, presented elsewhere
(Branco et al, 2008).
2.1 LX-Center
The LX-Center encompasses linguistic services
that are being developed, in all or part, and main-
tained at the University of Lisbon, Department of
Informatics, by the NLX-Natural Language and
Speech Group. At present, it makes available the
following functionalities:
? Sentence splitting
? Tokenization
? Nominal lemmatization
? Nominal morphological analysis
? Nominal inflection
? Verbal lemmatization
? Verbal morphological analysis
? Verbal conjugation
? POS-tagging
? Named entity recognition
? Annotated corpus concordancing
? Aligned wordnet browsing
These functionalities are provided by one or
more of the seven online services that integrate
the LX-Center. For instance, the LX-Suite service
accepts raw text and returns it sentence splitted,
tokenized, POS tagged, lemmatized and morpho-
logically analyzed (for both verbs and nominals).
Some other services, in turn, may support only one
of the functionalities above. For instance, the LX-
NER service ensures only named entity recogni-
tion.
These are the services offered by the LX-
Center:
? LX-Conjugator
? LX-Lemmatizer
? LX-Inflector
? LX-Suite
? LX-NER
? CINTIL concordancer
? MWN.PT browser
5
The access to each one of these services is ob-
tained by clicking on the corresponding button on
the left menu of the LX-Center front page.
Each of the seven services integrating the LX-
Center will be briefly presented in a different
subsection below. Fully fledged descriptions are
available at the corresponding web pages and in
the white papers possibly referred to there.
2.2 LX-Conjugator
The LX-Conjugator is an online service for fully-
fledged conjugation of Portuguese verbs. It takes
an infinitive verb form and delivers all the corre-
sponding conjugated forms. This service is sup-
ported by a tool based on general string replace-
ment rules for word endings supplemented by a list
of overriding exceptions. It handles both known
verbs and unknown verbs, thus conjugating neolo-
gisms (with orthographic infinitival suffix).
The Portuguese verbal inflection system is a
most complex part of the Portuguese morphology,
and of the Portuguese language, given the high
number of conjugated forms for each verb (ca. 70
forms in non pronominal conjugation), the num-
ber of productive inflection rules involved and the
number of non regular forms and exceptions to
such rules.
This complexity is further increased when the
so-called pronominal conjugation is taken into ac-
count. The Portuguese language has verbal clitics,
which according to some authors are to be ana-
lyzed as integrating the inflectional suffix system:
the forms of the clitics may depend on the Number
(Singular vs. Plural), the Person (First, Second,
Third or Second courtesy), the Gender (Masculine
vs. Feminine), the grammatical function which
they are in correspondence with (Subject, Direct
object or Indirect object), and the anaphoric prop-
erties (Pronominal vs. Reflexive); up to three cli-
tics (e.g. deu-se-lho / gave-One-ToHim-It) may be
associated with a verb form; clitics may occur in
so called enclisis, i.e. as a final part of the verb
form (e.g. deu-o / gave-It), or in mesoclisis, i.e.
as a medial part of the verb form (e.g. d?a-lo-ia
/ give-it-Condicional) ? when the verb form oc-
curs in certain syntactic or semantic contexts (e.g
in the scope of negation), the clitics appear in pro-
clisis, i.e. before the verb form (ex.: n?ao o deu /
NOT it gave); clitics follow specific rules for their
concatenation.
With LX-Conjugator, pronominal conjugation
can be fully parameterizable and is thus exhaus-
tively handled. Additionally, LX-Conjugator ex-
haustively handles a set of inflection cases which
tend not to be supported together in verbal conju-
gators: Compound tenses; Double forms for past
participles (regular and irregular); Past participle
forms inflected for number and gender (with tran-
sitive and unaccusative verbs); Negative impera-
tive forms; Courtesy forms for second person.
This service handles also the very few cases
where there may be different forms in different
variants: when a given verb has different ortho-
graphic representations for some of its inflected
forms (e.g. arguir in European vs. arg?uir in
American Portuguese), all such representations
will be displayed.
2.3 LX-Lemmatizer
The LX-Lemmatizer is an online service for fully-
fledged lemmatization and morphological analysis
of Portuguese verbs. It takes a verb form and de-
livers all the possible corresponding lemmata (in-
finitive forms) together with inflectional feature
values.
This service is supported by a tool based on
general string replacement rules for word endings
whose outcome is validated by the reverse proce-
dure of conjugation of the output and matching
with the original input. These rules are supple-
mented by a list of overriding exceptions. It thus
handles an open set of verb forms provided these
input forms bear an admissible verbal inflection
ending. Hence, this service processes both lexi-
cally known and unknown verbs, thus coping with
neologisms.
LX-Lemmatizer handles the same range of
forms handled and generated by the LX-
Conjugator. As for pronominal conjugation forms,
the outcome displays the clitic detached from
the lemma. The LX-Lemmatizer and the LX-
Conjugator can be used in ?roll-over? mode. Once
the outcome of say the LX-Conjugator on a given
input lemma is displayed, the user can click over
any one of the verbal forms in that conjugation ta-
ble. This activates the LX-Lemmatizer on that in-
put verb form, and then its possible lemmas, to-
gether with corresponding inflection feature val-
ues, are displayed. Now, any of these lemmas can
also be clicked on, which will activate back the
LX-Conjugator and will make the corresponding
conjugation table to be displayed.
6
2.4 LX-Inflector
The LX-Inflector is an online service for the
lemmatization and inflection of nouns and adjec-
tives of Portuguese. This service is also based on
a tool that relies on general rules for ending string
replacement, supplemented by a list of overrid-
ing exceptions. Hence, it handles both lexically
known and unknown forms, thus handling pos-
sible neologisms (with orthographic suffixes for
nominal inflection).
As input, this service takes a Portuguese nomi-
nal form ? a form of a noun or an adjective, in-
cluding adjectival forms of past participles ?, to-
gether with a bundle of inflectional feature values
? values of inflectional features of Gender and
Number intended for the output.
As output, it returns: inflectional features ?
the input form is echoed with the correspond-
ing values for its inflectional features of Gender
and Number, that resulted from its morphological
analysis; lemmata ? the lemmata (singular and
masculine forms when available) possibly corre-
sponding to the input form; inflected forms ? the
inflected forms (when available) of each lemma in
accordance with the values for inflectional features
entered. LX-Inflector processes both simple, pre-
fixed or non prefixed, and compound forms.
2.5 LX-Suite
The LX-Suite is an online service for the shal-
low processing of Portuguese. It accepts raw
text and returns it sentence splitted, tokenized,
POS tagged, lemmatized and morphologically an-
alyzed.
This service is based on a pipeline of a num-
ber of tools, including those supporting the ser-
vices described above. Those tools, for lemmati-
zation and morphological analysis, are inserted at
the end of the pipeline and are preceded by three
other tools: a sentence splitter, a tokenizer and a
POS tagger.
The sentence splitter marks sentence and para-
graph boundaries and unwraps sentences split over
different lines. An f-score of 99.94% was obtained
when testing it on a 12,000 sentence corpus.
The tokenizer segments the text into lexically
relevant tokens, using whitespace as the separator;
expands contractions; marks spacing around punc-
tuation or symbols; detaches clitic pronouns from
the verb; and handles ambiguous strings (con-
tracted vs. non contracted). This tool achieves an
f-score of 99.72%.
The POS tagger assigns a single morpho-
syntactic tag to every token. This tagger is based
on Hidden Markov Models, and was developed
with the TnT software (Brants, 2000). It scores
an accuracy of 96.87%.
2.6 LX-NER
The LX-NER is an online service for the recog-
nition of expressions for named entities in Por-
tuguese. It takes a segment of Portuguese text and
identifies, circumscribes and classifies the expres-
sions for named entities it contains. Each named
entity receives a standard representation.
This service handles two types of expressions,
and their subtypes. (i) Number-based expressions:
Numbers ? arabic, decimal, non-compliant, ro-
man, cardinal, fraction, magnitude classes; Mea-
sures ? currency, time, scientific units; Time ?
date, time periods, time of the day; Addresses ?
global section, local section, zip code; (ii) Name-
base expressions: Persons; Organizations; Loca-
tions; Events; Works; Miscellaneous.
The number-based component is built upon
handcrafted regular expressions. It was devel-
oped and evaluated against a manually constructed
test-suite including over 300 examples. It scored
85.19% precision and 85.91% recall. The name-
based component is built upon HMMs with the
help of TnT (Brants, 2000). It was trained over
a manually annotated corpus of approximately
208,000 words, and evaluated against an unseen
portion with approximately 52,000 words. It
scored 86.53% precision and 84.94% recall.
2.7 CINTIL Concordancer
The CINTIL-Concordancer is an online concor-
dancing service supporting the research usage of
the CINTIL Corpus.
The CINTIL Corpus is a linguistically inter-
preted corpus of Portuguese. It is composed of 1
Million annotated tokens, each one of which ver-
ified by human expert annotators. The annotation
comprises information on part-of-speech, lemma
and inflection of open classes, multi-word expres-
sions pertaining to the class of adverbs and to the
closed POS classes, and multi-word proper names
(for named entity recognition).
This concordancer permits to search for occur-
rences of strings in the corpus and returns them
together with their window of left and right con-
text. It is possible to search for orthographic forms
7
or through linguistic information encoded in their
tags. This service offers several possibilities with
respect to the format for displaying the outcome
of a given search (e.g. number of occurrences per
page, size of the context window, sorting the re-
sults in a given page, hiding the tags, etc.)
This service is supported by Poliqarp, a free
suite of utilities for large corpora processing
(Janus and Przepi?orkowski, 2006).
2.8 MWN.PT Browser
The MWN.PT Browser is an online service to
browse the MultiWordnet of Portuguese.
The MWN.PT is a lexical semantic network for
the Portuguese language, shaped under the on-
tological model of wordnets, developed by our
group. It spans over 17,200 manually validated
concepts/synsets, linked under the semantic rela-
tions of hyponymy and hypernymy. These con-
cepts are made of over 21,000 word senses/word
forms and 16,000 lemmas from both European
and American variants of Portuguese. They are
aligned with the translationally equivalent con-
cepts of the English Princeton WordNet and, tran-
sitively, of the MultiWordNets of Italian, Spanish,
Hebrew, Romanian and Latin.
It includes the subontologies under the concepts
of Person, Organization, Event, Location, and Art
works, which are covered by the top ontology
made of the Portuguese equivalents to all concepts
in the 4 top layers of the Princeton wordnet and
to the 98 Base Concepts suggested by the Global
Wordnet Association, and the 164 Core Base Con-
cepts indicated by the EuroWordNet project.
This browsing service offers an access point to
the MultiWordnet, browser
1
tailored to the Por-
tuguese wordnet. It offers also the possibility
to navigate the Portuguese wordnet diagrammat-
ically by resorting to Visuwords.
2
3 Outline
This is an outline of the script to be followed.
Step 1 : Presentation of the LX-Center.
Narrative: The text in Section 2.1 above.
Action: Displaying the page at
http://lxcenter.di.fc.ul.pt.
Step 2 : Presentation of LX-Conjugator.
Narrative: The text in Section 2.2 above.
Action: Running an example by selecting
1
http://multiwordnet.itc.it/
2
http://www.visuwords.com/
?see an example? option at the page
http://lxconjugator.di.fc.ul.pt.
Step 3 : Presentation of LX-Lemmatizer.
Narrative: The text in Section 2.3 above.
Action: Running an example by selecting
?see an example? option at the page
http://lxlemmatizer.di.fc.ul.pt;
clicking on one of the inflected forms in the
conjugation table generated; clicking on one
of the lemmas returned.
Step 4 : Presentation of LX-Inflector.
Narrative: The text in Section 2.4 above.
Action: Running an example by selecting
?see an example? option at the page
http://lxinflector.di.fc.ul.pt.
Step 5 : Presentation of LX-Suite.
Narrative: The text in Section 2.5 above.
Action: Running an example by selecting
?see an example? option at the page
http://lxsuite.di.fc.ul.pt.
Step 6 : Presentation of LX-NER.
Narrative: The text in Section 2.6 above.
Action: Running an example by copying one
of the examples in the page
http://lxner.di.fc..ul.pt
and hitting the ?Recognize? button.
Step 7 : Presentation of CINTIL Concordancer.
Narrative: The text in Section 2.7 above.
Action: Running an example by selecting
?see an example? option at the page
http://cintil.ul.pt.
Step 8 : Presentation of MWN.PT Browser.
Narrative: The text in Section 2.8 above.
Action: Running an example by selecting
?see an example? option at the page
http://mwnpt.di.fc.ul.pt/.
4 Requirements
This demonstration requires a computer (a laptop
we will bring along) and an Internet connection.
References
A. Branco, F. Costa, P. Martins, F. Nunes, J. Silva and
S. Silveira. 2008. ?LXService: Web Services of
Language Technology for Portuguese?. Proceed-
ings of LREC2008. ELRA, Paris.
D. Janus and A. Przepi?orkowski. 2006. ?POLIQARP
1.0: Some technical aspects of a linguistic search
engine for large corpora?. Proceedings PALC 2005.
T. Brants. 2000. ?TnT-A Statistical Part-of-speech
Tagger?. Proceedings ANLP2000.
8
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 57?64,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Self- or Pre-Tuning?
Deep linguistic processing of language variants
Anto?nio Branco
Universidade de Lisboa
Antonio.Branco@di.fc.ul.pt
Francisco Costa
Universidade de Lisboa
fcosta@di.fc.ul.pt
Abstract
This paper proposes a design strategy
for deep language processing grammars
to appropriately handle language vari-
ants. It allows a grammar to be re-
stricted as to what language variant it is
tuned to, but also to detect the variant
a given input pertains to. This is eval-
uated and compared to results obtained
with an alternative strategy by which the
relevant variant is detected with current
language identification methods in a pre-
processing step.
1 Introduction
This paper addresses the issue of handling dif-
ferent variants of a given language by a deep
language processing grammar for that language.
In the benefit of generalization and grammar
writing economy, it is desirable that a grammar
can handle language variants ? that share most
grammatical structures and lexicon ? in order to
avoid endless multiplication of individual gram-
mars, motivated by inessential differences.
From the viewpoint of analysis, however, in-
creased variant coverage typically opens the way
to increased spurious overgeneration. Conse-
quently, the ability for the grammar to be tuned
to the relevant dialect of the input is impor-
tant to control overgeneration arising from its
flexibility.
Control on what is generated is also desirable.
In general one wants to be able to parse as much
variants as possible, but at the same time be se-
lective in generation, by consistently generating
only in a given selected variant.
Closely related to the setting issue (addressed
in the next Section 2) is the tuning issue: if a
system can be restricted to a particular variety,
what is the best way to detect the variety of the
input? We discuss two approaches to this issue.
One of them consists in using pre-processing
components that can detect the language variety
at stake. This pre-tuning approach explores the
hypothesis that methods developed for language
identification can be used also to detect language
variants (Section 5).
The other approach is to have the computa-
tional grammar prepared for self-tuning to the
language variant of the input in the course of
processing that input (Section 4).
We evaluate the two approaches and compare
them (last Section 6).
2 Variant-sensitive Grammar
In this Section, we discuss the design options for
a deep linguistic processing grammar allowing
for its appropriate tuning to different language
variants. For the sake of concreteness of the dis-
cussion, we assume the HPSG framework (Pol-
lard and Sag, 1994) and a grammar that handles
two close variants of the same language, Euro-
pean and Brazilian Portuguese. These assump-
tions are merely instrumental, and the results
obtained can be easily extended to other lan-
guages and variants, and to other grammatical
frameworks for deep linguistic processing.
A stretch of text from a language L can dis-
play grammatical features common to all vari-
ants of L, or contain a construction that per-
tains to some or only one of its variants. Hence,
undesirable overgeneration due to the grammar
readiness to cope with all language variants can
57
ep-variant
variant
single-variant bp-variant
european-portuguese portuguese brazilian-portuguese
Figure 1: Type hierarchy under variant.
be put in check by restricting the grammar
to produce variant-?consistent? analyses. More
precisely, if the input string contains an element
that can only be found in variety v
1
and that in-
put string yields ambiguity in a different stretch
but only in varieties v
k
other than v
1
, this ambi-
guity will not give rise to multiple analyses if the
grammar can be designed so that it can be con-
strained to accept strings with marked elements
of at most one variety, v
1
.
The approach we propose seeks to implement
this mode of operation in analysis, with the im-
portant effect of permitting also to control the
variant under which generation should be per-
formed. It relies on the use of a feature VARIANT
to model variation. This feature is appropriate
for all signs and declared to be of type variant.
Given the working language variants assumed
here, its values are presented in Figure 1.
This attribute is constrained to take the ap-
propriate value in lexical items and construc-
tions specific to one of the two varieties. For
example, a hypothetical lexical entry for the lex-
ical item autocarro (bus, exclusive to European
Portuguese) would include the constraint that
the attribute VARIANT has the value ep-variant
and the corresponding Brazilian Portuguese en-
try for o?nibus would constrain the same feature
to bear the value bp-variant. The only two types
that are used to mark signs are ep-variant and
bp-variant. The remaining types presented in
Figure 1 are used to constrain grammar behav-
ior, as explained below.
Lexical items are not the only elements that
can have marked values in the VARIANT fea-
ture. Lexical and syntax rules can have them,
too. Such constraints model constructions that
markedly pertain to one of the dialects.
Feature VARIANT is structure-shared among
all signs comprised in a full parse tree. This
is achieved by having all lexical or syntactic
rules unifying their VARIANT feature with the
VARIANT feature of their daughters.
If two signs (e.g. from lexical items and syn-
tax rules) in the same parse tree have different
values for feature VARIANT (one has ep-variant
and the other bp-variant), they will unify to por-
tuguese, as can be seen from Figure 1. This type
means that lexical items or constructions spe-
cific to two different varieties are used together.
Furthermore, since this feature is shared among
all signs, it will be visible everywhere, for in-
stance in the root node.
It is possible to constrain feature VARIANT in
the root condition of the grammar so that the
grammar works in a variant-?consistent? fash-
ion: this feature just has to be constrained to
be of type single-variant (in root nodes) and
the grammar will accept either European Por-
tuguese or Brazilian Portuguese. Furthermore,
in the non natural condition where the input
string bears marked properties of both vari-
ants, that string will receive no analysis: feature
VARIANT will have the value portuguese in this
case, and there is no unifier for portuguese and
single-variant.
If this feature is constrained to be of type
european-portuguese in the root node, the gram-
mar will not accept any sentence with fea-
tures of Brazilian Portuguese, since they will be
marked to have a VARIANT of type bp-variant,
which is incompatible with european-portuguese.
It is also possible to have the grammar re-
ject European Portuguese (using type brazilian-
portuguese) or to ignore variation completely by
not constraining this feature in the start symbol.
With this grammar design it is thus possi-
ble to control beforehand the mode of operation
for the grammar, either for it to handle only
one variant or several. But it is also possible
to use the grammar to detect to which variety
input happens to belong. This self-tuning of
the grammar to the relevant variant is done by
parsing that input and placing no constraint on
feature VARIANT of root nodes, and then read-
ing the value of attribute VARIANT from the re-
sulting feature structure: values ep-variant and
bp-variant result from parsing text with proper-
ties specific to European Portuguese or Brazilian
Portuguese respectively; value variant indicates
that no marked elements were detected and the
text can be from both variants. Also here where
the language variant of the input is detected by
the grammar, the desired variant-?consistent?
58
behavior of the grammar is enforced.
If the input can be known to be specifically
European or Brazilian Portuguese before it is
parsed, the constraints on feature VARIANT can
be set accordingly to improve efficiency: When
parsing text known to be European Portuguese,
there is no need to explore analyses that are
markedly Brazilian Portuguese, for instance.
It is thus important to discuss what meth-
ods for language variant detection can be put
in place that support a possible pre-processing
step aimed at pre-tuning the grammar for the
relevant variant of the input. It is also impor-
tant to gain insight on the quality of the per-
formance of this method and on how the perfor-
mance of this pre-tuning setup compares with
the self-tuning approach. This is addressed in
the next Sections.
3 Experimental setup
Before reporting on the results obtained with the
experiments on the performance of the two ap-
proaches (self- and pre-tuning), it is important
to introduce the experimental conditions under
which such exercises were conducted.
3.1 Data
To experiment with any of these two approaches
to variant-tuning, two corpora of newspaper text
were used, CETEMPublico (204M tokens) and
CETENFolha (32M tokens). The first contains
text from the European newspaper O Pu?blico,
and the latter from the South American Folha
de Sa?o Paulo. These corpora are only minimally
annotated (paragraph and sentence boundaries,
inter alia), but are very large.
Some preprocessing was carried out: XML-
like tags, like the <s> and </s> tags marking
sentence boundaries, were removed and each in-
dividual sentence was put in a single line.
Some heuristics were also employed to remove
loose lines (parts of lists, etc.) so that only lines
ending in ., ! and ?, and containing more than 5
tokens (whitespace delimited) were considered.
Other character sequences that were judged ir-
relevant and potential misguiders for the pur-
pose at hand were normalized: URLs were re-
placed by the sequence URL, e-mail addresses
by MAIL, hours and dates by HORA and DATA,
etc. Names at the beginning of lines indicating
speaker (in an interview, for instance) were re-
moved, since they are frequent and the grammar
that will be used is not intended to parse name
plus sentence strings.
The remaining lines were ordered by length in
terms of words and the smallest 200K lines from
each of the two corpora were selected. Small
lines were preferred as they are more likely to
receive an analysis by the grammar.
Given the methods we will be employing for
pre-tuning reportedly perform well even with
small training sets (Section 5), only a modest
portion of text from these corpora was needed.
In the benefit of comparability of the two
approaches for grammar tuning, it is impor-
tant that all the lines in the working data are
parsable by the grammar. Otherwise, even if
in the pre-tuning approach the pre-processor
gets the classification right for non parsable sen-
tences, this will be of no use since the grammar
will not produce any result out of that. 90K lines
of text were thus randomly selected from each
corpus and checked as to whether they could be
parsed by the grammar. 25K of parsable lines of
the American corpus and 21K of parsable lines
of the European corpus were obtained (46K lines
out of 180K, representing 26% rate of parsabil-
ity for the grammar used ? more details on this
grammar in the next Section).
It is worth noting that the use of two corpora,
one from an European newspaper and the other
from an American newspaper, without further
annotation, does not allow their appropriate use
in the present set of experiments. The reason
is that if a sentence is found in the European
corpus, one can have almost absolute certainty
that it is possible in European Portuguese, but
one does not know if it is Brazilian Portuguese,
too. The same is true of any sentences in the
American corpus ? it can also be a sentence
of European Portuguese in case it only contains
words and structures common to both variants.
In order to prepare the data, a native speaker
of European Portuguese was asked to manually
decide from sentences found in the American
corpus whether they are markedly Brazilian Por-
tuguese. Conversely, a Brazilian informant de-
tected markedly European Portuguese sentences
from the European corpus.
From these parsed lines we drew around 1800
random lines of text from each corpus, and had
them annotated. The lines coming from the
American corpus were annotated for whether
they are markedly Brazilian Portuguese, and
59
vice-versa for the other corpus. Thus a three-
way classification is obtained: any sentence
was classified as being markedly Brazilian Por-
tuguese, European Portuguese or common to
both variants.
The large majority of the sentences were
judged to be possible in both European and
Brazilian Portuguese. 16% of the sentences in
the European corpus were considered not be-
longing to Brazilian Portuguese, and 21% of the
sentences in the American corpus were judged as
not being European Portuguese.1 Overall, 81%
of the text was common to both varieties.
10KB of text from each one of the three classes
were obtained. 140 lines, approximately 5KB,
were reserved for training and another 140 for
test. In total, the 30 K corpus included 116, 170,
493 and 41 sentence tokens for, respectively, 8,
7, 6 and 5 word length sentence types.
3.2 Variation
These training corpora were submitted to man-
ual inspection in order to identify and quantify
the sources of variant specificity. This is impor-
tant to help interpret the experimental results
and to gain insight on the current coverage of
the grammar used in the experiment.
This analysis was performed over the 140 lines
selected as markedly Brazilian Portuguese, and
assumed that the sources of variant specificity
should have broadly the same distribution in
the other 140K lines markedly European Por-
tuguese.
1. Mere orthographic differences (24%) e.g.
ac?a?o vs. acc?a?o (action)
2. Phonetic variants reflected in orthography
(9.3%) e.g. iro?nico vs. iro?nico (ironic)
1A hypothetical explanation for this asymmetry (16%
vs. 21%) is that one of the most pervasive differences
between European and Brazilian Portuguese, clitic place-
ment, is attenuated in writing: Brazilian text often dis-
plays word order between clitic and verb similar to Euro-
pean Portuguese, and different from oral Brazilian Por-
tuguese. Therefore, European text displaying European
clitic order tends not be seen as markedly European. In
fact, we looked at the European sentences with clitic
placement characteristic of European Portuguese that
were judged possible in Brazilian Portuguese. If they
were included in the markedly European sentences, 23%
of the European text would be unacceptable Brazilian
Portuguese, a number closer to the 21% sentences judged
to be exclusively Brazilian Portuguese in the American
corpus.
3. Lexical differences (26.9% of differences)
(a) Different form, same meaning (22.5%)
e.g. time vs. equipa (team)
(b) Same form, different meaning (4.4%)
e.g. policial (policeman/criminal novel
4. Syntactic differences (39.7%)
(a) Possessives w/out articles (12.2%)
(b) In subcategorization frames (9.8%)
(c) Clitic placement (6.4%)
(d) Singular bare NPs (5.4%)
(e) In subcat and word sense (1.9%)
(f) Universal todo + article (0.9%)
(g) Contractions of Prep+article (0.9%)
(h) Questions w/out SV inversion (0.9%)
(i) Postverbal negation (0.5%)
(j) other (0.5%)
About 1/3 of the differences found would dis-
appear if a unified orthography was adopted.
Differences that are reflected in spelling can be
modeled via multiple lexical entries, with con-
straints on feature VARIANT reflecting the vari-
ety in which the item with that spelling is used.
Interestingly, 40% of the differences are syn-
tactic in nature. These cases are expected to
be more difficult to detect with stochastic ap-
proaches than with a grammar.
4 Self-tuning
4.1 Grammar and baseline
The experiments on the self-tuning approach
were carried out with a computational grammar
for Portuguese developed with the LKB plat-
form (Copestake, 2002) that uses MRS for se-
mantic representation (Copestake et al, 2001)
(Branco and Costa, 2005). At the time of the
experiments reported here, this grammar was
of modest size. In terms of linguistic phenom-
ena, it covered basic declarative sentential struc-
tures and basic phrase structure of all cate-
gories, with a fully detailed account of the struc-
ture of NPs. It contained 42 syntax rules, 37
lexical rules (mostly inflectional) and a total
of 2988 types, with 417 types for lexical en-
tries. There were 2630 hand-built lexical entries,
mostly nouns, with 1000 entries. It was coupled
with a POS tagger for Portuguese, with 97% ac-
curacy (Branco and Silva, 2004).
60
In terms of the sources of variant specificity
identified above, this grammar was specifically
designed to handle the co-occurrence of prenom-
inal possessives and determiners and most of the
syntactic constructions related to clitic-verb or-
der. As revealed by the study of the training
corpus, these constructions are responsible for
almost 20% of marked sentences.
The lexicon contained lexical items markedly
European Portuguese and markedly Brazilian
Portuguese. These were taken from the Por-
tuguese Wiktionary, where this information is
available. Leaving aside the very infrequent
items, around 740 marked lexical items were
coded. Items that are variant specific found in
the training corpora (80 more) were also entered
in the lexicon.
These items, markedly belonging to one vari-
ant, were declined into their inflected forms and
the resulting set Lex
bsl
was used in the following
baseline for dialect tuning: for a sentence s and
N
ep
, resp. N
bp
, the number of tokens of items
in Lex
bsl
markedly European, resp. Brazilian
Portuguese, occurring in s, s is tagged as Euro-
pean Portuguese if N
ep
> N
bp
, or vice-versa, or
else, ?common? Portuguese if N
ep
= N
bp
= 0.
Known Predicted class
class EP BP Common Recall
EP 45 0 95 0.32
BP 3 45 92 0.32
Common 4 4 132 0.94
Precision 0.87 0.98 0.41
Table 1: Baseline: Confusion matrix.
For this baseline, the figure of 0.53 of overall
accuracy was obtained, detailed in Table 1.2
4.2 Results with self-tuning
The results obtained for the self-tuning mode
of operation are presented in Table 2.3 When
the grammar produced multiple analyses for a
2Naturally, extending the operation of this baseline
method beyond the terms of comparability with gram-
mars that handle each sentence at a time, namely by
increasingly extending the number of sentences in the
stretch of text being classified, will virtually lead it to
reach optimal accuracy.
3These figures concern the test corpus, with the three
conditions represented by 1/3 of the sentences, which are
all parsable. Hence, actual recall over a naturally occur-
ring text is expected to be lower. Using the estimate that
only 26% of input receives a parse, that figure for recall
would lie somewhere around 0.15 (= 0.57 x 0.26).
given sentence, that sentence was classified as
markedly European, resp. Brazilian, Portuguese
if all the parses produced VARIANT with type ep-
variant, resp. bp-variant. In all other cases, the
sentence would be classified as common to both
variants.
Known Predicted class
class EP BP Common Recall
EP 53 1 86 0.38
BP 6 61 73 0.44
Common 14 1 125 0.89
Precision 0.73 0.97 0.44
Table 2: Self-tuning: Confusion matrix.
Every sentence in the test data was classified,
and the figure of 0.57 was obtained for over-
all accuracy. The analysis of errors shows that
the sentence belonging to Brazilian Portuguese
or to ?common? Portuguese wrongly classified
as European Portuguese contain clitics follow-
ing the European Portuguese syntax, and some
misspellings conforming to the European Por-
tuguese orthography.
5 Pre-tuning
5.1 Language Detection Methods
Methods have been developed to detect the lan-
guage a given text is written in. They have
also been used to discriminate varieties of the
same language, although less often. (Lins and
Gonc?alves, 2004) look up words in dictionaries
to discriminate among languages, and (Oakes,
2003) runs stochastic tests on token frequencies,
like the chi-square test, in order to differentiate
between European and American English.
Many methods are based on frequency of byte
n-grams in text because they can simultaneously
detect language and character encoding (Li and
Momoi, 2001), and can reliably classify short
portions of text. They have been applied in web
browsers (to identify character encodings) and
information retrieval systems.
We are going to focus on methods based on
character n-grams. Because all information used
for classification is taken from characters, and
they can be found in text in much larger quanti-
ties than words or phrases, problems of scarcity
of data are attenuated. Besides, training data
can also be easily found in large amounts be-
cause corpora do not need to be annotated (it is
61
only necessary to know the language they belong
to). More importantly, methods based on char-
acter n-grams can reliably classify small portions
of text. The literature on automatic language
identification mentions training corpora as small
as 2K producing classifiers that perform with al-
most perfect accuracy for test strings as little as
500 Bytes (Dunning, 1994) and considering sev-
eral languages. With more training data (20K-
50K of text), similar quality can be achieved for
smaller test strings (Prager, 1999).
Many n-gram based methods have been ex-
plored besides the one we opted for.4 Many
can achieve perfect or nearly perfect classifica-
tion with small training corpora on small texts.
In previous work (Branco and Costa, 2007),
we did a comparative study on two classifiers
that use approaches very well understood in
language processing and information retrieval,
namely Vector Space and Bayesian models. We
retain here the latter as this one scored compar-
atively better for the current purposes.
In order to know which language L
i
? L gen-
erated string s, Bayesian methods can be used
to calculate the probabilities P (s|L
i
) of string s
appearing in language L
i
for all L
i
? L, the con-
sidered language set, and decide for the language
with the highest score (Dunning, 1994). That is,
in order to compute P (L
i
|s), we only compute
P (s|L
i
). The Bayes rule allows us to cast the
problem in terms of P (s|Li)P (Li )
P (s)
, but as is stan-
dard practice, the denominator is dropped since
we are only interested here in getting the highest
probability, not its exact value. The prior P (L
i
)
is also ignored, corresponding to the simplify-
ing assumption that all languages are equally
probable for the operation of the classifier. The
way P (s|L
i
) is calculated is also the standard
way to do it, namely assuming independence
and just multiplying the probabilities of charac-
ter c
i
given the preceding n-1 characters (using
n-grams), for all characters in the input (esti-
mated from n-gram counts in the training set).
For our experiments, we implemented the al-
gorithm described in (Dunning, 1994). Other
common strategies were also used, like prepend-
ing n?1 special characters to the input string to
harmonize calculations, summing logs of proba-
bilities instead of multiplying them to avoid un-
4See (Sibun and Reynar, 1996) and (Hughes et al,
2006) for surveys.
derflow errors, and using Laplace smoothing to
reserve probability mass to events not seen in
training.
5.2 Calibrating the implementation
5.2.1 Detection of languages
First of all, we want to check that the lan-
guage identification methods we are using, and
have implemented, are in fact reliable to identify
different languages. Hence, we run the classifier
on three languages showing strikingly different
characters and character sequences. This is a
deliberately easy test to get insight into the ap-
propriate setting of the two parameters at stake
here, size of of the n-gram in the training phase,
and size of the input in the running phase.
For this test, we used the Universal Declara-
tion of Human Rights texts.The languages used
were Finnish, Portuguese and Welsh.5
Several tests were conducted, splitting the
test data in chunks 1, 5, 10 and 20 lines long.
The classifier obtained perfect accuracy on all
test conditions (all chunk sizes), for all values of
n between 1 and 7 (inclusively). For n = 8 and
n = 9 there were errors only when classifying 1
line long items.
The average line length for the test corpora
was 138 characters for Finnish, 141 for Por-
tuguese and 121 for Welsh (133 overall). In the
corpora we will be using in the following experi-
ments, average line length is much lower (around
40 characters per line). To become closer to
our experimental conditions, we also evaluated
this classifiers with the same test corpora, but
truncated each line beyond the first 50 charac-
ters, yielding test corpora with an average line
length around 38 characters (since some were
smaller than that). The results are similar. The
Bayesian classifier performed with less than per-
fect accuracy also with n = 7 when classifying 1
line at a time.
Our classifier was thus performing well at dis-
criminating languages with short values of n,
and can classify short bits of text, even with
incomplete words.
5The Preamble and Articles 1?19 were used for train-
ing (8.1K of Finnish, 6.9K of Portuguese, and 6.1K of
Welsh), and Articles 20?30 for testing (4.6K of Finnish,
4.7K of Portuguese, and 4.0K of Welsh).
62
5.2.2 Detection of originating corpus
In order to study its suitability to discrimi-
nate also the two Portuguese variants, we ex-
perimented our implementation of the Bayesian
classifiers on 200K lines of text from each of the
two corpora. We randomly chose 20K lines for
testing and the remaining 180K for training. A
classification is considered correct if the classi-
fier can guess the newspaper the text was taken
from.
The average line length of the test sentences is
43 characters. Several input lengths were tried
out by dividing the test data into various sets
with varying size. Table 3 summarizes the re-
sults obtained.
Length of Test Item
1 line 5 lines 10 lines 20 lines
n = 2 0.84 0.99 1 1
n = 3 0.96 0.99 1 1
n = 4 0.96 1 1 1
n = 5 0.94 1 1 1
n = 6 0.92 0.99 1 1
n = 7 0.89 0.98 0.99 1
Table 3: Originating corpora: Accuracy
The accuracy of the classifier is surprisingly
high given that the sentences that cannot be at-
tributed to a single variety are estimated to be
around 81%.
5.2.3 Scaling down the training data
A final check was made with the classifier
to gain further insight on the comparability of
the results obtained under the two tuning ap-
proaches. It was trained on the data prepared
for the actual experiment, made of the 10K
with lines that have the shortest length and are
parsable, but using only the markedly European
and Brazilian Portuguese data (leaving aside the
sentences judged to be common to both). This
way the two setups can be compared, since in
the test of the Subsection just above much more
data was available for training.
Results are in Table 4. As expected, with
a much smaller amount of training data there
is an overall drop in the accuracy, with a no-
ticed bias at classifying items as European Por-
tuguese. The performance of the classifier de-
grades with larger values of n. Nevertheless, the
classifier is still very good with bigrams, with an
Length of Test Item
1 line 5 lines 10 lines 20 lines
n = 2 0.86 0.98 0.96 1
n = 3 0.82 0.73 0.64 0.5
n = 4 0.68 0.55 0.5 0.5
Table 4: Two-way classification: Accuracy
almost optimal performance, only slightly worse
than the one observed in the previous Subsec-
tion, when it was trained with more data.
From these preliminary tests, we learned that
we could expect a quasi optimal performance of
the classifier we implemented to act as a prepro-
cessor in the pre-tuning approach, when n = 2
and it is run under conditions very close to the
ones it will encounter in the actual experiment
aimed at comparing the two tuning approaches.
5.3 Results with pre-tuning
In the final experiment, the classifier should
discriminate between three classes, deciding
whether the input is either specifically Euro-
pean or Brazilian Portuguese, or else whether
it belongs to both variants. It was trained over
the 15K tokens/420 lines of training data, and
tested over the held out test data of identical
size.
Length of Test Item
1 line 5 lines 10 lines 20 lines
n = 2 0.59 0.67 0.76 0.76
n = 3 0.55 0.52 0.45 0.33
n = 4 0.48 0.39 0.33 0.33
Table 5: Three-way classification: Accuracy
The results are in Table 5. As expected, the
classifier based in bigrams has the best perfor-
mance for every size of the input, which im-
proves from 0.59 to 0.76 as the size of the input
gets from 1 line to 20 lines.
6 Discussion and conclusions
From the results above for pre-tuning, it is the
value 0.59, obtained for 1 line of input, that can
be put on a par with the value of 0.57 obtained
for self-tuning ? both of them to be appreciated
against the baseline of 0.53.
Interestingly, the performance of both ap-
proaches are quite similar, and quite encour-
aging given the limitations under which the
present pilot exercise was executed. But this is
63
also the reason why they should be considered
with the appropriate grano salis.
Note that there is much room for improve-
ment in both approaches. From the several
sources of variant specificity, the grammar used
was prepared to cope only with grammatical
constructs that are responsible for at most 20%
of them. Also the lexicon, that included a little
more than 800 variant-distinctive items, can be
largely improved.
As to the classifier used for pre-tuning, it im-
plements methods that may achieve optimal ac-
curacy with training data sets of modest size but
that need to be nevertheless larger than the very
scarce 15K tokens used this time. Using backoff
and interpolation will help to improve as well.
Some features potentially distinguish, how-
ever, the pre-tuning based on Bayesian classifier
from the self-tuning by the grammar.
Language detection methods are easy to scale
up with respect to the number of variants used.
In contrast, the size of the type hierarchy under
variant is exponential on the number of language
variants if all combinations of variants are taken
into account, as it seems reasonable to do.
N-grams based methods are efficient and can
be very accurate. On the other hand, like any
stochastic method, they are sensitive to training
data and tend to be much more affected than the
grammar in self-tuning by a change of text do-
main. Also in dialogue settings with turns from
different language variants, hence with small
lengths of texts available to classify and suc-
cessive alternation between language variants,
n-grams are likely to show less advantage than
self-tuning by fully fledged grammars.
These are issues over which more acute insight
will be gained in future work, which will seek
to improve the contributions put forward in the
present paper.
Summing up, a major contribution of the
present paper is a design strategy for type-
feature grammars that allows them to be appro-
priately set to the specific language variant of a
given input. Concomitantly, this design allows
the grammars either to be pre-tuned or to self-
tune to that dialect ? which, to the best of our
knowledge, consists in a new kind of approach to
handling language variation in deep processing.
In addition, we undertook a pilot experiment
which can be taken as setting the basis for a
methodology to comparatively assess the perfor-
mance of these different tuning approaches and
their future improvements.
References
Anto?nio Branco and Francisco Costa. 2005. LX-
GRAM ? deep linguistic processing of Portuguese
with HSPG. Technical report, Dept. of Informat-
ics, University of Lisbon.
Anto?nio Branco and Francisco Costa. 2007. Han-
dling language variation in deep processing. In
Proc. CLIN2007.
Anto?nio Branco and Joa?o Silva. 2004. Evaluat-
ing solutions for the rapid development of state-
of-the-art POS taggers for Portuguese. In Proc.
LREC2004.
Ann Copestake, Dan Flickinger, Carl Pollard, and
Ivan Sag. 2001. Minimal Recursion Semantics:
An introduction. Language and Computation, 3.
Ann Copestake. 2002. Implementing typed feature
structure grammars. CSLI.
Ted Dunning. 1994. Statistical identification of lan-
guage. Technical Report MCCS-94-273, Comput-
ing Research Lab, New Mexico State Univ.
Baden Hughes, Timothy Baldwin, Steven Bird,
Jeremy Nicholson, and Andrew MacKinlay. 2006.
Reconsidering language identification for written
language resources. In Proc. LREC2006.
Shanjian Li and Katsuhiko Momoi. 2001. A com-
posite approach to language/encoding detection.
In Proc. 19th International Unicode Conference.
Rafael Lins and Paulo Gonc?alves. 2004. Automatic
language identification of written texts. In Proc.
2004 ACM Symposium on Applied Computing.
Michael P. Oakes. 2003. Text categorization: Auto-
matic discrimination between US and UK English
using the chi-square test and high ratio pairs. Re-
search in Language, 1.
Carl Pollard and Ivan Sag. 1994. Head-driven phrase
structure grammar. CSLI.
John M. Prager. 1999. Linguini: Language iden-
tification for multilingual documents. Journal of
Management Information Systems, 16(3).
Penelope Sibun and Jeffrey C. Reynar. 1996. Lan-
guage identification: Examining the issues. In 5th
Symposium on Document Analysis and IR.
64
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 671?677,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Temporal information processing of a new language:
fast porting with minimal resources
Francisco Costa and Anto?nio Branco
Universidade de Lisboa
Abstract
We describe the semi-automatic adapta-
tion of a TimeML annotated corpus from
English to Portuguese, a language for
which TimeML annotated data was not
available yet. In order to validate this
adaptation, we use the obtained data to
replicate some results in the literature that
used the original English data. The fact
that comparable results are obtained indi-
cates that our approach can be used suc-
cessfully to rapidly create semantically an-
notated resources for new languages.
1 Introduction
Temporal information processing is a topic of nat-
ural language processing boosted by recent eval-
uation campaigns like TERN2004,1 TempEval-1
(Verhagen et al, 2007) and the forthcoming
TempEval-22 (Pustejovsky and Verhagen, 2009).
For instance, in the TempEval-1 competition, three
tasks were proposed: a) identifying the temporal
relation (such as overlap, before or after) hold-
ing between events and temporal entities such as
dates, times and temporal durations denoted by ex-
pressions (i.e. temporal expressions) occurring in
the same sentence; b) identifying the temporal re-
lation holding between events expressed in a doc-
ument and its creation time; c) identifying the tem-
poral relation between the main events expressed
by two adjacent sentences.
Supervised machine learning approaches are
pervasive in the tasks of temporal information pro-
cessing. Even when the best performing sys-
tems in these competitions are symbolic, there are
machine learning solutions with results close to
their performance. In TempEval-1, where there
were statistical and rule-based systems, almost
1http://timex2.mitre.org
2http://www.timeml.org/tempeval2
all systems achieved quite similar results. In the
TERN2004 competition (aimed at identifying and
normalizing temporal expressions), a symbolic
system performed best, but since then machine
learning solutions, such as (Ahn et al, 2007), have
appeared that obtain similar results.
These evaluations made available sets of anno-
tated data for English and other languages, used
for training and evaluation. One natural question
to ask is whether it is feasible to adapt the training
and test data made available in these competitions
to other languages, for which no such data still ex-
ist. Since the annotations are largely of a seman-
tic nature, not many changes need to be done in
the annotations once the textual material is trans-
lated. In essence, this would be a fast way to create
temporal information processing systems for lan-
guages for which there are no annotated data yet.
In this paper, we report on an experiment
that consisted in adapting the English data of
TempEval-1 to Portuguese. The results of ma-
chine learning algorithms over the data thus ob-
tained are compared to those reported for the En-
glish TempEval-1 competition. Since the results
are quite similar, this permits to conclude that
such an approach can rapidly generate relevant and
comparable data and is useful when porting tem-
poral information processing solutions to new lan-
guages.
The advantages of adapting an existing corpus
instead of annotating text from scratch are: i)
potentially less time consuming, if it is faster to
translate the original text than it is to annotate
new text (this can be the case if the annotations
are semantic and complex); b) the annotations can
be transposed without substantial modifications,
which is the case if they are semantic in nature;
c) less man power required: text annotation re-
quires multiple annotators in order to guarantee
the quality of the annotation tags, translation of
the markables and transposition of the annotations
671
in principle do not; d) the data obtained are com-
parable to the original data in all respects except
for language: genre, domain, size, style, annota-
tion decisions, etc., which allows for research to
be conducted with a derived corpus that is compa-
rable to research using the original corpus. There
is of course the caveat that the adaptation process
can introduce errors.
This paper proceeds as follows. In Section 2,
we provide a quick overview of the TimeML an-
notations in the TempEval-1 data. In Section 3,
it is described how the data were adapted to Por-
tuguese. Section 4 contains a brief quantitative
comparison of the two corpora. In Section 5, the
results of replicating one of the approaches present
in the TempEval-1 challenge with the Portuguese
data are presented. We conclude this paper in Sec-
tion 6.
2 Brief Description of the Annotations
Figure 1 contains an example of a document from
the TempEval-1 corpus, which is similar to the
TimeBank corpus (Pustejovsky et al, 2003).
In this corpus, event terms are tagged with
<EVENT>. The relevant attributes are tense,
aspect, class, polarity, pos, stem. The
stem is the term?s lemma, and pos is its part-of-
speech. Grammatical tense and aspect are encoded
in the features tense and aspect. The attribute
polarity takes the value NEG if the event term
is in a negative syntactic context, and POS other-
wise. The attribute class contains several lev-
els of information. It makes a distinction between
terms that denote actions of speaking, which take
the value REPORTING and those that do not.
For these, it distinguishes between states (value
STATE) and non-states (value OCCURRENCE),
and it also encodes whether they create an in-
tensional context (value I STATE for states and
value I ACTION for non-states).
Temporal expressions (timexes) are inside
<TIMEX3> elements. The most important fea-
tures for these elements are value, type and
mod. The timex?s value encodes a normal-
ized representation of this temporal entity, its
type can be e.g. DATE, TIME or DURATION.
The mod attribute is optional. It is used for ex-
pressions like early this year, which are anno-
tated with mod="START". As can be seen in
Figure 1 there are other attributes for timexes
that encode whether it is the document?s creation
time (functionInDocument) and whether its
value can be determined from the expression
alone or requires other sources of information
(temporalFunction and anchorTimeID).
The <TLINK> elements encode temporal re-
lations. The attribute relType represents the
type of relation, the feature eventID is a ref-
erence to the first argument of the relation.
The second argument is given by the attribute
relatedToTime (if it is a time interval or du-
ration) or relatedToEvent (if it is another
event; this is for task C). The task feature is the
name of the TempEval-1 task to which this tempo-
ral relation pertains.
3 Data Adaptation
We cleaned all TimeML markup in the
TempEval-1 data and the result was fed to
the Google Translator Toolkit.3 This tool com-
bines machine translation with a translation
memory. A human translator corrected the
proposed translations manually.
After that, we had the three collections of docu-
ments (the TimeML data, the English unannotated
data and the Portuguese unannotated data) aligned
by paragraphs (we just kept the line breaks from
the original collection in the other collections). In
this way, for each paragraph in the Portuguese data
we know all the corresponding TimeML tags in
the original English paragraph.
We tried using machine translation software (we
used GIZA++ (Och and Ney, 2003)) to perform
word alignment on the unannotated texts, which
would have enabled us to transpose the TimeML
annotations automatically. However, word align-
ment algorithms have suboptimal accuracy, so the
results would have to be checked manually. There-
fore we abandoned this idea, and instead we sim-
ply placed the different TimeML markup in the
correct positions manually. This is possible since
the TempEval-1 corpus is not very large. A small
script was developed to place all relevant TimeML
markup at the end of each paragraph in the Por-
tuguese text, and then each tag was manually repo-
sitioned. Note that the <TLINK> elements always
occur at the end of each document, each in a sep-
arate line: therefore they do not need to be reposi-
tioned.
During this manual repositioning of the anno-
tations, some attributes were also changed man-
3http://translate.google.com/toolkit
672
<?xml version="1.0" ?>
<TempEval>
ABC<TIMEX3 tid="t52" type="DATE" value="1998-01-14" temporalFunction="false"
functionInDocument="CREATION_TIME">19980114</TIMEX3>.1830.0611
NEWS STORY
<s>In Washington <TIMEX3 tid="t53" type="DATE" value="1998-01-14" temporalFunction="true"
functionInDocument="NONE" anchorTimeID="t52">today</TIMEX3>, the Federal Aviation Administration <EVENT
eid="e1" class="OCCURRENCE" stem="release" aspect="NONE" tense="PAST" polarity="POS" pos="VERB">released
</EVENT> air traffic control tapes from <TIMEX3 tid="t54" type="TIME" value="1998-XX-XXTNI"
temporalFunction="true" functionInDocument="NONE" anchorTimeID="t52">the night</TIMEX3> the TWA Flight
eight hundred <EVENT eid="e2" class="OCCURRENCE" stem="go" aspect="NONE" tense="PAST" polarity="POS"
pos="VERB">went</EVENT>down.</s>
...
<TLINK lid="l1" relType="BEFORE" eventID="e2" relatedToTime="t53" task="A"/>
<TLINK lid="l2" relType="OVERLAP" eventID="e2" relatedToTime="t54" task="A"/>
<TLINK lid="l4" relType="BEFORE" eventID="e2" relatedToTime="t52" task="B"/>
...
</TempEval>
Figure 1: Extract of a document contained in the training data of the first TempEval-1
ually. In particular, the attributes stem, tense
and aspect of <EVENT> elements are language
specific and needed to be adapted. Sometimes, the
pos attribute also needs to be changed, since e.g.
a verb in English can be translated as a noun in
Portuguese. The attribute class of the same kind
of elements can be different, too, because natural
sounding translations are sometimes not literal.
3.1 Annotation Decisions
When porting the TimeML annotations from En-
glish to Portuguese, a few decisions had to be
made. For illustration purposes, Figure 2 contains
the Portuguese equivalent of the extract presented
in Figure 1.
For <TIMEX3> elements, the issue is that if the
temporal expression to be annotated is a preposi-
tional phrase, the preposition should not be inside
the <TIMEX3> tags according to the TimeML
specification. In the case of Portuguese, this raises
the question of whether to leave contractions of
prepositions with determiners outside these tags
(in the English data the preposition is outside and
the determiner is inside).4 We chose to leave them
outside, as can be seen in that Figure. In this ex-
ample the prepositional phrase from the night/da
noite is annotated with the English noun phrase
the night inside the <TIMEX3> element, but the
Portuguese version only contains the noun noite
inside those tags.
For <EVENT> elements, some of the attributes
are adapted. The value of the attribute stem is
4The fact that prepositions are placed outside of temporal
expressions seems odd at first, but this is because in the orig-
inal TimeBank, from which the TempEval data were derived,
they are tagged as <SIGNAL>s. The TempEval-1 data does
not contain <SIGNAL> elements, however.
obviously different in Portuguese. The attributes
aspect and tense have a different set of
possible values in the Portuguese data, simply
because the morphology of the two languages
is different. In the example in Figure 1 the
value PPI for the attribute tense stands for
prete?rito perfeito do indicativo. We chose to
include mood information in the tense attribute
because the different tenses of the indicative and
the subjunctive moods do not line up perfectly
as there are more tenses for the indicative than
for the subjunctive. For the aspect attribute,
which encodes grammatical aspect, we only
use the values NONE and PROGRESSIVE,
leaving out the values PERFECTIVE and
PERFECTIVE PROGRESSIVE, as in Portuguese
there is no easy match between perfective aspect
and grammatical categories.
The attributes of <TIMEX3> elements carry
over to the Portuguese corpus unchanged, and the
<TLINK> elements are taken verbatim from the
original documents.
4 Data Description
The original English data for TempEval-1 are
based on the TimeBank data, and they are split
into one dataset for training and development and
another dataset for evaluation. The full data are or-
ganized in 182 documents (162 documents in the
training data and another 20 in the test data). Each
document is a news report from television broad-
casts or newspapers. A large amount of the doc-
uments (123 in the training set and 12 in the test
data) are taken from a 1989 issue of the Wall Street
Journal.
The training data comprise 162 documents with
673
<?xml version="1.0" encoding="UTF-8" ?>
<TempEval>
ABC<TIMEX3 tid="t52" type="DATE" value="1998-01-14" temporalFunction="false"
functionInDocument="CREATION_TIME">19980114</TIMEX3>.1830.1611
REPORTAGEM
<s>Em Washington, <TIMEX3 tid="t53" type="DATE" value="1998-01-14" temporalFunction="true"
functionInDocument="NONE" anchorTimeID="t52">hoje</TIMEX3>, a Federal Aviation Administration <EVENT
eid="e1" class="OCCURRENCE" stem="publicar" aspect="NONE" tense="PPI" polarity="POS" pos="VERB">publicou
</EVENT> gravaoes do controlo de trfego areo da <TIMEX3 tid="t54" type="TIME" value="1998-XX-XXTNI"
temporalFunction="true" functionInDocument="NONE" anchorTimeID="t52">noite</TIMEX3> em que o voo TWA800
<EVENT eid="e2" class="OCCURRENCE" stem="cair" aspect="NONE" tense="PPI" polarity="POS" pos="VERB">caiu
</EVENT>
.</s>
...
<TLINK lid="l1" relType="BEFORE" eventID="e2" relatedToTime="t53" task="A"/>
<TLINK lid="l2" relType="OVERLAP" eventID="e2" relatedToTime="t54" task="A"/>
<TLINK lid="l4" relType="BEFORE" eventID="e2" relatedToTime="t52" task="B"/>
...
</TempEval>
Figure 2: Extract of a document contained in the Portuguese data
2,236 sentences (i.e. 2236 <s> elements) and
52,740 words. It contains 6799 <EVENT> el-
ements, 1,244 <TIMEX3> elements and 5,790
<TLINK> elements. Note that not all the events
are included here: the ones expressed by words
that occur less than 20 times in TimeBank were
removed from the TempEval-1 data.
The test dataset contains 376 sentences and
8,107 words. The number of <EVENT> elements
is 1,103; there are 165 <TIMEX3>s and 758
<TLINK>s.
The Portuguese data of course contain the same
(translated) documents. The training dataset has
2,280 sentences and 60,781 words. The test data
contains 351 sentences and 8,920 words.
5 Comparing the two Datasets
One of the systems participating in the
TempEval-1 competition, the USFD system
(Hepple et al, 2007), implemented a very
straightforward solution: it simply trained classi-
fiers with Weka (Witten and Frank, 2005), using
as attributes information that was readily available
in the data and did not require any extra natural
language processing (for all tasks, the attribute
relType of <TLINK> elements is unknown and
must be discovered, but all the other information
is given).
The authors? objectives were to see ?whether a
?lite? approach of this kind could yield reasonable
performance, before pursuing possibilities that re-
lied on ?deeper? NLP analysis methods?, ?which
of the features would contribute positively to sys-
tem performance? and ?if any [machine learning]
approach was better suited to the TempEval tasks
than any other?. In spite of its simplicity, they ob-
tained results quite close to the best systems.
For us, the results of (Hepple et al, 2007) are in-
teresting as they allow for a straightforward evalu-
ation of our adaptation efforts, since the same ma-
chine learning implementations can be used with
the Portuguese data, and then compared to their
results.
The differences in the data are mostly due to
language. Since the languages are different, the
distribution of the values of several attributes are
different. For instance, we included both tense
and mood information in the tense attribute of
<EVENT>s, as mentioned in Section 3.1, so in-
stead of seven possible values for this attribute, the
Portuguese data contains more values, which can
cause more data sparseness. Other attributes af-
fected by language differences are aspect, pos,
and class, which were also possibly changed
during the adaptation process.
One important difference between the English
and the Portuguese data originates from the fact
that events with a frequency lower than 20 were
removed from the English TempEval-1 data. Since
there is not a 1 to 1 relation between English event
terms and Portuguese event terms, we do not have
the guarantee that all event terms in the Portuguese
data have a frequency of at least 20 occurrences in
the entire corpus.5
The work of (Hepple et al, 2007) reports on
both cross-validation results for various classifiers
over the training data and evaluation results on the
training data, for the English dataset. We we will
5In fact, out of 1,649 different stems for event terms in the
Portuguese training data, only 45 occur at least 20 times.
674
Task
Attribute A B C
EVENT-aspect ! ! !
EVENT-polarity ! ! ?
EVENT-POS ! ! !
EVENT-stem ! ? ?
EVENT-string ? ? ?
EVENT-class ? ! !
EVENT-tense ? ! !
ORDER-adjacent ! N/A N/A
ORDER-event-first ! N/A N/A
ORDER-event-between ? N/A N/A
ORDER-timex-between ? N/A N/A
TIMEX3-mod ! ? N/A
TIMEX3-type ! ? N/A
Table 1: Features used for the English TempEval-1
tasks. N/A means the feature was not applicable to
the task,!means the feature was used by the best
performing classifier for the task, and ? means it
was not used by that classifier. From (Hepple et
al., 2007).
be comparing their results to ours.
Our purpose with this comparison is to validate
the corpus adaptation. Similar results would not
necessarily indicate the quality of the adapted cor-
pus. After all, a word-by-word translation would
produce data that would yield similar results, but
it would also be a very poor translation, and there-
fore the resulting corpus would not be very inter-
esting. The quality of the translation is not at stake
here, since it was manually revised. But similar
results would indicate that the obtained data are
comparable to the original data, and that they are
similarly useful to tackle the problem for which
the original data were collected. This would con-
firm our hypothesis that adapting an existing cor-
pus can be an effective way to obtain new data for
a different language.
5.1 Results for English
The attributes employed for English by (Hepple et
al., 2007) are summarized in Table 1. The class is
the attribute relType of <TLINK> elements.
The EVENT features are taken from <EVENT>
elements. The EVENT-string attribute is the
character data inside the element. The other at-
tributes correspond to the feature of <EVENT>
with the same name. The TIMEX3 features
Task
Algorithm A B C
baseline 49.8 62.1 42.0
lazy.KStar 58.2 76.7 54.0
rules.DecisionTable 53.3 79.0 52.9
functions.SMO 55.1 78.1 55.5
rules.JRip 50.7 78.6 53.4
bayes.NaiveBayes 56.3 76.2 50.7
Table 2: Performance of several machine learn-
ing algorithms on the English TempEval-1 train-
ing data, with cross-validation. The best result
for each task is in boldface. From (Hepple et al,
2007).
also correspond to attributes of the relevant
<TIMEX3> element. The ORDER features are
boolean and computed as follows:
? ORDER-event-first is whether the
<EVENT> element occurs in the text before
the <TIMEX3> element;
? ORDER-event-between is whether an
<EVENT> element occurs in the text between
the two temporal entities being ordered;
? ORDER-timex-between is the same, but
for temporal expressions;
? ORDER-adjacent is whether both
ORDER-event-between and ORDER-
timex-between are false (but other
textual data may occur between the two
entities).
Cross-validation over the training data pro-
duced the results in Table 2. The base-
line used is the majority class baseline, as
given by Weka?s rules.ZeroR implemen-
tation. The lazy.KStar algorithm is a
nearest-neighbor classifier that uses an entropy-
based measure to compute instance similarity.
Weka?s rules.DecisionTable algorithm as-
signs to an unknown instance the majority class
of the training examples that have the same
attribute values as that instance that is be-
ing classified. functions.SMO is an imple-
mentation of Support Vector Machines (SVM),
rules.JRip is the RIPPER algorithm, and
bayes.NaiveBayes is a Naive Bayes classi-
fier.
675
Task
Algorithm A B C
baseline 49.8 62.1 42.0
lazy.KStar 57.4 77.7 53.3
rules.DecisionTable 54.2 78.1 51.6
functions.SMO 55.5 79.3 56.8
rules.JRip 52.1 77.6 52.1
bayes.NaiveBayes 56.0 78.2 53.5
trees.J48 55.6 79.0 59.3
Table 3: Performance of several machine learn-
ing algorithms on the Portuguese data for the
TempEval-1 tasks. The best result for each task
is in boldface.
5.2 Attributes
We created a small script to convert the XML an-
notated files into CSV files, that can be read by
Weka. In this process, we included the same at-
tributes as the USFD authors used for English.
For task C, (Hepple et al, 2007) are not very
clear whether the EVENT attributes used were re-
lated to just one of the two events being temporally
related. In any case, we used two of each of the
EVENT attributes, one for each event in the tempo-
ral relation to be determined. So, for instance, an
extra attribute EVENT2-tense is where the tense
of the second event in the temporal relation is kept.
5.3 Results
The majority class baselines produce the same
results as for English. This was expected: the
class distribution is the same in the two datasets,
since the <TLINK> elements were copied to the
adapted corpus without any changes.
For the sake of comparison, we used the same
classifiers as (Hepple et al, 2007), and we used the
attributes that they found to work best for English
(presented above in Table 1). The results for the
Portuguese dataset are in Table 3, using 10-fold
cross-validation on the training data.
We also present the results for Weka?s imple-
mentation of the C4.5 algorithm, to induce deci-
sion trees. The motivation to run this algorithm
over these data is that decision trees are human
readable and make it easy to inspect what deci-
sions the classifier is making. This is also true of
rules.JRip. The results for the decision trees
are in this table, too.
The results obtained are almost identical to the
results for the original dataset in English. The best
performing classifier for task A is the same as for
English. For task B, Weka?s functions.SMO
produced better results with the Portuguese data
than rules.DecisionTable, the best per-
forming classifier with the English data for this
task. In task C, the SVM algorithm was also the
best performing algorithm among those that were
also tried on the English data, but decision trees
produced even better results here.
For English, the best performing classifier for
each task on the training data, according to Ta-
ble 2, was used for evaluation on the test data: the
results showed a 59% F-measure for task A, 73%
for task B, and 54% for task C.
Similarly, we also evaluated the best algorithm
for each task (according to Table 3) with the Por-
tuguese test data, after training it on the entire
training dataset. The results are: in task A the
lazy.KStar classifier scored 58.6%, and the
SVM classifier scored 75.5% in task B and 59.4%
in task C, with trees.J48 scoring 61% in this
task.
The results on the test data are also fairly similar
for the two languages/datasets.
We inspected the decision trees and rule sets
produced by trees.J48 and rules.JRip, in
order to see what the classifiers are doing.
Task B is probably the easiest task to check this
way, because we expect grammatical tense to be
highly predictive of the temporal order between an
event and the document?s creation time.
And, indeed, the top of the tree induced by
trees.J48 is quite interesting:
eTense = PI: OVERLAP (388.0/95.0)
eTense = PPI: BEFORE (1051.0/41.0)
Here, eTense is the EVENT-tense attribute
of <EVENT> elements, PI stands for present in-
dicative, and PPI is past indicative (prete?rito per-
feito do indicativo). In general, one sees past
tenses associated with the BEFORE class and fu-
ture tenses associated with the AFTER class (in-
cluding the conditional forms of verbs). Infini-
tives are mostly associated with the AFTER class,
and present subjunctive forms with AFTER and
OVERLAP. Figure 3 shows the rule set induced by
the RIPPER algorithm.
The classifiers for the other tasks are more dif-
ficult to inspect. For instance, in task A, the event
term and the temporal expression that denote the
entities that are to be ordered may not even be di-
rectly syntactically related. Therefore, it is hard to
676
(eClass = OCCURRENCE) and ( eTense = INF) and ( ePolarity = POS) => lRelType= AFTER
(183.0/77.0)
( eTense = FI) => lRelType= AFTER (55.0/10.0)
(eClass = OCCURRENCE) and ( eTense = IR-PI+INF) => lRelType= AFTER (26.0/4.0)
(eClass = OCCURRENCE) and ( eTense = PC) => lRelType= AFTER (15.0/3.0)
(eClass = OCCURRENCE) and ( eTense = C) => lRelType= AFTER (17.0/2.0)
( eTense = PI) => lRelType= OVERLAP (388.0/95.0)
(eClass = ASPECTUAL) and ( eTense = PC) => lRelType= OVERLAP (9.0/2.0)
=> lRelType= BEFORE (1863.0/373.0)
Figure 3: rules.JRip classifier induced for task B. INF stands for infinitive, FI is future indicative,
IR-PI+INF is an infinitive form following a present indicative form of the verb ir (to go), PC is present
subjunctive, C is conditional, PI is present indicative.
see how interesting the inferred rules are, because
we do not know what would be interesting in this
scenario. In any case, the top of the induced tree
for task A is:
oAdjacent = True: OVERLAP (554.0/128.0)
Here, oAdjacent is the ORDER-adjacent
attribute. Assuming this attribute is an indication
that the event term and the temporal expression are
related syntactically, it is interesting to see that the
typical temporal relation between the two entities
in this case is an OVERLAP relation. The rest of
the tree is much more ad-hoc, making frequent use
of the stem attribute of <EVENT> elements, sug-
gesting the classifier is memorizing the data.
Task C, where two events are to be ordered, pro-
duced more complicated classifiers. Generally the
induced rules and the tree paths compare the tense
and the class of the two event terms, showing some
expected heuristics (such as, if the tense of the first
event is future and the tense of the second event
is past, assign AFTER). But there are also many
several rules for which we do not have clear intu-
itions.
6 Discussion
In this paper, we described the semi-automatic
adaptation of a TimeML annotated corpus from
English to Portuguese, a language for which
TimeML annotated data was not available yet.
Because most of the TimeML annotations are
semantic in nature, they can be transposed to a
translation of the original corpus, with few adap-
tations being required.
In order to validate this adaptation, we used the
obtained data to replicate some results in the liter-
ature that used the original English data.
The results for the Portuguese data are very sim-
ilar to the ones for English. This indicates that our
approach to adapt existing annotated data to a dif-
ferent language is fruitful.
References
David Ahn, Joris van Rantwijk, and Maarten de Ri-
jke. 2007. A cascaded machine learning approach
to interpreting temporal expressions. In Human
Language Technologies 2007: The Conference of
the North American Chapter of the Association for
Computational Linguistics; Proceedings of the Main
Conference, pages 420?427, Rochester, New York,
April. Association for Computational Linguistics.
Mark Hepple, Andrea Setzer, and Rob Gaizauskas.
2007. USFD: Preliminary exploration of fea-
tures and classifiers for the TempEval-2007 tasks.
In Proceedings of SemEval-2007, pages 484?487,
Prague, Czech Republic. Association for Computa-
tional Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
James Pustejovsky and Marc Verhagen. 2009.
Semeval-2010 task 13: evaluating events, time ex-
pressions, and temporal relations (tempeval-2). In
Proceedings of the Workshop on Semantic Evalua-
tions: Recent Achievements and Future Directions,
pages 112?116, Boulder, Colorado. Association for
Computational Linguistics.
James Pustejovsky, Patrick Hanks, Roser Saur??, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, and Marcia Lazo. 2003. The TIMEBANK
corpus. In Proceedings of Corpus Linguistics 2003,
pages 647?656.
M. Verhagen, R. Gaizauskas, F. Schilder, M. Hepple,
and J. Pustejovsky. 2007. SemEval-2007 Task 15:
TempEval temporal relation identification. In Pro-
ceedings of SemEval-2007.
Ian H. Witten and Eibe Frank. 2005. Data Mining:
Practical Machine Learning Tools and Techniques
with Java Implementations. Morgan Kaufmann, San
Francisco. second edition.
677
High Precision Analysis of NPs
with a Deep Processing
Grammar
Ant?nio Branco
Francisco Costa
Universidade de Lisboa (Portugal)
email: Antonio.Branco@di.fc.ul.pt
Abstract
In this paper we present LXGram, a general purpose grammar for the
deep linguistic processing of Portuguese that aims at delivering detailed
and high precision meaning representations. LXGram is grounded on the
linguistic framework of Head-Driven Phrase Structure Grammar (HPSG).
HPSG is a declarative formalism resorting to unification and a type sys-
tem with multiple inheritance. The semantic representations that LX-
Gram associates with linguistic expressions use the Minimal Recursion
Semantics (MRS) format, which allows for the underspecification of scope
effects. LXGram is developed in the Linguistic KnowledgeBuilder (LKB)
system, a grammar development environment that provides debugging
tools and efficient algorithms for parsing and generation. The implemen-
tation of LXGram has focused on the structure of Noun Phrases, and LX-
Gram accounts for many NP related phenomena. Its coverage continues
to be increased with new phenomena, and there is active work on ex-
tending the grammar?s lexicon. We have already integrated, or plan to
integrate, LXGram in a few applications, namely paraphrasing, treebank-
ing and language variant detection. Grammar coverage has been tested
on newspaper text.
31
32 Branco and Costa
1 Introduction
In this paper we present LXGram, a hand-built, general purpose computational gram-
mar for the deep linguistic processing of Portuguese, specially geared to high precision
processing of Noun Phrases. This grammar is based on the framework of Head-Driven
Phrase Structure Grammar (HPSG; Pollard and Sag (1994)), one of the most promi-
nent linguistic theories being used in natural language processing. Like several other
computational HPSGs, LXGram uses Minimal Recursion Semantics (MRS; Copes-
take et al (2005)) for the representation of meaning.
LXGram is developed in the Linguistic Knowledge Builder (LKB) system (Copes-
take, 2002), a development environment for constraint-based grammars. This envi-
ronment provides a GUI, debugging tools and very efficient algorithms for parsing
and generation with the grammars developed there (Malouf et al, 2000; Carroll et al,
1999).
Several broad-coverage grammars have been developed in the LKB. Currently, the
largest ones are for English (Copestake and Flickinger, 2000), German (M?ller and
Kasper, 2000) and Japanese (Siegel and Bender, 2002). The grammars developed
with the LKB are also supported by the PET parser (Callmeier, 2000), which allows
for faster parsing times due to the fact that the grammars are compiled into a binary
format in a first step. As the LKB grammars for other languages, LXGram is in active
development, and it is intended to be a broad-coverage, open-domain grammar for
Portuguese. At the same time, it produces detailed representations of meaning in
tandem with syntactic structures, making it useful for a wide range of applications.
In Section 2, we describe the framework foundations of the grammar. The major
design features of the grammar are introduced in Section 3. We talk about the coverage
of LXGram in Section 4. Section 5 presents some of the phenomena treated within
the NP domain and shows examples of implemented analyses relating to NP syntax
and semantics. In Section 6, results on the performance of the grammar are reported,
and in Section 7, we discuss applications where the grammar is or is being integrated.
Finally, the paper closes with concluding remarks in Section 8.
2 Foundations
LXGram adopts the HPSG framework, a popular linguistic theory with a large body of
literature covering many natural language phenomena. These insights can be directly
incorporated in the implementation of a computational grammar.
2.1 HPSG
HPSG resorts to a declarative formalism to model linguistic data. It employs a type
system (supporting multiple inheritance) and typed feature structures (recursive data
structures defining ?has-a? relations) in order to describe the properties of linguistic
objects (words, phrases, rules). Unification of types and feature structures is central
to HPSG, used to ensure that the various elements have compatible properties. For
instance, the fact that a transitive verb takes an NP as its complement is captured in
HPSG by defining a lexical type for transitive verbs, say transitive-verb-lex(eme), with
constraints like the following (among others), presented in the Attribute-Value Matrix
(AVM) format widely employed in HPSG:
High Precision Analysis of NPs with a Deep Processing Grammar 33
?
?
?
?
?
?
?
?
transitive-verb-lex
SYNSEM|LOCAL|CAT|VAL|COMPS
?
?
?
?
?
LOCAL|CAT
?
?
?
?
HEAD noun
VAL
[
SPR ??
COMPS ??
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
The NP complement of the verb is represented in this AVM as the value of the at-
tribute COMPS. This attribute takes a list as its value (indicated by the angle brackets).
In this case the sole element of this list describes an object with a HEAD feature of the
type noun and empty complements (the attribute COMPS) and specifier (the feature
SPR) (i.e. they have been saturated at the point where the verb combines with this
element), which is the HPSG description of an NP.
2.2 MRS
Minimal Recursion Semantics (MRS) is used as the format of semantic representa-
tions that LXGram associates with expressions from Portuguese. MRS has several
properties that are interesting for applications. A relevant one is the use of pointers to
represent scope effects that are handled via recursion in traditional formal semantics.
This use of pointers (called handles) allows for the underspecification of scope rela-
tions, which avoids listing all scope possibilities for ambiguous sentences (although
they can still be computed on demand with the LKBmachinery). This is a useful prop-
erty: scope does not need to be resolved in all applications (e.g. machine translation
does not require it), but at the same time scoped formulas can be obtained on demand
if required (e.g. for automated inference).
We provide an example MRS representation derived for the sentence ?todas as
equipas podem vencer? (all teams can win) in Figure 1. This MRS describes the
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
mrs
LTOP h1 h
INDEX e2 e
RELS
?
?
?
?
?
?
?
?
_todo_q_rel
LBL h3 h
ARG0 x6 x
RSTR h5 h
BODY h4 h
?
?
?
?
?
?
?
,
?
?
?
_equipa_n_rel
LBL h7 h
ARG0 x6
?
?
?
,
?
?
?
?
?
_poder_v_rel
LBL h8
ARG0 e2
ARG1 h9 h
?
?
?
?
?
,
?
?
?
?
?
_vencer_v_rel
LBL h10
ARG0 e11
ARG1 x6
?
?
?
?
?
?
HCONS
?
?
?
?
qeq
HARG h1
LARG h8
?
?
?
,
?
?
?
qeq
HARG h5
LARG h7
?
?
?
,
?
?
?
qeq
HARG h9
LARG h10
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: MRS for the sentence ?Todas as equipas podem vencer? (all teams can win)
two following scoped formulas, where the predicate _todo_q stands for a universal
quantifier:
? _todo_q(x6,_equipa_n(x6),_poder_v(e2,_vencer_v(e11,x6)))
? _poder_v(e2,_todo_q(x6,_equipa_n(x6),_vencer_v(e11,x6)))
34 Branco and Costa
The first reading is the one that says that each team has a chance to win, while the
second reading says that it is possible for there to be a situation in which all teams win
(false assuming common sense knowledge). A single MRS representation is obtained
for these two readings by instantiating the ARG1 feature of the relation _poder_v (can)
with the handle h9, which is related to the handle h10 labeling the relation _vencer_v
(win) via a qeq relation (equality modulo intervening quantifiers). This is the way of
saying that these two handles are the same (first reading) or that there is an intervening
generalized quantifier relation (second reading).
Semantic representations abstract from many grammatical and superficial details
of language, like word order, syntactic structure and morphology. As such, they are
very similar across different natural languages (modulo predicate names). This is also
true of MRS. Furthermore, semantic representations hide grammar implementation.
As such, they are the preferred grammar?s interface for applications, that do not need
any knowledge of the grammatical properties of Portuguese and may not need to look
at syntactic analysis.
The MRS format is also used with several other computational HPSGs, for other
languages. Several applications (e.g. Machine Translation) have been used with other
HPSGs that communicate with these grammars via the MRSs (Bond et al, 2004).
These applications can be easily integrated with grammars for different languages
that also use MRS: they are almost completely language independent.
3 Design Features
Given the foundational options, LXGram adheres to a number of important design
features.
Bidirectionality LXGram is bidirectional. The formalism employed is completely
declarative. It can be used for parsing (yielding syntactic analyses and semantic rep-
resentations from natural language input) and also for generation (yielding natural
language from meaning representations). As such it can be useful for a wide range of
applications.
Precision LXGram aims at high precision of linguistic processing. Modulo bugs,
the grammar cannot parse ungrammatical input. Although this feature may have a
negative impact on robustness, it is an important aspect of the grammar when it is
used for generation, as it means it is not possible to generate ungrammatical strings.1
It is indeed possible to mark some rules and some lexical entries to only be used for
parsing and not for generation. In the configuration files for the LKB one can list these
rules and lexical items. We are currently using this feature in order to be able to parse
input that is not ungrammatical but is marked with respect to register, but preventing
the grammar from generating such strings.
Importantly, the fact that it cannot parse ungrammatical input also means that the
grammar will not produce impossible analyses for grammatical sentences.
Broad Coverage LXGram development is aimed at a broad coverage. We also
seek to make LXGram neutral with respect to regional variation as much as possi-
ble. Currently, the grammar accommodates both European Portuguese and Brazilian
1We believe that dealing with ill-formed input is best done via other means (rather than let the grammar
overgenerate so it can parse more), like partial parsing or the integration with/falling back to other tools.
High Precision Analysis of NPs with a Deep Processing Grammar 35
Portuguese. Aspects of variation that are accounted for include lexical differences
(merely affecting spelling or more substantial ones) as well as syntactic discrepancies
(e.g. definite articles before possessives, word order between clitic pronouns and the
verb).
Efficiency The processors on which LXGram runs (LKB, PET) are very efficient.
In addition, there are grammar engineering techniques that improve efficiency (e.g.
(Flickinger, 2000)) that are also exploited in our implementation.
Robustness The LKB and PET systems provide several ways to combine a grammar
with the output of shallow tools, like part-of-speech taggers. Such integration can
improve grammar coverage, as the grammar needs information about all words in the
input, and some words may be missing in the grammar?s lexicon. We have success-
fully combined LXGram with a part-of-speech tagger and a morphological analyzer
(more in Section 6). The grammar code includes mappings from the input format
(XML) to the feature structures that are manipulated by the grammar.
Availability A version of LXGram is publicly available at http://nlxgroup.di.
fc.ul.pt/lxgram. LXGram can be used by applications without any knowledge of
the grammar?s implementation or internal workings. The LKB allows for applications
to communicate with the grammar via sockets, accepting parser input in XML or raw
text and returning semantic representations in XML, for which a DTD is available. It
is also possible to automatically produce a list of all the predicates known by the gram-
mar together with their arity and argument types (from the lexicon and syntax rules),
that can be manually annotated with comments and examples. The predicates corre-
sponding to lexical items are however quite transparent once the naming conventions
that are used are explained.
4 Coverage
4.1 Lexical Coverage
When one is using a lexicalist framework like HPSG, lexical coverage is a key issue
because all tokens in the input should be known by the grammar in order for the
grammar to produce a parse. Furthermore, the amount of information included in
the lexicon that is used by an HPSG is very large. Part-of-speech and morphological
information is not sufficient. For the correct assignment of semantic representations,
subcategorization frames as well as other information pertaining to semantics must
be correctly associated with every lexical item, something that cannot be known with
sufficient quality by just using shallower tools, like part-of-speech taggers.
In LXGram a hand-crafted lexicon containing several hundreds of nouns, adjectives
and verbs was developed. However, the manual creation of lexica with this amount
of information is time consuming and error prone. We are exploring methods to al-
leviate this problem. An option is to combine the grammar with shallower tools in
order to have access to some of the information needed and assume default values
for the information that cannot be obtained this way. We have already integrated
the grammar with a set of shallow tools (a part-of-speech tagger, a lemmatizer and a
morphological analyzer) in order to guess information about unknown words. Prelim-
inary results indicate an increase in coverage on unrestricted newspaper text from 2%
to 13%. Although this approach cannot guarantee correct semantic representations
36 Branco and Costa
(or even syntactic trees, since subcategorization frames constrain syntactic structure),
it can be useful in applications that only require some restricted amount of linguistic
information.
4.2 Overall Grammatical Coverage
In order to get a quantitative overview of the grammar, it can be characterized as
follows:
? 24,484 lines of code, including comments and excluding the lexicon;
? 53 syntax rules;
? 40 lexical rules, mostly inflectional;
? 3,154 total types;
? 414 types for lexical items;
? 2,718 hand-built lexical entries.
For a qualitative overview, these are the linguistic phenomena covered so far:
? Declarative sentences
? Yes-no questions e.g.: ?Portanto o Estado tem um gosto?? (So does the State
have preferences?)
? Imperative sentences e.g.: ?D?-me um desses bolos.? (Give me one of those
cakes)
? Some subcategorization frames of verbs, nouns and adjectives e.g.: ?a Pol?-
nia empatou com a Fran?a? (Poland tied with France); ?eu j? disse que pode ser
um dos mais baratos? (I told you already that it can be one of the cheapest);
?filho de um professor dos arredores de Viena? (son of a teacher from the out-
skirts of Vienna)
? Comparative constructions e.g.: ?a vida ? maior do que o cinema? (life is
larger than cinema)
? Noun phrase structure, including determiners, possessives, cardinal and
ordinal numerals, prepositional phrases, adjectives, etc. (examples in the
next section)
? Modification of verbal projections by prepositional and adverbial phrases
e.g.: ?No CAPC termina hoje a exposi??o? (the exhibit ends today at CAPC);
? Relative clauses e.g.: ?sete outros suspeitos que a pol?cia ainda procura? (seven
other suspects that the police are still looking for)
? Null subjects and objects e.g.: ?Sa?mos depois do jantar.? ((We) left after
dinner); ?Podemos comer l? perto.? (We can eat near there)
High Precision Analysis of NPs with a Deep Processing Grammar 37
? Floated Quantifiers e.g.: ?os ?ndices subiram todos? (the indices have all gone
up)
The development of the grammar is going on and this grammar is getting its cover-
age increased with important phenomena that are missing. In particular, for the near
future, we are working towards including more subcategorization frames for verbs,
nouns and adjectives, and implementing wh-questions, coordination and adverbial
subordination.
5 Noun Phrases
A special design feature of LXGram is that it includes a comprehensive implementa-
tion of Portuguese Noun Phrase structure, covering:
? Bare noun phrases (i.e. NPs lacking a determiner) e.g.: ?boa gest?o? (good
management); ?imagens da bancada? (images of the seats)
? Determiners and predeterminers e.g.: ?esta sua ?ltima produ??o? (this last
production of his); ?todos estes problemas? (all these problems); ?todos os par-
tidos pol?ticos? (all the political parties), ?aquele tempo todo? (all that time)
? Word order constraints among NP elements e.g.: ?as duas primeiras insti-
tui??es? (the two first institutions); ?os primeiros sete meses deste ano? (the
first seven months of this year); ?sete outros suspeitos que a pol?cia ainda
procura? (seven other suspects that the police are still looking for); ?os outros
tr?s membros do conselho? (the other three members of the council); ?os seus
dois primeiros anos pol?micos na Casa Branca? (his first two polemic years
in the White House); ?o primeiro grande conflito que aportava em Bel?m? (the
first great conflict that reached Berlin); ?outro lugar qualquer? (any other place);
?um lugar qualquer? (any place); ?qualquer outra solu??o? (any other solution)
? Prenominal and postnominal possessives e.g.: ?o seu terceiro maior parceiro
comercial? (its third major commercial partner); ?um adjunto seu que atendeu
ali o telefonema? (an assessor of his who answered the phone call there)
? Modification of adjectives e.g.: ?os escritores mais importantes? (the most
important writers); ?o discurso razoavelmente optimista? (the reasonably opti-
mistic speech)
? Missing nouns e.g.: ?dois que s?o g?meos? (two who are twins)
? Word order between adjectives and complements of nouns e.g.: ?o conhec-
imento essencial das pessoas? (the essential knowledge about people)
? Adjectives with the semantics of arguments of nouns e.g.: ?o veto americano
? renova??o do mandato? (the American veto to the renewal of the position)
Precision was given a lot of attention. For instance, many items are constrained not
to appear more than once in a given NP (determiners, possessives, cardinals, ordinals,
etc.). Scope phenomena are also handled (motivated by semantics), as well as order
constraints. Agreement is enforced.
38 Branco and Costa
We present some examples of phenomena for which LXGram provides interesting
semantic representations and that we have not found in the literature pertaining to
implemented grammars.
5.1 Floated Quantifiers
The first example relates to floated quantifiers. For all the sentences in (1), which are
all grammatical in Portuguese, LXGram provides the MRS equivalent of
all(x, price(x),will(go_up(x))):
(1) a. Todos
all
os
the
pre?os
prices
v?o
will
subir.
go up
b. Os
the
pre?os
prices
todos
all
v?o
will
subir.
go up
c. Os
the
pre?os
prices
v?o
will
todos
all
subir.
go up
d. Os
the
pre?os
prices
v?o
will
subir
go up
todos.
all
In all of these cases we associate empty semantics to the definite article (?os?).
Semantic information is percolated around the syntactic trees so that the universal
quantifier, which can be realized at several different places, ends up being linked to
the semantics of the NP subject in the semantic representations for all these sentences.
We also make sure that definite articles always carry quantifier semantics when no
floated quantifier is present.
The implementation revolves around allowing floated quantifiers to attach to verbs,
resulting in verb-headed nodes that combine with NP subjects lacking quantifier se-
mantics. Raising verbs, like the form ?v?o? in this example, constrain their subject
according to the constraints of the subject of their VP complement. For instance, the
last example (1d) receives a syntactic analysis like the one described by the following
tree:
[
subj-head-phrase
SUBJ ??
]



HH
HH
HH
1
 HH
Os
the
pre?os
prices
?
?
head-comp-phrase
SUBJ
?
1
?
?
?



HH
HH
H
[
SUBJ
?
1
?
]
v?o
will
?
?
floated-quant-phrase
SUBJ
?
1 NP?e,t?
?
?
?


HH
H
[
SUBJ
?
NP
?
]
subir
go up
todos
all
High Precision Analysis of NPs with a Deep Processing Grammar 39
Here, NP abbreviates a feature structure that describes a noun phrase (of the se-
mantic type ??e,t?,t?), and NP?e,t? abbreviates the constraints that describe an NP
introduced by a determiner lacking quantifier semantics (i.e. an NP with an MRS
representation that is similar to that of constituents with the the semantic type ?e,t?).
In HPSG, the SUBJ feature encodes the constraints on the subject that a constituent
selects. We use a dedicated syntax rule to combine ?subir? and ?todos? (floated-quant-
phrase), that creates a node requiring an NP with the semantic type ?e,t? as its subject.
The verb form ?v?o? is treated as a raising verb: in HPSG the syntactic requirements
on the subject of a raising verb are the same as the requirements on the subject of the
VP complement that that verb selects for. This is denoted by the boxed integers in this
tree (which represent unification).
In this example, the VP complement of ?v?o? is the phrase ?subir todos?, as these
two constituents are combined via the head-comp-phrase rule (selection of comple-
ments is represented in a way similar to the selection of subjects, but via the feature
COMPS instead of the feature SUBJ). The subject of the head-comp-phrase is the sub-
ject of its head daughter (?v?o?). The topmost node is the result of applying a syntactic
rule to project subjects to the left of their head (subj-head-phrase).
The example in (1c) is processed in a similar fashion:
[
subj-head-phrase
SUBJ ??
]




HH
HH
HH
H
2
 HH
Os
the
pre?os
prices
?
?
head-comp-phrase
SUBJ
?
2
?
?
?



HH
HH
H
?
?
floated-quant-phrase
SUBJ
?
2 NP?e,t?
?
?
?


HH
H
[
SUBJ
?
1
?
]
v?o
will
todos
all
[
SUBJ
?
1 NP
?
]
subir
go up
In this example, the complement of ?v?o? is the node spanning ?subir?, which
selects for a quantified subject. The subject of the raising verb is accordingly also
a quantified NP. Here, the rule to project a floated quantifier applies lower than the
construction that projects complements, creating a node that requires a non-quantified
subject. The SUBJ feature of head-complement constructions comes from the head
daughter (the floated-quant-phrase node in this example). Therefore, the node pro-
duced by the head-comp-phrase rule also requires a non-quantified subject.
Note that the composition of semantics with MRS in based on the concatenation of
the RELS and HCONS lists associated to the various constituents and passing around
40 Branco and Costa
the values of the features LTOP and INDEX (see Figure 1). It is not based on function
application. The composition of semantics with MRS is quite flexible.
5.2 Scope of Adjectives and Relative Clauses
The second example that we show here relates to the semantic scope between different
elements of noun phrases. In particular, we can see a distinction in the interpretation
of the two following examples:
(2) a. um
a
poss?vel
possible
m?dico
doctor
chin?s
Chinese
a possible Chinese doctor
b. um
a
poss?vel
possible
m?dico
doctor
que
who
?
is
chin?s
Chinese
a possible doctor who is Chinese
In the first NP an entity is described as possibly being a Chinese doctor. The second
NP describes an entity as possibly being a doctor and certainly being Chinese. Ac-
cordingly, LXGram delivers slightly different semantic representations for these two
NPs. The first case produces something similar to
?P. a(x, possible(doctor(x)? chinese(x)),P(x)).
The second NP is treated along the lines of
?P. a(x, possible(doctor(x))? chinese(x),P(x)).
These two different readings are derived simply by constraining the relative syntactic
scope of adjectives and relative clauses. Namely, LXGram forces prenominal adjec-
tives to attach higher than postnominal adjectives (2a) but lower than relative clauses
(2b). In this case, the scope differences in the semantic representations are simply
derived from the differences in syntactic scope:


HH
H
um


HH
H
poss?vel
 HH
m?dico chin?s



HH
HH
H
um


HH
HH
 HH
poss?vel m?dico
que ? chin?s
Of course, the following examples receive equivalent semantics:
(3) a. um
a
m?dico
doctor
chin?s
chinese
a Chinese doctor
b. um
a
m?dico
doctor
que
who
?
is
chin?s
Chinese
a doctor who is Chinese
High Precision Analysis of NPs with a Deep Processing Grammar 41
6 Evaluation
Some evaluation experiments were conducted to test LXGram?s coverage. In one of
them, a corpus with newspaper text (self-reference) was used, with 145 sentences.
For this experiment, we used a part-of-speech tagger and a morphological analyzer
(self-reference) in order to guess some information about out-of-vocabulary words. A
default value was assumed for the missing subcategorization information (all unknown
verbs were treated as transitive verbs). The average sentence length was 22 words. In
this experiment, 13.1% of all sentences received at least one parse by the grammar.2
On the same test corpus, the average time it took for a sentence to parse was 1.1
seconds on a P4 machine at 3GHz. The average amount of memory required to analyze
a sentence was 145.5MB.
In another experiment, with 180,000 short sentences (5 to 9 words) selected ran-
domly from two newspaper corpora (CETEMP?blico and CETENFolha), LXGram
had achieved 26% coverage, using a similar approach to handle unknown words (self-
reference).
During the development of LXGram we maintain several test suites, consisting of
example sentences for the implemented phenomena. The test suites use a controlled
vocabulary. Also, several examples attest several phenomena, in order to test the
interaction of the different modules. They are very useful to test the syntax rules of
the grammar and the semantics that LXGram produces, and for regression testing. The
test suite for NPs contains 851 sentences (429 of which are negative examples, that
the grammar should not parse). The average sentence length is 5.3 words (2?16). On
this test suite LXGram has 100% coverage and 0% overgeneration. The average time
needed to analyze a sentence is 0.11 seconds, with an average memory requirement
of 15.5MB. Plotting parse time by sentence length, we see an approximately linear
increase in parse time with this test suite.
7 Applications and Further Work
We have used LXGram to automatically discriminate between texts written in Euro-
pean Portuguese and Brazilian Portuguese, with encouraging results, which match the
results obtained with other dialect detection methodologies. (self-reference).3
Additionally, we are working towards integrating it with an existing question an-
swering system (self-reference).4 This is in part the reason for the special focus on
NPs, as these constituents are often short answers to factoid questions.
Because the grammar is entirely bidirectional, a paraphraser is gained for free from
the implementation of LXGram: the grammar can simply be used to generate from the
semantic representations that it derives from an input sentence, thus producing para-
phrases of the textual input. We are also working to integrate the grammar, running
under this functionality, into the QA system.
2Note that one of the HPSGs with the broadest coverage at the moment, the ERG, covers 17% of the
British National Corpus. The main cause of parse failure is out-of-vocabulary words.
3In particular, the results obtained with LXGram were quite similar to the results obtained with the
standard methods (based on character n-grams) that are used to identify the language in which a given text
is written, when used for this purpose.
4See (Bobrow et al, 2007) for a similar approach, where an LFG is employed in a question answering
system aiming at high precision.
42 Branco and Costa
On a par with the above lines of research, we are intensively using the grammar to
semi-automatically produce a treebank that contains syntactic representations and se-
mantic descriptions of the sentences in a newspaper corpus. LXGram has also served
in the past to implement and experiment with novel linguistic analyses of interesting
phenomena (self-reference). By making it freely available, we intend to encourage
this sort of experimentation also by other researchers. One can reap important bene-
fits from computationally implementing linguistic analyses: the debugging tools allow
for fast checking of correctness; the impact on other analyses that are already imple-
mented can be immediately assessed via regression testing, making it possible to test
the interaction between linguistic analyses for different phenomena; it is possible to
automatically compare different competing analyses for efficiency, based on test suites
or corpora.
8 Conclusions
In this paper we presented LXGram, a computational grammar for the deep linguistic
processing of Portuguese. LXGram is implemented in a declarative formalism. It
can be used for analysis as well as generation. It produces high precision syntactic
analyses and semantic representations. LXGram supports the two main varieties of
Portuguese: European and Brazilian Portuguese. It is not dependent on a particular
domain or genre.
So far the focus of the implementation was on noun phrases and basic sentence
structure, a coverage that is being extended in ongoingwork. The outcome of different
evaluation experiments shows scores that are in line with those obtained with similar
grammars for other languages.
References
Bobrow, D. G., B. Cheslow, C. Condoravdi, L. Karttunen, T. H. King, R. Nairn,
V. de Paiva, C. Price, and A. Zaenen (2007). PARC?s bridge and question answer-
ing system. In T. H. King and E. M. Bender (Eds.), Proceedings of the GEAF07
Workshop, Stanford, CA, pp. 46?66. CSLI Publications.
Bond, F., S. Fujita, C. Hashimoto, K. Kasahara, S. Nariyama, E. Nichols, A. Ohtani,
T. Tanaka, and S. Amano (2004). The Hinoki treebank: Working toward text un-
derstanding. In S. Hansen-Schirra, S. Oepen, and H. Uszkoreit (Eds.), COLING
2004 5th International Workshop on Linguistically Interpreted Corpora, Geneva,
Switzerland, pp. 7?10. COLING.
Callmeier, U. (2000). PET ? A platform for experimentation with efficient HPSG
processing techniques. Natural Language Engineering 6(1), 99?108. (Special
Issue on Efficient Processing with HPSG).
Carroll, J., A. Copestake, D. Flickinger, and V. Poznan?ski (1999). An efficient chart
generator for (semi-)lexicalist grammars. In Proceedings of the 7th European
Workshop on Natural Language Generation (EWNLG?99), Toulouse, pp. 86?95.
Copestake, A. (2002). Implementing Typed Feature Structure Grammars. Stanford:
CSLI Publications.
High Precision Analysis of NPs with a Deep Processing Grammar 43
Copestake, A. and D. Flickinger (2000). An open-source grammar development envi-
ronment and broad-coverage English grammar using HPSG. In Proceedings of the
Second conference on Language Resources and Evaluation (LREC-2000), Athens,
Greece.
Copestake, A., D. Flickinger, I. A. Sag, and C. Pollard (2005). Minimal Recursion
Semantics: An introduction. Journal of Research on Language and Computa-
tion 3(2?3), 281?332.
Flickinger, D. (2000). On building a more efficient grammar by exploiting types.
Natural Language Engineering 6(1), 15?28. (Special Issue on Efficient Processing
with HPSG).
Malouf, R., J. Carrol, and A. Copestake (2000). Efficient feature structure operations
without compilation. Natural Language Engineering 6(1), 29?46. (Special Issue
on Efficient Processing with HPSG).
M?ller, S. and W. Kasper (2000). HPSG analysis of German. In W. Wahlster (Ed.),
Verbmobil: Foundations of Speech-to-Speech Translation (Artificial Intelligence
ed.)., pp. 238?253. Berlin Heidelberg New York: Springer-Verlag.
Pollard, C. and I. Sag (1994). Head-Driven Phrase Structure Grammar. Stanford:
Chicago University Press and CSLI Publications.
Siegel, M. and E. M. Bender (2002). Efficient deep processing of Japanese. In Pro-
ceedings of the 3rd Workshop on Asian Language Resources and International
Standardization. Coling 2002 Post-Conference Workshop, Taipei, Taiwan, pp. 31?
38.
LXGram in the Shared Task
?Comparing Semantic
Representations?
of STEP 2008
Ant?nio Branco
Francisco Costa
Universidade de Lisboa (Portugal)
email: Antonio.Branco@di.fc.ul.pt
Abstract
LXGram is a hand-built Portuguese computational grammar based on
HPSG (syntax) and MRS (semantics). The LXGram system participated
in the STEP 2008 shared task which aims at comparing semantic repre-
sentations produced by NLP systems such as LXGram. Every partici-
pating team had to contribute a small text. The text that we submitted
for the shared task was originally in Portuguese (an excerpt from a news-
paper) and translated into English, to make a meaningful comparison at
the shared task possible. Likewise, the English texts contributed by the
other participating teams were translated into Portuguese. Because the
LXGram generates many different analyses (mainly due to PP attach-
ment ambiguities), the preferred analysis was selected manually. It was
required to extend LXGram?s lexicon and inventory of syntax rules to be
able to get a reasonable performance on the shared task data. Eventually,
our system was able to produce an analysis for 20 out of the 30 sentences
of the shared task data.
299
300 Branco and Costa
1 Introduction
This paper describes the participation of the Portuguese grammar LXGram in the
Shared Task of STEP 2008 ?Comparing Semantic Representations? (Bos, 2008). This
Shared Task was held in the University of Venice on 22?24 September 2008, with the
purpose of comparing semantic representations produced by different natural language
processing systems. This task had seven participating teams. Each team contributed
with a small text (up to five sentences long) to be processed by all the systems.
LXGram is a hand-built, general purpose computational grammar for the deep lin-
guistic processing of Portuguese. It is developed under the grammatical framework
of Head-Driven Phrase Structure Grammar, HPSG (Pollard and Sag, 1987, 1994; Sag
et al, 2003) and uses Minimal Recursion Semantics, MRS (Copestake et al, 2005) for
the representation of meaning. This grammar implementation is undertaken with the
LKB (Copestake, 2002) grammar development environment and its evaluation and
regression testing is done via [incr tsdb()] (Oepen, 2001). It is also intended to be
compatible with the PET parser (Callmeier, 2000).
The LinGO Grammar Matrix (version 0.9), an open-source kit for the rapid devel-
opment of grammars based on HPSG and MRS, was used as the initial code upon
which to build LXGram. The grammar is implemented in the LKB using the T DL
formalism (Krieger and Sch?fer, 1994), based on unification and on typed feature
structures, and whose types are organized in a multiple inheritance hierarchy.
For more information, please refer to a detailed implementation report (Branco and
Costa, 2008a) or on pages 31?43 of this volume (Branco and Costa, 2008b). A free
version of the grammar can also be obtained at http://nlx.di.fc.ul.pt/lxgram,
under an ELDA research license.
Section 2 introduces the main features of the Minimal Recursion Semantics for-
mat, which is employed in the semantic representations produced by LXGram. In
Section 3, the sample text that the LXGram team submitted is described, together
with an explanation of the representations derived by the grammar. Finally, Section 4
discusses the results for the full data set of the Shared Task.
2 Semantic Formalism
In LXGram, semantic information is encoded following Minimal Recursion Seman-
tics (MRS) format for semantic representation (Copestake et al, 2005). MRS has
several properties that makes it an interesting semantic representation format from the
point of view of computational semantics.
Notoriously, it allows underspecification of the scope of relevant operators, which
permits that a sentence with scope ambiguities can be given a single, underspeci-
fied representation. For some applications, for instance machine translation between
closely related languages from the same language family, the underspecified repre-
sentations may be sufficient and bring the benefit of avoiding possible combinatorial
explosion into as many parses as readings.
In a nutshell, the underspecification of scope is achieved by associating every basic
relation to a handle (in the feature structure for a relation, the feature LBL encodes
this handle) and describing the constraints that hold between these handles (in the
feature HCONS, handle constraints). These constraints can be stated in a way such that
LXGram in the Shared Task ?Comparing Semantic Representations? 301
some scope resolution options are allowed while others are discarded. Nevertheless,
there may applications for which it may be important to get fully specified semantic
representations. In this case, MRS permits that the different scope possibilities be
computed on demand from the underspecified representation.
Also worth referring in this very brief presentation of the gist of MRS, it is the
representation of conjunction with the relative order of conjuncts underspecified, by
giving the same handle to the different conjuncts. This avoids computing associativity
and commutativity of conjunction in situations where spurious overgeneration may
arise.
Please consult Branco and Costa (2008a) in this volume (pages 31?43) for an ex-
ample illustrating quantifier scope ambiguities and underspecification. Due to space
limitations, it is not possible to provide further details on the MRS formalism here.
For the presentation of MRS, please consult Copestake et al (2005).
3 Sample Text
The following sentences are our examples for the shared task:
(1) A
the
primeira
first
escola
school
de
of
treino
training
de
of
c?es-guias
leader dogs
do
of the
Pa?s
country
vai
goes
nascer
to be born
em
in
Mort?gua
Mort?gua
e
and
treinar?
will train
22
22
c?es-guias
leader dogs
por
per
ano.
year
The first school for the training of leader dogs in the country is going to be
created in Mort?gua and will train 22 leader dogs per year.
(2) Em
in
Mort?gua,
Mort?gua
Jo?o
Jo?o
Pedro
Pedro
Fonseca
Fonseca
e
and
Marta
Marta
Gomes
Gomes
coordenam
coordinate
o
the
projecto
project
que
that
sete
seven
pessoas
people
desenvolvem
develop
nesta
in this
escola.
school
In Mort?gua, Jo?o Pedro Fonseca and Marta Gomes coordinate the project
that seven people develop in this school.
(3) Visitaram
they visited
v?rios
several
espa?os
spaces
semelhantes
similar
em
in
Inglaterra
England
e
and
em
in
Fran?a,
France,
e
and
numa
in one
das
of the
escolas
schools
francesas
French
est?o
are
j?
already
em
in
est?gio
internship
duas
two
futuras
future
treinadoras.
trainers
They visited several similar places in England and in France, and two future
trainers are already doing internship in one of the French schools.
(4) Os
the
fundos
funding
comunit?rios
communitarian
asseguram
ensure
a
the
manuten??o
maintenance
da
of the
escola
school
at?
until
1999.
1999
The communitarian funding ensures the operation of the school until 1999.
302 Branco and Costa
(5) Gostar?amos
we would like
que
that
a
the
nossa
our
escola
school
funcionasse
worked
?
to the
semelhan?a
similarity
das
of the
francesas,
French
que
which
vivem
live
de
from
d?divas,
donations
do
from the
merchandising
merchandising
e
and
at?
even
das
from the
rifas
raffles
que
that
as
the
crian?as
children
vendem
sell
nas
in the
escolas.
schools
We would like our school to work similarly to the French ones, which live
from donations, from the merchandising and even from the raffles that chil-
dren sell in school.
These sentences were adapted from newspaper text. We have chosen them because
they display interesting phenomena.
The semantic representations that LXGram produces for these sentences are pre-
sented at the Shared Task website http://www.sigsem.org. An example is included
in Appendix B. Several analyses are obtained for these examples (e.g. one of the sen-
tences got 540 parses), the main reason being PP attachment ambiguity. The semantic
representations we present are the ones associated to the preferred analyses, which
were selected manually.
Note that since the representations could not be displayed in a single page, the value
of the feature RELS was split across multiple pages. To ensure readability, the values
of the other features (LTOP, INDEX and HCONS) are repeated on every page pertaining
to the same representation.
Some comments are in order concerning these representations:
? The morphological person, number and gender are encoded as features (PER-
SON, NUMBER, GENDER) of the relevant index (quantified variable) that is
present there. For indices, the boolean feature DIV is also used, that shows
the value + for plurals and mass nouns.
? Event variables are included for the relations introduced by verbs, adjectives,
prepositions and adverbs (under their ARG0 feature). The morphological in-
formation on the verbs is also encoded as features of these events. This is the
purpose of the features MOOD, TENSE and ASPECT. There is also a feature
SF (sentence force) that represents whether a sentence denotes a proposition, a
question or a command. The feature ELLIPTICAL-PUNCT denotes whether the
sentence ends with an ellipsis (. . .) and is useful in order to constrain what is
generated by the grammar.
? There is a tense_rel relation associated to each verb form. Its ARG0 feature is
the same as the ARG0 of the verb it is associated with. The purpose of this extra
relation is to make an event variable present in the semantic representations for
the copular sentences where the relevant predicate is provided by a noun (none
of these examples). In such cases this event will contain the morphological
information of the copular verb.
? Note that the information about whether adjectives have intersective semantics
(see ?franc?s???French??in sentence (3)) or non-intersective semantics (see
LXGram in the Shared Task ?Comparing Semantic Representations? 303
?futuro???future??in sentence (5)) is visible in the corresponding semantic
representations.
The names of the predicates that correspond to lexical items of several classes
(common nouns, verbs, adjectives, adverbs, prepositions, etc.) follow a naming con-
vention that includes a lemma field, a part-of-speech field and an optional sense field
(often reflecting subcategorization). Table 1 lists the predicates present in these rep-
resentations and provides the corresponding English lemmas. There are other special
relations in these representations:
? udef_q_rel
the quantifier for bare NPs
? proper_q_rel
the quantifier for proper names
? tense_rel
associated to every verbal relation (see discussion above)
? named_rel
associated to proper names
? name-precedes_rel
associated to proper names
? string-equals_rel
equality between strings
? indef_q_rel
associated to some indefinites. In particular it is the quantifier used for NPs that
are introduced by elements that can also follow determiners (e.g. cardinals and
vague quantifiers like ?v?rios???several?)
? cardinal_rel
constrains the cardinality of the set denoted by the expression linked to its ARG1
feature
? greater-or-equal_rel
the integer in its ARG0 is greater than or equal to the integer in its ARG1 feature
? plus_rel
the integer in its ARG0 is the result of summing the two integers in the TERM0
and TERM1 features
? int-equals_rel
equality between integers
? ellipsis-or-generic_n_1_rel
placeholder relation when there are missing nouns
304 Branco and Costa
Table 1: Correspondence of Portuguese MRS relations and English lemmas
MRS Relation English lemma
_ano_n_rel year
_?_semelhan?a_a_-de-_rel similarly
_assegurar_v_rel to ensure
_at?_a_rel even
_at?_p_rel until
_c?o-guia_n_rel leader dog
_comunit?rio_a_rel communitarian
_coordenar_v_rel to coordinate
_crian?a_n_rel child
_d?diva_n_-de-a-_rel donation
_de_p_rel of, from
_desenvolver_v_rel to develop
_e_coord_rel and
_em_p_rel in
_espa?o_n_rel space
_est?gio_n_rel internship
_este_a_rel this
_escola_n_rel school
_franc?s_a_rel French
_funcionar_v_rel to work
_fundo_n_rel funding
_futuro_a_rel future
_gostar_v_rel to like
_ir_v_aux_rel to be going to
_j?_a_rel already
_manuten??o_n_-de-por-_rel maintenance
_merchandising_n_rel merchandising
_nascer_v_rel to be born
_o_q_rel the
_pa?s_n_rel country
_por_p_rel per
_pessoa_n_rel person
_primeiro_a_rel first
_projecto_n_-de-por_rel project
_rifa_n_rel raffle
_semelhante_a_-a-_rel similar
_treinador_n_-de-_rel trainer
_treinar_v_rel to train
_treino_n_-de-por-_rel training
_um_q_rel a
_v?rios_a_scop_rel several
_vender_v_-a-_rel to sell
_visitar_v_rel to visit
_viver_v_rel to live
LXGram in the Shared Task ?Comparing Semantic Representations? 305
Sometimes some details of the semantic representations that are possible to obtain
depend on the features of the system where LXGram is developed and runs. In partic-
ular, for each feature that represents an argument of a relation (ARG0, ARG1, ARG2,
CARG, . . .), it must be stated in the configuration files whether it will contain a constant
(e.g. a string literal). For instance, we must say that the feature CARG always contains
a value, for visualization purposes. This fact sometimes constrains the display of the
semantic representations. It is the reason why the semantics for proper names and for
cardinals is more copious than what would seem necessary at first.
For instance, the semantics associated to ?7 pessoas? (?7 people?) in sentence (2)
is roughly ?x.cardinal_rel(e,_pessoa_n_rel(x), j1)? greater-or-equal_rel( j1, j2)?
int-equals( j2,7) (note that conjunction is denoted in MRS via identical labels for
relations). The information conveyed by the last two predicates could be simply given
by greater-or-equal_rel( j1,7). However, for that to display correctly we would have
to configure the system to display the second argument of the greater-or-equal_rel
relation as a constant. This will not always be the case: in the semantics for ?22? that
argument is the integer that is the result of summing ?20? and ?2? (number expressions
receive compositional semantics), represented with the help of the plus_rel relation.
The LKB does not allow one to compute arithmetic expressions.
These few sentences present some interesting problems for the computation of se-
mantic representation in general.
Typically, one is not able to resolve missing nouns, as this sometimes requires
access to pragmatic information. As a consequence, the semantics produced for sen-
tences with a missing noun (see sentence (5)) includes an ellipsis-or-generic_n_1_rel
instead of the relation corresponding to that noun.
Also, it is very hard if not impossible to recover missing arguments. See for in-
stance the semantics for the adjective ?semelhante? (?similar?) in sentence (3). The
missing argument is given the type r, instead of the type x of quantified variables, so
that we can omit a quantifier for it in the semantics and still be able to ask the system
for scoped solutions (the system would complain about free variables if these elements
were given the type x).
Finally, it is worth noting that there are some limitations of the semantic represen-
tations obtained given that the empirical coverage of the grammar is still in develop-
ment. Currently, the grammar does not make yet any distinction between restrictive
and non-restrictive relative clauses, as we have not focused on the fully-fledged im-
plementation of the semantics of non-restrictive relative clauses yet. This can be seen
in the semantics for the last example, where both relative clauses are semantically
combined with their head in the same way.
4 Performance in the Shared Task
There are seven small texts in the Shared Task. The sample text we submitted is text
4. We translated the other six texts into Portuguese before passing them to the system.
Translation of the Texts
The translations were done by the authors. We tried to make them as literal as possi-
ble in order to support comparability of the different systems taking part in the Shared
Task, but some bits were not literally translated as that would have produced unnatural
306 Branco and Costa
sentences. We also tried not to make the texts easy to parse by the system by simpli-
fying the texts in the translations. We present the translation for the texts 1, 2, 3, 5, 6
and 7 in the Appendix A, with English glosses.
Initial Coverage
When we tried to parse the other six texts of the Shared Task, we got 0% coverage.
The causes for parse failure were missing words in the lexicon and missing syntactic
constructions.
Since the aim of the Shared Task is not to evaluate data coverage but rather to
compare the semantic representations output by different NLP systems, we made an
effort to expand LXGram by enlarging the lexicon and implementing some syntax
rules, with the purpose of producing semantic representations for as many sentences
in the Shared Task data as possible, within the time constraints.
During this grammar expansion, we tried not to tune the grammar to these particular
sentences. We tried to make the implementation of new phenomena general. For this
reason, some phenomena were not implemented deliberately, because we felt that we
would not be able to produce general solutions for them within the time limit. This is
the case of WH- questions (present in the first text), which are not yet supported by
LXGram and whose implementation we did not want to rush.
Grammar Expansion
We added 97 lexical entries to the grammar. For some of these items, we had to create
new lexical types, because they have subcategorization frames for which there was
still no lexical type in the grammar. One example is the noun ?pedido? (order), which
was implemented as having two arguments realized by prepositional phrases, the first
one headed ?de? and the second one headed by ?a?. LXGram already contained lex-
ical types for nouns with two arguments, but introduced by different prepositions.
Although these two arguments of the noun were not present in the example where
this noun occurs (the third sentence of text 3), we nevertheless created a new lexical
type for this subcategorization frame. We could have used an existing lexical type for
nouns with no complements and that particular sentence would have parsed fine, but
the predicate for that noun would not be a two-place predicate in the MRS represen-
tation. We added 10 new lexical types.
The constructions that were implemented in LXGram in order to parse these sen-
tences were:
? the progressive. In European Portuguese, the progressive is expressed via a
form of the verb ?estar? (to be) combined with an infinitive preceded by the
preposition ?a?.
? temporal expressions headed by the verb ?haver? (there to be). The temporal
expression for some time (second sentence of text 2) is expressed in Portuguese
as ?h? algum tempo? (literally: there is some time). The verb form cannot be
analyzed as a preposition, because this sort of expression is syntactically com-
positional. For instance, the verb inflects for tense (it can appear in the imper-
fect if the main verb of the clause is in a past tense) and there can be adverbs
modifying it to its right (?h? j? algum tempo?, there is already some time, i.e.
LXGram in the Shared Task ?Comparing Semantic Representations? 307
for some time now). We created a unary syntax rule that takes as daughter a
clause headed by this verb and produces a mother node with the syntactic char-
acteristics of a clause introduced by a subordinating conjunction and modifying
another clause. This rule adds a relation similar to a relation introduced by
a subordinating conjunction, and it?s called abstract-temporal_x_rel. We take
this relation as having the meaning of ?since?, but with the two arguments re-
versed, and the Portuguese clause for that is known for some time gets analyzed
as meaning roughly there is some time (some time has passed) since that is
known. That is a very literal semantic representation, but it allows us to keep
the semantic composition mechanism completely monotonic.
? the impersonal pronoun ?se?. The most naturally sounding translation of it was
suspected that (last sentence of text 5) is ?suspeitou-se que?, with a verb in the
active voice and its subject being realized by a clitic pronoun. This clitic has to
appear adjacently to the verb, which is atypical for subjects in Portuguese.
? NP appositives. We also implemented a rule to allow NP apposition. This was
because of sentences like the second sentence in text 6.
Additionally, a few preprocessor rules were expanded. For instance, sentences like
the last sentence of text 7 require integer literals to be considered as proper names. We
cannot create lexical entries for all integers, so we added preprocessor rules in order
to contemplate the possibility of integers as proper names.
Final Results
After grammar expansion, 20 sentences out of the 30 sentences in all the texts of the
Shared Task got an analysis. The sentences that could not be parsed are the following:
? Text 1: sentences (c) and (d).
? Text 5: sentences (a), (c) and (d)
? Text 6: all sentences
? Text 7: sentences (a) and (b)
The two sentences of text 1 that could not be parsed contain WH- questions, which
are currently not supported by the system.
The sentence (a) of text 5 could not be parsed because it contains two sentences as
the complement of a verb. LXGram cannot yet combine two independent sentences,
and we chose to not implement this possibility because the combination of an n-way
ambiguous sentence with another m-way ambiguous sentence would be n ? m-way
ambiguous.
The sentence (c) of the same text was not parsed because of a semantically vacuous
clitic (not implemented yet) and a relative clause modifying another clause (also not
covered). LXGram does not support sentence relatives and we chose not to imple-
ment them yet because, if the relative pronoun is filling a subject position (as in that
sentence), the verb has to allow for propositional subjects. In LXGram, we currently
only have subcategorization frames for verbs that take NPs as subjects, and we have
to review all lexical entries for verbs before we can parse that sentence.
308 Branco and Costa
For the remaining sentences without a parse, the reason was efficiency. Several of
the sentences in the Shared Task data translate to Portuguese sentences that are very
long (over 40 words) or have a very high number of prepositions, producing many
attachment possibilities. Note that we were doing exhaustive search. In many cases
the parser would run out of memory. In order to alleviate this problem, we used the
PET parser instead of the LKB parser for the longer sentences. PET is considerably
faster, because it is implemented in C (the LKB is in Lisp), and it precompiles the
grammar into a binary format. Also, the input to PET can be preprocessed by a POS
tagger, in order to reduce lexical ambiguity. We did this preprocessing for some of the
longer sentences.
However, PET dumps MRS representations as text, and choosing the best parse
from this sort of output is not practical, especially for sentences with many readings.
So we exported the results into a format that can be read by [ incr tsdb() ], a tool for
the management of test suites and corpora. With this tool, it is possible to choose
parses by choosing discriminants derived from all analyses. Choosing or rejecting a
single discriminant can eliminate a large number of analyses in one step. However,
[ incr tsdb() ] calls the LKB to reconstruct the trees based on the output of PET (which
includes the names of the rules used and syntactic constituency), when one wants to
choose the best parse. Even though the parse forest has already been built by the
PET parser, the LKB can still run out of memory when it is reconstructing the feature
structures if the number of analyses is sufficiently large (we had a sentence with over
18000 parses).
We also tried commenting out some rules that were not necessary to parse these
sentences, with the purpose of reducing the search space. Examples include robustness
rules, for parsing strings with no verb.
In the near future, we will be working on a stochastic disambiguation module,
which PET supports, in order to constrain the parser?s search space and to keep only
the best n parses, so that we can avoid the efficiency problems that we are facing at
the moment.
Analyses
The semantic representations for the sentences that LXGram parsed successfully are
presented in the appendix. As mentioned before, we performed exhaustive search. We
chose the best parse manually.
We used [ incr tsdb() ] associated to the LKB in order to choose the preferred
reading. After that we exported the MRS representation. The LKB exports LaTeX
directly. We edited the exported LaTeX in order to make the representations fit into
the pages of the appendix. This involved manually adding newlines and page breaks.
We also corrected characters with diacritics, which did not display correctly, and we
removed characterization information: after the name of each predicate, there is a
pair of character positions indicating the substring in the input spanned by the lexical
items or rules associated to that predicate; they were removed because they are not
interpretable by someone who does not know the implementation details, e.g. the
semantics for null subjects span the substring of the entire VP since this piece of
semantics is introduced by a unary rule that takes a VP as daughter.
LXGram in the Shared Task ?Comparing Semantic Representations? 309
Discussion of the Results
We would like to comment on some of the semantic representations obtained with
LXGram.
As we have pointed out before, some details of the semantics are not completely
independent of language. For an example, see the discussion above about temporal
expressions headed by the verb ?haver?.
MRS does not directly support a treatment of intentionality. For instance, sentence
(c) of text 2 contains an intentional context: it does not assert the existence of ?other
cancers caused by viruses?. There is no standard way of representing this sort of
intentionality with MRS.
Also, MRS does not support conjunction of quantifiers. There is no MRS equiva-
lent to a lambda expression like ?P.Quant1(x,P(x))?Quant2(y,P(y)). The usual MRS
representations associated with NP coordination have to include an explicit relation for
the truth function involved (but taking referential indices as arguments), as well as an
extra quantifier relation (the relation used in these cases is called ude f_q_rel, which
is also the name for the quantifier of bare NPs).
Some phenomena are difficult to analyze. An example is in sentence (c) of text 7.
In the Portuguese translation, we have two coordinated NPs at the end of the sentence
(the best sounding translation requires a determiner before each of the two nouns),
which are followed by a PP. The Portuguese translation interprets this PP as realizing
an argument of both nouns (cf. federal government interest and federal government
tax incentives). We could not get this reading, because we do not allow PP arguments
to attach higher than determiners. The analysis that we present leaves the first noun
with this argument underspecified, as this PP attaches directly to the second noun in
the corresponding syntax tree. This possibility of PP attachment seems to be required
for cases of NP coordination like this one, but it can be a source of overgeneration
for NPs that are not coordinated. This phenomenon affects other NP elements, like
adjective phrases, that can also take scope over a coordination of NPs. The current
implementation forces all noun dependents that have a restrictive interpretation to
attach lower than determiners, as that is the place where the restrictor of the quantifier
for that NP is visible in the feature structures.
References
Bos, J. (2008). Introduction to the Shared Task on Comparing Semantic Representa-
tions. In J. Bos and R. Delmonte (Eds.), Semantics in Text Processing. STEP 2008
Conference Proceedings, Volume 1 of Research in Computational Semantics, pp.
257?261. College Publications.
Branco, A. and F. Costa (2008a). A computational grammar for deep linguistic pro-
cessing of Portuguese: LXGram, version A.4.1. Technical report, University of
Lisbon, Department of Informatics.
Branco, A. and F. Costa (2008b). High Precision Analysis of NPs with a Deep Pro-
cessing Grammar. In J. Bos and R. Delmonte (Eds.), Semantics in Text Processing.
STEP 2008 Conference Proceedings, Volume 1 of Research in Computational Se-
mantics, pp. 31?43. College Publications.
310 Branco and Costa
Callmeier, U. (2000). PET ? A platform for experimentation with efficient HPSG
processing techniques. Natural Language Engineering 6(1), 99?108. (Special
Issue on Efficient Processing with HPSG).
Copestake, A. (2002). Implementing Typed Feature Structure Grammars. Stanford:
CSLI Publications.
Copestake, A., D. Flickinger, I. A. Sag, and C. Pollard (2005). Minimal Recursion
Semantics: An introduction. Journal of Research on Language and Computa-
tion 3(2?3), 281?332.
Krieger, H.-U. and U. Sch?fer (1994). T DL ? A type description language for
constraint-based grammars. In Proceedings of the 15th International Conference
on Computational Linguistics, Kyoto, Japan, pp. 893?899.
Oepen, S. (2001). [incr tsdb()] ? competence and performance laboratory. User
manual. Technical report, Computational Linguistics, Saarland University, Saar-
br?cken, Germany. In preparation.
Pollard, C. and I. Sag (1987). Information-Based Syntax and Semantics, Vol. 1. Num-
ber 13 in CSLI Lecture Notes. Stanford: CSLI Publications.
Pollard, C. and I. Sag (1994). Head-Driven Phrase Structure Grammar. Stanford:
Chicago University Press and CSLI Publications.
Sag, I. A., T. Wasow, and E. M. Bender (2003). Syntactic Theory ? A Formal Intro-
duction (2nd ed.). Stanford: CSLI Publications.
LXGram in the Shared Task ?Comparing Semantic Representations? 311
Appendix A: Translations of the Texts for the Shared Task
Text 1
(1) Um
an
objecto
object
?
is
lan?ado
thrown
com
with
uma
a
velocidade
speed
horizontal
horizontal
de
of
20
20
m/s
m/s
de
from
um
a
penhasco
cliff
que
that
tem
has
125
125
m
m
de
of
altura.
height
An object is thrown with a horizontal speed of 20 m/s from a cliff that is 125 m high.
(2) O
the
objecto
object
cai
falls
pela
for the
altura
height
do
of the
penhasco.
cliff
The object falls for the height of the cliff.
(3) Se
if
a
the
resist?ncia
resistance
do
of the
ar
air
?
is
negligenci?vel,
negligible
quanto
how much
tempo
time
demora
takes
o
the
objecto
object
a
to
cair
fall
ao
to the
ch?o?
ground
If air resistance is negligible, how long does it take the object to fall to the ground?
(4) Qual
what
?
is
a
the
dura??o
duration
da
of the
queda?
fall
What is the duration of the fall?
Text 2
(1) O
the
cancro
cancer
cervical
cervical
?
is
causado
caused
por
by
um
a
v?rus.
virus
Cervical cancer is caused by a virus.
(2) Isso
that
?
is
conhecido
known
h?
there is
algum
some
tempo
time
e
and
levou
led
a
to
uma
a
vacina
vaccine
que
that
parece
seems
preveni-lo.
to prevent it
That has been known for some time and it has led to a vaccine that seems to prevent it.
(3) Os
the
investigadores
researchers
t?m
have
procurado
looked
outros
other
cancros
cancers
que
that
possam
may
ser
be
causados
caused
por
by
v?rus.
viruses
Researchers have been looking for other cancers that may be caused by viruses.
Text 3
(1) O
the
John
John
foi
went
a
to
um
a
restaurante.
restaurant
John went into a restaurant.
(2) Havia
there was
uma
a
mesa
table
no
in the
canto.
corner
There was a table in the corner.
(3) O
the
empregado
waiter
anotou
wrote down
o
the
pedido.
order
The waiter took the order.
(4) A
the
atmosfera
atmosphere
era
was
acolhedora
warm
e
and
simp?tica.
friendly
The atmosphere was warm and friendly.
(5) Ele
he
come?ou
began
a
to
ler
read
o
the
seu
his
livro.
book
He began to read his book.
312 Branco and Costa
Text 5
(1) Enquanto
as
os
the
3
3
canh?es
guns
do
of the
torre?o
Turret
2
2
eram
were
carregados,
loaded
um
a
membro
member
da
of the
equipa
crew
que
who
estava
was
a
to
operar
operate
o
the
canh?c?o
gun
central
central
gritou
yelled
ao
to the
telefone
phone
?Tenho
I have
aqui
here
um
a
problema.
problem.
Ainda
Still
n?o
not
estou
I am
preparado?.
ready
As the 3 guns of Turret 2 were being loaded, a crewman who was operating the center gun yelled
into the phone, ?I have a problem here. I am not ready yet.?
(2) Ent?o
then
o
the
explosivo
propellant
rebentou.
exploded
Then the propellant exploded.
(3) Quando
when
os
the
membros
members
da
of the
equipa
crew
do
of the
canh?o
gun
morreram,
died
estavam
they were
agachados
crouching
de
of
forma
way
n?o
not
natural,
natural
o que
which
sugeria
suggested
que
that
sabiam
they knew
que
that
se
DUMMY CLITIC
daria
would happen
uma
an
explos?o.
explosion
When the gun crew was killed they were crouching unnaturally, which suggested that they knew that
an explosion would happen.
(4) O
the
explosivo
propellant
que
that
foi
was
usado
used
era
was
feito
made
de
from
peda?os
chunks
de
of
nitrocelulose
nitrocellulose
que
that
foram
were
produzidos
produced
durante
during
a
the
Segunda
second
Guerra
world
Mundial
war
e
and
foram
were
reembalados
repackaged
em
in
1987
1987
em
in
sacos
bags
que
that
foram
were
feitos
made
em
in
1945.
1945
The propellant that was used was made from nitrocellulose chunks that were produced during World
War II and were repackaged in 1987 in bags that were made in 1945.
(5) Inicialmente,
initially
suspeitou-se
suspected IMPERSONAL SUBJECT
que
that
este
this
armazenamento
storage
poderia
might
ter
have
reduzido
reduced
a
the
estabilidade
stability
da
of the
p?lvora.
powder
Initially it was suspected that this storage might have reduced the powder?s stability.
Text 6
(1) Entre
amid
as
the
filas
rows
cerradas
tightly packed
de
of
casas
houses
do
of the
norte
north
de
of
Filad?lfia,
Philadelphia
uma
a
quinta
farm
urbana
urban
pioneira
pioneering
est?
is
a
to
produzir
produce
comida
food
local
local
fresca
fresh
para
for
uma
a
comunidade
community
que
that
frequentemente
often
n?o
not
a
it
tem,
has
e
and
a
to
gerar
generate
dinheiro
money
com
with
isso.
it
Amid the tightly packed row houses of North Philadelphia, a pioneering urban farm is providing
fresh local food for a community that often lacks it, and making money in the process.
(2) Greensgrow,
Greensgrow
um
a
terreno
plot
de
of
um
one
acre
acre
de
of
canteiros
beds
elevados
raised
e
and
estufas
greenhouses
no
on the
local
site
de
of
uma
a
antiga
former
f?brica
factory
de
of
galvaniza??o
galvanization
de
of
a?o,
steel
est?
is
a
to
ter
have
lucro
profit
vendendo
selling
os
the
pr?prios
own
vegetais
vegetables
e
and
ervas
herbs
assim como
as well as
uma
a
gama
range
de
of
produtos
products
de
from
agricultores
farmers
locais,
local
e
and
gerindo
managing
um
a
viveiro
nursery
que
that
vende
sells
plantas
plants
e
and
pl?ntulas.
seedlings
Greensgrow, a one-acre plot of raised beds and greenhouses on the site of a former steel-galvanizing
factory, is turning a profit by selling its own vegetables and herbs as well as a range of produce
from local growers, and by running a nursery selling plants and seedlings.
LXGram in the Shared Task ?Comparing Semantic Representations? 313
(3) A
the
quinta
farm
lucrou
earned
cerca de
about
10000
10000
d?lares
dollars
com
with
uma
a
receita
revenue
de
of
450000
450000
d?lares
dollars
em
in
2007,
2007
e
and
espera
hopes
ter
to have
um
a
lucro
profit
de
of
5%
5%
sobre
on
os
the
650000
650000
d?lares
dollars
de
of
receitas
revenue
neste
in this
ano,
year
o
the
seu
its
10??
10th
ano,
year
para
in order
poder
to be able
abrir
to open
outra
another
actividade
operation
noutro
in another
s?tio
place
de
of
Filad?lfia.
Philadelphia
The farm earned about $10,000 on revenue of $450,000 in 2007, and hopes to make a profit of 5
percent on $650,000 in revenue in this, its 10th year, so it can open another operation elsewhere in
Philadelphia.
Text 7
(1) O
the
desenvolvimento
development
moderno
modern
da
of the
tecnologia
techonology
e
and
aplica??es
applications
de
of
energia
energy
e?lica
wind.ADJECTIVE
j?
already
estava
was
numa
in a
fase
phase
avan?ada
advanced
nos
by the
anos
years
30,
30
quando
when
por
by
estimativa
estimation
cerca de
about
600000
600000
moinhos
mills
forneciam
supplied
?reas
areas
rurais
rural
com
with
electricidade
electricity
e
and
servi?os
services
de
of
bombeamento
pumping
de
of
?gua.
water
Modern development of wind-energy technology and applications was well underway by the 1930s,
when an estimated 600,000 windmills supplied rural areas with electricity and water-pumping ser-
vices.
(2) Quando
when
a
the
distribui??o
distribution
em
in
larga
broad
escala
scale
de
of
electricidade
electricity
chegou
arrived
?s
to the
quintas
farms
e
and
?s
to the
terras
small
pequenas,
towns
o
the
uso
use
de
of
energia
energy
e?lica
wind.ADJECTIVE
nos
in the
Estados Unidos
United States
come?ou
started
a
to
diminuir,
subside
mas
but
voltou
it went back
a
to
subir
raise
depois
after
da
of the
falta
shortage
de
of
petr?leo
oil
nos
in the
EUA
US
no
in the
come?o
beginning
dos
of the
anos
years
70.
70
Once broad-scale electricity distribution spread to farms and country towns, use of wind energy in
the United States started to subside, but it picked up again after the U.S. oil shortage in the early
1970s.
(3) Nos
in the
?ltimos
last
30
30
anos,
years
a
the
investiga??o
research
e
and
o
the
desenvolvimento
development
t?m
have
oscilado
fluctuated
de acordo
in accordance
com
with
o
the
interesse
interest
e
and
os
the
benef?cios
benefits
fiscais
fiscal
do
of the
governo
government
federal.
federal
Over the past 30 years, research and development has fluctuated with federal government interest
and tax incentives.
(4) Em
in
meados
middle
dos
of the
anos
years
80,
80
as
the
turbinas
turbines
e?licas
wind.ADJECTIVE
tinham
had
tipicamente
typically
uma
a
pot?ncia
power rating
m?xima
maximum
de
of
150
150
kW.
kW
In the mid-?80s, wind turbines had a typical maximum power rating of 150 kW.
(5) Em
In
2006,
2006
as
the
turbinas
turbines
comerciais
commercial
de
of
grande
large
escala
scale
s?o
are
comummente
commonly
avaliadas
rated
em
at
mais
more
de
than
1
1
MW
MW
e
and
est?o
are
dispon?veis
available
em
in
no
at the
m?ximo
most
4
4
MW
MW
de
of
capacidade.
capacity
In 2006, commercial, utility-scale turbines are commonly rated at over 1 MW and are available in
up to 4 MW capacity.
314 Branco and Costa
Appendix B: MRS Representation for Text 4, Sentence 1
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
mrs
LTOP h1 h
INDEX e2 e
RELS
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
_o_q_rel
LBL h3 h
ARG0 x4
?
?
?
?
?
?
?
x
PNG.PERSON 3rd
PNG.NUMBER singular
PNG.GENDER feminine
DIV -
?
?
?
?
?
?
?
RSTR h6 h
BODY h5 h
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
,
?
?
?
?
?
_primeiro_a_rel
LBL h7 h
ARG0 e8 e
ARG1 h9 h
?
?
?
?
?
,
?
?
?
_escola_n_rel
LBL h9
ARG0 x4
?
?
?
,
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
_de_p_rel
LBL h9
ARG0 e10 e
ARG1 x4
ARG2 x11
?
?
?
?
?
?
?
x
PNG.PERSON 3rd
PNG.NUMBER singular
PNG.GENDER masculine
DIV +
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
,
?
?
?
?
?
?
?
udef_q_rel
LBL h12 h
ARG0 x11
RSTR h14 h
BODY h13 h
?
?
?
?
?
?
?
,
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
_treino_n_-de-por-_rel
LBL h15 h
ARG0 x11
ARG1 r17 r
ARG2 x16
?
?
?
?
?
?
?
x
PNG.PERSON 3rd
PNG.GENDER masculine
PNG.NUMBER plural
DIV +
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
,
?
?
?
?
?
?
?
udef_q_rel
LBL h18 h
ARG0 x16
RSTR h20 h
BODY h19 h
?
?
?
?
?
?
?
,
?
?
?
_c?o-guia_n_rel
LBL h21 h
ARG0 x16
?
?
?
,
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
_de_p_rel
LBL h9
ARG0 e22 e
ARG1 x4
ARG2 x23
?
?
?
?
?
?
?
x
DIV -
PNG.NUMBER singular
PNG.GENDER masculine
PNG.PERSON 3rd
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
,
?
?
?
?
?
?
?
_o_q_rel
LBL h24 h
ARG0 x23
RSTR h26 h
BODY h25 h
?
?
?
?
?
?
?
,
?
?
?
_pa?s_n_rel
LBL h27 h
ARG0 x23
?
?
?
,
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
tense_rel
LBL h28 h
ARG0 e29
?
?
?
?
?
?
?
?
?
?
e
SF proposition
ELLIPTICAL-PUNCT -
E.MOOD indicativo
E.TENSE presente
E.ASPECT.PERF -
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
,
?
?
?
?
?
_ir_v_aux_rel
LBL h28
ARG0 e29
ARG1 h30 h
?
?
?
?
?
,
?
?
?
?
?
?
?
?
?
?
?
?
?
tense_rel
LBL h31 h
ARG0 e32
?
?
?
?
?
?
?
e
SF proposition
ELLIPTICAL-PUNCT bool
E.MOOD infinitivo-nao-flexionado
E.ASPECT.PERF -
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
,
?
?
?
?
?
_nascer_v_rel
LBL h31
ARG0 e32
ARG1 x4
?
?
?
?
?
,
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
_em_p_rel
LBL h31
ARG0 e33 e
ARG1 e32
ARG2 x34
?
?
?
?
?
?
?
x
PNG.PERSON 3rd
PNG.NUMBER singular
PNG.GENDER feminine
DIV -
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
,
?
?
?
?
?
?
?
proper_q_rel
LBL h35 h
ARG0 x34
RSTR h37 h
BODY h36 h
?
?
?
?
?
?
?
,
?
?
?
?
?
named_rel
LBL h38 h
ARG0 x34
ARG1 s39 s
?
?
?
?
?
,
?
?
?
?
?
string-equals_rel
LBL h38
ARG0 s39
CARG mort?gua
?
?
?
?
?
,
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
_e_coord_rel
LBL h40 h
C-ARG e2
L-HNDL h28
L-INDEX e29
R-HNDL h42 h
R-INDEX e41
?
?
?
?
?
?
?
?
?
?
e
SF proposition
ELLIPTICAL-PUNCT -
E.MOOD indicativo
E.TENSE futuro
E.ASPECT.PERF -
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
,
?
?
?
tense_rel
LBL h42
ARG0 e41
?
?
?
,
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
_treinar_v_rel
LBL h42
ARG0 e41
ARG1 x4
ARG2 x43
?
?
?
?
?
?
?
x
PNG.NUMBER plural
PNG.GENDER masculine
PNG.PERSON 3rd
DIV +
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
,
?
?
?
?
?
?
?
indef_q_rel
LBL h44 h
ARG0 x43
RSTR h45 h
BODY h46 h
?
?
?
?
?
?
?
,
?
?
?
?
?
?
?
?
cardinal_rel
LBL h47 h
ARG0 e49 e
ARG1 h50 h
ARG2 j48 j
?
?
?
?
?
?
?
?
,
?
?
?
?
?
?
greater-or-equal_rel
LBL h47
ARG0 j48
ARG1 j51 j
?
?
?
?
?
?
,
?
?
?
?
?
?
?
?
plus_rel
LBL h47
ARG0 j51
TERM0 j53 j
TERM1 j52 j
?
?
?
?
?
?
?
?
,
?
?
?
?
?
int-equals_rel
LBL h47
ARG0 j53
CARG 20
?
?
?
?
?
,
?
?
?
?
?
int-equals_rel
LBL h47
ARG0 j52
CARG 2
?
?
?
?
?
,
?
?
?
_c?o-guia_n_rel
LBL h50
ARG0 x43
?
?
?
,
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
_por_p_rel
LBL h42
ARG0 e54 e
ARG1 e41
ARG2 x55
?
?
?
?
?
?
?
x
PNG.PERSON 3rd
PNG.NUMBER singular
PNG.GENDER masculine
DIV +
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
,
?
?
?
?
?
?
?
udef_q_rel
LBL h56 h
ARG0 x55
RSTR h58 h
BODY h57 h
?
?
?
?
?
?
?
,
?
?
?
_ano_n_rel
LBL h59 h
ARG0 x55
?
?
?
?
HCONS
?
?
?
?
qeq
HARG h1
LARG h40
?
?
?
,
?
?
?
qeq
HARG h6
LARG h7
?
?
?
,
?
?
?
qeq
HARG h14
LARG h15
?
?
?
,
?
?
?
qeq
HARG h20
LARG h21
?
?
?
,
?
?
?
qeq
HARG h26
LARG h27
?
?
?
,
?
?
?
qeq
HARG h30
LARG h31
?
?
?
,
?
?
?
qeq
HARG h37
LARG h38
?
?
?
,
?
?
?
qeq
HARG h45
LARG h47
?
?
?
,
?
?
?
qeq
HARG h58
LARG h59
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
