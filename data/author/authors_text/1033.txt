51
52
53
54
55
56
57
58
Squibs
Reliability Measurement without Limits
Dennis Reidsma?
University of Twente
Jean Carletta??
University of Edinburgh
In computational linguistics, a reliability measurement of 0.8 on some statistic such as ? is
widely thought to guarantee that hand-coded data is fit for purpose, with 0.67 to 0.8 tolerable,
and lower values suspect. We demonstrate that the main use of such data, machine learning, can
tolerate data with low reliability as long as any disagreement among human coders looks like
random noise. When the disagreement introduces patterns, however, the machine learner can
pick these up just like it picks up the real patterns in the data, making the performance figures
look better than they really are. For the range of reliability measures that the field currently
accepts, disagreement can appreciably inflate performance figures, and even a measure of 0.8 does
not guarantee that what looks like good performance really is. Although this is a commonsense
result, it has implications for how we work. At the very least, computational linguists should
look for any patterns in the disagreement among coders and assess what impact they will have.
1. Introduction
In computational linguistics, 0.8 is often regarded as some kind of magical reliability
cut-off guaranteeing the quality of hand-coded data (e.g., Reithinger and Kipp 1998;
Shriberg et al 1998; Galley et al 2004), with 0.67 to 0.8 tolerable?although it is as
often honored in the breech as in the observance. The argument for the meaning of
0.8 arises originally from Krippendorff (1980, page 147), in a comment about practice
in the field of content analysis. He states that correlations found between two variables
using their hand-coded values ?tend to be insignificant? when the hand-codings have
a reliability below 0.8. He uses a specific reliability statistic, ?, for his measurements,
but Carletta (1996) implicitly assumes kappa-like metrics are similar enough in practice
for the rule of thumb to apply to them as well. A detailed discussion on the differences
and similarities of these, and other, measures is provided by Krippendorff (2004); in this
article we will use Cohen?s ? (1960) to investigate the value of the 0.8 reliability cut-off
for computational linguistics.
Modern computational linguists use data in a completely different way from 1970s
content analysts. Rather than correlating two variables, we use hand-coded data as
? University of Twente, Human Media Interaction, Room ZI2067, PO Box 217, NL-7500 AE Enschede,
The Netherlands, D.Reidsma@utwente.nl.
?? University of Edinburgh, Human Communication Research Centre, J.Carletta@ed.ac.uk.
Submission received: 4 September 2007; revised submission received: 20 December 2007; accepted for
publication: 6 April 2008.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 3
training and test material for automatic classifiers. The 0.8 rule of thumb is irrelevant
for this purpose, because classifiers will be affected by disagreement differently than
correlations. Furthermore, Krippendorff?s argument comes with a caveat: the disagree-
ment must be due to random noise. For his case of correlations, any patterns in the
disagreement could accidentally bolster the relationship perceived in the data, leading
to false results. To be sure that data is fit for the intended purpose, Krippendorff advises
the analyst to look for structure in the disagreement and consider how it might affect
data use. Although computational linguists have rarely followed this advice, it is just
as relevant to us. Machine-learning algorithms are designed specifically to look for, and
predict, patterns in noisy data. In theory, this makes random disagreement unimportant.
More data will yield more signal and the learner will ignore the noise. However, as
Craggs and McGee Wood (2005) suggest, this also makes systematic disagreement
dangerous, because it provides an unwanted pattern for the learner to detect. We
demonstrate that machine learning can tolerate data with a low reliability measurement
as long as the disagreement looks like random noise, and that when it does not, data
can have a reliability measure commonly held to be acceptable but produce misleading
results.
2. Method
To explain what is wrong with using 0.8 as a cut-off, we need to think about how data
is used for classification tasks. Consider Figure 1, which shows a relation between some
features A and a class label B. Learning labels from a set of features is a common task
in computational linguistics; for instance, in Shriberg et al (1998), which assumes a
pre-existing dialogue act segmentation, the labels are dialogue act types, and they are
learned from automatically derived prosodic features. In this way of using data, only
one of the variables?the output dialogue act label?is hand-coded. In the figure, the
real relationship between prosody and dialogue act label is shown on the left; R relates
the prosodic features A to the output act B.
Figure 1
Hand-coded target labels are used to train classifiers to automatically predict those labels
from features.
320
Reidsma and Carletta Reliability Measurement without Limits
In theory, there is one correct label for any given act. However, in practice hu-
man coders disagree, choosing different labels for the same act (sometimes even with
divergences that make one question whether there is one correct answer). The data
actually available for analysis is shown in the middle of the figure. Here, the automatic
features, A, are the same as before, but there are multiple, possibly differing labels
for the same act, Bobs, coming from different human annotators. Finally, on the right
the figure shows the classifier. It takes the same prosodic features A and uses them to
predict a dialogue act label Bpred on new data, using the relationship learned from the
observed data, RML. Projects vary in how they choose data from which to build the
classifier when coders disagree, but whatever they do is colored by the observations
they have available to them. We often think of reliability assessment as telling us how
much disagreement there is among the human coders, but the real issue is how their
individual interpretations of the coding scheme make RML differ from R.
There is a problem that arises for anyone using this methodology. Without the
?real? data, it is impossible to judge how well the learned relationship reflects the real
one. Classification performance for Bpred can only be calculated with respect to the ?ob-
served? data Bobs. In this article, we surmount this problem by simulating the real world
so that we can measure the differences between this ?observed? performance and the
?real? performance. Our simulation uses a Bayesian network (Pearl 1988) to create an
initial, ?real? data set with 3,000 samples of features (A) and their corresponding target
labels (B). For simplicity, we use a single five-valued feature and five possible labels.
The relative label frequencies vary between 17% and 25%. This gives us a small amount
of variation around what is essentially equally distributed data. We corrupt the labels
(B) to simulate the ?hand-coded? observed data (Bobs) corresponding to the output of
a human coder, and then train a neural network constructed using the WEKA toolkit
(Witten and Frank 2005) on 2,000 samples from Bobs. Finally, we calculate the neural
network?s performance twice, using as test data either the remaining 1,000 samples from
Bobs or the initial, ?real? versions of those same 1,000 samples.
There are three ways in which we need to vary our simulation in order to be sys-
tematic. The first is in the strength of the relationship between the features the machine
learner takes as input and the target labels, which we achieve simply by changing the
probabilities in the Bayesian network that creates the data set. In the simulation, we
vary the strength of the relationship in eight graded steps.1 The second is in the amount
of disagreement we introduce when we create the observed data (Bobs). We create 200
different versions of the hand-coded data that cover a range of values from ? = 0 to
1 We use Cramer?s phi to measure the strength of a relationship. Cramer?s phi is defined as
?c =
?
?2
(N) ? dfsmaller
with N the number of samples and dfsmaller the smallest degree of freedom of the two involved variables,
and is a measure of association for nominal variables with more than two values. It can be ?considered
like a correlation coefficient? (Aron and Aron 2003) that takes data set size into account and can easily
be derived for a Bayesian network from the priors and the conditional probability tables. We varied
the strength of the network between ?c = 0.06 and ?c = 0.45. Following Cohen (1988), for a five-way
distinction Aron and Aron (page 527) would consider 0.06 to represent a small real relationship?that
is, one with not much effect?and 0.3, a large one. Thus we describe 0.06 as ?weak,? 0.45 as ?very
strong,? and intermediate points as ?moderate? and ?strong.? It is an open question what strengths
of relationships actually occur in computational linguistics data, although there may be no point in
learning a relationship that?s too strong.
321
Computational Linguistics Volume 34, Number 3
Figure 2
Machine-learning performance obtained on annotations with noise-like disagreements for (a)
weak (?c = 0.06), (b) moderate (?c = 0.20), (c) strong (?c = 0.32), and (d) very strong (?c = 0.45)
relationships between the features and labels.
? = 1, by introducing a varying amount of observation errors in the simulated coding
process.2 The third is in the type of disagreement with which we degrade the real data
to create the observed data (Bobs), representing the types of coding errors the human
annotators make. Again for simplicity, we describe the effects of both random errors
and the overuse of a single coding label.
3. The Case of Noise
Figure 2 shows how a neural network performs when coders make random mistakes
in their coding task, that is, for noise-like disagreement, for the cases of (a) weak, (b)
moderate, (c) strong, and (d) very strong relationships between the features (A) and
labels (B). Here, the y axis shows ?accuracy,? or the percentage of samples in the test
2 To calculate ? for a specific simulated coding we generate two copies of additional ?real? data that has
not been used for training or testing, apply the same simulated human annotator to one copy, and a
second annotator who makes the same number of ?mistakes? to the other copy. This mimics the
common practice of having one annotator code the data, with a second annotator coding enough to
test the reliability.
322
Reidsma and Carletta Reliability Measurement without Limits
data for which the network chooses the correct label. The x axis varies the amount of
coder errors in the data to correspond to different ? values, with the two black lines
marking the values of ? = 0.67 and ? = 0.8.
Look first at the series depicted as a line. It shows accuracy measured by using
the ?observed? version of the test data, which is how testing is normally done. For each
relationship strength, as ? increases, so does accuracy. In all cases, at ? = 0 (that is, when
the coders fail to agree beyond what one would expect if they were all choosing their
labels randomly) accuracy is at 20%, which is what one would expect if the classifier
were choosing randomly as well. For any given ? value, the stronger the underlying
relationship, the more benefit the neural network can derive from the data. Now look
at the other of the two series, depicted as small squares. It shows accuracy measured
by using the ?real? version of the data. Interestingly, the ?real? performance, that is, the
power of the learned model to predict reality, is higher than performance as measured
against the observed data. This is because for some samples, the classifier?s predictions
are correct, but because the observations contain errors, the test data actually gets
them wrong. The stronger the relationship in the real data, the more marked this effect
becomes. The neural network is able to disregard noise-like coding errors at very low
? values simply because the errors contain no patterns for it to learn.
4. The Case of Overusing a Label
Now consider the case where instead of random coding errors, the coder over-uses
the least frequent one of the five labels for B. Figure 3 shows the results for this kind
of coding error. Remember that in the graphs, the series depicted as a line shows the
observed performance of the classifier?that is, performance as it is usually measured.
The two black lines again mark the ? values of interest (? = 0.67 and ? = 0.8).
The graphs show an entirely different effect from the one obtained for noise-like
coding errors: For lower values of ?, the observed performance is spuriously high. This
makes perfect sense?? is low when the pattern of label overuse is strong, and the neural
network picks it up. When the observed data is used to test performance, some of the
samples match not because the classifier gets the label right, but because it overuses
the same label as the human coder. For data with a very strong correlation between the
input features A and the output labels B, the turning point below which performance
is spuriously high occurs at around ? = 0.55 (Figure 3d), a value the community holds
to be pretty low but which is not unknown in published work. However, when the
underlying relationship to be learned is moderate or strong (Figures 3b and 3c), the
spuriously high results already occur for ? values commonly held to be tolerable. With
a weak relationship, the turning point can occur at ? > 0.8 (Figure 3a).
5. Discussion
Our simulation highlights a danger for current practice in computational linguistics,
among other fields. Overuse of a label is a realistic type of error for human annotators
to make. For instance, imagine a coding scheme for dialogue acts that distinguishes
backchannel utterances from utterances which indicate agreement. In data containing
many utterances where the speech consists of ?Yeah,? individual coders can easily have
a marked bias for either one of these two categories. Clearly, in actual coding, not all
disagreement will be of one type, but will contain a mix of different systematic and
noise-like errors. In addition, the underlying relationships that our systems attempt to
323
Computational Linguistics Volume 34, Number 3
Figure 3
Machine-learning performance obtained on annotations that suffered from over-coding for (a)
weak (?c = 0.06), (b) moderate (?c = 0.20), (c) strong (?c = 0.32), and (d) very strong (?c = 0.45)
relationships between the features and labels.
learn vary in strength. This makes discerning the degree of danger more difficult, but
does not change the substance of our argument.
Although the graphs we show are for a specific simulation, the general pattern we
describe is robust. In particular, using ? in place of ? does not markedly change the
results; neither does increasing or decreasing the data set size. Our simulations and
results are presented for a machine-learning context. However, that does not mean that
other types of data use are immune to the problems we describe here. Other statistical
uses of data will be affected in their own ways by the difference between structural and
noise-like disagreement.
6. Implications
At the moment, much of the effort we devote to reliability measurement as a community
is used to establish one or more overall reliability statistics for our data sets and to argue
about which reliability statistic is most appropriate. Methodological discussions focus
on questions such as how to force annotated data structures into the mathematical form
necessary to calculate ?, or what effects certain aspects of the annotation have on the
values of some metric rather than on possible uses of the resulting data (Marcu, Amorrortu,
324
Reidsma and Carletta Reliability Measurement without Limits
and Romera 1999; Di Eugenio and Glass 2004; Artstein and Poesio 2005). Computational
linguists are of course aware that no overall reliability measure can give a complete
story, but often fail to spend time analyzing coder disagreements further. Unfortunately,
our results suggest that current practice is insufficient, at least where the data is destined
to be input for a machine-learning process and quite possibly for other data uses as
well. This complements observations of Artstein and Poesio: Besides the fact that many
different ways of calculating reliability metrics lead to different values, which makes
comparing them to a threshold difficult (Artstein and Poesio in press), the very idea
of having any such single threshold in the first place turns out to be impossible to
hold. Instead of worrying about exactly how much disagreement there is in a data
set and how to measure it, we should be looking at the form the disagreement takes.
A headline measurement, no matter how it is expressed, will not show the difference
between noise-like and systematic disagreement, but this difference can be critical for
establishing whether or not a data set is fit for the purpose for which it is intended.
To tease out what sort of disagreement a data set contains, Krippendorff suggests
calculating odd-man-out and per-class reliability to find out which class distinctions are
problematic (1980, page 150). Bayerl and Paul (2007) discuss methods for determining
which factors (schema changes, coding team changes, etc.) were involved in causing
poor annotation quality. Wiebe, Bruce, and O?Hara (1999) suggest looking at the mar-
ginals and how they differ between coders to find indications of whether disagreements
are caused by systematic bias (as opposed to being random) and in which classes
they occur. Although clearly useful techniques, none of these diagnostics is specifically
designed to address the needs of machine learners which are designed to recognize pat-
terns. Overusing a label is just one simple example of a type of systematic disagreement
that adds unwanted patterns that a machine learner can find. Any spurious pattern
could be a problem. For this reason, we should be looking specifically for patterns in
the disagreement itself.
Our suggestion for one possible diagnostic technique is based on the following
observation: If the disagreements between two coders contain no pattern, any test for
association or correlation, when performed on only the disagreed items, should show
no relation between the labels assigned by the two coders. For certain patterns in the
disagreement, however, a correlation would show up. (To see this, consider the case
where one coder tends to label rhetorical questions as yes/no-questions and the other
coder assigns both labels correctly: If this happens often enough, tests for association
would come up with a relation between the labels for the two coders for the disagreed
items.) If the test shows a correlation, the disagreements add patterns for the machine
learner to find. Unfortunately, the converse does not necessarily hold: It is possible
that not all patterns that could be picked up by a machine learner will show up in
correlations between disagreed items, for example because the amount of multiply-
annotated data is too small. The computational linguistics community therefore needs
to develop additional diagnostics for patterns in the coder disagreements.
It should go without saying that analysts will benefit from keeping how they
intend to use the data firmly in mind at all times. As Krippendorff (2004, page 429)
recommends, one should test reliability for the ?distinctions that matter? and perform
?suitable experiments of the effects of unreliable data on the conclusions.? Patterns
found for an overall coding scheme will not always affect every possible data use. For
instance, we often build classifiers not for complete coding schemes, but for some subset
of the labels or some ?class map? that transforms the scheme into a smaller set of classes.
In these cases, what is important is disagreement for the subset or transformation, not
the entire scheme. Similarly, where classifier performance is reported per class, the
325
Computational Linguistics Volume 34, Number 3
reliability for that particular label will be the most important. Finally, different machine-
learning algorithms may react differently to different kinds of patterns in the data and
to combinations of patterns in different relative strengths. In complicated cases, perhaps
the safest way to assess whether or not there is a problem with systematic disagreement
is to run a simulation like the one we have reported but with the kind and scale of
disagreement suspected of the data, and to use that to estimate the possible effects of
unreliable data on the performance of machine-learning algorithms.
Acknowledgments
We thank Rieks op den Akker, Ron Artstein,
and Bonnie Webber for discussions that have
helped us frame this article, as well as the
anonymous reviewers for their thoughtful
comments. This work is supported by the
European IST Programme Project FP6-033812
(AMIDA, publication 36). This article only
reflects the authors? views and funding
agencies are not liable for any use that may
be made of the information contained herein.
References
Aron, Arthur and Elaine N. Aron. 2003.
Statistics for Psychology. Prentice Hall,
Upper Saddle River, NJ.
Artstein, Ron and Massimo Poesio. 2005.
Bias decreases in proportion to the number
of annotators. In Proceedings of FG-MoL
2005, pages 141?150, Edinburgh.
Artstein, Ron and Massimo Poesio. In press.
Inter-coder agreement for computational
linguistics. Computational Linguistics.
Bayerl, Petra Saskia and Karsten Ingmar
Paul. 2007. Identifying sources of
disagreement: Generalizability theory
in manual annotation studies.
Computational Linguistics, 33(1):3?8.
Carletta, Jean C. 1996. Assessing agreement
on classification tasks: The kappa statistic.
Computational Linguistics, 22(2):249?254.
Cohen, Jacob. 1960. A coefficient of
agreement for nominal scales. Educational
and Psychological Measurement, 20(1):37?46.
Cohen, Jacob. 1988. Statistical power analysis
for the behavioral sciences, 2nd edition.
Lawrence Erlbaum, Hillsdale, NJ.
Craggs, Richard and Mary McGee Wood.
2005. Evaluating discourse and dialogue
coding schemes. Computational Linguistics,
31(3):289?296.
Di Eugenio, Barbara and Michael Glass.
2004. The kappa statistic: A second look.
Computational Linguistics, 30(1):95?101.
Galley, Michel, Kathleen McKeown, Julia
Hirschberg, and Elizabeth Shriberg.
2004. Identifying agreement and
disagreement in conversational speech:
Use of Bayesian networks to model
pragmatic dependencies. In Proceedings
of the 42nd Meeting of the Association for
Computational Linguistics (ACL?04),
pages 669?676, Barcelona.
Krippendorff, Klaus. 1980. Content Analysis:
An Introduction to its Methodology,
volume 5 of The Sage CommText Series.
Sage Publications, London.
Krippendorff, Klaus. 2004. Reliability
in content analysis. Some common
misconceptions and recommendations.
Human Communication Research,
30(3):411?433.
Marcu, Daniel, Estibaliz Amorrortu, and
Magdalena Romera. 1999. Experiments in
constructing a corpus of discourse trees.
In Marilyn Walker, editor, Towards
Standards and Tools for Discourse Tagging:
Proceedings of the Workshop. Association
for Computational Linguistics, Somerset,
NJ, pages 48?57.
Pearl, Judea. 1988. Probabilistic Reasoning in
Intelligent Systems: Networks of Plausible
Inference. Morgan Kaufmann Publishers
Inc., San Francisco, CA.
Reithinger, Norbert and Michael Kipp.
1998. Large scale dialogue annotation in
Verbmobil. In Workshop Proceedings of
ESSLLI 98, pages 1?6, Saarbru?cken.
Shriberg, Elizabeth, Rebecca Bates, Paul
Taylor, Andreas Stolcke, Daniel Jurafsky,
Klaus Ries, Noah Coccaro, Rachel
Martin, Marie Meteer, and Carol Van
Ess-Dykema. 1998. Can prosody aid the
automatic classification of dialog acts in
conversational speech? Language and
Speech, 41(3-4):443?492.
Wiebe, Janyce M., Rebecca F. Bruce, and
Thomas P. O?Hara. 1999. Development
and use of a gold-standard data set
for subjectivity classifications. In
Proceedings of the 37th Annual Meeting
of the Association for Computational
Linguistics, pages 246?253, Morristown, NJ.
Witten, Ian H. and Eibe Frank. 2005. Data
Mining: Practical Machine Learning Tools and
Techniques, 2nd edition. Morgan Kaufmann,
San Francisco, CA.
326

This article has been cited by:
1. Ron Artstein, Massimo Poesio. 2008. Inter-Coder Agreement for Computational Linguistics.
Computational Linguistics 34:4, 555-596. [Abstract] [PDF] [PDF Plus]
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 367?374,
New York, June 2006. c?2006 Association for Computational Linguistics
Incorporating Speaker and Discourse Features into Speech
Summarization
Gabriel Murray, Steve Renals,
Jean Carletta, Johanna Moore
University of Edinburgh, School of Informatics
Edinburgh EH8 9LW, Scotland
gabriel.murray@ed.ac.uk, s.renals@ed.ac.uk,
jeanc@inf.ed.ac.uk, j.moore@ed.ac.uk
Abstract
We have explored the usefulness of incorporat-
ing speech and discourse features in an automatic
speech summarization system applied to meeting
recordings from the ICSI Meetings corpus. By an-
alyzing speaker activity, turn-taking and discourse
cues, we hypothesize that such a system can out-
perform solely text-based methods inherited from
the field of text summarization. The summariza-
tion methods are described, two evaluation meth-
ods are applied and compared, and the results
clearly show that utilizing such features is advanta-
geous and efficient. Even simple methods relying
on discourse cues and speaker activity can outper-
form text summarization approaches.
1. Introduction
The task of summarizing spontaneous spoken di-
alogue from meetings presents many challenges:
information is sparse; speech is disfluent and frag-
mented; automatic speech recognition is imper-
fect. However, there are numerous speech-specific
characteristics to be explored and taken advantage
of. Previous research on summarizing speech has
concentrated on utilizing prosodic features [1, 2].
We have examined the usefulness of additional
speech-specific characteristics such as discourse
cues, speaker activity, and listener feedback. This
speech features approach is contrasted with a sec-
ond summarization approach using only textual
features?a centroid method [3] using a latent se-
mantic representation of utterances. These indi-
vidual approaches are compared to a combined ap-
proach as well as random baseline summaries.
This paper also introduces a new evalua-
tion scheme for automatic summaries of meeting
recordings, using a weighted precision score based
on multiple human annotations of each meeting
transcript. This evaluation scheme is described
in detail below and is motivated by previous find-
ings [4] suggesting that n-gram based metrics like
ROUGE [5] do not correlate well in this domain.
2. Previous Work
In the field of speech summarization in general, re-
search investigating speech-specific characteristics
has focused largely on prosodic features such as F0
mean and standard deviation, pause information,
syllable duration and energy. Koumpis and Re-
nals [1] investigated prosodic features for summa-
rizing voicemail messages in order to send voice-
mail summaries to mobile devices. Hori et al [6]
have developed an integrated speech summariza-
tion approach, based on finite state transducers, in
which the recognition and summarization compo-
nents are composed into a single finite state trans-
ducer, reporting results on a lecture summariza-
tion task. In the Broadcast News domain, Maskey
and Hirschberg [7] found that the best summariza-
tion results utilized prosodic, lexical, and structural
features, while Ohtake et al [8] explored using
only prosodic features for summarization. Maskey
and Hirschberg similarly found that prosodic fea-
tures alone resulted in good quality summaries of
367
Broadcast News.
In the meetings domain (using the ICSI cor-
pus), Murray et al [2] compared text summariza-
tion approaches with feature-based approaches us-
ing prosodic features, with human judges favoring
the feature-based approaches. Zechner [9] inves-
tigated summarizing several genres of speech, in-
cluding spontaneous meeting speech. Though rel-
evance detection in his work relied largely on tf.idf
scores, Zechner also explored cross-speaker infor-
mation linking and question/answer detection, so
that utterances could be extracted not only accord-
ing to high tf.idf scores, but also if they were linked
to other informative utterances.
Similarly, this work aims to detect important
utterances that may not be detectable according
to lexical features or prosodic prominence, but
are nonetheless linked to high speaker activity,
decision-making, or meeting structure.
3. Summarization Approaches
The following subsections give detailed descrip-
tions of our two summarization systems, one of
which focuses on speech and discourse features
while the other utilizes text summarization tech-
niques and latent semantic analysis.
3.1. Speech and Discourse Features
In previous summarization work on the ICSI cor-
pus [2, 4], Murray et al explored multiple ways
of applying latent semantic analysis (LSA) to a
term/document matrix of weighted term frequen-
cies from a given meeting, a development of the
method in [10]. A central insight to the present
work is that additional features beyond simple term
frequencies can be included in the matrix before
singular value decomposition (SVD) is carried out.
We can use SVD to project this matrix of features
to a lower dimensionality space, subsequently ap-
plying the same methods as used in [2] for extract-
ing sentences.
The features used in these experiments in-
cluded features of speaker activity, discourse cues,
listener feedback, simple keyword spotting, meet-
ing location and dialogue act length (in words).
For each dialogue act, there are features indi-
cating which speaker spoke the dialogue act and
whether the same speaker spoke the preceding and
succeeding dialogue acts. Another set of features
indicates how many speakers are active on either
side of a given dialogue act: specifically, how
many speakers were active in the preceding and
succeeding five dialogue acts. To further gauge
speaker activity, we located areas of high speaker
interaction and indicated whether or not a given
dialogue act immediately preceded this region of
activity, with the motivation being that informa-
tive utterances are often provocative in eliciting re-
sponses and interaction. Additionally, we included
a feature indicating which speakers most often ut-
tered dialogue acts that preceded high levels of
speaker interaction, as one way of gauging speaker
status in the meeting. Another feature relating to
speaker activity gives each dialogue act a score ac-
cording to how active the speaker is in the meeting
as a whole, based on the intuition that the most ac-
tive speakers will tend to utter the most important
dialogue acts.
The features for discourse cues, listener feed-
back, and keyword spotting were deliberately su-
perficial, all based simply on detecting informative
words. The feature for discourse cues indicates the
presence or absence of words such as decide, dis-
cuss, conclude, agree, and fragments such as we
should indicating a planned course of action. Lis-
tener feedback was based on the presence or ab-
sence of positive feedback cues following a given
dialogue act; these include responses such as right,
exactly and yeah. Keyword spotting was based
on frequent words minus stopwords, indicating the
presence or absence of any of the top twenty non-
stopword frequent words. The discourse cues of
interest were derived from a manual corpus analy-
sis rather than being automatically detected.
A structural feature scored dialogue acts ac-
cording to their position in the meeting, with di-
alogue acts from the middle to later portion of the
meeting scoring higher and dialogue acts at the be-
ginning and very end scoring lower. This is a fea-
ture that is well-matched to the relatively unstruc-
tured ICSI meetings, as many meetings would be
expected to have informative proposals and agen-
das at the beginning and perhaps summary state-
ments and conclusions at the end.
Finally, we include a dialogue act length fea-
ture motivated by the fact that informative utter-
ances will tend to be longer than others.
The extraction method follows [11] by rank-
ing sentences using an LSA sentence score. The
368
matrix of features is decomposed as follows:
A = USV T
where U is an m?n matrix of left-singular vectors,
S is an n ? n diagonal matrix of singular values,
and V is the n?n matrix of right-singular vectors.
Using sub-matrices S and V T , the LSA sentence
scores are obtained using:
ScLSAi =
?
?
?
?
n
?
k=1
v(i, k)2 ? ?(k)2 ,
where v(i, k) is the kth element of the ith sen-
tence vector and ?(k) is the corresponding singular
value.
Experiments on a development set of 55 ICSI
meetings showed that reduction to between 5?15
dimension was optimal. These development ex-
periments also showed that weighting some fea-
tures slightly higher than others resulted in much
improved results; specifically, the discourse cues
and listener feedback cues were weighted slightly
higher.
3.2. LSA Centroid
The second summarization method is a textual ap-
proach incorporating LSA into a centroid-based
system [3]. The centroid is a pseudo-document
representing the important aspects of the docu-
ment as a whole; in the work of [3], this pseudo-
document consists of keywords and their modi-
fied tf.idf scores. In the present research, we take
a different approach to constructing the centroid
and to representing sentences in the document.
First, tf.idf scores are calculated for all words in
the meeting. Using these scores, we find the top
twenty keywords and choose these as the basis for
our centroid. We then perform LSA on a very large
corpus of Broadcast News and ICSI data, using the
Infomap tool1. Infomap provides a query language
with which we can retrieve word vectors for our
twenty keywords, and the centroid is thus repre-
sented as the average of its constituent keyword
vectors [12] [13].
Dialogue acts from the meetings are repre-
sented in much the same fashion. For each dia-
logue act, the vectors of its constituent words are
1http://infomap.stanford.edu
retrieved, and the dialogue act as a whole is the av-
erage of its word vectors. Extraction then proceeds
by finding the dialogue act with the highest cosine
similarity with the centroid, adding this to the sum-
mary, then continuing until the desired summary
length is reached.
3.3. Combined
The third summarization method is simply a com-
bination of the first two. Each system produces a
ranking and a master ranking is derived from these
two rankings. The hypothesis is that the strength
of one system will differ from the other and that
the two will complement each other and produce
a good overall ranking. The first system would be
expected to locate areas of high activity, decision-
making, and planning, while the second would lo-
cate information-rich utterances. This exempli-
fies one of the challenges of summarizing meeting
recordings: namely, that utterances can be impor-
tant in much different ways. A comprehensive sys-
tem that relies on more than one idea of importance
is ideal.
4. Experimental Setup
All summaries were 350 words in length, much
shorter than the compression rate used in [2] (10%
of dialogue acts). The ICSI meetings themselves
average around 10,000 words in length. The rea-
sons for choosing a shorter length for summaries
are that shorter summaries are more likely to be
useful to a user wanting to quickly overview and
browse a meeting, they present a greater summa-
rization challenge in that the summarizer must be
more exact in pinpointing the important aspects of
the meeting, and shorter summaries make it more
feasible to enlist human evaluators to judge the nu-
merous summaries on various criteria in the future.
Summaries were created on both manual tran-
scripts and speech recognizer output. The unit of
extraction for these summaries was the dialogue
act, and these experiments used human segmented
and labeled dialogue acts rather than try to detect
them automatically. In future work, we intend to
incorporate dialogue act detection and labeling as
part of one complete automatic summarization sys-
tem.
369
4.1. Corpus Description
The ICSI Meetings corpus consists of 75 meetings,
lasting approximately one hour each. Our test set
consists of six meetings, each with multiple hu-
man annotations. Annotators were given access
to a graphical user interface (GUI) for browsing
an individual meeting that included earlier human
annotations: an orthographic transcription time-
synchronized with the audio, and a topic segmen-
tation based on a shallow hierarchical decompo-
sition with keyword-based text labels describing
each topic segment. The annotators were told to
construct a textual summary of the meeting aimed
at someone who is interested in the research being
carried out, such as a researcher who does similar
work elsewhere, using four headings:
? general abstract: ?why are they meeting and
what do they talk about??;
? decisions made by the group;
? progress and achievements;
? problems described
The annotators were given a 200 word limit for
each heading, and told that there must be text for
the general abstract, but that the other headings
may have null annotations for some meetings. An-
notators who were new to the data were encour-
aged to listen to a meeting straight through before
beginning to author the summary.
Immediately after authoring a textual sum-
mary, annotators were asked to create an extractive
summary, using a different GUI. This GUI showed
both their textual summary and the orthographic
transcription, without topic segmentation but with
one line per dialogue act based on the pre-existing
MRDA coding [14]. Annotators were told to ex-
tract dialogue acts that together would convey the
information in the textual summary, and could be
used to support the correctness of that summary.
They were given no specific instructions about the
number or percentage of acts to extract or about
redundant dialogue acts. For each dialogue act ex-
tracted, they were then required in a second pass
to choose the sentences from the textual summary
supported by the dialogue act, creating a many-
to-many mapping between the recording and the
textual summary. Although the expectation was
that each extracted dialogue act and each summary
sentence would be linked to something in the op-
posing resource, we told the annotators that under
some circumstances dialogue acts and summary
sentences could stand alone.
We created summaries using both manual tran-
scripts as well as automatic speech recognition
(ASR) output. The AMI-ASR system [15] is de-
scribed in more detail in [4] and the average word
error rate (WER) for the corpus is 29.5%.
4.2. Evaluation Frameworks
The many-to-many mapping of dialogue acts to
summary sentences described in the previous sec-
tion allows us to evaluate our extractive summaries
according to how often each annotator linked a
given extracted dialogue act to a summary sen-
tence. This is somewhat analogous to Pyramid
weighting [16], but with dialogue acts as the SCUs.
In fact, we can calculate weighted precision, recall
and f-score using these annotations, but because
the summaries created are so short, we focus on
weighted precision as our central metric. For each
dialogue act that the summarizer extracts, we count
the number of times that each annotator links that
dialogue act to a summary sentence. For a given
dialogue act, it may be that one annotator links it
0 times, one annotator links it 1 time, and the third
annotator links it two times, resulting in an aver-
age score of 1 for that dialogue act. The scores for
all of the summary dialogue acts can be calculated
and averaged to create an overall summary score.
ROUGE scores, based on n-gram overlap be-
tween human abstracts and automatic extracts,
were also calculated for comparison [5]. ROUGE-
2, based on bigram overlap, is considered the most
stable as far as correlating with human judgments,
and this was therefore our ROUGE metric of inter-
est. ROUGE-SU4, which evaluates bigrams with
intervening material between the two elements of
the bigram, has recently been shown in the con-
text of the Document Understanding Conference
(DUC)2 to bring no significant additional informa-
tion as compared with ROUGE-2. Results from
[4] and from DUC 2005 also show that ROUGE
does not always correlate well with human judg-
ments. It is therefore included in this research in
the hope of further determining how reliable the
2http://duc.nist.gov
370
ROUGE metric is for our domain of meeting sum-
marization.
5. Results
The experimental results are shown in figure 1
(weighted precision) and figure 2 (ROUGE-2) and
are discussed below.
5.1. Weighted Precision Results
For weighted precision, the speech features ap-
proach was easily the best and scored significantly
better than the centroid and random approaches
(ANOVA,p<0.05), attaining an averaged weighted
precision of 0.52. The combined approach did
not improve upon the speech features approach
but was not significantly worse either. The ran-
domly created summaries scored much lower than
all three systems.
The superior performance of the speech fea-
tures approach compared to the LSA centroid
method closely mirrors results on the ICSI devel-
opment set, where the centroid method scored 0.23
and the speech features approach scored 0.42. For
the speech features approach on the test set, the
best feature by far was dialogue act length. Re-
moving this feature resulted in the precision score
being nearly halved. This mirrors results from
Maskey and Hirschberg [7], who found that the
length of a sentence in seconds and its length in
words were the two best features for predicting
summary sentences. Both the simple keyword
spotting and the discourse cue detection features
caused a lesser decline in precision when removed,
while other features of speaker activity had a neg-
ligible impact on the test results.
Interestingly, the weighted precision scores on
ASR were not significantly worse for any of the
summarization approaches. In fact, the centroid
approach scored very slightly higher on ASR out-
put than on manual transcripts. In [17] and [2] it
was similarly found that summarizing with ASR
output did not cause great deterioration in the qual-
ity of the summaries. It is not especially surpris-
ing that the speech features approach performed
similarly on both manual and ASR transcripts, as
many of its features based on speaker exchanges
and speaker activity would be unaffected by ASR
errors. The speech features approach is still signif-
icantly better than the random and centroid sum-
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
CombinedSpeechFeatsCentroidRandom
Summarization Approaches
PRECISION-MAN
PRECISION-ASR
Figure 1: Weighted Precision Results on Test Set
maries, and is not significantly better than the com-
bined approach on ASR.
5.2. ROUGE Results
The ROUGE results greatly differed from the
weighted precision results in several ways. First,
the centroid method was considered to be the best,
with a ROUGE-2 score of 0.047 compared with
0.041 for the speech features approach. Second,
there were not as great of differences between the
four systems according to ROUGE as there were
according to weighted precision. In fact, the ran-
dom summaries of manual transcripts are not sig-
nificantly worse than the other approaches, accord-
ing to ROUGE-2. Neither the combined approach
nor the speech features approach is significantly
worse than the centroid system, with the combined
approach generally scoring on par with the cen-
troid scores.
The third difference relates to summarization
on ASR output. ROUGE-2 has the random system
and the combined system showing sharp declines
when applied to ASR transcripts. The speech fea-
tures and centroid approaches do not show de-
clines. Random summaries are significantly worse
than both the centroid summaries (p<0.1) and
speech features summaries (p<0.05). Though the
combined approach declines on ASR output, it is
not significantly worse than the other systems.
To get an idea of a ROUGE-2 upper bound, for
each meeting in the test set we left one human ab-
stract out and compared it with the remaining ab-
stracts. The result was an average ROUGE-2 score
of .086.
371
 0.02
 0.04
 0.06
 0.08
 0.1
CombinedSpeechFeatsCentroidRandom
Summarization Approaches
ROUGE2-MAN
ROUGE2-ASR
UPPER BOUND
Figure 2: ROUGE-2 Results on Test Set
ROUGE-1 and ROUGE-SU4 show no signif-
icant differences between the centroid and speech
features approaches.
5.3. Correlations
There is no significant correlation between
macroaveraged ROUGE and weighted precision
scores across the meeting set, on both ASR and
manual transcripts. The Pearson correlation is
0.562 with a significance of p < 0.147. The Spear-
man correlation is 0.282 with a significance of p <
0.498. The correlation of scores across each test
meeting is worse yet, with a Pearson correlation
of 0.185 (p<0.208) and a Spearman correlation of
0.181 (p<0.271).
5.4. Sample Summary
The following is the text of a summary of meeting
Bed004 using the speech features approach:
-so its possible that we could do something like a summary
node of some sort that
-and then the question would be if if those are the things that you
care about uh can you make a relatively compact way of getting from
the various inputs to the things you care about
-this is sort of th the second version and i i i look at this maybe just
as a you know a a whatever uml diagram or you know as just a uh
screen shot not really as a bayes net as john johno said
-and um this is about as much as we can do if we dont w if we want
to avoid uh uh a huge combinatorial explosion where we specify ok if
its this and this but that is not the case and so forth it just gets really
really messy
-also it strikes me that we we m may want to approach the point
where we can sort of try to find a uh a specification for some interface
here that um takes the normal m three l looks at it
-so what youre trying to get out of this deep co cognitive linguistics is
the fact that w if you know about source source paths and goals and
nnn all this sort of stuff that a lot of this is the same for different tasks
-what youd really like of course is the same thing youd always like
which is that you have um a kind of intermediate representation
which looks the same o over a bunch of inputs and a bunch of outputs
-and pushing it one step further when you get to construction
grammar and stuff what youd like to be able to do is say you have
this parser which is much fancier than the parser that comes with uh
smartkom
-in independent of whether it about what is this or where is it or
something that you could tell from the construction you could pull
out deep semantic information which youre gonna use in a general
way
6. Discussion
Though the speech features approach was consid-
ered the best system, it is unclear why the com-
bined approach did not yield improvement. One
possibility relates to the extreme brevity of the
summaries: because the summaries are only 350
words in length, it is possible to have two sum-
maries of the same meeting which are equally
good but completely non-overlapping in content.
In other words, they both extract informative dia-
logue acts, but not the same ones. Combining the
rankings of two such systems might create a third
system which is comparable but not any better than
either of the first two systems alone. However, it
is still possible that the combined system will be
better in terms of balancing the two types of im-
portance discussed above: utterances that contain a
lot of informative content and keywords and utter-
ances that relate to decision-making and meeting
structure.
ROUGE did not correlate well with the
weighted precision scores, a result that adds to the
previous evidence that this metric may not be reli-
able in the domain of meeting summarization.
It is very encouraging that the summarization
approaches in general seem immune to the WER
of the ASR output. This confirms previous find-
ings such as [17] and [2], and the speech and
structural features used herein are particularly un-
affected by a moderately high WER. The reason
for the random summarizaton system not suffering
372
a sharp decline when applied to ASR may be due
to the fact that its scores were already so low that
it couldn?t deteriorate any further.
7. Future Work
The above results show that even a relatively small
set of speech, discourse, and structural features can
outperform a text summarization approach on this
data, and there are many additional features to be
explored. Of particular interest to us are features
relating to speaker status, i.e. features that help us
determine who is leading the meeting and who it is
that others are deferring to. We would also like to
more closely investigate the relationship between
areas of high speaker activity and informative ut-
terances.
In the immediate future, we will incorporate
these features into a machine-learning framework,
building support vector models trained on the ex-
tracted and non-extracted classes of the training
set.
Finally, we will apply these methods to the
AMI corpus [18] and create summaries of compa-
rable length for that meeting set. There are likely
to be differences regarding usefulness of certain
features due to the ICSI meetings being relatively
unstructured and informal and the AMI hub meet-
ings being more structured with a higher informa-
tion density.
8. Conclusion
The results presented above show that using fea-
tures related to speaker activity, listener feedback,
discourse cues and dialogue act length can outper-
form the lexical methods of text summarization ap-
proaches. More specifically, the fact that there are
multiple types of important utterances requires that
we use multiple methods of detecting importance.
Lexical methods and prosodic features are not nec-
essarily going to detect utterances that are relevant
to agreement, decision-making or speaker activity.
This research also provides further evidence that
ROUGE does not correlate well with human judg-
ments in this domain. Finally, it has been demon-
strated that high WER for ASR output does not
significantly decrease summarization quality.
9. Acknowledgements
Thanks to Thomas Hain and the AMI-ASR group
for speech recognition output. This work was
partly supported by the European Union 6th FWP
IST Integrated Project AMI (Augmented Multi-
party Interaction, FP6-506811, publication AMI-
150).
10. References
[1] K. Koumpis and S. Renals, ?Automatic sum-
marization of voicemail messages using lex-
ical and prosodic features,? ACM Transac-
tions on Speech and Language Processing,
vol. 2, pp. 1?24, 2005.
[2] G. Murray, S. Renals, and J. Carletta, ?Ex-
tractive summarization of meeting record-
ings,? in Proceedings of the 9th European
Conference on Speech Communication and
Technology, Lisbon, Portugal, September
2005.
[3] D. Radev, S. Blair-Goldensohn, and
Z. Zhang, ?Experiments in single and multi-
document summarization using mead,? in
The Proceedings of the First Document
Understanding Conference, New Orleans,
LA, September 2001.
[4] G. Murray, S. Renals, J. Carletta, and
J. Moore, ?Evaluating automatic summaries
of meeting recordings,? in Proceedings of
the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, Work-
shop on Machine Translation and Summa-
rization Evaluation (MTSE), Ann Arbor, MI,
USA, June 2005.
[5] C.-Y. Lin and E. H. Hovy, ?Automatic
evaluation of summaries using n-gram co-
occurrence statistics,? in Proceedings of
HLT-NAACL 2003, Edmonton, Calgary,
Canada, May 2003.
[6] T. Hori, C. Hori, and Y. Minami, ?Speech
summarization using weighted finite-state
transducers,? in Proceedings of the 8th Eu-
ropean Conference on Speech Communica-
tion and Technology, Geneva, Switzerland,
September 2003.
373
[7] S. Maskey and J. Hirschberg, ?Compar-
ing lexial, acoustic/prosodic, discourse and
structural features for speech summariza-
tion,? in Proceedings of the 9th European
Conference on Speech Communication and
Technology, Lisbon, Portugal, September
2005.
[8] K. Ohtake, K. Yamamoto, Y. Toma, S. Sado,
S. Masuyama, and S. Nakagawa, ?Newscast
speech summarization via sentence shorten-
ing based on prosodic features,? in Proceed-
ings of the ISCA and IEEE Workshop on
Spontaneous Speech Processing and Recog-
nition, Tokyo, Japan, April 2003,.
[9] K. Zechner, ?Automatic summarization of
open-domain multiparty dialogues in diverse
genres,? Computational Linguistics, vol. 28,
no. 4, pp. 447?485, 2002.
[10] Y. Gong and X. Liu, ?Generic text sum-
marization using relevance measure and la-
tent semantic analysis,? in Proceedings of
the 24th Annual International ACM SI-
GIR Conference on Research and Develop-
ment in Information Retrieval, New Orleans,
Louisiana, USA, September 2001, pp. 19?25.
[11] J. Steinberger and K. Jez?ek, ?Using latent
semantic analysis in text summarization and
summary evaluation,? in Proceedings of ISIM
2004, Roznov pod Radhostem, Czech Repub-
lic, April 2004, pp. 93?100.
[12] P. Foltz, W. Kintsch, and T. Landauer, ?The
measurement of textual coherence with la-
tent semantic analysis,? Discourse Processes,
vol. 25, 1998.
[13] B. Hachey, G. Murray, and D. Reitter, ?The
embra system at duc 2005: Query-oriented
multi-document summarization with a very
large latent semantic space,? in Proceedings
of the Document Understanding Conference
(DUC) 2005, Vancouver, BC, Canada, Octo-
ber 2005.
[14] E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, ,
and H. Carvey, ?The ICSI meeting recorder
dialog act (MRDA) corpus,? in Proceedings
of the 5th SIGdial Workshop on Discourse
and Dialogue, Cambridge, MA, USA, April-
May 2004, pp. 97?100.
[15] T. Hain, J. Dines, G. Garau, M. Karafiat,
D. Moore, V. Wan, R. Ordelman,
I.Mc.Cowan, J.Vepa, and S.Renals, ?An
investigation into transcription of conference
room meetings,? Proceedings of the 9th
European Conference on Speech Commu-
nication and Technology, Lisbon, Portugal,
September 2005.
[16] A. Nenkova and B. Passonneau, ?Evaluat-
ing content selection in summarization: The
pyramid method,? in Proceedings of HLT-
NAACL 2004, Boston, MA, USA, May 2004.
[17] R. Valenza, T. Robinson, M. Hickey, and
R. Tucker, ?Summarization of spoken audio
through information extraction,? in Proceed-
ings of the ESCA Workshop on Accessing In-
formation in Spoken Audio, Cambridge UK,
April 1999, pp. 111?116.
[18] J. Carletta, S. Ashby, S. Bourban, M. Flynn,
M. Guillemot, T. Hain, J. Kadlec,
V. Karaiskos, W. Kraaij, M. Kronen-
thal, G. Lathoud, M. Lincoln, A. Lisowska,
I. McCowan, W. Post, D. Reidsma, and
P. Wellner, ?The AMI meeting corpus:
A pre-announcement,? in Proceedings of
MLMI 2005, Edinburgh, UK, June 2005.
374
NAACL HLT Demonstration Program, pages 9?10,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
Automatic Segmentation and Summarization of Meeting Speech
Gabriel Murray, Pei-Yun Hsueh, Simon Tucker
Jonathan Kilgour, Jean Carletta, Johanna Moore, Steve Renals
University of Edinburgh
Edinburgh, Scotland
fgabriel.murray,p.hsuehg@ed.ac.uk
1 Introduction
AMI Meeting Facilitator is a system that per-
forms topic segmentation and extractive sum-
marisation. It consists of three components: (1)
a segmenter that divides a meeting into a num-
ber of locally coherent segments, (2) a summa-
rizer that selects the most important utterances
from the meeting transcripts. and (3) a com-
pression component that removes the less im-
portant words from each utterance based on the
degree of compression the user specied. The
goal of the AMI Meeting Facilitator is two-fold:
rst, we want to provide sucient visual aids for
users to interpret what is going on in a recorded
meeting; second, we want to support the devel-
opment of downstream information retrieval and
information extraction modules with the infor-
mation about the topics and summaries in meet-
ing segments.
2 Component Description
2.1 Segmentation
The AMI Meeting Segmenter is trained using a
set of 50 meetings that are seperate from the in-
put meeting. We rst extract features from the
audio and video recording of the input meeting
in order to train the Maximum Entropy (Max-
Ent) models for classifying topic boundaries and
non-topic boundaries. Then we test each utter-
ance in the input meeting on the Segmenter to
see if it is a topic boundary or not. The features
we use include the following ve categories: (1)
Conversational Feature: These include a set
of seven conversational features, including the
amount of overlapping speech, the amount of
silence between speaker segments, the level of
similarity of speaker activity, the number of cue
words, and the predictions of LCSEG (i.e., the
lexical cohesion statistics, the estimated poste-
rior probability, the predicted class). (2) Lex-
ical Feature: Each spurt is represented as a
vector space of uni-grams, wherein a vector is 1
or 0 depending on whether the cue word appears
in the spurt. (3) Prosodic Feature: These
include dialogue-act (DA) rate-of-speech, max-
imum F0 of the DA, mean energy of the DA,
amount of silence in the DA, precedent and sub-
sequent pauses, and duration of the DA. (4)
Motion Feature: These include the average
magnitude of speaker movements, which is mea-
sured by the number of pixels changed, over the
frames of 40 ms within the spurt. (5) Contex-
tual Feature: These include the dialogue act
types and the speaker role (e.g., project man-
ager, marketing expert). In the dialogue act an-
notations, each dialogue act is classied as one
of the 15 types.
2.2 Summarization
The AMI summarizer is trained using a set of
98 scenario meetings. We train a support vec-
tor machine (SVM) on these meetings, using 26
features relating to the following categories: (1)
Prosodic Features: These include dialogue-
act (DA) rate-of-speech, maximum F0 of the
DA, mean energy of the DA, amount of silence
in the DA, precedent and subsequent pauses,
9
and duration of the DA. (2) Speaker Fea-
tures: These features relate to how dominant
the speaker is in the meeting as a whole, and
they include percentage of the total dialogue
acts which each speaker utters, percentage of
total words which speaker utters, and amount
of time in meeting that each person is speak-
ing. (3) Structural Features: These features
include the DA position in the meeting, and the
DA position in the speaker's turn. (4) Term
Weighting Features: We use two types of
term weighting: tf.idf, which is based on words
that are frequent in the meeting but rare across
a set of other meetings or documents, and a sec-
ond weighting feature which relates to how word
usage varies between the four meeting partici-
pants.
After training the SVM, we test on each meet-
ing of the 20 meeting test set in turn, ranking
the dialogue acts from most probable to least
probable in terms of being extract-worthy. Such
a ranking allows the user to create a summary
of whatever length she desires.
2.3 Compression
Each dialogue act has its constituent words
scored using tf.idf, and as the user compresses
the meeting to a greater degree the browser
gradually removes the less important words from
each dialogue act, leaving only the most infor-
mative material of the meeting.
3 Related Work
Previous work has explored the eect of lexi-
cal cohesion and conversational features on char-
acterizing topic boundaries, following Galley et
al.(2003). In previous work, we have also studied
the problem of predicting topic boundaries at
dierent levels of granularity and showed that a
supervised classication approach performs bet-
ter on predicting a coarser level of topic segmen-
tation (Hsueh et al, 2006).
The amount of work being done on speech
summarization has accelerated in recent years.
Maskey and Hirschberg(September 2005) have
explored speech summarization in the domain
of Broadcast News data, nding that combin-
ing prosodic, lexical and structural features yield
the best results. On the ICSI meeting corpus,
Murray et al(September 2005) compared apply-
ing text summarization approaches to feature-
based approaches including prosodic features,
while Galley(2006) used skip-chain Conditional
Random Fields to model pragmatic dependen-
cies between meeting utterances, and ranked
meeting dialogue acts using a combination or
prosodic, lexical, discourse and structural fea-
tures.
4 acknowledgement
This work was supported by the European
Union 6th FWP IST Integrated Project AMI
(Augmented Multi- party Interaction, FP6-
506811)
References
M. Galley, K. McKeown, E. Fosler-Lussier, and
H. Jing. 2003. Discourse segmentation of multi-
party conversation. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics.
M. Galley. 2006. A skip-chain conditional ran-
dom eld for ranking meeting utterances by im-
portance. In Proceedings of EMNLP-06, Sydney,
Australia.
P. Hsueh, J. Moore, and S. Renals. 2006. Automatic
segmentation of multiparty dialogue. In the Pro-
ceedings of the 11th Conference of the European
Chapter of the Association for Computational Lin-
guistics.
S. Maskey and J. Hirschberg. September 2005. Com-
paring lexial, acoustic/prosodic, discourse and
structural features for speech summarization. In
Proceedings of the 9th European Conference on
Speech Communication and Technology, Lisbon,
Portugal.
G. Murray, S. Renals, and J. Carletta. Septem-
ber 2005. Extractive summarization of meeting
recordings. In Proceedings of the 9th European
Conference on Speech Communication and Tech-
nology, Lisbon, Portugal.
10
Animacy Encoding in English: why and how
Annie Zaenen
PARC & Stanford University
3333 Coyote Hill Road
Palo Alto, CA 94304]
zaenen@parc.com
Jean Carletta
HCRC-University of Edinburgh
2, Buccleuch Place
Edinburgh EH8LW, UK
J.Carletta@Edinburgh.ac.uk
Gregory Garretson
Boston University
Program in Applied Linguistics
96 Cummington St.,
Boston, MA 02215
gregory@bu.edu
Joan Bresnan
CSLI-Stanford University
220, Panama Street
Stanford CA 94305
bresnan@stanford.edu
Andrew Koontz-Garboden
CSLI-Stanford University
220, Panama Street
Stanford CA 94305
andrewkg@csli.stanford.edu
Tatiana Nikitina
CSLI-Stanford University
220, Panama Street
Stanford CA 94305
tann@Stanford.edu
M. Catherine O?Connor
Boston University
Program in Applied Linguistics
96 Cummington St.,
Boston, MA 02215
mco@bu.edu
Tom Wasow
CSLI-Stanford University
220, Panama Street
Stanford CA 94305
wasow@csli.stanford.edu
Abstract
We report on two recent medium-scale initiatives
annotating present day English corpora for animacy
distinctions.  We discuss the relevance of animacy for
computational linguistics, specifically generation, the
annotation categories used in the two studies and the
interannotator reliability for one of the studies.
1 Introduction
It has long been known that animacy is an
important category in syntactic and morphological
natural language analysis.  It is less evident that
this dimension should play an important role in
practical natural language processing.  After
reviewing some linguistic facts, we argue that it
does play a role in natural language generation and
translation, describe a schema for the annotation of
animacy distinctions, evaluate the reliability of the
scheme and discuss some results obtained.  We
conclude with some remarks on the importance of
animacy and other accessibility dimensions for the
architecture of generation schemes
2 The animacy dimension in natural
language
The animacy hierarchy is one of the accessibility
scales that are hypothesized to influence the
grammatical prominence that is given to the
realization of entities within a discourse. Three
important scales (sometimes conflated into one,
also called animacy hierarchy in Silverstein, 1976),
are the definiteness, the person and the animacy
hierarchy proper.  We assume these are three
different hierarchies that refer to different aspects
of entity representation within language: the
definiteness dimension is linked to the status of the
entity at a particular point in the discourse, the
person hierarchy depends on the participants
within the discourse, and the animacy status is an
inherent characteristic of the entities referred to.
Each of these aspects, however, orders entities on a
scale that makes them more or less salient or
?accessible? when humans use their language.
The importance of accessibility scales is not
widely recognized in computational treatments of
natural language.  This contrasts with the situation
in linguistics where such scales have been
recognized as playing an important role in the
organization of sentence syntax and discourse.
Even in natural language studies, however, their
importance has been underestimated because the
role of these scales is not always to distinguish
between grammatical and ungrammatical
utterances but often that of distinguishing mainly
between felicitous and infelicitous ones, especially
in languages such as English.
Grammaticality and acceptability
As long as one?s attention is limited to the
dist inct ion between grammatical  and
ungrammatical sentences, the importance of the
animacy hierarchy in particular is mainly relevant
for languages with a richer morphology than
English.   In such languages animacy distinctions
can influence grammaticality of e.g. case-marking
and voice selection.  To give just one example, in
Navaho, a bi-form is used rather than an yi-form
whenever the patient is animate and the agent is
inanimate, whereas the yi-form is used when the
agent is animate and the patient is inanimate as
illustrated in (1) (from Comrie 1989 p. 193).
(1) (a) At???d n?masi yi-d??l?d
      girl      potato burnt
      The girl burnt the potato.
(b) At???d n?masi bi-d??l?d
      girl      potato burnt
      The potato burnt the girl.
Other phenomena discussed in the literature are
agreement limited to animate noun phrases
(Comrie, 1989), overt case-marking of subject
limited to inanimates or overt case-marking of
objects limited to animates (Aissen, 2003, see also
Bossong 1985 and 1991), object agreement in
Bantu languages (see e.g. Woolford, 1999), choice
between direct and inverse voice in Menominee
(Bloomfield, 1962, see also, Trommer, n.d.).
Recent linguistic studies have highlighted the
importance of the category in languages such as
English. For instance the choice between the Saxon
genitive and the of-genitive (Rosenbach, 2002,
2003, O?Connor et al 2004, Leech et al 1994),
between the double NP and the prepositional
dative (Cueni et al work in progress) and between
active and passive (Rosenbach et al 2002, Bock et
al. 1992, McDonald et al 1993) and between
pronominal and full noun reference (Dahl and
Fraurud, 1996, based on Swedish data) have all
been shown to be sensitive to the difference
between animate and inanimate referents for the
arguments with variable realization.  In these cases
the difference between animate and inanimate does
not lead to a difference between a grammatical or
an ungrammatical sentence as in the cases cited in
the previous paragraph but to a difference in
acceptability.
Interaction between animacy and other scales
As mentioned above, the term ?animacy hierarchy?
is used in two ways, one to refer to an ordering
imposed on definiteness, animacy and person and
the other where it refers to animacy proper.  The
reason for this lies in the interaction between the
different factors that determine accessibility.
It is conceptually desirable to distinguish between
animacy and definiteness but in practice it is
frequently the case that a linguistic phenomenon is
conditioned by more than one of these
conceptually independent factors (see e.g. Comrie,
1989 for discussion).  The projects in the context
of which the annotation tasks described here were
performed (see description below, section 5) also
encode some of these interacting factors.  The
LINK-project encodes information status (see
Nissim et al 2004) and the Boston project encodes
definiteness and expression type (i.e., NP form) as
proxies for information status.
3 Animacy as a factor in generation and
translation
As long as animacy was discussed as a relevant
grammatical category in languages that had not
been studied from a computational point of view,
its importance for computational linguistics was
perceived as rather limited.  The fact that it
permeates the choice between constructions in
languages such as English changes this perception.
The category is of obvious importance for high
quality generation and translation.
For instance, if one is faced with the task of
generating a sentence from a three place predicate,
P (a,b,c), and one has the choice of rendering either
b or c as the direct object, knowing that c is human
and b is abstract would lead one to choose c ceteris
paribus.  However, everything else is rarely equal
and the challenge for generation will be to assign
the exact relative weights to factors such as
animacy, person distinction and recency.
Moreover, the importance of these factors needs to
be combined with that of heterogeneous
considerations such as the length of the resulting
expression.
In the context of translation we need also to keep
in mind the possibility that the details of the
animacy ranking might be different from language
to language and that the relative impact of animacy
and other accessibility scale factors might be
different from construction to construction.
4 The Animacy Hierarchy
Given the pervasive importance of animacy
information in human languages one might expect
it to be a well-understood linguistic category.
Nothing could be farther from the truth.  Linguistic
descriptions appeal to a hierarchy that in its
minimal form distinguishes between human, non-
human animate and inanimate but can contain
more distinctions, such as distinctions between
higher and lower animals (see Yamamoto, 1999 for
a particularly elaborated scheme).
What makes it difficult to develop clear categories
of animacy is that the linguistically relevant
distinctions between animate and non-animate and
between human and non-human are not the same
as the biological distinctions.  Part of this research
is devoted to discovering the principles that
underlie the distinctions; and the type of
distinctions proposed depend on the assumptions
that a researcher makes about the underlying
motivation for them, e.g. as a reflection of the
language user?s empathy with living beings (e.g.
Yamamoto, 1999).  What is of particular interest
for natural language processing is the observation
that the distinctions are most likely not the same
across languages (cf. Comrie, 1989) and can even
change over time in a given language. They are
similar to other scalar phenomena such as voicing
onset times that play a role in different languages
but where the categorization into voiced and
unvoiced does not correspond to the same physical
boundary in each language. But whereas voicing
onset times can be physically measured, we do not
have an objective measure of animacy. The
categories involved correspond to the degree to
which various entities are construed as human-like
by a given group of speakers and at this point we
have no language independent measure for this.
Moreover, languages make ample use of metaphor
and metonomy.  The intent of an animacy coding is
to encode the animacy status of the referent of the
linguistic expression. But sometimes in figurative
language it is not clear what the referent it.
Especially prevalent cases of metonomy are the
use of names to refer both to organizations (e.g.
IBM) and to characteristic members of them, and
the use of place names (e.g. Russia) to refer both to
organizational entities and geographical places or
inhabitants of them. Terms belonging to these
semantic classes are systematically ambiguous.
Whereas it is true that animacy can be determined
by looking at the entity an expression refers to, in
practice it is not always clear what the referent of
an expression is.
The notions that the animacy hierarchy appeals to,
then, are not a priori well defined. And work is
necessary on two levels: to better define which
distinctions play a role in English and to determine
where they play a role.  Conceptually, it might be
desirable to replace the idea of a hierarchy with
discrete classes by a partial ordering of entities.
This is, however, not the place to pursue this idea.
Fortunately, one doesn?t need to wait until the first
problem is solved completely to tackle the second.
The results obtained in certain linguistic contexts
are robust for the top and the bottom of the
hierarchy.  Uncertainty about the middle does not
prevent us from establishing the importance of the
dimension as such.  Refining the definition of
animacy will, however, be important for more
detailed studies of the interaction between the
various accessibility hierarchies. This more precise
notion will be needed for cross-linguistic studies,
and, in the context of natural language processing,
for high quality generation and translation.
5 Animacy Annotation
As we have discussed above, the animacy scale is
an important factor in the choice of certain
construction in English. But it is only a soft
constraint and as such outside of the realm of
things that native speakers have clear judgments
about.  The best ways to study such phenomena are
psychological experiments and corpus analysis.
The annotation exercise we engaged in is meant to
facilitate the latter.
Given the situation described with respect to
animacy categories, a natural way to proceed is to
start with a commonsensical approach and see
where it leads.  In 2000-2002, two rather similar
initiatives led to the need for animacy annotations:
one, the paraphrase-project, a collaboration
between Stanford and Edinburgh, concentrating on
the factors that influence the choice between
different sentence level syntactic paraphrases
(Bresnan et al 2002) and another concentrating on
the possessive alternation (O?Connor, 2000).  The
two projects used a very similar animacy
annotation scheme, developed in the context of the
O?Connor project.
The scheme was used in two different ways. The
Boston team coded 20,000 noun phrases in
?possessive? constructions from the Brown Corpus.
The first round of coding was automated, with the
animacy annotation based primarily on word lists
and morphological information. The second round
was performed manually by pairs of coders using a
decision tree. The two coders were required to
agree on each code; every case in which there was
not complete agreement was discussed by the rest
of the team, until a choice of code was made. This
way of annotating does not lend itself to a study of
reliability, except between the automated coder
and the human coders as a group. For more
information on this use of the coding system, see
Garretson & O?Connor (2004).
In what follows we concentrate on the use of the
coding scheme in the Stanford-Edinburgh
paraphrase project.
The overall aim of the paraphrase project is to
provide the community of linguists and
computational linguists with a corpus that can be
used to calculate the impact of the various factors
on different constructions.  The annotation scheme
assumes that the main distinction is three-way:
human, other animates and inanimates, but the two
latter categories are subdivided further as follows:
- Other animates: organizations, animals,
intelligent machines and vehicles.
- Inanimates: concrete inanimate, non-concrete
inanimate, place and time.
The category ?organization? is important because
organizations are often presented as groups of
humans engaging in actions that are typically
associated with humans ( they make
pronouncements, decisions, etc.).  The categories
place and time are especially important for the
possessive encoding as it has often been observed
that some spatial and temporal expressions are
realized as Saxon genitives (see e.g. Rosenbach
(2002)).
For the cases in which no clear decision could be
made, a category ?variable animacy? was invented,
and the coders were also given the option to defer
the decision by marking an item with ?oanim?.
The overall coding scheme, with a summary of the
instructions given to the coders, looks as follows
1
HUMAN
Refers to one or more humans; this includes
imaginary entities that are presented as human,
gods, elves, ghosts, etc.: things that look human
and act like humans.
ORG
This tag was proposed for collectivities of humans
when displaying some degree of group identity.
The properties that are deemed relevant can be
represented by the following implicational
hierarchy:
+/- chartered/official
+/- temporally stable
+/- collective voice/purpose
+/- collective action
+/- collective
The cut-off point between HUMAN and ORG was
put at ?having a collective voice/purpose?: so a
group with collective voice and purpose is deemed
to be an ORG, a group with collective action, such
as a mob, is not an ORG.
                                                       
1
 For a more extensive description of the annotation
scheme see Garretson et al 2004.
ANIMAL
Non-human animates, including viruses and
bacteria.
PLACE
The tag is used for nominals that ?refer to a place
as a place?. There are two different problems with
the delimitation of place. On the one hand, any
location can be a place, e.g. a table, a drawer, a
pinhead, ? The coding scheme takes the view that
only potential locations for humans are thought of
as ?places?.  On the other hand some places can be
thought of as ORGs.  The tag was applied in a
rather restricted way, for instance in a sentence
such as ?my house was built in 1960?, ?my house?
is coded as CONC (see below), whereas in ?I was
at my house?, it would be a PLACE.
TIME
This tag is meant to be applied to expressions
referring to periods of time.  It was applied rather
liberally.
CONCRETE
This tag is restricted to ?prototypical? concrete
objects or substances.  Excluded are things like air,
voice, wind and other intangibles. Body parts are
concrete.
NONCONC
This is the default category.  It is used for events,
and anything else that is not prototypically
concrete but clearly inanimate.
MAC
A minor tag used for intelligent machines, such as
computers or robots.
VEH
Another minor category used for vehicles as it
has been observed that these are treated as living
beings in some linguistic contexts (e.g. pronoun
selection in languages such as English where
normal gender distinctions only apply to animates).
OANIM
This tag is used when the coder is completely
unsure and wants to come back to the example
later.
VANIM
This tag can be used in conjunction with another
one to indicate that the coder is not entirely sure of
the code and thinks there are reasons to give
another code too.
Finally, NOT-UNDERSTOOD was supposed to
be used when the text as a whole was not clear.
Three coders coded the parsed part of the
Switchboard corpus (Godfrey et al 1992) over the
summer of 2003.  The corpus consists of around
600 transcribed dialogues on various
predetermined topics among speakers of American
English.  Before the annotation exercise began, the
dialogues were converted into XML (Carletta et al
2004).  The entities that needed to be annotated
(the NPs and possessives determiners) were
automatically selected and filtered for the coders.
The three coders were undergraduate students at
Stanford University who were paid for the work.
The schema presented above was discussed with
them and presented in the form of a decision tree.
Difficult cases were discussed but eventually each
coder worked independently.  599 dialogues were
annotated.
6 Coding reliability
The reliability of the annotation was evaluated
using the kappa statistic (Carletta, 1996).
Although there are no hard and fast rules about
what makes an acceptable kappa coefficient?it
depends on the use to which the data will be
put?many researchers in the computational
linguistics community have adopted the rule of
thumb that discourse annotation should have a
kappa of at least .8.
For the reliability study, we had three
individuals work separately to code the same four
dialogues with the animacy scheme.  Markables (in
this case NPs and possessives) had been extracted
automatically from the data, leading the coders to
mark around 10% of the overall set with a category
that indicated that they were not proper markables
and therefore not to be coded.  Omitting these
(non-) markables, for the data set overall, K=.92
(k=3, N=1081).
In general, coders did not agree about which cases
were problematic enough to mark as VANIM, and
omitting the markables that any coder marked as
problematic using the VANIM code leads to a
slight improvement (K=.96, k=3,N=1135).
It is important to note that these kappa coefficients
are so high primarily because two categories which
are easy to differentiate from each other, HUMAN
and NONCONC, swamp the rest of the categories.
The cross-coder reliability for them is satisfactory
but the intermediate categories were not defined
well enough to allow reliable coding.
Figure 1 shows the confusion matrix for the data
including markables that any coder marker
additionally as problematic using the VANIM
notation.  Considering the coders one pair at a
time, the matrix records the frequency with which
one coder of the pair chose the category named in
the row header whilst the other chose the category
named in the column header for the same
markable.
Although we were aware of the less than formal
definitions given for the categories, we had hoped
that the coders would share the intuitive
understanding of the developers of the categories.
This is obviously not the case for all categories.
What was also surprising was that allowing coders
to mark cases as problematic using the VANIM
code was not worthwhile, since the coders did not
often take advantage of this option and taking the
VANIM codes into account during analysis has
little effect.
Analysis of the four annotated dialogues points to
several sources for the intercoder disagreement.
? The categories TIME and PLACE were
defined in a way that did not coincide with the
coders? intuitive understanding of them.  The
tag TIME was supposed to refer to ?periods of
time?. This led to some wavering
interpretations for temporal expressions that do
not designate a once-occurring period of time.
For instance ?this time? and ?next time? were
coded as TIME by two coders but as
?NONCONC? by the third one.  Clearer
training on what was meant could have helped
here.
? As mentioned above, the choice
between HUMAN, ORG and NONCONC
depended on how the coders interpreted the
referent of the expression.  Although
guidelines were given about the difference
between HUMAN and ORG (see above), the
cut-off point wasn?t always clear
2
. The
distinction between ORGs as proposed in our
schema and less organized human groups
seems too fluctuating to be useful.
                                                       
2 All coders agreed that Vulcans are HUMAN.
Figure 1
? The vagueness of pronominal
reference: for instance a school as an
organization can be marked as ORG by the
coders but later in the dialogue there is
discussion about the what is done with napping
children in the school and one speaker says ?if
they (the children) fall asleep they kind of let
them sleep?, one coder interpreted that the
second ?they? as simply referring to the school
organization and marked it as ORG, whereas
another interpreted it as referring to a rather
vague group of humans, presumably some
teachers, and marked it as HUMAN.  This
vagueness of reference is quite prevalent in
spoken language, especially with the pronoun
?they?.
? Attention errors, e.g. vehicles were
supposed to get a special code but, presumably
because there were so few, this was sometimes
forgotten.  One coder coded ?a couple of
weeks? as HUMAN.  These kinds of mistakes
are unavoidable and the very tools that make
the encoding easier (e.g. the automatic
advancing from one markable to another)
might make them more frequent.
While the problems with ORG and HUMAN don?t
come as a surprise, the difficulties with PLACE,
TIME and CONCRETE are more surprising.  The
two minor classes, MAC and VEH and the
ANIMAL class occurred so seldom that no
significant results were obtained in this sample.
They were equally rare in the corpus as a whole.
7 Conclusion
We are not aware of any other medium-scale
attempts to annotate corpora of contemporary
English for animacy information apart from the
two mentioned here.  There are smaller efforts
concentrating on the genitive alternation (e.g.
Leech et al, 1994, Anschutz, 1997, Stefanowitsch,
2000)
3
.  The resources that have been created give
robust results for the opposition ?human? versus
?nonconcrete? entities in the large sense (as the
category was used as a catch-all).  This should
suffice for further inquiries about linguistic
processes that are sensitive to a binary opposition
in this dimension.  Moreover the Stanford-
Edinburgh effort is integrated in a corpus that has
already been marked up for syntactic information,
so correlations between syntactic constructions and
animacy (and information status) should be easy to
calculate.  It is also the first effort that studies
inter-annotator reliability.
Some studies based on the annotations are
currently being conducted.  The study by Cueni,
Bresnan, Nikitina and Baayen (2004) supplements
partial data from the work described here with
further annotations.  The work reported by
O?Connor et al (2004) derives from the Boston
use of the encoding described here. Within the
paraphrase project we are currently investigation
                                                       
3 Some related work is done in the context of entity
tracking sponsored by various US government programs
(ACE, TIDES, etc.).  The proposed annotation schemes
have problems in distinguishing between persons and
organizations or geo-political entities that are similar to
ours, but the basic categories and the aims of these
enterprises are different.  We have not reviewed them
here.
concr
ete
hum
an
non-
conc
not-
understood
oan
im
o
rg
pl
ace
ti
me
v
eh
concrete 31 9 19 10 0 0 5 0 1
human 148
9
27 11 0 3
3
0 4 0
nonconc 1256 3 3 3
3
23 5
3
1
notundersto
od
0 0 0 0 0 0
oanim 2 2 0 1 0
org 9
1
0 0 0
place 66 0 2
time 7
0
0
veh 3
the possible effect of animacy on constructions
such as Left-Dislocation and Topicalization.
Further work remains to be done, however, to
determine the exact nature of the distinctions in the
animacy dimension that are important for English
and for other languages. The annotations we
provide do not settle this issue. In that sense they
are insufficient to guide generation and translation
precisely. To investigate this further we will need
to devise more careful annotation schemes and
approach the problem via experiments where the
hypothesized relative animacy of various entities
can be carefully controlled. As mentioned above, it
might be better not to think in terms of robust large
categories but rather try to rank specific entities or
small categories relative to each other and to
gradually build up a more precise picture.  This is
most likely better done through controlled
experiments than through corpus annotation.
The annotated corpus, however, will be helpful to
determine where animacy plays a role and which
other factors it interacts with. This knowledge will
help devise more adequate generation and
translation architectures.
The Boston University noun Phrase Corpus is
publicly available at http://np.corpus.bu.edu.  The
paraphrase corpus will be made available to
subscribers to the Switchboard Corpus.
8 Acknowledgements
This work was in part supported by a Scottish
Enterprise Edinburgh-Stanford Link Grant
(265000-3102-R36766) and by NSF grant BCS-
0080377.  Thanks to Toni Jeanine Harris, Rebecca
Regos and Anna Cueni for the encoding work and
to Richard Crouch and Neal Snider for comments
and help. The usual disclaimers obtain.
References
Aissen, Judith, 2002. Differential object marking:
Iconicity vs. economy. NLLT, 21 435-483.
Anschutz, A. 1997. "How to Choose a Possessive
Noun Phrase Construction in Four Easy
Steps." Studies in Language 21, 1, 1-35.
Bock, J. K., Loebell, H. and Morey, R. (1992)
From conceptual roles to structural relations:
Bridging the syntactic cleft.  Psychological
Review 99: 150--171.
Bossong, Georg. 1985. D i f f e r e n t i e l l e
Objektmarkierung in den Neuiranischen
Sprachen,Gunter Narr Verlag, T?bingen.
Bossong, Georg. 1991. 'Differential Object
Marking in Romance and Beyond', in D.
Wanner and D.Kibbee (eds.), New Analyses in
Romance Linguistics: Selected Papers from
theXVIII Linguistic Symposium on Romance
Languages, John Benjamins, Amsterdam, pp.
143-170.
Bresnan, Joan, Dingare, Shipra, and Manning,
Christopher D. 2001.   Soft Constraints Mirror
Hard Constraints: Voice and Person in English
and Lummi.  Proceedings of the LFG '01
Conference. CSLI Online. 20 pages.
Bresnan, Joan, Carletta, Jean, Crouch, Richard,
Nissim, Malvina, Steedman, Mark, Wasow,
Tom and  Zaenen, Annie. 2002. Paraphrase
analysis for improved generation, LINK
project, HCRC Edinburgh-CLSI Stanford.
Carletta, Jean. 1996. Assessing agreement on
classification task: the kappa statstic.
Computational linguistics, 22 (2): 249-254
Carletta, Jean, Shipra Dingare, Malvina Nissim,
and Tatiana Nikitina. 2004. Using the NITE
XML Toolkit on the Switchboard Corpus to
study syntactic choice: a case study. In
Proceedings of the 4th International
Conference on Language Resources and
Evaluation (LREC2004), Lisbon, May 2004.
Comrie, Bernard, 1989, Language Universals and
Linguistic Typology, The University of Chicago
Press
Cueni, Anna, Joan Bresnan, Tatiana Nikitina and Harald
Baayen. 2004. Predicting the Dative Alteration,
Stanford University, ms. in preparation.
Dahl, ?sten and  Fraurud, Kari. 1996 Animacy in
Grammar and Discourse. In Thorstein
Fretheim & Jeanette K. Gundel (eds.),
Reference and Referent Accessibility, 47-64.
Amsterdam/Philadelphia: John Benjamins.
Garretson, Gregory, O'Connor, M. Catherine;
Skarabela, Barbora; & Hogan, Marjorie.
March 2004. Coding practices used in the
project Optimal Typology of Determiner
Phrases.
http://npcorpus.bu.edu/documentation/index.ht
ml
Garretson, Gregory, and O?Connor, M. Catherine.
2004. A combined automatic-and-manual
method for studying discourse features in
corpora. Paper to be presented at the Fifth
North American Symposium on Corpus
Linguistics,  May 21-23, Montclair State
University, NJ.
Godfrey, J. Holliman, E. & McDaniel J. 1992
SWITCHBOARD: Telephone speech corpus
for research and development. Proceedings of
ICASSP-92, 517-520.
Leech, G., B. Francis, et al (1994). The Use of
Computer Corpora in the Textual
Demonstrability of Gradience in Linguistic
Categories. Continuity In Linguistic Semantics.
C. Fuchs and B. Victorri. Amsterdam, John
Benjamins Publishing Company: 57-76.
McDonald, J.L., Bock, K. and M. Kelly. 1993.
Word and world order: semantic, phonological,
and metrical determinants of serial position.
Cognitive Psychology 25: 188-230.
Nissum, Malvina, Shipra Dingare, Jean Carletta
and Mark Steedman. 2004. An annotation
scheme for information status in dialogue.
Submitted to LREC 2004
O?Connor, M. Catherine. 2000. Optimality
typology of the DP: Markedness within the
nominal. NSP-grant. BCS-0080377.
O?Connor, M. Catherine., Anttila, Arto, Fong,
Vivienne and   Maling, Joan (2004).
Differential possessor expression in English:
Re-evaluating animacy and topicality effects.
Paper presented at the Annual Meeting of the
Linguistic Society of America, January 9-11,
Boston, MA.
Rosenbach, Anette (2002) Genitive Variation in
English. Conceptual Factors in Synchronic
and Diachronic Studies. Berlin/New York:
Walter de Gruyter.)
Rosenbach, Anette. 2003. Aspects of iconicity and
economy in the choice between the s-genitive
and the of-genitive in English. In: G?nter
Rohdenburg & Britta Mondorf (eds.).
Determinants of Grammatical Variation in
English Berlin/New York: de Gruyter.
Rosenbach, A., J. Aissen, and J. Bresnan. 2002.
Pilot study of the influence of animacy on
subject choice in a reading task.  (Heinrich
Heine University, UCSC, and Stanford)
Silverstein, Michael. 1976. Hierarchy of features
and ergativity. In Richard Dixon, editor,
Grammatical Categories in Australian
Languages. Australian Institute of Aboriginal
Studies.
Stefanowitsch, Anatol. 2000. Constructional
semantics as a limit to grammatical alternation:
The two genitives of English. in CLEAR
(Cognit ive Linguistics: Explorations,
Applications, Research), 3.
Trommer, Jochen. s.d. Direction Marking and Case
in Menominee,  http:/ /www.ling.uni-
osnabrueck.de/trommer/nim.pdf
Woolford, Ellen. 1999. Animacy hierarchy effects
on object agreement, in Paul Kotey ed. New
Dimensions in African Linguistics and
Languages (Trends in African Linguistics 3),
203-216.
Yamamoto, Mutsumi, 1999, Animacy and
Reference: a cognitive approach to corpus
linguistics , John Benjamins.
Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation
and/or Summarization, pages 33?40, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Evaluating Automatic Summaries of Meeting Recordings
Gabriel Murray
Centre for Speech Technology Research
University of Edinburgh
Edinburgh, United Kingdom
Steve Renals
Centre for Speech Technology Research
University of Edinburgh
Edinburgh, United Kingdom
Jean Carletta
Human Communication Research Centre
University of Edinburgh
Edinburgh, United Kingdom
Johanna Moore
Human Communication Research Centre
University of Edinburgh
Edinburgh, United Kingdom
Abstract
The research below explores schemes for
evaluating automatic summaries of busi-
ness meetings, using the ICSI Meeting
Corpus (Janin et al, 2003). Both au-
tomatic and subjective evaluations were
carried out, with a central interest be-
ing whether or not the two types of eval-
uations correlate with each other. The
evaluation metrics were used to compare
and contrast differing approaches to au-
tomatic summarization, the deterioration
of summary quality on ASR output ver-
sus manual transcripts, and to determine
whether manual extracts are rated signifi-
cantly higher than automatic extracts.
1 Introduction
In the field of automatic summarization, it is widely
agreed upon that more attention needs to be paid
to the development of standardized approaches to
summarization evaluation. For example, the cur-
rent incarnation of the Document Understanding
Conference is putting its main focus on the de-
velopment of evaluation schemes, including semi-
automatic approaches to evaluation. One semi-
automatic approach to evaluation is ROUGE (Lin
and Hovy, 2003), which is primarily based on n-
gram co-occurrence between automatic and human
summaries. A key question of the research con-
tained herein is how well ROUGE correlates with
human judgments of summaries within the domain
of meeting speech. If it is determined that the two
types of evaluations correlate strongly, then ROUGE
will likely be a valuable and robust evaluation tool in
the development stage of a summarization system,
when the cost of frequent human evaluations would
be prohibitive.
Three basic approaches to summarization are
evaluated and compared below: Maximal Marginal
Relevance, Latent Semantic Analysis, and feature-
based classification. The other major comparisons
in this paper are between summaries on ASR ver-
sus manual transcripts, and between manual and au-
tomatic extracts. For example, regarding the for-
mer, it might be expected that summaries on ASR
transcripts would be rated lower than summaries on
manual transcripts, due to speech recognition errors.
Regarding the comparison of manual and automatic
extracts, the manual extracts can be thought of as
a gold standard for the extraction task, represent-
ing the performance ceiling that the automatic ap-
proaches are aiming for.
More detailed descriptions of the summarization
approaches and experimental setup can be found in
(Murray et al, 2005). That work relied solely on
ROUGE as an evaluation metric, and this paper pro-
ceeds to investigate whether ROUGE alone is a reli-
able metric for our summarization domain, by com-
paring the automatic scores with recently-gathered
human evaluations. Also, it should be noted that
while we are at the moment only utilizing intrinsic
evaluation methods, our ultimate plan is to evalu-
ate these meeting summaries extrinsically within the
context of a meeting browser (Wellner et al, 2005).
33
2 Description of the Summarization
Approaches
2.1 Maximal Marginal Relevance (MMR)
MMR (Carbonell and Goldstein, 1998) uses the
vector-space model of text retrieval and is particu-
larly applicable to query-based and multi-document
summarization. The MMR algorithm chooses
sentences via a weighted combination of query-
relevance and redundancy scores, both derived using
cosine similarity. The MMR score ScMMR(i)for a
given sentence Si in the document is given by
ScMMR(i) =
?(Sim(Si, D))? (1? ?)(Sim(Si, Summ)) ,
where D is the average document vector, Summ
is the average vector from the set of sentences al-
ready selected, and ? trades off between relevance
and redundancy. Sim is the cosine similarity be-
tween two documents.
This implementation of MMR uses lambda an-
nealing so that relevance is emphasized while the
summary is still short and minimizing redundancy is
prioritized more highly as the summary lengthens.
2.2 Latent Semantic Analysis (LSA)
LSA is a vector-space approach which involves pro-
jecting the original term-document matrix to a re-
duced dimension representation. It is based on the
singular value decomposition (SVD) of an m ? n
term-document matrix A, whose elements Aij rep-
resent the weighted term frequency of term i in doc-
ument j. In SVD, the term-document matrix is de-
composed as follows:
A = USV T
where U is an m?n matrix of left-singular vectors,
S is an n ? n diagonal matrix of singular values,
and V is the n ? n matrix of right-singular vectors.
The rows of V T may be regarded as defining top-
ics, with the columns representing sentences from
the document. Following Gong and Liu (Gong and
Liu, 2001), summarization proceeds by choosing,
for each row in V T , the sentence with the highest
value. This process continues until the desired sum-
mary length is reached.
Two drawbacks of this method are that dimen-
sionality is tied to summary length and that good
sentence candidates may not be chosen if they do
not ?win? in any dimension (Steinberger and Jez?ek,
2004). The authors in (Steinberger and Jez?ek, 2004)
found one solution, by extracting a single LSA-
based sentence score, with variable dimensionality
reduction.
We address the same concerns, following the
Gong and Liu approach, but rather than extracting
the best sentence for each topic, the n best sentences
are extracted, with n determined by the correspond-
ing singular values from matrix S. The number of
sentences in the summary that will come from the
first topic is determined by the percentage that the
largest singular value represents out of the sum of all
singular values, and so on for each topic. Thus, di-
mensionality reduction is no longer tied to summary
length and more than one sentence per topic can be
chosen. Using this method, the level of dimension-
ality reduction is essentially learned from the data.
2.3 Feature-Based Approaches
Feature-based classification approaches have been
widely used in text and speech summarization, with
positive results (Kupiec et al, 1995). In this work
we combined textual and prosodic features, using
Gaussian mixture models for the extracted and non-
extracted classes. The prosodic features were the
mean and standard deviation of F0, energy, and du-
ration, all estimated and normalized at the word-
level, then averaged over the utterance. The two lex-
ical features were both TFIDF-based: the average
and the maximum TFIDF score for the utterance.
For our second feature-based approach, we de-
rived single LSA-based sentence scores (Steinberger
and Jez?ek, 2004) to complement the six features de-
scribed above, to determine whether such an LSA
sentence score is beneficial in determining sentence
importance. We reduced the original term-document
matrix to 300 dimensions; however, Steinberger and
Jez?ek found the greatest success in their work by re-
ducing to a single dimension (Steinberger, personal
communication). The LSA sentence score was ob-
tained using:
ScLSAi =
?
?
?
?
n?
k=1
v(i, k)2 ? ?(k)2 ,
34
where v(i, k) is the kth element of the ith sentence
vector and ?(k) is the corresponding singular value.
3 Experimental Setup
We used human summaries of the ICSI Meeting cor-
pus for evaluation and for training the feature-based
approaches. An evaluation set of six meetings was
defined and multiple human summaries were created
for these meetings, with each test meeting having ei-
ther three or four manual summaries. The remaining
meetings were regarded as training data and a single
human summary was created for these. Our sum-
maries were created as follows.
Annotators were given access to a graphical user
interface (GUI) for browsing an individual meeting
that included earlier human annotations: an ortho-
graphic transcription time-synchronized with the au-
dio, and a topic segmentation based on a shallow hi-
erarchical decomposition with keyword-based text
labels describing each topic segment. The annota-
tors were told to construct a textual summary of the
meeting aimed at someone who is interested in the
research being carried out, such as a researcher who
does similar work elsewhere, using four headings:
? general abstract: ?why are they meeting and
what do they talk about??;
? decisions made by the group;
? progress and achievements;
? problems described
The annotators were given a 200 word limit for each
heading, and told that there must be text for the gen-
eral abstract, but that the other headings may have
null annotations for some meetings.
Immediately after authoring a textual summary,
annotators were asked to create an extractive sum-
mary, using a different GUI. This GUI showed
both their textual summary and the orthographic
transcription, without topic segmentation but with
one line per dialogue act based on the pre-existing
MRDA coding (Shriberg et al, 2004) (The dialogue
act categories themselves were not displayed, just
the segmentation). Annotators were told to extract
dialogue acts that together would convey the infor-
mation in the textual summary, and could be used to
support the correctness of that summary. They were
given no specific instructions about the number or
percentage of acts to extract or about redundant dia-
logue act. For each dialogue act extracted, they were
then required in a second pass to choose the sen-
tences from the textual summary supported by the
dialogue act, creating a many-to-many mapping be-
tween the recording and the textual summary.
The MMR and LSA approaches are both unsuper-
vised and do not require labelled training data. For
both feature-based approaches, the GMM classifiers
were trained on a subset of the training data repre-
senting approximately 20 hours of meetings.
We performed summarization using both the hu-
man transcripts and speech recognizer output. The
speech recognizer output was created using base-
line acoustic models created using a training set
consisting of 300 hours of conversational telephone
speech from the Switchboard and Callhome cor-
pora. The resultant models (cross-word triphones
trained on conversational side based cepstral mean
normalised PLP features) were then MAP adapted
to the meeting domain using the ICSI corpus (Hain
et al, 2005). A trigram language model was em-
ployed. Fair recognition output for the whole corpus
was obtained by dividing the corpus into four parts,
and employing a leave one out procedure (training
the acoustic and language models on three parts of
the corpus and testing on the fourth, rotating to ob-
tain recognition results for the full corpus). This
resulted in an average word error rate (WER) of
29.5%. Automatic segmentation into dialogue acts
or sentence boundaries was not performed: the dia-
logue act boundaries for the manual transcripts were
mapped on to the speech recognition output.
3.1 Description of the Evaluation Schemes
A particular interest in our research is how automatic
measures of informativeness correlate with human
judgments on the same criteria. During the devel-
opment stage of a summarization system it is not
feasible to employ many hours of manual evalua-
tions, and so a critical issue is whether or not soft-
ware packages such as ROUGE are able to measure
informativeness in a way that correlates with subjec-
tive summarization evaluations.
35
3.1.1 ROUGE
Gauging informativeness has been the focus
of automatic summarization evaluation research.
We used the ROUGE evaluation approach (Lin
and Hovy, 2003), which is based on n-gram co-
occurrence between machine summaries and ?ideal?
human summaries. ROUGE is currently the stan-
dard objective evaluation measure for the Document
Understanding Conference 1; ROUGE does not as-
sume that there is a single ?gold standard? summary.
Instead it operates by matching the target summary
against a set of reference summaries. ROUGE-1
through ROUGE-4 are simple n-gram co-occurrence
measures, which check whether each n-gram in the
reference summary is contained in the machine sum-
mary. ROUGE-L and ROUGE-W are measures of
common subsequences shared between two sum-
maries, with ROUGE-W favoring contiguous com-
mon subsequences. Lin (Lin and Hovy, 2003) has
found that ROUGE-1 and ROUGE-2 correlate well
with human judgments.
3.1.2 Human Evalautions
The subjective evaluation portion of our research
utilized 5 judges who had little or no familiarity with
the content of the ICSI meetings. Each judge eval-
uated 10 summaries per meeting, for a total of sixty
summaries. In order to familiarize themselves with
a given meeting, they were provided with a human
abstract of the meeting and the full transcript of the
meeting with links to the audio. The human judges
were instructed to read the abstract, and to consult
the full transcript and audio as needed, with the en-
tire familiarization stage not to exceed 20 minutes.
The judges were presented with 12 questions at
the end of each summary, and were instructed that
upon beginning the questionnaire they should not re-
consult the summary itself. 6 of the questions re-
garded informativeness and 6 involved readability
and coherence, though our current research concen-
trates on the informativeness evaluations. The eval-
uations used a Likert scale based on agreement or
disagreement with statements, such as the following
Informativeness statements:
1. The important points of the meeting are repre-
sented in the summary.
1http://duc.nist.gov/
2. The summary avoids redundancy.
3. The summary sentences on average seem rele-
vant.
4. The relationship between the importance of
each topic and the amount of summary space
given to that topic seems appropriate.
5. The summary is repetitive.
6. The summary contains unnecessary informa-
tion.
Statements such as 2 and 5 above are measuring
the same impressions, with the polarity of the state-
ments merely reversed, in order to better gauge the
reliability of the answers. The readability/coherence
portion consisted of the following statements:
1. It is generally easy to tell whom or what is be-
ing referred to in the summary.
2. The summary has good continuity, i.e. the sen-
tences seem to join smoothly from one to an-
other.
3. The individual sentences on average are clear
and well-formed.
4. The summary seems disjointed.
5. The summary is incoherent.
6. On average, individual sentences are poorly
constructed.
It was not possible in this paper to gauge how
responses to these readability statements correlate
with automatic metrics, for the reason that auto-
matic metrics of readability and coherence have not
been widely discussed in the field of summariza-
tion. Though subjective evaluations of summaries
are often divided into informativeness and readabil-
ity questions, only automatic metrics of informative-
ness have been investigated in-depth by the summa-
rization community. We believe that the develop-
ment of automatic metrics for coherence and read-
ability should be a high priority for researchers in
summarization evaluation and plan on pursuing this
avenue of research. For example, work on coher-
ence in NLG (Lapata, 2003) could potentially in-
form summarization evaluation. Mani (Mani et al,
36
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 1  2  3  41=FB1, 2=LSA, 3=MMR, 4=FB2
ROUGE-1-MANROUGE-2-MANROUGE-L-MANROUGE-1-ASRROUGE-2-ASRROUGE-L-ASR
Figure 1: ROUGE Scores for the Summarization Ap-
proaches
1999) is one of the few papers to have discussed
measuring summary readability automatically.
4 Results
The results of these experiments can be analyzed
in various ways: significant differences of ROUGE
results across summarization approaches, deterio-
ration of ROUGE results on ASR versus manual
transcripts, significant differences of human eval-
uations across summarization approaches, deterio-
ration of human evaluations on ASR versus man-
ual transcripts, and finally, the correlation between
ROUGE and human evaluations.
4.1 ROUGE results across summarization
approaches
All of the machine summaries were 10% of the orig-
inal document length, in terms of the number of di-
alogue acts contained. Of the four approaches to
summarization used herein, the latent semantic anal-
ysis method performed the best on every meeting
tested for every ROUGE measure with the excep-
tion of ROUGE-3 and ROUGE-4. This approach
was significantly better than either feature-based ap-
proach (p<0.05), but was not a significant improve-
ment over MMR. For ROUGE-3 and ROUGE-4,
none of the summarization approaches were signifi-
cantly different from each other, owing to data spar-
sity. Figure 1 gives the ROUGE-1, ROUGE-2 and
ROUGE-L results for each of the summarization ap-
proaches, on both manual and ASR transcripts.
4.1.1 ASR versus Manual
The results of the four summarization approaches
on ASR output were much the same, with LSA and
MMR being comparable to each other, and each of
them outperforming the feature-based approaches.
On ASR output, LSA again consistently performed
the best.
Interestingly, though the LSA approach scored
higher when using manual transcripts than when
using ASR transcripts, the difference was small and
insignificant despite the nearly 30% WER of the
ASR. All of the summarization approaches showed
minimal deterioration when used on ASR output
as compared to manual transcripts, but the LSA
approach seemed particularly resilient, as evidenced
by Figure 1. One reason for the relatively small
impact of ASR output on summarization results is
that for each of the 6 meetings, the WER of the
summaries was lower than the WER of the meeting
as a whole. Similarly, Valenza et al(Valenza et
al., 1999) and Zechner and Waibel (Zechner and
Waibel, 2000) both observed that the WER of
extracted summaries was significantly lower than
the overall WER in the case of broadcast news. The
table below demonstrates the discrepancy between
summary WER and meeting WER for the six
meetings used in this research.
Meeting Summary WER Meeting WER
Bed004 27.0 35.7
Bed009 28.3 39.8
Bed016 39.6 49.8
Bmr005 23.9 36.1
Bmr019 28.0 36.5
Bro018 25.9 35.6
WER% for Summaries and Meetings
There was no improvement in the second feature-
based approach (adding an LSA sentence score) as
compared with the first feature-based approach. The
sentence score used here relied on a reduction to 300
dimensions, which may not have been ideal for this
data.
The similarity between the MMR and LSA ap-
proaches here mirrors Gong and Liu?s findings, giv-
ing credence to the claim that LSA maximizes rele-
vance and minimizes redundancy, in a different and
more opaque manner then MMR, but with similar
37
STATEMENT FB1 LSA MMR FB2
IMPORT. POINTS 5.03 4.53 4.67 4.83
NO REDUN. 4.33 2.60 3.00 3.77
RELEVANT 4.83 4.07 4.33 4.53
TOPIC SPACE 4.43 3.83 3.87 4.30
REPETITIVE 3.37 4.70 4.60 3.83
UNNEC. INFO. 4.70 6.00 5.83 5.00
Table 1: Human Scores for 4 Approaches on Manual
Transcripts
results. Regardless of whether or not the singular
vectors of V T can rightly be thought of as topics or
concepts (a seemingly strong claim), the LSA ap-
proach was as successful as the more popular MMR
algorithm.
4.2 Human results across summarization
approaches
Table 1 presents average ratings for the six state-
ments across four summarization approaches on
manual transcripts. Interestingly, the first feature-
based approach is given the highest marks on each
criterion. For statements 2, 5 and 6 FB1 is signif-
icantly better than the other approaches. It is par-
ticularly surprising that FB1 would score well on
statement 2, which concerns redundancy, given that
MMR and LSA explicitly aim to reduce redundancy
while the feature-based approaches are merely clas-
sifying utterances as relevant or not. The second
feature-based approach was not significantly worse
than the first on this score.
Considering the difficult task of evaluating ten ex-
tractive summaries per meeting, we are quite satis-
fied with the consistency of the human judges. For
example, statements that were merely reworded ver-
sions of other statements were given consistent rat-
ings. It was also the case that, with the exception
of evaluating the sixth statement, judges were able
to tell that the manual extracts were superior to the
automatic approaches.
4.2.1 ASR versus Manual
Table 2 presents average ratings for the six state-
ments across four summarization approaches on
ASR transcripts. The LSA and MMR approaches
performed better in terms of having less deteri-
STATEMENT FB1 LSA MMR FB2
IMPORT. POINTS 3.53 4.13 3.73 3.50
NO REDUN. 3.40 2.97 2.63 3.57
RELEVANT 3.47 3.57 3.00 3.47
TOPIC SPACE 3.27 3.33 3.00 3.20
REPETITIVE 4.43 4.73 4.70 4.20
UNNEC. INFO. 5.37 6.00 6.00 5.33
Table 2: Human Scores for 4 Approaches on ASR
Transcripts
 0
 1
 2
 3
 4
 5
 6
 7
 1  2  3  41=FB1, 2=LSA, 3=MMR, 4=FB2
HUMAN-1-MANHUMAN-1-ASR
Figure 2: INFORMATIVENESS-1 Scores for the
Summarization Approaches
oration of scores when used on ASR output in-
stead of manual transcripts. LSA-ASR was not
significantly worse than LSA on any of the 6 rat-
ings. MMR-ASR was significantly worse than
MMR on only 3 of the 6. In contrast, FB1-
ASR was significantly worse than FB1 for 5 of
the 6 approaches, reinforcing the point that MMR
and LSA seem to favor extracting utterances with
fewer errors. Figures 2, 3 and 4 depict the
how the ASR and manual approaches affect the
INFORMATIVENESS-1, INFORMATIVENESS-4
and INFORMATIVENESS-6 ratings, respectively.
Note that for Figure 6, a higher score is a worse rat-
ing.
4.3 ROUGE and Human correlations
According to (Lin and Hovy, 2003), ROUGE-
1 correlates particularly well with human judg-
ments of informativeness. In the human eval-
uation survey discussed here, the first statement
(INFORMATIVENESS-1) would be expected to
correlate most highly with ROUGE-1, as it is ask-
38
 0
 1
 2
 3
 4
 5
 6
 7
 1  2  3  41=FB1, 2=LSA, 3=MMR, 4=FB2
HUMAN-4-MANHUMAN-4-ASR
Figure 3: INFORMATIVENESS-4 Scores for the
Summarization Approaches
 3
 3.5
 4
 4.5
 5
 5.5
 6
 6.5
 7
 1  2  3  41=FB1, 2=LSA, 3=MMR, 4=FB2
HUMAN-6-MANHUMAN-6-ASR
Figure 4: INFORMATIVENESS-6 Scores for the
Summarization Approaches
ing whether the summary contains the important
points of the meeting. As could be guessed from the
discussion above, there is no significant correlation
between ROUGE-1 and human evaluations when
analyzing only the 4 summarization approaches
on manual transcripts. However, when looking
at the 4 approaches on ASR output, ROUGE-1
and INFORMATIVENESS-1 have a moderate and
significant positive correlation (Spearman?s rho =
0.500, p < 0.05). This correlation on ASR out-
put is strong enough that when ROUGE-1 and
INFORMATIVENESS-1 scores are tested for corre-
lation across all 8 summarization approaches, there
is a significant positive correlation (Spearman?s rho
= 0.388, p < 0.05).
The other significant correlations for ROUGE-
1 across all 8 summarization approaches are with
INFORMATIVENESS-2, INFORMATIVENESS-5
and INFORMATIVENESS-6. However, these are
negative correlations. For example, with regard to
INFORMATIVENESS-2, summaries that are rated
as having a high level of redundancy are given high
ROUGE-1 scores, and summaries with little redun-
dancy are given low ROUGE-1 scores. Similary,
with regard to INFORMATIVENESS-6, summaries
that are said to have a great deal of unnecessary in-
formation are given high ROUGE-1 scores. It is
difficult to interpret some of these negative correla-
tions, as ROUGE does not measure redundancy and
would not necessarily be expected to correlate with
redundancy evaluations.
5 Discussion
In general, ROUGE did not correlate well with the
human evaluations for this data. The MMR and
LSA approaches were deemed to be significantly
better than the feature-based approaches according
to ROUGE, while these findings were reversed ac-
cording to the human evaluations. An area of agree-
ment, however, is that the LSA-ASR and MMR-
ASR approaches have a small and insignificant de-
cline in scores compared with the decline of scores
for the feature-based approaches. One of the most
interesting findings of this research is that MMR and
LSA approaches used on ASR tend to select utter-
ances with fewer ASR errors.
ROUGE has been shown to correlate well with
human evaluations in DUC, when used on news cor-
pora, but the summarization task here ? using con-
versational speech from meetings ? is quite different
from summarizing news articles. ROUGE may sim-
ply be less applicable to this domain.
6 Future Work
It remains to be determined through further ex-
perimentation by researchers using various corpora
whether or not ROUGE truly correlates well with
human judgments. The results presented above are
mixed in nature, but do not present ROUGE as being
sufficient in itself to robustly evaluate a summariza-
tion system under development.
We are also interested in developing automatic
metrics of coherence and readability. We now have
human evaluations of these criteria and are ready to
39
begin testing for correlations between these subjec-
tive judgments and potential automatic metrics.
7 Acknowledgements
Thanks to Thomas Hain and the AMI-ASR group
for the speech recognition output. This work was
partly supported by the European Union 6th FWP
IST Integrated Project AMI (Augmented Multi-
party Interaction, FP6-506811, publication).
References
J. Carbonell and J. Goldstein. 1998. The use of MMR,
diversity-based reranking for reordering documents
and producing summaries. In Proc. ACM SIGIR,
pages 335?336.
Y. Gong and X. Liu. 2001. Generic text summarization
using relevance measure and latent semantic analysis.
In Proc. ACM SIGIR, pages 19?25.
T. Hain, J. Dines, G. Garau, M. Karafiat, D. Moore,
V. Wan, R. Ordelman, I.Mc.Cowan, J.Vepa, and
S.Renals. 2005. An investigation into transcription of
conference room meetings. Submitted to Eurospeech.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, and C. Wooters. 2003. The ICSI meeting corpus.
In Proc. IEEE ICASSP.
J. Kupiec, J. Pederson, and F. Chen. 1995. A trainable
document summarizer. In ACM SIGIR ?95, pages 68?
73.
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In ACL, pages 545?
552.
C.-Y. Lin and E. H. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
Proc. HLT-NAACL.
Inderjeet Mani, Barbara Gates, and Eric Bloedorn. 1999.
Improving summaries by revising them. In Proceed-
ings of the 37th conference on Association for Compu-
tational Linguistics, pages 558?565, Morristown, NJ,
USA. Association for Computational Linguistics.
G. Murray, S. Renals, and J. Carletta. 2005. Extractive
summarization of meeting recordings. Submitted to
Eurospeech.
E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, , and H. Car-
vey. 2004. The ICSI meeting recorder dialog act
(MRDA) corpus. In Proc. 5th SIGdial Workshop on
Discourse and Dialogue, pages 97?100.
J. Steinberger and K. Jez?ek. 2004. Using latent semantic
analysis in text summarization and summary evalua-
tion. In Proc. ISIM ?04, pages 93?100.
R. Valenza, T. Robinson, M. Hickey, and R. Tucker.
1999. Summarization of spoken audio through infor-
mation extraction. In Proc. ESCA Workshop on Ac-
cessing Information in Spoken Audio, pages 111?116.
Pierre Wellner, Mike Flynn, Simon Tucker, and Steve
Whittaker. 2005. A meeting browser evaluation test.
In CHI ?05: CHI ?05 extended abstracts on Human
factors in computing systems, pages 2021?2024, New
York, NY, USA. ACM Press.
K. Zechner and A. Waibel. 2000. Minimizing word error
rate in textual summaries of spoken language. In Proc.
NAACL-2000.
40
Towards an Alternative Implementation of NXT?s Query Language via
XQuery
Neil Mayo, Jonathan Kilgour, and Jean Carletta
University of Edinburgh
Abstract
The NITE Query Language (NQL) has
been used successfully for analysis of a
number of heavily cross-annotated data
sets, and users especially value its ele-
gance and flexibility. However, when us-
ing the current implementation, many of
the more complicated queries that users
have formulated must be run in batch
mode. For a re-implementation, we re-
quire the query processor to be capable
of handling large amounts of data at once,
and work quickly enough for on-line data
analysis even when used on complete cor-
pora. Early results suggest that the most
promising implementation strategy is one
that involves the use of XQuery on a mul-
tiple file data representation that uses the
structure of individual XML files to mirror
tree structures in the data, with redundancy
where a data node has multiple parents in
the underlying data object model.
1 Introduction
Computational linguistics increasingly requires
data sets which have been annotated for many dif-
ferent phenomena which relate independently to
the base text or set of signals, segmenting the data
in different, conflicting ways. The NITE XML
Toolkit, or NXT (Carletta et al, 2003), has been
used successfully on a range of text, spoken lan-
guage, and multimodal corpora to provide and
work with data of this character. Because the orig-
inal motivation behind it was to make up for a
dearth of tools that could be used to hand-annotate
and display such data, the initial implementation
of data search was required to work well on one
data observation at a time ? that is, one text, dia-
logue, or other language event ? and to be usable
but slow on multiple observations. However, the
clear and flexible design of NXT?s query language,
NQL (Heid et al, 2004; Carletta et al, in press),
makes it attractive for larger-scale data analysis,
and a number of users have up-translated exist-
ing data for the express purpose of improving their
search options.
We are now in the process of devising a strategy
for re-implementing the NQL processor to serve
the needs of this class of user better. In this paper,
we describe our requirements for the new imple-
mentation, outline the various implementation op-
tions that we have, and give early results suggest-
ing how well they meet our requirements. NQL
is arguably the most mature of the current special-
purpose facilities for searching data sets where the
data is not structured as a single tree, and there-
fore experiences with implementing it are likely
to provide lessons for search facilities that are still
to come.
2 NXT and the NITE Query Language
NXT is designed specifically for data sets with
multiple kinds of annotation. It requires data to be
represented as a set of XML files related to each
other using stand-off links, with a ?metadata? file
that provides two things: a catalogue of files con-
taining the audio or video signals used to capture
an observation together with the annotations that
describe them, and a specification of the corpus
design that describes the annotations and how they
relate to each other and to signal. Text corpora
are treated as signal-less, with the text as a base
level of ?annotation? to which other annotations
can point. Given data of this type, NXT provides
Java libraries for data modelling and search as well
27
as for building graphical user interfaces that can be
used to display annotations in synchrony with the
signals. NXT also comes with a number of fin-
ished GUIs for common tasks, some of which are
specific to existing data sets and some of which
are configurable to new corpus designs. NXT sup-
ports data exploration using a search GUI, callable
from any tool, that will run an NQL query and
highlight results on the tool?s display. Data search
is then usually done using one of a battery of com-
mand line utilities, that, for instance, count results
matching a given query or provide tab-delimited
output describing the query matches.
Because the data model and query language for
a tool are critical to our implementation choices,
we briefly describe them, as well as the current
NQL implementation.
2.1 NXT?s data handling
In NXT, annotations are described by types and
attribute value pairs, and can relate to a synchro-
nized set of signals via start and end times, to
representations of the external environment, and
to each other. Annotations can describe the be-
haviour of a single ?agent?, if more than one par-
ticipant is involved for the genre being coded, or
they can describe the entire language event; the
latter possibility is used for annotating written text
as well as for interaction within a pair or group
of agents. The data model ties the annotations
together into a multi-rooted tree structure that is
characterized by both temporal and structural or-
derings. Additional relationships can be overlaid
on this structure using pointers that carry no order-
ing implications. The same basic data objects that
are used for annotations are also used to build sets
of objects that represent referents from the real
world, to structure type ontologies, and to provide
access to other external resources stored in XML,
resulting in a rich and flexible system.
Data is stored in a ?stand-off? XML representa-
tion that uses the XML structure of individual files
to mirror the most prominent trees in the data it-
self, forming ?codings? of related annotations, and
pointers between files (represented by XLinks)
for other relationships. For example, the markup
<nt nite:start="1.8" nite:end="2.3"
cat="NP" nite:id="s1 506" wc="2">
<nite:child href="a1.words.xml#id(s1 6)"/>
<nite:child href="a1.words.xml#id(s1 7)"/>
</nt>
represents a noun phrase in a syntactic tree,
pointing to two words in a different file which con-
stitute the content of that syntactic structure.
This has the useful properties of allowing cor-
pus subsets to be assembled as needed; making it
easy to import annotations without perturbing the
rest of the data set; and keeping each individual
file simple to use in external processing. For in-
stance, the words for a single speaker can be stored
in a single file that contains no other data, mak-
ing it easy to pass through to a part-of-speech tag-
ger. NXT itself provides transparent corpus-wide
access to the data, and so tool users need not un-
derstand how the data is stored. A ?metadata? file
defines the structures of the individual files and
the relationships among them, as well as detail-
ing where to find the data and signals on the file
system.
2.2 NXT?s query language
Search is conducted in NXT by means of a dedi-
cated query language, NQL. A simple query finds
n-tuples of data objects (annotations and objects)
that satisfy certain conditions. The query expres-
sion consists of two parts, separated by a colon (:).
In the first part, variables representing the data ob-
jects are declared. These either match all data ob-
jects (e.g. ?($x)? for a variable named ?$x?) or are
constrained to draw matches from a designated set
of simple types (e.g. ?($w word ? sil)?, matching
data objects of the simple types ?word? or ?sil?).
The second part of the query expression specifies
a set of conditions that are required to hold for a
match, which are combined using negation (logi-
cal not, ?!?), conjunction (logical and, ?&&?), and
disjunction (logical or, ???). Queries return a struc-
ture that for each match lists the variables and ref-
erences the data objects they matched.
NQL has operators that allow match conditions
to be expressed for each of the essential properties
of a data object such as its identity, its attribute-
value pairs, its textual content, its timing, and its
relationships via the two types of structural links
(child and pointer). The attribute and textual con-
tent tests include the ability to match against either
the existence or the value of the attribute.
Attribute values or textual content can be tested
against explicit values or values of other attributes,
using equality, inequality, and the usual ordering
tests (conventionally represented as <, <=, >=,
and >). String values can also be tested against
28
regular expressions.
The temporal operators include the ability to
test whether a data object has timing information,
and to compare the start or end time with a given
point in time. The query language also has opera-
tors to test for some common timing relationships
between two data objects, such as overlap.
The structural operators test for dominance,
precedence, and pointer relationships. Precedence
can be tested against all of the orderings in the
overlapping annotations.
In addition, identity tests can be used to avoid
matches where different variables point to the
same data object. It is also possible in NQL to
?bind? variables within the query using existential
(?exists?) and universal (?forall?) quantifiers in the
variable declarations (which have the same mean-
ing as in first-order logic). Such bound variables
are not returned in the query result.
NXT also supports the sequencing of queries
into a ?complex? query using a double colon (::)
operator. The results for a complex query are re-
turned not as a flat list but as a tree structure. For
example, in a corpus of timed words from two
speakers, A and B,
($wa word):($wa@agent = "A")::
($wb word):($wb@agent="B") && ($wa
overlaps.with $wb)
will return a tree showing word overlaps; under-
neath each top level node, representing an overlap-
ping word from speaker A, will be a set of nodes
representing the words from speaker B that over-
lap that word of speaker A.
2.3 Comparison to other search facilitiies
The kinds of properties that linguists wish to use
in searching language data are cumbersome to ex-
press in general-purpose query languages. For
this reason, there are a number of other query
languages designed specifically for language cor-
pora, some of which are supported by implemen-
tation. LPath (Bird et al, 2006) and tgrep2 (Ro-
hde, nd) assume the data forms one ordered tree.
TigerSearch (Tiger Project, nd) is primarily for
single trees, but does allow some out-of-tree re-
lationships; the data model includes ?secondary
edges? that link a node to an additional parent and
that can be labelled, with query language opera-
tors that will test for the presence or absence of
such an edge, with or without a specific label. AT-
LAS (National Institute of Standards and Technol-
ogy, 2000) intends a query language over richer
structures, but the structures and query language
are still under development.
3 Requirements
We already have a successful NQL implementa-
tion as part of NXT, NXT Search. However, as
always, there are a number of things that could
be improved about it. We are considering a re-
implementation with the following aims in mind:
Faster query execution. Although many queries
run quite quickly in NXT Search, more com-
plicated queries can take long enough to ex-
ecute on a large corpus that they have to be
scheduled overnight. This is partially due
to the approach of checking every possible
combination of the variables declared in the
query, resulting in a large search space for
some queries. Our aim is to have the vast
majority of queries that exploit NXT?s multi-
rooted tree structure run quickly enough on
single observations that users will be happy
to run them in an interactive GUI environ-
ment.
The ability to load more data. NXT loads data
into a structure that is 5-7 times the size of
the data on disk. A smaller memory repre-
sentation would allow larger data sets to be
loaded for querying. Because it has a ?lazy?
implementation that only loads annotations
when they are required, the current perfor-
mance is sufficient for many purposes, as this
allows all of the annotations relating a single
observation to be loaded unless the observa-
tion is both long and very heavily annotated.
It is too limited (a) when the user requires a
query to relate annotations drawn from dif-
ferent observations, for instance, as a conve-
nience when working on sparse phenomena,
or when working on multiple-document ap-
plications such as the extraction of named en-
tities from newspaper articles; (b) for queries
that draw on very many kinds of annotation
all at the same time on longer observations;
and (c) when the user is in an interactive en-
vironment such as a GUI using a wide range
of queries on different phenomena. In the last
case, our goal could be achieved by memory
management that throws loaded data away in-
stead of increasing the loading capacity.
29
4 XQuery as a Basis for
Re-implementation
XQuery (Boag et al, 2005), currently a W3C
Candidate Recommendation, is a Turing-complete
functional query/programming language designed
for querying (sets of) XML documents. It sub-
sumes XPath, which is ?a language for addressing
parts of an XML document? (Clark and DeRose,
1999). XPath supports the navigation, selection
and extraction of fragments of XML documents,
by the specification of ?paths? through the XML
hierarchy. XQuery queries can include a mixture
of XML, XPath expressions, and function calls;
and also FLWOR expressions, which provide var-
ious programmatical constructs such as for, let,
where, orderby and return keywords for loop-
ing and variable assignment. XQuery is designed
to make efficient use of the inherent structure of
XML to calculate the results of a query.
XQuery thus appears a natural choice for query-
ing XML of the sort over which NQL oper-
ates. Although the axes exposed in XPath al-
low comprehensive navigation around tree struc-
tures, NXT?s object model allows individual nodes
to draw multiple parents from different trees that
make up the data; expressing navigation over this
multi-tree structure can be cumbersome in XPath
alone. XQuery allows us to combine fragments of
XML, selected by XPath, in meaningful ways to
construct the results of a given query.
There are other possible implementation op-
tions that would not use XQuery. The first of these
would use extensions to the standard XPath axes
to query concurrent markup, as has been demon-
strated by (Iacob and Dekhtyar, 2005). We have
not yet investigated this option.
The second is to come up with an indexing
scheme that allows us to recast the data as a rela-
tional database, the approach taken in LPath (Bird
et al, 2006). We chose not to explore this option.
It is not difficult to design a relational database to
match a particular NXT corpus as long as editing
is not enabled. However, a key part of NXT?s data
model permits annotations to descend recursively
through different layers of the same set of data
types, in order to make it easy to represent things
like syntax trees. This makes it difficult to build a
generic transform to a relational database - such a
transform would need to inspect the entire data set
to see what the largest depth is. It also makes it im-
possible to allow editing, at least without placing
some hard limit on the recursion. It is admittedly
true that any strategy based on XQuery will also
be limited to static data sets for the present, but
update mechanisms for XQuery are already begin-
ning to appear and are likely to become part of
some future standard.
5 Implementation Strategy
In our investigation, we compare two possible im-
plementation strategies to NXT Search, our exist-
ing implementation.
5.1 Using NXT?s stand-off format
The first strategy is to use XQuery directly on
NXT?s stand-off data storage format. The bulk of
the work here is in writing libraries of XQuery
functions that correctly interpret NXT?s stand-
off child links in order to allow navigation over
the same primary axes as are used in XPath,
but with multiple parenthood, and operating over
NXT?s multiple files. The libraries can resolve the
XLinks NXT uses both forwards and backwards.
Backwards resolution requires functions that ac-
cess the corpus metadata to find out which files
could contain annotations that could stand in the
correct relationship to the starting node. Built on
top of this infrastructure would be functions which
implement the NQL operators.
Resolving ancestors is a rather expensive opera-
tion which involves searching an entire coding file
for links to a node with a specified identity. Ad-
ditionally, if a query includes variables which are
not bound to a particular type, this precludes the
possibility of reducing the search space to particu-
lar coding files.
A drawback to using XPath to query a hierar-
chy which is serialised to multiple annotation files,
is that much of the efficiency of XPath expres-
sions can be lost through the necessity of resolving
XLinks at every child or parent step of the expres-
sion. This means that even the descendant and an-
cestor axes of XPath may not be used directly but
must be broken down into their constituent single-
step axes.
In addition to providing a transparent interface
for navigating the data, it may be necessary to pro-
vide additional indexing of the data, to increase ef-
ficiency and avoid the duplication of calculations.
An alternative is to overcome the standoff nature
of the data by resolving links explicitly, as de-
scribed in the following section.
30
5.2 Using a redundant data representation
The second strategy makes use of the classic trade-
off between memory and speed by employing a
redundant data representation that is both easy to
calculate from NXT?s data storage format and en-
sures that most of the navigation required exer-
cises common parts of XPath, since these are the
operations upon which XQuery implementations
will have concentrated their resources.
The particular redundancy we have in mind re-
lies on NXT?s concept of ?knitting? data. In
NXT?s data model, every node may have multi-
ple parents, but only one set of children. Where
multiple parents exist, at most one will be in the
same file as the child node, with the rest connected
by XLinks. ?Knitting? is the process of starting
with one XML file and recursively following chil-
dren and child links, storing the expanded result
as an XML file. The redundant representation we
used is then the smallest set of expanded files that
contains within it every child link from the origi-
nal data as an XML child. .
Although this approach has the advantage of us-
ing XPath more heavily than our first approach,
it has the added costs of generating the knitted
data and handling the redundancy. The knitting
stylesheet that currently ships with NXT is very
slow, but a very fast implementation of the knit-
ting process that works with NXT format data has
been developed and is expected as part of an up-
coming LTXML release (University of Edinburgh
Language Technology Group, nd). The cost of
dealing with redundancy depends on the branch-
ing structure of the corpus. To date, most corpora
with multiple parenthood have a number of quite
shallow trees that do not branch themselves but all
point to the same few base levels (e.g. orthogra-
phy), suggesting we can at least avoid exponential
expansion.
6 Tests
For initial testing, we chose a small set of queries
which would allow us to judge potential imple-
mentations in terms of whether they could do ev-
erything we need to do, whether they would give
the correct results, and how they would perform
against our stated requirements. This allows us to
form an opinion whilst only writing portions of the
code required for a complete NQL implementa-
tion. Our set of queries is therefore designed to in-
volve all of the basic operations required to exploit
NXT?s ability to represent multi-rooted trees and
to traverse a large amount of data, so that they are
computationally expensive and could return many
results. In the tests, we ran the queries over the
NXT translation for the Penn Treebank syntax an-
notated version of one Switchboard dialogue (Car-
letta et al, 2004), sw4114. The full dialogue is ap-
proximately 426Kb in physical size, and contains
over 1101 word elements.
6.1 Test queries
Our test queries were as follows.
? Query 1 (Dominance):
(exists $e nt)($w word):
$e@cat="NP" && $e?$w
(words dominated by an NP-category nt)
? Query 2 (Complex query with precedence
and dominance):
($w1 word)($w2 word):
TEXT($w1)="the" && $w1<>$w2
::
(exists $p nt): $p@cat="NP"
&& $p?$w1 && $p?$w2
(word pairs where the word ?the? precedes
the second word with respect to a common
NP dominator)
? Query 3 (Eliminative):
($a word)(forall $b
turn):!($b?$a)
(words not dominated by any turn)
In the data, the category ?nt? represents syntac-
tic non-terminals. The third query was chosen be-
cause it is particularly slow in the current NQL im-
plementation, but is easily expressed as a path and
therefore is likely to execute efficiently in XPath
implementations.
Although NXT?s object model also allows for
arbitrary relationships between nodes using point-
ers with named roles, increasing speed for queries
over them is only a secondary concern, and we
know that implementing operators over them is
possible in XQuery because it is very similar to re-
solving stand-off child links. For this reason, none
of our test queries involve pointers.
6.2 Test environment
For processing XQuery, we used Saxon
(www.saxonica.com), which provides an API
so that it can be called from Java. There are
31
several available XQuery interpreters, and they
will differ in their implementation details. We
chose Saxon because it appears to be most
complete and is well-supported. Alternative
interpreters, Galax (www.galaxquery.org) and
Qexo (www.gnu.org/software/qexo/), provided
only incomplete implementations at the time of
writing.
6.3 Comparability of results
It is not possible in a test like this to produce com-
pletely comparable results because the different
implementation strategies are doing very different
things to arrive at their results. For example, con-
sider our second query. Apart from some primitive
optimizations, on this and all queries, NXT Search
does an exhaustive search of all possible k-tuples
that match the types given in the query, varying
the rightmost variable fastest. Our XQuery imple-
mentation on stand-off data first finds matches to
$w1, $w2, and $np; then calls a function that cal-
culates the ancestries for matches to $w1 and $w2;
for each ($w1, $w2) pair, computes the intersec-
tion of the two ancestries; and finally filters this
intersection against the list of $np matches.
On the other hand, the implementation on the
knitted data is shown in figure 1. It first sets vari-
ables representing the XML document containing
our knitted data and all distinct nt elements within
that document which both have a category at-
tribute ?NP? and have further word descendants.
It then sets a variable to represent the sequence of
results. The results are calculated by taking each
NP-type element and checking its word descen-
dants for those pairs where a word ?the? precedes
another word. The implementation also applies
the condition that the NP-type element must not
have another NP element as an ancestor ? this
is to remove duplicates introduced by the way we
find the initial set of NPs.
In addition to the execution strategies, the meth-
ods used to start off processing were quite differ-
ent. For each of the implementations, we did what-
ever gave the best performance. For the XQuery-
based implementations, this meant writing a Java
class to start up a static context for the execution
of the query and reusing it to run the query repeat-
edly. For NXT, it meant using a shell script to run
the command-line utility SaveQueryResults
repeatedly on a set of observations, exiting each
time.
Figure 1: An XQuery rewritten for knitted data;
containing more direct XPath expressions.
let $doc := doc(?data/knitted/swbd/sw4114.syntax.xml?),
$nps := $doc//nt[@cat=?NP?][descendant::word] union (),
$result := (
for $np in $nps return (
let $w2 := $np//word, $w1 := $w2[text()=?the?]
for $a in $w1, $b in $w2
where (struct:node-precedes($a, $b)
and not($np/ancestor::nt[@cat=?NP?]))
(: only return for the uppermost common NP ancestor :)
return (element match {$a, $b})
)
) union ()
return element result {attribute count count($result), $result}
Our aim in performing the comparison is to as-
sess what is possible in each approach rather than
to do the same thing in each, and this is why
we have attempted to achieve best possible per-
formance in each context rather than making the
conditions as similar as possible. In all cases, the
figures we report are the mean timings over five
runs of what the Linux time command reports as
?real? time.
7 Speed Results
The results of our trial are shown in the follow-
ing table. Timings which are purely in seconds
are given to 2 decimal places; those which extend
into the minutes are given to the nearest second.
?NXT? means NXT Search; ?XQ? is the condi-
tion with XQuery using stand-off data; and ?XQ-
K? is the condition with XQuery using the redun-
dant knitted data.
Query
Impl
Q1 Q2 Q3
NXT 3.38s 1m24 18.25s
XQ 10.21s 3m24 14.42s
XQ-K 2.03s 2.17s 2.47s
Although it would be wrong to read too much
into our simple testing, these results do suggest
some tentative conclusions. The first is that us-
ing XQuery on NXT?s stand-off data format is
unlikely to increase execution speed except for
queries that are computationally very expensive
for NXT, and may decrease performance for other
queries. If users show any tolerance for delays,
it is more likely to be for the delays to the for-
mer, and therefore this does not seem a winning
32
strategy. On the other hand, using XQuery on the
knitted data provides useful (and sometimes im-
pressive) gains across the board.
It should be noted that our results are based
upon a single XQuery implementation and are
inevitably implementation-specific. Future work
will also attempt to make comparisons with al-
ternatives, including those provided by XML
databases.
7.1 Memory results
To explore our second requirement, the ability to
load more data, we generated a series of corpora
which double in size from an initial set of 4 chil-
dren with 2 parents.
We ran both NXT Search and XQuery in Saxon
on these corpora, with the Java Virtual Machine
initialised with increasing amounts of memory,
and recorded the maximum corpus size each was
able to handle. Both query languages were exer-
cised on NXT stand-off data, with the simple task
of calculating parent/child relationships. Results
are shown in the following table.
Max corpus size
(nodes, disk space)
Mem Mb NXT XQuery/Saxon
500 3 ? 217, 28Mb 3 ? 219, 111Mb
800 3 ? 218, 56Mb 3 ? 220, 224Mb
1000 3 ? 218, 56Mb 3 ? 220, 224Mb
These initial tests suggest that at its best, the
XQuery implementation in Saxon can manage
around 4 times as much data as NXT Search. It
is interesting to note that the full set of tests took
about 19 minutes for XQuery, but 18 hours for
NXT Search. That is, Saxon appears to be far
more efficient at managing large data sets. We
also discovered that the NXT results were differ-
ent when a different query was used; we hope to
elaborate these results more accurately in the fu-
ture.
We did not specifically run this test on the im-
plementation that uses XQuery on knitted data be-
cause the basic characteristics would be the same
as for the XQuery implementation with stand-off
data. The size of a knitted data version will de-
pend on the amount of redundancy that knitting
creates. Knitting has the potential to increase the
amount of memory required greatly, but it is worth
noting that it does not always do so. The knit-
ted version of the Switchboard dialogue used for
these tests is actually smaller than the stand-off
version, because the original stand-off stores ter-
minals (words) in a separate file from syntax trees
even though the terminals are defined to have only
one parent. That is, there can be good reasons for
using stand-off annotation, but it does have its own
costs, as XLinks take space.
7.2 Query rewriting
In the testing described far, we used the existing
version of NXT Search. Rather than writing a new
query language implementation, we could just in-
vest our resources in improvement of NXT Search
itself. It is possible that we could change the un-
derlying XML handling to use libraries that are
more memory-efficient, but this is unlikely to give
real scalability. The biggest speed improvements
could probably be made by re-ordering terms be-
fore query execution. Experienced query authors
can often speed up a query if they rewrite the terms
to minimize the size of the search space, assuming
they know the shape of the underlying data set.
Although we do not yet have an algorithm for this
rewriting, it roughly involves ignoring the ?exists?
quantifier, splitting the query into a complex one
with one variable binding per subquery, sequenc-
ing the component queries by increasing order of
match set size, and evaluating tests on the earli-
est subquery possible. For example, consider the
query
($w1 word):text($w1)="the" ::
($p nt):$p@cat eq "NP" && $p?$w1 ::
($w2 word): $p?$w2 && $w1<>$w2
This query, which bears a family resemblance
to query 2, takes 4.31s, which is a considerable
improvement. Of course, the result tree is a dif-
ferent shape from the one specified in the origi-
nal query, and so this strategy for gaining speed
improvements would incur the additional cost of
rewriting the result tree after execution.
7.2.1 Discussion
Our testing suggests that if we want to make
speed improvements, creating a new NQL imple-
mentation that uses XQuery on a redundant data
representation is a good option. Although not
the result we initially expected, it is perhaps un-
surprising. This XQuery implementation strat-
egy draws more heavily on XPath than the stand-
off strategy, and XPath is the most well-exercised
portion of XQuery. The advantages do not just
come from recasting our computations as opera-
tions over trees. XPath allows us, for instance, to
33
write a single expression that both binds a vari-
able and performs condition tests on it, rather than
requiring us to first bind the variable and then
loop through each combination of nodes to deter-
mine which satisfy the constraints. Using a re-
dundant data representation increases memory re-
quirements, but the XQuery-based strategies use
enough less memory that the redundancy in itself
will perhaps not be an issue. In order to settle this
question, we must think more carefully about the
size and shape of current and potential NXT cor-
pora.
Our other option for making speed improve-
ments is to augment NXT Search with a query
rewriting strategy. This needs further evalua-
tion because the improvements will vary widely
with the query being rewritten, but our initial test
worked surprisingly well. However, augmenting
the current NXT Search in this way will not reduce
its memory use, and it is not clear whether this im-
provement can readily be made by other means.
Acknowledgments
This work has been funded by a grant from Scot-
tish Enterprise via the Edinburgh-Stanford Link.
We are grateful to Stefan Evert for designing NQL
and for discussing its specification with us, and
to Jan-Torsten Milde and Felix Sasaki for mak-
ing available to us their own initial experiments
suggesting that this re-implementation would be
worth attempting.
References
[Bird et al2006] Steven Bird, Yi Chen, Susan David-
son, Haejoong Lee, and Yifeng Zheng. 2006. De-
signing and evaluating an XPath dialect for linguis-
tic queries. In 22nd International Conference on
Data Engineering, Atlanta, USA.
[Boag et al2005] Scott Boag, Don Chamberlin,
Mary F. Fernandez, Daniela Florescu, Jonathan
Robie, Jrme Simon, and Mugur Stefanescu. 2005.
Xquery 1.0: An XML Query Language, November.
http://www.w3.org/TR/xquery/; accessed 18 Jan 06.
[Carletta et al2003] J. Carletta, Stefan Evert, Ulrich
Heid, Jonathan Kilgour, Judy Robertson, and Holger
Voormann. 2003. The NITE XML Toolkit: flexible
annotation for multi-modal language data. Behav-
ior Research Methods, Instruments, and Computers,
35(3):353?363.
[Carletta et al2004] Jean Carletta, Shipra Dingare,
Malvina Nissim, and Tatiana Nikitina. 2004. Us-
ing the NITE XML Toolkit on the Switchboard Cor-
pus to study syntactic choice: a case study. In
Fourth Language Resources and Evaluation Confer-
ence, Lisbon, Portugal.
[Carletta et alin press] J. Carletta, S. Evert, U. Heid,
and J. Kilgour. in press. The NITE XML Toolkit:
data model and query language. Language Re-
sources and Evaluation Journal.
[Clark and DeRose1999] James Clark and Steve
DeRose. 1999. Xml path language (xpath) version
1.0, 16 November. http://www.w3.org/TR/xpath;
accessed 18 Jan 06.
[Heid et al2004] Ulrich Heid, Holger Voormann, Jan-
Torsten Milde, Ulrike Gut, Katrin Erk, and Sebastian
Pad. 2004. Querying both time-aligned and hierar-
chical corpora with NXT Search. In Fourth Lan-
guage Resources and Evaluation Conference, Lis-
bon, Portugal.
[Iacob and Dekhtyar2005] Ionut E. Iacob and Alex
Dekhtyar. 2005. Towards a query language for
multihierarchical xml: Revisiting xpath. In Eighth
International Workshop on the Web and Databases
(WebDB 2005), Baltimore, Maryland, USA, 16-17
June.
[National Institute of Standards and Technology2000]
National Institute of Standards and
Technology. 2000. ATLAS Project.
http://www.nist.gov/speech/atlas/; last update 6
Feb 2003; accessed 18 Jan 06.
[Rohdend] Doug Rohde. n.d. Tgrep2.
http://tedlab.mit.edu/ dr/Tgrep2/; accessed 18
Jan 06.
[Tiger Projectnd] Tiger Project. n.d. Linguistic inter-
pretation of a German corpus. http://www.ims.uni-
stuttgart.de/projekte/TIGER/; last update 17 Nov
2003; accessed 1 Mar 2004.
[University of Edinburgh Language Technology Groupnd]
University of Edinburgh Language Tech-
nology Group. n.d. LTG Software.
http://www.ltg.ed.ac.uk/software/; accessed 18
Jan 2006.
34
The NITE XML Toolkit: Demonstration from ve corpora
Jonathan Kilgour and Jean Carletta
University of Edinburgh
United Kingdom
jonathan@inf.ed.ac.uk
Abstract
The NITE XML Toolkit (NXT) is open
source software for working with multi-
modal, spoken, or text language corpora.
It is specifically designed to support the
tasks of human annotators and analysts
of heavily cross-annotated data sets, and
has been used successfully on a range of
projects with varying needs. In this text
to accompany a demonstration, we de-
scribe NXT along with four uses on dif-
ferent corpora that together span its most
novel features. The examples involve the
AMI and ICSI Meeting Corpora; a study
of multimodal reference; a syntactic anal-
ysis of Genesis in classical Hebrew; and
discourse annotation of Switchboard dia-
logues.
1 Introduction
Of the software packages that provide support for
working with language corpora, the NITE XML
Toolkit (NXT) is arguably the most mature, of-
fering to combine multiple audio and video sig-
nals with crossing structures of linguistic annota-
tion. It is currently in use on a range of corpora.
Current users both create annotations in NXT na-
tively and ?up-translate? existing data from other
tools into NXT?s storage format. Although its
biggest strengths are for multimodal language re-
search, some users have found it to be the right so-
lution for work on text and speech corpora with-
out video because of the way it handles anno-
tation and analysis. NXT is open source soft-
ware, available from Sourceforge and documented
at http://www.ltg.ed.ac.uk/NITE. It is written in
Java and uses the Java Media Framework (JMF)
for its handling of signals.
In this text to accompany a demonstration, we
summarize NXT?s functionality, comment on its
use for four corpora that together showcase its
most novel features, and describe funded future
development.
2 The NITE XML Toolkit
At its core, NXT consists of three libraries: one
for data handling, one for searching data, and one
for building GUIs for working with data. The data
handling libraries include support for loading, se-
rialization using a stand-off XML format, naviga-
tion around loaded data, and changes to the data
in line with a specific data model that is intended
especially for data sets that contain both timing in-
formation and overlapping structural markup. The
search facility implements a query language that
is designed particularly for the data model and al-
lows the user to find n-tuples of data objects that
match a set of conditions based on types, tempo-
ral conditions, and structural relationships in the
data. The GUI library defines signal players and
data displays that update against loaded data and
can highlight parts of the display that correspond
to current time on the signals or that correspond
to matches to a query typed into a standard search
interface. This includes support for selection as re-
quired for building annotation tools and a specific
transcription-oriented display.
NXT also contains a number of end user inter-
faces and utilities built on top of these libraries.
These include a generic display that will work for
any NXT format data, configurable GUIs for some
common hand-annotation tasks such as markup of
named entities, dialogue acts, and a tool for seg-
menting and labelling a signal as it plays. They
also include command line utilities for common
search tasks such as counting query results and
65
Figure 1: Named entity coding on the AMI corpus
some utilities for transforming data into, for in-
stance, tab-delimited tables.
Finally, a number of projects have contributed
sample data and annotation tools as well as mech-
anisms for transforming data to and from other
formats. Writing and testing a new up-translation
typically takes someone who understands NXT?s
format between one and three days. The actual
time depends on the complexity of the structure
represented in the input data and whether a parser
for the data format must be written from scratch.
Badly documented formats and ill-formed data
take longer to transform.
3 Examples
Example 1: The AMI and ICSI Meeting
Corpora
The AMI Project (http://www.amiproject.org) is
currently NXT?s biggest user, and is also its largest
provider of financial support. AMI, which is col-
lecting and transcribing 100 hours of meeting data
(Carletta et al, 2005) and annotating part or all of
it for a dozen different phenomena, is using NXT?s
data storage for its reference format, with data be-
ing generated natively using NXT GUIs as well
as up-translated from other sources. The project
uses ChannelTrans (ICSI, nd) for orthographic
transcription and Event Editor (Sumec, nd) for
straightforward timestamped labelling of video;
although NXT comes with an interface for the lat-
ter, Event Editor, which is Windows-only and not
based on JMF, has better video control and was al-
ready familiar to some of the annotators. For anno-
tation, AMI is using the configurable dialogue act
and named entity tools as well as tailored GUIs for
topic segmentation and extractive summarization
that links extracted dialogue acts to the sentences
of an abstractive summary they support. Figure 1
shows the named entity annotation tool as config-
ured for the AMI project. Aside from the sheer
scale of the exercise, the AMI effort is unique in
requiring simultaneous annotation of different lev-
els at different sites. NXT does not support data
management, but its stand-off XML data format
has made it relatively easy to manage the process
using a combination of a CVS repository for ver-
sion control, web forms for data upload, and wikis
for work assignment and progress reports.
The AMI Project piloted many of their
techniques on the ICSI Meeting Corpus
(Janin et al, 2003), which shares some character-
istics with the AMI corpus but is audio-only. More
information about this closely related use of NXT
can be found in (Carletta and Kilgour, 2005).
66
Figure 2: DIAGRAMS corpus: linking gesture and dialogue acts.
Example 2: Multimodal reference
This example is a small project that is looking at
the relationship between referring expressions and
the hand gestures used to point at a map. Although
the transcription, referring expression, and gesture
annotations were done in other tools and then up-
translated, NXT gave the best support for linking
referring expressions with gestures and analysing
the results. Figure 2 shows the linking tool. One
interesting aspect of this project was that the anal-
ysis was performed by a postgraduate psycholo-
gist. Analysts with no computational experience
find it more difficult to learn how to use the query
language, but several have done so. With this kind
of data set, simply the ability to play the signals
and annotations together and highlight query re-
sults provides insights into behaviours that are dif-
ficult to reach otherwise.
Example 3: Syntax in Genesis
This example is an annotation of Genesis in clas-
sical Hebrew that shows its structural division
into books, chapters, verses, and half-verses. The
data itself, which is purely textual, was originally
stored in an MS Access relational database, but
overlapping hierarchies in the structure made it
difficult to query in this format. After finding NXT
on the web and consulting us about the best way to
represent the data using the NXT data model, the
user successfully up-translated his data, searched
it using NQL, and exported counts to SPSS to cre-
ate corpus statistics.
Example 4: The Switchboard Corpus
The Switchboard Dialogue Corpus
(Godfrey et al, 1992) has been popu-
lar for computational discourse research.
(Carletta et al, 2004) describes an effort which
up-translated its Penn Treebank syntactic analysis
to NXT format, added annotations of ?markables?
for animacy, information structure, and corefer-
ence, and used this information all together. This
project made heavy use of NXT?s query language,
including the ability to index query results in the
data storage format itself for easy access. The
work is now being extended to align an improved
version of the transcriptions that includes word
timestamps derived by forced alignment with the
transcriptions used for the syntactic and discourse
67
annotation, and to add annotations for phonology
and syllable structure, all within the same corpus
structure.
4 Discussion
It should not be supposed from our list of ex-
amples that NXT has been used only for these
applications. Particularly novel NXT uses in-
clude simultaneous display of annotation with a
re-enactment of a human-computer tutorial dia-
logue driven by the dialogue system itself; hand-
annotation of head orientation from video using
a flock-of-birds motion sensor mounted on a cof-
fee mug; and annotation of the critiques expressed
in conversations about movies. However, most
NXT users are applying some kind of discourse
annotation. They choose NXT because they need
to combine signal labellings with annotations that
give structure over the top of orthography, because
they want to combine annotations from different
sources and so find the stand-off format attrac-
tive, or because they need the GUI library in or-
der to develop novel interfaces. Academic soft-
ware is often inadequately documented, and there-
fore only usable with the help of its developers.
It is inevitable given the size of the target user
community that most of them are at least ?friends
of friends?. Enough users have worked indepen-
dently of the developers that we are confident that
all but the newest parts of NXT are understandable
from the documentation.
Although NXT is mature enough for use, sev-
eral projects are investing in further development.
The largest current efforts are to create an annota-
tion and analysis tool with a time-aligned ?tiered?
data display and a query processor with better per-
formance (Mayo et al, 2006). Another priority is
better packaging, particularly of the configurable
interfaces and of the existing translations from the
formats used in transcription and other annotation
tools. Finally, contributing projects plan work that
will improve interoperability between NXT and
other tools, including eyetrackers, NLP applica-
tions such as part-of-speech taggers, and machine
learning software.
Acknowledgments
NXT prototyping and development has been sup-
ported primarily by the European Commission
(MATE, LE4-8370; NITE, IST-2000-2609; and
AMI, FP6-506811) and Scottish Enterprise via the
Edinburgh-Stanford Link. The examples shown
are by kind permission of Scottish Enterprise, the
AMI Project, and Dr. Matthew Anstey of Charles
Sturt University.
References
[Carletta and Kilgour2005] J.C. Carletta and J. Kilgour.2005. The NITE XML Toolkit meets the ICSIMeeting Corpus: Import, annotation, and brows-
ing. In S. Bengio and H. Bourlard, editors, Machine
Learning for Multimodal Interaction: First Interna-
tional Workshop, MLMI 2004, Martigny, Switzer-
land, June 21-23, 2004, Revised Selected Papers,Lecture Notes in Computer Science 3361, pages111?121. Springer-Verlag, Berlin.
[Carletta et al2004] Jean Carletta, Shipra Dingare,
Malvina Nissim, and Tatiana Nikitina. 2004. Usingthe NITE XML Toolkit on the switchboard corpus tostudy syntactic choice: a case study. In Fourth Lan-
guage Resources and Evaluation Conference, Lis-bon, Portugal.
[Carletta et al2005] J. Carletta, Simone Ashby, Se-bastien Bourban, Mike Flynn, Mael Guillemot,
Thomas Hain, Jaroslav Kadlec, Vasilis Karaiskos,Wessel Kraaij, Melissa Kronenthal, Guillaume Lath-oud, Mike Lincoln, Agnes Lisowska, Iain Mc-
Cowan, Wilfried M. Post, Dennis Reidsma, andPierre Wellner. 2005. The AMI Meeting Corpus:A pre-announcement. In S. Renals and S. Bengio,
editors, 2nd Joint Workshop on Multimodal Interac-
tion and Related Machine Learning Algorithms, Ed-inburgh.
[Godfrey et al1992] J. Godfrey, E. Holliman, andJ. McDaniel. 1992. Switchboard: Telephone speech
corpus for research development. In International
Conference on Acoustics, Speech, and Signal Pro-
cessing, pages 517?520, San Francisco, CA, USA.
[ICSInd] ICSI. n.d. Extensions to Tran-
scriber for meeting recorder transcription.http://www.icsi.berkeley.edu/Speech/mr/channeltrans.html; accessed 14 Oct 05.
[Janin et al2003] A. Janin, D. Baron, J. Edwards, D. El-
lis, D. Gelbart, N. Morgan, B. Peskin, T. Pfau,E. Shriberg, A. Stolcke, and C. Wooters. 2003. TheICSI Meeting Corpus. In IEEE International Con-
ference on Acoustics, Speech and Signal Processing,Hong Kong.
[Mayo et al2006] Neil Mayo, Jonathan Kilgour, andJean Carletta. 2006. Towards an alternative im-
plementation of NXT?s query language via XQuery.In EACL Workshop on Multi-dimensional Markup in
Natural Language Processing (NLPXML), Trento,
Italy.
[Sumecnd] Stanislav Sumec. n.d. Event Editor.
http://www.fit.vutbr.cz/research/grants/m4/editor/;accessed 14 Oct 05.
68
