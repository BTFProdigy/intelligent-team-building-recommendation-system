Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 116?126,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Generating artificial errors for grammatical error correction
Mariano Felice
Computer Laboratory
University of Cambridge
United Kingdom
mf501@cam.ac.uk
Zheng Yuan
Computer Laboratory
University of Cambridge
United Kingdom
zy249@cam.ac.uk
Abstract
This paper explores the generation of ar-
tificial errors for correcting grammatical
mistakes made by learners of English as
a second language. Artificial errors are in-
jected into a set of error-free sentences in a
probabilistic manner using statistics from
a corpus. Unlike previous approaches, we
use linguistic information to derive error
generation probabilities and build corpora
to correct several error types, including
open-class errors. In addition, we also
analyse the variables involved in the selec-
tion of candidate sentences. Experiments
using the NUCLE corpus from the CoNLL
2013 shared task reveal that: 1) training
on artificially created errors improves pre-
cision at the expense of recall and 2) dif-
ferent types of linguistic information are
better suited for correcting different error
types.
1 Introduction
Building error correction systems using machine
learning techniques can require a considerable
amount of annotated data which is difficult to ob-
tain. Available error-annotated corpora are often
focused on particular groups of people (e.g. non-
native students), error types (e.g. spelling, syn-
tax), genres (e.g. university essays, letters) or top-
ics so it is not clear how representative they are
or how well systems based on them will gener-
alise. On the other hand, building new corpora is
not always a viable solution since error annotation
is expensive. As a result, researchers have tried
to overcome these limitations either by compiling
corpora automatically from the web (Mizumoto et
al., 2011; Tajiri et al., 2012; Cahill et al., 2013) or
using artificial corpora which are cheaper to pro-
duce and can be tailored to their needs.
Artificial error generation allows researchers to
create very large error-annotated corpora with lit-
tle effort and control variables such as topic and
error types. Errors can be injected into candidate
texts using a deterministic approach (e.g. fixed
rules) or probabilities derived from manually an-
notated samples in order to mimic real data.
Although artificial errors have been used in pre-
vious work, we present a new approach based on
linguistic information and evaluate it using the test
data provided for the CoNLL 2013 shared task on
grammatical error correction (Ng et al., 2013).
Our work makes the following contributions.
First, we are the first to use linguistic informa-
tion (such as part-of-speech (PoS) information or
semantic classes) to characterise contexts of natu-
rally occurring errors and replicate them in error-
free text. Second, we apply our technique to a
larger number of error types than any other pre-
vious approach, including open-class errors. The
resulting datasets are used to train error correction
systems aimed at learners of English as a second
language (ESL). Finally, we provide a detailed de-
scription of the variables that affect artificial error
generation.
2 Related work
The use of artificial data to train error correction
systems has been explored by other researchers us-
ing a variety of techniques.
Izumi et al. (2003), for example, use artificial
errors to target article mistakes made by Japanese
learners of English. A corpus is created by replac-
ing a, an, the or the zero article by a different ar-
ticle chosen at random in more than 7,500 correct
sentences and used to train a maximum entropy
model. Results show an improvement for omis-
sion errors but no change for replacement errors.
Brockett et al. (2006) describe the use of a sta-
tistical machine translation (SMT) system for cor-
recting a set of 14 countable/uncountable nouns
116
which are often confusing for ESL learners. Their
training corpus consists of a large number of sen-
tences extracted from news articles which were de-
liberately modified to include typical countability
errors based on evidence from a Chinese learner
corpus. Their approach to artificial error injec-
tion is deterministic, using hand-coded rules to
change quantifiers (much? many), generate plu-
rals (advice? advices) or insert unnecessary de-
terminers. Experiments show their system was
generally able to beat the standard Microsoft Word
2003 grammar checker, although it produced a rel-
atively higher rate of erroneous corrections.
SMT systems are also used by Ehsan and Faili
(2013) to correct grammatical errors and context-
sensitive spelling mistakes in English and Farsi.
Training corpora are obtained by injecting arti-
ficial errors into well-formed treebank sentences
using predefined error templates. Whenever an
original sentence from the corpus matches one of
these templates, a pair of correct and incorrect sen-
tences is generated. This process is repeated mul-
tiple times if a single sentence matches more than
one error template, thereby generating many pairs
for the same original sentence. A comparison be-
tween the proposed systems and rule-based gram-
mar checkers show they are complementary, with
a hybrid system achieving the best performance.
2.1 Probabilistic approaches
A few researchers have explored probabilistic
methods in an attempt to mimic real data more ac-
curately. Foster and Andersen (2009), for exam-
ple, describe a tool for generating artificial errors
based on statistics from other corpora, such as the
Cambridge Learner Corpus (CLC).
1
Their experi-
ments show a drop in accuracy when artificial sen-
tences are used as a replacement for real incorrect
sentences, suggesting that they may not be as use-
ful as genuine text. Their report also includes an
extensive summary of previous work in the area.
Rozovskaya and Roth propose more sophis-
ticated probabilistic methods to generate artifi-
cial errors for articles (2010a) and prepositions
(2010b; 2011), also based on statistics from an
ESL corpus. In particular, they compile a set of
sentences from the English Wikipedia and apply
the following generation methods:
1
http://www.cup.cam.ac.uk/gb/elt/
catalogue/subject/custom/item3646603/
Cambridge-International-Corpus-
Cambridge-Learner-Corpus/
General
Target words (e.g. articles) are replaced with oth-
ers of the same class with probability x (varying
from 0.05 to 0.18). Each new word is chosen uni-
formly at random.
Distribution before correction (in ESL data)
Target words in the error-free text are changed
to match the distribution observed in ESL error-
annotated data before any correction is made.
Distribution after correction (in ESL data)
Target words in the error-free text are changed
to match the distribution observed in ESL error-
annotated data after corrections are made.
Native language-specific distributions
It has been observed that second language produc-
tion is affected by a learner?s native language (L1)
(Lee and Seneff, 2008; Leacock et al., 2010). A
common example is the difficulty in using English
articles appropriately by learners whose L1 has
no article system, such as Russian or Japanese.
Because word choice errors follow systematic pat-
terns (i.e. they do not occur randomly), this infor-
mation is extremely valuable for generating errors
more accurately.
L1-specific errors can be imitated by computing
word confusions in an error-annotated ESL cor-
pora and using these distributions to change tar-
get words accordingly in error-free text. More
specifically, if we estimate P(source|target) in an
error-tagged corpus (i.e. the probability of an
incorrect source word being used when the cor-
rect target is expected), we can generate more ac-
curate confusion sets where each candidate has
an associated probability depending on the ob-
served word. For example, supposing that a
group of learners use the preposition to in 10%
of cases where the preposition for should be used
(that is, P(source=to|target=for)=0.10), we can
replicate this error pattern by replacing the oc-
currences of the preposition for with to with a
probability of 0.10 in a corpus of error-free sen-
tences. When the source and target words are the
same, P(source=x|target=x) expresses the proba-
bility that a learner produces the correct/expected
word.
Because errors are generally sparse (and there-
fore error rates are low), replicating mistakes
based on observed probabilities can easily lead to
117
low recall. In order to address this issue during ar-
tificial error generation, Rozovskaya et al. (2012)
propose an inflation method that boosts confusion
probabilities in order to generate a larger propor-
tion of artificial instances. This reformulation is
shown to improve F-scores when correcting deter-
miners and prepositions.
Experiments reveal that these approaches yield
better results than assuming uniform probabilis-
tic distributions where all errors and correc-
tions are equally likely. In particular, classifiers
trained on artificially generated data outperformed
those trained on native error-free text (Rozovskaya
and Roth, 2010a; Rozovskaya and Roth, 2011).
However, it has also been shown that using arti-
ficially generated data as a replacement for non-
native error-corrected data can lead to poorer per-
formance (Sj?obergh and Knutsson, 2005; Foster
and Andersen, 2009). This would suggest that ar-
tificial errors are more useful than native data but
less useful than corrected non-native data.
Rozovskaya and Roth also control other vari-
ables in their experiments. On the one hand, they
only evaluate their systems on sentences that have
no spelling mistakes so as to avoid degrading per-
formance. This is particularly important when
training classifiers on features extracted with lin-
guistic tools (such as parsers or taggers) as they
could provide inaccurate results for malformed in-
put. On the other hand, the authors work on a lim-
ited set of error types (mainly articles and preposi-
tions) which are closed word classes and therefore
have reduced confusion sets. Thus, it becomes in-
teresting to investigate how their ideas extrapolate
to open-class error types, like verb form or content
word errors.
Their probabilistic error generation approach
has also been used by other researchers. Imamura
et al. (2012), for example, applied this method to
generate artificial incorrect sentences for Japanese
particle correction with an inflation factor ranging
from 0.0 (no errors) to 2.0 (double error rates).
Their results show that the performance of artifi-
cial corpora depends largely on the inflation rate
but can achieve good results when domain adapta-
tion is applied.
In a more exhaustive study, Cahill et al.
(2013) investigate the usefulness of automatically-
compiled sentences from Wikipedia revisions
for correcting preposition errors. A number
of classifiers are trained using error-free text,
automatically-compiled annotated corpora and ar-
tificial sentences generated using error probabili-
ties derived from Wikipedia revisions and Lang-
8.
2
Their results reveal a number of interesting
points, namely that artificial errors provide com-
petitive results and perform robustly across differ-
ent test sets. A learning curve analysis also shows
system performance increases as more training
data is used, both real and artificial.
More recently, some teams have also reported
improvements by using artificial data in their
submissions to the CoNLL 2013 shared task.
Rozovskaya et al. (2013) apply their inflation
method to train a classifier for determiner errors
that achieves state-of-the-art performance while
Yuan and Felice (2013) use naively-generated arti-
ficial errors within an SMT framework that places
them third in terms of precision.
3 Advanced generation of artificial
errors
Our work is based on the hypothesis that using
carefully generated artificial errors improves the
performance of error correction systems. This im-
plies generating errors in a way that resembles
available error-annotated data, using similar texts
and accurate injection methods. Like other proba-
bilistic approaches, our method assumes we have
access to an error-corrected reference corpus from
which we can compute error generation probabili-
ties.
3.1 Base text selection
We analyse a set of variables that we consider im-
portant for collecting suitable texts for error injec-
tion, namely:
Topic
Replicating errors on texts about the same topic
as the training/test data is more likely to produce
better results than out-of-domain data, as vocab-
ulary and word senses are more likely to be sim-
ilar. In addition, similar texts are more likely to
exhibit suitable contexts for error injection and
consequently help the system focus on particularly
useful information.
Genre
In cases where no a priori information about topic
is available (for example, because the test set is
2
http://lang-8.com/
118
unknown or the system will be used in different
scenarios), knowing the genre or type of text the
system will process can also be useful. Example
genres include expository (descriptions, essays,
reports, etc.), narrative (stories), persuasive (re-
views, advertisements, etc.), procedural (instruc-
tions, recipes, experiments, etc.) and transactional
texts (letters, interviews, etc.).
Style/register
As with the previous aspects, style (colloquial,
academic, etc.) and register (from formal writ-
ten to informal spoken) also affect production and
should therefore be modelled accurately in the
training data.
Text complexity/language proficiency
Candidate texts should exhibit the same reading
complexity as training/test texts and be written by
or targeted at learners with similar English profi-
ciency. Otherwise, the overlap in vocabulary and
grammatical structures is more likely to be small
and thus hinder error injection.
Native language
Because second language production is known to
be affected by a learner?s L1, using candidate texts
produced by groups of the same L1 as the train-
ing/test data should provide more suitable contexts
for error injection. When such texts are not avail-
able, using data by speakers of other L1s that ex-
hibit similar phenomena (e.g. no article system,
agglutinative languages, etc.) might also be use-
ful. However, finding error-free texts written in
English by a specific population can be difficult,
which is why most approaches resort to native
English text.
In our experiments, the aforementioned vari-
ables are manually controlled although we believe
many of them could be assessed automatically.
For example, topics could be estimated using text
similarity measures, genres could be predicted us-
ing structural information and L1s could be in-
ferred using a native language identifier.
3
For an analysis of other variables such as do-
main and error distributions, the reader should re-
fer to Cahill et al. (2013).
3
See the First Edition of the Shared Task on Native
Language Identification (Tetreault et al., 2013) at https://
sites.google.com/site/nlisharedtask2013/
3.2 Error replication
Our approach to artificial error generation is sim-
ilar to the one proposed by Rozovskaya and Roth
(2010a) in that we also estimate probabilities in
a corpus of ESL learners which are then used to
distort error-free text. However, unlike them, we
refine our probabilities by imposing restrictions
on the linguistic functions of the words and the
contexts where they occur. Because we extend
generation to open-class error types (such as verb
form errors), this refinement becomes necessary to
overcome disambiguation issues and lead to more
accurate replication.
Our work is the first to exploit linguistic infor-
mation for error generation, as described below.
Error type distributions
We compute the probability of each error type p(t)
occurring over the total number of relevant in-
stances (e.g. noun phrases are relevant instances
for article errors). During generation, p(t) is uni-
formly distributed over all the possible choices for
the error type (e.g. for articles, choices are a, an,
the or the zero article). Relevant instances are de-
tected in the base text and changed for an alter-
native at random using the estimated probabilities.
The probability of leaving relevant instances un-
changed is 1? p(t).
Morphology
We believe morphological information such as
person or number is particularly useful for identi-
fying and correcting specific error types, such as
articles, noun number or subject-verb agreement.
Thus, we compute the conditional probability of
words in specific classes for different morpholog-
ical contexts (such as noun number or PoS). The
following example shows confusion probabilities
for singular head nouns requiring an:
P(source-det=an|target-det=an
head-noun=NN
) = 0.942
P(source-det=the|target-det=an
head-noun=NN
) = 0.034
P(source-det=a|target-det=an
head-noun=NN
) = 0.015
P(source-det=other|target-det=an
head-noun=NN
) = 0.005
P(source-det=?|target-det=an
head-noun=NN
) = 0.004
PoS disambiguation
Most approaches to artificial error generation are
aimed at correcting closed-class words such as
articles or prepositions, which rarely occur with
119
a different part of speech in the text. However,
when we consider open-class error types, we
should perform PoS disambiguation since the
same surface form could play different roles in
a sentence. For example, consider generating
artificial verb form errors for the verb to play after
observing its distribution in an error-annotated
corpus. By using PoS tags, we can easily deter-
mine if an occurrence of the word play is a verb or
a noun and thus compute or apply the appropriate
probabilities:
P(source=play|target=play
V
) = 0.98
P(source=plays|target=play
V
) = 0.02
P(source=play|target=play
N
) = 0.84
P(source=plays|target=play
N
) = 0.16
Semantic classes
We hypothesise that semantic information about
concepts in the sentences can shed light on
specific usage patterns that may otherwise be
hidden. For example, we could refine confusion
sets for prepositions according to the type of
object they are applied to (a location, a recipient,
an instrument, etc.):
P(prep=in|noun class=location) = 0.39
P(prep=to|noun class=location) = 0.31
P(prep=at|noun class=location) = 0.16
P(prep=from|noun class=location) = 0.07
P(prep=?|noun class=location) = 0.05
P(prep=other|noun class=location) = 0.03
By abstracting from surface forms, we can also
generate faithful errors for words that have not
been previously observed, e.g. we may have not
seen hospital but we may have seen school, my
sister?s house or church.
Word senses
Polysemous words with the same PoS can exhibit
different patterns of usage for each of their mean-
ings (e.g. one meaning may co-occur with a spe-
cific preposition more often than the others). For
this reason, we introduce probabilities for each
word sense in an attempt to capture more accurate
usage. As an example, consider a hypothetical sit-
uation in which a group of learners confuse prepo-
sitions used with the word bank as a financial insti-
tution but they produce the right preposition when
it refers to a river bed:
P(prep=in|noun=bank
1
) = 0.76
P(prep=at|noun=bank
1
) = 0.18
P(prep=on|noun=bank
1
) = 0.06
P(prep=on|noun=bank
2
) = 1.00
Although it is rare that occurrences of the same
word will refer to different meanings within a
document (the so-called ?one sense per discourse?
assumption (Gale et al., 1992)), this is not the
case when large corpora containing different doc-
uments are used for characterising and generating
errors. In such scenarios, word sense disambigua-
tion should produce more accurate results.
Table 1 lists the actual probabilities computed
from each type of information and the errors they
are able to generate.
4 Experimental setup
4.1 Corpora and tools
We use the NUCLE v2.3 corpus (Dahlmeier et
al., 2013) released for the CoNLL 2013 shared
task on error correction, which comprises error-
annotated essays written in English by students
at the National University of Singapore. These
essays cover topics such as environmental pollu-
tion, health care, welfare, technology, etc. All the
sentences were manually annotated by human ex-
perts using a set of 27 error types, but we used the
filtered version containing only the five types se-
lected for the shared task: ArtOrDet (article or de-
terminer), Nn (noun number), Prep (preposition),
SVA (subject-verb agreements) and Vform (verb
form) errors. The training set of the NUCLE cor-
pus contains 57,151 sentences and 1,161,567 to-
kens while the test set comprises 1,381 sentences
and 29,207 tokens. The training portion of the cor-
pus was used to estimate the required conditional
probabilities and train a few variations of our sys-
tems while the test set was reserved to evaluate
performance.
Candidate native texts for error injection were
extracted from the English Wikipedia, controlling
the variables described Section 3.1 as follows:
Topic: We chose an initial set of 50 Wikipedia
articles based on keywords in the NUCLE
training data and proceeded to collect related
articles by following hyperlinks in their ?See
also? section. We retrieved a total of 494 arti-
cles which were later preprocessed to remove
120
Information Probability Generated error types
Error type distribution P(error type) ArtOrDet, Nn, Prep, SVA, Vform
Morphology P(source=determiner|target=determiner, head noun tag) ArtOrDet, SVA
P(source=verb tag|target=verb tag, subj head noun tag)
PoS disambiguation P(source=word|target=word, PoS) Nn, Vform
Semantic classes P(source=determiner|target=determiner, head noun class) ArtOrDet, Prep
P(source=preposition|target=preposition, head noun class)
Word senses P(source=preposition|verb sense + obj head noun sense) ArtOrDet, Prep, SVA
P(source=preposition|target=preposition, head noun sense)
P(source=preposition|target=preposition, dep adj sense)
P(source=determiner|target=determiner, head noun sense)
P(source=verb tag|target=verb tag, subj head noun sense)
Table 1: Probabilities computed for each type of linguistic information. Error codes correspond to the
five error types in the CoNLL 2013 shared task: ArtOrDet (article or determiner), Nn (noun number),
Prep (prepositions), SVA (subject-verb agreement) and Vform (verb form).
wikicode tags, yielding 54,945 sentences and
approximately 1,123,739 tokens.
Genre: Both NUCLE and Wikipedia contain ex-
pository texts, although they are not necessar-
ily similar.
Style/register: Written, academic and formal.
Text complexity/language proficiency: Essays
in the NUCLE corpus are written by ad-
vanced university students and are therefore
comparable to standard English Wikipedia
articles. For less sophisticated language,
the Simple English Wikipedia could be an
alternative.
Native language: English Wikipedia articles are
mostly written by native speakers whereas
NUCLE essays are not. This is the only dis-
cordant variable.
PoS tagging was performed using RASP
(Briscoe et al., 2006). Word sense dis-
ambiguation was carried out using the
WordNet::SenseRelate:AllWords Perl module
(Pedersen and Kolhatkar, 2009) which assigns
a sense from WordNet (Miller, 1995) to each
content word in a text. As for semantic in-
formation, we use WordNet classes which are
readily available in NLTK (Bird et al., 2009).
WordNet classes respond to a classification in
lexicographers? files
4
and are defined for content
words as shown in Table 2, depending on their
location in the hierarchy.
4
http://wordnet.princeton.edu/man/
lexnames.5WN.html
Part of speech WordNet classification
Adjective all, pertainyms, participial
Adverb all
Noun act, animal, artifact, attribute, body,
cognition, communication, event,
feeling, food, group, location, motive,
object, person, phenomenon, plant,
possession, process, quantity, relation,
shape, state, substance, time
Verb body, change, cognition,
communication, competition,
consumption, contact, creation,
emotion, motion, perception,
possession, social, stative, weather
Table 2: WordNet classes for content words.
Name Composition
ED errors based on error type distributions
MORPH errors based on morphology
POS errors based on PoS disambiguation
SC errors based on semantic classes
WSD errors based on word senses
Table 3: Generated artificial corpora based on dif-
ferent types of linguistic information.
4.2 Error generation
For each type of information in Table 1, we com-
pute the corresponding conditional probabilities
using the NUCLE training set. These probabili-
ties are then used to generate six different artificial
corpora using the inflation method (Rozovskaya et
al., 2012), as listed in Table 3.
4.3 System training
We approach the error correction task as a transla-
tion problem from incorrect into correct English.
Systems are built using an SMT framework and
different combinations of NUCLE and our artifi-
cial corpora, where the source side contains in-
121
Original Revised
C M U P R F
1
C M U P R F
1
NUCLE (baseline) 181 1462 513 0.2608 0.1102 0.1549 200 1483 495 0.2878 0.1188 0.1682
ED 53 1590 150 0.2611 0.0323 0.0574 62 1621 141 0.3054 0.0368 0.0657
MORPH 74 1569 333 0.1818 0.0450 0.0722 83 1600 324 0.2039 0.0493 0.0794
POS 42 1601 99 0.2979 0.0256 0.0471 42 1641 99 0.2979 0.0250 0.0461
SC 80 1563 543 0.1284 0.0487 0.0706 87 1596 536 0.1396 0.0517 0.0755
WSD 82 1561 305 0.2119 0.0499 0.0808 91 1592 296 0.2351 0.0541 0.0879
NUCLE+ED 173 1470 411 0.2962 0.1053 0.1554 194 1489 390 0.3322 0.1153 0.1712
NUCLE+MORPH 163 1480 427 0.2763 0.0992 0.1460 182 1501 408 0.3085 0.1081 0.1601
NUCLE+POS 164 1479 365 0.3100 0.0998 0.1510 182 1501 347 0.3440 0.1081 0.1646
NUCLE+SC 162 1481 488 0.2492 0.0986 0.1413 181 1502 469 0.2785 0.1075 0.1552
NUCLE+WSD 163 1480 413 0.2830 0.0992 0.1469 181 1502 395 0.3142 0.1075 0.1602
Table 4: Evaluation of our correction systems over the original and revised NUCLE test set using the M
2
Scorer. Columns C, M and U show the number of correct, missed and unnecessary corrections suggested
by each system. Results in bold show improvements over the baseline.
correct sentences and the target side contains their
corrected versions. Our setup is similar to the one
described by Yuan and Felice (2013) in that we
train a PoS-factored phrase-based model (Koehn,
2010) using Moses (Koehn et al., 2007), Giza++
(Och and Ney, 2003) for word alignment and the
IRSTLM Toolkit (Federico et al., 2008) for lan-
guage modelling. However, unlike them, we do
not optimise decoding parameters but use default
values instead.
We build 11 different systems in total: a base-
line system using only the NUCLE training set,
one system per artificial corpus and other addi-
tional systems using combinations of the NUCLE
training data and our artificial corpora. Each of
these systems uses a single translation model that
tackles all error types at the same time.
5 Results
Each system was evaluated in terms of precision,
recall and F
1
on the NUCLE test data using the
M
2
Scorer (Dahlmeier and Ng, 2012), the official
evaluation script for the CoNLL 2013 shared task.
Table 4 shows results of evaluation on the original
test set (containing only one gold standard correc-
tion per error) and a revised version (which allows
for alternative corrections submitted by participat-
ing teams).
Results reveal our ED and POS corpora are able
to improve precision for both test sets. It is surpris-
ing, however, that the least informed dataset (ED)
is one of the best performers although this seems
reasonable if we consider it is the only dataset that
includes artificial instances for all error types (see
Table 1). Hybrid datasets containing the NUCLE
training set plus an artificial corpus also gener-
ally improve precision, except for NUCLE+SC. It
could be argued that the reason for this improve-
ment is corpus size, since our hybrid datasets are
double the size of each individual set, but the small
differences in precision between the ED and POS
datasets and their corresponding hybrid versions
seem to contradict that hypothesis. In fact, re-
sults would suggest artificial and naturally occur-
ring errors are not interchangeable but rather com-
plementary.
The observed improvement in precision, how-
ever, comes at the expense of recall, for which
none of the systems is able to beat the baseline.
This contradicts results by Rozovskaya and Roth
(2010a), who show their error inflation method in-
creases recall, although this could be due to differ-
ences in the training paradigm and data. Still, re-
sults are encouraging since precision is generally
preferred over recall in error correction scenarios
(Yuan and Felice, 2013).
We also evaluated performance by error type on
the original (Table 5) and revised (Table 6) test
data using an estimation approach similar to the
one in CoNLL 2013. Results show that the per-
formance of each dataset varies by error type, sug-
gesting that certain types of information are bet-
ter suited for specific error types. In particular,
we find that on the original test set, ED achieves
the highest precision for article and determiners,
WSD maximises precision for prepositions and
SC achieves the highest recall and F
1
. When us-
ing hybrid sets, results improve overall, with the
highest precision being as follows: NUCLE+POS
(ArtOrDet), NUCLE+ED (Nn), NUCLE+WSD
122
ArtOrDet Nn Prep SVA/Vform Other
P R F
1
P R F
1
P R F
1
P R F
1
C M U
NUCLE (b) 0.2716 0.1551 0.1974 0.4625 0.0934 0.1555 0.1333 0.0386 0.0599 0.2604 0.1016 0.1462 0 0 34
ED 0.2813 0.0391 0.0687 0.6579 0.0631 0.1152 0.0233 0.0032 0.0056 0.0000 0.0000 ? 0 0 5
MORPH 0.1862 0.1058 0.1349 ? 0.0000 ? 0.0000 0.0000 ? 0.1429 0.0041 0.0079 0 0 7
POS 0.0000 0.0000 ? 0.4405 0.0934 0.1542 0.0000 0.0000 ? 0.1515 0.0203 0.0358 0 0 10
SC 0.1683 0.0739 0.1027 ? 0.0000 ? 0.0986 0.0932 0.0959 0.0000 0.0000 ? 0 0 21
WSD 0.2219 0.1029 0.1406 0.0000 0.0000 ? 0.1905 0.0257 0.0453 0.1875 0.0122 0.0229 0 0 8
NUCLE+ED 0.3185 0.1348 0.1894 0.5465 0.1187 0.1950 0.1304 0.0386 0.0596 0.2658 0.0854 0.1292 0 0 35
NUCLE+MORPH 0.2857 0.1507 0.1973 0.4590 0.0707 0.1225 0.1719 0.0354 0.0587 0.2817 0.0813 0.1262 0 0 30
NUCLE+POS 0.3384 0.1290 0.1868 0.4659 0.1035 0.1694 0.1884 0.0418 0.0684 0.2625 0.0854 0.1288 0 0 29
NUCLE+SC 0.2890 0.1290 0.1784 0.4500 0.0682 0.1184 0.1492 0.0868 0.1098 0.2836 0.0772 0.1214 0 0 34
NUCLE+WSD 0.3003 0.1449 0.1955 0.4667 0.0707 0.1228 0.1948 0.0482 0.0773 0.2632 0.0813 0.1242 0 0 30
Table 5: Error type analysis of our correction systems over the original NUCLE test set using the M
2
Scorer. Columns C, M and U show the number of correct, missed and unnecessary corrections outside
the main categories suggested by each system. Results in bold show improvements over the baseline.
ArtOrDet Nn Prep SVA/Vform Other
P R F
1
P R F
1
P R F
1
P R F
1
C M U
NUCLE (b) 0.3519 0.2026 0.2572 0.6163 0.1302 0.2150 0.2069 0.0682 0.1026 0.4105 0.1718 0.2422 0 0 34
ED 0.4063 0.0579 0.1014 0.7297 0.0684 0.1250 0.0465 0.0077 0.0132 0.1818 0.0183 0.0332 0 0 5
MORPH 0.2270 0.1311 0.1662 ? 0.0000 ? 0.0000 0.0000 ? 0.2857 0.0092 0.0179 0 0 7
POS 0.0000 0.0000 ? 0.5465 0.1169 0.1926 0.0000 0.0000 ? 0.4242 0.0631 0.1098 0 0 10
SC 0.2112 0.0944 0.1305 ? 0.0000 ? 0.1088 0.1221 0.1151 0.0000 0.0000 ? 0 0 21
WSD 0.2781 0.1313 0.1784 0.0000 0.0000 ? 0.2143 0.0347 0.0598 0.2000 0.0138 0.0259 0 0 8
NUCLE+ED 0.4334 0.1849 0.2592 0.7000 0.1552 0.2540 0.1685 0.0575 0.0857 0.4744 0.1630 0.2426 0 0 35
NUCLE+MORPH 0.3791 0.2006 0.2624 0.6308 0.1017 0.1752 0.2295 0.0536 0.0870 0.4714 0.1454 0.2222 0 0 30
NUCLE+POS 0.4601 0.1761 0.2547 0.6087 0.1383 0.2254 0.2424 0.0613 0.0979 0.4430 0.1549 0.2295 0 0 29
NUCLE+SC 0.3961 0.1773 0.2450 0.6154 0.0993 0.1709 0.1844 0.1250 0.1490 0.4848 0.1410 0.2184 0 0 34
NUCLE+WSD 0.3994 0.1933 0.2605 0.6308 0.1017 0.1752 0.2432 0.0690 0.1075 0.4667 0.1535 0.2310 0 0 30
Table 6: Error type analysis of our correction systems over the revised NUCLE test set using the M
2
Scorer. Columns C, M and U show the number of correct, missed and unnecessary corrections outside
the main categories suggested by each system. Results in bold show improvements over the baseline.
(Prep) and NUCLE+SC (SVA/Vform). As ex-
pected, the use of alternative annotations in the re-
vised test set improves results but it does not reveal
any qualitative difference between datasets.
Finally, when compared to other systems in the
CoNLL 2013 shared task in terms of F
1
, our best
systems would rank 9th on both test sets. This
would suggest that using an off-the-shelf SMT
system trained on a combination of real and ar-
tificial data can yield better results than other ma-
chine learning techniques (Yi et al., 2013; van den
Bosch and Berck, 2013; Berend et al., 2013) or
rule-based approaches (Kunchukuttan et al., 2013;
Putra and Szabo, 2013; Flickinger and Yu, 2013;
Sidorov et al., 2013).
6 Conclusions
This paper presents early results on the genera-
tion and use of artificial errors for grammatical
error correction. Our approach uses conditional
probabilities derived from an ESL error-annotated
corpus to replicate errors in native error-free data.
Unlike previous work, we propose using linguistic
information such as PoS or sense disambiguation
to refine the contexts where errors occur and thus
replicate them more accurately. We use five differ-
ent types of information to generate our artificial
corpora, which are later evaluated in isolation as
well as coupled to the original ESL training data.
General results show error distributions and PoS
information produce the best results, although this
varies when we analyse each error type separately.
These results should allow us to generate errors
more efficiently in the future by using the best ap-
proach for each error type.
We have also observed that precision improves
at the expense of recall and this is more pro-
nounced when using purely artificial sets. Finally,
artificially generated errors seem to be a comple-
ment rather than an alternative to genuine data.
7 Future work
There are a number of issues we plan to address in
future research, as described below.
Scaling up artificial data
The experiments presented here use a small and
manually selected collection of Wikipedia articles.
123
However, we plan to study the performance of our
systems as corpus size is increased. We are cur-
rently using a larger selection of Wikipedia ar-
ticles to produce new artificial datasets ranging
from 50K to 5M sentences. The resulting corpora
will be used to train new error correction systems
and study how precision and recall vary as more
data is added during the training process, similar
to Cahill et al. (2013).
Reducing differences between datasets
As shown in Table 1, we are unable to produce
the same set of errors for each different type of in-
formation. This is a limitation of our conditional
probabilities which encode different information
in each case. In consequence, comparing overall
results between datasets seems unfair as they do
not target the same error types. In order to over-
come this problem, we will define new probabili-
ties so that we can generate the same types of error
in all cases.
Exploring larger contexts
Our current probabilities model error contexts
in a limited way, mostly by considering rela-
tions between two or three words (e.g. arti-
cle+noun, verb+preposition+noun, etc.). In or-
der to improve error injection, we will define
new probabilities using larger contexts, such as
P(source=verb|target=verb, subject class, auxil-
iary verbs, object class) for verb form errors.
Using more specific contexts can also be useful for
correcting complex error types, such as the use of
pronouns, which often requires analysing corefer-
ence chains.
Using new linguistic information
In this work we have used five types of linguis-
tic information. However, we believe other types
of information and their associated probabilities
could also be useful, especially if we aim to cor-
rect more error types. Examples include spelling,
grammatical relations (dependencies) and word
order (syntax). Additionally, we believe the use
of semantic role labels can be explored as an al-
ternative to semantic classes, as they have proved
useful for error correction (Liu et al., 2010).
Mixed error generation
In our current experiments, each artificial corpus is
generated using only one type of information at a
time. However, having found that certain types of
information are more suitable than others for cor-
recting specific error types (see Tables 5 and 6), we
believe better artificial corpora could be created by
generating instances of each error type using only
the most appropriate linguistic information.
Acknowledgments
We would like to thank Prof Ted Briscoe for his
valuable comments and suggestions as well as
Cambridge English Language Assessment, a divi-
sion of Cambridge Assessment, for supporting this
research.
References
Gabor Berend, Veronika Vincze, Sina Zarrie?, and
Rich?ard Farkas. 2013. Lfg-based features
for noun number and article grammatical errors.
In Proceedings of the Seventeenth Conference
on Computational Natural Language Learning:
Shared Task, pages 62?67, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Steven Bird, Edward Loper, and Ewan Klein.
2009. Natural Language Processing with Python.
O?Reilly Media Inc.
Ted Briscoe, John Carroll, and Rebecca Watson.
2006. The second release of the RASP sys-
tem. In Proceedings of the COLING/ACL on
Interactive presentation sessions, COLING-ACL
?06, pages 77?80, Sydney, Australia. Association for
Computational Linguistics.
Chris Brockett, William B. Dolan, and Michael
Gamon. 2006. Correcting ESL Errors Using
Phrasal SMT Techniques. In Proceedings of the
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages
249?256, Sydney, Australia, July. Association for
Computational Linguistics.
Aoife Cahill, Nitin Madnani, Joel Tetreault, and
Diane Napolitano. 2013. Robust systems for
preposition error correction using wikipedia revi-
sions. In Proceedings of the 2013 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, pages 507?517, Atlanta, Georgia,
June. Association for Computational Linguistics.
Daniel Dahlmeier and Hwee Tou Ng. 2012.
Better evaluation for grammatical error correc-
tion. In Proceedings of the 2012 Conference of
the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, NAACL 2012, pages 568 ? 572,
Montreal, Canada.
124
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei
Wu. 2013. Building a Large Annotated Corpus
of Learner English: The NUS Corpus of Learner
English. In Proceedings of the 8th Workshop on
Innovative Use of NLP for Building Educational
Applications, BEA 2013, pages 22?31, Atlanta,
Georgia, USA, June. Association for Computational
Linguistics.
Nava Ehsan and Heshaam Faili. 2013. Grammatical
and context-sensitive error correction using a sta-
tistical machine translation framework. Software:
Practice and Experience, 43(2):187?206.
Marcello Federico, Nicola Bertoldi, and Mauro
Cettolo. 2008. IRSTLM: an open source toolkit
for handling large scale language models. In
Proceedings of the 9th Annual Conference of the
International Speech Communication Association,
INTERSPEECH 2008, pages 1618?1621, Brisbane,
Australia, September. ISCA.
Dan Flickinger and Jiye Yu. 2013. Toward
more precision in correction of grammatical er-
rors. In Proceedings of the Seventeenth Conference
on Computational Natural Language Learning:
Shared Task, pages 68?73, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Jennifer Foster and ?istein Andersen. 2009.
Generrate: Generating errors for use in grammati-
cal error detection. In Proceedings of the Fourth
Workshop on Innovative Use of NLP for Building
Educational Applications, pages 82?90, Boulder,
Colorado, June. Association for Computational
Linguistics.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse.
In Proceedings of the workshop on Speech
and Natural Language, HLT ?91, pages 233?
237, Harriman, New York. Association for
Computational Linguistics.
Kenji Imamura, Kuniko Saito, Kugatsu Sadamitsu, and
Hitoshi Nishikawa. 2012. Grammar error correc-
tion using pseudo-error sentences and domain adap-
tation. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics
(Volume 2: Short Papers), pages 388?392, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga,
Thepchai Supnithi, and Hitoshi Isahara. 2003.
Automatic Error Detection in the Japanese Learners?
English Spoken Data. In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics - Volume 2, ACL ?03, pages 145?
148, Sapporo, Japan. Association for Computational
Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Prague, Czech Republic.
Association for Computational Linguistics.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, New York, NY, USA,
1st edition.
Anoop Kunchukuttan, Ritesh Shah, and Pushpak
Bhattacharyya. 2013. Iitb system for conll 2013
shared task: A hybrid approach to grammatical er-
ror correction. In Proceedings of the Seventeenth
Conference on Computational Natural Language
Learning: Shared Task, pages 82?87, Sofia,
Bulgaria, August. Association for Computational
Linguistics.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical
Error Detection for Language Learners. Morgan
and Claypool Publishers.
John Lee and Stephanie Seneff. 2008. An analysis of
grammatical errors in non-native speech in English.
In Amitava Das and Srinivas Bangalore, editors,
Proceedings of the 2008 IEEE Spoken Language
Technology Workshop, SLT 2008, pages 89?92,
Goa, India, December. IEEE.
Xiaohua Liu, Bo Han, Kuan Li, Stephan Hyeonjun
Stiller, and Ming Zhou. 2010. SRL-based
verb selection for ESL. In Proceedings of
the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 1068?
1076, Cambridge, MA, October. Association for
Computational Linguistics.
George A. Miller. 1995. WordNet: a lexical
database for English. Communications of the ACM,
38(11):39?41, November.
Tomoya Mizumoto, Mamoru Komachi, Masaaki
Nagata, and Yuji Matsumoto. 2011. Mining
Revision Log of Language Learning SNS
for Automated Japanese Error Correction of
Second Language Learners. In Proceedings of
5th International Joint Conference on Natural
Language Processing, pages 147?155, Chiang Mai,
Thailand, November. Asian Federation of Natural
Language Processing.
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian
Hadiwinoto, and Joel Tetreault. 2013. The
CoNLL-2013 Shared Task on Grammatical Error
Correction. In Proceedings of the Seventeenth
Conference on Computational Natural Language
Learning: Shared Task, pages 1?12, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51, March.
125
Ted Pedersen and Varada Kolhatkar. 2009.
WordNet::SenseRelate::AllWords: a broad cover-
age word sense tagger that maximizes semantic
relatedness. In Proceedings of Human Language
Technologies: The 2009 Annual Conference of the
North American Chapter of the Association for
Computational Linguistics, Companion Volume:
Demonstration Session, NAACL-Demonstrations
?09, pages 17?20, Boulder, Colorado. Association
for Computational Linguistics.
Desmond Darma Putra and Lili Szabo. 2013.
Uds at conll 2013 shared task. In Proceedings
of the Seventeenth Conference on Computational
Natural Language Learning: Shared Task, pages
88?95, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Alla Rozovskaya and Dan Roth. 2010a. Training
paradigms for correcting errors in grammar and us-
age. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter
of the Association for Computational Linguistics,
HLT ?10, pages 154?162, Los Angeles, California.
Association for Computational Linguistics.
Alla Rozovskaya and Dan Roth. 2010b. Generating
confusion sets for context-sensitive error correction.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 961?970, Cambridge, Massachusetts.
Association for Computational Linguistics.
Alla Rozovskaya and Dan Roth. 2011. Algorithm
selection and model adaptation for ESL correction
tasks. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies - Volume 1, HLT
?11, pages 924?933, Portland, Oregon. Association
for Computational Linguistics.
Alla Rozovskaya, Mark Sammons, and Dan Roth.
2012. The UI system in the HOO 2012 shared task
on error correction. In Proceedings of the Seventh
Workshop on Building Educational Applications
Using NLP, pages 272?280, Montreal, Canada.
Association for Computational Linguistics.
Alla Rozovskaya, Kai-Wei Chang, Mark Sammons,
and Dan Roth. 2013. The University of
Illinois System in the CoNLL-2013 Shared Task.
In Proceedings of the Seventeenth Conference
on Computational Natural Language Learning:
Shared Task, pages 13?19, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Grigori Sidorov, Anubhav Gupta, Martin Tozer, Dolors
Catala, Angels Catena, and Sandrine Fuentes. 2013.
Rule-based system for automatic grammar correc-
tion using syntactic n-grams for english language
learning (l2). In Proceedings of the Seventeenth
Conference on Computational Natural Language
Learning: Shared Task, pages 96?101, Sofia,
Bulgaria, August. Association for Computational
Linguistics.
Jonas Sj?obergh and Ola Knutsson. 2005. Faking
errors to avoid making errors: Very weakly su-
pervised learning for error detection in writing.
In Proceedings of RANLP 2005, pages 506?512,
Borovets, Bulgaria, September.
Toshikazu Tajiri, Mamoru Komachi, and Yuji
Matsumoto. 2012. Tense and aspect error cor-
rection for esl learners using global context. In
Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (Volume
2: Short Papers), pages 198?202, Jeju Island, Korea,
July. Association for Computational Linguistics.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013. A Report on the First Native Language
Identification Shared Task. In Proceedings
of the Eighth Workshop on Innovative Use of
NLP for Building Educational Applications, pages
48?57, Atlanta, Georgia, June. Association for
Computational Linguistics.
Antal van den Bosch and Peter Berck. 2013. Memory-
based grammatical error correction. In Proceedings
of the Seventeenth Conference on Computational
Natural Language Learning: Shared Task, pages
102?108, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Bong-Jun Yi, Ho-Chang Lee, and Hae-Chang Rim.
2013. Kunlp grammatical error correction sys-
tem for conll-2013 shared task. In Proceedings
of the Seventeenth Conference on Computational
Natural Language Learning: Shared Task, pages
123?127, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Zheng Yuan and Mariano Felice. 2013. Constrained
grammatical error correction using statistical ma-
chine translation. In Proceedings of the Seventeenth
Conference on Computational Natural Language
Learning: Shared Task, pages 52?61, Sofia,
Bulgaria, August. Association for Computational
Linguistics.
126
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 96?103,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Linguistic Features for Quality Estimation
Mariano Felice
Research Group in Computational Linguistics
University of Wolverhampton
Stafford Street
Wolverhampton, WV1 1SB, UK
Mariano.Felice@wlv.ac.uk
Lucia Specia
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello
Sheffield, S1 4DP, UK
L.Specia@dcs.shef.ac.uk
Abstract
This paper describes a study on the contribu-
tion of linguistically-informed features to the
task of quality estimation for machine trans-
lation at sentence level. A standard regression
algorithm is used to build models using a com-
bination of linguistic and non-linguistic fea-
tures extracted from the input text and its ma-
chine translation. Experiments with English-
Spanish translations show that linguistic fea-
tures, although informative on their own, are
not yet able to outperform shallower features
based on statistics from the input text, its
translation and additional corpora. However,
further analysis suggests that linguistic infor-
mation is actually useful but needs to be care-
fully combined with other features in order to
produce better results.
1 Introduction
Estimating the quality of automatic translations is
becoming a subject of increasing interest within the
Machine Translation (MT) community for a num-
ber of reasons, such as helping human translators
post-editing MT, warning users about non-reliable
translations or combining output from multiple MT
systems. Different from most classic approaches for
measuring the progress of an MT system or compar-
ing MT systems, which assess quality by contrast-
ing system output to reference translations such as
BLEU (Papineni et al, 2002), Quality Estimation
(QE) is a more challenging task, aimed at MT sys-
tems in use, and therefore without access to refer-
ence translations.
From the findings of previous work on reference-
dependent MT evaluation, it is clear that metrics
exploiting linguistic information can achieve sig-
nificantly better correlation with human judgments
on quality, particularly at the level of sentences
(Gime?nez and Ma`rquez, 2010). Intuitively, this
should also apply for quality estimation metrics:
while evaluation metrics compare linguistic repre-
sentations of the system output and reference trans-
lations (e.g. matching of n-grams of part-of-speech
tags or predicate-argument structures), quality esti-
mation metrics would perform the (more complex)
comparison og linguistic representations of the input
and translation texts. The hypothesis put forward in
this paper is therefore that using linguistic informa-
tion to somehow contrast the input and translation
texts can be beneficial for quality estimation.
We test this hypothesis as part of the WMT-12
shared task on quality estimation. The system sub-
mitted to this task (WLV-SHEF) integrates linguis-
tic information to a strong baseline system using
only shallow statistics from the input and transla-
tion texts, with no explicit information from the MT
system that produced the translations. A variant
also tests the addition of linguistic information to
a larger set of shallow features. The quality esti-
mation problem is modelled as a supervised regres-
sion task using Support Vector Machines (SVM),
which has been shown to achieve good performance
in previous work (Specia, 2011). Linguistic features
are computed using a number of auxiliary resources
such as parsers and monolingual corpora.
The remainder of this paper is organised as fol-
lows. Section 2 gives an overview of previous work
96
on quality estimation, Section 3 describes the set of
linguistic features proposed in this paper, along with
general experimental settings, Section 4 presents our
evaluation and Section 5 provides conclusions and a
brief discussion of future work.
2 Related Work
Reference-free MT quality assessment was ini-
tially approached as a Confidence Estimation task,
strongly biased towards exploiting data from a Sta-
tistical MT (SMT) system and the translation pro-
cess to model the confidence of the system in the
produced translation. Blatz et al (2004) attempted
sentence-level assessment using a set of 91 features
(from the SMT system input and translation texts)
and automatic annotations such as NIST and WER.
Experiments on classification and regression using
different machine learning techniques produced not
very encouraging results. More successful experi-
ments were later run by Quirk (2004) in a similar
setting but using a smaller dataset with human qual-
ity judgments.
Specia et al (2009a) used Partial Least Squares
regression to jointly address feature selection and
model learning using a similar set of features and
datasets annotated with both automatic and human
scores. Black-box features (i.e. those extracted from
the input and translation texts only) were as discrim-
inative as glass-box features (i.e. those from the MT
system). Later work using black-box features only
focused on finding an appropriate threshold for dis-
criminating ?good? from ?bad? translations for post-
editing purposes (Specia et al, 2009b) and investi-
gating more objective ways of obtaining human an-
notation, such as post-editing time (Specia, 2011).
Recent approaches have started exploiting lin-
guistic information with promising results. Specia
et al (2011), for instance, used part-of-speech (PoS)
tagging, chunking, dependency relations and named
entities for English-Arabic quality estimation. Hard-
meier (2011) explored the use of constituency
and dependency trees for English-Swedish/Spanish
quality estimation. Focusing on word-error detec-
tion through the estimation of WER, Xiong et al
(2010) used PoS tags of neighbouring words and a
link grammar parser to detect words that are not con-
nected to the rest of the sentence. Work by Bach et
al. (2011) focused on learning patterns of linguis-
tic information (such as sequences of part-of-speech
tags) to predict sub-sentence errors. Finally, Pighin
and Ma`rquez (2011) modelled the expected projec-
tions of semantic roles from the input text into the
translations.
3 Method
Our work focuses on the use of a wide range of
linguistic information for representing different as-
pects of translation quality to complement shallow,
system-independent features that have been proved
to perform well in previous work.
3.1 Linguistic features
Non-linguistic features, such as sentence length or
n-gram statistics, are limited in their scope since
they can only account for very shallow aspects of
a translation. They convey no notion of meaning,
grammar or content and as a result they could be
very biased towards describing only superficial as-
pects. For this reason, we introduce linguistic fea-
tures that account for richer aspects of translations
and are in closer relation to the way humans make
their judgments. All of the proposed features, lin-
guistic or not, are MT-system independent.
The proposal of linguistic features was guided by
three main aspects of translation: fidelity, fluency
and coherence. The number of features that were
eventually extracted was inevitably limited by the
availability of suitable tools for the language pair
at hand, mainly for Spanish. As a result, many of
the features that were initially devised could not be
implemented (e.g. grammar checking). A total of
70 linguistic features were extracted, as summarised
below, where S and T indicate whether they refer to
the source/input or translation texts respectively:
? Sentence 3-gram log-probability and perplexity
using a language model (LM) of PoS tags [T]
? Number, percentage and ratio of content words
(N, V, ADJ) and function words (DET, PRON,
PREP, ADV) [S & T]
? Width and depth of constituency and depen-
dency trees for the input and translation texts
and their differences [S & T]
97
? Percentage of nouns, verbs and pronouns in the
sentence and their ratios between [S & T]
? Number and difference in deictic elements in
[S & T]
? Number and difference in specific types of
named entities (person, organisation, location,
other) and the total of named entities [S & T]
? Number and difference in noun, verb and
prepositional phrases [S & T]
? Number of ?dangling? (i.e. unlinked) deter-
miners [T]
? Number of explicit (pronominal, non-
pronominal) and implicit (zero pronoun)
subjects [T]
? Number of split contractions in Spanish (i.e.
al=a el, del=de el) [T]
? Number and percentage of subject-verb dis-
agreement cases [T]
? Number of unknown words estimated using a
spell checker [T]
While many of these features attempt to check
for general errors (e.g. subject verb disagreement),
others are targeted at usual MT errors (e.g. ?dan-
gling? determiners, which are commonly introduced
by SMT systems and are not linked to any words) or
target language peculiarities (e.g. Spanish contrac-
tions, zero subjects). In particular, studying deeper
aspects such as different types of subjects can pro-
vide a good indication of how natural a translation
is in Spanish, which is a pro-drop language. Such a
distinction is expected to spot unnatural expressions,
such as those caused by unnecessary pronoun repe-
tition.1
For subject classification, we identified all VPs
and categorised them according to their preceding
1E.g. (1) The girl beside me was smiling rather brightly.
She thought it was an honor that the exchange student should
be seated next to her. ? *La nin?a a mi lado estaba sonriente
bastante bien. Ella penso? que era un honor que el intercambio
de estudiantes se encuentra pro?ximo a ella. (superfluous)
(2) She is thought to have killed herself through suffocation us-
ing a plastic bag.? *Ella se cree que han matado a ella medi-
ante asfixia utilizando una bolsa de pla?stico. (confusing)
NPs. Thus, explicit subjects were classified as
pronominal (PRON+VP) or non-pronominal (NON-
PRON-NP+VP) while implicit subjects only in-
cluded elided (zero) subjects (i.e. a VP not preceded
by an NP).
Subject-verb agreement cases were estimated by
rules analysing person, number and gender matches
in explicit subject cases, considering also inter-
nal NP agreement between determiners, nouns, ad-
jectives and pronouns.2 Deictics, common coher-
ence indicators (Halliday and Hasan, 1976), were
checked against manually compiled lists.3 Unknown
words were estimated using the JMySpell4 spell
checker with the publicly available Spanish (es ES)
OpenOffice5 dictionary. In order to avoid incorrect
estimates, all named entities were filtered out before
spell-checking.
TreeTagger (Schmid, 1995) was used for PoS tag-
ging of English texts, while Freeling (Padro? et al,
2010) was used for PoS tagging in Spanish and
for constituency parsing, dependency parsing and
named entity recognition in both languages.
In order to compute n-gram statistics over PoS
tags, two language models of general and more
detailed morphosyntactic PoS were built using the
SRILM toolkit (Stolcke, 2002) on the PoS-tagged
AnCora corpus (Taule? et al, 2008).
3.2 Shallow features
In a variant of our system, the linguistic features
were complemented by a set of 77 non-linguistic
features:
? Number and proportion of unique tokens and
numbers in the sentence [S & T]
? Sentence length ratios [S & T]
? Number of non-alphabetical tokens and their
ratios [S & T]
? Sentence 3-gram perplexity [S & T]
2E.g. *Algunas de estas personas se convertira? en he?roes.
(number mismatch), *Barricadas fueron creados en la calle
Cortlandt. (gender mismatch), *Buena mentirosos esta?n cuali-
ficados en lectura. (internal NP gender and number mismatch).
3These included common deictic terms compiled from vari-
ous sources, such as hoy, all??, tu? (Spanish) or that, now or there
(English).
4http://kenai.com/projects/jmyspell
5http://www.openoffice.org/
98
? Type/Token Ratio variations: corrected TTR
(Carroll, 1964), Log TTR (Herdan, 1960),
Guiraud Index (Guiraud, 1954), Uber Index
(Dugast, 1980) and Jarvis TTR (Jarvis, 2002)
[S & T]
? Average token frequency from a monolingual
corpus [S]
? Mismatches in opening and closing brackets
and quotation marks [S & T]
? Differences in brackets, quotation marks, punc-
tuation marks and numbers [S & T]
? Average number of occurrences of all words
within the sentence [T]
? Alignment score (IBM-4) and percentage of
different types of word alignments by GIZA++
(from the SMT training alignment model pro-
vided)
Our basis for comparison is the set of 17 baseline
features, which are shallow MT system-independent
features provided by the WMT-12 QE shared task
organizers.
3.3 Building QE models
We created two main feature sets from the features
listed above for the WMT-12 QE shared task:
WLV-SHEF FS: all features, that is, baseline fea-
tures, shallow features (Section 3.2) and lin-
guistic features (Section 3.1).
WLV-SHEF BL: baseline features and linguistic
features (Section 3.1).
Additionally, we experimented with other variants
of these feature sets using 3-fold cross validation on
the training set, such as only linguistic features and
only non-linguistic features, but these yielded poorer
results and are not reported in this paper.
We address the QE problem as a regression task
by building SVM models with an epsilon regressor
and a radial basis function kernel using the LibSVM
toolkit (Chang and Lin, 2011). Values for the cost,
epsilon and gamma parameters were optimized us-
ing 5-fold cross validation on the training set.
MAE ? RMSE ? Pearson ?
Baseline 0.69 0.82 0.562
WLV-SHEF FS 0.69 0.85 0.514
WLV-SHEF BL 0.72 0.86 0.490
Table 1: Scoring performance
The training sets distributed for the shared task
comprised 1, 832 English sentences taken from news
texts and their Spanish translations produced by an
SMT system, Moses (Koehn et al, 2007), which
had been trained on a concatenation of Europarl and
news-commentaries data (from WMT-10). Transla-
tions were accompanied by a quality score derived
from an average of three human judgments of post-
editing effort using a 1-5 scale.
The models built for each of these two feature
sets were evaluated using the official test set of 422
sentences produced in the same fashion as the train-
ing set. Two sub-tasks were considered: (i) scor-
ing translations using the 1-5 quality scores, and
(ii) ranking translations from best to worse. While
quality scores were directly predicted by our mod-
els, sentence rankings were defined by ordering the
translations according to their predicted scores in de-
scending order, with no additional criteria to resolve
ties other than the natural ordering given by the sort-
ing algorithm.
4 Results and Evaluation
Table 1 shows the official results of our systems in
the scoring task in terms of Mean Absolute Error
(MAE) and Root Mean Squared Error (RMSE), the
metrics used in the shared task, as well as in terms
of Pearson correlation.
Results reveal that our models fall slightly be-
low the baseline, although this drop is not statisti-
cally significant in any of the cases (paired t-tests for
Baseline vs WLV-SHEF FS and Baseline vs WLV-
SHEF BL yield p > 0.05). This may suggest that
for this particular dataset the baseline features al-
ready cover all relevant aspects of quality on their
own, or simply that the representation of the lin-
guistic features is not appropriate for the task. The
quality of the resources used to extract the linguistic
features may also have been an issue. However, a
feature selection method may find a different com-
99
Figure 1: Comparison of true versus predicted scores
bination of features that outperforms the baseline, as
is later described in this section.
A correlation analysis between our predicted
scores and the gold standard (Figure 1) shows some
dispersion, especially for the WLV-SHEF FS set,
with lower Pearson coefficients when compared to
the baseline. The fluctuation of predicted values for
a single score is also very noticeable, spanning more
than one score band in some cases. However, if we
consider the RMSE achieved by our models, we find
that, on average, predictions deviate less than 0.9 ab-
solute points.
A closer look at the score distribution (Figure 2)
reveals our models had some difficulty predicting
scores in the 1-2 range, possibly affected by the
lower proportion of these cases in the training data.
In addition, it is interesting to see that the only sen-
tence with a true score of 1 is predicted as a very
good translation (with a score greater than 3.5). The
reason for this is that the translation has isolated
grammatical segments that our features might regard
as good but it is actually not faithful to the original.6
Although the cause for this behaviour can be traced
to inaccurate tokenisation, this reveals that our fea-
tures assess fidelity only superficially and deeper
semantically-aware indicators should be explored.
Results for the ranking task also fall below the
baseline as shown in Table 2, according to the two
official metrics: DeltaAvg and Spearman rank cor-
relation coefficient.
4.1 Further analysis
At first glance, the performance of our models seems
to indicate that the integration of linguistic infor-
6I won?t give it away. ? *He ganado ? t darle.
Figure 2: Scatter plot of true versus predicted scores
DeltaAvg ? Spearman ?
Baseline 0.55 0.58
WLV-SHEF FS 0.51 0.52
WLV-SHEF BL 0.50 0.49
Table 2: Ranking performance
mation is not beneficial, since both linguistically-
informed feature sets lead to poorer performance as
compared to the baseline feature set, which contains
only shallow, language-independent features. How-
ever, there could be many factors affecting perfor-
mance so further analysis was necessary to assess
their contribution.
Our first analysis focuses on the performance of
individual features. To this end, we built and tested
models using only one feature at a time and repeated
the process afterwards using the full WLV-SHEF FS
set without one feature at a time. In Table 3 we re-
port the 5-best and 5-worst performing features. Al-
though purely statistical features lead the rank, lin-
guistic features also appear among the top five (as
indicated by L?), showing that they can be as good
as other shallow features. It is interesting to note that
a few features appear as the top performing in both
columns (e.g. source bigrams in 4th frequency quar-
tile and target LM probability). These constitute the
truly top performing features.
Our second analysis studies the optimal subset of
features that would yield the best performance on the
test set, from which we could draw further conclu-
sions. Since this analysis requires training and test-
ing models using all the possible partitions of the
100
Rank One feature All but one feature
1 Source bigrams in 4th freq. quartile Source average token length
2 Source LM probability Source bigrams in 4th freq. quartile
3 Target LM probability Unknown words in target L?
4 Number of source bigrams Target LM probability
5 Target PoS LM probability L? Difference in constituency tree width L?
143 Percentage of target S-V agreement L? Difference in number of periods
144 Source trigrams in 2nd freq. quartile Number of source bigrams
145 Target location entities L? Target person entities L?
146 Source trigrams in 3rd freq. quartile Target Corrected TTR
147 Source average translations by inv. freq. Source trigrams in 3rd freq. quartile
Table 3: List of best and worst performing features
full feature set,7 it is infeasible in practice so we
adopted the Sequential Forward Selection method
instead (Alpaydin, 2010). Using this method, we
start from an empty set and add one feature at a time,
keeping in the set only the features that decrease the
error until no further improvement is possible. This
strategy decreases the number of iterations substan-
tially8 but it does not guarantee finding a global op-
timum. Still, a local optimum was acceptable for
our purpose. The optimal feature set found by our
selection algorithm is shown in Table 4.
Error rates are lower when using this optimal fea-
ture set (MAE=0.62 and RMSE=0.76) but the differ-
ence is only statistically significant when compared
to the baseline with 93% confidence level (paired t-
test with p <= 0.07). However, this analysis allows
us to see how many linguistic features get selected
for the optimal feature set.
Out of the total 37 features in the optimal set,
15 are linguistic (40.5%), showing that they are in
fact informative when strategically combined with
other shallow indicators. This also reveals that fea-
ture selection is a key issue for building a quality
estimation system that combines linguistic and shal-
low information. Using a sequential forward selec-
tion method, the optimal set is composed of both lin-
guistic and shallow features, reinforcing the idea that
they account for different aspects of quality and are
not interchangeable but actually complementary.
7For 147 features: 2147
8For 147 features, worst case is 147 ? (147 + 1)/2 =
10, 878.
5 Conclusions and Future Work
We have explored the use of linguistic informa-
tion for quality estimation of machine translations.
Our approach was not able to outperform a baseline
with only shallow features. However, further feature
analysis revealed that linguistic features are comple-
mentary to shallow features and must be strategi-
cally combined in order to be exploited efficiently.
The availability of linguistic tools for processing
Spanish is limited, and thus the linguistic features
used here only account for a few of the many aspects
involved in translation quality. In addition, comput-
ing linguistic information is a challenging process
for a number of reasons, mainly the fact that trans-
lations are often ungrammatical, and thus linguistic
processors may return inaccurate results, leading to
further errors.
In future work we plan to integrate more global
linguistic features such as grammar checkers, along
with deeper features such as semantic roles, hybrid
n-grams, etc. In addition, we have noticed that rep-
resenting information for input and translation texts
independently seems more appropriate than con-
trasting input and translation information within the
same feature. This representation issue is somehow
counter-intuitive and is yet to be investigated.
Acknowledgements
This research was supported by the European Com-
mission, Education & Training, Eramus Mundus:
EMMC 2008-0083, Erasmus Mundus Masters in
NLP & HLT programme.
101
Iter. Feature
1 Source bigrams in 4th frequency quartile
2 Target PoS LM probability L?
3 Source average token length
4 Guiraud Index of T
5 Unknown words in T L?
6 Difference in number of VPs between S and T L?
7 Diff. in constituency trees width of S and T L?
8 Non-alphabetical tokens in T
9 Ratio of length between S and T
10 Source trigrams in 4th frequency quartile
11 Number of content words in S L?
12 Source 3-gram perplexity
13 Ratio of PRON percentages in S and T L?
14 Number of NPs in T L?
15 Average number of source token translations with
p > 0.05 weighted by frequency
16 Source 3-gram LM probability
17 Target simple PoS LM probability L?
18 Difference in dependency trees depth of S and T L?
19 Number of NPs in S L?
20 Number of tokens in S
21 Number of content words in T L?
22 Source unigrams in 3rd frequency quartile
23 Source unigrams in 1st frequency quartile
24 Source unigrams in 2nd frequency quartile
25 Average number of source token translations with
p > 0.01 weighted by frequency
26 Ratio of non-alpha tokens in S and T
27 Difference of question marks between S and T nor-
malised by T length
28 Percentage of pron subjects in T L?
29 Percentage of verbs in T L?
30 Constituency trees width for S L?
31 Absolute diff. of question marks between S and T
32 Average num. of source token trans. with p > 0.2
33 Diff. of person entities between S and T L?
34 Diff. of periods between S and T norm. by T length
35 Diff. of semicolons between S and T normalised by
T length
36 Source 3-gram perplexity without end-of-sentence
markers
37 Absolute difference of periods between S and T
Table 4: An optimal set of features for the test set. The
number of iteration indicates the order in which features
were selected, giving a rough ranking of features by their
performance.
References
Ethem Alpaydin. 2010. Introduction to Machine Learn-
ing. Adaptive Computation and Machine Learning.
The MIT Press, Cambridge, MA, 2nd edition.
Nguyen Bach, Fei Huang, and Yaser Al-Onaizan. 2011.
Goodness: A method for measuring machine transla-
tion confidence. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 211?219,
Portland, Oregon, USA, June. Association for Compu-
tational Linguistics.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2004. Confidence estimation
for machine translation. Final Report of Johns Hop-
kins 2003 Summer Workshop on Speech and Lan-
guage Engineering, Johns Hopkins University, Balti-
more, Maryland, USA, March.
John Bissell Carroll. 1964. Language and Thought.
Prentice-Hall, Englewood Cliffs, NJ.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
A library for support vector machines. ACM Trans.
Intell. Syst. Technol., 2(3):1?27, May.
Daniel Dugast. 1980. La statistique lexicale. Slatkine,
Gene`ve.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010. Linguistic
measures for automatic machine translation evalua-
tion. Machine Translation, 24(3):209?240.
Pierre Guiraud. 1954. Les Caracte`res Statistiques du
Vocabulaire. Presses Universitaires de France, Paris.
Michael A. K. Halliday and Ruqaiya Hasan. 1976. Co-
hesion in English. Longman, London.
Christian Hardmeier. 2011. Improving machine transla-
tion quality prediction with syntactic tree kernels. In
Proceedings of the 15th conference of the European
Association for Machine Translation (EAMT 2011),
pages 233?240, Leuven, Belgium.
Gustav Herdan. 1960. Type-token Mathematics: A Text-
book of Mathematical Linguistics. Mouton & Co., The
Hague.
Scott Jarvis. 2002. Short texts, best-fitting curves and
new measures of lexical diversity. Language Testing,
19(1):57?84, January.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
102
Llus Padro?, Miquel Collado, Samuel Reese, Marina
Lloberes, and Irene Castello?n. 2010. Freeling
2.1: Five years of open-source language process-
ing tools. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC?10), Valletta, Malta, May. European Language
Resources Association (ELRA).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311?318.
Daniele Pighin and Llu??s Ma`rquez. 2011. Automatic
projection of semantic structures: an application to
pairwise translation ranking. In Fifth Workshop on
Syntax, Semantics and Structure in Statistical Trans-
lation (SSST-5), Portland, Oregon.
Christopher B. Quirk. 2004. Training a sentence-level
machine translation confidence metric. In Proceed-
ings of the International Conference on Language Re-
sources and Evaluation, volume 4 of LREC 2004,
pages 825?828, Lisbon, Portugal.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to german. In In Pro-
ceedings of the ACL SIGDAT-Workshop, pages 47?50,
Dublin, Ireland, August.
Lucia Specia, Marco Turchi, Nicola Cancedda, Marc
Dymetman, and Nello Cristianini. 2009a. Estimating
the sentence-level quality of machine translation sys-
tems. In Proceedings of the 13th Annual Conference
of the European Association for Machine Translation
(EAMT), pages 28?35, Barcelona, Spain, May.
Lucia Specia, Marco Turchi, Zhuoran Wang, John
Shawe-Taylor, and Craig Saunders. 2009b. Improv-
ing the confidence of machine translation quality esti-
mates. In Proceedings of the Twelfth Machine Transla-
tion Summit (MT Summit XII), pages 136?143, Ottawa,
Canada, August.
Lucia Specia, Najeh Hajlaoui, Catalina Hallett, and
Wilker Aziz. 2011. Predicting machine translation
adequacy. In Machine Translation Summit XIII, pages
19?23, Xiamen, China, September.
Lucia Specia. 2011. Exploiting objective annotations for
measuring translation post-editing effort. In Proceed-
ings of the 15th Conference of the European Associa-
tion for Machine Translation, pages 73?80, Leuven.
Andreas Stolcke. 2002. Srilman extensible language
modeling toolkit. In Proceedings of the 7th Inter-
national Conference on Spoken Language Process-
ing (ICSLP 2002), volume 2, pages 901?904, Denver,
USA, November.
Mariona Taule?, M. Antnia Mart??, and Marta Recasens.
2008. Ancora: Multilevel annotated corpora for cata-
lan and spanish. In Nicoletta Calzolari (Conference
Chair), Khalid Choukri, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, Stelios Piperidis, Mike Rosner, and
Daniel Tapias, editors, Proceedings of the Sixth In-
ternational Conference on Language Resources and
Evaluation (LREC?08), Marrakech, Morocco, May.
European Language Resources Association (ELRA).
Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Er-
ror detection for statistical machine translation using
linguistic features. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 604?611, Uppsala, Sweden.
103
Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 52?61,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Constrained grammatical error correction using
Statistical Machine Translation
Zheng Yuan
Computer Laboratory
University of Cambridge
United Kingdom
zy249@cam.ac.uk
Mariano Felice
Computer Laboratory
University of Cambridge
United Kingdom
mf501@cam.ac.uk
Abstract
This paper describes our use of phrase-
based statistical machine translation (PB-
SMT) for the automatic correction of er-
rors in learner text in our submission to
the CoNLL 2013 Shared Task on Gram-
matical Error Correction. Since the lim-
ited training data provided for the task
was insufficient for training an effective
SMT system, we also explored alternative
ways of generating pairs of incorrect and
correct sentences automatically from other
existing learner corpora. Our approach
does not yield particularly high perfor-
mance but reveals many problems that re-
quire careful attention when building SMT
systems for error correction.
1 Introduction
Most approaches to error correction for non-native
text are based on machine learning classifiers for
specific error types (Leacock et al, 2010; Dale
et al, 2012). Thus, for correcting determiner
or preposition errors, for example, a multiclass
model is built that uses a set of features from the
local context around the target and predicts the ex-
pected article or preposition. If the output of the
classifier is the same as the original sentence, the
sentence is not corrected. Otherwise, a correction
is made based on the predicted class. This is the
de facto approach to error correction and is widely
adopted in previous work.
Building effective classifiers requires identifica-
tion of features types from the text that discrimi-
nate well correcting each specific error type, such
as part-of-speech tags of neighbouring words, n-
gram statistics, etc., which in turn require addi-
tional linguistic resources. Classifiers designed to
correct only one type of error do not perform well
on nested or sequential errors. Correcting more
than one type of error requires building and com-
bining multiple classifiers. These factors make
the solution highly dependent on engineering de-
cisions (e.g. as regards features and algorithms)
as well as complex and laborious to extend to new
types.
An attractive and simpler alternative is to think
of error correction as a translation task. The un-
derlying idea is that a statistical machine transla-
tion (SMT) system should be able to translate text
written in ?bad? (incorrect) English into ?good?
(correct) English. An advantage of using this ap-
proach is that there is no need for an explicit en-
coding of the contexts that surround each error (i.e.
features) since SMT systems learn contextually-
appropriate source-target mappings from the train-
ing data. Likewise, they do not require any special
modification for correcting multiple error types se-
quentially, since they generate an overall corrected
version of the sentence fixing as much as possible
from what they have learnt. Provided the system is
trained using a sufficiently large parallel corpus of
incorrect-to-correct sentences, the model should
handle all the observed errors without any further
explicit information like previously detected error
types, context or error boundaries, and so forth.
The increasing performance of state-of-the-art
SMT systems also suggests they could prove suc-
cessful for other applications, such as error cor-
rection. In fact, SMT systems have been success-
fully used in a few such experiments, as we re-
port below. The work presented here builds upon
these initial experiments and explores the factors
that may affect the performance of such systems.
The remainder of this paper is organised as fol-
lows: Section 2 gives a summary of previous re-
search using SMT for error correction, Section 3
describes our approach and resources, and Sec-
tion 4 reports our experiments and results. Sec-
tion 5 discusses a number of issues related to the
performance of our system and reports some at-
52
tempts at improving it while Section 6 includes
our official performance in the shared task. Fi-
nally, Section 7 provides conclusions and ideas for
future work.
2 Related Work
Brockett et al (2006) describe the use of an
SMT system for correcting a set of 14 count-
able/uncountable nouns which are often confus-
ing for learners of English as a second language.
Their training data consists of a large corpus of
sentences extracted from news articles which were
deliberately modified to include typical countabil-
ity errors involving the target words as observed
in a Chinese learner corpus. Artificial errors are
introduced in a deterministic manner using hand-
coded rules including operations such as chang-
ing quantifiers (much? many), generating plurals
(advice? advices) or inserting unnecessary deter-
miners. Experiments show their SMT system was
generally able to beat the standard Microsoft Word
2003 grammar checker, although it produced a rel-
atively higher rate of erroneous corrections.
Similar experiments were carried out by Mizu-
moto et al (2011) for correcting Japanese as a
second language. However, their training corpus
comprised authentic learner sentences together
with corrections made by native speakers on a so-
cial learning network website. Because the origi-
nal data has no explicit annotation of error types,
the resulting SMT system is not type-constrained.
Their results show that the approach is a viable
way of obtaining very high performance at a rela-
tively low cost provided a large amount of train-
ing data is available. These claims were later
supported by similar experiments using English
texts written by Japanese students (Mizumoto et
al., 2012)
Ehsan and Faili (2013) trained SMT systems
for correcting grammatical errors and context-
sensitive spelling mistakes in English and Farsi.
Datasets are obtained by injecting artificial errors
into well-formed treebank sentences using prede-
fined error templates. Whenever an original sen-
tence from the corpus matches one of these tem-
plates, a pair of correct and incorrect sentences
is generated. This process is repeated multiple
times if a single sentence matches more than one
error template, thereby generating many pairs for
the same original sentence. A comparison be-
tween the proposed systems and rule-based gram-
mar checkers show they are complementary, with
a hybrid system achieving the best performance.
Other approaches using machine translation for
error correction are not aimed at training SMT sys-
tems but rather at using them as auxiliary tools for
producing round-trip translations (i.e. translations
into a pivot foreign language and back into En-
glish) which are used for subsequent post-editing
of the original sentence (Hermet and De?silets,
2009; Madnani et al, 2012). This differs from
our work in that we focus on training and adapt-
ing SMT systems to make all the targeted correc-
tions sequentially rather than using them as ?black
boxes? on top of which other systems are built.
3 Method
We approach error correction as a translation task
from incorrect into correct English. Several SMT
systems are built using different training data
and the best one is selected for further refine-
ment. Given the CoNLL-2013 shared task spec-
ification, systems are required to correct five spe-
cific error types involving articles and determin-
ers (ArtOrDet), noun number (Nn), prepositions
(Prep), subject-verb agreement (SVA) and verb
forms (Vform) and must ignore other errors in or-
der to achieve a good score.
3.1 Data
The training data provided for the task is a sub-
set of the NUCLE v2.3 corpus (Dahlmeier et al,
2013), which comprises essays written in English
by students at the National University of Singa-
pore. The original corpus contains around 1,400
essays, which amount to 1,220,257 tokens, but
since a portion of this data (25 essays of about 500
words each) was included in the test set, we es-
timate the remaining 1,375 essays in the training
set contain around 1,207,757 tokens. All the sen-
tences were manually annotated by human experts
using a set of 27 error types, although we used a
filtered version containing only the five types se-
lected for the shared task.
Because the size of the supplied training data
is too small to train an effective SMT system, we
used additional data from the Cambridge Learner
Corpus1 (CLC). In particular, we derived new
pairs of incorrect and correct sentences using the
1http://www.cup.cam.ac.uk/gb/elt/
catalogue/subject/custom/item3646603/
Cambridge-International-Corpus-
Cambridge-Learner-Corpus/
53
publicly available scripts from the First Certificate
in English (FCE) (Yannakoudakis et al, 2011) and
others from the International English Language
Testing System (IELTS) examinations, which in-
clude mainly academic writing. These corpora
include about 16,068 sentences (532,033 tokens)
and 64,628 sentences (1,361,841 tokens) respec-
tively. Given that the error annotation scheme used
in the CLC is more detailed than the one used in
NUCLE, a mapping had to be defined so that we
could produce corrections only for the five target
error types (Table 1).
3.2 Generating Artificial Errors
Following previous approaches, we decided to in-
crease the size of our training set by introducing
new sentences containing artificial errors. This
has many potential advantages. First, it is an eco-
nomic and efficient way of generating error-tagged
data, which otherwise requires manual annotation
and is difficult to obtain. Second, it allows us to
introduce only the types of errors we want, thus
giving us the ability to imitate the original NU-
CLE data and circumvent annotation incompati-
bility. Finally, we can choose our initial sentences
so that they match specific requirements, such as
topic, length, linguistic phenomena, etc.
Again, we use a publicly available portion of the
CLC formed by all the corrected samples featured
on the English Vocabulary Profile2 (EVP) website.
These sentences come from a variety of examina-
tions at different levels and amount to 18,830 sen-
tences and approximately 351,517 tokens.
In order to replicate NUCLE errors in EVP sen-
tences as accurately as possible, we applied the
following procedure:
1. We extract all the possible correction pat-
terns from the NUCLE v2.3 gold standard
and rewrite them as correct-fragment ?
incorrect-fragment. Two types of patterns are
extracted, one in terms of lexical items (i.e.
surface forms/words) and another using part-
of-speech (PoS) tags. Table 2 shows some
sample patterns.
2. For each correct sentence in the EVP (target),
we generate a pseudo-source sentence by ap-
plying zero or more of extracted rules.
2http://www.englishprofile.org/index.
php?option=com_content&view=article&id=
4&Itemid=5
Figure 1: An example of the artificial error injec-
tion process.
Our approach is very naive and assumes all
error-injection rules have equal probability.
The injection of errors is incremental and
non-overlapping. Figure 1 illustrates this pro-
cedure.
3. Lexical patterns take precedence over PoS
patterns. However, because the application of
a rule is decided randomly, a sentence might
end up being distorted by both types of pat-
terns, only one, or none at all (i.e. no er-
rors are introduced). In the last case, both
the source and target sentences contain cor-
rect versions.
4. A parallel corpus is built using the error-
injected sentences on the source side and
their original (correct) versions on the target
side.
As we explain in Section 4, this corpus is com-
bined with other training data in order to build dif-
ferent SMT systems.
3.3 Tools
All our systems were built using the Moses SMT
system (Koehn et al, 2007), together with Giza++
(Och and Ney, 2003) for word alignment and the
IRSTLM Toolkit (Federico et al, 2008) for lan-
guage modelling. For training factored models
(Koehn, 2010, Chapter 10) which use PoS infor-
mation, we use RASP?s PoS tagger (Briscoe et
al., 2006). Sentence segmentation, tokenisation
and PoS tagging for artificial error generation were
carried out using NLTK (Bird et al, 2009).
54
NUCLE v2.3 CLC
Error Category Tag Error Category Tag
Article or determiner ArtOrDet
Incorrect determiner inflection DI
Determiner agreement error AGD
Wrong determiner because of noun countability CD
Derivation of determiner error DD
Incorrect determiner form FD
Missing determiner MD
Replace determiner RD
Unnecessary determiner UD
Noun number Nn
Countability of noun error CN
Wrong noun form FN
Incorrect noun inflection IN
Noun agreement error AGN
Preposition Prep
Derivation of preposition error DT
Wrong preposition form FT
Missing preposition MT
Replace preposition RT
Unnecessary preposition UT
Subject-verb agreement SVA
Verb agreement error AGV
Determiner agreement error AGD
Verb form Vform
Wrong verb form FV
Incorrect verb inflection IV
Derivation of verb error DV
Incorrect tense of verb TV
Missing verb MV
Table 1: Mapping of error tags between NUCLE v2.3 and the CLC.
Lexical PoS
Pattern Example Pattern Example
has? have temperature has risen?
temperature have risen
NN? NNS information?
informations
to be used? to be use technology to be used?
technology to be use
DT NNP? NNP the US? US
during? for during the early 60s?
for the early 60s
NN VBZ VBN? NN VBP VBN expenditure is reduced ?
expenditure are reduced
Table 2: Sample error injection patterns extracted from the NUCLE v2.3 corpus.
4 Experiments and Results
We first built a baseline SMT system using only
the NUCLE v2.3 corpus and compared it to other
systems trained on incremental additions of the re-
maining corpora. All our systems were trained us-
ing 4-fold cross-validation where the training set
for each run always included the full FCE, IELTS
and EVP corpora but only 3/4 of the NUCLE data,
leaving the remaining fourth chunk for testing.
This training method allowed us to concentrate on
how the system performed on NUCLE data.
Performance was evaluated in terms of preci-
sion, recall and F1 as computed by the M2 Scorer
(Dahlmeier and Ng, 2012), with the maximum
number of unchanged words per edit set to 3 (an
initial suggestion by the shared task organisers
which was eventually changed for the official eval-
uation). The average performance of each system
is reported in Table 3.
In general, results show that precision tends to
drop as we add more training data whereas recall
and F1 slightly increase. This suggests that our
additional corpora do not resemble NUCLE very
much, although they allow the system to correct
some further errors. Contrary to our expectations,
the biggest difference between precision and re-
call is observed when we add the EVP-derived
data, which was deliberately engineered to repli-
cate NUCLE errors. Although it has been reported
that artificial errors often cause drops in perfor-
mance (Sjo?bergh and Knutsson, 2005; Foster and
Andersen, 2009), in our case this may also be due
to differences in form (e.g. sentence length, gram-
matical structures covered, error coding) and con-
tent (i.e. topics) between our source (EVP) and
target (NUCLE) corpora as well as poor control
over the artificial error generation process. In fact,
our method does not explicitly consider error con-
texts, error type distribution or other factors that
55
Model P R F1 ?
NUCLE 0.1505 0.1530 0.1517 0.0201
NUCLE+FCE 0.1547 0.1518 0.1532 0.0216
NUCLE+FCE+IELTS 0.1217 0.2068 0.1532 0.0151
NUCLE+FCE+IELTS+EVP 0.1187 0.2183 0.1538 0.0206
Table 3: Performance of our lexical SMT models.
The best results are marked in bold. Standard devi-
ation (?) indicates how stable/homogeneous each
dataset is (lower values are better).
certainly have an impact on the quality of the gen-
erated sentences and may introduce noise if not
controlled. Nevertheless, the system trained on all
four corpora yields the best F1 performance.
We also tested factored models which include
PoS information. Results are shown in Table 4.
The same behaviour is observed for the metrics,
although values for precision are now generally
higher while values for recall are lower. Again,
the best system in terms of F1 is the one trained on
all our corpora, slightly outperforming our previ-
ous best system.
5 Error Analysis and Further
Improvements
When building error correction systems, minimis-
ing the number of cases where correct language
is flagged as incorrect is often regarded as more
important than covering a large number of errors.
Technically, this means high precision is often pre-
ferred over high recall, especially when it is diffi-
cult to achieve both (as is the case for our systems).
A closer observation of the training data, transla-
tion tables and system output reveals a series of
issues that are affecting performance, which are
summarised below.
In order to test some solutions to these prob-
lems, we used our best system as a baseline and
retrained it to include each proposed modification
individually. Results are included in Table 5 and
referenced accordingly.
5.1 Size of training corpus
With slightly over a million tokens, the NUCLE
corpus seems too small to train an efficient SMT
system. However, the additional data we were able
to use differs from the NUCLE corpus in terms of
learner-level, native language, and the tasks being
attempted.
Model P R F1 ?
NUCLE 0.1989 0.1013 0.1342 0.0165
NUCLE+FCE 0.2248 0.0933 0.1319 0.0151
NUCLE+FCE+IELTS 0.1706 0.1392 0.1533 0.0163
NUCLE+FCE+IELTS+EVP 0.1696 0.1480 0.1581 0.0148
Table 4: Performance of our PoS factored
SMT models. The best results are marked in
bold. Standard deviation (?) indicates how sta-
ble/homogeneous each dataset is (lower values are
better).
5.2 Word reordering
In many cases, our system made corrections by re-
ordering words. Since the five error types in the
shared task rarely implied reordering, this caused
unnecessary edits that harmed precision, as in the
following example.
Original sentence
High Temperture Behaviour Of Candidate...
System hypothesis
High Behaviour Of Temperture Candidate...
Gold standard
High Temperture Behaviour Of Candidate...
(unchanged)
Disabling word reordering in our system helped
to avoid this problem and increased precision
without harming recall (Table 5 #1).
5.3 Limited translation model
Because of the relatively small size of our train-
ing corpus, the resulting phrase tables used by our
SMT systems contain very general alignments (i.e.
corrections) with high probability, which are often
applied in inappropriate contexts and result in a
large number of miscorrections.
In order to minimise this effect, we forced our
SMT system to output the alignments that were
used for correcting each sentence in our devel-
opment sets and deleted from the phrase table
those which consistently caused deviations from
the gold standard. This was done by manually
comparing our systems? hypotheses to their gold-
standard versions and identifying common pat-
terns in the alignments that led to miscorrections,
such as to? to the, have? have a, people? peo-
ple to, etc. 1,120 out of the total 11,421,886 align-
ments in the original translation table were re-
moved (?0,01%). Removing such alignments re-
56
sults in higher precision but lower recall, as shown
in Table 5 #2.
We also observed that the system was bi-
ased towards making unnecessary insertions of
the definite article before some specific nouns.
This means that the system would almost always
change words like cost, elderly or government for
the cost, the elderly or the government, regardless
of whether this fits the context or not. We believe
this is due to the lack of sufficient training sam-
ples where these words remained unaltered on the
source and target side, so we decided to augment
the NUCLE corpus by adding a copy of all the
corrected versions of the sentences on both sides.
Then, the system should learn that these words can
also remain unchanged in corrections. Table 5 #3
shows this improves precision but harms recall.
Out-of-vocabulary words (i.e. words not seen
during training) are a also common problem in
SMT systems, and this is directly related to the
amount of data available for training. In our sys-
tems, all out-of-vocabulary words were directly
transferred from source to target. That is, when-
ever our system encounters a word it has not seen
previously, it keeps it unchanged. Because of the
way our SMT system works, there is no explicit
generation of verb or noun forms so unless the sys-
tem has learnt this from appropriate contexts (for
example, that a progressive tense is consistently
being used after a preposition), it is unable to make
such corrections.
5.4 Inability to distinguish between
prepositions
We also observed that our systems did not often
correct prepositions. We believed this was due to
the PoS language model using the same tag for
all prepositions and therefore being unable to dis-
tinguish when each preposition must be used. In
fact, when using an ordinary PoS language model,
the original PoS patterns match those of the ex-
pected corrections (i.e. the expected correction has
a preposition and the hypothesis has one too) so no
change is proposed. The following example illus-
trates this problem.
Original sentence
... the need toward energy ...
DT NN PREP NN
System hypothesis
... the need toward energy ...
DT NN PREP NN
(unchanged)
Expected output (not in gold standard)
... the need for energy ...
DT NN PREP NN
However, when the PoS language model is
modified to use preposition-specific tags, the dif-
ference between the original sentence and the ex-
pected output should be detected and fixed by the
system, as shown below.
Original sentence
... the need toward energy ...
DT NN PREP TOWARD NN
System hypothesis
... the need for energy ...
DT NN PREP FOR NN
(unchanged)
Expected output (not in gold standard)
... the need for energy ...
DT NN PREP FOR NN
We expected this change to improve system per-
formance. Although it increased recall, it lowered
precision (Table 5 #4).
5.5 Unnecessary edits
In many cases, our system makes good corrections
which are not considered to belong to any of the
target error types, as illustrated in the following
example.
Original sentence
Thus, we should not compare now with the past
but we need to worried about the future prob-
lems that caused by this situation.
System hypothesis
Thus, we should not compare now with the past
but we need to worry about the future problems
that are caused by this situation.
Gold standard
Thus, we should not compare now with the past
but we need to worry about the future problems
that caused by this situation.
We believe this can be traced to two main
causes. First, there is no clear-cut definition of
each error type, so it is not possible to know the
annotation criteria or scope of each error type.
Therefore, inferring this information from the an-
notated examples may result in poor error map-
ping between the CLC and NUCLE, making the
system learn corrections that are not part of our
57
target set and miss others which are actually use-
ful. For example, it is not clear if ?verb form? er-
rors (Vform) include change of tense or the addi-
tion of missing verbs. Second, because SMT sys-
tems learn from all parts of a parallel corpus and
maximise fluency using a general language model,
it is hard to limit the corrections to a predefined
set of error types. Using a larger language model
based on the corrected version of the CLC con-
firms this: precision drops while recall improves
(Table 5 #5).
5.6 Gold-standard annotation
The original NUCLE corpus contains corrections
for 27 error types. However, the version used
for the shared task only includes 5 error types
and discards all the remaining corrections. Be-
cause nested and context-dependent errors are
very frequent, the systematic removal of annota-
tions which do not belong to these five types often
generates mutilated or partly-corrected sentences,
a deficiency that has also been reported in other
shared tasks (Kochmar et al, 2012). Here is a typ-
ical example.
Original sentence
These approaches may not produce effect soon,
but it is sustainable for the future generation.
Corrected sentence
These approaches may not produce [immediate
effects]Wci, but [they]Pref [are]SVA [useful]Wci
for the future [generations]Nn.
Type-constrained sentence
These approaches may not produce effect
soon, but it [are]SVA sustainable for the future
[generations]Nn.
These ill-formed sentences are particularly
harmful for SMT systems which, unlike classi-
fiers, work at a global rather than local level. As
a result, many corrections proposed by our sys-
tem are considered incorrect because they do not
match the gold-standard version, as shown below.
Original sentence
Although it is essential for all the fields, ...
System hypothesis
Although it is essential for all the fields, ...
(unchanged)
# System settings P R F1
0 NUCLE+FCE+IELTS+EVP 0.1696 0.1480 0.1581
1 Disabled reordering 0.1702 0.1480 0.1583
2 Removal of incorrect alignments 0.1861 0.1399 0.1598
3 Double NUCLE data 0.1792 0.1229 0.1458
4 Detailed Prep PoS tags 0.1632 0.1504 0.1565
5 Bigger LMs 0.1532 0.1676 0.1601
6 Final system (0+1+2+3+5) 0.1844 0.1375 0.1575
Table 5: Performance of the baseline system plus
different individual settings. Bold values indicate
an improvement over the original baseline system.
Gold standard
Although it [are]SVA essential for all the fields,
...
This raises the question of how to design an ef-
fective and challenging shared task.
5.7 Scoring criteria
The official evaluation using the M2 scorer is sen-
sitive to capitalisation and white space, although
these error types were not part of the task. Both
this fact and the lack of alternative corrections for
each gold-standard edit leave out many other valid
corrections, which in turn means true system per-
formance is underestimated.
5.8 Other factors
Differences between the training and test data can
also affect performance, such as changes in the
writers? native language, their level of language
proficiency or the topic of their compositions.
The final system submitted to the shared task
is a combination of our best factored model (i.e.
baseline) plus a selection of improvements (Ta-
ble 5 #6).
6 Official Evaluation Results
Systems were evaluated using a set of 50 essays
containing about 500 words each (?25,000 words
in total) which were written in response to two dif-
ferent prompts. One of these prompts had been
used for a subset of the training data while the
other was new. No error annotations were initially
available for this set. As we mentioned above,
the M2 scorer was set to be sensitive to capitalisa-
tion and white space as well as limit the maximum
number of unchanged tokens per edit to 2.
Initially, each participating team received their
official system results individually. After the gold-
standard annotations of the test set were released,
58
Evaluation round Corr.
edits
Prop.
edits
Gold
edits
P R F1
First (pre-revision) 166 424 1643 0.3915 0.1010 0.1606
Second (post-revision) 222 426 1565 0.5211 0.1419 0.2230
Table 6: Official results of our system before and
after revision of the test set annotations. The num-
ber of correct, proposed and gold edits are also in-
cluded for comparison.
many participants raised concerns about their ac-
curacy so they were given the opportunity to sub-
mit alternative annotations. These suggestions
were manually revised by a human annotator and
merged into a new test set which was used to re-
score all the submitted systems in a second official
evaluation round. Evaluation results of our sys-
tem in both rounds (before and after revision of
the test set annotations) are included in Table 6.
Although this measure helped overcome some of
the problems described in Section 5.6, other prob-
lems such as whitespace and case sensitivity were
not addressed.
In both evaluation rounds, our system scores
third in terms of precision, which is particularly
encouraging for error correction environments
where precision is preferred over recall. How-
ever, these values should be considerably higher
in order to prove useful in applications like self-
assessment and tutoring systems (Andersen et al,
2013).
Results also reveal precision on the test set is
considerably higher than in our cross-validation
experiments. This may be partly a result of the
larger amount of training data in our final system
and/or greater grammatical or thematic similarity
between the test and training sets.
Table 7 shows the distribution of system edits
by error type. The results suggest that lexical het-
erogeneity in the contexts surrounding errors is a
factor in performance, which might be improved
through larger training sets.
7 Conclusions and Future Work
In this paper we have described the use of SMT
techniques for building an error correction system.
We trained lexical and factored phrase-based sys-
tems using incremental combinations of training
data and observed that, in general, recall increases
at the expense of precision. However, this might
be due to structural and thematic differences in the
corpora we used. We also tried a relatively sim-
ple mechanism for injecting artificial errors into
Error Type Pre-revision Post-revisionCorr. Missed Unnec. Corr. Missed Unnec.
ArtOrDet 104 586 161 134 548 132
Nn 30 366 25 38 362 20
Prep 11 301 18 13 246 15
SVA 7 116 0 8 103 0
Vform 14 108 41 29 84 25
Other 0 0 13 0 0 12
TOTAL 166 1477 258 222 1343 204
Table 7: Distribution of system edits by error
type for the two official evaluation rounds (before
and after revision of the test annotations). ?Corr.?
stands for correct edits, ?Missed? for missed ed-
its and ?Unnec.? for unnecessary edits. The cate-
gory ?Other? includes changes made by our system
which do not belong to any of the other categories.
new data, which caused a drop in precision but in-
creased recall and F1.
Cross-validation experiments show that our sys-
tems were unable to achieve particularly high per-
formance (with precision, recall and F1 consis-
tently below 0.20). A careful analysis revealed
many factors that affect system performance, such
as annotation criteria, training parameters and cor-
pus size and heterogeneity. Our final system sub-
mitted to the CoNLL 2013 shared task was de-
signed to circumvent some of these problems and
maximise precision.
Plans for future work include more detailed er-
ror analysis and the implementation of new solu-
tions to avoid drops in performance. We would
also like to test our approach in an unrestricted
scenario (i.e. using corpora which are not limited
to a fixed number of error types) and use more
flexible evaluation schemes. We believe further
study of the methods used for generating artificial
errors is also vital to help SMT systems become a
useful approach to error correction.
Acknowledgements
We would like to thank Ted Briscoe and Ekaterina
Kochmar for their valuable comments and sugges-
tions. We are also grateful to ?istein Andersen
from iLexIR Ltd. for giving us additional feed-
back, providing us with corrected data to build
our language models and granting access to paired
samples of the CLC for training our systems. Our
gratitude goes also to Cambridge English Lan-
guage Assessment, a division of Cambridge As-
sessment, for supporting this research.
59
References
?istein E. Andersen, Helen Yannakoudakis, Fiona
Barker, and Tim Parish. 2013. Developing and test-
ing a self-assessment and tutoring system. In Pro-
ceedings of the Eighth Workshop on Innovative Use
of NLP for Building Educational Applications, BEA
2013, pages 32?41, Atlanta, GA, USA, June. Asso-
ciation for Computational Linguistics.
Steven Bird, Edward Loper, and Ewan Klein.
2009. Natural Language Processing with Python.
O?Reilly Media Inc.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Pro-
ceedings of the COLING/ACL on Interactive presen-
tation sessions, COLING-ACL ?06, pages 77?80,
Sydney, Australia. Association for Computational
Linguistics.
Chris Brockett, William B. Dolan, and Michael Ga-
mon. 2006. Correcting ESL Errors Using Phrasal
SMT Techniques. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 249?256, Sydney,
Australia, July. Association for Computational Lin-
guistics.
Daniel Dahlmeier and Hwee Tou Ng. 2012. Bet-
ter evaluation for grammatical error correction. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL 2012, pages 568 ? 572, Montreal, Canada.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a Large Annotated Corpus of
Learner English: The NUS Corpus of Learner En-
glish. In Proceedings of the 8th Workshop on In-
novative Use of NLP for Building Educational Ap-
plications, BEA 2013, Atlanta, Georgia, USA. To
appear.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. Hoo 2012: A report on the preposition and
determiner error correction shared task. In Pro-
ceedings of the Seventh Workshop on Building Ed-
ucational Applications Using NLP, pages 54?62,
Montre?al, Canada, June. Association for Computa-
tional Linguistics.
Nava Ehsan and Heshaam Faili. 2013. Grammatical
and context-sensitive error correction using a sta-
tistical machine translation framework. Software:
Practice and Experience, 43(2):187?206.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In Proceed-
ings of the 9th Annual Conference of the Interna-
tional Speech Communication Association, INTER-
SPEECH 2008, pages 1618?1621, Brisbane, Aus-
tralia, September. ISCA.
Jennifer Foster and ?istein Andersen. 2009. Gen-
errate: Generating errors for use in grammatical er-
ror detection. In Proceedings of the Fourth Work-
shop on Innovative Use of NLP for Building Edu-
cational Applications, pages 82?90, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
Matthieu Hermet and Alain De?silets. 2009. Using first
and second language models to correct preposition
errors in second language authoring. In Proceedings
of the Fourth Workshop on Innovative Use of NLP
for Building Educational Applications, EdAppsNLP
?09, pages 64?72, Boulder, Colorado. Association
for Computational Linguistics.
Ekaterina Kochmar, ?istein Andersen, and Ted
Briscoe. 2012. Hoo 2012 error recognition and cor-
rection shared task: Cambridge university submis-
sion report. In Proceedings of the Seventh Workshop
on Building Educational Applications Using NLP,
pages 242?250, Montreal, Canada. Association for
Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Prague, Czech Republic.
Association for Computational Linguistics.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, New York, NY, USA,
1st edition.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical
Error Detection for Language Learners. Morgan
and Claypool Publishers.
Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Exploring grammatical error correction with
not-so-crummy machine translation. In Proceedings
of the Seventh Workshop on Building Educational
Applications Using NLP, pages 44?53, Montreal,
Canada. Association for Computational Linguistics.
Tomoya Mizumoto, Mamoru Komachi, Masaaki Na-
gata, and Yuji Matsumoto. 2011. Mining Re-
vision Log of Language Learning SNS for Auto-
mated Japanese Error Correction of Second Lan-
guage Learners. In Proceedings of 5th International
Joint Conference on Natural Language Processing,
pages 147?155, Chiang Mai, Thailand, November.
Asian Federation of Natural Language Processing.
Tomoya Mizumoto, Yuta Hayashibe, Mamoru Ko-
machi, Masaaki Nagata, and Yuji Matsumoto. 2012.
The effect of learner corpus size in grammatical er-
ror correction of ESL writings. In Proceedings of
COLING 2012: Posters, pages 863?872, Mumbai,
60
India, December. The COLING 2012 Organizing
Committee.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51, March.
Jonas Sjo?bergh and Ola Knutsson. 2005. Faking er-
rors to avoid making errors: Very weakly supervised
learning for error detection in writing. In Proceed-
ings of RANLP 2005, pages 506?512, Borovets, Bul-
garia, September.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading esol texts. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
180?189, Portland, Oregon, USA, June. Association
for Computational Linguistics.
61
Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 15?24,
Baltimore, Maryland, 26-27 July 2014.
c?2014 Association for Computational Linguistics
Grammatical error correction using hybrid systems and type filtering
Mariano Felice Zheng Yuan ?istein E. Andersen
Helen Yannakoudakis Ekaterina Kochmar
Computer Laboratory, University of Cambridge, United Kingdom
{mf501,zy249,oa223,hy260,ek358}@cl.cam.ac.uk
Abstract
This paper describes our submission to the
CoNLL 2014 shared task on grammatical
error correction using a hybrid approach,
which includes both a rule-based and an
SMT system augmented by a large web-
based language model. Furthermore, we
demonstrate that correction type estima-
tion can be used to remove unnecessary
corrections, improving precision without
harming recall. Our best hybrid system
achieves state-of-the-art results, ranking
first on the original test set and second on
the test set with alternative annotations.
1 Introduction
Grammatical error correction has attracted con-
siderable interest in the last few years, especially
through a series of ?shared tasks?. These efforts
have helped to provide a common ground for eval-
uating and comparing systems while encouraging
research in the field. These shared tasks have pri-
marily focused on English as a second or foreign
language and addressed different error types. The
HOO 2011 task (Dale and Kilgarriff, 2011), for
example, included all error types whereas HOO
2012 (Dale et al., 2012) and the CoNLL 2013
shared task (Ng et al., 2013) were restricted to only
two and five types respectively.
In this paper, we describe our submission to the
CoNLL 2014 shared task (Ng et al., 2014), which
involves correcting all the errors in essays writ-
ten in English by students at the National Univer-
sity of Singapore. An all-type task poses a greater
challenge, since correcting open-class types (such
as spelling or collocation errors) requires different
correction strategies than those in closed classes
(such as determiners or prepositions).
In this scenario, hybrid systems or combinations
of correction modules seem more appropriate and
typically produce good results. In fact, most of
the participating teams in previous shared tasks
have used a combination of modules or systems
for their submissions, even for correcting closed-
class types (Dahlmeier et al., 2011; Bhaskar et
al., 2011; Rozovskaya et al., 2011; Ivanova et al.,
2011; Rozovskaya et al., 2013; Yoshimoto et al.,
2013; Xing et al., 2013; Kunchukuttan et al., 2013;
Putra and Szabo, 2013; Xiang et al., 2013).
In line with previous research, we present a hy-
brid approach that employs a rule-based error cor-
rection system and an ad-hoc statistical machine
translation (SMT) system, as well as a large-scale
language model to rank alternative corrections and
an error type filtering technique.
The remainder of this paper is organised as fol-
lows: Section 2 describes our approach and each
component in detail, Section 3 presents our experi-
ments using the CoNLL 2014 shared task develop-
ment set and Section 4 reports our official results
on the test set. Finally, we discuss the performance
of our system and present an error analysis in Sec-
tion 5 and conclude in Section 6.
2 Approach
We tackle the error correction task using a pipeline
of processes that combines results from multiple
systems. Figure 1 shows the interaction of the
components in our final hybrid system, producing
the results submitted to the CoNLL 2014 shared
task. The following sections describe each of these
components in detail.
2.1 Rule-based error correction system
(RBS)
The rule-based system is a component of the Self-
Assessment and Tutoring (SAT) system, a web
service developed at the University of Cambridge
aimed at helping intermediate learners of English
15
Figure 1: Overview of components and interac-
tions in our final hybrid system.
in their writing tasks
1
(Andersen et al., 2013). The
original SAT system provides three main function-
alities: 1) text assessment, producing an overall
score for a piece of text, 2) sentence evaluation,
producing a sentence-level quality score, and 3)
word-level feedback, suggesting specific correc-
tions for frequent errors. Since the focus of the
shared task is on strict correction (as opposed to
detection), we only used the word-level feedback
component of the SAT system.
This module uses rules automatically derived
from the Cambridge Learner Corpus
2
(CLC)
(Nicholls, 2003) that are aimed at detecting error-
ful unigrams, bigrams and trigrams. In order to
ensure high precision, rules are based on n-grams
that have been annotated as incorrect at least five
times and at least ninety per cent of the times
they occur. In addition to these corpus-derived
rules, many cases of incorrect but plausible deriva-
tional and inflectional morphology are detected by
means of rules derived from a machine-readable
dictionary. For further details on specific compo-
nents, we refer the reader to the aforementioned
paper.
Given an input text, the rule-based system pro-
duces an XML file containing a list of suggested
corrections. These corrections can either be ap-
plied to the original text or used to generate mul-
tiple correction candidates, as described in Sec-
tion 2.3.
2.2 SMT system
We follow a similar approach to the one described
by Yuan and Felice (2013) in order to train an SMT
1
The latest version of the system, called ?Write
& Improve?, is available at http://www.cambridge
english.org/writeandimprovebeta/.
2
More information at http://www.cambridge
.org/elt/catalogue/subject/custom/item36
46603/
system that can ?translate? from incorrect into cor-
rect English. Our training data comprises a set of
different parallel corpora, where the original (in-
correct) sentences constitute the source side and
corrected versions based on gold standard anno-
tations constitute the target side. These corpora
include:
? the NUCLE v3.1 corpus (Dahlmeier et al.,
2013), containing around 1,400 essays writ-
ten in English by students at the National
University of Singapore (approx. 1,220,257
tokens in 57,152 sentences),
? phrase alignments involving corrections ex-
tracted automatically from the NUCLE cor-
pus (with up to 7 tokens per side), which are
used to boost the probability of phrase align-
ments that involve corrections so as to im-
prove recall,
? the CoNLL 2014 shared task development
set, containing 50 essays from the previous
year?s test set (approx. 29,207 tokens in 1,382
sentences),
? the First Certificate in English (FCE) cor-
pus (Yannakoudakis et al., 2011), contain-
ing 1,244 exam scripts and 2 essays per
script (approx. 532,033 tokens in 16,068 sen-
tences),
? a subset of the International English Lan-
guage Testing System (IELTS) examination
dataset extracted from the CLC corpus, con-
taining 2,498 exam scripts and 2 essays per
script (approx. 1,361,841 tokens in 64,628
sentences), and
? a set of sentences from the English Vo-
cabulary Profile
3
(EVP), which have been
modified to include artificially generated er-
rors (approx. 351,517 tokens in 18,830 sen-
tences). The original correct sentences are a
subset of the CLC and come from examina-
tions at different proficiency levels. The ar-
tificial error generation method aims at repli-
cating frequent error patterns observed in the
NUCLE corpus on error-free sentences, as
described by Yuan and Felice (2013).
3
Sentences were automatically scraped from http://
www.englishprofile.org/index.php?option=
com_content&view=article&id=4&Itemid=5
16
Word alignment was carried out using pialign
(Neubig et al., 2011), after we found it outper-
formed GIZA++ (Och and Ney, 2000; Och and
Ney, 2003) and Berkeley Aligner (Liang et al.,
2006; DeNero and Klein, 2007) in terms of pre-
cision and F
0.5
on the development set. Instead
of using heuristics to extract phrases from the
word alignments learnt by GIZA++ or Berker-
ley Aligner, pialign created a phrase table directly
from model probabilities.
In addition to the features already defined by pi-
align, we added character-level Levenshtein dis-
tance to each mapping in the phrase table. This
was done to allow for the fact that, in error correc-
tion, most words translate into themselves and er-
rors are often similar to their correct forms. Equal
weights were assigned to these features.
We then built a lexical reordering model using
the alignments created by pialign. The maximum
phrase length was set to 7, as recommended in the
SMT literature (Koehn et al., 2003; Koehn, 2014).
The IRSTLM Toolkit (Federico et al., 2008)
was used to build a 4-gram target language model
with Kneser?Ney smoothing (Kneser and Ney,
1995) on the correct sentences from the NUCLE,
full CLC and EVP corpora.
Decoding was performed with Moses (Koehn et
al., 2007), using the default settings and weights.
No tuning process was applied. The resulting sys-
tem was used to produce the 10 best correction
candidates for each sentence in the dataset, which
were further processed by other modules.
Segmentation, tokenisation and part-of-speech
tagging were performed using NLTK (Bird et
al., 2009) for consistency with the shared task
datasets.
2.3 Candidate generation
In order to integrate corrections from multiple sys-
tems, we developed a method to generate all the
possible corrected versions of a sentence (candi-
dates). Candidates are generated by computing all
possible combinations of corrections (irrespective
of the system from which they originate), includ-
ing the original tokens to allow for a ?no correc-
tion? option. The list of candidates produced for
each sentence always includes the original (un-
modified) sentence plus any other versions derived
from system corrections.
In order for a combination of corrections to gen-
erate a valid candidate, all the corrections must be
Figure 2: An example showing the candidate gen-
eration process.
Model CE ME UE P R F
0.5
SMT IRSTLM 651 2766 1832 0.2621 0.1905 0.2438
Microsoft Web
N-grams
666 2751 1344 0.3313 0.1949 0.2907
Table 1: Performance of language models on the
development set after ranking the SMT system?s
10-best candidates per sentence. CE: correct ed-
its, ME: missed edits, UE: unnecessary edits, P:
precision, R: recall.
compatible; otherwise, the candidate is discarded.
We consider two or more corrections to be com-
patible if they do not overlap, in an attempt to
avoid introducing accidental errors. In addition,
if different correction sets produce the same can-
didate, we only keep one. Figure 2 illustrates the
candidate generation process.
2.4 Language model ranking
Generated candidates are ranked using a language
model (LM), with the most probable candidate be-
ing selected as the final corrected version.
We tried two different alternatives for ranking:
1) using the target LM embedded in our SMT sys-
tem (described in Section 2.2) and 2) using a large
n-gram LM built from web data. In the latter
case, we used Microsoft Web N-gram Services,
which provide access to large smoothed n-gram
language models (with n=2,3,4,5) built from web
documents (Gao et al., 2010). All our experiments
are based on the 5-gram ?bing-body:apr10? model.
The ranking performance of these two models
was evaluated on the 10-best hypotheses generated
by the SMT system for each sentence in the devel-
opment set. Table 1 shows the results from the
M
2
Scorer (Dahlmeier and Ng, 2012), the official
scorer for the shared task that, unlike previous ver-
sions, weights precision twice as much as recall.
Results show that using Microsoft?s Web LM
yields better performance, which is unsurprising
given the vast amounts of data used to build that
17
System CE ME UE P R F
0.5
RBS 95 3322 107 0.4703 0.0278 0.1124
SMT 452 2965 690 0.3958 0.1323 0.2830
Table 2: Results of individual systems on the de-
velopment set.
model. For this reason, we adopt Microsoft?s
model for all further experiments.
We also note that without normalisation, higher
probabilities may be assigned to shorter sentences,
which can introduce a bias towards preferring
deletions or skipping insertions.
2.5 Type filtering
Analysing performance by error type is very valu-
able for system development and tuning. How-
ever, this can only be performed for corrections
in the gold standard (either matched or missed).
To estimate types for unnecessary corrections, we
defined a set of heuristics that analyse differences
in word forms and part-of-speech tags between
the original phrases and their system corrections,
based on common patterns observed in the train-
ing data. We had previously used a similar strat-
egy to classify errors in our CoNLL 2013 shared
task submission (Yuan and Felice, 2013) but have
now included a few improvements and rules for
new types. Estimation accuracy is 50.92% on the
training set and 67.57% on the development set,
which we consider to be acceptable for our pur-
poses given that the final test set is more similar to
the development set.
Identifying types for system corrections is not
only useful during system development but can
also be exploited to filter out and reduce the num-
ber of proposed corrections. More specifically, if
a system proposes a much higher number of un-
necessary corrections than correct suggestions for
a specific error type, we can assume the system is
actually degrading the quality of the original text,
in which case it is preferable to filter out those er-
ror types. Such decisions will lower the total num-
ber of unnecessary edits, thus improving overall
precision. However, they will also harm recall,
unless the number of matched corrections for the
error type is zero (i.e. unless P
type
= 0). To avoid
this, only corrections for types having zero preci-
sion should be removed.
3 Experiments and results
We carried out a series of experiments on the de-
velopment set using different pipelines and com-
binations of systems in order to find an optimal
setting. The following sections describe them in
detail.
3.1 Individual system performance
Our first set of experiments were aimed at inves-
tigating individual system performance on the de-
velopment set, which is reported in Table 2. Re-
sults show that the SMT system has much better
performance, which is expected given that it has
been trained on texts similar to those in the test
set.
3.2 Pipelines
Since corrections from the RBS and SMT systems
are often complementary, we set out to explore
combination schemes that would integrate correc-
tions from both systems. Table 3 shows results for
different combinations, where RBS and SMT in-
dicate all corrections from the respective systems,
subscript ?c? indicates candidates generated from
a system?s individual corrections, subscript ?10-
best? indicates the 10-best list of candidates pro-
duced by the SMT system, ?>? indicates a pipeline
where the output of one system is the input to the
other and ?+? indicates a combination of candi-
dates from different systems. All these pipelines
use the RBS system as the first processing step in
order to perform an initial correction, which is ex-
tremely beneficial for the SMT system.
Results reveal that the differences between
these pipelines are small in terms of F
0.5
, although
there are noticeable variations in precision and re-
call. The best results are achieved when the 10
best hypotheses from the SMT system are ranked
with Microsoft?s LM, which confirms our results
in Table 1 showing that the SMT LM is outper-
formed by a larger web-based model.
A simple pipeline using the RBS system first
and the SMT system second (#3) yields per-
formance that is better than (or comparable to)
pipelines #1, #2 and #4, suggesting that there is no
real benefit in using more sophisticated pipelines
when only the best hypothesis from the SMT sys-
tem is used. However, performance is improved
when the 10 best SMT hypotheses are considered.
The only difference between pipelines #5 and #6
lies in the way corrections from the RBS system
18
# Pipeline CE ME UE P R F
0.5
?
1 RBS > SMT
c
> LM 372 3045 481 0.4361 0.1088 0.2723
2 RBS
c
+ SMT
c
> LM 400 3017 485 0.4520 0.1171 0.2875
3 RBS > SMT 476 2941 738 0.3921 0.1393 0.2877
4 RBS
c
> LM > SMT 471 2946 718 0.3961 0.1378 0.2881
5 RBS > SMT
10-best
> LM 678 2739 1368 0.3314 0.1984 0.2922
6 RBS
c
> LM > SMT
10-best
> LM 681 2736 1366 0.3327 0.1993 0.2934
Table 3: Results for different system pipelines on the development set.
System CE ME UE P R F
0.5
RBS
c
> LM > SMT
10-best
> LM 681 2736 1366 0.3327 0.1993 0.2934
RBS
c
> LM > SMT
10-best
> LM > Filter 681 2736 1350 0.3353 0.1993 0.2950
Table 4: Results for individual systems on the development set.
are handled. In the first case, all corrections are
applied at once whereas in the second, the sug-
gested corrections are used to generate candidates
that are subsequently ranked by our LM, often dis-
carding some of the suggested corrections.
3.3 Filtering
As described in Section 2.5, we can evaluate per-
formance by error type in order to identify and re-
move unnecessary corrections. In particular, we
tried to optimise our best hybrid system (#6) by
filtering out types with zero precision. Table 5
shows type-specific performance for this system,
where three zero-precision types can be identi-
fied: Reordering (a subset of Others that we treat
separately), Srun (run-ons/comma splices) and Wa
(acronyms). Although reordering was explicitly
disabled in our SMT system, a translation table
can still include this type of mappings if they are
observed in the training data (e.g. ?you also can?
? ?you can also?).
In order to remove such undesired corrections,
the following procedure was applied: first, in-
dividual corrections were extracted by compar-
ing the original and corrected sentences; second,
the type of each extracted correction was pre-
dicted, subsequently deleting those that matched
unwanted types (i.e. reordering, Srun or Wa); fi-
nally, the set of remaining corrections was applied
to the original text. This method improves pre-
cision while preserving recall (see Table 4), al-
though the resulting improvement is not statisti-
cally significant (paired t-test, p > 0.05).
4 Official evaluation results
Our submission to the CoNLL 2014 shared task is
the result of our best hybrid system, described in
the previous section and summarised in Figure 1.
The official test set comprised 50 new essays (ap-
prox. 30,144 tokens in 1,312 sentences) written in
response to two prompts, one of which was also
included in the training data.
Systems were evaluated using the M
2
Scorer,
which uses F
0.5
as its overall measure. As in previ-
ous years, there were two evaluation rounds. The
first one was based on the original gold-standard
annotations provided by the shared-task organis-
ers whereas the second was based on a revised
version including alternative annotations submit-
ted by the participating teams. Our submitted sys-
tem achieved the first and second place respec-
tively. The official results of our submission in
both evaluation rounds are reported in Table 6.
5 Discussion and error analysis
In order to assess how our system performed per
error type on the test set, we ran our type estima-
tion script and obtained the results shown in Ta-
ble 7. Although these results are estimated and
therefore not completely accurate,
4
they can still
provide valuable insights, at least at a coarse level.
The following sections discuss our main findings.
5.1 Type performance
According to Table 7, our system achieves the best
performance for types WOadv (adverb/adjective
position) and Wtone (tone), but these results are
4
Estimation accuracy was found to be 57.90% on the test
set.
19
Error type CE ME UE P R F
0.5
ArtOrDet 222 465 225 0.4966 0.3231 0.4485
Cit 0 6 0 ? 0.0000 ?
Mec 31 151 15 0.6739 0.1703 0.4235
Nn 138 256 136 0.5036 0.3503 0.4631
Npos 4 25 45 0.0816 0.1379 0.0889
Others 1 34 12 0.0769 0.0286 0.0575
Pform 1 25 22 0.0435 0.0385 0.0424
Pref 1 38 5 0.1667 0.0256 0.0794
Prep 61 249 177 0.2563 0.1968 0.2417
Reordering 0 1 12 0.0000 0.0000 ?
Rloc- 13 115 80 0.1398 0.1016 0.1300
SVA 32 86 25 0.5614 0.2712 0.4624
Sfrag 0 4 0 ? 0.0000 ?
Smod 0 16 0 ? 0.0000 ?
Spar 4 30 0 1.0000 0.1176 0.4000
Srun 0 55 28 0.0000 0.0000 ?
Ssub 7 64 15 0.3182 0.0986 0.2201
Trans 13 128 36 0.2653 0.0922 0.1929
Um 0 34 0 ? 0.0000 ?
V0 2 16 3 0.4000 0.1111 0.2632
Vform 28 90 68 0.2917 0.2373 0.2789
Vm 9 86 41 0.1800 0.0947 0.1525
Vt 18 137 53 0.2535 0.1161 0.2050
WOadv 0 12 0 ? 0.0000 ?
WOinc 2 35 71 0.0274 0.0541 0.0304
Wa 0 5 2 0.0000 0.0000 ?
Wci 28 400 241 0.1041 0.0654 0.0931
Wform 65 161 54 0.5462 0.2876 0.4630
Wtone 1 12 0 1.0000 0.0769 0.2941
TOTAL 681 2736 1366 0.3327 0.1993 0.2934
Table 5: Type-specific performance of our best hy-
brid system on the development set. Types with
zero precision are marked in bold.
Test set CE ME UE P R F
0.5
Original 772 1793 1172 0.3971 0.3010 0.3733
Revised 913 1749 1042 0.4670 0.3430 0.4355
Table 6: Official results of our system on the orig-
inal and revised test sets.
not truly representative as they only account for a
small fraction of the test data (0.64% and 0.36%
respectively).
The third best performing type is Mec, which
comprises mechanical errors (such as punctuation,
capitalisation and spelling mistakes) and repre-
sents 11.58% of the errors in the data. The remark-
ably high precision obtained for this error type
suggests that our system is especially suitable for
correcting such errors.
We also found that our system was particularly
good at enforcing different types of agreement, as
demonstrated by the results for SVA (subject?verb
agreement), Pref (pronoun reference), Nn (noun
number) and Vform (verb form) types, which add
up to 22.80% of the errors. The following example
shows a successful correction:
Error type CE ME UE P R F
0.5
ArtOrDet 185 192 206 0.4731 0.4907 0.4766
Mec 86 219 16 0.8431 0.2820 0.6031
Nn 122 106 143 0.4604 0.5351 0.4736
Npos 2 13 59 0.0328 0.1333 0.0386
Others 0 30 10 0.0000 0.0000 ?
Pform 8 26 21 0.2759 0.2353 0.2667
Pref 19 77 12 0.6129 0.1979 0.4318
Prep 100 159 144 0.4098 0.3861 0.4049
Reordering 0 0 7 0.0000 ? ?
Rloc- 23 89 116 0.1655 0.2054 0.1722
SVA 38 85 31 0.5507 0.3089 0.4762
Sfrag 0 4 0 ? 0.0000 ?
Smod 0 2 0 ? 0.0000 ?
Spar 0 10 0 ? 0.0000 ?
Srun 0 14 1 0.0000 0.0000 ?
Ssub 8 39 19 0.2963 0.1702 0.2581
Trans 17 54 39 0.3036 0.2394 0.2881
Um 2 21 0 1.0000 0.0870 0.3226
V0 8 20 15 0.3478 0.2857 0.3333
Vform 31 93 46 0.4026 0.2500 0.3588
Vm 7 27 35 0.1667 0.2059 0.1733
Vt 26 108 40 0.3939 0.1940 0.3266
WOadv 10 11 0 1.0000 0.4762 0.8197
WOinc 1 33 37 0.0263 0.0294 0.0269
Wci 33 305 146 0.1844 0.0976 0.1565
Wform 42 49 29 0.5915 0.4615 0.5600
Wtone 4 7 0 1.0000 0.3636 0.7407
TOTAL 772 1793 1172 0.3971 0.3010 0.3733
Table 7: Type-specific performance of our submit-
ted system on the original test set.
ORIGINAL SENTENCE:
He or she has the right not to tell anyone .
SYSTEM HYPOTHESIS:
They have the right not to tell anyone .
GOLD STANDARD:
They have the right not to tell anyone .
In other cases, our system seems to do a good
job despite gold-standard annotations:
ORIGINAL SENTENCE:
This is because his or her relatives have the
right to know about this .
SYSTEM HYPOTHESIS:
This is because their relatives have the right
to know about this .
GOLD STANDARD:
This is because his or her relatives have the
right to know about this . (unchanged)
The worst performance is observed for Others
(including Reordering) and Srun, which only ac-
count for 1.69% of the errors. We also note that
Reordering and Srun errors, which had explicitly
been filtered out, still appear in our final results,
20
which is due to differences in the edit extraction
algorithms used by the M
2
Scorer and our own im-
plementation. According to our estimations, our
system has poor performance on the Wci type (the
second most frequent), suggesting it is not very
successful at correcting idioms and collocations.
Corrections for more complex error types such
as Um (unclear meaning), which are beyond the
scope of this shared task, are inevitably missed.
5.2 Deletions
We have also observed that many mismatches be-
tween our system?s corrections and the gold stan-
dard are caused by unnecessary deletions, as in the
following example:
ORIGINAL SENTENCE:
I could understand the feeling of the carrier .
SYSTEM HYPOTHESIS:
I understand the feeling of the carrier .
GOLD STANDARD:
I could understand the feeling of the carrier .
(unchanged)
This effect is the result of using 10-best hy-
potheses from the SMT system together with LM
ranking. Hypotheses from an SMT system can in-
clude many malformed sentences which are effec-
tively discarded by the embedded target language
model and additional heuristics. However, rank-
ing these raw hypotheses with external systems
can favour deletions, as language models will gen-
erally assign higher probabilities to shorter sen-
tences. A common remedy for this is normali-
sation but we found it made no difference in our
experiments.
In other cases, deletions can be ascribed to dif-
ferences in the domain of the training and test sets,
as observed in this example:
ORIGINAL SENTENCE:
Nowadays , social media are able to dissemi-
nate information faster than any other media .
SYSTEM HYPOTHESIS:
Nowadays , the media are able to disseminate
information faster than any other media .
GOLD STANDARD:
Nowadays , social media are able to dissemi-
nate information faster than any other media .
(unchanged)
5.3 Uncredited corrections
Our analysis also reveals a number of cases where
the system introduces changes that are not in-
cluded in the gold standard but we consider im-
prove the quality of a sentence. For example:
ORIGINAL SENTENCE:
Demon is not easily to be defeated and it is
required much of energy and psychological
support .
SYSTEM HYPOTHESIS:
Demon is not easily defeated and it requires
a lot of energy and psychological support .
GOLD STANDARD:
The demon is not easily defeated and it re-
quires much energy and psychological sup-
port .
Adding alternative corrections to the gold stan-
dard alleviates this problem, although the list of
alternatives will inevitably be incomplete.
There are also a number of cases where the sen-
tences are considered incorrect as part of a longer
text but are acceptable when they are evaluated in
isolation. Consider the following examples:
ORIGINAL SENTENCE:
The opposite is also true .
SYSTEM HYPOTHESIS:
The opposite is true .
GOLD STANDARD:
The opposite is also true . (unchanged)
ORIGINAL SENTENCE:
It has erased the boundaries of distance and
time .
SYSTEM HYPOTHESIS:
It has erased the boundaries of distance and
time . (unchanged)
GOLD STANDARD:
They have erased the boundaries of distance
and time .
In both cases, system hypotheses are perfectly
grammatical but they are considered incorrect
when analysed in context. Such mismatch is the
result of discrepancies between the annotation and
evaluation criteria: while the gold standard is an-
notated taking discourse into account, system cor-
21
rections are proposed in isolation, completely de-
void of discursive context.
Finally, the inability of the M
2
Scorer to com-
bine corrections from different annotators (as op-
posed to selecting only one annotator?s corrections
for the whole sentence) can also result in underes-
timations of performance. However, it is clear that
exploring these combinations during evaluation is
a challenging task itself.
6 Conclusions
We have presented a hybrid approach to error cor-
rection that combines a rule-based and an SMT
error correction system. We have explored dif-
ferent combination strategies, including sequen-
tial pipelines, candidate generation and ranking.
In addition, we have demonstrated that error type
estimations can be used to filter out unnecessary
corrections and improve precision without harm-
ing recall.
Results of our best hybrid system on the offi-
cial CoNLL 2014 test set yield F
0.5
=0.3733 for
the original annotations and F
0.5
=0.4355 for alter-
native corrections, placing our system in the first
and second place respectively.
Error analysis reveals that our system is partic-
ularly good at correcting mechanical errors and
agreement but is often penalised for unnecessary
deletions. However, a thorough inspection shows
that the system tends to produce very fluent sen-
tences, even if they do not match gold standard
annotations.
Acknowledgements
We would like to thank Marek Rei for his valuable
feedback and suggestions as well as Cambridge
English Language Assessment, a division of Cam-
bridge Assessment, for supporting this research.
References
?istein E. Andersen, Helen Yannakoudakis, Fiona
Barker, and Tim Parish. 2013. Developing and test-
ing a self-assessment and tutoring system. In Pro-
ceedings of the Eighth Workshop on Innovative Use
of NLP for Building Educational Applications, BEA
2013, pages 32?41, Atlanta, GA, USA, June. Asso-
ciation for Computational Linguistics.
Pinaki Bhaskar, Aniruddha Ghosh, Santanu Pal, and
Sivaji Bandyopadhyay. 2011. May I check the
English of your paper!!! In Proceedings of the
Generation Challenges Session at the 13th Euro-
pean Workshop on Natural Language Generation,
pages 250?253, Nancy, France, September. Associ-
ation for Computational Linguistics.
Steven Bird, Edward Loper, and Ewan Klein.
2009. Natural Language Processing with Python.
O?Reilly Media Inc.
Daniel Dahlmeier and Hwee Tou Ng. 2012. Bet-
ter evaluation for grammatical error correction. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL 2012, pages 568?572, Montreal, Canada.
Daniel Dahlmeier, Hwee Tou Ng, and Thanh Phu Tran.
2011. NUS at the HOO 2011 Pilot Shared Task. In
Proceedings of the Generation Challenges Session
at the 13th European Workshop on Natural Lan-
guage Generation, pages 257?259, Nancy, France,
September. Association for Computational Linguis-
tics.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a Large Annotated Corpus of
Learner English: The NUS Corpus of Learner En-
glish. In Proceedings of the 8th Workshop on Inno-
vative Use of NLP for Building Educational Appli-
cations, BEA 2013, pages 22?31, Atlanta, Georgia,
USA, June.
Robert Dale and Adam Kilgarriff. 2011. Helping
Our Own: The HOO 2011 Pilot Shared Task. In
Proceedings of the Generation Challenges Session
at the 13th European Workshop on Natural Lan-
guage Generation, pages 242?249, Nancy, France,
September. Association for Computational Linguis-
tics.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A Report on the Preposition
and Determiner Error Correction Shared Task. In
Proceedings of the Seventh Workshop on Building
Educational Applications Using NLP, pages 54?62,
Montr?eal, Canada, June. Association for Computa-
tional Linguistics.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In Pro-
ceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 17?24,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In Proceed-
ings of the 9th Annual Conference of the Interna-
tional Speech Communication Association, INTER-
SPEECH 2008, pages 1618?1621, Brisbane, Aus-
tralia, September. ISCA.
Jianfeng Gao, Patrick Nguyen, Xiaolong Li, Chris
Thrasher, Mu Li, and Kuansan Wang. 2010. A
Comparative Study of Bing Web N-gram Language
Models for Web Search and Natural Language Pro-
cessing. In Web N-gram Workshop, Workshop of the
22
33rd Annual International ACM SIGIR Conference
(SIGIR 2010), pages 16?21, Geneva, Switzerland,
July.
Elitza Ivanova, Delphine Bernhard, and Cyril Grouin.
2011. Handling Outlandish Occurrences: Using
Rules and Lexicons for Correcting NLP Articles. In
Proceedings of the Generation Challenges Session
at the 13th European Workshop on Natural Lan-
guage Generation, pages 254?256, Nancy, France,
September. Association for Computational Linguis-
tics.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, volume I,
pages 181?184, Detroit, Michigan, May.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology, vol-
ume 1 of NAACL ?03, pages 48?54, Edmonton,
Canada. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Prague, Czech Republic.
Association for Computational Linguistics.
Philipp Koehn, 2014. Moses: Statistical Ma-
chine Translation System ? User Manual and Code
Guide. University of Edinburgh, April. Available
online at http://www.statmt.org/moses/
manual/manual.pdf.
Anoop Kunchukuttan, Ritesh Shah, and Pushpak Bhat-
tacharyya. 2013. IITB System for CoNLL 2013
Shared Task: A Hybrid Approach to Grammati-
cal Error Correction. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 82?87, Sofia,
Bulgaria, August. Association for Computational
Linguistics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL,
Main Conference, pages 104?111, New York City,
USA, June. Association for Computational Linguis-
tics.
Graham Neubig, Taro Watanabe, Eiichiro Sumita,
Shinsuke Mori, and Tatsuya Kawahara. 2011. An
unsupervised model for joint phrase alignment and
extraction. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 632?
641, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian
Hadiwinoto, and Joel Tetreault. 2013. The CoNLL-
2013 Shared Task on Grammatical Error Correction.
In Proceedings of the Seventeenth Conference on
Computational Natural Language Learning: Shared
Task, pages 1?12, Sofia, Bulgaria, August. Associa-
tion for Computational Linguistics.
Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian
Hadiwinoto, Raymond Hendy Susanto, and Christo-
pher Bryant. 2014. The CoNLL-2014 Shared Task
on Grammatical Error Correction. In Proceedings of
the Eighteenth Conference on Computational Natu-
ral Language Learning: Shared Task (CoNLL-2014
Shared Task), Baltimore, Maryland, USA, June. As-
sociation for Computational Linguistics. To appear.
Diane Nicholls. 2003. The Cambridge Learner Cor-
pus: Error coding and analysis for lexicography and
ELT. In Dawn Archer, Paul Rayson, Andrew Wil-
son, and Tony McEnery, editors, Proceedings of
the Corpus Linguistics 2003 conference, pages 572?
581, Lancaster, UK. University Centre for Computer
Corpus Research on Language, Lancaster Univer-
sity.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?00, pages 440?447, Hong
Kong, October. Association for Computational Lin-
guistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51, March.
Desmond Darma Putra and Lili Szabo. 2013. UdS
at CoNLL 2013 Shared Task. In Proceedings of
the Seventeenth Conference on Computational Natu-
ral Language Learning: Shared Task, pages 88?95,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Alla Rozovskaya, Mark Sammons, Joshua Gioja, and
Dan Roth. 2011. University of Illinois System in
HOO Text Correction Shared Task. In Proceedings
of the Generation Challenges Session at the 13th Eu-
ropean Workshop on Natural Language Generation,
pages 263?266, Nancy, France, September. Associ-
ation for Computational Linguistics.
Alla Rozovskaya, Kai-Wei Chang, Mark Sammons,
and Dan Roth. 2013. The University of Illinois
System in the CoNLL-2013 Shared Task. In Pro-
ceedings of the Seventeenth Conference on Compu-
tational Natural Language Learning: Shared Task,
pages 13?19, Sofia, Bulgaria, August. Association
for Computational Linguistics.
23
Yang Xiang, Bo Yuan, Yaoyun Zhang, Xiaolong Wang,
Wen Zheng, and Chongqiang Wei. 2013. A hy-
brid model for grammatical error correction. In Pro-
ceedings of the Seventeenth Conference on Compu-
tational Natural Language Learning: Shared Task,
pages 115?122, Sofia, Bulgaria, August. Associa-
tion for Computational Linguistics.
Junwen Xing, Longyue Wang, Derek F. Wong, Lidia S.
Chao, and Xiaodong Zeng. 2013. UM-Checker: A
Hybrid System for English Grammatical Error Cor-
rection. In Proceedings of the Seventeenth Confer-
ence on Computational Natural Language Learn-
ing: Shared Task, pages 34?42, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading esol texts. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
180?189, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Ippei Yoshimoto, Tomoya Kose, Kensuke Mitsuzawa,
Keisuke Sakaguchi, Tomoya Mizumoto, Yuta
Hayashibe, Mamoru Komachi, and Yuji Matsumoto.
2013. NAIST at 2013 CoNLL Grammatical Er-
ror Correction Shared Task. In Proceedings of the
Seventeenth Conference on Computational Natural
Language Learning: Shared Task, pages 26?33,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Zheng Yuan and Mariano Felice. 2013. Constrained
grammatical error correction using statistical ma-
chine translation. In Proceedings of the Seventeenth
Conference on Computational Natural Language
Learning: Shared Task, pages 52?61, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
24
