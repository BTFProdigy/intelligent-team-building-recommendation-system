Proceedings of the 43rd Annual Meeting of the ACL, pages 306?313,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Lexicalization in Crosslinguistic Probabilistic Parsing:
The Case of French
Abhishek Arun and Frank Keller
School of Informatics, University of Edinburgh
2 Buccleuch Place, Edinburgh EH8 9LW, UK
a.arun@sms.ed.ac.uk, keller@inf.ed.ac.uk
Abstract
This paper presents the first probabilistic
parsing results for French, using the re-
cently released French Treebank. We start
with an unlexicalized PCFG as a base-
line model, which is enriched to the level
of Collins? Model 2 by adding lexical-
ization and subcategorization. The lexi-
calized sister-head model and a bigram
model are also tested, to deal with the flat-
ness of the French Treebank. The bigram
model achieves the best performance:
81% constituency F-score and 84% de-
pendency accuracy. All lexicalized mod-
els outperform the unlexicalized baseline,
consistent with probabilistic parsing re-
sults for English, but contrary to results
for German, where lexicalization has only
a limited effect on parsing performance.
1 Introduction
This paper brings together two strands of research
that have recently emerged in the field of probabilis-
tic parsing: crosslinguistic parsing and lexicalized
parsing. Interest in parsing models for languages
other than English has been growing, starting with
work on Czech (Collins et al, 1999) and Chinese
(Bikel and Chiang, 2000; Levy and Manning, 2003).
Probabilistic parsing for German has also been ex-
plored by a range of authors (Dubey and Keller,
2003; Schiehlen, 2004). In general, these authors
have found that existing lexicalized parsing models
for English (e.g., Collins 1997) do not straightfor-
wardly generalize to new languages; this typically
manifests itself in a severe reduction in parsing per-
formance compared to the results for English.
A second recent strand in parsing research has
dealt with the role of lexicalization. The conven-
tional wisdom since Magerman (1995) has been that
lexicalization substantially improves performance
compared to an unlexicalized baseline model (e.g., a
probabilistic context-free grammar, PCFG). How-
ever, this has been challenged by Klein and Man-
ning (2003), who demonstrate that an unlexicalized
model can achieve a performance close to the state
of the art for lexicalized models. Furthermore, Bikel
(2004) provides evidence that lexical information
(in the form of bi-lexical dependencies) only makes
a small contribution to the performance of parsing
models such as Collins?s (1997).
The only previous authors that have directly ad-
dressed the role of lexicalization in crosslinguistic
parsing are Dubey and Keller (2003). They show
that standard lexicalized models fail to outperform
an unlexicalized baseline (a vanilla PCFG) on Ne-
gra, a German treebank (Skut et al, 1997). They
attribute this result to two facts: (a) The Negra an-
notation assumes very flat trees, which means that
Collins-style head-lexicalization fails to pick up the
relevant information from non-head nodes. (b) Ger-
man allows flexible word order, which means that
standard parsing models based on context free gram-
mars perform poorly, as they fail to generalize over
different positions of the same constituent.
As it stands, Dubey and Keller?s (2003) work does
not tell us whether treebank flatness or word order
flexibility is responsible for their results: for English,
the annotation scheme is non-flat, and the word or-
der is non-flexible; lexicalization improves perfor-
mance. For German, the annotation scheme is flat
and the word order is flexible; lexicalization fails to
improve performance. The present paper provides
the missing piece of evidence by applying proba-
bilistic parsing models to French, a language with
non-flexible word order (like English), but with a
treebank with a flat annotation scheme (like Ger-
man). Our results show that French patterns with En-
glish: a large increase of parsing performance can be
obtained by using a lexicalized model. We conclude
that the failure to find a sizable effect of lexicaliza-
tion in German can be attributed to the word order
flexibility of that language, rather than to the flatness
of the annotation in the German treebank.
The paper is organized as follows: In Section 2,
we give an overview of the French Treebank we use
for our experiments. Section 3 discusses its anno-
tation scheme and introduces a set of tree transfor-
mations that we apply. Section 4 describes the pars-
306
<NP>
<w lemma="eux" ei="PROmp"
ee="PRO-3mp" cat="PRO"
subcat="3mp">eux</w>
</NP>
Figure 1: Word-level annotation in the French Tree-
bank: eux ?they? (cat: POS tag, subcat: subcate-
gory, ei, ee: inflection)
ing models, followed by the results for the unlexi-
calized baseline model in Section 6 and for a range
of lexicalized models in Section 5. Finally, Section 7
provides a crosslinguistic comparison involving data
sets of the same size extracted from the French, En-
glish, and German treebanks.
2 The French Treebank
2.1 Annotation Scheme
The French Treebank (FTB; Abeille? et al 2000) con-
sists of 20,648 sentences extracted from the daily
newspaper Le Monde, covering a variety of authors
and domains (economy, literature, politics, etc.).1
The corpus is formatted in XML and has a rich mor-
phosyntactic tagset that includes part-of-speech tag,
?subcategorization? (e.g., possessive or cardinal), in-
flection (e.g., masculine singular), and lemma in-
formation. Compared to the Penn Treebank (PTB;
Marcus et al 1993), the POS tagset of the French
Treebank is smaller (13 tags vs. 36 tags): all punc-
tuation marks are represented as the single PONCT
tag, there are no separate tags for modal verbs, wh-
words, and possessives. Also verbs, adverbs and
prepositions are more coarsely defined. On the other
hand, a separate clitic tag (CL) for weak pronouns is
introduced. An example for the word-level annota-
tion in the FTB is given in Figure 1
The phrasal annotation of the FTB differs from
that for the Penn Treebank in several aspects. There
is no verb phrase: only the verbal nucleus (VN) is
annotated. A VN comprises the verb and any clitics,
auxiliaries, adverbs, and negation associated with it.
This results in a flat syntactic structure, as in (1).
(1) (VN (V sont) (ADV syste?matiquement) (V
arre?te?s)) ?are systematically arrested?
The noun phrases (NPs) in the FTB are also flat; a
noun is grouped together with any associated deter-
miners and prenominal adjectives, as in example (2).
Note that postnominal adjectives, however, are ad-
joined to the NP in an adjectival phrase (AP).
1The French Treebank was developed at Universite? Paris 7.
A license can be obtained by emailing Anne Abeille? (abeille@
linguist.jussieu.fr).
<w compound="yes" lemma="d?entre"
ei="P" ee="P" cat="P">
<w catint="P">d?</w>
<w catint="P">entre</w>
</w>
Figure 2: Annotation of compounds in the French
Treebank: d?entre ?between? (catint: compound-
internal POS tag)
(2) (NP (D des) (A petits) (N mots) (AP (ADV tre`s)
(A gentils))) ?small, very gentle words?
Unlike the PTB, the FTB annotates coordinated
phrases with the syntactic tag COORD (see the left
panel of Figure 3 for an example).
The treatment of compounds is also different in
the FTB. Compounds in French can comprise words
which do not exist otherwise (e.g., insu in the com-
pound preposition a` l?insu de ?unbeknownst to?) or
can exhibit sequences of tags otherwise ungrammat-
ical (e.g., a` la va vite ?in a hurry?: Prep + Det +
finite verb + adverb). To account for these proper-
ties, compounds receive a two-level annotation in
the FTB: a subordinate level is added for the con-
stituent parts of the compound (both levels use the
same POS tagset). An example is given in Figure 2.
Finally, the FTB differs from the PTB in that it
does not use any empty categories.
2.2 Data Sets
The version of the FTB made available to us (ver-
sion 1.4, May 2004) contains numerous errors. Two
main classes of inaccuracies were found in the data:
(a) The word is present but morphosyntactic tags
are missing; 101 such cases exist. (b) The tag in-
formation for a word (or a part of a compound) is
present but the word (or compound part) itself is
missing. There were 16,490 instances of this error
in the dataset.
Initially we attempted to correct the errors, but
this proved too time consuming, and we often found
that the errors cannot be corrected without access to
the raw corpus, which we did not have. We therefore
decided to remove all sentences with errors, which
lead to a reduced dataset of 10,552 sentences.
The remaining data set (222,569 words at an av-
erage sentence length of 21.1 words) was split into
a training set, a development set (used to test the
parsing models and to tune their parameters), and a
test set, unseen during development. The training set
consisted of the first 8,552 sentences in the corpus,
with the following 1000 sentences serving as the de-
velopment set and the final 1000 sentences forming
the test set. All results reported in this paper were
obtained on the test set, unless stated otherwise.
307
3 Tree Transformations
We created a number of different datasets from the
FTB, applying various tree transformation to deal
with the peculiarities of the FTB annotation scheme.
As a first step, the XML formatted FTB data was
converted to PTB-style bracketed expressions. Only
the POS tag was kept and the rest of the morphologi-
cal information for each terminal was discarded. For
example, the NP in Figure 1 was transformed to:
(3) (NP (PRO eux))
In order to make our results comparable to re-
sults from the literature, we also transformed the
annotation of punctuation. In the FTB, all punc-
tuations is tagged uniformly as PONCT. We re-
assigned the POS for punctuation using the PTB
tagset, which differentiates between commas, peri-
ods, brackets, etc.
Compounds have internal structure in the FTB
(see Section 2.1). We created two separate data sets
by applying two alternative tree transformation to
make FTB compounds more similar to compounds
in other annotation schemes. The first was collaps-
ing the compound by concatenating the compound
parts using an underscore and picking up the cat
information supplied at the compound level. For ex-
ample, the compound in Figure 2 results in:
(4) (P d? entre)
This approach is similar to the treatment of com-
pounds in the German Negra treebank (used by
Dubey and Keller 2003), where compounds are not
given any internal structure (compounds are mostly
spelled without spaces or apostrophes in German).
The second approach is expanding the compound.
Here, the compound parts are treated as individual
words with their own POS (from the catint tag),
and the suffix Cmp is appended the POS of the com-
pound, effectively expanding the tagset.2 Now Fig-
ure 2 yields:
(5) (PCmp (P d?) (P entre)).
This approach is similar to the treatment of com-
pounds in the PTB (except hat the PTB does not use
a separate tag for the mother category). We found
that in the FTB the POS tag of the compound part
is sometimes missing (i.e., the value of catint is
blank). In cases like this, the missing catint was
substituted with the cat tag of the compound. This
heuristic produces the correct POS for the subparts
of the compound most of the time.
2An alternative would be to retain the cat tag of the com-
pound. The effect of this decision needs to be investigated in
future work.
XP
X COORD
C XP
X
XP
X C XP
X
XP
X C X
Figure 3: Coordination in the FTB: before (left) and
after transformation (middle); coordination in the
PTB (right)
As mentioned previously, coordinate structures
have their own constituent label COORD in the
FTB annotation. Existing parsing models (e.g., the
Collins models) have coordination-specific rules,
presupposing that coordination is marked up in PTB
format. We therefore created additional datasets
where a transformation is applied that raises coor-
dination. This is illustrated in Figure 3. Note that
in the FTB annotation scheme, a coordinating con-
junction is always followed by a syntactic category.
Hence the resulting tree, though flatter, is still not
fully compatible with the PTB treatment of coordi-
nation.
4 Probabilistic Parsing Models
4.1 Probabilistic Context-Free Grammars
The aim of this paper is to further explore the
crosslinguistic role of lexicalization by applying lex-
icalized parsing models to the French Treebank pars-
ing accuracy. Following Dubey and Keller (2003),
we use a standard unlexicalized PCFG as our base-
line. In such a model, each context-free rule RHS ?
LHS is annotated with an expansion probability
P(RHS|LHS). The probabilities for all the rules with
the same left-hand side have to sum up to one and
the probability of a parse tree T is defined as the
product of the probabilities of each rule applied in
the generation of T .
4.2 Collins? Head-Lexicalized Models
A number of lexicalized models can then be applied
to the FTB, comparing their performance to the un-
lexicalized baseline. We start with Collins? Model 1,
which lexicalizes a PCFG by associating a word w
and a POS tag t with each non-terminal X in the
tree. Thus, a non-terminal is written as X(x) where
x = ?w, t? and X is constituent label. Each rule now
has the form:
P(h) ? Ln(ln) . . .L1(l1)H(h)R1(r1) . . .Rm(rm)(1)
Here, H is the head-daughter of the phrase, which
inherits the head-word h from its parent P. L1 . . .Ln
and R1 . . .Rn are the left and right sisters of H. Either
n or m may be zero, and n = m for unary rules.
308
The addition of lexical heads leads to an enor-
mous number of potential rules, making direct esti-
mation of P(RHS|LHS) infeasible because of sparse
data. Therefore, the generation of the RHS of a rule
given the LHS is decomposed into three steps: first
the head is generated, then the left and right sisters
are generated by independent 0th-order Markov pro-
cesses. The probability of a rule is thus defined as:
P(RHS|LHS) =
P(Ln(ln) . . .L1(l1)H(h),R1(r1) . . .Rm(rm)|P(h))
= Ph(H|P,h)??m+1i=1 Pr(Ri(ri)|P,h,H,d(i))
??n+1i=1 Pl(Li(li)|P,h,H,d(i))
(2)
Here, Ph is the probability of generating the head, Pl
and Pr are the probabilities of generating the left and
right sister respectively. Lm+1(lm+1) and Rm+1(rm+1)
are defined as stop categories which indicate when to
stop generating sisters. d(i) is a distance measure, a
function of the length of the surface string between
the head and the previously generated sister.
Collins? Model 2 further refines the initial model
by incorporating the complement/adjunct distinction
and subcategorization frames. The generative pro-
cess is enhanced to include a probabilistic choice of
left and right subcategorization frames. The proba-
bility of a rule is now:
Ph(H|P,h)?Plc(LC|P,H,h)?Prc(RC|P,H,h)
??m+1i=1 Pr(Ri(ri)|P,h,H,d(i),RC)
??n+1i=1 Pl(Li(li)|P,h,H,d(i),LC)
(3)
Here, LC and RC are left and right subcat frames,
multisets specifying the complements that the head
requires in its left or right sister. The subcat re-
quirements are added to the conditioning context. As
complements are generated, they are removed from
the appropriate subcat multiset.
5 Experiment 1: Unlexicalized Model
5.1 Method
This experiment was designed to compare the per-
formance of the unlexicalized baseline model on
four different datasets, created by the tree trans-
formations described in Section 3: compounds
expanded (Exp), compounds contracted (Cont),
compounds expanded with coordination raised
(Exp+CR), and compounds contracted with coordi-
nation raised (Cont+CR).
We used BitPar (Schmid, 2004) for our unlexi-
calized experiments. BitPar is a parser based on a
bit-vector implementation of the CKY algorithm. A
grammar and lexicon were read off our training set,
along with rule frequencies and frequencies for lex-
ical items, based on which BitPar computes the rule
Model LR LP CBs 0CB ?2CB Tag Cov
Exp 59.97 58.64 1.74 39.05 73.23 91.00 99.20
Exp+CR 60.75 60.57 1.57 40.77 75.03 91.08 99.09
Cont 64.19 64.61 1.50 46.74 76.80 93.30 98.48
Cont+CR 66.11 65.55 1.39 46.99 78.95 93.22 97.94
Table 1: Results for unlexicalized models (sentences
?40 words); each model performed its own POS
tagging.
probabilities using maximum likelihood estimation.
A frequency distribution for POS tags was also read
off the training set; this distribution is used by BitPar
to tag unknown words in the test data.
All models were evaluated using standard Par-
seval measures of labeled recall (LR), labeled pre-
cision (LP), average crossing brackets (CBs), zero
crossling brackets (0CB), and two or less crossing
brackets (?2CB). We also report tagging accuracy
(Tag), and coverage (Cov).
5.2 Results
The results for the unlexicalized model are shown in
Table 1 for sentences of length ?40 words. We find
that contracting compounds increases parsing per-
formance substantially compared to expanding com-
pounds, raising labeled recall from around 60% to
around 64% and labeled precision from around 59%
to around 65%. The results show that raising co-
ordination is also beneficial; it increases precision
and recall by 1?2%, both for expanded and for non-
expanded compounds.
Note that these results were obtained by uni-
formly applying coordination raising during evalu-
ation, so as to make all models comparable. For the
Exp and Cont models, the parsed output and the gold
standard files were first converted by raising coordi-
nation and then the evaluation was performed.
5.3 Discussion
The disappointing performance obtained for the ex-
panded compound models can be partly attributed
to the increase in the number of grammar rules
(11,704 expanded vs. 10,299 contracted) and POS
tags (24 expanded vs. 11 contracted) associated with
that transformation.
However, a more important point observation is
that the two compound models do not yield compa-
rable results, since an expanded compound has more
brackets than a contracted one. We attempted to ad-
dress this problem by collapsing the compounds for
evaluation purposes (as described in Section 3). For
example, (5) would be contracted to (4). However,
this approach only works if we are certain that the
model is tagging the right words as compounds. Un-
309
fortunately, this is rarely the case. For example, the
model outputs:
(6) (NCmp (N jours) (N commerc?ants))
But in the gold standard file, jours and commerc?ants
are two distinct NPs. Collapsing the compounds
therefore leads to length mismatches in the test data.
This problem occurs frequently in the test set, so that
such an evaluation becomes pointless.
6 Experiment 2: Lexicalized Models
6.1 Method
Parsing We now compare a series of lexicalized
parsing models against the unlexicalized baseline es-
tablished in the previous experiment. Our is was to
test if French behaves like English in that lexicaliza-
tion improves parsing performance, or like German,
in that lexicalization has only a small effect on pars-
ing performance.
The lexicalized parsing experiments were run us-
ing Dan Bikel?s probabilistic parsing engine (Bikel,
2002) which in addition to replicating the models
described by Collins (1997) also provides a con-
venient interface to develop corresponding parsing
models for other languages.
Lexicalization requires that each rule in a gram-
mar has one of the categories on its right hand side
annotated as the head. These head rules were con-
structed based on the FTB annotation guidelines
(provided along with the dataset), as well as by us-
ing heuristics, and were optimized on the develop-
ment set. Collins? Model 2 incorporates a comple-
ment/adjunct distinction and probabilities over sub-
categorization frames. Complements were marked
in the training phase based on argument identifica-
tion rules, tuned on the development set.
Part of speech tags are generated along with
the words in the models; parsing and tagging are
fully integrated. To achieve this, Bikel?s parser
requires a mapping of lexical items to ortho-
graphic/morphological word feature vectors. The
features implemented (capitalization, hyphenation,
inflection, derivation, and compound) were again
optimized on the development set.
Like BitPar, Bikel?s parser implements a prob-
abilistic version of the CKY algorithm. As with
normal CKY, even though the model is defined in
a top-down, generative manner, decoding proceeds
bottom-up. To speed up decoding, the algorithm im-
plements beam search. Collins uses a beam width of
104, while we found that a width of 105 gave us the
best coverage vs. parsing speed trade-off.
Label FTB PTB Negra Label FTB PTB Negra
SENT 5.84 2.22 4.55 VPpart 2.51 ? ?
Ssub 4.41 ? ? VN 1.76 ? ?
Sint 3.44 ? ? PP 2.10 2.03 3.08
Srel 3.92 ? ? NP 2.45 2.20 3.08
VP ? 2.32 2.59 AdvP 2.24 ? 2.08
VPinf 3.07 ? ? AP 1.34 ? 2.22
Table 2: Average number of daughter nodes per con-
stituents in three treebanks
Flatness As already pointed out in Section 2.1,
the FTB uses a flat annotation scheme. This can
be quantified by computing the average number of
daughters for each syntactic category in the FTB,
and comparing them with the figures available for
PTB and Negra (Dubey and Keller, 2003). This is
done in Table 2. The absence of sentence-internal
VPs explains the very high level of flatness for the
sentential category SENT (5.84 daughters), com-
pared to the PTB (2.44), and even to Negra, which is
also very flat (4.55 daughters). The other sentential
categories Ssub (subordinate clauses), Srel (relative
clause), and Sint (interrogative clause) are also very
flat. Note that the FTB uses VP nodes only for non-
finite subordinate clauses: VPinf (infinitival clause)
and VPpart (participle clause); these categories are
roughly comparable in flatness to the VP category
in the PTB and Negra. For NP, PPs, APs, and AdvPs
the FTB is roughly as flat as the PTB, and somewhat
less flat than Negra.
Sister-Head Model To cope with the flatness of
the FTB, we implemented three additional parsing
models. First, we implemented Dubey and Keller?s
(2003) sister-head model, which extends Collins?
base NP model to all syntactic categories. This
means that the probability function Pr in equation (2)
is no longer conditioned on the head but instead on
its previous sister, yielding the following definition
for Pr (and by analogy Pl):
Pr(Ri(ri)|P,Ri?1(ri?1),d(i))(4)
Dubey and Keller (2003) argue that this implicitly
adds binary branching to the grammar, and therefore
provides a way of dealing with flat annotation (in
Negra and in the FTB, see Table 2).
Bigram Model This model, inspired by the ap-
proach of Collins et al (1999) for parsing the Prague
Dependency Treebank, builds on Collins? Model 2
by implementing a 1st order Markov assumption for
the generation of sister non-terminals. The latter are
now conditioned, not only on their head, but also on
the previous sister. The probability function for Pr
(and by analogy Pl) is now:
Pr(Ri(ri)|P,h,H,d(i),Ri?1,RC)(5)
310
Model LR LP CBs 0CB ?2CB Tag Cov
Model 1 80.35 79.99 0.78 65.22 89.46 96.86 99.68
Model 2 80.49 79.98 0.77 64.85 90.10 96.83 99.68
SisterHead 80.47 80.56 0.78 64.96 89.34 96.85 99.57
Bigram 81.15 80.84 0.74 65.21 90.51 96.82 99.46
BigramFlat 80.30 80.05 0.77 64.78 89.13 96.71 99.57
Table 3: Results for lexicalized models (sentences
?40 words); each model performed its own POS
tagging; all lexicalized models used the Cont+CR
data set
The intuition behind this approach is that the model
will learn that the stop symbol is more likely to fol-
low phrases with many sisters. Finally, we also ex-
perimented with a third model (BigramFlat) that ap-
plies the bigram model only for categories with high
degrees of flatness (SENT, Srel, Ssub, Sint, VPinf,
and VPpart).
6.2 Results
Constituency Evaluation The lexicalized models
were tested on the Cont+CR data set, i.e., com-
pounds were contracted and coordination was raised
(this is the configuration that gave the best perfor-
mance in Experiment 1).
Table 3 shows that all lexicalized models achieve
a performance of around 80% recall and precision,
i.e., they outperform the best unlexicalized model by
at least 14% (see Table 1). This is consistent with
what has been reported for English on the PTB.
Collins? Model 2, which adds the comple-
ment/adjunct distinction and subcategorization
frames achieved only a very small improvement
over Collins? Model 1, which was not statistically
significant using a ?2 test. It might well be that
the annotation scheme of the FTB does not lend
itself particularly well to the demands of Model 2.
Moreover, as Collins (1997) mentions, some of
the benefits of Model 2 are already captured by
inclusion of the distance measure.
A further small improvement was achieved us-
ing Dubey and Keller?s (2003) sister-head model;
however, again the difference did not reach sta-
tistical significance. The bigram model, however,
yielded a statistically significant improvement over
Collins? Model 1 (recall ?2 = 3.91, df = 1, p? .048;
precision ?2 = 3.97, df = 1, p ? .046). This is con-
sistent with the findings of Collins et al (1999)
for Czech, where the bigram model upped depen-
dency accuracy by about 0.9%, as well as for En-
glish where Charniak (2000) reports an increase
in F-score of approximately 0.3%. The BigramFlat
model, which applies the bigram model to only those
labels which have a high degree of flatness, performs
Model LR LP CBs 0CB ?2CB Tag Cov
Exp+CR 65.50 64.76 1.49 42.36 77.48 100.0 97.83
Cont+CR 69.35 67.93 1.34 47.43 80.25 100.0 96.97
Model1 81.51 81.43 0.78 64.60 89.25 98.54 99.78
Model2 81.69 81.59 0.78 63.84 89.69 98.55 99.78
SisterHead 81.08 81.56 0.79 64.35 89.57 98.51 99.57
Bigram 81.78 81.91 0.78 64.96 89.12 98.81 99.67
BigramFlat 81.14 81.19 0.81 63.37 88.80 98.80 99.67
Table 4: Results for lexicalized and unlexical-
ized models (sentences ?40 words) with correct
POS tags supplied; all lexicalized models used the
Cont+CR data set
at roughly the same level as Model 1.
The models in Tables 1 and 3 implemented their
own POS tagging. Tagging accuracy was 91?93%
for BitPar (unlexicalized models) and around 96%
for the word-feature enhanced tagging model of the
Bikel parser (lexicalized models). POS tags are an
important cue for parsing. To gain an upper bound
on the performance of the parsing models, we reran
the experiments by providing the correct POS tag
for the words in the test set. While BitPar always
uses the tags provided, the Bikel parser only uses
them for words whose frequency is less than the un-
known word threshold. As Table 4 shows, perfect
tagging increased parsing performance in the lexi-
calized models by around 3%. This shows that the
poor POS tagging performed by BitPar is one of the
reasons of the poor performance of the lexicalized
models. The impact of perfect tagging is less dras-
tic on the lexicalized models (around 1% increase).
However, our main finding, viz., that lexicalized
models outperform unlexicalized models consider-
able on the FTB, remains valid, even with perfect
tagging.3
Dependency Evaluation We also evaluated our
models using dependency measures, which have
been argued to be more annotation-neutral than
Parseval. Lin (1995) notes that labeled bracketing
scores are more susceptible to cascading errors,
where one incorrect attachment decision causes the
scoring algorithm to count more than one error.
The gold standard and parsed trees were con-
verted into dependency trees using the algorithm de-
scribed by Lin (1995). Dependency accuracy is de-
fined as the ratio of correct dependencies over the to-
tal number of dependencies in a sentence. (Note that
this is an unlabeled dependency measure.) Depen-
dency accuracy and constituency F-score are shown
3It is important to note that the Collins model has a range
of other features that set it apart from a standard unlexicalized
PCFG (notably Markovization), as discussed in Section 4.2. It
is therefore likely that the gain in performance is not attributable
to lexicalization alone.
311
Model Dependency F-score
Cont+CR 73.09 65.83
Model 2 83.96 80.23
SisterHead 84.00 80.51
Bigram 84.20 80.99
Table 5: Dependency vs. constituency scores for lex-
icalized and unlexicalized models
in Table 5 for the most relevant FTB models. (F-
score is computed as the geometric mean of labeled
recall and precision.)
Numerically, dependency accuracies are higher
than constituency F-scores across the board. How-
ever, the effect of lexicalization is the same on both
measures: for the FTB, a gain of 11% in dependency
accuracy is observed for the lexicalized model.
7 Experiment 3: Crosslinguistic
Comparison
The results reported in Experiments 1 and 2 shed
some light on the role of lexicalization for parsing
French, but they are not strictly comparable to the
results that have been reported for other languages.
This is because the treebanks available for different
languages typically vary considerably in size: our
FTB training set was about 8,500 sentences large,
while the standard training set for the PTB is about
40,000 sentences in size, and the Negra training set
used by Dubey and Keller (2003) comprises about
18,600 sentences. This means that the differences in
the effect of lexicalization that we observe could be
simply due to the size of the training set: lexicalized
models are more susceptible to data sparseness than
unlexicalized ones.
We therefore conducted another experiment in
which we applied Collins? Model 2 to subsets of
the PTB that were comparable in size to our FTB
data sets. We combined sections 02?05 and 08 of
the PTB (8,345 sentences in total) to form the train-
ing set, and the first 1,000 sentences of section 23
to form our test set. As a baseline model, we also
run an unlexicalized PCFG on the same data sets.
For comparison with Negra, we also include the re-
sults of Dubey and Keller (2003): they report the
performance of Collins? Model 1 on a data set of
9,301 sentences and a test set of 1,000 sentences,
which are comparable in size to our FTB data sets.4
The results of the crosslinguistic comparison are
shown in Table 6.5 We conclude that the effect of
4Dubey and Keller (2003) report only F-scores for the re-
duced data set (see their Figure 1); the other scores were pro-
vided by Amit Dubey. No results for Model 2 are available.
5For this experiments, the same POS tagging model was ap-
plied to the PTB and the FTB data, which is why the FTB fig-
Corpus Model LR LP CBs 0CB ?2CB
FTB Cont+CR 66.11 65.55 1.39 46.99 78.95
Model 2 79.20 78.58 0.83 63.33 89.23
PTB Unlex 72.79 75.23 2.54 31.56 58.98
Model 2 86.43 86.79 1.17 57.80 82.44
Negra Unlex 69.64 67.27 1.12 54.21 82.84
Model 1 68.33 67.32 0.83 60.43 88.78
Table 6: The effect of lexicalization on different cor-
pora for training sets of comparable size (sentences
?40 words)
lexicalization is stable even if the size of the train-
ing set is held constant across languages: For the
FTB we find that lexicalization increases F-score by
around 13%. Also for the PTB, we find an effect of
lexicalization of about 14%. For the German Negra
treebank, however, the performance of the lexical-
ized and the unlexicalized model are almost indis-
tinguishable. (This is true for Collins? Model 1; note
that Dubey and Keller (2003) do report a small im-
provement for the lexicalized sister-head model.)
8 Related Work
We are not aware of any previous attempts to build
a probabilistic, treebank-trained parser for French.
However, there is work on chunking for French. The
group who built the French Treebank (Abeille? et al,
2000) used a rule-based chunker to automatically
annotate the corpus with syntactic structures, which
were then manually corrected. They report an un-
labeled recall/precision of 94.3/94.2% for opening
brackets and 92.2/91.4% for closing brackets, and a
label accuracy of 95.6%. This result is not compara-
ble to our results for full parsing.
Giguet and Vergne (1997) present use a memory-
based learner to predict chunks and dependencies
between chunks. The system is evaluated on texts
from Le Monde (different from the FTB texts). Re-
sults are only reported for verb-object dependencies,
for which recall/precision is 94.04/96.39%. Again,
these results are not comparable to ours, which were
obtained using a different corpus, a different depen-
dency scheme, and for a full set of dependencies.
9 Conclusions
In this paper, we provided the first probabilis-
tic, treebank-trained parser for French. In Exper-
iment 1, we established an unlexicalized baseline
model, which yielded a labeled precision and re-
call of about 66%. We experimented with a num-
ber of tree transformation that take account of the
peculiarities of the annotation of the French Tree-
ures are slightly lower than in Table 3.
312
bank; the best performance was obtained by rais-
ing coordination and contracting compounds (which
have internal structure in the FTB). In Experiment 2,
we explored a range of lexicalized parsing models,
and found that lexicalization improved parsing per-
formance by up to 15%: Collins? Models 1 and 2
performed at around 80% LR and LP. No signifi-
cant improvement could be achieved by switching to
Dubey and Keller?s (2003) sister-head model, which
has been claimed to be particularly suitable for tree-
banks with flat annotation, such as the FTB. A small
but significant improvement (to 81% LR and LP)
was obtained by a bigram model that combines fea-
tures of the sister-head model and Collins? model.
These results have important implications for
crosslinguistic parsing research, as they allow us
to tease apart language-specific and annotation-
specific effects. Previous work for English (e.g.,
Magerman, 1995; Collins, 1997) has shown that lex-
icalization leads to a sizable improvement in pars-
ing performance. English is a language with non-
flexible word order and with a treebank with a non-
flat annotation scheme (see Table 2). Research on
German (Dubey and Keller, 2003) showed that lex-
icalization leads to no sizable improvement in pars-
ing performance for this language. German has a
flexible word order and a flat treebank annotation,
both of which could be responsible for this counter-
intuitive effect. The results for French presented in
this paper provide the missing piece of evidence:
they show that French behaves like English in that
it shows a large effect of lexicalization. Like En-
glish, French is a language with non-flexible word
order, but like the German Treebank, the French
Treebank has a flat annotation. We conclude that
Dubey and Keller?s (2003) results for German can be
attributed to a language-specific factor (viz., flexible
word order) rather than to an annotation-specific fac-
tor (viz., flat annotation). We confirmed this claim in
Experiment 3 by showing that the effects of lexical-
ization observed for English, French, and German
are preserved if the size of the training set is kept
constant across languages.
An interesting prediction follows from the claim
that word order flexibility, rather than flatness of
annotation, is crucial for lexicalization. A language
which has a flexible word order (like German), but
a non-flat treebank (like English) should show no
effect of lexicalization, i.e., lexicalized models are
predicted not to outperform unlexicalized ones. In
future work, we plan to test this prediction for Ko-
rean, a flexible word order language whose treebank
(Penn Korean Treebank) has a non-flat annotation.
References
Abeille?, Anne, Lionel Clement, and Alexandra Kinyon. 2000.
Building a treebank for French. In Proceedings of the 2nd In-
ternational Conference on Language Resources and Evalu-
ation. Athens.
Bikel, Daniel M. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In Proceedings of the
2nd International Conference on Human Language Technol-
ogy Research. Morgan Kaufmann, San Francisco.
Bikel, Daniel M. 2004. A distributional analysis of a lexicalized
statistical parsing model. In Dekang Lin and Dekai Wu, ed-
itors, Proceedings of the Conference on Empirical Methods
in Natural Language Processing. Barcelona, pages 182?189.
Bikel, Daniel M. and David Chiang. 2000. Two statistical pars-
ing models applied to the Chinese treebank. In Proceedings
of the 2nd ACL Workshop on Chinese Language Processing.
Hong Kong.
Charniak, Eugene. 2000. A maximum-entropy-inspired parser.
In Proceedings of the 1st Conference of the North American
Chapter of the Association for Computational Linguistics.
Seattle, WA, pages 132?139.
Collins, Michael. 1997. Three generative, lexicalised models
for statistical parsing. In Proceedings of the 35th Annual
Meeting of the Association for Computational Linguistics
and the 8th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics. Madrid, pages 16?23.
Collins, Michael, Jan Hajic?, Lance Ramshaw, and Christoph
Tillmann. 1999. A statistical parser for Czech. In Pro-
ceedings of the 37th Annual Meeting of the Association for
Computational Linguistics. University of Maryland, College
Park.
Dubey, Amit and Frank Keller. 2003. Probabilistic parsing for
German using sister-head dependencies. In Proceedings of
the 41st Annual Meeting of the Association for Computa-
tional Linguistics. Sapporo, pages 96?103.
Giguet, Emmanuel and Jacques Vergne. 1997. From part-of-
speech tagging to memory-based deep syntactic analysis. In
Proceedings of the International Workshop on Parsing Tech-
nologies. Boston, pages 77?88.
Klein, Dan and Christopher Manning. 2003. Accurate unlexi-
calized parsing. In Proceedings of the 41st Annual Meeting
of the Association for Computational Linguistics. Sapporo.
Levy, Roger and Christopher Manning. 2003. Is it harder to
parse Chinese, or the Chinese treebank? In Proceedings of
the 41st Annual Meeting of the Association for Computa-
tional Linguistics. Sapporo.
Lin, Dekang. 1995. A dependency-based method for evaluating
broad-coverage parsers. In Proceedings of the International
Joint Conference on Artificial Intelligence. Montreal, pages
1420?1425.
Magerman, David. 1995. Statistical decision-tree models for
parsing. In Proceedings of the 33rd Annual Meeting of the
Association for Computational Linguistics. Cambridge, MA,
pages 276?283.
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: The Penn Treebank. Computational Linguistics
19(2):313?330.
Schiehlen, Michael. 2004. Annotation strategies for probabilis-
tic parsing in German. In Proceedings of the 20th Interna-
tional Conference on Computational Linguistics. Geneva.
Schmid, Helmut. 2004. Efficient parsing of highly ambiguous
context-free grammars with bit vectors. In Proceedings of
the 20th International Conference on Computational Lin-
guistics. Geneva.
Skut, Wojciech, Brigitte Krenn, Thorsten Brants, and Hans
Uszkoreit. 1997. An annotation scheme for free word order
languages. In Proceedings of the 5th Conference on Applied
Natural Language Processing. Washington, DC.
313
Proceedings of the Third Workshop on Statistical Machine Translation, pages 139?142,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Towards better Machine Translation Quality
for the German?English Language Pairs
Philipp Koehn Abhishek Arun Hieu Hoang
School of Informatics
University of Edinburgh
pkoehn@inf.ed.ac.uk a.arun@sms.ed.ac.uk h.hoang@sms.ed.ac.uk
Abstract
The Edinburgh submissions to the shared task
of the Third Workshop on Statistical Machine
Translation (WMT-2008) incorporate recent
advances to the open source Moses system.
We made a special effort on the German?
English and English?German language pairs,
leading to substantial improvements.
1 Introduction
Edinburgh University participated in the shared task
of the ThirdWorkshop on Statistical Machine Trans-
lation (WMT-2008), which is partly funded by the
EUROMATRIX project, which also funds our work.
In this project, we set out to build machine trans-
lation systems for all language pairs of official EU
languages. Hence, we also participated in the shared
task in all language pairs.
For all language pairs, we used the Moses decoder
(Koehn et al, 2007), which follows the phrase-based
statistical machine translation approach (Koehn
et al, 2003), with default settings as a starting
point. We recently added minimum Bayes risk de-
coding and reordering constraints to the decoder. We
achieved consistent increase in BLEU scores with
these improvements, showing gains of up to 0.9%
BLEU on the 2008 news test set.
Most of our efforts were focused on the language
pairs German?English and English?German. For
both language pairs, we explored language-specific
and more general improvements, resulting in gains
of up to 1.5% BLEU for German?English and 1.4%
BLEU for English?German.
2 Recent Improvements
Over the last months, we added minimum Bayes risk
decoding and additional reordering constraints to the
Moses decoder. The WMT-2008 shared task offered
the opportunity to assess these components over a
large range of language pairs and tasks.
For all our experiments, we trained solely on the
Europarl corpus, which allowed us to treat the 2007
news commentary test set (nc-test2007) as a stand-
in for the 2008 news test set (news-2008), for which
we have no in-domain training data. This may have
resulted in lower performance due to less (and very
relevant) training data, but it also allowed us to opti-
mize for a true out-of-domain test set.
The baseline training uses Moses default param-
eters. We use a maximum sentence length of 80, a
phrase translation table with the five traditional fea-
tures, lexicalized reordering, and lowercase training
and test data. All reported BLEU scores are not case-
sensitive, computed using the NIST tool.
2.1 Minimum Bayes Risk Decoding
Minimum Bayes risk decoding was proposed by Ku-
mar and Byrne (2004). Instead of selecting the trans-
lation with the highest probability, minimum Bayes
risk decoding selects the translation that is most sim-
ilar to the highest scoring translations. Intuitively,
this avoid the selection of an outlier as the best trans-
lation, since the decision rule prefers translations
that are similar to other high-scoring translations.
Minimum Bayes risk decoding is defined as:
eMBR = argmaxe
?
e?
L(e, e?) p(e?|f)
As similarity function L, we use sentence-level
BLEU with add-one smoothing. As highest scoring
translations, we consider the top 100 distinct trans-
lations, for which we convert the translation scores
into a probability distribution p (with a scaling fac-
tor of 1). We tried other n-best list sizes and scaling
factors, with very similar outcomes.
139
Language Pair Baseline MBR MP MBR+MP
Spanish?German news 11.7 11.8 (+0.1) 11.9 (+0.2) 12.0 (+0.3)
Spanish?German ep 20.7 21.0 (+0.3) 20.8 (+0.1) 21.0 (+0.3)
German?Spanish news 16.2 16.3 (+0.1) 16.4 (+0.2) 16.6 (+0.4)
German?Spanish ep 28.5 28.6 (+0.1) 28.5 (?0.0) 28.6 (+0.1)
Spanish?English news 19.8 20.2 (+0.4) 20.2 (+0.4) 20.3 (+0.5)
Spanish?English ep 33.6 33.7 (+0.1) 33.6 (?0.0) 33.7 (+0.1)
English?Spanish news 20.1 20.5 (+0.4) 20.5 (+0.4) 20.7 (+0.6)
English?Spanish ep 33.1 33.1 (?0.0) 33.0 (?0.1) 33.1 (?0.0)
French?English news 18.5 19.1 (+0.6) 19.1 (+0.6) 19.2 (+0.7)
French?English ep 33.5 33.5 (?0.0) 33.4 (?0.1) 33.5 (?0.0)
English?French news 17.8 18.0 (+0.2) 18.2 (+0.4) 18.3 (+0.5)
English?French ep 31.1 31.1 (?0.0) 31.1 (?0.0) 31.1 (?0.0)
Czech?English news 14.2 14.4 (+0.2) 14.3 (+0.1) 14.5 (+0.3)
Czech?English nc 22.8 23.0 (+0.2) 22.9 (+0.2) 23.0 (+0.2)
English?Czech news 9.6 9.6 (?0.0) 9.7 (+0.1) 9.6 (?0.0)
English?Czech nc 12.9 13.0 (+0.1) 12.9 (?0.0) 13.0 (+0.1)
Hungarian?English news 7.9 8.3 (+0.4) 8.5 (+0.6) 8.8 (+0.9)
English?Hungarian news 6.1 6.3 (+0.2) 6.4 (+0.3) 6.5 (+0.4)
average news - +0.26 +0.33 +0.46
average ep - +0.08 ?0.02 +0.08
Table 1: Improvements in BLEU on the test sets test2008 (ep), newstest2008 (news) and nc-test2008 (nc) for minimum
Bayes risk decoding (MBR) and the monotone-at-punctuation reordering (MP) constraint.
2.2 Monotone at Punctuation
The reordering models in phrase-based translation
systems are known to be weak, since they essentially
relies on the interplay of language model, a general
preference for monotone translation, and (in the case
of lexicalized reordering) a local model based on a
window of neighboring phrase translations. Allow-
ing any kind of reordering typically reduces transla-
tion performance, so reordering is limited to a win-
dow of (in our case) six words.
One noticeable weakness is that the current model
frequently reorders words beyond clause bound-
aries, which is almost never well-motivated, and
leads to confusing translations. Since clause bound-
aries are often indicated by punctuation such as
comma, colon, or semicolon, it is straight-forward
to introduce a reordering constraint that addresses
this problem.
Our implementation of a monotone-at-punc-
tuation reordering constraint (Tillmann and Ney,
2003) requires that all input words before clause-
separating punctuation have be translated, before
words afterwards are covered. Note that this con-
straint does not limit in any way phrase translations
that span punctuation.
2.3 Results
Table 1 summarizes the impact of minimum
Bayes risk decoding (MBR) and the monotone-
at-punctuation reordering constraint (MP). Scores
show higher gains for out-of-domain news test sets
(+0.46) than for in-domain Europarl sets (+0.08).
3 German?English
Translating between German and English is surpris-
ingly difficult, given that the languages are closely
related. The main sources for this difficulty is the
different syntactic structure at the clause level and
the rich German morphology, including the merging
of noun compounds.
In prior work, we addressed reordering with a
pre-order model that transforms German for train-
ing and testing according to a set of hand-crafted
rules (Collins et al, 2005). Employing this method
to our baseline system leads to an improvement of
+0.8 BLEU on the nc-test2007 set and +0.5 BLEU on
the test2007 set.
140
German?English nc-test2007 test2007
baseline 20.3 27.6
tokenize hyphens 20.1 (?0.2) 27.6 (?0.0)
tok. hyph. + truecase 20.7 (+0.4) 27.8 (+0.2)
Table 2: Impact of truecasing on case-sensitive BLEU
In a more integrated approach, factored transla-
tion models (Koehn and Hoang, 2007) allow us to
consider grammatical coherence in form of part-
of-speech language models. When translating into
output words, we also generate a part-of-speech tag
along with each output word. Since there are only 46
POS tags in English, we are able to train high-order
n-gram models of these sequences. In our experi-
ments, we used a 7-gram model, yielding improve-
ments of +0.2/?0.1. We obtained the POS tags using
Brill?s tagger (Brill, 1995).
Next, we considered the problem of unknown in-
put words, which is partly due to hyphenated words,
noun compounds, and morphological variants. Us-
ing the baseline model, 907 words (1.78%) in nc-
test2007 and 262 (0.47%) in test2007 are unknown.
First we separate our hyphens by tokenizing words
such as high-risk into high @-@ risk. This reduces
the number of unknown words to 791/224. Unfor-
tunately, it hurts us in terms of BLEU (?0.1/?0.1).
Second, we split compounds using the frequency-
based method (Koehn and Knight, 2003), reducing
the number of unknown words to than half, 424/94,
improving BLEU on nc-test2007 (+0.5/?0.2).
A final modification to the data preparation is
truecasing. Traditionally, we lowercase all training
and test data, but especially in German, case marks
important distinctions. German nouns are capital-
ized, and keeping case allows us to make the dis-
tinction between, say, the noun Wissen (knowledge)
and the verb wissen (to know). By truecasing, we
only change the case of the first word of a sentence
to its most common form. This method still needs
some refinements, such as the handling of headlines
or all-caps text, but it did improve performance over
the hyphen-tokenized baseline (+0.3/+0.2) and the
original baseline (+0.2/+0.1).
Note that truecasing simplifies the recasing prob-
lem, so a better way to gauge its effect is to look
at the case-sensitive BLEU score. Here the dif-
ference are slightly larger over both the hyphen-
tokenized baseline (+0.6/+0.2) and the original base-
German?English nc-test2007 test2007
baseline 21.3 28.4
pos lm 21.5 (+0.2) 28.3 (?0.1)
reorder 22.1 (+0.8) 28.9 (+0.5)
tokenize hyphens 21.2 (?0.1) 28.3 (?0.1)
tok. hyph. + split 21.8 (+0.5) 28.2 (?0.2)
tok. hyph. + truecase 21.5 (+0.2) 28.5 (+0.1)
mp 21.6 (+0.3) 28.2 (?0.2)
mbr 21.4 (+0.1) 28.3 (?0.1)
big beam 21.3 (?0.0) 28.3 (?0.1)
Table 3: Impact of individual modifications for German?
English, measured in BLEU on the development sets
German?English nc-test2007 test2007
baseline 21.3 28.4
+ reorder 22.1 (+0.8) 28.9 (+0.5)
+ tokenize hyphens 22.1 (+0.8) 28.9 (+0.5)
+ truecase 22.7 (+1.3) 28.9 (+0.5)
+ split 23.0 (+1.7) 29.1 (+0.7)
+ mbr 23.1 (+1.8) 29.3 (+0.9)
+ mp 23.3 (+2.0) 29.2 (+0.8)
Table 4: Impact of combined modifications for German?
English, measured in BLEU on the development sets
line (+0.4/+0.2). See the Table 2 for details.
As for the other language pairs, using the
monotone-at-punctuation reordering constraint
(+0.3/?0.2) and minimum Bayes risk decoding
(+0.1/?0.1) mostly helps. We also tried bigger
beam sizes (stack size 1000, phrase table limit 50),
but without gains in BLEU (?0.0/?0.1).
Table 3 summarizes the contributions of the indi-
vidual modifications we described above. For our fi-
nal system, we added the improvements one by one
(see Table 4), except for the bigger beam size and
the POS language model. This led to an overall in-
crease of +2.0/+0.8 over the baseline. Due to a bug
in splitting, the system we submitted to the shared
task had a score of only +1.5/+0.6 over the baseline.
4 English?German
For English?German, we applied many of the same
methods as for the inverse language pair. Tok-
enizing out hyphens has questionable impact (?
0.1/+0.1), while truecasing shows minor gains
(?0.0/+0.1), slightly higher for case-sensitive scor-
ing (+0.2/+0.3). We have not yet developed a
method that is the analog of the compound splitting
141
English?German nc-test2007 test-2007
baseline 14.6 21.0
tokenize hyphens 14.5 (?0.1) 21.1 (+0.1)
tok. hyph. + truecase 14.6 (?0.0) 21.1 (+0.1)
morph lm 15.7 (+1.1) 21.2 (+0.2)
mbr 14.9 (+0.3) 21.0 (?0.0)
mp 14.8 (+0.2) 20.9 (?0.1)
big beam 14.7 (+0.1) 21.0 (?0.0)
Table 5: Impact of individual modifications for English?
German, measured in BLEU on the development sets
method ? compound merging. We consider this an
interesting challenge for future work.
While the rich German morphology on the source
side mostly poses sparse data problems, on the tar-
get side it creates the problem of which morpholog-
ical variant to choose. The right selection hinges
on grammatical agreement within noun phrases, the
role that each noun phrase plays in the clause, and
the grammatical nature of the subject of a verb. We
use LoPar (Schmidt and Schulte im Walde, 2000),
which gives us morphological features such as
case, gender, count, although in limited form, it of-
ten opts for more general categories such as not gen-
itive. We include these features in a sequence model,
as we used a sequence model over part-of-speech
tags previously. The gains of this method are espe-
cially strong for the out-of-domain set (+1.1/+0.2).
Minimum Bayes risk decoding (+0.3/?0.0),
themonotone-at-punctuation reordering constraint
(+0.2/?0.1), and bigger beam sizes (+0.1/?0.0)
have similar impact as for the other language pairs.
See Table 5 for a summary of all modifications. By
combining everything except for the bigger beam
size, we obtain overall gains of +1.4/+0.4 over the
baseline. For details, refer to Table 6.
5 Conclusions
We built Moses systems trained on either only Eu-
roparl data or, for Czech and Hungarian, the avail-
able training data. We showed gains with minimum
Bayes risk decoding and a reordering constraint in-
volving punctuation. For German?English, we em-
ployed further language-specific improvements.
Acknowledgements: This work was supported in part
under the EuroMatrix project funded by the European
Commission (6th Framework Programme).
English?German nc-test2007 test2007
baseline 14.6 21.0
+ tokenize hyphens 14.5 (?0.1) 21.1 (+0.1)
+ truecase 14.6 (?0.0) 21.1 (+0.1)
+ morph lm 15.4 (+0.8) 21.3 (+0.3)
+ mbr 15.7 (+1.1) 21.4 (+0.4)
+ mp 16.0 (+1.4) 21.4 (+0.4)
Table 6: Impact of combined modifications for English?
German, measured in BLEU on the development sets
References
Brill, E. (1995). Transformation-based error-driven learning
and natural language processing: A case study in part of
speech tagging. Computational Linguistics, 21(4).
Collins, M., Koehn, P., and Kucerova, I. (2005). Clause re-
structuring for statistical machine translation. In Proceed-
ings of the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?05), pages 531?540, Ann Arbor,
Michigan. Association for Computational Linguistics.
Koehn, P. and Hoang, H. (2007). Factored translation mod-
els. In Proceedings of the 2007 Joint Conference on Empiri-
cal Methods in Natural Language Processing and Computa-
tional Natural Language Learning (EMNLP-CoNLL), pages
868?876.
Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico,
M., Bertoldi, N., Cowan, B., Shen, W., Moran, C., Zens, R.,
Dyer, C., Bojar, O., Constantin, A., and Herbst, E. (2007).
Moses: Open source toolkit for statistical machine trans-
lation. In Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics Companion Vol-
ume Proceedings of the Demo and Poster Sessions, pages
177?180, Prague, Czech Republic. Association for Compu-
tational Linguistics.
Koehn, P. and Knight, K. (2003). Empirical methods for com-
pound splitting. In Proceedings of Meeting of the Euro-
pean Chapter of the Association of Computational Linguis-
tics (EACL).
Koehn, P., Och, F. J., and Marcu, D. (2003). Statistical phrase
based translation. In Proceedings of the Joint Conference on
Human Language Technologies and the Annual Meeting of
the North American Chapter of the Association of Computa-
tional Linguistics (HLT-NAACL).
Kumar, S. and Byrne, W. (2004). Minimum bayes-risk decod-
ing for statistical machine translation. In Proceedings of the
Joint Conference on Human Language Technologies and the
Annual Meeting of the North American Chapter of the Asso-
ciation of Computational Linguistics (HLT-NAACL).
Schmidt, H. and Schulte im Walde, S. (2000). Robust German
noun chunking with a probabilistic context-free grammar. In
Proceedings of the International Conference on Computa-
tional Linguistics (COLING).
Tillmann, C. and Ney, H. (2003). Word reordering and a dy-
namic programming beam search algorithm for statistical
machine translation. Computational Linguistics, 29(1).
142
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 102?110,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Monte Carlo inference and maximization for phrase-based translation
Abhishek Arun?
a.arun@sms.ed.ac.uk
Phil Blunsom?
pblunsom@inf.ed.ac.uk
Chris Dyer?
redpony@umd.edu
Adam Lopez?
alopez@inf.ed.ac.uk
Barry Haddow?
bhaddow@inf.ed.ac.uk
Philipp Koehn?
pkoehn@inf.ed.ac.uk
?Department of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
?Department of Linguistics
University of Maryland
College Park, MD 20742, USA
Abstract
Recent advances in statistical machine
translation have used beam search for
approximate NP-complete inference within
probabilistic translation models. We present
an alternative approach of sampling from the
posterior distribution defined by a translation
model. We define a novel Gibbs sampler
for sampling translations given a source
sentence and show that it effectively explores
this posterior distribution. In doing so
we overcome the limitations of heuristic
beam search and obtain theoretically sound
solutions to inference problems such as
finding the maximum probability translation
and minimum expected risk training and
decoding.
1 Introduction
Statistical machine translation (SMT) poses the
problem: given a foreign sentence f , find the
translation e? that maximises the conditional
posterior probability p(e|f). This probabilistic
formulation of translation has driven development
of state-of-the-art systems which are able to learn
from parallel corpora which were generated for
other purposes ? a direct result of employing a
mathematical framework that we can reason about
independently of any particular model.
For example, we can train SMT models using
maximum likelihood estimation (Brown et al, 1993;
Och and Ney, 2000; Marcu and Wong, 2002). Alter-
natively, we can train to minimise probabilistic con-
ceptions of risk (expected loss) with respect to trans-
lation metrics, thereby obtaining better results for
those metrics (Kumar and Byrne, 2004; Smith and
Eisner, 2006; Zens and Ney, 2007). We can also use
Bayesian inference techniques to avoid resorting to
heuristics that damage the probabilistic interpreta-
tion of the models (Zhang et al, 2008; DeNero et
al., 2008; Blunsom et al, 2009).
Most models define multiple derivations for each
translation; the probability of a translation is thus
the sum over all of its derivations. Unfortunately,
finding the maximum probability translation is NP-
hard for all but the most trivial of models in this
setting (Sima?an, 1996). It is thus necessary to resort
to approximations for this sum and the search for its
maximum e?.
The most common of these approximations is
the max-derivation approximation, which for many
models can be computed in polynomial time via
dynamic programming (DP). Though effective for
some problems, it has many serious drawbacks for
probabilistic inference:
1. It typically differs from the true model maxi-
mum.
2. It often requires additional approximations in
search, leading to further error.
3. It introduces restrictions on models, such as
use of only local features.
4. It provides no good solution to compute the
normalization factor Z(f) required by many prob-
abilistic algorithms.
In this work, we solve these problems using a
Monte Carlo technique with none of the above draw-
backs. Our technique is based on a novel Gibbs
sampler that draws samples from the posterior dis-
tribution of a phrase-based translation model (Koehn
et al, 2003) but operates in linear time with respect
to the number of input words (Section 2). We show
102
that it is effective for both decoding (Section 3) and
minimum risk training (Section 4).
2 A Gibbs sampler for phrase-based
translation models
We begin by assuming a phrase-based translation
model in which the input sentence, f , is segmented
into phrases, which are sequences of adjacent
words.1 Each foreign phrase is translated into the
target language, to produce an output sentence e
and an alignment a representing the mapping from
source to target phrases. Phrases are allowed to be
reordered during translation.
The model is defined with a log-linear form,
with feature function vector h and parametrised by
weight vector ?, as described in Koehn et al (2003).
P (e, a|f ;?) = exp [? ? h(e, a, f)]?
?e?,a?? exp [? ? h(e?, a?, f)]
(1)
The features h of the model are usually few and
are themselves typically probabilistic models
indicating e.g, the relative frequency of a target
phrase translation given a source phrase (translation
model), the fluency of the target phrase (language
model) and how phrases reorder with respect
to adjacent phrases (reordering model). There
is a further parameter ? that limits how many
source language words may intervene between
two adjacent target language phrases. For the
experiments in this paper, we use ? = 6.
2.1 Gibbs sampling
We use Markov chain Monte Carlo (MCMC) as an
alternative to DP search (Geman and Geman, 1984;
Metropolis and Ulam, 1949). MCMC probabilis-
tically generates sample derivations from the com-
plete search space. The probability of generating
each sample is conditioned on the previous sam-
ple, forming a Markov chain. After a long enough
interval (referred to as the burn-in) this chain returns
samples from the desired distribution.
Our MCMC sampler uses Gibbs sampling, which
obtains samples from the joint distribution of a set
of random variables X = {X1, . . . , Xn}. It starts
with some initial state (X1 = x10, . . . , Xn = xn0),
and generates a Markov chain of samples, where
1These phrases are not necessarily linguistically motivated.
each sample is the result of applying a set of Gibbs
operators to the previous sample. Each operator is
defined by specifying a subset of the random vari-
ables Y ? X , which the operator updates by sam-
pling from the conditional distribution P (Y |X \Y ).
The set X \ Y is referred to as the Markov blanket
and is unchanged by the operator.
In the case of translation, we require a Gibbs sam-
pler that produces a sequence of samples, SN1 =
(e1, a1) . . . (eN , aN ), that are drawn from the dis-
tribution P (e, a|f). These samples can thus be used
to estimate the expectation of a function h(e, a, f)
under the distribution as follows:
EP (a,e|f)[h] = limN??
1
N
N?
i=1
h(ai, ei, f) (2)
Taking h to be an indicator function
h = ?(a, a?)?(e, e?) provides an estimate of
P (a?, e?|f), and using h = ?(e, e?) marginalises over
all derivations a?, yielding an estimate of P (e?|f).
2.2 Gibbs operators
Our sampler consists of three operators. Examples
of these are depicted in Figure 1.
The RETRANS operator varies the translation of a
single source phrase. Segmentation, alignment, and
all other translations are held constant.
The MERGE-SPLIT operator varies the source
segmentation at a single word boundary. If the
boundary is a split point in the current hypothesis,
the adjoining phrases can be merged, provided
that the corresponding target phrases are adjacent
and the phrase table contains a translation of the
merged phrase. If the boundary is not a split point,
the covering phrase may be split, provided that
the phrase table contains a translation of both new
phrases. Remaining segmentation points, phrase
alignment and phrase translations are held constant.
The REORDER operator varies the target phrase
order for a pair of source phrases, provided that
the new alignment does not violate reordering limit
?. Segmentation, phrase translations, and all other
alignments are held constant.
To illustrate the RETRANS operator, we will
assume a simplified model with two features: a
bigram language model Plm and a translation model
Ptm. Both features are assigned a weight of 1.
103
c?est un re?sultat remarquable
it is some result remarkable
(a)
Initial
c?est un re?sultat remarquable
but some result remarkable
(b)
Retrans
c?est un re?sultat remarquable
it is a result remarkable
(c)
Merge
c?est un re?sultat remarquable
it is a remarkable result
(d)
Reorder
1
Figure 1: Example evolution of an initial hypothesis via
application of several operators, with Markov blanket
indicated by shading.
We denote the start of the sentence with S and the
language model context with C. Assuming the
French phrase c?est can be translated either as it is or
but, the RETRANS operator at step (b) stochastically
chooses an English phrase, e? in proportion to the
phrases? conditional probabilities.
P (but|c?est,C) = Ptm(but|c?est) ? Plm(S but some)Z
and
P (it is|c?est,C) = Ptm(it is|c?est) ? Plm(S it is some)Z
where
Z = Ptm(but|c?est) ? Plm(S but some) +
Ptm(it is|c?est) ? Plm(S it is some)
Conditional distributions for the MERGE-SPLIT and
REORDER operators can be derived in an analogous
fashion.
A complete iteration of the sampler consists of
applying each operator at each possible point in the
sentence, and a sample is collected after each opera-
tor has performed a complete pass.
2.3 Algorithmic complexity
Since both the RETRANS and MERGE-SPLIT oper-
ators are applied by iterating over source side word
positions, their complexity is linear in the size of the
input.
The REORDER operator iterates over the positions
in the input and for the source phrase found at that
position considers swapping its target phrase with
that of every other source phrase, provided that the
reordering limit is not violated. This means that it
can only consider swaps within a fixed-length win-
dow, so complexity is linear in sentence length.
2.4 Experimental verification
To verify that our sampler was behaving as expected,
we computed the KL divergence between its
inferred distribution q?(e|f) and the true distribution
over a single sentence (Figure 2). We computed
the true posterior distribution p(e|f) under an
Arabic-English phrase-based translation model
with parameters trained to maximise expected
BLEU (Section 4), summing out the derivations for
identical translations and computing the partition
term Z(f). As the number of iterations increases,
the KL divergence between the distributions
approaches zero.
3 Decoding
The task of decoding amounts to finding the single
translation e? that maximises or minimises some cri-
terion given a source sentence f . In this section
we consider three common approaches to decod-
ing, maximum translation (MaxTrans), maximum
derivation (MaxDeriv), and minimum risk decoding
(MinRisk):
e? =
?
?
?
argmax(e,a) p(e, a|f) (MaxDeriv)
argmaxe p(e|f) (MaxTrans)
argmine?e? `e?(e)p(e?|f) (MinRisk)
In the minimum risk decoder, `e?(e) is any real-
valued loss (error) function that computes the error
of one hypothesis e with respect to some reference
e?. Our loss is a sentence-level approximation of
(1 ? BLEU).
As noted in section 2, the Gibbs sampler can
be used to provide an estimate of the probability
distribution P (a, e|f) and therefore to determine
the maximum of this distribution, in other words
the most likely derivation. Furthermore, we can
marginalise over the alignments to estimate P (e|f)
104
Iterations
KL d
iverg
ence
l l l
l l
l
l l
l
l
l
l
l l l
l l
10 100 1000 10000 100000 1000000
0.00
1
0.01
0.1
KL Divergence
Figure 2: The KL divergence of the true posterior distri-
bution and the distribution estimated by the Gibbs sam-
pler at different numbers of iterations for the Arabic
source sentence r}ys wzrA? mAlyzyA yzwr Alflbyn (in
English, The prime minister of Malaysia visits the Philip-
pines).
and so obtain the most likely translation. The Gibbs
sampler can therefore be used as a decoder, either
running in max-derivation and max-translation
mode. Using the Gibbs sampler in this way makes
max-translation decoding tractable, and so will
help determine whether max-translation offers any
benefit over the usual max-derivation. Using the
Gibbs sampler as a decoder also allows us to verify
that it is producing valid samples from the desired
distribution.
3.1 Training data and preparation.
The experiments in this section were performed
using the French-English and German-English
parallel corpora from the WMT09 shared translation
task (Callison-Burch et al, 2009), as well as 300k
parallel Arabic-English sentences from the NIST
MT evaluation training data.2 For all language
pairs, we constructed a phrase-based translation
model as described in Koehn et al (2003), limiting
the phrase length to 5. The target side of the parallel
corpus was used to train a 3-gram language model.
2The Arabic-English training data consists of the
eTIRR corpus (LDC2004E72), the Arabic news corpus
(LDC2004T17), the Ummah corpus (LDC2004T18), and the
sentences with confidence c > 0.995 in the ISI automatically
extracted web parallel corpus (LDC2006T02).
For the German and French systems, the DEV2006
set was used for model tuning and the TEST2007
(in-domain) and NEWS-DEV2009B (out-of-domain)
sets for testing. For the Arabic system, the MT02
set (10 reference translations) was used for tuning
and MT03 (4 reference translations) was used for
evaluation. To reduce the size of the phrase table,
we used the association-score technique suggested
by Johnson et al (2007a). Translation quality is
reported using case-insensitive BLEU (Papineni et
al., 2002).
3.2 Translation performance
For the experiments reported in this section, we
used feature weights trained with minimum error
rate training (MERT; Och, 2003) . Because MERT
ignores the denominator in Equation 1, it is invari-
ant with respect to the scale of the weight vector
? ? the Moses implementation simply normalises
the weight vector it finds by its `1-norm. However,
when we use these weights in a true probabilistic
model, the scaling factor affects the behaviour of
the model since it determines how peaked or flat the
distribution is. If the scaling factor is too small, then
the distribution is too flat and the sampler spends
too much time exploring unimportant probability
regions. If it is too large, then the distribution is too
peaked and the sampler may concentrate on a very
narrow probability region. We optimised the scaling
factor on a 200-sentence portion of the tuning set,
finding that a multiplicative factor of 10 worked best
for fr-en and a multiplicative factor of 6 for de-en. 3
The first experiment shows the effect of different
initialisations and numbers of sampler iterations on
max-derivation decoding performance of the sam-
pler. The Moses decoder (Koehn et al, 2007) was
used to generate the starting hypothesis, either in
full DP max-derivation mode, or alternatively with
restrictions on the features and reordering, or with
zero weights to simulate a random initialisation, and
the number of iterations varied from 100 to 200,000,
with a 100 iteration burn-in in each case. Figure 3
shows the variation of model score with sampler iter-
ation, for the different starting points, and for both
language pairs.
3We experimented with annealing, where the scale factor is
gradually increased to sharpen the distribution while sampling.
However, we found no improvements with annealing.
105
?
20.1
?
20.0
?
19.9
?
19.8
?
19.7
?
19.6
Iterations
Mod
el sc
ore
100 1000 10000
French?English
Initialisationfull
mono
nolm
zero
?
40.6
?
40.4
?
40.2
?
40.0
?
39.8
Iterations
Mod
el sc
ore
100 1000 10000 100000
German?English
Initialisationfull
mono
nolm
zero
Figure 3: Mean maximum model score, as a function of iteration number and starting point. The starting point can
either be the full max-derivation translation (full), the monotone translation (mono), the monotone translation with no
language model (nolm) or the monotone translation with all weights set to zero (zero).
Comparing the best model scores found by the
sampler, with those found by the Moses decoder
with its default settings, we found that around
50,000 sampling iterations were required for
fr-en and 100,000 for de-en, for the sampler to
give equivalent model scores to Moses. From
Figure 3 we can see that the starting point did not
have an appreciable effect on the model score of
the best derivation, except with low numbers of
iterations. This indicates that the sampler is able
to move fairly quickly towards the maximum of
the distribution from any starting point, in other
words it has good mobility. Running the sampler
for 100,000 iterations took on average 1670 seconds
per sentence on the French-English data set and
1552 seconds per sentence on German-English.
A further indication of the dependence of sampler
accuracy on the iteration count is provided by Fig-
ure 4. In this graph, we show the mean Spearman?s
rank correlation between the nbest lists of deriva-
tions when ranked by (i) model score and (ii) the
posterior probability estimated by the sampler. This
measure of sampler accuracy also shows a logarith-
mic dependence on the sample size.
3.3 Minimum risk decoding
The sampler also allows us to perform minimum
Bayes risk (MBR) decoding, a technique introduced
by Kumar and Byrne (2004). In their work, as an
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Iterations
Corr
elati
on
100 1000 10000 100000
Language Pairsfr?ende?en
Figure 4: Mean Spearman?s rank correlation of 1000-best
list of derivations ranked according to (i) model score and
(ii) posterior probability estimated by sampler. This was
measured on a 200 sentence subset of DEV2006.
approximation of the model probability distribution,
the expected loss of the decoder is calculated by
summing over an n-best list. With the Gibbs sam-
pler, however, we should be able to obtain a much
more accurate view of the model probability distri-
bution. In order to compare max-translation, max-
derivation and MBR decoding with the Gibbs sam-
pler, and the Moses baseline, we ran experiments
106
fr-en de-en
in out in out
Moses 32.7 19.1 27.4 15.9
MaxD 32.6 19.1 27.0 15.5
MaxT 32.6 19.1 27.4 16.0
MBR 32.6 19.2 27.3 16.0
Table 1: Comparison of the BLEU score of the Moses
decoder with the sampler running in max-derivation
(MaxD), max-translation (MaxT) and minumum Bayes
risk (MBR) modes. The test sets are TEST2007 (in) and
NEWS-DEV2009B (out)
on both European language pairs, using both the in-
domain and out-of-domain test sets. The sampler
was initialised with the output of Moses with the
feature weights set to zero and restricted to mono-
tone, and run for 100,000 iterations with a 100 iter-
ation burn-in. The scale factors were set to the same
values as in the previous experiment. The relative
translation quality (measured according to BLEU) is
shown in Table 1.
3.4 Discussion
These results show very little difference between the
decoding methods, indicating that the Gibbs sam-
pling decoder can perform as well as a standard DP
based max-derivation decoder with these models,
and that there is no gain from doing max-translation
or MBR decoding. However it should be noted that
the model used for these experiments was optimised
by MERT, for max-derivation decoding, and so the
experiments do not rule out the possibility that max-
translation and MBR decoding will offer an advan-
tage on an appropriately optimised model.
4 Minimum risk training
In the previous section, we described how our sam-
pler can be used to search for the best translation
under a variety of decoding criteria (max deriva-
tion, translation, and minimum risk). However, there
appeared to be little benefit to marginalizing over
the latent derivations. This is almost certainly a side
effect of the MERT training approach that was used
to construct the models so as to maximise the per-
formance of the model on its single best derivation,
without regard to the shape of the rest of the dis-
tribution (Blunsom et al, 2008). In this section we
describe a further application of the Gibbs sampler:
to do unbiased minimum risk training.
While there have been at least two previous
attempts to do minimum risk training for MT, both
approaches relied on biased k-best approximations
(Smith and Eisner, 2006; Zens and Ney, 2007).
Since we sample from the whole distribution, we
will have a more accurate risk assessment.
The risk, or expected loss, of a probabilistic trans-
lation model on a corpus D, defined with respect to
a particular loss function `e?(e), where e? is the refer-
ence translation and e is a hypothesis translation
L = ?
?e?,f??D
?
e
p(e|f)`e?(e) (3)
This value can be trivially computed using equa-
tion (2). In this section, we are concerned with find-
ing the parameters ? that minimise (3). Fortunately,
with the log-linear parameterization of p(e|f), L is
differentiable with respect to ?:
?L
??k
= ?
?e?,f??D
?
e
p(e|f)`e?(e)
(
hk ? Ep(e|f)[hk]
)
(4)
Equation (4) is slightly more complicated to com-
pute using the sampler since it requires the feature
expectation in order to evaluate the final term. How-
ever, this can be done simply by making two passes
over the samples, computing the feature expecta-
tions on the first pass and the gradient on the second.
We have now shown how to compute our
objective (3), the expected loss, and a gradient
with respect to the model parameters we want
to optimise, (4), so we can use any standard
first-order optimization technique. Since the
sampler introduces stochasticity into the gradient
and objective, we use stochastic gradient descent
methods which are more robust to noise than
more sophisticated quasi-Newtonian methods
like L-BFGS (Schraudolph et al, 2007). For the
experiments below, we updated the learning rate
after each step proportionally to difference in
successive gradients (Schraudolph, 1999).
For the experiments reported in this section, we
used sample sizes of 8000 and estimated the gradi-
ent on sets of 100 sentences drawn randomly (with
replacement) from the development corpus. For a
107
Training Decoder MT03
Moses Max Derivation 44.6
MERT Moses MBR 44.8
Gibbs MBR 44.9
Moses Max Derivation 40.6
MinRisk MaxTrans 41.8
Gibbs MBR 42.9
Table 2: Decoding with minimum risk trained systems,
compared with decoding with MERT-trained systems on
Arabic to English MT03 data
loss function we use 4-gram (1 ? BLEU) computed
individually for each sentence4. By examining per-
formance on held-out data, we find the model con-
verges typically in fewer than 20 iterations.
4.1 Training experiments
During preliminary experiments with training, we
observed on a held-out data set (portions of MT04)
that the magnitude of the weights vector increased
steadily (effectively sharpening the distribution), but
without any obvious change in the objective. Since
this resulted in poor generalization we added a reg-
ularization term of ||~? ? ~?||2/2?2 to L. We initially
set the means to zero, but after further observing that
the translations under all decoding criteria tended to
be shorter than the reference (causing a significant
drop in performance when evaluated using BLEU),
we found that performance could be improved by
setting ?WP = ?0.5, indicating a preference for a
lower weight on this parameter.
Table 2 compares the performance on Arabic to
English translation of systems tuned with MERT
(maximizing corpus BLEU) with systems tuned to
maximise expected sentence-level BLEU. Although
the performance of the minimum risk model under
all decoding criteria is lower than that of the orig-
inal MERT model, we note that the positive effect
of marginalizing over derivations as well as using
minimum risk decoding for obtaining good results
on this model. A full exploration of minimum risk
training is beyond the scope of this paper, but these
initial experiments should help emphasise the versa-
tility of the sampler and its utility in solving a variety
of problems. In the conclusion, we will, however,
4The ngram precision counts are smoothed by adding 0.01
for n > 1
discuss some possible future directions that can be
taken to make this style of training more competitive
with standard baseline systems.
5 Discussion and future work
We have described an algorithmic technique that
solves certain problems, but also verifies the utility
of standard approximation techniques. For exam-
ple, we found that on standard test sets the sampler
performs similarly to the DP max-derivation solu-
tion and equally well regardless of how it is ini-
tialised. From this we conclude that at least for
MERT-trained models, the max-derivation approx-
imation is adequate for finding the best translation.
Although the training approach presented in
Section 4 has a number of theoretical advantages,
its performance in a one-best evaluation falls short
when compared with a system tuned for optimal
one-best performance using MERT. This contradicts
the results of Zens and Ney (2007), who optimise
the same objective and report improvements over a
MERT baseline. We conjecture that the difference
is due to the biased k-best approximation they used.
By considering only the most probable derivations,
they optimise a smoothed error surface (as one
does in minimum risk training), but not one that
is indicative of the true risk. If our hypothesis
is accurate, then the advantage is accidental and
ultimately a liability. Our results are in line with
those reported by Smith and Eisner (2006) who
find degradation in performance when minimizing
risk, but compensate by ?sharpening? the model
distribution for the final training iterations,
effectively maximising one-best performance
rather minimising risk over the full distribution
defined by their model. In future work, we will
explore possibilities for artificially sharpening the
distribution during training so as to better anticipate
the one-best evaluation conditions typical of MT.
However, for applications which truly do require a
distribution over translations, such as re-ranking,
our method for minimising expected risk would be
the objective of choice.
Using sampling for model induction has two fur-
ther advantages that we intend to explore. First,
although MERT performs quite well on models with
108
small numbers of features (such as those we consid-
ered in this paper), in general the algorithm severely
limits the number of features that can be used since
it does not use gradient-based updates during opti-
mization, instead updating one feature at a time. Our
training method (Section 4) does not have this limi-
tation, so it can use many more features.
Finally, for the DP-based max-derivation approx-
imation to be computationally efficient, the features
characterizing the steps in the derivation must be
either computable independently of each other or
with only limited local context (as in the case of the
language model or distortion costs). This has led to
a situation where entire classes of potentially use-
ful features are not considered because they would
be impractical to integrate into a DP based trans-
lation system. With the sampler this restriction is
mitigated: any function of h(e, f, a) may partici-
pate in the translation model subject only to its own
computability. Freed from the rusty manacles of
dynamic programming, we anticipate development
of many useful features.
6 Related work
Our sampler is similar to the decoder of Germann
et al (2001), which starts with an approximate solu-
tion and then incrementally improves it via operators
such as RETRANS and MERGE-SPLIT. It is also
similar to the estimator of Marcu and Wong (2002),
who employ the same operators to search the align-
ment space from a heuristic initialisation. Although
the operators are similar, the use is different. These
previous efforts employed their operators in a greedy
hill-climbing search. In contrast, our operators are
applied probabilistically, making them theoretically
well-founded for a variety of inference problems.
Our use of Gibbs sampling follows from its
increasing use in Bayesian inference problems in
NLP (Finkel et al, 2006; Johnson et al, 2007b).
Most closely related is the work of DeNero
et al (2008), who derive a Gibbs sampler for
phrase-based alignment, using it to infer phrase
translation probabilities. The use of Monte Carlo
techniques to calculate posteriors is similar to that
of Chappelier and Rajman (2000) who use those
techniques to find the best parse under models where
the derivation and the parse are not isomorphic.
To our knowledge, we are the first to apply Monte
Carlo methods to maximum translation and mini-
mum risk translation. Approaches to the former
(Blunsom et al, 2008; May and Knight, 2006) rely
on dynamic programming techniques which do not
scale well without heuristic approximations, while
approaches to the latter (Smith and Eisner, 2006;
Zens et al, 2007) use biased k-best approximations.
7 Conclusion
We have described a Gibbs sampler for approxi-
mating two intractable problems in SMT: maximum
translation decoding (and its variant, minimum risk
decoding) and minimum risk training. By using
Monte Carlo techniques we avoid the biases associ-
ated with the more commonly used DP based max-
derivation (or k-best derivation) approximation. In
doing so we provide a further tool to the translation
community that we envision will allow the devel-
opment and analysis of increasing theoretically well
motivated techniques.
Acknowledgments
This research was supported in part by the GALE
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-06-2-001; and by the
EuroMatrix project funded by the European Commission
(6th Framework Programme). The project made use of
the resources provided by the Edinburgh Compute and
Data Facility (http://www.ecdf.ed.ac.uk/).
The ECDF is partially supported by the eDIKT initiative
(http://www.edikt.org.uk/).
References
P. Blunsom, T. Cohn, and M. Osborne. 2008. A discrim-
inative latent variable model for statistical machine
translation. In Proc. of ACL-HLT.
P. Blunsom, T. Cohn, and M. Osborne. 2009. Bayesian
synchronous grammar induction. In Advances in Neu-
ral Information Processing Systems 21, pages 161?
168.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
C. Callison-Burch, P. Koehn, C. Monz, and J. Schroeder,
editors. 2009. Proc. of Workshop on Machine Trans-
lations, Athens.
J.-C. Chappelier and M. Rajman. 2000. Monte-Carlo
sampling for NP-hard maximization problems in the
109
framework of weighted parsing. In Natural Language
Processing ? NLP 2000, number 1835 in Lecture Notes
in Artificial Intelligence, pages 106?117. Springer.
J. DeNero, A. Bouchard, and D. Klein. 2008. Sam-
pling alignment structure under a Bayesian translation
model. In Proc. of EMNLP.
J. R. Finkel, C. D. Manning, and A. Y. Ng. 2006. Solv-
ing the problem of cascading errors: Approximate
bayesian inference for linguistic annotation pipelines.
In Proc. of EMNLP.
S. Geman and D. Geman. 1984. Stochastic relaxation,
Gibbs distributions and the Bayesian restoration of
images. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 6:721?741.
U. Germann, M. Jahr, K. Knight, D. Marcu, and
K. Yamada. 2001. Fast decoding and optimal decod-
ing for machine translation. In Proceedings of ACL.
Association for Computational Linguistics, July.
J. Johnson, J. Martin, G. Foster, and R. Kuhn. 2007a.
Improving translation quality by discarding most of
the phrasetable. In Proc. of EMNLP-CoNLL, Prague.
M. Johnson, T. Griffiths, and S. Goldwater. 2007b.
Bayesian inference for PCFGs via Markov chain
Monte Carlo. In Proc. of NAACL-HLT, pages 139?
146, Rochester, New York, April.
P. Koehn, F. Och, and D.Marcu. 2003. Statistical phrase-
based translation. In Proc. of HLT-NAACL, pages 48?
54, Morristown, NJ, USA.
P. Koehn, H. Hoang, A. B. Mayne, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proc. of ACL
Demonstration Session, pages 177?180, June.
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk
decoding for statistical machine translation. In Pro-
cessings of HLT-NAACL.
D. Marcu and W. Wong. 2002. A phrase-based, joint
probability model for statistical machine translation.
In Proc. of EMNLP, pages 133?139.
J. May and K. Knight. 2006. A better n-best list: Prac-
tical determinization of weighted finite tree automata.
In Proc. of NAACL-HLT.
N. Metropolis and S. Ulam. 1949. The Monte Carlo
method. Journal of the American Statistical Associa-
tion, 44(247):335?341.
F. Och and H. Ney. 2000. A comparison of alignment
models for statistical machine translation. In Proc. of
COLING, Saarbrucken, Germany, July.
F. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL, pages 160?167,
Sapporo, Japan, July.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL, pages 311?318.
N. N. Schraudolph, J. Yu, and S. Gu?nter. 2007. A
stochastic quasi-Newton method for online convex
optimization. In Proc. of Artificial Intelligence and
Statistics.
N. N. Schraudolph. 1999. Local gain adaptation in
stochastic gradient descent. Technical Report IDSIA-
09-99, IDSIA.
K. Sima?an. 1996. Computational complexity of proba-
bilistic disambiguation by means of tree grammars. In
Proc. of COLING, Copenhagen.
D. A. Smith and J. Eisner. 2006. Minimum risk
annealing for training log-linear models. In Proc. of
COLING-ACL, pages 787?794.
R. Zens and H. Ney. 2007. Efficient phrase-table repre-
sentation for machine translation with applications to
online MT and speech translation. In Proc. of NAACL-
HLT, Rochester, New York.
R. Zens, S. Hasan, and H. Ney. 2007. A systematic com-
parison of training criteria for statistical machine trans-
lation. In Proc. of EMNLP, pages 524?532, Prague,
Czech Republic.
H. Zhang, C. Quirk, R. C. Moore, and D. Gildea. 2008.
Bayesian learning of non-compositional phrases with
synchronous parsing. In Proc. of ACL: HLT, pages
97?105, Columbus, Ohio.
110
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 365?374,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
A Unified Approach to Minimum Risk Training and Decoding
Abhishek Arun, Barry Haddow and Philipp Koehn
School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
a.arun@sms.ed.ac.uk, {bhaddow,pkoehn}@inf.ed.ac.uk
Abstract
We present a unified approach to perform-
ing minimum risk training and minimum
Bayes risk (MBR) decoding with BLEU
in a phrase-based model. Key to our ap-
proach is the use of a Gibbs sampler that
allows us to explore the entire probabil-
ity distribution and maintain a strict prob-
abilistic formulation across the pipeline.
We also describe a new sampling algo-
rithm called corpus sampling which al-
lows us at training time to use BLEU in-
stead of an approximation thereof. Our
approach is theoretically sound and gives
better (up to +0.6%BLEU) and more sta-
ble results than the standard MERT opti-
mization algorithm. By comparing our ap-
proach to lattice MBR, we are also able to
gain crucial insights about both methods.
1 Introduction
According to statistical decision theory, the opti-
mal decision rule for any statistical model is the
solution that minimizes its risk (expected loss).
This solution is often referred to as the Minimum
Bayes Risk (MBR) solution (Kumar and Byrne,
2004). Since machine translation (MT) mod-
els are typically evaluated by BLEU (Papineni et
al., 2002), a loss function which rewards partial
matches, the MBR solution is to be preferred to
the Maximum A Posteriori (MAP) solution.
In most statistical MT (SMT) systems, MBR
is implemented as a reranker of a list1 of trans-
lations generated by a first-pass decoder. This de-
coder typically assigns unnormalised log probabil-
ities (known as scores) to each translation hypoth-
1We use the term list to denote any enumerable represen-
tation of translation hypotheses e.g n-best list, translation lat-
tice or forest.
esis, so these scores must be converted to proba-
bilities in order to apply MBR. In order to perform
this conversion, it is first necessary to compute the
normalization function Z. Since Z is defined as
an intractable sum over all possible translations, it
is approximated by summing over the translations
in the list. The second step is to find the correct
scale factor for the scores using a hyper-parameter
search over held-out data. This is needed because
the model parameters for the first-pass decoder are
normally learnt using MERT (Och, 2003), which
is invariant under scaling of the scores.
Both these steps are theoretically unsatisfactory
methods of estimating the posterior probability
distribution since the approximation to Z is an un-
bounded term and the scaling factor is an artificial
way of inducing a probability distribution.
Recently, (Tromble et al, 2008; Kumar et al,
2009) have shown that using a search lattice to im-
prove the estimation of the true probability distri-
bution can lead to improved MBR performance.
However, these approaches still rely on MERT for
training the base model, and in fact introduce sev-
eral extra parameters which must also be estimated
using either grid search or a second MERT run.
The lattice pruning required to make these tech-
niques tractable is quite drastic, and is in addi-
tion to the pruning already performed during the
search. Such extensive pruning is liable to render
any probability estimates heavily biased (Blunsom
and Osborne, 2008; Bouchard-Co?te? et al, 2009).
Here, we present a unified approach to training
and decoding in a phrase-based translation model
(Koehn et al, 2003) which keeps the objective
constant across the translation pipeline and so ob-
viates the need for any extra hyper-parameter fit-
ting. We use the phrase-based Gibbs sampler of
Arun et al (2009) at training time to compute the
gradient of our minimum risk training objective in
order to apply first-order optimization techniques,
365
and at test time we use it to estimate the posterior
distribution required by MBR (Section 3).
We experimented with two different objective
functions for training (Section 4). First, follow-
ing (Arun et al, 2009), we define our objective
at the sentence-level using a sentence-level variant
of BLEU. Then, in order to reduce the mismatch
between training and test loss functions, we also
tried directly optimising the expected corpus level
BLEU, where we introduce a novel sampling tech-
nique, which we call corpus sampling to calculate
the required expectations.
The methods presented in this paper are theo-
retically sound. Moreover, experimental evidence
on three language pairs shows that our training
regime is more stable than MERT, able to gener-
alize better and generally leads to improvement in
translation when used with sampling based MBR
(Section 5). An added benefit is that the trained
weights also lead to better performance when used
with a beam-search based decoder.
2 Inference methods for MT
We assume a phrase-based machine translation
model, defined with a log-linear form, with feature
function vector h and parametrized by weight vec-
tor ?, as described in Koehn et al (2003). The in-
put sentence, f , is segmented into phrases, which
are sequences of adjacent words. Each source
phrase is translated into the target language, to
produce an output sentence e and an alignment
a representing the mapping from source to target
phrases. Phrases are allowed to be reordered.
p(e, a|f ;?) =
exp [? ? h(e, a, f)]
?
?e?,a?? exp [? ? h(e
?, a?, f)]
(1)
MAP decoding under this model consists of
finding the most likely output string, e?:
e? = argmaxe
?
a?4(e,f)
p(e, a|f) (2)
where4(e, f) is the set of all derivations of output
string e given source string f .
Summing over all the derivations is intractable,
making approximations necessary. The most com-
mon of these approximations is the Viterbi approx-
imation, which simply chooses the most likely
derivation ?e?, a??. This approximation can be
computed in polynomial time via dynamic pro-
gramming (DP). Though fast and effective for
many problems, it has two serious drawbacks for
probabilistic inference. First, the error incurred
by the Viterbi maximum with respect to the true
model maximum is unbounded. Second, the DP
solution requires substantial pruning and restricts
the use of non-local features. The latter problem
persists even in the variational approximations of
Li et al (2009), which attempt to solve the former.
2.1 Gibbs sampling for phrase-based MT
An alternate approximate inference method for
phrase-based MT without any of the previously
mentioned drawbacks is the Gibbs sampler (Ge-
man and Geman, 1984) of Arun et al (2009)
which draws samples from the posterior distribu-
tion of the translation model. For the work pre-
sented in this paper, we use this sampler.
The sampler produces a sequence of samples,
SN1 = (e1, a1) . . . (eN , aN ), that are drawn from
the distribution p(e, a|f). These samples can be
used to estimate the expectation of a function
h(e, a, f) as follows:
Ep(a,e|f)[h] = lim
N??
1
N
N?
i=1
h(ai, ei, f) (3)
3 Decoding
In this work, we are interested in performing MBR
decoding with BLEU. We define the MBR decision
rule following Tromble et al (2008):
e? = argmax
e?H
?
e??E
BLEUe(e
?)p(e?|f) (4)
where H refers to the hypothesis space from
which translations are chosen, E refers to the
evidence space used for calculating risk and
BLEUe(e?) is a gain function that indicates the re-
ward of hypothesising e? when the reference solu-
tion is e.
To perform MBR decoding using the sampler,
let the function h in Equation 3 be the indica-
tor function h = ?(a, a?)?(e, e?). Then, Equa-
tion 3 provides an estimate of p(a?, e?|f), and using
h = ?(e, e?) marginalizes over all derivations a?,
yielding an estimate of p(e?|f). MBR is computed
at the sentence-level while BLEU is a corpus-level
metric, so instead we use a sentence-level approx-
imation of BLEU.2
The sampler can be used to perform two other
decoding tasks: the mode of the estimated dis-
tribution p(a?, e?|f) is the maximum derivation
(MaxDeriv) solution while the mode of p(e?|f) is
the maximum translation (MaxTrans) solution.
2The ngram precision counts are smoothed by adding 0.01
for n > 1
366
4 Minimum Risk Training
In order to train models suitable for use with Max-
Trans or MBR decoding, we need to employ a
training method which takes account of the whole
distribution. To this end, we employ minimum risk
training to find weights ? for Equation 1 that mini-
mize the expected loss on the training set. We con-
sider two variants of minimum risk training: sen-
tence sampling optimizes an objective defined at
the sentence level and corpus sampling a corpus-
based objective.
4.1 Sentence sampling
Since BLEU, the metric we care about, is a gain
function, our objective function maximizes the ex-
pected gain of our model. The expected gain, G
of a probabilistic translation model on a corpus D,
defined with respect to the gain function BLEUe?(e)
is given by
G =
?
?e?,f??D
?
e,a
p(e, a|f)BLEUe?(e) (5)
where e? is the reference translation, e is a hypoth-
esis translation and BLEU refers to the sentence-
level approximation of the metric.
Using the probabilistic formulation of Equation
1, the optimization of the objective in (5) is facil-
itated by the fact that it is continuous and differ-
entiable with respect to the model parameters ? to
give
?G
??k
=
?
?e?,f?
?D
?
e,a
BLEUe?(e)
?p
??k
where
?p
??k
=
(
hk ? Ep(e,a|f)[hk]
)
p(e, a|f)
(6)
Since the gradient is expressed in terms of ex-
pectations of feature values, it can easily be calcu-
lated using the sampler and then first-order opti-
mization techniques can be applied to find optimal
values of ?. Because of the noise introduced by
the sampler, we used stochastic gradient descent
(SGD), with a learning rate that gets updated after
each step proportionally to difference in succes-
sive gradients (Schraudolph, 1999).
While our initial formulation of minimum risk
training is similar to that of Arun et al (2009), in
preliminary experiments we observed a tendency
for translation performance on held-out data to
quickly increase to a maximum and then plateau.
Hypothesizing that we were being trapped in lo-
cal maxima as G is non-convex, we decided to
employ deterministic annealing (Rose, 1998) to
smooth the objective function to ensure that the
optimizer explored as large a region as possible of
the space before it settled on an optimal weight set.
Our instantiation of deterministic annealing (DA)
is based on the work of Smith and Eisner (2006),
and involves the addition of an entropic prior to
the objective in Equation 5 to give
G? =
?
?e?,f??D
[(
?
e,a
p(e, a|f)BLEUe?(e)
)
+ T.H(p)
]
where H(p) is the entropy of the probability dis-
tribution p(e, a|f), and T > 0 is a temperature
paramater which is gradually lowered as the opti-
mization progresses according to some annealing
schedule.
Differentiating with respect to ?k then shows
that the annealed gradient is given by the follow-
ing expression:
?
?e?,f?
?D
?
e,a
(BLEUe?(e)? T (1 + log p))
?p
??k
where
?p
??k
=
(
hk ? Ep(e,a|f)[hk]
)
p(e, a|f)
A high value of T leads the optimizer to find
weights which describe a fairly flat distribution,
whereas a lower value of T pushes the optimizer
towards a more peaked distribution. We perform
10 to 20 iterations of SGD at each temperature.
In their deterministic annealing formulation,
(Smith and Eisner, 2006; Li and Eisner, 2009), ex-
press the parameterization of the distribution ? as
??? (where ? is the scaling factor) and perform op-
timization in two steps, the first optimizing ?? and
the second optimizing ?. We experimented with
this two stage optimization process, but found that
simply performing an unconstrained optimization
on ? gave better results.
4.2 Corpus sampling
While the objective functions in Equations 5 and
4.1 use a sentence-level variant of BLEU, the
model?s test-time performance is evaluated with
corpus level BLEU. The lack of correlation be-
tween sentence-level BLEU and corpus BLEU is
well-known (Chiang et al, 2008a). Therefore, in
an effort to address this issue, we tried maximizing
expected corpus BLEU directly.
In other words, given a training corpus of the
form ?CF , CE?? where CF is a set of source sen-
tences and CE? its corresponding reference transla-
tions, we consider a gain function defined on the
367
hypothesized translation CE of the input CF with
respect to CE? .
The objective in equation 5 therefore becomes:
G =
?
CE
P (CE |CF )BLEUCE? (CE) (7)
The pair (CE , CF ) is denoted as a cor-
pus sample corresponding to a sequence
(e1, a1), . . . , (eN , aN ) of derivations of the
corresponding source strings f1, . . . , fN of
source corpus CF .
Although the sampler described in Section 2
generates samples at the sentence level, we can use
it to generate corpus samples by applying the fol-
lowing procedure (see Figure 1). For each source
sentence f i in the corpus, we generate a sequence
of samples (ei1, a
i
1), . . . , (e
i
n, a
i
n) using the sam-
pler. From each of these sequences of samples, we
then resample new sequences of derivation sam-
ples, one for each source sentence in the corpus.
The first corpus sample is then obtained by iter-
ating through the source sentences and taking the
first resampled derivation for each sentence, then
the second corpus sample by taking the second re-
sampled derivation, and so on. The resampling
step is necessary to eliminate any biases due to the
order of the generated samples.
The corpus sampling procedure invariably gen-
erates a set of samples which are all distinct and so
would give us a uniform estimate of the probabil-
ity distribution P (CE |CF ). However this is not a
problem since we are not interested in evaluating
the actual distribution; we just need to calculate
expectations of feature values and BLEU scores
over the distribution. The feature values of a cor-
pus sample are the average of the feature values of
its constituting derivations and its BLEU score is
computed based on the yield of its derivations.
When training using corpus sampling we pro-
cess the training corpus in batches ?CF , CE??, treat-
ing each batch as a corpus in its own right, and
updating the weights after each batch.
The gradient for the objective function in (7) is:
?G
??k
=
?
CE
BLEUCE? (CE)
?P
??k
where
?P
??k
=
(
hCk ? EP (CE |CF )[h
C
k ]
)
P (CE |CF )
where hCk is the k-th component of a corpus
sample feature vector.
During deterministic annealing for sentence
sampling, the entropy term is computed over the
f1 f2 f3
A D K
B E L
A F L
C G L
B H M
f1 f2 f3
A F L
B E L
S
A
M
P
L
E
 
F
R
O
M
 
E
M
P
I
R
I
C
A
L
 
D
I
S
T
R
I
B
U
T
I
O
N
Extract Corpus 
Samples
f1 f2 f3
{A, F, L }
Corpus Sample 1
{B, E, L }
Corpus Sample 2
SAMPLE FROM 
P(e,a | f)
Figure 1: Example illustrating the extraction of 2
corpus samples for a corpus of source sentences
f1, f2, f3. In the first step, we sample 5 deriva-
tions for each source sentence. We then resample
2 derivations from the empirical distributions of
each source sentence.
distribution p(e, a|f) of each individual sentence.
While corpus sampling, we are considering the
distribution P (CE |CF ) but the estimated distribu-
tion is always uniform. So we define the entropic
prior term over the distribution p(e, a|f) of the
sentences making up the corpus sample.
The annealed corpus sampling objective is
therefore:
?
CE
P (CE |CF )BLEUCE? (CE)+
T
|CF |
?
f?CF
H(p(e, a|f))
The gradient of this objective is of similar form
to the sentence sampling gradient in Equation (6).
5 Experiments
5.1 Training Data and Preparation
The experiments in this section were performed
using the Europarl section of the French-English
and German-English parallel corpora from the
WMT09 shared translation task (Callison-Burch et
al., 2009), as well as 300k parallel Arabic-English
sentences from the NIST MT evaluation train-
ing data.3 For all language pairs, we constructed
3The Arabic-English training data consists of the
eTIRR corpus (LDC2004E72), the Arabic news corpus
(LDC2004T17), the Ummah corpus (LDC2004T18), and the
368
a phrase-based translation model as described in
Koehn et al (2003), limiting the phrase length to
5. The target side of the parallel corpus was used
to train 3-gram language models. For the German
and French systems, the DEV2006 set was used
for model tuning and the first half of TEST2007
(in-domain) for heldout testing. Final testing was
performed on NEWS-DEV2009B (out-of-domain)
and the first half of TEST2008 (in-domain). For
the Arabic system, the MT02 set (10 reference
translations) was used for tuning and MT03 and
MT05 (4 reference translations, each) were used
for held-out testing and final testing respectively.
To reduce the size of the phrase table, we used the
association-score technique suggested by Johnson
et al (2007). Translation quality is reported using
case-insensitive BLEU.
5.2 Baseline
Our baseline system is phrase-based
Moses (Koehn et al, 2007) with feature weights
trained using MERT. Moses and the Gibbs
sampler use identical feature sets.4
The MERT optimization algorithm uses multi-
ple random restarts to avoid getting stuck in a poor
local optima. Therefore, every time MERT is run,
it produces a slightly different final weight vector
leading to varying test set results. While this char-
acteristic of MERT is typically ignored, we ac-
count for it by performing MERT training 10 times
for each of the 3 language pairs, decoding the test
sets with each of the 10 optimized weight sets. We
present the best and the worst test set results along
with the mean and the standard deviation (?) of
these results in Table 1. We report results using
the Moses implementation of Viterbi, nbest MBR
and lattice MBR decoding (Kumar et al, 2009). 5
For both nbest and lattice MBR decoding, the hy-
pothesis set was composed of the top 1000 unique
translations produced by the Viterbi decoder, and
the same 1000 translations were used as evidence
set for nbest MBR.
As Table 1 shows, translation results using
MERT optimized weights vary markedly from one
sentences with confidence c > 0.995 in the ISI automatically
extracted web parallel corpus (LDC2006T02).
4We use 5 translation model scores, distance-based distor-
tion, language model and word penalty. The reordering limit
is set to 6 for all experiments.
5For nbest and lattice MBR decoding, we optimized for
the scaling factor using a grid-search on held-out data. For
lattice MBR decoding, we optimized the lattice density and
set the p and r parameters as per Tromble et al (2008).
tuning run to the other, with results varying from
a range of 0.3% BLEU to 1.3% BLEU when using
Viterbi decoding. We also see that, bar in-domain
German to English, MBR decoding gives a small
improvement on all other datasets.
Surprisingly, lattice MBR only gives improve-
ments on two datasets and actually leads to a drop
in performance on the other 3 datasets. We discuss
possible reasons for this in Section 6.
5.3 Sentence sampling
At training time, the optimization algorithm is ini-
tialized with zero weights and the sampler is ini-
tialized with a random derivation from Moses. To
get rid of any initialization biases, the first 100
samples are discarded.6 We then run the sampler
for 1000 iterations after which we perform reheat-
ing whereby the distribution is progressively flat-
tened. Samples are not collected during this pe-
riod. Reheating allows the sampler more mobil-
ity around the search space thus possibly escaping
any local optima it might be trapped in. We subse-
quently run the sampler for 1000 more iterations.
We denote this procedure as running 2 chains of
the sampler. We use batch sizes of 96 randomly
selected sentences for SGD optimization.
During DA, our cooling schedule is an exponen-
tially decaying one with decay rate set to 0.9, per-
forming 20 iterations of SGD optimization at each
temperature setting. Five training runs were per-
formed and the BLEU scores averaged. The fea-
ture weights were output every 50 iterations and
performance measured on the heldout set by run-
ning the sampler as a decoder. At decode time,
we use the same sampler configurations as during
training but run 2 chains each for 5000 iterations.
For MBR decoding, we use the entirety of this
sample set as our evidence set and use the top 1000
most probable translations as the hypothesis set.
5.4 Corpus sampling
For our corpus sampling experiments, we sample
using the same procedure as in sentence sampling
but using 2 chains of 2000 iterations. We then
resample 2000 corpus samples from the empiri-
cal distribution estimated from the first 4000 sam-
ples. For Arabic-English training, we used batch
sizes of 100 randomly selected sentences for ex-
periments without DA and batches of 400 random
6This procedure is referred to as burn-in in the MCMC
literature.
369
Viterbi nMBR lMBR
min max mean ? min max mean ? min max mean ?
AR-EN MT05 43.7 44.3 44.0 0.17 44.2 44.5 44.4 0.13 44.2 44.6 44.5 0.12
FR-EN In 33.1 33.4 33.3 0.10 33.2 33.6 33.4 0.12 32.3 32.7 32.6 0.13
FR-EN Out 19.1 19.6 19.4 0.18 19.3 19.7 19.5 0.12 19.1 19.4 19.3 0.12
DE-EN In 27.6 27.9 27.8 0.10 27.6 27.9 27.7 0.10 27.2 27.5 27.4 0.10
DE-EN Out 14.9 16.2 15.7 0.33 15.0 16.3 15.7 0.33 15.3 16.4 16.0 0.30
Table 1: Baseline results - MERT trained models decoded using Viterbi, nbest MBR (nMBR) and lattice
MBR (lMBR). MERT was run 10 times for each language pair. We report minimum, maximum, mean
and standard deviation of test set BLEU scores across the 10 runs.
200 400 600 80015
20
25
30
Training iterations
Bleu
Sentence Sampling, Without DA
l l l l l l l l l l l l l l l l l
27.4
l MaxDerivMaxTransMBR 100 300 500 70015
20
25
30
Training iterations
Bleu
Sentence Sampling, With DA
l
l
l
l
l l
l l l l l l l l l 28.2
l MaxDerivMaxTransMBR 50 100 150 200 25015
20
25
30
Training iterations
Bleu
Corpus Sampling, Without DA
l l l l l l l l l l l l l 28.1
l MaxDerivMaxTransMBR 50 150 250 35015
20
25
30
Training iterations
Bleu
Corpus Sampling, With DA
l l
l l
l l l
l l l l l l l l l l 28.5
l MaxDerivMaxTransMBR
Figure 2: Heldout performance for German-English training averaged across 5 minimum risk training
runs. Best scores achieved are indicated by dotted line.
sentences with DA. The size of the batches cor-
responds to the number of sentences that form a
corpus sample. For German/French to English ex-
periments, we used batches of 100 random sen-
tences for training with and without DA. We per-
form 10 optimizations at each temperature setting
during deterministic annealing. Test time condi-
tions are identical to the sentence sampling ones
and we measure performance on a held-out set af-
ter every 20 iterations of the learner.
5.5 Results
Figures 2 and 3 show the scores on the German-
English and Arabic-English held-out sets respec-
tively comparing all four training regimes: corpus
vs sentence sampling, DA vs without DA. Results
for French-English training are similar.
We focus our analysis on the Arabic-English ex-
perimental setup. Without deterministic anneal-
ing, the learner converges quickly, usually after
just 20 iterations, after which performance de-
grades steadily. The magnitudes of the weights
are large, sharpening the distribution. There is
not much diversity amongst the sampled deriva-
tions, i.e. the entropy of the sample set is low.
Therefore, all 3 decoding regimes give very simi-
lar results. With the addition of the entropic prior,
the model is slow to converge before the so-called
phase transition occurs (usually after around 50
iterations), after which performance goes up to
reach a peak (45.2 BLEU) higher than that without
the prior (44.2 BLEU), before steadily declining.
The entropic prior encourages diversity among the
sample set, especially at high temperature settings.
In the presence of diversity, the benefits of
marginalization over derivations is clear: Max-
Trans does better than MaxDeriv and MBR does
best, confirm recent findings of (Blunsom et al,
2008; Arun et al, 2009) that MaxTrans improves
over MaxDeriv decoding for models trained to ac-
count for multiple derivations. As the temperature
decreases to zero, the model sharpens, effectively
intent on maximizing one-best performance and
thus voiding the benefits of MaxTrans and MBR.
Figures 2 and 3 also show that corpus sampling
improves over sentence sampling, although not by
much (+ 0.3 BLEU).
5.6 Comparison with MERT baseline
Having established the superiority of the pipeline
of expected corpus BLEU training with DA fol-
lowed by MBR decoding over other alternatives
considered, we compare it to the best results ob-
tained with MERT optimized Moses (bold scores
from Table 1). To account for sampler variance
during both training and decoding, we average
scores across 50 runs; 10 decoding runs each using
the best weight set from 5 training runs. Results
370
0 200 600 1000 140030
35
40
45
50
Training iterations
Bleu
Sentence Sampling, Without DA
l l l l l l l l l l l l l l l l l l l l l l l l l l l l l
44.2
l MaxDerivMaxTransMBR 0 500 1000 150030
35
40
45
50
Training iterations
Bleu
Sentence Sampling With DA
l
l l l
l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l 45.2
l MaxDerivMaxTransMBR 20 40 60 80 100 14030
35
40
45
50
Training iterations
Bleu
Corpus Sampling, Without DA
l l l l l l l 44.5
l MaxDerivMaxTransMBR 100 200 300 40030
35
40
45
50
Training iterations
Bleu
Corpus Sampling, With DA
l
l l
l l l l l l l l l l l l l l l l l l 45.5
l MaxDerivMaxTransMBR
Figure 3: Heldout performance for Arabic-English training averaged across 5 minimum risk training
runs. Best scores achieved are indicated by dotted line.
are shown in Table 2.7
We observe that on 3 out of 5 datasets, the sam-
pler results are much more stable than MERT and
as stable on the other 2 datasets. We attribute the
improved stability to the more powerful optimiza-
tion algorithm used by the sampler which uses gra-
dient information to steer the model towards better
weights. MERT, alternatively, optimizes one fea-
ture at a time using line search and therefore does
not explore the full feature space as thoroughly.
Translation results with the sampler are better
than with MERT on 2 datasets, are equal on an-
other 2 and worse in one case. The improvements
withe the sampler are obtained in the case of out-
of-domain data suggesting that the minimum risk
training objective generalizes better than the 1-
best objective of MERT.
MERT/Moses Sampler
Test set Best ? MBR ?
AR-EN MT05 44.5 (lMBR) 0.12 44.5 0.14
FR-EN In 33.4 (nMBR) 0.12 33.2 0.06
FR-EN Out 19.5 (nMBR) 0.12 19.8 0.05
DE-EN In 27.8 (Viterbi) 0.10 27.8 0.11
DE-EN Out 16.0 (lMBR) 0.30 16.6 0.12
Table 2: Final results comparing MERT/Moses
pipeline with unified sampler pipeline. Sampler
uses corpus sampling during training and MBR
decoding at test time. Moses results are aver-
aged across decoding runs using weights from
10 MERT runs and sampler results are averaged
across 10 decoding runs for each of 5 different
training runs. We report BLEU scores and standard
deviation (?).
7The MBR decoding times, averaged over 10 decoding
runs of 50 sentences each, are 10 secs/sent for Moses nbest
MBR, 40 secs/sent for Moses lattice MBR and 180 secs/sent
for the sampler.
Viterbi nMBR lMBR Sampler
MBR
AR-EN MT05 44.2 44.4 44.8 44.8
FR-EN In 33.1 33.2 33.3 33.3
FR-EN Out 19.6 19.8 19.9 19.9
DE-EN In 27.7 27.9 28.0 28.0
DE-EN Out 16.0 16.3 16.6 16.6
Table 3: Comparison of decoding methods using
expected BLEU trained weights. We report Viterbi,
nbest MBR (nMBR) and lattice MBR (lMBR) de-
coding scores vs best sampler MBR decoding per-
formance. We selected the best weight set based
on performance on heldout data.
5.7 Moses with expected BLEU weights
In a final set of experiments, we reran the Moses
decoder this time using weights obtained through
expected BLEU optimization. Here, for each lan-
guage pair, we picked the weight set that gave the
best results on held-out data. Note that the results
which we show in Table 3 are over one run only,
so are not strictly comparable to those in Table 2
which are averaged over several training and de-
coding runs. We also report the best results ob-
tained with the sampler MBR decoder using these
weights.
In contrast to Table 1, here we see a consistent
improvement across all test-sets when going from
Viterbi decoding to n-best then to lattice MBR.
Except for in-domain French-English, the transla-
tion results are superior to the best scores shown
(in bold) in Table 1, confirming that the minimum
risk training objective is able to find good weight
sets. Interestingly, we also observe that sampler
MBR gets the same exact results for all test sets as
lattice MBR.
371
6 Discussion
We have shown that the sampler of Arun et al
(2009) can be used to perform minimum risk train-
ing over an unpruned search space. Our pro-
posed corpus sampling technique, like MERT, is
able to optimize corpus BLEU directly whereas
alternate parameter estimation techniques usually
employed in SMT optimize approximations of
BLEU. Chiang et al (2008b) accounts for the on-
line nature of the MIRA optimization algorithm
by smoothing the sentence-level BLEU precision
counts of a translation with a weighted average of
the precision counts of previously decoded sen-
tences, thus approximating corpus BLEU. As
for minimum risk training, prior implementations
have either used sentence-level BLEU (Zens et al,
2007) or a linear approximation to BLEU (Smith
and Eisner, 2006; Li and Eisner, 2009).
At test time, the sampler works best as an MBR
decoder, but also allows us to verify past claims
about the benefits of marginalizing over align-
ments during decoding. We compare the sam-
pler MBR decoder?s performance against MERT-
optimized Moses run under three different decod-
ing regimes, finding that the sampler does as well
or better on 4 out of 5 datasets.
Our training and testing pipeline has the advan-
tage of being able to handle a large number of both
local and global features so we expect in the future
to outperform the standard MERT and dynamic
programming-based search pipeline further.
As shown in Section 5.2, lattice MBR in some
cases leads to a marked drop in performance. (Ku-
mar et al, 2009) mention that the linear approx-
imation to BLEU used in their lattice MBR algo-
rithm is not guaranteed to match corpus BLEU, es-
pecially on unseen test sets. To account for these
cases, they allow their algorithm to back-off to the
MAP solution. One possible reason for the drop
in performance in our lattice MBR experiments is
that the implementation we use does not employ
this back-off strategy.
Table 3 provides valuable insights as to the mer-
its of the lattice MBR approach versus our own
sampling based pipeline. Firstly, whereas with
MERT optimized weights, the benefits of lattice
MBR are debatable (Table 1), running Moses with
minimum risk trained weights gives results that
are in line with what we would expect - lattice
MBR does systematically better than competing
decoding algorithms. This suggests that the unbi-
ased minimum risk training criterion used by the
sampler is a better fit for lattice MBR than the
MERT criterion, and also that the mismatch be-
tween linear and corpus BLEU mentioned before
might not be the reason for the results in Table 1.
Secondly, we find that sampling MBR matches
lattice MBR on the minimum risk trained weights.
The MBR sampler uses samples drawn from the
distribution as hypothesis and evidence sets, typi-
cally 1000 samples for the former and 10000 sam-
ples for the latter. In the lattice MBR experiments
of Tromble et al (2008), it is shown that this size
of hypothesis set is sufficient. Their evidence set,
however, is significantly larger than ours.8Table 3
suggests that, since it is not biased by heuris-
tic pruning, the sampler?s limited evidence set is
enough to give a good estimate of the probabil-
ity distribution whereas beam-search based MBR
needs to scale from using n-best lists to lattices to
get equivalent results.
Sampling the phrase-based model is expensive,
meaning that lattice MBR is still faster (around
4x) to run than sampler MBR. However, due to
the unified nature of the training and decoding cri-
terion in our approach, the minimum risk trained
weights can be plugged directly into the sam-
pler MBR decoder, whereas lattice MBR requires
an additional expensive step of tuning the model
hyper-parameters (Kumar et al, 2009).
In future work, we also intend to look at more
efficient ways of generating samples. One pos-
sibility is to interleave Gibbs sampling steps us-
ing low order ngram language model distributions
with Metropolis-Hasting steps that use higher or-
der language model distributions.
7 Related Work
Expected BLEU training for phrase-based models
has been successfully attempted by (Smith and
Eisner, 2006; Zens et al, 2007), however they both
used biased n-best lists to approximate the pos-
terior distribution. Li and Eisner (2009) present
work on performing expected BLEU training with
deterministic annealing on translation forests gen-
erated by Hiero (Chiang, 2007). Since BLEU does
not factorize over the search graph, they use the
linear approximation of Tromble et al (2008) in-
stead.
Pauls et al (2009) present an alternate training
criterion over translation forests called CoBLEU,
8up to 1081 as per Tromble et al (2008)
372
similar in spirit to expected BLEU training, but
aimed to maximize the expected counts of n-grams
appearing in reference translations. This training
criterion is used in conjunction with consensus de-
coding (DeNero et al, 2009), a linear-time ap-
proximation of MBR.
In contrast to the approaches above, the algo-
rithms presented in this paper are able to explore
an unpruned search space. By using corpus sam-
pling, we can perform minimum risk training with
corpus BLEU rather than any approximations of
this metric. Also, since we maintain a probabilis-
tic formulation across training and decoding, our
approach does not require a grid-search for a scal-
ing factor as in Tromble et al (2008).
8 Conclusions
We have presented a unified approach to the task
of parameter estimation and decoding for a phrase-
based system using the standard translation eval-
uation metric, BLEU. Using a Gibbs sampler to
explore the entire probability distribution allows
us to implement two probabilistic sound algo-
rithms, minimum risk training and its equivalent,
MBR decoding, in an unbiased way. The proba-
bilistic formulation also allows us to use gradient
based optimization techniques which produce sta-
ble model parameters. At decoding time, we show
the benefits of marginalizing over derivations and
that MBR gives better results than other decoding
criteria.
Since our optimization algorithm can cope with
a large number of features, in future work, we
plan to incorporate more expressive features in
the model. We use a Gibbs sampler for inference
so there is scope for exploring non-local features
which might not easily be added to dynamic pro-
gramming based models.
Acknowledgments
This research was supported in part by the GALE pro-
gram of the Defense Advanced Research Projects Agency,
Contract No. HR0011-06-2-001; and by the EuroMa-
trix project funded by the European Commission (6th
Framework Programme). The project made use of
the resources provided by the Edinburgh Compute and
Data Facility (http://www.ecdf.ed.ac.uk/). The
ECDF is partially supported by the eDIKT initiative
(http://www.edikt.org.uk/).
References
Abhishek Arun, Chris Dyer, Barry Haddow, Phil Blunsom,
Adam Lopez, and Philipp Koehn. 2009. Monte carlo in-
ference and maximization for phrase-based translation. In
Proceedings of CoNLL, pages 102?110.
Phil Blunsom and Miles Osborne. 2008. Probabilistic infer-
ence for machine translation. In Proc. of EMNLP 2008.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. A
discriminative latent variable model for statistical machine
translation. In Proc. of ACL-HLT.
Alexandre Bouchard-Co?te?, Slav Petrov, and Dan Klein.
2009. Randomized pruning: Efficiently calculating ex-
pectations in large dynamic programs. In Advances in
Neural Information Processing Systems 22, pages 144?
152.
Chris Callison-Burch, Philipp Koehn, Christoph Monz, and
Josh Schroeder, editors. 2009. Proc. of Workshop on Ma-
chine Translations.
David Chiang, Steve DeNeefe, Yee Seng Chan, and
Hwee Tou Ng. 2008a. Decomposability of transla-
tion metrics for improved evaluation and efficient algo-
rithms. In Proceedings of the 2008 Conference on Empir-
ical Methods in Natural Language Processing, pages 610?
619, Honolulu, Hawaii, October. Association for Compu-
tational Linguistics.
David Chiang, Yuval Marton, and Philip Resnik. 2008b. On-
line large-margin training of syntactic and structural trans-
lation features. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Processing,
pages 224?233, Honolulu, Hawaii, October. Association
for Computational Linguistics.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
John DeNero, David Chiang, and Kevin Knight. 2009. Fast
consensus decoding over translation forests. In Proceed-
ings of ACL/AFNLP, pages 567?575.
Stuart Geman and Donald Geman. 1984. Stochastic relax-
ation, Gibbs distributions and the Bayesian restoration of
images. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 6:721?741.
J.H. Johnson, J. Martin, G. Foster, and R. Kuhn. 2007.
Improving translation quality by discarding most of the
phrasetable. In Proc. of EMNLP-CoNLL, Prague.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proc. of HLT-NAACL, pages 48?54,
Morristown, NJ, USA.
P. Koehn, H. Hoang, A. Birch Mayne, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst.
2007. Moses: Open source toolkit for statistical machine
translation. In Proceedings of ACL Demos, pages 177?
180.
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk decod-
ing for statistical machine translation. In Processings of
HLT-NAACL.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and Franz
Och. 2009. Efficient minimum error rate training and
minimum bayes-risk decoding for translation hypergraphs
and lattices. In Proceedings of ACL/AFNLP, pages 163?
171.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-risk
training on translation forests. In Proceedings of EMNLP,
pages 40?51.
373
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009. Vari-
ational decoding for statistical machine translation. In
Proceedings of ACL/AFNLP, pages 593?601.
F. Och. 2003. Minimum error rate training in statistical ma-
chine translation. In Proceedings of ACL, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proceedings of ACL, pages 311?318.
Adam Pauls, John Denero, and Dan Klein. 2009. Consensus
training for consensus decoding in machine translation. In
Proceedings of EMNLP, pages 1418?1427.
Kenneth Rose. 1998. Deterministic annealing for clustering,
compression, classification, regression, and related opti-
mization problems. In Proceedings of the IEEE, pages
2210?2239.
Nicol N. Schraudolph. 1999. Local gain adaptation in
stochastic gradient descent. Technical Report IDSIA-09-
99, IDSIA.
David A. Smith and Jason Eisner. 2006. Minimum risk an-
nealing for training log-linear models. In Proceedings of
COLING-ACL, pages 787?794.
Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang
Macherey. 2008. Lattice Minimum Bayes-Risk decod-
ing for statistical machine translation. In Proceedings of
EMNLP, pages 620?629.
Richard Zens, Sasa Hasan, and Hermann Ney. 2007. A sys-
tematic comparison of training criteria for statistical ma-
chine translation. In Proceedings of EMNLP, pages 524?
532.
374
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 261?271,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
SampleRank Training for Phrase-Based Machine Translation
Barry Haddow
School of Informatics
University of Edinburgh
bhaddow@inf.ed.ac.uk
Abhishek Arun
Microsoft UK
abarun@microsoft.com
Philipp Koehn
School of Informatics
University of Edinburgh
pkoehn@inf.ed.ac.uk
Abstract
Statistical machine translation systems are
normally optimised for a chosen gain func-
tion (metric) by using MERT to find the best
model weights. This algorithm suffers from
stability problems and cannot scale beyond
20-30 features. We present an alternative al-
gorithm for discriminative training of phrase-
basedMT systems, SampleRank, which scales
to hundreds of features, equals or beats MERT
on both small and medium sized systems, and
permits the use of sentence or document level
features. SampleRank proceeds by repeatedly
updating the model weights to ensure that the
ranking of output sentences induced by the
model is the same as that induced by the gain
function.
1 Introduction
In phrase-based machine translation (PBMT), the
standard approach is to express the probability dis-
tribution p(a, e|f) (where f is the source sentence
and (a, e) is the aligned target sentence) in terms of
a linear model based on a small set of feature func-
tions
p(a, e|f) ? exp
(
n?
i=1
wihi(a, e, f)
)
(1)
The feature functions {hi} typically include log
probabilities of generative models such as trans-
lation, language and reordering, as well as non-
probabilistic features such as word, phrase and dis-
tortion penalties. The feature weights w = {wi}
are normally trained using MERT (minimum error
rate training) (Och, 2003), to maximise performance
as measured by an automated metric such as BLEU
(Papineni et al, 2002). MERT training uses a par-
allel data set (known as the tuning set) consisting of
about 1000-2000 sentences, distinct from the data
set used to build the generative models. Optimis-
ing the weights in Equation (1) is often referred to
as tuning the MT system, to differentiate it from the
process of training the generative models.
MERT?s inability to scale beyond 20-30 features,
as well as its instability (Foster and Kuhn, 2009)
have led to investigation into alternative ways of
tuning MT systems. The development of tuning
methods is complicated, however by, the use of
BLEU as an objective function. This objective in
its usual form is not differentiable, and has a highly
non-convex error surface (Och, 2003). Furthermore
BLEU is evaluated at the corpus level rather than at
the sentence level, so tuning methods either have to
consider the entire corpus, or resort to a sentence-
level approximation of BLEU. It is unlikely, how-
ever, that the difficulties in discriminative MT tun-
ing are due solely to the use of BLEU as a metric ?
because evaluation of translation is so difficult, any
reasonable gain function is likely to have a complex
relationship with the model parameters.
Gradient-based tuning methods, such as mini-
mum risk training, have been investigated as pos-
sible alternatives to MERT. Expected BLEU is nor-
mally adopted as the objective since it is differen-
tiable and so can be optimised by a form of stochas-
tic gradient ascent. The feature expectations re-
quired for the gradient calculation can be obtained
from n-best lists or lattices (Smith and Eisner, 2006;
Li and Eisner, 2009), or using sampling (Arun et al,
2010), both of which can be computationally expen-
sive.
261
Margin-based techniques such as perceptron
training (Liang et al, 2006) and MIRA (Chiang et
al., 2008; Watanabe et al, 2007) have also been
shown to be able to tune MT systems and scale to
large numbers of features, but these generally in-
volve repeatedly decoding the tuning set (and so
are expensive) and require sentence-level approxi-
mations to the BLEU objective.
In this paper we present an alternative method of
tuning MT systems known as SampleRank, which
has certain advantages over other methods in use to-
day. SampleRank operates by repeatedly sampling
pairs of translation hypotheses (for a given source
sentence) and updating the feature weights if the
ranking induced by the MT model (1) is different
from the ranking induced by the gain function (i.e.
BLEU). By considering the translation hypotheses
in batches, it is possible to directly optimise corpus
level metrics like BLEU without resorting to sentence
level approximations.
Tuning using SampleRank does not limit the size
of the feature set in the same way as MERT does,
and indeed it will be shown that SampleRank can
successfully train a model with several hundred fea-
tures. Using just the core PBMT features and train-
ing using SampleRank will be shown to achieve
BLEU scores which equal or exceed those produced
by MERT trained models.
Since SampleRank does not require repeated de-
coding of the tuning set, and is easily parallelisable,
it can run at an acceptable speed, and since it always
maintains a complete translation hypothesis, it opens
up the possibility of sentence or document level fea-
tures1.
2 Method
2.1 SampleRank Training
SampleRank (Culotta, 2008; Wick et al, 2009) is
an online training algorithm that was introduced for
parameter learning in weighted logics, and has been
applied to complex graphical models (Wick et al,
2011). Assume a probabilistic model p(y|x) admit-
ting a log-linear parametrisation
p(y|x) ? exp
?
i
(wi?i(x, y)) (2)
1As long as the batches described in Section 2.2.1 respect
document boundaries.
where {?i} are a set of feature functions and {wi}
are corresponding feature weights. SampleRank can
be used to optimise the feature weights to maximise
a given gain function.
SampleRank is a supervised training algorithm,
requiring a set of labelled training data D =
{(x1, y1}, . . . , (xn, yn)}, where the xi are the inputs
and the yi the outputs. The algorithm works by con-
sidering each training example (xi, yi) in turn, and
repeatedly sampling pairs of outputs from a neigh-
bourhood defined in the space of all possible out-
puts, updating the weights when the ranking of the
pair due to the model scores is different from the
ranking due to the gain function. So if the sampled
pair of outputs for xi is (y, y?), where p(y?|xi) >
p(y|xi), the weights are updated iff gain(y?, yi) <
gain(y, yi).
The sampled pairs are drawn from a chain which
can be constructed in a similar way to an MCMC
(Markov Chain Monte Carlo) chain.
In (Culotta, 2008) different strategies are explored
for building the chain, choosing the neighbourhood
and updating the weights.
2.2 SampleRank Training for Machine
Translation
We adapted SampleRank for the tuning of PBMT
systems, as summarised in Algorithm 1. The defi-
nitions of the functions in the algorithm (described
in the following subsections) draw inspiration from
work on MIRA training for MT (Watanabe et al,
2007; Chiang et al, 2008). SampleRank is used to
optimise the parameter weights in (1) using the tun-
ing set.
2.2.1 Gain Function
The first thing that needs to be defined in Algo-
rithm 1 is the gain function. For this we use BLEU,
the most popular gain function for automated MT
evaluation, although the procedure described here
will work with any gain function that can be evalu-
ated quickly. Using BLEU, however, creates a prob-
lem, as BLEU is defined at the corpus level rather
than the sentence level, and in previous work on
SampleRank, the training data is processed one ex-
ample at a time. In other work on online train-
ing for SMT, (Liang et al, 2006; Chiang et al,
2008), sentence-level approximations to BLEU were
262
Algorithm 1 The SampleRank algorithm for tuning
phrase-based MT systems.
Require: Tuning data:
D = {(f1, e1), . . . , (fn, en)}
Require: gain(y, y?): A function which scores a
set of hypotheses (y?) against a set of references
(y).
Require: score(x, y): A function which computes
a model score for a set of hypotheses y and
source sentences x.
1: for epoch = 1 to number of epochs do
2: A? D
3: while A is non-empty do
4: Pick (x, y), a batch of sentence pairs, ran-
domly from A, and remove.
5: Initialise y0, a set of translation hypotheses
for x.
6: for s = 1 to number of samples do
7: N ? ChooseNeighbourhood(ys?1)
8: y? ? ChooseSample(N)
9: y+ ? ChooseOracle(N)
10: if gain(y,y
?)?gain(y,y+)
score(x,y?)?score(x,y+) < 0 then
11: UpdateWeights()
12: end if
13: ys ? y?
14: end for
15: end while
16: end for
employed, however in this work we directly opti-
mise corpus BLEU by processing the data in small
batches. Using batches was found to work better
than processing the data sentence by sentence.
So the while loop in Algorithm 1 iterates through
the tuning data in batches of parallel sentences,
rather than single sentences. One complete pass
through the tuning data is known as an epoch, and
normally SampleRank training is run for several
epochs. The gain on a particular batch is calcu-
lated by scoring the current set of hypotheses for
the whole batch against the references for that batch.
When calculating BLEU, a smoothing constant of
0.01 is added to all counts in order to avoid zero
counts.
2.2.2 Sample Generation
For each iteration of the while loop in Algo-
rithm 1, a new batch of parallel sentences is cho-
sen from the tuning set, and a corresponding new
set of translation hypotheses must be generated (the
y0 in line 5 of Algorithm 1). These initial hypothe-
ses are generated by glossing. For each word in the
source, the most likely translation option (according
to the weighted phrase-internal score) is selected,
and these translations are joined together monoton-
ically. This method of initialisation was chosen be-
cause it was simple and fast, and experiments with
an alternative method of initialisation (where the de-
coder was run with random scores assigned to hy-
potheses) showed very little difference in perfor-
mance.
Once the initial set of hypotheses for the new
batch is created, the SampleRank innermost loop
(lines 6-14 in Algorithm 1) proceeds by repeatedly
choosing a sample hypothesis set (y?) and an oracle
hypothesis set (y+), corresponding to the source side
of the batch (x).
Given the current hypothesis set ys?1 =
(e1, . . . , ek), the sample and oracle are chosen as
follows. Firstly, a hypothesis ej is selected randomly
from ys?1 , and a neighbourhood of alternate hy-
potheses N 3 ej generated using operators from
Arun et al (2009) (explained shortly). Model scores
are calculated for all the hypotheses in N , converted
to probabilities using Equation (1), and a sample e?j
taken from N using these probabilities. The sam-
ple hypothesis set (y?) is then the current hypothesis
set (ys?1) with ej replaced by e?j . The oracle is cre-
ated, analogously Chiang et al (2008), by choosing
e+j ? N to maximise the sum of gain (calculated on
the batch) and model score. The oracle hypothesis
set (y+) is then ys?1 with ej replaced by e
+
j .
We now describe how the neighbourhood is cho-
sen. Given a single hypothesis ej , a neighbourhood
is generated by first randomly choosing one of the
two operators MERGE-SPLIT or REORDER, then ran-
domly choosing a point of application for the op-
erator, then applying it to generate the neighbour-
hood. The MERGE-SPLIT operator can be applied
at any inter-word position, and generates its neigh-
bourhood by listing all hypotheses obtained by op-
tionally merging or splitting the phrases(s) touching
263
that position, and retranslating them. The REORDER
operator applies at a pair of target phrases (subject
to distortion limits) and generates a neighbourhood
containing two hypotheses, one with the original or-
der and one with the chosen phrases swapped. The
distortion limits and translation option pruning used
by the operators matches those used in decoding, so
together they are able to explore the same hypothe-
sis space as the decoder. A fuller explanation of the
two operators is give in Arun et al (2009).
2.2.3 Weight Updates
After choosing the sample and oracle hypothe-
sis set (y? and y+), the weight update may be per-
formed. The weights of the model are updated if the
relative ranking of the sample hypothesis set and the
oracle hypothesis set provided by the model score is
different from that provided by the gain. The model
score function score(x, y) is defined for a hypothe-
sis set y = e1, . . . ek as follows:
score(x, y) =
k?
j=1
(
n?
i=1
wihi(aj , ej , fj)
)
(3)
where x = f1, . . . fk are the corresponding source
sentences. The weight update is performed iff
score(x, y?) 6= score(x, y+) and the following con-
dition is satisfied:
gain(y, y?)? gain(y, y+)
score(x, y?)? score(x, y+)
< 0 (4)
where the gain() function is just the BLEU score.
The weight update used in this work is a MIRA-
like update from ws?1 to ws defined as follows:
ws = argmin
w
(?w ?ws?1?+ C?) (5)
subject to
scorew(x, y
+)? scorew(x, y
?) + ?
?M ? (gain(y, y+)? gain(y, y?))
(6)
The margin scaling M is set to be gain(y, y+), so
that ranking violations of low BLEU solutions are as-
signed a lower importance than ranking violations of
high BLEU solutions. The ? in (5) is a slack variable,
whose influence is controlled by C (set to 0.01), and
which has the effect of ?clipping? the magnitude of
the weight updates. Since there is only one con-
straint, there is no need to use an iterative method
such as Hildreth?s, because it is straightforward to
solve the optimisation in (5) and (6) exactly using its
Lagrangian dual, following (Crammer et al, 2006).
The weight update is then given by
ws = ws?1 + min
(
b
?a?2
, C
)
a
where a = h(a+j , e
+
j , fj)? h(a
?
j , e
?
j , fj)
and b = M
(
gain(y, y+)? gain(y, y?)
)
?
(
score(x, y+)? gain(y, y?)
)
After updating the weights, the current hypothesis
set (ys) is updated to be the sample hypothesis set
(y?), as in line 13 of Algorithm 1, and then the next
sample is generated.
2.2.4 Implementation Considerations
After each iteration of the inner loop of Algorithm
1, the weights are collected, and the overall weights
output by the tuning algorithm are the average of all
these collected weights. When each new batch is
loaded at the start of the inner loop, a period of burn-
in is run, analogous to the burn-in used in MCMC
sampling, where no weight updates are performed
and weights are not collected.
In order to help the stability of the tuning algo-
rithm, and to enable it to process the tuning data
more quickly, several chains are run in parallel, each
with their own set of current weights, and each pro-
cessing a distinct subset of the tuning data. The
weights are mixed (averaged) after each epoch. The
same technique is frequently adopted for the aver-
aged perceptron (McDonald et al, 2010).
3 Experiments
3.1 Corpora and Baselines
The experiments in this section were conducted with
French-English and German-English sections of the
WMT20112 shared task data. In particular, we used
News-Commentary data (nc11), and Europarl data
(ep11) for training the generative models. Phrase
tables were built from lowercased versions of the
2http://www.statmt.org/wmt11/
264
parallel texts using the standard Moses3 training
pipeline, with the target side of the texts used to
build Kneser-Ney smoothed language models using
the SRILM toolkit4. These data sets were used to
build two phrase-based translation systems: WMT-
SMALL and WMT-LARGE.
The WMT-SMALL translation system uses a trans-
lation model built from just the nc11 data (about
115,000 sentences), and a 3-gram language model
built from the target side of this data set. The fea-
tures used in the WMT-SMALL translation system
were the five Moses translation features, a language
model feature, a word penalty feature and a distor-
tion distance feature.
To build the WMT-LARGE translation system, both
the ep11 data set and the nc11 data set were con-
catenated together before building the translation
model out of the resulting corpus of about 2 mil-
lion sentences. Separate 5-gram language models
were built from the target side of the two data sets
and then they were interpolated using weights cho-
sen to minimise the perplexity on the tuning set
(Koehn and Schroeder, 2007). In the WMT-LARGE
system, the eight core features were supplemented
with the six features of the lexicalised reordering
model, which was trained on the same data as was
used to build the translation model. Whilst a train-
ing set size of 2 million sentences would not nor-
mally be sufficient to build a competitive system for
an MT shared task, it is sufficient to show that how
SampleRank training performs on a realistic sized
system, whilst still allowing for plenty of experime-
nation with the algorithm?s parameters.
For tuning, the nc-devtest2007 was used,
with the first half of nc-test2007 corpus
used for heldout testing and nc-test2008 and
newstest2010 reserved for final testing. The
tuning and heldout sets are about 1000 sentences in
size, whereas the final test sets are approximately
2000 sentences each.
In Table 1, the performance (in BLEU5) of
untrained and MERT-tuned models on the
heldout set is shown6. The untuned models
3http://www.statmt.org/moses/
4http://www-speech.sri.com/projects/
srilm/
5Calculated with multi-bleu.perl
6All BLEU scores and standard deviations are rounded to one
use the default weights output by the Moses
train-model.perl script, whereas the perfor-
mance of the tuned models is the mean across five
different MERT runs.
All decoding in this paper is with Moses, using
default settings.
Pair System untuned MERT-tuned
fr-en WMT-SMALL 28.0 29.2 (0.2)
WMT-LARGE 29.4 32.5 (0.1)
de-en WMT-SMALL 25.0 25.3 (0.1)
WMT-LARGE 26.6 26.8 (0.2)
Table 1: Untrained and MERT-trained performance
on heldout. MERT training is repeated five times,
with the table showing the mean BLEU, and standard
deviation in brackets.
3.2 SampleRank Training For Small Models
First we look at how SampleRank training compares
to MERT training using the WMT-SMALL models.
Using the smaller models allows reasonably quick
experimentation with a large range of different pa-
rameter settings.
For these experiments, the epoch size is set at
1024, and we vary both the number of cores and the
number of samples used in training. The number of
cores n is set to either 1,2,4,8 or 16, meaning that
each epoch we split the tuning data into n different,
non-overlapping shards, passing a different shard to
each process, so the shard size k is set to 1024/n. In
each process, a burn of 100 ?k samples is run (with-
out updating the weights), followed by either 100?k
or 500?k samples with weight updates, using the al-
gorithm described in Section 2.2. After an epoch is
completed, the current weights are averaged across
all processes to give the new current weights in each
process. At intervals of 50000 samples in each core,
weights are averaged across all samples so far, and
across all cores, and used to decode the heldout set
to measure performance.
In Figure 1, learning curves are shown for the
100 sample-per-sentence case, for 1, 4 and 16 cores,
for French-English. The training is repeated five
times and the error bars in the graph indicate the
decimal place.
265
l
l l
l l l l
l l l l l
l l l
l l l l l l l l l
l l
l
l
l
l
l
l
l
l l
l
l l l
27
28
29
30
31
Samples per core (thousands)
Bleu
0 500 1000 1500 2000
(a) 1 core
l
l
l
l l l l l
l
l
l
l
l
l l l
l
l
l
l
l
l
l l
l
27
28
29
30
31
Samples per core (thousands)
Bleu
0 500 1000 1500 2000 2500
(b) 4 cores
l
l
ll l
l l l l l l l
l
l ll l
27
28
29
30
31
Samples per core (thousands)
Bleu
0 500 1000 1500 2000 2500
(c) 16 cores
Figure 1: SampleRank learning curves for the WMT-SMALL French-English system, for 1, 4 and 16 cores.
The dashed line shows the mean MERT performance, which has a standard deviation of 0.2.
spread across the different training runs. Increasing
the number of cores makes a clear difference to the
training, with the single core training run failing to
reach the the level of MERT, and the 16 core train-
ing run exceeding the mean MERT performance by
more than 0.5 BLEU. Using a single core also results
in a much bigger training variance, which makes
sense as using more cores and averaging weights
reduces the adverse effect of a single chain going
astray. The higher BLEU score achieved when us-
ing the larger number of cores is probably because
a larger portion of the parameter space is being ex-
plored.
In one sense, the x axes of the graphs in Figure 1
are not comparable, since increasing the number of
cores and keeping the number of samples per core
increases the total computing time. However even if
the single core training was run for much longer, it
did not reach the level of performance obtained by
multi-core training. Limited experimentation with
increasing the core count to 32 did not show any ap-
preciable gain, despite greatly increasing the com-
puting resources required.
The training runs shown in Figure 1 take between
21 hours (for 16 cores) and 35 hours (for a single
core)7. In the 16 core runs each core is doing the
same amount of work as in the single core runs, so
the difference in time is due to the extra effort in-
volved in dealing with larger batches. These times
are for the 100 samples-per-sentence condition, and
7The processors are Intel Xeon 5450 (3GHz)
increasing to 500 samples-per-sentence provides a
speed-up of about 25%, since proportionally less
time is spent on burn-in. Most of the time is spent
in BLEU evaluation, so improved memoisation and
incremental evaluation would reduce training time.
In Table 2 the mean maximum BLEU achieved on
the heldout set at each parameter setting is shown.
By this it is meant that for each of the five training
runs at each (samples,cores) setting, the maximum
BLEU on heldout data is observed, and these max-
ima are averaged across the five runs. It can be seen
that changing the samples-per-sentence makes little
difference, but there is a definite effect of increasing
the core count.
Cores 100 Samples 500 Samples
1 29.1 (0.2) 29.2 (0.1)
2 29.3 (0.1) 29.3 (0.1)
4 29.6 (0.1) 29.5 (0.1)
8 30.0 (0.0) 29.9 (0.1)
16 30.0 (0.1) 29.8 (0.1)
Table 2: Mean maximum heldout performance for
SampleRank training of the French-English WMT-
SMALL model. Standard deviations are shown in
brackets.
The learning curves for the equivalent German-
English model are shown in Figure 2 and show a
fairly different behaviour to their French-English
counterparts. Again, using more cores helps to im-
266
prove and stabilise the performance, but there is lit-
tle if any improvement throughout training. As with
MERT training, SampleRank training of the model
weights makes little difference to the BLEU score,
suggesting a fairly flat error surface.
Table 3 shows the mean maximum BLEU score
on heldout data, the equivalent of Table 2, but for
German-English. The results show very little varia-
tion as the samples-per-sentence and core counts are
changed.
Cores 100 Samples 500 Samples
1 25.2 (0.0) 25.3 (0.1)
2 25.4 (0.1) 25.4 (0.1)
4 25.4 (0.1) 25.4 (0.1)
8 25.4 (0.1) 25.4 (0.1)
16 25.3 (0.1) 25.4 (0.1)
Table 3: Mean maximum heldout performance for
SampleRank training of the German-English WMT-
SMALL model. Standard deviations are shown in
brackets
3.3 SampleRank Training for Larger Models
For the training of the WMT-LARGE systems with
SampleRank, similar experiments to those in Sec-
tion 3.2 were run, although only for 8 and 16 cores.
The learning curves for the two language pairs (Fig-
ure 3) show roughly similar patterns to those in
the previous section, in that the French-English sys-
tem gradually increases performance through train-
ing to reach a maximum, as opposed to the German-
English system with its fairly flat learning curve.
Training times are around 27 hours for the 500 sam-
ple curve shown in Figure 3, increasing to 64 hours
for 100 samples-per-sentence.
In Table 4, the mean maximum BLEU scores are
shown for each configuration. of each language pair,
calculated in the manner described in the previous
section. For the larger system, SampleRank shows
a smaller advantage over MERT for French-English,
and little if any gain for German-English. For both
large and small German-English models, neither of
the parameter tuning algorithms are able to lift BLEU
scores very far above the scores obtained from the
untuned weights set by the Moses training script.
Pair Cores 100 Samples 500 Samples
fr-en 8 32.6 (0.1) 32.7 (0.1)
16 32.8 (0.1) 32.9 (0.1)
de-en 8 26.9 (0.0) 27.0 (0.1)
16 26.8 (0.1) 26.9 (0.1)
Table 4: Mean (and standard deviation) of maximum
heldout performance for SampleRank training of the
WMT-LARGE model.
3.4 SampleRank Training for Larger Feature
Sets
The final set of experiments are concerned with us-
ing SampleRank training for larger feature sets than
the 10-20 typically used in MERT-trained models.
The models considered in this section are based on
the WMT-SMALL systems, but also include a fam-
ily of part-of-speech tag based phrase boundary fea-
tures.
The phrase boundary features are defined by con-
sidering the target-side part-of-speech tag bigrams
spanning each phrase boundary in the hypothesis,
and allowing a separate feature to fire for each bi-
gram. Dummy phrases with parts-of-speech <S>
and </S> are inserted at the start and end of the
sentence, and also used to construct phrase bound-
ary features. The example in Figure 4 shows the
phrase-boundary features from a typical hypothe-
sis. The idea is similar to a part-of-speech language
model, but discriminatively trained, and targeted at
how phrases are joined together in the hypothesis.
The target-side part-of-speech tags are added us-
ing the Brill tagger, and incorporated into the phrase
table using the factored translation modelling capa-
bilities of Moses (Koehn and Hoang, 2007).
Adding the phrase boundary features to the WMT-
SMALL system increased the feature count from 8
to around 800. Training experiments were run for
both the French-English and German-English mod-
els, using the same configuration as in Section 3.2,
varying the number of cores (8 or 16) and the num-
ber of samples per sentence (100 or 500). Train-
ing times were similar to those for the WMT-SMALL
system. The mean maximum scores on heldout are
shown in Table 5. We suspect that these features are
fixing some short range reordering problems which
267
l l l
l
l l l
l
l l l l l l l l l l l l l l l l l l
l l l l l
l
l
23
24
25
26
27
Samples per core (thousands)
Bleu
0 500 1000 1500 2000 2500
(a) 1 core
l
l l
l l l l l l l l l l l l l l l l l
l
l l l
23
24
25
26
27
Samples per core (thousands)
Bleu
0 500 1000 1500 2000 2500
(b) 4 cores
l l l l
l
l l l l l l l l l l l l l l l ll l l
l
23
24
25
26
27
Samples per core (thousands)
Bleu
0 500 1000 1500 2000 2500
(c) 16 cores
Figure 2: SampleRank learning curves for the WMT-SMALL German-English system, for 1, 4 and 16 cores.
The dashed line shows the mean MERT performance, which has a standard deviation of 0.1.
l
l l
l
l
l l
l l
l l l l l l l
l l l ll
l
30
31
32
33
34
Samples per core (thousands)
Bleu
0 500 1000 1500 2000 2500
(a) French-English
l
l
l l
l
l l l l l l l l l l l l l
l l l l
l
l ll
l
l
l
l
l l
24
25
26
27
28
Samples per core (thousands)
Bleu
0 500 1000 1500 2000 2500
(b) German-English
Figure 3: SampleRank learning curves for the WMT-LARGE French-English and German-English systems,
using 8 cores and 500 samples per sentence. The dashed line shows the mean MERT performance, which
has a standard deviation of 0.07 (fr-en) and 0.2 (de-en).
occur in the former language pair, but since the re-
ordering problems in the latter language pair tend to
be longer range, adding these extra features just tend
to add extra noise to the model.
3.5 Comparison of MERT and SampleRank on
Test Data
Final testing was performed on the nc-test2008
and newstest2010 data sets. The former is quite
similar to the tuning and heldout data, whilst the lat-
ter can be considered to be ?out-of-domain?, so pro-
vides a check to see whether the model weights are
being tuned too heavily towards the domain.
For the SampleRank experiments on the test set,
the best training configurations were chosen from
the results in Tables 2, 3, 4 and 5, and the best per-
forming weight sets for each of the five runs for this
configuration. For the MERT trained models, the
same five models from Table 1 were used. The test
set results are shown in Table 6.
The patterns observed on the heldout data carry
over, to a large extent, to the test data. This is
especially true for the WMT-SMALL system, where
similar improvements (for French-English) over the
MERT trained system are observed on the SampleR-
ank trained system. For the WMT-LARGE system,
the slightly improved performance that SampleRank
offered on the in-domain data is no longer there, al-
268
Hypothesis [europe ?s] [after] [racial] [house divided against itself]
Tags <S> NNP POS IN JJ NN VBN IN PRP </S>
This produces five phrase boundary features: <S>:NNP, POS:IN, IN:JJ, JJ:NN and PRP:</S>.
Figure 4: The definition of the phrase boundary feature from part-of-speech tags
fr-en de-en
Training System nc-test2008 newstest2010 nc-test2008 newstest2010
MERT WMT-SMALL 28.1 (0.1) 19.6 (0.1) 25.9 (0.1) 16.4 (0.2)
SampleRank WMT-SMALL 28.7 (0.0) 20.1 (0.1) 25.9 (0.1) 16.6 (0.1)
SampleRank WMT-SMALL+pb 28.8 (0.1) 19.8 (0.1) 25.9 (0.1) 16.7 (0.1)
MERT WMT-LARGE 30.1 (0.1) 22.9 (0.1) 28.0 (0.2) 19.1 (0.2)
SampleRank WMT-LARGE 30.0 (0.1) 23.6 (0.3) 28.1 (0.1) 19.5 (0.2)
Table 6: Comparison of MERT trained and SampleRank trained models on the test sets. The WMT-
SMALL+pb model is the model with phrase boundary features, as described in Section 3.4
Pair Cores 100 Samples 500 Samples
fr-en 8 30.2 (0.0) 30.2 (0.0)
16 30.3 (0.0) 30.3 (0.00)
de-en 8 25.1 (0.1) 25.1 (0.0)
16 25.0 (0.1) 25.0 (0.0)
Table 5: Mean (and standard deviation) of maximum
heldout performance for SampleRank training of the
WMT-SMALL model, with the phrase boundary fea-
ture.
though interestingly there is a reasonable improve-
ment on out-of-domain, over the MERT trained
model, similar to the effect observed in (Arun et
al., 2010). Finally, the improvements offered by the
phrase boundary feature are reduced, perhaps an in-
dication of some over-fitting.
4 Related Work
Whilst MERT (Och, 2003) is still the dominant al-
gorithm used for discriminative training (tuning) of
SMT systems, research into improving on MERT?s
line search has tended to focus either on gradient-
based or margin-based techniques.
Gradient-based techniques require a differentiable
objective, and expected sentence BLEU is the most
popular choice, beginning with Smith and Eisner
(2006). They used n-best lists to calculate the fea-
ture expectations required for the gradient, optimis-
ing a second order Taylor approximation of expected
sentence BLEU. They also introduced the idea of de-
terministic annealing to the SMT community, where
an entropy term is added to the objective in train-
ing, and has its temperature progressively lowered
in order to sharpen the model probability distribu-
tion. The work of Smith and Eisner was extended
by Li and Eisner (2009) who were able to obtain
much better estimates of feature expectations by us-
ing a packed chart instead of an n-best list. They
also demonstrated that their method could extend to
large feature sets, although their experiments were
only run on small data sets.
An alternative method of calculating the feature
expectations for expected BLEU training is Monte-
Carlo Markov Chain (MCMC) approximation, and
this was explored in (Arun et al, 2009) and (Arun et
al., 2010). The sampling methods introduced in this
earlier work form the basis of the current work, al-
though in using the sampler for expected BLEU train-
ing, many samples must be collected before making
a parameter weight update, as opposed to the cur-
rent work where weights may be updated after ev-
ery sample. One novel feature of Arun et al (2010)
is that they were able to train to directly maximise
corpus BLEU, instead of its sentence-based approx-
imation, although this only made a small difference
to the results. The training methods in (Arun et al,
269
2010) are very resource intensive, with the experi-
ments running for 48 hours on around 40 cores, on
a pruned phrase table derived from Europarl, and a
3-gram language model.
Instead of using expected BLEU as a training ob-
jective, Blunsom et al (2008) trained their model to
directly maximise the log-likelihood of the discrim-
inative model, estimating feature expectations from
a packed chart. Their model treats derivations as
a latent variable, directly modelling the translation
probability.
Margin-based techniques have the advantage that
they do not have to employ expensive and com-
plex algorithms to calculate the feature expectations.
Typically, either perceptron ((Liang et al, 2006),
(Arun and Koehn, 2007)) or MIRA ((Watanabe et
al., 2007), (Chiang et al, 2008)) is employed, but
in both cases the idea is to repeatedly decode sen-
tences from the tuning set, and update the parame-
ter weights if the best hypothesis according to the
model differs from some ?oracle? sentence. The ap-
proaches differ in the way they compute the oracle
sentence, as well as the way the weights are updated.
Normally sentences are processed one-by-one, with
a weight update after considering each sentence, and
sentence BLEU is used as the objective. However
Chiang et al (2008) introduced an approximation to
corpus BLEU by using a rolling history. Both papers
on MIRA demonstrated its ability to extend to large
numbers of features.
In the only known application of SampleRank to
SMT, Roth et al (2010) deploys quite a different
translation model to the usual phrase-based model,
allowing overlapping phrases and implemented as a
factor graph. Decoding is with a rather slow stochas-
tic search and performance is quite poor, but this
model, in common with the training algorithm pre-
sented in the current work, permits features which
depend on the whole sentence.
5 Discussion and Conclusions
The results presented in Table 6 show that Sam-
pleRank is a viable method of parameter tuning for
phrase-based MT systems, beating MERT in many
cases, and equalling it in others. It is also able to
do what MERT cannot do, and scale to a large num-
ber of features, with the phrase boundary feature of
Section 3.4 providing a ?proof-of-concept?.
A further potential advantage of SampleRank is
that it allows training with features which depend
on the whole sentence, or even the whole document,
since a full set of hypotheses is retained through-
out training. Of course adding these features pre-
cludes decoding with the usual dynamic program-
ming based decoders, and would require an alterna-
tive method, such as MCMC (Arun et al, 2009).
As with the other alternatives to MERT men-
tioned in this paper, SampleRank training presents
the problem of determining convergence. With
MERT this is straightforward, since training (nor-
mally) comes to a halt when the estimated tuning
BLEU stops increasing and the weights stop chang-
ing. With methods such as minimum risk training,
MIRA and SampleRank, some kind of early stop-
ping criterion is usually employed, which lengthens
training unnecessarily, and adds costly decodes to
the training process. Building up sufficient practical
experience with each of these methods will offset
these problems somewhat.
Another important item for future work is to com-
pare SampleRank training with MIRA training, in
terms of performance, speed and ability to handle
large feature sets.
The code used for the experiments in this paper is
available under an open source license8.
Acknowledgements
This research was supported by the EuroMatrixPlus
project funded by the European Commission (7th Frame-
work Programme) and by the GALE program of the
Defense Advanced Research Projects Agency, Contract
No. HR0011-06-2-001. The project made use of the re-
sources provided by the Edinburgh Compute and Data
Facility (http://www.ecdf.ed.ac.uk/). The
ECDF is partially supported by the eDIKT initiative
(http://www.edikt.org.uk/).
The authors would like to thank Sebastian Riedel for
helpful discussions related to this work.
References
Abhishek Arun and Philipp Koehn. 2007. Online Learn-
ing Methods For Discriminative Training of Phrase
8https://mosesdecoder.svn.sourceforge.
net/svnroot/mosesdecoder/branches/
samplerank
270
Based Statistical Machine Translation. In Proceedings
of MT Summit.
Abhishek Arun, Chris Dyer, Barry Haddow, Phil Blun-
som, Adam Lopez, and Philipp Koehn. 2009. Monte
Carlo inference and maximization for phrase-based
translation. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), pages 102?110, Boulder, Colorado,
June. Association for Computational Linguistics.
Abhishek Arun, Barry Haddow, and Philipp Koehn.
2010. A Unified Approach to Minimum Risk Training
and Decoding. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metrics-
MATR, pages 365?374, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A Discriminative Latent Variable Model for Statistical
Machine Translation. In Proceedings of ACL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online Large-Margin Training of Syntactic and Struc-
tural Translation Features. In Proceedings of EMNLP.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online Passive-
Aggressive Algorithms. Journal of Machine Learning
Research, 7:551?585, March.
Aron Culotta. 2008. Learning and inference in weighted
logic with application to natural language processing.
Ph.D. thesis, University of Massachusetts, May.
George Foster and Roland Kuhn. 2009. Stabilizing
Minimum Error Rate Training. In Proceedings of the
Fourth Workshop on Statistical Machine Translation,
pages 242?249, Athens, Greece, March. Association
for Computational Linguistics.
Philipp Koehn and Hieu Hoang. 2007. Factored Transla-
tion Models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 868?876.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in Domain Adaptation for Statistical Machine Transla-
tion. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, pages 224?227, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Zhifei Li and Jason Eisner. 2009. First- and Second-
order Expectation Semirings with Applications to
Minimum-Risk Training on Translation Forests. In
Proceedings of EMNLP.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An End-to-End Discriminative Ap-
proach to Machine Translation. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 761?768, Syd-
ney, Australia, July. Association for Computational
Linguistics.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed Training Strategies for the Structured Per-
ceptron. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
456?464, Los Angeles, California, June. Association
for Computational Linguistics.
Franz J. Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings of
ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Benjamin Roth, Andrew McCallum, Marc Dymetman,
and Nicola Cancedda. 2010. Machine Translation
Using Overlapping Alignments and SampleRank. In
Proceedings of AMTA.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In Proceed-
ings of COLING/ACL, pages 787?794, Morristown,
NJ, USA. Association for Computational Linguistics.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online Large-Margin Training for Sta-
tistical Machine Translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 764?
773, Prague, Czech Republic, June. Association for
Computational Linguistics.
Michael Wick, Khashayar Rohanimanesh, Aron Culotta,
and Andrew McCallum. 2009. SampleRank: Learn-
ing Preferences from Atomic Gradients. In Proceed-
ings of NIPS Workshop on Advances in Ranking.
Michael Wick, Khashayar Rohanimanesh, Kedare Bel-
lare, Aron Culotta, and Andrew McCallum. 2011.
SampleRank: training factor graphs with atomic gra-
dients. In Proceedings of ICML.
271
