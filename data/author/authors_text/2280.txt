Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 837?846, Prague, June 2007. c?2007 Association for Computational Linguistics 
Extracting Data Records from Unstructured Biomedical Full Text 
Donghui Feng       Gully Burns       Eduard Hovy 
Information Sciences Institute 
University of Southern California 
Marina del Rey, CA, 90292 
{donghui, burns, hovy}@isi.edu 
 
 
Abstract 
In this paper, we address the problem of 
extracting data records and their attributes 
from unstructured biomedical full text. 
There has been little effort reported on this 
in the research community. We argue that 
semantics is important for record extraction 
or finer-grained language processing tasks. 
We derive a data record template including 
semantic language models from unstruc-
tured text and represent them with a dis-
course level Conditional Random Fields 
(CRF) model. We evaluate the approach 
from the perspective of Information Extrac-
tion and achieve significant improvements 
on system performance compared with 
other baseline systems. 
1 Introduction 
The discovery and extraction of specific types of 
information, and its (re)structuring and storage into 
databases, are critical tasks for data mining, 
knowledge acquisition, and information integration 
from large corpora or heterogeneous resources 
(e.g., Muslea et al, 2001; Arasu and Garcia-
Molina, 2003). For example, webpages of products 
on Amazon may contain a list of data records such 
as books, watches, and electronics. Automatic 
extraction of individual records will facilitate the 
access and management of data resources. 
Most current approaches address this problem 
for structured or semi-structured text, for instance, 
from XML format files or lists and/or tabular data 
records on webpages (e.g., Liu et al, 2003; Zhu et 
al., 2006). The techniques applied rely strongly on 
the analysis of document structure derived from 
the webpage?s html tags (e.g., the DOM tree 
model). 
Regarding unstructured text, most Information 
Extraction (IE) work has focused on named entities 
(people, organizations, places, etc.). Such IE treats 
each extracted element as a separate record. Much 
less work has focused on the case where several 
related pieces of information have to be extracted 
to jointly comprise a single data record. In this 
work, it is usually assumed that there is only one 
record for each document (e.g., Kristjannson et al, 
2004). Almost no work tries to extract multiple 
data records from a single document. Multiple data 
records can be scattered across the narrative in free 
text. The problem becomes much harder as there 
are no explicit boundaries between data records 
and no heavily indicative format features (like html 
tags) to utilize. 
With the exponential increase of unstructured 
text resources (e.g., digitalized publications, papers 
and/or technical reports), knowledge needs have 
made it a necessity to explore this problem. For 
example, biomedical papers contain numerous ex-
periments and findings. But the large volume and 
rate of publication have made it infeasible to read 
through the articles and manually identify data re-
cords and attributes. 
We present a study to extract data records and 
attributes from the biomedical research literature. 
This is part of an effort to develop a Knowledge 
Base Management System to benefit neuroscience 
research. Specifically we are interested in knowl-
edge of various aspects (attributes) of Tract-tracing 
Experiments (TTE) (data records) in neuroscience. 
The goal of TTE experiments is to chart the inter-
connectivity of the brain by injecting tracer chemi-
cals into a region of the brain and identifying cor-
responding labeled regions where the tracer is 
837
  
Figure 1. An example of data records and attributes in a research article. 
taken up and transported to (Burns et al, 2007). 
To extract data records from the research litera-
ture, we need to solve two sub-problems: discover-
ing individual attributes of records and grouping 
them into one or more individual records, each re-
cord representing one TTE experiment. Each at-
tribute may contain a list of words or phrases and 
each record may contain a list of attributes.  
Listing each sentence from top to bottom, we 
call the first problem the Horizontal Problem (HP) 
and the second the Vertical Problem (VP). Figure 
1 provides an example of a TTE research article 
with colored fragments representing attributes and 
dashed frames representing data records. For in-
stance, the third dashed frame represents one ex-
periment record having three attributes with corre-
sponding biological interpretations: ?no labeled 
cells?, ?the DCN?, and ?the contralateral AVCN?. 
We view the HP and VP problems as two se-
quential labeling problems and describe our ap-
proach using two-level Conditional Random Fields 
(CRF) (Lafferty et al, 2001) models to extract data 
records and their attributes.  
The HP problem (finding individual attribute 
values) is solved using a sentence-level CRF label-
ing model that integrates a rich set of linguistic 
features. For the VP problem, we apply a dis-
course-level CRF model to identify individual ex-
periments (data records). This model utilizes deep 
semantic knowledge from the HP results (attribute 
labels within sentences) together with semantic 
language models and achieves significant im-
provements over baseline systems.  
This paper mainly focuses on the VP problem, 
since linguistic features for the HP problem is the 
general IE topic of much past research (e.g., Peng 
and McCallum, 2004). We apply various feature 
combinations to learn the most suitable and indica-
tive linguistic features. 
The remainder of this paper is organized as fol-
lows: in the next section we discuss related work. 
Following that, we present the approach to extract 
data records in Section 3. We give extensive ex-
perimental evaluations in Section 4 and conclude 
in Section 5. 
2 Related Work 
As mentioned, data record extraction has been 
extensively studied for structured and semi-
structured resources (e.g., Muslea et al, 2001; 
Arasu and Garcia-Molina, 2003; Liu et al, 2003; 
Zhu et al, 2006). Most of those approaches rely on 
the analysis of document structure (reflected in, for 
example, html tags), from which record templates 
are derived. However, this approach does not apply 
to unstructured text. The reason lies in the 
difficulty of representing a data record template in 
free text without formatting tags and integrating it 
838
 into a learning system. We show how to address 
this problem by deriving data record templates 
through language analysis and representing them 
with a discourse level CRF model. 
Given the problem of identifying one or more 
records in free text, it is natural to turn toward text 
segmentation. The Natural Language Processing 
(NLP) community has come up with various 
solutions towards topic-based text segmentation 
(e.g., Hearst, 1994; Choi, 2000; Malioutov and 
Barzilay, 2006). Most unsupervised text 
segmentation approaches work under optimization 
criteria to maximize the intra-segment similarity 
and minimize the inter-segment similarity based on 
word distribution statistics. However, this 
approach cannot be applied directly to data record 
extraction. A careful study of our corpus shows 
that data records share many words and phrases 
and are not distinguishable based on word 
similairties. In other words, different experiments 
(records) always belong to the same topic and there 
is no way to segment them using standard topic 
segmentation techniques (even if one views the 
problem as a finer-level segmentation than 
traditional text segmentation). In addition, most 
text segmentation approaches require a 
prespecified number of segments, which in our 
domain cannot be provided. 
(Wick et al, 2006) report extracting database re-
cords by learning record field compatibility. How-
ever, in our case, the field compatibility is hard to 
distinguish even by a human expert. Cluster-based 
or pairwise field similarity measures do not apply 
to our corpora without complex knowledge reason-
ing. Most of Wick et al?s data (faculty and stu-
dent?s homepages) contains one record. 
In addition, as explained below, we have found 
that surface word statistics alone are not sufficient 
to derive data record templates for extraction. 
Some (limited) form of semantic understanding of 
text is necessary. We therefore first perform some  
sentence level extraction (following the HP 
problem) and then integrate semantic labels and 
semantic language model features into a discourse 
level CRF model to represent the template for 
extracting data records in the future. 
Recently an increasing number of research ef-
forts on text mining and IE have used CRF models 
(e.g., Peng and McCallum, 2004). The CRF model 
provides a compact way to integrate different types 
of features when sequential labeling is important. 
Recent work includes improved model variants 
(e.g., Jiao et al, 2006; Okanohara et al, 2006) and 
applications such as web data extraction (Pinto et 
al., 2003), scientific citation extraction (Peng and 
McCallum, 2004), and word alignment (Blunsom 
and Cohn, 2006). But none of them have used 
CRFs for discourse level data record extraction. 
We use a CRF model to represent a data record 
template and integrate various knowledge as CRF 
features. Instead of traditional work on the sen-
tence level, our focus here is on the discourse level. 
As this has not been carefully explored, we ex-
periment with various selected features. 
For the biomedical domain, our work will facili-
tate biomedical research by supporting the con-
struction of Knowledge Base Management Sys-
tems (e.g., Stephan et al, 2001; Hahn et al, 2002; 
Burns and Cheng, 2006). Unlike the well-studied 
problem of relation extraction from biomedical 
text, our work focuses on grouping extracted at-
tributes across sentences into meaningful data re-
cords. TTE experiment is only one of many ex-
perimental types in biology. Our work can be gen-
eralized to many different types of data records to 
facilitate biology research. 
In the next section, we present our approach to 
extracting data records. 
3 Extracting Data Records 
Inspired by the idea of Noun Phrase (NP) chunking 
in a single sentence, we view the data records 
extraction problem as discourse chunking from a 
sequence of sentences using a sequential labeling 
CRF model. 
3.1 Sequential Labeling Model: CRF 
The CRF model addresses the problem of labeling 
sequential tokens while relaxing the strong 
independence assumptions of Hidden Markov 
Models (HMMs) and avoiding the presence of 
label bias from having few successor states. For 
each current state, we obtain the conditional 
probability of its output states given previously 
assigned values of input states. For most language 
processing tasks, this model is simply a linear-
chain Markov Random Fields model. 
In typical labeling processes using CRFs each 
token is viewed as a labeling unit. For our prob-
lem, we process each input document 
),...,,( 21 nsssD =  as a sequence of individual sen-
839
 tences, with a corresponding labeling sequence of 
labels, ),...,,( 21 nlllL = , so that each sentence corre-
sponds to only one label. In our problem, each data 
record corresponds to a distinct TTE experiment. 
Similar to NP chunking, we define three labels for 
sentences, ?B_REC? (beginning of record), 
?I_REC? (inside record), and ?O? (other). The de-
fault label ?O? indicates that this sentence is be-
yond our concern. 
The CRF model is trained to maximize the 
probability of )|( DLP , that is, given an input 
document D, we find the most probable labeling 
sequence L. The decision rule for this procedure is: 
)|(maxarg? DLPL
L
=                                        (1) 
A CRF model of the two sequences is character-
ized by a set of feature functions kf and their corre-
sponding weights k? . As in Markov fields, the 
conditional probability )|( DLP  can be computed 
using Equation 2. 
??
???
???=
= ?
T
t k
ttkk
S
tDllf
Z
DLP
1
1 ),,,(*exp
1
)|( ?        (2) 
where ),,,( 1 tDllf ttk ? is a feature function, represent-
ing either the state transition feature ),,( 1 Dllf ttk ?  or 
the feature of output state ),( Dlf tk given the input 
sequence. All these feature functions are user-
defined boolean functions. 
CRF works under the framework of supervised 
learning, which requires a pre-labeled training set 
to learn and optimize system parameters to maxi-
mize the probability or its log format. Equipped 
with this model, we investigate how to apply it and 
prepare features accordingly. 
3.2 Feature Preparation 
The CRF model provides a compact, unified 
framework to integrate features. However, unlike 
sentence-level processing, where features are very 
intuitive and circumscribed, it is not obvious what 
features are most indicative for our problem. We 
therefore explore three categories of features for 
discourse level chunking. 
3.2.1 Semantic Attribute Labels 
Most text segmentation approaches compute 
surface word similarity scores in given corpora 
without semantic analysis. However, in our case, 
data records have very similar characteristics and 
share most of the words. They are not 
distinguishable just from an analysis of surface 
word statistics. We have to understand the 
semantics before we can make decisions about data 
record extraction.  
In our case, we care about the four types of at-
tributes of each data record (one TTE experiment). 
Table 1 gives the definitions of the four attributes 
for each data record. 
Name Description 
injectionLocation the named brain region where the injection was made. 
tracerChemical the tracer chemical used. 
labelingLocation the region/location where the labeling was found. 
labelingDescription 
a description of labeling, in-
cluding label density or label 
type. 
Table 1. Attributes of data records (a TTE experiment). 
To obtain this semantic attributes information of 
individual sentences (the HP problem), we first 
apply another sentence-level CRF model to label 
each sentence. We consider five categories of fea-
tures based on language analysis. Table 2 shows 
the features for each category. 
Name Feature Description 
TOPOGRAPHY Is word topog-
raphic? 
BRAIN_REGION Is word a region 
name? 
TRACER Is word a tracer 
chemical? 
DENSITY Is word a den-
sity term? 
Lexicon 
Knowledge 
LABELING_TYPE Does word de-
note a labeling 
type? 
Surface 
Word 
Word Current word 
Context    
Window 
CONT-INJ If current word 
is within a win-
dow of injection 
context 
Prev-word Previous word Window 
Words Next-word Next word 
Root-form Root form of 
the word if dif-
ferent 
Gov-verb The governing 
verb 
Subject The sentence 
subject  
Dependency 
Features 
Object The sentence 
object 
Table 2. The features for labeling words. 
840
 a. Lexicon knowledge. We used names of brain 
structures taken from brain atlases (Swanson, 
2004), standard terms to denote neuro-
anatomical topographical relationships (e.g., 
?rostral?), the name or abbreviation of the 
tracer chemical used (e.g., ?PHAL?), and 
commonsense descriptions for descriptions of 
the labeling (e.g., ?dense?, ?light?).  
b. Surface and window word. The current 
word and the words around are important in-
dicators of the most probable label. 
c. Context window. The TTE is a description of 
the inject-label-findings process. Whenever a 
word having a root form of ?injection? or 
?deposit? appears, we generate a context 
window and all the words falling into this 
window are assigned a feature of ?CONT-
INJ?.  
d. Dependency features. We apply a depend-
ency parser MiniPar (Lin, 1998) to parse each 
sentence, and then derive four types of fea-
tures from the parsing result. These features 
are (a) root form of every word, (b) the sub-
ject within the sentence, (c) the object within 
the sentence, and (d) the governing verbs. 
The labeling system assigns a label for every to-
ken in each sentence. We achieved the best per-
formance with an F-score of 0.79 (based on a pre-
cision of 0.80 and a recall of 0.78). This is not the 
focus of this paper. Please refer to our previous 
work (Burns et al, 2007) for details. 
 
 
 
 
 
 
 
Figure 2. An example of semantic attribute labels. 
With the sentence-level understanding of each 
sentence, we obtain the semantic attribute labels 
for the data records. Figure 2 gives an example 
sentence with semantic attribute labels. Here 
<tracerChemical>, <labelingLocation>, and <la-
belingDescription> are recognized by the system, 
and the attribute names will be used as features for 
this sentence. 
3.2.2 Semantic Language Model 
Since text narratives might adhere to logical ways 
of expressing facts, language models for each sen-
tence will also provide good features to extract 
data records. However, in biomedical research arti-
cles many of the technical words/phrases used in 
the narrative are repeated across experiments, mak-
ing the surface word language model of little use in 
deriving generalized data record templates. Con-
sidering this, we replace in each sentence the la-
beled fragments with their attribute labels and then 
derive semantic language models from that format. 
By ?semantic language model? we therefore mean 
a combination of semantic labels and surface 
words.  
For example, in the sentence shown in Figure 2, 
we have the semantic language model trigrams 
location-of-<tracerChemical>, sites-in-
<injectionLocation>, and <labelingDescription>-
followed-the. In addition, we also query WordNet 
for the root form of each word to generalize the 
semantic language models. This for example pro-
duces the semantic language model trigrams site-
in-<injectionLocation> and <labelingDescription>-
follow-the. 
We believe the collected semantic language 
models represent an inherent structure of unstruc-
tured data records. By integrating them as features 
with a CRF model, we expect to represent data re-
cord templates and use the learned model to extract 
new data records.  
However, it is not clear what semantic language 
models are most indicative and useful. A bag-of-
words (language models) approach may bring 
much noise in. We show below a comparison of 
regular language models and semantic language 
models in evaluations.  
3.2.3 Layout and Word Heuristics 
The previous two categories of features come from 
the discovery of semantic components of sentences 
and their narrative form word analysis. When in-
terviewing the neuroscience expert annotator, we 
learned that some layout and word level heuristics 
may also help to delineate individual data records. 
Table 3 gives the two types of heuristic features. 
When a sentence contains heuristic words, it 
will be assigned to a word heuristic feature. If the 
sentence is at the boundary of a paragraph, it will 
be assigned a layout heuristic feature, namely the 
first or the last sentence in the paragraph.  
<SENT FILE="1995-360-213-ns.xml" INDEX= "63"> 
Regardless of the precise location of <tracerChemical> 
PHAL </tracerChemical> injection sites in <injectionLo-
cation> the MEA </injectionLocation> , <labelingDe-
scription> labeled axons </labelingDescription> followed 
the same basic routes . 
</SENT> 
841
 Name Feature Descrip-tion 
EXP_B_WORD 
INJECT 
CASE 
EXPERIMENT 
APPLICATION 
DEPOSIT 
PLACEMENT 
INTRODUCTION 
Heuristic 
words for 
beginning 
of an ex-
periment 
descrip-
tion 
POS_IN_PARA FIRST_IN_PARA 
LAST_IN_PARA 
Position of 
the sen-
tence in 
the para-
graph 
Table 3. The heuristic features. 
4 Empirical Evaluation 
To evaluate the effectiveness and performance of 
our technique, we conducted extensive experi-
ments to measure the data record extraction ap-
proach. 
4.1 Experimental Setup 
We used the machine learning package MALLET 
(McCallum, 2002) to conduct the CRF model 
training and labeling. 
We have obtained the digital publications of 
9474 Journal of Comparative Neurology (JCN)1 
articles from 1982 to 2005. We have converted the 
PDF format into plain text, maintaining paragraph 
breaks (some errors still occur though).  A simple 
heuristic based approach identifies semantic sec-
tions of the paper (e.g, Introduction, Results, Dis-
cussion). As most experimental descriptions appear 
in the Results section, we only process the Results 
section. A neuroscience expert manually annotated 
the data records in the Results section of 58 re-
search articles. The total number of sentences in 
the Results section of the 58 files is 6630 (averag-
ing 114.3 sentences per article). 
 Training Set Testing Set 
Docs 39 19 
Data Records 249 133 
Table 4. Experiment configuration. 
We randomly divided this material into training 
and testing sets under a 2:1 ratio, giving 39 docu-
ments in the training set and 19 in the testing set. 
                                                 
1 http://www3.interscience.wiley.com/cgi-bin/jhome/31248 
Table 4 gives the numbers of documents and data 
records in the training and the testing set. 
4.2 Evaluation Metrics 
To evaluate data record extraction, we notice it is 
not fair to strictly evaluate the boundaries of data 
records because this does not penalize the near-
miss and false positive of data records in a reason-
able way; sentences near a boundary that contain 
no relevant record information can be included or 
omitted without affecting the results. Hence the 
standard Pk (Beeferman et al, 1997) and WinDiff 
(Pevzner and Hearst, 2002) measures for text seg-
mentation are not so suitable for our task. 
As we are concerned with the usefulness of 
knowledge in extracted data records, we instead 
evaluate from the perspective of IE. We measure 
system performance on the quality of the extracted 
data records. For each extracted data record, it will 
be aligned to one of the data records in the gold 
standard using the ?dominance rule? (if the data 
record can be aligned to multiple records in the 
gold standard, it will be aligned to the one with 
highest overlap). Then we evaluate the precision, 
recall, and F1 scores of extracted units of the data 
record. The units are the attributes in data records. 
system by the units extracted  theof #
unitscorrect   # of
precision =   (3) 
standard gold in the units  theof #
 unitscorrect   # of
recall =                (4) 
ecallrprecision
recall*precision
F +=
*2
1                                    (5) 
These measures provide an indication of the 
completeness and correctness of each extracted 
record (experiment). We also measure the number 
of distinct records extracted, compared with the 
gold standard as appearing in the document. 
4.3 Experiment Results 
To fully compare the effectiveness of our semantic 
analysis functionality, we evaluated system per-
formance for all the following systems:  
TextTiling (TT): To compare with text segmen-
tation techniques, we use TextTiling (Hearst, 1994) 
with default parameters as the first baseline sys-
tem. 
Random Guess (RG): In order to demonstrate 
the data balance of all the possible labels in the 
testing set, we also use another baseline system 
with random decisions for each sentence.  
842
 Domain Heuristics (DH): In a regular TTE ex-
periment, only one tracer chemical will typically 
be used. Given this heuristic, we assume each data 
record contains one tracer chemical. In this system, 
we first locate sentences with identified trace 
chemicals, and then we greedily expand backward 
and forward until another new tracer chemical ap-
pears or no other attribute is included. 
Surface Text (ST): To measure the effective-
ness of the semantic analysis (attribute labels and 
semantic language models), the ST system utilizes 
only standard surface word language models and 
heuristic features. 
Semantic Analysis (SEM): The SEM system 
uses all the semantic features available (including 
identified attributes and semantic language models) 
and two heuristic features. 
Table 5 shows the final performance of these 
different systems. The second column provides the 
numbers of extracted data records. In this task, a 
larger number does not necessarily mean a better 
system, as a system might produce too many false 
positives. The remaining three columns represent 
the precision, recall, and F1 scores, averaged over 
all data records. With our approach, the system 
performance is significantly improved compared 
with other systems. System TT fails in this task as 
it only outputs the full document as one single re-
cord. 
 # of     
Records 
Prec. Rec. F1 
TT 19 0.3861 1.0 0.5571 
RG 758 0.6331 0.0913 0.1595 
DH 162 0.6703 0.4902 0.5663 
ST 82 0.8182 0.8339 0.8260 
SEM 72 0.8505 0.9258 0.8865 
Table 5. System performance. 
To investigate how plain text language models 
and semantic language models affect system per-
formance, we also experimented with all the lan-
guage models. Table 6 shows comparisons of three 
types of language models. Systems with semantic 
analysis always work better than those with only 
surface text analysis. Without semantic analysis, 
unigram features work better than bigram and tri-
gram features. This matches our intuition: without 
generalizing to semantic language models, higher 
order language models will be relatively sparse and 
contain much noise. However, when taking into 
account the semantic features, we found that bi-
gram and trigram semantic language model fea-
tures outperformed unigrams. They are especially 
important in boosting the recall scores as they cap-
ture more generalized information when derived. 
Unigram (%) Bigram (%) Trigram (%)  
Prec/Rec/F1 Prec/Rec/F1 Prec/Rec/F1 
ST 81.8/83.4/82.6 69.1/88.4/77.6 57.9/88.8/70.1 
SEM 85.1/86.6/85.6 85.1/92.6/88.7 82.2/92.7/87.1 
Table 6. Language model comparisons. 
As an example, Table 7 gives a list of high qual-
ity bigram semantic language models ranked by 
their information gains based on the training data. 
through_<labelingLocation> rat_no 
<labelingDescription>_be of_<tracerChemical> 
<labelingLocation>_( <tracerChemical>_be 
<tracerChemical>_injection be_inject 
into_<injectionLocation> be_center 
<labelingDescription>_from inject_with 
<tracerChemical>_in injection_of 
in_<labelingLocation> in_experiment 
Table 7. An example list of top-ranked bigrams. 
The main difficulty for data record extraction 
from unstructured text lies in deriving and repre-
senting a template for future extraction. We actu-
ally take advantage of CRF and represent the tem-
plate with a CRF model.  
Each data record is measured with precision, re-
call, and F1 scores. Figure 3 depicts the distribu-
tion of extracted data records according to these 
measures in the best system. 
Distribution
0
5
10
15
20
25
30
35
40
45
50
55
60
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Performance
# 
of
 e
xt
ra
ct
ed
 r
ec
or
ds
Prec
Rec
F1
 
Figure 3. Data records performance distribution. 
The results are encouraging, especially given the 
complexity and flexibility of data record descrip-
tions in the unstructured text. In Figure 3, Axis X 
843
 represents the value interval for precision, recall, 
and F1, and Axis Y represents the number of ex-
tracted records with their corresponding values. 
For example, 57 records have recall scores falling 
into [0.9, 1.0].  
Figure 4 gives an example alignment between 
system result and the gold standard. Each record is 
represented by a range of sentences. The numbers 
following each record in the system result are indi-
vidual data record?s precision and recall scores. 
          System                                   Gold 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 4. An example of record extraction in one doc. 
This is a real example from the testing set. For 
records R1, R3, and R6, the system can extract the 
exact sentences contained. For record R2 and R5, 
although they do not exactly match at the sentence 
level, the extracted record contains the entire re-
quired set of attributes as in the gold standard.  
4.4 Error Analysis and Discussion 
When we investigated the errors, we found that 
sometimes the extracted data records combined 
two or more smaller gold standard records, or vice 
versa. As shown in Figure 4, extracted records R4 
and R7 are both combinations of records in the 
gold standard. This is partially due to the granular-
ity definition problem. Authors may mention sev-
eral approaches/symptoms to one type of experi-
ment for a single purpose. In this case, it is almost 
infeasible to have annotators strictly agree on 
granularity and thus to teach the system to acquire 
this knowledge. For example, in the gold standard, 
the annotator annotated three successive sentences 
as three separate records but the system output 
those as only one data record. In this extreme case, 
it is too hard to expect the system to perform well. 
In our approach, the semantic attribute labels 
and semantic language models require the result of 
the initial sentence-level labeling, which has an F-
score of 0.79. The error may propagate into the 
data record extraction procedure and lower overall 
system performance. 
In our current experiments, we also assume all 
the attributes within one segment belong to one 
record. However, the situation of embedded data 
records will make this problem harder. For exam-
ple, authors sometimes compare the current ex-
periment with other approaches in referenced pa-
pers. In this case, those attributes should be ex-
cluded from the records. We need to invent rules or 
constraints to filter them out. When such reference 
occurs at experiment boundaries, it brings higher 
risk for correct results.  
It is a very hard problem to extract from unstruc-
tured text neat structured records. The annotators 
sometimes employ background knowledge or rea-
soning when performing manual extraction; such 
knowledge cannot today be easily modeled and 
integrated into learning systems.  
In our study, we also compared some feature se-
lection approaches. Similar to (Yang and Pedersen, 
1997), we tried Feature Instance Frequency, Mu-
tual Information, Information Gain, and CHI-
square test. But we eventually found that the sys-
tem including all the features worked best, and 
with all the other configurations unchanged, fea-
ture instance frequency worked at almost the same 
level as other complex measures such as mutual 
information and information gain.  
5 Conclusion and Future Work 
In this paper, we explored the problem of extract-
ing data records from unstructured text. The lack 
of structure makes it difficult to derive meaningful 
objects and their values without resorting to deeper 
language analysis techniques. We derived indica-
tive linguistic features to represent data record 
templates in free text, using a two-pass approach in 
which the second pass used the IE labels derived 
from the first to compose attributes into coherent 
data records. We evaluated the results from an IE 
perspective and reported potential problems of er-
ror generation. 
? 
R1:S12~S29 (1.0/1.0) 
? 
R2: S31~S41 (1.0/1.0) 
 
R3: S42~S52 (1.0/1.0) 
? 
R4: S56~S73 
(0.517/1.0) 
? 
R5: S75~S88 (1.0/1.0) 
? 
R6: S91~S106(1.0/1.0) 
? 
R7: S108~S118 
(0.523/1.0)  
? 
? 
R1': S12~S29 
? 
R2': S31~S40  
? 
R3': S42~S52 
? 
R4': S56~S63 
? 
R5': S65~S73 
R6': S74~S88 
.. 
R7': S91~S106 
? 
R8': S108~S114 
R9': S115~S118 
? 
844
 For the future, we plan to explore additional fea-
ture types and feature selection strategies to deter-
mine what is ?good? for unstructured record tem-
plates to improve our results. More effort will also 
be put into the sentence-level analysis to reduce 
error propagations. In addition, ontology based 
knowledge inference strategies might be useful to 
validate attributes in single record and in turn help 
data record extraction. The last thing under our 
direction is to explore new models if applicable.  
We hope this thought-provoking problem will 
attract more attention from the community. In the 
future, we plan to make our corpus available to the 
community. The solution to this problem will 
highly affect the access of knowledge in large scale 
unstructured text corpora. 
Acknowledgements 
The work was supported in part by an ISI seed 
funding, and in part by a grant from the National 
Library of Medicine (RO1 LM07061). The authors 
want to thank Feng Pan for his helpful suggestions 
with the manuscript. We would also like to thank 
the anonymous reviewers for their valuable com-
ments. 
References 
Arasu, A., and Garcia-Molina, H. 2003. Extracting 
structured data from web pages. In Proc. of SIMOD-
2003.  
Beeferman, D., Berger, A., and Lafferty, J. 1997. Text 
segmentation using exponential models. In Proc. of 
EMNLP-1997.  
Blunsom, P. and Cohn, T. 2006. Discriminative word 
alignment with conditional random fields. In Proc. of 
ACL-2006.  
Brazma, A., et al, 2001. Minimum information about a 
microarray experiment (MIAME)-toward standards 
for microarray data. Nat Genet, 29(4): p. 365-71.  
Burns, G.A. and Cheng, W.-C. 2006. Tools for knowl-
edge acquisition within the NeuroScholar system and 
their application to anatomical tract-tracing data. In 
Journal of Biomedical Discovery and Collaboration.  
Burns, G., Feng, D., and Hovy, E.H. 2007. Intelligent 
Approaches to Mining the Primary Research Litera-
ture: Techniques, Systems, and Examples. Book 
Chapter in Computational Intelligence in Bioinfor-
matics, Springer-Verlag, Germany. 
Choi, F. Y. Y. 2000. Advances in domain independent 
linear text segmentation. In Proc. of NAACL-2000.  
Hahn, U., Romacher, M., and Schulz, S. 2002. Creating 
knowledge repositories from biomedical reports the 
MEDSYNDIKATE text mining system. In Proc. of 
PSB-2002. 
Hearst, M. 1994. Multi-paragraph segmentation of ex-
pository text. In Proc. of ACL-1994.  
Jiao, F., Wang, S., Lee, C., Greiner, R., and 
Schuurmans, D. 2006. Semi-supervised conditional 
random fields for improved sequence segmentation 
and labeling. In Proc. of ACL-2006.  
Kristjannson, T., Culotta, A. Viola, P., and McCallum, 
2004. A. Interactive information extraction with con-
strained conditional random fields. In Proc. of AAAI-
2004. 
Lafferty, J., McCallum, A. and Pereira, F. 2001 Condi-
tional Random Fields: probabilistic models for seg-
menting and labeling Sequence Data. In Proc. of 
ICML-2001. 
Lin, D. 1998. Dependency-based evaluation of MINI-
PAR. In Proc. of Workshop on the Evaluation of 
Parsing Systems.  
Liu, B., Grossman, R., and Zhai, Y. 2003. Mining data 
records in web pages. In Proc. of SIGKDD-2003.  
Malioutov, I. and Barzilay, R. 2006. Minimum cut 
model for spoken lecture segmentation. In Proc. of 
ACL-2006.  
McCallum, A.K. 2002. MALLET: A Machine Learning 
for Language Toolkit. http://mallet.cs.umass.edu. 
Muslea, I., Minton, S., and Knoblock, C.A. 2001. 
Hierarchical wrapper induction for semistructured 
information sources. Autonomous Agents and Multi-
Agent Systems 4:93-114. 
Okanohara, D., Miyao, Y., Tsuruoka, Y., and Tsujii, J. 
2006. Improving the scalability of semi-markov con-
ditional random fields for named entity recognition. 
In Proc. of ACL-2006.  
Peng, F. and McCallum, A. 2004. Accurate information 
extraction from research papers using conditional 
random fields. In Proc. of HLT-NAACL-2004.  
Pevzner, L., and Hearst, M. 2002. A Critique and Im-
provement of an Evaluation Metric for Text Segmen-
tation. Computational Linguistics. 
Pinto, D., A. McCallum, X. Wei, and W.B. Croft. 2003. 
Table Extraction Using Conditional Random Fields. 
In Proc. of SIGIR-2003.  
845
 Stephan, K.E. et al, 2001. Advanced database method-
ology for the Collation of Connectivity data on the 
Macaque brain (CoCoMac). Philos Trans R Soc Lond 
B Biol Sci, 356(1412).  
Swanson, L.W. 2004. Brain Maps: Structure of the Rat 
Brain. 3rd edition, Elsevier Academic Press.  
Wick, M., Culotta, A., and McCallum, A. 2006. Learn-
ing field compatibilities to extract database records 
from unstructured text. In Proc. of EMNLP-2006. 
Yang, Y., and Pedersen, J. 1997. A comparative study 
on feature selection in text categorization. In Proc. of 
ICML-1997, pp. 412-420.  
Zhu, J., Nie, Z., Wen, J., Zhang, B., and Ma, W. 2006. 
Simultaneous record detection and attribute labeling 
in web data extraction. In Proc. of KDD-2006. 
846
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 596?603, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Handling Biographical Questions with Implicature 
 
 
Donghui Feng Eduard Hovy 
Information Sciences Institute Information Sciences Institute 
University of Southern California University of Southern California 
Marina del Rey, CA, 90292 Marina del Rey, CA, 90292 
donghui@isi.edu hovy@isi.edu 
 
 
 
 
Abstract 
Traditional question answering systems 
adopt the following framework: parsing 
questions, searching for relevant docu-
ments, and identifying/generating an-
swers. However, this framework does not 
work well for questions with hidden as-
sumptions and implicatures. In this paper, 
we describe a novel idea, a cascading 
guidance strategy, which can not only 
identify potential traps in questions but 
further guide the answer extraction pro-
cedure by recognizing whether there are 
multiple answers for a question. This is 
the first attempt to solve implicature prob-
lem for complex QA in a cascading fash-
ion using N-gram language models as 
features. We here investigate questions 
with implicatures related to biography 
facts in a web-based QA system, Power-
Bio. We compare the performances of 
Decision Tree, Na?ve Bayes, SVM (Sup-
port Vector Machine), and ME (Maxi-
mum Entropy) classification methods. 
The integration of the cascading guidance 
strategy can help extract answers for 
questions with implicatures and produce 
satisfactory results in our experiments. 
1 Motivation 
Question Answering has emerged as a key area in 
natural language processing (NLP) to apply ques-
tion parsing, information extraction, summariza-
tion, and language generation techniques (Clark et 
al., 2004; Fleischman et al, 2003; Echihabi et al, 
2003; Yang et al, 2003; Hermjakob et al, 2002; 
Dumais et al, 2002). Traditional question answer-
ing systems adopt the framework of parsing ques-
tions, searching for relevant documents, and then 
pinpointing and generating answers. However, this 
framework includes potential dangers. For exam-
ple, to answer the question ?when did Beethoven 
get married??, a typical QA system would identify 
the question target to be a ?Date? and would apply 
techniques to identify the date Beethoven got mar-
ried. Since Beethoven never married, this direct 
approach is likely to deliver wrong answers. The 
trick in the question is the implicature that Beetho-
ven got married. In the main task of QA track of 
TREC 2003, the performances of most systems on 
providing ?NIL? when no answer is possible range 
from only 10% to 30% (Voorhees, 2003). 
Just as some questions have no answer, others 
may have multiple answers. For instance, with 
?who was Ronald Reagan?s wife??, a QA system 
may give only ?Nancy Davis? as the answer. How-
ever, there is another correct answer: Jane Wyman. 
The problem here is the implicature in the question 
that Reagan only got married once. 
An implicature is anything that is inferred from 
an utterance but that is not a condition for the truth 
of the utterance (Gazdar, 1979; Levinson, 1983). 
Implicatures in questions either waste computa-
tional effort or impair the performance of a QA 
system or both. Therefore, when answering ques-
tions, it is prudent to identify the questions with 
implicatures before processing starts.  
In this paper, we describe a novel idea to solve 
the problem: a strategy of cascading guidance. This 
is the first attempt to solve implicature problem for 
complex QA in a cascading fashion using N-gram 
596
language models as features. The cascading guid-
ance part is designed to be inserted immediately 
before the search procedure to handle questions 
with implicatures. It can not only first identify the 
potential ?no answer? traps but also identify 
whether multiple answers for this question are 
likely.  
To investigate the performance of the cascading 
guidance strategy, we here study two types of 
questions related to biography facts in a web-based 
biography QA system, PowerBio. This web-based 
QA system extracts biographical facts from the 
web obtained by querying a web search engine 
(Google in our case).  Figure 1 provides the two 
types of questions we selected, which we refer to 
as SPOUSE_QUESTION and CHIL-
D_QUESTION.  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. SPOUSE_QUESTION and 
CHILD_QUESTION 
 
Both types of questions have implicatures to jus-
tify the use of the cascading guidance strategy. In-
tuitively, to answer these questions, we have two 
issues related to implicatures to clarify:  
 
? Does the person have a spouse/child?  
? What's the number of answers for this ques-
tion? (One or many?) 
 
We therefore create two successive classifica-
tion engines in the cascading classifier.  
For learning, our approach queries the search 
engine with every person listed in the training set, 
extracts related features from the documents, and 
trains the cascading classifiers. For application, 
when a new question is given, the cascading classi-
fier is applied before activation of the search sub-
system. We compare the performances of four 
popular classification approaches in the cascading 
classifier, namely Decision Tree, Na?ve Bayes, 
SVM (Support Vector Machine), and ME (Maxi-
mum Entropy) classifications. 
The paper is structured as follows: related work 
is discussed in Section 2. We introduce our cascad-
ing guidance technique in Section 3, including De-
cision Tree, Na?ve Bayes and SVM (Support 
Vector Machine) and ME (Maximum Entropy) 
classifications. The experimental results are pre-
sented in Section 4. We discuss related issues and 
future work in Section 5.  
2 Related Work 
Question Answering has attracted much attention 
from the areas of Natural Language Processing, 
Information Retrieval and Data Mining (Fleisch-
man et al, 2003; Echihabi et al, 2003; Yang et al, 
2003; Hermjakob et al, 2002; Dumais et al, 2002; 
Hermjakob et al, 2000). It is tested in several ven-
ues, including the TREC and CLEF Question An-
swering tracks (Voorhees, 2003; Magnini et al, 
2003). Most research efforts in the Question An-
swering community have focused on factoid ques-
tions and successful Question Answering systems 
tend to have similar underlying pipelines structures 
(Prager et al, 2004; Xu et al, 2003; Hovy et al, 
2000; Moldovan et al, 2000). 
Recently more techniques for answer extraction, 
answer selection, and answer validation have been 
proposed (Lita et al, 2004; Soricut and Brill, 2004; 
Clark et al, 2004).  
Prager et al (2004) proposed applying constraint 
satisfaction obtained by asking auxiliary questions 
to improve system performance. This approach 
requires the creation of auxiliary questions, which 
may be complex to automate. 
Ravichandran and Hovy (2002) proposed auto-
matically learning surface text patterns for answer 
extraction. However, this approach will not work if 
no explicit answers exist in the source. The first 
reason is that in that situation the anchors to learn 
the patterns cannot be determined. Secondly, most 
of the facts without explicit values are not ex-
pressed with long patterns including anchors. For 
example, the phrase ?the childless marriage? gives 
enough information that a person has no child. But 
it is almost impossible to learn such surface text 
patterns following (Ravichandran and Hovy, 2002). 
Reported work on question processing focuses 
mainly on the problems of parsing questions, de-
termining the question target for search subsystem 
I. SPOUSE_QUESTION 
    E.g. Who is <PERSON>?s wife?      
           Who is <PERSON>?s husband? 
           Whom did <PERSON> marry? 
            ? 
II. CHILD_QUESTION 
    E.g. Who is <PERSON>?s son?      
           Who is <PERSON>?s daughter? 
           Who is <PERSON>?s child? 
?
597
(Pasca and Harabagiu, 2001; Hermjakob et al, 
2000). Saquete et al (2004) decompose complex 
temporal questions into simpler ones based on the 
temporal relationships in the question. 
To date, there has been little published work on 
handling implicatures in questions. Just-In-Time 
Information Seeking Agents (JITISA) was pro-
posed by Harabagiu (2001) to process questions in 
dialogue and implicatures. The agents are created 
based on pragmatic knowledge. Traditional answer 
extraction and answer fusion approaches assume 
the question is always correct and explicit answers 
do exist in the corpus. Reported work attempts to 
rank the candidate answer list to boost the correct 
one into top position. This is not enough when 
there may not be an answer for the question posed.  
For biographical fact extraction and generation, 
Zhou et al (2004) and Schiffman et al (2001) use 
summarization techniques to generate human biog-
raphies. Mann and Yarowsky (2005) propose fus-
ing the extracted information across documents to 
return a consensus answer. In their approach, they 
did not consider multiple values or no values for 
biography facts, although multiple facts are com-
mon for some biography attributes, such as multi-
ple occupations, children, books, places of 
residence, etc. In these cases a consensus answer is 
not adequate. 
Our work differs from theirs because we are not 
only working on information/answer extraction; 
the focus in this paper is the guidance for answer 
extraction of questions (or IE task for values) with 
implicatures. This work can be of great help for 
immediate biographical information extraction. 
We describe details of the cascading guidance 
technique and investigate how it will help for ques-
tion answering in Section 3.  
3 Cascading Guidance Technique 
We turn to the Web by querying a web search en-
gine (Google in our case) to find evidence to create 
guidance for answer extraction. 
3.1 Classification Procedure 
The cascading classifier is applied after the name 
of the person and the answer types are identified. 
Figure 2 gives the pipeline of the classification 
procedure. 
With the identified person name, we query the 
search engine (Google) to obtain the top N web 
pages/documents. A simple data cleaning program 
only keeps the content texts in the web page, which 
is broken up into separate sentences. Following 
that, topic sentences are identified with the key-
word topic identification technique. For each topic 
we provide a list of possible related keywords and 
any sentences containing both the person?s name 
(or reference) and at least one of the keywords will 
be selected. The required features are extracted 
from the topic sentences and passed to the cascad-
ing classifier as supporting evidence to generate 
guidance for answer extraction. 
 
Figure 2. Procedure of Cascading Classifier 
3.2 Feature Extraction 
Intuitively, sentences elaborating a biographical 
fact in a given topic should have similar styles 
(short patterns) of organizing words and phrases. 
Here, topic means an aspect of biographical facts, 
e.g., marriage, children, birthplace, and so on. In-
spired by this, we consider taking N-grams in sen-
tences as our features. However, N-gram features 
not closely related to the topic will bring more 
noise into the system. Therefore, we only take the 
N-grams within a fixed-length window around the 
topic keywords for features calculation, and pass 
them as evidence to cascading classifier.  
Classification Results 
Search EnginePerson 
Name 
Web 
Pages 
Data Cleaner
Sentence breaker 
Cascading 
Classifier  
Clean 
Topic  
Sentences
Topic  
Identification
Feature  
Extraction 
598
For N-grams, instead of using the multiplication 
of conditional probabilities of each word in the N-
gram, we only consider the last conditional prob-
ability (see below). The reason is that the last con-
ditional probability is a strong sign of the pattern?s 
importance and how this sequence of words is or-
ganized. Simply multiplying all the conditional 
probabilities will decrease the value and require 
normalization. Realizing that in a set of documents 
the frequency of each N-gram is very important 
information, we combine the last conditional prob-
ability with the frequency. 
The computation for each feature of unigram, 
bigram and trigram are defined as the following 
formulas:  
)(*)( iiunigram wfreqwpf =                             (1) 
),(*)|( 11 iiiibigram wwfreqwwpf ??=             (2) 
),,(*),|( 1212 iiiiiitrigram wwwfreqwwwpf ????=     
                                                                           (3) 
We here investigate four kinds of classifiers, 
namely Decision Tree, Na?ve Bayes, Support Vec-
tor Machine (SVM), and Maximum Entropy (ME).  
3.3 Classification Approaches 
The cascading classifier is composed of two suc-
cessive parts. Given the set of extracted features, 
the classification result could lead to different re-
sponses to the question, either answering with ?no 
value? with strong confidence or directing the an-
swer extraction model how many answers should 
be sought. 
For text classification, there are several well-
studied classifiers in the machine learning and 
natural language processing communities.  
 
Decision Tree Classification 
The Decision Tree classifier is simple and matches 
human intuitions perfectly while it has been proved 
efficient in many application systems. The basic 
idea is to break up the classification decision into a 
union of a set of simpler decisions based on N-
gram features. Due to the large feature set, we use 
C5.0, the decision tree software package developed 
by RuleQuest Research (Quinlan, 1993), instead of 
C4.5. 
 
Na?ve Bayes Classification 
The Na?ve Bayes classifier utilizes Bayes' rule as 
follows. Supposing we have the feature 
set { }nfffF ,...,, 21= , the probability that person 
p belongs to a class c is given as: 
)|'(maxarg
'
FcPc
c
=     (4) 
Based on Bayes? rule, we have 
)'()'|(maxarg
)(
)'()'|(
maxarg
)|'(maxarg
'
'
'
cPcFP
FP
cPcFP
FcPc
c
c
c
=
=
=
    (5) 
This was used for both successive classifiers of the 
cascading engine. 
 
SVM Classification 
SVM (Support Vector Machines) has attracted 
much attention since it was introduced in (Boser et 
al., 1992). As a special and effective approach for 
kernel based methods, SVM creates non-linear 
classifiers by applying the kernel trick to maxi-
mum-margin hyperplanes.  
Suppose nipi ,...,1, =  represent the training set 
of persons, and the classes for classifications are 
},{ 21 ccC = (for simplicity, we represent the 
classes with { }1,1?=C ). Then the classification 
task requires the solution of the following optimi-
zation problem (Hsu et al, 2003): 
0
1))((
2
1
min
1
,,
?
??+
+ ?
=
i
ii
T
i
n
i
i
T
b
bpctosubject
M
?
???
?????
    (6) 
We use the SVM classification package 
LIBSVM (Chang and Lin, 2001) in our problem. 
 
ME Classification 
ME (Maximum Entropy) classification is used here 
to directly estimate the posterior probability for 
classification. 
Suppose p represents the person and the classes 
for classifications are { }21,ccC = , we have M fea-
ture functions Mmpchm ,...,1),,( = . For each fea-
ture function, we have a model 
parameter Mmm ,...,1, =? . The classification with 
599
maximum likelihood estimation can be defined as 
follows (Och and Ney, 2002): 
? ?
?
=
==
=
'
1
]),(exp[
]),(exp[
)|()|(
1
'
1
c
M
m
mm
M
m
mm
pch
pch
pcppcP M
?
?
?
    (7) 
The decision rule to choose the most probable 
class is (Och and Ney, 2002): { }
??
?
??
?=
=
?
=
M
m
mm
c
c
pch
pcPc
1
),(maxarg
)|(maxarg?
?            (8) 
We use the published package YASMET 1  to 
conduct parameters training and classification. 
YASMET requires supervised learning for the 
training of maximum entropy model. 
The four classification approaches are assem-
bled in a cascading fashion. We discuss their per-
formance next. 
4 Experiments and Results 
4.1 Experimental Setup 
We download from infoplease.com 2  and biogra-
phy.com 3  two corpora of people?s biographies, 
which include 24,975 and 24,345 bios respectively. 
We scan each whole corpus and extract people 
having spouse information. To create the data set, 
we manually check and categorize each person as 
having multiple spouses, only one spouse, or no 
spouse. Similarly, we obtained another list of per-
sons having multiple children, only one child, and 
no child. The sizes of data extracted are given in 
Table 1.  
 
Type Child Spouse 
No_value 25 20 
One_value 35 32 
Multiple_values 107 43 
Table 1. Extracted experimental data 
 
For the cascading classification, in the first step, 
when classifying whether a person has a 
spouse/child or not, we merge the last two subsets 
                                                          
1 http://www.fjoch.com/YASMET.html 
2 http://www.infoplease.com/people.html 
3 http://www.biography.com/search/index.jsp 
with one value and multiple values into one. Table 
2 presents the data used for each level of classifica-
tion. 
 
 class Child Spouse
No_value 25 20 First-level 
Classification With_value 142 75 
One_value 35 32 Second-level 
Classification Multiple_value 107 43 
Table 2. Data set used for classification 
To investigate the performances of our cascad-
ing classifiers, we divided the two sets into training 
set and testing set, with half of them in the training 
set and half in the testing set. 
4.2 Empirical Results 
For each situation of the two questions, when the 
answer type has been determined to be the child or 
spouse of a person, we send the person?s name to 
Google and collect the top N documents. As de-
scribed in Figure 2, topic sentences in each docu-
ment are selected by keyword matching. A window 
with the length of w is applied to the sentence. All 
word sequences in the window are selected for fea-
ture calculation. We take all the three N-gram lan-
guage models (unigram, bigram, and trigram) in 
the window for feature computation. Table 3 gives 
the sizes of the bigram feature sets for first-level 
classification as we take more and more documents 
into the system. 
 
Top N Docs Child Spouse 
1 3468 1958 
10 27733 12325 
20 46431 27331 
30 61057 36637 
40 76687 43771 
50 87020 50868 
60 96393 61632 
70 108053 67712 
80 118947 73306 
90 130526 77370 
100 139722 82339 
Table 3. Sizes of feature sets 
 
As described in Section 3, the feature values are 
applied in the classifiers. Tables 4 and 5 give the 
best performances of the 4 classifiers in the two 
situations when we select the top N articles using 
N-gram probability for feature computation. 
Due to the large size of the feature set, C5.0, 
SVM, and ME packages will not work at some 
600
point as more documents are encountered. The Na-
?ve Bayes classification is more scalable as we use 
intermediate file to store probability tables. 
 
Precision First-level 
Classification 
Second-level 
Classification
C5.0 82.90% 65.70% 
Na?ve 
Bayes 
87.80% 72.86% 
SVM 84.15% 75.71% 
ME 86.59% 75.71% 
Table 4. Precision scores for child classification 
 
 
Precision First-level 
Classification 
Second-level 
Classification
C5.0 80.90% 56.80% 
Na?ve 
Bayes 
83.00% 
 
59.46% 
SVM 78.72% 54.05% 
ME 78.72% 51.35% 
Table 5. Precision scores for spouse classification 
 
 
Feature # of times  
identified  
(out of 75) 
p(wi|wi-2,wi-1) 
and his wife 35  0.6786 
her husband , 33 0.3082 
and her husband 26   0.5476 
was married to 20 0.8621 
with his wife 14   0.875 
her second husband   13 0.6667 
her marriage to 13 0.5 
ex - wife           12 0.3333 
ex - husband 11    0.6667 
her first husband     10  0.75 
second husband ,     10       1 
his first wife 8 0.3333 
first husband ,  7        0.6667 
second wife ,   7 0.3333 
his first marriage     5        0.1667 
s second wife 5 0.75 
Table 6. Example trigram features for second-level 
classification for Spouse (one or multiple values) 
 
The feature set has a large number of features. 
However, not all of them will be used for each per-
son. We studied the number of times features are 
identified/used in the training and testing sets and 
their probabilities. Table 6 presents a list of some 
trigram features for second-level classification 
(one or multiple values) for Spouse. Obviously, 
indicating features have a large probability as ex-
pected. The second column gives the number of 
times the feature is used out of the training and 
testing set (75 persons in total). 
 
Will more complex N-gram features work bet-
ter? 
Intuitively, being less ambiguous, more complex 
N-gram features carry more precise information 
and therefore should work better than simple ones. 
We studied the performances for different N-gram 
language model features. Below are the results of 
Na?ve Bayes first-level classification for Child, 
using different N-gram features. 
 
 
Top N 
Docs 
Unigram Bigram Trigram 
1 34.78% 54.35% 67.39% 
10 30.48% 79.27% 86.59% 
20 26.83% 82.93% 85.37% 
30 24.39% 81.71% 86.59% 
Table 7. Comparisons of classification precisions 
using different N-gram features for child 
 
From Table 7, we can infer that bigram features 
work better than unigram features, and trigram fea-
tures work better than bigrams when we select dif-
ferent numbers of top N documents. Trigram 
features actually bring enough evidence in classifi-
cation. However, when we investigated 4-grams 
language features in the collected data, most of 
them are very sparse in the feature space of all the 
cases. Applying 4-grams or higher may not help in 
our task. 
 
Will more data/documents help? 
The performance of corpus-based statistical ap-
proaches usually depends on the size of corpus. A 
traditional view for most NLP problems is that 
more data will help to improve the system?s per-
formance. However, for data collected from a 
search engine, this may not be the case, since web 
data is usually ambiguous and noisy. We therefore 
investigate the data size?s effect on system per-
formance. Figure 3 gives the precision curves of 
the Na?ve Bayes classifier for the first-level classi-
fication for Child. 
Except for the case of top 1, where the top 
document alone may not contain too much useful 
information on selected topics, precision scores 
only have slight variations for increasing numbers 
of documents. For bigram features, over the top 50 
601
through top 70 documents, the precision scores 
even get a little worse.  
 
Performances on Top N Docs
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 10 20 30 40 50 60 70 80 90 100
Top N
Precision
Bigram
Trigram
 
Figure 3. Performance on top N documents 
4.3 Examples 
Equipped with the cascading guiding strategy, we 
are able to handle questions containing implica-
tures. In our system, when we can determine the 
answer type is child or spouse, the cascading guid-
ing system will help the answer extraction part to 
extract answers from the designated corpus. Figure 
4 gives two examples of the strategy.  
 
Figure 4. Classification Example for question 
 
For the first question, the classifier recognizes 
there is no spouse for the target person and returns 
information for the answer generation. The fea-
tures used here are the first-level classification re-
sult for SPOUSE_QUESTION. For the second 
question, the classifier recognizes the target person 
has a child first, followed by recognizing that the 
answer has multiple values. In this way, the strat-
egy integrated to the question answering system 
can improve the system?s performance by handling 
questions with implicatures. 
5 Discussion and Future Work 
Questions may have implicatures due to the flexi-
bility of human language and conversation. In real 
question-answering systems, failure to handle them 
may either waste huge computation cost or impair 
system?s performance. The traditional QA frame-
work does not work well for questions containing 
implicatures. We describe a novel idea in this pa-
per to identify potential traps in biographical ques-
tions and recognize whether there are multiple 
answers for a question. 
Question-Answering systems, even when fo-
cused upon biographies, have to handle many facts, 
such as birth date, birth place, parents, training, 
accomplishments, etc. These values can be ex-
tracted using typical text harvesting approaches. 
However, when there are no values for some bio-
graphical information, the task becomes much 
more difficult because text seldom explicitly states 
a negative. For example, the following two ques-
tions require schools attended:  
 
? Where did <person> graduate from? 
? What university did <person> attend? 
 
Our program scanned the two corpora of bios 
and found only 2 out 49320 bios explicitly stating 
that the subject never attended any school. There-
fore, for some types of information, it will be much 
harder to identify null values through evidence 
from text. Some more complicated reasoning and 
inference may be required. Classifiers for some 
biographical facts may need to incorporate extra 
knowledge from other resources. The inherent rela-
tions between biography facts can also be used to 
validate each other. For example, the relations of 
marriage and child, birth place and childhood 
home, etc. may provide clues for cross-validation. 
We plan to investigate these problems in the future.  
Acknowledgements 
We wish to thank the anonymous reviewers for 
their helpful feedback and corrections. Also we 
thank Lei Ding, Feng Pan, and Deepak Ravi-
chandran for their valuable comments on this work.  
 
References  
Boser, B.E., Guyon, I. and Vapnik, V. 1992. A training 
algorithm for optimal margin classifiers. Proceedings 
of the ACM COLT 1992. 
Chang, C. and Lin, C. 2001. LIBSVM -- A library for 
support vector machines. Software available at 
http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 
Q1: Who is Sophia Smith?s spouse? 
 Classified: <NO_SPOUSE> 
 Answer: She did not marry. 
 
Q2: Who is John Ritter?s child?  
 Classified: <HAVING_CHILD> 
 Classified: <MULTIPLE_VALUES> 
 ? 
602
Chu-Carroll, J., Czuba, K., Prager, J., and Ittycheriah, 
A. 2003. In question answering, two heads are better 
than one. Proceedings of HLT-NAACL-2003. 
Clark, S., Steedman, M. and Curran, J.R. 2004. Object-
extraction and question-parsing using CCG. Proceed-
ings EMNLP-2004, pages 111-118, Barcelona, Spain. 
Dumais, S., Banko, M., Brill, E., Lin, J., and Ng, A. 
2002. Web question answering: is more always bet-
ter? Proceedings of SIGIR-2002.  
Echihabi, A. and Marcu, D. 2003. A noisy channel ap-
proach to question answering. Proceedings of ACL-
2003. 
Fleischman, M., Hovy, E.H., and Echihabi, A. 2003. 
Offline strategies for online question answering: an-
swering questions before they are asked. Proceedings 
of ACL-2003. 
Gazdar, G. 1979. Pragmatics: Implicature, presupposi-
tion, and logical form. New York: Academic Press. 
Harabagiu, S. 2001. Just-In-Time Question Answering. 
Invited talk in Proceedings of the Sixth Natural Lan-
guage Processing Pacific Rim Symposium 2001. 
Hermjakob, U., Echihabi, A., and Marcu, D. 2002. 
Natural language based reformulation resource and 
web exploitation for question answering. Proceed-
ings of TREC-2002. 
Hermjakob, U., Hovy, E.H., and Lin, C. 2000. Knowl-
edge-based question answering. TREC-2000. 
Hovy, E.H., Gerber, L., Hermjakob, U., Junk, M., and 
Lin, C. 2000. Question answering in Webclopedia. 
Proceedings of TREC-2000. 
Hovy, E.H., Hermjakob, U., Lin, C., and Ravichandran, 
D. 2002. Using knowledge to facilitate factoid an-
swer pinpointing. Proceedings of COLING-2002. 
Hsu, C.-W., Chang, C.-C., and Lin, C.-J. 2003. A Prac-
tical Guide to Support Vector Classification. Avail-
able at: 
http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.
pdf.  
Levinson, S. 1983. Pragmatics. Cambridge University 
Press. 
Lita L.V. and Carbonell, J. 2004. Instance-based ques-
tion answering: a data driven approach. Proceedings 
of EMNLP 2004. 
Magnini, B., Romagnoli, S., Vallin, A., Herrera, J., Pe-
?as, A., Peinado, V., Verdejo, F., Rijke, M. 2003. 
The Multiple Language Question Answering Track at 
CLEF 2003. CLEF 2003: 471-486. 
Mann, G. and Yarowsky, D. 2005. Multi-field informa-
tion extraction and cross-document fusion. Proceed-
ings of ACL-2005. 
Moldovan, D., Clark, D., Harabagiu, S., and Maiorano, 
S. 2003. Cogex: A logic prover for question answer-
ing. Proceedings of ACL-2003. 
Moldovan, D., Harabagiu, S., Pasca, M., Mihalcea, R., 
Girju, R., Goodrum, R., and Rus, V. 2000. The struc-
ture and performance of an open-domain question 
answering system. Proceedings of ACL-2000. 
Nyberg, E. et al 2003. A multi strategy approach with 
dynamic planning. Proceedings of TREC-2003. 
Och, F. J.and Ney, H. 2002. Discriminative training and 
maximum entropy models for statistical machine 
translation. Proceedings of ACL 2002 pp. 295-302. 
Pasca, M. and Harabagiu, S. 2001. High Performance 
Question/Answering. Proceedings of SIGIR-2001.  
Prager, J. M., Chu-Carroll, J., and Czuba, K.W.. 2004. 
Question answering using constraint satisfaction. 
Proceedings of the 42nd Meeting of the Association 
for Computational Linguistics (ACL'04). 
Quinlan, J. R. 1993. C4.5: Programs for machine learn-
ing. Morgan Kaufmann, San Mateo, CA, 1993. 
Ravichandran, D. and Hovy, E.H. 2002. Learning Sur-
face Text Patterns for a Question Answering System. 
Proceedings of ACL-2002. 
Saquete, E., Mart?nez-Barco, P., Mu?oz, R., and Vicedo, 
J.L. 2004. Splitting complex temporal questions for 
question answering systems. Proceedings of ACL'04. 
Schiffman, B., Mani, I., and Concepcion, K.J. 2001. 
Producing biographical summaries: combining lin-
guistic knowledge with corpus statistics. Proceedings 
of ACL/EACL-2001. 
Soricut, R. and Brill, E. 2004. Automatic question an-
swering: beyond the factoid. Proceedings of 
HLT/NAACL-2004, Boston, MA. 
Voorhees, E.M. 2003. Overview of the trec 2003 ques-
tion answering track. Proceedings of TREC-2003. 
Xu, J., Licuanan, A., Weischedel, R. 2003. TREC 2003 
QA at BBN: Answering Definitional Questions. Pro-
ceedings of TREC 2003. 
Yang, H., Chua, T.S., Wang, S., and Koh, C.K. 2003. 
Structured use of external knowledge for eventbased 
open domain question answering. Proceedings of 
SIGIR-2003. 
Zhou, L., Ticrea, M., and Hovy, E.H. 2004. Multi-
document biography summarization. Proceedings of 
EMNLP-2004. 
603
Towards Automated Semantic Analysis on Biomedical Research Articles
 
Donghui Feng         Gully Burns         Jingbo Zhu         Eduard Hovy 
Information Sciences Institute 
University of Southern California 
Marina del Rey, CA, 90292 
{donghui, burns, jingboz, hovy}@isi.edu 
 
Abstract 
In this paper, we present an empirical 
study on adapting Conditional Random 
Fields (CRF) models to conduct semantic 
analysis on biomedical articles using ac-
tive learning. We explore uncertainty-
based active learning with the CRF model 
to dynamically select the most informa-
tive training examples. This abridges the 
power of the supervised methods and ex-
pensive human annotation cost. 
1 Introduction 
Researchers have experienced an increasing need 
for automated/semi-automated knowledge acquisi-
tion from the research literature. This situation is 
especially serious in the biomedical domain where 
the number of individual facts that need to be 
memorized is very high. 
Many successful information extraction (IE) 
systems, work in a supervised fashion, requiring 
human annotations for training. However, human 
annotations are either too expensive or not always 
available and this has become a bottleneck to de-
veloping supervised IE methods to new domains. 
Fortunately, active learning systems design 
strategies to select the most informative training 
examples. This process can achieve certain levels 
of performance faster and reduce human annota-
tion (e.g., Thompson et al, 1999; Shen et al, 2004). 
In this paper, we present an empirical study on 
adapting CRF model to conduct semantic analysis 
on biomedical research literature. We integrate an 
uncertainty-based active learning framework with 
the CRF model to dynamically select the most in-
formative training examples and reduce human 
annotation cost. A systematic study with exhaus-
tive experimental evaluations shows that it can 
achieve satisfactory performance on biomedical 
data while requiring less human annotation. 
Unlike direct estimation on target individuals in 
traditional active learning, we use two heuristic 
certainty scores, peer comparison certainty and set 
comparison certainty, to indirectly estimate se-
quences labeling quality in CRF models. 
We partition biomedical research literature by 
experimental types. In this paper, our goal is to 
analyze various aspects of useful knowledge about 
tract-tracing experiments (TTE). This type of ex-
periments has prompted the development of sev-
eral curated databases but they have only partial 
coverage of the available literature (e.g., Stephan et 
al., 2001). 
2 Related Work 
Knowledge Base Management Systems allow 
individual users to construct personalized 
repositories of knowledge statements based on 
their own interaction with the research literature 
(Stephan et al, 2001; Burns and Cheng, 2006). But 
this process of data entry and curation is manual. 
Current approaches on biomedical text mining (e.g., 
Srinivas et al, 2005; OKanohara et al, 2006) tend 
to address the tasks of named entity recognition or 
relation extraction, and our goal is more complex: 
to extract computational representations of the 
minimum information in a given experiment type. 
Pattern-based IE approaches employ seed data 
to learn useful patterns to pinpoint required fields 
values (e.g. Ravichandran and Hovy, 2002; Mann 
and Yarowsky, 2005; Feng et al, 2006). However, 
this only works if the data corpus is rich enough to 
learn variant surface patterns and does not neces-
sarily generalize to more complex situations, such 
as our domain problem. Within biomedical articles, 
sentences tend to be long and the prose structure 
tends to be more complex than newsprint. 
871
The CRF model (Lafferty et al, 2001) provides 
a compact way to integrate different types of fea-
tures for sequential labeling problems. Reported 
work includes improved model variants (e.g., Jiao 
et al, 2006) and applications such as web data ex-
traction (Pinto et al, 2003), scientific citation ex-
traction (Peng and McCallum, 2004), word align-
ment (Blunsom and Cohn, 2006), and discourse-
level chunking (Feng et al, 2007). 
Pool-based active learning was first successfully 
applied to language processing on text classifica-
tion (Lewis and Gale, 1994; McCallum and Nigam, 
1998; Tong and Koller, 2000). It was also gradu-
ally applied to NLP tasks, such as information ex-
traction (Thompson et al, 1999); semantic parsing 
(Thompson et al, 1999); statistical parsing (Tang 
et al, 2002); NER (Shen et al, 2004); and Word 
Sense Disambiguation (Chen et al, 2006). In this 
paper, we use CRF models to perform a more com-
plex task on the primary TTE experimental results 
and adapt it to process new biomedical data. 
3 Semantic Analysis with CRF Model 
3.1 What knowledge is of interest? 
The goal of TTE is to chart the interconnectivity of 
the brain by injecting tracer chemicals into a region 
of the brain and then identifying corresponding 
labeled regions where the tracer is transported to. 
A typical TTE paper may report experiments about 
one or many labeled regions.  
Name Description 
injectionLocation the named brain region where the injection was made. 
tracerChemical the tracer chemical used. 
labelingLocation the region/location where the labeling was found. 
labelingDescription a description of labeling, den-sity or label type. 
Table 1. Minimum knowledge schema for a TTE. 
 
 
 
 
 
 
 
 
 
 
Figure 1. An extraction example of TTE description. 
In order to construct the minimum information 
required to interpret a TTE, we consider a set of 
specific components as shown in Table 1. 
Figure 1 gives an example of description of a 
complete TTE in a single sentence. In the research 
articles, this information is usually spread over 
many such sentences.  
3.2 CRF Labeling 
We use a plain text sentence for input and attempt 
to label each token with a field label. In addition to 
the four pre-defined fields, a default label, ?O?, is 
used to denote tokens beyond our concern.  
In this task, we consider five types of features 
based on language analysis as shown in Table 2. 
Name Feature Description 
TOPOGRAPHY Is word topog-
raphic? 
BRAIN_REGION Is word a region 
name? 
TRACER Is word a tracer 
chemical? 
DENSITY Is word a density 
term? 
Lexical 
Knowledge 
LABELING_TYPE Does word denote 
a labeling type? 
Surface Word Word Current word 
Context    
Window 
CONT_INJ If current word if 
within a window 
of injection con-
text 
Prev-word Previous word Window 
Words Next-word Next word 
Root-form Root form of the 
word if different 
Gov-verb The governing 
verb 
Subj The sentence 
subject  
Dependency 
Features 
Obj The sentence 
object 
Table 2. The features for system labeling. 
Lexical Knowledge. We define lexical items rep-
resenting different aspects of prior knowledge. To 
this end we use names of brain structures taken 
from brain atlases, standard terms to denote neuro-
anatomical topographical spatial relationships, and 
common sense words for labeling descriptions. We 
collect five separate lexicons as shown in Table 3. 
Lexicons # of terms # of words 
BRAIN_REGION 1123 5536 
DENSITY 8 10 
LABELING_TYPE 9 13 
TRACER 30 30 
TOPOGRAPHY 9 36 
Total 1179 5625 
Table 3. The five lexicons. 
The NY injection ( Fig . 9B ) encompassed  
 
tracerChemical 
most of the pons and was very dense in  
 
injectionLocation 
the region of the MLF. 
 
labelingLocation 
872
Surface word. The word token is an important 
indicator of the probable label for itself.  
Context Window. The TTE is a description of the 
inject-label-findings context. Whenever we find a 
word with a root form of ?injection? or ?deposit?, 
we generate a context window around this word 
and all the words falling into this window are as-
signed a feature of ?CON_INJ?. This means when 
labeling these words the system should consider 
the very current context. 
Window Words. We also use all the words occur-
ring in the window around the current word. We 
set the window size to only include the previous 
and following words (window size = 1).  
Dependency Features. To untangle word relation-
ships within each sentence, we apply the depend-
ency parser MiniPar (Lin, 1998) to parse each sen-
tence, and then derive four types of features. These 
features are (a) root form of word, (b) the subject 
in the sentence, (c) the object in the sentence, and 
(d) the governing verb for each word. 
4 Uncertainty-based Active Learning 
Active learning was initially introduced for 
classification tasks. The intuition is to always add 
the most informative examples to the training set to 
improve the system as much as possible.  
We apply an uncertainty/certainty score-based 
approach. Unlike traditional classification tasks, 
where disagreement or uncertainty is easy to obtain 
on target individuals, information extraction tasks 
in our problem take a whole sequence of tokens 
that might include several slots as processing units. 
We therefore need to make decisions on whether a 
full sequence should be returned for labeling. 
Estimations on confidence for single segments 
in the CRF model have been proposed by (Culotta 
and McCallum, 2004; Kristjannson et al, 2004). 
However as every processing unit in the data set is 
at the sentence level and we make decisions at the 
sentence level to train better sequential labeling 
models, we define heuristic scores at the sentence 
level.  
Symons et al (2006) presents multi-criterion for 
active learning with CRF models, but our motiva-
tion is from a different perspective. The labeling 
result for every sentence corresponds to a decoding 
path in the state transition network. Inspired by the 
decoding and re-ranking approaches in statistical 
machine translation, we use two heuristic scores to 
measure the degree of correctness of the top label-
ing path, namely, peer comparison certainty and 
set comparison certainty. 
Suppose a sentence S includes n words/tokens 
and a labeling path at position m in the ranked N-
best list is represented by ),...,,( 110 ?= nm lllL . Then 
the probability of this labeling path is represented 
by )( mLP , and we have the following two equa-
tions to define the peer comparison certainty 
score, )(SScore peer  and set comparison certainty 
score, )(SScoreset : 
)(
)(
)(
2
1
LP
LP
SScorepeer =                                      (1) 
?
=
=
N
k
k
set
LP
LP
SScore
1
1
)(
)(
)(                                    (2) 
For peer comparison certainty (Eq. 1), we calcu-
late the ratio of the top-scoring labeling path prob-
ability to the second labeling path probability. A 
high ratio means there is a big jump from the top 
labeling path to the second one. The higher the ra-
tio score, the higher the relative degree of correct-
ness for the top labeling path, giving system higher 
confidence for those with higher peer comparison 
certainty scores. Sentences with lowest certainty 
score will be sent to the oracle for manual labeling. 
In the labeling path space, if a labeling path is 
strong enough, its probability score should domi-
nate all the other path scores. In Equation 2, we 
compute the set comparison certainty score by con-
sidering the portion of the probability of the path in 
the overall N-best labeling path space. A large 
value means the top path dominates all the other 
labeling paths together giving the system a higher 
confidence on the current path over others. 
We start with a seed training set including k la-
beled sentences. We then train a CRF model with 
the training data and use it to label unlabeled data. 
The results are compared based on the certainty 
scores and those sentences with the lowest cer-
tainty scores are sent to an oracle for human label-
ing. The new labeled sentences are then added to 
the training set for next iteration.  
5 Experimental Results 
We first investigated how the active learning steps 
could help for the task. Second, we evaluated how 
the CRF labeling system worked with different sets 
of features. We finally applied the model to new 
873
biomedical articles and examined its performance 
on one of its subsets. 
5.1 Experimental Setup 
We have obtained 9474 Journal of Comparative 
Neurology (JCN)1 articles from 1982 to 2005. For 
sentence labeling, we collected 21 TTE articles 
from the JCN corpus. They were converted from 
PDF files to XML files, and all of the article sec-
tions were identified using a simple rule-based ap-
proach. As most of the meaningful descriptions of 
TTEs appear in the Results section, we only proc-
essed the Results section. The 21 files in total in-
clude 2009 sentences, in which 1029 sentences are 
meaningful descriptions for TTEs and 980 sen-
tences are not related to TTEs.  
We randomly split the sentences into a training 
pool and a testing pool, under a ratio 2:1. The 
training pool includes 1338 sentences, with 685 of 
them related to TTEs, while 653 not. Testing was 
based on meaningful sentences in the testing pool. 
Table 4 gives the configurations in the data pools. 
 # of        
Related 
Sentences  
# of        
Unrelated 
Sentences 
Sum 
Training Pool 685 653 1338 
Testing Pool 344 327 671 
Sum 1029 980 2009 
Table 4. Training and testing pool configurations. 
5.2 Evaluation Metrics 
As the label ?O? dominates the data set (70% out 
of all tokens), a simple accuracy score would pro-
vide an inappropriate high score for a baseline sys-
tem that always chooses ?O?. We used Precision, 
Recall, and F_Score to evaluate only meaningful 
labels. 
5.3 How well does active learning work? 
For the active learning procedure, we initially se-
lected a set of seed sentences related to TTEs from 
the training pool. At every step we trained a CRF 
model and labeled sentences in the rest of the train-
ing pool. As described in section 4, those with the 
lowest rank on certainty scores were selected. If 
they are related to a TTE, human annotation will 
be added to the training set. Otherwise, the system 
will keep on selecting sentences until it finds 
enough related sentences. 
                                                 
1 http://www3.interscience.wiley.com/cgi-bin/jhome/31248 
People have found active learning in batch mode 
is more efficient, as in some cases a single addi-
tional training example will not improve a classi-
fier/system that much. In our task, we chose the 
bottom k related sentences with the lowest cer-
tainty scores. We conducted various experiments 
for k = 2, 5, and 10. We also compared experi-
ments with passive learning, where at every step 
the new k related sentences were randomly se-
lected from the corpus. Figures 2, 3, and 4 give the 
learning curves for precision, recall, and F_Scores 
when k = 10. 
 
Figure 2. Learning curve for Precision. 
 
Figure 3. Learning curve for Recall. 
 
Figure 4. Learning curve for F_Score. 
From these figures, we can see active learning 
approaches required fewer training examples to 
achieve the same level of performance. As we it-
eratively added new labeled sentences into the 
training set, the precision scores of active learning 
were steadily better than that of passive learning as 
the uncertain examples were added to strengthen 
874
existing labels. However, the recall curve is 
slightly different. Before some point, the recall 
score of passive learning was a little better than 
active learning. The reason is that examples se-
lected by active learning are mainly used to foster 
existing labels but have relatively weaker im-
provements for new labels, while passive learning 
has the freedom to add new knowledge for new 
labels and improve recall scores faster. As we keep 
on using more examples, the active learning 
catches up with and overtakes passive learning on 
recall score. 
These experiments demonstrate that under the 
framework of active learning, examples needed to 
train a CRF model can be greatly reduced and 
therefore make it feasible to adapt to other domains. 
5.4 How well does CRF labeling work? 
As we added selected annotated sentences, the sys-
tem performance kept improving. We investigated 
system performance at the final step when all the 
related sentences in the training pool are selected 
into the training set. The testing set alo only in-
cludes the related sentences. This results in 685 
training sentences and 344 testing sentences. 
To establish a baseline for our labeling task, we 
simply scanned every sentence for words or 
phrases from each lexicon. If the term was present, 
then we labeled the word based on the lexicon in 
which it appeared. If words appeared in multiple 
lexicons, we assigned labels randomly. 
System Features Prec. Recall F_Score 
Baseline 0.4067 0.1761 0.2458 
Lexicon 0.5998 0.3734 0.4602 
Lexicon                   
+ Surface Words 
0.7663 0.7302 0.7478 
Lexicon                   
+ Surface Words     
+ Context Window 
0.7717 0.7279 0.7491 
Lexicon + Surface 
Words + Context 
Window + Window 
Words 
0.8076 0.7451 0.7751 
Lexicon + Surface 
Words + Context 
Window + Window 
Words + Depend-
ency Features  
0.7991 0.7828 0.7909 
Table 5. Precision, Recall, and F_Score for labeling. 
We tried exhaustive feature combinations. Table 
5 shows system performance with different feature 
combinations. All systems performed significantly 
higher than the baseline. The sole use of lexicon 
knowledge produced poor performance, and the 
inclusion of surface words produced significant 
improvement. The use of window words boosted 
precision and recall. The performance with all the 
features generated an F_score of 0.7909. 
We explored how system performance reflects 
different labels. Figure 5 and 6 depict the detailed 
distribution of system labeling from the perspec-
tive of precision and recall respectively for the sys-
tem with the best performance. Most errors oc-
curred in the confusion of injectionLocation and 
labelingLocation, or of the meaningful labels and 
?O?. 
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
injLoc labelDesp labelLoc tracer
O
injLoc
labelDesp
labelLoc
tracer
 
Figure 5. Precision confusion matrix distribution. 
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
injLoc labelDesp labelLoc tracer
O
injLoc
labelDesp
labelLoc
tracer
 
Figure 6. Recall confusion matrix distribution. 
The worst performance occurred for files that 
distinguish themselves from others by using fairly 
different writing styles. We believe given more 
training data with different writing styles, the sys-
tem could achieve a better overall performance. 
5.5 On New Biomedical Data 
Under this active learning framework, we have 
shown a CRF model can be trained with less anno-
tation cost than using traditional passive learning. 
We adapted the trained CRF model to new bio-
medical research articles. 
Out of the 9474 collected JCN articles, more 
than 230 research articles are on TTEs. The whole 
processing time for each document varies from 20 
seconds to 90 seconds. We sent the new system-
labeled files back to a biomedical knowledge ex-
pert for manual annotation. The time to correct one 
automatically labeled document is dramatically 
reduced, around 1/3 of that spent on raw text. 
We processed 214 new research articles and ex-
amined a subset including 16 articles. We evalu-
875
ated it in two aspects: the overall performance and 
the performance averaged at the document level. 
Table 6 gives the performance on the whole new 
subset and that averaged on 16 documents. The 
performance is a little bit lower than reported in 
the previous section as the new document set might 
include different styles of documents. We exam-
ined system performance at each document. Figure 
7 gives the detailed evaluation for each of the 16 
documents. The average F_Score of the document 
level is around 74%. For those documents with 
reasonable TTE description, the system can 
achieve an F_Score of 87%. The bad documents 
had a different description style and usually mixed 
the TTE descriptions with general discussion.  
 Prec. Recall F_Score 
Overall 0.7683 0.7155 0.7410 
Averaged per Doc. 0.7686 0.7209 0.7418 
Table 6. Performance on the whole new subset and                 
the averaged performance per document. 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
11 4 12 0 6 2 14 7 8 15 3 1 13 10 9 5 Doc No.
Precision 
Recall 
F-score
 
Figure 7. System performance per document. 
6 Conclusions and Future Work 
In this paper, we explored adapting a supervised 
CRF model for semantic analysis on biomedical 
articles using an active learning framework. It 
abridges the power of the supervised approach and 
expensive human costs. We are also investigating 
the use of other certainty measures, such as aver-
aged field confidence scores over each sentence. 
In the long run we wish to generalize the frame-
work to be able to mine other types of experiments 
within the biomedical research literature and im-
pact research in those domains. 
References 
Blunsom, P. and Cohn, T. 2006. Discriminative word align-
ment with conditional random fields. In ACL-2006. 
Burns, G.A. and Cheng, W.C. 2006. Tools for knowledge 
acquisition within the NeuroScholar system and their ap-
plication to anatomical tract-tracing data. In Journal of 
Biomedical Discovery and Collaboration. 
Chen, J., Schein, A., Ungar, L., and Palmer, M. 2006. An em-
pirical study of the behavior of active learning for word 
sense disambiguation. In Proc. of HLT-NAACL 2006.  
Culotta, A. and McCallum, A. 2004. Confidence estimation 
for information extraction. In HLT-NAACL-2004, short pa-
pers. 
Feng, D., Burns, G., and Hovy, E.H. 2007. Extracting data 
records from unstructured biomedical full text. In 
Proc. of EMNLP-CONLL-2007. 
Feng, D., Ravichandran, D., and Hovy, E.H. 2006. Mining and 
re-ranking for answering biographical queries on the web. 
In Proc. of AAAI-2006. 
Jiao, F., Wang, S., Lee, C., Greiner, R., and Schuurmans, D. 
2006. Semi-supervised conditional random fields for im-
proved sequence segmentation and labeling. In Proc. of 
ACL-2006. 
Kristjannson, T., Culotta, A., Viola, P., and McCallum, A. 
2004. Interactive information extraction with constrained 
conditional random fields. In Proc. of AAAI-2004.  
Lafferty, J., McCallum, A., and Pereira, F. 2001. Conditional 
random fields: probabilistic models for segmenting and la-
beling sequence data. In Proc. of ICML-2001. 
Lewis, D.D. and Gale, W.A. 1994. A sequential algorithm for 
training text classifiers. In Proc. of SIGIR-1994. 
Lin, D. 1998. Dependency-based evaluation of MINIPAR. In 
Workshop on the Evaluation of Parsing Systems. 
Mann, G.S. and Yarowsky, D. 2005. Multi-field information 
extraction and cross-document fusion. In Proc. of ACL-
2005. 
McCallum, A.K. 2002. MALLET: a machine Learning for 
language toolkit. http://mallet.cs.umass.edu.  
McCallum, A. and Nigam, K. 1998. Employing EM in pool-
based active learning for text classification. In Proc. of 
ICML-98.  
OKanohara, D., Miyao, Y., Tsuruoka, Y., and Tsujii, J. 2006. 
Improving the scalability of semi-markov conditional ran-
dom fields for named entity recognition. In ACL-2006. 
Peng, F. and McCallum, A. 2004. Accurate information ex-
traction from research papers using conditional random 
fields. In Proc. of HLT-NAACL-2004. 
Pinto, D., McCallum, A., Wei, X., and Croft, W.B. 2003. Ta-
ble extraction using conditional random fields. In SIGIR-
2003. 
Ravichandran, D. and Hovy, E.H. 2002. Learning surface text 
patterns for a question answering system. In ACL-2002.  
Shen, D., Zhang, J., Su, J., Zhou, G., and Tan, C.L. 2004. 
Multi-criteria-based active learning for named entity rec-
ognition. In Proc. of ACL-2004. 
Srinivas, et al, 2005. Comparison of vector space model 
methodologies to reconcile cross-species neuroanatomical 
concepts. Neuroinformatics, 3(2). 
Stephan, K.E., et al, 2001. Advanced database methodology 
for the Collation of Connectivity data on the Macaque 
brain (CoCoMac). Philos Trans R Soc Lond B Biol Sci. 
Symons et al, 2006. Multi-Criterion Active Learning in Con-
ditional Random Fields.  In ICTAI-2006. 
Tang, M., Luo, X., and Roukos, S. 2002. Active learning for 
statistical natural language parsing. In ACL-2002. 
Thompson, C.A., Califf, M.E., and Mooney, R.J. 1999. Active 
learning for natural language parsing and information ex-
traction. In Proc. of ICML-99. 
Tong, S. and Koller, D. 2000. Support vector machine active 
learning with applications to text classification. In Proc. of 
ICML-2000. 
876
 Cooperative Model Based Language Understanding in Dialogue 
 
Donghui Feng 
Information Sciences Institute, 
University of Southern California 
4676 Admiralty Way 
Marina Del Rey, CA 90292-6695 
donghui@isi.edu 
 
 
 
 
Abstract 
In this paper, we propose a novel 
Cooperative Model for natural language 
understanding in a dialogue system. We 
build this based on both Finite State Model 
(FSM) and Statistical Learning Model 
(SLM). FSM provides two strategies for 
language understanding and have a high 
accuracy but little robustness and flexibility. 
Statistical approach is much more robust 
but less accurate. Cooperative Model 
incorporates all the three strategies together 
and thus can suppress all the shortcomings 
of different strategies and has all the 
advantages of the three strategies.  
 
 
1 Introduction 
 
In this paper, we propose a novel language 
understanding approach, Cooperative Model, for a 
dialogue system. It combines both Finite State Model 
and Statistical Learning Model for sentence 
interpretation. 
This approach is implemented in the project MRE 
(Mission Rehearsal Exercise). The goal of MRE is to 
provide an immersive learning environment in which 
army trainees experience the sights, sounds and 
circumstances they will encounter in real-world 
scenarios (Swartout et al, 2001). In the whole 
procedure, language processing part plays the role to 
support the communication between trainees and 
computers. 
In the language processing pipeline, audio signals 
are first transformed into natural language sentences by 
speech recognition. Sentence interpretation part is used 
to ?understand? the sentence and extract an information 
case frame for future processing such as dialogue 
management and action planning. We adopt the 
Cooperative Model as the overall frame of sentence 
interpretation, which incorporates two mainly used 
language processing approaches: the Finite State Model 
and the Statistical Learning Model. Currently there is 
relatively little work on the cooperation of the two kinds 
of models for language understanding. 
The Cooperative Model has great advantages. It 
balances the shortcomings of each separate model. It is 
easy to implement the parsing algorithm and get the 
exact expected result for finite state model (FSM) but 
it?s difficult and tedious to design the finite state 
network by hand. Also, the finite state model is not too 
robust and the failure of matching produces no results. 
On the other hand, statistical learning model (SLM) can 
deal with unexpected cases during designing and 
training by giving a set of candidate results with 
confidence scores. It is a must to provide some kind of 
rules to select results needed. However, applying it may 
not give a completely satisfactory performance. 
The rest of this paper is organized as follows: 
Section 2 describes the case frame as the semantic 
representation produced by the cooperative model. In 
section 3, we explain our cooperative language 
understanding model and discuss two different 
strategies of the Finite State Model and the Statistical 
Learning Model. We analyze the experimental results in 
Section 4. Section 5 concludes with on-going research 
and future work. 
 
2 Semantic Representation 
 
The goal of automated natural language understanding 
is to parse natural language string, extract meaningful 
information and store them for future processing. For 
our application of training environment, it?s impossible 
to parse sentences syntactically and we here directly 
produce the nested information frames as output. The 
topmost level of the information frame is defined as 
follows: 
 
 
 
Figure 1. Topmost-Level Information Frame 
 
In the definition, <semantic-object> consists of 
<i-form> := ( ^mood <mood> 
^sem <semantic-object>) 
 three types: question, action and proposition. Here, 
question refers to requests for information, action refers 
to orders and suggestions except requests, and all the 
rest falls into the category of proposition. 
Each of these types can also be further 
decomposed as Figure 2 and 3.  
 
 
 
 
 
 
 
 
 
 
 
Figure 2. Second-Level Information Frame 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3. Third-Level Information Frame 
 
These information frames can be further extended 
and nested as necessary. In our application, most of the 
information frames obtained contain at most three levels. 
In Figure 4, we give an example of information frame 
for the English sentence ?who is not critically hurt??. 
All the target information frames in our domain are 
similar to that format. 
 
 
 
 
 
 
 
 
 
 
Figure 4. Example Nested Information Frame 
Since the information frames are nested, for the 
statistical learning model to be addressed, ideally both 
the semantic information and structural information 
should be represented correctly. Therefore we use prefix 
strings to represent the cascading level of each 
slot-value pair. The case frame in Figure 4 can be 
re-represented as shown in Figure 5. Here we assume 
that the slots in the information frame are independent 
of each other. Reversely the set of meaning items can be 
restored to a normal nested information frame. 
 
 
 
 
 
 
 
 
 
Figure 5. Re-representation to handle cascading 
 
We introduce the cooperative model in the 
following section to extract meaningful information 
frames for all the English sentences in our domain. 
 
3 Cooperative Model 
 
The Cooperative Model (CM) combines two 
commonly-used methods in natural language processing, 
Finite State Model (FSM) and Statistical Learning 
Model (SLM). We discuss them in section 3.1 and 3.2 
respectively. 
 
3.1 Finite State Model 
The main idea of finite state model is to put all the 
possible input word sequences and their related output 
information on the arcs. 
For our application, the input is a string composed 
of a sequence of words, and the output should be a 
correctly structured information frame. We apply two 
strategies of FSM. The Series Mode refers to build a 
series of finite state machine with each corresponding to 
a single slot. The Single Model builds only one complex 
Finite State Machine that incorporates all the sentence 
patterns and slot-value pairs. 
 
3.1.1 Strategy I: Series Model of Finite State 
Machine 
For this strategy, we analyze our domain to obtain a list 
of all possible slots. From the perspective of linguistics, 
a slot can be viewed as characterized by some specific 
words, say, a set of feature words. We therefore can 
make a separate semantic filter for each slot. Each 
sentence passes through a series of filters and as soon as 
<question> := (  ^type question 
^q-slot <prop-slot-name> 
^prop <proposition>) 
 
<action>  := (  ^type action-type 
^name <event-name> 
^<prop-slot-name> <val>) 
 
<proposition> := <state> | <event> | <relation> 
<state> :=  (  ^type state 
^object-id ID 
^polarity <pol> 
?) 
 
<event>  := (  ^type event-type 
^name <event-name> 
^<prop-slot-name> <val> 
?) 
 
<relation> := (  ^type relation 
^relation <rel-name> 
^arg1 <semantic-object> 
     ^arg2 <semantic-object>) 
<i> ^mood interrogative  
    <i> ^sem <t0> 
<i> <t0> ^type question  
<i> <t0> ^q-slot agent  
<i> <t0> ^prop <t1> 
<i> <t0> <t1> ^type event-type  
<i> <t0> <t1> ^time present  
<i> <t0> <t1> ^polarity negative  
<i> <t0> <t1> ^degree critical-injuries  
<i> <t0> <t1> ^attribute health-status  
<i> <t0> <t1> ^value health-bad 
Input Sentence:  who is not critically hurt?
Output Information Frame:  
(<i>  ^mood interrogative  
^sem <t0>)  
(<t0> ^type question  
^q-slot agent  
^prop <t1>)  
(<t1> ^type event-type  
^time present  
^polarity negative  
^degree critical-injuries  
^attribute health-status  
^value health-bad) 
 we find the ?feature? words, we extract their 
corresponding slot-value pairs. All the slot-value pairs 
extracted produce the final nested case frame. 
 
  Sentence                   Information Frame 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6. An Example from Series Model of FSM 
 
Figure 6 is an example of the way that series 
model of finite state machine works. For example, three 
slot-value pairs are extracted from the word ?who?. 
Practically, we identified 27 contexts and built 27 finite 
state machines as semantic filters, with each one 
associated with a set of feature words. The number of 
arcs for each finite state machine ranges from 4 to 70 
and the size of the feature word set varies from 10 to 50.  
This strategy extracts semantic information based 
on the mapping between words and slots. It is relatively 
easy to design the finite state machine networks and 
implement the parsing algorithm. For every input 
sentence it will provide all possible information using 
the predefined mappings. Even if the sentence contains 
no feature words, the system will end gracefully with an 
empty frame. However, this method doesn?t take into 
account the patterns of word sequences. Single word 
may have different meanings under different situations. 
In most cases it is also difficult to put one word into one 
single class; sometimes a word can even belong to 
different slots? feature word sets that can contradict each 
other. On the other hand, the result produced may have 
some important slot-value pairs missed and the number 
of slots is fixed. 
 
3.1.2 Strategy II: Single Model of Finite State 
Machine 
In this strategy we only build a big finite state network. 
When a new sentence goes into the big FSM parser, it 
starts from ?START? state and a successful matching of 
prespecified patterns or words will move forward to 
another state. Any matching procedure coming to the 
?END? state means a successful parsing of the whole 
sentence. And all the outputs on the arcs along the path 
compose the final parsing result. If no patterns or words 
are successfully matched at some point, the parser will 
die and return failure.  
This strategy requires all the patterns to be 
processed with this finite state model available before 
designing the finite state network. The target sentence 
set includes 65 sentence patterns and 23 classes of 
words and we combine them into a complex finite state 
network manually. Figure 7 gives some examples of the 
collected sentence patterns and word classes. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7. Target Sentence Patterns 
 
Aimed at processing these sentences, we design 
our finite state network consisting of 128 states. This 
network covers more than 20k commonly-used 
sentences in our domain. It will return the exact parsing 
result without missing any important information. If all 
of the input sentences in the application belong to the 
target sentence set of this domain, this approach 
perfectly produces all of the correct results. However, 
the design of the network is done totally by hand, which 
is very tedious and time-consuming. The system is not 
very flexible or robust and it?s difficult to add new 
sentences into the network before a thorough 
investigation of the whole finite state network. It is not 
convenient and efficient for extension and maintenance. 
Finite state models can?t process any sentence with 
new sentence patterns. However in reality most systems 
require more flexibility, robustness, and more powerful 
processing abilities on unexpected sentences. The 
statistical machine learning model gives us some light 
on that. We discuss learning models in Section 3.2.  
 
3.2 Statistical Learning Model 
 
3.2.1 Na?ve Bayes Learning 
Na?ve Bayes learning has been widely used in natural 
language processing with good results such as statistical 
syntactic parsing (Collins, 1997; Charniak, 1997), 
hidden language understanding (Miller et al, 1994). 
We represent the mappings between words and 
their potential associated meanings (meaning items 
including level information and slot-value pairs) with 
P(M|W). W refers to words and M refers to meaning 
items. With Bayes? theorem, we have the formula 3.1. 
P(W)
 P(M) * M)|P(Wmaxarg W)|P(Mmaxarg =    (3.1) 
 Here P(W|M) refers to the probability of words 
given their meanings. 
who  
is  
driving  
the  
car 
(<i> 
  ^mood interrogative 
^sem  <t0>) 
(<t0> 
^type question 
^q-slot agent 
^prop <t1>) 
(<t1> 
^type action 
^event drive 
^patient car 
^time present) 
$phrase1 = what is $agent doing; 
$phrase2 = [and|how about] (you|me|[the]  
           $vehicle|$agent); 
? 
 
$agent = he|she|$people-name|[the] ($person_civ | 
        $person_mil| $squad); 
$vehicle = ambulance | car | humvee | helicopter  
         |medevac; 
? 
 In our domain, we can view P (W) as a constant 
and transform Formula 3.1 to Formula 3.2 as follows: 
P(M)*M)|P(WmaxargW)|P(Mmaxarg
mm
=     (3.2) 
 
3.2.2 Training Set and Testing Set 
We created the training sentences and case frames by 
running full range of variation on Finite State Machine 
described in Section 3.1.2. This gives a set of 20, 677 
sentences. We remove ungrammatical sentences and 
have 16,469 left. Randomly we take 7/8 of that as the 
training set and 1/8 as the testing set.  
 
3.2.3 Meaning Model 
The meaning model P(M) refers to the probability of 
meanings. In our application, meanings are represented 
by meaning items. We assume each meaning item is 
independent of each other at this point. In the meaning 
model, the meaning item not only includes slot-value 
pairs but level information. Let C(mi) be the number of 
times the meaning item mi appears the training set, we 
obtain P(M) as follows: 
?
=
= n
j 1
j
i
i
)C(m
)C(m)P(m
 
(3.3) 
This can be easily obtained by counting all the 
meaning items of all the information frames in the 
training set. 
 
3.2.4 Word Model 
In the na?ve Bayes learning approach, P(W|M) stands 
for the probability of words appearing under given 
meanings. And from the linguistic perspective, the 
patterns of word sequences can imply strong 
information of meanings. We introduce a language 
model based on a Hidden Markov Model (HMM). The 
word model can be described as P (wi | mj, wi-2wi-1), P 
(wi | mj, wi-1) or P (wi | mj) for trigram model, bigram 
model, and unigram model respectively. They can be 
calculated with the following formulas: 
)w w,m(#
)w w,m(#
 )w w,m|P(w
1-i2-ij
1-i2-ij
1-i2-iji of
wof i=   (3.4) 
) w,m(#
) w,m(#
 )w,m|P(w
1-ij
1-ij
1-iji of
wof i=       (3.5) 
)m(#
) ,m(#
 )m|P(w
j
j
ji of
wof i=            (3.6) 
 
3.2.5 Weighted Sum Voting and Pruning 
We parse each sentence based on the na?ve Bayes 
learning Formula 3.2. Each word in the sentence can be 
associated with a set of candidate meaning items. Then 
we normalize each candidate set of meaning items and 
use the voting schema to get the final result set with a 
probability for each meaning item. 
However, this inevitably produces noisy results. 
Sometimes the meanings obtained even contradict other 
useful meaning items. We employ two cutoff strategies 
to eliminate such noise. The first is to cut off 
unsatisfactory meaning items based on a gap in 
probability. The degree of jump can be defined with an 
arbitrary threshold value. The second is to group all the 
slot-value pairs with the same name and take the top one 
as the result. 
 
3.3 Cooperative Mechanism 
In the previous two sections, we discussed two 
approaches in our natural language understanding 
system. However, neither is completely satisfactory.  
Cooperative Model can combine all three 
approaches from these two models. The main idea is to 
run the three parsing models together whenever a new 
sentence comes into the system. With the statistical 
learning model, we obtain a set of information frames. 
For the result we get from single model of finite state 
machine, if an information frame exists, it means the 
sentence is stored in the finite state network. We 
therefore assign a score 1.0. The result should be no 
worse than any information frame we get from 
statistical learning model. Otherwise, it means this 
sentence is not stored in our finite state work, we can 
ignore this result. In the end, we combine this 
information frame with the frame set from statistical 
learning model and rank them according to the 
confidence scores. Generally we can consider the one 
with the highest confidence score as our parsing result. 
The cooperative model takes all advantages of the 
three methods and combines them together. The 
cooperative mechanism also suppresses the 
disadvantages of those methods. The series model of the 
finite state machine has the advantage of mapping 
between word classes and contexts, though it sometimes 
may lose some information, and it contains real 
semantic knowledge. The statistical learning model can 
produce a set of information frames based on the word 
patterns and its noise can be removed by the result of 
the series model of the finite state machine. For the 
single finite state machine model, if it can parse 
sentence successfully, the result will always be the best 
one. Therefore through the cooperation of the three 
methods, it can either produce the exact result for 
sentences stored in the finite state network or return the 
most probable result through statistical machine 
learning method if no sentence matching occurs. Also 
the noise is reduced by the other finite state machine 
model. The cooperative model is robust and has the 
ability to learn in our target domain. 
 
4 Experimental Results 
 
The cooperative model will demonstrate its ability on 
sentence processing no matter whether the sentence is in 
 the original sentence set. However, currently we only 
have simple preference rule for the cooperation and 
haven?t obtained the overall performance. In this section, 
we?ll compare the different models? performance to 
demonstrate the cooperative model?s potential ability. 
Based on our target sentence patterns and word 
classes, we built a blind set with 159 completely new 
sentences. Although all the words used belong to this 
domain these sentences don?t appear in the training set 
and the testing set. In the evaluation of its performance, 
we compare the results of the three approaches and get 
Table 1. As we can see from this table, finite state 
method is better in the relative processing speed and for 
processing existing patterns while statistical model is 
better for processing new sentence patterns, which 
makes the system very robust. 
 
 Sentences 
in Domain 
Speed Existing 
Patterns 
New  
Patterns 
Series 
of 
FSM 
Fixed Fast 100% Partial 
Result 
Single 
FSM 
Fixed Fast 100% Die 
Stat 
Model 
Open Slow 85%(pre) 
95%(rec) 
75%(pre) 
92%(rec) 
Table 1. Results Comparison 
 
On the other hand, we investigate the performance 
of statistical model in more detail on the blind test. 
Given the whole blind testing set, the statistical learning 
model produced 159 partially correct information 
frames. We manually corrected them one by one. This 
took us 97 minutes in total. To measure this efficiency, 
we also built all the real information frames for the 
blind test set manually, one by one. It took 366 minutes 
to finish all the 159 information frames. This means it is 
much more efficient to process a completely new 
sentence set with the statistical learning model. 
We next investigate the precision and recall of this 
statistical learning model. Taking the result frames we 
manually built as the real answers, we define precision, 
recall, and F-score to measure the system?s 
performance.  
model learning from pairs value-slot of #
pairs value-slotcorrect  of #precsion =  
answer real from pairs value-slot of #
pairs value-slotcorrect  of #recall =  
recall precision 
)recall *precision (*2F_Score +=
 
Our testing strategy is to randomly select some 
portion of the new blind set and add it into the training 
set. Then we test the system with sentences in the rest of 
the blind set. As more and more new sentences are 
added into the training set (1/4, 1/3, 1/2, etc) we can see 
the performance changing accordingly. We investigate 
the three models: P(M|W), P(W|M) and P(M)*P(M|W). 
All of them are tested with same testing strategy.  
 
Portion 0 1/4 1/3 1/2 2/3 
Prec 0.7131 0.7240 0.7243 0.7311 0.7370 
Rec 0.8758 0.8909 0.8964 0.9133 0.9254 
F-Score 0.7815 0.7943 0.7966 0.8073 0.8152 
Table 2. Result of P (M|W) 
 
Portion 0 1/4 1/3 1/2 2/3 
Prec 0.7218 0.7416 0.7444 0.7429 0.7540 
Rec 0.8871 0.9161 0.9276 0.9270 0.9386 
F-Score 0.7913 0.8147 0.8208 0.8197 0.8304 
Table 3. Result of P (W|M) 
 
Portion 0 1/4 1/3 1/2 2/3 
Prec 0.7545 0.7693 0.7704 0.7667 0.7839 
Rec 0.8018 0.8296 0.8407 0.8372 0.8323 
F-Score 0.7745 0.7950 0.8021 0.7985 0.8035 
Table 4. Result of P (W|M) * P (M) 
 
From the three tables, we can see that as new 
sentences are added into the training set, the 
performance improves. Comparing Tables 2, 3 and 4, 
the poor performance of P (W|M)* P (M) is partially due 
to unbalance in the training set. The higher occurrences 
of some specific meaning items increase P(M) and 
affect the result during voting. 
 
5 Conclusions 
 
In this paper we proposed a cooperative model 
incorporating finite state model and statistical model for 
language understanding. It takes all of their advantages 
and suppresses their shortcomings. The successful 
incorporation of the methods can make our system very 
robust and scalable for future use. 
We notice that the series model of the finite state 
machine model actually incorporates some semantic 
knowledge from human beings. Ongoing research work 
includes finding new ways to integrate semantic 
knowledge to our system. For the statistical learning 
model, the quality and the different configurations of 
training set highly affect the performance of models 
trained and thus their abilities to process sentences. The 
balance of training set is also a big issue. How to build a 
balanced training set with single finite state machine 
model will remain our important work in the future. For 
the learning mechanism, Na?ve Bayesian learning 
requires more understanding of different factors? roles 
and their importance. These problems should be 
investigated in future work. 
 
 
Acknowledgements 
 
The author would like to thank Deepak Ravichandran 
for his invaluable help of the whole work. 
  
 
References 
Eugene Charniak. 1997. Statistical Parsing with a Context-free 
Grammar and Word Statistics. Proc. Of AAAI-97. pp. 
598-603. 
 
M. Collins. 1997. Three Generative, Lexicalised Models for 
Statistical Parsing. Proc. of the 35th ACL. 
 
S. Miller, R. Bobrow, R. Ingria, and R. Schwartz. 1994. 
Hidden Understanding Models of Natural Language," 
Proceedings of the Association of Computational 
Linguistics, pp. 25-32. 
 
W. Swartout, et al 2001. Toward the Holodeck: Integrating 
Graphics, Sound, Character and Story. Proceedings of 5th 
International Conference on Autonomous Agents 
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 208?215,
New York, June 2006. c?2006 Association for Computational Linguistics
Learning to Detect Conversation Focus of Threaded Discussions 
 
 
Donghui Feng        Erin Shaw        Jihie Kim        Eduard Hovy 
Information Sciences Institute 
University of Southern California 
Marina del Rey, CA, 90292 
{donghui, shaw, jihie, hovy}@isi.edu 
 
 
Abstract 
In this paper we present a novel feature-
enriched approach that learns to detect the 
conversation focus of threaded discus-
sions by combining NLP analysis and IR 
techniques. Using the graph-based algo-
rithm HITS, we integrate different fea-
tures such as lexical similarity, poster 
trustworthiness, and speech act analysis of 
human conversations with feature-
oriented link generation functions. It is 
the first quantitative study to analyze hu-
man conversation focus in the context of 
online discussions that takes into account 
heterogeneous sources of evidence. Ex-
perimental results using a threaded dis-
cussion corpus from an undergraduate 
class show that it achieves significant per-
formance improvements compared with 
the baseline system. 
1 Introduction 
Threaded discussion is popular in virtual cyber 
communities and has applications in areas such as 
customer support, community development, inter-
active reporting (blogging) and education. Discus-
sion threads can be considered a special case of 
human conversation, and since we have huge re-
positories of such discussion, automatic and/or 
semi-automatic analysis would greatly improve the 
navigation and processing of the information.  
A discussion thread consists of a set of messages 
arranged in chronological order. One of the main 
challenges in the Question Answering domain is 
how to extract the most informative or important 
message in the sequence for the purpose of answer-
ing the initial question, which we refer to as the 
conversation focus in this paper. For example, 
people may repeatedly discuss similar questions in 
a discussion forum and so it is highly desirable to 
detect previous conversation focuses in order to 
automatically answer queries (Feng et al, 2006).  
Human conversation focus is a hard NLP (Natu-
ral Language Processing) problem in general be-
cause people may frequently switch topics in a real 
conversation. The threaded discussions make the 
problem manageable because people typically fo-
cus on a limited set of issues within a thread of a 
discussion. Current IR (Information Retrieval) 
techniques are based on keyword similarity meas-
ures and do not consider some features that are 
important for analyzing threaded discussions. As a 
result, a typical IR system may return a ranked list 
of messages based on keyword queries even if, 
within the context of a discussion, this may not be 
useful or correct. 
Threaded discussion is a special case of human 
conversation, where people may express their 
ideas, elaborate arguments, and answer others? 
questions; many of these aspects are unexplored by 
traditional IR techniques. First, messages in 
threaded discussions are not a flat document set, 
which is a common assumption for most IR sys-
tems. Due to the flexibility and special characteris-
tics involved in human conversations, messages 
within a thread are not necessarily of equal impor-
tance. The real relationships may differ from the 
analysis based on keyword similarity measures, 
e.g., if a 2nd message ?corrects? a 1st one, the 2nd 
message is probably more important than the 1st. 
IR systems may give different results. Second, 
messages posted by different users may have dif-
ferent degrees of correctness and trustworthiness, 
which we refer to as poster trustworthiness in this 
paper. For instance, a domain expert is likely to be 
more reliable than a layman on the domain topic.  
208
In this paper we present a novel feature-enriched 
approach that learns to detect conversation focus of 
threaded discussions by combining NLP analysis 
and IR techniques. Using the graph-based algo-
rithm HITS (Hyperlink Induced Topic Search, 
Kleinberg, 1999), we conduct discussion analysis 
taking into account different features, such as lexi-
cal similarity, poster trustworthiness, and speech 
act relations in human conversations. We generate 
a weighted threaded discussion graph by applying 
feature-oriented link generation functions. All the 
features are quantified and integrated as part of the 
weight of graph edges. In this way, both quantita-
tive features and qualitative features are combined 
to analyze human conversations, specifically in the 
format of online discussions. 
To date, it is the first quantitative study to ana-
lyze human conversation that focuses on threaded 
discussions by taking into account heterogeneous 
evidence from different sources. The study de-
scribed here addresses the problem of conversation 
focus, especially for extracting the best answer to a 
particular question, in the context of an online dis-
cussion board used by students in an undergraduate 
computer science course. Different features are 
studied and compared when applying our approach 
to discussion analysis. Experimental results show 
that performance improvements are significant 
compared with the baseline system. 
The remainder of this paper is organized as fol-
lows: We discuss related work in Section 2. Sec-
tion 3 presents thread representation and the 
weighted HITS algorithm. Section 4 details fea-
ture-oriented link generation functions. Compara-
tive experimental results and analysis are given in 
Section 5. We discuss future work in Section 6. 
2 Related Work 
Human conversation refers to situations where two 
or more participants freely alternate in speaking 
(Levinson, 1983). What makes threaded discus-
sions unique is that users participate asynchro-
nously and in writing. We model human 
conversation as a set of messages in a threaded 
discussion using a graph-based algorithm. 
Graph-based algorithms are widely applied in 
link analysis and for web searching in the IR com-
munity. Two of the most prominent algorithms are 
Page-Rank (Brin and Page, 1998) and the HITS 
algorithm (Kleinberg, 1999). Although they were 
initially proposed for analyzing web pages, they 
proved useful for investigating and ranking struc-
tured objects. Inspired by the idea of graph based 
algorithms to collectively rank and select the best 
candidate, research efforts in the natural language 
community have applied graph-based approaches 
on keyword selection (Mihalcea and Tarau, 2004), 
text summarization (Erkan and Radev, 2004; Mi-
halcea, 2004), word sense disambiguation (Mihal-
cea et al, 2004; Mihalcea, 2005), sentiment 
analysis (Pang and Lee, 2004), and sentence re-
trieval for question answering (Otterbacher et al, 
2005). However, until now there has not been any 
published work on its application to human con-
versation analysis specifically in the format of 
threaded discussions. In this paper, we focus on 
using HITS to detect conversation focus of 
threaded discussions.  
Rhetorical Structure Theory (Mann and Thom-
son, 1988) based discourse processing has attracted 
much attention with successful applications in sen-
tence compression and summarization. Most of the 
current work on discourse processing focuses on 
sentence-level text organization (Soricut and 
Marcu, 2003) or the intermediate step (Sporleder 
and Lapata, 2005). Analyzing and utilizing dis-
course information at a higher level, e.g., at the 
paragraph level, still remains a challenge to the 
natural language community. In our work, we util-
ize the discourse information at a message level. 
Zhou and Hovy (2005) proposed summarizing 
threaded discussions in a similar fashion to multi-
document summarization; but then their work does 
not take into account the relative importance of 
different messages in a thread. Marom and Zuker-
man (2005) generated help-desk responses using 
clustering techniques, but their corpus is composed 
of only two-party, two-turn, conversation pairs, 
which precludes the need to determine relative im-
portance as in a multi-ply conversation. 
In our previous work (Feng et al, 2006), we im-
plemented a discussion-bot to automatically an-
swer student queries in a threaded discussion but 
extract potential answers (the most informative 
message) using a rule-based traverse algorithm that 
is not optimal for selecting a best answer; thus, the 
result may contain redundant or incorrect informa-
tion. We argue that pragmatic knowledge like 
speech acts is important in conversation focus 
analysis. However, estimated speech act labeling 
between messages is not sufficient for detecting 
209
human conversation focus without considering 
other features like author information. Carvalho 
and Cohen (2005) describe a dependency-network 
based collective classification method to classify 
email speech acts. Our work on conversation focus 
detection can be viewed as an immediate step fol-
lowing automatic speech act labeling on discussion 
threads using similar collective classification ap-
proaches. 
We next discuss our approach to detect conver-
sation focus using the graph-based algorithm HITS 
by taking into account heterogeneous features. 
3 Conversation Focus Detection 
In threaded discussions, people participate in a 
conversation by posting messages. Our goal is to 
be able to detect which message in a thread con-
tains the most important information, i.e., the focus 
of the conversation. Unlike traditional IR systems, 
which return a ranked list of messages from a flat 
document set, our task must take into account 
characteristics of threaded discussions.  
First, messages play certain roles and are related 
to each other by a conversation context. Second, 
messages written by different authors may vary in 
value. Finally, since postings occur in parallel, by 
various people, message threads are not necessarily 
coherent so the lexical similarity among the mes-
sages should be analyzed. To detect the focus of 
conversation, we integrate a pragmatics study of 
conversational speech acts, an analysis of message 
values based on poster trustworthiness and an 
analysis of lexical similarity. The subsystems that 
determine these three sources of evidence comprise 
the features of our feature-based system. 
Because each discussion thread is naturally rep-
resented by a directed graph, where each message 
is represented by a node in the graph, we can apply 
a graph-based algorithm to integrate these sources 
and detect the focus of conversation. 
3.1 Thread Representation 
A discussion thread consists of a set of messages 
posted in chronological order. Suppose that each 
message is represented by mi, i =1,2,?, n. Then 
the entire thread is a directed graph that can be rep-
resented by G= (V, E), where V is the set of nodes 
(messages), V= {mi,i=1,...,n}, and E is the set of 
directed edges. In our approach, the set V is auto-
matically constructed as each message joins in the 
discussion. E is a subset of VxV. We will discuss 
the feature-oriented link generation functions that 
construct the set E in Section 4. 
We make use of speech act relations in generat-
ing the links. Once a speech act relation is identi-
fied between two messages, links will be generated 
using generation functions described in next sec-
tion. When mi is a message node in the thread 
graph, VmF i ?)( represents the set of nodes that 
node mi points to (i.e., children of mi), and 
VmB i ?)( represents the set of nodes that point to 
mi (i.e., parents of mi). 
3.2 Graph-Based Ranking Algorithm: HITS 
Graph-based algorithms can rank a set of objects in 
a collective way and the affect between each pair 
can be propagated into the whole graph iteratively. 
Here, we use a weighted HITS (Kleinberg, 1999) 
algorithm to conduct message ranking. 
Kleinberg (1999) initially proposed the graph-
based algorithm HITS for ranking a set of web 
pages. Here, we adjust the algorithm for the task of 
ranking a set of messages in a threaded discussion. 
In this algorithm, each message in the graph can be 
represented by two identity scores, hub score and 
authority score. The hub score represents the qual-
ity of the message as a pointer to valuable or useful 
messages (or resources, in general). The authority 
score measures the quality of the message as a re-
source itself. The weighted iterative updating com-
putations are shown in Equations 1 and 2. ?
?
+ =
)(
1 )(*)(
ij mFm
j
r
iji
r mauthoritywmhub           (1) 
?
?
+ =
)(
1 )(*)(
ij mBm
j
r
jii
r mhubwmauthority           (2) 
where r and r+1 are the numbers of iterations. 
The number of iterations required for HITS to 
converge depends on the initialization value for 
each message node and the complexity of the 
graph. Graph links can be induced with extra 
knowledge (e.g. Kurland and Lee, 2005). To help 
integrate our heterogeneous sources of evidence 
with our graph-based HITS algorithm, we intro-
duce link generation functions for each of the three 
features, (gi, i=1, 2, 3), to add links between mes-
sages. 
4 Feature-Oriented Link Generation 
210
Conversation structures have received a lot of at-
tention in the linguistic research community (Lev-
inson, 1983). In order to integrate conversational 
features into our computational model, we must 
convert a qualitative analysis into quantitative 
scores. For conversation analysis, we adopted the 
theory of Speech Acts proposed by (Austin, 1962; 
Searle, 1969) and defined a set of speech acts (SAs) 
that relate every pair of messages in the corpus. 
Though a pair of messages may only be labeled 
with one speech act, a message can have multiple 
SAs with other messages. 
We group speech acts by function into three 
categories, as shown in Figure 1. Messages may 
involve a request (REQ), provide information 
(INF), or fall into the category of interpersonal 
(INTP) relationship. Categories can be further di-
vided into several single speech acts.  
 
Figure 1. Categories of Message Speech Act. 
The SA set for our corpus is given in Table 1. A 
speech act may a represent a positive, negative or 
neutral response to a previous message depending 
on its attitude and recommendation. We classify 
each speech act as a direction as POSITIVE (+), 
NEGATIVE (?) or NEUTRAL, referred to as SA 
Direction, as shown in the right column of Table 1. 
The features we wish to include in our approach 
are lexical similarity between messages, poster 
trustworthiness, and speech act labels between 
message pairs in our discussion corpus.  
The feature-oriented link generation is con-
ducted in two steps. First, our approach examines 
in turn all the speech act relations in each thread 
and generates two types of links based on lexical 
similarity and SA strength scores. Second, the sys-
tem iterates over all the message nodes and assigns 
each node a self-pointing link associated with its 
poster trustworthiness score. The three features are 
integrated into the thread graph accordingly by the 
feature-oriented link generation functions. Multiple 
links with the same start and end points are com-
bined into one. 
Speech
Act Name Description Dir. 
ACK Acknowl-edge 
Confirm or             
acknowledge + 
CANS Complex Answer 
Give answer requiring a 
full description of pro-
cedures, reasons, etc. 
 
COMM Command Command or            announce  
COMP Compli-ment 
Praise an argument or 
suggestion + 
CORR Correct Correct a wrong answer or solution ? 
CRT Criticize Criticize an argument ? 
DESC Describe Describe a fact or    situation  
ELAB Elaborate Elaborate on a previous argument or question  
OBJ Object Object to an argument or suggestion ? 
QUES Question Ask question about a specific problem  
SANS Simple Answer 
Answer with a short 
phrase or few words      
(e.g. factoid, yes/no) 
 
SUG Suggest Give advice or suggest a solution  
SUP Support Support an argument or suggestion + 
Table 1. Types of message speech acts in corpus. 
4.1 Lexical Similarity 
Discussions are constructed as people express 
ideas, opinions, and thoughts, so that the text itself 
contains information about what is being dis-
cussed. Lexical similarity is an important measure 
for distinguishing relationships between message 
pairs. In our approach, we do not compute the lexi-
cal similarity of any arbitrary pair of messages, 
instead, we consider only message pairs that are 
present in the speech act set. The cosine similarity 
between each message pair is computed using the 
TF*IDF technique (Salton, 1989). 
Messages with similar words are more likely to 
be semantically-related. This information is repre-
sented by term frequency (TF). However, those 
Inform:    
INF 
Interpersonal: 
INTP 
COMM  
QUES  
Speech 
Act Request: 
REQ 
ACK 
COMP 
CRT  
OBJ  
SUP  
CANS 
CORR 
DESC 
ELAB 
SANS 
SUG 
211
with more general terms may be unintentionally 
biased when only TF is considered so Inverse 
Document Frequency (IDF) is introduced to miti-
gate the bias. The lexical similarity score can be 
calculated using their cosine similarity. 
),(cos_ ji
l mmsimW =                      (3) 
For a given a speech act, SAij(mi?mj), connect-
ing message mi and mj, the link generation function 
g1 is defined as follows:  
)()(1
l
ijij WarcSAg =                          (4) 
The new generated link is added to the thread 
graph connecting message node mi and mj with a 
weight of Wl. 
4.2 Poster Trustworthiness 
Messages posted by different people may have dif-
ferent degrees of trustworthiness. For example, 
students who contributed to our corpus did not 
seem to provide messages of equal value. To de-
termine the trustworthiness of a person, we studied 
the responses to their messages throughout the en-
tire corpus. We used the percentage of POSITIVE 
responses to a person?s messages to measure that 
person?s trustworthiness. In our case, POSITIVE 
responses, which are defined above, included SUP, 
COMP, and ACK. In addition, if a person?s mes-
sage closed a discussion, we rated it POSITIVE. 
Suppose the poster is represented by kperson , 
the poster score, pW , is a weight calculated by 
))((
))(_(
)(
k
k
k
p
personfeedbackcount
personfeedbackpositivecount
personW =  
                                                                        (5) 
For a given single speech act, SAij(mi?mj), the 
poster score indicates the importance of message 
mi by itself and the generation function is given by  
)()(2
p
iiij WarcSAg =                               (6) 
The generated link is self-pointing, and contains 
the strength of the poster information. 
4.3 Speech Act Analysis 
We compute the strength of each speech act in a 
generative way, based on the author and trustwor-
thiness of the author. The strength of a speech act 
is a weighted average over all authors. 
)(
)(
)(
)()( k
P
person
persons personW
SAcount
SAcount
dirsignSAW
k
k?= (7) 
where the sign function of direction is defined with 
Equation 8. 
??
??=
             Otherwise     1
NEGATIVE isdir  if     1
)(dirsign                      (8) 
All SA scores are computed using Equation 7 
and projected to [0, 1]. For a given speech act, 
SAij(mi?mj), the generation function will generate 
a weighted link in the thread graph as expressed in 
Equation 9. 
??
??
?=
               Otherwise      )(
NEUTRAL is  if      )(
)(3 s
ij
ij
s
ii
ij Warc
SAWarc
SAg     (9) 
The SA scores represent the strength of the rela-
tionship between the messages. Depending on the 
direction of the SA, the generated link will either 
go from message mi to mj or from message mi to mi 
(i.e., to itself). If the SA is NEUTRAL, the link will 
point to itself and the score is a recommendation to 
itself. Otherwise, the link connects two different 
messages and represents the recommendation de-
gree of the parent to the child message. 
5 Experiments 
5.1 Experimental Setup  
We tested our conversation-focus detection ap-
proach using a corpus of threaded discussions from 
three semesters of a USC undergraduate course in 
computer science. The corpus includes a total of 
640 threads consisting of 2214 messages, where a 
thread is defined as an exchange containing at least 
two messages. 
Length of thread Number of threads 
3 139 
4 74 
5 47 
6 30 
7 13 
8 11 
Table 2. Thread length distribution. 
From the complete corpus, we selected only 
threads with lengths of greater than two and less 
than nine (messages). Discussion threads with 
lengths of only two would bias the random guess 
of our baseline system, while discussion threads 
with lengths greater than eight make up only 3.7% 
of the total number of threads (640), and are the 
least coherent of the threads due to topic-switching 
and off-topic remarks. Thus, our evaluation corpus 
included 314 threads, consisting of 1307 messages, 
with an average thread length of 4.16 messages per 
212
thread. Table 2 gives the distribution of the lengths 
of the threads. 
The input of our system requires the identifica-
tion of speech act relations between messages. Col-
lective classification approaches, similar to the 
dependency-network based approach that Carvalho 
and Cohen (2005) used to classify email speech 
acts, might also be applied to discussion threads. 
However, as the paper is about investigating how 
an SA analysis, along with other features, can 
benefit conversation focus detection, so as to avoid 
error propagation from speech act labeling to sub-
sequent processing, we used manually-annotated 
SA relationships for our analysis. 
Code Frequency Percentage (%) 
ACK 53 3.96 
CANS 224 16.73 
COMM 8 0.6 
COMP 7 0.52 
CORR 20 1.49 
CRT 23 1.72 
DESC 71 5.3 
ELAB 105 7.84 
OBJ 21 1.57 
QUES 450 33.61 
SANS 23 1.72 
SUG 264 19.72 
SUP 70 5.23 
Table 3. Frequency of speech acts. 
The corpus contains 1339 speech acts. Table 3 
gives the frequencies and percentages of speech 
acts found in the data set. Each SA generates fea-
ture-oriented weighted links in the threaded graph 
accordingly as discussed previously. 
 
Number of best     
answers 
Number of threads 
1 250 
2 56 
3 5 
4 3 
Table 4. Gold standard length distribution. 
We then read each thread and choose the mes-
sage that contained the best answer to the initial 
query as the gold standard. If there are multiple 
best-answer messages, all of them will be ranked 
as best, i.e., chosen for the top position. For exam-
ple, different authors may have provided sugges-
tions that were each correct for a specified 
situation. Table 4 gives the statistics of the num-
bers of correct messages of our gold standard. 
We experimented with further segmenting the 
messages so as to narrow down the best-answer 
text, under the assumption that long messages 
probably include some less-than-useful informa-
tion. We applied TextTiling (Hearst, 1994) to seg-
ment the messages, which is the technique used by 
Zhou and Hovy (2005) to summarize discussions. 
For our corpus, though, the ratio of segments to 
messages was only 1.03, which indicates that our 
messages are relatively short and coherent, and that 
segmenting them would not provide additional 
benefits. 
5.2 Baseline System 
To compare the effectiveness of our approach with 
different features, we designed a baseline system 
that uses a random guess approach. Given a dis-
cussion thread, the baseline system randomly se-
lects the most important message. The result was 
evaluated against the gold standard. The perform-
ance comparisons of the baseline system and other 
feature-induced approaches are presented next. 
5.3 Result Analysis and Discussion 
We conducted extensive experiments to investigate 
the performance of our approach with different 
combinations of features. As we discussed in Sec-
tion 4.2, each poster acquires a trustworthiness 
score based on their behavior via an analysis of the 
whole corpus. Table 5 is a sample list of some 
posters with their poster id, the total number of 
responses (to their messages), the total number of 
positive responses, and their poster scores pW . 
 
Poster 
ID 
    Total 
Response 
  Positive 
Response  
pW  
193 1 1 1 
93 20 18 0.9 
38 15 12 0.8 
80 8 6 0.75 
47 253 182 0.719 
22 3 2 0.667 
44 9 6 0.667 
91 6 4 0.667 
147 12 8 0.667 
32 10 6 0.6 
190 9 5 0.556 
97 20 11 0.55 
12 2 1 0.5 
Table 5. Sample poster scores. 
213
Based on the poster scores, we computed the 
strength score of each SA with Equation 7 and pro-
jected them to [0, 1]. Table 6 shows the strength 
scores for all of the SAs. Each SA has a different 
strength score and those in the NEGATIVE cate-
gory have smaller ones (weaker recommendation). 
 
SA )(SAWs  SA )(SAWs  
CANS 0.8134 COMM 0.6534 
DESC 0.7166 ELAB 0.7202 
SANS 0.8281 SUG 0.8032 
QUES 0.6230   
ACK 0.6844 COMP 0.8081 
SUP 0.8057   
CORR 0.2543 CRT 0.1339 
OBJ 0.2405   
Table 6. SA strength scores. 
We tested the graph-based HITS algorithm with 
different feature combinations and set the error rate 
to be 0.0001 to get the algorithm to converge. In 
our experiments, we computed the precision score 
and the MRR (Mean Reciprocal Rank) score 
(Voorhees, 2001) of the most informative message 
chosen (the first, if there was more than one). Ta-
ble 7 shows the performance scores for the system 
with different feature combinations. The perform-
ance of the baseline system is shown at the top. 
The HITS algorithm assigns both a hub score 
and an authority score to each message node, re-
sulting in two sets of results. Scores in the HITS_ 
AUTHORITY rows of Table 7 represent the re-
sults using authority scores, while HITS_HUB 
rows represent the results using hub scores.  
Due to the limitation of thread length, the lower 
bound of the MRR score is 0.263. As shown in the 
table, a random guess baseline system can get a 
precision of 27.71% and a MRR score of 0.539.  
When we consider only lexical similarity, the 
result is not so good, which supports the notion 
that in human conversation context is often more 
important than text at a surface level. When we 
consider poster and lexical score together, the per-
formance improves. As expected, the best per-
formances use speech act analysis. More features 
do not always improve the performance, for exam-
ple, the lexical feature will sometimes decrease 
performance. Our best performance produced a 
precision score of 70.38% and an MRR score of 
0.825, which is a significant improvement over the 
baseline?s precision score of 27.71% and its MRR 
score of 0.539. 
Algorithm  & 
Features 
Correct   
(out of 314) 
Precision 
(%) MRR 
Baseline 87 27.71 0.539 
Lexical 65 20.70 0.524 
Poster 90 28.66 0.569 
SA 215 68.47 0.819 
Lexical +  
Poster 91 28.98 0.565 
Lexical +      
SA 194 61.78 0.765 
Poster +        
SA 221 70.38 0.825 H
IT
S_
A
U
T
H
O
R
IT
Y
 
Lexical +  
Poster + 
SA 
212 67.52 0.793 
Lexical 153 48.73 0.682 
Poster 79 25.16 0.527 
SA 195 62.10 0.771 
Lexical +  
Poster 158 50.32 0.693 
Lexical +      
SA 177 56.37 0.724 
Poster +       
SA 207 65.92 0.793 
H
IT
S_
H
U
B
 
Lexical + 
Poster + 
SA 
196 62.42 0.762 
Table 7. System Performance Comparison. 
Another widely-used graph algorithm in IR is 
PageRank (Brin and Page, 1998). It is used to in-
vestigate the connections between hyperlinks in 
web page retrieval. PageRank uses a ?random 
walk? model of a web surfer?s behavior. The surfer 
begins from a random node mi and at each step 
either follows a hyperlink with the probability of d, 
or jumps to a random node with the probability of 
(1-d). A weighted PageRank algorithm is used to 
model weighted relationships of a set of objects. 
The iterative updating expression is 
? ??
?
+ +?=
)(
)(
1 )(*)1()(
ij
jk
mBm
j
r
mFm
jk
ji
i
r mPR
w
w
ddmPR  (10) 
where r and r+1 are the numbers of iterations. 
 
We also tested this algorithm in our situation, 
but the best performance had a precision score of 
only 47.45% and an MRR score of 0.669. It may 
be that PageRank?s definition and modeling ap-
proach does not fit our situation as well as the 
HITS approach. In HITS, the authority and hub- 
214
based approach is better suited to human conversa-
tion analysis than PageRank, which only considers 
the contributions from backward links of each 
node in the graph. 
6 Conclusions and Future Work 
We have presented a novel feature-enriched ap-
proach for detecting conversation focus of threaded 
discussions for the purpose of answering student 
queries. Using feature-oriented link generation and 
a graph-based algorithm, we derived a unified 
framework that integrates heterogeneous sources 
of evidence. We explored the use of speech act 
analysis, lexical similarity and poster trustworthi-
ness to analyze discussions. 
From the perspective of question answering, this 
is the first attempt to automatically answer com-
plex and contextual discussion queries beyond fac-
toid or definition questions. To fully automate 
discussion analysis, we must integrate automatic 
SA labeling together with our conversation focus 
detection approach. An automatic system will help 
users navigate threaded archives and researchers 
analyze human discussion. 
Supervised learning is another approach to de-
tecting conversation focus that might be explored. 
The tradeoff and balance between system perform-
ance and human cost for different learning algo-
rithms is of great interest. We are also exploring 
the application of graph-based algorithms to other 
structured-objects ranking problems in NLP so as 
to improve system performance while relieving 
human costs. 
Acknowledgements 
The work was supported in part by DARPA grant DOI-
NBC Contract No. NBCHC050051, Learning by Read-
ing, and in part by a grant from the Lord Corporation 
Foundation to the USC Distance Education Network. 
The authors want to thank Deepak Ravichandran, Feng 
Pan, and Rahul Bhagat for their helpful suggestions 
with the manuscript. We would also like to thank the 
HLT-NAACL reviewers for their valuable comments. 
References 
Austin, J. 1962. How to do things with words. Cam-
bridge, Massachusetts: Harvard Univ. Press. 
Brin, S. and Page, L. 1998. The anatomy of a large-
scale hypertextual web search engine. Computer 
Networks and ISDN Systems, 30(1-7):107--117. 
Carvalho, V.R. and Cohen, W.W. 2005. On the collec-
tive classification of email speech acts. In Proceed-
ings of SIGIR-2005, pp. 345-352. 
Erkan, G. and Radev, D. 2004. Lexrank: graph-based 
centrality as salience in text summarization. Journal 
of Artificial Intelligence Research (JAIR). 
Feng, D., Shaw, E., Kim, J., and Hovy, E.H. 2006. An 
intelligent discussion-bot for answering student que-
ries in threaded discussions. In Proceedings of Intel-
ligent User Interface (IUI-2006), pp. 171-177.  
Hearst, M.A. 1994. Multi-paragraph segmentation of 
expository text. In Proceedings of ACL-1994.  
Kleinberg, J. 1999. Authoritative sources in a hyper-
linked environment. Journal of the ACM, 46(5). 
Kurland, O. and Lee L. 2005. PageRank without hyper-
links: Structural re-ranking using links induced by 
language models. In Proceedings of SIGIR-2005.  
Levinson, S. 1983. Pragmatics. Cambridge Univ. Press. 
Mann, W.C. and Thompson, S.A. 1988. Rhetorical 
structure theory: towards a functional theory of text 
organization. Text, 8 (3), pp. 243-281. 
Marom, Y. and Zukerman, I. 2005. Corpus-based gen-
eration of easy help-desk responses. Technical Re-
port, Monash University. Available at: 
http://www.csse.monash.edu.au/publications/2005/tr-
2005-166-full.pdf. 
Mihalcea, R. 2004. Graph-based ranking algorithms for 
sentence extraction, applied to text summarization. In 
Companion Volume to ACL-2004. 
Mihalcea, R. 2005. unsupervised large-vocabulary word 
sense disambiguation with graph-based algorithms 
for sequence data labeling. In HLT/EMNLP 2005. 
Mihalcea, R. and Tarau, P. 2004. TextRank: bringing 
order into texts. In Proceedings of EMNLP 2004. 
Mihalcea, R., Tarau, P. and Figa, E. 2004. PageRank on 
semantic networks, with application to word sense 
disambiguation. In Proceedings of COLING 2004. 
Otterbacher, J., Erkan, G., and  Radev, D. 2005. Using 
random walks for question-focused sentence re-
trieval. In Proceedings of HLT/EMNLP 2005.  
Pang, B. and Lee, L. 2004. A sentimental education: 
sentiment analysis using subjectivity summarization 
based on minimum cuts. In ACL-2004. 
Salton, G. 1989. Automatic Text Processing, The Trans-
formation, Analysis, and Retrieval of Information by 
Computer. Addison-Wesley, Reading, MA, 1989. 
Searle, J. 1969. Speech Acts. Cambridge: Cambridge 
Univ. Press. 
Soricut, R. and Marcu, D. 2003. Sentence level dis-
course parsing using syntactic and lexical informa-
tion. In Proceedings of HLT/NAACL-2003. 
Sporleder, C. and Lapata, M. 2005. Discourse chunking 
and its application to sentence compression. In Pro-
ceedings of HLT/EMNLP 2005. 
Voorhees, E.M. 2001. Overview of the TREC 2001 
question answering track. In TREC 2001. 
Zhou, L. and Hovy, E.H. 2005. Digesting virtual ?geek
?culture: the summarization of technical internet re-
lay chats. In Proceedings of ACL 2005. 
215
A New Approach for English-Chinese Named Entity Alignment 
Donghui Feng? 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way, Suite 1001 
Marina Del Rey, CA, U.S.A, 90292 
donghui@isi.edu  
Yajuan Lv?                             Ming Zhou? 
?Microsoft Research Asia 
5F Sigma Center, No.49 Zhichun Road, Haidian 
Beijing, China, 100080 
{t-yjlv, mingzhou}@microsoft.com 
 
Abstract? 
Traditional word alignment approaches cannot 
come up with satisfactory results for Named 
Entities. In this paper, we propose a novel 
approach using a maximum entropy model for 
named entity alignment. To ease the training 
of the maximum entropy model, bootstrapping 
is used to help supervised learning. Unlike 
previous work reported in the literature, our 
work conducts bilingual Named Entity 
alignment without word segmentation for 
Chinese and its performance is much better 
than that with word segmentation. When 
compared with IBM and HMM alignment 
models, experimental results show that our 
approach outperforms IBM Model 4 and 
HMM significantly. 
1 Introduction 
This paper addresses the Named Entity (NE) 
alignment of a bilingual corpus, which means 
building an alignment between each source NE and 
its translation NE in the target language. Research 
has shown that Named Entities (NE) carry 
essential information in human language (Hobbs et 
al., 1996). Aligning bilingual Named Entities is an 
effective way to extract an NE translation list and 
translation templates. For example, in the 
following sentence pair, aligning the NEs, [Zhi 
Chun road] and [???] can produce a translation 
template correctly. 
? Can I get to [LN Zhi Chun road] by eight 
o?clock? 
? ????? [LN ???]?? 
In addition, NE alignment can be very useful for 
Statistical Machine Translation (SMT) and Cross-
Language Information Retrieval (CLIR). 
A Named Entity alignment, however, is not easy 
to obtain. It requires both Named Entity 
Recognition (NER) and alignment be handled 
correctly. NEs may not be well recognized, or only 
                                                     
? The work was done while the first author was 
visiting Microsoft Research Asia. 
parts of them may be recognized during NER. 
When aligning bilingual NEs in different 
languages, we need to handle many-to-many 
alignments. And the inconsistency of NE 
translation and NER in different languages is also a 
big problem. Specifically, in Chinese NE 
processing, since Chinese is not a tokenized 
language, previous work (Huang et al, 2003) 
normally conducts word segmentation and 
identifies Named Entities in turn. This involves 
several problems for Chinese NEs, such as word 
segmentation error, the identification of Chinese 
NE boundaries, and the mis-tagging of Chinese 
NEs. For example, ?????? in Chinese is really 
one unit and should not be segmented as [ON ??
?]/? . The errors from word segmentation and 
NER will propagate into NE alignment. 
In this paper, we propose a novel approach using 
a maximum entropy model to carry out English-
Chinese Named Entity1 alignment. NEs in English 
are first recognized by NER tools. We then 
investigate NE translation features to identify NEs 
in Chinese and determine the most probable 
alignment. To ease the training of the maximum 
entropy model, bootstrapping is used to help 
supervised learning. 
On the other hand, to avoid error propagations 
from word segmentation and NER, we directly 
extract Chinese NEs and make the alignment from 
plain text without word segmentation. It is unlike 
previous work reported in the literature. Although 
this makes the task more difficult, it greatly 
reduces the chance of errors introduced by 
previous steps and therefore produces much better 
performance on our task. 
To justify our approach, we adopt traditional 
alignment approaches, in particular IBM Model 4 
(Brown et al, 1993) and HMM (Vogel et al, 
1996), to carry out NE alignment as our baseline 
systems. Experimental results show that in this task 
our approach outperforms IBM Model 4 and HMM 
significantly. Furthermore, the performance 
                                                     
1 We only discuss NEs of three categories: Person 
Name (PN), Location Name (LN), and Organization 
Name (ON). 
without word segmentation is much better than that 
with word segmentation. 
The rest of this paper is organized as follows: In 
section 2, we discuss related work on NE 
alignment. Section 3 gives the overall framework 
of NE alignment with our maximum entropy 
model. Feature functions and bootstrapping 
procedures are also explained in this section. We 
show experimental results and compare them with 
baseline systems in Section 4. Section 5 concludes 
the paper and discusses ongoing future work. 
2 Related Work 
Translation knowledge can be acquired via word 
and phrase alignment. So far a lot of research has 
been conducted in the field of machine translation 
and knowledge acquisition, including both 
statistical approaches (Cherry and Lin, 2003; 
Probst and Brown, 2002; Wang et al, 2002; Och 
and Ney, 2000; Melamed, 2000; Vogel et al, 1996) 
and symbolic approaches (Huang and Choi, 2000; 
Ker and Chang, 1997). 
However, these approaches do not work well on 
the task of NE alignment. Traditional approaches 
following IBM Models (Brown et al, 1993) are not 
able to produce satisfactory results due to their 
inherent inability to handle many-to-many 
alignments. They only carry out the alignment 
between words and do not consider the case of 
complex phrases like some multi-word NEs. On 
the other hand, IBM Models allow at most one 
word in the source language to correspond to a 
word in the target language (Koehn et al, 2003; 
Marcu, 2001). Therefore they can not handle 
many-to-many word alignments within NEs well. 
Another well-known word alignment approach, 
HMM (Vogel et al, 1996), makes the alignment 
probabilities depend on the alignment position of 
the previous word. It does not explicitly consider 
many-to-many alignment either. 
Huang et al (2003) proposed to extract Named 
Entity translingual equivalences based on the 
minimization of a linearly combined multi-feature 
cost. But they require Named Entity Recognition 
on both the source side and the target side. 
Moore?s (2003) approach is based on a sequence of 
cost models. However, this approach greatly relies 
on linguistic information, such as a string repeated 
on both sides, and clues from capital letters that are 
not suitable for language pairs not belonging to the 
same family. Also, there are already complete 
lexical compounds identified on the target side, 
which represent a big part of the final results. 
During the alignment, Moore does not hypothesize 
that translations of phrases would require splitting 
predetermined lexical compounds on the target set. 
These methods are not suitable for our task, 
since we only have NEs identified on the source 
side, and there is no extra knowledge from the 
target side. Considering the inherent characteristics 
of NE translation, we can find several features that 
can help NE alignment; therefore, we use a 
maximum entropy model to integrate these features 
and carry out NE alignment. 
3 NE Alignment with a Maximum Entropy 
Model  
Without relying on syntactic knowledge from 
either the English side or the Chinese side, we find 
there are several valuable features that can be used 
for Named Entity alignment. Considering the 
advantages of the maximum entropy model 
(Berger et al, 1996) to integrate different kinds of 
features, we use this framework to handle our 
problem. 
Suppose the source English NE 
,ene },...,{ 21 ne eeene = consists of n English 
words and the candidate Chinese NE 
,cne },...,{ 21 mc cccne = is composed of m 
Chinese characters.  Suppose also that we have M 
feature functions .,...,1),,( Mmneneh ecm = For 
each feature function, we have a model parameter 
.,...,1, Mmm =? The alignment probability can 
be defined as follows (Och and Ney, 2002): 
? ?
?
=
=
=
=
'
1
]),(exp[
]),(exp[
)|()|(
1
'
1
c
M
ne
M
m
ecmm
M
m
ecmm
ecec
neneh
neneh
nenepneneP
?
?
?
(3.1) 
The decision rule to choose the most probable 
aligned target NE of the English NE is (Och and 
Ney, 2002): 
{ }
??
?
??
?
=
=
?
=
M
m
ecmm
ne
ec
ne
c
neneh
nenePen
c
c
1
),(maxarg
)|(maxarg?
?
   (3.2) 
In our approach, considering the characteristics 
of NE translation, we adopt 4 features: translation 
score, transliteration score, the source NE and 
target NE?s co-occurrence score, and distortion 
score for distinguishing identical NEs in the same 
sentence. Next, we discuss these four features in 
detail. 
3.1 Feature Functions 
3.1.1 Translation Score 
It is important to consider the translation 
probability between words in English NE and 
characters in Chinese NE. When processing 
Chinese sentence without segmentation, word here 
refers to single Chinese character. 
The translation score here is used to represent 
how close an NE pair is based on translation 
probabilities. Supposing the source English NE 
ene consists of n English words, 
}...,{ 21 ne eeene = and the candidate Chinese NE 
cne is composed of m Chinese 
characters, }...,{ 21 mc cccne = , we can get the 
translation score of these two bilingual NEs based 
on the translation probability between ei and cj: 
??
= =
=
m
j
n
i
ijce ecpneneS
1 1
)|(),(      (3.3) 
Given a parallel corpus aligned at the sentence 
level, we can achieve the translation probability 
between each English word and each Chinese 
character )|( ij ecp via word alignments with IBM 
Model 1 (Brown et al, 1993). Without word 
segmentation, we have to calculate every possible 
candidate to determine the most probable 
alignment, which will make the search space very 
large. Therefore, we conduct pruning upon the 
whole search space. If there is a score jump 
between two adjacent characters, the candidate will 
be discarded. The scores between the candidate 
Chinese NEs and the source English NE are 
calculated via this formula as the value of this 
feature. 
3.1.2 Transliteration Score 
Although in theory, translation scores can build 
up relations within correct NE alignments, in 
practice this is not always the case, due to the 
characteristics of the corpus. This is more obvious 
when we have sparse data. For example, most of 
the person names in Named Entities are sparsely 
distributed in the corpus and not repeated regularly. 
Besides that, some English NEs are translated via 
transliteration (Lee and Chang, 2003; Al-Onaizan 
and Knight, 2002; Knight and Graehl, 1997) 
instead of semantic translation. Therefore, it is 
fairly important to make transliteration models. 
Given an English Named Entity e, 
}...,{ 21 neeee = , the procedure of transliterating e 
into a Chinese Named Entity c, }...,{ 21 mcccc = , 
can be described with Formula (3.4) (For 
simplicity of denotation, we here use e and c to 
represent English NE and Chinese NE instead of 
ene and cne ). 
)|(maxarg ecPc
c
=
)        (3.4) 
According to Bayes? Rule, it can be transformed 
to: 
)|(*)(maxarg cePcPc
c
=
)    (3.5) 
Since there are more than 6k common-used 
Chinese characters, we need a very large training 
corpus to build the mapping directly between 
English words and Chinese characters. We adopt a 
romanization system, Chinese PinYin, to ease the 
transformation. Each Chinese character 
corresponds to a Chinese PinYin string. And the 
probability from a Chinese character to PinYin 
string is 1)|( ?crP , except for polyphonous 
characters. Thus we have: 
)|(*)|(*)(maxarg rePcrPcPc
c
=
)   (3.6) 
Our problem is: Given both English NE and 
candidate Chinese NEs, finding the most probable 
alignment, instead of finding the most probable 
Chinese translation of the English NE. Therefore 
unlike previous work (Lee and Chang, 2003; 
Huang et al, 2003) in English-Chinese 
transliteration models, we transform each 
candidate Chinese NE to Chinese PinYin strings 
and directly train a PinYin-based language model 
with a separate English-Chinese name list 
consisting of 1258 name pairs to decode the most 
probable PinYin string from English NE. 
To find the most probable PinYin string from 
English NE, we rewrite Formula (3.5) as the 
following: 
)|(*)(maxarg rePrPr
r
=
)      (3.7) 
where r represents the romanization (PinYin 
string), }...,{ 21 mrrrr = . For each of the factor, we 
have 
)|()|(
1
?
=
=
m
i
ii rePreP      (3.8) 
)|()|()()( 1
3
2121 ?
=
??= i
m
i
ii rrrPrrPrPrP   (3.9) 
where ie  is an English syllable and ir  is a 
Chinese PinYin substring. 
For example, we have English NE ?Richard? and 
its candidate Chinese NE ?????. Since both the 
channel model and language model are PinYin 
based, the result of Viterbi decoding is from ?Ri 
char d? to ?Li Cha De?. We transform ????? to 
the PinYin string ?Li Cha De?. Then we compare 
the similarity based on the PinYin string instead of 
with Chinese characters directly. This is because 
when transliterating English NEs into Chinese, it is 
very flexible to choose which character to simulate 
the pronunciation, but the PinYin string is 
relatively fixed. 
For every English word, there exist several ways 
to partition it into syllables, so here we adopt a 
dynamic programming algorithm to decode the 
English word into a Chinese PinYin sequence. 
Based on the transliteration string of the English 
NE and the PinYin string of the original candidate 
Chinese NE, we can calculate their similarity with 
the XDice coefficient (Brew and McKelvie, 1996). 
This is a variant of Dice coefficient which allows 
?extended bigrams?. An extended bigram (xbig) is 
formed by deleting the middle letter from any 
three-letter substring of the word in addition to the 
original bigrams. 
Suppose the transliteration string of the English 
NE and the PinYin string of the candidate Chinese 
NE are tle  and pyc , respectively. The XDice 
coefficient is calculated via the following formula: 
)()(
)()(2
),(
pytl
pytl
pytl
cxbigsexbigs
cxbigsexbigs
ceXDice
+
?
=
I   (3.10) 
Another point to note is that foreign person 
names and Chinese person names have different 
translation strategies. The transliteration 
framework above is only applied on foreign names. 
For Chinese person name translation, the surface 
English strings are exactly Chinese person names? 
PinYin strings. To deal with the two situations, let 
sure  denote the surface English string, the final 
transliteration score is defined by taking the 
maximum value of the two XDice coefficients: 
)),(),,(max(
),(
surpytlpy ecXDiceecXDice
ecTl =
  (3.11) 
This formula does not differentiate foreign 
person names and Chinese person names, and 
foreign person names? transliteration strings or 
Chinese person names? PinYin strings can be 
handled appropriately. Besides this, since the 
English string and the PinYin string share the same 
character set, our approach can also work as an 
alternative if the transliteration decoding fails. 
For example, for the English name ?Cuba?, the 
alignment to a Chinese NE should be ????. If 
the transliteration decoding fails, its PinYin string, 
?Guba?, still has a very strong relation with the 
surface string ?Cuba? via the XDice coefficient. 
This can make the system more powerful. 
3.1.3 Co-occurrence Score 
Another approach is to find the co-occurrences 
of source and target NEs in the whole corpus. If 
both NEs co-occur very often, there exists a big 
chance that they align to each other. The 
knowledge acquired from the whole corpus is an 
extra and valuable feature for NE alignment. We 
calculate the co-occurrence score of the source 
English NE and the candidate Chinese NE with the 
following formula: 
?= )(*,
),(
)|(
e
ec
ecco necount
nenecount
neneP     (3.12) 
where ),( ec nenecount  is the number of times 
cne  and ene  appear together and )(*, enecount  
is the number of times that ene  appears. This 
probability is a good indication for determining 
bilingual NE alignment. 
3.1.4 Distortion Score 
When translating NEs across languages, we 
notice that the difference of their positions is also a 
good indication for determining their relation, and 
this is a must when there are identical candidates in 
the target language. The bigger the difference is, 
the less probable they can be translations of each 
other. Therefore, we define the distortion score 
between the source English NE and the candidate 
Chinese NE as another feature. 
Suppose the index of the start position of the 
English NE is i, and the length of the English 
sentence is m. We then have the relative position of 
the source English NE
m
ipose = , and the 
candidate Chinese NE?s relative 
position ,cpos 1,0 ?? ce pospos . The distortion 
score is defined with the following formula: 
)(1),( ceec posposABSneneDist ??= (3.13) 
where ABS means the absolute value. If there 
are multiple identical candidate Chinese NEs at 
different positions in the target language, the one 
with the largest distortion score will win. 
3.2 Bootstrapping with the MaxEnt Model 
To apply the maximum entropy model for NE 
alignment, we process in two steps: selecting the 
NE candidates and training the maximum entropy 
model parameters. 
3.2.1 NE Candidate Selection 
To get an NE alignment with our maximum 
entropy model, we first use NLPWIN (Heidorn, 
2000) to identify Named Entities in English. For 
each word in the recognized NE, we find all the 
possible translation characters in Chinese through 
the translation table acquired from IBM Model 1. 
Finally, we have all the selected characters as the 
?seed? data. With an open-ended window for each 
seed, all the possible sequences located within the 
window are considered as possible candidates for 
NE alignment. Their lengths range from 1 to the 
empirically determined length of the window. 
During the candidate selection, the pruning 
strategy discussed above is applied to reduce the 
search space. 
For example, in Figure 1, if ?China? only has a 
translation probability over the threshold value 
with ???, the two seed data are located with the 
index of 0 and 4. Supposing the length of the 
window to be 3, all the candidates around the seed 
data including ????, with the length ranging 
from 1 to 3, are selected. 
 
 
 
 
 
 
Figure 1. Example of Seed Data 
3.2.2 MaxEnt Parameter Training 
With the four feature functions defined in 
Section 3.1, for each identified NE in English, we 
calculate the feature scores of all the selected 
Chinese NE candidates. 
To achieve the most probable aligned Chinese 
NE, we use the published package YASMET2 to 
conduct parameter training and re-ranking of all 
the NE candidates. YASMET requires supervised 
learning for the training of the maximum entropy 
model. However, it is not easy to acquire a large 
annotated training set. Here bootstrapping is used 
to help the process. Figure 2 gives the whole 
procedure for parameter training. 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2. Parameter Training 
4 Experimental Results 
4.1 Experimental Setup 
We perform experiments to investigate the 
performance of the above framework. We take the 
LDC Xinhua News with aligned English-Chinese 
sentence pairs as our corpus. 
The incremental testing strategy is to investigate 
the system?s performance as more and more data 
are added into the data set. Initially, we take 300 
                                                     
2 http://www.isi.edu/~och/YASMET.html 
sentences as the standard testing set, and we 
repeatedly add 5k more sentences into the data set 
and process the new data. After iterative re-ranking, 
the performance of alignment models over the 300 
sentence pairs is calculated. The learning curves 
are drawn from 5k through 30k sentences with the 
step as 5k every time. 
4.2 Baseline System 
A translated Chinese NE may appear at a 
different position from the corresponding English 
NE in the sentence. IBM Model 4 (Brown et al, 
1993) integrates a distortion probability, which is 
complete enough to account for this tendency. The 
HMM model (Vogel et al, 1996) conducts word 
alignment with a strong tendency to preserve 
localization from one language to another. 
Therefore we extract NE alignments based on the 
results of these two models as our baseline systems. 
For the alignments of IBM Model 4 and HMM, we 
use the published software package, GIZA++ 3 
(Och and Ney, 2003) for processing. 
Some recent research has proposed to extract 
phrase translations based on the results from IBM 
Model (Koehn et al, 2003). We extract English-
Chinese NE alignments based on the results from 
IBM Model 4 and HMM. The extraction strategy 
takes each of the continuous aligned segments as 
one possible candidate, and finally the one with the 
highest frequency in the whole corpus wins. 
 
 
 
 
 
 
 
 
Figure 3. Example of Extraction Strategy 
Figure 3 gives an example of the extraction 
strategy. ?China? here is aligned to either ???? 
or ???. Finally the one with a higher frequency in 
the whole corpus, say, ????, will be viewed as 
the final alignment for ?China?. 
4.3 Results Analysis 
Our approach first uses NLPWIN to conduct 
NER. Suppose S? is the set of identified NE with 
NLPWIN. S is the alignment set we compute with 
our models based on S?, and T is the set consisting 
of all the true alignments based on S?. We define 
the evaluation metrics of precision, recall, and F-
score as follows: 
                                                     
3 http://www.isi.edu/~och/GIZA++.html 
[China] hopes to further economic ? [EU]. 
 
 
? ? ? ? ? ? ? ? ? ?? 
1. Set the coefficients i? as uniform 
distribution; 
2. Calculate all the feature scores to get the 
N-best list of the Chinese NE candidates; 
3. Candidates with their values over a given 
threshold are considered to be correct and 
put into the re-ranking training set; 
4. Retrain the parameters i?  with YASMET;
5. Repeat from Step 2 until i?  converge, and 
take the current ranking as the final result. 
[China] hopes to further economic ? [EU]. 
 
 
? ? ? ? ? ? ? ? ? ?? 
Aligned Candidates:  China ??? 
                               China ?? 
S
TS
precision
I
=           (4.1) 
T
TS
recall
I
=           (4.2) 
recallprecision
recallprecisionscoreF
+
??
=?
2  (4.3) 
4.3.1 Results without Word Segmentation 
Based on the testing strategies discussed in 
Section 4.1, we perform all the experiments on 
data without word segmentation and get the 
performance for NE alignment with IBM Model 4, 
the HMM model, and the maximum entropy model. 
Figure 4, 5, and 6 give the learning curves for 
precision, recall, and F-score, respectively, with 
these experiments. 
Precision Without Word Segmentation
0
0.2
0.4
0.6
0.8
1
5k 10k 15k 20k 25k 30k data size
pr
ec
is
io
n IBM Model
HMM
MaxEnt
Upper Bound
 
Figure 4. Learning Curve with Precision 
Recall Without Word Segmentation
0
0.2
0.4
0.6
0.8
1
5k 10k 15k 20k 25k 30k data size
re
ca
ll IBM Model
HMM
MaxEnt
 
Figure 5. Learning Curve with Recall 
F-score Without Word Segmentation
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
5k 10k 15k 20k 25k 30k data size
F-
sc
or
e IBM Model
HMM
MaxEnt
 
Figure 6. Learning Curve with F-score 
From these curves, we see that HMM generally 
works a little better than IBM Model 4, both for 
precision and for recall. NE alignment with the 
maximum entropy model greatly outperforms IBM 
Model 4 and HMM in precision, recall, and F-
Score. Since with this framework, we first use 
NLPWIN to recognize NEs in English, we have 
NE identification error. The precision of NLPWIN 
on our task is about 77%. Taking this into account, 
we know our precision score has actually been 
reduced by this rate. In Figure 4, this causes the 
upper bound of precision to be 77%. 
4.3.2 Comparison with Results with Word 
Segmentation 
To justify that our approach of NE alignment 
without word segmentation really reduces the error 
propagations from word segmentation and 
thereafter NER, we also perform all the 
experiments upon the data set with word 
segmentation. The segmented data is directly taken 
from published LDC Xinhua News corpus. 
 
 precision recall F-score 
MaxEnt 
(Seg) 
0.56705 0.734491 0.64 
MaxEnt 
(Unseg) 
0.636015 0.823821 0.717838 
HMM 
(Seg) 
0.281955 0.372208 0.320856 
HMM 
(Unseg) 
0.291859 0.471464 0.360531 
IBM 4 
(Seg) 
0.223062 0.292804 0.253219 
IBM 4 
(Unseg) 
0.251185 0.394541 0.30695 
Table 1. Results Comparison 
Table 1 gives the comparison of precision, recall, 
and F-score for the experiments with word 
segmentation and without word segmentation 
when the size of the data set is 30k sentences. 
For HMM and IBM Model 4, performance 
without word segmentation is always better than 
with word segmentation. For maximum entropy 
model, the scores without word segmentation are 
always 6 to 9 percent better than those with word 
segmentation. This owes to the reduction of error 
propagation from word segmentation and NER. 
For example, in the following sentence pair with 
word segmentation, the English NE ?United 
States? can no longer be correctly aligned to ??
??. Since in the Chinese sentence, the incorrect 
segmentation takes ?????? as one unit. But if 
we conduct alignment without word segmentation, 
???? can be correctly aligned. 
? Greek Prime Minister Costas Simitis visits 
[United States] .  
? ?? ?? ?? ? ? ???? . 
Similar situations exist when HMM and IBM 
Model 4 are used for NE alignment. When 
compared with IBM Model 4 and HMM with word 
segmentation, our approach with word 
segmentation also has a much better performance 
than them. This demonstrates that in any case our 
approach outperforms IBM Model 4 and HMM 
significantly. 
4.3.3 Discussion 
Huang et al?s (2003) approach investigated 
transliteration cost and translation cost, based on 
IBM Model 1, and NE tagging cost by an NE 
identifier. In our approach, we do not have an NE 
tagging cost. We use a different type of translation 
and transliteration score, and add a distortion score 
that is important to distinguish identical NEs in the 
same sentence. 
Experimental results prove that in our approach 
the selected features that characterize NE 
translations from English to Chinese help much for 
NE alignment. The co-occurrence score uses the 
knowledge from the whole corpus to help NE 
alignment. And the transliteration score addresses 
the problem of data sparseness. For example, 
English person name ?Mostafizur Rahman? only 
appears once in the data set. But with the 
transliteration score, we get it aligned to the 
Chinese NE ?????????? correctly. 
Since in ME training we use iterative 
bootstrapping to help supervised learning, the 
training data is not completely clean and brings 
some errors into the final results. But it avoids the 
acquisition of large annotated training set and the 
performance is still much better than traditional 
alignment models. The performance is also 
impaired by the English NER tool. Another 
possible reason for alignment errors is the 
inconsistency of NE translation in English and 
Chinese. For example, usually only the last name 
of foreigners is translated into Chinese and the first 
name is ignored. This brings some trouble for the 
alignment of person names. 
5 Conclusions 
Traditional word alignment approaches cannot 
come up with satisfactory results for Named Entity 
alignment. In this paper, we propose a novel 
approach using a maximum entropy model for NE 
alignment. To ease the training of the MaxEnt 
model, bootstrapping is used to help supervised 
learning. Unlike previous work reported in the 
literature, our work conducts bilingual Named 
Entity alignment without word segmentation for 
Chinese, and its performance is much better than 
with word segmentation. When compared with 
IBM and HMM alignment models, experimental 
results show that our approach outperforms IBM 
Model 4 and HMM significantly. 
Due to the inconsistency of NE translation, some 
NE pairs can not be aligned correctly. We may 
need some manually-generated rules to fix this. We 
also notice that NER performance over the source 
language can be improved using bilingual 
knowledge. These problems will be investigated in 
the future. 
6 Acknowledgements 
Thanks to Hang Li, Changning Huang, Yunbo 
Cao, and John Chen for their valuable comments 
on this work. Also thank Kevin Knight for his 
checking of the English of this paper. Special 
thanks go to Eduard Hovy for his continuous 
support and encouragement while the first author 
was visiting MSRA. 
References  
Al-Onaizan, Y. and Knight, K. 2002. Translating 
Named Entities Using Monolingual and 
Bilingual Resources. ACL 2002, pp. 400-408. 
Philadelphia. 
Berger, A. L.; Della Pietra, S. A.; and Della Pietra, 
V. J. 1996. A Maximum Entropy Approach to 
Natural Language Processing. Computational 
Linguistics, vol. 22, no. 1, pp. 39-68. 
Brew, C. and McKelvie, D. 1996. Word-pair 
extraction for lexicography. The 2nd 
International Conference on New Methods in 
Language Processing, pp. 45?55. Ankara. 
Brown, P. F.; Della Pietra, S. A.; Della Pietra, V. J. 
;and Mercer, R. L. 1993. The Mathematics of 
Statistical Machine Translation: Parameter 
Estimation. Computational Linguistics, 
19(2):263-311. 
Cherry, C. and Lin, D. 2003. A Probability Model 
to Improve Word Alignment. ACL 2003. 
Sapporo, Japan. 
Darroch, J. N. and Ratcliff, D. 1972. Generalized 
Iterative Scaling for Log-linear Models. Annals 
of Mathematical Statistics, 43:1470-1480. 
Heidorn, G. 2000. Intelligent Writing Assistant. A 
Handbook of Natural Language Processing: 
Techniques and Applications for the Processing 
of Language as Text. Marcel Dekker. 
Hobbs, J. et al 1996. FASTUS: A Cascaded Finite-
State Transducer for Extracting Information 
from Natural Language Text, MIT Press. 
Cambridge, MA. 
Huang, F.; Vogel, S. and Waibel, A. 2003. 
Automatic Extraction of Named Entity 
Translingual Equivalence Based on Multi-
Feature Cost Minimization. ACL 2003 Workshop 
on Multilingual and Mixed-language NER. 
Sapporo, Japan. 
Huang, J. and Choi, K. 2000. Chinese-Korean 
Word Alignment Based on Linguistic 
Comparison. ACL-2000. Hongkong. 
Ker, S. J. and Chang, J. S. 1997. A Class-based 
Approach to Word Alignment. Computational 
Linguistics, 23(2):313-343. 
Knight, K. and Graehl, J. 1997. Machine 
Transliteration. ACL 1997, pp. 128-135. 
Koehn, P.; Och, F. J. and Marcu, D. 2003. 
Statistical Phrase-Based Translation. 
HLT/NAACL 2003. Edmonton, Canada. 
Lee, C. and Chang, J. S. 2003. Acquisition of 
English-Chinese Transliterated Word Pairs from 
Parallel-Aligned Texts, HLT-NAACL 2003 
Workshop on Data Driven MT, pp. 96-103. 
Marcu, D. 2001. Towards a Unified Approach to 
Memory- and Statistical-Based Machine 
Translation. ACL 2001, pp. 378-385. Toulouse, 
France. 
Melamed, I. D. 2000. Models of Translation 
Equivalence among Words. Computational 
Linguistics, 26(2): 221-249. 
Moore, R. C. 2003. Learning Translations of 
Named-Entity Phrases from Parallel Corpora. 
EACL-2003. Budapest, Hungary. 
Och, F. J. and Ney, H. 2003. A Systematic 
Comparison of Various Statistical Alignment 
Models, Computational Linguistics, volume 29, 
number 1, pp. 19-51. 
Och, F. J. and Ney, H. 2002. Discriminative 
Training and Maximum Entropy Models for 
Statistical Machine Translation. ACL 2002, pp. 
295-302. 
Och, F. J. and Ney, H. 2000. Improved Statistical 
Alignment Models. ACL 2000, pp: 440-447. 
Probst, K. and Brown, R. 2002. Using Similarity 
Scoring to Improve the Bilingual Dictionary for 
Word Alignment. ACL-2002, pp: 409-416. 
Vogel, S.; Ney, H. and Tillmann, C. 1996. HMM-
Based Word Alignment in Statistical Translation. 
COLING?96, pp. 836-841. 
Wang, W.; Zhou, M.; Huang, J. and Huang, C. 
2002. Structural Alignment using Bilingual 
Chunking. COLING-2002. 
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 120?121,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Adaptive Information Extraction for Complex Biomedical Tasks 
 
Donghui Feng            Gully Burns            Eduard Hovy 
Information Sciences Insitute 
University of Southern California 
Marina del Rey, CA, 90292 
{donghui, burns, hovy}@isi.edu 
Abstract 
Biomedical information extraction tasks are of-
ten more complex and contain uncertainty at 
each step during problem solving processes. We 
present an adaptive information extraction 
framework and demonstrate how to explore un-
certainty using feedback integration. 
1 Adaptive Information Extraction 
Biomedical information extraction (IE) tasks are 
often more complex and contain uncertainty at each 
step during problem solving processes.  
When in the first place the desired information is 
not easy to define and to annotate (even by humans), 
iterative IE cycles are to be expected. There might 
be gaps between the domain knowledge representa-
tion and computer processing ability. Domain 
knowledge might be hard to represent in a clear 
format easy for computers to process. Computer sci-
entists may need time to understand the inherent 
characteristics of domain problems so as to find ef-
fective approaches to solve them. All these issues 
mandate a more expressive IE process.  
In these situations, the traditional, straightfor-
ward, and one-pass problem-solving procedure, con-
sisting of definition-learning-testing, is no longer 
adequate for the solution.  
 
Figure 1. Adaptive information extraction. 
For more complex tasks requiring iterative cycles, 
an adaptive and extended IE framework has not yet 
been fully defined although variants have been ex-
plored. We describe an adaptive IE framework to 
characterize the activities involved in complex IE 
tasks. Figure 1 depicts the adaptive information ex-
traction framework.  
This procedure emphasizes one important adap-
tive step between the learning and application 
phases. If the IE result is not adequate, some adapta-
tions are required:  
Our study focuses on extracting tract-tracing ex-
periments (Swanson, 2004) from neuroscience arti-
cles. The goal of tract-tracing experiment is to chart 
the interconnectivity of the brain by injecting tracer 
chemicals into a region of the brain and then identi-
fying corresponding labeled regions where the tracer 
is transported to (Burns et al, 2007). Our work is 
performed in the context of NeuroScholar1, a project 
that aims to develop a Knowledge Base Manage-
ment System to benefit neuroscience research.  
We show how this new framework evolves to 
meet the demands of the more complex scenario of 
biomedical text mining. 
2 Feedback Integration 
This task requires finding the knowledge describing 
one or more experiments within an article as well as 
identifying desired fields within individual sen-
tences. Significant complexity arises from the pres-
ence of a variable number of records (experiments) 
in a single research article --- anywhere from one to 
many. 
 
Table 1. An example tract-tracing experiment. 
Table 1 provides an example of a tract-tracing ex-
periment. In this experiment, when the tracer was 
injected into the injection location ?the contralateral 
AVCN?, ?no labeled cells? was found in the label-
ing location ?the DCN?. 
For sentence level fields labeling, the perform-
ance of F1 score is around 0.79 (Feng et al, 2008). 
                                                          
1 http://www.neuroscholar.org/ 
120
We here show how the adaptive information extrac-
tion framework is applied to labeling individual sen-
tences. Please see Feng et al (2007) for the details 
of segmenting data records. 
2.1 Choosing Learning Approach via F1 
A natural way to label sentences is to obtain (by 
hand or learning) patterns characterizing each field 
(Feng et al, 2006; Ravichandran and Hovy, 2002). 
We tried to annotate field values for the biomedical 
data, but we found few intuitive clues that rich sur-
face text patterns could be learned with this corpus.  
This insight, Feedback F1, caused us to give up 
the idea of learning surface text patterns as usual, 
and switch to the Conditional Random Fields (CRF) 
(Lafferty et al, 2001) for labeling sentences instead. 
In contrast to fixed-order patterns, the CRF model 
provides a compact way to integrate different types 
of features for sequential labeling problems and can 
reach state-of-the-art level performance. 
2.2 Determining Knowledge Schema via F2 
In the first place, it is not clear what granularity of 
knowledge/information can be extracted from text 
and whether the knowledge representation is suitable 
for computer processing. We tried a series of ap-
proaches, using different levels of granularity and 
description, in order to obtain formulation suitable 
for IE. Figure 2 represents the evolution of the 
knowledge schema in our repeated activities.  
 
Figure 2. Knowledge schema evolution. 
 
Figure 3. System performance at stage 1 and 2. 
We initially started with the schema in the left-
most column but our pilot study showed that some 
fields, for example, ?label_type?, had too many 
variations in text description, making it very hard for 
CRF to learn clues about it. We then switched to the 
second schema but ended up seeing that the field 
?injectionSpread? needed more domain knowledge 
and was therefore not able to be learned by the sys-
tems. The last column is the final schema after those 
pilot studies. Figure 3 shows system performance 
(overall and the worst field) corresponding to the 
first and the second representation schemas. 
2.3 Exploring Features via F3 
To train CRF sentence labeling systems, it is vital to 
decide what features to use and how to prepare those 
features. Through the cycle of Feedback F3, we ex-
plored five categories of features and their combina-
tions to determine the best features for optimal 
system performance. Table 2 shows system per-
formance with different feature combinations.  
System Features Prec. Recall F_Score 
Baseline 0.4067 0.1761 0.2458 
Lexicon 0.5998 0.3734 0.4602 
Lexicon                   
+ Surface Words 
0.7663 0.7302 0.7478 
Lexicon                   
+ Surface Words     
+ Context Window 
0.7717 0.7279 0.7491 
Lexicon + Surface 
Words + Context 
Window + Window 
Words 
0.8076 0.7451 0.7751 
Lexicon + Surface 
Words + Context 
Window + Window 
Words + Depend-
ency Features  
0.7991 0.7828 0.7909 
Table 2. Precision, Recall, and F_Score for labeling. 
Please see Feng et al (2008) for the details of the 
sentence level extraction and feature preparation,  
3 Conclusions 
In this paper, we have shown an adaptive informa-
tion extraction framework for complex biomedical 
tasks. Using the iterative development cycle, we 
have been able to explore uncertainty at different 
levels using feedback integration.  
References  
Burns, G., Feng, D., and Hovy, E.H. 2007. Intelligent Approaches to 
Mining the Primary Research Literature: Techniques, Systems, and 
Examples. Book Chapter in Computational Intelligence in Bioinfor-
matics, Springer-Verlag, Germany. 
Feng, D., Burns, G., and Hovy, E.H. 2007. Extracting Data Records 
from Unstructured Biomedical Full Text. In Proc. of EMNLP 2007.  
Feng, D., Burns, G., Zhu, J., and Hovy, E.H. 2008. Towards Automated 
Semantic Analysis on Biomedical Research Articles. In Proc. of 
IJCNLP-2008. Poster Paper.  
Feng, D., Ravichandran, D., and Hovy, E.H. 2006. Mining and re-
ranking for answering biographical queries on the web. In Proc. of 
AAAI-2006. pp. 1283-1288. 
Lafferty, J., McCallum, A. and Pereira, F. 2001. Conditional random 
fields: probabilistic models for segmenting and labeling sequence 
data. In Proc. of ICML-2001. 
Ravichandran, D. and Hovy, E.H. 2002. Learning surface text patterns 
for a question answering system. In Proceedings of ACL-2002. 
Swanson, L.W. 2004. Brain maps: structure of the rat brain. 3rd edition, 
Elsevier Academic Press. 
121
Proceedings of the 2009 Workshop on the People?s Web Meets NLP, ACL-IJCNLP 2009, pages 51?56,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Acquiring High Quality Non-Expert Knowledge from                         
On-demand Workforce 
 
Donghui Feng           Sveva Besana          Remi Zajac  
AT&T Interactive Research 
Glendale, CA, 91203 
{dfeng, sbesana, rzajac}@attinteractive.com 
 
  
 
Abstract 
Being expensive and time consuming, human 
knowledge acquisition has consistently been a 
major bottleneck for solving real problems. In 
this paper, we present a practical framework 
for acquiring high quality non-expert knowl-
edge from on-demand workforce using Ama-
zon Mechanical Turk (MTurk). We show how 
to apply this framework to collect large-scale 
human knowledge on AOL query classifica-
tion in a fast and efficient fashion. Based on 
extensive experiments and analysis, we dem-
onstrate how to detect low-quality labels from 
massive data sets and their impact on collect-
ing high-quality knowledge. Our experimental 
findings also provide insight into the best 
practices on balancing cost and data quality for 
using MTurk. 
1 Introduction 
Human knowledge acquisition is critical for 
training intelligent systems to solve real prob-
lems, both for industry applications and aca-
demic research. For example, many machine 
learning and natural language processing tasks 
require non-trivial human labeled data for super-
vised learning-based approaches. Traditionally 
this has been collected from domain experts, 
which we refer to as expert knowledge. 
However, acquiring in-house expert knowl-
edge is usually very expensive, time consuming, 
and has consistently been a major bottleneck for 
many research problems. For example, tremen-
dous efforts have been put into creating TREC 
corpora (Voorhees, 2003).  
As a result, several research projects spon-
sored by NSF and DARPA aim to construct 
valuable data resources via human labeling; these 
are exemplified by PennTree Bank (Marcus et 
al., 1993), FrameNet (Baker et al, 1998), and 
OntoNotes (Hovy et al, 2006).  
In addition, there are projects such as Open 
Mind Common Sense (OMCS) (Stork, 1999; 
Singh et al, 2002), ISI LEARNER (Chklovski, 
2003), and the Fact Entry Tool by Cycorp (Be-
lasco et al, 2002) where knowledge is gathered 
from volunteers. 
One interesting approach followed by von 
Ahn and Dabbish (2004), applied to image label-
ing on the Web, is to collect valuable input from 
entertained labelers. Turning label acquisition 
into a computer game addresses tediousness, 
which is one of the main reasons that it is hard to 
gather large quantities of data from volunteers.     
More recently researchers have begun to ex-
plore approaches for acquiring human knowl-
edge from an on-demand workforce such as 
Amazon Mechanical Turk1. MTurk is a market-
place for jobs that require human intelligence. 
There has been an increase in demand for 
crowdsourcing prompted by both the academic 
community and industry needs. For instance, 
Microsoft/Powerset uses MTurk for search rele-
vance evaluation and other companies are lever-
aging turkers to clean their data sources. 
However, while it is cheap and fast to obtain 
large-scale non-expert labels using MTurk, it is 
still unclear how to leverage its capability more 
efficiently and economically to obtain sufficient 
useful and high-quality data for solving real 
problems.  
In this paper, we present a practical frame-
work for acquiring high quality non-expert 
knowledge using MTurk. As a case study we 
have applied this framework to obtain human 
classifications on AOL queries (determining 
whether a query might be a local search or not). 
Based on extensive experiments and analysis, we 
show how to detect bad labelers/labels from 
massive data sets and how to build high-quality 
labeling sets. Our experiments also provide in-
                                                 
1 Amazon Mechanical Turk:  http://www.mturk.com/ 
51
sight into the best practices for balancing cost 
and data quality when using MTurk. 
The remainder of this paper is organized as 
follows: In Section 2, we review related work 
using MTurk. We describe our methodology in 
Section 3 and in Section 4 we present our ex-
perimental results and further analysis. In Sec-
tion 5 we draw conclusions and discuss our plans 
for future work. 
2 Related Work 
It is either infeasible or very time and cost con-
suming to acquire in-house expert human knowl-
edge. To obtain valuable human knowledge (e.g., 
in the format of labeled data), many research 
projects in the natural language community have 
been funded to create large-scale corpora and 
knowledge bases, such as PenTreeBank (Marcus 
et al, 1993), FrameNet (Baker et al, 1998), 
PropBank (Palmer et al, 2005), and OntoNotes 
(Hovy et al, 2006). 
MTurk has been attracting much attention 
within several research areas since its release. Su 
et al (2007) use MTurk to collect large-scale 
review data. Kaisser and Lowe (2008) report 
their work on generating research collections of 
question-answering pairs using MTurk. Sorokin 
and Forsyth (2008) outsource image-labeling 
tasks to MTurk. Kittur et al (2008) use MTurk 
as the paradigm for user studies. In the natural 
language community Snow et al (2008) report 
their work on collecting linguistic annotation for 
a variety of natural language tasks including 
word sense disambiguation, word similarity, and 
textual entailment recognition. 
However, most of the reported work focuses 
on how to apply data collected from MTurk to 
their applications. In our work, we concentrate 
on presenting a practical framework for using 
MTurk by separating the process into a valida-
tion phase and a large-scale submission phase.  
By analyzing workers? behavior and their data 
quality, we investigate how to detect low-quality 
labels and their impact on collected human 
knowledge; in addition, during the validation 
step we study how to best use MTurk to balance 
payments and data quality. Although our work is 
based on the submission of a classification task, 
the framework and approaches can be adapted 
for other types of tasks. 
In the next section, we will discuss in more 
detail our practical framework for using MTurk. 
3 Methodology 
3.1 Amazon Mechanical Turk 
Amazon launched their MTurk service in 2005. 
This service was initially used for internal pro-
jects and eventually fulfilled the demand for us-
ing human intelligence to perform various tasks 
that computers currently cannot do or do very 
well.  
MTurk users naturally fall into two roles: a re-
quester and a turker. As a requester, you can de-
fine your Human Intelligent Tasks (HITs), de-
sign suitable templates, and submit your tasks to 
be completed by turkers. A turker may choose 
from HITs that she is eligible to work on and get 
paid after the requester approves her work. The 
work presented in this paper is mostly from the 
perspective of a requester. 
3.2 Key Issues 
While it is quite easy to start using MTurk, re-
questers have to confront the following: how can 
we obtain sufficient useful and high-quality data 
for solving real problems efficiently and eco-
nomically?  
In practice, there are three key issues to con-
sider when answering this question. 
Key Issues Description 
Data    
Quality 
Is the labeled data good enough for 
practical use? 
Cost What is the sweet spot for payment? 
Scale How efficiently can MTurk be used 
when handling large-scale data sets? 
Can the submitted job be done in a 
timely manner?  
Table 1. Key issues for using MTurk. 
Requesters want to obtain high-quality data on 
a large scale without overpaying turkers. Our 
proposed framework will address these key is-
sues.  
3.3 Approaches 
Since not all tasks collecting non-expert knowl-
edge share the same characteristics and suitable 
applications, there is not a one-size-fits-all solu-
tion as the best practice when using MTurk.  
In our approach, we divide the process into 
two phases:  
? Validation Phase.  
? Large-scale Submission Phase.  
The first phase gives us information used to 
determine if MTurk is a valid approach for a 
given problem and what the optimal parameters 
for high quality and a short turn-around time are. 
52
We have to determine the right cost for the task 
and the optimal number of labels. We empiri-
cally determine these parameters with an MTurk 
submission using a small amount of data. These 
optimal parameters are then used for the large-
scale submission phase.  
Most data labeling tasks require subjective 
judgments. One cannot expect labeling results 
from different labelers to always be the same. 
The degree of agreement among turkers varies 
depending on the complexity and ambiguity of 
individual tasks. Typically we need to obtain 
multiple labels for each HIT by assigning multi-
ple turkers to the same task. 
Researchers mainly use the following two 
quantitative measures to assess inter-agreement: 
observed agreement and kappa statistics.  
P(A) is the observed agreement among anno-
tators. It represents the portion where annotators 
produce identical labels. This is very natural and 
straightforward. However, people argue this may 
not necessarily reflect the exact degree of agree-
ment due to chance agreement.  
P(E)  is the hypothetical probability of chance 
agreement. In other words, P(E)  represents the 
degree of agreement if both annotators conduct 
annotations randomly (according to their own 
prior probability).  
We can also use the kappa coefficient as a 
quantitative measure of inter-person agreement. 
It is a commonly used measure to remove the 
effect of chance agreement. It was first intro-
duced in statistics (Cohen, 1960) and has been 
widely used in the language technology commu-
nity, especially for corpus-driven approaches 
(Carletta, 1996; Krippendorf, 1980). Kappa is 
defined with the following equation:  
kappa = P(A) ? P(E)
1? P(E)  
Generally it is viewed more robust than ob-
served agreement P(A)  because it removes 
chance agreement P(E) . 
DetectOutlier( P) 
    for each turker p ? P  
collect the label set L  from p 
for each label l ? L  
    /* compared with others? majority voting */ 
    compute its agreement with others 
compute P(A) p  (or kappa p ) 
    analyze the distribution of P(A) 
    return outlier turkers 
Figure 1. Outlier detection algorithm. 
We use these measures to automatically detect 
outlier turkers producing low-quality results. 
Figure 1 shows our algorithm for automatically 
detecting outlier turkers.  
4 Experiments 
Based on our proposed framework and ap-
proaches, as a case study we conducted experi-
ments on a classification task using MTurk.  
The classification task requires the turker to 
determine whether a web query is a local search 
or not. For example, is the user typing this query 
looking for a local business or not? The labeled 
data set can be used to train a query classifier for 
a web search system. 
This capability will make search systems able 
to distinguish local search queries from other 
types of queries and to apply specific search al-
gorithms and data resources to better serve users? 
information needs.  
For example, if a person types ?largest biomed 
company in San Diego? and the web search sys-
tems can recognize this query as a local search 
query, it will apply local search algorithms on 
listing data instead of or as well as generating a 
general web search request.  
4.1 Validation Phase 
We downloaded the publicly available AOL 
query log2 and used this as our corpus. We first 
scanned all queries with geographic locations 
(including states, cities, and neighborhoods) and 
then randomly selected a set of queries for our 
experiments. 
For the validation phase, 700 queries were 
first labeled in-house by domain experts and we 
refer to this set as expert labels. To obtain the 
optimal parameters including the desired number 
of labels and payment price, we designed our 
HITs and experiments in the following way:  
We put 10 queries into one HIT, requested 15 
labels for each query/HIT, and varied payment 
for each HIT in four separate runs. Our payments 
include $0.01, $0.02, $0.05, and $0.10 per HIT. 
The goal is to have HITs completed in a timely 
fashion and have them yield high-quality data.  
We submitted our HITs to MTurk in four dif-
ferent runs with the following prices: $0.01, 
$0.02, $0.03, and $0.10. According to our pre-
defined evaluation measures and our outlier de-
tection algorithm, we investigated how to obtain 
the optimal parameters. Figure 2. shows the task 
completion statistics for the four different runs. 
                                                 
2 AOL Log Data: http://www.gregsadetsky.com/aol-data/ 
53
 
Figure 2. Task completion statistics. 
As shown in Figure 2, with the increase of 
payments, the average hourly rate increases from 
$0.72 to $9.73 and the total turn-around time 
dramatically decreases from more than 47 hours 
to about 1.5 hours. In the meantime, people tend 
to become more focused on the tasks and spend 
less time per HIT. 
In addition, as we increase payment, more 
people tend to stay with the task and take it more 
seriously as evidenced by the quality of the la-
beled data. This results in fewer numbers of 
workers overall as well as fewer outliers as 
shown in Figure 3.  
 
Figure 3. Total number of workers and outliers. 
We investigate two types of agreements, inter-
turker agreement and agreement between turkers 
and our in-house experts. For inter non-expert 
agreements, we compute each turker?s agreement 
with all others? majority voting results.  
Payment 
(USD) 0.01 0.02 0.05 0.10 
Median of 
inter-
turker 
agreement 0.8074 0.8583 0.9346 0.9028 
Table 2. Median of inter-turker agreements. 
As in our outlier detection algorithm, we ana-
lyzed the distribution of inter-turker agreements. 
Table 2 shows the median values of inter-turker 
agreement as we vary the payment prices. The 
median value keeps on increasing when the price 
increases from $0.01, to $0.02 and $0.05. How-
ever, it drops as the price increases from $0.05 to 
$0.10. This implies that turkers do not necessar-
ily improve their work quality as they get paid 
more. One of the possible explanations for this 
phenomenon is that when the reward is high 
people tend to work towards completing the task 
as fast as possible instead of focusing on submit-
ting high-quality data. This trend may be intrin-
sic to the task we have submitted and further ex-
periments will show if this turker behavior is 
task-independent.    
 
Figure 4. Agreement with experts. 
 
Figure 5. Inter non-expert agreement. 
We also analyzed agreement between non-
experts and experts. Figure 4 depicts the trend of 
the agreement scores with the increase of number 
of labels and payments. For example, given 
seven labels per query, in the experiment with 
the $0.05 payment, the majority voting of non-
expert labels has an agreement of 0.9465 with 
expert labeling. As explained earlier we do not 
necessarily obtain the best data qual-
ity/agreement with the $0.10 payment. Instead, 
we get the highest agreement with the $0.05 
payment. We have determined this rate to be the 
54
sweet spot in terms of cost. Also, seven labels 
per query produce a very high agreement with no 
further significant improvement when we in-
crease the number of labels.  
For inter non-expert agreements, we found 
similar trends in terms of different payments and 
number of labels as shown in Figure 5. 
As mentioned above, our algorithm is able to 
detect turkers producing low-quality data. One 
natural question is: how will their labels affect 
the overall data quality? 
We studied this problem in two different 
ways. We evaluated the data quality by removing 
either all polluted queries or only outliers? labels. 
Here polluted queries refer to those queries re-
ceiving at least one label from outliers. By re-
moving polluted queries, we only investigate the 
clean data set without any outlier labels. The 
other alternative is to only remove outliers? la-
bels for specific queries but others? labels for 
those queries will be kept. Both the agreement 
between experts and non-experts and inter-non-
experts agreement show similar trends: data 
quality without outliers? labels is slightly better 
since there is less noise. However, as outliers? 
labels may span a large number of queries, it 
may not be feasible to remove all polluted que-
ries. For example, in one of our experiments, 
outliers? labels pollute more than half of all the 
records. We cannot simply remove all the queries 
with outliers? labels due to consideration of cost.  
On the other hand, the effect of outliers? labels 
is not that significant if a certain number of re-
quested labels per query are collected. As shown 
in Figure 6, noisy data from outliers can be over-
ridden by assigning more labelers. 
 
Figure 6. Agreement with Experts (removing 
outliers? labels (payment = $0.05)).  
From the validation phase of the query classi-
fication task, we determine that the optimal pa-
rameters are paying $0.05 per HIT and request-
ing seven labels per query. Given this number of 
labels, the effect of outliers? labels can be over-
ridden for the final result.  
4.2 Large-scale Submission Phase 
Having obtained the optimal parameters from the 
validation phase, we are then ready to make a 
large-scale submission.  
For this phase, we paid $0.05 per HIT and re-
quested seven labels per query/HIT. Following 
similar filtering and sampling approaches as in 
the validation phase, we selected 22.5k queries 
from the AOL search log. Table 3 shows the de-
tected outliers for this large-scale submission.  
Total Number of Turkers 228 
Number of Outlier Turkers 23 
Outlier Ratio 10.09% 
Table 3. Number of turkers and outliers. 
Based on the distribution of inter-turker 
agreement, any turkers with agreement less than 
0.6501 are recognized as outliers. For a total 
number of 15,750 HITs, 228 turkers contributed 
to the labeling effort and 10.09% of them were 
recognized as outliers.  
Table 4 shows the number of labels from the 
outliers and the approval ratio of collected data. 
About 10.08% of labels are from outlier turkers 
and rejected.  
Total Number of Labels 157,500 
Number of Outlier Labels 15,870 
Approval Ratio 89.92% 
Table 4. Total number of labels. 
We have experimented using MTurk for a web 
query classification task. With learned optimal 
parameters from the validation phase, we col-
lected large-scale high-quality non-expert labels 
in a fast and economical way. These data will be 
used to train query classifiers to enhance web 
search systems handling local search queries. 
5 Conclusions and Future Work 
In this paper, we presented a practical framework 
for acquiring high quality non-expert knowledge 
from an on-demand and scalable workforce. Us-
ing Amazon Mechanical Turk, we collected 
large-scale human classification knowledge on 
web search queries.  
To learn the best practices when using MTurk, 
we presented a two-phase approach, a validation 
phase and a large-scale submission phase. We 
conducted extensive experiments to obtain the 
optimal parameters on the number of labelers 
and payments in the validation phase. We also 
presented an algorithm to automatically detect 
55
outlier turkers based on the agreement analysis 
and investigated the effect of removing an inac-
curately labeled set.  
Acquiring high-quality human knowledge will 
remain a major concern and a bottleneck for in-
dustry applications and academic problems. Un-
like traditional ways of collecting in-house hu-
man knowledge, MTurk provides an alternative 
way to acquire non-expert knowledge. As shown 
in our experiments, given appropriate quality 
control, we have been able to acquire high-
quality data in a very fast and efficient way. We 
believe MTurk will attract more attention and 
usage in broader areas. 
In the future, we are planning to investigate 
how this framework can be applied to different 
types of human knowledge acquisition tasks and 
how to leverage large-scale labeled data sets for 
solving natural language processing problems.  
References  
Baker, C.F., Fillmore, C.J., and Lowe, J.B. 1998. The 
Berkeley FrameNet Project. In Proc. of COLING-
ACL-1998.  
Belasco, A., Curtis, J., Kahlert, R., Klein, C., Mayans, 
C., and Reagan, P. 2002. Representing Knowledge 
Gaps Effectively. In Practical Aspects of Knowl-
edge Management, (PAKM).  
Carletta, J. 1996. Assessing agreement on classifica-
tion tasks: the kappa statistic. Computational Lin-
guistics. 22(2):249?254. 
Chklovski, T. 2003. LEARNER: A System for Ac-
quiring Commonsense Knowledge by Analogy. In 
Proc. of Second International Conference on 
Knowledge Capture (KCAP 2003).  
Cohen, J. 1960. A coefficient of agreement for nomi-
nal scales. Educational and Psychological Meas-
urement. Vol.20, No.1, pp.37-46.  
Colowick, S.M. and Pool, J. 2007. Disambiguating for 
the web: a test of two methods. In Proc. of the 4th 
international Conference on Knowledge Capture 
(K-CAP 2007).  
Hovy, E., Marcus, M., Palmer, M., Ramshaw, L., and 
Weischedel, R. 2006. OntoNotes: The 90% Solu-
tion. In Proc. of HLT-NAACL-2006.  
Kaisser, M. and Lowe, J.B. 2008. Creating a Research 
Collection of Question Answer Sentence Pairs with 
Amazon's Mechanical Turk. In Proc. of the Fifth 
International Conference on Language Resources 
and Evaluation (LREC-2008).  
Kittur, A., Chi, E. H., and Suh, B. 2008. Crowdsourc-
ing user studies with Mechanical Turk. In Proc. of 
the 26th Annual ACM Conference on Human Fac-
tors in Computing Systems (CHI-2008). 
Krippendorf, K. 1980. Content Analysis: An introduc-
tion to its methodology. Sage Publications.  
Marcus, M., Marcinkiewicz, M.A., and Santorini, B. 
1993. Building a large annotated corpus of English: 
the Penn Treebank. Computational Linguistics. 
19:2, June 1993.  
Nakov, P. 2008. Paraphrasing Verbs for Noun Com-
pound Interpretation. In Proc. of the Workshop on 
Multiword Expressions (MWE-2008).  
Palmer, M., Gildea, D., and Kingsbury, P. 2005. The 
Proposition Bank: A Corpus Annotated with Se-
mantic Roles. Computational Linguistics. 31:1.  
Sheng, V.S., Provost, F., and Ipeirotis, P.G. 2008. Get 
another label? improving data quality and data 
mining using multiple, noisy labelers. In Proc. of 
the 14th ACM SIGKDD international Conference 
on Knowledge Discovery and Data Mining (KDD-
2008).  
Singh, P., Lin, T., Mueller, E., Lim, G., Perkins, T., 
and Zhu, W. 2002. Open Mind Common Sense: 
Knowledge acquisition from the general public. In 
Meersman, R. and Tari, Z. (Eds.), LNCS: Vol. 
2519. On the Move to Meaningful Internet Sys-
tems: DOA/CoopIS/ODBASE (pp. 1223-1237). 
Springer-Verlag.  
Snow, R., O?Connor, B., Jurafsky, D., and Ng, A.Y. 
2008. Cheap and Fast ? But is it Good? Evaluating 
Non-Expert Annotations for Natural Language 
Tasks . In Proc. of EMNLP-2008.  
Sorokin, A. and Forsyth, D. 2008. Utility data annota-
tion with Amazon Mechanical Turk. In Proc. of the 
First IEEE Workshop on Internet Vision at CVPR-
2008.  
Stork, D.G. 1999. The Open Mind Initiative. IEEE 
Expert Systems and Their Applications. pp. 16-20, 
May/June 1999.  
Su, Q., Pavlov, D., Chow, J., and Baker, W.C. 2007. 
Internet-scale collection of human-reviewed data. 
In Proc. of the 16th international Conference on 
World Wide Web (WWW-2007).  
Von Ahn, L. and Dabbish, L. 2004. Labeling Images 
with a Computer Game. In Proc. of ACM Confer-
ence on Human Factors in Computing Systmes 
(CHI). pp. 319-326. 
Voorhees, E.M. 2003. Overview of TREC 2003. In 
Proc. of TREC-2003.  
56
