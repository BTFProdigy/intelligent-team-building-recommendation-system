Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 196?205,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Clarification Potential of Instructions
Luciana Benotti
TALARIS Team - LORIA (Universite? Henri Poincare?, INRIA)
BP 239, 54506 Vandoeuvre-le`s-Nancy, France
Luciana.Benotti@loria.fr
Abstract
Our hypothesis is that conversational im-
plicatures are a rich source of clarification
questions. In this paper we do two things.
First, we motivate the hypothesis in theo-
retical, practical and empirical terms. Sec-
ond, we present a framework for generat-
ing the clarification potential of an instruc-
tion by inferring its conversational impli-
catures with respect to a particular con-
text. General means-ends inference, be-
yond classical planning, turns out to be
crucial.
1 Introduction
Practical interest in clarification requests (CRs)
no longer needs to be awakened in dialogue
system designers (Gabsdil, 2003; Purver, 2004;
Rodr??guez and Schlangen, 2004; Rieser and
Moore, 2005; Skantze, 2007). In sociolinguistics
and discourse analysis, repair has been an even
more favored theme for almost three decades now;
see (Schegloff, 1987) as a representative example.
However, the theoretical scope of the phenomena
and its implications for a theory of meaning are
still being delineated. Recently, it has been pro-
posed that clarification should be a basic compo-
nent in an adequate theory of meaning:
The basic criterion for adequacy of a theory of
meaning is the ability to characterize for any ut-
terance type the update that emerges in the after-
math of successful mutual understanding and the
full range of possible clarification requests other-
wise ? this is the early 21st century analogue of
truth conditions. (Ginzburg, 2009, p.4)
In this view, repairs are not a necessary evil but
an intrinsic mechanism of language. In fact, inter-
preting an utterance centrally involves characteriz-
ing the space of possible requests of clarification
of the utterance, that is its clarification potential.
We believe that Ginzburg?s comment points in the
right direction; we discuss the motivations from
a theoretical perspective in Section 2.1. In Sec-
tion 2.2 we review a state-of-the-art definition of
the notion of clarification from the perspective of
dialogue system designers. This review makes ev-
ident the necessity of further refining the notion
of clarification if it is going to play such a cen-
tral role in a theory of meaning. In Section 2.3 we
present our findings in the corpus SCARE (Stoia et
al., 2008) which empirically motivates our work.
We believe that it is crucial to redefine the no-
tion of clarification in functional terms. Because
we know that the task is difficult, we restrict our-
selves to one utterance type, instructions, and to
a particular interaction level, the task-level. In the
rest of the paper (Sections 3 and 4), we present
a framework that generates the task-level clarifi-
cation potential of an instruction by inferring its
particularized conversational implicatures.
The following exchange illustrate the kinds of
interactions our framework models:
(1) A(1): Turn it on.
B(2): By pushing the red button?
(Rodr??guez and Schlangen, 2004, p.102)
Roughly speaking, our framework takes as in-
put sentences like A(1) and explains how B(2)
can be generated. In particular, the framework in-
dicates what kinds of information resources and
what kind of inferences are involved in the process
of generating utterances like B(2). In other words,
the goal of the framework is to explain why A(1)
and B(2) constitute a coherent dialogue by saying
how B(2) is relevant to A(1).
196
2 Background and motivation
In this section, we motivate our framework from
the theoretical perspective of pragmaticists inter-
ested in the relevance of clarifications for a theory
of meaning, from the practical perspective of di-
alogue system designers, and from the empirical
perspective of a human-human corpus that pro-
vides evidence for the necessity of such a frame-
work.
2.1 Theoretical: Relevance of clarifications
Modeling how listeners draw inferences from
what they hear, is a basic problem for theories
of understanding natural language. An important
part of the information conveyed is inferred in con-
text, given the nature of conversation as a goal-
oriented enterprise; as illustrated by the following
classical example by Grice:
(2) A: I am out of petrol.
B: There is a garage around the corner.
; B thinks that the garage is open.
(Grice, 1975, p.311)
B?s answer conversationally implicates (;) in-
formation that is relevant to A. In Grice?s terms, B
made a relevance implicature: he would be flout-
ing the conversational maxim of relevance unless
he believes that it?s possible that the garage is
open. A conversational implicature (CI) is dif-
ferent from an entailment in that it is cancelable
without contradiction. B can append material that
is inconsistent with the CI ? ?but I don?t know
whether it?s open?. Since the CI can be canceled,
B knows that it does not necessarily hold and then
both B or A are able to reinforce or clarify it with-
out repetition.
It is often controversial whether something is
actually a CI or not (people have different intu-
itions, which is not surprising given that people
have different background assumptions). In dia-
logue, CRs provide good evidence of the impli-
catures that have been made simply because they
make implicatures explicit. Take for instance the
clarification request which can naturally follow
Grice?s example.
(3) A: and you think it?s open?
B will have to answer and support the impli-
cature (for instance with ?yes, it?s open till mid-
night?) if he wants to get it added to the common
ground; otherwise, if he didn?t mean it, he can well
reject it without contradiction with ?well, you have
a point there, they might have closed?.
Our hypothesis is that CIs are a rich source of
clarification requests. And our method for gener-
ating the potential CRs of an utterance will be then
to infer (some of) the CIs of that utterance with re-
spect to a particular context.
2.2 Practical: Kinds of clarifications
Giving a precise definition of a clarification re-
quest is more difficult than might be thought at
first sight. Rodr??guez and Schlangen (2004) rec-
ognize this problem by saying:
Where we cannot report reliability yet is for the
task of identifying CRs in the first place. This is
not a trivial problem, which we will address in fu-
ture work. As far as we can see, Purver, Ginzburg
and Healey have not tested for reliability for doing
this task either. (Rodr??guez and Schlangen, 2004,
p.107)
One of the most developed classifications of
CRs is the one presented in (Purver, 2004). How-
ever, Purver?s classification relies mainly on the
surface form of the CRs. The attempts found in the
literature to give a classification of CRs accord-
ing to their functions (Rodr??guez and Schlangen,
2004; Rieser and Moore, 2005) are based on the
four-level model of communication independently
developed by Clark (1996) and Allwood (1995).
The model is summarized in Figure 1 (from the
point of view of the hearer).
Level Clark Allwood
4 consideration reaction
3 understanding understanding
2 identification perception
1 attention contact
Figure 1: The four levels of communication
Most of the previous work on clarifications has
concentrated on levels 1 to 3 of communication.
For instance, Schlangen (2004) proposed a fined-
grained classification of CRs but only for level
3. Gabsdil (2003) proposes a test for identifying
CRs. The test says that CRs cannot be preceded
by explicit acknowledgements. But in the follow-
ing example, presented by Gabsdil himself, the CR
uttered by F can well start with an explicit ?ok?.
197
(4) G: I want you to go up the left hand side of it
towards the green bay and make it a slightly
diagonal line, towards, sloping to the right.
F: So you want me to go above the carpen-
ter? (Gabsdil, 2003, p.30)
The kind of CR showed in 4, also called clarifi-
cation of intentions or task level clarifications,
are in fact very frequent in dialogue; they have
been reported to be the second or third most com-
mon kind of CR (the most common being ref-
erence resolution). (Rodr??guez and Schlangen,
2004) reports that 22% of the CRs found by them
in a German task-oriented spoken dialogue be-
longed to level 4, while (Rieser and Moore, 2005)
reports 8% (a high percentage considering that the
channel quality was poor and caused a 31% of
acoustic problems).
Fourth level CRs are not only frequent but
there are studies that show that the hearer in fact
prefers them. That is, if the dialogue shows a
higher amount of task related clarifications (in-
stead of, conventional CRs such as ?what??) hear-
ers qualitative evaluate the task as more success-
ful (Skantze, 2007). (Gabsdil, 2003) and (Rieser
and Moore, 2005) also agree that for task-oriented
dialogues the hearer should present a task-level re-
formulation to be confirmed rather than asking for
repetition, thereby showing his subjective under-
standing to the other dialogue participants. Gabs-
dil briefly suggests a step in this direction:
Task-level reformulations might benefit from sys-
tems that have access to effects of action opera-
tors or other ways to compute task-level implica-
tions. (Gabsdil, 2003, p.29 and p.34)
In the rest of the paper we propose a framework
that formalizes how to compute task-level impli-
catures and that suggests a finer-grained classifi-
cation for CRs in level 4. But first, in Section 2.3
we present empirical findings that motivate such a
framework.
2.3 Empirical: The SCARE corpus
The SCARE corpus (Stoia et al, 2008) consists
of fifteen English spontaneous dialogues situated
in an instruction giving task1. It was collected
using the Quake environment, a first-person vir-
tual reality game. The task consists of a direction
giver (DG) instructing a direction follower (DF)
1The corpus is freely available for research in
http://slate.cse.ohio-state.edu/quake-corpora/scare/
on how to complete several tasks in a simulated
game world. The corpus contains the collected au-
dio and video, as well as word-aligned transcrip-
tions.
The DF had no prior knowledge of the world
map or tasks and relied on his partner, the DG, to
guide him on completing the tasks. The DG had
a map of the world and a list of tasks to complete
(detailed in Appendix A.3). The partners spoke
to each other through headset microphones; they
could not see each other. As the participants col-
laborated on the tasks, the DG had instant feed-
back of the DF?s location in the simulated world,
because the game engine displayed the DF?s first
person view of the world on both the DG?s and
DF?s computer monitors.
We analyzed the 15 transcripts that constitute
the SCARE corpus while watching the associated
videos to get familiarized with the experiment and
evaluate its suitability for our purposes. Then, we
randomly selected one dialogue; its transcript con-
tains 449 turns and its video lasts 9 minutes and 12
seconds. Finally, we classified the clarification re-
quests according to the levels of communication
(see Figure 1). We found 29 clarification requests;
so 6.5% of the turns are CRs. From these 29 CRs,
65% belong to the level 4 of Table 1, and 31% be-
longed to level 3 (most of them related to reference
resolution). Only 4% of the CRs were acoustic
(level 2) since the channel used was very reliable.
In fact we only found one CR of the form
?what?? and it was a signal of incredulity of the
effect of an action as can be seen below:
DG(1): and then cabinet should open
DF(2): did it
DF(3): nothing in it
DG(4): what?
DG(5): There should be a silencer there
Interestingly, the ?what?? form of CR was re-
ported as the most frequently found in ?ordinary?
dialogue in (Purver et al, 2003). This is not the
case in the SCARE corpus. Furthermore, ?what??
is usually assumed to be a CR that indicates a low
level of coordination and is frequently classified as
belonging to level 1 or 2. However, this is not the
case in our example in which the CR is evidently
related to the task structure and thus belongs to
level 4. This is an example of why surface form is
not reliable when classifying CRs.
198
2.4 Preliminary conclusions
In this preliminary study, the SCARE corpus
seems to present more CRs than the corpus ana-
lyzed by previous work (which reports that 4% of
the dialogue turns are CR). Furthermore, in dis-
tinction to results reported in Ginzburg (2009),
most CRs occur at level 4. We believe this is nat-
urally explained in politeness theory (Brown and
Levinson, 1987).
The participants were punished if they per-
formed steps of the task that they were not sup-
posed to (see the instructions in Appendix A.1).
This punishment might take precedence over the
dispreference for CRs that is universal in dialogue
due to politeness. CRs are perceived as a form of
disagreement which is universally dispreferred ac-
cording to politeness theory. The pairs of partici-
pants selected were friends so the level of intimacy
among them was high, lowering the need of polite-
ness strategies; a behavior that is also predicted
by politeness theory. Finally, the participants re-
ceived a set of instructions before the task started
(see Appendix A) that includes information on the
available actions in the simulated world and their
expected effects. The participants make heavy use
of this to produce high level clarification requests,
instead of just signaling misunderstanding.
From these observations we draw the prelim-
inary conclusion that clarification strategies de-
pend on the information that is available to the
dialogue participants (crucially including the in-
formation available before the dialogue starts) and
on the constraints imposed on the interaction, such
as politeness constraints. In Section 3 we describe
the four information resources of our framework
whose content depends on the information avail-
able to the dialogue participants. In Section 4 we
introduce the reasoning tasks that use the informa-
tion resources to infer the clarification potential of
instructions. The study of the interaction between
politeness constraints and clarification strategies
seems promising, and we plan to address it in fu-
ture work.
3 The information resources
The inference framework uses four information re-
sources whose content depends on the information
available to the dialogue participants. We describe
each of them in turn and we illustrate their content
using the SCARE experimental setup.
3.1 The world model
Since the kind of utterance that the framework
handles are instructions that are supposed to be
executed in a simulated world, the first required
information resource is a model of this world. The
world model is a knowledge base that represents
the physical state of the simulated world. This
knowledge base has complete and accurate infor-
mation about the world that is relevant for com-
pleting the task at hand. It specifies properties of
particular individuals (for example, an individual
can be a button or a cabinet). Relationships be-
tween individuals are also represented here (such
as the relationship between an object and its loca-
tion). Such a knowledge base can be thought as a
first-order model.
The content of the world model for the SCARE
setup is a representation of the factual information
provided to the DG before the experiment started,
namely, a relational model of the map he received
(see Figure 3 in Appendix A.3). Crucially, such
a model contains all the functions associated with
the buttons in the world and the contents of the
cabinets (which are indicated on the map).
3.2 The dialogue model
Usually, this knowledge base starts empty; it is as-
sumed to represent what the DF knows about the
world. The information learned, either through
the contributions made during the dialogue or by
navigating the simulated world, are incrementally
added to this knowledge base. The knowledge is
also represented as a relational model and in fact
this knowledge base will usually (but not neces-
sarily) be a submodel of the world model.
The DF initial instructions in the SCARE setup
include almost no factual information (as you
can verify looking at his instructions in Ap-
pendix A.2). The only factual information that
he received were pictures of some objects in the
world so that he is able to recognize them. Such
information is relevant mainly for referent resolu-
tion and this is not the focus of the current paper.
Therefore, for our purposes we can assume that the
dialogue model of the SCARE experiment starts
empty.
3.3 The world actions
Crucially, the framework also includes the defi-
nitions of the actions that can be executed in the
world (such as the actions take or open). Each ac-
199
tion is specified as a STRIPS-like operator (Fikes
et al, 1972) detailing its arguments, preconditions
and effects. The preconditions indicate the condi-
tions that the world scenario must satisfy so that
the action can be executed; the effects determine
how the action changes the world when it is exe-
cuted. These actions specify complete and accu-
rate information about how the world behaves and
together with the world model is assumed to rep-
resent what the DG knows about the world.
The SCARE world action database will contain
a representation of the specification of the quake
controls (see Appendix A.1) received by both par-
ticipants and the extra action information that the
DG received. First, he received a specification of
the action hide that was not received by the DF.
Second, if the DG read the instructions carefully,
he knows that pressing a button can also cause
things to move. The representation of this last ac-
tion schema is shown in Appendix A.3.1.
3.4 The potential actions
The potential actions include representation of ac-
tions that the DF learned from the instructions he
received before beginning the task. This includes
the quake controls (see Appendix A.1) and also
the action knowledge that he acquired during his
learning phase (see appendix A.2). In the learning
phase the direction follower learned that the effect
of pressing a button can open a cabinet (if it was
closed) or close it (if it was opened). Such knowl-
edge is represented as a STRIPS-like operator like
one showed in Appendix A.2.1.
3.5 Preliminary conclusions
An action language like PDDL (Gerevini and
Long, 2005) can be used to specify the two action
databases introduced above (in fact, the STRIPS
fragment is enough). PDDL is the official lan-
guage of the International Conference on Auto-
mated Planning and Scheduling since 1998. This
means that most off-the-shelf planners that are
available nowadays support this language, such as
FF (Hoffmann and Nebel, 2001) and SGPlan (Hsu
et al, 2006).
As we said in the previous section, the world
model and the dialogue model are just relational
structures like the one showed in Figure 3 (in the
appendix). These relational structures can be di-
rectly expressed as a set of literals which is the
format used to specify the initial state of a plan-
ning problem.
The information resources then constitute al-
most everything that is needed in order to specify a
complete planning problem, as expected by cur-
rent planners, the only element that the framework
is missing is the goal. With a set of action schemas
(i.e. action operators), an initial state and a goal as
input, a planner is able to return a sequence of ac-
tions (i.e. a plan) that, when executed in the initial
state, achieves the goal.
Planning is a means-end inference task, a
kind of practical inference as defined by Kenny
in (Kenny, 1966); and is a very popular inference
task indeed as evidenced by the amount of work
done in the area in the last two decades. However,
planning is not the only interesting means-end in-
ference task. One of the goals of the next section
is to show exactly this: there is more to practical
inference than planning.
4 The inference tasks
In this section we do two things. First, we say how
current off-the-shelf planners can be used to infer
part of the clarification potential of instructions.
In particular we define what the missing element,
the goal, is and we illustrate this with fragments of
human-human dialogue of the SCARE corpus. In-
cidentally, we also show that clarification potential
can not only be used for generating and interpret-
ing CRs but also for performing acceptance and
rejection acts. Second, we motivate and start to
define one means-ends inference task that is not
currently implemented, but that is crucial for in-
ferring the clarification potential of instructions.
In order to better understand the examples be-
low you may want to read the Appendix A first.
The information in the Appendix was available to
the participants when they performed the experi-
ments and it?s heavily used in the inferences they
draw.
4.1 Planning: A means-end inference task
Shared-plan recognition ?and not artificial intel-
ligence planning? has been used for utterance in-
terpretation (Lochbaum, 1998; Carberry and Lam-
bert, 1999; Blaylock and Allen, 2005). In such
plan recognition approaches each utterance adds
a constraint to the plan that is partially filled out,
and the goal of the conversation has to be inferred
during the dialogue; that is, a whole dialogue is
mapped to one shared plan. In our approach, each
instruction is interpreted as a plan instead; that is,
200
we use planning at the utterance level and not at
dialogue level.
Artificial intelligence planning has been used at
utterance level (called micro-planning) for gener-
ation (Koller and Stone, 2007). We use artificial
intelligence planning for interpretation of instruc-
tions instead.
In our framework, the goal of the planning
problem are the preconditions of instruction for
which the clarification potential is being calcu-
lated. Now, the planning problem has a goal,
but there are two action databases and two initial
states. Which one will be used for finding the clar-
ification potential? In fact, all four.
When the DG gives an instruction, the DF has
to interpret it in order to know what actions he has
to perform (step 1 of the inference). The interpre-
tation consists in trying to construct a plan that,
when executed in the current state of the game
world, achieves the goals of the instruction. The
specification of such planning problem is as fol-
lows. The preconditions of the instruction are the
goal of the planning problem, the dialogue model
is the initial state and the potential actions are the
action operators. With this information the off-
the-shelf planner will find a plan, a sequence of
actions that are the implicatures of the instruction.
Then (step 2 of the inference), an attempt to ex-
ecute the plan on the the world model and using
the world actions occurs. Whenever the plan fails,
there is a potential clarification.
Using clarification potential to clarify: In the
dialogue below, the participants are trying to move
a picture from a wall to another wall (task 1 in Ap-
pendix A.3). The instruction that is being inter-
preted is the one uttered by the DG in (1). Using
the information in the potential action database,
the DF infers a plan that involves two implicatures,
namely picking up the picture (in order to achieve
the precondition of holding the picture), and going
to the wall (inference step 1). However, this plan
will fail when executed on the world model be-
cause the picture is not takeable and thus it cannot
be picked, resulting in a potential clarification (in-
ference step 2). This potential clarification, fore-
shadowed by (3), is finally made explicit by the
CR in (4).
DG(1): well, put it on the opposite wall
DF(2): ok, control picks the .
DF(3): control?s supposed to pick things up and .
DF(4): am I supposed to pick this thing?
A graphical representation of both steps of in-
ference involved in this example is shown in Sec-
tion B of the Appendix2.
But also to produce evidence of rejection: In
the dialogue below, the DG utters the instruction
(1) knowing that the DF will not be able to follow
it; the DG is just thinking aloud. If taken seriously,
this instruction would involve the action resolve
the reference ?cabinet nine?. A precondition of
this action is that the DF knows the numbers of the
cabinets, but both participants know this is not the
case, only the DG can see the map. That?s why the
rejection in (2) is received with laughs and the DG
continues his loud thinking in (3) while looking at
the map.
DG(1): we have to put it in cabinet nine .
DF(2): yeah, they?re not numbered [laughs]
DG(3): [laughs] where is cabinet nine .
And to produce evidence of acceptance: The
following dialogue fragment continues the frag-
ment above. Now, the DG finally says where cab-
inet nine is in (4). And the DF comes up with the
plan that he incrementally grounds making it ex-
plicit in (5), (7), and (9) while he is executing it;
the plan achieves the precondition of the instruc-
tion put of being near the destination of the action,
in this case ?near cabinet nine?. Uttering the steps
of the plan that were not made explicit by the in-
struction is indeed a frequently used method for
performing acceptance acts.
DG(4): it?s . kinda like back where you started .
so
DF(5): ok . so I have to go back through here .
DG(6): yeah
DF(7): and around the corner .
DG(8): right
DF(9): and then do I have to go back up the steps
DG(10): yeah
DF(11): alright, this is where we started
DG(12): ok . so your left ca- . the left one
DF(13): alright, so how do I open it?
In (13) the DF is not able to find a plan that
achieves another precondition of the action put,
namely that the destination container is opened, so
he directly produces a CR about the precondition.
2The correct plan to achieve (1) involves pressing button
12, as you (and the DG) can verify on the map (in the Ap-
pendix).
201
4.2 Beyond classical planning: Other
important means-end inference tasks
Consider the following example, here the DG just
told the DF to press a button, in turn (1), with no
further explanation. As a result of the action a cab-
inet opened, and the DF predicted that the follow-
ing action requested would be (5). In (6) the DG
confirms this hypothesis.
DG(1): press the button on the left [pause]
DG(2): and . uh [pause]
DF(3): [pause]
DG(4): [pause]
DF(5): put it in this cabinet?
DG(6): put it in that cabinet, yeah
The inference that the DF did in order to pro-
duce (5) can be defined as another means-end in-
ference task which involves finding the next rele-
vant actions. The input of such task would also
consist of an initial state, a set of possible ac-
tions but it will contain one observed action (in
the example, action (1)). Inferring the next rele-
vant action consists in inferring the affordabilities
(i.e. the set of executable actions) of the initial
state and the affordabilities of the state after the
observed action was executed. The next relevant
actions will be those actions that were activated
by the observed action. In the example above, the
next relevant action that will be inferred is ?put
the thing you are carrying in the cabinet that just
opened?, just what the DF predicted in (5).
The definition of this inference task needs refin-
ing but it already constitutes an interesting exam-
ple of a new form of means-ends reasoning.
There are further examples in the corpus that
suggest the need for means-end inferences in situ-
ations in which a classical planner would just say
?there is no plan?. These are cases in which no
complete plan can be found but the DF is anyway
able to predict a possible course of action. For in-
stance, in the last dialogue of Section 4.1, the DF
does not stops in (13) and waits for an answer but
he continues with:
DF(14): one of the buttons?
DG(15): yeah, it?s the left one
Other CRs similar to this one, where a param-
eter of the action is ambiguous, is missing or is
redundant, were also found in the corpus.
4.3 Preliminary Conclusions
The inference-tasks we discussed or just hinted to
in this paper do not give a complete characteriza-
tion of the kinds of clarification requests of level
4. It covers 14 of the 19 CRs in the SCARE di-
alogue analyzed in Section 2.3. CRs not covered
at all have to do mainly with the fact that people
do not completely remember (or trust) the instruc-
tions during the experiments or what themselves
(or their partner) said a few turns before, such as
the following one:
DG(1): you?ve to . like jump on it or something .
DF(2): I don?t know if I can jump
Here, the DF does not remember that he can
jump using the Spacebar as stated in the instruc-
tions he received (Appendix A.1).
In order to account for these cases it is nec-
essary to consider how conversation is useful for
overcoming also this issue. The fact that people?s
memory is non reliable is intrinsic to communica-
tion and here again, communication must provide
intrinsic mechanisms to deal with it. Modeling
such things are challenges that a complete theory
of communication will have to face.
5 Conclusions
Conversational implicatures are negotiable, this
is the characteristic that distinguishes them from
other kinds of meanings (like entailments). Dia-
logue provides an intrinsic mechanism for carry-
ing out negotiations of meaning, namely clarifi-
cations. So our hypothesis is that conversational
implicatures are a rich source of clarification re-
quests.
In order to investigate this hypothesis, we re-
viewed theoretical work from pragmatics, prac-
tical work from the dialogue system community
and we presented empirical evidence from spon-
taneous dialogues situated in an instruction giving
task. Also, we presented a framework in which
(part of) the clarification potential of an instruc-
tion is generated by inferring its conversational
implicatures. We believe that this is a step towards
defining a clear functional criteria for identifying
and classifying the clarification requests at level 4
of communication.
But much more remains to be done. The empir-
ical results we present here are suggestive but pre-
liminary; we are currently in the process of eval-
uating their reliability measuring inter-annotator
202
agreement. Moreover, in the course of this work
we noticed a promising link between clarifica-
tion strategies and politeness constraints which we
plan to develop in future work. Also, we are par-
ticularly interested in means-ends reasoning other
than planning, something we have merely hinted
at in this paper; these tasks still need to be for-
mally defined, implemented and tested. Finally,
we are considering the GIVE challenge (Byron et
al., 2009) as a possible setting for evaluating our
work (our framework could predict potential clar-
ification requests from the users).
There is lot to do yet, but we believe that the
interplay between conversational implicatures and
clarification mechanisms will play a crucial role in
future theories of communication.
References
Jens Allwood. 1995. An activity based approach
to pragmatics. In Abduction, Belief and Context
in Dialogue: Studies in Computational Pragmatics,
pages 47?80. University of Go?teborg.
Nate Blaylock and James Allen. 2005. A collaborative
problem-solving model of dialogue. In Proceedings
of the 6th SIGdial Workshop on Discourse and Dia-
logue, pages 200?211, Lisbon, Portugal.
Penelope Brown and Stephen Levinson. 1987. Polite-
ness: Some universals in language usage. Studies in
Interactional Sociolinguistics.
Donna Byron, Alexander Koller, Kristina Striegnitz,
Justine Cassell, Robert Dale, Johanna Moore, and
Jon Oberlander. 2009. Report on the First NLG
Challenge on Generating Instructions in Virtual En-
vironments (GIVE). In Proc. of the 12th European
Workshop on Natural Language Generation, pages
165?173, Athens, Greece. ACL.
Sandra Carberry and Lynn Lambert. 1999. A process
model for recognizing communicative acts and mod-
eling negotiation subdialogues. Computational Lin-
guistics, 25(1):1?53.
Herbert Clark. 1996. Using Language. Cambridge
University Press, New York.
Richard Fikes, Peter Hart, and Nils Nilsson. 1972.
Learning and executing generalized robot plans. Ar-
tificial Intelligence, 3:251?288.
Malte Gabsdil. 2003. Clarification in spoken dialogue
systems. In Proc of the AAAI Spring Symposium.
Workshop on Natural Language Generation in Spo-
ken and Written Dialogue, pages 28?35.
Alfonso Gerevini and Derek Long. 2005. Plan con-
straints and preferences in PDDL3. Technical Re-
port R.T. 2005-08-47, Brescia University, Italy.
Jonathan Ginzburg. 2009. The interactive Stance:
Meaning for Conversation. CSLI Publications.
Paul Grice. 1975. Logic and conversation. In P. Cole
and J. L. Morgan, editors, Syntax and Semantics:
Vol. 3: Speech Acts, pages 41?58. Academic Press.
Jo?rg Hoffmann and Bernhard Nebel. 2001. The
FF planning system: Fast plan generation through
heuristic search. JAIR, 14:253?302.
Chih-Wei Hsu, Benjamin W. Wah, Ruoyun Huang,
and Yixin Chen. 2006. New features in SGPlan
for handling soft constraints and goal preferences in
PDDL3.0. In Proc of ICAPS.
Anthony Kenny. 1966. Practical inference. Analysis,
26:65?75.
Alexander Koller and Matthew Stone. 2007. Sentence
generation as planning. In Proc. of ACL-07, Prague.
Karen E. Lochbaum. 1998. A collaborative planning
model of intentional structure. Comput. Linguist.,
24(4):525?572.
Matthew Purver, Jonathan Ginzburg, and Patrick
Healey. 2003. On the means for clarification in
dialogue. In Current and New Directions in Dis-
course and Dialogue, pages 235?255. Kluwer Aca-
demic Publishers.
Matthew Purver. 2004. The Theory and Use of Clari-
fication Requests in Dialogue. Ph.D. thesis, King?s
College, University of London.
Verena Rieser and Johanna Moore. 2005. Implications
for generating clarification requests in task-oriented
dialogues. In Proc of ACL, pages 239?246.
Kepa Rodr??guez and David Schlangen. 2004. Form,
intonation and function of clarification requests in
german task oriented spoken dialogues. In Proc of
SEMDIAL, pages 101?108.
Emanuel Schegloff. 1987. Some sources of misunder-
standing in talk-in-interaction. Linguistics, 8:201?
218.
David Schlangen. 2004. Causes and strategies for re-
questing clarification in dialogue. In Proc of SIG-
DIAL.
Gabriel Skantze. 2007. Error Handling in Spoken Di-
alogue Systems. Ph.D. thesis, KTH - Royal Institute
of Technology, Sweden.
Laura Stoia, Darla Shockley, Donna Byron, and Eric
Fosler-Lussier. 2008. SCARE: A situated corpus
with annotated referring expressions. In Proc of
LREC.
Laura Stoia. 2007. Noun Phrase Generation for Sit-
uated Dialogs. Ph.D. thesis, Ohio State University,
USA.
203
A Instructions for the DG and DF
In this section, we specify the information that
was available to the DG and the DF before the
SCARE experiment started (adapted from (Stoia,
2007)). These instructions are crucial for our
study since they define the content of the infor-
mation resources of the inference framework de-
scribed in this paper.
A.1 Instructions for both
The following specification of the Quake controls,
that is, the possible actions in the simulated world,
were received by all participants.
1. Use the arrow keys for movement:
? Walk forward: ?
? Walk backward: ?
? Turn right: ?
? Turn left: ?
2. To jump: use Spacebar.
3. To press a button: Walk over the button.
You will see it depress.
4. To pick up an object: Step onto the item
then press Ctrl (Control key).
5. To drop an object: Hit TAB to see the list of
items that you are currently carrying. Press
the letter beside the item you wish to drop.
Press TAB again to make the menu go away.
The participants also received the following pic-
tures of possible objects in the simulated world so
that they are able to recognize them.
Buttons Cabinet
The following things were indicated as being
objects that the DF can pick up and move:
Quad damage Rebreather Silencer
They also received the following warning: You
will not be timed, but penalty points will be taken
for pushing the wrong buttons or placing things in
the wrong cabinets.
A.2 Instructions for the Direction Follower
Only the DF received the following information:
Phase 1: Learning the controls First you will
be put into a small map with no partner, to get ac-
customed to the quake controls (detailed in Sec-
tion A.1). Practice moving around using the arrow
keys. Practice these actions:
1. Pick up the Rebreather or the Quad Damage.
2. Push the blue button to open the cabinet.
3. Drop the Quad Damage or the Rebreather in-
side the cabinet and close the door by pushing
the button again.
Phase 2: Completing the task In this phase you
will be put in a new location. Your partner will
direct you in completing 5 tasks. He will see the
same view that you are seeing, but you are the only
one that can move around and act in the world.
A.2.1 Implications for the Potential Actions
In phase 1, when the DF is learning the con-
trols, he learns that buttons can have the effect
of opening closed cabinets and closing open cab-
inets. Such action is formalized as follows in
PDDL (Gerevini and Long, 2005) and is included
in the possible action database:
(:action press_button
:parameters (?x ?y)
:precondition
(button ?x)
(cabinet ?y)
(opens ?x ?y)
:effects
(when (open ?y) (closed ?y))
(when (closed ?y) (open ?y)))
Notice that this action operator has conditional
effects in order to specify the action more suc-
cinctly. However, it is not mandatory for the action
language to support conditional effects. This ac-
tion could be specified with two actions in which
the antecedent of the conditional effect is now a
precondition.
A.3 Instructions for the Direction Giver
Only the DG received the following information:
Phase 1: Planning the task Your packet con-
tains a map of the quake world with 5 objectives
that you have to direct your partner to perform.
Read the instructions and take your time to plan
the directions you want to give to your partner.
204
Figure 2: Map received by the DG (upper floor)
Phase 2: Directing the follower In this phase
your partner will be placed into the world in the
start position. Your monitor will show his/her
view of the world as he/she moves around. He/she
has no knowledge of the tasks, and has not re-
ceived a map. You have to direct him/her through
speech in order to complete the tasks. The objec-
tive is to complete all 5 tasks, but the order does
not matter.
The tasks are:
1. Move the picture to the other wall.
2. Move the boxes on the long table so that the
final configuration matches the picture below.
Picture Long table
3. Hide the Rebreather in Cabinet9. To hide an
item you have to find it, pick it up, drop it in
the cabinet and close the door.
4. Hide the Silencer in Cabinet4.
5. Hide the Quad Damage in Cabinet14.
6. At the end, return to the starting point.
A.3.1 Implications for the World Actions
The functions of the buttons that can move
things can be represented in the following action
schema. If the thing is in it?s original location (its
location when the game starts), we say that is thing
is not-moved. If the thing is in the goal position
then we say that the thing is moved.
(:action press_button
:parameters (?x ?y)
:precondition
(button ?x)
(thing ?y)
(moves ?x ?y)
:effects
(when (moved ?y) (not-moved ?y))
(when (not-moved ?y) (moved ?y)))
A.3.2 Implications for the World Model
The world model is a relational model that rep-
resents the information provided by the map, in-
cluding the functions of the buttons and the con-
tents of the cabinets.
Figure 3: Fragment of the SCARE world model
B Clarification Potential Inference Steps
The following pictures illustrate how the impli-
catures of the instruction ?put the picture on the
opposite wall? are calculated using the dialogue
model (Figure 4) and used to predict the CR ?Am
I supposed to pick up this thing?? (Figure 5).
Figure 4: Step 1 - Calculating the implicatures
Figure 5: Step 2 - Predicting the CR
205
Proceedings of the 8th International Conference on Computational Semantics, pages 4?17,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
A computational account of comparative
implicatures for a spoken dialogue agent
?
Luciana Benotti
LORIA/INRIA, France
benottil@loria.fr
David Traum
ICT/USC, USA
traum@ict.usc.edu
Abstract
Comparative constructions are common in dialogue, especially in
negotiative dialogue where a choice must be made between different
options, and options must be evaluated using multiple metrics. Com-
paratives explicitly assert a relationship between two elements along a
scale, but they may also implicate positions on the scale especially if
constraints on the possible values are present. Dialogue systems must
often understand more from a comparative than the explicit assertion
in order to understand why the comparative was uttered. In this paper
we examine the pragmatic meaning of comparative constructions from
a computational perspective.
1 Introduction
It is a big challenge for computational semantics of dialogue that much of
the meaning of an utterance is conveyed not just through the compositional
meanings of the words themselves, but in relation to the situation in which
the utterance is performed, including the background knowledge, common
ground, goals, and ontologies of the participants. A number of pragmatic
principles have been proposed to bridge this gap, including Grice?s maxims
(and the associated concepts of Implicature and Relevance)
[
6
]
, Accommo-
dation
[
12
]
and Bridging
[
2
]
.
?
This work was sponsored by the U.S. Army Research, Development, and Engineering
Command (RDECOM). Statements and opinions expressed do not necessarily reflect the
position or the policy of the United States Government, and no official endorsement should
be inferred.
4
While these principles can provide elegant explanations of how meaning
can be conveyed between people, they often require fairly strong assump-
tions about the common knowledge between participants and the ontologies
that this knowledge must be organized in. There are two general prob-
lems in developing computational accounts of these phenomena. First, it is
sometimes difficult to specify the required knowledge and relationships in a
computational way, such that a reasoner given the input and context can
compute a particular, specific meaning as opposed to other possibilities that
are not as appropriate. Second, even if the principles are sufficiently clear so
that a computational account can be formulated, there may still be a prob-
lem providing a given computational dialogue system with the appropriate
knowledge to carry out the inferences in a way that is congruent with human
interpretations. For a hand-constructed limited domain, the system designer
will often take shortcuts and represent only the knowledge that is necessary
to carry out and understand tasks in that domain. These limitations often
render the dialogue system unable to reason about the domain in as much
detail as a knowledgeable human would, but often this characterization is
sufficient for the purposes of the conversation.
In this paper we examine one aspect of computational non-compositional
meaning: the pragmatic meaning of comparative constructions. Compara-
tive constructions are common in dialogue, especially in negotiative dialogue
where a choice must be made between different options, and options must
be evaluated using multiple metrics. Comparatives explicitly assert a rela-
tionship between two elements along a scale, but they may also implicate
positions on the scale especially if either information about the position of
one of the compared items or constraints on the possible values are present.
Dialogue systems must often understand more from a comparative than the
explicit assertion in order to understand why the comparative was uttered;
why it is relevant to the dialogue.
In the next section, we present linguistic background on comparatives
and conversational implicature. In section 3, we review some issues of how
implicatures play out in dialogue in which multiple participants are involved,
and a listener can clarify a lack of understanding. In section 4, we introduce
the computational framework in which the present work is implemented.
In section 5, we present extensions to this framework which provide the
computational agents with the ability to understand implicatures arising
from comparatives. In section 6 we evaluate this with respect to the scenario
of SASO-EN
[
15
]
, in which an army captain, a doctor, and a town elder
discuss the best location for a medical clinic. Finally, we discuss related
issues, remaining problems, and future work in Section 7.
5
2 Linguistic background
In this section we review some of the previous work on comparatives con-
structions and conversational implicatures in order to establish the necessary
theoretical basis for the discussion on comparative implicatures in the rest
of the paper.
2.1 Comparative constructions
In the classical literature on semantics of natural language comparatives (see
e.g.,
[
4
]
), comparative constructions such as (1) are analyzed as a relation
between two degrees as in (2).
(1) Downtown is safer than the market.
(2) Degree to which downtown is safe > Degree to which the market is
safe.
The problem now, of course, is what a degree is and what are the prop-
erties of the relation >. Abstractly, a degree can be considered just as a
collection of objects ? the collection of those objects that share the same
degree with respect to a given property P . These collections are defined as
the equivalence classes of the equivalence relation =
P
, which in turn is de-
fined in terms of an order >
P
among objects. The relation > among degrees
is defined as a lifting of >
P
to the set of equivalent classes of =
P
. Finally,
scales are defined in terms of degrees, a scale S
P
is a sequence of degrees of
P ordered by the relation >.
Summing up then, once we know what >
P
is for a property P we know
what degrees of P are and how to compare them on scale S
P
. All is good
and well, but this approach assumes the relation >
P
as given. Such a strong
assumption was already criticized by
[
9
]
and certainly cannot be made in a
dialogue system where information about the domain of discourse (in partic-
ular, any order >
P
for a given property P ) is incomplete and is constructed
(and negotiated) during the dialogue. As dialogue system builders, the is-
sue that interests us is, not so much how to determine the truth value of a
particular comparative utterance, but mainly how comparatives contribute
to the construction of the information about the domain. So, for our task
it is crucial to figure out ?where do scales come from??
6
2.2 Conversational implicatures
Modelling how listeners draw inferences from what they hear is a basic
problem for the theories of understanding natural language. An important
part of the information conveyed is inferred, as in the following classical
example by Grice
[
6
]
:
(3) A: I am out of petrol.
B: There is a garage around the corner.
; B thinks that the garage is open and has petrol to sell.
B?s answer conversationally implicates (;) information that is relevant
to A. In Grice?s terms, B made a relevance implicature, he would be flouting
Grice?s maxim of relevance unless he believes that the garage is open. A
conversational implicature (CI) is different from an entailment in that it is
cancelable without contradiction (B can append material that is inconsistent
with the CI ??but I don?t know whether it?s open?). Since the CI can be
cancelled, B knows that it does not necessarily hold and then both B or
A are able to reinforce it (or negotiate it) without repetition. CIs are non-
conventional, they are not part of the conventional meaning of the words but
calculable from their utterance in context given the nature of conversation
as a goal-oriented enterprise.
Scalar implicatures are a type of CI that are particularly relevant to
this paper since they involve the use of scales, as comparatives do. These
implicatures are inferred on the basis of the assumption that the speaker is
trying to make his utterance sufficiently informative for the current purposes
of the exchange. A typical example is:
(4) A is hungry (and B knows it).
A: Did somebody eat the brownies that I bought this morning?
B: Fred ate some of them.
; B thinks that Fred didn?t eat all of them, there are some left.
Theories of scalar implicature have been deeply influenced by Horn?s
dissertation work
[
8
]
. A Horn scale is an ordered sequence of expressions
such as ?some,many,most, all? and ?warm, hot, scalding?. The recipe to
calculate the scalar implicature is the following. The use of a weaker word
in the scale (such as some) implicates that (the speaker believes that) the
stronger form (such as all) is not the case, as exemplified in (4). However,
there are cases in which such a recipe does not apply, such as in (5).
7
(5) A realizes that the brownies were injected with strychnine and suspects
that somebody may have eaten from them (and B knows it).
A: Did somebody eat the brownies that I bought this morning?
B: Fred ate some of them.
In this example the scalar implicature is (at the very least) less likely to
arise, it?s not relevant for the current purposes of the exchange; it doesn?t
matter how many brownies Fred ate, he needs medical attention. Comparing
(4) and (5) we can see that conversational implicatures are affected by the
conversational goals. We believe this to be an starting point for answering
the question left open in Section 2.1: ?Where do scales come from?? and
we will investigate it in next section.
3 Comparative implicatures in dialogue
In this Section we are going to introduce comparative implicatures and to
develop two lines of thought introduced in the previous section, but now in
the context of dialogue. First, we relate the cancellability and negotiability
of CIs and clarification requests in dialogue. It?s often controversial whether
something is actually a CI or not (people sometimes have different intu-
itions). Dialogue provide us with an extra tools for identifying the partici-
pants? beliefs about implicatures: feedback and negotiation of CIs. Listeners
can give both positive (e.g. acknowledgements) and negative (e.g. clarifi-
cation requests) feedback about their understanding and the acceptability
of utterances, which can shed light on how they interpret CIs. Moreover,
speakers can negotiate whether something is really implicated, or whether
an implicature is meant by the speaker. We also investigate the fact that
conversational implicatures change when the conversational goals change.
Conversational goals establish structure on the information that is being
discussed, determining which distinctions are relevant and which are not for
the a given interaction.
3.1 Clarifications
In dialogue, a useful way to spot content that was meant but not actually
said is to look at the kinds of dialogue feedback that is given. Clarification
requests are one important part of this - focusing on cases where the initial
listener is not sure of what the speaker means. Here we use the phrase
clarification requests in its broad sense, including all those questions that
would not make sense without the previous utterance in the dialogue.
8
Many times the clarification requests make the implicatures explicit, this
is illustrated by the fact that the clarification request in (6) can follow Grice?s
example in (3).
(6) A: and you think it?s open?
Here we present clarifications as a test that helps us identify not only
the potential conversational implicatures but also the presuppositions in a
dialogue. (7) is an example of a well studied case of inferrable content: the
presupposition triggered by definite descriptions. In this exchange, A might
believe that the intended referent is salient for different reasons: because it
was mentioned before, because it?s bigger, etc. If such a piece of information
is available to B then he will be able to infer the presupposition and add
it to the conversational record. Otherwise, B might well signal A that the
presupposition did not get through, like in (7).
(7) There are 100 cups on a table.
A: Pick up THE cup.
B: Which cup? (I don?t know which one you are referring to.)
The presence of a conversational implicature must be capable of being
worked out, so the hearer might well inquire about them. The speaker will
have to answer and support the implicature if he wants to get it added to
the common ground. In
[
13
]
, the authors present a study of the different
kinds of clarification requests (CRs) found in a dialogue corpus. Their re-
sults show that the most frequent CRs in their corpus (51.74%) are due to
problems with referring expressions (such as example (7)) and the second
most common (22.17%) are examples like the following.
(8) A: Turn it on.
B: By pushing the red button?
Here, the CR made explicit a potential requirement for the performance
of the requested action. By saying ?Turn it on? A meant that B had to
push the red button but this was not explicitly mentioned in the command
uttered by A. Such an implicature can be inferred from knowledge about the
task (or in Clark terms, from knowledge about the interaction level of joint
action
[
3
]
). From this examples it should be clear that implicated content
is a rich source of CRs.
9
3.2 Goals and Contextual Scales
As described in Section 2, comparative sentences explicitly claim an ordering
of two items on a relevant scale. However, with more context, a comparative
can implicate degrees on a scale as well. If we know the degree of one of
the items then we have a fixed range for the other item. If this range
contains only one possible degree, then the comparative also sets the degree
for this item. In the simplest possible scale in which a comparative could be
applied, there are only two degrees, and a comparative indicates the degrees
of each item even without more knowledge of the degrees of either item. For
example, consider (9) inspired by
[
5
]
:
(9) B is drawing and A is giving instructions to B:
A: Draw two squares, one bigger than the other.
B: Done.
A: Now, paint the small square blue and the big one red.
; A thinks that one of the squares can be described as small and the
other as big.
The question remains, however, of how the scales are selected? These
are not generalized implicatures that always arise. Every time we say ?Fred
is taller than Tom? we don?t mean that ?Fred is tall? and ?Tom is short?.
As discussed in Section 2, conversational implicatures are affected by con-
versational goals. We believe that this is the path that we have to follow in
order to explain comparative implicatures as particularized implicatures.
Predicates such as tall and small are vague in that they can refer to
different ranges of a scale and in fact different scales. Small refers to size,
which as a default we might think of as a continuous scale measure, however,
as in the example above, we might prefer to use a simpler scale. This
phenomena was already noticed by
[
7
]
who says:
There is a range of structures we can impose on scales. These
map complex scales into simpler scales. For example, in much
work in qualitative physics the actual measurement of some pa-
rameter may be anything on the real line, but this is mapped
into one of three values ? positive, zero, and negative.
How are such structures over scales defined? The intuitive idea is that
the structure only distinguishes those values that are relevant for the goals
of the agent; that is when the attribute takes a particular value, it plays a
causal role in deciding whether some agent?s goal can be achieved or not.
10
In the example above, we have only small and big as relevant values for
size, and a comparative in this context will implicate that we are using this
binary scale as well as the degrees that each of the items of comparison have.
4 Computational framework
The computational framework we have used for the implementation is the
ICT Virtual Human dialogue manager
[
14; 16
]
. This dialogue manager al-
lows virtual humans to participate in bilateral or multiparty task-oriented
conversations in a variety of domains, including teamand non-teamnegotiation.
This dialogue manager follows the information state approach
[
11
]
, with a
rich set of dialogue acts at different levels. The dialogue manager is em-
bedded within the Soar cognitive architecture
[
10
]
, and decisions about in-
terpreting and producing speech compete with other cognitive and physical
operations. When an utterance has been perceived (either from processing of
Automatic Speech Recognizer and Natural Language Understanding (NLU)
components which present hypotheses of context-independent semantic rep-
resentations, or messages from self or other agents), the dialogue manager
processes these utterances, making decisions about pragmatic information
such as the set of speech acts that this utterance performs as well as the
meanings of referring expressions. Updates are then performed to the in-
formation state on the basis of the recognized acts. The representations of
semantic and pragmatic elements for questions and assertions are presented
in
[
14
]
. The semantic structure derives from the task model used for plan-
ning, emotion and other reasoning. (10a) shows an example proposition in
this domain, where propositions have an object id, an attribute, and a value
(whose type is defined by the attribute). (10b) shows an assertion speech
act, with this proposition as content.
(10) a. (<prop1> ?attribute safety ?object market ?type state ?value no)
b. (<da1> ?action assert ?actor doctor ?addressee captain ?content
<prop1>)
In the next section, we extend this framework to allow comparative
propositions and speech acts arising from implicatures of comparatives.
5 Implementing comparative implicatures
We have added an ability for the ICT dialogue manager to handle compara-
tive constructions and comparative implicatures. Some of the attributes in
11
the task model for a given domain will be scalar, where there is some implied
ordering of the possible values. This will not be the case for all attributes,
for example, the location allows for different possible places that an entity
can be, but there is no scale among them. For each scalar attribute, P , in
the task model of our domain, we can create a comparative attribute >
P
that will compare objects and values according to the designated scale for
P . (11a) means that X is higher on the P scale than Y. In our system, when
a comparative of the form ?X is P -er than Y ? is uttered, the NLU gener-
ates a semantic frame that has the structure shown in (11a). The dialogue
manager will then create an assertion speech act as represented in (11b) and
then infer the conversational implicatures that arise.
(11) a. (<prop1> ?attribute >
P
?object X ?type state ?value Y )
b. (<da1> ?action assert ?actor A ?addressee B ?content <prop1>)
The comparative implicatures depend on both the nature of the scale
and the available information about the positions of the compared items.
For the special case of a binary scale (e.g. yes > no), the comparative
construction itself can generate multiple implicatures, as in (12).
(12) a. By the definition of the scale S = ?no, yes? and its association with
the attribute P then we know that, if <prop> ?attribute P ?value
V , then V ? S
b. As the utterance asserts (<prop1> ?attribute >
P
?object X ?value
Y ), it is interpreted as asserting (<prop2> ?attribute P ?object
X ?value V 1) and (<prop3> ?attribute P ?object Y ?value V 2),
where values V 1 and V 2 are not known but it is known that V 1 >
V 2
From (12a) we have that V 1 ? S and V 2 ? S, from (12b) we have that
V 1 > V 2. Since S has 2 elements and yes > no, there is a unique valuation
for V 1 and V 2, namely V 1 = yes and V 2 = no. Once the values of V 1
and V 2 are determined the following two dialogue acts are generated as
part of the interpretation and reinserted in the dialogue manager cycle. The
information state will not only be updated with the comparative assertion
?X is P -er than Y ? but also with the two following dialogue acts. The first
one asserts that ?X is P?. And the second one asserts that ?Y is not P?.
(13) (<prop2> ?attribute P ?object X ?type state ?value yes)
(<da2> ?action assert ?actor A ?addressee B ?content <prop2>)
12
(14) (<prop3> ?attribute P ?object Y ?type state ?value no)
(<da3> ?action assert ?actor A ?addressee B ?content <prop3>)
This is the first part of the problem. The question now is when and
how to update the information state with (13) and (14): before, at the same
time or after the explicit assertion (11b)); we will discuss these issues in
next subsection. Moreover, in some contexts the conversational implicature
carried by the comparative will not get through, how are such information
states recognized and what is done instead will be addressed in Section 7.
5.1 When and how is the information state updated?
When interpreting an utterance U that has a conversational implicature I,
the dialogue manager needs to have accessible the content of the implicature
before generating a response. The three approaches presented below make
explicit the implicature in the dialogue, just as if its verbalization has been
uttered during the dialogue. Since the inferred content is made explicit,
these approaches can be seen as implementations of the Principle of Explicit
Addition
[
1
]
which applies both to accommodation of content that has been
lexically triggered (such as presuppositions) as well as content that has not
(such as conversational implicatures).
The after approach: A first approach is to add the inferred assertion I
as further input received by the dialogue manager once it finished
processing the utterance U . The idea for the implementation is simple:
just re-insert the frame of the inferred assertion(s) as an input to the
dialogue manager, as if it had been uttered in the dialogue.
At the same time: A second approach is to consider that a single asser-
tion performs multiple speech acts. The implementation of this option
in the current computational framework is also quite straightforward
because the framework already allows for multiple speech acts asso-
ciated with an utterance and then the information state is correctly
updated.
The before approach: The third approach is to interrupt the processing
of the explicit assertion U to update the information state first with the
implicature I. Such an implementation requires significant changes to
the dialogue manager to interrupt processing on the current assertion
and first interpret the inferred speech acts before re-interpreting.
13
The crucial difference among the three implementations is what content
will be available in the information state when the rest of the content has to
be interpreted. In the same time approach all the updates are independent,
the information state does not contain the implicatures, nor the explicit
assertion when interpreting any of them. In the after approach, U is avail-
able in the information state when interpreting I; by contrast, in the before
approach I is available when interpreting U .
These differences are relevant when they interact with other parts of the
interpretation process, such as reference resolution. Consider for instance
the following utterance:
(15) A: I went to the hospital Saint-Joseph, the British clinic was too far.
A: The doctor gave me some medicine
The utterance ?The doctor gave me some medicine? would normally
be interpreted as implicating that A saw a doctor in the hospital Saint-
Joseph. In order to properly interpret this utterance, it?s important that
the implicature is in the context before resolving the referring expression
?the doctor?. We leave for future work the interaction with other aspects
of interpretation, such as word sense disambiguation.
6 A case-study
We have evaluated the implementation in the context of the SASO-EN sce-
nario
[
15
]
, where both comparatives and binary scales are present in the task
model. This domain contains over 60 distinct states that the agents con-
sider as relevant for plan and negotiation purposes. There are currently 11
attributes used for this domain, 7 of which are binary scales and 4 of which
are non-scalar. Adding the comparative implicature rules from the previous
section allows understanding of additional arguments that were not previ-
ously dealt with adequately by the system. Consider the following fragment
of a dialogue among the Captain (a human agent), the Elder and the Doctor
(two virtual agents) about the location of a clinic.
(16) Captain: Doctor would you be willing to move the clinic downtown?
Doctor: It is better to keep the clinic here in the marketplace.
Captain: Well, downtown is safer than the market
Elder: Why do you think that the market is not safe?
During the interpretation of the comparative uttered by the Captain,
the dialogue manager receives the following semantic frame:
14
(17) (<prop> ?attribute safer ?object downtown ?type state ?value mar-
ket)
Then the inferred rules developed in Section 5 are applied and the in-
formation state is updated not only with an assertion of (17) but also with
the following two assertions:
(18) (<prop1> ?attribute safety ?object downtown ?type state ?value yes)
(<prop1> ?attribute safety ?object market ?type state ?value no)
Since these propositions assert the fact that the market is not safe before
the Elder generates a response, he can directly address and query the reason
for one of these implicatures with ?why do you think that the market is not
safe??
In the general case, we might have to reason about the appropriate scale
to use for this attribute, but in our domain, only one scale is relevant, so
this additional inference is not needed.
Without the comparative implicature rules, neither the elder nor the
doctor would recognize that the captain is asserting something about the
safety of each of the locations and will not be able to properly assess the
argument about desirability of moving the clinic.
7 Discussion
As mentioned in Section 3, the relevant information sources of a dialogue,
such as the task model, shed light on how the scales that are relevant for
calculating implicatures should be constructed. However, once comparative
implicatures are inferred they interact with the structure of the dialogue
in relevant ways. When implicated assertions are already in the context,
because they have been uttered before, the information state does not need
to be updated again with them.
If A says ? and implicates ? but A said ? before in the dialogue then
the context of the dialogue does not have to be updated again with ?. In
this case, we say that the implicature has been bound (to use standard
terminology in the area) and the information state is not modified by it.
The result is a coherent dialogue, a dialogue that gives the intuition of
?continuing on the same topic or stressing the same point? such as in the
following example:
(19) Captain: the market is not safe
Captain: downtown is safer than the market
15
The point can be better illustrated when the two contributions are not
made by the same speaker. In this case, A says ? and B says ? which
implicates ?, then ? has the effect of accepting ? and adding it to the
common ground. This implements the intuition that, in (20), the elder
seems to be supporting the captain in his claim that the market is not safe.
(20) Captain: the market is not safe
Elder: downtown is safer than the market
Finally, implicature cancellation is implemented in a simplistic way in
the current framework. When A says ? and implicates ? but ?? is already
in the common ground, the implicature is simply ignored.
Our implementation captures the intuition that the relevant values of
the properties in a domain are those that are causally related to the agent?s
goals. This is used in order to construct or locate appropriate scales and
infer useful implicatures that interact with the dialogue structure in different
ways. However, not all the predictions achieved by such an implementation
are explainable and further refinement is needed. Consider the following
dialogue:
(21) C: Downtown is safer than the market
C: The US base is safer than downtown
If we apply the inference rules developed in Section 5 to this exchange,
which implicature gets through will depend on the order in which the utter-
ances in (21) are said. If they are said in the order shown in (21) then the
implicature that downtown is safe will get through; if they are uttered in
the opposite order, then the implicature that downtown is not safe will get
through. We leave the study of these kinds of interactions to further work.
It is clear that treating conversational implicatures in dialogue is a com-
plex problem, and that it interacts in relevant ways with other key features
of dialogue, namely positive and negative evidence of understanding. It is
important that the theory of implicatures acknowledges the fact that con-
versational implicatures are a phenomena that arises in conversation and
that then needs to be studied in its environment to be fully understood.
This paper starts by motivating this big picture, and then relates it to the
semantic theory of comparatives and the pragmatic theory of implicatures
in order to address the practical problem of comparative implicatures that
arises in an implemented dialogue agent.
16
References
[
1
]
D. Beaver and H. Zeevat. Accommodation. In The Oxford Handbook of Lin-
guistic Interfaces, pages 503?539. OUP, 2007.
[
2
]
H. Clark. Bridging. In The 1975 Workshop on Theoretical issues in natural
language processing, pages 169?174. ACL, 1975.
[
3
]
H. Clark. Using Language. CUP, 1996.
[
4
]
M. Cresswell. The semantics of degree. In Montague Grammar, 1976.
[
5
]
D. DeVault and M. Stone. Interpreting vague utterances in context. In Proc.
of COLING04, pages 1247?1253, 2004.
[
6
]
H. Grice. Logic and conversation. In P. Cole and J. L. Morgan, editors, Syntax
and Semantics: Vol. 3: Speech Acts, pages 41?58. AP, 1975.
[
7
]
J. Hobbs. Discourse and inference. To appear.
[
8
]
L. Horn. On the semantic properties of logical operators in english. PhD thesis,
University of California Los Angeles (UCLA), 1972.
[
9
]
E. Klein. A semantics for positive and comparative adjectives. Linguistics and
Philosophy, 4:1?45, 1980.
[
10
]
J. Laird, A. Newell, and P. Rosenbloom. SOAR: an architecture for general
intelligence. AI, 33(1):1?64, September 1987.
[
11
]
S. Larsson and D. Traum. Information state and dialogue management in the
TRINDI dialogue move engine toolkit. Natural Language Engineering, 6:323?
340, September 2000.
[
12
]
D. Lewis. Scorekeeping in a language game. Journal of Philosophical Logic,
8:339?359, 1979.
[
13
]
K. Rodr??guez and D. Schlangen. Form, intonation and function of clarification
requests in german task oriented spoken dialogues. In CATALOG04, pages
101?108, 2004.
[
14
]
D. Traum. Semantics and pragmatics of questions and answers for dialogue
agents. In Proc. of IWCS-5, pages 380?394, 2003.
[
15
]
D. Traum, S. Marsella, J. Gratch, J. Lee, and A. Hartholt. Multi-party,
multi-issue, multi-strategy negotiation for multi-modal virtual agents. In
H. Prendinger, J. Lester, and M. Ishizuka, editors, IVA, volume 5208 of LNCS,
pages 117?130. Springer, 2008.
[
16
]
D. Traum, W. Swartout, J. Gratch, and S. Marsella. A virtual human dialogue
model for non-team interaction. In L. Dybkjaer and W. Minker, editors, Recent
Trends in Discourse and Dialogue. Springer, 2008.
17
Proceedings of the EACL 2009 Demonstrations Session, pages 1?4,
Athens, Greece, 3 April 2009. c?2009 Association for Computational Linguistics
Frolog: an accommodating text-adventure game
Luciana Benotti
TALARIS Team - LORIA (Universite? Henri Poincare?, INRIA)
BP 239, 54506 Vandoeuvre-le`s-Nancy, France
Luciana.Benotti@loria.fr
Abstract
Frolog is a text-adventure game whose goal
is to serve as a laboratory for testing prag-
matic theories of accommodation. To
this end, rather than implementing ad-hoc
mechanisms for each task that is neces-
sary in such a conversational agent, Frolog
integrates recently developed tools from
computational linguistics, theorem prov-
ing and artificial intelligence planning.
1 Introduction
If we take a dialogue perspective on Lewis? (1979)
notion of accommodation and assume that the
state of a dialogue is changed by the acts per-
formed by the dialogue participants, it is natural to
interpret Lewis? broad notion of accommodation
as tacit (or implicit) dialogue acts. This is the ap-
proach adopted by Kreutel and Matheson (2003)
who formalize the treatment of tacit dialogue acts
in the information state update framework. Ac-
cording to them, accommodation is ruled by the
following principle:
Context Accommodation (CA): For any move m
that ocurrs in a given scenario sci: if assignment
of a context-dependent interpretation to m in sci
fails, try to accommodate sci to a new context
sci+1 in an appropriate way by assuming implicit
dialogue acts performed in m, and start interpre-
tation of m again in sci+1.
The authors concentrate on the treatment of im-
plicit acceptance acts but suggest that the CA prin-
ciple can be seen as a general means of context-
dependent interpretation. This principle opens up
the question of how to find the appropriate tacit di-
alogue acts. Finding them is an inference problem
that is addressed using special-purpose algorithms
in (Thomason et al, 2006), where the authors
present a unified architecture for both context-
dependent interpretation and context-dependent
generation. In Frolog, we investigate how this in-
ference process can be implemented using recent
tools from artificial intelligence planning.
The resulting framework naturally lends itself
to studying the pressing problem for current the-
ories of accommodation called missing accommo-
dation (Beaver and Zeevat, 2007). These theories
can neither explain why accommodation is some-
times easier and sometimes much more difficult,
nor how cases of missing accommodation relate to
clarification subdialogues in conversation. We re-
view what Frolog has to offer to the understanding
of accommodation in general and missing accom-
modation in particular in Section 3. But first, we
have to introduce Frolog and describe its compo-
nents, and we do so in Section 2.
2 The text-adventure game
Text-adventures are computer games that simulate
a physical environment which can be manipulated
by means of natural language requests. The game
provides feedback in the form of natural language
descriptions of the game world and of the results
of the players? actions.
Frolog is based on a previous text-adventure
called FrOz (Koller et al, 2004) and its design
is depicted in Figure 1. The architecture is or-
ganized in three natural language understanding
(NLU) modules and three natural language gener-
ation (NLG) modules, and the state of the interac-
tion is represented in two knowledge bases (KBs).
The two KBs codify, in Description Logic (Baader
et al, 2003), assertions and concepts relevant for a
given game scenario. The game KB represents the
true state of the game world, while the player KB
keeps track of the player?s beliefs about the game
world. Frolog?s modules are scenario-independent;
the player can play different game scenarios by
plugging in the different information resources
that constitute the scenario.
Frolog uses generic external tools for the most
heavy-loaded tasks (depicted in grey in Figure 1);
1
Open the chest
Grammar
and
LexiconsParsing
Reference
Resolution
KB Manager
Player KB
Game KB
Action
Execution
Accommodation
Action
Database
Content
Determination
Reference
Generation
Realization
The chest is open
Figure 1: Architecture of Frolog
namely, a generic parser and a generic realizer
for parsing and realization, an automated theorem
prover for knowledge base management, and ar-
tificial intelligence planners for implementing its
accommodating capabilities. The rest of the mod-
ules (depicted in white) were implemented by us
in Prolog and Java. Frolog?s interface shows the in-
teraction with the player, the input/output of each
module and the content of the KBs.
We now present Frolog?s modules in pairs of an
NLU module and its NLG counterpart; each pair
uses a particular kind of information resource and
has analogous input/output.
2.1 Parsing and Realization
The parsing and the realization modules use the
same linguistic resources, namely a reversible
grammar, a lemma lexicon and a morphological
lexicon represented in the XMG grammatical for-
malism (Crabbe? and Duchier, 2004). The XMG
grammar used specifies a Tree Adjoining Gram-
mar (TAG) of around 500 trees and integrates a
semantic dimension a` la (Gardent, 2008). An ex-
ample of the semantics associated with the player
input ?open the chest? is depicted in Figure 2.
NP
?
A = you
S
NP? VP NP?
V
open N
open(E) chest
agent (E,A) chest(C)
patient(E,C)
NP
the NP*
det(C)
? ?
open(E), agent(E,you), patient(E,C), chest(C), det(C)
Figure 2: Parsing/realization for ?open the chest?
The parsing module performs the syntactic
analysis of a command issued by the player, and
constructs its semantic representation using the
TAG parser Tulipa (Kallmeyer et al, 2008) (illus-
trated in the Figure 2 by ?). The realization mod-
ule works in the opposite direction, verbalizing the
results of the execution of the command from the
semantic representation using the TAG surface re-
alizer GenI (Gardent and Kow, 2007) (illustrated
in the Figure 2 by ?).
2.2 Reference Resolution and Reference
Generation
The reference resolution (RR) module is respon-
sible for mapping the semantic representations of
definite and indefinite noun phrases and pronouns
to individuals in the knowledge bases (illustrated
in Figure 3 by ?). The reference generation (RG)
module performs the inverse task, that is it gener-
ates the semantic representation of a noun phrase
that uniquely identifies an individual in the knowl-
edge bases (illustrated in the Figure 3 by ?). The
algorithms used for RR and RG are described
in (Koller et al, 2004).
det(C), chest(C), little(C), has-location(C,T), table(T)
? ?
little
chest

table
little
chest
big
chest
has-location
has-lo
cation
Figure 3: RR/RG for ?the little chest on the table?
Frolog uses the theorem prover
RACER (Haarslev and Mo?ller, 2001) to query
the KBs and perform RR and RG. In order to
manage the ambiguity of referring expressions
two levels of saliency are considered. The player
KB is queried (instead of the game KB) naturally
capturing the fact that the player will not refer to
individuals he doesn?t know about (even if they
exist in the game KB). Among the objects that the
player already knows, a second level of saliency is
modelled employing a simple stack of discourse
referents which keeps track of the most recently
referred individuals. A new individual gets into
the player KB when the player explores the world.
2
2.3 Action Execution and Content
Determination
These two last modules share the last information
resource that constitute an scenario, namely, the
action database. The action database includes the
definitions of the actions that can be executed by
the player (such as take or open). Each action is
specified as a STRIPS-like operator (Fikes et al,
1972) detailing its arguments, preconditions and
effects as illustrated below. The arguments show
the thematic roles of the verb (for instance, the
verb open requires a patient and an agent), the pre-
conditions indicate the conditions that the game
world must satisfy so that the action can be exe-
cuted (for instance, in order to open the chest, it
has to be accessible, unlocked and closed); the ef-
fects determine how the action changes the game
world when it is executed (after opening the chest,
it will be open).
action: open(E) agent(E,A) patient(E,P)
preconditions: accessible(P), not(locked(P)), closed(P)
effects: opened(P)
Executing a player?s command amounts to ver-
ifying whether the preconditions of the actions in-
volved by the command hold in the game world
and, if they do, changing the game KB according
to the effects. After the command is executed, the
content determination module constructs the se-
mantic representation of the effects that were ap-
plied, updates the player KB with it and passes it
to the next module for its verbalization (so that the
player knows what changed in the world). For our
running example the following modules will ver-
balize ?the chest is open? closing a complete cycle
of the system as illustrated in Figure 1.
If a precondition of an action does not hold then
Frolog tries to accommodate as we will explain in
following section.
3 Accommodation in Frolog
In the previous section we presented the execu-
tion of the system when everything ?goes well?,
that is (to come back to the terminology used
in Section 1) when the assignment of a context-
dependent interpretation to the player?s move suc-
ceeds. However, during the interaction with Frolog,
it often happens that the player issues a command
that cannot be directly executed in the current state
of the game but needs accommodation or clarifica-
tion. This is the topic of the next two subsections.
3.1 Tacit acts are inferable and executable:
accommodation succeeds
Suppose that the player has just locked the little
chest and left its key on the table when she real-
izes that she forgot to take the sword from it, so
she utters ?open the chest?. If Frolog is in its non-
accommodating mode then it answers ?the chest
is locked? because the precondition not(locked(P))
does not hold in the game world. In this mode, the
interactions with the game can get quite long and
repetitive as illustrated below.
Non-accommodating mode Accommodating mode
P: open the chest P: open the chest
F: the chest is locked F: the chest is open
P: unlock it
F: you don?t have the key
...
In its accommodating mode, Frolog tries to ac-
commodate the current state sci of the game to a
new state sci+1 in which the precondition hold, by
assuming tacit dialogue acts performed, and starts
the interpretation of the command again in sci+1.
That is, the game assumes that ?take the key and
unlock the chest with it? are tacit acts that are per-
formed when the player says ?open the chest?.
The inference of such tacit dialogue acts is done
using artificial intelligence planners. The planning
problems are generated on the fly during a game
each time a precondition does not hold; the ini-
tial state being the player KB, the goal being the
precondition that failed, and the action schemas
those actions available in the action database. The
size of the plans can be configured, when the
length is zero we say that Frolog is in its non-
accommodating mode. For detailed discussion
of the subtleties involved in the kind of infor-
mation that has to be used to infer the tacit acts
see (Benotti, 2007).
Two planners have been integrated in Frolog
(the player can decide which one to use): Black-
box (Kautz and Selman, 1999) which is fast
and deterministic and PKS (Petrick and Bacchus,
2004) which can reason over non-deterministic
actions. For detailed discussion and examples
including non-deterministic actions see (Benotti,
2008).
3.2 Accommodation fails: clarification starts
Tacit acts are inferred using the information avail-
able to the player (the player KB) but their exe-
cution is verified with respect to the accurate and
complete state of the world (the game KB). So
3
Frolog distinguishes three ways in which accom-
modation can fail: there is no plan, there is more
than one plan, or there is a plan which is not ex-
ecutable in the game world. For reasons of space
we will only illustrate the last case here.
Suppose that the golden key, which was lying
on the table, was taken by a thief without the
player knowing. As a consequence, the key is on
the table in the player KB, but in the game KB
the thief has it. In this situation, the player issues
the command ?Open the chest? and the sequence
of tacit acts inferred (given the player beliefs) is
?take the key from the table and unlock the chest
with it?. When trying to execute the tacit acts,
the game finds the precondition that does not hold
and verbalizes it with ?the key is not on the table,
you don?t know where it is?. Such answer can be
seen as a clarification request (CR), it has the ef-
fect of assigning to the player the responsability
of finding the key before trying to open the chest.
The same responsability that would be assigned by
more commonly used CR that can happen in this
scenario, namely ?Where is the key??.
In the game, such clarifications vary according
to the knowledge that is currently available to the
player. If the player knows that the dragon has the
key and she can only take it while the dragon is
asleep an answer such as ?the dragon is not sleep-
ing? is generated in the same fashion.
4 Conclusion and future work
In this paper we have presented a text-adventure
game which is an interesting test-bed for experi-
menting with accommodation. The text-adventure
framework makes evident the strong relation be-
tween accommodation and clarification (which is
not commonly studied), highlighting the impor-
tance of investigating accommodation in dialogue
and not in isolation.
Our work is in its early stages and can be ad-
vanced in many directions. We are particularly in-
terested in modifying the architecture of the sys-
tem in order to model reference as another action
instead of preprocessing references with special-
purpose algorithms. In this way we would not
only obtain a more elegant architecture, but also
be able to investigate the interactions between ref-
erence and other kinds of actions, which occur in
every-day conversations.
References
F. Baader, D. Calvanese, D. McGuinness, D. Nardi, and
P. Patel-Schneider. 2003. The Description Logic
Handbook: Theory, Implementation, and Applica-
tions. Cambridge University Press.
D. Beaver and H. Zeevat. 2007. Accommodation.
In The Oxford Handbook of Linguistic Interfaces,
pages 503?539. Oxford University Press.
L. Benotti. 2007. Incomplete knowledge and tacit ac-
tion: Enlightened update in a dialogue game. In
Proc. of DECALOG, pages 17?24.
L. Benotti. 2008. Accommodation through tacit sens-
ing. In Proc. of LONDIAL, pages 75?82.
B. Crabbe? and D. Duchier. 2004. Metagrammar redux.
In Proc. of CSLP04.
R. Fikes, P. Hart, and N. Nilsson. 1972. Learning and
executing generalized robot plans. AI, 3:251?288.
C. Gardent and E. Kow. 2007. A symbolic approach to
near-deterministic surface realisation using tree ad-
joining grammar. In Proc. of ACL07.
C. Gardent. 2008. Integrating a unification-based se-
mantics in a large scale lexicalised tree adjoininig
grammar for french. In Proc. of COLING08.
V. Haarslev and R. Mo?ller. 2001. RACER system
description. In Proc. of IJCAR01, number 2083 in
LNAI, pages 701?705.
L. Kallmeyer, T. Lichte, W. Maier, Y. Parmentier,
J. Dellert, and K. Evang. 2008. TuLiPA: Towards
a multi-formalism parsing environment for grammar
engineering. In Proc. of the WGEAF08.
H. Kautz and B. Selman. 1999. Unifying SAT-based
and graph-based planning. In Proc. of IJCAI99,
pages 318?325.
A. Koller, R. Debusmann, M. Gabsdil, and K. Strieg-
nitz. 2004. Put my galakmid coin into the dispenser
and kick it: Computational linguistics and theorem
proving in a computer game. JoLLI, 13(2):187?206.
J. Kreutel and C. Matheson. 2003. Context-dependent
interpretation and implicit dialogue acts. In Perspec-
tives on Dialogue in the New Millenium, pages 179?
192. John Benjamins.
D. Lewis. 1979. Scorekeeping in a language game.
Philosophical Logic, 8:339?359.
R. Petrick and F. Bacchus. 2004. Extending the
knowledge-based approach to planning with incom-
plete information and sensing. In Proc. of ICP-
KRR04, pages 613?622.
R. Thomason, M. Stone, and D. DeVault. 2006. En-
lightened update: A computational architecture for
presupposition and other pragmatic phenomena. In
Proc. of Workshop on Presup. Accommodation.
4
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 62?67,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
Prototyping virtual instructors from human-human corpora
Luciana Benotti
PLN Group, FAMAF
National University of Co?rdoba
Co?rdoba, Argentina
luciana.benotti@gmail.com
Alexandre Denis
TALARIS team, LORIA/CNRS
Lorraine. Campus scientifique, BP 239
Vandoeuvre-le`s-Nancy, France
alexandre.denis@loria.fr
Abstract
Virtual instructors can be used in several ap-
plications, ranging from trainers in simulated
worlds to non player characters for virtual
games. In this paper we present a novel
algorithm for rapidly prototyping virtual in-
structors from human-human corpora without
manual annotation. Automatically prototyp-
ing full-fledged dialogue systems from cor-
pora is far from being a reality nowadays. Our
algorithm is restricted in that only the virtual
instructor can perform speech acts while the
user responses are limited to physical actions
in the virtual world. We evaluate a virtual in-
structor, generated using this algorithm, with
human users. We compare our results both
with human instructors and rule-based virtual
instructors hand-coded for the same task.
1 Introduction
Virtual human characters constitute a promising
contribution to many fields, including simulation,
training and interactive games (Kenny et al, 2007;
Jan et al, 2009). The ability to communicate using
natural language is important for believable and ef-
fective virtual humans. Such ability has to be good
enough to engage the trainee or the gamer in the ac-
tivity. Nowadays, most conversational systems oper-
ate on a dialogue-act level and require extensive an-
notation efforts in order to be fit for their task (Rieser
and Lemon, 2010). Semantic annotation and rule
authoring have long been known as bottlenecks for
developing conversational systems for new domains.
In this paper, we present novel a algorithm for
generating virtual instructors from automatically an-
notated human-human corpora. Our algorithm,
when given a task-based corpus situated in a virtual
world, generates an instructor that robustly helps a
user achieve a given task in the virtual world of the
corpus. There are two main approaches toward au-
tomatically producing dialogue utterances. One is
the selection approach, in which the task is to pick
the appropriate output from a corpus of possible out-
puts. The other is the generation approach, in which
the output is dynamically assembled using some
composition procedure, e.g. grammar rules. The se-
lection approach to generation has only been used
in conversational systems that are not task-oriented
such as negotiating agents (Gandhe and Traum,
2007), question answering characters (Kenny et al,
2007), and virtual patients (Leuski et al, 2006). Our
algorithm can be seen as a novel way of doing robust
generation by selection and interaction management
for task-oriented systems.
In the next section we introduce the corpora used
in this paper. Section 3 presents the two phases of
our algorithm, namely automatic annotation and di-
alogue management through selection. In Section 4
we present a fragment of an interaction with a vir-
tual instructor generated using the corpus and the
algorithm introduced in the previous sections. We
evaluate the virtual instructor in interactions with
human subjects using objective as well as subjec-
tive metrics. We present the results of the evaluation
in Section 5. We compare our results with both hu-
man and rule-based virtual instructors hand-coded
for the same task. Finally, Section 6 concludes the
paper proposing an improved virtual instructor de-
signed as a result of our error analysis.
62
2 The GIVE corpus
The Challenge on Generating Instructions in Vir-
tual Environments (GIVE; Koller et al (2010)) is
a shared task in which Natural Language Gener-
ation systems must generate real-time instructions
that guide a user in a virtual world. In this paper, we
use the GIVE-2 Corpus (Gargett et al, 2010), a cor-
pus of human instruction giving in virtual environ-
ments. We use the English part of the corpus which
consists of 63 American English written discourses
in which one subject guided another in a treasure
hunting task in 3 different 3D worlds.
The task setup involved pairs of human partners,
each of whom played one of two different roles. The
?direction follower? (DF) moved about in the vir-
tual world with the goal of completing a treasure
hunting task, but had no knowledge of the map of
the world or the specific behavior of objects within
that world (such as, which buttons to press to open
doors). The other partner acted as the ?direction
giver? (DG), who was given complete knowledge of
the world and had to give instructions to the DF to
guide him/her to accomplish the task.
The GIVE-2 corpus is a multimodal corpus which
consists of all the instructions uttered by the DG, and
all the object manipulations done by the DF with the
corresponding timestamp. Furthermore, the DF?s
position and orientation is logged every 200 mil-
liseconds, making it possible to extract information
about his/her movements.
3 The unsupervised conversational model
Our algorithm consists of two phases: an annotation
phase and a selection phase. The annotation phase
is performed only once and consists of automatically
associating the DG instruction to the DF reaction.
The selection phase is performed every time the vir-
tual instructor generates an instruction and consists
of picking out from the annotated corpus the most
appropriate instruction at a given point.
3.1 The automatic annotation
The basic idea of the annotation is straightforward:
associate each utterance with its corresponding re-
action. We assume that a reaction captures the se-
mantics of its associated instruction. Defining re-
action involves two subtle issues, namely boundary
determination and discretization. We discuss these
issues in turn and then give a formal definition of
reaction.
We define the boundaries of a reaction as follows.
A reaction rk to an instruction uk begins right af-
ter the instruction uk is uttered and ends right before
the next instruction uk+1 is uttered. In the follow-
ing example, instruction 1 corresponds to the reac-
tion ?2, 3, 4?, instruction 5 corresponds to ?6?, and
instruction 7 to ?8?.
DG(1): hit the red you see in the far room
DF(2): [enters the far room]
DF(3): [pushes the red button]
DF(4): [turns right]
DG(5): hit far side green
DF(6): [moves next to the wrong green]
DG(7): no
DF(8): [moves to the right green and pushes it]
As the example shows, our definition of bound-
aries is not always semantically correct. For in-
stance, it can be argued that it includes too much
because 4 is not strictly part of the semantics of 1.
Furthermore, misinterpreted instructions (as 5) and
corrections (e.g., 7) result in clearly inappropriate
instruction-reaction associations. Since we want to
avoid any manual annotation, we decided to use this
naive definition of boundaries anyway. We discuss
in Section 5 the impact that inappropriate associa-
tions have on the performance of a virtual instructor.
The second issue that we address here is dis-
cretization of the reaction. It is well known that there
is not a unique way to discretize an action into sub-
actions. For example, we could decompose action 2
into ?enter the room? or into ?get close to the door
and pass the door?. Our algorithm is not dependent
on a particular discretization. However, the same
discretization mechanism used for annotation has to
be used during selection, for the dialogue manager
to work properly. For selection (i.e., in order to de-
cide what to say next) any virtual instructor needs
to have a planner and a planning domain represen-
tation, i.e., a specification of how the virtual world
works and a way to represent the state of the virtual
world. Therefore, we decided to use them in order
to discretize the reaction.
Now we are ready to define reaction formally. Let
Sk be the state of the virtual world when uttering in-
63
struction uk, Sk+1 be the state of the world when
uttering the next utterance uk+1 and D be the plan-
ning domain representation. The reaction to uk is
defined as the sequence of actions returned by the
planner with Sk as initial state, Sk+1 as goal state
and D as planning domain.
The annotation of the corpus then consists of au-
tomatically associating each utterance to its (dis-
cretized) reaction.
3.2 Selecting what to say next
In this section we describe how the selection phase is
performed every time the virtual instructor generates
an instruction.
The instruction selection algorithm consists in
finding in the corpus the set of candidate utterances
C for the current task plan P ; P being the se-
quence of actions returned by the same planner and
planning domain used for discretization. We define
C = {U ? Corpus | U.Reaction is a prefix of P}.
In other words, an utterance U belongs to C if the
first actions of the current plan P exactly match the
reaction associated to the utterance. All the utter-
ances that pass this test are considered paraphrases
and hence suitable in the current context.
While P does not change, the virtual instructor
iterates through the set C, verbalizing a different ut-
terance at fixed time intervals (e.g., every 3 seconds).
In other words, the virtual instructor offers alterna-
tive paraphrases of the intended instruction. When
P changes as a result of the actions of the DF, C is
recalculated.
It is important to notice that the discretization
used for annotation and selection directly impacts
the behavior of the virtual instructor. It is crucial
then to find an appropriate granularity of the dis-
cretization. If the granularity is too coarse, many
instructions in the corpus will have an empty asso-
ciated reaction. For instance, in the absence of the
representation of the user orientation in the planning
domain (as is the case for the virtual instructor we
evaluate in Section 5), instructions like ?turn left?
and ?turn right? will have empty reactions making
them indistinguishable during selection. However,
if the granularity is too fine the user may get into sit-
uations that do not occur in the corpus, causing the
selection algorithm to return an empty set of candi-
date utterances. It is the responsibility of the virtual
instructor developer to find a granularity sufficient
to capture the diversity of the instructions he wants
to distinguish during selection.
4 A virtual instructor for a virtual world
We implemented an English virtual instructor for
one of the worlds used in the corpus collection we
presented in Section 2. The English fragment of the
corpus that we used has 21 interactions and a total
of 1136 instructions. Games consisted on average
of 54.2 instructions from the human DG, and took
about 543 seconds on average for the human DF to
complete the task.
On Figures 1 to 4 we show an excerpt of an in-
teraction between the system and a real user that we
collected during the evaluation. The figures show a
2D map from top view and the 3D in-game view. In
Figure 1, the user, represented by a blue character,
has just entered the upper left room. He has to push
the button close to the chair. The first candidate ut-
terance selected is ?red closest to the chair in front of
you?. Notice that the referring expression uniquely
identifies the target object using the spatial proxim-
ity of the target to the chair. This referring expres-
sion is generated without any reasoning on the tar-
get distractors, just by considering the current state
of the task plan and the user position.
Figure 1: ?red closest to the chair in front of you?
After receiving the instruction the user gets closer
to the button as shown in Figure 2. As a result of the
new user position, a new task plan exists, the set of
candidate utterances is recalculated and the system
selects a new utterance, namely ?the closet one?.
The generation of the ellipsis of the button or the
64
Figure 2: ?the closet one?
Figure 3: ?good?
Figure 4: ?exit the way you entered?
chair is a direct consequence of the utterances nor-
mally said in the corpus at this stage of the task plan
(that is, when the user is about to manipulate this ob-
ject). From the point of view of referring expression
algorithms, the referring expression may not be op-
timal because it is over-specified (a pronoun would
be preferred as in ?click it?), Furthermore, the in-
struction contains a spelling error (?closet? instead
of ?closest?). In spite of this non optimality, the in-
struction led our user to execute the intended reac-
tion, namely pushing the button.
Right after the user clicks on the button (Figure 3),
the system selects an utterance corresponding to the
new task plan. The player position stayed the same
so the only change in the plan is that the button no
longer needs to be pushed. In this task state, DGs
usually give acknowledgements and this then what
our selection algorithm selects: ?good?.
After receiving the acknowledgement, the user
turns around and walks forward, and the next action
in the plan is to leave the room (Figure 4). The sys-
tem selects the utterance ?exit the way you entered?
which refers to the previous interaction. Again, the
system keeps no representation of the past actions
of the user, but such utterances are the ones that are
found at this stage of the task plan.
5 Evaluation and error analysis
In this section we present the results of the evalu-
ation we carried out on the virtual instructor pre-
sented in Section 4 which was generated using the
dialogue model algorithm introduced in Section 3.
We collected data from 13 subjects. The partici-
pants were mostly graduate students; 7 female and
6 male. They were not English native speakers but
rated their English skills as near-native or very good.
The evaluation contains both objective measures
which we discuss in Section 5.1 and subjective mea-
sures which we discuss in Section 5.2.
5.1 Objective metrics
The objective metrics we extracted from the logs of
interaction are summarized in Table 1. The table
compares our results with both human instructors
and the three rule-based virtual instructors that were
top rated in the GIVE-2 Challenge. Their results cor-
respond to those published in (Koller et al, 2010)
which were collected not in a laboratory but con-
necting the systems to users over the Internet. These
hand-coded systems are called NA, NM and Saar.
We refer to our system as OUR.
65
Human NA Saar NM OUR
Task success 100% 47% 40% 30% 70%
Canceled 0% 24% n/a 35% 7%
Lost 0% 29% n/a 35% 23%
Time (sec) 543 344 467 435 692
Mouse actions 12 17 17 18 14
Utterances 53 224 244 244 194
Table 1: Results for the objective metrics
In the table we show the percentage of games that
users completed successfully with the different in-
structors. Unsuccessful games can be either can-
celed or lost. To ensure comparability, time until
task completion, number of instructions received by
users, and mouse actions are only counted on suc-
cessfully completed games.
In terms of task success, our system performs bet-
ter than all hand-coded systems. We duly notice that,
for the GIVE Challenge in particular (and proba-
bly for human evaluations in general) the success
rates in the laboratory tend to be higher than the suc-
cess rate online (this is also the case for completion
times) (Koller et al, 2009).
In any case, our results are preliminary given the
amount of subjects that we tested (13 versus around
290 for GIVE-2), but they are indeed encouraging.
In particular, our system helped users to identify bet-
ter the objects that they needed to manipulate in the
virtual world, as shown by the low number of mouse
actions required to complete the task (a high number
indicates that the user must have manipulated wrong
objects). This correlates with the subjective evalu-
ation of referring expression quality (see next sec-
tion).
We performed a detailed analysis of the instruc-
tions uttered by our system that were unsuccessful,
that is, all the instructions that did not cause the in-
tended reaction as annotated in the corpus. From the
2081 instructions uttered in the 13 interactions, 1304
(63%) of them were successful and 777 (37%) were
unsuccessful.
Given the limitations of the annotation discussed
in Section 3.1 (wrong annotation of correction ut-
terances and no representation of user orientation)
we classified the unsuccessful utterances using lexi-
cal cues into 1) correction (?no?,?don?t?,?keep?, etc.),
2) orientation instruction (?left?, ?straight?, ?behind?,
etc.) and 3) other. We found that 25% of the unsuc-
cessful utterances are of type 1, 40% are type 2, 34%
are type 3 (1% corresponds to the default utterance
?go? that our system utters when the set of candidate
utterances is empty). Frequently, these errors led to
contradictions confusing the player and significantly
affecting the completion time of the task as shown in
Table 1. In Section 6 we propose an improved virtual
instructor designed as a result of this error analysis.
5.2 Subjective metrics
The subjective measures were obtained from re-
sponses to the GIVE-2 questionnaire that was pre-
sented to users after each game. It asked users to rate
different statements about the system using a contin-
uous slider. The slider position was translated to a
number between -100 and 100. As done in GIVE-
2, for negative statements, we report the reversed
scores, so that in Tables 2 and 3 greater numbers
are always better. In this section we compare our re-
sults with the systems NA, Saar and NM as we did
in Section 5.1, we cannot compare against human in-
structors because these subjective metrics were not
collected in (Gargett et al, 2010).
The GIVE-2 Challenge questionnaire includes
twenty-two subjective metrics. Metrics Q1 to Q13
and Q22 assess the effectiveness and reliability of
instructions. For almost all of these metrics we got
similar or slightly lower results than those obtained
by the three hand-coded systems, except for three
metrics which we show in Table 2. We suspect that
the low results obtained for Q5 and Q22 relate to
the unsuccessful utterances identified and discussed
in Section 5.1. The high unexpected result in Q6 is
probably correlated with the low number of mouse
actions mentioned in Section 5.1.
NA Saar NM OUR
Q5: I was confused about which direction to go in
29 5 9 -12
Q6: I had no difficulty with identifying the objects the
system described for me
18 20 13 40
Q22: I felt I could trust the system?s instructions
37 21 23 0
Table 2: Results for the subjective measures assessing the
efficiency and effectiveness of the instructions
Metrics Q14 to Q20 are intended to assess the nat-
66
uralness of the instructions, as well as the immer-
sion and engagement of the interaction. As Table 3
shows, in spite of the unsuccessful utterances, our
system is rated as more natural and more engaging
(in general) than the best systems that competed in
the GIVE-2 Challenge.
NA Saar NM OUR
Q14: The system?s instructions sounded robotic
-4 5 -1 28
Q15: The system?s instructions were repetitive
-31 -26 -28 -8
Q16: I really wanted to find that trophy
-11 -7 -8 7
Q17: I lost track of time while solving the task
-16 -11 -18 16
Q18: I enjoyed solving the task
-8 -5 -4 4
Q19: Interacting with the system was really annoying
8 -2 -2 4
Q20: I would recommend this game to a friend
-30 -25 -24 -28
Table 3: Results for the subjective measures assessing the
naturalness and engagement of the instructions
6 Conclusions and future work
In this paper we presented a novel algorithm for
rapidly prototyping virtual instructors from human-
human corpora without manual annotation. Using
our algorithm and the GIVE corpus we have gener-
ated a virtual instructor1 for a game-like virtual en-
vironment. We obtained encouraging results in the
evaluation with human users that we did on the vir-
tual instructor. Our system outperforms rule-based
virtual instructors hand-coded for the same task both
in terms of objective and subjective metrics. It is
important to mention that the GIVE-2 hand-coded
systems do not need a corpus but are tightly linked
to the GIVE task. Our algorithm requires human-
human corpora collected on the target task and en-
vironment, but it is independent of the particular in-
struction giving task. For instance, it could be used
for implementing game tutorials, real world naviga-
tion systems or task-based language teaching.
In the near future we plan to build a new version
of the system that improves based on the error anal-
ysis that we did. For instance, we plan to change
1Demo at cs.famaf.unc.edu.ar/?luciana/give-OUR
our discretization mechanism in order to take orien-
tation into account. This is supported by our algo-
rithm although we may need to enlarge the corpus
we used so as not to increase the number of situa-
tions in which the system does not find anything to
say. Finally, if we could identify corrections auto-
matically, as suggested in (Raux and Nakano, 2010),
we could get another increase in performance, be-
cause we would be able to treat them as corrections
and not as instructions as we do now.
In sum, this paper presents a novel way of au-
tomatically prototyping task-oriented virtual agents
from corpora who are able to effectively and natu-
rally help a user complete a task in a virtual world.
References
Sudeep Gandhe and David Traum. 2007. Creating spo-
ken dialogue characters from corpora without annota-
tions. In Proceedings of Interspeech, Belgium.
Andrew Gargett, Konstantina Garoufi, Alexander Koller,
and Kristina Striegnitz. 2010. The GIVE-2 corpus of
giving instructions in virtual environments. In Proc. of
the LREC, Malta.
Dusan Jan, Antonio Roque, Anton Leuski, Jacki Morie,
and David Traum. 2009. A virtual tour guide for
virtual worlds. In Proc. of IVA, pages 372?378, The
Netherlands. Springer-Verlag.
Patrick Kenny, Thomas D. Parsons, Jonathan Gratch, An-
ton Leuski, and Albert A. Rizzo. 2007. Virtual pa-
tients for clinical therapist skills training. In Proc. of
IVA, pages 197?210, France. Springer-Verlag.
Alexander Koller, Kristina Striegnitz, Donna Byron, Jus-
tine Cassell, Robert Dale, Sara Dalzel-Job, Johanna
Moore, and Jon Oberlander. 2009. Validating the
web-based evaluation of nlg systems. In Proc. of ACL-
IJCNLP, Singapore.
Alexander Koller, Kristina Striegnitz, Andrew Gargett,
Donna Byron, Justine Cassell, Robert Dale, Johanna
Moore, and Jon Oberlander. 2010. Report on the sec-
ond challenge on generating instructions in virtual en-
vironments (GIVE-2). In Proc. of INLG, Dublin.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006. Building effective question
answering characters. In Proc. of SIGDIAL, pages 18?
27, Australia. ACL.
Antoine Raux and Mikio Nakano. 2010. The dynamics
of action corrections in situated interaction. In Proc.
of SIGDIAL, pages 165?174, Japan. ACL.
Verena Rieser and Oliver Lemon. 2010. Learning hu-
man multimodal dialogue strategies. Natural Lan-
guage Engineering, 16:3?23.
67
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 181?186,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Corpus-based interpretation of instructions in virtual environments
Luciana Benotti1 Mart??n Villalba1 Tessa Lau2 Julia?n Cerruti3
1 FaMAF, Medina Allende s/n, Universidad Nacional de Co?rdoba, Co?rdoba, Argentina
2IBM Research ? Almaden, 650 Harry Road, San Jose, CA 95120 USA
3IBM Argentina, Ing. Butty 275, C1001AFA, Buenos Aires, Argentina
{benotti,villalba}@famaf.unc.edu.ar, tessalau@us.ibm.com, jcerruti@ar.ibm.com
Abstract
Previous approaches to instruction interpre-
tation have required either extensive domain
adaptation or manually annotated corpora.
This paper presents a novel approach to in-
struction interpretation that leverages a large
amount of unannotated, easy-to-collect data
from humans interacting with a virtual world.
We compare several algorithms for automat-
ically segmenting and discretizing this data
into (utterance, reaction) pairs and training a
classifier to predict reactions given the next ut-
terance. Our empirical analysis shows that the
best algorithm achieves 70% accuracy on this
task, with no manual annotation required.
1 Introduction and motivation
Mapping instructions into automatically executable
actions would enable the creation of natural lan-
guage interfaces to many applications (Lau et al,
2009; Branavan et al, 2009; Orkin and Roy, 2009).
In this paper, we focus on the task of navigation and
manipulation of a virtual environment (Vogel and
Jurafsky, 2010; Chen and Mooney, 2011).
Current symbolic approaches to the problem are
brittle to the natural language variation present in in-
structions and require intensive rule authoring to be
fit for a new task (Dzikovska et al, 2008). Current
statistical approaches require extensive manual an-
notations of the corpora used for training (MacMa-
hon et al, 2006; Matuszek et al, 2010; Gorniak and
Roy, 2007; Rieser and Lemon, 2010). Manual anno-
tation and rule authoring by natural language engi-
neering experts are bottlenecks for developing con-
versational systems for new domains.
This paper proposes a fully automated approach
to interpreting natural language instructions to com-
plete a task in a virtual world based on unsupervised
recordings of human-human interactions perform-
ing that task in that virtual world. Given unanno-
tated corpora collected from humans following other
humans? instructions, our system automatically seg-
ments the corpus into labeled training data for a clas-
sification algorithm. Our interpretation algorithm is
based on the observation that similar instructions ut-
tered in similar contexts should lead to similar ac-
tions being taken in the virtual world. Given a previ-
ously unseen instruction, our system outputs actions
that can be directly executed in the virtual world,
based on what humans did when given similar in-
structions in the past.
2 Corpora situated in virtual worlds
Our environment consists of six virtual worlds de-
signed for the natural language generation shared
task known as the GIVE Challenge (Koller et al,
2010), where a pair of partners must collaborate to
solve a task in a 3D space (Figure 1). The ?instruc-
tion follower? (IF) can move around in the virtual
world, but has no knowledge of the task. The ?in-
struction giver? (IG) types instructions to the IF in
order to guide him to accomplish the task. Each cor-
pus contains the IF?s actions and position recorded
every 200 milliseconds, as well as the IG?s instruc-
tions with their timestamps.
We used two corpora for our experiments. The
Cm corpus (Gargett et al, 2010) contains instruc-
tions given by multiple people, consisting of 37
games spanning 2163 instructions over 8:17 hs. The
181
Figure 1: A screenshot of a virtual world. The world
consists of interconnecting hallways, rooms and objects
Cs corpus (Benotti and Denis, 2011), gathered using
a single IG, is composed of 63 games and 3417 in-
structions, and was recorded in a span of 6:09 hs. It
took less than 15 hours to collect the corpora through
the web and the subjects reported that the experi-
ment was fun.
While the environment is restricted, people de-
scribe the same route and the same objects in ex-
tremely different ways. Below are some examples of
instructions from our corpus all given for the same
route shown in Figure 1.
1) out
2) walk down the passage
3) nowgo [sic] to the pink room
4) back to the room with the plant
5) Go through the door on the left
6) go through opening with yellow wall paper
People describe routes using landmarks (4) or
specific actions (2). They may describe the same
object differently (5 vs 6). Instructions also differ
in their scope (3 vs 1). Thus, even ignoring spelling
and grammatical errors, navigation instructions con-
tain considerable variation which makes interpreting
them a challenging problem.
3 Learning from previous interpretations
Our algorithm consists of two phases: annotation
and interpretation. Annotation is performed only
once and consists of automatically associating each
IG instruction to an IF reaction. Interpretation is
performed every time the system receives an instruc-
tion and consists of predicting an appropriate reac-
tion given reactions observed in the corpus.
Our method is based on the assumption that a re-
action captures the semantics of the instruction that
caused it. Therefore, if two utterances result in the
same reaction, they are paraphrases of each other,
and similar utterances should generate the same re-
action. This approach enables us to predict reactions
for previously-unseen instructions.
3.1 Annotation phase
The key challenge in learning from massive amounts
of easily-collected data is to automatically annotate
an unannotated corpus. Our annotation method con-
sists of two parts: first, segmenting a low-level in-
teraction trace into utterances and corresponding re-
actions, and second, discretizing those reactions into
canonical action sequences.
Segmentation enables our algorithm to learn from
traces of IFs interacting directly with a virtual world.
Since the IF can move freely in the virtual world, his
actions are a stream of continuous behavior. Seg-
mentation divides these traces into reactions that fol-
low from each utterance of the IG. Consider the fol-
lowing example starting at the situation shown in
Figure 1:
IG(1): go through the yellow opening
IF(2): [walks out of the room]
IF(3): [turns left at the intersection]
IF(4): [enters the room with the sofa]
IG(5): stop
It is not clear whether the IF is doing ?3, 4? be-
cause he is reacting to 1 or because he is being
proactive. While one could manually annotate this
data to remove extraneous actions, our goal is to de-
velop automated solutions that enable learning from
massive amounts of data.
We decided to approach this problem by experi-
menting with two alternative formal definitions: 1) a
strict definition that considers the maximum reaction
according to the IF behavior, and 2) a loose defini-
tion based on the empirical observation that, in sit-
uated interaction, most instructions are constrained
by the current visually perceived affordances (Gib-
son, 1979; Stoia et al, 2006).
We formally define behavior segmentation (Bhv)
as follows. A reaction rk to an instruction uk begins
182
right after the instruction uk is uttered and ends right
before the next instruction uk+1 is uttered. In the
example, instruction 1 corresponds to ?2, 3, 4?. We
formally define visibility segmentation (Vis) as fol-
lows. A reaction rk to an instruction uk begins right
after the instruction uk is uttered and ends right be-
fore the next instruction uk+1 is uttered or right after
the IF leaves the area visible at 360? from where uk
was uttered. In the example, instruction 1?s reaction
would be limited to ?2? because the intersection is
not visible from where the instruction was uttered.
The Bhv and Vis methods define how to segment
an interaction trace into utterances and their corre-
sponding reactions. However, users frequently per-
form noisy behavior that is irrelevant to the goal of
the task. For example, after hearing an instruction,
an IF might go into the wrong room, realize the er-
ror, and leave the room. A reaction should not in-
clude such irrelevant actions. In addition, IFs may
accomplish the same goal using different behaviors:
two different IFs may interpret ?go to the pink room?
by following different paths to the same destination.
We would like to be able to generalize both reactions
into one canonical reaction.
As a result, our approach discretizes reactions into
higher-level action sequences with less noise and
less variation. Our discretization algorithm uses an
automated planner and a planning representation of
the task. This planning representation includes: (1)
the task goal, (2) the actions which can be taken in
the virtual world, and (3) the current state of the
virtual world. Using the planning representation,
the planner calculates an optimal path between the
starting and ending states of the reaction, eliminat-
ing all unnecessary actions. While we use the clas-
sical planner FF (Hoffmann, 2003), our technique
could also work with classical planning (Nau et al,
2004) or other techniques such as probabilistic plan-
ning (Bonet and Geffner, 2005). It is also not de-
pendent on a particular discretization of the world in
terms of actions.
Now we are ready to define canonical reaction ck
formally. Let Sk be the state of the virtual world
when instruction uk was uttered, Sk+1 be the state of
the world where the reaction ends (as defined by Bhv
or Vis segmentation), and D be the planning domain
representation of the virtual world. The canonical
reaction to uk is defined as the sequence of actions
returned by the planner with Sk as initial state, Sk+1
as goal state and D as planning domain.
3.2 Interpretation phase
The annotation phase results in a collection of (uk,
ck) pairs. The interpretation phase uses these pairs to
interpret new utterances in three steps. First, we fil-
ter the set of pairs into those whose reactions can be
directly executed from the current IF position. Sec-
ond, we group the filtered pairs according to their
reactions. Third, we select the group with utterances
most similar to the new utterance, and output that
group?s reaction. Figure 2 shows the output of the
first two steps: three groups of pairs whose reactions
can all be executed from the IF?s current position.
Figure 2: Utterance groups for this situation. Colored
arrows show the reaction associated with each group.
We treat the third step, selecting the most similar
group for a new utterance, as a classification prob-
lem. We compare three different classification meth-
ods. One method uses nearest-neighbor classifica-
tion with three different similarity metrics: Jaccard
and Overlap coefficients (both of which measure the
degree of overlap between two sets, differing only
in the normalization of the final value (Nikravesh et
al., 2005)), and Levenshtein Distance (a string met-
ric for measuring the amount of differences between
two sequences of words (Levenshtein, 1966)). Our
second classification method employs a strategy in
which we considered each group as a set of pos-
sible machine translations of our utterance, using
the BLEU measure (Papineni et al, 2002) to select
which group could be considered the best translation
of our utterance. Finally, we trained an SVM clas-
sifier (Cortes and Vapnik, 1995) using the unigrams
183
Corpus Cm Corpus Cs
Algorithm Bhv Vis Bhv Vis
Jaccard 47% 54% 54% 70%
Overlap 43% 53% 45% 60%
BLEU 44% 52% 54% 50%
SVM 33% 29% 45% 29%
Levenshtein 21% 20% 8% 17%
Table 1: Accuracy comparison between Cm and Cs for
Bhv and Vis segmentation
of each paraphrase and the position of the IF as fea-
tures, and setting their group as the output class us-
ing a libSVM wrapper (Chang and Lin, 2011).
When the system misinterprets an instruction we
use a similar approach to what people do in order
to overcome misunderstandings. If the system exe-
cutes an incorrect reaction, the IG can tell the system
to cancel its current interpretation and try again us-
ing a paraphrase, selecting a different reaction.
4 Evaluation
For the evaluation phase, we annotated both the Cm
and Cs corpora entirely, and then we split them in
an 80/20 proportion; the first 80% of data collected
in each virtual world was used for training, while
the remaining 20% was used for testing. For each
pair (uk, ck) in the testing set, we used our algorithm
to predict the reaction to the selected utterance, and
then compared this result against the automatically
annotated reaction. Table 1 shows the results.
Comparing the Bhv and Vis segmentation strate-
gies, Vis tends to obtain better results than Bhv. In
addition, accuracy on the Cs corpus was generally
higher than Cm. Given that Cs contained only one
IG, we believe this led to less variability in the in-
structions and less noise in the training data.
We evaluated the impact of user corrections by
simulating them using the existing corpus. In case
of a wrong response, the algorithm receives a second
utterance with the same reaction (a paraphrase of the
previous one). Then the new utterance is tested over
the same set of possible groups, except for the one
which was returned before. If the correct reaction
is not predicted after four tries, or there are no ut-
terances with the same reaction, the predictions are
registered as wrong. To measure the effects of user
corrections vs. without, we used a different evalu-
ation process for this algorithm: first, we split the
corpus in a 50/50 proportion, and then we moved
correctly predicted utterances from the testing set to-
wards training, until either there was nothing more
to learn or the training set reached 80% of the entire
corpus size.
As expected, user corrections significantly im-
prove accuracy, as shown in Figure 3. The worst
algorithm?s results improve linearly with each try,
while the best ones behave asymptotically, barely
improving after the second try. The best algorithm
reaches 92% with just one correction from the IG.
5 Discussion and future work
We presented an approach to instruction interpreta-
tion which learns from non-annotated logs of hu-
man behavior. Our empirical analysis shows that
our best algorithm achieves 70% accuracy on this
task, with no manual annotation required. When
corrections are added, accuracy goes up to 92%
for just one correction. We consider our results
promising since state of the art semi-unsupervised
approaches to instruction interpretation (Chen and
Mooney, 2011) reports a 55% accuracy on manually
segmented data.
We plan to compare our system?s performance
against human performance in comparable situa-
tions. Our informal observations of the GIVE cor-
pus indicate that humans often follow instructions
incorrectly, so our automated system?s performance
may be on par with human performance.
Although we have presented our approach in the
context of 3D virtual worlds, we believe our tech-
nique is also applicable to other domains such as the
web, video games, or Human Robot Interaction.
Figure 3: Accuracy values with corrections over Cs
184
References
Luciana Benotti and Alexandre Denis. 2011. CL system:
Giving instructions by corpus based selection. In Pro-
ceedings of the Generation Challenges Session at the
13th European Workshop on Natural Language Gener-
ation, pages 296?301, Nancy, France, September. As-
sociation for Computational Linguistics.
Blai Bonet and He?ctor Geffner. 2005. mGPT: a proba-
bilistic planner based on heuristic search. Journal of
Artificial Intelligence Research, 24:933?944.
S.R.K. Branavan, Harr Chen, Luke Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement learning for
mapping instructions to actions. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
82?90, Suntec, Singapore, August. Association for
Computational Linguistics.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1?
27:27. Software available at http://www.csie.
ntu.edu.tw/?cjlin/libsvm.
David L. Chen and Raymond J. Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In Proceedings of the 25th
AAAI Conference on Artificial Intelligence (AAAI-
2011), pages 859?865, August.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20:273?297.
Myroslava O. Dzikovska, James F. Allen, and Mary D.
Swift. 2008. Linking semantic and knowledge repre-
sentations in a multi-domain dialogue system. Journal
of Logic and Computation, 18:405?430, June.
Andrew Gargett, Konstantina Garoufi, Alexander Koller,
and Kristina Striegnitz. 2010. The GIVE-2 corpus
of giving instructions in virtual environments. In Pro-
ceedings of the 7th Conference on International Lan-
guage Resources and Evaluation (LREC), Malta.
James J. Gibson. 1979. The Ecological Approach to Vi-
sual Perception, volume 40. Houghton Mifflin.
Peter Gorniak and Deb Roy. 2007. Situated language
understanding as filtering perceived affordances. Cog-
nitive Science, 31(2):197?231.
Jo?rg Hoffmann. 2003. The Metric-FF planning sys-
tem: Translating ?ignoring delete lists? to numeric
state variables. Journal of Artificial Intelligence Re-
search (JAIR), 20:291?341.
Alexander Koller, Kristina Striegnitz, Andrew Gargett,
Donna Byron, Justine Cassell, Robert Dale, Johanna
Moore, and Jon Oberlander. 2010. Report on the sec-
ond challenge on generating instructions in virtual en-
vironments (GIVE-2). In Proceedings of the 6th In-
ternational Natural Language Generation Conference
(INLG), Dublin.
Tessa Lau, Clemens Drews, and Jeffrey Nichols. 2009.
Interpreting written how-to instructions. In Proceed-
ings of the 21st International Joint Conference on Ar-
tificial Intelligence, pages 1433?1438, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Techni-
cal Report 8.
Matt MacMahon, Brian Stankiewicz, and Benjamin
Kuipers. 2006. Walk the talk: connecting language,
knowledge, and action in route instructions. In Pro-
ceedings of the 21st National Conference on Artifi-
cial Intelligence - Volume 2, pages 1475?1482. AAAI
Press.
Cynthia Matuszek, Dieter Fox, and Karl Koscher. 2010.
Following directions using statistical machine trans-
lation. In Proceedings of the 5th ACM/IEEE inter-
national conference on Human-robot interaction, HRI
?10, pages 251?258, New York, NY, USA. ACM.
Dana Nau, Malik Ghallab, and Paolo Traverso. 2004.
Automated Planning: Theory & Practice. Morgan
Kaufmann Publishers Inc., California, USA.
Masoud Nikravesh, Tomohiro Takagi, Masanori Tajima,
Akiyoshi Shinmura, Ryosuke Ohgaya, Koji Taniguchi,
Kazuyosi Kawahara, Kouta Fukano, and Akiko
Aizawa. 2005. Soft computing for perception-based
decision processing and analysis: Web-based BISC-
DSS. In Masoud Nikravesh, Lotfi Zadeh, and Janusz
Kacprzyk, editors, Soft Computing for Information
Processing and Analysis, volume 164 of Studies in
Fuzziness and Soft Computing, chapter 4, pages 93?
188. Springer Berlin / Heidelberg.
Jeff Orkin and Deb Roy. 2009. Automatic learning
and generation of social behavior from collective hu-
man gameplay. In Proceedings of The 8th Interna-
tional Conference on Autonomous Agents and Mul-
tiagent SystemsVolume 1, volume 1, pages 385?392.
International Foundation for Autonomous Agents and
Multiagent Systems, International Foundation for Au-
tonomous Agents and Multiagent Systems.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Verena Rieser and Oliver Lemon. 2010. Learning hu-
man multimodal dialogue strategies. Natural Lan-
guage Engineering, 16:3?23.
Laura Stoia, Donna K. Byron, Darla Magdalene Shock-
ley, and Eric Fosler-Lussier. 2006. Sentence planning
185
for realtime navigational instructions. In Proceedings
of the Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers, NAACL-
Short ?06, pages 157?160, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Adam Vogel and Dan Jurafsky. 2010. Learning to fol-
low navigational directions. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 806?814, Stroudsburg,
PA, USA. Association for Computational Linguistics.
186
Proceedings of the NAACL HLT 2010 Young Investigators Workshop on Computational Approaches to Languages of the Americas,
pages 132?140, Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Dialogue Systems for Virtual Environments
Luciana Benotti, Paula Estrella, Carlos Areces
Grupo de Procesamiento de Lenguaje Natural (PLN)
Seccio?n de Ciencias de la Computacio?n
Facultad de Matema?tica, Astronom??a y F??sica (FaMAF)
Universidad Nacional de Co?rdoba, Argentina
Abstract
We present an on-going research project car-
ried out at the Universidad Nacional de Co?rdo-
ba in Argentina. This project investigates the-
oretical and practical research questions re-
lated to the development of a dialogue system
situated in a virtual environment. We describe
the PLN research group in which this project
is being developed and, in particular, we spell
out the areas of expertise of the authors. More-
over, we discuss relevant past, current and fu-
ture collaborations of the research group.
1 Introduction
The goal of this project is to implement a dialogue
system which automatically generates instructions
in order to help a user to fulfill a given task in a 3D
virtual environment. In this context, we will investi-
gate fundamental issues about human-computer in-
teraction. The expected results of the project can be
classified in three areas: pragmatics of interaction;
information representation and inference; and eval-
uation of dialogue systems. Once a working proto-
type is finished, we will adapt it to the specific task
of language learning, using the system as a virtual
language teacher. Our prototype will teach English
to native Spanish speakers. Hence, it will need to
understand and produce both languages.
Initially, we will investigate a model of unidirec-
tional linguistic interaction (i.e., linguistic informa-
tion flows only from the system to the user). In sub-
sequent stages, the model will be extended to allow
bidirectional language exchange. For example, the
user may ask clarifications to the system or redefine
the goal of the interaction.
The architecture of the envisioned dialogue sys-
tem presents both theoretical and practical chal-
lenges. On the theoretical side, heuristics are needed
in order to govern decisions such as what to say,
when, and how (given the current context). In addi-
tion, the system should implement inference meth-
ods in order to figure out how to modify the cur-
rent situation and reach the task goal. The complex-
ity of the theoretical issues is reflected, in practice,
in a system of multiple components: a natural lan-
guage generator, a planner, a 3D interactive envi-
ronment, to mention a few. Designing and imple-
menting all these components from scratch would
require a prohibitive effort. Instead we will adapt
tools already implemented and freely available for
prototyping this kind of systems, such as the plat-
form GIVE1, Generating Instructions in Virtual En-
vironments (Byron et al, 2009).
The quality of each of the components of the sys-
tem affects the perception users have of it. It is im-
perative to carry out extensive evaluation. We plan
to adapt and apply different evaluation techniques
and metrics from the area of Machine Translation to
assess the performance of the system.
The plan of the paper is as follows. Section 2
describes the project in detail. Section 3 spells out
the expected results as well as their foreseen impact
in the Argentinean socio-economic landscape. Sec-
tion 4 presents the PLN research group including its
lines of research. Section 5 discuss past, current and
future collaborations that are relevant to the project.
1http://www.give-challenge.org
132
2 Description of the Project
This section first introduces the virtual environ-
ment in which our dialogue system will be situated,
namely the GIVE platform, which is the basic ar-
chitecture of our dialogue system. Then we explain
in detail the tasks that our situated dialogue system
will implement, and we spell out the evaluation chal-
lenges that such a system poses. We close the sec-
tion discussing the application of our dialogue sys-
tem for the task of second language learning.
2.1 The Virtual Environment
In the scenario proposed by GIVE (Byron et al,
2009), a human user carries out a ?treasure hunt? in
a 3D virtual environment and the task of the genera-
tion system is to provide real-time, natural language
instructions that help the user find the hidden trea-
sure.
In the GIVE setup, the instruction giving system
must guide the user through interconnected rooms.
The final goal is to get a trophy which is hidden in
a safe. In order to achieve this goal, the system in-
structs the user to perform several subtasks such as
deactivating alarms and opening the safe combina-
tion by pressing a sequence of buttons on the walls
of the rooms.
Figure 1: The user?s view of the 3D world
Figure 1 shows a screen-shot of the user?s view on
the 3D world. On the top of the picture, the current
instruction generated by the dialogue system is dis-
played. The picture shows a closed door and an open
door that has an activated alarm (that looks like a red
tile) in the doorway. There are five visible buttons in
this room (two yellow, two red and one green) and
the instruction giver is instructing the user to press
a red button. Pressing a button can have different
effects such as opening a door, moving an object,
deactivating an alarm, etc.
The characteristics of the world, including the
functions of the buttons, are described in the world
specification by the world designers. The user can
move freely around the world (using the direction
keys as indicated in the bottom of the screen) but
she can loose the game if she triggers an alarm. The
user can also ask for help pressing ?H? if she did not
manage to read or understand the last instruction.
For the correct definition of the interaction poli-
cies of our prototype we need a corpus that pro-
vides examples of typical interactions in the domain.
GIVE provides tools for collecting such a corpus in
the form of a Wizard of Oz platform that records all
details of the interaction, thus allowing to easily ob-
tain a corpus of interaction in virtual environments
annotated automatically.
2.2 The Dialogue System Tasks
From the collected corpus we will begin the design,
implementation and testing of our dialogue system.
The main components that we will have to design
and implement can be organized using the tradi-
tional four tasks that a dialogue system should ad-
dress: (1) content planning, (2) generation of refer-
ring expressions, (3) management of the interaction
context, and (4) interpretation of user responses.
(1) Content Planning: Given the envisioned setup
we described before, the first task of the system is to
obtain a plan to reach the desired goal, from the cur-
rent state. The plan will contain physical actions to
be performed in the virtual environment. The second
step is to decide how to transmit this sequence of ac-
tions to the user. E.g, to decide how many actions
to communicate per instruction, and how to aggre-
gate them coherently. The result of the action ag-
gregation process can be represented as a tree de-
scribing the task structure at different levels of ab-
straction. The third and final step is to decide how
to navigate the tree of actions to verbalize the in-
structions (for example, post or preorder as explored
in (Foster et al, 2009)). We will investigate different
133
aggregation policies (e.g., aggregating actions that
manipulate similar objects) and innovative ways in
which to navigate the task tree (e.g., moving to a
lower level of abstraction in case of misunderstand-
ings). Plan computation can be solved using clas-
sical planners (Kautz and Selman, 1999; Hoffmann
and Nebel, 2001; Nau et al, 2004). However, while
there are planners that work well when optimized for
certain applications, none provides services such as
the generation of alternative plans, or the generation
of incomplete plans in case of the absence of plan.
One of the goals of the project is to design and im-
plement these extensions to classical planning algo-
rithms. We will also study the theoretical behavior
(e.g., complexity) of these new algorithms.
(2) Generation of Referring Expressions: Once
content planning is complete, the next step is to gen-
eration adequate referring expressions. This task
involves producing a phrase that describes a refer-
able entity so that the user can identify it (e.g., ?the
vase on the table?). To be acceptable, these expres-
sions should be ?natural:? they should be at the same
time sufficiently but not overly constrained, and they
should not impose on the user a heavier cognitive
load than necessary. For example, producing the
expression ?the vase that is not above the chair or
sofa or under the table? would probably not be ac-
ceptable. Areces et al (2008b) propose to use sym-
bolic minimization of the model that represents the
state of the world, in order to obtain a logical repre-
sentation that describe each object uniquely. In our
project we will implement this method and evaluate
it within the dialogue system.
(3) Management of the Interaction Context: To
manage the use of the interaction context we will use
existing knowledge maintenance systems such as
RACER2 or Pellet3, which support inference tasks
such as definition, maintenance and querying of on-
tologies. These systems have been used as infer-
ence engines in numerous applications in the area
and, in particular, in dialogue systems for text ad-
ventures (Benotti, 2009b). Once we have studied
the behavior of these inference engines on the task,
we will analyze its limitations and investigate the re-
quired extensions.
2http://www.racer-systems.com
3http://clarkparsia.com/pellet
(4) Interpretation of User Responses: The inter-
pretation of user responses in the unidirectional sys-
tem is relatively simple: it amounts to discretizing
the continuous flow of user behavior in the 3D world
into actions meaningful for the domain task. In a
first stage, we will use the discretizer provided by
GIVE. After evaluating it we can determine whether
or not this module meets the requirements of our
task and what are its limitations. In the bidirectional
system, however, the interpretation of user responses
is the task that will require more attention. To start
with, the bidirectional system should be expanded
with capabilities for processing statements coming
from the user (namely, parsing, semantic construc-
tion, resolution of references, etc.). We will study,
in particular, two types of user contributions: re-
quests for clarification of the instruction given (what
we call ?short-term repairs?), and for redefinition of
goals (what we call ?long-term repairs?). We will
implement short-term repairs using the approach de-
scribed in (Purver, 2006). For long-term repairs we
will use the guidelines of (Blaylock, 2005).
A sample interaction with the unidirectional sys-
tem guiding the player in the identification of a par-
ticular blue button is as follows:
(1) System says: Push a blue button.
The user focuses a blue button.
System says: Not this one.
Look for another one.
The user turns and focuses another blue button.
System says: Yes this one!
The user pushes the button.
This interaction illustrates the tasks described
above. To begin with, the verbalization of the in-
struction ?Push a blue button? is making explicit one
of the steps of the plan that needs to be performed in
order to achieve the task goal. As we can see, the
system implements in this case a referring strategy
which does not uniquely identify the referent (the
system generates ?a blue button? when there is more
than one blue button in the domain). But it is ca-
pable of producing further details about the referent
if the user focus in the wrong object. Finally, this
example makes evident that the interpretation of the
user responses is crucial even in a linguistically uni-
directional system. The user cannot make linguistic
134
contributions but can change the context by perform-
ing physical acts, the correct interpretation of such
acts is essential if the system is to react coherently.
2.3 Evaluation
To determine the quality of the obtained prototypes
we propose to create a quality model following the
ISO/IEC 9126 and 14528 standards for the evalua-
tion of software products (ISO/IEC, 2001; ISO/IEC,
1999). These standards were successfully applied
to the Machine Translation domain, resulting in the
FEMTI4, Framework for the Evaluation of Machine
Translation (Estrella et al, 2005). FEMTI guides
evaluators towards creating parameterized evalua-
tion plans that include various aspects of the to-be-
evaluated system and offer a relevant set of met-
rics. The identification of relevant metrics can be
performed using various methods, e.g., based on
previous experience (Hajdinjak and Mihelic, 2006;
Litman and Pan, 2002), conducting surveys or re-
quirement specifications (Lecoeuche et al, 1998), or
collecting such data through Wizard of Oz experi-
ments (Dahlba?ck et al, 1998). After developing a
quality model, several methodologies to assess vari-
ous aspects of the system can be applied: automatic
metrics, subjective metrics or metrics based on the
task (to evaluate both the contribution of each com-
ponent and the quality of the whole system).
The GIVE platform is used every year as a uni-
fied framework for evaluating generation systems.
Systems have to generate natural language instruc-
tions and be able to participate in a real-time interac-
tion situated in a 3D environment. The GIVE Chal-
lenge is one of the shared tasks endorsed by ACL?s
special interests groups in generation, dialogue and
semantics. We plan to participate in the challenge,
which will serve as an additional source of informa-
tion about aspects of the system that need improve-
ment. The evaluation metrics used in the Challenge
(such as average reference identification time) are
described in (Byron et al, 2009). In (Amoia et al,
2010) we extended such metrics in order to measure
alingment between system and user. Once the pro-
totype is evaluated and improved using the results of
the challenge, we will investigate its use as a virtual
language tutor as described in the next section.
4http://www.issco.unige.ch/femti/
2.4 An Application: A Virtual Tutor
The project outcome will be a system capable of giv-
ing natural language instructions situated in a virtual
3D environment. The technology and theoretical ad-
vances of the project could be used in various appli-
cations, but one of the most interesting character-
istics we plan to investigate is that, a priori, by just
changing the linguistic resources, the language of in-
teraction with the system (input and output) can be
changed as desired. After obtaining a first prototype
of an instruction giving dialogue system, we will in-
vestigate its use for distance learning, adapting the
system to operate as a foreign language tutor (Wik
and Hjalmarsson, 2009).
A one-way system that generates instructions in
English can be used to test the user understanding
of a foreign language. The correct interpretation of
the instructions can be evaluated from the proper ex-
ecution of the instructions. The two-way system
will allow the user to formulate clarifications (ei-
ther in their native language or in the foreign lan-
guage). The user may also redefine the objective to
be achieved during the interaction, and thus select
the type of vocabulary he wants to practice.
Virtual worlds (like Second Life) are being
rapidly incorporated into education, both initial
and superior (Doswell, 2005; Molka-Danielsen and
Deutschmann, 2009). The use of a virtual tutor
has certain advantages over a human tutor. Eng-
wall (2004) mentioned the following. (1) Amount
of practice: the chance to practice the new language
is essential for learning, and a virtual tutor provides
opportunities only limited by the technological re-
sources. (2) Prestige: a student may feel embar-
rassed about making mistakes with a human tutor,
and this might limit his willingness to speak in the
foreign language. (3) Augmented Reality: a virtual
tutor can provide additional material (e.g., examples
in context, explanatory images, etc.) with less effort
than a human tutor.
Such a virtual tutor can be used in distance learn-
ing. To develop distance learning systems, it is es-
sential to model the user?s learning progress. This
requires a system aware of the evolution of the user,
and that takes into account their achievements and
their problems. The system must be able to interpret
requirements, and generate appropriate responses,
135
for non-experts uses whose knowledge evolves dur-
ing the interaction. Moreover, the system must be
able to properly represent both the information con-
cerning the course material, and information about
the evolution of the user. For example, the system
must be able to diagnose what part of the course ma-
terial should be reviewed from the wrong answers of
the user. Finally, the system must be able to evaluate
the user interaction in order to decide which learning
objectives have been achieved. The theoretical and
practical results of the project contribute to solving
these difficult problems.
3 Impact of the Project
This project aims to achieve a balance between a
system which is sufficiently generic to be applica-
ble in different areas, and specific enough to ben-
efit from the efficient use of existing techniques
for knowledge management, planning and natural
language processing. Designing and implementing
such a system is a multidisciplinary effort leading to
research in diverse scientific areas:
Pragmatics is an interdisciplinary field which inte-
grates insights from linguistics (e.g., conversational
implicatures (Grice, 1975)), sociology (e.g., conver-
sational analysis (Schegloff, 1987)) and philosophy
(e.g., theory of speech acts (Austin, 1962)). It aims
to explore how the context (in which a conversation
is situated) contributes to the meaning (of everything
that is said during that conversation). The meaning
conveyed during a conversation depends not only on
linguistic information (entities in focus, grammati-
cal and morphological rules, etc.) but also on extra-
linguistic information (physical situation of conver-
sation, previous experiences of speakers, etc.). As a
result, the same sentence may mean different things
in different contexts. The area of pragmatics studies
the process by which a sentence is disambiguated
using its context. A dialogue system needs to have
pragmatic capabilities in order to interact in a nat-
ural way with its users. In particular, it must define
what kind of contextual information should be repre-
sented; and what inference tasks on a sentence and
context are necessary in order to interpret an utter-
ance. In such a system it is important that sentences
makes explicit the right amount of information: too
much information will delay and bore the user, but if
the information is not enough the user will not know
how to perform the task and make mistakes.
One of the major contributions of the project in
this area will be a virtual laboratory for pragmatic
theories: a controlled environment for studying in-
teraction set in a world where physical actions and
language intermingle. The prototype will let us in-
vestigate the impact that different instruction giving
policies (e.g., post order on the tree structure of the
task) have on successful achievement of the goal.
Similar studies have been done before (e.g., (Fos-
ter et al, 2009)) but they usually assume a prede-
termined task. Since our prototype allows for the
specification of the virtual world, the available ac-
tions, and the goal, we will be able to determine
when the impact associated to a particular policy is
dependent on the task or not. We will also investi-
gate short and long term repairs. Repairs are usu-
ally caused by conversational implicatures (Benotti,
2009a). Modeling these implicatures in a generic di-
alogue system is difficult because they are too open
ended. However, since the present prototype pro-
vides a situated interaction, restricted to the virtual
world, it will be possible to test the relationship be-
tween implicatures, the type of repairs they give rise
to, and the inference tasks needed to predict them.
Inference can be understood as any operation that
transforms implicit information in explicit informa-
tion. This definition is general enough to cover tasks
ranging from logical inference (i.e., deduction in
a formal language) to inference tasks common in
AI (e.g., planning and non-monotonic inference), as
well as statistical operations (e.g. obtaining estima-
tors on a data set). A dialogue system has to contin-
ually perform inference operations. E.g., inference
is needed to interpret information received from the
user, incorporate it to the system?s data repository,
and then decide what should be conveyed back to
the user. The very problem of deciding what kind
of logical representation and what type of inference
to use in a given situation is complex (propositional
logic vs. first-order logic, validity vs. model check-
ing, logical inference vs. statistical inference). Inde-
pendently of which type of inference is used, they
are usually computationally expensive. The chal-
lenge here is to find the appropriate balance between
the expressivity of the representation formalism and
136
the cost of the required inference methods.
The main contribution of the project in this area
is in the design, development and study of planning
algorithms. A typical planning system takes three
inputs ?initial state, possible actions and expected
goal? and returns a sequence of actions (a plan) that
when sequentially applied to the initial state, ends
in a state that satisfies the goal. Different methods
to obtain a plan have been studied (forward chain-
ing, backward chaining, coding in terms of proposi-
tional satisfiability, etc.), and they are currently im-
plemented in systems that can solve many planning
tasks efficiently. However, most of these systems
make assumptions that simplify the problem (deter-
ministic atomic time, complete information, absence
of a background theory, etc.). And most of them re-
turn a single plan. We will investigate algorithms
that eliminate some of these simplifications (in par-
ticular, we will study planning with incomplete in-
formation and based on a background theory). We
will also provide extended planning services: alter-
native plans, minimal plans, conditional plans, in-
complete plans, affordability of a given state, etc.
Evaluation of natural language generation systems
is one of the most difficult tasks in the area of NLP.
A given concept can be expressed in many different
ways, all of them correct. Hence, it is not possible
to determining the quality of a generated sentence
simply by, for example, comparing the result with a
gold standard. The problem of absence of gold stan-
dards is shared with another area of the NLP, namely
Machine Translation, for which various evaluation
methodologies, both direct and indirect, have been
proposed. Direct methods applies a metric to the
text generated by the system, while indirect meth-
ods evaluates the performance of the system through
the use of the generated text to perform some task.
But none of these methods is a standard and gener-
ally accepted methodology, which has been proven
to be effective in all cases. Since what is being eval-
uated in this project is a system that interacts via the
generation of natural language instructions, we can
determine its performance through quantitative met-
rics (e.g., average task completion time), qualitative
metrics (e.g., general user satisfaction) and metrics
based on the context (e.g., how well the system ad-
dressed the user needs in particular situations). We
will study the portability of evaluation techniques
from the domain of machine translation and multi-
modal human-computer interaction to the evaluation
of the system proposed in this project.
One of the main contributions of our project at
this respect is the integration of assessment tech-
niques from different areas into a methodology for
evaluating dialog systems for virtual environments,
aiming to estimate their usability and effectiveness.
This methodology could be used both to determine
whether a system is suitable for a task type and
user, and to compare the performance of different
systems of the same type. Another contribution
will be the study and application of software eval-
uation standards to the developed systems, creat-
ing a standardized quality model and proposing a
set of appropriate metrics to assess each of the as-
pects of the model. Finally, the annotated corpus
of human-human interaction, together with the cor-
pus of human-machine interaction collected during
the project will be made public. Such corpora will
serve, for example, to design more general platforms
for evaluating dialog systems, going beyond the as-
pects evaluated by existing platforms like GIVE.
Impact in the Argentinean Landscape: Natural
language processing, and in particular the field of
dialogue systems is a rapidly growing area in devel-
oped countries. The automatic processing of natu-
ral language has become a strategic capability for
companies and the wider community. However, this
area is extremely underdeveloped in Argentina. This
can be attributed to several factors. (a) The relative
youth of the area of NLP, which implies a relative
dearth of trained professionals throughout the world.
(b) The underdevelopment of the area of research in
Artificial Intelligence and Formal Linguistics in Ar-
gentina, for historical reasons and lack of industry
demand. (c) Poor interaction between the few re-
searchers in NLP that are in the region.
NLP is a strategic research area for Argentina
which can achieve academic excellence and indus-
try relevance. We believe in supporting the devel-
opment of this area by promoting the following.
(a) Training of human resources through doctoral
programs and courses taught in Argentina by in-
ternationally renowned professionals. (b) Incorpo-
ration of trained human resources to contribute to
137
the growth and diversification of the critical mass in
the area. (c) Improving interaction between various
groups and individual researchers in NLP, through
the organization of workshops, courses, visits, co-
tutoring, coordinated specialization programs, etc.
The particular topics investigated in the frame-
work of this project are of relevance in the current
Argentinean landscape for at least two reasons. On
the one hand, the project integrates and develops var-
ious key aspects of the area of computational lin-
guistics (syntax, semantics, pragmatics, representa-
tion, inference, evaluation); an area which, as we
mentioned, is today almost nonexistent in Argentina.
This project will be a step towards reversing this sit-
uation. On the other hand, the ultimate goal of the
project is to investigate the use of the developed plat-
form for distance education (specifically, as a tool
for language learning). Distance education is a valu-
able resource to overcome the problem of centraliza-
tion of educational resources in the country.
4 Introducing the Research Group
The PLN5 research group, in which the describe
scientific project will be carried out, was funded
in 2005. Te group is developing an important
role in human resource training, delivering courses
to undergraduate and postgraduate student at the
Universidad de Co?rdoba and other universities. It
also works in the development of various research
projects and integration with other groups in the re-
gion, both within Argentina and with neighboring
countries (Chile, Brazil and Uruguay).
The current project pools together many of the
key areas of expertise of the members of the group.
To begin with, some members of the group special-
ize in computational logic, particularly in the theo-
retical and applied study of languages for knowledge
representation (e.g., modal, hybrid and description
logics). They have also developed automated theo-
rem provers for these languages6. In relation with
the study of knowledge representation, they have
also investigated and developed algorithms for gen-
erating referring expressions (Areces et al, 2008b).
The second line of research of the PLN group that
is relevant for this project is context-based evalua-
5http://www.cs.famaf.unc.edu.ar/?pln
6http://www.glyc.dc.uba.ar/intohylo/
tion. Members of the group have proposed an eval-
uation model for machine translation systems which
relates the context of use to potentially important
quality characteristics (Estrella et al, 2008; Estrella
et al, 2009). This model is general enough to be
applied to other systems that produce natural lan-
guage like the ones proposed in this paper. Thanks to
the background on machine translation systems the
team has experience evaluating and comparing natu-
ral language output produced in different languages
(Spanish and English in particular), which will be
relevant for the development of the language tutor
described in Section 2.4. Finally, the team has ex-
perience developing and evaluating multimodal cor-
pora like those described in Section 2 (Estrella and
Popescu-Belis, 2008).
The third line of research that is relevant for this
project is pragmatics. In this area the team has im-
plemented a conversational agent which is able to
infer and negotiate conversational implicatures us-
ing inference tasks such as classical planning and
planning under incomplete information (Benotti,
2009b). We have also investigated how to infer
conversational implicatures triggered by compara-
tive utterances (Benotti and Traum, 2009). Recently
we have done corpus-based work, which shows what
kinds of implicatures are inferred and negotiated by
human dialogue participants during a task situated
in a 3D virtual environment (Benotti, 2009a).
Other lines of research in the PLN group are not
directly related to the project at this stage, but might
become relevant in the future. They include gram-
mar induction, text mining, statistical syntactic anal-
ysis and ontology population from raw text.
5 Ongoing and Future Collaborations
The members of the PLN in general and the authors
of this paper in particular have several collaborations
with national and international research groups in
computational linguistics and related fields that are
relevant for this project.
At the international level, we have ongoing col-
laboration with the TIM/ISSCO7 Multilingual In-
formation Processing Department at the University
of Geneva, with the Idiap Research Institute8 and
7http://www.issco.unige.ch/en
8http://www.idiap.ch
138
with some members of the PAI9, Pervasive Artifi-
cial Intelligence group of the University of Fribourg.
These collaborations include the evaluation of NLP
systems and the development of multilingual and
multimodal human language technology systems.
Members of the group have a long standing col-
laboration with the TALARIS10 group of the Labo-
ratoire Lorrain de Recherche en Informatique et ses
Applications (LORIA). The main research topic at
TALARIS is computational linguistics with strong
emphasis on semantics and inference. In the frame-
work of this collaboration we are participating in the
2010 edition of the GIVE Challenge. In the pro-
cess of designing the systems that will participate in
the challenge we jointly investigated the use of dif-
ferent referring strategies in situated instruction giv-
ing (Amoia et al, 2010).
We have also collaborated with the Virtual Hu-
mans group of the Institute for Creative Technolo-
gies11 from the University of Southern California.
In particular we computationally modeled the in-
ference of conversational implicatures triggered by
comparative utterances (Benotti and Traum, 2009).
The Institute for Creative Technologies offers In-
ternship programs every year that we plan to use in
order to strengthen our collaboration.
All these collaborations are directly related to the
main theme of the project described in this arti-
cle. The PLN group has also research collaborations
with other international research teams in the frame-
work of other scientific programs. For example, the
PLN group has being part of a recently finished in-
ternational project MICROBIO12 on ontology popu-
lation from raw text. The project was funded by the
Stic-Amsud13 program, a scientific-technological
cooperation program integrated by France, Argen-
tine, Brazil, Chile, Paraguay, Peru and Uruguay. The
expertise obtained during this project might be use-
ful in the future when trying to extend our GIVE on-
tologies to new domains. Similarly, the team main-
tain scientific relations with the University of Texas
at Austin (mainly with Dr. J. Moore in projects re-
9http://diuf.unifr.ch/pai/wiki
10http://talaris.loria.fr
11http://ict.usc.edu/projects/virtual_humans
12http://www.microbioamsud.net
13http://www.sticamsud.org
lated to the development of the ACL214 prover); and
with the Research team Symbiose15 of the Institut de
Recherche en Informatique et Syste?mes Ale?atoires
(working on the use of linguistic techniques for the
modelisation of genomic sequences).
At the national level, the group has inten-
sively collaborated with GLyC16, Grupo de Lo?gica,
Lenguaje y Computabilidad on knowledge represen-
tation and inference (see, e.g. (Areces and Gor??n,
2005; Areces et al, 2008a)). GLyC is part of the
Computer Science Department of the Universidad
de Buenos Aires. During 2010, teams PLN and
GLyC will join forces and collaborate in the organi-
zation of ELiC17, the First School in Computational
Linguistics in Argentina, which will take place in
July at the Universidad de Buenos Aires. ELiC 2010
will be co-located with the ECI18, Escuela de Cien-
cias Informa?ticas which has a long standing repu-
tation as a high-quality winter school in Computer
Science in Argentina, and is being organized yearly
since 1987. With ELiC we aim at creating, for the
first time, a space to introduce the field of computa-
tional linguistics to graduate students in Argentina.
Thanks to the support of the North American Chap-
ter of the Association for Computational Linguis-
tics (NAACL) and of the Universidad de Buenos
Aires, ELiC is offering student travel grants and fee
waivers to encourage participation.
The PLN group is also contacting other groups
working in computational linguistics in Argentina
like the research group in Artificial Intelligence from
the Universidad Nacional del Comahue19. Taking
advantage of previous co-participation in different
project we plan to organize exchange programs in
the framework of a research network.
Finally, the PLN group is planning to orga-
nize a workshop on Computational Linguistics as
a satellite event of IBERAMIA 201020, the Ibero-
American Conference on Artificial Intelligence, that
will be organized by the Universidad del Sur, in the
city of Bah??a Blanca, Argentina.
14http://www.cs.utexas.edu/users/moore/acl2
15http://www.irisa.fr/symbiose
16http://www.glyc.dc.uba.ar
17http://www.glyc.dc.uba.ar/elic2010
18http://www.dc.uba.ar/events/eci/2009/eci2009
19http://www.uncoma.edu.ar/
20http://cs.uns.edu.ar/iberamia2010
139
References
M. Amoia, A. Denis, L. Benotti, and C. Gardent. 2010.
Evaluating referring strategies in situated instruction
giving. Topics in Cognitive Science. Submitted.
C. Areces and D. Gor??n. 2005. Ordered resolution with
selection for H(@). In F. Baader and A. Voronkov,
editors, Proc. of LPAR 2004, volume 3452 of LNCS,
pages 125?141. Springer.
C. Areces, D. Figueira, S. Figueira, and S. Mera. 2008a.
Expressive power and decidability for memory log-
ics. In Logic, Language, Information and Computa-
tion, volume 5110 of LNCSs, pages 56?68. Springer.
C. Areces, A. Koller, and K. Striegnitz. 2008b. Referring
expressions as formulas of description logic. In Proc.
of INLG-08.
J. Austin. 1962. How to do Things with Words. Oxford
University Press.
L. Benotti and D. Traum. 2009. A computational ac-
count of comparative implicatures for a spoken dia-
logue agent. In Proc. of IWCS-8.
L. Benotti. 2009a. Clarification potential of instructions.
In SIGDIAL-09.
L. Benotti. 2009b. Frolog: An accommodating text-
adventure game. In Proc. of EACL-09.
N. Blaylock. 2005. Towards tractable agent-based dia-
logue. Ph.D. thesis, University of Rochester, Depart-
ment of Computer Science.
D. Byron, A. Koller, K. Striegnitz, J. Cassell, R. Dale,
J. Moore, and J. Oberlander. 2009. Report on the 1st
GIVE challenge. In Proc. of ENLG, pages 165?173.
N. Dahlba?ck, A. Jo?nsson, and L. Ahrenberg. 1998. Wiz-
ard of Oz studies?why and how. In Readings in intel-
ligent user interfaces, pages 610?619. Morgan Kauf-
mann Publishers Inc.
J. Doswell. 2005. It?s virtually pedagogical: pedagogi-
cal agents in mixed reality learning environments. In
Proc. of SIGGRAPH-05, page 25. ACM.
O. Engwall, P. Wik, J. Beskow, and G. Granstro?m. 2004.
Design strategies for a virtual language tutor. In
S. Kim and D. Young, editors, Proc. of ICSLP-04, vol-
ume 3, pages 1693?1696.
P. Estrella and A. Popescu-Belis. 2008. Multi-eval: an
evaluation framework for multimodal dialogue anno-
tations. Poster at the Joint IM2 and ASSI.
P. Estrella, A. Popescu-Belis, and N. Underwood. 2005.
Finding the system that suits you best: Towards the
normalization of MT evaluation. In Proc. of ASLIB-
05, pages 23?34.
P. Estrella, A. Popescu-Belis, and M. King. 2008. Im-
proving contextual quality models for MT evaluation
based on evaluators? feedback. In Proc. of LREC-08.
P. Estrella, A. Popescu-Belis, and M. King. 2009. The
femti guidelines for contextual mt evaluation: princi-
ples and tools. In W. Daelemans and V. Hoste, ed-
itors, Evaluation of Translation Technology. Linguis-
tica Antverpiensia.
M. Foster, M. Giuliani, A. Isard, C. Matheson, J. Ober-
lander, and A. Knoll. 2009. Evaluating description
and reference strategies in a cooperative human-robot
dialogue system. In Proc. of IJCAI-09.
P. Grice. 1975. Logic and conversation. In P. Cole
and J. Morgan, editors, Syntax and Semantics: Vol. 3:
Speech Acts, pages 41?58. Academic Press.
M. Hajdinjak and F. Mihelic. 2006. The paradise evalu-
ation framework: Issues and findings. Computational
Linguistics, 32(2):263?272.
J. Hoffmann and B. Nebel. 2001. The FF planning
system: Fast plan generation through heuristic search.
JAIR, 14:253?302.
ISO/IEC. 1999. 14598-1:1999 (E) ? Information Tech-
nology ? Software Product Evaluation ? Part 1: Gen-
eral Overview.
ISO/IEC. 2001. 9126-1:2001 (E) ? Software Engineer-
ing ? Product Quality ? Part 1:Quality Model.
H. Kautz and B. Selman. 1999. Unifying SAT-based
and graph-based planning. In Proc of the IJCAI, pages
318?325.
R. Lecoeuche, C. Mellish, and D. Robertson. 1998. A
framework for requirements elicitation through mixed-
initiative dialogue. In Proc. ICRE-98. IEEE.
D. Litman and S. Pan. 2002. Designing and evaluating
an adaptive spoken dialogue system. User Modeling
and User-Adapted Interaction, 12(2-3):111?137.
J. Molka-Danielsen and M. Deutschmann, editors. 2009.
Learning and Teaching in the Virtual World of Second
Life. Tapir Academic Press.
D. Nau, M. Ghallab, and P. Traverso. 2004. Automated
Planning: Theory & Practice. Morgan Kaufmann
Publishers Inc.
M. Purver. 2006. CLARIE: Handling clarification re-
quests in a dialogue system. Research on Language
and Computation, 4(2-3):259?288.
E. Schegloff. 1987. Some sources of misunderstanding
in talk-in-interaction. Linguistics, 8:201?218.
P. Wik and A. Hjalmarsson. 2009. Embodied conversa-
tional agents in computer assisted language learning.
Speech Commun., 51(10):1024?1037.
140
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 67?70,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Negotiating causal implicatures
Luciana Benotti
Universidad Nacional de Co?rdoba
Grupo PLN
Ciudad Universitaria
5000 Co?rdoba, Argentina
luciana.benotti@gmail.com
Patrick Blackburn
INRIA Nancy Grand-Est
Equipe TALARIS
615, rue du Jardin Botanique
54602 Villers le`s Nancy, France
patrick.blackburn@loria.fr
Abstract
In this paper we motivate and describe
a dialogue manager which is able to in-
fer and negotiate causal implicatures. A
causal implicature is a type of Gricean re-
lation implicature, and the ability to infer
them is crucial in situated dialogue. Be-
cause situated dialogue interleaves conver-
sational acts and physical acts, the dia-
logue manager needs to have a grasp on
causal implicatures in order not only to de-
cide what physical acts to do next but also
to generate causally-aware clarifications.
1 Introduction
In conversation, an important part of the content
conveyed is not explicitly said, rather it is impli-
cated. However, Grice (1975)?s classic concept of
conversational implicature (CI) is far from fully
understood. Traditionally CIs have been classified
using the Gricean maxims: there are relation CIs
(also known as relevance CIs), quantity CIs, qual-
ity CIs and manner CIs. In formal pragmatics, the
most studied CIs are quantity CIs, probably be-
cause they are the ones most obviously amenable
to theoretical analysis; see (Geurts, in press) for
a survey of the state of the art. Far less studied
(and traditionally regarded as somewhat obscure)
are relation CIs. Obscure perhaps, but crucial: it
has been argued that they subsume all other types
of CIs (Wilson and Sperber, 2004). This paper is a
first step towards their formalization.
We shall analyze a kind of CI that we call causal
CIs. Causal CIs are relation CIs as defined by
Grice (1975) where the crucial relation is task do-
main causality. Consider the following example:
Mary: The chest is locked, the crown is inside
Bill: Give me the crown
Bill causally implicated: Unlock the chest
In order to carry out the task action required by
Bill (to give him the crown) it is necessary to un-
lock the chest. Hence we say that Bill is implicat-
ing, by trading on the domain causal relations (af-
ter all, the contents of a chest are not accessible un-
less the chest is unlock) that Mary is to unlock the
chest. Now, once Mary has inferred the causal CI,
she may accept this inference silently or negotiate
it. Mary might decide to silently accept it because
she knows how to get the key; in this case we will
say that Mary constructed an internal bridge from
the current task situation (that is, the crown being
inside the locked chest) to the proposal made by
Bill (giving him the crown). If Mary decides she
has insufficient information to construct the inter-
nal bridge (maybe she has no key, or sees that the
lock is rusty) she may start a sub-dialogue that we
will call an external bridge; she might say, for ex-
ample: But how can I unlock the chest? The in-
ternal process of bridging is what in the literature
has been called accommodation (Lewis, 1979) or
bridging (Clark, 1975). The external processes of
bridging constitutes a large part of what we call
conversation.
This paper presents a dialogue system (called
Frolog) which infers and negotiates causal CIs in
the context of situated task-oriented dialogue; the
framework is intended as a proof-of-concept of the
ideas just sketched. We proceed as follows. In
Section 2, we motivate the study of causal CIs in
dialogue. In Section 3 we present Frolog?s dia-
logue manager which infers causal CIs in situated
dialogue. And in Section 4 we illustrate how the
negotiation (external bridging) of causal CIs incre-
mentally grounds a pragmatic goal proposed by
one of the dialogue participants. Section 5 con-
cludes the paper.
2 Causal implicatures and dialogue
The motivation for our work is both theoretical
and practical. On the theoretical side, we believe
67
that it is crucial to explore CIs in the setting of
naturally occurring dialogues. Strangely enough
(after all, Grice did call them conversational im-
plicatures) this view appears to be novel, perhaps
even controversial. In the formal pragmatics lit-
erature, CIs are often simply viewed as inferences
drawn by a hearer on the basis of a speaker?s ut-
terance, contextual information, and the Gricean
maxims. We find this perspective too static. CIs
(especially relations CIs) are better viewed as in-
trinsically interactional inferences that arise from
the dynamics of conversation. As conversations
progress, speakers and hearers switch roles: mean-
ing are negotiated and inference becomes bidirec-
tional (Thomason et al, 2006). Moreover, even
within a single turn, hearers are not restricted to
simply drawing (or failing to draw) ?the? CI: in
fact, choosing between internal and external bridg-
ing is better viewed as part of the process of nego-
tiating what the CI at stake actually is. We be-
lieve that interactive perspectives will be neces-
sary to extend the theory of CIs beyond the rel-
atively narrow domain of quantity CIs. We also
believe that the dialog-centered approach we ad-
vocate may have practical consequences. In par-
ticular, modeling the external process of bridging
is a step towards having a pragmatically incremen-
tal dialogue manager in the spirit of that sketched
in (Bu? and Schlangen, 2010).
This is a broad goal, in this paper we focus on
clausal implicatures. This restriction gives us an
empirical handle of CIs. It is not controversial that
(in non-conversational activities) the causal rela-
tions between acts define the expectations of the
interaction. But also in conversational activities
situated in a physical task causal relations guide
the interaction; we did an empirical study on such
a kind of corpus (Benotti, 2009) and we found that,
in this corpus, most CIs for which there is evidence
(because they are made explicit in a clarification
request) can be explained in terms of causal rela-
tions. For our empirical study, we annotated and
classified the clarification requests (CRs) that ap-
pear in the SCARE corpus (Stoia et al, 2008).
3 Inferring causal implicatures
In order to model the causal CIs that we observed
in the SCARE corpus, and to experiment with dif-
ferent strategies for negotiating these CIs, we de-
signed a system that mimics the instruction giving
setup of the SCARE corpus. In our setup, the DF
is a dialogue system that we will call Frolog. The
human participant that plays the role of the DG we
will call ?the player?.
In a nutshell, Frolog uses an off-the-shelf plan-
ner to compute causal implicatures. That is, it
uses classical planning (a well explored and com-
putationally efficient AI technique) to fill out the
micro-structure of discourse (the bridging infor-
mation required in the next step).1 We do so us-
ing the planner BLACKBOX (Kautz and Selman,
1999). Like all classical planners, BLACKBOX
takes three inputs: the initial state, the goal, and
the available actions. The question of what these
three elements should be raises a number of issues.
In Frolog, two types of information are regis-
tered: complete and accurate information about
the game world in the world KB and a represen-
tation of the common ground in the interaction
KB. Which of these should be used in the initial
state? In fact, we need both: we infer the actions
intended by the player using the information in the
interaction KB but we have to verify this sequence
of actions on the world KB to check if it can actu-
ally be executed.
Let us now define what the goal of the planning
problem should be. Frolog should act to make the
preconditions of the action true with one restric-
tion. The restriction is that it must be possible for
Frolog to manipulate these preconditions. How-
ever, we don?t need to worry about this restric-
tion because the planner should take care of which
propositions are manipulable by Frolog and which
are not, given the current state. So we can just de-
fine the goal as the conjunction of all the precon-
ditions of the command uttered by the player.
To complete the picture, the actions available to
the planner are all the actions in the game action
database. This means that we are assuming that
all the actions that can be executed, are mutually
known to Frolog and the player.
In order to be able to perform bridging to the
mutual information it must be mutually known
what the preconditions and the effects of the ac-
tions involved are. The assumption that the player
and Frolog know the exact specification of all the
actions that can be executed in the game world is
1Thus the work reported here is very different from the
traditional work of (Perrault and Allen, 1980; Allen and
Allen, 1994): classic papers in the plan-based tradition use
plan recognition (a more computationally expensive task) to
interpret utterances by inserting them into the plan the macro-
structure (the global shape) of discourse.
68
a simplifying assumption. We make it because it
enables us to avoid deciding (and implementing)
how differing knowledge about actions get coordi-
nated; dealing with such issues simply too difficult
at present.
4 Negotiating with a dialogue system
In this section we present a worked out example of
how the solutions of the planning problems intro-
duced in the previous section can be used in order
to perform internal bridging and trigger external
bridging when appropriate.
In this example Frolog does not have enough
knowledge yet in order to infer the bridge.2 This
is the case in instruction (37) in Figure 1. In this
instruction the player wants Frolog to open the
chest but Frolog does not know how to do this.
In the screen-shot in the right you can see state of
the interaction KB from turns (37) to (39). In this
model we can see that the player does not have
information about what fits into the chest, in or-
der to unlock it. This is why the planner is not
able to find a plan that achieves the precondition
(unlocked chest1) of the command ?open the
chest? in instruction (37). The instruction in (39)
fails as well making the obstacle explicit. Utter-
ance (40) is an example of a CR caused by a re-
quired and unknown thematic relation filler (the
second most frequent CR in the SCARE corpus).
Player(32): look at the chestFrolog(33): the chest is closed and          locked and woodenPlayer(34): look at FrologFrolog(35): Frolog is aliveFrolog(36): Frolog holds the sword, the worm                   and the golden keyPlayer(37): open the chestFrolog(38): Frolog can't do that!   the chest is not unlockedPlayer(39): unlock the chestFrolog(40): Frolog can't do that!   Frolog doesn't know what fits into the chestPlayer(41): unlock the chest with the golden keyFrolog(42): [Frolog unlocks the chest] Frolog(43): the chest is unlockedPlayer(44): lock the chest with the keyFrolog(45): [Frolog locks the chest] Frolog(46): the chest is lockedPlayer(47): open the chestFrolog(48): Frolog can't do that!   the chest is not unlocked fitsin??????
Figure 1: External bridging in Frolog
Frolog?s behavior continues as expected until
turn (48) in which Frolog again is not able to
bridge the command open the chest. It is to be
expected that the fact that the golden key fits into
2For illustrations of other reasons that trigger negotiation
of CIs see (Benotti, 2010).
the chest is mutual knowledge by now but Frolog
is not reacting accordingly.
In order to overcome this issue there are two
options: (1) either we allow different sources to
update the mutual information loosing uniformity
in the update process or (2) we add actions that
manipulate this kind of information. Here we de-
scribe option (1) that turns out to be the proce-
dural version of ?strict accommodation? (as de-
fined in (Beaver and Geurts, in press)), and when
generalized is of a piece with abduction (Hobbs,
1985). For an discussion of (2) we refer the reader
to (Benotti, 2010).
We could say that the intuition behind the strict
notion of accommodation is that when the speaker
utters a sentence, she is not only trying to achieve
the obvious effects of the utterance, but is also
communicating the ways in which she assumes the
world to be, and on which the success of the utter-
ance depends. Following this intuition it can be
argued that the intention behind the utterance ?un-
lock the chest with the golden key? in turn (41) is
twofold. It is clear that the player wants to achieve
the effects of the action, that is, she wants to have
the chest unlocked. But the player also expects
Frolog to recognize the assumptions she is making
and on which the success of the utterance depends.
In particular, she expects Frolog to recognize that
she is assuming that the golden key fits into the
chest lock (this is why Frolog can coherently ask
why do you think that the golden key will fit?).
This means that, when an action is executed, the
interaction KB will be updated not only with the
effects of the action but also with its preconditions.
And those preconditions that were not in the inter-
action KB before will be verbalized as in turn (9)
in Figure 2.
Player(32): olkth:cse:tse4cFrolog(3): Frolog thecs io sdhsa :::::::::::::::Frklkg:7ke4opc:hokn::::::::::::::::nsac:iuc4duock:cse:tse4cPlayer(92):lkkh:ac:yk Frolog(n): kow hr5 hlv65Frolog(,): kow doli h goli5e m5kPlayer('2): olkth:cse:tse4c:nucs:cse:gkl7eo:heyFrolog(y): 8Frolog welotm' sd5 td5's                 !vsd sd5 goli5e m5k4Frolog(0): sd5 td5's v' welotm5iFrolog(2): cse:hey:iuc4duock:cse:tse4cPlayer(3w2):lkth:cse:tse4cFrolog([[): 8Frolog lotm' sd5 td5's :::::::::::::::::!vsd sd5 goli5e m5k4Frolog([3): sd5 td5's v' lotm5iPlayer(392):lkkh:ac:cse:tse4cFrolog([n): sd5 td5's v' tlo'5i hei          lotm5i hei !ooi5ePlayer(3f2):k1eo:cse:tse4cFrolog([u): 8Frolog welotm' sd5 td5's !vsd sd5 goli5e m5k4Frolog([y): sd5 td5's v' welotm5iFrolog([0): 8Frolog o]5e' sd5 td5's4Frolog([2): sd5 td5's v' o]5eFrolog(3?): sd5 td5's doli h goli5e tro!e
:action?open?:arguments??????(agent??w)?(theme??x)???:precondition?????????(accessible??x)?????????(closed??x)?????????(unlocked??x)?:effect?????????(not(closed??x))?????????(open??x)
:action?unlock?:arguments??????(agent??w)?(theme??x)?(inst??y)??:precondition?????????(accessible??x)?????????(locked??x)?????????(fitsin??y??x)?????????(hold??w??y)????:effect?????????(not(locked??x))?????????(unlocked??x)?????????
Figure 2: External bridging becomes internal
69
The rest of the interaction (from turns (10)
to (15)) show that once the proposition (fitsin
key1 chest1) is added to the interaction KB the
action ?open the chest? can be internally bridged
even when the chest is locked. Because the player
and Frolog mutually know which key fits into the
chest.
5 Discussion
Clearly, our inference framework is limited in
many ways. But we think we?ve made a small
step in the right direction. Dialogue systems are
reaching a development level in which they cannot
elude drawing inferences for much longer. This
paper is a step in this direction.
Causal implicatures are a kind of relation
implicature (historically Grice?s most obscure
and crucial implicature) whose inference?we?ve
argued?is essential in situated dialogue if our di-
alogue systems are not to violate the expectations
of the user. Causal relations have a direct impact
on the coherence structure of situated dialogues
such as those in the SCARE corpus; in the SCARE
corpus most pragmatic clarification requests make
explicit causal implicatures.
We need to have a grasp on causal impli-
catures in order for our dialogue systems not
only to decide what physical acts to do next?
internal bridging?but also to generate causally-
aware clarification requests?external bridging.
Of course the inference framework presented here
has many limitations that we discussed through-
out the paper and probably classical planning is
not the formalism that we will finally want to use
in our dialogue systems (at least not in its present
form). Our model is intended as a proof of con-
cept, and intentionally stays at a level of formal-
ization that is still simple enough so as not to loose
our intuitions. The two intuitions that we don?t
want to loose sight of are (1) utterances are to be
interpreted in a context and need to be connected
to this context (through some kind of relation, be-
ing causality one of the most important ones in
situated dialogue) in order to be grounded (2) the
process of connecting utterances to the context is
a joint process, it is a negotiation that involves de-
cisions of all the dialogue participants.
With the intuitions in place we plan to extend
this work mainly by porting the inference frame-
work into new domains.
There is lot to do yet, but we believe that the
negotiation of causal implicatures is a step towards
an incremental dialogue manager.
References
James Allen and Richard Allen. 1994. Natural lan-
guage understanding. Addison Wesley, 2nd edition.
David Beaver and Bart Geurts. in press. Presup-
position. In Handbook of Semantics. Mouton de
Gruyter.
Luciana Benotti. 2009. Clarification potential of in-
structions. In Proc. of SIGDIAL, pages 196?205,
London, United Kingdom.
Luciana Benotti. 2010. Implicature as an Interactive
Process. Ph.D. thesis, Universite? Henri Poincare?,
INRIA Nancy Grand Est, France. Supervised by
P. Blackburn. Reviewed by N. Asher and B. Geurts.
Okko Bu? and David Schlangen. 2010. Modelling
sub-utterance phenomena in spoken dialogue sys-
tems. In The 2010 Workshop on the Semantics and
Pragmatics of Dialogue, Poznan?, Poland.
Herbert Clark. 1975. Bridging. In Proc. of the Work-
shop on Theoretical issues in natural language pro-
cessing, pages 169?174, Morristown, USA. ACL.
Bart Geurts. in press. Quantity implicatures. Cam-
bridge University Press.
Paul Grice. 1975. Logic and conversation. In P. Cole
and J. Morgan, editors, Syntax and Semantics, vol-
ume 3, pages 41?58. Academic Press, New York.
Jerry Hobbs. 1985. Granularity. In Proceedings of
the 9th International Joint Conference on Artificial
Intelligence, pages 432?435. Morgan Kaufmann.
Henry Kautz and Bart Selman. 1999. Unifying SAT-
based and graph-based planning. In Proceedings of
the 16th International Joint Conference on Artificial
Intelligence, pages 318?325, Stockholm, Sweden.
David Lewis. 1979. Scorekeeping in a language game.
Journal of Philosophical Logic, 8:339?359.
Raymond Perrault and James Allen. 1980. A plan-
based analysis of indirect speech acts. Computa-
tional Linguistics, 6(3-4):167?182.
Laura Stoia, Darla Shockley, Donna Byron, and Eric
Fosler-Lussier. 2008. SCARE: A situated corpus
with annotated referring expressions. In Proc. of
LREC.
Richmond Thomason, Matthew Stone, and David De-
Vault. 2006. Enlightened update: A computa-
tional architecture for presupposition and other prag-
matic phenomena. In Presupposition Accommoda-
tion. Ohio State Pragmatics Initiative.
Deirdre Wilson and Dan Sperber. 2004. Relevance
theory. In Handbook of Pragmatics, pages 607?632.
Blackwell, Oxford.
70
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 68?77,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Giving instructions in virtual environments by corpus based selection
Luciana Benotti
PLN Group, FAMAF
National University of Co?rdoba
Co?rdoba, Argentina
luciana.benotti@gmail.com
Alexandre Denis
TALARIS team, LORIA/CNRS
Lorraine. Campus scientifique, BP 239
Vandoeuvre-le`s-Nancy, France
alexandre.denis@loria.fr
Abstract
Instruction giving can be used in several
applications, ranging from trainers in sim-
ulated worlds to non player characters for
virtual games. In this paper we present a
novel algorithm for rapidly prototyping virtual
instruction-giving agents from human-human
corpora without manual annotation. Automat-
ically prototyping full-fledged dialogue sys-
tems from corpora is far from being a reality
nowadays. Our approach is restricted in that
only the virtual instructor can perform speech
acts while the user responses are limited to
physical actions in the virtual worlds.
We have defined an algorithm that, given a
task-based corpus situated in a virtual world,
which contains human instructor?s speech acts
and the user?s responses as physical actions,
generates a virtual instructor that robustly
helps a user achieve a given task in the vir-
tual world. We explain how this algorithm
can be used for generating a virtual instructor
for a game-like, task-oriented virtual world.
We evaluate the virtual instructor with human
users using task-oriented as well as user satis-
faction metrics. We compare our results with
both human and rule-based virtual instructors
hand-coded for the same task.
1 Introduction
Virtual human characters constitute a promising
contribution to many fields, including simulation,
training and interactive games (Kenny et al, 2007;
Jan et al, 2009). The ability to communicate using
natural language is important for believable and ef-
fective virtual humans. Such ability has to be good
enough to engage the trainee or the gamer in the ac-
tivity. Nowadays, most conversational systems oper-
ate on a dialogue-act level and require extensive an-
notation efforts in order to be fit for their task (Rieser
and Lemon, 2010). Semantic annotation and rule
authoring have long been known as bottlenecks for
developing conversational systems for new domains.
In this paper, we present a novel algorithm for
generating virtual instructors from automatically an-
notated human-human corpora. Our algorithm,
when given a task-based corpus situated in a virtual
world, generates an instructor that robustly helps a
user achieve a given task in the virtual world of the
corpus. There are two main approaches toward au-
tomatically producing dialogue utterances. One is
the selection approach, in which the task is to pick
the appropriate output from a corpus of possible out-
puts. The other is the generation approach, in which
the output is dynamically assembled using some
composition procedure, e.g. grammar rules. The se-
lection approach to generation has only been used
in conversational systems that are not task-oriented
such as negotiating agents (Gandhe and Traum,
2007a), question answering characters (Kenny et al,
2007), and virtual patients (Leuski et al, 2006). To
the best of our knowledge, our algorithm is the first
one proposed for doing corpus based generation and
interaction management for task-oriented systems.
The advantages of corpus based generation are
many. To start with, it affords the use of complex
and human-like sentences without detailed analysis.
Moreover, the system may easily use recorded au-
dio clips rather than speech synthesis and recorded
video for animating virtual humans. Finally, no
68
rule writing by a dialogue expert or manual an-
notations is needed. The disadvantage of corpus
based generation is that the resulting dialogue may
not be fully coherent. For non-task oriented sys-
tems, dialogue management through corpus based
methods has shown coherence related problems.
Shawar and Atwell (2003; 2005) present a method
for learning pattern matching rules from corpora in
order to obtain the dialogue manager for a chat-
bot. Gandhe and Traum (2007b) investigate several
dialogue models for negotiating virtual agents that
are trained on an unannotated human-human corpus.
Both approaches report that the dialogues obtained
by these methods are still to be improved because
the lack of dialogue history management results in
incoherences. Since in task-based systems, the di-
alogue history is restricted by the structure of the
task, the absence of dialogue history management is
alleviated by tracking the current state of the task.
In the next section we introduce the corpora used
in this paper. Section 3 presents the two phases of
our algorithm, namely automatic annotation and di-
alogue management through selection. In Section 4
we present a fragment of an interaction with a virtual
instructor generated using the corpus and the algo-
rithm introduced in the previous sections. We evalu-
ate the virtual instructor in interactions with human
subjects using objective as well as subjective met-
rics. We present the results of the evaluation in Sec-
tion 5. We compare our results with both human
and rule-based virtual instructors hand-coded for the
same task. Finally, Section 7 discusses the weak-
nesses of the approach for developing instruction
giving agents, as well as its advantages and draw-
backs with respect to hand-coded systems. In this
last section we also discuss improvements on our al-
gorithms designed as a result of our error analysis.
2 The GIVE corpus
The Challenge on Generating Instructions in Vir-
tual Environments (GIVE; Koller et al (2010)) is
a shared task in which Natural Language Gener-
ation systems must generate real-time instructions
that guide a user in a virtual world. In this paper,
we use the GIVE-2 Corpus (Gargett et al, 2010), a
freely available corpus of human instruction giving
in virtual environments. We use the English part of
the corpus which consists of 63 American English
written discourses in which one subject guided an-
other in a treasure hunting task in 3 different 3D
worlds.
The task setup involved pairs of human partners,
each of whom played one of two different roles. The
?direction follower? (DF) moved about in the vir-
tual world with the goal of completing a treasure
hunting task, but had no knowledge of the map of
the world or the specific behavior of objects within
that world (such as, which buttons to press to open
doors). The other partner acted as the ?direction
giver? (DG), who was given complete knowledge of
the world and had to give instructions to the DF to
guide him/her to accomplish the task.
The GIVE-2 corpus is a multi-modal corpus
which consists of all the instructions uttered by the
DG, and all the object manipulations done by the DF
with the corresponding timestamp. Furthermore, the
DF?s position and orientation is logged every 200
milliseconds, making it possible to extract informa-
tion about his/her movements.
3 The unsupervised conversational model
Our algorithm consists of two phases: an annotation
phase and a selection phase. The annotation phase
is performed only once and consists of automatically
associating the DG instruction to the DF reaction.
The selection phase is performed every time the vir-
tual instructor generates an instruction and consists
of picking out from the annotated corpus the most
appropriate instruction at a given point.
3.1 The automatic annotation
The basic idea of the annotation is straightforward:
associate each utterance with its corresponding re-
action. We assume that a reaction captures the se-
mantics of its associated instruction. Defining re-
action involves two subtle issues, namely boundary
determination and discretization. We discuss these
issues in turn and then give a formal definition of
reaction.
We define the boundaries of a reaction as follows.
A reaction Rk to an instruction Uk begins right af-
ter the instruction Uk is uttered and ends right before
the next instruction Uk+1 is uttered. In the follow-
ing example, instruction 1 corresponds to the reac-
69
tion ?2, 3, 4?, instruction 5 corresponds to ?6?, and
instruction 7 to ?8?.
DG(1): hit the red you see in the far room
DF(2): [enters the far room]
DF(3): [pushes the red button]
DF(4): [turns right]
DG(5): hit far side green
DF(6): [moves next to the wrong green]
DG(7): no
DF(8): [moves to the right green and pushes it]
As the example shows, our definition of bound-
aries is not always semantically correct. For in-
stance, it can be argued that it includes too much
because 4 is not strictly part of the semantics of 1.
Furthermore, misinterpreted instructions (as 5) and
corrections (e.g., 7) result in clearly inappropriate
instruction-reaction associations. Since we want to
avoid any manual annotation, we decided to use this
naive definition of boundaries anyway. We discuss
in Section 5 the impact that inappropriate associa-
tions have on the performance of a virtual instructor.
The second issue that we address here is dis-
cretization of the reaction. It is well known that there
is not a unique way to discretize an action into sub-
actions. For example, we could decompose action 2
into ?enter the room? or into ?get close to the door
and pass the door?. Our algorithm is not dependent
on a particular discretization. However, the same
discretization mechanism used for annotation has to
be used during selection, for the dialogue manager
to work properly. For selection (i.e., in order to de-
cide what to say next) any virtual instructor needs
to have a planner and a planning problem: i.e., a
specification of how the virtual world works (i.e.,
the actions), a way to represent the states of the vir-
tual world (i.e., the state representation) and a way
to represent the objective of the task (i.e., the goal).
Therefore, we decided to use them in order to dis-
cretize the reaction.
For the virtual instructor we present in Section 4
we used the planner LazyFF and the planning prob-
lem provided with the GIVE Framework. The
planner LazyFF is a reimplementation (in Java) of
the classical artificial intelligence planner FF (Hoff-
mann and Nebel, 2001). The GIVE framework (Gar-
gett et al, 2010) provides a standard PDDL (Hsu et
al., 2006) planning problem which formalizes how
the GIVE virtual worlds work. Both the LazzyFF
planner and the GIVE planning problem are freely
available on the web1.
Now we are ready to define reaction formally. Let
Sk be the state of the virtual world when uttering in-
struction Uk, Sk+1 be the state of the world when ut-
tering the next utterance Uk+1 and Acts be the rep-
resentation of the virtual world actions. The reaction
to Uk is defined as the sequence of actions returned
by the planner with Sk as the initial state, Sk+1 as
the goal state and Acts as the actions.
Given this reaction definition, the annotation of
the corpus then consists of automatically associat-
ing each utterance to its (discretized) reaction. The
simple algorithm that implements this annotation is
shown in Figure 1. Moreover, we provide a fragment
of the resulting annotated corpus in Appendix A.
1: Acts? world possible actions
2: for all utterance Uk in the corpus do
3: Sk ? world state at Uk
4: Sk+1 ? world state at Uk+1
5: Uk.Reaction? plan(Sk, Sk+1, Acts)
6: end for
Figure 1: Annotation algorithm
3.2 Selecting what to say next
In this section we describe how the selection phase is
performed every time the virtual instructor generates
an instruction.
The instruction selection algorithm, displayed in
Figure 2, consists in finding in the corpus the set of
candidate utterances C for the current task plan P
(P is the sequence of actions that needs to be exe-
cuted in the current state of the virtual world in or-
der to complete the task). We define C = {U ?
Corpus | P starts with U.Reaction}. In other words,
an utterance U belongs to C if the first actions of the
current plan P exactly match the reaction associated
to the utterance U . All the utterances that pass this
test are considered paraphrases and hence suitable in
the current context.
Whenever the plan P changes, as a result of the
actions of the DF, we call the selection algorithm in
order to regenerate the set of candidate utterances C.
1http://www.give-challenge.org/
70
1: C ? ?
2: Plan? current task plan
3: for all utterance U in the corpus do
4: if Plan starts with U.Reaction then
5: C ? C ? {U}
6: end if
7: end for
8: return C
Figure 2: Selection algorithm
While the plan P doesn?t change, because the
DF is staying still, the virtual instructor offers al-
ternative paraphrases of the intended instruction.
Each paraphrase is selected by picking an utterance
from C and verbalizing it, at fixed time intervals
(every 3 seconds). The order in which utterances
are selected depends on the length of the utterance
reaction (in terms of number of actions), starting
from the longest ones. Hence, in general, instruc-
tions such as ?go back again to the room with the
lamp? are uttered before instructions such as ?go
straight?, because the reaction of the former utter-
ance is longer than the reaction of the later.
It is important to notice that the discretization
used for annotation and selection directly impacts
the behavior of the virtual instructor. It is crucial
then to find an appropriate granularity of the dis-
cretization. If the granularity is too coarse, many
instructions in the corpus will have an empty reac-
tion. For instance, in the absence of the representa-
tion of the user orientation in the planning domain
(as is the case for the virtual instructor we evaluate
in Section 5), instructions like ?turn left? and ?turn
right? will have empty reactions making them indis-
tinguishable during selection. However, if the gran-
ularity is too fine the user may get into situations
that do not occur in the corpus, causing the selec-
tion algorithm to return an empty set of candidate
utterances. It is the responsibility of the virtual in-
structor developer to find a granularity sufficient to
capture the diversity of the instructions he wants to
distinguish during selection.
4 A virtual instructor for a virtual world
We implemented an English virtual instructor for
one of the worlds used in the corpus collection we
presented in Section 2. The English fragment of the
corpus that we used has 21 interactions and a total
of 1136 instructions. Games consisted on average
of 54.2 instructions from the human DG, and took
about 543 seconds on average for the human DF to
complete the task.
On Figures 4 to 7 we show an excerpt of an in-
teraction between the system and a user. The fig-
ures show a 2D map from top view and the 3D in-
game view. In Figure 4, the user, represented by a
blue character, has just entered the upper left room.
He has to push the button close to the chair. The
first candidate utterance selected is ?red closest to
the chair in front of you?. Notice that the referring
expression uniquely identifies the target object us-
ing the spatial proximity of the target to the chair.
This referring expression is generated without any
reasoning on the target distractors, just by consid-
ering the current state of the task plan and the user
position.
Figure 4: ?red closest to the chair in front of you?
After receiving the instruction the user gets closer
to the button as shown in Figure 5. As a result of the
new user position, a new task plan exists, the set of
candidate utterances is recalculated and the system
selects a new utterance, namely ?the closet one?.
The generation of the ellipsis of the button or the
chair is a direct consequence of the utterances nor-
mally said in the corpus at this stage of the task plan
(that is, when the user is about to manipulate this ob-
ject). From the point of view of referring expression
algorithms, the referring expression may not be op-
timal because it is over-specified (a pronoun would
71
L go
yes left
straight now go back
go back out now go back out
closest the door down the passage
go back to the hallway nowin to the shade room
go back out of the room out the way you came in
exit the way you entered ok now go out the same door
back to the room with the lamp go back to the door you came in
Go through the opening on the left okay now go back to the original room
okay now go back to where you came from ok go back again to the room with the lamp
now i ned u to go back to the original room Go through the opening on the left with the yellow wall paper
Figure 3: All candidate selected utterances when exiting the room in Figure 7
Figure 5: ?the closet one?
Figure 6: ?good?
be preferred as in ?click it?), Furthermore, the in-
struction contains a spelling error (?closet? instead
Figure 7: ?go back to the room with the lamp?
of ?closest?). In spite of this non optimality, the in-
struction led our user to execute the intended reac-
tion, namely pushing the button.
Right after the user clicks on the button (Figure 6),
the system selects an utterance corresponding to the
new task plan. The player position stayed the same
so the only change in the plan is that the button no
longer needs to be pushed. In this task state, DGs
usually give acknowledgements and this is then what
our selection algorithm selects: ?good?.
After receiving the acknowledgement, the user
turns around and walks forward, and the next ac-
tion in the plan is to leave the room (Figure 7). The
system selects the utterance ?go back to the room
with the lamp? which refers to the previous interac-
tion. Again, the system keeps no representation of
the past actions of the user, but such utterances are
the ones that are found at this stage of the task plan.
72
We show in Figure 3 all candidate utterances se-
lected when exiting the room in Figure 7. That is,
for our system purposes, all the utterances in the fig-
ure are paraphrases of the one that is actually uttered
in Figure 7. As we explained in Section 3.2, the
utterance with the longest reaction is selected first
(?go back to the room with the lamp?), the second
utterance with the longest reaction is selected sec-
ond (?ok go back again to the room with the lamp?),
and so on. As you can observe in Figure 3 the ut-
terances in the candidate set can range from tele-
graphic style like ?L? to complex sentences like ?Go
through the opening on the left with the yellow wall
paper?. Several kinds of instructions are displayed,
acknowledgements such as ?yes?, pure moving in-
structions like ?left? or ?straight?, instructions that
refer to the local previous history such as ?go back
out the room? or ?ok now go out the same door? and
instructions that refer back to the global history such
as ?okay now go back to the original room?.
Due to the lack of orientation consideration in our
system, some orientation dependent utterances are
inappropriate in this particular context. For instance,
?left? is incorrect given that the player does not have
to turn left but go straight in order to go through
the correct door. However, most of the instructions,
even if quite different among themselves, could have
been successfully used in the context of Figure 7.
5 Evaluation and error analysis
In this section we present the results of the evalu-
ation we carried out on the virtual instructor pre-
sented in Section 4 which was generated using the
dialogue model algorithm introduced in Section 3.
We collected data from 13 subjects. The partici-
pants were mostly graduate students; 7 female and
6 male. They were not English native speakers but
rated their English skills as near-native or very good.
The evaluation contains both objective measures
which we discuss in Section 5.1 and subjective mea-
sures which we discuss in Section 5.2.
5.1 Objective metrics
The objective metrics we extracted from the logs of
interaction are summarized in Table 1. The table
compares our results with both human instructors
and the three rule-based virtual instructors that were
top rated in the GIVE-2 Challenge. Their results cor-
respond to those published in (Koller et al, 2010)
which were collected not in a laboratory but con-
necting the systems to users over the Internet. These
hand-coded systems are called NA, NM and Saar.
We refer to our system as OUR.
Human NA Saar NM OUR
Task success 100% 47% 40% 30% 70%
Canceled 0% 24% n/a 35% 7%
Lost 0% 29% n/a 35% 23%
Time (sec) 543 344 467 435 692
Mouse actions 12 17 17 18 14
Utterances 53 224 244 244 194
Table 1: Results for the objective metrics
In the table we show the percentage of games that
users completed successfully with the different in-
structors. Unsuccessful games can be either can-
celed or lost. We also measured the average time
until task completion, and the average number of ut-
terances users received from each system. To ensure
comparability, we only counted successfully com-
pleted games.
In terms of task success, our system performs bet-
ter than all hand-coded systems. We duly notice that,
for the GIVE Challenge in particular (and proba-
bly for human evaluations in general) the success
rates in the laboratory tend to be higher than the suc-
cess rate online (this is also the case for completion
times) (Koller et al, 2009). Koller et al justify this
difference by stating that the laboratory subject is
being discouraged from canceling a frustrating task
while the online user is not. However, it is also pos-
sible that people canceled less because they found
the interaction more natural and engaging as sug-
gested by the results of the subjective metrics (see
next section).
In any case, our results are preliminary given the
amount of subjects that we tested, but they are in-
deed encouraging. In particular, our system helped
users to identify better the objects that they needed
to manipulate in the virtual world, as shown by the
low number of mouse actions required to complete
the task (a high number indicates that the user must
have manipulated wrong objects). This correlates
with the subjective evaluation of referring expres-
sion quality (see next section).
73
We performed a detailed analysis of the instruc-
tions uttered by our system that were unsuccessful,
that is, all the instructions that did not cause the in-
tended reaction as annotated in the corpus. From the
2081 instructions uttered in total (adding all the ut-
terances of the 13 interactions), 1304 (63%) of them
were successful and 777 (37%) were unsuccessful.
Given the limitations of the annotation discussed
in Section 3.1 (wrong annotation of correction utter-
ances and no representation of user orientation) we
classified the unsuccessful utterances using lexical
cues into 1) correction like ?no? or ?wrong?, 2) ori-
entation instruction such as ?left? or ?straight?, and
3) other. We found that 25% of the unsuccessful ut-
terances are of type 1, 40% are type 2, 34% are type
3 (1% corresponds to the default utterance ?go? that
our system utters when the set of candidate utter-
ances is empty). In Section 7 we propose an im-
proved virtual instructor designed as a result of this
error analysis.
5.2 Subjective metrics
The subjective measures were obtained from re-
sponses to the GIVE-2 questionnaire that was pre-
sented to users after each game. It asked users to rate
different statements about the system using a contin-
uous slider. The slider position was translated to a
number between -100 and 100. As done in GIVE-
2, for negative statements, we report the reversed
scores, so that in Tables 2 and 3 greater numbers
indicates that the system is better (for example, Q14
shows that OUR system is less robotic than the rest).
In this section we compare our results with the sys-
tems NA, Saar and NM as we did in Section 5.1, we
cannot compare against human instructors because
these subjective metrics were not collected in (Gar-
gett et al, 2010).
The GIVE-2 Challenge questionnaire includes
twenty-two subjective metrics. Metrics Q1 to Q13
and Q22 assess the effectiveness and reliability of
instructions. For almost all of these metrics we got
similar or slightly lower results than those obtained
by the three hand-coded systems, except for three
metrics which we show in Table 2. We suspect that
the low results obtained for Q5 and Q22 relate to
the unsuccessful utterances identified and discussed
in Section 5.1 (for instance, corrections were some-
times contradictory causing confusion and resulting
in subjects ignoring them as they advanced in the in-
teraction). The high unexpected result in Q6, that
is indirectly assessing the quality of referring ex-
pressions, demonstrates the efficiency of the refer-
ring process despite the fact that nothing in the algo-
rithms is dedicated to reference. This good result is
probably correlated with the low number of mouse
actions mentioned in Section 5.1.
NA Saar NM OUR
Q5: I was confused about which direction to go in
29 5 9 -12
Q6: I had no difficulty with identifying the objects the
system described for me
18 20 13 40
Q22: I felt I could trust the system?s instructions
37 21 23 0
Table 2: Results for the significantly different subjective
measures assessing the effectiveness of the instructions
(the greater the number, the better the system)
Metrics Q14 to Q20 are intended to assess the nat-
uralness of the instructions, as well as the immer-
sion and engagement of the interaction. As Table 3
shows, in spite of the unsuccessful utterances, our
system is rated as more natural and more engaging
(in general) than the best systems that competed in
the GIVE-2 Challenge.
NA Saar NM OUR
Q14: The system?s instructions sounded robotic
-4 5 -1 28
Q15: The system?s instructions were repetitive
-31 -26 -28 -8
Q16: I really wanted to find that trophy
-11 -7 -8 7
Q17: I lost track of time while solving the task
-16 -11 -18 16
Q18: I enjoyed solving the task
-8 -5 -4 4
Q19: Interacting with the system was really annoying
8 -2 -2 4
Q20: I would recommend this game to a friend
-30 -25 -24 -28
Table 3: Results for the subjective measures assessing
the naturalness and engagement of the instructions (the
greater the number, the better the system)
74
6 Portability to other virtual environments
The hand-coded systems, which we compared to, do
not need a corpus in a particular GIVE virtual world
in order to generate instructions for any GIVE vir-
tual world, while our system cannot do without such
corpus. These hand-coded systems are designed to
work on different GIVE virtual worlds without the
need of training data, hence their algorithms are
more complex (e.g. they include domain indepen-
dent algorithms for generation of referring expres-
sions) and take a longer time to develop.
Our algorithm is independent of any particular
virtual world. In fact, it can be ported to any other
instruction giving task (where the DF has to per-
form a physical task) with the same effort than re-
quired to port it to a new GIVE world. This is not
true for the hand-coded GIVE systems. The inputs
of our algorithm are an off-the-shelf planner, a for-
mal planning problem representation of the task and
a human-human corpus collected on the very same
task the system aims to instruct. It is important to
notice that any virtual instructor, in order to give in-
structions that are both causally appropriate at the
point of the task and relevant for the goal cannot do
without such planning problem representation. Fur-
thermore, it is quite a normal practice nowadays to
collect a human-human corpus on the target task do-
main. It is reasonable, then, to assume that all the
inputs of our algorithm are already available when
developing the virtual instructor, which was indeed
the case for the GIVE framework.
Another advantage of our approach is that vir-
tual instructor can be generated by developers with-
out any knowledge of generation of natural language
techniques. Furthermore, the actual implementation
of our algorithms is extremely simple as shown in
Figures 1 and 2. This makes our approach promising
for application areas such as games and simulation
training.
7 Future work and conclusions
In this paper we presented a novel algorithm for
automatically prototyping virtual instructors from
human-human corpora without manual annotation.
Using our algorithms and the GIVE corpus we have
generated a virtual instructor for a game-like vir-
tual environment. A video of our virtual instruc-
tor is available in http://cs.famaf.unc.edu.ar/
?luciana/give-OUR. We obtained encouraging re-
sults in the evaluation with human users that we did
on the virtual instructor. In our evaluation, our sys-
tem outperforms rule-based virtual instructors hand-
coded for the same task both in terms of objective
and subjective metrics. We plan to participate in the
GIVE Challenge 20112 in order to get more evalua-
tion data from online users and to evaluate our algo-
rithms on multiple worlds.
The algorithms we presented solely rely on the
plan to define what constitutes the context of utter-
ing. It may be interesting though to make use of
other kinds of features. For instance, in order to inte-
grate spatial orientation and differentiate ?turn left?
and ?turn right?, the orientation can be either added
to the planning domain or treated as a context fea-
ture. While it may be possible to add orientation
in the planning domain of GIVE, it is not straight-
forward to include the diversity of possible features
in the same formalization, like modeling the global
discourse history or corrections. Thus we plan to in-
vestigate the possibility of considering the context of
an utterance as a set of features, including plan, ori-
entation, discourse history and so forth, in order to
extend the algorithms presented in terms of context
building and feature matching operations.
In the near future we plan to build a new version
of the system that improves based on the error anal-
ysis that we did. For instance, we plan to take ori-
entation into account during selection. As a result
of these extensions however we may need to enlarge
the corpus we used so as not to increase the number
of situations in which the system does not find any-
thing to say. Finally, if we could identify corrections
automatically, as suggested in (Raux and Nakano,
2010), we could get an increase in performance, be-
cause we would be able to treat them as corrections
and not as instructions as we do now.
In sum, this paper presents the first existing al-
gorithm for fully-automatically prototyping task-
oriented virtual agents from corpora. The generated
agents are able to effectively and naturally help a
user complete a task in a virtual world by giving
her/him instructions.
2http://www.give-challenge.org/research
75
References
Sudeep Gandhe and David Traum. 2007a. Creating spo-
ken dialogue characters from corpora without annota-
tions. In Proceedings of 8th Conference in the Annual
Series of Interspeech Events, pages 2201?2204, Bel-
gium.
Sudeep Gandhe and David Traum. 2007b. First
steps toward dialogue modelling from an un-annotated
human-human corpus. In IJCAI Workshop on Knowl-
edge and Reasoning in Practical Dialogue Systems,
Hyderabad, India.
Andrew Gargett, Konstantina Garoufi, Alexander Koller,
and Kristina Striegnitz. 2010. The GIVE-2 corpus
of giving instructions in virtual environments. In Pro-
ceedings of the 7th International Conference on Lan-
guage Resources and Evaluation (LREC), Malta.
Jo?rg Hoffmann and Bernhard Nebel. 2001. The FF plan-
ning system: Fast plan generation through heuristic
search. JAIR, 14:253?302.
Chih-Wei Hsu, Benjamin W. Wah, Ruoyun Huang,
and Yixin Chen. 2006. New features in SGPlan
for handling soft constraints and goal preferences in
PDDL3.0. In Proceedings of ICAPS.
Dusan Jan, Antonio Roque, Anton Leuski, Jacki Morie,
and David Traum. 2009. A virtual tour guide for vir-
tual worlds. In Proceedings of the 9th International
Conference on Intelligent Virtual Agents, IVA ?09,
pages 372?378, Berlin, Heidelberg. Springer-Verlag.
Patrick Kenny, Thomas D. Parsons, Jonathan Gratch, An-
ton Leuski, and Albert A. Rizzo. 2007. Virtual pa-
tients for clinical therapist skills training. In Proceed-
ings of the 7th international conference on Intelligent
Virtual Agents, IVA ?07, pages 197?210, Berlin, Hei-
delberg. Springer-Verlag.
Alexander Koller, Kristina Striegnitz, Donna Byron, Jus-
tine Cassell, Robert Dale, Sara Dalzel-Job, Johanna
Moore, and Jon Oberlander. 2009. Validating the
web-based evaluation of nlg systems. In Proceedings
of ACL-IJCNLP 2009 (Short Papers), Singapore.
Alexander Koller, Kristina Striegnitz, Andrew Gargett,
Donna Byron, Justine Cassell, Robert Dale, Johanna
Moore, and Jon Oberlander. 2010. Report on the sec-
ond NLG challenge on generating instructions in vir-
tual environments (GIVE-2). In Proceedings of the In-
ternational Natural Language Generation Conference
(INLG), Dublin.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006. Building effective question
answering characters. In Proceedings of the 7th SIG-
dial Workshop on Discourse and Dialogue, SigDIAL
?06, pages 18?27, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Antoine Raux and Mikio Nakano. 2010. The dynam-
ics of action corrections in situated interaction. In
Proceedings of the SIGDIAL 2010 Conference, pages
165?174, Tokyo, Japan, September. Association for
Computational Linguistics.
Verena Rieser and Oliver Lemon. 2010. Learning hu-
man multimodal dialogue strategies. Natural Lan-
guage Engineering, 16:3?23.
Bayan Abu Shawar and Eric Atwell. 2003. Using di-
alogue corpora to retrain a chatbot system. In Pro-
ceedings of the Corpus Linguistics Conference, pages
681?690, United Kingdom.
Bayan Abu Shawar and Eric Atwell. 2005. Using cor-
pora in machine-learning chatbot systems. volume 10,
pages 489?516.
76
A Automatically annotated fragment of the GIVE corpus
Utterance: make a left and exit the room
Reaction: ?move(b2-room-1-9,room-1-9), move(room-1-9,room-1-8), move(room-1-8,room-1-7),
move(room-1-7,room-1-6), move(room-1-6,room-1-3), move(room-1-3,room-1-4),
move(room-1-4,room-1-5), move(room-1-5,d3-room-1-5)?
Utterance: go forward and turn 90 degrees
Reaction: ?move(d3-room-1-5,d3-room-2), move(d3-room-2,room-2)?
Utterance: go into the room on the right
Reaction: ?move(room-2,d6-room-2), move(d6-room-2,a2-d6-room-3), move(a2-d6-room-3,room-3)?
Utterance: push the green button to the left of the red button
Reaction: ?move(room-3,b6-room-3), manipulate-stateless(b6), move(b6-room-3,room-3)?
Utterance: go into the room on your right
Reaction: ?move(room-3,d11-room-3), move(d11-room-3,d11-room-7), move(d11-room-7,room-7)?
Utterance: turn 90 degrees right and push the red button
Reaction: ?move(room-7,b11-room-7), manipulate(b11), move(b11-room-7,room-7)?
Utterance: on your right, push the yellow button
Reaction: ?move(room-7,b10-room-7), manipulate-stateless(b10), move(b10-room-7,room-7)?
Utterance: turn 180 degrees and push the red button next to the plant
Reaction: ?move(room-7,b12-room-7), manipulate-stateless(b12), move(b12-room-7,room-7)?
Utterance: turn 180 degrees and push the blue button in the middle of the yellow and blue button
Reaction: ?move(room-7,b8-b9-room-7), manipulate-stateless(b9), move(b8-b9-room-7,room-7)?
Utterance: turn 90 degrees left
Reaction: ??
Utterance: go into the room on the right
Reaction: ?move(room-7,d10-room-7), move(d10-room-7,d10-room-6), move(d10-room-6,room-6)?
Utterance: turn right and proceed down the room
Reaction: ??
Utterance: push the red button next to the blue button on your right
Reaction: ?move(room-6,b13-b14-room-6), manipulate(b14), move(b13-b14-room-6,room-6)?
Utterance: turn left 120 degrees left
Reaction: ??
Utterance: and walk through the hall
Reaction: ?move(room-6,d9-room-6), move(d9-room-6,d9-room-5), move(d9-room-5,room-5)?
77
Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 33?37,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
A Natural Language Instructor for pedestrian navigation based in
generation by selection
Santiago Avalos
LIIS Group, FaMAF
Universidad Nacional de C?ordoba
C?ordoba, Argentina
santiagoe.avalos@gmail.com
Luciana Benotti
LIIS Group, FaMAF
Universidad Nacional de C?ordoba
C?ordoba, Argentina
luciana.benotti@gmail.com
Abstract
In this paper we describe a method for
developing a virtual instructor for pedes-
trian navigation based on real interactions
between a human instructor and a human
pedestrian. A virtual instructor is an agent
capable of fulfilling the role of a human
instructor, and its goal is to assist a pedes-
trian in the accomplishment of different
tasks within the context of a real city.
The instructor decides what to say using
a generation by selection algorithm, based
on a corpus of real interactions generated
within the world of interest. The instructor
is able to react to different requests by the
pedestrian. It is also aware of the pedes-
trian position with a certain degree of un-
certainty, and it can use different city land-
marks to guide him.
1 Introduction and previous work
Virtual instructors are conversational agents that
help a user perform a task. These agents can be
useful for many purposes, such as language learn-
ing (Nunan, 2004), training in simulated envi-
ronments (Kim et al., 2009) and entertainment
(Dignum, 2012; Jan et al., 2009).
Navigation agents generate verbal route direc-
tions for users to go from point A to point B in
a given world. The wide variety of techniques to
accomplish this task, range from giving complete
route directions (all route information in a single
instruction), to full interactive dialogue systems
which give incremental instructions based on the
position of the pedestrian. Although it can recog-
nize pre-established written requests, the instruc-
tor presented in this work is not able to interpret
utterances from the pedestrian, leaving it unable to
generate a full dialogue. The instructor?s decisions
are based on the pedestrian actual task, his posi-
tion in the world, and the previous behavior from
different human instructors. In order to guide a
user while performing a task, an effective instruc-
tor must know how to describe what needs to be
done in a way that accounts for the nuances of
the virtual world and that is enough to engage the
trainee or gamer in the activity.
There are two main approaches toward automat-
ically producing instructions. One is the selection
approach, in which the task is to pick the appropri-
ate output from a corpus of possible outputs. The
other is the composition approach, in which the
output is dynamically assembled using some com-
position procedure, e.g. grammar rules.
The natural language generation algorithm used
in this work is a modified version of the generation
by selection method described in (Benotti and De-
nis, 2011).
The advantages of generation by selection are
many: it affords the use of complex and human-
like sentences, the system is not bound to use writ-
ten instructions (it may easily use recorded audio
clips, for example), and finally, no rule writing by
a dialogue expert or manual annotations is needed.
The disadvantage of generation by selection is that
the resulting dialogue may not be fully coherent
(Shawar and Atwell, 2003; Shawar and Atwell,
2005; Gandhe and Traum, 2007).
In previous work, the selection approach to
generation has been used in non task-oriented
conversational agents such as negotiating agents
(Gandhe and Traum, 2007), question answering
characters (Leuski et al., 2006) and virtual pa-
tients (Kenny et al., 2007). In the work pre-
sented in this paper, the conversational agent is
task-oriented.
In Section 2 we introduce the framework used
in the interaction between the navigation agent and
the human pedestrians. We discuss the creation of
the human interaction corpus and the method for
natural language generation in Section 3; And in
Section 4 we explain the evaluation methods and
33
the expected results.
2 The GRUVE framework
One of the major problems in developing systems
that generate navigation instructions for pedestri-
ans is evaluating them with real users in the real
world. This evaluations are expensive, time con-
suming, and need to be carried out not just at the
end of the project but also during the development
cycle.
Consequently, there is a need for a common
platform to effectively compare the performances
of several verbal navigation systems developed by
different teams using a variety of techniques.
The GIVE challenge developed a 3D virtual in-
door environment for development and evaluation
of indoor pedestrian navigation instruction sys-
tems (Byron et al., 2007; Koller et al., 2007).
In this framework, users walk through a building
with rooms and corridors, and interact with the
world by pressing buttons. The user is guided by
a navigation system that generates route instruc-
tions.
The GRUVE framework presented in (Ja-
narthanam et al., 2012) is a web-based environ-
ment containing a simulated real world in which
users can simulate walking on the streets of real
cities whilst interacting with different navigation
systems. This system focus on providing a simu-
lated environment where people can look at land-
marks and navigate based on spatial and visual in-
structions provided to them. GRUVE also pro-
vides a embedded navigation agent, the Buddy
System, which can be used to test the framework.
Apart from the virtual environment in which they
are based an important difference between GIVE
and GRUVE is that, in GRUVE, there is a cer-
tain degree of uncertainty about the position of the
user.
Figure 1: Snapshot of the GRUVE web-client.
GRUVE presents navigation tasks in a game-
world overlaid on top of the simulated real world.
The main task consists of a treasure hunting simi-
lar to the one presented in GIVE. In our work, we
use a modified version of the original framework,
in which the main task has been replaced by a set
of navigation tasks.
The web-client (see Figure 1) includes an inter-
action panel that lets the user interact with his nav-
igation system. In addition to user location infor-
mation, users can also interact with the navigation
system using a fixed set or written utterances. The
interaction panel provided to the user consists of a
GUI panel with buttons and drop-lists which can
be used to construct and send requests to the sys-
tem in form of abstract semantic representations
(dialogue actions).
3 The virtual instructor
The virtual instructor is a natural language agent
that must help users reach a desired destination
within the virtual world. Our method for devel-
oping an instructor consists of two phases: an an-
notation phase and a selection phase. In Section
3.1 we describe the annotation phase. This is per-
formed only once, when the instructor is created,
and it consists of automatically generating a cor-
pus formed by associations between each instruc-
tion and the reaction to it. In Section 3.2 we de-
scribe how the utterance selection is performed ev-
ery time the virtual instructor generates an instruc-
tion.
3.1 Annotation
As described in (Benotti and Denis, 2011), the cor-
pus consists in recorded interactions between two
people in two different roles: the Direction Giver
(DG), who has knowledge of how to perform the
task, and creates the instructions, and the Direc-
tion Follower (DF), who travels through the envi-
ronment following those instructions.
The representation of the virtual world is given
by a graph of nodes, each one representing an in-
tersection between two streets in the city. GRUVE
provides a planner that can calculate the optimal
path from any starting point to a selected desti-
nation (this plan consists in the list of nodes the
user must travel to reach the desired destination).
As the DF user walks through the environment, he
cannot change the world that surrounds him. This
simplifies the automatic annotation process, and
34
the logged atoms are:
? user position: latitude and longitude, indicat-
ing position relative to the world.
? user orientation: angle between 0-360, indi-
cating rotation of the point of view.
In order to define the reaction associated to each
utterance, it is enough to consider the position to
which the user arrives after an instruction has been
given, and before another one is requested. Nine
destinations within the city of Edinburgh were se-
lected to be the tasks to complete (the task is to
arrive to each destination, from a common starting
point, see Figure 2). Each pair of DG and DF had
to complete all tasks and record their progress.
Figure 2: The 9 selected tasks .
For the creation of the corpus, a slightly mod-
ified version of the GRUVE wizards-desk was
used. This tool is connected to the GRUVE web-
client, and allows a human user to act as DF, gen-
erating instructions to assist the user in the com-
pletion of the task and monitoring his progression.
Each instruction generated by a DG was numbered
in order, in relation to each task. For example: if
the fifth instruction given by the third DG, while
performing the second task, was ?Go forward and
cross the square?, then that instruction was num-
bered as follows:
5.3.2? ?Go forward and cross the square?.
This notation was included to maintain the gener-
ation order between instructions (as the tasks were
given in an arbitrary specific order for each DG).
With last-generated, we refer to the instructions
that were generated in the last 3 runs of each DG.
This notion is needed to evaluate the effect of the
increasing knowledge of the city (this metric is ex-
plained in Section 4).
As discussed in (Benotti and Denis, 2011) mis-
interpreted instructions and corrections result in
clearly inappropriate instruction-reaction associa-
tions. Since we want to avoid any manual anno-
tation, but we also want to minimize the quantity
of errors inside the corpus, we decided to create
a first corpus in which the same person portraits
the roles of DG and DF. This allows us to elim-
inate the ambiguity of the instruction interpreta-
tion on the DF side, and eliminates correction in-
structions (instructions that are of no use for guid-
ance, but were made to correct a previous error
from the DG, or a wrong action from the DF).
Later on, each instruction in this corpus was per-
formed upon the virtual world by various others
users, their reactions compared to the original re-
action, and scored. For each task, only the instruc-
tions whose score exceeded an acceptance thresh-
old remained in the final corpus.
3.2 Instruction selection
The instruction selection algorithm, displayed in
Algorithm 1 consists in finding in the corpus the
set of candidate utterances C for the current task
plan P, which is the sequence of actions that needs
to be executed in the current state of the virtual
world in order to complete the task. We use the
planner included in GRUVE to create P. We de-
fine:
C = {U ? Corpus | P starts with U.Reaction}
In other words, an utterance U belongs to C if the
first action of the current plan P exactly matches
the reaction associated to the utterance U. When-
ever the plan P changes, as a result of the actions
of the DF, we call the selection algorithm in order
to regenerate the set of candidate utterances C.
Algorithm 1 Selection Algorithm
C ? ?
action? nextAction(currentObjective)
for all Utterance U ? Corpus do
if action = U.Reaction then
C ? C ? U
end if
end for
All the utterances that pass this test are consid-
ered paraphrases and hence suitable in the current
context. Given a set of candidate paraphrases, one
has to consider two cases: the most frequent case
when there are several candidates and the possible
case when there is no candidate.
35
? No candidate available: If no instruction is
selected because the current plan cannot be
matched with any existing reaction, a default,
neutral, instruction ?go? is uttered.
? Multiple candidates available: When multi-
ple paraphrases are available, the agent must
select one to transmit to the user. In this case,
the algorithm selects one from the set of the
last-generated instructions for the task (see
Section 3.1).
4 Evaluation and expected results
Is this section we present the metrics and evalua-
tion process that will be performed to test the vir-
tual instructor presented in Section 3, which was
generated using the dialogue model algorithm in-
troduced in Section 3.2.
4.1 Objective metrics
The objective metrics are summarized below:
? Task success: successful runs.
? Canceled: runs not finished.
? Lost: runs finished but failed.
? Time (sec): average for successful runs.
? Utterances: average per successful run.
With this metrics, we will compare 3 systems:
agents A, B and C.
Agent A is the GRUVE buddy system, which
is provider by the GRUVE Challenge organizers
as a baseline. Agent B consists of our virtual in-
structor, configured to select a random instruction
when presented with multiple candidates (see Sec-
tion 3.1). Agent C is also our virtual instructor, but
when presented with several candidates, C selects
a candidate who is also part of the last-generated
set. As each task was completed in different or-
der by each DG when the corpus was created, it
is expected that in every set of candidates, the
most late-generated instructions were created with
greater knowledge of the city.
4.2 Subjective metrics
The subjective measures will be obtained from re-
sponses to a questionnaire given to each user at the
end of the evaluation, based partially on the GIVE-
2 Challenge questionnaire (Koller et al., 2010). It
ask users to rate different statements about the sys-
tem using a 0 to 10 scale.
The questionnaire will include 19 subjective
metrics presented below:
Q1: The system used words and phrases that
were easy to understand.
Q2: I had to re-read instructions to understand
what I needed to do.
Q3: The system gave me useful feedback about my
progress.
Q4: I was confused about what to do next.
Q5: I was confused about which direction to go
in.
Q6: I had no difficulty with identifying the objects
the system described for me.
Q7: The system gave me a lot of unnecessary
Information.
Q8: The system gave me too much information all
at once.
Q9: The system immediately offered help when I
was in trouble.
Q10: The system sent instructions too late.
Q11: The systems instructions were delivered too
early.
Q12: The systems instructions were clearly
worded.
Q13: The systems instructions sounded robotic.
Q14: The systems instructions were repetitive.
Q15: I lost track of time while solving the overall
task.
Q16: I enjoyed solving the overall task.
Q17: Interacting with the system was really
annoying.
Q18: The system was very friendly.
Q19: I felt I could trust the systems instructions.
Metrics Q1 to Q12 assess the effectiveness and
reliability of instructions, while metrics Q13 to
Q19 are intended to assess the naturalness of the
instructions, as well as the immersion and engage-
ment of the interaction.
4.3 Expected results
Based on the results obtained by (Benotti and De-
nis, 2011) in the GIVE-2 Challenge, we expect a
good rate of successful runs for the agent. Further-
more, the most interesting part of the evaluation
resides in the comparison between agents B and C.
We expect that the different selection methods of
this agents, when presented with multiple instruc-
tion candidates, can provide information about the
form in which the level of knowledge of the vir-
tual world or environment modifies the capacity
of a Direction Giver to create correct, and useful,
instructions.
36
References
Luciana Benotti and Alexandre Denis. 2011. Giv-
ing instructions in virtual environments by corpus
based selection. In Proceedings of the SIGDIAL
2011 Conference, SIGDIAL ?11, pages 68?77. As-
sociation for Computational Linguistics.
D. Byron, A. Koller, J. Oberlander, L. Stoia, and
K. Striegnitz. 2007. Generating instructions in vir-
tual environments (give): A challenge and evalua-
tion testbed for nlg. In Proceedings of the Work-
shop on Shared Tasks and Comparative Evaluation
in Natural Language Generation.
Frank Dignum. 2012. Agents for games and simula-
tions. Autonomous Agents and Multi-Agent Systems,
24(2):217?220, March.
S. Gandhe and D. Traum. 2007. First steps toward
dialogue modelling from an un-annotated human-
human corpus. In IJCAI Workshop on Knowledge
and Reasoning in Practical Dialogue Systemss.
Dusan Jan, Antonio Roque, Anton Leuski, Jacki Morie,
and David Traum. 2009. A virtual tour guide for
virtual worlds. In Proceedings of the 9th Interna-
tional Conference on Intelligent Virtual Agents, IVA
?09, pages 372?378, Berlin, Heidelberg. Springer-
Verlag.
Srinivasan Janarthanam, Oliver Lemon, and Xingkun
Liu. 2012. A web-based evaluation framework for
spatial instruction-giving systems. In Proceedings
of the ACL 2012 System Demonstrations, ACL ?12,
pages 49?54. Association for Computational Lin-
guistics.
Patrick Kenny, Thomas D. Parsons, Jonathan Gratch,
Anton Leuski, and Albert A. Rizzo. 2007. Vir-
tual patients for clinical therapist skills training. In
Proceedings of the 7th International Conference on
Intelligent Virtual Agents, IVA ?07, pages 197?210,
Berlin, Heidelberg. Springer-Verlag.
Julia M. Kim, Randall W. Hill, Jr., Paula J. Durlach,
H. Chad Lane, Eric Forbell, Mark Core, Stacy
Marsella, David Pynadath, and John Hart. 2009. Bi-
lat: A game-based environment for practicing nego-
tiation in a cultural context. Int. J. Artif. Intell. Ed.,
19(3):289?308, August.
A. Koller, J. Moore, B. Eugenio, J. Lester, L. Stoia,
D. Byron, J. Oberlander, and K. Striegnitz. 2007.
Shared task proposal: Instruction giving in virtual
worlds. In In Workshop on Shared Tasks and Com-
parative Evaluation in Natural Language Genera-
tion.
Alexander Koller, Kristina Striegnitz, Andrew Gargett,
Donna Byron, Justine Cassell, Robert Dale, Johanna
Moore, and Jon Oberlander. 2010. Report on the
second nlg challenge on generating instructions in
virtual environments (give-2). In Proceedings of
the 6th International Natural Language Generation
Conference, INLG ?10, pages 243?250. Association
for Computational Linguistics.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006. Building effective ques-
tion answering characters. In Proceedings of the 7th
SIGdial Workshop on Discourse and Dialogue, Sig-
DIAL ?06, pages 18?27. Association for Computa-
tional Linguistics.
David Nunan. 2004. Task-based language teaching.
University Press, Cambridge.
B.A. Shawar and E. Atwell. 2003. Using dialogue
corpora to retrain a chatbot system. In Proceedings
of the Corpus Linguistics Conference, pages 681?
690.
B.A. Shawar and E. Atwell. 2005. Using corpora
in machine-learning chatbot systems. International
Journal of Corpus Linguistics, 10:489?516.
37
Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 38?42,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Mining human interactions to construct a virtual guide for a virtual fair
Andr
?
es Luna
LIIS Group, FaMAF
Universidad Nacional de C?ordoba
C?ordoba, Argentina
andres.ignacio.luna@gmail.com
Luciana Benotti
LIIS Group, FaMAF
Universidad Nacional de C?ordoba
C?ordoba, Argentina
luciana.benotti@gmail.com
Abstract
In this paper we describe how we mine in-
teractions between a human guide and a
human visitor to build a virtual guide. A
virtual guide is an agent capable of fulfill-
ing the role of a human guide. Its goal is
to guide visitors to each booth of a virtual
fair and to provide information about the
company or organization through interac-
tive objects located at the fair.
The guide decides what to say, using a
graph search algorithm, and decides how
to say using generation by selection based
on contextual features. The guide decides
where to speak at the virtual fair by creat-
ing clusters using a data classification al-
gorithm to learn in what positions the hu-
man guide decided to talk.
1 Introduction and previous work
Fairs are spaces where companies that offer simi-
lar products and services meet to promote them. A
virtual fair emulates a real fair and can be available
before the real fair happens in order to promote it
to its potential visitors.
The virtual fair used in this work is a tourism
fair that took place in Mexico, where visitors could
find in each company?s booth interactive video and
links to tourist companies? websites promoting
particular products. The goal of the virtual guide
is to walk the user through the virtual fair, provid-
ing information about the companies? booths and
inviting them to click on interactive objects to ob-
tain more information.
In (Jan et al., 2009) the authors describe a vir-
tual guide used to promote an island in the online
game Second Life whose goal was to provide in-
formation to US army veterans. Our approach dif-
fers to that of (Jan et al., 2009) in that the virtual
guide learns where to speak and how to realize
its contributions from an automatically annotated
corpus, rather than by using manually designed
rules. However, our guide is not able to interpret
utterances from the visitor, its decisions are only
based on the visitor behavior. Natural language
generation is achieved by adapting the generation
by selection method described in (Benotti and De-
nis, 2011a; Benotti and Denis, 2011b).
The generation by selection method affords the
use of complex and human-like sentences, and
it does not need rule writing by a dialogue ex-
pert or manual annotations, among other of their
many advantages. The disadvantage of corpus
based generation is that the resulting dialogue may
not be fully coherent. Shawar and Atwell (2003;
2005) present a method for learning pattern match-
ing rules from corpora in order to obtain the
dialogue manager for a chatbot. Gandhe and
Traum (2007a; 2007b) investigate several dia-
logue models for negotiating virtual agents that are
trained on an unannotated human-human corpus.
Both approaches report that the dialogues obtained
by these methods are still to be improved because
the lack of dialogue history management results
in incoherence. Since in task-based systems, the
dialogue history is restricted by the structure of
the task, the absence of dialogue history manage-
ment is alleviated by tracking the current state of
the task.
In Section 2 we introduce the corpus used by
this work. We discuss the clustering method used
on the corpus in Section 3; the clustering is used
to decide where to speak. After that, we describe
in Section 4 the mechanisms for instruction gener-
ation and graph search used to guide the visitors.
Later, in Section 5 we show the results obtained
in the evaluation process and compare our sys-
tem?s performance with other virtual instructors.
Finally, in Section 6 we elaborate a conclusion
about the virtual guide performance and capabili-
ties, as well as discuss the possible improvements.
38
2 Virtual guide human-human corpus
We collected a corpus using a human guide in a
wizard of Oz setup (Kelley, 1983). The corpus is
comprised by 5 correct sessions in total performed
by the same virtual tour guide, and according to
the desired behavior and actions as specified for
both participants. We recorded 2 hours and 2 min-
utes of virtual fair guided visits which produced
a total of 136 utterances, having employed 18.02
words and 89.29 characters in average per utter-
ance. 9 different interactive objects were clicked
located in 4 different booths in average per ses-
sion. In Figure 1 we show an aerial view of the vir-
tual fair and the occurrence of utterances, marked
in blue.
Figure 1: Map of registered utterances in corpus.
A higher color intensity denotes a higher utterance
density in the area.
3 Behavior-based utterance clustering
The generation by selection method that we use in
this work is based on contextual features, in partic-
ular it is based on the position of the visitor inside
the virtual fair and the actions that are affordable
from that region in the fair. Deciding whether two
positions in the fair have the same affordances, or,
as we call it, fall into the same region is critical to
select appropriate utterances from the corpus de-
pending on the guide?s location and task progress.
The discretization employed in (Benotti and
Denis, 2011a) was geometrical discretization, di-
viding the world in regions based on the area vis-
ible to the guide. Instead of doing a geometri-
cal discretization our virtual fair discretization was
behavior-oriented which means that regions are
delimited by clustering utterances that were ut-
tered in a close position from each other. In the
corpus utterances tend to cluster around decision
points, locations there is more than one affordable
and salient action available to the user and when
the help and direction of the guide is required.
Geometrical region identification based on vis-
ibility normally requires a larger corpus in order
to get a correct utterance generation, because the
chance of having a region without any utterance
occurrence inside is higher. In such discretiza-
tion, different regions may contain a very differ-
ent number of utterances while using behavior-
oriented discretization results in regions with a
similar number of utterances each. That is why
the behavior-oriented discretization is an advan-
tage for our virtual guide, since our corpus is con-
siderably smaller to that used in (Benotti and De-
nis, 2011a).
We ran a modified version of the k-means clus-
tering algorithm (Pakhira, 2009) that avoids empty
clusters over our corpus to group instructions. As
paraphrase instructions, while performing a task,
occur in a same decision point, then we wanted
close instructions to be in the same cluster, and
therefore our criteria of ?similarity? between them
was euclidean distance. Ideally, different decision
points should be in different clusters to guarantee
selected utterances are appropriate in every situa-
tion.
Let us visualize virtual fair as a directed graph
(V,E) where V = regions, and if a, b ? V
then (a, b) ? E if and only if there is at least
one utterance in the corpus whose immediate re-
action was moving from region a to the region b.
If we choose a low number of clusters the k-means
clustering algorithm would cluster instructions of
different nature, and conversely a too high value
would make the virtual fair disconnected. Then, to
obtain an optimal clustering -and therefore an op-
timal discretization- we maximize the k parameter
such that the virtual fair?s graph is still connected.
Discretization is finally obtained by matching
every position (x, y) in the environment to the
nearest cluster?s centroid. We show in Figure 2 the
virtual fair discretized in k = 22 regions, as that
number was the maximum number of clusters we
39
could reach without breaking the graph connectiv-
ity. Regions are delimited by lines and centroids
are represented by white squares.
Figure 2: Virtual fair divided in k = 22 regions
4 The virtual guide
The virtual guide must direct visitors through the
fair to interactive objects in order to complete its
promotion duty in each visit session. We show in
Figure 3 a situation in which a visitor is near an
interactive object and the virtual guide encourages
him/her to click it generating an utterance whose
translation is ?If you click on the green cube you
will access Lawson?s website where you can learn
more about them and the communication services
they offer?.
We can see the use of a referring expression, a
negative politeness strategy (Brown and Levinson,
1987) to suggest an action but not impose it while
some information about the Lawson firm is given.
In subsection 4.1 we discuss about the corpus
automatic annotation. Then we describe how ut-
terances are selected in subsection 4.2.
4.1 Corpus annotation
Our annotation process was simpler and more
straightforward than (Benotti and Denis, 2011a),
where artificial intelligence planning is used to
normalize reactions, mainly due to the fact that
users can not change the virtual fair state during
their visit, they can only change their own posi-
Figure 3: The virtual guide took the visitor to an
interactive object and encourages him/her to ma-
nipulate it
tion and visibility area (defined by the orientation
in the virtual fair) and manipulate interactive ob-
jects.
In a virtual fair visit, the set of user?s relevant
actions are:
? Move from one region to another
? Change orientation to left or right
? Click on an interactive object
Consequently, the set of atoms representing a
virtual fair?s state was simplified to
? user-region(region)
? user-orientation(x,y,z,w)
1
? clicked(anInteractiveObject)
In short, to do automatic annotation on the vir-
tual guide?s corpus, it was sufficient to observe the
subsequent action to each utterance by looking for
a change on any of the atoms shown above, and
annotating and associating the corresponding reac-
tion to the utterance and the valid atoms set when
it was said.
4.2 Selecting what to say
The virtual guide?s goal is to make the visitor visit
a number of given objectives, namely a set of
stands and interactive objects. Using the virtual
fairs discretization and taking the directed graph
representation we presented in Section 3, the vir-
tual guide uses the A* algorithm to obtain a path,
that is a sequence of actions, from its current po-
sition to the region where the next objective is lo-
cated. In case the visitor got lost or simply took
an alternative path, the virtual guide recalculates
the shortest path and proceeds to guide the visitor
through it.
1
In quaternion representation
40
Clearly, in order to do this calculation it is criti-
cal that every objective is reachable from any node
in the graph, so choosing a k parameter in the dis-
cretization process must be done taking care of
that.
The virtual guide gives the visitor a new instruc-
tion depending on next actions to perform using
the selection algorithm taken from (Benotti and
Denis, 2011a), shown in Algorithm 1. The al-
gorithm obtains set of utterances C, all of which
have a reaction that corresponds to the sequence
of actions that the virtual guide wants the visitor
to perform next.
Algorithm 1 Virtual guide?s selection algorithm
C ? ?
action? nextAction(currentObjective)
for all Utterance U ? Corpus do
if action ? U.Reaction then
C ? C ? U
end if
end for
5 Evaluation results
In the evaluation process 11 evaluators partici-
pated, completing the proposed visit to the virtual
fair, each manipulating 9 interactive objects. Eval-
uators were also asked to complete a questionnaire
after the tour, in which we wanted to obtain several
subjective metrics. We were particularly interested
in the questions
? S1: I had difficulties identifying the objects
that the system described for me
? S2: The Utterances sounded robotic
? S3: The system was repetitive
where we previously supposed the virtual guide
would have better results than other virtual instruc-
tors, if we consider the results showed in (Benotti
and Denis, 2011a).
We compared our virtual guide results with the
two best symbolic systems built for another vir-
tual environment, the GIVE-2 Challenge. Those
systems were NA from INRIA and SAAR from
University of Saarland (see (Koller et al., 2010)).
Furthermore, we checked if the virtual guide re-
sults were similar to another virtual instructor, also
built for GIVE-2, called OUR, in which generation
by selection was applied to make natural language
generation possible.
In Table 1 we show the results for each virtual
instructor in the three categories we are interested.
We can see that the virtual guide obtained signif-
icantly better results than the SAAR and NA and
in questions S1, S2 and S3, as we had supposed.
All three questions range from 1 (one) to 9 (nine),
the lower the number the better the system (since
questions are negative).
Table 1: Results comparison between virtual guide
and three GIVE-2 systems
Question NA SAAR OUR VP
S1 4.1 4 3 1.81
S2 5.2 4.75 3.6 1.82
S3 6.55 6.3 5.4 2
6 Conclusions and future work
In this paper we described the construction of a
virtual guide for a virtual fair with the purpose of
guiding visitors through the stands and to interac-
tive objects located inside the fair. Inmersive vir-
tual fairs and expositions constitute a promising
way to promote such events.
On our evaluation, the virtual guide had com-
parable results than the virtual instructor GIVE-2
implemented using generation by selection, using
a much smaller corpus. Our guide got better re-
sults that the two best performing symbolic sys-
tems. These results are preliminary, but also en-
couraging.
A possible extension of this work could be that
virtual guide can continue to improve its behavior
by learning online when input from a human guide
of the fair is available. If more corpus is available
in this way the virtual guide could discard those
utterances that do not lead most visitors to perform
the intended reaction.
As a result of this work we conclude that vir-
tual guide met the basic functions of navigation
and natural language generation that we expected
and that the resulting prototype is ready to be
deployed at the virtualization of events website
http://www.inixiavf.com/.
References
Luciana Benotti and Alexandre Denis. 2011a. Giving
instructions in virtual environments by corpus based
selection. In Proceedings of the SIGDIAL 2011
41
Conference, SIGDIAL ?11, pages 68?77, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Luciana Benotti and Alexandre Denis. 2011b. Pro-
totyping virtual instructors from human-human cor-
pora. In Proceedings of the ACL-HLT 2011 Sys-
tem Demonstrations, pages 62?67, Portland, Ore-
gon, June. Association for Computational Linguis-
tics.
Penelope Brown and Stephen Levinson. 1987. Polite-
ness: Some Universals in Language Usage. Studies
in Interactional Sociolinguistics. Cambridge Univer-
sity Press.
Sudeep Gandhe and David Traum. 2007a. Creating
spoken dialogue characters from corpora without an-
notations. In Proceedings of 8th Conference in the
Annual Series of Interspeech Events, pages 2201?
2204, Belgium.
Sudeep Gandhe and David Traum. 2007b. First steps
toward dialogue modelling from an un-annotated
human-human corpus. In IJCAI Workshop on
Knowledge and Reasoning in Practical Dialogue
Systems, Hyderabad, India.
Dusan Jan, Antonio Roque, Anton Leuski, Jacki Morie,
and David Traum. 2009. A virtual tour guide for
virtual worlds. In Proceedings of the 9th Interna-
tional Conference on Intelligent Virtual Agents, IVA
?09, pages 372?378, Berlin, Heidelberg. Springer-
Verlag.
John F. Kelley. 1983. An empirical methodology for
writing user-friendly natural language computer ap-
plications. In Proceedings of the SIGCHI Confer-
ence on Human Factors in Computing Systems, CHI
?83, pages 193?196, New York, NY, USA. ACM.
Alexander Koller, Kristina Striegnitz, Andrew Gargett,
Donna Byron, Justine Cassell, Robert Dale, Johanna
Moore, and Jon Oberlander. 2010. Report on the
second nlg challenge on generating instructions in
virtual environments (give-2). In Proceedings of
the 6th International Natural Language Generation
Conference, INLG ?10, pages 243?250, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Malay K. Pakhira. 2009. A modified k-means algo-
rithm to avoid empty clusters. International Journal
of Recent Trends in Engineering, 1(1):220?226.
Bayan Abu Shawar and Eric Atwell. 2003. Using
dialogue corpora to retrain a chatbot system. In
Proceedings of the Corpus Linguistics Conference,
pages 681?690, United Kingdom.
Bayan Abu Shawar and Eric Atwell. 2005. Using
corpora in machine-learning chatbot systems. vol-
ume 10, pages 489?516.
42
