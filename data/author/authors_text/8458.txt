Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 472?475,
Prague, June 2007. c?2007 Association for Computational Linguistics
UVAVU: WordNet Similarity and Lexical Patterns
for Semantic Relation Classication
Willem Robert van Hage
TNO Science & Industry
Stieltjesweg 1, 2628CK Delft
the Netherlands
wrvhage@few.vu.nl
Sophia Katrenko
HCSL, University of Amsterdam
Kruislaan 419, 1098VA Amsterdam
the Netherlands
katrenko@science.uva.nl
Abstract
The system we propose to learning seman-
tic relations consists of two parallel com-
ponents. For our final submission we used
components based on the similarity mea-
sures defined over WordNet and the patterns
extracted from the Web and WMTS. Other
components using syntactic structures were
explored but not used for the final run.
1 Experimental Set-up
The system we used to classify the semantic rela-
tions consists of two parallel binary classifiers. We
ran this system for each of the seven semantic re-
lations separately. Each classifier predicts for each
instance of the relation whether it holds or not. The
predictions of all the classifiers are aggregated for
each instance by disjunction. That is to say, each in-
stance is predicted to be false by default unless any
of the classifiers gives evidence against this.
To generate the submitted predictions we used
two parallel classifiers: (1) a classifier that com-
bines eleven WordNet-based similarity measures,
see Sec. 2.1, and (2) a classifier that learns lexical
patterns from Google and the Waterloo Multi-Text
System (WMTS)(Turney, 2004) snippets and ap-
plies these on the same corpora, see Sec. 2.2.
Three other classifiers we experimented with, but
that were not used to generate the submitted predic-
tions: (3) a classifier that uses string kernel methods
on the dependency paths of the training sentences,
see Sec. 3.1, (4) a classifier that uses string kernels
on the local context of the subject and object nom-
inals in the training sentences, see Sec. 3.2 and (5)
a classifier that uses hand-made lexical patterns on
Google and WMTS, see Sec. 3.3.
2 Submitted Run
2.1 WordNet-based Similarity Measures
WordNet 3.0 (Fellbaum, 1998) is the most fre-
quently used lexical database of English. As this re-
source consists of lexical and semantic relations, its
use constitutes an appealing option to learning rela-
tions. In particular, we believe that given two men-
tions of the same semantic relation, their arguments
should also be similar. Or, in analogy learning terms,
if R1(X1,Y1) and R2(X2,Y2) are relation mentions ofthe same type, then X1 :: Y1 as X2 :: Y2. Our prelim-inary experiments with WordNet suggested that few
arguments of each relation are connected by imme-
diate hyperonymy or meronymy relations. As a re-
sult, we decided to use similarity measures defined
over WordNet (Pedersen et al, 2004). The Word-
Net::Similarity package (Pedersen et al, 2004) in-
cludes 11 different measures, which mostly use ei-
ther the WordNet glosses (lesk or vector measures)
or the paths between a pair of concepts (lch; wup) to
determine their relatedness.
To be able to use WordNet::Similarity, we
mapped all WordNet sense keys from the training
and test sets to the earlier WordNet version (2.1).
Given a relation R(X ,Y ), we computed the related-
ness scores for each pair of arguments X and Y . The
scores together with the sense keys of arguments
were further used as features for the machine learn-
ing method. As there is no a priori knowledge on
what measures are the most important for each rela-
472
tion, all of them were used and no feature selection
step has been taken.
We experimented with a number of machine
learning methods such as k-nearest neighbour al-
gorithm, logistic regression, bayesian networks and
others. For each relation a method performing best
on the training set was selected (using 5-fold cross-
validation).
2.2 Learnt Lexical Patterns
This classifier models the intuition that when a pair
of nominals is used in similar phrases as another pair
they share at least one relation, and when no such
phrases can be found they do not share any relation.
Applied to the semantic relation classification prob-
lem this means that when a pair in the test set can be
found in the same patterns as pairs from the training
set, the classification for the pair will be true.
To find the patterns we followed step 1 to 6 de-
scribed in (Turney, 2006), with the exception that
we used both Google and the WMTS to compute
pattern frequency.
First we extracted the pairs of nominals ?X ,Y ?
from the training sentences and created one Google
query and a set of WMTS queries for each pair.
The Google queries were of the form "X * Y"
OR "Y * X". Currently, Google performs mor-
phological normalization on every query, so we
did not make separate queries for various endings
of the nominals. For the WMTS we did make
separate queries for various morphological varia-
tions. We used the following set of suffixes: ?-
tion(s|al)?, ?-ly?, ?-ist?, ?-ical?, ?-y?, ?-ing?, ?-ed?,
?-ies?, and ?-s?. For this we used Peter Turney?s
pairs Perl package. The WMTS queries looked
like [n]>([5].."X"..[i].."Y"..[5]) and
[n]>([5].."Y"..[i].."X"..[5]) for i =
1,2,3 and n = i+12, and for each variation of X and
Y . Then we extracted sentences from the Google
snippets and cut out a context of size 5, so that
we were left with similar text segments as those
returned by the WMTS queries. We merged the
lists of text segments and counted all n-grams that
contained both nominals for n = 1 to 6. We sub-
stituted the nominals by variables in the n-grams
with a count greater than 10 and used these as pat-
terns for the classifier. An example of such a pat-
tern for the Cause-Effect relation is "generation
of Y by X". After this we followed step 3 to
6 of (Turney, 2006), which left us with a matrix
for each of the seven semantic relations, where each
row represented a pair of nominals and each column
represented the frequency of a pattern, and where
each pair was classified as either true or false. The
straightforward way to find pattern frequencies for
the pairs in the test set would be to fill in these pat-
terns with the pairs of nominals from the test set.
This was not feasible given the time limitation on
the task. So instead, for each pair of nominals in
the test set we gathered the top-1000 snippets and
computed pattern frequencies by counting how of-
ten the nominals occur in every pattern on this set
text segments. We constructed a matrix from these
frequencies in the same way as for the training set,
but without classifications for the pairs. We experi-
mented with various machine learning algorithms to
predict the classes of the pairs. We chose to use k-
nearest neighbors, because it was the only algorithm
that gave more subtle predictions than true for every
pair or false for every pair. For each semantic rela-
tion we used the value of k that produced the highest
F1 score on 5-fold cross validation on the trainingdata.
3 Additional Runs
3.1 String Kernels on Dependency Paths
It has been a long tradition to use syntactic structures
for relation extraction task. Some of the methods
as in (Katrenko and Adriaans, 2004) have used in-
formation extracted from the dependency trees. We
followed similar approach by considering the paths
between each pair of arguments X and Y . Ideally, if
each syntactic structure is a tree, there is only one
path from one node to the other. After we have ex-
tracted paths, we used them as input for the string
kernel methods (Hal Daum? III, 2004). The advan-
tage of using string kernels is that they can handle
sequences of different lengths and already proved to
be efficient for a number of tasks.
All sentences in the training data were parsed
using MINIPAR (Lin, 1998). From each depen-
dency tree we extracted a dependency path (if any)
between the arguments by collecting all lemmas
(nodes) and syntactic functions (edges). The se-
quences we obtained were fed into string kernel.
473
To assess the results, we carried out 5-fold cross-
validation. Even by optimizing the parameters of
the kernel (such as the length of subsequences) for
each relation, the highest accuracy we obtained was
equal 61,54% (on Origin-Entity relation) and the
lowest was accuracy for the Instrument-Agency re-
lation (50,48%).
3.2 String Kernels on Local Context
Alternatively to syntactic information, we also ex-
tracted the snippets of the fixed length from each
sentence. For each relation mention of R(X ,Y ), all
tokens between the relation arguments X and Y were
collected along with at most three tokens to the left
and to the right. Unfortunately, the results we ob-
tained on the training set were comparable to those
obtained by string kernels on dependency paths and
less accurate than the results provided by WordNet
similarity measures or patterns extracted from the
Web and WMTS. As a consequence, string kernel
methods were not used for the final submission.
3.3 Manually-created Lexical Patterns
The results of the method described in Sec. 2.2 are
quite far below what we expected given earlier re-
sults in the literature (Turney, 2006; van Hage, Ka-
trenko, and Schreiber, 2005; van Hage, Kolb, and
Schreiber, 2006; Berland and Charniak, 2006; Et-
zioni et al, 2004). We think this is caused by
the fact that many pairs in the training set are non-
stereotypical examples. So often the most com-
monly described relation of such a pair is not the re-
lation we try to classify with the pair. For example,
common associations with the pair ?body,parents?
are that it is the parents? body, or that the parents
are member of some organizing body, while it is a
positive example for the Product-Producer relation.
We wanted to see if this could be the case by testing
whether more intuitive patterns give better results on
the test set. The patterns we manually created for
each relation are shown in Table 1. If a pair gives
any results for these patterns on Google or WMTS,
we classify the pair as true, otherwise we classify
it as false. The results are shown in Table 2. We
did not use these results for the submitted run, be-
cause only automatic runs were permitted. The man-
ual patterns did not yield many useful results at all.
Apparently intuitive patterns do not capture what is
required to classify the relations in the test set. The
patterns we used for the Part-Whole (6) relation had
an average Precision of .50, which is much lower
than the average Precision found in (van Hage, Kolb,
and Schreiber, 2006), which was around 0.88. We
conclude that both the sets of training and test ex-
amples capture different semantics of the relations
than the intuitive ones, which causes common sense
background knowledge, such as Google to produce
bad results.
rel. patterns
1. X causes Y, X caused by Y, X * cause Y
2. X used Y, X uses Y, X * with a Y
3. X made by Y, X produced by Y, Y makes X,
Y produces X
4. Y comes from X, X * source of Y, Y * from * X
5. Y * to * X, Y * for * X, used Y for * X
6. X in Y, Y contains X, X from Y
7. Y contains X, X in Y, X containing Y, X into Y
Table 1: Hand-written patterns.
relation N Prec. Recall F1 Acc.1. Cause-Effect 6 1 0.15 0.25 0.56
2. Instr.-Agency 2 1 0.05 0.10 0.54
3. Prod.-Prod. 4 0.75 0.05 0.09 0.35
4. Origin-Ent. 6 0.33 0.05 0.09 0.35
5. Theme-Tool 2 0 0 0 0.56
6. Part-Whole 16 0.50 0.31 0.38 0.64
7. Cont.-Cont. 11 0.54 0.16 0.24 0.50
Table 2: Results for hand-written lexical patterns on
Google and WMTS.
4 Results
4.1 WordNet-based Similarity Measures
Table 3 shows the results of the WordNet-based sim-
ilarity measure method. In the ?methods? column,
the abbreviation LR stands for logistic regression,
K-NN stands for k-nearest neighbour, and DT stands
for decision trees.
relation method Prec. Recall F1 Acc.1. Cause-Effect LR 0.48 0.51 0.49 0.45
2. Instr.-Agency DT 0.65 0.63 0.64 0.62
3. Prod.-Prod. DT 0.67 0.50 0.57 0.46
4. Origin-Ent. LR 0.50 0.47 0.49 0.49
5. Theme-Tool LR 0.54 0.52 0.53 0.62
6. Part-Whole DT 0.54 0.73 0.62 0.67
7. Cont.-Cont. 2-NN 0.66 0.55 0.60 0.62
Table 3: Results for similarity-measure methods.
474
4.2 Learnt Lexical Patterns
Table 4 shows the results of the learnt lexical pat-
terns method. For all relations we used the k-nearest
neighbour method.
relation method Prec. Recall F1 Acc.1. Cause-Effect 3-NN 0.53 0.76 0.63 0.54
2. Instr.-Agency 2-NN 0.47 0.89 0.62 0.46
3. Prod.-Prod. 2-NN 0 0 0 0.33
4. Origin-Ent. 2-NN 0.47 0.22 0.30 0.54
5. Theme-Tool 3-NN 0.39 0.93 0.55 0.38
6. Part-Whole 2-NN 0.36 1 0.53 0.36
7. Cont.-Cont. 2-NN 0.51 0.97 0.67 0.51
Table 4: Results for learnt lexical patterns on Google
and WMTS.
5 Discussion
Our methods had the most difficulty with classify-
ing relation 1, 3 and 4. We wanted to see if hu-
man assessors perform less consistent for those re-
lations. If so, then those relations would simply be
harder to classify. Otherwise, our system performed
worse for those relations. We manually assessed ten
sample sentences from the test set, five of which
were positive examples and five were false exam-
ples. The result of a comparison with the test set is
shown in Table 5. The numbers listed there repre-
sent the fraction of examples on which we agreed
with the judges of the test set. There was quite a
inter-judge agreement
relation judge 1 judge 2
1. Cause-Effect 0.93 0.93
2. Instrument-Agency 0.77 0.77
3. Product-Producer 0.87 0.80
4. Origin-Entity 0.80 0.77
5. Theme-Tool 0.80 0.77
6. Part-Whole 0.97 1.00
7. Content-Container 0.77 0.77
Table 5: Inter-judge agreement.
large variation in the inter-judge agreement, but for
relation 1 and 3 the consensus was high. We con-
clude that the reason for our low performance on
those relations are not caused by the difficulty of
the sentences, but due to other reasons. Our intu-
ition is that the sentences, especially those of rela-
tion 1 and 3, are easily decidable by humans, but
that they are non-stereotypical examples of the re-
lation, and thus hard to learn. The following ex-
ample sentence breaks common-sense domain and
range restrictions: Product-Producer #142 ?And, of
course, everyone wants to prove the truth of their be-
liefs through experience, but the <e1>belief</e1>
begets the <e2>experience</e2>.? The common-
sense domain and range restriction of the Product-
Producer relation are respectively something like
?Entity? and ?Agent?. However, ?belief? is generally
not considered to be an entity, and ?experience? not
an agent. The definition of Product-Producer rela-
tion used for the Challenge is more flexible and al-
lows therefore many examples which are difficult to
find by such common-sense resources as Google or
WordNet.
References
Matthew Berland and Eugene Charniak. 1999. Finding
Parts in Very Large Corpora. In Proceedings of ACL1999.
Christiane Fellbaum (ed.). 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Hal Daum? III. 2004. SVMsequel Tutorial Manual.Available at http://www.cs.utah.edu/
?hal/SVMsequel/svmsequel.pdf
Oren Etzioni et al 2004. Methods for Domain-
Independent Information Extraction from the Web: AnExperimental Comparison. In Proceedings of AAAI2004.
Willem Robert van Hage, Sophia Katrenko, and GuusSchreiber. 2005. A Method to Combine Linguis-tic Ontology-Mapping Techniques. In Proceedings ofISWC 2005.
Willem Robert van Hage, Hap Kolb, and Guus Schreiber.2006. A Method for Learning Part-Whole Relations.In Proceedings of ISWC 2006.
Sophia Katrenko and Pieter Adriaans. 2007. Learn-ing Relations from Biomedical Corpora Using Depen-
dency Trees. In KDECB, LNBI, vol. 4366.
Dekang Lin. 1998. Dependency-based Evaluation ofMINIPAR. In Workshop on the Evaluation of ParsingSystems, Granada, Spain.
Ted Pedersen, Patwardhan, and Michelizzi. 2004. Word-Net::Similarity - Measuring the Relatedness of Con-
cepts. In the Proceedings of AAAI-04, San Jose, CA.
Peter Turney. 2006. Expressing Implicit SemanticRelations without Supervision. In Proceedings ofCOLING-ACL 2006.
Peter Turney. 2004. The MultiText Project Home Page,University of Waterloo, School of Computer Science,
http://www.multitext.uwaterloo.ca
475
Proceedings of the The 1st Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 11?20,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
GAF: A Grounded Annotation Framework for Events
Antske Fokkens, Marieke van Erp, Piek Vossen
The Network Institute
VU University Amsterdam
antske.fokkens@vu.nl
marieke.van.erp@vu.nl
piek.vossen@vu.nl
Sara Tonelli
FBK
Trento, Italy
satonelli@fbk.eu
Willem Robert van Hage
SynerScope B.V.
Eindhoven, The Netherlands
willem.van.hage
@synerscope.com
Luciano Serafini, Rachele Sprugnoli
FBK
Trento, Italy
serafini@fbk.eu
sprugnoli@fbk.eu
Jesper Hoeksema
The Network Institute
VU University Amsterdam
j.e.hoeksema@vu.nl
Abstract
This paper introduces GAF, a grounded an-
notation framework to represent events in a
formal context that can represent information
from both textual and extra-textual sources.
GAF makes a clear distinction between men-
tions of events in text and their formal rep-
resentation as instances in a semantic layer.
Instances are represented by RDF compliant
URIs that are shared across different research
disciplines. This allows us to complete textual
information with external sources and facili-
tates reasoning. The semantic layer can inte-
grate any linguistic information and is com-
patible with previous event representations in
NLP. Through a use case on earthquakes in
Southeast Asia, we demonstrate GAF flexibil-
ity and ability to reason over events with the
aid of extra-linguistic resources.
1 Introduction
Events are not only described in textual documents,
they are also represented in many other non-textual
sources. These sources include videos, pictures,
sensors or evidence from data registration such as
mobile phone data, financial transactions and hos-
pital registrations. Nevertheless, many approaches
to textual event annotation consider events as text-
internal-affairs, possibly across multiple documents
but seldom across different modalities. It follows
from the above that event representation is not ex-
clusively a concern for the NLP community. It also
plays a major role in several other branches of in-
formation science such as knowledge representation
and the Semantic Web, which have created their own
models for representing events.
We propose a grounded annotation framework
(GAF) that allows us to interconnect different ways
of describing and registering events, including non-
linguistic sources. GAF representations can be used
to reason over the cumulated and linked sources of
knowledge and information to interpret the often in-
complete and fragmented information that is pro-
vided by each source. We make a clear distinction
between mentions of events in text or any other form
of registration and their formal representation as in-
stances in a semantic layer.
Mentions in text are annotated using the Terence
Annotation Format (Moens et al, 2011, TAF) on top
of which the semantic layer is realized using Seman-
tic Web technologies and standards. In this semantic
layer, instances are denoted with Uniform Resource
Identifiers (URIs). Attributes and relations are ex-
pressed according to the Simple Event Model (Van
Hage et al, 2011, SEM) and other established on-
tologies. Statements are grouped in named graphs
based on provenance and (temporal) validity, en-
abling the representation of conflicting information.
External knowledge can be related to instances from
a wide variety of sources such as those found in the
Linked Open Data Cloud (Bizer et al, 2009a).
Instances in the semantic layer can optionally be
linked to one or more mentions in text or to other
sources. Because linking instances is optional, our
11
representation offers a straightforward way to in-
clude information that can be inferred from text,
such as implied participants or whether an event is
part of a series that is not explicitly mentioned. Due
to the fact that each URI is unique, it is clear that
mentions connected to the same URI have a coref-
erential relation. Other relations between instances
(participants, subevents, temporal relations, etc.) are
represented explicitly in the semantic layer.
The remainder of this paper is structured as fol-
lows. In Section 2, we present related work and ex-
plain the motivation behind our approach. Section 3
describes the in-text annotation approach. Our se-
mantic annotation layer is presented in Section 4.
Sections 5-7 present GAF through a use case on
earthquakes in Indonesia. This is followed by our
conclusions and future work in section 8.
2 Motivation and Background
Annotation of events and of relations between them
has a long tradition in NLP. The MUC confer-
ences (Grishman and Sundheim, 1996) in the 90s
did not explicitly annotate events and coreference
relations, but the templates used for evaluating the
information extraction tasks indirectly can be seen
as annotation of events represented in newswires.
Such events are not ordered in time or further related
to each other. In response, Setzer and Gaizauskas
(2000) describe an annotation framework to create
coherent temporal orderings of events represented
in documents using closure rules. They suggest that
reasoning with text independent models, such as a
calendar, helps annotating textual representations.
More recently, generic corpora, such as Prop-
bank (Palmer et al, 2005) and the Framenet cor-
pus (Baker et al, 2003) have been built according to
linguistic principles. The annotations aim at prop-
erly representing verb structures within a sentence
context, focusing on verb arguments, semantic roles
and other elements. In ACE 2004 (Linguistic Data
Consortium, 2004b), event detection and linking is
included as a pilot task for the first time, inspired by
annotation schemes developed for named entities.
They distinguish between event mentions and the
trigger event, which is the mention that most clearly
expresses its occurrence (Linguistic Data Consor-
tium, 2004a). Typically, agreement on the trigger
event is low across annotators (around 55% (Moens
et al, 2011)). Timebank (Pustejovsky et al, 2006b)
is a more recent corpus for representing events and
time-expressions that includes temporal relations in
addition to plain coreference relations.
All these approaches have in common that they
consider the textual representation as a closed world
within which events need to be represented. This
means that mentions are linked to a trigger event
or to each other but not to an independent semantic
representation. More recently, researchers started to
annotate events across multiple documents, such as
the EventCorefBank (Bejan and Harabagiu, 2010).
Cross-document coreference is more challenging for
establishing the trigger event, but it is in essence not
different from annotating textual event coreference
within a single document. Descriptions of events
across documents may complement each other pro-
viding a more complete picture, but still textual de-
scriptions tend to be incomplete and sparse with re-
spect to time, place and participants. At the same
time, the comparison of events becomes more com-
plex. We thus expect even lower agreement in as-
signing trigger events across documents. Nothman
et al (2012) define the trigger as the first new ar-
ticle that mentions an event, which is easier than
to find the clearest description and still report inter-
annotator agreement of .48 and .73, respectively.
Recent approaches to automatically resolve event
coreference (cf. Chambers and Jurafsky (2011a),
Bejan and Harabagiu (2010)) use some background
data to establish coreference and other relations be-
tween events in text. Background information, in-
cluding resources, and models learned from textual
data do not represent mentions of events directly but
are useful to fill gaps of knowledge in the textual
descriptions. They do not alter the model for anno-
tation as such.
We aim to take these recent efforts one step fur-
ther and propose a grounded annotation framework
(GAF). Our main goal is to integrate information
from text analysis in a formal context shared with
researchers across domains. Furthermore, GAF is
flexible enough to contain contradictory informa-
tion. This is both important to represent sources
that (partially) contradict each other and to com-
bine alternative annotations or output of different
NLP tools. Because conflicting information may be
12
present, provenance of information is provided in
our framework, so that we may decide which source
to trust more or use it as a feature to decide which in-
terpretation to follow. Different models of event rep-
resentation exist that can contribute valuable infor-
mation. Therefore our model is compliant with prior
approaches regardless of whether they are manual or
automatic. Finally, GAF makes a clear distinction
between instances and instance mentions avoiding
the problem of determining a trigger event. Addi-
tionally, it facilitates the integration of information
from extra-textual sources and information that can
be inferred from texts, but is not explicitly men-
tioned. Sections 5 to 7 will explain how we can
achieve this with GAF.
3 The TERENCE annotation format
The TERENCE Annotation Format (TAF) is de-
fined within the TERENCE Project1 with the goal
to include event mentions, temporal expressions and
participant mentions in a single annotation proto-
col (Moens et al, 2011). TAF is based on ISO-
TimeML (Pustejovsky et al, 2010), but introduces
several adaptations in order to fit the domain of chil-
dren?s stories for which it was originally developed.
The format has been used to annotate around 30 chil-
dren stories in Italian and 10 in English.
We selected TAF as the basis for our in-text anno-
tation for three reasons. First, it incorporates the (in
our opinion crucial) distinction between instances
and instance mentions. Second, it adapts some con-
solidated paradigms for linguistic annotation such as
TimeML for events and temporal expressions and
ACE for participants and participant mentions (Lin-
guistic Data Consortium, 2005). It is thus compat-
ible with other annotation schemes. Third, it inte-
grates the annotation of event mentions, participants
and temporal expressions into a unified framework.
We will elaborate briefly on these properties below.
As mentioned, TAF makes a clear distinction be-
tween instances and instance mentions. Originally,
this distinction only applied to nominal and named
entities, similar to ACE (Linguistic Data Consor-
tium, 2005), because children?s stories can gener-
ally be treated as a closed world, usually present-
1ICT FP7 Programme, ICT-2010-25410, http://www.
terenceproject.eu/
ing a simple sequence of events that do not corefer.
Event coreference and linking to other sources was
thus not relevant for this domain. In GAF, we ex-
tend the distinction between instances and instance
mentions to events to model event coreference, link
them to other sources and create a consistent model
for all instances.
Children?s stories usually include a small set of
characters, event sequences (mostly in chronologi-
cal order), and a few generic temporal expressions.
In the TERENCE project, modeling characters in
the stories is necessary. This requires an extension
of TimeML to deal with event participants. Puste-
jovsky et al (2006a) address the need to include ar-
guments in TimeML annotations, but that proposal
did not include specific examples and details on how
to perform annotation (e.g., on the participants? at-
tributes). Such guidelines were created for TAF.
The TAF annotation of event mentions largely
follows TimeML in annotating tense, aspect, class,
mood, modality and polarity and temporal expres-
sions. However, there are several differences be-
tween TAF and TimeML. First, temporal expres-
sions are not normalized into the ISO-8601 form,
because most children?s stories are not fixed to a spe-
cific date. In GAF, the normalization of expressions
takes place in the semantic layer as these go beyond
the scope of the text. As a result, temporal vague-
ness of linguistic expressions in text do not need to
be normalized in the textual representation to actual
time points and remain underspecified.2
In TAF, events and participant mentions are linked
through a has participant relation, which is defined
as a directional, one-to-one relation from the event
to the participant mentions. Only mentions corre-
sponding to mandatory arguments of the events in
the story are annotated. Annotators look up each
verb in a reference dictionary providing information
on the predicate-argument structure of each verb.
This makes annotation easier and generally not con-
troversial. However, this kind of information can be
provided only by annotators having a good knowl-
edge of linguistics.
All annotations are performed with the Celct An-
2Note that we can still use existing tools for normalization
at the linguistic level: early normalizations can be integrated
in the semantic layer alongside normalizations carried out at a
later point.
13
sem:sub
EventOf
sem:Event sem:Actor sem:Place sem:Time
sem:hasTime
sem:hasActor
sem:hasPlace
sem:PlaceType
sem:placeType
sem:EventType
sem:eventType
sem:ActorType
sem:actorType
sem:TimeType
sem:Type
sem:timeType
sem:Core
sem:subTypeOf
C
o
r
e
 
C
l
a
s
s
e
s
(
F
o
r
e
i
g
n
)
T
y
p
e
 
S
y
s
t
e
m
Literal sem:hasTimeStamp
Literal sem:hasTimeStamp
Figure 1: The SEM ontology
notation Tool (Bartalesi Lenzi et al, 2012), an online
tool supporting TimeML that can easily be extended
to include participant information. The annotated
file can be exported to various XML formats and im-
ported into the semantic layer. The next section de-
scribes SEM, the event model used in our semantic
layer, and how it complements the TAF annotations.
4 The Simple Event Model
The Simple Event Model (SEM) is an RDF
schema (Carroll and Klyne, 2004; Guha and Brick-
ley, 2004) to express who did what, where, and
when. There are many RDF schemas and OWL on-
tologies (Motik et al, 2009) that describe events,
e.g., Shaw et al (2009), Crofts et al (2008) and
Scherp et al (2009). SEM is among the most
flexible and easiest to adapt to different domains.
SEM describes events and related instances such as
the place, time and participants (called Actors in
SEM) by representing the interactions between the
instances with RDF triples. SEM models are se-
mantic networks that include events, places, times,
participants and all related concepts, such as their
types.
An overview of all the classes in the SEM ontol-
ogy and the relations connecting them is shown in
Figure 1. Nodes can be identified by URIs, which
universally identify them across all RDF models. If
for example one uses the URI used by DBpedia3
(Bizer et al, 2009b) for the 2004 catastrophe in In-
3http://dbpedia.org
donesia, then one really means the same event as ev-
erybody else who uses that URI. SEM does not put
any constraints on the RDF vocabulary, so vocabu-
laries can easily be reused. Places and place types
can for example be imported from GeoNames4 and
event types from the RDF version of WordNet.
SEM supports two types of abstraction: gener-
alization with hierarchical relations from other on-
tologies, such as the subclass relation from RDFS,
and aggregation of events into superevents with the
sem:subEventOf relation, as exemplified in Fig-
ure 2. Other types of abstractions can be represented
using additional schemas or ontologies in combina-
tion with SEM. For instance, temporal aggregation
can be done with constructs from the OWL Time
ontology (Hobbs and Pan, 2004).
Relations between events and other instances,
which could be other events, places, actors, times,
or external concepts, can be modeled using the
sem:eventProperty relation. This relation can
be refined to represent specific relations, such as
specific participation, causality or simultaneity rela-
tions. The provenance of information in the SEM
graph is captured through assigning contexts to
statements using the PROV Data Model (Moreau et
al., 2012). In this manner, all statements derived
from a specific newspaper article are stored in a
named graph that represents that origin. Conflicting
statements can be stored in different named graphs,
and can thus coexist. This gives us the possibility
4http://www.geonames.org/ontology/
14
sem:Event
sem:Place
sem:EventType
sem:Time
dbpedia:2004_Indian_Ocean_
earthquake_and_ tsunami
rdf:type
"December 2004 
Earthquake and 
Tsunami"@en
rdfs:label
rdf:type
rdf:type
"3.316"^^xsd:decimal
"2004-12-26"^^xsd:date
"95.854"^^xsd:decimal
wgs84:long
wgs84:lat
owltime:inXSD
DateTime
sem:hasPlace sem:hasTime
naacl:INSTANCE_186
rdf:type
sem:subEventOf
wn30:synset-
earthquake-noun-1
sem:eventType
rdf:type
naacl:INSTANCE_188
rdf:type
sem:subEventOf
naacl:INSTANCE_198
sem:hasTime
naacl:TIMEX3_81 "2004"str:anchorOfnwr:denotedBy
naacl:INSTANCE_MENTION_118
nwr:denotedBy "temblor"@en
str:anchorOf
nwr:denotedBy
"tsunami"@en
naacl:INSTANCE_MENTION_120
str:anchorOf
naacl:INSTANCE_189
sem:subEventOf
naacl:INSTANCE_MENTION_121
nwr:denotedBy
"swept"@en
str:anchorOf
sem:hasPlace
naacl:INSTANCE_67
naacl:INSTANCE_MENTION_19nwr:denotedBy
"Indian Ocean"@en
str:anchorOf
taf:LOCATION
taf:NSUBJ
geonames:1545739
skos:exactMatch
gaf:G1
gaf:G2
gaf:G3
gaf:G4
gaf:G5
dbpedia:Bloomberg
sem:accordingTo
taf:annotation_
2013_03_24
sem:accordingTo
gaf:annotation_
2013_04_29
sem:accordingTo
gaf:annotation_
2013_04_29
sem:accordingTo
taf:annotation_
2013_03_24
sem:accordingTo
sem:
derived
From
gaf:causes
Figure 2: Partial SEM representation of December 26th 2004 Earthquake
of delaying or ignoring the resolution of the conflict,
which enables use cases that require the analysis of
the conflict itself.
5 The GAF Annotation Framework
This section explains the basic idea behind GAF by
using texts on earthquakes in Indonesia. GAF pro-
vides a general model for event representation (in-
cluding textual and extra-textual mentions) as well
as exact representation of linguistic annotation or
output of NLP tools. Simply put, GAF is the combi-
nation of textual analyses and formal semantic rep-
resentations in RDF.
5.1 A SEM for earthquakes
We selected newspaper texts on the January 2009
West Papua earthquakes from Bejan and Harabagiu
(2010) to illustrate GAF. This choice was made be-
cause the topic ?earthquake? illustrates the advan-
tage of sharing URIs across domains. Gao and
Hunter (2011) propose a Linked Data model to cap-
ture major geological events such as earthquakes,
volcano activity and tsunamis. They combine infor-
mation from different seismological databases with
the intention to provide more complete information
to experts which may help to predict the occurrence
of such events. The information can also be used
in text interpretation. We can verify whether in-
terpretations by NLP tools correspond to the data
and relations defined by geologists or, through gen-
eralization, which interpretation is the most sensi-
ble given what we know about the events. General
information on events obtained from automatic text
processing, such as event templates (Chambers and
Jurafsky, 2011b) or typical event durations (Gusev
et al, 2010) can be integrated in SEM in a similar
manner. Provenance indications can be used to in-
dicate whether information is based on a model cre-
ated by an expert or an automatically derived model
obtained by a particular approach.
Figure 2 provides a fragment of a SEM represen-
tation for the earthquake and tsunami of December
26 2004.5 The model is partially inspired by Gao
and Hunter (2011)?s proposal. It combines infor-
mation extracted from texts with information from
DBpedia. The linking between the two can be es-
tablished either manually or automatically through
5The annotation and a larger representation including the
sentence it represents can be found on the GAF website http:
//wordpress.let.vu.nl/gaf.
15
an entity linking system.6 The combined event of
the earthquake and tsunami is represented by a DB-
pedia URI. The node labeled naacl:INSTANCE 186
represents the earthquake itself. The unambiguous
representation of the 2004 earthquake leads us to ad-
ditional information about it, for instance that the
earthquake is an event (sem:Event) and that the
sem:EventType is an earthquake, in this case
represented by a synset from WordNet, but also the
exact date it occurred and the exact location (cf
sem:hasTime, sem:hasPlace).
5.2 Integrating TAF representations into SEM
TAF annotations are converted to SEM relations.
For example, the TAF as participant relations
are translated to sem:hasActor relations, and
temporal relations are translated to sem:hasTime.
We use the relation nwr:denotedBy to link in-
stances to their mentions in the text which are repre-
sented by their unique identifiers in Figure 2.
Named graphs are used to model the source of
information as discussed in Section 4. The re-
lation sem:accordingTo indicates provenance
of information in the graph.7 For instance, the
mentions from the text in named graph gaf:G1
come from the source dbpedia:Bloomberg.
Relations between instances (e.g. between IN-
STANCE 189 and INSTANCE 188) are derived
from a specific grammatical relation in the text
(here, that tsunami is subject of swept) indicated
by the nwr:derivedFrom relation from gaf:G5
to gaf:G4. The grammatical relations included
in graph gaf:G5 come from a TAF annotation
(tag:annotation 2013 03 24).
6 GAF Earthquake Examples
This section takes a closer look at a few selected sen-
tences from the text that illustrate different aspects
of GAF. Figure 2 showed how a URI can provide a
formal context including important background in-
6Entity linking is the task of associating a mention to an
instance in a knowledge base. Several approaches and tools for
entity linking w.r.t. DBpedia and other data sets in the Linked
Open Data cloud are available and achieve good performances,
such as DBpedia Spotlight (Mendes et al, 2011); see (Rizzo
and Troncy, 2011) for a comparison of tools.
7The use of named graphs in this way to denote context is
compatible with the method used by Bozzato et al (2012).
formation on the event. Several texts in the corpus
refer to the tsunami of December 26, 2004, a 9.1
temblor in 2004 caused a tsunami and The catastro-
phe four years ago, among others. Compared to time
expressions such as 2004 and four years ago, time
indications extracted from external sources like DB-
pedia are not only more precise, but also permit us to
correctly establish the fact that these expressions re-
fer to the same event and thus indicate the same time.
The articles were published in January 2009: a direct
normalization of time indications would have placed
the catastrophe in 2005. The flexibility to combine
these seemingly conflicting time indications and de-
lay normalization can be used to correctly interpret
that four years ago early January 2009 refers to an
event taking place at the end of December 2004.
A fragment relating to one of the earthquakes of
January 2009: The quake struck off the coast [...] 75
kilometers (50 miles) west of [....] Manokwari pro-
vides a similar example. The expressions 75 kilo-
meters and 50 miles are clearly meant to express
the same distance, but not identical. The location
is most likely neither exactly 75 km nor 50 miles.
SEM can represent an underspecified location that
is included in the correct region. The exact location
of the earthquake can be found in external resources.
We can include both distances as expressions of the
location and decide whether they denote the general
location or include the normalized locations as alter-
natives to those from external resources.
Different sources may report different details.
Details may only be known later, or sources may
report from a different perspective. As provenance
information can be incorporated into the semantic
layer, we can represent different perspectives, and
choose which one to use when reasoning over the
information. For example, the following phrases
indicate the magnitude of the earthquakes that
struck Manokwari on January 4, 2009:
the 7.7 magnitude quake (source: Xinhuanet)
two quakes, measuring 7.6 and 7.4 (source: Bloomberg)
One 7.3-magnitude tremor (source: Jakartapost)
The first two magnitude indicators (7.7, 7.6)
are likely to pertain to the same earthquake, just as
the second two (7.4, 7.3) are. Trust indicators can
be found through the provenance trace of each men-
16
tion. Trust indicators can include the date on which
it was published, properties of the creation process,
the author, or publisher (Ceolin et al, 2010).
Furthermore, because the URIs are shared across
domains, we can link the information from the text
to information from seismological databases, which
may contain the exact measurement for the quake.
Similarly, external information obtained through
shared links can help us establish coreference. Con-
sider the sentences in Figure 3. There are several
ways to establish that the same event is meant in all
three sentences by using shared URIs and reasoning.
All sentences give us approximate time indications,
location of the affected area and casualties. Rea-
soning over these sentences combined with external
knowledge allows us to infer facts such as that un-
dersea [...] off [...] Aceh will be in the Indian Ocean,
or that the affected countries listed in the first sen-
tence are countries around the Indian Ocean, which
constitutes the Indian Ocean Community. The num-
ber of casualties in combination of the approximate
time indication or approximate location suffices to
identify the earthquake and tsunami in Indonesia on
December 26, 2004. The DBpedia representation
contains additional information such as the magni-
tude, exact location of the quake and a list of affected
countries, which can be used for additional verifica-
tion. This example illustrates how a formal context
using URIs that are shared across disciplines of in-
formation science can help to determine exact refer-
ents from limited or imprecise information.
7 Creating GAF
GAF entails integrating linguistic information
(e.g. TAF annotations) into RDF models (e.g. SEM).
The information in the model includes provenance
that points back to specific annotations. There are
two approaches to annotate text according to GAF.
The first approach is bottom-up. Mentions are
marked in the text as well as relations between them
(participants, time, causal relations, basically any-
thing except coreference). Consequently, these an-
notations are converted to SEM representations as
explained above. Coreference is established by link-
ing mentions to the same instance in SEM. The sec-
ond approach is top-down. Here, annotators mark
relations between instances (events, their partici-
pants, time relations, etc.) directly into SEM and
then link these to mentions in the text.
As mention in Section 2, inter-annotator agree-
ment on event annotation is generally low showing
that it is challenging. The task is somewhat simpli-
fied in GAF, since it removes the problem of identi-
fying an event trigger in the text. The GAF equiva-
lent of the event trigger in other linguistic annotation
approaches is an instance in SEM. However, other
challenges such as which mentions to select are in
principle not addressed by GAF, though differences
in inter-annotator agreement may be found depend-
ing on whether the bottom-up approach or the top-
down approach is selected. The formal context of
SEM may help frame annotations, especially for do-
mains such as earthquakes, where expert knowledge
was used to create basic event models. This may
help annotators while defining the correct relations
between events. On the other hand, the top-down
approach may lead to additional challenges, because
annotators are forced to link events to unambiguous
instances leading to hesitations as to when new in-
stances should be introduced.
Currently, we only use the bottom-up approach.
The main reason is the lack of an appropriate anno-
tation tool to directly annotate information in SEM.
We plan to perform comparative studies between the
two annotation approaches in future work.
8 Conclusion and Future Work
We presented GAF, an event annotation framework
in which textual mentions of events are grounded in
a semantic model that facilitates linking these events
to mentions in external (possibly non-textual) re-
sources and thereby reasoning. We illustrated how
GAF combines TAF and SEM through a use case
on earthquakes. We explained that we aim for a
representation that can combine textual and extra-
linguistic information, provides a clear distinction
between instances and instance mentions, is flexi-
ble enough to include conflicting information and
clearly marks the provenance of information.
GAF ticks all these boxes. All instances are rep-
resented by URIs in a semantic layer following stan-
dard RDF representations that are shared across re-
search disciplines. They are thus represented com-
pletely independent of the source and clearly distin-
17
There have been hundreds of earthquakes in Indonesia since a 9.1 temblor in 2004 caused a
tsunami that swept across the Indian Ocean, devastating coastal communities and leaving more
than 220,000 people dead in Indonesia, Sri Lanka, India, Thailand and other countries.
(Bloomberg, 2009-01-07 01:55 EST)
The catastrophe four years ago devastated Indian Ocean community and killed more than 230,000
people, over 170,000 of them in Aceh at northern tip of Sumatra Island of Indonesia.
(Xinhuanet, 2009-01-05 13:25:46 GMT)
In December 2004, a massive undersea quake off the western Indonesian province of Aceh
triggered a giant tsunami that left at least 230,000 people dead and missing in a dozen
countries facing the Indian Ocean. (Aljazeera, 2009-01-05 08:49 GMT)
Figure 3: Sample sentences mentioning the December 2004 Indonesian earthquake from sample texts
guished from mentions in text or mentions in other
sources. The Terence Annotation Format (TAF) pro-
vides a unified framework to annotate events, par-
ticipants and temporal expressions (and the corre-
sponding relations) by leaning on past, consolidated
annotation experiences such TimeML and ACE. We
will harmonize TAF, the Kyoto Annotation Format
(Bosma et al, 2009, KAF) and the NLP Interchange
Format (Hellmann et al, 2012, NIF) with respect
to the textual representation in the near future. The
NAF format includes the lessons learned from these
predecessors: layered standoff representations using
URI as identifiers and where possible standardized
data categories. The formal semantic model (SEM)
provides the flexibility to include conflicting infor-
mation as well as indications of the provenance of
this information. This allows us to use inferencing
and reasoning over the cumulated and aggregated
information, possibly exploiting the provenance of
the type of information source. This flexibility also
makes our representation compatible with all ap-
proaches dealing with event representation and de-
tections mentioned in Section 2. It can include au-
tomatically learned templates as well as specific re-
lations between events and time expressed in text.
Moreover, it may simultaneously contain output of
different NLP tools.
The proposed semantic layer may be simple, its
flexibility in importing external knowledge may in-
crease complexity in usage as it can model events in
every thinkable domain. To resolve this issue, it is
important to scope the domain by importing the ap-
propriate vocabularies, but no more. When keeping
this in mind, reasoning with SEM is shown to be rich
but still versatile (Van Hage et al, 2012).
While GAF provides us with the desired granu-
larity and flexibility for the event annotation tasks
we envision, a thorough evaluation still needs to be
carried out. This includes an evaluation of the anno-
tations created with GAF compared to other anno-
tation formats, as well as testing it within a greater
application. A comparative study of top-down and
bottom-up annotation will also be carried out. As al-
ready mentioned in Section 7, there is no appropriate
modeling tool for SEM yet. We are currently using
the CAT tool to create TAF annotations and convert
those to SEM, but will develop a tool to annotate the
semantic layer directly for this comparative study.
The most interesting effect of the GAF annota-
tions is that it provides us with relatively simple ac-
cess to a vast wealth of extra-linguistic information,
which we can utilize in a variety of NLP tasks; some
of the reasoning options that are made available by
the pairing up with Semantic Web technology may
for example aid us in identifying coreference rela-
tions between events. Investigating the implications
of this combination of NLP and Semantic Web tech-
nologies lies at the heart of our future work.
Acknowledgements
We thank Francesco Corcoglioniti for his helpful
comments and suggestions. The research lead-
ing to this paper was supported by the European
Union?s 7th Framework Programme via the News-
Reader Project (ICT-316404) and by the Biogra-
phyNed project, funded by the Netherlands eScience
Center (http://esciencecenter.nl/). Partners in Biog-
raphyNed are Huygens/ING Institute of the Dutch
Academy of Sciences and VU University Amster-
dam.
18
References
Collin F. Baker, Charles J. Fillmore, and Beau Cronin.
2003. The structure of the FrameNet database. Inter-
national Journal of Lexicography, 16(3):281?296.
Valentina Bartalesi Lenzi, Giovanni Moretti, and Rachele
Sprugnoli. 2012. CAT: the CELCT Annotation Tool.
In Proceedings of LREC 2012.
Cosmin Bejan and Sandra Harabagiu. 2010. Unsuper-
vised event coreference resolution with rich linguistic
features. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
1412?1422.
Christian Bizer, Tom Heath, and Tim Berners-Lee.
2009a. Linked data - the story so far. International
Journal on Semantic Web and Information Systems,
5(3):1?22.
Christian Bizer, Jens Lehmann, Georgi Kobilarov, So?ren
Auer, Christian Becker, Richard Cyganiak, and Sebas-
tian Hellmann. 2009b. DBpedia - A crystallization
point for the Web of Data. Web Semantics: Science,
Services and Agents on the World Wide Web, 7(3):154
? 165.
Wauter Bosma, Piek Vossen, Aitor Soroa, German Rigau,
Maurizio Tesconi, Andrea Marchetti, Monica Mona-
chini, and Carlo Aliprandi. 2009. KAF: a generic se-
mantic annotation format. In Proceedings of the 5th
International Conference on Generative Approaches
to the Lexicon GL 2009, Pisa, Italy.
Loris Bozzato, Francesco Corcoglioniti, Martin Homola,
Mathew Joseph, and Luciano Serafini. 2012. Manag-
ing contextualized knowledge with the ckr (poster). In
Proceedings of the 9th Extended Semantic Web Con-
ference (ESWC 2012), May 27-31.
Jeremy J. Carroll and Graham Klyne. 2004. Re-
source description framework (RDF): Concepts and
abstract syntax. W3C recommendation, W3C, Febru-
ary. http://www.w3.org/TR/2004/REC-rdf-concepts-
20040210/.
Davide Ceolin, Paul Groth, and Willem Robert Van Hage.
2010. Calculating the trust of event descriptions using
provenance. Proceedings Of The SWPM.
Nathanael Chambers and Dan Jurafsky. 2011a.
Template-based information extraction without the
templates. In Proceedings of ACL-2011.
Nathanael Chambers and Dan Jurafsky. 2011b.
Template-based information extraction without the
templates. In Proceedings of ACL-2011, Portland, OR.
Nick Crofts, Martin Doerr, Tony Gill, Stephen Stead,
and Matthew Stiff. 2008. Definition of the CIDOC
Conceptual Reference Model. Technical report,
ICOM/CIDOC CRM Special Interest Group. version
4.2.5.
Lianli Gao and Jane Hunter. 2011. Publishing, link-
ing and annotating events via interactive timelines: an
earth sciences case study. In DeRiVE 2011 (Detec-
tion, Representation, and Exploitation of Events in the
Semantic Web) Workshop in conjunction with ISWC
2011, Bonn, Germany.
Ralph Grishman and Beth Sundheim. 1996. Message
understanding conference - 6: A brief history. In Pro-
ceedings of the 16th conference on Computational lin-
guistics (COLING?96), pages 466?471.
Ramanathan V. Guha and Dan Brickley. 2004.
RDF vocabulary description language 1.0: RDF
schema. W3C recommendation, W3C, Febru-
ary. http://www.w3.org/TR/2004/REC-rdf-schema-
20040210/.
Andrey Gusev, Nathanael Chambers, Pranav Khaitan,
Divye Khilnani, Steven Bethard, and Dan Jurafsky.
2010. Using query patterns to learn the duration of
events. In Proceedings of ISWC 2010.
Sebastian Hellmann, Jens Lehmann, and So?ren Auer.
2012. NIF: An ontology-based and linked-data-aware
NLP Interchange Format. Working Draft.
Jerry R Hobbs and Feng Pan. 2004. An ontology of time
for the semantic web. ACM Transactions on Asian
Language Information Processing (TALIP), 3(1):66?
85.
Linguistic Data Consortium. 2004a. Annotation
Guidelines for Event Detection and Characterization
(EDC). http://projects.ldc.upenn.edu/
ace/docs/EnglishEDCV2.0.pdf.
Linguistic Data Consortium. 2004b. The ACE 2004
Evaluation Plan. Technical report, LDC.
Linguistic Data Consortium. 2005. ACE (Automatic
Content Extraction) English annotation guidelines for
entities. Version 6.6, July.
Pablo N. Mendes, Max Jakob, Andre?s Garc??a-Silva, and
Christian Bizer. 2011. Dbpedia spotlight: shedding
light on the web of documents. In Proceedings of the
7th International Conference on Semantic Systems, I-
Semantics ?11, pages 1?8.
Marie-Francine Moens, Oleksandr Kolomiyets,
Emanuele Pianta, Sara Tonelli, and Steven Bethard.
2011. D3.1: State-of-the-art and design of novel
annotation languages and technologies: Updated
version. Technical report, TERENCE project ? ICT
FP7 Programme ? ICT-2010-25410.
Luc Moreau, Paolo Missier, Khalid Belhajjame, Reza
B?Far, James Cheney, Sam Coppens, Stephen Cress-
well, Yolanda Gil, Paul Groth, Graham Klyne, Timo-
thy Lebo, Jim McCusker, Simon Miles, James Myers,
Satya Sahoo, and Curt Tilmes. 2012. PROV-DM: The
PROV Data Model. Technical report.
Boris Motik, Bijan Parsia, and Peter F. Patel-
Schneider. 2009. OWL 2 Web Ontology
19
Language structural specification and functional-
style syntax. W3C recommendation, W3C,
October. http://www.w3.org/TR/2009/
REC-owl2-syntax-20091027/.
Joel Nothman, Matthew Honnibal, Ben Hachey, and
James R. Curran. 2012. Event linking: Ground-
ing event reference in a news archive. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers),
pages 228?232, Jeju Island, Korea, July. Association
for Computational Linguistics.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106, 2013/03/12.
James Pustejovsky, Jessica Littman, and Roser Saur?`.
2006a. Argument Structure in TimeML. In Dagstuhl
Seminar Proceedings. Internationales Begegnungs-
und Forschungszentrum.
James Pustejovsky, Jessica Littman, Roser Saur??, and
Marc Verhagen. 2006b. Timebank 1.2 documentation.
Technical report, Brandeis University, April.
James Pustejovsky, Kiyong Lee, Harry Bunt, and Lau-
rent Romary. 2010. ISO-TimeML: An international
standard for semantic annotation. In Proceedings o
the Fifth International Workshop on Interoperable Se-
mantic Annotation.
Giuseppe Rizzo and Raphae?l Troncy. 2011. NERD:
A framework for evaluating named entity recognition
tools in the Web of data. In Workshop on Web Scale
Knowledge Extraction, colocated with ISWC 2011.
Ansgar Scherp, Thomas Franz, Carsten Saathoff, and
Steffen Staab. 2009. F?a model of events based on
the foundational ontology dolce+ dns ultralight. In
Proceedings of the fifth international conference on
Knowledge capture, pages 137?144. ACM.
Andrea Setzer and Robert J. Gaizauskas. 2000. Annotat-
ing events and temporal information in newswire texts.
In LREC. European Language Resources Association.
Ryan Shaw, Raphae?l Troncy, and Lynda Hardman. 2009.
LODE: Linking Open Descriptions of Events. In 4th
Annual Asian Semantic Web Conference (ASWC?09),
Shanghai, China.
Willem Robert Van Hage, Ve?ronique Malaise?, Roxane
Segers, Laura Hollink, and Guus Schreiber. 2011. De-
sign and use of the simple event model (SEM). Jour-
nal of Web Semantics.
Willem Robert Van Hage, Marieke Van Erp, and
Ve?ronique Malaise?. 2012. Linked open piracy: A
story about e-science, linked data, and statistics. Jour-
nal on Data Semantics, 1(3):187?201.
20
