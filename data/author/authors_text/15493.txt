Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 909?919,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Cache-based Document-level Statistical Machine Translation 
 
 
Zhengxian Gong1 Min Zhang2 Guodong Zhou1* 
1 School of Computer Science and Technology 
Soochow University, Suzhou, China 215006 
2 Institute for Infocomm Research, Singapore 138632 
{zhxgong, gdzhou}@suda.edu.cn mzhang@i2r.a-star.edu.sg 
  
 
Abstract 
Statistical machine translation systems are 
usually trained on a large amount of bilingual 
sentence pairs and translate one sentence at a 
time, ignoring document-level information. In 
this paper, we propose a cache-based approach 
to document-level translation. Since caches 
mainly depend on relevant data to supervise 
subsequent decisions, it is critical to fill the 
caches with highly-relevant data of a reasonable 
size. In this paper, we present three kinds of 
caches to store relevant document-level infor-
mation: 1) a dynamic cache, which stores bilin-
gual phrase pairs from the best translation 
hypotheses of previous sentences in the test 
document; 2) a static cache, which stores rele-
vant bilingual phrase pairs extracted from simi-
lar bilingual document pairs (i.e. source 
documents similar to the test document and 
their corresponding target documents) in the 
training parallel corpus; 3) a topic cache, which 
stores the target-side topic words related with 
the test document in the source-side. In particu-
lar, three new features are designed to explore 
various kinds of document-level information in 
above three kinds of caches. Evaluation shows 
the effectiveness of our cache-based approach 
to document-level translation with the perfor-
mance improvement of 0.81 in BLUE score 
over Moses. Especially, detailed analysis and 
discussion are presented to give new insights to 
document-level translation. 
1 Introduction 
During last decade, tremendous work has been 
done to improve the quality of statistical machine 
__________________ 
* Corresponding author. 
translation (SMT) systems. However, there is still 
a huge performance gap between the state-of-the-
art SMT systems and human translators. Bond 
(2002) suggested nine ways to improve machine 
translation by imitating the best practices of human 
translators (Nida, 1964), with parsing the entire 
document before translation as the first priority. 
However, most SMT systems still treat parallel 
corpora as a list of independent sentence-pairs and 
ignore document-level information.  
Document-level information can and should be 
used to help document-level machine translation. 
At least, the topic of a document can help choose 
specific translation candidates, since when taken 
out of the context from their document, some 
words, phrases and even sentences may be rather 
ambiguous and thus difficult to understand. Anoth-
er advantage of document-level machine transla-
tion is its ability in keeping a consistent translation.  
However, document-level translation has drawn 
little attention from the SMT research community.  
The reasons are manifold. First of all, most of pa-
rallel corpora lack the annotation of document 
boundaries (Tam, 2007). Secondly, although it is 
easy to incorporate a new feature into the classical 
log-linear model (Och, 2003), it is difficult to cap-
ture document-level information and model it via 
some simple features. Thirdly, reference transla-
tions of a test document written by human transla-
tors tend to have flexible expressions in order to 
avoid producing monotonous texts. This makes the 
evaluation of document-level SMT systems ex-
tremely difficult.  
Tiedemann (2010) showed that the repetition 
and consistency are very important when modeling 
natural language and translation. He proposed to 
employ cache-based language and translation 
models in a phrase-based SMT system for domain 
909
 adaptation. Especially, the cache in the translation 
model dynamically grows up by adding bilingual 
phrase pairs from the best translation hypotheses of 
previous sentences. One problem with the dynamic 
cache is that those initial sentences in a test docu-
ment may not benefit from the dynamic cache. 
Another problem is that the dynamic cache may be 
prone to noise and cause error propagation. This 
explains why the dynamic cache fails to much im-
prove the performance. 
This paper proposes a cache-based approach for 
document-level SMT using a static cache and a 
dynamic cache. While such a approach applies to 
both phrase-based and syntax-based SMT, this pa-
per focuses on phrase-based SMT. In particular, 
the static cache is employed to store relevant bilin-
gual phrase pairs extracted from similar bilingual 
document pairs (i.e. source documents similar to 
the test document and their target counterparts) in 
the training parallel corpus while the dynamic 
cache is employed to store bilingual phrase pairs 
from the best translation hypotheses of previous 
sentences in the test document. In this way, our 
cache-based approach can provide useful data at 
the beginning of the translation process via the 
static cache. As the translation process continues, 
the dynamic cache grows and contributes more and 
more to the translation of subsequent sentences. 
Our motivation to employ similar bilingual doc-
ument pairs in the training parallel corpus is simple: 
a human translator often collects similar bilingual 
document pairs to help translation. If there are 
translation pairs of sentences/phrases/words in 
similar bilingual document pairs, this makes the 
translation much easier. Given a test document, our 
approach imitates this procedure by first retrieving 
similar bilingual document pairs from the training 
parallel corpus, which has often been applied in 
IR-based adaptation of SMT systems (Zhao et 
al.2004; Hildebrand et al2005; Lu et al2007) and 
then extracting bilingual phrase pairs from similar 
bilingual document pairs to store them in a static 
cache. 
However, such a cache-based approach may in-
troduce many noisy/unnecessary bilingual phrase 
pairs in both the static and dynamic caches. In or-
der to resolve this problem, this paper employs a 
topic model to weaken those noisy/unnecessary 
bilingual phrase pairs by recommending the de-
coder to choose most likely phrase pairs according 
to the topic words extracted from the target-side 
text of similar bilingual document pairs. Just like a 
human translator, even with a big bilingual dictio-
nary, is often confused when he meets a source 
phrase which corresponds to several possible trans-
lations. In this case, some topic words can help 
reduce the perplexity. In this paper, the topic words 
are stored in a topic cache. In some sense, it has 
the similar effect of employing an adaptive lan-
guage model with the advantage of avoiding the 
interpolation of a global language model with a 
specific domain language model. 
The rest of this paper is organized as follows. 
Section 2 reviews the related work. Section 3 
presents our cache-based approach to document-
level SMT. Section 4 presents the experimental 
results. Session 5 gives new insights on cache-
based document-level translation. Finally, we 
conclude this paper in Section 6. 
2 Related work 
There are only a few studies on document-level 
SMT. Representative work includes Zhao et al 
(2006), Tam et al (2007), Carpuat (2009). 
Zhao et al (2006) assumed that the parallel sen-
tence pairs within a document pair constitute a 
mixture of hidden topics and each word pair fol-
lows a topic-specific bilingual translation model. It 
shows that the performance of word alignment can 
be improved with the help of document-level in-
formation, which indirectly improves the quality of 
SMT.  
Tam et al (2007) proposed a bilingual-LSA 
model on the basis of a parallel document corpus 
and built a topic-based language model for each 
language. By automatically building the corres-
pondence between the source and target language 
models, this method can match the topic-based 
language model and improve the performance of 
SMT. 
Carpuat (2009) revisited the ?one sense per dis-
course? hypothesis of Gale et al (1992) and gave a 
detailed comparison and analysis of the ?one trans-
lation per discourse? hypothesis. However, she 
failed to propose an effective way to integrate doc-
ument-level information into a SMT system. For 
example, she simply recommended some transla-
tion candidates to replace some target words in the 
post-process stage.  
In principle, the cache-based approach can be 
well suited for document-level translation. Basical-
910
 ly, the cache is analogous to ?cache memory? in 
hardware terminology, which tracks short-term 
fluctuation (Iyer et al, 1999). As the cache 
changes with different documents, the document-
level information should be capable of influencing 
SMT.  
Previous cache-based approaches mainly point 
to cache-based language modeling (Kuhn and Mori, 
1990), which uses a large global language model to 
mix with a small local model estimated from recent 
history data. However, applying such a language 
model in SMT is very difficult due to the risk of 
introducing extra noise (Raab, 2007).  
For cache-based translation modeling, Nepveu 
et al (2004) explored user-edited translations in 
the context of interactive machine translation. Tie-
demann (2010) proposed to fill the cache with bi-
lingual phrase pairs from the best translation 
hypotheses of previous sentences in the test docu-
ment. Both Nepveu et al (2004) and Tiedemann 
(2010) also explored traditional cache-based lan-
guage models and found that a cache-based lan-
guage model often contributes much more than a 
cache-based translation model. 
3 Cache-based document-level SMT 
Given a test document, our system works as fol-
lows:  
1) clears the static, topic and dynamic caches 
when switching to a new test document dx; 
2) retrieves a set of most similar bilingual docu-
ment pairs dds for dx from the training parallel 
corpus using the cosine similarity with tf-idf 
weighting; 
3) fills the static cache with bilingual phrase pairs 
extracted from dds;  
4) fills the topic cache with topic words extracted 
from the target-side documents of dds; 
5) for each sentence in the test document, trans-
lates it using cache-based SMT and conti-
nuously expands the dynamic cache with 
bilingual phrase pairs obtained from the best 
translation hypothesis of the  previous sen-
tences. 
In this way, our cache-based approach can pro-
vide useful data at the beginning of the translation 
process via the static cache. As the translation 
process continues, the dynamic cache grows and 
contributes more and more to the translation of 
subsequent sentences. Besides, the possibility of 
choosing noisy/unnecessary bilingual phrase pairs 
in both the static and dynamic caches is wakened 
with the help of the topic words in the topic cache. 
In particular, only the most similar document pair 
is used to construct the static cache and the topic 
cache unless specified. 
In this section, we first introduce the basic 
phrase-based SMT system and then present our 
cache-based approach to achieve document-level 
SMT with focus on constructing the caches (static, 
dynamic and topic) and designing their corres-
ponding features. 
3.1 Basic phrase-based SMT system 
It is well known that the translation process of 
SMT can be modeled as obtaining the best transla-
tion e of the source sentence f by maximizing fol-
lowing posterior probability (Brown et al, 1993): 
)()|(maxarg)|(maxarg ePefPfePe lm
ee
best ==  (1) 
where P(e|f) is a translation model and Plm is a lan-
guage model. 
Our system adopted Moses (a state-of-art 
phrase-based SMT system) as a baseline, which 
follows Koehn et al (2003) and mainly adopts six 
groups of popular features: 1) two phrase transla-
tion probabilities (two directions): Pphr(e|f) and 
Pphr(f|e); 2) two word translation probabilities (two 
directions) : Pw(e|f) and Pw(f|e); 3) one language 
model (target language): LM(e); 4) one phrase pe-
nalty (target language): PP(f); 5) one word penalty 
(target language):WP(e); 6) a lexicalized reorder-
ing model. Besides, the log-linear model as de-
scribed in (Och and Ney, 2003) is employed to 
linearly interpolate these features for obtaining the 
best translation according to the formula (2): 
)},(max{arg
1
fehe m
M
m
mbest ?
=
= l   (2) 
where hm(e , f) is a feature function, and ?m is the 
weight of hm(e , f)  optimized by a discriminative 
training method on a held-out development data. 
In principle, a phrase-based SMT system can 
provide the best phrase segmentation and align-
ment that cover a bilingual sentence pair. Here, a 
segmentation of a sentence into K phrases is de-
fined as: 
(f~e)?  ? (f , e , ~)      (3) 
 
where tuple (f , e ) refers to a phrase pair, and ~ 
indicates corresponding alignment information.  
911
 3.2 Dynamic Cache 
Our dynamic cache is mostly inspired by Tiede-
mann (2010), which adopts a dynamic cache to 
store relevant bilingual phrase pairs from the best 
translation hypotheses of previous sentences in the 
test document. In particular, a specific feature is 
incorporated S      to capture useful document-
level information in the dynamic cache: 
?
?
=
=
-?
=
?>>=<<= K
i ic
K
i
i
iicc
cccache ffI
efefeIfeS
1
1
)(
),,()|(
(4)
 
where ie-? is a decay factor to avoid the depen-
dence of the feature?s contribution on the cache 
size. Given <ec, fc> an existing phrase pair in the 
dynamic cache and <ei,fi> a phrase pair in a new 
hypothesis, if ( ei=ec ? fi=fc ) is true (i.e. full match-
ing), function I(.) returns 1 , otherwise 0.  
One problem with the dynamic cache in Tiede-
mann (2010) is that it continuously updates the 
weight of a phrase pair in the dynamic cache. This 
may cause noticeable computational burden with 
the increasing number of phrase pairs in the dy-
namic cache. In addition, as a source phrase (fc) 
may occur many times in the dynamic cache, the 
weights for related phrase pairs may degrade se-
verely and thus his decoder needs a decay factor, 
which is difficult to optimize. Finally, Tiedemann 
(2010) only allowed full matching. This largely 
lowers down the probability of hitting the dynamic 
cache and thus much affects its effectiveness. 
To overcome above problems, we only employ 
the bilingual phrase pairs in the dynamic cache to 
inform the decoder whether one bilingual phrase 
pair exists in the dynamic cache or not, which is 
slightly similar to (Nepveu et al 2004) ,thus avoid-
ing extra computational burden and the fine-tuning 
of the decay factor.  In particular, following new 
feature is incorporated to better explore the dynam-
ic cache:   = ? dpairmatch(e , f )        (5) 
where dpairmatch(  ,  )    = ???
?? 1   (e = e ? f = f )                       ?  e  = e ? f  = f ?? e ?> 3    ?  e = e  ? f = f  ?? e ?> 3 0    other                                               
Here, F  is called the dynamic cache feature. 
Assume (ec,fc ) is a phrase pair in the dynamic 
cache and (ei,fi) is a phrase pair candidate for a 
new hypothesis. Besides full matching, we intro-
duce a symbol of ?^? for sub-phrase, such as e   for 
a sub-phrase of ei and  e   for a sub-phrase of e  , to 
allow partial matching. Finally, F  measures the 
overall value of a target candidate f  by summing 
over the scores of K phrase pairs. 
Obviously, F  rewards both full matching and 
partial matching. In order to avoid too much noise, 
we put some constraints on the number of words in 
the target phrase of <ec,fc> or <ei,fi>, such as ? e ?> 3 , where " ?? "  measures the number of 
non-blank characters in a phrase. For example, if 
phrase pair ?, ??||| and reduced? occurs in the 
cache, phrase pair ?,|||and? is not rewarded because 
such shorter phrase pairs occur frequently and may 
largely degrade the effect of the cache. In accor-
dance, the dynamic cache only contains phrase 
pairs whose target phrases contain 4 or more non-
blank characters. 
3.3 Static Cache 
In Tiedemann (2010), initial sentences in a test 
document fail to benefit from the dynamic cache 
due to the lack of contents in the dynamic cache at 
the beginning of the translation process. To over-
come this problem, a static cache is included to 
store relevant bilingual phrase pairs extracted from 
similar bilingual document pairs in the training 
parallel corpus. In particular, a static cache feature F  is designed to capture useful information in the 
static cache in the same way as the dynamic cache 
feature, shown in Formula (5). 
For this purpose, all the document pairs in the 
training parallel corpus are aligned at the phrase 
level using 2-fold cross-validation. That is, we 
adopt 50% of the training parallel corpus to train a 
model using Moses and apply the model to enforce 
phrase alignment of the remaining training data, 
and vice versa. Here, the enforcement is done by 
guaranteeing the occurrence of the target phrase 
candidate of a source phrase in the sentence pair. 
Besides, all the words pairs trained on the whole 
training parallel corpus are included in both folds 
to ensure at least one possible translation. Finally, 
the phrase pairs in the best translation hypothesis 
of a sentence pair is retrieved from the decoder. In 
this way, we can extract a set of phrase pairs for 
each bilingual document pairs. 
Given a test document, we first find a set of sim-
ilar source documents by computing the Cosine 
similarity using the TF-IDF weighting scheme and 
their corresponding target documents, from the 
training parallel corpus. Then, the phrase pairs ex-
912
 tracted from these similar bilingual document pairs 
are collected into the static cache.  To avoid noise, 
we filter out those phrase pairs which occur less 
than two times in the training parallel corpus. 
 
?? ||| exports 
?? ||| slowdown 
?? ||| stock market 
?? ||| leading 
?? ||| exchange 
?? ||| vitality 
?? ||| speed up the 
???? ||| economists 
?? ?? ||| export growth 
?? ?? ||| various reasons 
?? ?? ||| a well-known international 
?? ??? ||| congressional committee 
? ?? ? ?? ||| pessimistic predictions 
?? ?? ? ?? ||| maintain a certain growth 
?? ?? ?? ||| a drop in the dollar exchange rate 
Table 1: Phrase pairs extracted from a document pair 
with an economic topic 
Similar to the dynamic cache, we only consider 
those phrase pairs whose target phrases contain 4 
or more non-blank characters to avoid noise. We 
do not deliberately remove long phrase pairs. It is 
possible to use these long phrase pairs if our test 
document is very similar to one training document 
pair.  Table 1 shows some bilingual phrase pairs 
extracted from a document pair, which reports a 
piece of news about ?impact on slowdown in US 
economic growth?. Obviously, these phrase pairs 
are closely related to economics. 
3.4 Topic Cache 
Both the dynamic and static caches may still intro-
duce noisy/unnecessary bilingual phrase pairs even 
with constraints on the length of phrases and their 
occurrence frequency in the training parallel cor-
pus. In order to resolve this problem, this paper 
adopts a topic cache to store relevant topic words 
and employs a topic cache feature to weaken those 
noisy/unnecessary phrase pairs. 
Given w  is a topic word in the topic cache, the 
topic cache feature F  is designed as follows:   =  topicexist(e , f )          (6) 
where topicexist(e , f ) =   1   (w ? e )                                    0    other                                            
Here, the target phrase which contains a topic word w  will be rewarded. w  is derived by a topic mod-
el, LDA (Latent Dirichlet Allocation). This is dif-
ferent from the previous work (Tam, 2007), which 
mainly interpolated a topic language model with a 
general language model and added additional two 
adaptive lexicon probabilities in his phrase table. 
In principle, LDA is a probabilistic model of 
text data, which provides a generative analog of 
PLSA (Blei et al, 2003), and is primarily meant to 
reveal hidden topics in text documents. Like most 
of the text mining techniques, LDA assumes that 
documents are made up of words and the ordering 
of the words within a document is unimportant (i.e. 
the ?bag-of-words? assumption).  
Figure 1 shows the principle of LDA, where ? is 
the parameter of the uniform Dirichlet prior on the 
per-document topic distributions, ? is the parame-
ter of the uniform Dirichlet prior on the per-topic 
word distribution, ?i is the topic distribution for 
document i, zij is the topic for the jth word in doc-
ument i, and wij is the specific word. Among all 
variables, wij is the only observable variable with 
all the other variables latent. In particular, K de-
notes the number of topics considered in the model 
and ? is a K*V (V is the dimension of the vocabu-
lary) Markov matrix each line of which denotes the 
word distribution of a topic. The inner plate over z 
and w illustrates the repeated sampling of topics 
and words until N words have been generated for document d. The plate surrounding ? illustrates the 
sampling of a distribution over topics for each 
document d for a total of M documents. The plate 
surrounding ? illustrates the repeated sampling of 
word distributions for each topic z until K topics 
have been generated. 
We use a LDA tool1 to build a topic model using 
the target-side documents in the training parallel 
corpus. Using LDA, we can obtain the topic distri-
bution of each word w, namely p(z|w) for topic z ?K. Moreover, using the obtained word topic dis-
tributions, we can infer the topic distribution of a 
new document, namely p(z|d) for each topic z ?K. 
Given a test document, we first find the most 
similar source document from the training data in 
                                                        
1 http://www.arbylon.net/projects/ 
Figure 1: LDA  
913
 the same way as done in the static cache. After that, 
we retrieve its corresponding target document. 
Then, the topic of the target document is deter-
mined by its major topic, with the maximum p(z|d). 
Finally, we load some topic words corresponding 
to this topic z into the topic cache. In particular, 
our LDA model deploy the setting of K=15, ?=0.5 
and ?=0.1. Besides, only top 1000 topic words are 
reserved for each topic. Table 2 shows top 10 topic 
words for five topics. 
 
Topic 1 Topic 2 Topic 3 Topic4 Topic5 
company  
corporation  
limited  
manager  
board  
branch  
companies  
ltd  
business  
personnel  
army  
armed  
military  
officers  
forces  
units  
troops  
force  
soldiers  
police  
party  
represents  
study  
theory  
leadership  
political  
cadres  
speech  
comrade  
central  
bush  
united  
adminis-
tration  
policy  
president  
clinton  
office  
secretary  
powell  
relations  
election  
olympic  
games  
votes  
bid  
gore  
presi-
dential  
party  
won  
speech 
Table 2: Topic words extracted from target-side doc-
uments  
 
4 Experimentation 
We have systematically evaluated our cache-based 
approach to document-level SMT on the Chinese-
English translation task. 
4.1 Experimental Setting 
Here, we use SRI language modeling toolkit to 
train a trigram general language model on English 
newswire text, mostly from the Xinhua portion of 
the Gigaword corpus (2007) and performed word 
alignment on the training parallel corpus using 
GIZA++(Och and Ney,2000) in two directions. For 
evaluation, the NIST BLEU script (version 13) 
with the default setting is used to calculate the 
Bleu score (Papineni et al 2002), which measures 
case-insensitive matching of n-grams with n up to 
4. To see whether an improvement is statistically 
significant, we also conduct significance tests us-
ing the paired bootstrap approach (Koehn, 2004)2. 
In this paper, ?***?, ?**?, and ?*? denote 
p-values less than or equal to 0.01, in-between 
(0.01, 0.05), and bigger than 0.05, which mean 
significantly better, moderately better and slightly 
better, respectively. 
                                                        
2 http://www.ark.cs.cmu.edu/MT 
In this paper, we use FBIS as the training data, 
the 2003 NIST MT evaluation test data as the de-
velopment data, and the 2005 NIST MT test data 
as the test data. Table 3 shows the statistics of 
these data sets (with document boundaries anno-
tated). 
 
   Corpus Sentences Documents 
Role Name 
Train FBIS 239413 10353 
Dev NIST2003 919 100 
Test NIST2005 1082 100 
Table 3: Corpus statistics 
In particular, the sizes of the static, topic and 
dynamic caches are fine-tuned to 2000, 1000 and 
5000 items, respectively. For the dynamic cache, 
we only keep those most recently-visited items, 
while for the static cache; we always keep the most 
frequently-occurring items. 
4.2 Experimental Results 
Table 4 shows the contribution of various caches in 
our cache-based document-level SMT system. The 
column of ?BLEU_W? means the BLEU score 
computed over the whole test set and ?BLEU_D? 
corresponds to the average BLEU score over sepa-
rated documents. 
 
System BLEU on 
Dev(%) 
BLEU on Test(%) 
BLEU_W NIST BLEU_D 
Moses 29.87 25.76 7.784 25.08 
Fd 29.90 26.03 (*) 7.852 25.39 
Fd+Fs 30.29 26.30 (**) 7.884 25.86 
Fd+Ft 30.11 26.24 (**) 7.871 25.74 
Fd+Fs+Ft 30.50 26.42 (***) 7.896 26.11 
Fd+Fs+Ft 
with merg-
ing 
- 26.57 (***) 7.901 26.32 
Table 4: Contribution of various caches in our cache-
based document-level SMT system. Note that signific-
ance tests are done against Moses. 
Contribution of dynamical cache (Fd)  
Table 4 shows that the dynamic cache slightly im-
proves the performance by 0.27 (*) in BLEU_W. 
This is similar to Tiedemann (2010). However, 
detailed analysis indicates that the dynamic cache 
does have negative effect on about one third of 
documents, largely due to the instability of the dy-
namic cache at the beginning of translating a doc-
ument. Figure 2 shows the distribution of the 
914
 BLEU_D difference of 100 test documents (sorted 
by BLEU_D). It shows that about 55% of test 
documents benefit from the dynamic cache. 
Contribution of static cache (Fs) 
Table 4 shows that the combination of the static 
cache with the dynamic cache further improves the 
performance by 0.27(*) in BLEU_W. This sug-
gests the effectiveness of the static cache in elimi-
nating the instability of the dynamic cache when 
translating first few sentences of a test document. 
Together, the dynamic and static caches much im-
prove the performance by 0.54 (**) in BLEU_W 
over Moses.  Figure 3 shows the distribution of the 
BLEU_D difference of 100 test documents (sorted 
by BLEU_D), with more positive effect on those 
borderline documents, compared to Figure 2. 
Contribution of topic cache (Ft) 
Table 4 shows that the topic cache has comparable 
effect on improving the performance as the static 
cache when combined with the dynamic cache 
(0.48 vs. 0.54 in BLEU_W). Figure 4 shows the 
effectiveness of combining the dynamic and topic 
caches (sorted by BLEU_D). 
However, detailed analysis shows that the topic 
cache and the static cache are quite complementary 
by contributing on different test documents, largely 
due to that while the static cache tends to keep 
translation consistent, the topic cache plays like a 
document-specific language model. This is justi-
fied by Table 4 that the combination of the dynam-
ic, static and topic caches significantly improve the 
performance by 0.66 (***) in BLEU_W, and by 
Figure 5 that about 75% of test documents benefit 
from the combination of the three caches (sorted 
by BLEU_D). 
Contribution of merging phrase pairs of similar 
document pairs 
Here, the number of similar documents we adopt is 
different from previous experiments. In the pre-
vious experiments, we only cache bilingual phrase 
pairs extracted from the most similar document. 
Here, we merge phrase pairs for several most simi-
lar documents (5 at most) which have the same 
topic. 
Figure 2: Contribution of employing the dynamic 
cache on different test documents 
-8.00%
-6.00%
-4.00%
-2.00%
0.00%
2.00%
4.00%
6.00%
8.00%
1 8 15 22 29 36 43 50 57 64 71 78 85 92 99
fd-moses
 
Figure 3: Contribution of combining the dynamic and 
static cache on different test documents 
-6.00%
-4.00%
-2.00%
0.00%
2.00%
4.00%
6.00%
1 8 15 22 29 36 43 50 57 64 71 78 85 92 99
fd+fs-moses
 Figure 4: Contribution of combining the dynamic 
and topic caches 
-6.00%
-4.00%
-2.00%
0.00%
2.00%
4.00%
6.00%
8.00%
10.00%
1 7 13 19 25 31 37 43 49 55 61 67 73 79 85 91 97
fd+ft-moses
 
Figure 5: Contribution of combining the three caches 
-4.00%
-2.00%
0.00%
2.00%
4.00%
6.00%
8.00%
10.00%
1 9 17 25 33 41 49 57 65 73 81 89 97
fd+fs+ft-moses
915
  Table 4 shows that employing this trick can fur-
ther improve the performance by 0.15 in BLEU_W. 
As a result, the cache-based approach significantly 
improve the performance by 0.81 (***) in 
BLEU_W over Moses. 
5 Discussion 
In this section, we explore in more depth why the 
static cache can help the dynamic cache, some 
constrained factors which impact the effectiveness 
of our cache-based approach. 
Effectiveness of the static cache  
We investigate why the static cache affects the per-
formance. Basically, it is difficult for the dynamic 
cache to capture such similar information in the 
static cache. 
In principle, the static cache can both influence 
the initial and subsequent sentences; however sub-
sequent ones can be affected by multiple caches. In 
order to give an insight of the static cache, we eva-
luate its effectiveness on the first sentence for each 
test document. Figure 6 shows the contribution of 
the static cache on translating these first sentences 
(y-axis shows BLEU value of the first sentence for 
each test document). It notes that the most BLEU 
scores of them are zeros because of the length limi-
tation of first sentences. 
Furthermore, we count the hit (matching) fre-
quency of the static cache for each test documents. 
Since we use 1 or 0 for the static cache feature, it is 
easy to retrieve its effect for each test document. 
Our statistics shows that the hit frequency on static 
cache fluctuates between 5 and 18 for each test 
document. Without the static cache, the hit fre-
quency of the dynamic cache is 504 on whole test 
sets, this figure increases to 685 with the static 
cache. This means that the static cache significant-
ly enlarges the effectiveness of the dynamic cache 
by including more relevant phrase pairs to the dy-
namic cache, largely due to the positive impact of 
the static cache on the initial sentences of each test 
document. 
Size of topic cache  
Table 5 shows the impact of the topic cache when 
the number of the retained topic words for each 
topic increases from 500 to 2000. It shows that too 
more topic words actually harm the performance, 
due to the increase of noise. 1000 topic words 
seem a lot largely due to that we didn?t do stem-
ming for our topic modeling since we hope to in-
troduce some tense information of them in the 
future.  
 
Number of topic words BLEU_W 
500 26.27 
700 26.31 
1000 26.42 
1500 26.23 
2000 26.19 
Table 5: Impact of the topic cache size 
Influenced translations  
In order to explore how our cache-based system 
impacts on translation results, we manually in-
spected 5 documents respectively which is im-
proved or degraded in translation quality compared 
to the baseline Moses output. Those documents 
have 107 sentences in sum. 
The good effectiveness of each kind of cache 
can be observed by the example 1 and 2 showed in 
Table 6. Both the example 1 and 2 come from the 
same document whose ?BLEU_D? score exceeds 
Moses with 8.4 point. The example 1 benefits from 
the topic cache which contains the item of ?action?. 
The example 2 benefits from the static cache which 
contains a phrase pair of  ??? ||| promised to? 
while Moses use ?commitment? for ???? , which 
may be the reason for  missing the part of ?prime 
minister? in Moses output. Furthermore, due to the 
phrase pair of ??? ??||| the ceasefire agree-
ment? existing in our static cache, our decoder 
keeps using ?ceasefire? to translate ???? in the 
whole document while Moses randomly use ?cea-
sefire? or ?cease-fire? for this translation.  
 Figure 6: Contribution of the static cache on the first 
sentence of each test document  
(i.e. with empty dynamic cache)   
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0 20 40 60 80 100 120
Fd FdFs
916
 1 ?? ?? ? ?? ?? ? ? ?? ? ?? , ?? ? ?? ?? ?? ? 
Moses: official forecasts said that preparatory work will be carried out in july and then launched a political maneuver . 
Ours:   official forecasts said that preparatory work will be carried out in july , then began a political action . 
Reference: officials expected that "preparations would take place until July, after which political action will begin".          
2 ?? ? ? ? , ????? ? , ??? ?? ?? ?? ? ? ? ???? ?? ?? ?? ?? , ?
? ? ?? ? ?? , ??? ? ? ?? ? ???? ? ? ?? ?? ? ? 
Moses: on this point , said that israeli commitment to the palestinian authorities to respect the cease-fire agreement , 
where they are well under control , israel will stop its military actions against palestinians . 
Ours: on this point , said that israeli prime minister promised to respect the ceasefire agreement , the palestinian au-
thorities to properly control their areas and israel will stop its military actions against palestinians . 
Reference:For this point , MENA said Israeli Prime Minister Sharon has promised to " stop Israeli military operations 
against the Palestinians insofar as they continue to respect the ceasefire deal and control their territory . " 
3 17 ? ? ?? 3000 ? ? ?? ? ??? ? ?? ?? ?? ? ? ??? ?? ? ???? ? ?? 
?? ????? ? ?? ?? ? ?? ?? ? ?? ?? ??? ?? ??? ? ?? ? ?? ?? 
? ?? ?? ? 
Moses: on the evening , nearly 3,000 residents in the downtown square of the weapons held by the municipal govern-
ment , watched a song and dance soiree , having colorful lighting disguise of ancient buildings around the square , sing-
ing and dancing famous artists staged different regions of ethnic song and dance . 
Ours: later on, nearly 3,000 residents in the downtown square to watch the government of having a song and dance 
performances were held under the disguise of colorful lighting around the square , a famous ancient buildings and local 
artists of different ethnic song and dance . 
Reference: On the night of the 17th , nearly 3,000 residents watched a wonderful gala of songs and dances , organized 
by the municipal government , at Plaza da Armas . Colorful lights lighted up ancient architecture around the plaza . 
Famous artists including singers and dancers staged performances of national songs and dances of different regions . 
4 ?? ? ?? ?? ? ? ?? ? ? ? 2.14 ???? ?? ? 2600 ? ???? ? ? ?? ? ?? 
? 800? ?? ? ? ? ?? ??? ? 31% ? 
Moses: at lima 's urban area from the beginning of 2600 square to 2.14 million square kilometers , while the popula-
tion has increased to 8 percent of the country 's total , about 31% . 
Ours: lima , the urban area from the beginning of 2600 square kilometers to 2.14 million square kilometers , but also 
increased to about 8 million population , the country 's total population of about 31% . 
Reference: The area of Lima city has expanded to more than 2,600 square kilometers from the original 2.14 square ki-
lometers when the city was founded , while the population has increased to around 8 million , roughly accounting for 
31% of the nation's total . 
Table 6: Positive and negative examples  
The example 3 and 4 also come from the same 
document however whose performance degrades 
with 2.17 point.  We don?t think the translation 
quality for example 4 in our system is worse than 
Moses. However, the translation quality for exam-
ple 3 in our system is very bad and especially 
showed on ?re-ordering?. We found this sentence 
did not match any item in our static cache and top-
ic cache. Although this phenomenon also happens 
in other documents, but this is the most typical 
negative example among these documents.  
Document-specific characteristics  
It seems that using the same weight for the whole 
test sets (all documents) is not very reasonable. 
Actually, if we can determine those negative doc-
uments which are not suitable for the cache-based 
approach, our cache-based approach may gain 
much improvement. Tiedemann (2010) explored 
the correlation to document length, baseline per-
formance and source document repetition. Howev-
er, it seems that there are no obvious rules to filter 
out those negative documents. Besides, there may 
be two more document-specific factors: repetition 
of the reference text and document style.  
Tiedemann (2010) only considered the repetition 
of the test text in the source side. Since BLEU 
score is computed against the reference text, the 
repetition in the reference text may greatly influ-
ence the performance of our cache-based approach 
to document-level SMT. As for document style, it 
is quite possible that a document may contain sev-
eral topics. Therefore, it may be useful to track 
such change over topics and refresh various caches 
when there is a topic change. We will leave the 
above issues to the future work. 
6 Conclusion 
We have shown that our cache-based approach 
significantly improves the performance with the 
help of various caches, such as the dynamic, static 
and topic caches, although the cache-based ap-
917
 proach may introduce some negative impact on 
BLEU scores for certain documents.  
In the future, we will further explore how to re-
flect document divergence during training and dy-
namically adjust cache weights according to 
different documents.  
There are many useful components in training 
documents, such as named entity, event and co-
reference. In this experiment, we only adopt the 
flat data in our cache. However, the structured data 
may improve the correctness of matching and thus 
effectively avoid noise. We will explore more ef-
fective ways to pick up various kinds of useful in-
formation from the training parallel corpus to 
expand our cache-based approach. Besides, we will 
resort to comparable corpora to enlarge our cache-
based approach to document-level SMT.  
Acknowledgments 
This research was supported by Projects 90920004, 
60873150, and 61003155 under the National Natu-
ral Science Foundation of China, Project 
20093201110006 under the Specialized Research 
Fund for the Doctoral Program of Higher Educa-
tion of China. 
References  
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 
2003. Latent Dirichlet Allocation. Journal of machine 
Learning Research 3, pages 993?1022. 
Francis Bond. 2002. Toward a Science of Machine 
Translation. Asian Association of Machine Transla-
tion (AAMT) Journal, N0.22, Tokyo, Japan, pages 
12-20. 
PF Brown, SA Della Pietra, VJ Della Pietra, RL Merc-
er.1992.The Mathematics of Statistical Machine 
Translation: Parameter Estimation. Computational 
Linguistics. 19(2):263-309. 
Marine Carpuat. 2009. One Translation per Discourse. 
In Proc. of the NAACL HLT workshop on Semantic 
Evaluation, pages 19-26. 
John DeNero, Alexandre Buchard-C?ot?e, and Dan 
Klein. 2008. Sampling Alignment Structure under a 
Bayesian Translation Model. In Proc. of EMNLP 
2008, pages 314?323, Honolulu, October. 
William A. Gale, Kenneth W. Church, and David Ya-
rowsky. 1992. One Sense per Discourse. In Proceed-
ings of the workshop on Speech and Natural 
Language, Harriman, NY. 
Almut Silja Hildebrand, Matthias Eck, Stephan Vo-
gel,and Alex Waibel. 2005. Adaptation of the Trans-
lation Model for Statistical Machine Translation 
based on Information Retrieval. Proceedings of 
EAMT 2005:133-142. 
Rukmini M. Iyer and Mari Ostendorf. 1999. Modeling 
Long Distance Dependence in Language:Topic 
Mixtures Versus Dynamic Cache Models. IEEE 
Transactions on speech and audio processing, 7(1). 
Philipp Koehn, Franz Josef Och ,and DanielMarcu.2003. 
Statistical Phrase-Based Translation. Proceedings of 
the 2003 Conference of the North American Chapter 
of the Association for Computational Linguistics on 
Human Language Technology, pages 48-54. 
Philipp Koehn.2004.Statistical Significance Tests for 
Machine Translation Evaluation. In Proc. of EMNLP 
2004,  pages 388?395. 
Roland Kuhn and Renato De Mori. 1990. A Cache-
based Natural Language Model for Speech Recogni-
tion. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence,12(6):570-583. 
Yajuan Lu, Jin Huang, and Qun Liu. 2007. Improving 
statistical machine translation performance by train-
ing data selection and optimization. In Proc. of  
EMNLP 2007,pages 343?350, Prague, Czech Repub-
lic, June. 
Daniel Marcu and William Wong. 2002. A phrase-based 
Joint Probability Model for Statistical Machine 
Translation. In Proc. of  EMNLP 2002, July. 
Laurent Nepveu, Guy Lapalme, Philippe Langlais and 
George Foster.2004. Adaptive Language and Trans-
lation Models for Interactive Machine Translation. In 
Proc. of EMNLP 2004, pages 190-197. 
Eugene A. Nida. 1964. Toward a Science of Translating. 
Leiden, Netherlands:E.J.Brill. 
Franz Josef Och. 2003. Minimum Error Rate Training in 
Statistical Machine Translation. In Proc. of ACL, 
pages 160?167. 
Franz Josef Och and Hermann Ney. 2000. Improved 
Statistical Alignment Models. In Proc. of ACL, pages 
440?447. 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
jing Zhu. 2002. BLEU: A Method for Automatic 
Evaluation of Machine Translation. In Proc. of 
ACL02, pages 311?318. 
Martin Raab. 2007. Language Modeling for Machine 
Translation. VDM Verlag, Saarbrucken, Germany. 
918
 G. Salton and C. Buckley. 1988. Term-weighting Ap-
proaches in Automatic Text Retrieval. Information 
Processing and management,24(5):513-523,1988. 
Yik-Cheung Tam, Ian Lane and Tanja Schultz. 2007. 
Bilingual ISA-based Adaptation for Statistical Ma-
chine Translation. Machine Translation, 28:187-207. 
Jorg Tiedemann. 2010. Context Adaptation in Statistical 
Machine Translation Using Models with Exponen-
tially Decaying Cache. In Proc. of the 2010 work-
shop on domain adaptation for Natural Language 
Processing, ACL 2010, pages 8-15. 
Joern Wuebker and Arne Mauser and Hermann Ney. 
2010. Training Phrase Translation Models with Leav-
ing-One-Out. In Proc. of ACL, pages 475-484. 
Bing Zhao, Matthias Eck, and Stephan Vogel. 
2004.Language Model Adaptation for Statistical Ma-
chine Translation with Structured Query Models. In 
COLING 2004, Geneva, August. 
Bing Zhao and Eric P. Xing .2006. BiTAM:Bilingual 
Topic Ad-Mixture Models for Word Alignment. In 
Proc. of ACL2006. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
919
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 276?285, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
N-gram-based Tense Models for Statistical Machine Translation
Zhengxian Gong1 Min Zhang2 Chewlim Tan3 Guodong Zhou1?
1 School of Computer Science and Technology, Soochow University, Suzhou, China 215006
2 Human Language Technology, Institute for Infocomm Research, Singapore 138632
3 School of Computing, National University of Singapore, Singapore 117417
{zhxgong, gdzhou}@suda.edu.cn mzhang@i2r.a-star.edu.sg tancl@comp.nus.edu.sg
Abstract
Tense is a small element to a sentence, how-
ever, error tense can raise odd grammars and
result in misunderstanding. Recently, tense
has drawn attention in many natural language
processing applications. However, most of
current Statistical Machine Translation (SMT)
systems mainly depend on translation model
and language model. They never consider and
make full use of tense information. In this pa-
per, we propose n-gram-based tense models
for SMT and successfully integrate them in-
to a state-of-the-art phrase-based SMT system
via two additional features. Experimental re-
sults on the NIST Chinese-English translation
task show that our proposed tense models are
very effective, contributing performance im-
provement by 0.62 BLUE points over a strong
baseline.
1 Introduction
For many NLP applications, such as event extraction
and summarization, tense has been regarded as a key
factor in providing temporal order. However, tense
information has been largely overlooked by current
SMT research. Consider the following example:
SRC:??B$?e?(J , ?U?Ny3??
,???N?I???m??'X"
REF:The embargo is a result of the Cold War and does not
reflect the present situation nor the partnership between China
and the EU.
MOSES: the embargo is the result of the cold war, not reflect
the present situation, it did not reflect the partnership with the
european union.
?*Corresponding author.
Although the translated text produced by Moses1
is understandable, it has very odd tense combination
from the grammatical aspect, i.e. with tense incon-
sistency (is/does in REF vs. is/did in Moses). Ob-
viously, slight modification, such as changing ?is?
into ?was?, can much improve the readability of the
translated text. It is also interesting to note that such
modification can much affect the evaluation. If we
change ?did? to ?does?, the BLEU-4 score increases
from 22.65 to 27.86 (as matching the 4-gram ?does
not reflect the? in REF). However, if we change ?is?
to ?was?, the BLEU score decreases from 22.65 to
21.44.
The above example seems special. To testify its
impact on SMT in wider range, we design a special
experiment based on the 2005 NIST MT data (see
section 6.1). This data has 4 references. We choose
one reference and modify its sentences with error
tense2. After that, we use other 3 references to mea-
sure this reference. The modified reference leads to
a sharp drop in BLEU-4 score, from 52.46 to 50.27
in all. So it is not a random phenomenon that tense
can affect translation results.
The key is how to detect tense errors and choose
correct tenses during the translation procedure. By
carefully comparing the references with Moses out-
put, we obtain the following useful observations,
Observation(1): to most simple sentences, coor-
dinate verbs should be translated with the same tense
while they have different tense in Moses output;
Observation(2): to some compound sentences,
1http://www.statmt.org/moses/
2Such changes are small by mainly modifying one auxiliary
verb for a sentence, such as ?is? was?, ?has? had?.
276
the subordinate clause should have the consisten-
t tense with its main clause while Moses fails;
Observation(3): the diversity of tense usage in a
document is common. However, in most cases, the
neighbored sentences tends to share the same main
tense. In some extreme examples, one tense (past or
present), even dominates the whole document.
One possible solution to model above observa-
tions is using rules. Dorr (2002) refers to six ba-
sic English tense structures and defines the possible
paired combinations of ?present, past, future?. But
the practical cases are very complicated, especial-
ly in news domain. There are a lot of complicat-
ed sentences in news articles. Our preliminary in-
vestigation shows that such six paired combinations
can only cover limited real cases in Chinese-English
SMT.
This paper proposes a simple yet effective method
to model above observations. For each target sen-
tence in the training corpus, we first parse it and ex-
tract its tense sequence. Then, a target-side tense
n-gram model is constructed. Such model can be
used to estimate the rationality of tense combina-
tion in a sentence and thus supervise SMT to reduce
tense inconsistency errors against Observations (1)
and (2) in the sentence-level. In comparison, Ob-
servation (3) actually reflects the tense distributions
among one document. After extracting each main
tense for each sentence, we build another tense n-
gram model in the document-level. For clarity, this
paper denotes document-level tense as ?inter-tense?
and sentence-level tense as ?intra-tense?.
After that, we propose to integrate such tense
models into SMT systems in a dynamic way. It
is well known there are many errors in the current
MT output (David et al2006). Unlike previously
making trouble with reference texts, the BLEU-4 s-
core cannot be influenced obviously by modifying
a small part of abnormal sentences in a static way.
In our system, both inter-tense and intra-tense mod-
el are integrated into a SMT system via additional
features and thus can supervise the decoding proce-
dure. During decoding, once some words with cor-
rect tense can be determined, with the help of lan-
guage model and other related features, the smal-
l component??tense??can affect surrounding words
and improve the performance of the whole sentence.
Our experimental results (see the examples in Sec-
tion 6.4) show the effectiveness of this way.
Rather than the rule-based model, our models are
fully statistical-based. So they can be easily scaled
up and integrated into either phrase-based or syntax-
based SMT systems. In this paper, we employ a
strong phrase-based SMT baseline system, as pro-
posed in Gong et al2011), which uses document as
translation unit, for better incorporating document-
level information.
The rest of this paper is organized as follows: Sec-
tion 2 reviews the related work. Section 3 and 4 are
related to tense models. Section 3 describes the pre-
processing work for building tense models. Section
4 presents how to build target-side tense models and
discuss their characteristics. Section 5 shows our
way of integrating such tense models into a SMT
system. Session 6 gives the experimental results. Fi-
nally, we conclude this paper in Section 7.
2 Related Work
In this section, we focus on related work on integrat-
ing the tense information into SMT. Since both inter-
and intra-tense models need to analyze and extract
tense information, we also give a brief overview on
tense prediction (or tagging).
2.1 Tense Prediction
The tense prediction task often needs to build a mod-
el based on a large corpus annotated with temporal
relations and thus its focus is on how to recognize,
interpret and normalize time expressions. As a rep-
resentative, Lapata and Lascarides (2006) proposed
a simple yet effective data-intensive approach. In
particular, they trained models on main and subor-
dinate clauses connected with some special tempo-
ral marker words, such as ?after? and ?before?, and
employed them in temporal inference.
Another typical task is cross-lingual tense pred-
ication. Some languages, such as English, are in-
flectional, whose verbs can express tense via certain
stems or suffix, while others, such as Chinese of-
ten lack inflectional forms. Take Chinese to English
translation as example, if Chinese text contains par-
ticle word ?
(Le)?, the nearest Chinese verb prefers
to be translated into English verb with the past tense.
Ye and Zhang (2005), Ye et al2007) and Liu et al
(2011) focus on labeling the tenses for keywords in
277
source-side language.
Ye and Zhang (2005) first built a small amoun-
t of manually-labeled data, which provide the tense
mapping from Chinese text to English text. Then,
they trained a CRF-based tense classifier to label
tense on Chinese documents. Ye et al2007) fur-
ther reported that syntactic features contribute most
to the marking of aspectual information. Liu et al
(2011) proposed a parallel mapping method to au-
tomatically generate annotated data. In particular,
they used English verbs to label tense information
for Chinese verbs via a parallel Chinese-English cor-
pus.
It is reasonable to label such source-side verb to
supervise the translation process since the tense of
English sentence is often determined by verbs. The
problem is that due to the diversity of English ver-
b inflection, it is difficult to map such Chinese tense
information into the English text. To our best knowl-
edge, although above works attempt to serve for
SMT, all of them fail to address how to integrate
them into a SMT system.
2.2 Machine Translation with Tense
Dorr (1992) described a two-level knowledge repre-
sentation model based on Lexical Conceptual Struc-
tures (LCS) for machine translation which integrates
the aspectual information and the lexical-semantic
information. Her system is based on an inter-lingual
model and does not belong to a SMT system.
Olsen et al2001) relied on LCS to generate
appropriately-tensed English translations for Chi-
nese. In particular, they addressed tense reconstruc-
tion on a binary taxonomy (present and past) for
Chinese text and reported that incorporating lexical
aspect features of telicity can obtain a 20% to 35%
boost in accuracy on tense realization.
Ye et al2006) showed that incorporating latent
features into tense classifiers can boost the perfor-
mance. They reported the tense resolution results
based on the best-ranked translation text produced
by a SMT system. However, they did not report the
variation of translation performance after introduc-
ing tense information.
3 Preprocessing for Tense Modeling
In this paper, tense modeling is done on the target-
side language. Since our experiments are done
on Chinese to English SMT, our tense models are
learned only from the English text. In the literature,
the taxonomy of English tenses typically includes
three basic tenses (i.e. present, past and future) plus
their combination with the progressive and perfec-
tive aspects. Here, we consider four basic tenses:
present, past, future and UNK (unknown) and ignore
the aspectual information. Furthermore, we assume
that one sentence has only one main tense but maybe
has many subordinate tenses.
This section describes the preprocessing work of
building tense models, which mainly involves how
to extract tense sequence via tense verbs.
3.1 Tense Verbs
Lapata et al006) used syntactic parse trees to find
clauses connected with special aspect markers and
collected them to train some special classifiers for
temporal inference. Inspired by their work, we use
the Stanford parser3 to parse tense sequence for each
sentence.
Take the following three typical sentences as ex-
amples, (a) is a simple sentence which contains two
coordinate verbs, while (b) and (c) are compound
sentences and (b) contains a quoted text.
(a) Japan?s constitution renounces the right to go to war and
prohibits the nation from having military forces except for self-
defense.
(b) ?We also hope Hong Kong will not be affected by diseases
like the severe acute respiratory syndrome again!? , added Ms.
Yang.
(c) Cheng said he felt at home in Hong Kong and he sincerely
wished Hong Kong more peaceful and more prosperous.
Figure 1 shows the parse tree with Penn Treebank
style for each sentence, which has strict level struc-
tures and POS tags for all the terminal words. Here,
the level structures mainly contribute to extract main
tense for each sentence (to be described in Section
3.2) and POS tags are utilized to detect tense verbs,
i.e. verbs with tense information.
Normally, POS tags in the parse tree can distin-
guish five different forms of verbs: the base form
(tagged as VB), and forms with overt endings D for
3http://nlp.stanford.edu/software/lex-parser.shtml
278
Figure 1: The Stanford parse trees with Penn Treebank style
past tense, G for present participle, N for past par-
ticiple, and Z for third person singular present. It is
worth noting that VB, VBG and VBN cannot deter-
mine the specific tenses by themselves. In addition,
the verbs with POS tag ?MD? need to be special-
ly considered to distinguish future tense from other
tenses.
Algorithm 1 illustrates how to determine what
tense a node has. If the return value is not ?UNK?,
the node belongs to a tense verb.
Algorithm 1 Determine the tense of a node.
Input:
The TreeNode of one parse tree, leafnode;
Output:
The tense, tense;
1: tense = ?UNK ??
2: Obtaining the POS tag lpostag from leafnode;
3: Obtaining the word lword from leafnode;
4: if (lpostag in [?V BP ??, ?V BZ ??]) then
5: tense = ?present??
6: else if (lpostag == ?V BD??]) then
7: tense = ?past??
8: else if (lpostag == ?MD??]) then
9: if (lword in [?will??, ?ll??, ?shall??]) then
10: tense = ?future??
11: else if (lword in [?would??, ?could??]) then
12: tense = ?past??
13: else
14: tense = ?present??
15: end if
16: end if
17: return tense;
3.2 Tense Extraction Based on Tense Verbs
As described in Section 1, the inter-tense
(document-level) refers to the main tense of
one sentence and the intra-tense (sentence-level)
corresponds to all tense sequence of one sentence.
This section introduces how to recognize the main
tense and extract all useful tense sequence for each
sentence.
The idea of determining the main tense is to find
the tense verb located in the top level of a parse tree.
According to the Penn Treebank style, the method
to determine the main tense can be described as fol-
lows:
(1) Traverse the parse tree top-down until a tree node
containing more than one child is identified, denot-
ed as Sm .
(2) Consider each child of Sm with tag ?VP?, recursive-
ly traverse such ?VP? node to find a tense verb. If
found, use it as the main tense and return the tense;
if not, go to step (3).
(3) Consider each child of Sm with tag ?S?, which ac-
tually corresponds to subordinate clause of this sen-
tence. Starting from the first subordinate clause, ap-
ply the similar policy of step (2) to find the tense
verb. If not found, search remaining subordinate
clauses.
(4) If no tense verb found, return ?UNK? as the main
tense.
Here, ?VP? nodes dominated by Sm directly are
preferred over those located in subordinate clauses.
This is to ensure that the main tense is decided by
the top-level tense verb.
279
Take Figure 1 as an example, the main tense of
sentence (a) and (b) can be determined only by step
(2). The tense verb of ?(VBZ renounces)? dominat-
ed in the ?VP? tag determines that (a) is in present
tense. Similarly the node ?(VBD added)? indicates
that (b) is in past tense. Sentence (c) needs to be fur-
ther treated by step (3) since there is no ?VP? nodes
dominated by Sm directly. The node ?(VBD said)?
located in the first subordinate clause shows its main
tense is ?past?.
The next task is to extract the tense sequence for
each sentence. They are determined by all tense
verbs in this sentence according to the strict top-
down order. For example, the tense sequence of
sentence (a), (b) and (c) are {present, present},
{present, future, past} and {past, past, past}. In or-
der to explore whether the main tense of intra-tense
model has an impact on SMT or not, we introduce
a special marker ?*? to denote the main tense. So
the tense sequence marked with main tense of (a),
(b) and (c) are {present*, present},{present, future,
past*} and {past*, past, past}. It is worth noting, the
intra-tense model (see Section 4) based on the latter
tense sequence is different to the former.
4 N-gram-based Tense Models
4.1 Tense N-gram Estimation
After applying the previous method to extract tense
for an English text corpus, we can obtain a big tense
corpus.
Given the current tense is indexed as ti, we call
the previous n ? 1 tenses plus the current tense as
tense n-gram.
Based on the tense corpus, tense n-gram statistics
can be done according to the Formula 1.
P (ti|t(i?(n?1)), ..., t(i?1)) =
count(t(i?(n?1)), . . . , t(i?1), ti)
count(t(i?(n?1)), ..., t(i?1))
(1)
Here, the function of ?count? return the tense n-gram
frequency. In order to avoid doing specific smooth-
ing work, we estimate tense n-gram probability us-
ing SRI language modeling (SRILM) tool (Stolcke,
2002).
To compute the probability of intra-tense n-gram,
we first extract all tense sequence for each sentence
and put them into a new file. Based on this new file,
we can get the intra-tense n-gram model via SRILM
tool.
To compute the probability of inter-tense n-gram,
we need to extract the main tense for each sentence
at first. Then, for each document, we re-organized
the main tenses of all sentences into a special line.
After putting all these special lines into a new file,
we can use SRILM to obtain the inter-tense n-gram
model.
4.2 Characteristic of Tense N-gram Models
We construct n-gram-based tense models on English
Gigaword corpus (LDC2003T05). This corpus is
used to build language model for most SMT sys-
tems. It includes 30221 documents (we remove such
files: file size is less than 1K or the number of con-
tinuous ?UNK? tenses is greater than 5).
Figure 2 shows the unigram and bigram probabil-
ities (Log10-style) for intra-tense and inter-tense.
The part (a) and (c) in Figure 2 refer to unigram.
The horizontal axis indicts tense type, and the ver-
tical axis shows its probabilities. The parts (a) and
(c) also indicate ?present? and ?past? are two main
tense types in news domain.
The part (b) and (d) refer to bigram. The horizon-
tal axis indicts history tense. Each different color-
ful bar indicts one current tense. The vertical axis
shows the transfer possibilities from a history tense
to a current tense.
The part (b)4 reflects transfer possibilities of tense
types in one sentence. It also slightly reflects some
linguistic information. For example, in one sen-
tence, the probability of co-occurrence of ?present
? present?, ?past ? past? and ?future ? present?
is more than other combinations, which can be a-
gainst tense inconsistency errors described in Obser-
vation (1) and (2) (see Section 1). However, it seem-
s strange that ?present? past? exceeds ?present?
future?. We checked our corpus and found a lot of
sentences like this??the bill has been . . . , he said. ?.
The part (d) shows tense type can be switched be-
tween two neighbored sentences. However, it shows
the strong tendency to use the same tense type for
4The co-occurrence of the ?UNK? tense and other tense
types in one sentence cannot happen, so the ?UNK? tense is
omitted.
280
Figure 2: statistics of intra-tense and inter-tense N-gram
neighbored sentences. This statistics conform to the
previous observation (3) very much.
5 Integrating N-gram-based Tense Models
into SMT
In this section, we discuss how to integrate the pre-
vious tense models into a SMT system.
5.1 Basic phrase-based SMT system
It is well known that the translation process of SMT
can be modeled as obtaining the best translation e
of the source sentence f by maximizing following
posterior probability(Brown et al1993):
ebest = argmax
e
P (e|f)
= argmax
e
P (f |e)Plm(e)
(2)
where P (e|f) is a translation model and Plm is a
language model.
Our baseline is a modified Moses, which follows
Koehn et al2003) and adopts similar six groups
of features. Besides, the log-linear model ( Och and
Ney, 2000) is employed to linearly interpolate these
features for obtaining the best translation according
to the formula 3:
ebest = argmax
e
M?
m=1
?mhm(e, f) (3)
where hm(e, f) is a feature function, and ?m is
the weight of hm(e, f) optimized by a discrimina-
tive training method on a held-out development da-
ta(Och, 2003).
5.2 The Workflow of Our System
Our system works as follows:
When a hypothesis has covered all source-side
words during the decoding procedure, the decoder
first obtains tense sequence for such hypothesis and
computes intra-tense feature Fs(see Section 5.3). At
the same time, it recognizes the main tense of this
hypothesis and associate the main tense of previous
sentence to compute inter-tense feature Fm (see Sec-
tion 5.3).
Next, the decoder uses such two additional feature
values to re-score this hypothesis automatically and
choose one hypothesis with highest score as the final
translation.
After translating one sentence, the decoder caches
its main tense and pass it to the next sentence.
When one document has been processed, the de-
coder cleans this cache.
In order to successfully implement above work-
flow, we should firstly design some related features,
then resolve another key problem of determining
tense (especially main tense) for SMT output. They
are described in Section 5.3 and 5.4 respectively.
5.3 Two Additional Features
Although the previous tense models show strong
tendency to use the consistent tenses for one sen-
tence or one document, other tense combinations al-
so can be permitted. So we should use such models
in a soft and dynamic way. We design two features:
inter-tense feature and intra-tense feature. And the
weight of each feature is tuned by the MERT script
in Moses packages.
Given main tense sequence of one documen-
t t1, . . . , tm, the inter-tense feature Fm is calculated
according to the following formula:
Fm =
m?
i=2
P (ti|ti?(n?1), . . . , t(i?1)) (4)
The P (?) of formula 4 can be estimated by the for-
mula 1. It is worth noting the first sentence of one
281
document often scares tense information since it cor-
responds to the title at most cases. To the first sen-
tence, the P (?) value is set to 14 (4 tense types).
Given tense sequence of one sentence
s1, . . . , se (e > 1), the intra-tense feature Fs
is calculated as follows:
Fs = e?1
?
?
?
?
e?
i=2
P (si|si?(n?1), . . . , s(i?1)) (5)
Here the square-root operator is used to avoid pun-
ishing translations with long tense sequence. It is
worth noting if the sentence only contains one tense,
the P (?) value of formula 5 is also set to 14 .
Since the average length of intra-tense sequence
is about 2.5, we mainly consider intra-tense bigram
model and thus n equals to 2. 5
5.4 Determining Tense For SMT Output
The current SMT systems often produce odd transla-
tions partly because of abnormal word ordering and
uncompleted text etc. For these abnormal translated
texts, the syntactic parser cannot work well in our
initial experiments, so the previous method to parse
main tense and tense sequence of regular texts can-
not be applied here too.
Fortunately, the solely utilization of Stanford POS
tagger for our SMT output is not bad although it has
the same issues described in Och et al2002). The
reason may be that phrase-based SMT contains short
contexts that POS tagger can utilize while the syntax
parser fails.
Once obtaining a completed hypothesis, the de-
coder will pass it to the Stanford POS tagger and ac-
cording to tense verbs to get alense sequence for
this hypothesis. However, since the POS tagger can
not return the information about level structures, the
decoder cannot recognize the main tense from such
tense sequence.
Liu et al2011) once used target-side verbs to la-
bel tense of source-side verbs. It is natural to consid-
er whether Chinese verbs can provide similar clues
in an opposite direction.
Since Chinese verbs have good correlation with
English verbs (described in section 6.2), we obtain
5In our experiment, the intra-tense bigram model can ob-
tain the comparable performance to the trigram model. And the
inter-tense trigram model can not exceed the bigram one.
Figure 3: trees for parallel sentences
main tense for SMT output according to such tense
verb, which corresponds to the ?VV?(Chinese POS
labels are different to English ones, ?VV? refers to
Chinese verb) node in the top level of the source-side
parse tree. Take Figure 3 as an example, the English
node ?(VBD announced)? is a tense verb which can
tell the main tense for this English sentence. The
Chinese verb ?(VV??)? in the top-level of the
Chinese parse tree is just the corresponding part for
this English verb.
So, before translating one sentence, the decoder
first parses it and records the location of one Chinese
?VV? node which located in the top-level, denotes
this location as Sarea.
Once a completed hypothesis is generated, ac-
cording to the phrase alignment information, the de-
coder can map Sarea into the English location Tarea
and obtain the main tense according to the POS tags
in Tarea .
If Tarea does not contain tense verb, such as the
verb POS tags in the list of {VB, VBN, VBG},
which cannot tell tense type by themselves, our sys-
tem permits to find main tense in the left/right 3
words neighbored to Tarea. And the proportion that
the top-level verb of Chinese has a verb correspon-
dence in English can reach to 83% in this way.
6 Experimentation
6.1 Experimental Setting for SMT
In our experiment, SRI language modeling toolk-
it was used to train a 5-Gram general language
model on the Xinhua portion of the Gigaword cor-
pus. Word alignment was performed on the train-
ing parallel corpus using GIZA++ ( Och and Ney,
2000) in two directions. For evaluation, the NIST
282
BLEU script (version 13) with the default setting is
used to calculate the BLEU score (Papineni et al
2002), which measures case-insensitive matching of
4-grams. To see whether an improvement is statisti-
cally significant, we also conduct significance tests
using the paired bootstrap approach (Koehn, 2004).
In this paper, ?***? and ?**? denote p-values equal
to 0.05, and bigger than 0.05, which mean signifi-
cantly better, moderately better respectively.
Corpus Sentences Documents
Role Name
Train FBIS 228455 10000
Dev NIST2003 919 100
Test NIST2005 1082 100
Table 1: Corpus statistics
We use FBIS as the training data, the 2003 NIST
MT evaluation test data as the development data, and
the 2005 NIST MT test data as the test data. Table 1
shows the statistics of these data sets (with document
boundaries annotated).
6.2 The Correlation of Chinese Verbs and
English Verbs
In this section, an additional experiment is designed
to show English Verbs have close correspondence
with Chinese Verbs.
We use the Stanford POS tagger to tag the Chi-
nese and English sentences in our training corpus
respectively at first. Then we utilize Giza++ to build
alignment for these special Word-POS pairs. Ac-
cording to the alignment results, we find the corre-
sponding relation for some special POS tags in two
languages.
Chinese Verb POS English POS Number
VV Verb VBD 89830
POS VBP 27276
VBZ 32588
MD 40378
VBG 86025
VBN 75019
VB 153596
In sum: 504712
Other Non-Verb 149318
Verb Corresponding Ratio 0.77169
Table 2: The Chinese and English Verb Pos Alignment
The ?Number? column of Table 2 shows the num-
bers of Chinese words with ?VV? tag correspond-
ing to English words with different verb POS tags.
We found Chinese verbs have more than 77% possi-
bilities to align to English verbs in total. However,
our method will fail when some special Chinese sen-
tences only contain noun predicates.
6.3 Experimental Results
All the experiment results are showed on the table 3.
Our Baseline is a modified Moses. The major modi-
fication is input and output module in order to trans-
late using document as unit. The performance of our
baseline exceeds the baseline reported by Gong et al
(2011) about 2 percent based on the similar training
and test corpus.
System BLEU BLEU on Test(%)
Dev(%) BLEU NIST
Moses Md(Baseline) 29.21 28.30 8.4528
Baseline+Fm 30.56 28.87(***) 8.7935
Baseline+Fs 31.28 28.61(**) 8.5645
Baseline+Fs(?) 31.04 28.74(**) 8.6271
Baseline+Fm+Fs 31.75 28.88(***) 8.7987
Baseline+Fm+Fs(*) 31.42 28.92(***) 8.8201
Table 3: The performance of using different feature com-
binations
The system denoted as ?Baseline+Fm? integrates
the inter-tense feature. The performance boosts
0.57(***) in BLEU score.
The system denoted as ?Baseline+Fs? integrates
the intra-tense feature into the baseline. The im-
provement is less than the inter-tense model, on-
ly 0.31(**). It seems the tenses in one sentence
has more flexible formats than the document-level
ones. It is worth noting, this method can gain high-
er performance on the develop data than the one of
?Baseline+Fm? while fail to improve the test data.
Maybe the related weight is tuned over-fit.
The system denoted as ?Baseline+Fs(*)? is s-
lightly different from ?Baseline+Fs?. This experi-
ment is to check whether the main tense has an im-
pact on intra-tense model or not (see Section 3.2).
Here, the intra-tense model based on the tense se-
quence with main tense marker is slightly different
to the model showed in Figure 2. The results are
slightly better than the previous system by 0.13.
Finally, we use the two features together
(Baseline+Fm+Fs). The best way improved the
performance by 0.62(***) in BLEU score over our
baseline.
283
6.4 Discussion
Table 4 shows special examples whose intra-tenses
are changed in our proposed system. The exam-
ple 1 and 2 show such modification can improve
the BLEU score but the example 3 obtains negative
impact. From these examples, we can see not only
tense verbs have changed but also their surrounding
words have subtle variation.
No. BLEU Translation sentence
1 8.64 Baseline: the gulf countries , the bahraini royal fam-
ily members by the military career of part of the
banned to their marriage stories like children , have
become the theme of television films .
19.71 Ours: the gulf country is a member of the bahraini
royal family , a risk by military career risks part of
the banned to their marriage like children , has be-
come a story of the television and film industry .
2 17.16 Baseline:economists said that the sharp appreciation
of the euro , in the recent investigation continues to
have an impact on economic confidence , it is esti-
mated that the economy is expected to rebound to
pick up .
24.25 Ours: economists said that the sharp appreciation of
the euro , in the recent investigation continued to
have an impact on economic confidence and there-
fore no reason to predict the economy expected to
pick up a rebound .
3 73.03 Baseline: the middle east news agency said that , af-
ter the concerns of all parties concerned in the mid-
dle east peace process , israel and palestine , egypt ,
the united states , russia and several european coun-
tries will hold a meeting in washington .
72.95 Ours: the middle east news agency said that after the
concerns of all parties in the middle east peace pro-
cess , israel and palestine , egypt , the united states ,
russia and several european countries held a meeting
in washington .
Table 4: Examples with tense variation using intra-tense
model
From the results showed on Table 3, the
document-level tense model seems more effective
than the sentence-level one. We manually choose
and analyzed 5 documents with significant improve-
ment in the test data. The part (a) of Figure 4 shows
the main tense distributions of one reference. The
main tense distributions for the baseline and our pro-
posed system are showed in the part (b) and (c) re-
spectively. These documents have different numbers
of sentences but all less than 10. The vertical axis in-
dicates different tense: 1 to ?past?, 2 to ?present?, 3
to ?future? and 4 to ?UNK?. It is obvious that our
system has closer distributions to the ones of this
reference.
The examples in Table 5 indicate the joint impact
of inter-tense and intra-tense model on SMT. Sen-
Src:
1)????K???^??? , |????; ??
??"
2)nVd")?|?+ECnd??Oc  ??W
?	?|? ,??Loc5????gONnV
d"??+<??\?! ?;"
Ref:
1)Israeli settlers blockaded a major road to protest a mortar attack
on the settlement area.
2)PLO leader Abbas had also been allowed to go to the West Bank
town of Bethlehem , which is the first time in the past four years
Israeli authorities have allowed a senior Palestinian leader to attend
Christmas celebrations.
Baseline:
1)israel has imposed a main road to protest by mortars attack .
2)the palestinian leader also visited the west bank cities and towns
to bethlehem , which in the past four years , the israeli authorities
allowed the palestinian leading figures attended the ceremony .
Ours:
1)israel has imposed a main road to protest against the mortars at-
tack .
2)leader of the palestinian liberation organization have also been
allowed to go to the west bank towns , bethlehem in the past four
years . this is the first time the israeli authorities allow palestinian
leading figures attended the ceremony .
Table 5: the joint impact of inter- tense and intra-tense
models on SMT
tence 1) and 2) are two neighbored sentences in one
document. Both the reference and our output tend
to use the same main tense type, but the former is in
?past? tense and the latter is in ?present? tense. The
baseline cannot show such tendency. Although our
main tense is different to the reference one, the con-
sistent tenses in document level bring better trans-
lation results than the baseline. And the tenses in
sentence level also show better consistency than the
baseline.
7 Conclusion
This paper explores document-level SMT from the
tense perspective. In particular, we focus on how to
build document-level and sentence-level tense mod-
els and how to integrate such models into a popular
SMT system.
Compared with the inter-tense model which great-
ly improves the performance of SMT, the intra-tense
model needs to be further explored. The reasons are
many folds, e.g. the failure to exclude quoted texts
when modeling intra-tense, since tenses in quoted
texts behave much diversely from normal texts. In
the future work, we will focus on modeling intra-
tense variation according to specific sentence types
and using more features to improve it.
284
Figure 4: the comparison of the inter-tense distributions for reference, baseline and our proposed system
Acknowledgments
This research is supported by part by NUS FRC
Grant R252-000-452-112, the National Natural Sci-
ence Foundation of China under grant No.90920004
and 61003155, the National High Technology Re-
search and Development Program of China (863
Program) under grant No.2012AA011102.
References
P.F. Brown, S.A. Della Pietra, V.J. Della Pietra and R.L.
Mercer. 1992. The Mathematics of Statistical Ma-
chine Translation: Parameter Estimation. Computa-
tional Linguistics, 19(2):263-311.
Vilar David, Jia Xu, DH`aro L. F., and Hermann Ney.
2006. Error Analysis of Machine Translation Output.
In Proceedings of the 5th International Conference on
Language Resources and Evaluation, pages 697-702,
Genoa, Italy.
Bonnie J. Dorr. 1992. A parameterized approach to
integrating aspect with lexical-semantics for machine
translation. In Proceedings of of ACL-2002, pages
257-264.
Bonnie J. Dorr and Terry Gaasterland. 2002. Constraints
on the Generation of Tense, Aspect, and Connecting
Words from Temporal Expressions. Technical Reports
from UMIACS.
Zhengxian Gong, Min Zhang and Guodong Zhou.
2011. Cached-based Document-level Statistical Ma-
chine Translation. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 909-919.
Philipp Koehn, Franz Josef Och ,and DanielMarcu. 2003.
Statistical Phrase-Based Translation. In Proceedings
of NAACL-2003, pages 48-54.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Natu-
ral Language Processing, pages 388-395.
Mirella Lapata, Alex Lascarides. 2006. Learning
Sentence-internal Temporal Relations. Journal of Ar-
tificial Intelligence Research, 27:85-117.
Feifan Liu, Fei Liu and Yang Liu. 2011. Learning from
Chinese-English Parallel Data for Chinese Tense Pre-
diction. In Proceedings of IJCNLP-2011, pages 1116-
1124,Chiang Mai, Thailand.
Franz Josef Och and Hermann Ney. 2000. Improved Sta-
tistical Alignment Models. In Proceedings of of ACL-
2000, pages 440-447.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
et.al. 2002. A smorgasbord of Features for Statistical
Machine Translation. In Proceedings of NAACL-2004,
pages 440-447.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings of
ACL-2003,pages 160-167, Sapporo, Japan,July.
Mari Olsen, David Traum,Carol Van Ess-Dykema and
Amy Weinberg. 2001. Implicit Cues for Explicit Gen-
eration: Using Telicity as a Cue for Tense Structure in
a Chinese to English MT System. In Proceedings of
MT Summit VIII, Santiago de Compostella, Spain.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
jing Zhu. 2002. BLEU: A Method for Automatic E-
valuation of Machine Translation. In Proceedings of
ACL-2002, pages 311-318.
Andreas Stolcke. 2002. SRILM-an extensible language
modeling toolkit. In Proceedings of the Internation-
al Conference on Spoken Language Processing,pages
901-904.
Yang Ye and Zhu Zhang. 2005. Tense tagging for verbs
in cross-lingual context: A case study. In Proceedings
of IJCNLP-2005, pages 885-895.
Yang Ye, V.li Fossum, Steven Abney. 2006. Laten-
t features in Temporal Reference Translation. Fifth
SIGHAN Workshop on Chinese Language Processing,
pages 48-55.
Yang Ye, Karl-Michael Schnelder, Steven Abney. 2007.
Aspect Marker Generation in English-to-Chinese Ma-
chine Translation. Proceedings of MT Summit XI,
pages 521-527,Copenhagen, Denmark.
285
