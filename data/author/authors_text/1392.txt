75
76
77
78
Abstract
AKT is a major research project
applying a variety of technologies to
knowledge management. Knowledge
is a dynamic, ubiquitous resource,
which is to be found equally in an
expert's head, under terabytes of data,
or explicitly stated in manuals. AKT
will extend knowledge management
technologies to exploit the potential
of the semantic web, covering the use
of knowledge over its entire lifecycle,
from acquisition to maintenance and
deletion. In this paper we discuss how
HLT will be used in AKT and how
the use of HLT will affect different
areas of KM, such as knowledge
acquisition, retrieval and publishing.
1 Introduction
As globalisation reduces the competitive
advantage existing between companies, the role
of proprietary information and its appropriate
management becomes all-important. A
company?s value depends more and more on
?intangible assets?1 which exist in the minds of
employees, in databases, in files and in a
multitude of documents. It is the goal of
knowledge management (KM) technologies to
make computer systems which provide access to
this intangible knowledge present in a company
or organisation.  The system must make it
possible to share, store and retrieve the
collective expertise of all the people in an
organization. At present, many companies spend
                                                     
1 A term coined by Karl-Erik Sveiby
considerable resources on knowledge
management; estimates range between 7 and
10% of revenues (Davenport 1998).
In developing a knowledge management
system, the knowledge must first be captured or
acquired in some form which is usable by a
computer. The knowledge acquisition
bottleneck, so well-known in AI, is just as
important in knowledge management. The
acquisition of knowledge does not become less
difficult in a business environment and often
requires a sea-change in company culture in
order to persuade users to accommodate to the
technology adopted, precisely because
knowledge acquisition is so difficult.
Once knowledge has been acquired, it must be
managed, i.e. modelled, updated and published.
Modelling means representing information in a
way that is both manageable and easy to
integrate with the rest of the company?s
knowledge. Updating is necessary because
knowledge is dynamic. Part of its importance
for a company or individual lies in the fact that
knowledge is ever changing and keeping up
with the change is a crucial dimension in
knowledge management. Publishing is the
process that allows sharing the knowledge
across the company. These needs have
crystallised in efforts to develop the so-called
Semantic Web. It is envisaged that in the future,
the content currently available on the Web (both
Internets and Intranets) as raw data will be
automatically annotated with machine-readable
semantic information.  In such a case, we will
no longer speak of information retrieval but
rather of Knowledge Retrieval because instead
of obtaining thousands of potentially relevant or
irrelevant documents, only the dozen or so
documents that are truly needed by the user will
be presented to them.
Using HLT for Acquiring, Retrieving and Publishing Knowledge in AKT:
Position Paper
K. Bontcheva, C. Brewster, F. Ciravegna, H. Cunningham,
L. Guthrie, R. Gaizauskas, Y. Wilks
Department of Computer Science, the University of Sheffield,
Regent Court, 211 Portobello Street, S1 4DP Sheffield, UK
Email: N.Surname@dcs.shef.ac.uk
 In this paper we present the way Human
Language Technology (HLT) is used to address
several facets of the KM problem:  acquiring,
retrieving, and publishing knowledge. The work
presented in this paper is supported by the AKT
project (Advanced Knowledge Technologies), a
multimillion pound six year research project
funded by the EPSRC in the UK. AKT, started
in 2000, involves the University of
Southampton, the Open University, the
University of Edinburgh, the University of
Aberdeen, and the University of Sheffield
together with a large number of major UK
companies. Its objectives are to develop
technologies to cope with the six main
challenges of knowledge management:
? acquisition ? reuse
? modelling ? publication
? retrieval/extraction ? maintenance
These challenges will be addressed by the
University of Sheffield in the context of AKT
by the application of a variety of human
language technologies. Here, we consider only
the contribution of HLT to the acquisition of
knowledge, its retrieval and extraction, its
publication, and finally the role of appropriate
HLT infrastructure to the completion of these
goals.
2 Knowledge Acquisition
Knowledge acquisition (KA) is concerned with
the process of turning data into coherent
knowledge for a computer program.  The need
for effective KA methods increases as the
quantity of data available electronically
increases year by year, and the importance it
plays in our society is more and more
recognised. The challenge, we believe, lies in
designing effective techniques for acquiring the
vast amounts of (largely) tacit knowledge. KA is
a complex process, which traditionally is
extremely time consuming.
Existing KA methodologies are varied but
almost always require a great deal of manual
input. One methodology, often used in Expert
Systems, involves the time-consuming process
of structured interviews (?protocols?), which are
then analysed by knowledge engineers in order
to codify and model the knowledge of an expert
in a particular domain. Even if a complex expert
system is not required, all forms of KA are very
labour intensive. Yahoo currently employs over
100 people to keep its category hierarchy up to
date (Dom 1999). Some methodologies have
started to appear to automate this process,
although still limited to some steps in the KA
process. They depend on replacing the
introspection of knowledge engineers or the
extended elicitations of the protocol methods
(Ericsson and Simon 1984) by using Human
Language Technologies, more specifically
Information Extraction, Natural Language
Processing and Information Retrieval.
 Although knowledge acquisition produces
data (knowledge) for use by a computer
program, the form and content of that
knowledge is often debated in the research
community.  Ontologies have emerged as one of
the most popular means of modelling the
knowledge of a domain.  The meaning of this
word varies somewhat in the literature, but
minimally it is a hierarchical taxonomy of
categories, concepts or words. Ontologies can
act as an index to the memory of an organisation
and facilitate semantic searches and the retrieval
of knowledge from the corporate memory as it
is embodied in documents and other archives.
Repeated research has shown their usefulness,
especially for specific domains (J?rvelin and
Kek?l?inen 2000). The process of ontology
construction is illustrated in the rest of this
section.
2.1 Taxonomy construction
We propose to introduce automation in the stage
of taxonomy construction mainly in order to
eliminate or reduce the need for extensive
elicitation of data.  In the literature approaches
to construction of taxonomies of concepts have
been proposed (Brown et al 1992, McMahon
and Smith 1996, Sanderson and Croft 1999).
Such approaches either use a large collection of
documents as their sole data source, or they can
attempt to use existing concepts to extend the
taxonomy (Agirre et al2000, Scott 1998).  We
intend to develop a semi-automatic method that,
starting from a seed ontology sketched by the
user, produces the final ontology via a cycle of
refinements by eliciting knowledge from a
collection of texts. In this approach the role of
the user should only be that of proposing an
initial ontology and validating/changing the
different versions proposed by the system.
We intend to integrate a methodology for
automatic hierarchy definition (such as
(Sanderson and Croft 1999)) with a method for
the identification of terms related to a concept
in a hierarchy (such as (Scott 1998)). The
advantage of this integration is that, as
knowledge is continually changing, we can
reconstruct an appropriate domain specific
ontology very rapidly. This does not preclude
incorporating an existing ontology and using the
tools to extend and update it on the basis of
appropriate texts. Finally an ontology defined in
this way has the particular advantage that it
overcomes the well-known ?Tennis problem?
associated with many predefined ontologies
such as WordNet, i.e where terms closely
related in a given domain are structurally very
distant such as ball and court, for example.
In addition we intend to employ classic
Information Extraction techniques (described
below) such as named entity recognition
(Humphreys et al 1998) in order to pre-process
the text, as the identification of complex terms
such as proper names, dates, numbers, etc,
allows to reduce data sparseness in learning
(Ciravegna 2000).
We plan to introduce many cycles of ontology
learning and validation. At each stage the
defined ontology can be: i) validated/corrected
by a user/expert; ii) used to retrieve a larger set
of appropriate documents to be used for further
refinement (J?rvelin and Kek?l?inen 2000); iii)
passed on to the next development stage.
2.2 Learning Other Relations
This stage proceeds to build on the skeletal
ontology in order to specify, as much as
possible without human intervention, relations
among concepts in the ontology, other than
ISAs. In order to flesh the concept relations, we
need to identify relations such as synonymy,
meronymy, antonymy and other relations. We
plan to integrate a variety of methods existing in
the literature, e.g. by using recurrences in verb
subcategorisation as a symptom of general
relations (Basili et al 1998), by using Morin?s
user-guided approach to identify the correct
lexico/syntactic environment (Morin 1999), and
by using methods such as (Hays 1997) to locate
specific cases of synonymy.
3 Knowledge Extraction
Assuming that the shape of knowledge has been
acquired and adequately modelled, it will have
to be stored in a repository from which it is
retrieved as and when needed. On the one hand
there is the problem of retrieving instances in
order to populate the resulting knowledge base.
On the other hand, considering that repositories
could become very substantial in size, there is
the necessity to navigate the repository in order
to extract the knowledge when needed. In this
section we focus on the problem of knowledge
base population, as it is in our opinion the most
challenging from the HLT point of view.
3.1 Knowledge Base  Population
Instance identification for Knowledge Base
population can be performed by HLT-based
document analysis. With the term documents,
we mean a wide variety of types of texts such as
plain texts, web pages, knowledge elicitation
interview transcriptions (protocols), etc.  For the
sake of this paper we limit our analysis to
language related tasks only, ignoring the
problem of multi-media information. As a first
step instance identification requires the
identification of relevant documents containing
citation of the interesting information
(document classification). Then it requires the
ability to identify and extract information from
documents (Information Extraction from text).
3.2 Document Classification
Text classification for IE purposes has been
explored both in the MUC conferences as well
as in some commercially oriented projects
(Ciravegna et al 2000). In concrete terms
classification is used in order to identify the
scenario to apply to a specific set of texts, while
IE will identify (i.e. index) the instances in the
texts.  In most cases of application document
classification is quite straightforward, being
limited to the Boolean classification of a
document between relevant/irrelevant (single
scenario application as in the MUC
conferences). In cases in which knowledge may
be distributed along a number of different
detailed scenarios, full document classification
is then needed. In such cases, two main
characteristics are relevant for the classification
approach: flexibility and refinability (Ciravegna
et al 1999). Flexibility is needed with respect
to both the number of the categories and the
granularity of the classification to be coped
with. Three main types of classification can be
identified: coarse-grained, fine-grained, and
content-based. Coarse-grained classification is
performed among a relatively small number of
classes (e.g., some dozens) that are sharply
different (e.g., sport vs finance). This can be
obtained reliably and efficiently by the
application of statistical classifiers. Fine-
grained classification is performed over a
usually larger number of classes that can be
very similar (e.g., discriminating between news
about private bond issues and news about public
bond issues). This type of classification
generally requires some more knowledge-
oriented approaches such as pattern-based
classification. Sometimes categories are so
similar that classification needs to be content-
based, i.e. it can be performed only by
extracting the news content (e.g., finding news
articles issued by English financial institutions
referring to amounts in excess of 100,000 Euro).
In this case some forms of shallow adaptive
Information Extraction can be used (see next
section). Refinability concerns the possibility
of performing classification in a sequence of
steps, each one providing a more precise
classification (from coarse-grained to content-
based). In the current technological situation
coarse-grained classification can be performed
quickly, while the systems available for more
fine-grained classification are much slower and
less general purpose. When the amount of
textual material is large an incremental
approach, based on some level of coarse-grained
classification further refined by successive
analysis, proves to be very effective. A refinable
classification is generally performed over a
hierarchy of classes. A refinement may revise
the categories assigned to specific texts with
more specialised classes from the hierarchy.
More complex techniques are invoked only
when needed and, in any case, within an already
detected context (Ciravegna et al 1999).
We plan to produce a number of solutions for
text classification, adaptable to different
scenarios and situations, following the criteria
mentioned above.
3.3 Information Extraction
Information extraction from text (IE) is the
process of mapping of texts into fixed format
output (templates) representing the key
information (Gaizauskas 1997). In using IE for
KM, templates represent an intermediate format
for mapping the information in the texts into
ontology instances. Templates can be semi-
automatically derived from the ontology. We
plan to use IE for a number of passes: on the
one hand, we plan to populate a knowledge base
with instances as mentioned above. On the other
hand, IE can be used to monitor relevant
changes in the information, providing a
fundamental contribution to the problem of
knowledge updating. We have a long experience
in IE from texts, Sheffield having actively
participated in the MUC conferences and in the
TIPSTER project, activities that historically
have made a fundamental contribution to
making IE as we now know it.  The new
challenge we are currently addressing is
adaptivity. Adaptivity is a major goal for
Information Extraction, especially in the case of
its application to knowledge management, as
KM is a process that has to be distributed
throughout companies. The real value of IE will
become apparent when it can be adapted to new
applications and scenarios directly by the final
user without the intervention of IE experts. The
goal for research in adaptive IE is to create
systems adaptable to new applications/domains
by using only an analyst?s knowledge, i.e.
knowledge about the domain/scenario.
There are two directions of research in
adaptive IE, both involving the use of Machine
Learning. On the one hand machine learning is
used to automate as much as possible the tasks
an IE expert would perform in application
development (Cardie 1997) (Yangarber et al
2000). The goal here is to reduce the porting
time to a new application (and hence the cost).
This area of research comes mainly from the
MUC community. Currently, the technology
makes use mainly of NLP-intensive
technologies and the type of texts addressed are
mainly journal articles.
On the other hand, there is an attempt to make
IE systems adaptable to new
domains/applications by using only an analyst?s
knowledge, i.e. knowledge about the
domain/scenario only (Kushmerick et al 1997),
(Califf 1998), (Muslea et al 1998), (Freitag and
McCallum 1999), (Soderland 1999), (Freitag
and Kushmerick 2000), (Ciravegna 2001a).
Most research has so far focused on Web-
related texts (e.g. web pages, email, etc.)
Successful commercial products have been
created and there is an increasing interest on IE
in the Web-related market.  Current adaptive
technologies make no use of natural language
processing in the web context, as extra linguistic
structures (e.g. HTML tags, document
formatting, and ungrammatical stereotypical
language) are the elements used to identify
information. Linguistically intensive approaches
are difficult or unnecessary in such cases. When
these non-linguistic approaches are used on
texts with a reduced (or no) structure, they tend
to be ineffective.
There is a technological gap between adaptive
IE on free texts and adaptive IE on web-related
texts. For the purposes of KM, such a gap has to
be bridged so to create a set of technologies able
to cover the whole range of potential
applications for different kinds of texts, as the
type of texts to be analysed for KM may vary
dramatically from case to case. We plan to
bridge this gap via the use of lazy natural
language processing. We intend to use an
approach where the system starts with a range
of potential methodologies (from shallow to
linguistically intensive) and learns from a
training corpus which is the most effective
approach for the particular case under
consideration. A number of factors can
influence the choice: from the type of texts to be
analysed to the type of information the user is
able to provide in adapting the system. In the
first case the system will have to identify what
type of task is under consideration and select the
correct level of analysis  (e.g. language based
for free texts). Formally in this case the level of
language analysis is one of the parameters the
learner will have to learn. Concerning the type
of tagging the user is able to provide: different
users are able to provide different levels of
information in training the system: IE-trained
users are able to provide sophisticated tagging,
maybe inclusive of syntactic, semantic or
pragmatic information. Na?ve users on the other
hand are only able to provide some basic
information (e.g. to spot the relevant
information in the texts and highlight it in
different colours). We plan to develop a system
able to cope with a wide of variety of situations
by starting from the (LP)2 algorithm and
enhancing its learning capabilities on free texts
(Ciravegna 2001) and developing a powerful
human-computer interface for system adaptation
(Ciravegna and Petrelli 2001).
4 Knowledge Publishing
Knowledge is only effective if it is delivered in
the right form, at the right place, to the right
person at the right time. Knowledge publishing
is the process that allows getting knowledge to
the people who need it in a form that they can
use. As a matter of fact, different users need to
see knowledge presented and visualised in quite
different ways. The dynamic construction of
appropriate perspectives is a challenge which, in
AKT, we will address from the perspective of
generating automatically such presentations
from the ontologies acquired by the KA and KE
methods, discussed in the previous sections.
Natural Language Generation (NLG) systems
automatically produce language output (ranging
from a single sentence to an entire document)
from computer-accessible data, usually encoded
in a knowledge or data base (Reiter 2000). NLG
techniques have already been used successfully
in a number of application domains, the most
relevant of which is automatic production of
technical documentation (Reiter et al 1995),
(Paris et al 1996). In the context of KM and
knowledge publishing in particular, NLG is
needed for knowledge diffusion and
documenting ontologies. The first task is
concerned with personalised presentation of
knowledge, in the form needed by each specific
user and tailored to the correct language type
and the correct level of details. The latter is a
very important issue, because as discussed
earlier, knowledge is dynamic and needs to be
updated frequently. Consequently, the
accompanying documentation which is vital for
the understanding and successful use of the
acquired knowledge, needs to be updated in
sync. The use of NLG simplifies the ontology
maintenance and update tasks, so that the
knowledge engineer can concentrate on   the
knowledge itself, because the documentation is
automatically updated as the ontology changes.
The NLG-based knowledge publishing tools
will also utilise the ontology instances extracted
from documents using the IE approaches
discussed in Section 3.3. The dynamically
generated documentation will not only include
these instances, as soon as they get extracted,
but it will also provide examples of their
occurrence in the documents, thus facilitating
users? understanding and use of the ontology.
Our approach to knowledge publishing is based
on an existing framework for generation of user-
adapted hypertext explanations (Bontcheva
2001), (Bontcheva and Wilks 2001). The
framework incorporates a powerful agent
modelling module, which is used to tailor the
explanations to the user?s knowledge, task, and
preferences. We are now also extending the
personalisation techniques to account for user
interests. The main challenge for NLG will be
to develop robust and efficient techniques for
knowledge publishing which can operate on
large-scale knowledge resources and support the
personalised presentation of diverse
information, such as speech, video, text,
graphics (see (Maybury 2001)).
The other challenge in using NLG for
knowledge publishing is to develop tools and
techniques that will enable knowledge
engineers, instead of linguists, to create and
customise the linguistic resources (e.g., domain
lexicon) at the same time as they create and edit
the ontology.  In order to allow such inter-
operability with the KA tools, we will integrate
the NLG tools in the GATE infrastructure,
discussed next.
5 HLT Infrastructure
The range and complexity of the task of
knowledge management make imperative the
need for standardisation. While there has been
much talk about the re-use of knowledge
components such ontologies, much less has
been undertaken to standardise the
infrastructure for tools and their development.
The types of data structures typically involved
are large and complex, and without good tools
to manage and allow succinct viewing of the
data we will continue to work below our
potential. The University of Sheffield has
pioneered in the Gate and Gate 2 projects the
development of an architecture for text
engineering (Cunningham et al 1997),
(Cunningham et al 2000). Given the modular
architecture and component structure of Gate, it
is natural to build on this basis to extend the
capabilities of Gate so as to provide the most
suitable possible environment for tool
development, implementation and evaluation in
AKT. The system will provide a single
interaction and deployment point for the roll-out
of HLT in Knowledge Management. We expect
Gate2 to act as the skeleton for a large range of
knowledge management activities within AKT
and plan to extend its capabilities within the life
of the AKT project by integrating with suitable
ontological and lexical databases in order to
permit the use of  the Gate system with large
bodies of heterogeneous data
6 Conclusion and Future Work
We have presented how we plan to use HLT for
helping KM in AKT. We believe that HLT can
make a substantial contribution to the following
issues in  KM:
? Cost reduction: KM is an expensive task,
especially in the acquisition phase. HLT can
aid in automating both the acquisition of the
structure of the ontology to be learnt and in
populating such ontology with instances. It
will also provide support for automatic
knowledge documentation.
? Time reduction: KM is a slow task: HLT
can help in making it more efficient by
reducing the need for the human effort;
? Subjectivity reduction: this is a main
problem in knowledge identification and
selection. Subjective knowledge is difficult
to integrate with the rest of the company?s
knowledge and its use is somehow difficult.
KM constitutes a challenge for HLT as it
provides a number of fields of application and
in particular it challenges the integration of a set
of techniques for a common goal.
Acknowledgement
This work is supported under the Advanced
Knowledge Technologies (AKT)
Interdisciplinary Research Collaboration (IRC),
which is sponsored by the UK Engineering and
Physical Sciences Research Council under grant
number GR/N15764/01. The AKT IRC
comprises the Universities of Aberdeen,
Edinburgh, Sheffield, Southampton and the
Open University.
References
Agirre, E. O. Ansa, E. Hovy, and  D.
Mart?nez 2000. Enriching very large ontologies
using the WWW, Proceedings of the ECAI 2000
workshop ?Ontology Learning?.
Basili, R., R. Catizone, M. Stevenson, P.
Velardi, M. Vindigni, and Y. Wilks. 1998. ?An
Empirical Approach to Lexical Tuning?.
Proceedings of the Adapting Lexical and
Corpus Resources to Sublanguages and
Applications Workshop, held jointly with 1st
LREC Granada, Spain.
Bontcheva, K. 2001. Generating adaptive
hypertext explainations with a nested agent
model.  Ph. D. Thesis, University of Sheffield.
Bontcheva, K. and Wilks, Y. 2001. Dealing
with Dependencies between Content Planning
and Surface Realisation in a Pipeline
Generation Architecture. Proceedings of the 17th
International Joint Conference on Artificial
Intelligence (IJCAI2001), Seattle.
Brown, P.F., Peter F., V. J. Della Pietra, P.
V. DeSouza, J. C. Lai, and R. L. Mercer. 1992.
Class-based n-gram models of natural language.
Computational Linguistics, 18, 467-479.
 Califf, M. E. 1998. Relational Learning
Techniques for Natural Language Information
Extraction. Ph.D. thesis, Univ. Texas, Austin,
www/cs/utexas.edu/users/mecaliff
C. Cardie, `Empirical methods in
information extraction', AI Journal,18(4), 65-
79, (1997).
F. Ciravegna, A. Lavelli, N. Mana, J.
Matiasek, L. Gilardoni, S. Mazza, M. Ferraro,
W. J. Black F. Rinaldi, and D. Mowatt.
FACILE: Classifying Texts Integrating Pattern
Matching and Information Extraction. In
Proceedings of the 16th International Joint
Conference On Artificial Intelligence
(IJCAI99), Stockholm, Sweden, 1999.
F. Ciravegna, A. Lavelli,, L. Gilardoni, S.
Mazza, W. J. Black, M. Ferraro, N. Mana, J.
Matiasek, F. Rinaldi. Flexible Text
Classification for Financial
Applications: The FACILE System. In
Proceedings of Prestigious Applications sub-
conference (PAIS2000) sub-conference of the
14th European Conference On Artificial
Intelligence (ECAI2000), Berlin, Germany,
August, 2000.
 Ciravegna, F. 2001. Adaptive Information
Extraction from Text by Rule Induction and
Generalisation. Proceedings of the 17th
International Joint Conference on Artificial
Intelligence (IJCAI2001), Seattle.
Ciravegna, F. and D. Petrelli. 2001. User
Involvement in customizing Adaptive
Information Extraction from Texts: Position
Paper. Proceedings of the IJCAI01 Workshop on
Adaptive Text Extraction and Mining, Seattle.
Cunningham, H., K. Humphreys, R.
Gaizauskas and Y. Wilks. 1997. Software
Infrastructure for Natural Language Processing.
Proceedings of the Fifth Conference on Applied
Natural Language Processing (ANLP-97).
Cunningham H., K. Bontcheva, V. Tablan
and Y. Wilks. 2000. Software Infrastructure for
Language Resources: a Taxonomy of Previous
Work and a Requirements Analysis.
Proceedings of the Second Conference on
Language Resources Evaluation, Athens.
Dom, B. 1999. Automatically finding the
best pages on the World Wide Web (CLEVER).
Search Engines and Beyond: Developing
efficient knowledge management systems.
Boston, MA.
Ericsson, K. A. and H. A. Simon. 1984.
Protocol Analysis: verbal reports as data. MIT
Press, Cambridge, Mass.
Freitag, D. and A. McCallum. 1999
Information Extraction with HMMs and
Shrinkage. AAAI-99 Workshop on Machine
Learning for Information Extraction, Orlando,
FL. (www.isi.edu/~muslea/RISE/ML4IE/)
Freitag, D. and N. Kushmerick. 2000.
Boosted wrapper induction. F. Ciravegna, R.
Basili, R. Gaizauskas, ECAI2000 Workshop on
Machine Learning for Information Extraction,
Berlin, 2000, (www.dcs.shef.ac.uk/~fabio/ecai-
workshop.html)
Hays, P. R. 1997. Collocational Similarity:
Emergent Patterns in Lexical Environments,
PhD. Thesis. School of English, University of
Birmingham
Humphreys, K., R. Gaizauskas, S. Azzam, C.
Huyck, B. Mitchell, H. Cunningham and  Y.
Wilks. 1998. Description of the University of
Sheffield LaSIE-II System as used for MUC-7.
Proceedings of the 7th Message Understanding
Conference.
J?rvelin, K.  and J. Kek?l?inen. 2000. IR
evaluation methods for retrieving highly
relevant documents. Proceedings of the 23rd
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval, , Athens.
Kushmerick, N., D. Weld, and R.
Doorenbos. 1997. Wrapper induction for
information extraction. Proceedings of 15th
International Conference on Artificial
Intelligence, IJCAI-97.
Manchester, P. 1999. Survey ? Knowledge
Management. Financial Times, 28.04.99.
Maybury, M.. 2001. Human Language
Technologies for Knowledge Management:
Challenges and Opportunities. Workshop on
Human Language Technology and Knowledge
Management. Toulouse, France.
McMahon, J. G. and  F. J. Smith. 1996
Improving Statistical Language Models
Performance with Automatically  Generated
Word Hierarchies. Computational Linguistics,
22(2), 217-247, ACL/MIT.
Morin, E. 1999. Using Lexico-Syntactic
patterns to Extract Semantic Relations between
Terms from Technical Corpus, TKE 99,
Innsbruck, Austria.
Muslea, I., S. Minton, and C. Knoblock.
1998. Wrapper induction for semi-structured,
web-based information sources. Proceedings of
the Conference on Autonomous Learning and
Discovery CONALD-98.
Paris, C. , K. Vander Linden. 1996.
DRAFTER: An interactive support tool for
writing multilingual instructions, IEEE
Computer, Special Issue on Interactive NLP.
Reiter, E. 1995. NLG vs. Templates.
Proceedings of the 5th European workshop on
natural language generation, (ENLG-95),
Leiden.
Reiter, E. , C. Mellish and J. Levine. 1995
Automatic generation of technical
documentation. Journal of Applied Artificial
Intelligence,  9(3) 259-287, 1995
Sanderson, M.  and B. Croft. 1999. Deriving
concept hierarchies from text. Proceedings of
the 22nd ACM SIGIR Conference, 206-213.
Scott, M. 1998. Focusing on the Text and Its
Key Words. TALC 98 Proceedings, Oxford,
Humanities Computing Unit, Oxford University.
Soderland, S. 1999. Learning information
extraction rules for semi-structured and free
text. Machine Learning, (1), 1-44.
Yangarber, R., R. Grishman, P. Tapanainen
and S. Huttunen. 2000. Automatic Acquisition
of Domain Knowledge for Information
Extraction. Proceedings of COLING 2000: The
18th International Conference on
Computational Linguistics, Saarbr?cken.
	
				

Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 991?1002,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Harnessing different knowledge sources to measure semantic relatedness 
under a uniform model 
 
Ziqi Zhang Anna Lisa Gentile Fabio Ciravegna 
Department of Computer Science, University of Sheffield 
211 Portobello, Regent Court 
Sheffield, S1 4DP 
z.zhang@dcs.shef.ac.
uk 
a.l.gentile@dcs.shef
.ac.uk 
f.ciravegna@dcs.shef
.ac.uk 
 
Abstract 
Measuring semantic relatedness between 
words or concepts is a crucial process to 
many Natural Language Processing tasks. 
Exiting methods exploit semantic evidence 
from a single knowledge source, and are 
predominantly evaluated only in the 
general domain. This paper introduces a 
method of harnessing different knowledge 
sources under a uniform model for 
measuring semantic relatedness between 
words or concepts. Using Wikipedia and 
WordNet as examples, and evaluated in 
both the general and biomedical domains, it 
successfully combines strengths from both 
knowledge sources and outperforms state-
of-the-art on many datasets. 
1    Introduction 
Semantic relatedness (SR) measures how much 
two (strings of) words or concepts are related by 
encompassing all kinds of relations between them 
(Strube and Ponzetto, 2006). It is more general 
than semantic similarity. SR is often an important 
pre-processing step to many complex Natural 
Language Processing (NLP) tasks, such as Word 
Sense Disambiguation (Leacock and Chodorow, 
1998; Han and Zhao, 2010), and information 
retrieval (Finkelstein et al, 2002). In the 
biomedical domain, SR is an important technique 
for discovering gene functions and interactions 
(Wu et al, 2005; Ye et al, 2005).  
There is an abundant literature on measuring 
SR between words or concepts. Typically, these 
methods extract semantic evidence of words and 
concepts from a background knowledge source, 
with which their relatedness is assessed. The 
knowledge sources can be unstructured documents 
or (semi-)structured resources such as Wikipedia, 
WordNet, and domain specific ontologies (e.g., the 
Gene Ontology1).  
In this paper, we identify two issues that have 
not been addressed in the previous works. First, 
existing works typically employ a single 
knowledge source of semantic evidence. Research 
(Strube and Ponzetto, 2006; Zesch and Gurevych, 
2010; Zhang et al, 2010) has shown that the 
accuracy of an SR method differs depending on the 
choice of the knowledge sources, and there is no 
conclusion which knowledge source is superior to 
others. Zhang et al (2010) argue that this indicates 
different knowledge sources may complement each 
other. Second, the majority of SR methods have 
been evaluated in general domains only, except a 
few earlier WordNet-based methods that have been 
adapted to biomedical ontologies and evaluated in 
that domain (Lord et al, 2003; Pedersen et al, 
2006; Pozo et al, 2008). Given the significant 
attention that SR has received in specific domains 
(Pesquita et al, 2007), evaluation of SR methods 
in specific domains is increasingly important.  
This paper addresses these issues by proposing 
a generic and uniform model for computing SR 
between words or concepts using multiple 
knowledge sources, and evaluating the proposed 
method in both general and specific domains. The 
method combines and integrates semantic evidence 
of words or concepts extracted from any 
knowledge source in a generic graph 
representation, with which the SR between 
concepts or words is computed. Using two of the 
most popular general-domain knowledge sources, 
                                                         
1 http://www.geneontology.org/, last retrieved in Mar. 2011 
991
Wikipedia and WordNet as examples, the method 
is evaluated on 7 benchmarking datasets, including 
three datasets from the biomedical domain and 
four from the general domain. It has achieved 
excellent results: compared to the baselines that 
use each single knowledge sources, combining 
both knowledge sources has improved the accuracy 
on all datasets by 2~11%; compared to state-of-
the-art on the general domain datasets, the method 
achieves the best results on three datasets; and on 
the other three biomedical datasets, it obtains the 
best result in one case; and second and third best 
results on the other two among eight participating 
methods, where all other competitors exploit some 
domain-specific knowledge sources.  
The remainder of this paper is organized as 
follows. Section 2 discusses related work; Section 
3 presents the proposed method; Section 4 
describes the experiments and evaluation; Section 
5 discusses results and findings; Section 6 
concludes this paper. 
2    Related work 
2.1    SR methods 
Methods for computing SR can be classified into 
path based, Information Content (IC) based, 
statistical and hybrid methods. Path based 
methods (Hirst and St-Onge, 1998; Leacock and 
Chodorow, 1998; Pekar and Staab, 2002; Rada et 
al., 1989; Wu and Palmer, 1994) measure SR 
between words or concepts as a function of their 
distance in a semantic network, usually calculated 
based on the path connecting the words or concepts 
by certain semantic (typically is-a) links. IC based 
methods (Jiang and Conrath, 1997; Lin, 1998; 
Pirro et al, 2009; Resnik, 1995; Seco et al, 2004) 
assess relatedness between words or concepts by 
the amount of information they share, usually 
determined by a higher level concept that 
subsumes both concepts in a taxonomic structure. 
Statistical methods measure relatedness between 
words or concepts based on their distribution of 
contextual evidence. This can be formalized as co-
occurrence statistics collected from unstructured 
documents (Chen et al, 2006; Cilibrasi and 
Vitanyi, 2007; Matsuo et al, 2006), or 
distributional concept or word vectors with 
features extracted from either unstructured 
documents (Harrington, 2010; Wojtinnek and 
Pulman, 2011) or (semi-)structured knowledge 
resources (Agirre et al, 2009; Gabrilovich and 
Markovitch, 2007; Gouws et al, 2010; Zesch and 
Gurevych, 2007; Zhang et al, 2010). Hybrid 
methods combine different purebred methods in 
certain ways. For example Riensche et al (2007) 
employ both an IC based method (Resnik, 1995) 
and a statistical method (cosine vector similarity) 
in their study. Pozo et al (2008) derive a taxonomy 
of terms from unstructured documents by applying 
hierarchical clustering based on corpus statistics, 
then apply path based method on this taxonomy to 
compute SR. Han and Zhao (2010) use one IC 
based method and two statistical methods to 
compute SR, then derive an aggregated score.  
2.2    SR knowledge sources and domains 
Computing SR requires background knowledge 
about concepts or words, which can be extracted 
from unstructured corpora, semi-structured and 
structured knowledge resources. Unstructured 
corpora are easier to create and cheaper to 
maintain, however, semantic relations between 
words or concepts are implicit. Methods (Chen et 
al., 2006; Cilibrasi and Vitanyi 2007; Matsuo et al, 
2006) that exploit unstructured corpora typically 
depend on distributional statistics, and thus may 
ignore important semantic evidences present in 
(semi-)structured knowledge sources (Pan and 
Farrell, 2007). Recent studies (Harrington, 2010; 
Pozo et al, 2008; Wojtinnek and Pulman, 2011) 
propose to pre-process a corpus to learn a semantic 
network, with which SR is computed. This creates 
high pre-processing cost; also, the choice of corpus 
and its size often have a direct correlation with the 
accuracy of SR methods (Batet et al, 2010). 
(Semi-)Structured knowledge sources on the 
other hand, organize semantic knowledge about 
concepts and words explicitly and interlink them 
with semantic relations. They have been popular 
choices in the studies of SR, and they include 
lexical resources such as WordNet, Wiktionary, 
and (semi-)structured encyclopedic resources such 
as Wikipedia. WordNet has been used in earlier 
studies (Hirst and St-Onge, 1998; Jiang and 
Conrath, 1997; Lin, 1998; Leacock and Chodorow 
1998; Resnik, 1995; Seco et al, 2004; Wu and 
Palmer, 1994) and is still a preferred knowledge 
source in recent works (Agirre et al, 2009). 
However, its effectiveness may be hindered by its 
lack of coverage of specialized lexicons and 
domain specific concepts (Strube and Ponzetto, 
992
2006; Zhang et al, 2010). Wikipedia and 
Wiktionary are collaboratively maintained know-
ledge sources and therefore may overcome this 
limitation. Wikipedia in particular, is found to have 
reasonable coverage of many domains (Holloway 
et al, 2007; Halavais, 2008). It has become 
increasingly popular in SR studies recently. 
However, research (Zesch and Gurevych, 2010) 
have shown that methods based on Wikipedia have 
no clear advantage over WordNet-based methods 
on some general domain datasets in terms of 
accuracy, while Zhang et al (2010) argue that 
different knowledge sources may complement each 
other, and SR methods may benefit from 
harnessing different knowledge sources.  
Several studies (Lord et al, 2003; Pedersen et 
al., 2006; Petrakis et al, 2006; Pozo et al, 2008) 
have adapted state-of-the-art to domain specific 
knowledge sources (e.g., the Gene Ontology, the 
MeSH2) and evaluated them therein. Despite these 
efforts, a large proportion of state-of-the-art is still 
only evaluated in the general domain.  
2.3    SR methods similar to this work 
Few works have attempted at combining different 
knowledge sources in SR studies, especially (semi-
)structured knowledge sources. The closest studies 
are Han and Zhao (2010) and Tsang and Stevenson 
(2010). Han and Zhao firstly compute SR between 
words using three state-of-the-art SR methods 
separately. Next, one score is chosen subject to an 
arbitrary preference order, and used to create a 
connected graph of weighted edges between 
words. A recursive function is then applied to the 
graph to compute final SR scores between words. 
Essentially, each SR method is applied in isolation 
and features from different sources are used 
separately with each distinctive method. Although 
this retains advantages of each method, the 
limitations of them are also combined.  
Tsang and Stevenson (2010) combine WordNet 
and unstructured documents by weighing each 
word found in WordNet using its frequency 
observed in a large corpus. The frequencies 
however, are sensitive to the choice of corpus, thus 
different corpora may result in different accuracies. 
Furthermore, their method is only applicable to 
computing SR between pairs of sets of words or 
concepts.  
                                                         
2 http://www.nlm.nih.gov/mesh/ last retrieved in March 2011 
3    Methodology  
We define a set of requirements for SR methods 
that harness different knowledge sources: 
? It should improve over the same method 
based on a single knowledge source 
? It should be generic and applicable to any 
knowledge source 
? It should be robust in dealing with 
knowledge source specific features but 
also tolerate the quality and coverage 
issues of individual knowledge source 
Our method of harnessing different knowledge 
sources contains four steps. Firstly (Section 3.1), 
each word or word segment is searched in each 
knowledge source to identify their contexts that is 
specific to that knowledge source. We define a 
context as the representation of meaning or a 
concept for a word. In the following, we say that 
each context is associated with a distinct concept. 
Secondly (Section 3.2), for each concept of an 
input word, features are extracted from its context 
and a graph representation of each concept and 
their features is created. Thirdly (Section 3.3), 
cross-source contexts are mapped where they refer 
to the same concept, thus their features from 
different sources can be combined to derive an 
enriched representation. This creates a final, 
uniform graph representation where input words 
are connected by shared features of their 
underlying candidate concepts. Then (Section 3.4) 
the graph is submitted to a generic algorithm to 
compute SR between words. 
In the following, we discuss details with respect 
to different types of knowledge sources, while 
focusing on Wikipedia and WordNet in our 
experiments for two reasons. First, they are used 
by the majority of SR methods and are therefore 
most representative knowledge sources. Second, 
they have strongly distinctive and complementary 
characteristics, which make ideal testbeds for the 
requirements. On one hand, WordNet is a lexical 
resource containing rich and strict semantic 
relations between words, but lacks coverage of 
specialized vocabularies. On the other hand, 
Wikipedia is a semi-structured resource with good 
coverage of domains and named entities, but the 
semantic knowledge is organized in a looser way. 
993
3.1    Context retrieval 
Given a pair of words or word segments, we firstly 
identify contexts representing the underlying 
meanings or concepts from each knowledge 
source. For lexical resources, this could be 
distinctive word senses. In WordNet (WN), a 
context corresponds to a single synset, which 
corresponds to a concept. We search each word in 
WordNet and extract all possible synsets. Let w be 
a word or word segment (e.g., ?cat?), and   
   
    
      
      
    be the set of k concepts of w 
extracted from WordNet.  
Using Wikipedia (WK) as an example semi-
structured resource, the context can be an article 
that describes a unique concept. Thus we search 
for underlying articles that describe different 
concepts. Firstly, we search w in Wikipedia, where 
three situations may be anticipated. If a single non-
disambiguation page describing a concept is 
returned, the concept is selected and the retrieval is 
complete. In the second case, a disambiguation 
page linking to all possible concept pages may be 
returned. This page lists all underlying concepts 
and entities referenced by w as links and a short 
description with each link. In this case, we always 
keep the first concept page, which is found often to 
be the most common sense of the word; 
additionally, we select other concept pages whose 
short descriptions contain the word w. We do not 
select all linked pages because many of these in 
fact link to a concept relevant to w, but not 
necessarily a candidate sense of w. Thirdly, if no 
pages are returned for w, we search for the most 
relevant page using w as keyword(s) in an inverted 
index of all Wikipedia pages (e.g., via search 
engines). We denote concepts retrieved from 
Wikipedia as   
       
      
      
   .  
For unstructured sources such as documents, a 
simple approach could be defining a word context 
as a text passage around each occurrence of w, and 
grouping similar contexts of w as representation of 
its underlying meanings, or concepts. Alternatively, 
more complex approaches such as Pozo et al 
(2008) and Harrington (2010) may be applied to 
extract a lexical network of words, whereby similar 
methods to WordNet can be applied. 
3.2 Feature extraction and representation 
Next, for each concept identified from a 
knowledge source, features are extracted from their 
corresponding contexts. In our case, for each 
    
  , we follow the work by Zhang et al 
(2010) to extract four types of features from their 
corresponding Wikipedia pages. Figure 1 shows an 
example representation of a concept and its 
Wikipedia features: 
? Words from page titles and redirection 
links (can be considered as synonyms) 
? Words from categories, used as higher 
level hypernyms in some studies (Zesch et 
al., 2010; Strube and Ponzetto, 2006) 
? Words from outgoing links 
? Top n most frequent words from a page 
 
Figure 1. Representation of the concept ?cat, the 
mammal? using different types of features 
extracted from Wikipedia. The shaded circle 
represents the concept; ovals represent feature 
values; edges connecting feature values to the 
concept and <labels> represent feature types 
 
For each     
  , we extract ten features from 
WordNet: hypernyms, hyponyms, meronyms, 
holonyms, synonyms, antonyms, attributes, ?see 
also? words, ?related? words, and gloss. These are 
also represented in the same way as in Figure 1.  
With unstructured sources, contextual words 
can be used as features. Alternatively, if a lexical 
network is extracted, features may be extracted in a 
similar way to those of WordNet. 
 
Additionally, with WordNet and Wikipedia, we 
also propose several intra-resource feature merging 
strategies to study the effect of feature 
diversification. This is because, while some 
approaches (such as Agirre et al, 2009; 
Harrington, 2010; Yeh et al, 2009) do not 
distinguish different feature types in graph 
construction, or adopt a bag-of-words feature 
representation (such as Zesch and Gurevych, 
2010), others (such as Yazdani and Popescu-Belis, 
2010; Zhang et al, 2010) have used differentiated 
994
feature types and weights in their model. We 
therefore carry out studies to investigate this issue. 
Specifically, for the original four Wikipedia 
features, we create a bag-of-words feature that 
simply merges all feature types (i.e., all edges in 
Figure 1 will have the same label). For the original 
ten WordNet features, we propose two merged 
representations corresponding to that of Wikipedia, 
so as to support the studies of feature enrichment 
in the following section. We introduce a bag-of-
words feature that collapses all different feature 
types, and a four-feature representation as follow: 
? wn-synant merges WordNet synonyms and 
antonyms.  
? wn-hypoer merges WordNet hypernyms 
and hyponyms, collectively representing 
features by ?is-a? semantic relation 
? wn-assc merges WordNet meronyms, 
holonyms, related and ?see also?, which 
are features corresponding to associative 
relations  
? wn-dist merges WordNet gloss and 
attributes that generally describe a concept.  
3.3 Concept mapping and feature enrichment 
Our method essentially harnesses different 
knowledge sources by combining features 
extracted from different sources in a uniform 
model. This requires two sub-processes: cross-
source concept mapping and cross-source 
feature enrichment.  
In cross-source concept mapping, concepts 
extracted from different knowledge sources are 
mapped according to similar meanings such that 
cross-source features can be combined. To do so, 
we select the concepts from one knowledge source 
as the reference concept set; then concepts from 
other knowledge sources are mapped to reference 
concepts of similar meanings. There can be 
different criteria of choosing reference knowledge 
source concepts. Empirically, we found it 
necessary to choose the knowledge source with 
broader coverage and richer features. This will be 
discussed later in Section 5. Following this 
strategy, in our example,   
   is chosen as 
reference concepts, and for each   
     
  we 
select a   
     
   such that   
   and   
   refer to 
the same meaning. To do so, we apply a simple 
maximum set overlap metric to their feature 
values. Let F(c) be a function that returns all 
feature values of c as bag-of-words, then for each 
  
     
  , it is mapped to a   
   such that 
     
          
     is maximized among all 
  
     
  . The resulting concept candidates are 
denoted as   
    
, where   
    
=    
     
    is a 
mapped set of concepts potentially referring to the 
same meaning. If   
     then   
    
 
  
   
        
  . 
Next, cross-source feature enrichment creates 
a uniform feature representation for each mapped 
sets of concepts. The process can be considered as 
enriching the features from one knowledge source 
with others. The most straightforward approach is 
to simply collect features extracted from each 
knowledge source on to a single graph, retaining 
the diversity in feature types. For example, Figure 
2 shows a graph representation based on the 
collection of the four Wikipedia features and the 
four derived WordNet features. We refer to this 
approach as ?feature combination?.  
 
Figure 2. Representation of ?cat, the mammal? 
after concept mapping and feature combination 
 
On the other hand, cross-source features may be 
merged according to their semantics.  For example, 
WordNet and Wikipedia contain features based on 
synonyms of concepts; while Wikipedia and 
unstructured documents contain word distribution-
al features. Thus we define ?feature integration? 
as merging feature types from different knowledge 
sources into single types of features based on their 
similarity in semantics.  With WordNet and Wiki-
pedia, we integrate features as below (Figure 3): 
? merged-synant merges Wikipedia page 
titles and redirection links with wn-synant 
? merged-hypoer merges merges Wikipedia 
categories with wn-hypoer 
995
? merged-assc merges Wikipedia links with 
wn-assc. We consider Wikipedia links bear 
other associative relations and are 
therefore merged with features extracted 
by other WordNet relations 
? merged-dist merges Wikipedia frequent n 
words with wn-dist.  
 
Figure 3. Representation of ?cat, the mammal? 
after concept mapping and feature integration 
 
Note that the difference between cross-source 
feature combination and integration is that the 
former introduces more types of features, whereas 
the latter retains same number of feature types but 
increases feature values for each type. Both have 
the effect of establishing additional path (via 
features) between concepts, but in different ways. 
 
With intra-resource feature diversification, cross-
source feature combination and feature 
integration, we create a total of nine intra- and 
cross-source feature representations to be tested 
with the uniform random walk model: 
? four types of Wikipedia features (wk-4F) 
? one type of Wikipedia features (wk-1F) 
? ten types of WordNet features (wn-10F) 
? four types of WordNet features (wn-4F) 
? one type of WordNet features (wn-1F) 
? wk-4F combines wn-4F: wk-4F+wn4F,C 
? wk-4F integrates wn-4F: wk-4F+wn4F,I 
? wk-1F combines wn-1F: wk-1F+wn1F,C 
? wk-1F integrates wn-1F: wk-1F+wn1F,I 
3.4 Computing SR using the graph 
The algorithm for computing SR using the graph is 
based on the idea of random walk. It formalizes the 
idea that taking successive steps along the paths in 
a graph, the ?easier? it is to arrive at a target node 
starting from a source node, the more related the 
two nodes are. Following the previous steps, the 
feature representations of all candidate concepts 
relevant to the input word pairs are joined, which 
creates a single undirected, weighted, bi-partite 
graph. Let G = (V, E) be the graph, where V is the 
set of nodes (concepts and feature values); E is the 
set of edges (feature types) that connect concepts 
and features. As shown in Figure 4, different 
concepts are connected if they share same values 
of same types of features, namely, there exists a 
path that connects one concept to another.  
 
Figure 4. Paths are established between different 
concepts if they share values of same feature types 
<bold underlined> 
Using Figure 4 it is easier to comprehend the 
difference between feature combination and 
integration. Since concept nodes can only be 
connected by same types of edges (feature types), 
feature combination increases the chances of 
connectivity by adding in more types of edges, 
while integration merges similar types of edges 
across knowledge sources and increases the 
number of feature nodes connected by each type.  
From the graph, we start by building an 
adjacency matrix W of initial probability 
distribution: 
??
??
?
??
??
? ?????? ? ?
otherwise
EjililEi
lw
W Ll k
k
ij
k
,0
),(,|),(:),(|
)( [1] 
Where Wij is the i
th-line and jth-column entry of W, 
indexed by V; l(i, j) is a function that returns the 
type of edge (i.e., type of feature) connecting 
nodes i and j; L is the set of all possible types; w(l) 
returns the weight for that type. Essentially, L is 
the collection of all feature types, and w(l) assigns 
996
a weight to a particular feature type. Next, we 
compute the transition probability matrix P(t)(j|i) = 
[(D?1W)t]ij (Dii = ?kWik), which returns the 
probability of reaching other nodes from a starting 
node on the graph after t steps. In this method, we 
follow the work by Rowe and Ciravegna (2010) to 
set t=2 in order to preserve locally connected 
nodes. Next, we extract the probability vectors 
corresponding to concept nodes from P, and 
compute pair-wise relatedness using the cosine 
function. Effectively, this formalizes the notion 
that two concepts related to a third concept is also 
semantically related, which is similar to the 
hypothesis proposed by Patwardhan and Pedersen 
(2006) in their method based on second-order 
context vectors. The final SR between the input 
word pair is the maximum pair-wise concept SR. 
4    Experiment and evaluation 
We evaluate the method based on correlation 
against human judgment (gold standard) on seven 
benchmarking datasets covering both general and 
technical domains. These include four general 
domain datasets: the Rubenstein and Goodenough 
(1965) dataset containing 65 pairs of nouns 
(RG65); the Miller and Charles (1991) dataset that 
is a subset of the RG-65 dataset and contains 30 
pairs (MC30); the Finkelstein et al (2002) dataset 
with 353 pairs of words, including nouns, verbs, 
adjectives, as well as named entities. This contains 
two subsets, a set of 153 pairs (Fin153) and a set of 
200 (Fin200) pairs each annotated by a different 
groups of annotators. Zesch and Gurevych (2010) 
show largely varying Inter-Annotator-Agreement 
(IAA) between the two sets (Table 1), and argue 
that they should be treated as separate datasets. 
Three biomedical datasets are selected to evaluate 
domain-specific performance of the proposed 
method. These include a set of 36 MeSH term pairs 
in Petrakis et al (2006) (MeSH36), 30 pairs of 
medical terms annotated by a group of physicians 
as in Pedersen et al (2006) (Ped30-p) and the same 
set annotated by a different group of medical 
coders (Ped30-c). Table 1 shows statistics of the 
seven datasets.  
The correlation is computed using the 
Spearman rank order coefficient for two reasons. 
First, it is a better metric than other alternatives 
(Zesch and Gurevych, 2010). Second, it is 
consistent with the majority of studies such that 
results can be compared.  
 
Dataset Size Domain IAA 
MC30 30 General 0.9 
RG65 65 General 0.8 
Fin153 153 General 0.73 
Fin200 200 General 0.55 
Ped30-p 30 Biomedical 0.68 
Ped30-c 30 Biomedical 0.78 
MeSH36 36 Biomedical - 
Table 1: Information of benchmarking datasets 
 
We distribute feature weights w(l) across 
different feature types L evenly in each feature 
representation. Although Zhang et al (2010) show 
that discriminated feature weights leads to 
improved accuracy; this is not the focus of this 
study. Since we aim to investigate the effects of 
harnessing different knowledge sources, we 
obtained baseline performances by applying the 
method to those feature representations based on 
single knowledge sources (i.e., wk-4F, wk-1F, wn-
10F, wn-4F, wn-1F). Tables 2 and 3 show the best 
results obtained with baselines and corresponding 
knowledge sources and feature representation.  
 
Dataset Corr. Feature Coverage (% pairs) 
MC30 0.77 wn-1F 77% 
RG65 0.71 wn-1F 65% 
Fin153 0.45 wn-4F 82% 
Fin200 0.35 wn-4F 76% 
Ped30-p 0.66 wn-4F 33% 
Ped30-c 0.8 wn-4F 33% 
MeSH36 0.49 wn-1F 50% 
Table 2: Correlation obtained using WordNet.  
Many word pairs are not covered due to sparse 
feature space and lack of coverage. Only covered 
pairs are accounted. 
 
Dataset Corr. Feature 
MC30 0.74 wk-1F 
RG65 0.67 wk-1F 
Fin153 0.7 wk-1F 
Fin200 0.51 wk-4F 
Ped30-p 0.53 wk-4F 
Ped30-c 0.58 wk-4F 
MeSH36 0.73 wk-4F 
Table 3: Correlation obtained using only 
Wikipedia. All word pairs are 100% covered. 
 
997
Tables 4 ? 6 show results obtained with 
enriched feature representation. 
 
 Combination (C) Integration (I) 
Dataset wn-4F + 
wk-4F 
wn-1F + 
wk-1F 
wn-4F + 
wk-4F 
wn-1F 
+ wk-1F 
MC30 0.77 0.8 0.8 0.79 
RG65 0.74 0.73 0.73 0.729 
Fin153 0.73 0.75 0.74 0.73 
Fin200 0.52 0.54 0.53 0.54 
Ped30-p 0.63 0.52 0.64 0.47 
Ped30-c 0.64 0.52 0.67 0.49 
MeSH36 0.7 0.694 0.75 0.7 
Table 4: Correlation obtained using both 
knowledge sources. Word pairs are 100% covered. 
 
 KS and # of feature types 
 WN WK WK+WN,C WK+WN, I  
MC30 1 1 1 4 
RG65 1 1 4 4 
Fin153 4 1 1 4 
Fin200 4 4 1 1 
Ped30-p 4 4 4 4 
Ped30-c 4 4 4 4 
MeSH36 1 4 4 4 
Table 5: Number of feature types with which best 
results are obtained on each dataset. KS: 
Knowledge Source 
 
 Single KS Multiple KS Impr. 
Dataset Best corr. Best corr. Strategy  
MC30 0.74 0.8 C/I 0.06 
RG65 0.67 0.74 C 0.07 
Fin153 0.7 0.75 C 0.05 
Fin200 0.51 0.54 C/I 0.03 
Ped30-p 0.53 0.64 I 0.11 
Ped30-c 0.58 0.67 I 0.09 
MeSH36 0.73 0.75 I 0.02 
Table 6: Improvement achieved by harnessing 
multiple KSs. Best correlation with single KS is 
based on Wikipedia, which provides 100% 
coverage of word pairs. 
 
 
Tables 7 and 8 compare our method against state-
of-the-art. For Table 8, figures for other state-of-
the-art systems can be found in corresponding 
publications; while we only list the best 
performing systems for comparison. 
 
 
 
 
 
 MC30 RG65 Fin153 Fin200 KS 
best of 
WN+WK  
0.8 0.74 0.75 0.54 Both 
Rad89* 0.75 0.79 0.33 0.24 WN 
LC98* 0.75 0.79 0.33 0.24 WN 
WP94* 0.77 0.78 0.38 0.24 WN 
HS98* 0.76 0.79 0.33 0.32 WN 
Res95* 0.72 0.74 0.35 0.26 WN 
JC97* 0.68 0.58 0.28 0.10 WN 
Lin98* 0.67 0.60 0.27 0.17 WN 
Zes07* 0.77 0.82 0.6 0.51 WK 
GM07* 0.67 0.75 0.69 0.51 WK 
Zha10 0.71 0.76 0.71 0.46 WK 
Table 73: Comparison against state-of-the-art in the 
general domain. (* figures from Zesch and 
Gurevych, 2010) 
 
 Ped30-p Ped30-c MeSH36 KS 
best of 
WN+WK 
0.64 0.67 0.75 WN+
WK 
Pet06 best - - 0.74 MeSH 
Ped06 best 0.84 0.75 - GO, D 
Ped06 second 0.62 0.68 - GO, D 
Table 84: Comparison against state-of-the-art in the 
biomedical domain. GO ? Gene Ontology; D ? 
document sets.  
 
Given the fact that some datasets (i.e., MC30, 
Ped30-p, Ped30-c, MeSH36) have a relatively low 
sample size, we cannot always be sure that 
correlation values are accurate or occurred by 
chance. Therefore, we measure the statistical 
significance of correlation by computing the p-
value for the correlation values reported for our 
system in Tables 7 and 8. For all cases, a p-value 
of less than 0.001 is obtained, which indicates that 
correlation values are statistically significant. 
                                                         
3 Rada (1989) (Rad89); Leacock and Chodorow (1998) 
(LC98); Wu and Palmer (1994) (WP04); Hirst and St-Onge 
(1998) (HS98); Resnik (1995) (Res95); Jiang and Conrath 
(1997) (JC97); Lin (1998) (Lin98); Zesch and Gurevych 
(2007) (ZG07); Gabrilovich and Markovitch (2007) (GM07); 
Zhang et al (2010) (Zha10) 
4 Petrakis et al (2006) (Pet06); Pedersen et al (2006) (Ped06). 
Original participating systems can be found in these works. 
998
5    Discussion  
Single v.s. multiple knowledge sources As shown 
in Table 6, considering the best performances 
across all feature enrichment strategies and feature 
sets, the proposed method successfully harnessed 
different knowledge sources and improved over the 
baselines using single knowledge sources by 0.02 
~ 0.11. The biggest improvement (0.11) is on a 
domain-specific dataset, on which the method 
based on single knowledge source performed 
poorly in terms of coverage and accuracy. The best 
enrichment strategy that has consistently improved 
the baselines is wk-4F+wn-4F, Integration (Table 
4 v.s. Table 3).  With features enriched from 
multiple knowledge sources, the method also 
consistently improved over their corresponding 
single-source features on all datasets, except 
MeSH36, on which wk-4F+wn-4F, Combination 
(Table 4) slightly reduced the accuracy obtained 
with wk-4F (Table 3) only.  
The large proportion of uncovered word pairs 
using WordNet is due to its lack of coverage of 
specialized lexicons, and sparser semantic content. 
For example, of all 115 distinctive terms in the 
Ped30 and MeSH36 datasets, 30% are not included 
in WordNet. And of all 447 distinctive words in all 
general domain datasets, only 69% have multiple 
synonyms. Features such as attributes and ?see 
also? are present for less than 20 words. This is the 
reason that some approaches using WordNet (e.g., 
Agirre et al, 2009) require a graph of all WordNet 
lexicons to be built, thus intermediate words may 
?bridge? input words even if they do not connect 
directly by their features. Nevertheless, the 
improvement in accuracy and 100% coverage after 
harnessing both knowledge sources suggests that 
they complement each other well. On one hand, 
Wikipedia brings its strength in domain and 
content coverage; on the other hand, WordNet 
brings useful semantic evidences for words that are 
covered. 
Concept mapping and feature enrichment 
methods While the set overlap based method for 
cross-source concept mapping using the reference 
knowledge source concepts is simple and proved 
successful, the accuracy of mapping and its 
correlation with the accuracy of the SR method 
was not studied. This will be explored in the future. 
Also, alternative mapping methods will be 
investigated. For example, Toral and Mu?oz (2006) 
describe a different method of mapping Wikipedia 
articles to WordNet synsets; one could also adopt a 
simple disambiguation process to select the best 
candidate concept from each knowledge source 
suited for the input word pairs, whereby cross-
source concept mapping becomes straightforward. 
In terms of feature enrichment strategies, there is 
no strong indication (Table 6) of which (feature 
combination v.s. integration) is more effective, 
although the system consistently outperforms the 
baselines (Table 4 v.s. Table 3) with the wk-
4F+wn-4F, Integration strategy. 
Feature diversification v.s. unification Table 
5 suggests that in most cases, differentiating 
feature types leads to better results than merging 
them uniformly, despite the knowledge sources 
used. This is consistent with the findings by Zhang 
et al (2010). This can be understandable since 
although unifying feature types effectively 
increases possibility of sharing features, equally, 
this may also increase the proportion of noisy 
features. For example, consider the Wikipedia 
article of ?Horse? (animal), which has a category 
label ?livestock?; and the article ?Famine?, which 
has an outgoing link ?livestock? (in a sentence 
describing diseases that caused decline of livestock 
production). By differentiating the feature types 
?has_category? and ?has_outlink?, the two 
concepts will not be connected even if they both 
have the same word ?livestock? in their feature 
representation. However, using a bag-of-words 
representation where feature types are 
undistinguished, the strength of their relatedness is 
boosted by sharing this word, which may be 
uninteresting in this occasion. 
Compared against state-of-the-art, the 
proposed method has achieved promising results. 
Overall, by harnessing different knowledge sources, 
the method achieves, and in many cases, 
outperforms state-of-the-art. In the general domain, 
it outperforms state-of-the-art on three out of four 
datasets. It is worth noting that all methods based 
on WordNet generally have poor performance on 
the Fin153 and Fin200 datasets (Table 7). Despite 
the heterogeneity in these datasets, this may also 
relate to the quality of the feature space generated 
with WordNet. In fact methods using Wikipedia 
perform better on these datasets. With enriched 
features from both knowledge sources, the 
accuracies are further improved.   
999
In the biomedical domain, the proposed method 
outperforms state-of-the-art on one dataset and 
produces competitive results on others. Note that 
all other methods exploit domain-specific 
ontologies and corpora. The Ped06 best and Ped06 
second methods also depend on a corpus of one 
million documents. These results further confirmed 
the benefits of our method: harnessing knowledge 
from general-purpose knowledge sources of 
limited domain coverage, it is possible to achieve 
results that rival methods based on well-curated 
and specially tailored domain-specific knowledge 
sources. This is an encouraging finding. Although 
there are abundant resources in the biomedical 
domain for this type of tasks, such resources may 
be scarce in other domains and are expensive to 
build. However, the results suggest that the 
proposed method offers a more affordable 
approach that provides reasonable coverage and 
quality, even if individual general knowledge 
sources may be limited in themselves. 
Generality of the method. The proposed 
method represents features extracted from different 
knowledge sources in a generic manner, which 
facilitates cross-source feature enrichment and 
requires generic algorithm computation. As 
discussed in Section 3, semantic evidence of words 
and concepts may be extracted from different 
knowledge sources in different ways, while 
harnessed in the generic model. In contrast, other 
methods using multiple knowledge sources (e.g., 
Han and Zhao, 2010; Tsang and Stevenson, 2010) 
introduce algorithms that are bound to the 
knowledge sources, which may limit their 
adaptability and portability. 
6    Conclusion  
This paper introduced a generic method of 
harnessing different knowledge sources to compute 
semantic relatedness. We have shown empirically 
that different knowledge sources contain 
complementary semantic evidence, which, when 
combined together under a uniform model, can 
improve the accuracy of SR methods. Moreover, 
we have demonstrated its robustness in dealing 
with knowledge sources of different quality and 
coverage. Several remaining issues will be studied 
in the future. First, additional knowledge sources 
will be studied, particularly unstructured corpora 
and domain-specific resources. The experiments 
have shown that although harnessing different 
knowledge sources achieved encouraging results 
on biomedical datasets, they are still far from being 
perfect. While it should be appreciated that the 
results are obtained using only general purpose 
knowledge sources, it would be interesting to 
investigate whether harnessing domain specific 
knowledge sources (where available) further 
improves the performance. Second, different 
methods of concept mapping will be studied. We 
will also design methods for assessing the quality 
of mapping, and analyze their correlations with the 
SR methods. Third, analyses will be carried out to 
uncover the differences between feature 
combination and integration that have led to 
different accuracies. 
Acknowledgments 
Part of this research has been funded under the EC 
7th Framework Program, in the context of the 
SmartProducts project (231204). 
References  
Agirre, E., Alfonseca, E., Hall, K., Kravalova, J., Pasca, 
M., Soroa, A. 2009. A Study on Similarity and 
Relatedness Using Distributional and WordNet-
based Approaches. In Proceedings of NAACL?09 
Batet, M., S?nchez, D., Valls, A. 2010. An ontology-
based measure to compute semantic similarity in 
biomedicine. In Journal of Biomedical Informatics, 
44(1), 118-125 
Chen, H., Lin, M., Wei, Y. 2006. Novel association 
measures using web search with double checking. 
Proceedings of COLING?06-ACL?06, pp. 1009-
1016 
Cilibrasi, R., Vitanyi, P. 2007. The Google Similarity 
Distance. In IEEE Transactions on Knowledge and 
Data Engineering. 19(3), 370-383 
Finkelstein, L., Gabrilovich, E., Matias, Y., Rivlin, E., 
Solan, Z., Wolfman, G., and Ruppin, E. (2002). 
Placing search in context: the concept revisited. In 
ACM Transactions on Information Systems, 20 (1), 
pp. 116 ? 131 
Gabrilovich, E., Markovitch, S. 2007. Computing 
semantic relatedness using Wikipedia-based explicit 
semantic analysis. In proceeding of IJCAI'07 
Gouws, S., Rooyen, G., Engelbrecht, H. 2010. 
Measuring conceptual similarity by spreading 
activation over Wikipedia?s hyperlink structure. 
Proceedings of the 2nd Workshop on The People?s 
Web Meets NLP: Collaboratively Constructed 
Semantic Resources 
1000
Halavais , A. 2008. An Analysis of Topical Coverage of 
Wikipedia. Journal of Computer-Mediated 
Communication, 13(2) 
Han, X., Zhao, J. 2010. Structural semantic relatedness: 
a knowledge-based method to named entity 
disambiguation. In the 48th Annual Meeting of the 
Association for Computational Linguistics. 
Harrington, B. 2010. A semantic network approach to 
measuring relatedness. In Proceedings of COLING? 
10 
Hirst, G., and St-Onge, D. 1998. Lexical chains as 
representation of context for the detection and 
correction malapropisms. In Christiane Fellbaum 
(ed.), WordNet: An Electronic Lexical Database and 
Some of Its Applications, pp. 305?332. Cambridge, 
MA: The MIT Press. 
Holloway, T., Bozicevic, M., B?rner, K. 2007. 
Analyzing and visualizing the semantic coverage of 
Wikipedia and its authors. In Journal of Complexity, 
Special issue on Understanding Complex Systems, 
12(3), 30-40 
Jiang, J. and D. Conrath. 1997. Semantic similarity 
based on corpus statistics and lexical taxonomy. 
Proceedings of the International Conference on 
Research in Computational Linguistics, pp. 19-33 
Leacock, C., Chodorow, M. 1998. Combining local 
context and WordNet similarity for word sense 
identification. In C. Fellbaum (Ed.), WordNet. An 
Electronic Lexical Database, Chp. 11, pp. 265-283. 
Lin, D. 1998. An information-theoretic definition of 
similarity. Proceedings of the Fifteenth International 
Conference on Machine Learning, pp. 296-304 
Lord, P., Stevens, R., Brass, A., Goble, C. 2003. 
Investigating semantic similarity measures across 
the Gene Ontology: the relationship between 
sequence and annotation. In Bioinformatics, 19(10), 
pp. 1275?1283 
Matsuo, Y., T. Sakaki., K., Uchiyama, M., Ishizuka. 
2006. Graph-based word clustering using a web 
search engine. In Proceedings of the Conference on 
Empirical Methods in Natural Language Processing 
(EMNLP), pp.542-550 
Miller, G., Charles, W. 1991. Contextual correlates of 
semantic similarity. In Language and Cognitive 
Processes, 6(1): 1-28 
Pan, F., Farrell, R. 2007. Computing semantic similarity 
between skill statements for approximate matching. 
In Proceedings of NAACL-HLT?07, pp. 572-579 
Patwardhan, S., Pedersen, T. 2006. Using WordNet-
based context vectors to estimate the semantic 
relatedness of concepts. Proceedings of the EACL 
2006 Workshop on Making Sense of Sense: 
Bringing Computational Linguistics and 
Psycholinguistics Together 
Pedersen, T., Pakhomov, S., Patwardhan, S., Chute, C. 
2006. Measures of semantic similarity and 
relatedness in the biomedical domain. Journal of 
Biomedical Informatics 40(3), 288-299 
Pekar, V., Staab, S. 2002. Taxonomy learning: factoring 
the structure of a taxonomy into a semantic 
classification decision. Proceedings of COLING?02. 
pp. 786-792 
Pesquita, C., Faria, D., Bastos, H., Falc?o, A., Couto, F. 
(2007). Evaluating GO-based Semantic Similarity 
Measures. ISMB/ECCB 2007 SIG Meeting Program 
Materials, International Society for Computational 
Biology 2007 
Petrakis, E., Varelas, G., Hliaoutakis, A., Raftopoulou, 
P. 2006. Design and evaluation of semantic 
similarity measures for concepts stemming from the 
same or different ontologies. In 4th Workshop on 
Multimedia Semantics (WMS'06), pp. 44-52. 
Pirro, G. 2009. A semantic similarity metric combining 
features and intrinsic information content. In Data 
and Knowledge Engineering, 68(11), pp. 1289-1308 
Pozo A., Pazos F., Valencia, A. 2008. Defining 
functional distances over gene ontology. In BMC 
Bioinformatics 9, pp.50 
Rada, R., Mili, H., Bicknell, E., Blettner, M. 1989. 
Development and application of a metric on 
semantic nets. In IEEE Transactions on Systems, 
Man and Cybernetics 19(1), pp.17-30 
Resnik, P. (1995). Using information content to evaluate 
semantic similarity in a taxonomy. In Proceedings of 
IJCAI-95, pp. 448-453 
Riensche, R., Baddeley, B., Sanfilippo, A., Posse, C., 
Gopalan, B. 2007. XOA: Web-Enabled Cross-
Ontological Analytics. IEEE Congress on Services, 
pp. 99-105 
Rowe, M., Ciravegna, F. 2010. Disambiguating identity 
web references using Web 2.0 data and semantics. 
M Rowe and F Ciravegna. The Journal of Web 
Semantics. 
Rubenstein, H., Goodenough, J. 1965. Contextual 
correlates of synonymy. In Communications of the 
ACM, 8(10):627-633 
Seco, N., and Hayes, T. 2004. An intrinsic information 
content metric for semantic similarity in WordNet. 
In Proceedings of the 16th European conference on 
Artificial Intelligence 
Strube, M., Ponzetto, S. 2006. WikiRelate! Computing 
semantic relatedness using Wikipedia. In 
Proceedings of the 21st national conference on 
Artificial intelligence (AAAI) 
Toral, A., Mu?oz, R. 2006. A Proposal to Automatically 
Build and Maintain Gazetteers for Named Entity 
Recognition by using Wikipedia. In Proceedings of 
Workshop on New Text, ACL?06. 
Tsang, V., Stevenson, S. 2010. A graph-theoretic 
framework for semantic distance. In Journal of 
Computational Linguistics, 36(1). 
1001
Wojtinnek, P., Pulman, S. 2011. Semantic relatedness 
from automatically generated semantic networks. In 
Proceedings of the Ninth International Conference 
on Computational Semantics (IWCS?11) 
Wu, Z. Palmer, M. 1994. Verbs semantics and lexical 
selection. Proceedings of the 32nd annual meeting 
on Association for Computational Linguistics, pp. 
133-138 
Wu, H., Su, Z., Mao, F., Olman, V., Xu, Y. 2005. 
Prediction of functional modules based on 
comparative genome analysis and gene ontology 
application. Nucleic Acids Research, 33, pp. 2822?
2837.  
Yazdani, M., Popescu-Belis, A. 2010. A random walk 
framework to compute textual semantic similarity: a 
unified model for three benchmark tasks. IEEE 
Fourth International Conference on Semantic 
Computing (ICSC), pp. 424-429 
Ye, P., Peyser, B., Pan, X., Boek, J., Spencer, F., Bader, 
J. 2005. Gene function prediction from congruent 
synthetic lethal interactions in yeast. In Molecular 
system biology 
Yeh, E., Ramage, D., Manning, C., Agirre, E., Soroa, A. 
2009. WikiWalk: random walks on Wikipedia for 
semantic relatedness. In Proceedings of the 
TextGraphs-4, Workshop on Graph-based Methods 
for Natural Language Processing, ACL2009 
Zesch, T., and Gurevych, I. 2007. Analysis of the 
Wikipedia category graph for NLP applications. In 
Proceedings of the TextGraphs-2 Workshop 
(NAACL-HLT 2007), pp. 1?8 
Zesch, T., Gurevych, I. 2010. Wisdom of crowds versus 
wisdom of linguists: measuring the semantic 
relatedness of words. In Journal of Natural 
Language Engineering, 16, pp. 25-59 
Zhang, Z., Gentile, A., Xia, L., Iria, J., Chapman, S. 
2010. A random graph walk based approach to 
compute semantic relatedness using knowledge from 
Wikipedia. In Proceedings of LREC?10. 
 
1002
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 289?293,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Mining Equivalent Relations from Linked Data 
 
 
Ziqi Zhang1 Anna Lisa Gentile1 Isabelle Augenstein1 
Eva Blomqvist2 Fabio Ciravegna1 
1 Department of Computer Science, 
University of Sheffield, UK 
2 Department of Computer and Information 
Science, Link?ping University, Sweden 
{z.zhang, a.l.gentile, i.augenstein, 
f.ciravegna}@dcs.shef.ac.uk, eva.blomqvist@liu.se
 
 
Abstract 
Linking heterogeneous resources is a major re-
search challenge in the Semantic Web. This 
paper studies the task of mining equivalent re-
lations from Linked Data, which was insuffi-
ciently addressed before. We introduce an un-
supervised method to measure equivalency of 
relation pairs and cluster equivalent relations. 
Early experiments have shown encouraging 
results with an average of 0.75~0.87 precision 
in predicting relation pair equivalency and 
0.78~0.98 precision in relation clustering. 
1 Introduction 
Linked Data defines best practices for exposing, 
sharing, and connecting data on the Semantic 
Web using uniform means such as URIs and 
RDF. It constitutes the conjunction between the 
Web and the Semantic Web, balancing the rich-
ness of semantics offered by Semantic Web with 
the easiness of data publishing. For the last few 
years Linked Open Data has grown to a gigantic 
knowledge base, which, as of 2013, comprised 
31 billion triples in 295 datasets1.  
A major research question concerning Linked 
Data is linking heterogeneous resources, the fact 
that publishers may describe analogous infor-
mation using different vocabulary, or may assign 
different identifiers to the same referents. Among 
such work, many study mappings between ontol-
ogy concepts and data instances (e.g., Isaac et al 
2007; Mi et al, 2009; Le et al, 2010; Duan et al, 
2012). An insufficiently addressed problem is 
linking heterogeneous relations, which is also 
widely found in data and can cause problems in 
information retrieval (Fu et al, 2012). Existing 
work in linking relations typically employ string 
similarity metrics or semantic similarity mea-
                                                 
1 http://lod-cloud.net/state/ 
sures that require a-priori domain knowledge and 
are limited in different ways (Zhong et al, 2002; 
Volz et al, 2009; Han et al, 2011; Zhao and 
Ichise, 2011; Zhao and Ichise, 2012).  
This paper introduces a novel method to dis-
cover equivalent groups of relations for Linked 
Data concepts. It consists of two components: 1) 
a measure of equivalency between pairs of rela-
tions of a concept and 2) a clustering process to 
group equivalent relations. The method is unsu-
pervised; completely data-driven requiring no a-
priori domain knowledge; and also language in-
dependent. Two types of experiments have been 
carried out using two major Linked Data sets: 1) 
evaluating the precision of predicting equivalen-
cy of relation pairs and 2) evaluating the preci-
sion of clustering equivalent relations. Prelimi-
nary results have shown encouraging results as 
the method achieves between 0.75~0.85 preci-
sion in the first set of experiments while 
0.78~0.98 in the latter. 
2 Related Work  
Research on linking heterogeneous ontological 
resources mostly addresses mapping classes (or 
concepts) and instances (Isaac et al 2007; Mi et 
al., 2009; Le et al, 2010; Duan et al, 2012; 
Schopman et al, 2012), typically based on the 
notions of similarity. This is often evaluated by 
string similarity (e.g. string edit distance), se-
mantic similarity (Budanitsky and Hirst, 2006), 
and distributional similarity based on the overlap 
in data usage (Duan et al, 2012; Schopman et 
al., 2012). There have been insufficient studies 
on mapping relations (or properties) across on-
tologies. Typical methods make use of a combi-
nation of string similarity and semantic similarity 
metrics (Zhong et al, 2002; Volz et al, 2009; 
Han et al, 2011; Zhao and Ichise, 2012). While 
string similarity fails to identify equivalent rela-
tions if their lexicalizations are distinct, semantic 
similarity often depends on taxonomic structures 
289
in existing ontologies (Budanitsky and Hirst, 
2006). Unfortunately many Linked Data instanc-
es use relations that are invented arbitrarily or 
originate in rudimentary ontologies (Parundekar 
et al, 2012). Distributional similarity has also 
been used to discover equivalent or similar rela-
tions. Mauge et al (2012) extract product proper-
ties from an e-commerce website and align 
equivalent properties using a supervised maxi-
mum entropy classification method. We study 
linking relations on Linked Data and propose an 
unsupervised method. Fu et al (2012) identify 
similar relations using the overlap of the subjects 
of two relations and the overlap of their objects. 
On the contrary, we aim at identifying strictly 
equivalent relations rather than similarity in gen-
eral. Additionally, the techniques introduced our 
work is also related to work on aligning multilin-
gual Wikipedia resources (Adar et al, 2009; 
Bouma et al, 2009) and semantic relatedness 
(Budanitsky and Hirst, 2006). 
3 Method 
Let t denote a 3-tuple (triple) consisting of a sub-
ject (ts), predicate (tp) and object (to). Linked Da-
ta resources are typed and its type is called class. 
We write type (ts) = c meaning that ts is of class c. 
p denotes a relation and rp is a set of triples 
whose tp=p, i.e., rp={t | tp = p}. 
Given a specific class c, and its pairs of rela-
tions (p, p?) such that rp={t|tp=p, type(ts)=c} and 
rp?={t|tp=p?, type (ts)=c}, we measure the equiv-
alency of p and p? and then cluster equivalent 
relations. The equivalency is calculated locally 
(within same class c) rather than globally (across 
all classes) because two relations can have iden-
tical meaning in specific class context but not 
necessarily so in general. For example, for the 
class Book, the relations dbpp:title and foaf:name 
are used with the same meaning, however for 
Actor, dbpp:title is used interchangeably with 
awards dbpp:awards (e.g., Oscar best actor). 
In practice, given a class c, our method starts 
with retrieving all t from a Linked Data set 
where type(ts)=c, using the universal query lan-
guage SPARQL with any SPARQL data end-
point. This data is then used to measure equiva-
lency for each pair of relations (Section 3.1). The 
equivalence scores are then used to group rela-
tions in equivalent clusters (Section 3.2). 
3.1 Measure of equivalence 
The equivalence for each distinct pair of rela-
tions depends on three components. 
Triple overlap evaluates the degree of over-
lap2 in terms of the usage of relations in triples. 
Let SO(p) be the collection of subject-object 
pairs from rp and SOint the intersection 
)r(SO)r(SO)'p,p(SO 'ppint ??
           [1] 
then the triple overlap TO(p, p?) is calculated as 
}|r|
|)r,r(SO|,|r|
|)r,r(SO|{MAX
'p
'ppint
p
'ppint
        [2] 
Intuitively, if two relations p and p? have a 
large overlap of subject-object pairs in their data 
instances, they are likely to have identical mean-
ing. The MAX function allows addressing infre-
quently used, but still equivalent relations (i.e., 
where the overlap covers most triples of an in-
frequently used relation but only a very small 
proportion of a much more frequently used).  
Subject agreement While triple overlap looks 
at the data in general, subject agreement looks at 
the overlap of subjects of two relations, and the 
degree to which these subjects have overlapping 
objects. Let S(p) return the set of subjects of rela-
tion p, and O(p|s) returns the set of objects of 
relation p whose subjects are s, i.e.: 
}st,pt|t{)s|r(O)s|p(O spop ????
        [3] 
we define: 
)r(S)r(S)'p,p(S 'ppint ??
         [4] 
|)'p,p(S|
otherwise,
|)s|'p(O)s|p(O|if,
int
)'p,p(Ss int
?
?
??
??
0
01
         [5] 
|)'p(S)p(S|/|)'p,p(S| int ???
        [6] 
then the agreement AG(p, p?) is  
????)'p,p(AG            [7] 
Equation [5] counts the number of overlapping 
subjects whose objects have at least one overlap. 
The higher the value of ?, the more the two rela-
tions ?agree? in terms of their shared subjects. 
For each shared subject of p and p? we count 1 if 
they have at least 1 overlapping object and 0 oth-
erwise. This is because both p and p? can be 
1:many relations and a low overlap value could 
mean that one is densely populated while the 
other is not, which does not necessarily mean 
they do not ?agree?. Equation [6] evaluates the 
degree to which two relations share the same set 
of subjects. The agreement AG(p, p?) balances 
the two factors by taking the product. As a result, 
                                                 
2 In this paper overlap is based on ?exact? match. 
290
relations that have high level of agreement will 
have more subjects in common, and higher pro-
portion of shared subjects with shared objects. 
Cardinality ratio is a ratio between cardinali-
ty of the two relations. Cardinality of a relation 
CD(p) is calculated based on data: 
|)r(S|
|r|)p(CD
p
p?
         [8] 
and the cardinality ratio is calculated as 
)}'p(CD),p(CD{MAX
)}'p(CD),p(CD{IN)'p,p(CDR ?
       [9] 
The final equivalency measure integrates all 
the three components to return a value in [0, 2]: 
)'p,p(CDR
)'p,p(AG)'p,p(TO)'p,p(E ??
              [10] 
The measure will favor two relations that have 
similar cardinality.  
3.2 Clustering 
We apply the measure to every pair of relations 
of a concept, and keep those with a non-zero 
equivalence score. The goal of clustering is to 
create groups of equivalent relations based on the 
pair-wise equivalence scores. We use a simple 
rule-based agglomerative clustering algorithm 
for this purpose. First, we rank all relation pairs 
by their equivalence score, then we keep a pair if 
(i) its score and (ii) the number of triples covered 
by each relation are above a certain threshold, 
TminEqvl and TminTP respectively. Each pair forms 
an initial cluster. To merge clusters, given an 
existing cluster c and a new pair (p, p?) where 
either p?c or p??c, the pair is added to c if E(p, 
p?) is close (as a fractional number above the 
threshold TminEqvlRel) to the average scores of all 
connected pairs in c. This preserves the strong 
connectivity in a cluster. This is repeated until no 
merge action is taken. Adjusting these thresholds 
allows balancing between precision and recall. 
4 Experiment Design 
To our knowledge, there is no publically availa-
ble gold standard for relation equivalency using 
Linked Data. We randomly selected 21 concepts 
(Figure 1) from the DBpedia ontology (v3.8): 
Actor, Aircraft, Airline, Airport, Automobile, 
Band, BasketballPlayer, Book, Bridge, Comedian, 
Film, Hospital, Magazine, Museum, Restaurant, 
Scientist, TelevisionShow, TennisPlayer, Theatre, 
University, Writer 
Figure 1. Concepts selected for evaluation. 
We apply our method to each concept to dis-
cover clusters of equivalent relations, using as 
SPARQL endpoint both DBpedia3 and Sindice4 
and report results separately. This is to study 
how the method performs in different conditions: 
on one hand on a smaller and cleaner dataset 
(DBpedia); on the other hand on a larger and 
multi-lingual dataset (Sindice) to also test cross-
lingual capability of our method. We chose rela-
tively low thresholds, i.e. TminEqvl=0.1, TminTP= 
0.01% and TminEqvlRel=0.6, in order to ensure high 
recall without sacrificing much precision.  
Four human annotators manually annotated 
the output for each concept. For this preliminary 
evaluation, we have limited the amount of anno-
tations to a maximum of 100 top scoring pairs of 
relations per concept, resulting in 16~100 pairs 
per concept (avg. 40) for DBpedia experiment 
and 29~100 pairs for Sindice (avg. 91). The an-
notators were asked to rate each edge in each 
cluster with -1 (wrong), 1 (correct) or 0 (cannot 
decide). Pairs with 0 are ignored in the evalua-
tion (about 12% for DBpedia; and 17% for Sin-
dice mainly due to unreadable encoded URLs for 
certain languages). To evaluate cross-lingual 
pairs, we asked annotators to use translation 
tools. Inter-Annotator-Agreement (observed 
IAA) is shown in Table 1. Also using this data, 
we derived a gold standard for clustering based 
on edge connectivity and we evaluate (i) the pre-
cision of top n% (p@n%) ranked equivalent rela-
tion pairs and (ii) the precision of clustering for 
each concept.  
 Mean High Low 
DBpedia 0.79 0.89 0.72 
Sindice 0.75 0.82 0.63 
Table 1. IAA on annotating pair equivalency 
So far the output of 13 concepts has been an-
notated. This dataset 5  contains ?1800 relation 
pairs and is larger than the one by Fu et al 
(2012). Annotation process shows that over 75% 
of relation pairs in the Sindice experiment con-
tain non-English relations and mostly are cross-
lingual. We used this data to report performance, 
although the method has been applied to all the 
21 concepts, and the complete results can be vis-
ualized at our demo website link. Some examples 
are shown in Figure 2.  
                                                 
3 http://dbpedia.org/sparql 
4 http://sparql.sindice.com/ 
5 http://staffwww.dcs.shef.ac.uk/people/Z.Zhang/ re-
sources/paper/acl2013short/web/ 
291
 Figure 2. Examples of visualized clusters 
5 Result and Discussion 
Figure 3 shows p@n% for pair equivalency6 and 
Figure 4 shows clustering precision.  
 
Figure 3. p@n%. The box plots show the ranges of 
precision at each n%; the lines show the average. 
 
Figure 4. Clustering precision  
As it is shown in Figure 2, Linked Data rela-
tions are often heterogeneous. Therefore, finding 
equivalent relations to improve coverage is im-
portant. Results in Figure 3 show that in most 
cases the method identifies equivalent relations 
with high precision. It is effective for both sin-
gle- and cross-language relation pairs. The worst 
performing case for DBpedia is Aircraft (for all 
n%), mostly due to duplicating numeric valued 
objects of different relations (e.g., weight, length, 
capacity). The decreasing precision with respect 
to n% suggests the measure effectively ranks 
correct pairs to the top. This is a useful feature 
from IR point of view. Figure 4 shows that the 
method effectively clusters equivalent relations 
with very high precision: 0.8~0.98 in most cases. 
                                                 
6 Per-concept results are available on our website. 
Overall we believe the results of this early proof-
of-concept are encouraging. As a concrete exam-
ple to compare against Fu et al (2012), for Bas-
ketballPlayer, our method creates separate clus-
ters for relations meaning ?draft team? and ?for-
mer team? because although they are ?similar? 
they are not ?equivalent?. 
We noticed that annotating equivalent rela-
tions is a non-trivial task. Sometimes relations 
and their corresponding schemata (if any) are 
poorly documented and it is impossible to under-
stand the meaning of relations (e.g., due to acro-
nyms) and even very difficult to reason based on 
data. Analyses of the evaluation output show that 
errors are typically found between highly similar 
relations, or whose object values are numeric 
types. In both cases, there is a very high proba-
bility of having a high overlap of subject-object 
pairs between relations. For example, for Air-
craft, the relations dbpp:heightIn and dbpp: 
weight are predicted to be equivalent because 
many instances have the same numeric value for 
the properties. Another example are the Airport 
properties dbpp:runwaySurface, dbpp:r1Surface, 
dbpp:r2Surface etc., which according to the data 
seem to describe the construction material (e.g., 
concrete, asphalt) of airport runways. The rela-
tions are semantically highly similar and the ob-
ject values have a high overlap. A potential solu-
tion to such issues is incorporating ontological 
knowledge if available. For example, if an ontol-
ogy defines the two distinct properties of Airport 
without explicitly defining an ?equivalence? re-
lation between them, they are unlikely to be 
equivalent even if the data suggests the opposite.  
6 Conclusion 
This paper introduced a data-driven, unsuper-
vised and domain and language independent 
method to learn equivalent relations for Linked 
Data concepts. Preliminary experiments show 
encouraging results as it effectively discovers 
equivalent relations in both single- and multi-
lingual settings. In future, we will revise the 
equivalence measure and also experiment with 
clustering algorithms such as (Beeferman et al, 
2000). We will also study the contribution of 
individual components of the measure in such 
task. Large scale comparative evaluations (incl. 
recall) are planned and this work will be extend-
ed to address other tasks such as ontology map-
ping and ontology pattern mining (Nuzzolese et 
al., 2011).  
 
292
Acknowledgement 
Part of this research has been sponsored by the 
EPSRC funded project LODIE: Linked Open 
Data for Information Extraction, EP/J019488/1. 
Additionally, we also thank the reviewers for 
their valuable comments given for this work. 
 
References  
Eytan Adar, Michael Skinner, Daniel Weld. 
2009. Information Arbitrage across Multi-
lingual Wikipedia. Proceedings of the Second 
ACM International Conference on Web 
Search and Data Mining, pp. 94 ? 103. 
Gosse Bouma, Sergio Duarte, Zahurul Islam. 
2009. Cross-lingual Alignment and Comple-
tion of Wikipedia Templates. Proceedings of 
the Third International Workshop on Cross 
Lingual Information Access: Addressing the 
Information Need of Multilingual Societies, 
pp. 61 ? 69   
Doug Beeferman, Adam Berger. 2000. Agglom-
erative clustering of a search engine query log. 
Proceedings of the sixth ACM SIGKDD inter-
national conference on Knowledge discovery 
and data mining, pp. 407-416. 
Alexander Budanitsky and Graeme Hirst. 2006. 
Evaluating WordNet-based Measures of Se-
mantic Distance. Computational Linguistics, 
32(1), pp.13-47. 
Songyun Duan, Achille Fokoue, Oktie Has-
sanzadeh, Anastasios Kementsietsidis, Kavitha 
Srinivas, and Michael J. Ward. 2012. In-
stance-Based Matching of Large Ontologies 
Using Locality-Sensitive Hashing. ISWC 
2012, pp. 46 ? 64 
Linyun Fu, Haofen Wang, Wei Jin, Yong Yu. 
2012. Towards better understanding and uti-
lizing relations in DBpedia. Web Intelligence 
and Agent Systems , Volume 10 (3) 
Andrea Nuzzolese, Aldo Gangemi, Valentina 
Presutti, Paolo Ciancarini. 2011. Encyclopedic 
Knowledge Patterns from Wikipedia Links. 
Proceedings of the 10th International Semantic 
Web Conference, pp. 520-536 
Lushan Han, Tim Finin and Anupam Joshi. 2011. 
GoRelations: An Intuitive Query System for 
DBpedia. Proceedings of the Joint Internation-
al Semantic Technology Conference 
Antoine Isaac, Lourens van der Meij, Stefan 
Schlobach, Shenghui Wang. 2007. An empiri-
cal study of instance-based ontology match-
ing. Proceedings of the 6th International Se-
mantic Web Conference and the 2nd Asian 
conference on Asian Semantic Web Confer-
ence, pp. 253-266 
Ngoc-Thanh Le, Ryutaro Ichise, Hoai-Bac Le. 
2010. Detecting hidden relations in geograph-
ic data. Proceedings of the 4th International 
Conference on Advances in Semantic Pro-
cessing, pp. 61 ? 68 
Karin Mauge, Khash Rohanimanesh, Jean-David 
Ruvini. 2012. Structuring E-Commerce Inven-
tory. Proceedings of ACL2012, pp. 805-814 
Jinhua Mi, Huajun Chen, Bin Lu, Tong Yu, 
Gang Pan. 2009. Deriving similarity graphs 
from open linked data on semantic web. Pro-
ceedings of the 10th IEEE International Con-
ference on Information Reuse and Integration, 
pp. 157?162. 
Rahul Parundekar, Craig Knoblock,  Jos? Luis. 
Ambite. 2012. Discovering Concept Cover-
ings in Ontologies of Linked Data Sources. 
Proceedings of ISWC2012, pp. 427?443. 
Balthasar Schopman, Shenghui Wang, Antoine 
Isaac, Stefan Schlobach. 2012. Instance-Based 
Ontology Matching by Instance Enrichment. 
Journal on Data Semantics, 1(4), pp 219-236 
Julius Volz, Christian Bizer, Martin Gaedke, 
Georgi Kobilarov. 2009. Silk ? A Link Discov-
ery Framework for the Web of Data. Proceed-
ings of the 2nd Workshop on Linked Data on 
the Web 
Lihua Zhao, Ryutaro Ichise. 2011. Mid-ontology 
learning from linked data. Proceedings of the 
Joint International Semantic Technology Con-
ference, pp. 112 ? 127. 
Lihua Zhao, Ryutaro Ichise. 2012. Graph-based 
ontology analysis in the linked open data. Pro-
ceedings of the 8th International Conference 
on Semantic Systems, pp. 56 ? 63 
Jiwei Zhong, Haiping Zhu, Jianming Li and 
Yong Yu. 2002. Conceptual Graph Matching 
for Semantic Search. The 2002 International 
Conference on Computational Science. 
293
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 37?42,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
Real-Time Detection, Tracking, and Monitoring of Automatically
Discovered Events in Social Media
Miles Osborne
?
Edinburgh
Sean Moran
Edinburgh
Richard McCreadie
Glasgow
Alexander Von Lunen
Loughborough
Martin Sykora
Loughborough
Elizabeth Cano
Aston
Neil Ireson
Sheffield
Craig Macdonald
Glasgow
Iadh Ounis
Glasgow
Yulan He
Aston
Tom Jackson
Loughborough
Fabio Ciravegna
Sheffield
Ann O?Brien
Loughborough
Abstract
We introduce ReDites, a system for real-
time event detection, tracking, monitoring
and visualisation. It is designed to as-
sist Information Analysts in understand-
ing and exploring complex events as they
unfold in the world. Events are automat-
ically detected from the Twitter stream.
Then those that are categorised as be-
ing security-relevant are tracked, geolo-
cated, summarised and visualised for the
end-user. Furthermore, the system tracks
changes in emotions over events, sig-
nalling possible flashpoints or abatement.
We demonstrate the capabilities of ReD-
ites using an extended use case from the
September 2013 Westgate shooting inci-
dent. Through an evaluation of system la-
tencies, we also show that enriched events
are made available for users to explore
within seconds of that event occurring.
1 Introduction and Challenges
Social Media (and especially Twitter) has become
an extremely important source of real-time infor-
mation about a massive number of topics, ranging
from the mundane (what I had for breakfast) to the
profound (the assassination of Osama Bin Laden).
?
Corresponding author: miles@inf.ed.ac.uk
Detecting events of interest, interpreting and mon-
itoring them has clear economic, security and hu-
manitarian importance.
The use of social media message streams for
event detection poses a number of opportunities
and challenges as these streams are: very high in
volume, often contain duplicated, incomplete, im-
precise and incorrect information, are written in
informal style (i.e. short, unedited and conver-
sational), generally concern the short-term zeit-
geist; and finally relate to unbounded domains.
These characteristics mean that while massive and
timely information sources are available, domain-
relevant information may be mentioned very infre-
quently. The scientific challenge is therefore the
detection of the signal within that noise. This chal-
lenge is exacerbated by the typical requirement
that documents must be processed in (near) real-
time, such that events can be promptly acted upon.
The ReDites system meets these requirements
and performs event detection, tracking, summari-
sation, categorisation and visualisation. To the
best of our understanding, it is the first published,
large-scale, (near) real-time Topic Detection and
Tracking system that is tailored to the needs of in-
formation analysts in the security sector. Novel as-
pects of ReDites include the first large-scale treat-
ment of spuriously discovered events and tailoring
the event stream to the security domain.
37
Figure 1: System Diagram
2 Related Work
A variety of event exploration systems have previ-
ously been proposed within the literature. For in-
stance, Trend Miner
1
enables the plotting of term
times series, drawn from Social Media (Preot?iuc-
Pietro and Cohn, 2013). It has a summarisation
component and is also multilingual. In contrast,
our system is focussed instead upon documents
(Tweets) and is more strongly driven by real-
time considerations. The Social Sensor (Aiello et
al., 2013) system facilitates the tracking of pre-
defined events using social streams.
In contrast, we track all automatically discov-
ered events we find in the stream. The Twitci-
dent (Abel et al., 2012) project deals with user-
driven searching through Social Media with re-
spect to crisis management. However, unlike
ReDites, these crises are not automatically dis-
covered. The LRA Crisis Tracker
2
has a similar
purpose as ReDites. However, while LRA uses
crowdsourcing, our ReDites system is fully auto-
matic.
3 System Overview and Architecture
Figure 1 gives a high-level system description.
The system itself is loosely coupled, with ser-
vices from different development teams coordi-
nating via a Thrift interface. An important as-
pect of this decoupled design is that it enables ge-
ographically dispersed teams to coordinate with
each other. Event processing is comprised of the
following main 4 steps:
1) New events are detected. An event is described
by the first Tweet that we find discussing it and
is defined as something that is captured within a
single Tweet (Petrovic et al., 2010).
1
http://www.trendminer-project.eu/
2
http://www.lracrisistracker.com/
2) When an event is first discovered it may initially
have little information associated with it. Further-
more, events evolve over time. Hence, the sec-
ond step involves tracking the event ? finding new
posts relating to it as they appear and maintaining
a concise updated summary of them.
3) Not all events are of interest to our intended
audience, so we organise them. In particular, we
determine whether an event is security-related (or
otherwise), geolocate it, and detect how prominent
emotions relating to that event evolve.
4) Finally, we visualise the produced stream of
summarised, categorised and geolocated events
for the analyst(s), enabling them to better make
sense of the mass of raw information present
within the original Twitter stream.
Section 6 further describes these four steps.
4 Data and Statistics
For the deployment of ReDites, we use the Twit-
ter 1% streaming sample. This provides approx-
imately four million Tweets per day, at a rate of
about 50 Tweets a second. Table 1 gives some
illustrative statistics on a sample of data from
September 2013 to give a feel for the rate of data
and generated events we produce. Table 2 gives
timing information, corresponding with the major
components of our system: time to process and
time to transfer to the next component, which is
usually a service on another machine on the in-
ternet. The latency of each step is measured in
seconds over a 1000 event sample. ?Transfer? la-
tencies is the time between one step completing
and the output arriving at the next step to be pro-
cessed (Thrift transfer time). Variance is the aver-
age deviation from the mean latency over the event
sample.
When processing the live stream, we ingest data
at an average rate of 50 Tweets per second and de-
tect an event (having geolocated and filtered out
non-English or spam Tweets) with a per-Tweet la-
tency of 0.6?0.55 seconds. Figure 2 gives laten-
cies for the various major components of the sys-
tem. All processing uses commodity machines.
5 The Westgate Shopping Mall Attack
As an illustrative example of a complex recent
event, we considered a terrorist attack on the 21st
of September, 2013.
3
This event is used to demon-
strate how our system can be used to understand it.
3
https://en.wikipedia.org/wiki/Westgate shopping mall shooting
38
Measure Event Detection Tracking and Summ Emotion Ident Security Class
Detection Transfer Ranking Summ Transfer Ident Transfer Class
Latency (sec.) 0.6226 0.7929 2.2892 0.0409 0.0519 0.2881 0.1032 0.1765
Variance (sec.) 0.5518 0.2987 1.3079 0.0114 0.0264 0.1593 0.0195 0.0610
Table 2: Event exploration timing and timing variance (seconds)
Data Rate
Tweets 35 Million
Detected events 533k
Categorised (security-related) events 5795
Table 1: Data statistics, 1st September - 30th
September 2013
In summary, a shopping Mall in Kenya was at-
tacked from the 21st of September until the 24th
of September. This event was covered by tradi-
tional newswire, by victims caught up in it as well
as by terrorist sympathisers, often in real-time.
As we later show, even though we only operate
over 1% of the Twitter Stream, we are still able to
find many (sub) events connected with this attack.
There were 6657 mentions of Westgate in Septem-
ber 2013 in our 1% of sample Tweets.
6 Major Components
6.1 Event Detection
Building upon an established Topic Detection and
Tracking (TDT) methodology, which assumes that
each new event corresponds with seeing some
novel document. the event detection component
uses a hashing approach that finds novel events
4
in constant time (Petrovic et al., 2010). To make
it scale and process thousands of documents each
second, it can optionally be distributed over a clus-
ter of machines (via Storm
5
) (McCreadie et al.,
2013). The system favours recall over precision
and has been shown to have high recall, but a low
precision (Petrovic et al., 2013). Given that we are
presenting discovered events to a user and we do
not want to overload them with false positives, we
need to take steps to increase precision (ie present
fewer false positives).
We use a content categoriser to determine
whether a discovered event is worth reporting.
Using more than 100k automatically discovered
events from the Summer of 2011, we created a
training set and manually labelled each Tweet:
4
An event is defined as something happening at a given
time and place. Operationally, this means something that can
be described within a Tweet.
5
http://storm.incubator.apache.org/
was it content bearing (what you might want to
read about in traditional newswire) or irrelevant
/ not useful. With this labelled data, we used
a Passive-Aggressive algorithm to build a con-
tent classifier. Features were simply unigrams in
Tweets. This dramatically improves precision, to
70%, with a drop in recall to 25% (when tested
on 73k unseen events, manually labelled by two
annotators). We can change the precision-recall
boundary as needed by adjusting the associated
decision boundary. We do not consider non-
English language Tweets in this work and they are
filtered out (Lui and Baldwin, 2012).
Geolocation is important, as we are particu-
larly interested in events that occur at a spe-
cific location. We therefore additionally geolo-
cate any Tweets that were not originally ge-
olocated. To geotag those Tweets that do not
have any geo-location information we use the
Tweet text and additional Tweet metadata (lan-
guage, city/state/country name, user description
etc), to learn a L
1
penalised least squares regres-
sor (LASSO) to predict the latitude and longitude.
The model is learnt on nearly 20 million geo-
located Tweets collected from 2010-2014. Exper-
iments on a held-out test dataset show we can lo-
calise Tweets to within a mean distance of 433 km
of their true location. This performance is based
on the prediction of individual tweet location and
not, as in most previous work, on the location of a
user who is represented by a set of tweets. Further-
more we are not restricted to a single, well-defined
area (such as London) and we also evaluate over a
very large set of unfiltered tweets.
Turning to the Westgate example, the first men-
tion of it in our data was at 10:02 UTC. There were
57 mentions of Westgate in discovered events,
of which 42 mentioned Kenya and 44 mentioned
Nairobi. The first mention itself in Twitter was at
09:38 UTC. We declared it an event (having seen
enough evidence and post-processing it) less than
one second later:
Westgate under siege. Armed thugs. Gun-
shots reported. Called the managers, phones are
off/busy. Are cops on the way?
We also captured numerous informative sub-
39
events covering different aspects and sides of the
central Westgate siege event, four of these are il-
lustrated below:
Post Time Tweet
10:05am RT @ItsMainaKageni: My friend Ruhila Adatia
passed away together with her unborn child. Please
keep her family and new husband in your thou
10:13am RT howden africa: Kenya police firing tear gas and
warning shots at Kenyan onlookers. Crowd getting
angry #westgate
10:10am RT @BreakingNews: Live video: Local news cov-
erage of explosions, gunfire as smoke billows from
Nairobi, Kenya, mall - @KTNKenya
10:10am ?Purportedly official Twitter account for al-Shabaab
Tweeting on the Kenyan massacre HSM Press
(http://t.co/XnCz9BulGj)
6.2 Tracking and Summarisation
The second component of the event exploration
system is Tracking and Summarisation (TaS). The
aim of this component is to use the underlying
Tweet stream to produce an overview for each
event produced by the event detection stage, up-
dating this overview as the event evolves. Track-
ing events is important when dealing with live, on-
going disasters, since new information can rapidly
emerge over time.
TaS takes as input a Tweet representing an event
and emits a list of Tweets summarising that event
in more detail. TaS is comprised of two dis-
tinct sub-components, namely: real-time tracking;
and event summarisation. The real-time track-
ing component maintains a sliding window of
Tweets from the underlying Tweet stream. As
an event arrives, the most informative terms con-
tained
6
form a search query that is used to retrieve
new Tweets about the event. For example, tak-
ing the Tweet about the Westgate terrorist attack
used in the previous section as input on September
21st 2013 at 10:15am, the real-time tracking sub-
component retrieved the following related Tweets
from the Twitter Spritzer (1%) steam
7
(only 5/100
are shown):
ID Post Time Tweet Score
1 10:05am Westgate under siege. Armed thugs. Gun-
shots reported. Called the managers, phones are
off/busy. Are cops on the way?
123.7
2 10:13am DO NOT go to Westgate Mall. Gunshots and
mayhem, keep away until further notice.
22.9
3 10:13am RT DO NOT go to Westgate Mall. Gunshots
and mayhem, keep away until further notice.
22.9
4 10:10am Good people please avoid Westgate Mall. @Po-
liceKE @IGkimaiyo please act ASAP, reports
of armed attack at #WestgateMall
22.2
5 10:07am RT @steve enzo: @kirimiachieng these thugs
won?t let us be
11.5
6
Nouns, adjectives, verbs and cardinal numbers
7
https://dev.twitter.com/docs/streaming-
apis/streams/public
The second TaS sub-component is event sum-
marisation. This sub-component takes as input the
Tweet ranking produced above and performs ex-
tractive summarisation (Nenkova and McKeown,
2012) upon it, i.e. it selects a subset of the ranked
Tweets to form a summary of the event. The goals
of event summarisation are two-fold. First, to re-
move any Tweets from the above ranking that are
not relevant to the event (e.g. Tweet 5 in the exam-
ple above). Indeed when an event is first detected,
there may be few relevant Tweets yet posted. The
second goal is to remove redundancy from within
the selected Tweets, such as Tweets 2 and 3 in the
above example, thereby focussing the produced
summary on novelty. To tackle the first of these
goals, we leverage the score distribution of Tweets
within the ranking to identify those Tweets that are
likely background noise. When an event is first
detected, few relevant Tweets will be retrieved,
hence the mean score over the Tweets is indicative
of non-relevant Tweets. Tweets within the rank-
ing whose scores diverge from the mean score in
the positive direction are likely to be on-topic. We
therefore, make an include/exclude decision for
each Tweet t in the ranking R:
include(t, R) =
?
?
?
?
?
?
?
?
?
?
?
1 if score(t)? SD(R) > 0
and |SD(R)? score(t)| >
? ?
1
|R|
?
t
?
?R
|SD(R)? score(t
?
)|
0 otherwise
(1)
where SD(R) is the standard deviation of scores
inR, score(t) is the retrieval score for Tweet t and
? is a threshold parameter that describes the mag-
nitude of the divergence from the mean score that
a Tweet must have before it is included within the
summary. Then, to tackle the issue of redundancy,
we select Tweets in a greedy time-ordered man-
ner (earliest first). A similarity (cosine) threshold
between the current Tweet and each Tweet previ-
ously selected is used to remove those that are tex-
tually similar, resulting in the following extractive
summary:
ID Post Time Tweet Score
1 10:05am Westgate under siege. Armed thugs.
Gunshots reported. Called the man-
agers, phones are off/busy. Are cops
on the way?
123.7
2 10:13am DO NOT go to Westgate Mall. Gun-
shots and mayhem, keep away until
further notice.
22.9
4 10:10am Good people please avoid Westgate
Mall. @PoliceKE @IGkimaiyo please
act ASAP, reports of armed attack at
#WestgateMall
22.2
Finally, the TaS component can be used to track
40
events over time. In this case, instead of tak-
ing a new event as input from the event detec-
tion component, a previously summarised event
can be used as a surrogate. For instance, a user
might identify an event that they want to track.
The real-time search sub-component retrieves new
Tweets about the event posted since that event was
last summarised. The event summarisation sub-
component then removes non-relevant and redun-
dant Tweets with respect to those contained within
the previous summary, producing a new updated
summary.
6.3 Organising Discovered Events
The events we discover are not targeted at infor-
mation analysts. For example, they contain sports
updates, business acquisitions as well as those that
are genuinely relevant and can bear various opin-
ions and degrees of emotional expression. We
therefore take steps to filter and organise them for
our intended audience: we predict whether they
have a specific security-focus and finally predict
an emotional label for events (which can be useful
when judging changing viewpoints on events and
highlighting extreme emotions that could possibly
motivate further incidents).
6.3.1 Security-Related Event Detection
We are particularly interested in security-related
events such as violent events, natural disasters, or
emergency situations. Given a lack of in-domain
labelled data, we resort to a weakly supervised
Bayesian modelling approach based on the previ-
ously proposed Violence Detection Model (VDM)
(Cano et al., 2013) for identifying security events.
In order to differentiate between security and
non-security related events, we extract words re-
lating to security events from existing knowledge
sources such as DBpedia and incorporate them as
priors into the VDM model learning. It should be
noted that such a word lexicon only provides ini-
tial prior knowledge into the model. The model
will automatically discover new words describing
security-related events.
We trained the VDM model on a randomly
sampled 10,581 Tweets from the TREC Mi-
croblog 2011 corpus (McCreadie et al., 2012)
and tested the model on 1,759 manually labelled
Tweets which consist of roughly the same num-
ber of security-related and non-security related
Tweets. Our results show that the VDM model
achieved 85.8% in F-measure for the identification
of security-related Tweets, which is not far from
the F-measure of 90% obtained using the super-
vised Naive Bayes classifier despite using no la-
belled data in the model.
Here, we derived word priors from a total
of 32,174 documents from DBpedia and ex-
tracted 1,612 security-related words and 1,345
non-security-related words based on the measure-
ment of relative word entropy. We then trained
the VDM model by setting the topic number to
50 and using 7,613 event Tweets extracted from
the Tweets collected during July-August 2011 and
September 2013 in addition to 10,581 Tweets from
the TREC Microblog 2011 corpus. In the afore-
mentioned Westgate example, we classify 24% of
Tweets as security-related out of a total of 7,772
summary Tweets extracted by the TaS component.
Some of the security-related Tweets are listed be-
low
8
:
ID Post Time Tweet
1 9:46am Like Bin Laden kind of explosion?
?@The realBIGmeat:
There is an explosion at westgate!?
2 10:08am RT @SmritiVidyarthi: DO NOT go to Westgate
Mall. Gunshots and mayhem, keep away till further no-
tice.
3 10:10am RT @juliegichuru: Good people please avoid
Westgate. @PoliceKE @IGkimaiyo please act
ASAP, reports of armed attack at #WestgateMall.
4 10:13am there has bn shooting @ Westgate which is suspected
to b of gangs.......there is tension rt nw....
6.3.2 Emotion
Security-related events can be fraught, with emo-
tionally charged posts possibly evolving over time,
reflecting ongoing changes in underlying events.
Eight basic emotions, as identified in the psychol-
ogy literature (see (Sykora et al., 2013a) for a de-
tailed review of this literature) are covered, specif-
ically; anger, confusion, disgust, fear, happiness,
sadness, shame and surprise. Extreme values ?as
well as their evolution? can be useful to an ana-
lyst (Sykora et al., 2013b). We detect enotions in
Tweets and support faceted browing. The emotion
component assigns labels to Tweets representing
these emotions. It is based upon a manually con-
structed ontology, which captures the relationships
between these emotions and terms (Sykora et al.,
2013a).
We sampled the summarised Tweets of the
Westgate attack, starting from the event detection
and following the messages over a course of seven
days. In the relevant Tweets, we detected that
8
Note some Tweets happen on following days.
41
8.6% had emotive terms in them, which is in line
with the aforementioned literature. Some example
expressions of emotion include:
Time Tweet Emotions
03:34 -) Ya so were those gunshots outside Fear
of gables?! I?m terrified ?
06:27 -) I?m so impressed @ d way. Kenyans r handling d siege. Surprise
14:32 -) All you xenophobic idiots spewing anti-Muslim Fear
bullshit need to -get in one of these donation lines Disgust
and see how wrong you ?
For Westgate, the emotions of sadness, fear and
surprise dominated. Very early on the emotions of
fear and sadness were expressed, as Twitter users
were terrified by the event and saddened by the
loss of lives. Sadness and fear were ? over time ?
the emotions that were stated most frequently and
constantly, with expressions of surprise, as users
were shocked about what was going on, and some
happiness relating to when people managed to
escape or were rescued from the mall. Generally
speaking, factual statements in the Tweets were
more prominent than emotive ones. This coincides
with the emotive Tweets that represented fear and
surprise in the beginning, as it was not clear what
had happened and Twitter users were upset and
tried to get factual information about the event.
6.4 Visual Analytics
The visualisation component is designed to facili-
tate the understanding and exploration of detected
events. It offers faceted browsing and multiple vi-
sualisation tools to allow an information analyst
to gain a rapid understanding of ongoing events.
An analyst can constrain the detected events us-
ing information both from the original Tweets (e.g.
hashtags, locations, user details) and from the up-
dated summaries derived by ReDites. The ana-
lyst can also view events using facet values, loca-
tions/keywords in topic maps and time/keywords
in multiple timelines. By combining informa-
tion dimensions, the analyst can determine pat-
terns across dimensions to determine if an event
should be acted upon ? e.g the analyst can choose
to view Tweets, which summarise highly emotive
events, concerning middle eastern countries.
7 Discussion
We have presented ReDites, the first published
system that carries out large-scale event detection,
tracking summarisation and visualisation for the
security sector. Events are automatically identified
and those that are relevant to information analysts
are quickly made available for ongoing monitor-
ing. We showed how the system could be used
to help understand a complex, large-scale security
event. Although our system is initially specialised
to the security sector, it is easy to repurpose it to
other domains, such as natural disasters or smart
cities. Key aspects of our approach include scala-
bility and a rapid response to incoming data.
Acknowledgements
This work was funded by EPSRC grant
EP/L010690/1. MO also acknowledges sup-
port from grant ERC Advanced Fellowship
249520 GRAMPLUS.
References
F. Abel, C. Hauff, G.-J. Houben, R. Stronkman, and K. T.
Semantics + filtering + search = twitcident. exploring in-
formation in social web streams. In Proc. of HT, 2012.
L. M. Aiello et al. L. Aiello, G. Petkos, C. Martin, D. Corney,
S. Papadopoulos, R. Skraba, A. Goker, Y. Kompatsiaris,
A. Jaimes Sensing trending topics in Twitter. Transac-
tions on Multimedia Journal, 2012.
A.E. Cano, Y. He, K. Liu, and J. Zhao. A weakly supervised
bayesian model for violence detection in social media. In
Proc. of IJCNLP, 2013.
M. Lui and T. Baldwin. Langid.py: An off-the-shelf lan-
guage identification tool. In Proc. of ACL, 2012.
R. McCreadie, C. Macdonald, I. Ounis, M. Osborne, and S.
Petrovic. Scalable distributed event detection for twitter.
In Proc. of Big Data, 2013.
R. McCreadie, I. Soboroff, J. Lin, C. Macdonald, I. Ounis and
D. McCullough. On building a reusable Twitter corpus. In
Proc. of SIGIR, 2012.
A. Nenkova and K. McKeown. A survey of text summariza-
tion techniques. In Mining Text Data Journal, 2012.
S. Petrovic, M. Osborne, and V. Lavrenko. Streaming first
story detection with application to Twitter. In Proc. of
NAACL, 2010.
S. Petrovic, M. Osborne, R. McCreadie, C. Macdonald, I.
Ounis, and L. Shrimpton. Can Twitter replace newswire
for breaking news? In Proc. of WSM, 2012.
D. Preot?iuc-Pietro and T. Cohn. A temporal model of text pe-
riodicities using gaussian processes. In Proc. of EMNLP,
2012.
M. D. Sykora, T. W. Jackson, A. O?Brien, and S. Elayan.
Emotive ontology: Extracting fine-grained emotions from
terse, informal messages. Computer Science and Informa-
tion Systems Journal, 2013.
M. D. Sykora, T. W. Jackson, A. O?Brien, and S. Elayan.
National security and social media monitoring. In Proc.
of EISIC, 2013.
42
