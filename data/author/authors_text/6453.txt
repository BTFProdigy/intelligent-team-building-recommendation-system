Proceedings of NAACL HLT 2007, pages 260?267,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Using ?Annotator Rationales? to Improve
Machine Learning for Text Categorization?
Omar F. Zaidan and Jason Eisner
Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218, USA
{ozaidan,jason}@cs.jhu.edu
Christine D. Piatko
JHU Applied Physics Laboratory
11100 Johns Hopkins Road
Laurel, MD 20723 USA
christine.piatko@jhuapl.edu
Abstract
We propose a new framework for supervised ma-
chine learning. Our goal is to learn from smaller
amounts of supervised training data, by collecting a
richer kind of training data: annotations with ?ra-
tionales.? When annotating an example, the hu-
man teacher will also highlight evidence support-
ing this annotation?thereby teaching the machine
learner why the example belongs to the category. We
provide some rationale-annotated data and present a
learning method that exploits the rationales during
training to boost performance significantly on a sam-
ple task, namely sentiment classification of movie
reviews. We hypothesize that in some situations,
providing rationales is a more fruitful use of an an-
notator?s time than annotating more examples.
1 Introduction
Annotation cost is a bottleneck for many natural lan-
guage processing applications. While supervised
machine learning systems are effective, it is labor-
intensive and expensive to construct the many train-
ing examples needed. Previous research has ex-
plored active or semi-supervised learning as possible
ways to lessen this burden.
We propose a new way of breaking this annotation
bottleneck. Annotators currently indicate what the
correct answers are on training data. We propose
that they should also indicate why, at least by coarse
hints. We suggest new machine learning approaches
that can benefit from this ?why? information.
For example, an annotator who is categorizing
phrases or documents might also be asked to high-
light a few substrings that significantly influenced
her judgment. We call such clues ?rationales.? They
need not correspond to machine learning features.
?This work was supported by the JHU WSE/APL Partner-
ship Fund; National Science Foundation grant No. 0347822 to
the second author; and an APL Hafstad Fellowship to the third.
In some circumstances, rationales should not be
too expensive or time-consuming to collect. As long
as the annotator is spending the time to study exam-
ple xi and classify it, it may not require much extra
effort for her to mark reasons for her classification.
2 Using Rationales to Aid Learning
We will not rely exclusively on the rationales, but
use them only as an added source of information.
The idea is to help direct the learning algorithm?s
attention?helping it tease apart signal from noise.
Machine learning algorithms face a well-known
?credit assignment? problem. Given a complex da-
tum xi and the desired response yi, many features of
xi could be responsible for the choice of yi. The
learning algorithm must tease out which features
were actually responsible. This requires a lot of
training data, and often a lot of computation as well.
Our rationales offer a shortcut to solving this
?credit assignment? problem, by providing the
learning algorithm with hints as to which features
of xi were relevant. Rationales should help guide
the learning algorithm toward the correct classifica-
tion function, by pushing it toward a function that
correctly pays attention to each example?s relevant
features. This should help the algorithm learn from
less data and avoid getting trapped in local maxima.1
In this paper, we demonstrate the ?annotator ra-
tionales? technique on a text categorization problem
previously studied by others.
1To understand the local maximum issue, consider the hard
problem of training a standard 3-layer feed-forward neural net-
work. If the activations of the ?hidden? layer?s features (nodes)
were observed at training time, then the network would de-
compose into a pair of independent 2-layer perceptrons. This
turns an NP-hard problem with local maxima (Blum and Rivest,
1992) to a polytime-solvable convex problem. Although ratio-
nales might only provide indirect evidence of the hidden layer,
this would still modify the objective function (see section 8) in
a way that tended to make the correct weights easier to discover.
260
3 Discriminative Approach
One popular approach for text categorization is to
use a discriminative model such as a Support Vec-
tor Machine (SVM) (e.g. (Joachims, 1998; Dumais,
1998)). We propose that SVM training can in gen-
eral incorporate annotator rationales as follows.
From the rationale annotations on a positive ex-
ample ??xi , we will construct one or more ?not-quite-
as-positive? contrast examples ??vij . In our text cat-
egorization experiments below, each contrast docu-
ment ??vij was obtained by starting with the original
and ?masking out? one or all of the several rationale
substrings that the annotator had highlighted (rij).
The intuition is that the correct model should be less
sure of a positive classification on the contrast exam-
ple ??vij than on the original example ~xi, because
??vij
lacks evidence that the annotator found significant.
We can translate this intuition into additional con-
straints on the correct model, i.e., on the weight vec-
tor ~w. In addition to the usual SVM constraint on
positive examples that ~w ? ??xi ? 1, we also want (for
each j) that ~w ? ~xi ? ~w ?
??vij ? ?, where ? ? 0 con-
trols the size of the desired margin between original
and contrast examples.
An ordinary soft-margin SVM chooses ~w and ~? to
minimize
1
2
?~w?2 + C(
?
i
?i) (1)
subject to the constraints
(?i) ~w ? ??xi ? yi ? 1? ?i (2)
(?i) ?i ? 0 (3)
where ??xi is a training example, yi ? {?1,+1} is
its desired classification, and ?i is a slack variable
that allows training example ??xi to miss satisfying
the margin constraint if necessary. The parameter
C > 0 controls the cost of taking such slack, and
should generally be lower for noisier or less linearly
separable datasets. We add the contrast constraints
(?i, j) ~w ? (??xi ?
??vij) ? yi ? ?(1? ?ij), (4)
where ??vij is one of the contrast examples con-
structed from example ??xi , and ?ij ? 0 is an asso-
ciated slack variable. Just as these extra constraints
have their own margin ?, their slack variables have
their own cost, so the objective function (1) becomes
1
2
?~w?2 + C(
?
i
?i) + Ccontrast(
?
i,j
?ij) (5)
The parameter Ccontrast ? 0 determines the impor-
tance of satisfying the contrast constraints. It should
generally be less than C if the contrasts are noisier
than the training examples.2
In practice, it is possible to solve this optimization
using a standard soft-margin SVM learner. Dividing
equation (4) through by ?, it becomes
(?i, j) ~w ? ??xij ? yi ? 1? ?ij , (6)
where ??xij
def
=
??xi?
??vij
? . Since equation (6) takes
the same form as equation (2), we simply add the
pairs (??xij , yi) to the training set as pseudoexam-
ples, weighted by Ccontrast rather than C so that the
learner will use the objective function (5).
There is one subtlety. To allow a biased hyper-
plane, we use the usual trick of prepending a 1 el-
ement to each training example. Thus we require
~w ? (1,??xi) ? 1 ? ?i (which makes w0 play the
role of a bias term). This means, however, that we
must prepend a 0 element to each pseudoexample:
~w ? (1,~xi)?(1,
??vij)
? = ~w ? (0,
??xij) ? 1? ?ij .
In our experiments, we optimize ?, C, and
Ccontrast on held-out data (see section 5.2).
4 Rationale Annotation for Movie Reviews
In order to demonstrate that annotator rationales
help machine learning, we needed annotated data
that included rationales for the annotations.
We chose a dataset that would be enjoyable to re-
annotate: the movie review dataset of (Pang et al,
2002; Pang and Lee, 2004).3 The dataset consists
of 1000 positive and 1000 negative movie reviews
obtained from the Internet Movie Database (IMDb)
review archive, all written before 2002 by a total of
312 authors, with a cap of 20 reviews per author per
2Taking Ccontrast to be constant means that all rationales
are equally valuable. One might instead choose, for example,
to reduce Ccontrast for examples xi that have many rationales,
to prevent xi?s contrast examples vij from together dominating
the optimization. However, in this paper we assume that an xi
with more rationales really does provide more evidence about
the true classifier ~w.
3Polarity dataset version 2.0.
261
category. Pang and Lee have divided the 2000 docu-
ments into 10 folds, each consisting of 100 positive
reviews and 100 negative reviews.
The dataset is arguably artificial in that it keeps
only reviews where the reviewer provided a rather
high or rather low numerical rating, allowing Pang
and Lee to designate the review as positive or neg-
ative. Nonetheless, most reviews contain a difficult
mix of praise, criticism, and factual description. In
fact, it is possible for a mostly critical review to give
a positive overall recommendation, or vice versa.
4.1 Annotation procedure
Rationale annotators were given guidelines4 that
read, in part:
Each review was intended to give either a positive or a neg-
ative overall recommendation. You will be asked to justify why
a review is positive or negative. To justify why a review is posi-
tive, highlight the most important words and phrases that would
tell someone to see the movie. To justify why a review is nega-
tive, highlight words and phrases that would tell someone not to
see the movie. These words and phrases are called rationales.
You can highlight the rationales as you notice them, which
should result in several rationales per review. Do your best to
mark enough rationales to provide convincing support for the
class of interest.
You do not need to go out of your way to mark everything.
You are probably doing too much work if you find yourself go-
ing back to a paragraph to look for even more rationales in it.
Furthermore, it is perfectly acceptable to skim through sections
that you feel would not contain many rationales, such as a re-
viewer?s plot summary, even if that might cause you to miss a
rationale here and there.
The last two paragraphs were intended to provide
some guidance on how many rationales to annotate.
Even so, as section 4.2 shows, some annotators were
considerably more thorough (and slower).
Annotators were also shown the following exam-
ples5 of positive rationales:
? you will enjoy the hell out of American Pie.
? fortunately, they managed to do it in an interesting and
funny way.
? he is one of the most exciting martial artists on the big
screen, continuing to perform his own stunts and daz-
zling audiences with his flashy kicks and punches.
? the romance was enchanting.
and the following examples5 of negative rationales:
4Available at http://cs.jhu.edu/?ozaidan/rationales.
5For our controlled study of annotation time (section 4.2),
different examples were given with full document context.
Figure 1: Histograms of rationale counts per document (A0?s
annotations). The overall mean of 8.55 is close to that of the
four annotators in Table 1. The median and mode are 8 and 7.
? A woman in peril. A confrontation. An explosion. The
end. Yawn. Yawn. Yawn.
? when a film makes watching Eddie Murphy a tedious ex-
perience, you know something is terribly wrong.
? the movie is so badly put together that even the most
casual viewer may notice themiserable pacing and stray
plot threads.
? don?t go see this movie
The annotation involves boldfacing the rationale
phrases using an HTML editor. Note that a fancier
annotation tool would be necessary for a task like
named entity tagging, where an annotator must mark
many named entities in a single document. At any
given moment, such a tool should allow the annota-
tor to highlight, view, and edit only the several ra-
tionales for the ?current? annotated entity (the one
most recently annotated or re-selected).
One of the authors (A0) annotated folds 0?8 of
the movie review set (1,800 documents) with ra-
tionales that supported the gold-standard classifica-
tions. This training/development set was used for
all of the learning experiments in sections 5?6. A
histogram of rationale counts is shown in Figure 1.
As mentioned in section 3, the rationale annotations
were just textual substrings. The annotator did not
require knowledge of the classifier features. Thus,
our rationale dataset is a new resource4 that could
also be used to study exploitation of rationales un-
der feature sets or learning methods other than those
considered here (see section 8).
4.2 Inter-annotator agreement
To study the annotation process, we randomly se-
lected 150 documents from the dataset. The doc-
262
Rationales % rationales also % rationales also % rationales also % rationales also % rationales also
per document annotated by A1 annotated by A2 annotated by AX annotated by AY ann. by anyone else
A1 5.02 (100) 69.6 63.0 80.1 91.4
A2 10.14 42.3 (100) 50.2 67.8 80.9
AX 6.52 49.0 68.0 (100) 79.9 90.9
AY 11.36 39.7 56.2 49.3 (100) 75.5
Table 1: Average number of rationales and inter-annotator agreement for Tasks 2 and 3. A rationale by Ai (?I think this is a great
movie!?) is considered to have been annotated also by Aj if at least one of Aj?s rationales overlaps it (?I think this is a great
movie!?). In computing pairwise agreement on rationales, we ignored documents where Ai and Aj disagreed on the class. Notice
that the most thorough annotatorAY caught most rationales marked by the others (exhibiting high ?recall?), and that most rationales
enjoyed some degree of consensus, especially those marked by the least thorough annotator A1 (exhibiting high ?precision?).
uments were split into three groups, each consisting
of 50 documents (25 positive and 25 negative). Each
subset was used for one of three tasks:6
? Task 1: Given the document, annotate only the
class (positive/negative).
? Task 2: Given the document and its class, an-
notate some rationales for that class.
? Task 3: Given the document, annotate both the
class and some rationales for it.
We carried out a pilot study (annotators AX and
AY: two of the authors) and a later, more controlled
study (annotators A1 and A2: paid students). The
latter was conducted in a more controlled environ-
ment where both annotators used the same annota-
tion tool and annotation setup as each other. Their
guidelines were also more detailed (see section 4.1).
In addition, the documents for the different tasks
were interleaved to avoid any practice effect.
The annotators? classification accuracies in Tasks
1 and 3 (against Pang & Lee?s labels) ranged from
92%?97%, with 4-way agreement on the class for
89% of the documents, and pairwise agreement also
ranging from 92%?97%. Table 1 shows how many
rationales the annotators provided and how well
their rationales agreed.
Interestingly, in Task 3, four of AX?s ratio-
nales for a positive class were also partially
highlighted by AY as support for AY?s (incorrect)
negative classifications, such as:
6Each task also had a ?warmup? set of 10 documents to be
annotated before that tasks?s 50 documents. Documents for
Tasks 2 and 3 would automatically open in an HTML editor
while Task 1 documents opened in an HTML viewer with no
editing option. The annotators recorded their classifications for
Tasks 1 and 3 on a spreadsheet.
min./KB A1 time A2 time AX time AY time
Task 1 0.252 0.112 0.150 0.422
Task 2 0.396 0.537 0.242 0.626
Task 3 0.399 0.505 0.288 1.01
min./doc. A1 time A2 time AX time AY time
Task 1 1.04 0.460 0.612 1.73
min./rat. A1 time A2 time AX time AY time
Task 2 0.340 0.239 0.179 0.298
Task 3 0.333 0.198 0.166 0.302
Table 2: Average annotation rates on each task.
? Even with its numerous flaws, the movie all comes to-
gether, if only for those who . . .
? ?Beloved? acts like an incredibly difficult chamber
drama paired with a ghost story.
4.3 Annotation time
Average annotation times are in Table 2. As hoped,
rationales did not take too much extra time for most
annotators to provide. For each annotator except
A2, providing rationales only took roughly twice the
time (Task 3 vs. Task 1), even though it meant mark-
ing an average of 5?11 rationales in addition to the
class.
Why this low overhead? Because marking the
class already required the Task 1 annotator to read
the document and find some rationales, even if s/he
did not mark them. The only extra work in Task 3
is in making them explicit. This synergy between
class annotation and rationale annotation is demon-
strated by the fact that doing both at once (Task 3)
was faster than doing them separately (Tasks 1+2).
We remark that this task?binary classification on
full documents?seems to be almost a worst-case
scenario for the annotation of rationales. At a purely
mechanical level, it was rather heroic of A0 to at-
tach 8?9 new rationale phrases rij to every bit yi
of ordinary annotation. Imagine by contrast a more
local task of identifying entities or relations. Each
263
lower-level annotation yi will tend to have fewer ra-
tionales rij , while yi itself will be more complex and
hence more difficult to mark. Thus, we expect that
the overhead of collecting rationales will be less in
many scenarios than the factor of 2 we measured.
Annotation overhead could be further reduced.
For a multi-class problem like relation detection, one
could ask the annotator to provide rationales only for
the rarer classes. This small amount of extra time
where the data is sparsest would provide extra guid-
ance where it was most needed. Another possibility
is passive collection of rationales via eye tracking.
5 Experimental Procedures
5.1 Feature extraction
Although this dataset seems to demand discourse-
level features that contextualize bits of praise and
criticism, we exactly follow Pang et al (2002) and
Pang and Lee (2004) in merely using binary uni-
gram features, corresponding to the 17,744 un-
stemmed word or punctuation types with count ? 4
in the full 2000-document corpus. Thus, each docu-
ment is reduced to a 0-1 vector with 17,744 dimen-
sions, which is then normalized to unit length.7
We used the method of section 3 to place addi-
tional constraints on a linear classifier. Given a train-
ing document, we create several contrast documents,
each by deleting exactly one rationale substring
from the training document. Converting documents
to feature vectors, we obtained an original exam-
ple ??xi and several contrast examples
??vi1,
??vi2, . . ..8
Again, our training method required each original
document to be classified more confidently (by a
margin ?) than its contrast documents.
If we were using more than unigram features, then
simply deleting a rationale substring would not al-
ways be the best way to create a contrast document,
as the resulting ungrammatical sentences might
cause deep feature extraction to behave strangely
(e.g., parse errors during preprocessing). The goal in
creating the contrast document is merely to suppress
7The vectors are normalized before prepending the 1 corre-
sponding to the bias term feature (mentioned in section 3).
8The contrast examples were not normalized to precisely
unit length, but instead were normalized by the same factor used
to normalize ??xi . This conveniently ensured that the pseudoex-
amples ??xij
def
=
~xi?
??vij
? were sparse vectors, with 0 coordinates
for all words not in the jth rationale.
features (n-grams, parts of speech, syntactic depen-
dencies . . . ) that depend in part on material in one
or more rationales. This could be done directly by
modifying the feature extractors, or if one prefers to
use existing feature extractors, by ?masking? rather
than deleting the rationale substring?e.g., replacing
each of its word tokens with a special MASK token
that is treated as an out-of-vocabulary word.
5.2 Training and testing procedures
We transformed this problem to an SVM problem
(see section 3) and applied SVMlight for training and
testing, using the default linear kernel. We used only
A0?s rationales and the true classifications.
Fold 9 was reserved as a test set. All accuracy
results reported in the paper are the result of testing
on fold 9, after training on subsets of folds 0?8.
Our learning curves show accuracy after training
on T < 9 folds (i.e., 200T documents), for various
T . To reduce the noise in these results, the accuracy
we report for training on T folds is actually the aver-
age of 9 different experiments with different (albeit
overlapping) training sets that cover folds 0?8:
1
9
8?
i=0
acc(F9 | ?
?, Fi+1 ? . . . ? Fi+T ) (7)
where Fj denotes the fold numbered j mod 9, and
acc(Z | ?, Y ) means classification accuracy on the
set Z after training on Y with hyperparameters ?.
To evaluate whether two different training meth-
ods A and B gave significantly different average-
accuracy values, we used a paired permutation test
(generalizing a sign test). The test assumes in-
dependence among the 200 test examples but not
among the 9 overlapping training sets. For each
of the 200 test examples in fold 9, we measured
(ai, bi), where ai (respectively bi) is the number
of the 9 training sets under which A (respectively
B) classified the example correctly. The p value
is the probability that the absolute difference be-
tween the average-accuracy values would reach or
exceed the observed absolute difference, namely
| 1200
?200
i=1
ai?bi
9 |, if each (ai, bi) had an independent
1/2 chance of being replaced with (bi, ai), as per the
null hypothesis that A and B are indistinguishable.
For any given value of T and any given train-
ing method, we chose hyperparameters ?? =
264
Figure 2: Classification accuracy under five different experi-
mental setups (S1?S5). At each training size, the 5 accura-
cies are pairwise significantly different (paired permutation test,
p < 0.02; see section 5.2), except for {S3,S4} or {S4,S5} at
some sizes.
(C, ?,Ccontrast) to maximize the following cross-
validation performance:9
?? = argmax
?
8?
i=0
acc(Fi | ?, Fi+1 ? . . . ? Fi+T )
(8)
We used a simple alternating optimization procedure
that begins at ?0 = (1.0, 1.0, 1.0) and cycles repeat-
edly through the three dimensions, optimizing along
each dimension by a local grid search with resolu-
tion 0.1.10 Of course, when training without ratio-
nales, we did not have to optimize ? or Ccontrast.
6 Experimental Results
6.1 The value of rationales
The top curve (S1) in Figure 2 shows that perfor-
mance does increase when we introduce rationales
for the training examples as contrast examples (sec-
tion 3). S1 is significantly higher than the baseline
curve (S2) immediately below it, which trains an or-
dinary SVM classifier without using rationales. At
the largest training set size, rationales raise the accu-
racy from 88.5% to 92.2%, a 32% error reduction.
9One might obtain better performance (across all methods
being compared) by choosing a separate ?? for each of the 9
training sets. However, to simulate real limited-data training
conditions, one should then find the ?? for each {i, ..., j} us-
ing a separate cross-validation within {i, ..., j} only; this would
slow down the experiments considerably.
10For optimizing along the C dimension, one could use the
efficient method of Beineke et al (2004), but not in SVMlight.
The lower three curves (S3?S5) show that learn-
ing is separately helped by the rationale and the
non-rationale portions of the documents. S3?S5
are degraded versions of the baseline S2: they are
ordinary SVM classifiers that perform significantly
worse than S2 (p < 0.001).
Removing the rationale phrases from the train-
ing documents (S3) made the test documents much
harder to discriminate (compared to S2). This sug-
gests that annotator A0?s rationales often covered
most of the usable evidence for the true class.
However, the pieces to solving the classification
puzzle cannot be found solely in the short rationale
phrases. Removing all non-rationale text from the
training documents (S5) was even worse than re-
moving the rationales (S3). In other words, we can-
not hope to do well simply by training on just the
rationales (S5), although that approach is improved
somewhat in S4 by treating each rationale (similarly
to S1) as a separate SVM training example.
This presents some insight into why our method
gives the best performance. The classifier in S1
is able to extract subtle patterns from the corpus,
like S2, S3, or any other standard machine learn-
ing method, but it is also able to learn from a human
annotator?s decision-making strategy.
6.2 Using fewer rationales
In practice, one might annotate rationales for only
some training documents?either when annotating a
new corpus or when adding rationales post hoc to
an existing corpus. Thus, a range of options can be
found between curves S2 and S1 of Figure 2.
Figure 3 explores this space, showing how far the
learning curve S2 moves upward if one has time to
annotate rationales for a fixed number of documents
R. The key useful discovery is that much of the ben-
efit can actually be obtained with relatively few ra-
tionales. For example, with 800 training documents,
annotating (0%, 50%, 100%) of themwith rationales
gives accuracies of (86.9%, 89.2%, 89.3%). With
the maximum of 1600 training documents, annotat-
ing (0%, 50%, 100%) with rationales gives (88.5%,
91.7%, 92.2%).
To make this point more broadly, we find that the
R = 200 curve is significantly above the R = 0
curve (p < 0.05) at all T ? 1200. By contrast, the
R = 800, R = 1000, . . . R = 1600 points at each T
265
Figure 3: Classification accuracy for T ? {200, 400, ..., 1600}
training documents (x-axis) when only R ? {0, 200, ..., T} of
them are annotated with rationales (different curves). The R =
0 curve above corresponds to the baseline S2 from Figure 2.
S1?s points are found above as the leftmost points on the other
curves, where R = T .
value are all-pairs statistically indistinguishable.
The figure also suggests that rationales and docu-
ments may be somewhat orthogonal in their benefit.
When one has many documents and few rationales,
there is no longer much benefit in adding more doc-
uments (the curve is flattening out), but adding more
rationales seems to provide a fresh benefit: ratio-
nales have not yet reached their point of diminishing
returns. (While this fresh benefit was often statisti-
cally significant, and greater than the benefit from
more documents, our experiments did not establish
that it was significantly greater.)
The above experiments keep all of A0?s rationales
on a fraction of training documents. We also exper-
imented with keeping a fraction of A0?s rationales
(chosen randomly with randomized rounding) on all
training documents. This yielded no noteworthy or
statistically significant differences from Figure 3.
These latter experiments simulate a ?lazy annota-
tor? who is less assiduous than A0. Such annotators
may be common in the real world. We also suspect
that they will be more desirable. First, they should
be able to add more rationales per hour than the A0-
style annotator from Figure 3: some rationales are
simply more noticeable than others, and a lazy anno-
tator will quickly find the most noticeable ones with-
out wasting time tracking down the rest. Second, the
?most noticeable? rationales that they mark may be
the most effective ones for learning, although our
random simulation of laziness could not test that.
7 Related Work
Our rationales resemble ?side information? in ma-
chine learning?supplementary information about
the target function that is available at training time.
Side information is sometimes encoded as ?virtual
examples? like our contrast examples or pseudoex-
amples. However, past work generates these by
automatically transforming the training examples
in ways that are expected to preserve or alter the
classification (Abu-Mostafa, 1995). In another for-
mulation, virtual examples are automatically gener-
ated but must be manually annotated (Kuusela and
Ocone, 2004). Our approach differs because a hu-
man helps to generate the virtual examples. Enforc-
ing a margin between ordinary examples and con-
trast examples also appears new.
Other researchers have considered how to reduce
annotation effort. In active learning, the annotator
classifies only documents where the system so far is
less confident (Lewis and Gale, 1994), or in an in-
formation extraction setting, incrementally corrects
details of the system?s less confident entity segmen-
tations and labelings (Culotta andMcCallum, 2005).
Raghavan et al (2005) asked annotators to iden-
tify globally ?relevant? features. In contrast, our ap-
proach does not force the annotator to evaluate the
importance of features individually, nor in a global
context outside any specific document, nor even to
know the learner?s feature space. Annotators only
mark text that supports their classification decision.
Our methods then consider the combined effect of
this text on the feature vector, which may include
complex features not known to the annotator.
8 Future Work: Generative models
Our SVM contrast method (section 3) is not the only
possible way to use rationales. We would like to ex-
plicitly model rationale annotation as a noisy pro-
cess that reflects, imperfectly and incompletely, the
annotator?s internal decision procedure.
A natural approach would start with log-linear
models in place of SVMs. We can define a proba-
bilistic classifier
p?(y | x)
def
=
1
Z(x)
exp
k?
h=1
?hfh(x, y) (9)
266
where ~f(?) extracts a feature vector from a classified
document.
A standard training method would be to choose ?
to maximize the conditional likelihood of the train-
ing classifications:
argmax
~?
n?
i=1
p?(yi | xi) (10)
When a rationale ri is also available for each
(xi, yi), we propose to maximize a likelihood that
tries to predict these rationale data as well:
argmax
~?
n?
i=1
p?(yi | xi) ? p??(ri | xi, yi, ?) (11)
Notice that a given guess of ? might make equa-
tion (10) large, yet accord badly with the annotator?s
rationales. In that case, the second term of equa-
tion (11) will exert pressure on ? to change to some-
thing that conforms more closely to the rationales.
If the annotator is correct, such a ? will generalize
better beyond the training data.
In equation (11), p?? models the stochastic process
of rationale annotation. What is an annotator actu-
ally doing when she annotates rationales? In par-
ticular, how do her rationales derive from the true
value of ? and thereby tell us about ?? Building a
good model p?? of rationale annotation will require
some exploratory data analysis. Roughly, we expect
that if ?hfh(xi, y) is much higher for y = yi than
for other values of y, then the annotator?s ri is corre-
spondingly more likely to indicate in some way that
feature fh strongly influenced annotation yi. How-
ever, we must also model the annotator?s limited pa-
tience (she may not annotate all important features),
sloppiness (she may indicate only indirectly that fh
is important), and bias (tendency to annotate some
kinds of features at the expense of others).
One advantage of this generative approach is that
it eliminates the need for contrast examples. Con-
sider a non-textual example in which an annotator
highlights the line crossing in a digital image of the
digit ?8? to mark the rationale that distinguishes it
from ?0.? In this case it is not clear how to mask out
that highlighted rationale to create a contrast exam-
ple in which relevant features would not fire.11
11One cannot simply flip those highlighted pixels to white
9 Conclusions
We have proposed a quite simple approach to im-
proving machine learning by exploiting the clever-
ness of annotators, asking them to provide enriched
annotations for training. We developed and tested
a particular discriminative method that can use ?an-
notator rationales??even on a fraction of the train-
ing set?to significantly improve sentiment classifi-
cation of movie reviews.
We found fairly good annotator agreement on the
rationales themselves. Most annotators provided
several rationales per classification without taking
too much extra time, even in our text classification
scenario, where the rationales greatly outweigh the
classifications in number and complexity. Greater
speed might be possible through an improved user
interface or passive feedback (e.g., eye tracking).
In principle, many machine learning methods
might be modified to exploit rationale data. While
our experiments in this paper used a discriminative
SVM, we plan to explore generative approaches.
References
Y. S. Abu-Mostafa. 1995. Hints. Neural Computation, 7:639?
671, July.
P. Beineke, T. Hastie, and S. Vaithyanathan. 2004. The sen-
timental factor: Improving review classification via human-
provided information. In Proc. of ACL, pages 263?270.
A. L. Blum and R. L. Rivest. 1992. Training a 3-node neural
network is NP-complete. Neural Networks, 5(1):117?127.
A. Culotta and A. McCallum. 2005. Reducing labeling effort
for structured prediction tasks. In AAAI, pages 746?751.
S. Dumais. 1998. Using SVMs for text categorization. IEEE
Intelligent Systems Magazine, 13(4), July/August.
T. Joachims. 1998. Text categorization with support vector
machines: Learning with many relevant features. In Proc. of
the European Conf. on Machine Learning, pages 137?142.
P. Kuusela and D. Ocone. 2004. Learning with side informa-
tion: PAC learning bounds. J. of Computer and System Sci-
ences, 68(3):521?545, May.
D. D. Lewis and W. A. Gale. 1994. A sequential algorithm for
training text classifiers. In Proc. of ACM-SIGIR, pages 3?12.
B. Pang and L. Lee. 2004. A sentimental education: Sen-
timent analysis using subjectivity summarization based on
minimum cuts. In Proc. of ACL, pages 271?278.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up?
Sentiment classification using machine learning techniques.
In Proc. of EMNLP, pages 79?86.
H. Raghavan, O. Madani, and R. Jones. 2005. Interactive fea-
ture selection. In Proc. of IJCAI, pages 41?46.
or black, since that would cause new features to fire. Possibly
one could simply suppress any feature that depends in any way
on the highlighted pixels, but this would take away too many
important features, including global features.
267
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 357?360,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Arabic Cross-Document Coreference Detection
Asad Sayeed,
1,2
Tamer Elsayed,
1,2
Nikesh Garera,
1,6
David Alexander,
1,3
Tan Xu,
1,4
Douglas W. Oard,
1,4,5
David Yarowsky,
1,6
Christine Piatko
1
1
Human Language Technology Center of Excellence, Johns Hopkins University, Baltimore,
MD, USA?
2
Dept. of Computer Science, University of Maryland, College Park, MD,
USA?
3
BBN Technologies, Cambridge, MA, USA?
4
College of Information Studies,
University of Maryland, College Park, MD, USA?
5
UMIACS, University of Maryland, College
Park, MD, USA?
6
Dept. of Computer Science, Johns Hopkins University, Baltimore, MD, USA
{asayeed,telsayed}@cs.umd.edu, ngarera@cs.jhu.edu, dalexand@bbn.com,
{tanx,oard}@umd.edu, yarowsky@cs.jhu.edu, Christine.Piatko@jhuapl.edu
Abstract
We describe a set of techniques for Ara-
bic cross-document coreference resolu-
tion. We compare a baseline system of
exact mention string-matching to ones that
include local mention context information
as well as information from an existing
machine translation system. It turns out
that the machine translation-based tech-
nique outperforms the baseline, but local
entity context similarity does not. This
helps to point the way for future cross-
document coreference work in languages
with few existing resources for the task.
1 Introduction
Our world contains at least two noteworthy
George Bushes: President George H. W. Bush and
President George W. Bush. They are both fre-
quently referred to as ?George Bush.? If we wish
to use a search engine to find documents about
one of them, we are likely also to find documents
about the other. Improving our ability to find all
documents referring to one and none referring to
the other in a targeted search is a goal of cross-
document entity coreference detection. Here we
describe some results from a system we built to
perform this task on Arabic documents. We base
our work partly on previous work done by Bagga
and Baldwin (Bagga and Baldwin, 1998), which
has also been used in later work (Chen and Mar-
tin, 2007). Other work such as Lloyd et al (Lloyd,
2006) focus on techniques specific to English.
The main contribution of this work to cross-
document coreference lies in the conditions under
which it was done. Even now, there is no large-
scale resource?in terms of annotated data?for
cross-document coreference in Arabic as there is
in English (e.g. WebPeople (Artiles, 2008)). Thus,
we employed techniques for high-performance
processing in a resource-poor environment. We
provide early steps in cross-document coreference
detection for resource-poor languages.
2 Approach
We treat cross-document entities as a set of graphs
consisting of links between within-document enti-
ties. The graphs are disjoint. Each of our systems
produces a list of such links as within-document
entity pairs (A,B). We obtain within-document
entities by running the corpus through a within-
document coreference resolver?in this case, Serif
from BBN Technologies.
To create the entity clusters, we use a union-
find algorithm over the pairs. If links (A,B)
and (C,B) appear in the system output, then
{A,B,C} are one entity. Similarly, if (X,Y )
and (Z, Y ) appear in the output, then it will find
that {X,Y, Z} are one entity. If the algorithm
later discovers link (B,Z) in the system output, it
will decide that {A,B,C,X, Y, Z} are an entity.
This is efficiently implemented via a hash table
whose keys and values are both within-document
entity IDs, allowing the implementation of easily-
searched linked lists.
2.1 The baseline system
The baseline system uses a string matching cri-
terion to determine whether two within-document
entities are similar enough to be considered as part
of the same cross-document entity. Given within-
document entities A and B, the criterion is imple-
mented as follows:
1. Find the mention strings {a
1
, a
2
, . . .} and
357
{b
1
, b
2
, . . .} of A and B, respectively that are
the longest for that within-document entity
in the given document. (There may be more
than one longest mention of equal length for
a given entity.)
2. If any longest mention strings a
n
and b
m
exist
such that a
n
= b
m
(exact string match), then
A and B are considered to be part of the same
cross-document entity. Otherwise, they are
considered to be different entities.
When the system decides that two within-
document entities are connected as a single cross-
document entity, it emits a link between within-
document entities A and B represented as the pair
(A, B). We maintain a list of such links, but we
omit all links between within-document entities in
the same document.
The output of the system is a list of pairwise
links. The following two experimental systems
also produce lists of pairwise links. Union is per-
formed between the baseline system?s list and the
lists produced by the other systems to create lists
of pairs that include the information in the base-
line. However, each of the following systems?
outputs are merged separately with the baseline.
By including the baseline results in each system,
we are able to clarify the potential of each addi-
tional technique to improve performance over a
technique that is cheap to run under any circum-
stances, especially given that our experiments are
focused on increasing the number of links in an
Arabic context where links are likely to be dis-
rupted by spelling variations.
2.2 Translingual projection
We implement a novel cross-language approach
for Arabic coreference resolution by expanding
the space of exact match comparisons to approxi-
mate matches of English translations of the Arabic
strings. The intuition for this approach is that of-
ten the Arabic strings of the same named entity
may differ due to misspellings, titles, or aliases
that can be corrected in the English space. The
English translations were obtained using a stan-
dard statistical machine translation system (Chi-
ang, 2007; Li, 2008) and then compared using an
alias match.
The algorithm below describes the approach,
applied to any Arabic named entities that fail the
baseline string-match test:
1. For a given candidate Arabic named entity
pair (A,B), we project them into English by
translating the mentions using a standard sta-
tistical machine translation toolkit. Using the
projected English pair, say, (A
?
, B
?
) we per-
form the following tests to determine whether
A and B are co-referent:
(a) We do an exact string-match test in the
English space using the projected enti-
ties (A
?
, B
?
). The exact string match test
is done exactly as in the baseline system,
using the set of longest named entities in
their respective co-reference chains.
(b) If (A
?
, B
?
) fail in the exact string-match
test as in the baseline, then we test
whether they belong to a list of high con-
fidence co-referent named-entity pairs
1
precomputed for English using alias-
lists derived from Wikipedia.
(c) If (A
?
, B
?
) fails (a) and (b) then (A,B)
is deemed as non-coreferent.
While we hypothesize that translingual projection
via English should help in increasing recall since
it can work with non-exact string matches, it may
also help in increasing precision based on the as-
sumption that a name of American or English ori-
gin might have different variants in Arabic and that
translating to English can help in merging those
variants, as shown in figure 1.
 ????? ??????
 ????? 
?????? 
 ??????? 
 ???????
(Ms. Aisha)
(Aisha)
(Clenton)
(Clinton)
(Cilinton)
Aisha
Aisha
Clinton
Clinton
Clinton
Translate
via SMT
Figure 1: Illustration of translingual projection
method for resolving Arabic named entity strings
via English space. The English strings in paren-
theses indicate the literal glosses of the Arabic
strings prior to translation.
2.3 Entity context similarity
The context of mentions can play an important role
in merging or splitting potential coreferent men-
1
For example: (Sean Michael Waltman, Sean Waltman)
are high confidence-matches even though they are not an
exact-string match.
358
tions. We hypothesize that two mentions in two
different documents have a good chance of refer-
ring to the same entity if they are mentioned in
contexts that are topically very similar. A way of
representing a mention context is to consider the
words in the mention?s neighborhood. The con-
text of a mention can be defined as the words that
surround the mention in a window of n (50 in our
experiments) tokens centered by the mention. In
our experiments, we used highly similar contexts
to link mentions that might be coreferent.
Computing context similarity between every
pair of large number of mentions requires a highly
scalable and efficient mechanism. This can be
achieved using MapReduce, a distributed comput-
ing framework (Dean, 2004)
Elsayed et al (Elsayed, 2008) proposed an ef-
ficient MapReduce solution for the problem of
computing the pairwise similarity matrix in large
collections. They considered a ?bag-of-words?
model where similarity of two documents d
i
and d
j
is measured as follows: sim(d
i
, d
j
) =
?
t?d
i
?d
j
w
t,d
i
? w
t,d
j
, where w(t, d) is the weight
of term t in document d. A term contributes to
each pair that contains it. The list of documents
that contain a term is what is contained in the post-
ings of an inverted index. Thus, by processing
all postings, the pairwise similarity matrix can be
computed by summing term contributions. We use
the MapReduce framework for two jobs, inverted
indexing and pairwise similarity.
Elsayed et al suggested an efficient df-cut strat-
egy that eliminates terms that appear in many doc-
uments (having high df ) and thus contribute less
in similarity but cost in computation (e.g., a 99%
df-cut means that the most frequent 1% of the
terms were discarded). We adopted that approach
for computing similarities between the contexts
of two mentions. The processing unit was rep-
resented as a bag of n words in a window sur-
rounding each mention of a within-document en-
tity. Given a relatively small mention context, we
used a high df-cut value of 99.9%.
3 Experiments
We performed our experiments in the context of
the Automatic Content Extraction (ACE) eval-
uation of 2008, run by the National Institute
of Standards and Technology (NIST). The eval-
uation corpus contained approximately 10,000
documents from the following domains: broad-
cast conversation transcripts, broadcast news tran-
scripts, conversational telephone speech tran-
scripts, newswire, Usenet Newsgroup/Discussion
Groups, and weblogs. Systems were required to
process the large source sets completely. For per-
formance measurement after the evaluation, NIST
selected 412 of the Arabic source documents out
of the larger set (NIST, 2008).
For development purposes we used the NIST
ACE 2005 Arabic data with within-document
ground truth. This consisted of 1,245 documents.
We also used exactly 12,000 randomly selected
documents from the LDC Arabic Gigaword Third
Edition corpus, processed through Serif. The Ara-
bic Gigaword corpus was used to select a thresh-
old of 0.4956 for the context similarity technique
via inspection of (A,B) link scores by a native
speaker of Arabic.
It must be emphasized that there was no ground
truth available for this task in Arabic. Performing
this task in the absence of significant training or
evaluation data is one emphasis of this work.
3.1 Evaluation measures
We used NIST?s scoring techniques to evaluate the
performance of our systems. Scoring for the ACE
evaluation is done using an scoring script provided
by NIST which produces many kinds of statistics.
NIST mainly uses a measure called the ACE value,
but it also computes B-cubed.
B-Cubed represents the task of finding cross-
document entities in the following way: if a user
of the system is searching for a particular Bush
and finds document D, he or she should be able to
find all of the other documents with the same Bush
in them as links from D?that is, cross-document
entities represent graphs connecting documents.
Bagga and Baldwin are able to define precision,
recall, and F-measure over a collection of docu-
ments in this way.
The ACE Value represents a score similar to
B-Cubed, except that every mention and within-
document entity is weighted in NIST?s specifica-
tion by a number of factors. Every entity is worth 1
point, a missing entity worth 0, and attribute errors
are discounted by multiplying by a factor (0.75 for
CLASS, 0.5 for TYPE, and 0.9 for SUBTYPE).
Before scoring can be accomplished, the enti-
ties found by the system must be mapped onto
those found in the reference provided by NIST.
The ACE scorer does this document-by-document,
359
selecting the mapping that produces the highest
score. A description of the evaluation method and
entity categorization is available at (NIST, 2008).
3.2 Results and discussion
The results of running the ACE evaluation script
on the system output are shown in table 1. The
translingual projection system achieves higher
scores than all other systems on all measures. Al-
though it achieves only a 2 point improvement
over the baseline ACE value, it should be noted
that this represents a substantial number of at-
tributes per cross-document entity that it is getting
right.
Thresh B-Cubed ACE
System hold Prec Rec F Val.
Baseline 37.5 44.1 40.6 19.2
TrnsProj 38.4 44.8 41.3 21.2
CtxtSim 0.2 37.6 35.2 36.4 15.9
CtxtSim 0.3 37.4 43.8 40.3 18.9
CtxtSim 0.4 37.5 44.1 40.6 19.3
CtxtSim 0.4956 37.5 44.1 40.6 19.3
CtxtSim 0.6 37.5 44.1 40.6 19.2
Table 1: Scores from ACE evaluation script.
On the other hand, as the context similarity
threshold increases, we notice that the B-Cubed
measures reach identical values with the baseline
but never exceed it. But as it decreases, it loses
B-Cubed recall and ACE value.
While two within-document entities whose
longest mention strings match exactly and are le-
gitimately coreferent are likely to be mentioned in
the same contexts, it seems that a lower (more lib-
eral) threshold introduces spurious links and cre-
ates a different entity clustering.
Translingual projection appears to include links
that exact string matching in Arabic does not?
part of its purpose is to add close matches to those
found by exact string matching. It is able to in-
clude these links partly because it allows access to
resources in English that are not available for Ara-
bic such as Wikipedia alias lists.
4 Conclusions and Future Work
We have evaluated and discussed a set of tech-
niques for cross-document coreference in Arabic
that can be applied in the absence of significant
training and evaluation data. As it turns out, an
approach based on machine translation is slightly
better than a string-matching baseline, across all
measures. It worked by using translations from
Arabic to English in order to liberalize the string-
matching criterion, suggesting that using further
techniques via English to discover links may be
a fruitful future research path. This also seems
to suggest that a Bagga and Baldwin-style vector-
space model may not be the first approach to pur-
sue in future work on Arabic.
However, varying other parameters in the con-
text similarity approach should be tried in order
to gain a fuller picture of performance. One of
them is the df-cut of the MapReduce-based sim-
ilarity computation. Another is the width of the
word token window we used?we may have used
one that is too tight to be better than exact Arabic
string-matching.
References
Javier Artiles and Satoshi Sekine and Julio Gonzalo
2008. Web People Search?Results of the first eval-
uation and the plan for the second. WWW 2008.
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space
model. COLING-ACL 1998.
Y. Chen and J. Martin. 2007. Towards robust unsuper-
vised personal name disambiguation. EMNLP.
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2).
J. Dean and S. Ghemawat. 2004. MapReduce: Simpli-
fied Data Processing on Large Clusters. OSDI.
T. Elsayed and J. Lin and D. W. Oard. 2008. Pair-
wise Document Similarity in Large Collections with
MapReduce. ACL/HLT.
Z. Li and S. Khudanpur. 2008. A Scalable Decoder for
Parsing-based Machine Translation with Equivalent
Language Model State Maintenance. ACL SSST.
L. Lloyd and Andrew Mehler and Steven Skiena. 2006.
Identifying Co-referential Names Across Large Cor-
pora. Combinatorial Pattern Matching.
NIST. 2008. Automatic Content Extraction 2008 Eval-
uation Plan (ACE08).
360
Named Entity Recognition using Hundreds of Thousands of Features
James Mayfield and Paul McNamee and Christine Piatko
The Johns Hopkins University Applied Physics Laboratory
11100 Johns Hopkins Road, Laurel, Maryland 20723-6099 USA
{mayfield,mcnamee,piatko}@jhuapl.edu
Abstract
We present an approach to named entity recog-
nition that uses support vector machines to cap-
ture transition probabilities in a lattice. The
support vector machines are trained with hun-
dreds of thousands of features drawn from the
CoNLL-2003 Shared Task training data. Mar-
gin outputs are converted to estimated prob-
abilities using a simple static function. Per-
formance is evaluated using the CoNLL-2003
Shared Task test set; Test B results were F?=1
= 84.67 for English, and F?=1 = 69.96 for Ger-
man.
1 Introduction
Language independence is difficult to achieve in named
entity recognition (NER) because different languages ap-
pear to require different features. Most NER systems (or
taggers) are severely limited in the number of features
they may consider, because the computational expense of
handling large numbers of features is high, and because
the risk of overtraining increases with the number of fea-
tures. Thus, the feature set must be finely tuned to be
effective. Such constrained feature sets are naturally lan-
guage dependent.
Increasing the number of features that a tagger can han-
dle would ameliorate this problem, because the designer
could select many relatively simple features in lieu of a
few highly tuned features. Because support vector ma-
chines (SVMs) (Vapnik, 1995) can handle large numbers
of parameters efficiently while simultaneously limiting
overtraining, they are good candidates for application to
named entity recognition. This paper proposes a novel
way to use SVMs for named entity recognition called
SVM-Lattice, describes a large feature space that we used
on the CoNLL-2003 Shared Task (Tjong Kim Sang and
De Meulder, 2003), and presents results from that task.
2 Model
We are interested in a lattice-based approach to named
entity recognition. In this approach, each sentence is pro-
cessed individually. A lattice is built with one column
per word of the sentence (plus a start state). Each column
contains one vertex for each possible tag. Each vertex in
one column is connected by an edge to every vertex in the
next column that may legitimately follow it (some tran-
sitions, such as from I-LOC to B-PER are disallowed).
Given such a lattice, our task is first to assign probabili-
ties to each of the arcs, then to find the highest likelihood
path through the lattice based on those probabilities. This
path corresponds to the highest likelihood tagging of the
sentence.
Hidden Markov models break the probability calcula-
tions into two pieces: transition probabilities (the proba-
bility of moving from one vertex to another independent
of the word at the destination node), and emission proba-
bilities (the probability that a given word would be gener-
ated from a certain state independent of the path taken to
get to that state). These probability distributions are cal-
culated separately because the training data are typically
too sparse to support a reasonable maximum likelihood
estimate of the joint probability. However, there is no
reason that these two distributions could not be combined
given a suitable estimation technique.
A support vector machine is a binary classifier that uses
supervised training to predict whether a given vector is in
a target class. All SVM training and test data occupy
a single high-dimensional vector space. In its simplest
form, training an SVM amounts to finding the hyperplane
that separates the positive training samples from the neg-
ative samples by the largest possible margin. This hyper-
plane is then used to classify the test vectors; those that
lie on one side of the hyperplane are classified as mem-
bers of the positive class, while others are classified as
members of the negative class. In addition to the clas-
sification decision, the SVM also produces a margin for
each vector?its distance from the hyperplane.
SVMs have two useful properties for our purposes.
First, they can handle very high dimensional spaces, as
long as individual vectors are sparse (i.e., each vector has
extent along only a small subset of the dimensions). Sec-
ondly, SVMs are resistant to overtraining, because only
the training vectors that are closest to the hyperplane
(called support vectors) dictate the parameters for the hy-
perplane. So SVMs would seem to be ideal candidates
for estimating lattice probabilities.
Unfortunately, SVMs do not produce probabilities, but
rather margins. In fact, one of the reasons that SVMs
work so well is precisely because they do not attempt to
model the entire distribution of training points. To use
SVMs in a lattice approach, then, a mechanism is needed
to estimate probability of category membership given a
margin.
Platt (1999) suggests such a method. If the range of
possible margins is partitioned into bins, and positive and
negative training vectors are placed into these bins, each
bin will have a certain percentage of positive examples.
These percentages can be approximated by a sigmoid
function: P (y = 1 | f) = 1/(1 + exp(Ax + b)). Platt
gives a simple iterative method for estimating sigmoid
parameters A and B, given a set of training vectors and
their margins.
This approach can work well if a sufficient number of
positive training vectors are available. Unfortunately, in
the CoNLL-2003 shared task, many of the possible label
transitions have few exemplars. Two methods are avail-
able to handle insufficient training data: smoothing, and
guessing.
In the smoothing approach, linear interpolation is used
to combine the model for the source to target pair that
lacks sufficient data with the model made from a com-
bination of all transitions going to the target label. For
example, we could smooth the probabilities derived for
the I-ORG to I-LOC transition with the probability that
any tag would transition to the I-LOC state at the same
point in the sentence.
The second approach is to guess at an appropriate
model without examining the training data. While in the-
ory this could prove to be a terrible approach, in practice
for the Shared Task, selection of fixed sigmoid parame-
ters works better than using Platt?s method to train the
parameters. Thus, we fix A = ?2 and b = 0. We con-
tinue to believe that Platt?s method or something like it
will ultimately lead to superior performance, but our cur-
rent experiments use this untrained model.
Our overall approach then is to use SVMs to estimate
lattice transition probabilities. First, due to the low fre-
quency of B-XXX tags in the training data, we convert
each B-XXX tags to the corresponding I-XXX tag; thus,
our system never predicts B-XXX tags. Then, we featur-
ize the training data, forming sparse vectors suitable for
input to our SVM package, SVMLight 5.00 (Joachims,
1999). Our feature set is described in the following sec-
tion. Next, we train one SVM for each transition type
seen in the training data. We used a cubic kernel for all
of our experiments; this kernel gives a consistent boost
over a linear kernel, while still training in a reasonable
amount of time. If we were to use Platt?s approach, the re-
sulting classifiers would be applied to further (preferably
held-out) training data to produce a set of margins, which
would be used to estimate appropriate sigmoid parame-
ters for each classifier. Sigmoid estimates that suffered
from too few positive input vectors would be replaced
by static estimates, and the sigmoids would optionally be
smoothed.
To evaluate a test set, the test input is featurized using
the same features as were used with the training data, re-
sulting in a separate vector for each word of the input.
Each classifier built during the training phase is then ap-
plied to each test vector to produce a margin. The margin
is mapped to a probability estimate using the static sig-
moid described above. When all of the probabilities have
been estimated and applied to the lattice, a Viterbi-like
algorithm is used to find the most likely path through the
lattice. This path identifies the final tag for each word of
the input sentence.
3 Features
The advantage of the ability to handle large numbers of
features is that we do not need to consider how well a
feature is likely to work in a particular language before
proposing it. We use the following features:
1. the word itself, both unchanged and lower-cased;
2. the character 3-grams and 4-grams that compose the
word;
3. the word?s capitalization pattern and digit pattern;
4. the inverse of the word?s length;
5. whether the word contains a dash;
6. whether the word is inside double quote marks;
7. the inverse of the word?s position in the sentence,
and of the position of that sentence in the document;
8. the POS, CHUNK and LEMMA features from the
training data;
9. whether the word is part of any entity, according
to a previous application of the TnT-Subcat tagger
(Brants, 2000) (see below) trained on the tag set {O,
I-ENTITY} (Test A F?=1 performance was 94.70
English and 74.33 German on this tag set); and
Run Description Test LOC MISC ORG PER Overall
1. Tnt Test A 86.67 79.60 73.04 88.54 82.90
Test B 81.28 68.98 65.71 82.84 75.54
2. Tnt + subcat Test A 91.46 81.41 80.63 91.64 87.49
Test B 85.71 68.41 73.82 87.95 80.68
3. SVM-Lattice Test A 92.14 84.86 83.70 93.73 89.63
Test B 87.09 72.81 78.84 90.40 83.92
4. SVM-Lattice+ Test A 93.75 86.02 85.90 93.91 90.85
Test B 88.77 74.19 79.00 90.67 84.67
Table 1: English evaluation results. F?=1 measures for subcategories, and overall.
Run Description Test LOC MISC ORG PER Overall
1. Tnt Test A 59.51 49.58 48.71 53.77 53.29
Test B 66.16 46.45 50.00 64.51 59.01
2. Tnt + subcat Test A 67.62 54.97 56.18 65.04 61.46
Test B 66.13 46.01 55.35 74.07 62.90
3. SVM-Lattice Test A 67.04 54.18 65.77 64.01 63.48
Test B 68.47 51.88 60.67 73.07 65.47
4. SVM-Lattice+ Test A 72.58 58.13 65.76 74.92 68.72
Test B 73.60 50.98 63.69 80.20 69.96
Table 2: German evaluation results. F?=1 measures for subcategories, and overall.
10. the maximum likelihood estimate, based on the
training data, of the word?s prior probability of being
in each class.
In some runs, we also use:
11. the tag assigned by a previous application of the
SVM-Lattice tagger, or by another tagger.
Each of these features is applied not just to the word
being featurized, but also to a range of words on either
side of it. We typically use a range of three (or, phrased
differently, a centered window of seven). We also ap-
plied some of these features to the environment of the
first occurrence of the word in the document. For ex-
ample, if the first occurrence of ?Bush? in the document
were followed by ?League,? then the second occurrence
of ?Bush? would receive the feature ?first-occurrence-is-
followed-by-league.?
Some values of the above features will be encountered
during testing but not during training. For example, a
word that occurs in the test set but not the training set will
lack a known value for the first feature in the list above.
To handle these cases, we assign any feature that appears
only once in the training data to a special ?never-before-
seen? class. This gives us examples at training time of
unseen features, which we can then train on.
Using the Shared Task English training data, this ap-
proach to featurization leads to a feature space of well
over 600,000 features, while the German data results in
over a million features. Individual vectors typically have
extent along a few hundred of these features.
There is a significant practical consideration in apply-
ing the method. The vectors produced by the featur-
izer for input to the SVM package are voluminous, lead-
ing to significant I/O costs, and slowing tag assignment.
Two methods might ameliorate this problem. First, sim-
ple compression techniques would be quite effective in
reducing file sizes, if the SVM package would support
them. Secondly, most vectors represent negative exam-
ples; a portion of these could probably be eliminated en-
tirely without significantly affecting system performance.
We have done no tuning of our feature set, preferring
to spend our time adding new features and relying on the
SVMs to ignore useless features. This is advantageous
when applying the technique to a language that we do
not understand (such as any of the world?s various non-
English languages).
4 Results
We evaluated our approach using the CoNLL-2003 En-
glish and German training and test sets, and the conll-
eval scoring software. We ran two baseline tests using
Thorsten Brants? TnT tagger (2000), and two tests of
SVM-Lattice:
1. TnT: The TnT tagger applied as distributed.
2. TnT+subcat: The TnT tagger applied to a refined
tag set. Each tag type was subcategorized into about
forty subtag types; each instance of a tag in the text
was then replaced by the appropriate subtag. For ex-
ample, a number (e.g., 221) that was part of a loca-
tion received an I-LOC-alldigits tag; a location with
an initial capital letter (e.g., Baker) received an I-
LOC-initcap tag; and one of the 30 most common
words (e.g., of) that was part of a location received a
(word-specific) I-LOC-of tag. This run served both
to calibrate the SVM-Lattice performance scores,
and to provide input for the SVM-Lattice+ run be-
low.
3. SVM-Lattice: Features 1-10 (listed above in the
Features section)
4. SVM-Lattice+: Features 1-11, using the output of
runs SVM-Lattice and TnT+subcat as input fea-
tures.
Scores for each English test are shown in Table 1; Ger-
man tests are shown in Table 2. Table 3 shows the re-
sults of the SVM-Lattice+ run in more detail. The results
show that the technique performs well, at least compared
with the baseline technique provided with the CoNLL-
2003 data (whose English Test B F?=1 measure is 59.61
English and 30.30 German).
5 Conclusion
The SVM-Lattice approach appears to give good results
without language-specific tuning; it handily outperforms
the CoNLL-2003 Shared Task baseline, and beats a basic
HMM tagger as well. Use of SVMs allows the introduc-
tion of a large number of features. These features can
be introduced with little concern for dependency among
features, and without significant knowledge of the target
language. It is likely that our results reflect some degree
of overfitting, given the large number of parameters we
use; however, we suspect this effect is not large. Thus,
the SVM-Lattice technique is particularly well suited to
language-neutral entity recognition. We expect it will
also perform well on other tasks that can be cast as tag-
ging problems, such as part-of-speech tagging and syn-
tactic chunking.
Acknowledgments
Significant theoretical and implementation contributions
were made to this work by Claudia Pearce, for which we
are grateful.
We gratefully acknowledge the provision of the
Reuters Corpus Vol. 1: English language, 1996-08-20
to 1997-08-19 by Reuters Limited.
English devel. Precision Recall F?=1
LOC 94.42% 93.09% 93.75
MISC 88.80% 83.41% 86.02
ORG 85.24% 86.58% 85.90
PER 92.79% 95.06% 93.91
overall 90.97% 90.73% 90.85
English test Precision Recall F?=1
LOC 88.22% 89.33% 88.77
MISC 74.89% 73.50% 74.19
ORG 79.31% 78.69% 79.00
PER 89.71% 91.65% 90.67
overall 84.45% 84.90% 84.67
German devel. Precision Recall F?=1
LOC 72.77% 72.40% 72.58
MISC 71.00% 49.21% 58.13
ORG 72.57% 60.11% 65.76
PER 83.70% 67.81% 74.92
overall 75.48% 63.07% 68.72
German test Precision Recall F?=1
LOC 75.08% 72.17% 73.60
MISC 63.62% 42.54% 50.98
ORG 69.20% 58.99% 63.69
PER 86.53% 74.73% 80.20
overall 75.97% 64.82% 69.96
Table 3: Results for the development and test evaluations
for the English and German tasks.
References
Thorsten Brants. 2000. TnT-A statistical part-of-speech
tagger. In Proceedings of ANLP-2000. Seattle, Wash-
ington.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In C. Burges B. Scho?lkopf and
A. Smola, editors, Support Vector Learning. MIT
Press.
John C. Platt. 1999. Probabilistic Outputs for Sup-
port Vector Machines and Comparisons to Regular-
ized Likelihood Methods. In B. Scholkopf A. Smola,
P. Bartlett and D. Schuurmans, editors, Advances in
Large Margin Classifiers. MIT Press.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the CoNLL-2003 Shared Task: Language
Independent Named Entity Recognition. In Proceed-
ings of CoNLL-2003. Edmonton, Canada.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag.
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 585?595,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
We?re Not in Kansas Anymore: Detecting Domain Changes in Streams
??Mark Dredze and ??Tim Oates and ??Christine Piatko
?Human Language Technology Center of Excellence,
?Center for Language and Speech Processing,
?Applied Physics Lab
Johns Hopkins University
?University of Maryland, Baltimore County
mdredze@cs.jhu.edu, oates@umbc.edu, christine.piatko@jhuapl.edu
Abstract
Domain adaptation, the problem of adapting
a natural language processing system trained
in one domain to perform well in a differ-
ent domain, has received significant attention.
This paper addresses an important problem for
deployed systems that has received little at-
tention ? detecting when such adaptation is
needed by a system operating in the wild,
i.e., performing classification over a stream
of unlabeled examples. Our method uses A-
distance, a metric for detecting shifts in data
streams, combined with classification margins
to detect domain shifts. We empirically show
effective domain shift detection on a variety of
data sets and shift conditions.
1 Introduction
Consider a named entity recognition system trained
on newswire stories. Given annotated documents
containing sentences like ?Tony Hayward has faced
fresh criticism for taking time off to go sailing . . .?
we would like to learn a model that will allow us to
recognize that ?Obama? and ?BP? are named enti-
ties in a sentence like ?Obama summoned BP ex-
ecutives . . .?. When all of the documents come
from one data distribution, like newswire articles,
this tends to work well. However, the sentence
?OBAMA SUMMONED BP EXECUTIVES . . .?
from transcribed broadcast news, and others like it,
will probably lead to poor results because the fea-
tures it relies on have changed. For example, capi-
talization patterns are no longer a good indicator of
the presence of a named entity and appositives are
not indicated by punctuation. This problem of do-
main shift is a pervasive problem in NLP in which
any kind of model ? a parser, a POS tagger, a senti-
ment classifier ? is tested on data that do not match
the training data.
Given a model and a stream of unlabeled in-
stances, we are interested in automatically detecting
changes in the feature distribution that negatively
impact classification accuracy. For example, a senti-
ment classification model trained on book reviews
may heavily weight n-grams features like ?uplift-
ing? and ?page turner?. Those features may never
occur in reviews of kitchen appliances that get mixed
in at test time, and useful features in this new do-
main like ?efficient? and ?noisy compressor? will
have never been seen during training and therefore
not be in the model. Furthermore, we do not assume
labeled instances are available to help detect these
harmful changes. Other tasks related to changes
in data distributions, like detecting concept drift in
which the labeling function changes, may require la-
beled instances, but that is not the focus of this paper.
There is significant work on the related problem
of adapting a classifier for a known domain shift.
Versions of this problem include adapting using only
unlabeled target domain data (Blitzer et al, 2006;
Blitzer et al, 2007; Jiang and Zhai, 2007), adapt-
ing using a limited amount of target domain labeled
data (Daume?, 2007; Finkel and Manning, 2009), and
learning across multiple domains simultaneously in
an online setting (Dredze and Crammer, 2008b).
However, in practical settings, we do not know if
the data distribution will change, and certainly not
when. Additionally, we will not know to what do-
585
main the shift will happen. A discussion forum de-
voted to science fiction books may change over time
to focus more on fantasy and then narrow to discus-
sions of vampire fiction. Maybe this shift is harm-
less and it is possible to identify the sentiment of the
discussants with the original model with no loss in
accuracy. If not, we seek methods that detect this
shift and trigger the use of an adaptation method.
Our domain shift detection problem can be de-
composed into two subproblems: detecting distribu-
tional changes in streams of real numbers, and rep-
resenting a stream of examples as a stream of real
numbers informative for distribution change detec-
tion. We select the A-distance metric (Kifer et al,
2004) to solve the first subproblem since it has been
previously used in other domain adaptation work
(Blitzer et al, 2006; Blitzer et al, 2007). Our main
contribution is towards the second problem, repre-
senting examples as real numbers for this task. We
demonstrate that classification margins, which in-
corporate information about features that most im-
pact system accuracy, can effectively solve the sec-
ond subproblem. Furthermore, we show that the pre-
viously proposed Confidence Weighted learning al-
gorithm (Dredze et al, 2008) can provide a more
informative measure than a simple margin for this
task. Our experiments include evaluations on com-
monly used domain adaptation data and false change
scenarios, as well as comparisons to supervised de-
tection methods that observe label values, or have
knowledge of the target domain.
We begin with a description of our task and pre-
vious applications to language data. After describ-
ing the data used in this paper, we discuss the A-
distance metric and how it has previously been used
for adaptation. We then show that margin based
methods effectively capture information to detect
domain shifts, and propose an alternate way of gen-
erating informative margin values. Finally, we com-
pare our results to settings with supervised knowl-
edge, and close with a survey of related work.
2 Domain Shifts in Language Data
The study of domain shifts in language data has been
the purview of domain adaptation and transfer learn-
ing, which seek to adapt or transfer a model learned
on one source domain with labeled data to another
target domain with few or no labeled examples. For-
mally, errors from such transfers have two sources:
differences in feature distributions and changes to la-
beling functions (annotation standards) (Ben-David
et al, 2006; Ben-David et al, 2009). Empirical work
on NLP domain shifts has focused on the former.
For example, Blitzer et al (2007) learned correspon-
dences between features across domains and Jiang
and Zhai (2007) weighted source domain examples
by their similarity to the target distribution.
We continue in this tradition by making two as-
sumptions about our setting. First, a change in do-
main will be signaled by a change in the feature
distributions. That is, new words, phrases, syntac-
tic structures, etc. signal that the system has shifted
to a new domain. Second, while there may be a
change in the labeling function, i.e., features have a
different meaning in each domain, this will be a sec-
ondary concern. For example, both Daume? (2007)
and Dredze and Crammer (2008b) assume that do-
mains are more similar than different.
A similar problem to the one we consider is that
of concept drift, where a stream of examples are
labeled with a shifting labeling function (concept)
(Nishida and Yamauchi, 2007; Widmer and Kubat,
1996). While concept drift is similar there are two
important differences. First, concept drift can be
measured using a stream of labeled examples, so
system accuracy is directly measured. For exam-
ple, Klinkenberg and Joachims (2000) detect con-
cept drift with support vector machines, using es-
timates of leave-one-out performance to adaptively
adjust and maintain a training window that mini-
mizes estimated generalization error. This is pos-
sible only because class labels arrive with the exam-
ples in the stream. Another concept drift detection
algorithm, STEPD, uses a statistical test to continu-
ally monitor the possibly changing stream, measur-
ing system accuracy directly, again using the labels
it receives for each example (Nishida, 2008). Ob-
viously, no such labels are available in our unsuper-
vised setting. Second, concept drift assumes only
changes in the labeling function, whereas domain
adaptation relies on feature distribution changes.
Several properties of detecting domain shifts in
natural language streams distinguish it from tradi-
tional domain adaptation, concept drift, and other
related tasks:
586
? No Target Distribution Examples Blitzer et
al. (2007) estimate the loss in accuracy from
domain shift by discriminating between two
data distributions. In our setting, we have no
knowledge of the target distribution.
? No Labeled Target Data Some approaches to
domain adaptation assume a limited number of
labeled examples (Daume?, 2007; Dredze and
Crammer, 2008b; Finkel and Manning, 2009).
We assume no labels in our setting.
? Online Setting Domain adaptation typically
assumes a batch transfer between two domains.
We consider a purely stream (online) setting.
? Computationally Constrained Our approach
must be fast, as we expect to run our domain
shift detector alongside a deployed NLP sys-
tem. This limits both computation and storage.
? Unknown Adaptation A critical assumption
of previous work is that a domain change has
occurred. We must ascertain this ourselves.
Despite these challenges, we show unsupervised
stream-based methods that effectively identify shifts
in domain in language data. Furthermore, our meth-
ods are tied directly to the learning task so are sen-
sitive to changes in actual task accuracy. Our meth-
ods have low false positive rates of change detection,
which is important since examples within a single
domain display a large amount of variance, which
could be mistaken for a domain change.
Once a change is detected, any number of actions
may be appropriate. The maintainer of the system
may be notified that performance is suffering, la-
bels can be obtained for a sample of instances from
the stream for retraining, or large volumes of unla-
beled instances can be used for instance reweighting
(Jiang and Zhai, 2007).
3 Datasets
We begin the presentation of our methods by de-
scribing the data used in our experiments. We se-
lected three data sets commonly used in domain
adaptation: spam (Jiang and Zhai, 2007), ACE 2005
named entity recognition (Jiang and Zhai, 2007),
and sentiment (Blitzer et al, 2007). Sentiment and
spam are binary and ACE is multi-class. Note that
in all experiments, a shift in the domain yields a de-
crease in system accuracy.
The goal of the spam data is to classify an email
(bag-of-words) as either spam or ham (not-spam).
Each email user may have different preferences and
features. We used unigram and bigram features, fol-
lowing Dredze and Crammer (2008b) for feature ex-
traction, and used the three task A users as three
domains. The ACE 2005 named entity recognition
dataset includes 7 named entity class labels (person,
organization, location, geopolitical entity, facility,
vehicle, weapon) for 5 text genres (newswire, broad-
cast news, broadcast conversations, conversational
telephone speech, weblogs). We use 4000 examples
from each genre and used Jiang and Zhai?s feature-
extracted data.1 The sentiment data contains reviews
from Amazon for four product types: books, dvds,
electronics, and kitchen. We include an additional
two types (music and video from Dredze and Cram-
mer) in our false shift experiments and use unigram
and bigram features, following Blitzer et al
4 The A-Distance
Our approach to detecting domain shifts in data
streams that negatively impact system accuracy is
based on the ability to (1) detect distributional
changes in streams of real numbers and (2) con-
vert document streams to streams of informative real
numbers. This section describes how we achieve the
former, and the next section describes the latter.
Theoretical work on domain adaptation showed
that the A-distance (Kifer et al, 2004), a stream
based measure of difference between two arbitrary
probability distributions P and P ?, can be used to
evaluate the difference between two domain distri-
butions (Ben-David et al, 2006). In a batch set-
ting this corresponds to learning a linear classi-
fier to discriminate the domains, and Blitzer et al
(2007) showed correlations with the error from do-
main adaptation. Given our interest in streaming
data we return to the original stream formulation of
A-distance.
The A-distance detects differences between two
arbitrary probability distributions by dividing the
range of a random variable into a set of (possibly
1We thank Jing Jiang for the feature-extracted ACE data.
587
Figure 1: The A-distance is computed between two win-
dows (P and P ? in a stream of real-valued data. The sam-
ples in each window are divided into intervals, and the
A-distance measures the change in the distributions over
these intervals between the two windows.
overlapping) intervals, and then measures changes
in the probability that a value drawn for that variable
falls into any one of the intervals. If such a change is
large, a change in the underlying distribution is de-
clared. LetA be a set of real intervals and letA ? A
be one such interval. For that interval, P (A) is the
probability that a value drawn from some unknown
distribution falls in A. The A-distance between P
and P ?, i.e. the difference between two distributions
over the intervals, is defined as follows:
dA(P, P
?) = 2 sup A?A|P (A)? P
?(A)|.
Two distributions are said to be different when, for
a user-specified threshold , dA(P, P ?) > . The
A-distance is distribution independent. That is, it
makes no assumptions about the form of the under-
lying distribution nor about the form of the change
that might occur, either algorithmically or in the un-
derlying theory. Unlike the L1 norm, theA-distance
can be shown to require finitely many samples to de-
tect distribution differences, a property that is crucial
for streaming, sample-based approaches.
Since the A-distance processes a stream of real
numbers, we need to represent an example using a
real number, such as the classification margin for
that example. The first n of these numbers in the
stream are a sample from P , and the most recent
n are a sample from P ?. We signal a domain shift
when the A-distance between P and P ? is large
(greater than ). Larger values of n result in more
accurate estimates of P (A) and slower detection of
changes.
The two windows of samples of size n are shown
graphically in Fig. 1. Each increment on the hori-
zontal axis represents the arrival of a new document.
The vertical axis is some value computed from each
document, such as its classification margin. To com-
pute P and P ?, one needs to specifyA and n, which
are shown as two stacks of boxes that are identical
except for their position. The width of each box is
n, the number of examples used to estimate P (A)
and P (A?) for A ? A, where the real interval A cor-
responds to the vertical span of the box. The value
P (A) is simply the number of documents whose real
value falls inside that interval A divided by n. Note
that the first n documents in the stream are used to
compute P , and as each new document arrives the
location of the stack of boxes used to compute P ? is
shifted to the right by one.
In Fig. 1, the number of examples whose real
value falls in the top two intervals for P is approxi-
mately the same, with no example?s value falling in
the lower two intervals. For P ?, almost every one
of the n document values falls in the second interval
from the top, virtually assuring that dA(P, P ?) will
be large. Though the intervals in the figure do not
overlap, they typically do.
Given n and intervals A, the value of  is chosen
by randomization testing. Because theA-distance is
distribution independent, a sample of size m n is
drawn from any distribution that spansA. This sam-
ple is treated as a stream as described above, and
the largest value of dA(P, P ?) is stored. The sam-
ple is permuted and this process is repeated l times.
Note that any change detection would be a false pos-
itive because all values were sampled from the same
distribution. The values dA(P, P ?) are sorted from
largest to smallest, and  is chosen to be the b?lcth
such value where parameter ? is a user specified
false positive probability.
Both the time and space complexity of our ap-
proach based on the A-distance are small. Given
n and A, n instances must be stored in the sliding
window and 2|A| counters are required to represent
P and P ?. Note that both values are constants based
on user specified parameters, not on the size of the
stream. Processing a new instance involves comput-
ing its margin and updating P and P ?, all of which
can occur in constant time.
588
Figure 2: Each column of plots is a representative result using an SVM on a single run over a sentiment data shift:
dvds? electronics, electronics? books, and kitchen? books, from left to right. The horizontal axis is the number
of instances from the stream processed by the classifier. The top plot is the accuracy of the classifier on the last 100
instances. The bottom plot is the absolute value of the SVM classification margin. The vertical line at 500 instances
marks the point of domain shift. Horizontal dotted lines indicate the mean of the accuracy/margin before and after the
domain shift. Note that in all cases, the mean accuracy drops, as do the mean margin values, demonstrating that both
can indicate domain shifts.
5 A-Distance Over Margins
Since shifts in domains correlate with changes in
distributions, it is natural to begin by considering the
observed features in each example. When we shift
from a source domain (e.g., book reviews) to a target
domain (e.g., dvd reviews) we expect a change in the
distribution for common source words (?author? and
?plot? become less common). Since the A-distance
assumes a stream of single values, we can apply an
A-distance detector to each feature (e.g., unigram
and bigram count) individually. However, our exten-
sive experiments with this approach (omitted here)
show that it suffers from a number of flaws, such as
a high false positive rate if all features are tracked,
the difficult problem of identifying an informative
subset of features for tracking, and deciding how
many such features need to change before a shift has
occurred, which turns out to be highly variable be-
tween shifts.
Therefore, our goal is to use a single A-distance
tracker by collapsing each example to a single value.
One way of doing this is to consider the classifica-
tion margin produced by the classifier. The mar-
gin weighs features by their importance in classi-
fication. When more important features disappear,
we expect the magnitude of the margin to decrease.
Additionally, features that change but do not in-
fluence system performance are effectively ignored
since they do not influence the margin. This ap-
proach has the advantage of task sensitivity, only
tracking changes that impact task accuracy. Initial
experiments showed effectiveness with the unsigned
(absolute value of the) margin, which we use in all
experiments.
We begin by examining visually the information
content of the margin with regards to predicting a
domain shift. The caption of Fig. 2 describes the
setup, and the first row of the figure illustrates the
effects of the shift on the source domain classifier?s
empirical accuracy, measured on a window of the
previous 100 examples. The horizontal dashed lines
indicate the average accuracy before and after the
shift. Note that in each case, average classification
accuracy drops after the shift. However, at any one
point the accuracy displays considerable variance.
Thus, while classification accuracy clearly suffers, it
is difficult to measure this even in a supervised set-
ting with labeled examples when considering a small
portion of the stream.
The second row of Fig. 2 shows the average un-
signed margin value of an SVM classifier computed
over the previous 100 examples in the stream. The
two dashed horizontal lines indicate the average
margin value over source and target examples. There
is a clear drop in the average margin value after the
shift. This difference suggests that the margin can
be examined directly to detect a domain shift. How-
ever, these values vary considerably so extracting
useful information is not trivial.
We evaluated the ability of A-distance trackers to
detect such changes in margin values by simulat-
ing domain shifts using each domain pair in a task
(books to dvds, weblogs to newswire, etc.). For
each domain shift setting, we first trained a classi-
fier on 1000 source domain instances. In our ex-
periments, we used three different classification al-
gorithms: Support Vector Machines (SVM) (Chang
589
0 200 400 600 800 1000 1200 1400CWPM0
200
400
600
800
1000
1200
1400
SVM
spamace2005sentiment 0 200 400 600 800 1000 1200 1400CWPM0
200
400
600
800
1000
1200
1400
MIRA
0 200 400 600 800 1000 1200CWPM0
200
400
600
800
1000
1200
CW
0 50 100 150 200 250 300CWPM0
50
100
150
200
250
300
SVM
0 50 100 150 200 250 300CWPM0
50
100
150
200
250
300
MIRA
0 50 100 150 200 250 300CWPM0
50
100
150
200
250
300
CW
Figure 3: The mean number of instances after a domain change at which theA-distance tracker detects a change. Each
point represents the mean number of instances for CWPM (x-axis) and the SVM, MIRA and CW methods (y-axis).
Datasets are indicated by different markers. The second row zooms each plot to the bottom left corner of the first row.
Points above the diagonal indicate SVM, MIRA or CW took longer to detect a change than CWPM.
and Lin, 2001), MIRA (Crammer et al, 2006) and
Confidence Weighted (CW) learning (Dredze et al,
2008). We evaluated each trained classifier on 500
test examples to measure accuracy on the source do-
main, and then used it to label examples in a stream.
The first 500 examples in the stream were used for
calibrating our change detection methods. The next
500 examples were from the source domain, fol-
lowed by 1500 examples from the target domain.
Over these 2000 examples we ran each of our de-
tection methods. Experiments were repeated over
10 fixed random data permutations.
We automatically select A-distance intervals as
follows. First, we computed the mean and variance
of the 500 calibration margins and then added inter-
vals for .5 standard deviations away from the mean
in each direction, .5 to 1 standard deviation in each
direction, and intervals for 1 standard deviation to
??. We also added three evenly spaced overlap-
ping intervals. To calibrate a FP rate of 0.05 we
sampled from a Gaussian with the above mean and
variance and used n = 200, m = 10000 and l = 50.
The results for each experiment (38 shifts re-
peated averaged over 10 runs each) are shown in
Fig. 3.2 Each plot represents one of the three classi-
fiers (SVM, MIRA, CW) plotted on the vertical axis,
where each point?s y-value indicates the number of
examples observed after a shift occurred before the
A-distance detector registered a change. Smaller
values (lower points) are preferred. The second row
of plots highlights the 0 to 300 region of the first
row. (The x-axis will be discussed in the next sec-
tion.) Notice that in many cases, a change was reg-
istered within 300 examples, showing that domain
shifts can be reasonably detected using the margin
values alone.
Equally important to detecting changes is robust-
ness to false changes. We evaluated the margin de-
tector for false positives in two ways. First, we
logged any incorrectly detected changes before the
shift. For all three algorithms, there were very few
false positives (Table 1). The highest false positive
rate was about 1% (CW), while for the SVM experi-
ments, not a single detector fired prematurely in any
experiment.
2The method plotted on the x-axis will be introduced in
the next section. To evaluate the three methods in this section
(SVM, MIRA, CW) compare the y-values.
590
Second, we sought to test the robustness of the
method over a long stream of examples where no
change occurred. In this experiment, we selected 11
domains that had a sufficient number of examples
to consider a long stream of source domain exam-
ples.3 Rather than use 500 source domain examples
followed by 1500 target domain examples, all 2000
examples were from the source domain. All other
settings were the same. For the SVM detector, out
of 110 runs we detected 6 false positives, 3 of which
were for the same data set (kitchen) (see Table 1.)
6 Confidence Weighted Margins
In the previous section, we showed that margin val-
ues could be used to detect domain shifts. We now
explore ways to reduce the number of target domain
examples needed to detect domain shift by improv-
ing the margin values.
Margin values are often taken as a measure of
prediction confidence. From this perspective, the
A-distance margin tracker identifies when predic-
tion confidence drops. Another task that relies on
margins as measures of confidence is active learn-
ing, where uncertainty sampling for margin based
systems is determined based on the magnitude of
the predicted margin. Dredze and Crammer (2008a)
showed how Confidence Weighted (CW) learning
could be used to generate a more informative mea-
sure of confidence for active learning.
CW is an online algorithm inspired by the MIRA
update (Crammer et al, 2006), which ensures a pos-
itive margin while minimizing parameter change.
CW replaces the Euclidean distance used in the
MIRA update with the KL divergence over Gaussian
distributions. CW learning maintains a Gaussian
distribution over linear weight vectors with mean
? ? RN and diagonal covariance ? ? RN?N .
Maintaining a distribution over prediction func-
tions is appropriate for our task where we con-
sider margin values as confidence. We re-
place the margin |w ? x|, where w is a stan-
dard linear classifier, with a probabilistic margin
|
(
Prw?N (?i,?i) [sign(w ? z) = 1]
)
?12 | .Dredze and
Crammer showed that this probabilistic margin can
be translated into a corrected geometric margin,
3ACE: bc, bn, cts, nw, wl; Sentiment: books, dvd, electron-
ics, kitchen, music, video
which is computed as the normalized margin as M? =
M/
?
V , whereM is the meanM = ? ?x and V the
variance V = x>?x of a univariate Gaussian dis-
tribution over the unsigned-margin M = w ? x. We
call this method CWPM, for Confidence Weighted
Probabilistic Margin.
We compared using CWPM to the standard mar-
gins produced by an SVM, MIRA and CW classifier
in the last section. Fig. 3 shows the results of these
comparisons. In each plot, CWPM (normalized mar-
gin) is plotted on the x-axis, indicating how many
examples from the target domain were observed be-
fore the detector identified a change. The y-axis in
each plot is the number of instances observed for
the SVM, MIRA and CW methods. As before, each
point is the average of the 10 randomized runs used
above (assuming that detectors that did not fire do so
at the end of the stream.) Points above the diagonal
indicate that CWPM detected a change sooner than
the comparative method. Of the 38 shifts, CWPM
detected domain shifts faster than an SVM 34 times,
MIRA 26 times and CW 27 times.
We repeated the experiments to detect false posi-
tives for each margin based method. Table 1 shows
the false positives for the 38 domain shifts consid-
ered as well as the 11 false shift domain shifts. The
false positive rates are among the lowest for CWPM.
This shows that CWPM is a more useful indicator
for detecting domain changes.
7 Gradual Shifts
We have shown detection of sudden shifts between
the source and target domains. However, some shifts
may happen gradually over time. We evaluate this
by modifying the stream as follows: the first 500
instances come from the source domain, and the re-
maining 1500 are sampled randomly from the source
and target domains. The probability of an instance
being drawn from the target domain at time i is
pi(x = target) = i1500 , where i counts from the start
of the shift at index 500. The probability of sam-
pling target domain data increases uniformly over
the stream. At index 750 after the start of the shift
each domain is equally likely. The ACE and Sen-
timent datasets had sufficient data to be evaluated
in this setting. Fig. 4 shows CWPM still performs
best, but results are close (SVM: 22 of 32, MIRA &
591
0 200 400 600 800 1000 1200 1400 1600CWPM0
200
400
600
800
1000
1200
1400
1600
SVM
ace2005sentiment 0 200 400 600 800 1000 1200 1400 1600CWPM0
200
400
600
800
1000
1200
1400
1600
MIRA
0 200 400 600 800 1000 1200 1400 1600CWPM0
200
400
600
800
1000
1200
1400
1600
CW
Figure 4: Gradual shift detection with SVM, MIRA or CW vs. CWPM. There were no false positives.
Domain Shift FPs
Algorithm True Shift False Shift
Sec. 5: SVM 0 6
Sec. 6: MIRA 2 13
Sec. 6: CW 5 10
Sec. 6: CWPM 1 6
Total tests 380 110
Table 1: False positives (FPs) observed in true domain
shift and false domain shift experiments for methods in
corresponding sections. Each setting was run 10 times,
resulting in 380 true domain shifts and 110 false shifts.
CW: 17 of 32). As expected, detections happen later
in the stream. The closer results are likely due to
the increased difficulty of the task. With less clear
information, there it is more difficult for all the al-
gorithms to recognize a change, and performance
across the methods begins to equalize. Even in this
more difficult setting, CWPM is the best performer.
8 Comparison to Supervised Information
So far we have considered applying A-distance
tracking to information freely available in a real
world system: the classification margins. As a use-
ful baseline for comparison, we can measure using
supervised sources of information, where additional
information is provided that is not normally avail-
able. In particular, we investigate two types of su-
pervised knowledge: the labels of examples in the
stream and knowledge of the target domain. In each
case, we compare using the A-distance and CWPM
versus applying the A-distance to supervised infor-
mation.
8.1 Classifier Accuracy
In Sec. 4 we showed that both the margin and recent
classifier accuracy indicate when shifts in domains
occur (Fig. 2). We developed techniques based on
the margin, which is available at test time. We now
consider knowledge of the true labels for these test
examples, which allows for tracking classifier accu-
racy. We can use the A-distance to detect when un-
expected changes in accuracy occur.
For each test example classified by the system,
we evaluated whether the system was correct in its
prediction by examining the label. If the classifier
was correct, we output a 1; otherwise, we output a
0. Over this 1/0 stream produced by checking clas-
sifier accuracy we ran an A-distance detector, with
intervals set for 1s and 0s (10,000 uniform samples
to calibrate the threshold for a false positive rate of
0.05.) If an unusual number of 0s or 1s occur ?
more or less mistakes than on the source domain ? a
change is detected.4 Results on this accuracy stream
are compared to CWPM (Fig. 5.) Despite this su-
pervised information, CWPM still detects domain
changes faster than with labeled examples. Consider
again Fig. 2, which shows both accuracy and margin
values over time. While the average accuracy drops,
the instantaneous value is very noisy, suggesting that
even this additional information may not yield bet-
ter domain shift detection. This will be interesting
to explore in future work.
4An alternate approach would be to measure accuracy di-
rectly as a real valued number. However, our experiments
showed the discrete approach to be more effective.
592
0 200 400 600 800 1000 1200 1400 1600CWPM0
200
400
600
800
1000
1200
1400
1600
Accu
racy 
Dete
ctor
0 50 100 150 200 250 300CWPM0
50
100
150
200
250
300
Accu
racy 
Dete
ctor
Figure 5: An A-distance accuracy detector, run over a stream of 1s and 0s indicating correct and incorrect predictions
of the classifier on examples in a stream. The bulk of points above the line indicate that CWPM is more effective at
detecting domain change. CWPM had a single false positive and the accuracy detector had no false positives.
8.2 Domain Classification
Next, we consider another source of supervision:
a selection of examples known to be from the tar-
get domain. In this setting, we know that a shift
will occur and we know to which domain it will oc-
cur. This requires a sample of (unlabeled) target do-
main examples when the target domain is not known
ahead of time. Using a common approach to detect-
ing domain differences when data is available from
both domains (Ben-David et al, 2009; Blitzer et al,
2007; Rai et al, 2010), we train a binary classifier
to differentiate between the source and target do-
main. We learn a CW classifier on 1000 examples
(500 from each domain) that do not appear in the
test stream. We then label each example as either
?source? or ?target? and output a 1 or 0 accordingly.
Over this 0/1 stream, we run an A-distance detector
with two intervals, one for 1s and one for 0s. The
remaining setup is identical to theA-distance exper-
iments above.
Fig. 6 shows the detection rate of CWPM versus
A-distance over the domain classifier stream. As ex-
pected, the detection rate for the domain classifier
is very fast, in almost every case (save 1) less than
400 examples after the shift happens. When CWPM
is slow to detect a change (over 400 examples), the
domain classifier is the clear winner. However, in
the majority of experiments, especially for ACE and
spam data, both detectors register a change quickly.
These results suggest that while a sample of target
domain examples is very helpful, our CWPM ap-
proach can also be effective when such samples are
not available.
9 Related Work
Early NLP work in the unsupervised setting moni-
tored classification confidence values, setting a con-
fidence threshold based on a break-even heuristic,
monitoring the rate of (presumed) irrelevant exam-
ples based on this threshold, and signaling a change
when this rate increased (Lanquillon, 1999).
Confidence estimation has been used for specific
NLP components such as information extraction.
The correctness of fields extracted via a conditional
random field extractor has been shown to corre-
late well to an estimate obtained by a constrained
forward-backward technique (Culotta and McCal-
lum, 2004). EM-based confidence estimation has
been used to estimate the confidence of patterns
derived from partially supervised relation extrac-
tion (Agichtein, 2006). Confidence estimation has
also been used to improve the overall effectiveness
of NLP systems. Confidence estimates obtained via
neural networks have shown gains for speech recog-
nition, spoken language understanding, and machine
translation (Gandrabur et al, 2006). Pipeline models
using confidence estimates at one stage as weights
for further downstream stages improve over base-
line dependency parsing and named entity recogni-
tion pipeline models (Bunescu, 2008).
An alternative formulation of domain adaptation
trains on different corpora from many different do-
mains, then uses linear combinations of models
trained on the different corpora(McClosky et al,
2010).
Work in novelty detection is relevant to the task
of detecting domain shifts (Scholkopf et al, 2000),
593
0 200 400 600 800 1000 1200CWPM0
200
400
600
800
1000
1200
Dom
ain C
lassif
ier
0 50 100 150 200 250 300CWPM0
50
100
150
200
250
300
Dom
ain C
lassif
ier
Figure 6: A-distance over a stream of 1s and 0s produced by a supervised classifier trained to differentiate between
the source and target domain. Samples from the unseen target domain is very effective. However, for many shifts, the
margin based A-distance detector is still competitive. CWPM had a single false positive while the domain classifier
stream had 2 false positives in these experiments.
though the rate of occurrence of novel instances is
more informative in our setting than the mere fact
that novel instances are observed.
We are also motivated by the problem of detect-
ing genre shift in addition to domain shift, as in the
ACE 2005 data set shifts from newswire to tran-
scripts and blogs. Different text genres occur in tra-
ditional settings, such as broadcast news transcripts
and newswire, and have begun to proliferate with
the variety of social media technologies now avail-
able including weblogs. Static genre classification
has been explored using a variety of techniques, in-
cluding exploiting punctuation (Kessler et al, 1997;
Dewdney et al, 2001), TF-IDF statistics (Lee and
Myaeng, 2002), and part-of-speech statistics and
histograms (Finn and Kushmerick, 2006; Feldman
et al, 2009).
Finally, statistical estimation in a streaming con-
text has been considered in data mining applica-
tions (Muthukrishnan, 2005). Change detection
via sequential hypothesis testing has been effective
for streaming applications such as network intrusion
detection (Muthukrishnan et al, 2007). Detecting
new events in a stream of Twitter posts can be done
using constant time and space similarity measures
based on a modification of locality sensitive hash-
ing (Petrovic? et al, 2010).
10 Conclusion
While there are a number of methods for domain
adaptation, a system first needs to determine that a
domain shift has occurred. We have presented meth-
ods for automatically detecting such domain shifts
from a stream of (unlabeled) examples that require
limited computation and memory by virtue of op-
erating on fixed-size windows of data. Our meth-
ods were evaluated empirically on a variety of do-
main shifts using NLP data sets and are shown to
be sensitive to shifts while maintaining a low rate of
false positives. Additionally, we showed improved
detection results using a probabilistic margin based
on Confidence Weighted learning. Comparisons to
detection with supervised information show that our
results are effective even in unlabeled settings. Our
methods are promising as tools to accompany the de-
ployment of domain adaptation algorithms, so that a
complete system can first identify when a domain
shift has occurred before automatically adapting to
the new domain.
Acknowledgments
Thanks to the HLTCOE text processing group for
many helpful discussions.
References
Eugene Agichtein. 2006. Confidence estimation meth-
ods for partially supervised information extraction. In
SDM.
Shai Ben-David, John Blitzer, Koby Crammer, and Fer-
nando Pereira. 2006. Analysis of representations for
domain adaptation. In NIPS.
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Vaughan.
2009. A theory of learning from different domains.
Machine Learning.
594
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, Bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In As-
sociation for Computational Linguistics (ACL).
Razvan C. Bunescu. 2008. Learning with probabilistic
features for improved pipeline models. In EMNLP.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/?cjlin/libsvm.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research (JMLR).
Aron Culotta and Andrew McCallum. 2004. Confidence
estimation for information extraction. In North Amer-
ican Chapter of the Association for Computational
Linguistics - Human Language Technologies (NAACL-
HLT).
Hal Daume?. 2007. Frustratingly easy domain adaptation.
In Association for Computational Linguistics (ACL).
Nigel Dewdney, Carol VanEss-Dykema, and Richard
MacMillan. 2001. The form is the substance: clas-
sification of genres in text. In Workshop on Human
Language Technology and Knowledge Management.
Mark Dredze and Koby Crammer. 2008a. Active learn-
ing with confidence. In Association for Computational
Linguistics (ACL).
Mark Dredze and Koby Crammer. 2008b. Online meth-
ods for multi-domain learning and adaptation. In
EMNLP.
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification. In
ICML.
S. Feldman, M. A. Marin, M. Ostendorf, and M. R.
Gupta. 2009. Part-of-speech histograms for genre
classification of text. In International Conference on
Acoustics, Speech, and Signal Processing (ICASSP).
Jenny Rose Finkel and Christopher D. Manning. 2009.
Hierarchical Bayesian domain adaptation. In NAACL-
HLT.
Aidan Finn and Nicholas Kushmerick. 2006. Learning to
classify documents according to genre: Special topic
section on computational analysis of style. J. Am. Soc.
Inf. Sci. Technol., 57(11):1506?1518.
Simona Gandrabur, George Foster, and Guy Lapalme.
2006. Confidence estimation for NLP applications.
ACM Trans. Speech Lang. Process., 3(3):1?29.
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in NLP. In Association for
Computational Linguistics (ACL).
Brett Kessler, Geoffrey Numberg, and Hinrich Schu?tze.
1997. Automatic detection of text genre. In Associa-
tion for Computational Linguistics (ACL).
Daniel Kifer, Shai Ben-David, and Johannes Gehrke.
2004. Detecting change in data streams. In Very Large
Data Bases (VLDB).
Ralf Klinkenberg and Thorsten Joachims. 2000. Detect-
ing concept drift with support vector machines. In In-
ternational Conference on Machine Learning (ICML).
C. Lanquillon. 1999. Information filtering in changing
domains. In IJCAI.
Yong-Bae Lee and Sung Hyon Myaeng. 2002. Text
genre classification with genre-revealing and subject-
revealing features. In SIGIR.
David McClosky, Eugene Charniak, and Mark Johnson.
2010. Automatic domain adaptation for parsing. In
NAACL-HLT, pages 28?36, Los Angeles, California,
June. Association for Computational Linguistics.
S. Muthukrishnan, Eric van den Berg, and Yihua Wu.
2007. Sequential change detection on data streams. In
IEEE International Conference on Data Mining Work-
shops (ICDMW).
S. Muthukrishnan. 2005. Data streams: Algorithms and
applications. Foundations and Trends in Theoretical
Computer Science, 1(2).
Kyosuke Nishida and Koichiro Yamauchi. 2007. Detect-
ing concept drift using statistical testing. In Discovery
Science.
Kyosuke Nishida. 2008. Learning and Detecting Con-
cept Drift. Ph.D. thesis, Hokkaido University, Japan.
Sas?a Petrovic?, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with application
to twitter. In NAACL-HLT, pages 181?189, June.
P. Rai, A. Saha, H. Daume? III, and S. Venkatasubrama-
nian. 2010. Domain Adaptation meets Active Learn-
ing. In Workshop on Active Learning for Natural Lan-
guage Processing (ALNLP), page 27.
Bernhard Scholkopf, Robert Williamson, Alex Smola,
John Shawe-Taylor, and John Platt. 2000. Support
vector method for novelty detection. In NIPS.
Gerhard Widmer and Miroslav Kubat. 1996. Learning
in the presence of concept drift and hidden contexts.
Machine Learning, 23:69?101.
595
Use of Modality and Negation in
Semantically-Informed Syntactic MT
Kathryn Baker?
U.S. Department of Defense
Michael Bloodgood??
University of Maryland
Bonnie J. Dorr?
University of Maryland
Chris Callison-Burch?
Johns Hopkins University
Nathaniel W. Filardo?
Johns Hopkins University
Christine Piatko?
Johns Hopkins University
Lori Levin||
Carnegie Mellon University
Scott Miller#
BBN Technologies
? U.S. Department of Defense, 9800 Savage Rd., Suite 6811, Fort Meade, MD 20755.
E-mail: kathrynlb@gmail.com.
?? Center for Advanced Study of Language, University of Maryland, 7005 52nd Avenue, College Park, MD
20742. E-mail: meb@umd.edu.
? Department of Computer Science and UMIACS, University of Maryland, AV Williams Building 3153,
College Park, MD 20742. E-mail: bonnie@umiacs.umd.edu.
? Center for Language and Speech Processing, Johns Hopkins University, 3400 N. Charles Street,
Hackerman Hall 320, Baltimore MD 21218. E-mail: {ccb,nwf}@cs.jhu.edu.
? Applied Physics Laboratory, Johns Hopkins University, 11000 Johns Hopkins Rd., Laurel, MD 20723.
E-mail: christine.piatko@jhuapl.edu.
|| Carnegie Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213.
E-mail: lsl@cs.cmu.edu.
# BNN Technologies, 10 Moulton Street, Cambridge, MA 02138. E-mail: smiller@bbn.com.
Submission received: 27 March 2011; revised submission received: 28 September 2011; accepted for
publication: 30 November 2011.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 2
This article describes the resource- and system-building efforts of an 8-week Johns Hopkins
University Human Language Technology Center of Excellence Summer Camp for Applied Lan-
guage Exploration (SCALE-2009) on Semantically Informed Machine Translation (SIMT). We
describe a new modality/negation (MN) annotation scheme, the creation of a (publicly available)
MN lexicon, and two automated MN taggers that we built using the annotation scheme and
lexicon. Our annotation scheme isolates three components of modality and negation: a trigger
(a word that conveys modality or negation), a target (an action associated with modality or
negation), and a holder (an experiencer of modality). We describe how our MN lexicon was
semi-automatically produced and we demonstrate that a structure-based MN tagger results in
precision around 86% (depending on genre) for tagging of a standard LDC data set.
We apply our MN annotation scheme to statistical machine translation using a syntactic
framework that supports the inclusion of semantic annotations. Syntactic tags enriched with
semantic annotations are assigned to parse trees in the target-language training texts through
a process of tree grafting. Although the focus of our work is modality and negation, the tree
grafting procedure is general and supports other types of semantic information. We exploit this
capability by including named entities, produced by a pre-existing tagger, in addition to the MN
elements produced by the taggers described here. The resulting system significantly outperformed
a linguistically naive baseline model (Hiero), and reached the highest scores yet reported on the
NIST 2009 Urdu?English test set. This finding supports the hypothesis that both syntactic and
semantic information can improve translation quality.
1. Introduction
This article describes the resource- and system-building efforts of an 8-week Johns
Hopkins Human Language Technology Center of Excellence Summer Camp for Ap-
plied Language Exploration (SCALE-2009) on Semantically InformedMachine Translation
(SIMT) (Baker et al 2010a, 2010b, 2010c, 2010d). Specifically, we describe our modal-
ity/negation (MN) annotation scheme, a (publicly available) MN lexicon, and two
automated MN taggers that were built using the lexicon and annotation scheme.
Our annotation scheme isolates three components of modality and negation: a
trigger (a word that conveys modality or negation), a target (an action associated with
modality or negation), and a holder (an experiencer of modality). Two examples of MN
tagging are shown in Figure 1.
Note that modality and negation are unified into single MN tags (e.g., the ?Able?
modality tag is combined with ?NOT? to form the ?NOTAble? tag) and also that
Figure 1
Modality/negation tagging examples.
412
Baker et al Modality and Negation in SIMT
MN tags occur in pairs of triggers (e.g., TrigAble and TrigNegation) and targets (e.g.,
TargNOTAble).
We apply our modality and negation mechanism to the problem of Urdu?English
machine translation using a technique that we call tree grafting. This technique incorpo-
rates syntactic labels and semantic annotations in a unified and coherent framework for
implementing semantically informed machine translation. Our framework is not lim-
ited to the semantic annotations produced by the MN taggers that are the subject of this
article and we exploit this capability to additionally include named-entity annotations
produced by a pre-existing tagger. By augmenting hierarchical phrase-based translation
rules with syntactic labels that were extracted from a parsed parallel corpus, and further
augmenting the parse trees with markers for modality, negation, and entities (through
the tree grafting process), we produced a better model for translating Urdu and English.
The resulting system significantly outperformed the linguistically naive baseline Hiero
model, and reached the highest scores yet reported on the NIST 2009 Urdu?English
translation task.
We note that although our largest gains were from syntactic enrichments to the
model, smaller (but significant) gains were achieved by injecting semantic knowledge
into the syntactic paradigm. Verbal semantics (modality and negation) contributed
slightly more gains than nominal semantics (named entities) and their combined gains
were the sum of their individual contributions.
Of course, the limited semantic types we explored (modality, negation, and en-
tities) are only a small piece of the much larger semantic space, but demonstrating
success on these semantic aspects of language, the combination of which has been
unexplored by the statistical machine translation community, bodes well for (larger)
improvements based on the incorporation of other semantic aspects (e.g., relations and
temporal knowledge). Moreover, we believe this syntactic framework to be well suited
for further exploration of the impact of many different types of semantics on the quality
of machine-translation (MT) output. Indeed, it would not have been possible to initiate
the current study without the foundational work that gave rise to a syntactic paradigm
that could support these semantic enrichments.
In the SIMT paradigm, semantic elements (e.g., modality/negation) are identified
in the English portion of a parallel training corpus and projected to the source language
(in our case, Urdu) during a process of syntactic alignment. These semantic elements are
subsequently used in the translation rules that are extracted from the parallel corpus.
The goal of adding them to the translation rules is to constrain the space of possible
translations to more grammatical and more semantically coherent output. We explored
whether including such semantic elements could improve translation output in the face
of sparse training data and few source language annotations. Results were encouraging.
Translation quality, as measured by the Bleu metric (Papineni et al 2002), improved
when the training process for the Joshua machine translation system (Li et al 2009)
used in the SCALE workshop included MN annotation.
We were particularly interested in identifying modalities and negation because
they can be used to characterize events in a variety of automated analytic processes.
Modalities and negation can distinguish realized events from unrealized events, beliefs
from certainties, and can distinguish positive and negative instances of entities and
events. For example, the correct identification and retention of negation in a particular
language?such as a single instance of the word ?not??is very important for a correct
representation of events and likewise for translation.
The next two sections examine related work and the motivation behind the SIMT
approach. Section 4 defines the theoretical framework for ourMN lexicon and automatic
413
Computational Linguistics Volume 38, Number 2
MN taggers. Section 5 presents the MN annotation scheme used by our human annota-
tors and describes the creation of a MN lexicon based on this scheme. Section 6 presents
two types of MN taggers?one that is string-based and one that is structure-based?
and evaluates the effectiveness of the structure-based tagger. Section 7 then presents
implementation details of the semantically informed syntactic system and describes
the results of its application. Finally, Section 8 presents conclusions and future work.
2. Related Work
The development of annotation schemes has become an area of computational lin-
guistics development in its own right, often separate from machine learning applica-
tions. Many projects began as strictly linguistic projects that were later adapted for
computational linguistics. When an annotation scheme is consistent and well devel-
oped, its subsequent application to NLP systems is most effective. For example, the
syntactic annotation of parse trees in the Penn Treebank (Marcus, Marcinkiewicz, and
Santorini 1993) had a tremendous effect on parsing and onNatural Language Processing
in general.
In the case of semantic annotations, each tends to have its unique area of focus.
Although the labeling conventions may differ, a layer of modality annotation over
verb role annotation, for example, can have a complementary effect of providing more
information, rather than being viewed as a competing scheme. We review some of the
major semantic annotation efforts here.
Propbank (Palmer, Gildea, and Kingsbury 2005) is a set of annotations of predicate?
argument structure over parse trees. First annotated as an overlay to the Penn
Treebank, Propbank annotation now exists for other corpora. Propbank annotation aims
to answer the question Who did what to whom? for individual predicates. It is tightly
coupled with the behavior of individual verbs. FrameNet (Baker, Fillmore, and Lowe
1998), a frame-based lexical database that associates each word in the database with
a semantic frame and semantic roles, is also associated with annotations at the lexical
level.WordNet (Fellbaum 1998) is a verywidely used online lexical taxonomywhich has
been developed in numerous languages.WordNet nouns, verbs, adjectives, and adverbs
are organized into synonym sets. PropBank, FrameNet, and WordNet cover the word
senses and argument-taking properties of many modal predicates.
The Prague Dependency Treebank (Hajic? et al 2001; Bo?hmova?, Cinkova?, and
Hajic?ova? 2005) (PDT) is a multi-level system of annotation for texts in Czech and other
languages, with its roots in the Prague school of linguistics. Besides a morphological
layer and an analytical layer, there is a Tectogrammatical layer. The Tectogrammatical
layer includes functional relationships, dependency relations, and co-reference. The
PDT also integrates propositional and extra-propositional meanings in a single anno-
tation framework.
The Penn Discourse Treebank (PDTB) (Webber et al 2003; Prasad et al 2008)
annotates discourse connectives and their arguments over a portion of the Penn
Treebank. Within this framework, senses are annotated for the discourse connectives
in a hierarchical scheme. Relevant to the current work, one type of tag in the scheme is
the Conditional tag, which includes hypothetical, general, unreal present, unreal past,
factual present, and factual past arguments.
The PDTB work is related to that of Wiebe, Wilson, and Cardie (2005) for estab-
lishing the importance of attributing a belief or assertion expressed in text to its agent
(equivalent to the notion of holder in our scheme). The annotation scheme is designed to
capture the expression of opinions and emotions. In the PDTB, each discourse relation
414
Baker et al Modality and Negation in SIMT
and its two arguments are annotated for attribution. The attribute features are the
Source or agent, the Type (assertion propositions, belief propositions, facts, and eventu-
alities), scopal polarity, and determinacy. Scopal polarity is annotated on relations and
their arguments to identify cases when verbs of attribution are negated on the surface
but the negation takes scope over the embedded clause. An example is the sentence
?Having the dividend increases is a supportive element in the market outlook but I don?t
think it?s a main consideration.? Here, the second argument (the clause following but) is
annotated with a ?Neg? marker, meaning ?I think it?s not a main consideration.?
Wilson, Wiebe, and Hoffman (2009) describe the importance of correctly inter-
preting polarity in the context of sentiment analysis, which is the task of identifying
positive and negative opinions, emotions, and evaluations. The authors have estab-
lished a set of features to distinguish between positive and negative polarity and discuss
the importance of correctly analyzing the scope of the negation and the modality (e.g.,
whether the proposition is asserted to be real or not real).
A major annotation effort for temporal and event expressions is the TimeML spec-
ification language, which has been developed in the context of reasoning for question
answering (Saur??, Verhagen, and Pustejovsky 2006). TimeML, which includes modality
annotation on events, is the basis for creating the TimeBank and FactBank corpora
(Pustejovsky et al 2006; Saur?? and Pustejovsky 2009). In FactBank, event mentions are
marked with their degree of factuality.
Recent work incorporating modality annotation includes work on detecting cer-
tainty and uncertainty. Rubin (2007) describes a scheme for five levels of certainty,
referred to as Epistemic modality, in news texts. Annotators identify explicit certainty
markers and also take into account Perspective, Focus, and Time. Focus separates
certainty into facts and opinions, to include attitudes. In our scheme, Focus would be
covered by want and belief modality. Also, separating focus and uncertainty can allow
the annotation of both on one trigger word. Prabhakaran, Rambow, and Diab (2010)
describe a scheme for automatic committed belief tagging. Committed belief indicates
the writer believes the proposition. The authors use a previously annotated corpus of
committed belief, non-committed belief, and not applicable (Diab et al 2009), and derive
features for machine learning from parse trees. The authors desire to combine their
work with FactBank annotation.
The CoNLL-2010 shared task (Farkas et al 2010) was about the detection of cues
for uncertainty and their scope. The task was described as ?hedge detection,? that is,
finding statements which do not or cannot be backed up with facts. Auxiliary verbs
such as may, might, can, and so forth, are one type of hedge cue. The training data for
the shared task included the BioScope corpus (Szarvas et al 2008), which is manually
annotated with negation and speculation cues and their scope, and paragraphs from
Wikipedia possibly containing hedge information. Our scheme also identifies cues in
the form of triggers, but our desired outcome is to cover the full range of modalities
and not just certainty and uncertainty. To identify scope, we use syntactic parse trees,
as was allowed in the CoNLL task.
The textual entailment literature includes modality annotation schemes. Identifying
modalities is important to determine whether a text entails a hypothesis. Bar-Haim et al
(2007) include polarity based rules and negation and modality annotation rules. The
polarity rules are based on an independent polarity lexicon (Nairn, Condorovdi, and
Karttunen 2006). The annotation rules for negation andmodality of predicates are based
on identifying modal verbs, as well as conditional sentences and modal adverbials.
The authors read the modality off parse trees directly using simple structural rules for
modifiers.
415
Computational Linguistics Volume 38, Number 2
Earlier work describing the difficulty of correctly translating modality using ma-
chine translation includes Sigurd and Gawro?nska (1994) andMurata et al (2005). Sigurd
and Gawro?nska (1994) write about rule based frameworks and how using alternate
grammatical constructions such as the passive can improve the rendering of the modal
in the target language. Murata et al (2005) analyze the translation of Japanese into
English by several systems, showing they often render the present incorrectly as the
progressive. The authors trained a support vector machine to specifically handle modal
constructions, whereas our modal annotation approach is a part of a full translation
system.
We now consider other literature, relating to tree-grafting and machine translation.
Our tree-grafting approach builds on a technique used for tree augmentation in Miller
et al (2000), where parse-tree nodes are augmented with semantic categories. In that
earlier work, tree nodes were augmented with relations, whereas we augmented tree
nodes with modality and negation. The parser is subsequently retrained for both
semantic and syntactic processing. The semantic annotations were done manually by
students who were provided a set of guidelines and then merged with the syntactic
trees automatically. In our work we tagged our corpus with entities, modality, and
negation automatically and then grafted them onto the syntactic trees automatically,
for the purpose of training a statistical machine translation system. An added benefit of
the extracted translation rules is that they are capable of producing semantically tagged
Urdu parses, despite the fact that the training data were processed by only an English
parser and tagger.
Related work in syntax-based MT includes that of Huang and Knight (2006), where
a series of syntax rules are applied to a source language string to produce a target
language phrase structure tree. The Penn English Treebank (Marcus, Marcinkiewicz,
and Santorini 1993) is used as the source for the syntactic labels and syntax trees are
relabeled to improve translation quality. In this work, node-internal and node-external
information is used to relabel nodes, similar to earlier work where structural context
was used to relabel nodes in the parsing domain (Klein and Manning 2003). Klein
and Manning?s methods include lexicalizing determiners and percent markers, making
more fine-grained verb phrase (VP) categories, and marking the properties of sister
nodes on nodes. All of these labels are derivable from the trees themselves and not
from an auxiliary source. Wang et al (2010) use this type of node splitting in machine
translation and report a small increase in BLEU score.
We use the methods described in Zollmann and Venugopal (2006) and Venugopal,
Zollmann, and Vogel (2007) to induce synchronous grammar rules, a process which
requires phrase alignments and syntactic parse trees. Venugopal, Zollmann, and Vogel
(2007) use generic non-terminal category symbols, as in Chiang (2005), as well as gram-
matical categories from the Stanford parser (Klein and Manning 2003). Their method
for rule induction generalizes to any set of non-terminals. We further refine this process
by adding semantic notations onto the syntactic non-terminals produced by a Penn
Treebank trained parser, thus making the categories more informative.
In the parsing domain, the work of Petrov and Klein (2007) is related to the current
work. In their work, rule splitting and rule merging are applied to refine parse trees
during machine learning. Hierarchical splitting leads to the creation of learned cate-
gories that have linguistic relevance, such as a breakdown of a determiner category into
two subcategories of determiners by number, that is, this and that group together as do
some and these. We augment parse trees by category insertion in cases where a semantic
category is inserted as a node in a parse tree, after the English side of the corpus has
been parsed by a statistical parser.
416
Baker et al Modality and Negation in SIMT
3. SIMTMotivation
As in many of the frameworks described herein, the aim of the SIMT effort was to
provide a generalized framework for representing structured semantic information,
such as modality and negation. Unlike many of the previous semantic annotation efforts
(where the emphasis tends to be on English), however, our approach is designed to
be directly integrated into a translation engine, with the goal of translating highly
divergent language pairs, such as Urdu and English. As such, our choice of annotation
scheme?illustrated in the trigger-target example shown in Figure 1?was based on a
simplified structural representation that is general enough to accommodate divergent
modality/negation phenomena, easy for language experts to follow, and straightfor-
ward to integrate into a tree-grafting mechanism for MT. Our objective is to investigate
whether incorporating this sort of information into machine translation systems could
produce better translations, particularly in settings where only small parallel corpora
are available.
It is informative to look at an example translation to understand the challenges of
translating important semantic elements when working with a low-resource language
pair. Figure 2 shows an example taken from the 2008 NIST Urdu?English translation
task, and illustrates the translation quality of a state-of-the-art Urdu?English system
(prior to the SIMT effort). The small amount of training data for this language pair (see
Figure 2
An example of Urdu?English translation. Shown are an Urdu source document, a reference
translation produced by a professional human translator, and MT output from a phrase-based
model (Moses) without linguistic information, which is representative of state-of-the-art MT
quality before the SIMT effort.
417
Computational Linguistics Volume 38, Number 2
Table 1
The size of the various data sets used for the experiments in this article including the training,
development (dev), incremental test set (devtest), and blind test set (test). The dev/devtest was a
split of the NIST08 Urdu?English test set, and the blind test set was NIST09.
Urdu English
set lines tokens types tokens types
training 202k 1.7M 56k 1.7M 51k
dev 981 21k 4k 19k 4k
devtest 883 22k 4k 19?20k 4k
test 1,792 42k 6k 38?41k 5k
Table 1) results in significantly degraded translation quality compared, for example, to
an Arabic?English system that has more than 100 times the amount of training data.
The output in Figure 2 was produced using Moses (Koehn et al 2007), a state-of-
the-art phrase-based MT system that by default does not incorporate any linguistic
information (e.g., syntax or morphology or transliteration knowledge). As a result,
words that were not directly observed in the bilingual training data were untranslatable.
Names, in particular, are problematic. For example, the lack of translation for Nagaland
and Nagas induces multiple omissions throughout the translated text, thus producing
several instances where the holder of a claim (or belief ) is missing. This is because out-of-
vocabulary words are deleted from the Moses output.
We use syntactic and semantic tags as higher-order symbols inside the translation
rules used by the translation models. Generic symbols in translation rules (i.e., the
non-terminal symbol ?X?) were replaced with structured information at multiple levels
of abstraction, using a tree-grafting approach that we describe subsequently. Figure 3
Figure 3
The evolution of a semantically informed approach to our synchronous context-free grammars.
At the start of the 8 weeks the decoder used translation rules with a single generic non-terminal
symbol. Later syntactic categories were used, and by the end of the workshop the translation
rules included semantic elements such as modalities and negation, as well as named entities.
418
Baker et al Modality and Negation in SIMT
illustrates the evolution of the translation rules that we used, first replacing ?X? with
grammatical categories and then with categories corresponding to semantic units.
The semantic units that we examined in this effort weremodalities and negation (in-
dications that a statement represents something that has/hasn?t taken place or is/isn?t
a belief or an intention) and named entities (such as people or organizations). Other
semantic units, such as relations between entities and events, were not part of this effort
but we believe they could be similarly incorporated into the framework. We chose to
examine semantic units that canonically exhibit two different syntactic types: verbal, in
the case of modality and negation, and nominal, in the case of named entities.
Although used in this effort, named entities were not the focus of our research
efforts in SIMT. Rather, we focused on the development of an annotation scheme
for modality and negation and its use in MT, while relying on a pre-existing hidden
Markov model (HMM)-based tagger derived from Identifinder (Bikel, Schwartz, and
Weischedel 1999) to produce entity tags. Thus, the remainder of this article will focus
on our MN annotation scheme, two MN taggers produced by the effort, and on the
integration of semantics in the SIMT paradigm.
4. Modality and Negation
Modality is an extra-propositional component of meaning. In John may go to NY, the
basic proposition is John go to NY and the word may indicates modality and is called the
trigger in our work. van der Auwera and Amman (2005) define core cases of modality:
John must go to NY (epistemic necessity), John might go to NY (epistemic possibility),
John has to leave NY now (deontic necessity), and John may leave NY now (deontic pos-
sibility). Larreya (2009) defines the core cases slightly differently as root and epistemic.
Root modality in Larreya?s taxonomy includes physical modality (He had to stop. The
road was blocked) and deontic modality (You have to stop). Epistemic modality includes
problematic modality (You must be tired) and implicative modality (You have to be mad to
do that). Many semanticists (Kratzer 1991, von Fintel and Iatridou 2006) define modality
as quantification over possible worlds. John might leave NY means that there exist some
possible worlds in which John leaves NY. Another view of modality relates more to a
speaker?s attitude toward a proposition (McShane, Nirenburg, and Zacharski).
We incorporate negation as an inextricably intertwined component of modality,
using the term ?modality/negation (MN)? to refer to our resources (lexicons) and
processes (taggers). We adopt the view that modality includes several types of attitudes
that a speaker might have (or not have) toward an event or state. From the point of
view of the reader or listener, modality might indicate factivity, evidentiality, or senti-
ment. Factivity is related to whether an event, state, or proposition happened or didn?t
happen. It distinguishes things that happened from things that are desired, planned,
or probable. Evidentiality deals with the source of information and may provide clues
to the reliability of the information. Did the speaker have first-hand knowledge of
what he or she is reporting, or was it hearsay or inferred from indirect evidence?
Sentiment deals with a speaker?s positive or negative feelings toward an event, state,
or proposition.
Our project was limited to modal words and phrases?and their negations?that
are related to factivity. Beyond the core cases of modality, however, we include some
aspects of speaker attitude such as intent and desire. We included these because they
are often not separable from the core cases of modality. For example, He had to go may
include the ideas that someone wanted him to go, that he might not have wanted to go,
419
Computational Linguistics Volume 38, Number 2
that at some point after coercion he intended to go, and that at some point he was able
to go (Larreya 2009).
Our focus was on the eight modalities in Figure 4, where P is a proposition (the
target of the triggering modality) and H is the holder (experiencer or cognizer of the
modality). Some of the eight factivity-related modalities may overlap with sentiment
or evidentiality. For example, want indicates that the proposition it scopes over may
not be a fact (it may just be desired), but it also expresses positive sentiment toward
the proposition it scopes over. We assume that sentiment and evidentiality are covered
under separate coding schemes, and that words like want would have two tags, one for
sentiment and one for factivity.
5. The Modality/Negation Annotation Scheme
The challenge of creating an MN annotation scheme was to deal with the complex
scoping ofmodalities with each other andwith negation, while at the same time creating
a simplified operational procedure that could be followed by language experts without
special training. Here we describe our MN annotation framework, including a set
of linguistic simplifications, and then we present our methodology for creation of a
publicly available MN lexicon. The modality annotation scheme is fully documented in
a set of guidelines that were written with English example sentences (Baker et al 2010c).
The guidelines can be used to derive hand-tagged evaluation data for English and they
also include a section that contains a set of Urdu trigger-word examples.
During the SCALE workshop, some Urdu speakers used the guidelines to annotate
a small corpus of Urdu by hand, which we reserved for future work. The Urdu corpus
could be useful as an evaluation corpus for automatically tagged Urdu, such as one
derived from rule projection in the Urdu?English MT system, a method we describe
further in Section 7. Also, although we did not annotate a very large Urdu corpus, more
data could be manually annotated to train an automatic Urdu tagger in the future.
5.1 Anatomy of Modality/Negation in Sentences
In sentences that express modality, we identify three components: a trigger, a target, and
a holder. The trigger is the word or string of words that expresses modality or negation.
The target is the event, state, or relation over which the modality scopes. The holder is
Figure 4
Eight modalities used for tagging. H = the holder of the modality; P = the proposition over
which the modality has scope.
420
Baker et al Modality and Negation in SIMT
the experiencer or cognizer of themodality. The trigger can be a word such as should, try,
able, likely, or want. It can also be a negative element such as not or n?t. Often, modality
or negation is expressed without a lexical trigger. For a typical declarative sentence
(e.g., John went to NY), the default modality is strong belief when no lexical trigger is
present. Modality can also be expressed constructionally. For example, Requirement can
be expressed in Urdu with a dative subject and infinitive verb followed by a verb that
means to happen or befall.
5.2 Linguistic Simplifications for Efficient Operationalization
Six linguistic simplifications were made for the sake of efficient operationalization of
the annotation task. The first linguistic simplification deals with the scope of modality
and negation. The first given sentence indicates scope of modality over negation. The
second sentence indicates scope of negation over modality:
 He tried not to criticize the president.
 He didn?t try to criticize the president.
The interaction of modality with negation is complex, but was operationalized eas-
ily in the menu of 13 choices shown in Figure 5. First consider the case where negation
scopes over modality. Four of the 13 choices are composites of negation scoping over
modality. For example, the annotators can choose try or not try as two separate modali-
ties. Five modalities (Require, Permit, Want, Firmly Believe, and Believe) do not have a
negated form. For three of these modalities (Want, Firmly Believe, and Believe), this is
because they are often transparent to negation. For example, I do not believe that he left NY
sometimes means the same as I believe he didn?t leave NY. Merging the two is obviously
a simplification, but it saves the annotators from having to make a difficult decision.
Figure 5
Thirteen menu choices for Modality/Negation annotation. H = the holder of the modality;
P = the proposition over which the modality has scope.
421
Computational Linguistics Volume 38, Number 2
The second linguistic simplification is related to a duality in meaning between
require and permit. Not requiring P to be true is similar in meaning to permitting P to
be false. Thus, annotators were instructed to label not require P to be true as Permit P to be
false. Conversely, not Permit P to be truewas labeled as Require P to be false.
After the annotator chooses the modality, the scoping of modality over negation
takes place as a second decision. For example, for the sentence John tried not to go to NY,
the annotator first identifies go as the target of a modality and then chooses try as the
modality. Finally, the annotator chooses false as the polarity of the target.
The third simplification relates to entailments between modalities. Many words
have complex meanings that include components of more than one modality. For ex-
ample, if one managed to do something, one tried to do it and one probably wanted to
do it. Thus, annotators were provided a specificity-ordered modality list as in Figure 5,
andwere asked to choose the first applicable modality. We note that this list corresponds
to two independent ?entailment groupings,? ordered by specificity:
 {requires ? permits}
 {succeeds ? tries ? intends ? is able ? wants}
Inside the entailment groupings, the ordering corresponds to an entailment relation:
For example, succeeds can only occur if tries has occurred. Also, the {requires ? . . . }
entailment grouping is taken to be more specific than (ordered before) the {succeeds ?
. . . } entailment grouping. Moreover, both entailment groupings are taken to be more
specific than believes, which is not in an entailment relation with any of the other
modalities.
The fourth simplification, already mentioned, is that sentences without an overt
trigger word are tagged as firmly believes. This heuristic works reasonably well for the
types of documents we were working with, although one could imagine genres such
as fiction in which many sentences take place in an alternate possible world (imagined,
conditional, or counterfactual) without explicit marking.
The fifth linguistic simplification is that we did not require annotators to mark
nested modalities. For a sentence like He might be able to go to NY the target word go
is marked as ability, but might is not annotated for Belief modality. This decision was
based on time limits on the annotation task; there was not enough time for annotators
to deal with syntactic scoping of modalities over other modalities.
Finally, we did not mark the holder H because of the short time frame for workshop
preparation. We felt that identifying the triggers and targets would be most beneficial
in the context of machine translation.
5.3 The English Modality/Negation Lexicon
Using the given framework, we created an MN lexicon that was incorporated into an
MN tagging scheme to be described in Section 6. Entries in the MN lexicon consist of:
(1) A string of one or more words: for example, should or have need of . (2) A part of
speech for each word: The part of speech helps us avoid irrelevant homophones such as
the noun can. (3) An MN designator: one of the 13 modality/negation cases described
previously. (4) A head word (or trigger): the primary phrasal constituent to cover
cases where an entry is a multi-word unit (e.g., the word hope in hope for). (5) One or
more subcategorization codes derived from the Longman Dictionary of Contemporary
English (LDOCE).
422
Baker et al Modality and Negation in SIMT
We produced the full English MN lexicon semi-automatically. First, we gathered a
small seed list of MN trigger words and phrases from our modality annotation manual
(Baker et al 2010c). Then, we expanded this small list of MN trigger words by running
an on-line search for each of the words, specifically targeting free on-line thesauri (e.g.,
thesaurus.com), to find both synonymous and antonymous words. From these we
manually selected the words we thought triggered modality (or their corresponding
negative variants) and filtered out words that we thought didn?t trigger modality. The
resulting list of MN trigger words and phrases contained about 150 lemmas.
We note that most intransitive (LDOCE) codes were not applicable to modality/
negation constructions. For example, hunger (in the Want modality class) has a modal
reading of ?desire? when combined with the preposition for (as in she hungered for a
promotion), but we do not consider it to be modal when it is used in the somewhat
archaic sentence He hungered, meaning that he did not have enough to eat. Thus the
LDOCE code I associated with the verb hungerwas hand-changed to I-FOR. There were
43 such cases. Once the LDOCE codes were hand-verified (and modified accordingly),
the mapping to subcategorization codes was applied.
The MN lexicon is publicly available at http://www.umiacs.umd.edu/?bonnie/
ModalityLexicon.txt. An example of an entry is given in Figure 6, for the verb need.
6. Automatic Modality/Negation Annotation
An MN tagger produces text or structured text in which modality or negation triggers
and/or targets are identified. Automatic identification of the holders of modalities was
beyond the scope of our project because the holder is often not explicitly stated in the
sentence in which the trigger and target occur. This section describes two types of MN
taggers?one that is string-based and one that is structure-based.
6.1 The String-Based English Modality/Negation Tagger
The string-based tagger operates on text that has been tagged with parts of speech
by a Collins-style statistical parser (Miller et al 1998). The tagger marks spans of
words/phrases that exactly match MN trigger words in the MN lexicon described
previously, and that exactly match the same parts of speech. This tagger identifies the
target of each modality/negation using the heuristic of tagging the next non-auxiliary
verb to the right of the trigger. Spans of words can be tagged multiple times with
different types of triggers and targets.
Figure 6
Modality lexicon entry for need.
423
Computational Linguistics Volume 38, Number 2
We found the string-based MN tagger to produce output that matched about 80%
of the sentence-level tags produced by our structure-based tagger, the results of which
are described next. Although string-based tagging is fast and reasonably accurate in
practice, we opted to focus on the indepth analysis of modality/negation of our SIMT
results using the more accurate structure-based tagger.
6.2 The Structure-Based English Modality/Negation Tagger
The structure-based MN tagger operates on text that has been parsed (Miller et al
1998). We used a version of the parser that produces flattened trees. In particular, the
flattener deletes VP nodes that are immediately dominated by VP or S and noun phrase
(NP) nodes that are immediately dominated by PP or NP. The parsed sentences are
processed by TSurgeon rules. Each TSurgeon rule consists of a pattern and an action.
The pattern matches part of a parse tree and the action alters the parse tree. More
specifically, the pattern finds an MN trigger word and its target and the action inserts
tags such as TrigRequire and TargRequire for triggers and targets for the modality
Require. Figure 7 shows output from the structure-based MN tagger. (Note that the
sentence is disfluent: Pakistan which could not reach semi-final, in a match against South
African team for the fifth position Pakistan defeated South Africa by 41 runs.) The example
shows that could is a trigger for the Ability modality and not is a trigger for negation.
Reach is a target for both Ability and Negation, which means that it is in the category of
?H is not able [to make P true/false]? in our coding scheme. Reach is also a trigger for
the Succeed modality and semi-final is its target.
The TSurgeon patterns are automatically generated from the verb class codes in
the MN lexicon along with a set of 15 templates. Each template covers one situation
such as the following: the target is the subject of the trigger; the target is the direct
object of the trigger; the target heads an infinitival complement of the trigger; the target
is a noun modified by an adjectival trigger, and so on. The verb class codes indicate
Figure 7
Sample output from the structure-based MN tagger.
424
Baker et al Modality and Negation in SIMT
which templates are applicable for each trigger word. For example, a trigger verb in the
transitive class may use two target templates, one in which the trigger is in active voice
and the target is a direct object (need tents) and one in which the trigger is in passive
voice and the target is a subject (tents are needed).
In developing the TSurgeon rules, we first conducted a corpus analysis for 40 of the
most common trigger words in order to identify and debug the most broadly applicable
templates. We then used LDOCE to assign verb classes to the remaining verbal triggers
in the MN lexicon, and we associated one or more debugged templates with each verb
class. In this way, the initial corpus work on a limited number of trigger words was
generalized to a longer list of trigger words. Because the TSurgeon patterns are tailored
to the flattened structures produced by our parser, it is not easily ported to new parser
outputs. The MN lexicon itself is portable, however. Switching parsers would entail
writing new TSurgeon templates, but the trigger words in the MN lexicon would still
be automatically assigned to templates based on their verb classes.
The following example shows an example of a TSurgeon pattern?action pair for a
sentence like They were required to provide tents. The pattern?action pair is intended to
be used after a pre-processing stage in which labels such as ?VoicePassive? and ?AUX?
have been assigned. ?VoicePassive? is inserted by a pre-processing TSurgeon pattern
because, in some cases, the target of a passive modality trigger word is in a different
location from the target of the corresponding active modality trigger word. ?AUX? is
inserted during pre-processing to distinguish auxiliary uses of have and be from their
uses as main verbs. The pattern portion of the pattern?action pair matches a node with
label VB that is not already tagged as a trigger and that is passive and dominates the
string ?required?. The VB node is also a sister to an S node, and the S node dominates a
VB that is not an auxiliary (provide in this case). The action portion of the pattern?action
pair inserts the string ?TargReq? as the second daughter of the second VB and inserts
the string ?TrigReq? as the second daughter of the first VB.
VB=trigger !< /^Trig/ < VoicePassive < required $..
(S < (VB=target !< AUX))
insert (TargReq) >2 target
insert (TrigReq) >2 trigger
Verb-specific patterns such as this one were generalized in order to gain coverage of
the whole modality lexicon. The specific lexical item, required, was replaced with a vari-
able, as were the labels ?TrigReq? and ?TargReq.? The pattern was then given a name,
V3-passive-basic, where V3 is a verb class tag from LDOCE (described in Section 5.3)
for verbs that take infinitive complements. We then looked up the LDOCE verb class
labels for all of the verbs in the modality lexicon. Using this information, we could then
generate a set of new, verb-specific patterns for each V3 verb in the modality lexicon.
6.3 Evaluating the Effectiveness of Structure-Based MN Tagging
We performed amanual inspection of the structure-based tagging output. We calculated
precision by examining 229 instances of modality triggers that were tagged by our
tagger from the English side of the NIST 09 MTEval training sentences. We analyzed
precision in two steps, first checking for the correct syntactic position of the target and
then checking the semantic correctness of the trigger and target. For 192 of the 229
triggers (around 84%), the targets were tagged in the correct syntactic location.
For example, for the sentence A solution must be found to this problem shown in
Figure 8, the word must is a modality trigger word, and the correct target is the first
425
Computational Linguistics Volume 38, Number 2
Figure 8
Example of embedded target head found inside VP must be found.
non-auxiliary verb heading a verb phrase that is contained in the syntactic complement
of must. The syntactic complement of must is the verb phrase be found to this problem.
The syntactic head of that verb phrase, be, is skipped because it is an auxiliary verb. The
correct (embedded) target found is the head of the syntactic complement of be.
The 192 modality instances with structurally correct targets do not all have seman-
tically correct tags. In this example, must is tagged as TrigBelief, where the correct tag
would be TrigRequire. Also, because theMN lexiconwas usedwithout respect to word
sense, words were sometimes erroneously identified as triggers. This includes non-
modal uses of work (work with refugees), reach (reach a destination), and attack (attack
a physical object), in constrast to modal uses of these words: work for peace (effort), reach
a goal (succeed), and attack a problem (effort). Fully correct tagging of modality would
need to include word sense disambiguation.
For 37 of the 229 triggers we examined, a target was not tagged in the correct syn-
tactic position. In 12 of 37 incorrectly tagged instances the targets are inside compound
nouns or coordinate structures (NP or VP), which are not yet handled by the modality
tagger. The remaining 25 of the 37 incorrectly tagged instances had targets that were lost
because the tagger does not yet handle all cases of nested modalities. Nested modalities
occur in sentences like They did not want to succeed in winning where the target words
want and succeed are also modality trigger words. Proper treatment of nested modalities
requires consideration of scope and compositional semantics.
Nesting was treated in two steps. First, the modality tagger marked each word as a
trigger and/or target. In They did not want to succeed in winning, not is marked as a trigger
for negation, want is marked as a target of negation and a trigger of wanting, succeed is
marked as a trigger of succeeding and a target of wanting, and win is marked as a target
of succeeding. The second step in the treatment of nested modalities occurs during tree
grafting, where the meanings of the nested modalities are composed. The tree grafting
program correctly composes some cases of nested modalities. For example, the tag
TrigAble composed with TrigNegation results in the target tag TargNOTAble, as shown
in Figure 9. In other cases, where compositional semantics are not yet accommodated,
the tree grafting program removed target labels from the trees, and those cases were
counted as incorrect for the purpose of this evaluation.
Figure 9
Example of modality composed with negation: TrigAble and TrigNegation combine to form
NOTAble.
426
Baker et al Modality and Negation in SIMT
In the 229 instances that we examined, there were 14 in which a light verb or noun
was the correct syntactic target, but not the correct semantic target. Decision would be
a better target than taken in The decision should be taken on delayed cases on the basis of
merit.We counted sentences with semantically light targets as correct in our evaluation
because our goal was to identify the syntactic head of the target. The semantics of the
target is a general issue, and we often find lexico-syntactic fluff between the trigger and
the most semantically salient target in sentences likeWe succeeded in our goal of winning
the war where ?success in war? is the salient meaning.
With respect to recall, the tagger primarily missed special forms of negation in
noun phrases and prepositional phrases: There was no place to seek shelter; The buildings
should be reconstructed, not with RCC, but with the wood and steel sheets. More complex
constructional and phrasal triggers were also missed: President Pervaiz Musharraf has said
that he will not rest unless the process of rehabilitation is completed. Finally, we discovered
some omissions from our MN lexicon: It is not possible in the middle of winter to re-open
the roads. Further annotation experiments are planned, which will be analyzed to close
such gaps and update the lexicon as appropriate.
Providing a quantitative measure of recall was beyond the scope of this project.
At best we could count instances of sentences containing trigger words that were not
tagged. We are also aware of many cases of modality that were not covered such as
the modal uses of the future tense auxiliary will as in That?ll be John (conjecture), I?ll do
the dishes (volition), He won?t do it (non-volition), and It will accommodate five (ability)
(Larreya 2009). Because of the complexity and subtlety of modality and negation, how-
ever, it would be impractical to count every clause (such as the not rest unless clause
above) that had a nuance of non-factivity.
7. Semantically Informed Syntactic MT
This section describes the incorporation of our structured-based MN tagging into an
Urdu?English machine-translation system using tree grafting for combining syntactic
symbols with semantic categories (e.g., modality/negation). We note that a de facto
Urdu MN tagger resulted from identifying the English MN trigger and target words in
a parallel English?Urdu corpus, and then projecting the trigger and target labels to the
corresponding words in Urdu syntax trees.
7.1 Refinement of Translation Grammars with Semantic Categories
We used synchronous context-free grammars (SCFGs) as the underlying formalism
for our statistical models of translation. SCFGs provide a convenient and theoretically
grounded way of incorporating linguistic information into statistical models of transla-
tion, by specifying grammar rules with syntactic non-terminals in the source and target
languages. We refine the set of non-terminal symbols so that they not only include
syntactic categories, but also semantic categories.
Chiang (2005) re-popularized the use of SCFGs for machine translation, with the
introduction of his hierarchical phrase-based machine translation system, Hiero. Hiero
uses grammars with a single non-terminal symbol ?X? rather than using linguistically
informed non-terminal symbols. When moving to linguistic grammars, we use Syntax
Augmented Machine Translation (SAMT) developed by Venugopal, Zollmann, and
Vogel (2007). In SAMT the ?X? symbols in translation grammars are replaced with
nonterminal categories derived from parse trees that label the English side of the
427
Computational Linguistics Volume 38, Number 2
Figure 10
A sentence on the English side of the bilingual parallel training corpus is parsed with a
syntactic parser, and also tagged with our modality tagger. The tags are then grafted onto
the syntactic parse tree to form new categories like VP-TargNOTAble and VP-TargRequire.
Grafting happens prior to extracting translation rules, which happens normally except for
the use of the augmented trees.
Urdu?English parallel corpus.1 We refine the syntactic categories by combining them
with semantic categories. Recall that this progression was illustrated in Figure 3.
We extracted SCFG grammar rules containing modality, negation, and named enti-
ties using an extraction procedure that requires parse trees for one side of the parallel
corpus. Although it is assumed that these trees are labeled and bracketed in a syntac-
tically motivated fashion, the framework places no specific requirement on the label
inventory. We take advantage of this characteristic by providing the rule extraction
algorithm with augmented parse trees containing syntactic labels that have semantic
annotations grafted onto them so that they additionally express semantic information.
Our strategy for producing semantically grafted parse trees involves three steps:
1. The English sentences in the parallel training data are parsed with a
syntactic parser. In our work, we used the lexicalized probabilistic context
free grammar parser provided by Basis Technology Corporation.
2. The English sentences are MN-tagged by the system described herein and
named-entity-tagged by the Phoenix tagger (Richman and Schone 2008).
3. The modality/negation and entity markers are grafted onto the syntactic
parse trees using a tree-grafting procedure. The grafting procedure was
implemented as part of the SIMT effort. Details are further spelled out in
Section 7.2.
Figure 10 illustrates how modality tags are grafted onto a parse tree. Note that
although we focus the discussion here on the modality and negation, our framework
is general and we were able to incorporate other semantic elements (specifically, named
entities) into the SIMT effort.
Once the semantically grafted trees have been produced for the parallel corpus, the
trees are presented, along with word alignments (produced by the Berkeley aligner),
to the rule extraction software to extract synchronous grammar rules that are both
1 For non-constituent phrases, composite CCG-style categories are used (Steedman 1999).
428
Baker et al Modality and Negation in SIMT
syntactically and semantically informed. These grammar rules are used by the decoder
to produce translations. In our experiments, we used the Joshua decoder (Li et al 2009),
the SAMT grammar extraction software (Venugopal and Zollmann 2009), and special
purpose-built tree-grafting software.
Figure 11 shows example semantic rules that are used by the decoder. The verb
phrase rules are augmented with modality and negation, taken from the semantic
categories listed in Table 2. Because these get marked on the Urdu source as well as
the English translation, semantically enriched grammars also act as very simple named
entity or MN taggers for Urdu. Only entities, modality, and negation that occurred in
the parallel training corpus are marked in the output, however.
7.2 Tree-Grafting Algorithm
The overall scheme of our tree-grafting algorithm is to match semantic tags to syntactic
categories. There are two inputs to the process. Each is derived from a common text
file of sentences. The first input is a list of standoff annotations for the semantically
tagged word sequences in the input sentences, indexed by sentence number. The second
is a list of parse trees for the sentences in Penn Treebank format, indexed by sentence
number.
Table 2 lists the modality/negation types that were produced by the MN tagger. For
example, the sentence The students are able to swim is tagged as The students are ?TrigAble?
to ?TargAble swim?. The distinction between ?Negation? and ?NOT? corresponds to the
difference between negation that is inherently expressed in the triggering lexical item
and negation that is expressed explicitly as a separate lexical item. Thus, I achieved
my goal is tagged ?Succeed? and I did not achieve my goal is tagged as ?NOTSucceed,?
Figure 11
Example translation rules with tags for modality, negation, and entities combined with
syntactic categories.
429
Computational Linguistics Volume 38, Number 2
Table 2
Modality tags with their negated versions. Note that Require and Permit are in a dual relation,
and thus RequireNegation is represented as NOTPermit and PermitNegation is represented
as NOTRequire.
Require NOTRequire
Permit NOTPermit
Succeed NOTSucceed
SucceedNegation NOTSucceedNegation
Effort NOTEffort
EffortNegation NOTEffortNegation
Intend NOTIntend
IntendNegation NOTIntendNegation
Able NOTAble
AbleNegation NOTAbleNegation
Want NOTWant
WantNegation NOTWantNegation
Belief NOTBelief
BeliefNegation NOTBeliefNegation
Firm Belief NOTFirm Belief
Firm BeliefNegation NOTFirm BeliefNegation
Negation
but I failed to win is tagged as ?SucceedNegation,? and I did not fail to win is tagged as
?NOTSucceedNegation.?
The tree-grafting algorithm proceeds as follows. For each tagged sentence, we
iterate over the list of semantic tags. For each semantic tag, there is an associated word
or sequence of words. For example, the modality tag TargAble may tag the word swim.
For each semantically tagged word, we find the parent node in the correspond-
ing syntactic parse tree that dominates that word. For a word sequence, we find and
compare the parent nodes for all of the words. Each node in the syntax tree has a
category label. The following tests are then made and tree grafts applied:
 If there is a single node in the parse tree that dominates all and only the
words with the semantic tag, graft the name of the semantic tag onto
the highest corresponding syntactic constituent in the tree. For example,
in Figure 10, which shows the grafting process for modality tagging,
the semantic tag TargNOTAble that ?hand over? receives is grafted onto
the VB node that dominates all and only the words ?hand over.? Then the
semantic tag TargNOTAble is passed up the tree to the VP node, which is
the highest corresponding syntactic constituent.
 If the semantic tag corresponds to words that are adjacent daughters in
a syntactic constituent, but less than the full constituent, insert a node
dominating those words into the parse tree, as a daughter of the original
syntactic constituent. The name of the semantic tag is grafted onto the new
node and becomes its category label. This is a case of tree augmentation by
node insertion.
 If a syntactic constituent selected for grafting has already been labeled
with a semantic tag, overlay the previous tag with the current tag. We
chose to tag in this manner simply because our system was not set up to
handle the grafting of multiple tags onto a single constituent. An example
430
Baker et al Modality and Negation in SIMT
of this occurs in the sentence ?The Muslims had obtained Pakistan.? If the
NP node dominating Pakistan is grafted with a named entity tag such as
NP-GPE, we overlay this with the NP-TargSucceed tag in a modality
tagging scheme.
 In the case of a word sequence, if the words covered by the semantic tag
fall across two different syntactic constituents, do nothing. This is a case of
crossing brackets.
Our tree-grafting procedure was simplified to accept a single semantic tag per
syntactic tree node as the final result. The algorithm keeps the last tag seen as the tag of
precedence. In practice, we established a precedence ordering for modality/negation
tags over named entity tags by grafting named entity tags first and modality/negation
second. Our intuition was that, in case of a tie, finer-grained verbal categories would be
more helpful to parsing than finer-grained nominal categories.2 In cases where a word
was tagged both as a MN target and a MN trigger, we gave precedence to the target tag.
This is because, although MN targets vary, MN triggers are generally identifiable with
lexical items. Finally, we used the simplified specificity ordering of MN tags described
in Section 5.2 to ensure precedence of more specific tags over more general ones. Table 2
lists the modality/negation types from highest (Require modality) to lowest (Negation)
precedence.3
7.3 SIMT Results
We evaluated our tree grafting approach by performing a series of translation experi-
ments. Each version of our translation systemwas trained on the same bilingual training
data. The bilingual parallel corpus that we used was distributed as part of the 2008
NIST Open Machine Translation Evaluation Workshop.4 The training set contained
88,108 Urdu?English sentence pairs, and a bilingual dictionary with 113,911 entries.
For our development and test sets, we split the NIST MT-08 test set into two portions
(with each document going into either test or dev, and preserving the genre split).
Our test set contained 883 Urdu sentences, each with four translations into English,
and our dev set contained 981 Urdu sentences, each with four reference translations.
To extract a syntactically informed translation model, we parsed the English side of
the training corpus using a Penn Treebank?trained parser (Miller et al 1998). For the
experiments that involved grafting named entities onto the parse trees, we tagged
the English side of the training corpus with the Phoenix tagger (Richman and Schone
2008). We word-aligned the parallel corpus with the Berkeley aligner. All models used a
5-gram language model trained on the English Gigaword corpus (v5) using the SRILM
toolkit with modified KN smoothing. The Hiero translation grammar was extracted
using the Joshua toolkit (Li et al 2009). The other translation grammars were extracted
using the SAMT toolkit (Venugopal and Zollmann 2009).
2 In testing we found that grafting named entities first and MN last yielded a slightly higher BLEU score
than the reverse order.
3 Future work could include exploring additional methods of resolving tag conflicts or combining tag
types on single nodes, for example, by inserting multiple intermediate nodes (effectively using unary
rewrite rules) or by stringing tag names together.
4 http://www.itl.nist.gov/iad/mig/tests/mt/2008/doc/.
431
Computational Linguistics Volume 38, Number 2
Figure 12
Results for a range of experiments conducted during the SIMT effort show the score for our
top-performing baseline systems derived from a hierarchical phrase-based model (Hiero).
Substantial improvements obtained when syntax was introduced along with feature functions
(FFs) and further improvements resulted from the addition of semantic elements. The scores
are lowercased BLEU calculated on the held-out devtest set. NE = named entities.
Figure 12 gives the results for a number of experiments conducted during the SIMT
effort.5 The experiments are broken into three groups: baselines, syntax, and semantics.
To contextualize our results we experimented with a number of different baselines
that were composed from two different approaches to statistical machine translation?
phrase-based and hierarchical phrase-based SMT?along with different combinations
of language model sizes and word aligners. Our best-performing baseline was a Hiero
model. The Bleu score for this baseline on the development set was 22.9 Bleu points.
After experimenting with syntactically motivated grammar rules, we conducted
experiments on the effects of incorporating semantic elements (e.g., named entities and
modality/negation) into the translation grammars. In our devtest set our taggers tagged
on average 3.5 named entities per sentence and 0.35 MN markers per sentence. These
were included by grafting modality, negation, and named-entity markers onto the parse
trees. Individually, each of these made modest improvements over the syntactically
informed system alone. Grafting named entities onto the parse trees improved the Bleu
score by 0.2 points. Modality/negation improved it by 0.3 points. Doing both simulta-
neously had an additive effect and resulted in a 0.5 Bleu score improvement over syntax
alone. This improvement was the largest improvement that we got from anything other
than the move from linguistically naive models to syntactically informed models.
We used bootstrap resampling to test whether the differences in Bleu scores were
statistically significant (Koehn 2004). All of the results were a significant improvement
over Hiero (at p ? 0.01). The difference between the syntactic system and the syntactic
system with named entities is not significant (p = 0.38). The differences between the
5 These experiments were conducted on the devtest set, containing 883 Urdu sentences (21,623 Urdu
words) and four reference translations per sentence. The BLEU score for these experiments is measured
on uncased output.
432
Baker et al Modality and Negation in SIMT
syntactic system and the syntactic system with MN, and between the syntactic system
and the syntactic system with both MN and named entities were both significant at
(p ? 0.05).
Figure 13 shows example output from the final SIMT system in comparison to
the pre-SIMT results and the translation produced by a human (reference). An error
analysis of this example output illustrates that SIMT enhancements have resulted in the
elimination of misleading translation output in several cases:
1. pre-SIMT: China had the experience of Pakistan?s first nuclear bomb.
SIMT: China has the first nuclear bomb test.
reference: China has conducted the experiment of Pakistan?s first nuclear bomb.
2. pre-SIMT: the nuclear bomb in 1998 that Pakistan may experience
SIMT: the experience of the atom bomb Pakistan in May 1998
reference: the atom bomb, whose experiment was done in 1998
by Pakistan
3. pre-SIMT: He said that it is also present proof of that Dr. Abdul Qadeer
Khan after the Chinese design
SIMT: He said that there is evidence that Dr. Abdul Qadeer Khan has
also used the Chinese design
reference: He said that the proof to this also exists in that Dr. Abdul
Qadeer Khan used the Chinese design
The article in question pertains to claims by Thomas Reid that China allowed Pakistan
to detonate a nuclear weapon at its test site. In the first example, however, the reader is
potentially misled by the pre-SIMT output to believe that Pakistan launched a nuclear
bomb on China. The SIMT output leaves out the mention of Pakistan, but correctly con-
veys the firm belief that the bomb event is a test (closely resembling the term experiment
in the human reference), not a true bombing event. This is clearly an improvement over
the misleading pre-SIMT output.
In the second example, the pre-SIMT output misleads the reader to believe that
Pakistan is (or will be) attacked, through the use of the phrase may experience, where
may is poorly placed. (We note here that this is a date translation error, i.e., the month
of May should be next to the year 1998, further adding to the potential for confusion.)
Unfortunately, the SIMT output also uses the term experience (rather than experiment,
which is in the human reference), but in this case the month is correctly positioned in
the output, thus eliminating the potential for confusionwith respect to themodality. The
lack of a modal appropriately neutralizes the statement so that it refers to an abstract
event associated with the atom bomb, rather than an attack on the country.
In the third example, where the Chinese design used by Dr. Abdul Qandeer Khan is
argued to be proof of the nuclear testing relationship between Pakistan and China, the
first pre-SIMT output potentially leads the reader to believe that Dr. Abdul Qadeer is
after the Chinese design (not that he actually used it), whereas the SIMT output conveys
the firm belief that the Chinese design has been used by Dr. Abdul Qadeer. This output
very closely matches the human reference.
Note that even in the title of the article, the SIMT system produces much more
coherent English output than that of the linguistically naive system. The figure also
shows improvements due to transliteration, which are described in Irvine et al (2010).
The scores reported in Figure 12 do not include transliteration improvements.
433
Computational Linguistics Volume 38, Number 2
Figure 13
An example of the improvements to Urdu?English translation before and after the SIMT effort.
Output is from the baseline Hiero model, which does not use linguistic information, and from
the final model, which incorporates syntactic and semantic information.
434
Baker et al Modality and Negation in SIMT
8. Conclusions and Future Work
We developed a modality/negation lexicon and a set of automatic MN taggers, one of
which?the structure-based tagger?results in 86% precision for tagging of a standard
LDC data set. The MN tagger has been used to improve machine translation output
by imposing semantic constraints on possible translations in the face of sparse training
data. The tagger is also an important component of a language-understanding module
for a related project.
We have described a technique for translation that shows particular promise
for low-resource languages. We have integrated linguistic knowledge into statistical
machine translation in a unified and coherent framework. We demonstrated that
augmenting hierarchical phrase-based translation rules with semantic labels (through
?grafting?) resulted in a 0.5 Bleu score improvement over syntax alone.
Although our largest gains were from syntactic enrichments to the Hiero model,
demonstrating success on the integration of semantic aspects of language bodes well
for additional improvements based on the incorporation of other semantic aspects. For
example, we hypothesize that incorporating relations and temporal knowledge into
the translation rules would further improve translation quality. The syntactic grafting
framework is well-suited to support the exploration of the impact of many different
types of semantics on MT quality, though in this article we focused on exploring the
impact of modality and negation.
An important future study is one that focuses on demonstrating whether further
improvements in modality/negation identification are likely to lead to further gains in
translation performance. Such a study would benefit from the inclusion of a more de-
tailed manual evaluation to determine if modality and negation is adequately conveyed
in the downstream translations. This work would be additionally enhanced through
experimentation on other language pair(s) and larger corpora.
The work presented here represents the first small steps toward a full integration
of MT and semantics. Efforts underway in DARPA?s GALE program demonstrated the
potential for combining MT and semantics (termed distillation) to answer the informa-
tion needs of monolingual speakers using multilingual sources. Proper recognition of
modalities and negation is crucial for handling those information needs effectively.
In previous work, however, semantic processing proceeded largely independently of
the MT system, operating only on the translated output. Our approach is significantly
different in that it combines syntax, semantics, and MT into a single model, offering
the potential advantages of joint modeling and joint decision-making. It would be
interesting to explore whether the integration of MT with syntax and semantics can be
extended to provide a single-model solution for tasks such as cross-language informa-
tion extraction and question answering, and to evaluate our integrated approach (e.g.,
using GALE distillation metrics).
Acknowledgments
We thank Aaron Phillips for help with
conversion of the output of the entity tagger
for ingest by the tree-grafting program. We
thank Anni Irvine and David Zajic for their
help with experiments on an alternative
Urdu modality/negation tagger based on
projection and training an HMM-based
tagger derived from Identifinder (Bikel,
Schwartz, and Weischedel 1999). For their
helpful ideas and suggestions during the
development of the modality framework,
we are indebted to Mona Diab, Eduard
Hovy, Marge McShane, Teruko Mitamura,
Sergei Nirenburg, Boyan Onyshkevych,
and Owen Rambow. We also thank Basis
Technology Corporation for their generous
contribution of software components to this
work. This work was supported, in part,
by the Johns Hopkins Human Language
435
Computational Linguistics Volume 38, Number 2
Technology Center of Excellence (HLTCOE),
by the National Science Foundation under
grant IIS-0713448, and by BBN Technologies
under GALE DARPA/IPTO contract no.
HR0011-06-C-0022. Any opinions, findings,
and conclusions or recommendations
expressed in this material are those of the
authors and do not necessarily reflect the
views of the sponsor.
References
Baker, Collin F., Charles J. Fillmore, and
John B. Lowe. 1998. The Berkeley
FrameNet project. In Proceedings of the
36th Annual Meeting of the Association
for Computational Linguistics and
17th International Conference on
Computational Linguistics - Volume 1,
ACL ?98, pages 86?90, Stroudsburg, PA.
Baker, Kathryn, Steven Bethard, Michael
Bloodgood, Ralf Brown, Chris Callison-
Burch, Glen Coppersmith, Bonnie J. Dorr,
Nathaniel W. Filardo, Kendall Giles, Ann
Irvine, Michael Kayser, Lori Levin, Justin
Martineau, James Mayfield, Scott Miller,
Aaron Phillips, Andrew Philpot, Christine
Piatko, Lane Schwartz, and David Zajic.
2010a. Semantically informed machine
translation. Technical Report 002,
Human Language Technology Center of
Excellence, Johns Hopkins University,
Baltimore, MD.
Baker, Kathryn, Michael Bloodgood,
Chris Callison-Burch, Bonnie J. Dorr,
Nathaniel W. Filardo, Lori Levin, Scott
Miller, and Christine Piatko. 2010b.
Semantically-informed machine
translation: A tree-grafting approach.
In Proceedings of The Ninth Biennial
Conference of the Association for Machine
Translation in the Americas, Denver, CO.
Baker, Kathryn, Michael Bloodgood, Mona
Diab, Bonnie J. Dorr, Ed Hovy, Lori Levin,
Marjorie McShane, Teruko Mitamura,
Sergei Nirenburg, Christine Piatko, Owen
Rambow, and Gramm Richardson. 2010c.
SIMT SCALE 2009?Modality annotation
guidelines. Technical Report 004, Human
Language Technology Center of
Excellence, Johns Hopkins University,
Baltimore, MD.
Baker, Kathryn, Michael Bloodgood,
Bonnie J. Dorr, Nathanial W. Filardo,
Lori Levin, and Christine Piatko.
2010d. A modality lexicon and its use
in automatic tagging. In Proceedings of
the Seventh International Conference on
Language Resources and Evaluation
(LREC), pages 1402?1407, Mediterranean
Conference Center, Valletta.
Bar-Haim, Roy, Ido Dagan, Iddo Greental,
and Eyal Shnarch. 2007. Semantic
inference at the lexical-syntactic level.
In Proceedings of the 22nd National
Conference on Artificial intelligence -
Volume 1, pages 871?876, Vancouver,
British Columbia.
Bikel, Daniel M., Richard Schwartz, and
Ralph M. Weischedel. 1999. An algorithm
that learns what?s in a name.Machine
Learning, 34(1?3):211?231.
Bo?hmova?, Alena, Silvie Cinkova?, and
Eva Hajic?ova?. 2005. A manual for
tectogrammatical layer annotation of the
Prague Dependency Treebank [English
translation]. Technical Report #30, U?FAL
MFF UK, Prague, Czech Republic.
Chiang, David. 2005. A hierarchical
phrase-based model for statistical machine
translation. In Proceedings of the 43rd
Annual Meeting of the Association for
Computational Linguistics (ACL-2005),
pages 263?270, Ann Arbor, MI.
Diab, Mona T., Lori Levin, Teruko Mitamura,
Owen Rambow, Vinodkumar
Prabhakaran, and Weiwei Guo. 2009.
Committed belief annotation and tagging.
In Proceedings of the Third Linguistic
Annotation Workshop, ACL-IJCNLP ?09,
pages 68?73, Stroudsburg, PA.
Farkas, Richa?rd, Veronika Vincze, Gyo?rgy
Mo?ra, Ja?nos Csirik, and Gyo?rgy Szarvas.
2010. The CoNLL-2010 shared task:
Learning to detect hedges and their scope
in natural language text. In Proceedings of
the Fourteenth Conference on Computational
Natural Language Learning?Shared Task,
CoNLL ?10: Shared Task, pages 1?12,
Stroudsburg, PA.
Fellbaum, Christiane, editor. 1998.WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Hajic?, Jan, Eva Hajic?ova?, Petr Pajas, Jarmila
Panevova?, Petr Sgall, and Barbora Vidova?
Hladka?. 2001. Prague Dependency
Treebank 1.0 (Final Production Label),
UFAL MFF UK, Prague, Czech Republic.
Huang, Bryant and Kevin Knight. 2006.
Relabeling syntax trees to improve
syntax-based machine translation
quality. In HLT-NAACL, New York.
Irvine, Ann, Mike Kayser, Zhifei Li, Wren
Thornton, and Chris Callison-Burch.
2010. Integrating output from specialized
modules in machine translation:
Transliteration in Joshua. Proceedings
of the Human Language Technology
436
Baker et al Modality and Negation in SIMT
and North American Chapter of the
Association for Computational Linguistics,
pages 240?247. The Prague Bulletin of
Mathematical Linguistics, 93:107?116.
Klein, Dan and Christopher D. Manning.
2003. Accurate unlexicalized parsing. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics,
pages 423?430, Sapporo, Japan.
Koehn, Philipp. 2004. Statistical significance
tests for machine translation evaluation. In
Proceedings of EMNLP 2004, pages 388?395,
Barcelona.
Koehn, Philipp, Hieu Hoang, Alexandra
Birch, Chris Callison-Burch, Marcello
Federico, Nicola Bertoldi, Brooke Cowan,
Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine
translation. In Proceedings of the ACL-2007
Demo and Poster Sessions, Prague, Czech
Republic, pages 177?180.
Kratzer, Angelika. 1991. Modality. In
Arnim von Stechow and Dieter, editors,
Semantics: An International Handbook of
Contemporary Research. De Gruyter,
Berlin, pages 639?650.
Larreya, Paul. 2009. Towards a typology of
modality in language. In Raphael Salkie,
Pierre Busuttil, and Johan van der Auwera,
editors,Modality in English: Theory and
Description. Mouton de Gruyter, Paris,
pages 9?30.
Li, Zhifei, Chris Callison-Burch, Chris Dyer,
Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar
Zaidan. 2009. Joshua: An open source
toolkit for parsing-based machine
translation. In Proceedings of the Fourth
Workshop on Statistical Machine Translation,
pages 135?139, Athens.
Marcus, Mitchell P., Mary Ann
Marcinkiewicz, and Beatrice Santorini.
1993. Building a large annotated corpus of
English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
McShane, Marjorie, Sergei Nirenburg, and
Ron Zacharski. 2004. Mood and modality:
Out of the theory and into the fray. Natural
Language Engineering, 19(1):57?89.
Miller, Scott, Heidi Fox, Lance Ramshaw,
and Ralph Weischedel. 1998. SIFT:
Statistically-derived information from
text. In Seventh Message Understanding
Conference (MUC-7), Washington, DC,
Miller, Scott, Heidi J. Fox, Lance A.
Ramshaw, and Ralph M. Weischedel. 2000.
A novel use of statistical parsing to extract
information from text. In Proceedings of
Applied Natural Language Processing
and the North American Association for
Computational Linguistics, pages 226?233,
Seattle, Washington.
Murata, Masaki, Kiyotaka Uchimoto, Qing
Ma, Toshiyuki Kanamaru, and Hitoshi
Isahara. 2005. Analysis of machine
translation systems? errors in tense,
aspect, and modality. In Proceedings of the
19th Asia-Pacific Conference on Language,
Information and Computing (PACLIC 2005),
Taipei, Taiwan.
Nairn, Rowan, Cleo Condorovdi, and
Lauri Karttunen. 2006. Computing
relative polarity for textual inference.
In Proceedings of the International Workshop
on Inference in Computational Semantics
(ICoS-5), pages 66?76, Buxton, England.
Palmer, Martha, Daniel Gildea, and
Paul Kingsbury. 2005. The Proposition
Bank: An annotated corpus of semantic
roles. Computational Linguistics,
31:71?106.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2002. Bleu:
A method for automatic evaluation of
machine translation. In Proceedings of the
40th Annual Meeting of the Association for
Computational Linguistics (ACL-2002),
pages 311?318, Philadelphia, PA.
Petrov, Slav and Dan Klein. 2007. Learning
and inference for hierarchically split
PCFGs. In Proceedings of the 22nd American
Association for Artificial Intelligence,
pages 1663?1666, Vancouver, British
Columbia, Canada.
Prabhakaran, Vinodkumar, Owen Rambow,
and Mona Diab. 2010. Automatic
committed belief tagging. In Proceedings
of the 23rd International Conference on
Computational Linguistics: Posters, COLING
?10, pages 1014?1022, Beijing, China.
Prasad, Rashmi, Nikhil Dinesh, Alan Lee,
Eleni Miltsakaki, Livio Robaldo, Aravind
Joshi, and Bonnie Webber. 2008. The Penn
Discourse TreeBank 2.0. In Proceedings of
the Sixth International Language Resources
and Evaluation (LREC?08), pages 28?30,
Marrakech.
Pustejovsky, James, Marc Verhagen, Roser
Saur??, Jessica Littman, Robert Gaizauskas,
Graham Katz, Inderjeet Mani, Robert
Knippen, and Andrea Setzer. 2006.
TimeBank 1.2. Linguistic Data Consortium,
Philadelphia, PA.
Richman, Alexander and Patrick Schone.
2008. Mining wiki resources for
multilingual named entity recognition.
437
Computational Linguistics Volume 38, Number 2
In Proceedings of ACL-08: HLT, pages 1?9,
Columbus, OH.
Rubin, Victoria L. 2007. Stating with
certainty or stating with doubt:
Intercoder reliability results for manual
annotation of epistemically modalized
statements. In Proceedings of the Human
Language Technology and North American
Chapter of the Association for Computational
Linguistics (Short Papers), pages 141?144,
Rochester, NY.
Saur??, Roser and James Pustejovsky. 2009.
FactBank: A corpus annotated with
event factuality. Language Resources and
Evaluation, 43(3):227?268.
Saur??, Roser, Marc Verhagen, and James
Pustejovsky. 2006. Annotating and
recognizing event modality in text.
In Proceedings of the 19th International
Florida Artificial Intelligence Research
Society Conference, pages 333?339,
Melbourne Beach, FL.
Sigurd, Bengt and Barbara Gawro?nska.
1994. Modals as a problem for MT. In
Proceedings of the 15th International
Conference on Computational Linguistics
(COLING) - Volume 1, pages 120?124,
Kyoto, Japan.
Steedman, Mark. 1999. Alternating
quantifier scope in CCG. In Proceedings of
the 37th Annual Meeting of the Association
for Computational Linguistics (ACL),
College Park, MD.
Szarvas, Gyo?rgy, Veronika Vincze, Richa?rd
Farkas, and Ja?nos Csirik. 2008. The
BioScope corpus: Annotation for negation,
uncertainty and their scope in biomedical
texts. In Proceedings of the Workshop on
Current Trends in Biomedical Natural
Language Processing, pages 38?45,
Stroudsburg, PA.
van der Auwera, Johan and Andreas
Ammann. 2005. Overlap between
situational and epistemic modal marking.
In Martin Haspelmath, Matthew S. Dryer,
David Gil, and Bernard Comrie, editors,
World Atlas of Language Structures. Oxford
University Press, New York, chapter 76,
pages 310?313.
Venugopal, Ashish and Andreas Zollmann.
2009. Grammar based statistical MT on
Hadoop: An end-to-end toolkit for large
scale PSCFG based MT. Prague Bulletin of
Mathematical Linguistics, 91:67?78.
Venugopal, Ashish, Andreas Zollmann, and
Stephan Vogel. 2007. An efficient two-pass
approach to synchronous-CFG driven
statistical MT. In Proceedings of the
Human Language Technology Conference
of the North American Chapter of the
Association for Computational Linguistics
(HLT/NAACL-2007), pages 500?507,
Rochester, NY.
von Fintel, Kai and Sabine Iatridou.
2006. How to say ought in foreign: The
composition of weak necessity modals.
In Proceedings of the 6th Workshop on
Formal Linguistics, Florianopolis, Brazil,
August 2006.
Wang, Wei, Jonathan May, Kevin Knight,
and Daniel Marcu. 2010. Re-structuring,
re-labeling, and re-aligning for
syntax-based machine translation.
Computational Linguistics, 36(2):247?277.
Webber, Bonnie, Aravid Joshi, Matthew
Stone, and Alistair Knott. 2003. Anaphora
and discourse structure. Computational
Linguistics, 29:545?587.
Wiebe, Janyce, Theresa Wilson, and Claire
Cardie. 2005. Annotating expressions
of opinions and emotions in language.
Language Resources and Evaluation,
39(2?3):165?210.
Wilson, Theresa, Janyce Wiebe, and Paul
Hoffman. 2009. Recognizing contextual
polarity: An exploration of features
for phrase-level sentiment analysis.
Computational Linguistics, 35:399?433.
Zollmann, Andreas and Ashish Venugopal.
2006. Syntax augmented machine
translation via chart parsing. In
Proceedings on the Workshop on Statistical
Machine Translation, pages 138?141,
New York City.
438
Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012),
pages 57?64, Jeju, Republic of Korea, 13 July 2012. c?2012 Association for Computational Linguistics
Statistical Modality Tagging
from Rule-based Annotations and Crowdsourcing
Vinodkumar Prabhakaran Michael Bloodgood Mona Diab
CS CASL CCLS
Columbia University University of Maryland Columbia University
vinod@cs.columbia.edu meb@umd.edu mdiab@ccls.columbia.edu
Bonnie Dorr Lori Levin Christine D. Piatko
CS and UMIACS LTI APL
University of Maryland Carnegie Mellon University Johns Hopkins University
bonnie@umiacs.umd.edu lsl@cs.cmu.edu christine.piatko@jhuapl.edu
Owen Rambow Benjamin Van Durme
CCLS HLTCOE
Columbia University Johns Hopkins University
rambow@ccls.columbia.edu vandurme@cs.jhu.edu
Abstract
We explore training an automatic modality
tagger. Modality is the attitude that a speaker
might have toward an event or state. One of
the main hurdles for training a linguistic tag-
ger is gathering training data. This is par-
ticularly problematic for training a tagger for
modality because modality triggers are sparse
for the overwhelming majority of sentences.
We investigate an approach to automatically
training a modality tagger where we first gath-
ered sentences based on a high-recall simple
rule-based modality tagger and then provided
these sentences to Mechanical Turk annotators
for further annotation. We used the resulting
set of training data to train a precise modality
tagger using a multi-class SVM that delivers
good performance.
1 Introduction
Modality is an extra-propositional component of
meaning. In John may go to NY, the basic propo-
sition is John go to NY and the word may indi-
cates modality. Van Der Auwera and Ammann
(2005) define core cases of modality: John must
go to NY (epistemic necessity), John might go to
NY (epistemic possibility), John has to leave now
(deontic necessity) and John may leave now (de-
ontic possibility). Many semanticists (e.g. Kratzer
(1981), Kratzer (1991), Kaufmann et al (2006)) de-
fine modality as quantification over possible worlds.
John might go means that there exist some possi-
ble worlds in which John goes. Another view of
modality relates more to a speakers attitude toward
a proposition (e.g. McShane et al (2004)).
Modality might be construed broadly to include
several types of attitudes that a speaker wants to ex-
press towards an event, state or proposition. Modal-
ity might indicate factivity, evidentiality, or senti-
ment (McShane et al, 2004). Factivity is related to
whether the speaker wishes to convey his or her be-
lief that the propositional content is true or not, i.e.,
whether it actually obtains in this world or not. It
distinguishes things that (the speaker believes) hap-
pened from things that he or she desires, plans, or
considers merely probable. Evidentiality deals with
the source of information and may provide clues to
the reliability of the information. Did the speaker
57
have firsthand knowledge of what he or she is re-
porting, or was it hearsay or inferred from indirect
evidence? Sentiment deals with a speaker?s positive
or negative feelings toward an event, state, or propo-
sition.
In this paper, we focus on the following five
modalities; we have investigated the belief/factivity
modality previously (Diab et al, 2009b; Prab-
hakaran et al, 2010), and we leave other modalities
to future work.
? Ability: can H do P?
? Effort: does H try to do P?
? Intention: does H intend P?
? Success: does H succeed in P?
? Want: does H want P?
We investigate automatically training a modality
tagger by using multi-class Support Vector Ma-
chines (SVMs). One of the main hurdles for training
a linguistic tagger is gathering training data. This is
particularly problematic for training a modality tag-
ger because modality triggers are sparse for the over-
whelming majority of the sentences. Baker et al
(2010) created a modality tagger by using a semi-
automatic approach for creating rules for a rule-
based tagger. A pilot study revealed that it can boost
recall well above the naturally occurring proportion
of modality without annotated data but with only
60% precision. We investigated an approach where
we first gathered sentences based on a simple modal-
ity tagger and then provided these sentences to an-
notators for further annotation, The resulting anno-
tated data also preserved the level of inter-annotator
agreement for each example so that learning algo-
rithms could take that into account during training.
Finally, the resulting set of annotations was used for
training a modality tagger using SVMs, which gave
a high precision indicating the success of this ap-
proach.
Section 2 discusses related work. Section 3 dis-
cusses our procedure for gathering training data.
Section 4 discusses the machine learning setup
and features used to train our modality tagger and
presents experiments and results. Section 5 con-
cludes and discusses future work.
2 Related Work
Previous related work includes TimeML (Sauri et
al., 2006), which involves modality annotation on
events, and Factbank (Sauri and Pustejovsky, 2009),
where event mentions are marked with degree of fac-
tuality. Modality is also important in the detection of
uncertainty and hedging. The CoNLL shared task in
2010 (Farkas et al, 2010) deals with automatic de-
tection of uncertainty and hedging in Wikipedia and
biomedical sentences.
Baker et al (2010) and Baker et al (2012) ana-
lyze a set of eight modalities which include belief,
require and permit, in addition to the five modalities
we focus on in this paper. They built a rule-based
modality tagger using a semi-automatic approach to
create rules. This earlier work differs from the work
described in this paper in that the our emphasis is on
the creation of an automatic modality tagger using
machine learning techniques. Note that the anno-
tation and automatic tagging of the belief modality
(i.e., factivity) is described in more detail in (Diab et
al., 2009b; Prabhakaran et al, 2010).
There has been a considerable amount of inter-
est in modality in the biomedical domain. Negation,
uncertainty, and hedging are annotated in the Bio-
scope corpus (Vincze et al, 2008), along with infor-
mation about which words are in the scope of nega-
tion/uncertainty. The i2b2 NLP Shared Task in 2010
included a track for detecting assertion status (e.g.
present, absent, possible, conditional, hypothetical
etc.) of medical problems in clinical records.1 Apos-
tolova et al (2011) presents a rule-based system for
the detection of negation and speculation scopes us-
ing the Bioscope corpus. Other studies emphasize
the importance of detecting uncertainty in medical
text summarization (Morante and Daelemans, 2009;
Aramaki et al, 2009).
Modality has also received some attention in the
context of certain applications. Earlier work de-
scribing the difficulty of correctly translating modal-
ity using machine translation includes (Sigurd and
Gawro?nska, 1994) and (Murata et al, 2005). Sig-
urd et al (1994) write about rule based frameworks
and how using alternate grammatical constructions
such as the passive can improve the rendering of the
modal in the target language. Murata et al (2005)
1https://www.i2b2.org/NLP/Relations/
58
analyze the translation of Japanese into English
by several systems, showing they often render the
present incorrectly as the progressive. The authors
trained a support vector machine to specifically han-
dle modal constructions, while our modal annotation
approach is a part of a full translation system.
The textual entailment literature includes modal-
ity annotation schemes. Identifying modalities is
important to determine whether a text entails a hy-
pothesis. Bar-Haim et al (2007) include polarity
based rules and negation and modality annotation
rules. The polarity rules are based on an indepen-
dent polarity lexicon (Nairn et al, 2006). The an-
notation rules for negation and modality of predi-
cates are based on identifying modal verbs, as well
as conditional sentences and modal adverbials. The
authors read the modality off parse trees directly us-
ing simple structural rules for modifiers.
3 Constructing Modality Training Data
In this section, we will discuss the procedure we
followed to construct the training data for build-
ing the automatic modality tagger. In a pilot study,
we obtained and ran the modality tagger described
in (Baker et al, 2010) on the English side of the
Urdu-English LDC language pack.2 We randomly
selected 1997 sentences that the tagger had labeled
as not having the Want modality and posted them on
Amazon Mechanical Turk (MTurk). Three differ-
ent Turkers (MTurk annotators) marked, for each of
the sentences, whether it contained the Want modal-
ity. Using majority rules as the Turker judgment,
95 (i.e., 4.76%) of these sentences were marked as
having a Want modality. We also posted 1993 sen-
tences that the tagger had labeled as having a Want
modality and only 1238 of them were marked by the
Turkers as having a Want modality. Therefore, the
estimated precision of this type of approach is only
around 60%.
Hence, we will not be able to use the (Baker et
al., 2010) tagger to gather training data. Instead,
our approach was to apply a simple tagger as a first
pass, with positive examples subsequently hand-
annotated using MTurk. We made use of sentence
data from the Enron email corpus,3 derived from the
2LDC Catalog No.: LDC2006E110.
3http://www-2.cs.cmu.edu/?enron/
version owing to Fiore and Heer,4 further processed
as described by (Roark, 2009).5
To construct the simple tagger (the first pass), we
used a lexicon of modality trigger words (e.g., try,
plan, aim, wish, want) constructed by Baker et al
(2010). The tagger essentially tags each sentence
that has a word in the lexicon with the corresponding
modality. We wrote a few simple obvious filters for a
handful of exceptional cases that arise due to the fact
that our sentences are from e-mail. For example, we
filtered out best wishes expressions, which otherwise
would have been tagged as Want because of the word
wishes.
The words that trigger modality occur with very
different frequencies. If one is not careful, the
training data may be dominated by only the com-
monly occurring trigger words and the learned tag-
ger would then be biased towards these words. In
order to ensure that our training data had a diverse
set of examples containing many lexical triggers and
not just a lot of examples with the same lexical trig-
ger, for each modality we capped the number of sen-
tences from a single trigger to be at most 50. After
we had the set of sentences selected by the simple
tagger, we posted them on MTurk for annotation.
The Turkers were asked to check a box indicat-
ing that the modality was not present in the sentence
if the given modality was not expressed. If they did
not check that box, then they were asked to highlight
the target of the modality. Table 1 shows the number
of sentences we posted on MTurk for each modal-
ity.6 Three Turkers annotated each sentence. We
restricted the task to Turkers who were adults, had
greater than a 95% approval rating, and had com-
pleted at least 50 HITs (Human Intelligence Tasks)
on MTurk. We paid US$0.10 for each set of ten sen-
tences.
Since our data was annotated by three Turkers,
for training data we used only those examples for
which at least two Turkers agreed on the modality
and the target of the modality. This resulted in 1,008
examples. 674 examples had two Turkers agreeing
and 334 had unanimous agreement. We kept track
of the level of agreement for each example so that
4http://bailando.sims.berkeley.edu/enron/enron.sql.gz
5Data received through personal communication
6More detailed statistics on MTurk annotations are available
at http://hltcoe.jhu.edu/datasets/.
59
Modality Count
Ability 190
Effort 1350
Intention 1320
Success 1160
Want 1390
Table 1: For each modality, the number of sentences re-
turned by the simple tagger that we posted on MTurk.
our learner could weight the examples differently
depending on the level of inter-annotator agreement.
4 Multiclass SVM for Modality
In this section, we describe the automatic modal-
ity tagger we built using the MTurk annotations de-
scribed in Section 3 as the training data. Section 4.1
describes the training and evaluation data. In Sec-
tion 4.2, we present the machinery and Section 4.3
describes the features we used to train the tagger.
In Section 4.4, we present various experiments and
discuss results. Section 4.5, presents additional ex-
periments using annotator confidence.
4.1 Data
For training, we used the data presented in Section 3.
We refer to it as MTurk data in the rest of this paper.
For evaluation, we selected a part of the LU Corpus
(Diab et al, 2009a) (1228 sentences) and our expert
annotated it with modality tags. We first used the
high-recall simple modality tagger described in Sec-
tion 3 to select the sentences with modalities. Out
of the 235 sentences returned by the simple modal-
ity tagger, our expert removed the ones which did
not in fact have a modality. In the remaining sen-
tences (94 sentences), our expert annotated the tar-
get predicate. We refer to this as the Gold dataset
in this paper. The MTurk and Gold datasets differ in
terms of genres as well as annotators (Turker vs. Ex-
pert). The distribution of modalities in both MTurk
and Gold annotations are given in Table 2.
4.2 Approach
We applied a supervised learning framework us-
ing multi-class SVMs to automatically learn to tag
Modality MTurk Gold
Ability 6% 48%
Effort 25% 10%
Intention 30% 11%
Success 24% 9%
Want 15% 23%
Table 2: Frequency of Modalities
modalities in context. For tagging, we used the Yam-
cha (Kudo and Matsumoto, 2003) sequence labeling
system which uses the SVMlight (Joachims, 1999)
package for classification. We used One versus All
method for multi-class classification on a quadratic
kernel with a C value of 1. We report recall and pre-
cision on word tokens in our corpus for each modal-
ity. We also report F?=1 (F)-measure as the har-
monic mean between (P)recision and (R)ecall.
4.3 Features
We used lexical features at the token level which can
be extracted without any parsing with relatively high
accuracy. We use the term context width to denote
the window of tokens whose features are considered
for predicting the tag for a given token. For example,
a context width of 2 means that the feature vector
of any given token includes, in addition to its own
features, those of 2 tokens before and after it as well
as the tag prediction for 2 tokens before it. We did
experiments varying the context width from 1 to 5
and found that a context width of 2 gives the optimal
performance. All results reported in this paper are
obtained with a context width of 2. For each token,
we performed experiments using following lexical
features:
? wordStem - Word stem.
? wordLemma - Word lemma.
? POS - Word?s POS tag.
? isNumeric - Word is Numeric?
? verbType - Modal/Auxiliary/Regular/Nil
? whichModal - If the word is a modal verb,
which modal?
60
We used the Porter stemmer (Porter, 1997) to ob-
tain the stem of a word token. To determine the
word lemma, we used an in-house lemmatizer using
dictionary and morphological analysis to obtain the
dictionary form of a word. We obtained POS tags
from Stanford POS tagger and used those tags to
determine verbType and whichModal features. The
verbType feature is assigned a value ?Nil? if the word
is not a verb and whichModal feature is assigned a
value ?Nil? if the word is not a modal verb. The fea-
ture isNumeric is a binary feature denoting whether
the token contains only digits or not.
4.4 Experiments and Results
In this section, we present experiments performed
considering all the MTurk annotations where two
annotators agreed and all the MTurk annotations
where all three annotators agreed to be equally cor-
rect annotations. We present experiments applying
differential weights for these annotations in Section
4.5. We performed 4-fold cross validation (4FCV)
on MTurk data in order to select the best feature
set configuration ?. The best feature set obtained
waswordStem,POS,whichModal with a context
width of 2. For finding the best performing fea-
ture set - context width configuration, we did an ex-
haustive search on the feature space, pruning away
features which were proven not useful by results at
stages. Table 3 presents results obtained for each
modality on 4-fold cross validation.
Modality Precision Recall F Measure
Ability 82.4 55.5 65.5
Effort 95.1 82.8 88.5
Intention 84.3 61.3 70.7
Success 93.2 76.6 83.8
Want 88.4 64.3 74.3
Overall 90.1 70.6 79.1
Table 3: Per modality results for best feature set ? on
4-fold cross validation on MTurk data
We also trained a model on the entire MTurk data
using the best feature set ? and evaluated it against
the Gold data. The results obtained for each modal-
ity on gold evaluation are given in Table 4. We at-
tribute the lower performance on the Gold dataset to
its difference from MTurk data. MTurk data is en-
tirely from email threads, whereas Gold data con-
tained sentences from newswire, letters and blogs
in addition to emails. Furthermore, the annotation
is different (Turkers vs expert). Finally, the distri-
bution of modalities in both datasets is very differ-
ent. For example, Ability modality was merely 6%
of MTurk data compared to 48% in Gold data (see
Table 2).
Modality Precision Recall F Measure
Ability 78.6 22.0 34.4
Effort 85.7 60.0 70.6
Intention 66.7 16.7 26.7
Success NA 0.0 NA
Want 92.3 50.0 64.9
Overall 72.1 29.5 41.9
Table 4: Per modality results for best feature set ? evalu-
ated on Gold dataset
We obtained reasonable performances for Effort
and Want modalities while the performance for other
modalities was rather low. Also, the Gold dataset
contained only 8 instances of Success, none of which
was recognized by the tagger resulting in a recall
of 0%. Precision (and, accordingly, F Measure) for
Success was considered ?not applicable? (NA), as no
such tag was assigned.
4.5 Annotation Confidence Experiments
Our MTurk data contains sentence for which at least
two of the three Turkers agreed on the modality and
the target of the modality. In this section, we investi-
gate the role of annotation confidence in training an
automatic tagger. The annotation confidence is de-
noted by whether an annotation was agreed by only
two annotators or was unanimous. We denote the set
of sentences for which only two annotators agreed as
Agr2 and that for which all three annotators agreed
as Agr3.
We present four training setups. The first setup
is Tr23 where we train a model using both Agr2
and Agr3 with equal weights. This is the setup we
used for results presented in the Section 4.4. Then,
we have Tr2 and Tr3, where we train using only
Agr2 and Agr3 respectively. Then, for Tr23W , we
61
TrainingSetup
Tested on Agr2 and Agr3 Tested on Agr3 only
Precision Recall F Measure Precision Recall F Measure
Tr23 90.1 70.6 79.1 95.9 86.8 91.1
Tr2 91.0 66.1 76.5 95.6 81.8 88.2
Tr3 88.1 52.3 65.6 96.8 71.7 82.3
Tr23W 89.9 70.5 79.0 95.8 86.5 90.9
Table 5: Annotator Confidence Experiment Results; the best results per column are boldfaced
(4-fold cross validation on MTurk Data)
train a model giving different cost values for Agr2
and Agr3 examples. The SVMLight package al-
lows users to input cost values ci for each training
instance separately.7 We tuned this cost value for
Agr2 and Agr3 examples and found the best value
at 20 and 30 respectively.
For all four setups, we used feature set ?. We per-
formed 4-fold cross validation on MTurk data in two
ways ? we tested against a combination of Agr2
and Agr3, and we tested against only Agr3. Results
of these experiments are presented in Table 5. We
also present the results of evaluating a tagger trained
on the whole MTurk data for each setup against the
Gold annotation in Table 6. The Tr23 tested on both
Agr2 andAgr3 presented in Table 5 and Tr23 tested
on Gold data presented in Table 6 correspond to the
results presented in Table 3 and Table 4 respectively.
TrainingSetup Precision Recall F Measure
Tr23 72.1 29.5 41.9
Tr2 67.4 27.6 39.2
Tr3 74.1 19.1 30.3
Tr23W 73.3 31.4 44.0
Table 6: Annotator Confidence Experiment Results; the
best results per column are boldfaced
(Evaluation against Gold)
One main observation is that including annota-
tions of lower agreement, but still above a threshold
(in our case, 66.7%), is definitely helpful. Tr23 out-
performed both Tr2 and Tr3 in both recall and F-
7This can be done by specifying ?cost:<value>? after the
label in each training instance. This feature has not yet been
documented on the SVMlight website.
measure in all evaluations. Also, even when evaluat-
ing against only the high confident Agr3 cases, Tr2
gave a high gain in recall (10 .1 percentage points)
over Tr3, with only a 1.2 percentage point loss on
precision. We conjecture that this is because there
are far more training instances in Tr2 than in Tr3
(674 vs 334), and that quantity beats quality.
Another important observation is the increase in
performance by using varied costs for Agr2 and
Agr3 examples (the Tr23W condition). Although
it dropped the performance by 0.1 to 0.2 points
in cross-validation F measure on the Enron cor-
pora, it gained 2.1 points in Gold evaluation F mea-
sure. These results seem to indicate that differential
weighting based on annotator agreement might have
more beneficial impact when training a model that
will be applied to a wide range of genres than when
training a model with genre-specific data for appli-
cation to data from the same genre. Put differently,
using varied costs prevents genre over-fitting. We
don?t have a full explanation for this difference in
behavior yet. We plan to explore this in future work.
5 Conclusion
We have presented an innovative way of combining
a high-recall simple tagger with Mechanical Turk
annotations to produce training data for a modality
tagger. We show that we obtain good performance
on the same genre as this training corpus (annotated
in the same manner), and reasonable performance
across genres (annotated by an independent expert).
We also present experiments utilizing the number of
agreeing Turkers to choose cost values for training
examples for the SVM. As future work, we plan to
extend this approach to other modalities which are
62
not covered in this study.
6 Acknowledgments
This work is supported, in part, by the Johns Hop-
kins Human Language Technology Center of Ex-
cellence. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of the sponsor. We thank several anony-
mous reviewers for their constructive feedback.
References
Emilia Apostolova, Noriko Tomuro, and Dina Demner-
Fushman. 2011. Automatic extraction of lexico-
syntactic patterns for detection of negation and spec-
ulation scopes. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies: short papers -
Volume 2, HLT ?11, pages 283?287, Portland, Oregon.
Eiji Aramaki, Yasuhide Miura, Masatsugu Tonoike,
Tomoko Ohkuma, Hiroshi Mashuichi, and Kazuhiko
Ohe. 2009. Text2table: Medical text summarization
system based on named entity recognition and modal-
ity identification. In Proceedings of the BioNLP 2009
Workshop, pages 185?192, Boulder, Colorado, June.
Association for Computational Linguistics.
Kathryn Baker, Michael Bloodgood, Bonnie J. Dorr,
Nathaniel W. Filardo, Lori S. Levin, and Christine D.
Piatko. 2010. A modality lexicon and its use in auto-
matic tagging. In LREC.
Kathryn Baker, Michael Bloodgood, Bonnie J. Dorr,
Chris Callison-Burch, Nathaniel W. Filardo, Christine
Piatko, Lori Levin, and Scott Miller. 2012. Use of
modality and negation in semantically-informed syn-
tactic mt. Computational Linguistics, 38(22).
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In Proceedings of the 22nd Na-
tional Conference on Artificial intelligence - Volume 1,
pages 871?876, Vancouver, British Columbia, Canada.
AAAI Press.
Mona Diab, Bonnie Dorr, Lori Levin, Teruko Mitamura,
Rebecca Passonneau, Owen Rambow, and Lance
Ramshaw. 2009a. Language Understanding Anno-
tation Corpus. Linguistic Data Consortium (LDC),
USA.
Mona Diab, Lori Levin, Teruko Mitamura, Owen Ram-
bow, Vinodkumar Prabhakaran, and Weiwei Guo.
2009b. Committed belief annotation and tagging. In
Proceedings of the Third Linguistic Annotation Work-
shop, pages 68?73, Suntec, Singapore, August. Asso-
ciation for Computational Linguistics.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Szarvas,
Gyo?rgy Mo?ra, and Ja?nos Csirik, editors. 2010. Pro-
ceedings of the Fourteenth Conference on Computa-
tional Natural Language Learning. Association for
Computational Linguistics, Uppsala, Sweden, July.
Thorsten Joachims, 1999. Making large-scale support
vector machine learning practical, pages 169?184.
MIT Press, Cambridge, MA, USA.
Stefan Kaufmann, Cleo Condoravdi, and Valentina
Harizanov, 2006. Formal Approaches to Modality,
pages 72?106. Mouton de Gruyter.
Angelika Kratzer. 1981. The Notional Category of
Modality. In H. J. Eikmeyer and H. Rieser, editors,
Words, Worlds, and Contexts, pages 38?74. de Gruyter,
Berlin.
Angelika Kratzer. 1991. Modality. In Arnim von Ste-
chow and Dieter Wunderlich, editors, Semantics: An
International Handbook of Contemporary Research.
de Gruyter.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods
for kernel-based text analysis. In 41st Meeting of the
Association for Computational Linguistics (ACL?03),
Sapporo, Japan.
Marjorie McShane, Sergei Nirenburg, and Ron
Zacharsky. 2004. Mood and modality: Out of
the theory and into the fray. Natural Language
Engineering, 19(1):57?89.
Roser Morante and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts. In
Proceedings of the BioNLP 2009 Workshop, pages 28?
36, Boulder, Colorado, June. Association for Compu-
tational Linguistics.
Masaki Murata, Kiyotaka Uchimoto, Qing Ma, Toshiyuki
Kanamaru, and Hitoshi Isahara. 2005. Analysis of
machine translation systems? errors in tense, aspect,
and modality. In Proceedings of the 19th Asia-Pacific
Conference on Language, Information and Computa-
tion (PACLIC), Tapei.
Rowan Nairn, Cleo Condorovdi, and Lauri Karttunen.
2006. Computing relative polarity for textual infer-
ence. In Proceedings of the International Workshop on
Inference in Computational Semantics, ICoS-5, pages
66?76, Buxton, England.
M. F. Porter, 1997. An algorithm for suffix stripping,
pages 313?316. Morgan Kaufmann Publishers Inc.,
San Francisco, CA, USA.
Vinodkumar Prabhakaran, Owen Rambow, and Mona
Diab. 2010. Automatic committed belief tagging.
In Coling 2010: Posters, pages 1014?1022, Beijing,
China, August. Coling 2010 Organizing Committee.
63
Brian Roark. 2009. Open vocabulary language model-
ing for binary response typing interfaces. Technical
report, Oregon Health and Science University.
Roser Sauri and James Pustejovsky. 2009. Factbank:
a corpus annotated with event factuality. Language
Resources and Evaluation, 43(3):227?268.
Roser Sauri, Marc Verhagen, and James Pustejovsky.
2006. Annotating and recognizing event modality in
text. In FLAIRS Conference, pages 333?339.
Bengt Sigurd and Barbara Gawro?nska. 1994. Modals
as a problem for MT. In Proceedings of the 15th In-
ternational Conference on Computational Linguistics
(COLING) Volume 1, COLING ?94, pages 120?124,
Kyoto, Japan.
Johan Van Der Auwera and Andreas Ammann, 2005.
Overlap between situational and epistemic modal
marking, chapter 76, pages 310?313. Oxford Univer-
sity Press.
Veronika Vincze, Gy orgy Szarvas, Richa?d Farkas,
Gy orgy Mora, and Ja?nos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9(Suppl 11):S9+.
64
