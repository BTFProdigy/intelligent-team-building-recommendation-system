Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 448?453,
Dublin, Ireland, August 23-24, 2014.
NTNU: Measuring Semantic Similarity with
Sublexical Feature Representations and Soft Cardinality
Andr
?
e Lynum, Partha Pakray, Bj
?
orn Gamb
?
ack Sergio Jimenez
{andrely,parthap,gamback}@idi.ntnu.no sgjimenezv@unal.edu.co
Norwegian University of Science and Technology Universidad Nacional de Colombia
Trondheim, Norway Bogot?a, Colombia
Abstract
The paper describes the approaches taken
by the NTNU team to the SemEval 2014
Semantic Textual Similarity shared task.
The solutions combine measures based
on lexical soft cardinality and character
n-gram feature representations with lexi-
cal distance metrics from TakeLab?s base-
line system. The final NTNU system is
based on bagged support vector machine
regression over the datasets from previous
shared tasks and shows highly competi-
tive performance, being the best system on
three of the datasets and third best overall
(on weighted mean over all six datasets).
1 Introduction
The Semantic Textual Similarity (STS) shared task
aims at providing a unified framework for evaluat-
ing textual semantic similarity, ranging from ex-
act semantic equivalence to completely unrelated
texts. This is represented by the prediction of
a similarity score between two sentences, drawn
from a particular category of text, which ranges
from 0 (different topics) to 5 (exactly equivalent)
through six grades of semantic similarity (Agirre
et al., 2013). This paper describes the NTNU
submission to the SemEval 2014 STS shared task
(Task 10). The approach is based on the lexical
and distributional features of the baseline Take-
Lab system from the 2012 shared task (
?
Sari?c et al.,
2012), but improves on it in three ways: by adding
two new categories of features and by using a bag-
ging regression model to predict similarity scores.
The new feature categories added are based on
soft cardinality and character n-grams, described
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence de-
tails:http://creativecommons.org/licenses/by/4.0/
in Section 2. The parameters of the two cate-
gories are optimised over several corpora and the
features are combined through support vector re-
gression (Section 3) to create the actual systems
(Section 4). As Section 5 shows, the new mea-
sures give the baseline system a substantial boost,
leading to very competitive results in the shared
task evaluation.
2 Feature Generation Methods
The methods used for creating new features utilise
soft cardinality and character n-grams. Soft cardi-
nality (Jimenez et al., 2010) was used successfully
for the STS task in previous SemEval editions
(Jimenez et al., 2012a; Jimenez et al., 2013a).
The NTNU systems utilise an ensemble of such 18
measures, based only on surface text information,
which were extracted using soft cardinality with
different similarity functions, as further described
in Section 2.1.
Section 2.2 then introduces the similarity mea-
sures based on character n-gram feature represen-
tations, which proved themselves as the strongest
features in the STS 2013 task (Marsi et al., 2013).
The measures used here replace character n-gram
features with cluster frequencies or vector val-
ues based on the n-gram collocational structure
learned in an unsupervised manner from text data.
A variety of n-gram feature representations were
trained on subsets of Wikipedia and the best per-
forming ones were used for the new measures,
which are based on cosine similarity between the
document vectors derived from each sentence in a
given pair.
2.1 Soft Cardinality Measures
Soft cardinality resembles classical set cardinality
as it is a method for counting the number of ele-
ments in a set, but differs from it in that similarities
among elements are being considered for the ?soft
counting?. The soft cardinality of a set of words
448
A = {a
1
, a
2
, .., a
|A|
} (a sentence) is defined by:
|A|
sim
=
|A|
?
i=1
w
a
i
?
|A|
j=1
sim(a
i
, a
j
)
p
(1)
Where p is a parameter that controls the cardinal-
ity?s softness (p?s default value is 1) and w
a
i
are
weights for each word, obtained through inverse
document frequency (idf ) weighting. sim(a
i
, a
j
)
is a similarity function that compares two words
a
i
and a
j
using the symmetrized Tversky?s index
(Tversky, 1977; Jimenez et al., 2013a) represent-
ing them as sets of 3-grams of characters. That
is, a
i
= {a
i,1
, a
i,2
, ..., a
i,|a
i
|
} where a
i,n
is the n
th
character trigram in the word a
i
in A. Thus, the
proposed word-to-word similarity is given by:
sim(a
i
, a
j
)=
|c|
?(?|a
min
|+(1??)|a
max
|)+|c|
(2)
?
?
?
?
?
|c| = |a
i
? a
j
|+ bias
sim
|a
min
| = min {|a
i
\ a
j
|, |a
j
\ a
i
|}
|a
max
| = max {|a
i
\ a
j
|, |a
j
\ a
i
|}
The sim function is equivalent to the Dice?s co-
efficient if the three parameters are given their de-
fault values, namely ? = 0.5, ? = 1 and bias = 0.
The soft cardinalities of any pair of sentencesA,
B andA?B can be obtained using Eq. 1. The soft
cardinality of the intersection is approximated by
|A?B|
sim
= |A|
sim
+|B|
sim
?|A?B|
sim
. These
four basic soft cardinalities are algebraically re-
combined to produce an extended set of 18 fea-
tures as shown in Table 1. The featureSTS
sim
is a
parameterized similarity function built by reusing
at word level the symmetrized Tversky?s index
(Eq. 2), whose parameters are tuned from training
data (as further described in Subsection 3.2).
Although this method is based purely on string
matching, the soft cardinality has been shown to
be a very strong baseline for semantic textual com-
parison. The word-to-word similarity sim in Eq. 1
could be replaced by other similarity functions
based on semantic networks or any distributional
representation making this method able to capture
more complex semantic relations among words.
2.2 Sublexical Feature Representations
We have created a set of similarity measures based
on induced representations of character n-grams.
The measures are based on similarity between
STS
sim
(|A|?|A?B|)
/|A|
|A|
(|A|?|A?B|)
/|A?B|
|B|
|B|
/|A?B|
|A ?B|
(|B|?|A?B|)
/|B|
|A ?B|
(|B|?|A?B|)
/|A?B|
|A| ? |A ?B|
|A?B|
/|A|
|B| ? |A ?B|
|A?B|
/|B|
|A ?B| ? |A ?B|
|A?B|
/|A?B|
|A|
/|A?B|
(|A?B|?|A?B|)
/|A?B|
NB: in this table only, | ? | is short for | ? |
sim
Table 1: Soft cardinality features.
document vectors, here the centroid of the individ-
ual term vector representations, which are trained
on character n-grams rather than full words. The
vector representations are induced in an unsuper-
vised manner from large unannotated corpora us-
ing word clustering, topic learning and word rep-
resentation learning methods.
In this paper, three different methods have
been used for creating the character n-gram fea-
ture representations: Brown Clusters (Brown et
al., 1992), Latent Semantic Indexing (LSI) topics
(Deerwester et al., 1990), and log linear skip-gram
models (Mikolov et al., 2013). The Brown clusters
were trained using the implementation by Liang
(2005), while the LSI topic vectors and log linear
skip-gram representations were trained using the
Gensim topic modelling framework (
?
Reh?u?rek and
Sojka, 2010). In addition, tf-idf (Term-Frequency
Inverse Document Frequency) weighting was used
when training LSI topic models. We used a cosine
distance measure between document vectors con-
sisting of the centroid of the term representation
vectors. For Brown clusters, the normalized term
frequency vectors were used with the cluster IDs
instead of the terms themselves. For LSI topic rep-
resentations, the tf-idf weighted topic mixture for
each term was used as the term representation. For
the log linear skip-grams, the word representations
were extracted from the model weight matrix.
3 Feature and Parameter Optimisation
The extracted features and the parameters for the
two methods described in the previous section
were optimised over several sets of training data.
As no training data was explicitly provided for the
STS evaluation campaign this year, we used dif-
ferent training sets from past campaigns and from
Wikipedia for the new test sets.
449
Test set Training set
deft-forum
MSRvid 2012 train and test +
OnWN 2012 and 2013 test
deft-news MSRvid 2012 train + test
headlines headlines 2013 test
images MSRvid 2012 train + test
OnWN OnWN 2012 and 2013 test
tweet-news
SMTeuroparl 2012 test +
SMTnews 2012 test
Table 2: Training-test set pairs.
3.1 Training Data and Pre-processing
The training-test sets pairs used for optimising the
parameters of the soft cardinality methods were
selected from the STS 2012 and STS 2013 task,
as shown in Table 2. The character n-gram repre-
sentation vectors were trained in an unsupervised
manner on two subsets of Wikipedia consisting,
respectively, of the first 12 million words (10
8
characters, hence referred to as Wiki8) and of 125
million words (10
9
characters; Wiki9).
First, however, the training data had to be pre-
processed. Thus, before extracting the idf weights
and the soft cardinality features, all the texts
shown in Table 2 were passed through the follow-
ing four pre-processing steps:
(i) tokenization and stop-word removal (pro-
vided by NLTK, Bird et al. (2009)),
1
(ii) conversion to lowercase characters,
(iii) punctuation and special character removal
(e.g., ?.?, ?;?, ?$?, ?&?), and
(iv) Porter stemming.
Character n-grams including whitespace were
generated from the Wikipedia texts, which in con-
trast only were pre-processed in a 3-step chain:
(i) removal of punctuation and extra whites-
pace,
(ii) replacing numbers with their single digit
word (?one?, ?two?, etc.), and
(iii) lowercasing all text.
1
http://www.nltk.org/
Data ? ? bias p ?
?
?
?
bias
?
deft-forum 1.01 -1.01 0.24 0.93 -2.71 0.42 1.63
deft-news 3.36 -0.64 1.37 0.44 2.36 0.72 0.02
headlines 0.36 -0.29 4.17 0.85 -4.50 0.43 0.19
images 1.12 -1.11 0.93 0.64 -0.98 0.50 0.11
OnWN 0.53 -0.53 1.01 1.00 -4.89 0.52 0.46
tweet-news 0.13 0.14 2.80 0.01 2.66 1.74 0.45
Table 3: Optimal parameters used for each dataset.
3.2 Soft Cardinality Parameter Optimisation
The first feature in Table 1, STS
sim
, was used to
optimise the four parameters ?, ?, bias, and p in
the following way. First, we built a text similarity
function reusing Eq. 2 for comparing two sets of
words (instead of two sets of character 3-grams)
and replacing the classic cardinality |?| by the soft
cardinality | ? |
sim
from Eq. 1. This text similarity
function adds three parameters (?
?
, ?
?
, and bias
?
)
to the initial four parameter set (?, ?, bias, and p).
Second, these seven parameters were set to their
default values and the scores obtained from this
function for each pair of sentences were compared
to the gold standards in the training data using
Pearson?s correlation. The parameter search space
was then explored iteratively using hill-climbing
until reaching optimal Pearson?s correlation. The
criterion for assignment of training-test set pairs
was by closeness of average character length. The
optimal training parameters are shown in Table 3.
3.3 Parameters for N-gram Feature Training
The character n-gram feature representation vec-
tors were trained while varying the parameters of
n-gram size, cluster size, and term frequency cut-
offs for all models. For the log linear skip-gram
models, our intuition is that a larger skip-gram
context is needed than the 5 or 10 wide skip-grams
used to train word-based representations due to the
smaller term vocabulary and dependency between
adjacent n-grams, so instead we trained models us-
ing skip-gram widths of 25 or 50 terms. Term fre-
quency cut-offs were set to limit the model size,
but also potentially serve as a regularization on
the resulting measure. In detail, the following sub-
lexical representation measures are used:
? Log linear skip-gram representations of char-
acter 3- and 4-grams of size 1000 and 2000,
respectively. Trained on the Wiki8 corpus us-
ing a skip gram window of size 25 and 50,
and frequency cut-off of 5.
450
? Brown clusters with size 1024 of character 4-
grams using a frequency cut-off of 20.
? Brown clusters of character 3-, 4- and 5-
grams with cluster sizes of resp. 1024, 2048
and 1024. The representations are trained on
the Wiki9 corpus with successively increas-
ing frequency cut-offs of 20, 320 and 1200.
? LSI topic vectors based on character 4-grams
of size 2000. Trained on the Wiki8 corpus
using a frequency cut-off of 5.
? LSI topic vectors based on character 4-grams
of size 1000. Trained on the Wiki9 corpus
using a frequency cut-off of 80.
3.4 Similarity Score Regression
The final sentence pair similarity score is predicted
by a Support Vector Regression (SVR) model with
a Radial Basis (RBF) kernel (Vapnik et al., 1997).
The model is trained on all the test data for the
2013 STS shared task combined with all the trial
and test data of the 2012 STS shared task.
The combined dataset hence consists of about
7,500 sentence pairs from nine different text cat-
egories: five sets from the annotated data sup-
plied to STS 2012, based on Microsoft Research
Paraphrase and Video description corpora (MSR-
par and MSvid), statistical machine translation
system output (SMTeuroparl and SMTnews), and
sense mappings between OntoNotes and WordNet
(OnWN); and four sets from the STS 2013 test
data: headlines (news headlines), SMT, OnWN,
and FNWM (mappings of sense definitions from
FrameNet and WordNet).
The SVR model was trained as a bagged classi-
fier, that is, for each run, 100 regression models
were trained with 80% of the samples and fea-
tures of the original training set drawn with re-
placement. The outputs of all models were then
averaged into a final prediction. This bagged train-
ing procedure adds extra regularization, which can
reduce the instability of prediction accuracy be-
tween different test data categories.
The prediction pipeline was implemented with
the Scikit-learn software framework (Pedregosa et
al., 2011), and the SVR models were trained with
the implementation?s default parameters: cost
penalty (C) 1.0, margin () 0.1, and RBF precision
(?) 1/|featurecount|.
We were unable to improve the performance
over these defaults by cross validation parameter
search unless the models were trained for specific
text categories. Consequently no parameter opti-
mization was performed during training of the fi-
nal systems.
4 Submitted Systems
The three submitted systems consist of one us-
ing only the soft cardinality features described in
Section 3.2 (NTNU-run1), one system using a
baseline set of lexical measures and WordNet aug-
mented similarity in addition to the new sublexical
representation measures (NTNU-run2), and one
(NTNU-run3) which combines the output from
the other two systems by taking the mean of the
two sets of predictions. NTNU-run3 thus repre-
sents a combination of the measures and methods
introduced by NTNU-run1 and NTNU-run2.
In addition to the sublexical feature measures
described in Section 3.3, NTNU-run2 uses the fol-
lowing baseline features adapted from the Take-
Lab 2012 system submission (
?
Sari?c et al., 2012).
? Simple lexical features: Relative document
length differences, number overlap, case
overlap, and stock symbol named entity
recognition.
? Lemma and word n-gram overlap of orders 1-
3, frequency weighted lemma and word over-
lap, and WordNet augmented overlap.
? Cosine similarity between the summed word
representation vectors from each sentence us-
ing LSI models based on large corpora with
or without frequency weighting.
The specific measures used in the submitted
systems were found by training the regression
model on the STS 2012 shared task data and eval-
uating on the STS 2013 test data. We used a step-
wise forward feature selection method by compar-
ing mean (but unweighted) correlation on the four
test categories in order to identify the subset of
measures to include in the final system.
The system composes a feature set of similar-
ity scores from these 20 baseline measures and the
nine sublexical representation measures, and uses
these to train a bagged SVM regressor as described
in Section 3.4 in order to predict the final semantic
similarity score for new sentence pairs.
451
NTNU-run1 NTNU-run2 NTNU-run3 Best
Dataset r rank r rank r rank r
deft-forum 0.4369 16 0.5084 2 0.5305 1 0.5305
deft-news 0.7138 14 0.7656 6 0.7813 2 0.7850
headlines 0.7219 17 0.7525 13 0.7837 1 0.7837
images 0.8000 9 0.8129 4 0.8343 1 0.8343
OnWN 0.8348 7 0.7767 20 0.8502 4 0.8745
tweet-news 0.4109 33 0.7921 1 0.6755 13 0.7921
mean 0.6531 20 0.7347 4 0.7426 2 0.7429
weighted mean 0.6631 21 0.7491 4 0.7549 3 0.7610
Table 4: Final evaluation results for the submitted systems.
5 Results and Discussion
The final evaluation results for the three submit-
ted systems are shown in Table 4, where the right-
most column (?Best?) for comparison displays the
performance figures obtained by any of the 38 sys-
tems on each dataset.
The systems using sublexical representation
based measures show competitive performance,
ranking third and fourth among the submitted sys-
tems with a weighted mean correlation of ?0.75.
They also produced the best result in four out of
the six text categories in the evaluation dataset,
with NTNU-run3 being the #1 system on deft-
forum, headlines and images, #2 on deft-news, and
#4 on OnWN. It would thus have been the clear
winner if it had not been for its sub-par perfor-
mance on the tweet-news dataset, which on the
other hand is the category NTNU-run2 was the
best of all systems on.
The system based solely on soft cardinality fea-
tures, NTNU-run1, displays more modest perfor-
mance ranking at 21
st
place (of the in total 38 sub-
mitted systems) with ?0.66 correlation. This is a
bit surprising, since this method for obtaining fea-
tures from pairs of texts was used successfully in
other SemEval tasks such as cross-lingual textual
entailment (Jimenez et al., 2012b) and student re-
sponse analysis (Jimenez et al., 2013b). Similarly,
Croce et al. (2012) used soft cardinality represent-
ing text as a bag of dependencies (syntactic soft
cardinality) obtaining the best results in the typed-
similarity task (Croce et al., 2013).
From our results it can be noted that for most
categories the sublexical representation measures
show strong performance in NTNU-run2, with a
significantly better result for the combined sys-
tem NTNU-run3. This indicates that while the soft
cardinality features are weaker predictors overall,
they are complimentary to the sublexical and lex-
ical features of NTNU-run2. It is also indicative
that this is not the case for the tweet-news cate-
gory, where the text is more ?free form? and less
normative, so it would be expected that sublexical
approaches should have stronger performance.
Acknowledgements
This work was made possible with the support
from Department of Computer and Information
Science, Norwegian University of Science and
Technology.
Partha Pakray was 2013?2014 supported by an
ERCIM Alain Bensoussan Fellowship.
The NTNU systems are partly based on code
made available by the Text Analysis and Knowl-
edge Engineering Laboratory, Department of
Electronics, Microelectronics, Computer and In-
telligent Systems, Faculty of Electrical Engineer-
ing and Computing, University of Zagreb.
452
References
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 shared
task: Semantic textual similarity. In Second Joint
Conference on Lexical and Computational Seman-
tics (*SEM), Volume 1: Proceedings of the Main
Conference and the Shared Task: Semantic Textual
Similarity, pages 32?43, Atlanta, Georgia, USA,
June.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media, Inc.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467?479.
Danilo Croce, Valerio Storch, P. Annesi, and Roberto
Basili. 2012. Distributional compositional seman-
tics and text similarity. In 2012 IEEE Sixth Interna-
tional Conference on Semantic Computing (ICSC),
pages 242?249, September.
Danilo Croce, Valerio Storch, and Roberto Basili.
2013. UNITOR-CORE TYPED: Combining text
similarity and semantic filters through SV regres-
sion. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 1: Pro-
ceedings of the Main Conference and the Shared
Task: Semantic Textual Similarity, pages 59?65, At-
lanta, Georgia, USA, June.
Scott C. Deerwester, Susan T Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. JASIS,
41(6):391?407.
Sergio Jimenez, Fabio Gonzalez, and Alexander Gel-
bukh. 2010. Text comparison using soft cardi-
nality. In Edgar Chavez and Stefano Lonardi, ed-
itors, String Processing and Information Retrieval,
volume 6393 of LNCS, pages 297?302. Springer,
Berlin, Heidelberg.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012a. Soft cardinality: A parameterized
similarity function for text comparison. In Proceed-
ings of the Sixth International Workshop on Seman-
tic Evaluation (SemEval 2012), Montr?eal, Canada,
7-8 June.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012b. Soft cardinality+ ML: Learning adap-
tive similarity functions for cross-lingual textual en-
tailment. In Proceedings of the Sixth International
Workshop on Semantic Evaluation (SemEval 2012),
Montr?eal, Canada, 7-8 June.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2013a. SOFTCARDINALITY-CORE: Im-
proving text overlap with distributional measures for
semantic textual similarity. In Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM), Volume 1: Proceedings of the Main Con-
ference and the Shared Task: Semantic Textual Sim-
ilarity, Atlanta, Georgia, USA, June.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2013b. SOFTCARDINALITY: Hierarchical
text overlap for student response analysis. In Second
Joint Conference on Lexical and Computational Se-
mantics (*SEM), Volume 1: Proceedings of the Main
Conference and the Shared Task: Semantic Textual
Similarity, Atlanta, Georgia, USA, June.
Percy Liang. 2005. Semi-supervised learning for nat-
ural language. Ph.D. thesis, Massachusetts Institute
of Technology.
Erwin Marsi, Hans Moen, Lars Bungum, Gleb Sizov,
Bj?orn Gamb?ack, and Andr?e Lynum. 2013. NTNU-
CORE: Combining strong features for semantic sim-
ilarity. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 1: Pro-
ceedings of the Main Conference and the Shared
Task: Semantic Textual Similarity, pages 66?73, At-
lanta, Georgia, USA, June.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
Radim
?
Reh?u?rek and Petr Sojka. 2010. Software
Framework for Topic Modelling with Large Cor-
pora. In Proceedings of the LREC 2010 Workshop
on New Challenges for NLP Frameworks, pages 45?
50, Valletta, Malta, May. ELRA.
Amos Tversky. 1977. Features of similarity. Psycho-
logical Review, 84(4):327?352, July.
Vladimir Vapnik, Steven E. Golowich, and Alex
Smola. 1997. Support vector method for function
approximation, regression estimation, and signal
processing. In Michael C. Mozer, Michael I. Jordan,
and Thomas Petsche, editors, Advances in Neural
Information Processing Systems, volume 9, pages
281?287. MIT Press, Cambridge, Massachusetts.
Frane
?
Sari?c, Goran Glava?s, Mladen Karan, Jan
?
Snajder,
and Bojana Dalbelo Ba?si?c. 2012. TakeLab: Sys-
tems for measuring semantic text similarity. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 441?448,
Montr?eal, Canada, 7-8 June.
453
Proc. of 5th Workshop on Cognitive Aspects of Computational Language Learning (CogACLL) @ EACL 2014, pages 49?54,
Gothenburg, Sweden, April 26 2014.
c?2014 Association for Computational Linguistics
Agent-based modeling of language evolution
Torvald Lekvam Bj
?
orn Gamb
?
ack Lars Bungum
Department of Computer and Information Science, Sem S?lands vei 7?9
Norwegian University of Science and Technology, 7491 Trondheim, Norway
torvald@lekvam.no {gamback,larsbun}@idi.ntnu.no
Abstract
Agent-based models of language evolution
have received a lot of attention in the last
two decades. Researchers wish to under-
stand the origin of language, and aim to
compensate for the lacking empirical evi-
dence by utilizing methods from computer
science and artificial life. The paper looks
at the main theories of language evolution:
biological evolution, learning, and cultural
evolution. In particular, the Baldwin effect
in a naming game model is elaborated on
by describing a set of experimental simu-
lations. This is on-going work and ideas
for further investigating the social aspects
of language evolution are also discussed.
1 Introduction
What is language? It is interesting how we can
take a train of thought and transfer this into an-
other person?s mind by pushing the air around us.
Human language, this complex medium that dis-
tinctly separates humans from animals, has baf-
fled scientists for centuries. While animals also
use language, even with a degree of syntax (Kako,
1999), spoken human language exhibits a vastly
more complex structure and spacious variation.
To understand how language works ? how it
is used, its origin and fundamentals ? our best
information sources are the languages alive (and
some extinct but documented ones). Depending on
definition, there are 6,000?8,000 languages world-
wide today, showing extensive diversity of syntax,
semantics, phonetics and morphology (Evans and
Levinson, 2009). Still, these represent perhaps
only 2% of all languages that have ever existed
(Pagel, 2000). As this is a rather small window, we
want to look back in time. But there is a problem
in linguistic history: our reconstruction techniques
can only take us back some 6,000 to 7,000 years.
Beyond this point, researchers can only speculate
on when and how human language evolved: either
as a slowly proceeding process starting millions
of years (Ma) ago, e.g., 7 Ma ago with the first
appearance of cognitive capacity or 2.5 Ma ago
with the first manufacture of stone implements; or
through some radical change taking place about
100 ka ago with the appearance of the modern hu-
mans or 50?60 ka ago when they started leaving
Africa (Klein, 2008; Tattersall, 2010).
The rest of this introduction covers some key
aspects of language evolution. Section 2 then fo-
cuses on computational models within the field,
while Section 3 describes a specific naming game
model. Finally, Section 4 discusses the results and
some ideas for future work.
1.1 Theories of origin: the biological aspect
There are two main ideas in biological evolution
as to why humans developed the capacity to com-
municate through speech. The first states that lan-
guage (or more precisely the ability to bear the full
structure of language) came as an epiphenomenon,
a by-product (spandrel) of an unrelated mutation.
This theory assumes that a mental language fac-
ulty could not by itself evolve by natural selection;
there would simply be too many costly adaptations
for it to be possible. Thus there should exist an in-
nate capacity in the form of a universal grammar
(Chomsky, 1986), which can hold a finite number
of rules enabling us to carry any kind of language.
According to the second idea, language
emerged in a strictly adaptational process (Pinker
and Bloom, 1990). That is, that language evolu-
tion can be explained by natural selection, in the
same way as the evolution of other complex traits
like echolocation in bats or stereopsis in monkeys.
Both ideas ? innate capacity vs natural selection
? have supporters, as well as standpoints that
hold both aspects as important, but at different lev-
els (Deacon, 2010; Christiansen and Kirby, 2003).
49
1.2 Theories of origin: the cultural aspect
Biology aside, the forces behind the emergence of
human language are not strictly genetic (and do
not operate only on a phylogenetic time scale).
Kirby (2002) argues that, in addition to biological
evolution, there are two more complex adaptive
(dynamical) systems influencing natural language;
namely cultural evolution (on the glossogenetic
time scale) and learning (which operates on a in-
dividual level, on the ontogenetic time scale).
In addition, there is the interesting Darwinian
idea that cultural learning can guide biological
evolution, a process known as the Baldwin effect
(Baldwin, 1896; Simpson, 1953). This theory ar-
gues that culturally learned traits (e.g., a univer-
sal understanding of grammar or a defense mech-
anism against a predator) can assimilate into the
genetic makeup of a species. Teaching each mem-
ber in a population the same thing over and over
again comes with great cost (time, faulty learn-
ing, genetic complexity), and the overall popula-
tion saves a lot of energy if a learned trait would
become innate. On the other hand, there is a cost
of genetic assimilation as it can prohibit plastic-
ity in future generations and make individuals less
adaptive to unstable environments.
There has been much debate recently whether
language is a result of the Baldwin effect or not
(Evans and Levinson, 2009; Chater et al., 2009;
Baronchelli et al., 2012, e.g.), but questions, hypo-
theses, and simulations fly in both directions.
2 Language evolution and computation
Since the 90s, there has been much work on sim-
ulation of language evolution in bottom-up sys-
tems with populations of autonomous agents. The
field is highly influenced by the work of Steels and
Kirby, respectively, and has been summarized and
reviewed both by themselves and others (Steels,
2011; Kirby, 2002; Gong and Shuai, 2013, e.g.).
Computational research in this field is limited
to modeling very simplified features of human
language in isolation, such as strategies for nam-
ing colors (Bleys and Steels, 2011; Puglisi et al.,
2008), different aspects of morphology (Dale and
Lupyan, 2012), and similar. This simplicity is im-
portant to keep in mind, since it is conceivable that
certain features of language can be highly influ-
enced by other features in real life.
A language game simulation (Steels, 1995) is
a model where artificial agents interact with each
other in turn in order to reach a cooperative goal;
to make up a shared language of some sort, all
while minimizing their cognitive effort. All agents
are to some degree given the cognitive ability to
bear language, but not given any prior knowledge
of how language should look like or how consen-
sus should unfold. No centralized anchors are in-
volved: a simulation is all self-organized.
Agents are chosen (mostly at random) as hearer
and speaker, and made to exchange an utterance
about a certain arbitrary concept or meaning in
their environment. If the agents use the same lan-
guage (i.e., the utterance is understood by both
parties), the conversation is a success. If the
speaker utters something unfamiliar to the hearer,
the conversation is termed a failure. If an agent
wants to express some concept without having any
utterances for it, the agent is assumed to have the
ability to make one up and add this to its memory.
While interpretation in real life is a complex af-
fair, it is mostly assumed that there is a fairly direct
connection between utterance and actual meaning
in language game models (emotions and social sit-
uations do not bias how language is interpreted).
A simple language game normally is charac-
terized by many synonyms spawning among the
agents. As agents commence spreading their own
utterances around, high-weighted words start to be
preferred. Consensus is reached when all agents
know the highest weighted word for each concept.
Commonly, the agents aim to reach a single co-
herent language, but the emergence of multilin-
gualism has also been simulated (Lipowska, 2011;
Roberts, 2012). Cultural evolution can be captured
by horizontal communication between individuals
in the same generation or vertical communication
from adults to children. The latter typically lets the
agents breed, age and die, with the iterated learn-
ing model (Smith et al., 2003) being popular.
A variety of language games exist, from sim-
ple naming games, where the agents? only topic
concerns one specific object (Lipowska, 2011), to
more cognitive grounding games (Steels and Loet-
zsch, 2012). There have also been studies on some
more complex types of interaction, such as spa-
tial games (Spranger, 2013), factual description
games (van Trijp, 2012) and action games (Steels
and Spranger, 2009), where the agent communi-
cation is about objects in a physical environment,
about real-world events, and about motoric behav-
iors, respectively.
50
3 The Baldwin effect in a naming game
Several researchers have created simulations to in-
vestigate the Baldwin effect, starting with Hinton
and Nowlan (1987). Cangelosi and Parisi (2002)
simulate agents who evolve a simple grammatical
language in order to survive in a world filled with
edible and poisonous mushrooms. Munroe and
Cangelosi (2002) used this model to pursue the
Baldwin effect, with partially blind agents initially
having to learn features of edible mushrooms, but
with the learned abilities getting more and more
assimilated into the genome over the generations.
Chater et al. (2009) argue that only stable parts of
language may assimilate into the genetic makeup,
while variation within the linguistic environment
is too unstable to be a target of natural selection.
Watanabe et al. (2008) use a similar model, but in
contrast state that genetic assimilation not neces-
sarily requires a stable linguistic environment.
Lipowska (2011) has pursued the Baldwin ef-
fect in a simple naming game model with the in-
tention of mixing up a language game in a simu-
lation that incorporates both learning, cultural and
biological evolution. The model places a set of
agents in a square lattice of a linear size L, where
every agent is allowed ? by a given probability p
? to communicate with a random neighbor.
At each time step, a random agent is chosen and
p initially decides whether the agent is allowed to
communicate or will face a ?population update?.
Every agent has an internal lexicon of N words
with associated weights (w
j
: 1 ? j ? N ). When-
ever a chosen speaker is to utter a word, the agent
selects a word i from its lexicon with the probabil-
ity w
i
/
?
N
j=1
w
j
. If the lexicon is empty (N = 0),
a word is made up. A random neighbor in the lat-
tice is then chosen as the hearer. If both agents
know the uttered word, the dialog is deemed a
success, and if not, a failure. Upon success, both
agents increase the uttered word?s weight in their
lexica by a learning ability variable. Each agent k
is equipped with such a variable l (0 < l
k
< 1).
This learning ability is meant to, in its simplicity,
reflect the genetic assimilation.
Instead of engaging in communication, the cho-
sen agent is occasionally updated, by a probability
1? p. Agents die or survive with a probability p
s
which is given by an equation that takes into ac-
count age, knowledge (lexicon weights in respect
to the population?s average weights), and simula-
tion arguments. If the agent has a high-weighted
lexicon and is young of age, and therefore survives
at a given time step, the agent is allowed to breed
if there are empty spaces in its neighborhood.
All in all, each time step can terminate with
eight different scenarios: in addition to the two
communication scenarios (success or failure), the
scenario where the agent dies, as well as the one
where the agents lives but only has non-empty
neighbors (so that no change is possible), there are
four possibilities for breeding. If the agent breeds,
the off-spring either inherit the parent?s learning
ability or gain a new learning ability, with a proba-
bility p
m
. With the same mutation probability, the
off-spring also either gains a new word or inherits
the parent?s highest-weight word.
This model was implemented with the aim to
reproduce Lipowska?s results. She argues that her
model is fairly robust to both population size and
her given arguments; however, our experiments
do not support this: as the Baldwin effect unfold,
it does not follow the same abrupt course as in
Lipowska?s model. This could be due to some as-
sumptions that had to be made, since Lipowska
(2011), for instance, presents no details on how
age is calculated. We thus assume that every time
an agent is allowed to communicate, its age gets
incremented. Another possibility could be to in-
crement every agent?s age at every time step, so
that agents get older even if they do not commu-
nicate. Furthermore, the initial values for learn-
ability are not clearly stated. Lipowska uses sev-
eral different values in her analysis. We have used
0.5, which makes a decrease in learnability a part
of the evolutionary search space as well.
Simulations with parameters similar to those
used by Lipowska (2011) [iterations = 200, 000,
mutation
chance
= 0.01, L = 25, p = 0.4, l = 0.5],
produce results as in Figure 1, showing the highest
weighted word per agent after 50k and 150k time
steps, with each agent being a dot in a ?heat map?;
black dots indicate dead agents (empty space).
The number of groups are reduced over time, and
their sizes grow, as more agents agree on a lex-
icon and as favorable mutations spread through
the population, (as indicated by agent learnability;
Figure 2). Even after 200k iterations, consensus is
not reached (which it was in Lipowska?s simula-
tion), but the agent population agrees on one word
if the simulation is allowed to run further. It is nat-
ural to assume that the difference lays in the details
of how age is calculated, as noted above.
51
Figure 1: Ca 16 different words dominate the pop-
ulation at iteration 50k and nine at iteration 150k.
Figure 2: Mutations favoring learnability at itera-
tion 50k spread substantially by iteration 150k.
Diverting from Lipowska?s parameters and
skewing towards faster turnover (higher mutation
rate, higher possibility of survival with richer lex-
icon/higher age, etc.), gives behavior similar to
hers, with faster and more abrupt genetic assim-
ilation, as shown Figure 3. The upper line in the
figure represents the fraction of agents alive in the
lattice. It is initially fully populated, but the popu-
lation decreases with time and balances at a point
where death and birth are equally tensioned.
Agents with higher learnability tend to live
longer, and the lower graph in Figure 3 shows the
average learnability in the population. It is roughly
sigmoid (S-shaped; cf. Lipowska?s experiment) as
a result of slow mutation rate in the first phase,
followed by a phase with rapid mutation rate (ca
100k?170k) as the learnability also gets inherited,
and decreasing rate towards the end when mu-
tations are more likely to ruin agent learnability
(when the learning ability l is at its upper limit).
As can be seen in Figure 4, the agents rapidly get
to a stable weighted lexicon before the Baldwin
effect shows itself around time step 100k.
As mentioned, Lipowska?s model did not reflect
the robustness argued in her paper: for other val-
ues of p, the number of empty spots in the popu-
lation lattice starts to diverge substantially, and for
some values all agents simply die. As population
sizes vary, the number of iterations must also be
adjusted to get similar results. If not, the agents
will not reach the same population turn-over as
Figure 3: Fraction of agents alive in the lattice and
average learnability in the population (s-shaped).
Figure 4: Average sum of weights in agent lexica.
for smaller population sizes since only one agent
may be updated per iteration. Lipowska (2011)
compensated with higher mutation rate on simu-
lations with different population sizes; however,
these could be two variables somewhat more inde-
pendent of each other. The model would have been
much more stable if it contained aspects of a typi-
cal genetic algorithm, where agents are allowed to
interact freely within generations. This way, the
model could be acting more upon natural selec-
tion (and in search of the Baldwin effect), instead
of relying on well-chosen parameters to work.
4 Discussion and future work
Language is a complex adaptive system with nu-
merous variables to consider. Thus we must make
a number of assumptions when studying language
and its evolution, and can only investigate certain
aspects at a time through simplifications and ab-
stractions. As this paper has concentrated on the
agent-based models of the field, many studies re-
flecting such other aspects had to be left out.
In addition, there has lately been a lot of work
studying small adjustments to the agent-based
models, in order to make them more realistic by,
for example, having multiple hearers in a lan-
guage game conversations (Li et al., 2013), dif-
ferent topologies (Lei et al., 2010; Lipowska and
Lipowski, 2012), and more heterogeneous popula-
tions (Gong et al., 2006).
52
In general, though, simulations on language
evolution tend to have relatively small and fixed
sizes (Baronchelli et al., 2006; Vogt, 2007) ? and
few studies seem to take social dynamics (Gong et
al., 2008; Kalampokis et al., 2007) or geography
into account (Patriarca and Heinsalu, 2009).
Further work is still needed to make existing
models more realistic and to analyze relations be-
tween different models (e.g., by combining them).
Biological evolution could be studied with more
flexible (or plastic) neural networks. Cultural evo-
lution could be investigated under more realistic
geographical and demographical influence, while
learning could be analyzed even further in light of
social dynamics, as different linguistic phenom-
ena unfold. Quillinan (2006) presented a model
concerning how a network of social relationships
could evolve with language traits. This model
could be taken further in combination with exist-
ing language games or it could be used to show
how language responds to an exposure of continu-
ous change in a complex social network.
Notably, many present models have a rather
na??ve way of selecting cultural parents, and a
genetic algorithm for giving fitness to agents in
terms of having (assimilated) the best strategies
for learning (e.g., memory efficiency), social con-
ventions (e.g., emotions, popularity), and/or sim-
ple or more advanced grammar could be explored.
A particular path we aim to pursue is to study a
language game with a simple grammar under so-
cial influence (e.g., with populations in different
fixed and non-fixed graphs, with multiple hearers),
contained within a genetic algorithm. In such a
setting, the agents must come up with strategies
for spreading and learning new languages, and
need to develop fault-tolerant models for speaking
with close and distant neighbors. This could be a
robust model where a typical language game could
be examined, in respect to both biological and cul-
tural evolution, with a more realistic perspective.
Acknowledgments
We would like thank the three anonymous review-
ers for several very useful comments. Thanks also
to Keith Downing for providing feedback on work
underlying this article.
The third author is supported by a grant from the
Norwegian University of Science and Technology.
Part of this work was funded by the PRESEMT
project (EC grant number FP7-ICT-4-248307).
References
James Mark Baldwin. 1896. A new factor in evolution.
The American Naturalist, 30(354):441?451.
Andrea Baronchelli, Maddalena Felici, Vittorio Loreto,
Emanuele Caglioti, and Luc Steels. 2006. Sharp
transition towards shared vocabularies in multi-
agent systems. Journal of Statistical Mechanics:
Theory and Experiment, 2006(06):P06014.
Andrea Baronchelli, Nick Chater, Romualdo Pastor-
Satorras, and Morten H. Christiansen. 2012. The
biological origin of linguistic diversity. PLoS ONE,
7(10):e48029.
Joris Bleys and Luc Steels. 2011. Linguistic selec-
tion of language strategies. In G. Kampis, I. Kar-
sai, and E. Szathm?ary, editors, Advances in Artificial
Life. Darwin Meets von Neumann, volume 2, pages
150?157. Springer.
Angelo Cangelosi and Domenico Parisi. 2002. Com-
puter simulation: A new scientific approach to the
study of language evolution. In Angelo Cangelosi
and Domenico Parisi, editors, Simulating the Evolu-
tion of Language, chapter 1, pages 3?28. Springer,
London.
Nick Chater, Florencia Reali, and Morten H Chris-
tiansen. 2009. Restrictions on biological adaptation
in language evolution. Proceedings of the National
Academy of Sciences, 106(4):1015?1020.
Noam Chomsky. 1986. Knowledge of language: Its
nature, origins, and use. Greenwood.
Morten H. Christiansen and Simon Kirby. 2003.
Language evolution: consensus and controversies.
TRENDS in Cognitive Sciences, 7(7):300?307.
Rick Dale and Gary Lupyan. 2012. Understanding
the origins of morphological diversity: the linguis-
tic niche hypothesis. Advances in Complex Systems,
15(03n04):1150017.
Terrence W. Deacon. 2010. A role for relaxed se-
lection in the evolution of the language capacity.
Proceedings of the National Academy of Sciences,
107(Supplement 2):9000?9006.
Nicholas Evans and Stephen C. Levinson. 2009. The
myth of language universals: Language diversity
and its importance for cognitive science. Behavioral
and Brain Sciences, 32(05):429?448.
Tao Gong and Lan Shuai. 2013. Computer simulation
as a scientific approach in evolutionary linguistics.
Language Sciences, 40:12?23.
Tao Gong, James W. Minett, and William S-Y Wang.
2006. Language origin and the effects of individ-
uals popularity. In Proceedings of the 2006 IEEE
Congress on Evolutionary Computation, pages 999?
1006, Vancouver, British Columbia, Jul. IEEE.
53
Tao Gong, James W. Minett, and William S-Y Wang.
2008. Exploring social structure effect on language
evolution based on a computational model. Connec-
tion Science, 20(2-3):135?153.
Geoffrey E Hinton and Steven J Nowlan. 1987. How
learning can guide evolution. Complex systems,
1(3):495?502.
Edward Kako. 1999. Elements of syntax in the sys-
tems of three language-trained animals. Animal
Learning & Behavior, 27(1):1?14.
Alkiviadis Kalampokis, Kosmas Kosmidis, and Panos
Argyrakis. 2007. Evolution of vocabulary on scale-
free and random networks. Physica A: Statistical
Mechanics and its Applications, 379(2):665 ? 671.
Simon Kirby. 2002. Natural language from artificial
life. Artificial Life, 8(2):185?215.
Richard G. Klein. 2008. Out of Africa and the evo-
lution of human behavior. Evolutionary Anthropol-
ogy: Issues, News, and Reviews, 17(6):267?281.
Chuang Lei, Jianyuan Jia, Te Wu, and Long Wang.
2010. Coevolution with weights of names in struc-
tured language games. Physica A: Statistical Me-
chanics and its Applications, 389(24):5628?5634.
Bing Li, Guanrong Chen, and Tommy W.S. Chow.
2013. Naming game with multiple hearers. Com-
munications in Nonlinear Science and Numerical
Simulation, 18(5):1214?1228.
Dorota Lipowska and Adam Lipowski. 2012. Naming
game on adaptive weighted networks. Artificial Life,
18(3):311?323.
Dorota Lipowska. 2011. Naming game and compu-
tational modelling of language evolution. Compu-
tational Methods in Science and Technology, 17(1?
2):41?51.
Steve Munroe and Angelo Cangelosi. 2002. Learning
and the evolution of language: the role of cultural
variation and learning costs in the Baldwin effect.
Artificial Life, 8(4):311?339.
Mark Pagel. 2000. The history, rate and pattern of
world linguistic evolution. In Ch. Knight, J.R. Hur-
ford, and M. Studdert-Kennedy, editors, The Evo-
lutionary Emergence of Language: Social Func-
tion and the Origins of Linguistic Form, chapter 22,
pages 391?416. Cambridge University Press.
Marco Patriarca and Els Heinsalu. 2009. Influence
of geography on language competition. Physica A:
Statistical Mechanics and its Applications, 388(2?
3):174?186.
Steven Pinker and Paul Bloom. 1990. Natural lan-
guage and natural selection. Behavioral and Brain
Sciences, 13:707?784.
Andrea Puglisi, Andrea Baronchelli, and Vittorio
Loreto. 2008. Cultural route to the emergence of
linguistic categories. Proceedings of the National
Academy of Sciences, 105(23):7936?7940.
Justin Quillinan. 2006. Social networks and cultural
transmission. Master of Science Thesis, School
of Philosophy, Psychology and Language Sciences,
University of Edinburgh, Edinburgh, Scotland, Aug.
Sean Geraint Roberts. 2012. An evolutionary ap-
proach to bilingualism. Ph.D. thesis, School of Phi-
losophy, Psychology and Language Sciences, Uni-
versity of Edinburgh, Edinburgh, Scotland, Oct.
George Gaylord Simpson. 1953. The Baldwin effect.
Evolution, 7(2):110?117.
Kenny Smith, Simon Kirby, and Henry Brighton.
2003. Iterated learning: A framework for the emer-
gence of language. Artificial Life, 9(4):371?386.
Michael Spranger. 2013. Evolving grounded spa-
tial language strategies. KI-K?unstliche Intelligenz,
27(2):1?10.
Luc Steels and Martin Loetzsch. 2012. The grounded
naming game. In L. Steels, editor, Experiments in
Cultural Language Evolution, pages 41?59. John
Benjamins.
Luc Steels and Michael Spranger. 2009. How ex-
perience of the body shapes language about space.
In Proceedings of the 21st International Joint Con-
ference on Artificial Intelligence, pages 14?19,
Pasadena, California, Jul. IJCAI.
Luc Steels. 1995. A self-organizing spatial vocabulary.
Artificial Life, 2(3):319?332.
Luc Steels. 2011. Modeling the cultural evolution of
language. Physics of Life Reviews, 8(4):339?356.
Ian Tattersall. 2010. Human evolution and cognition.
Theory in Biosciences, 129(2?3):193?201.
Remi van Trijp. 2012. The evolution of case systems
for marking event structure. In L. Steels, editor,
Experiments in Cultural Language Evolution, pages
169?205. John Benjamins.
Paul Vogt. 2007. Group size effects on the emer-
gence of compositional structures in language. In
F. Almeida e Costa, L.M. Rocha, E. Costa, I Har-
vey, and A. Coutinho, editors, Advances in Artifi-
cial Life: Proceedings of the 9th European Confer-
ence (ECAL 2007), pages 405?414, Lisbon, Portu-
gal, Sep. Springer.
Yusuke Watanabe, Reiji Suzuki, and Takaya Arita.
2008. Language evolution and the Baldwin effect.
Artificial Life and Robotics, 12(1-2):65?69.
54
