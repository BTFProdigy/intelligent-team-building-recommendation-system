Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 615?620,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Question Answering with Subgraph Embeddings
Antoine Bordes
Facebook AI Research
112 avenue de Wagram,
Paris, France
abordes@fb.com
Sumit Chopra
Facebook AI Research
770 Broadway,
New York, USA
spchopra@fb.com
Jason Weston
Facebook AI Research
770 Broadway,
New York, USA
jase@fb.com
Abstract
This paper presents a system which learns
to answer questions on a broad range of
topics from a knowledge base using few
hand-crafted features. Our model learns
low-dimensional embeddings of words
and knowledge base constituents; these
representations are used to score natural
language questions against candidate an-
swers. Training our system using pairs of
questions and structured representations of
their answers, and pairs of question para-
phrases, yields competitive results on a re-
cent benchmark of the literature.
1 Introduction
Teaching machines how to automatically answer
questions asked in natural language on any topic
or in any domain has always been a long stand-
ing goal in Artificial Intelligence. With the rise
of large scale structured knowledge bases (KBs),
this problem, known as open-domain question an-
swering (or open QA), boils down to being able
to query efficiently such databases with natural
language. These KBs, such as FREEBASE (Bol-
lacker et al., 2008) encompass huge ever growing
amounts of information and ease open QA by or-
ganizing a great variety of answers in a structured
format. However, the scale and the difficulty for
machines to interpret natural language still makes
this task a challenging problem.
The state-of-the-art techniques in open QA can
be classified into two main classes, namely, infor-
mation retrieval based and semantic parsing based.
Information retrieval systems first retrieve a broad
set of candidate answers by querying the search
API of KBs with a transformation of the ques-
tion into a valid query and then use fine-grained
detection heuristics to identify the exact answer
(Kolomiyets and Moens, 2011; Unger et al., 2012;
Yao and Van Durme, 2014). On the other hand,
semantic parsing methods focus on the correct in-
terpretation of the meaning of a question by a se-
mantic parsing system. A correct interpretation
converts a question into the exact database query
that returns the correct answer. Interestingly, re-
cent works (Berant et al., 2013; Kwiatkowski et
al., 2013; Berant and Liang, 2014; Fader et al.,
2014) have shown that such systems can be ef-
ficiently trained under indirect and imperfect su-
pervision and hence scale to large-scale regimes,
while bypassing most of the annotation costs.
Yet, even if both kinds of system have shown the
ability to handle large-scale KBs, they still require
experts to hand-craft lexicons, grammars, and KB
schema to be effective. This non-negligible hu-
man intervention might not be generic enough to
conveniently scale up to new databases with other
schema, broader vocabularies or languages other
than English. In contrast, (Fader et al., 2013) pro-
posed a framework for open QA requiring almost
no human annotation. Despite being an interesting
approach, this method is outperformed by other
competing methods. (Bordes et al., 2014b) in-
troduced an embedding model, which learns low-
dimensional vector representations of words and
symbols (such as KBs constituents) and can be
trained with even less supervision than the system
of (Fader et al., 2013) while being able to achieve
better prediction performance. However, this ap-
proach is only compared with (Fader et al., 2013)
which operates in a simplified setting and has not
been applied in more realistic conditions nor eval-
uated against the best performing methods.
In this paper, we improve the model of (Bor-
des et al., 2014b) by providing the ability to an-
swer more complicated questions. The main con-
tributions of the paper are: (1) a more sophisti-
cated inference procedure that is both efficient and
can consider longer paths ((Bordes et al., 2014b)
considered only answers directly connected to the
615
question in the graph); and (2) a richer represen-
tation of the answers which encodes the question-
answer path and surrounding subgraph of the KB.
Our approach is competitive with the current state-
of-the-art on the recent benchmark WEBQUES-
TIONS (Berant et al., 2013) without using any lex-
icon, rules or additional system for part-of-speech
tagging, syntactic or dependency parsing during
training as most other systems do.
2 Task Definition
Our main motivation is to provide a system for
open QA able to be trained as long as it has ac-
cess to: (1) a training set of questions paired with
answers and (2) a KB providing a structure among
answers. We suppose that all potential answers are
entities in the KB and that questions are sequences
of words that include one identified KB entity.
When this entity is not given, plain string match-
ing is used to perform entity resolution. Smarter
methods could be used but this is not our focus.
We use WEBQUESTIONS (Berant et al., 2013)
as our evaluation bemchmark. Since it contains
few training samples, it is impossible to learn on
it alone, and this section describes the various data
sources that were used for training. These are sim-
ilar to those used in (Berant and Liang, 2014).
WebQuestions This dataset is built using FREE-
BASE as the KB and contains 5,810 question-
answer pairs. It was created by crawling questions
through the Google Suggest API, and then obtain-
ing answers using Amazon Mechanical Turk. We
used the original split (3,778 examples for train-
ing and 2,032 for testing), and isolated 1k ques-
tions from the training set for validation. WE-
BQUESTIONS is built on FREEBASE since all an-
swers are defined as FREEBASE entities. In each
question, we identified one FREEBASE entity us-
ing string matching between words of the ques-
tion and entity names in FREEBASE. When the
same string matches multiple entities, only the en-
tity appearing in most triples, i.e. the most popular
in FREEBASE, was kept. Example questions (an-
swers) in the dataset include ?Where did Edgar
Allan Poe died?? (baltimore) or ?What degrees
did Barack Obama get?? (bachelor of arts,
juris doctor).
Freebase FREEBASE (Bollacker et al., 2008)
is a huge and freely available database of
general facts; data is organized as triplets
(subject, type1.type2.predicate, object),
where two entities subject and object (identi-
fied by mids) are connected by the relation type
type1.type2.predicate. We used a subset, cre-
ated by only keeping triples where one of the
entities was appearing in either the WEBQUES-
TIONS training/validation set or in CLUEWEB ex-
tractions. We also removed all entities appearing
less than 5 times and finally obtained a FREEBASE
set containing 14M triples made of 2.2M entities
and 7k relation types.
1
Since the format of triples
does not correspond to any structure one could
find in language, we decided to transform them
into automatically generated questions. Hence, all
triples were converted into questions ?What is the
predicate of the type2 subject?? (using the
mid of the subject) with the answer being object.
An example is ?What is the nationality of the
person barack obama?? (united states). More
examples and details are given in a longer version
of this paper (Bordes et al., 2014a).
ClueWeb Extractions FREEBASE data allows
to train our model on 14M questions but these have
a fixed lexicon and vocabulary, which is not real-
istic. Following (Berant et al., 2013), we also cre-
ated questions using CLUEWEB extractions pro-
vided by (Lin et al., 2012). Using string match-
ing, we ended up with 2M extractions structured
as (subject, ?text string?, object) with both
subject and object linked to FREEBASE. We
also converted these triples into questions by using
simple patterns and FREEBASE types. An exam-
ple of generated question is ?Where barack obama
was allegedly bear in?? (hawaii).
Paraphrases The automatically generated ques-
tions that are useful to connect FREEBASE triples
and natural language, do not provide a satisfac-
tory modeling of natural language because of their
semi-automatic wording and rigid syntax. To
overcome this issue, we follow (Fader et al., 2013)
and supplement our training data with an indirect
supervision signal made of pairs of question para-
phrases collected from the WIKIANSWERS web-
site. On WIKIANSWERS, users can tag pairs of
questions as rephrasings of each other: (Fader et
al., 2013) harvested a set of 2M distinct questions
from WIKIANSWERS, which were grouped into
350k paraphrase clusters.
1
WEBQUESTIONS contains ?2k entities, hence restrict-
ing FREEBASE to 2.2M entities does not ease the task for us.
616
3 Embedding Questions and Answers
Inspired by (Bordes et al., 2014b), our model
works by learning low-dimensional vector embed-
dings of words appearing in questions and of enti-
ties and relation types of FREEBASE, so that repre-
sentations of questions and of their corresponding
answers are close to each other in the joint embed-
ding space. Let q denote a question and a a can-
didate answer. Learning embeddings is achieved
by learning a scoring function S(q, a), so that S
generates a high score if a is the correct answer to
the question q, and a low score otherwise. Note
that both q and a are represented as a combina-
tion of the embeddings of their individual words
and/or symbols; hence, learning S essentially in-
volves learning these embeddings. In our model,
the form of the scoring function is:
S(q, a) = f(q)
>
g(a). (1)
Let W be a matrix of R
k?N
, where k is the di-
mension of the embedding space which is fixed a-
priori, andN is the dictionary of embeddings to be
learned. LetN
W
denote the total number of words
and N
S
the total number of entities and relation
types. WithN = N
W
+N
S
, the i-th column ofW
is the embedding of the i-th element (word, entity
or relation type) in the dictionary. The function
f(.), which maps the questions into the embed-
ding spaceR
k
is defined as f(q) =W?(q), where
?(q) ? N
N
, is a sparse vector indicating the num-
ber of times each word appears in the question q
(usually 0 or 1). Likewise the function g(.) which
maps the answer into the same embedding space
R
k
as the questions, is given by g(a) = W?(a).
Here ?(a) ? N
N
is a sparse vector representation
of the answer a, which we now detail.
3.1 Representing Candidate Answers
We now describe possible feature representations
for a single candidate answer. (When there are
multiple correct answers, we average these rep-
resentations, see Section 3.4.) We consider three
different types of representation, corresponding to
different subgraphs of FREEBASE around it.
(i) Single Entity. The answer is represented as
a single entity from FREEBASE: ?(a) is a 1-
of-N
S
coded vector with 1 corresponding to
the entity of the answer, and 0 elsewhere.
(ii) Path Representation. The answer is
represented as a path from the entity
mentioned in the question to the answer
entity. In our experiments, we consid-
ered 1- or 2-hops paths (i.e. with either
1 or 2 edges to traverse): (barack obama,
people.person.place of birth, honolulu)
is a 1-hop path and (barack obama,
people.person.place of birth, location.
location.containedby, hawaii) a 2-hops
path. This results in a ?(a) which is a
3-of-N
S
or 4-of-N
S
coded vector, expressing
the start and end entities of the path and the
relation types (but not entities) in-between.
(iii) Subgraph Representation. We encode both
the path representation from (ii), and the en-
tire subgraph of entities connected to the can-
didate answer entity. That is, for each entity
connected to the answer we include both the
relation type and the entity itself in the repre-
sentation ?(a). In order to represent the an-
swer path differently to the surrounding sub-
graph (so the model can differentiate them),
we double the dictionary size for entities, and
use one embedding representation if they are
in the path and another if they are in the sub-
graph. Thus we now learn a parameter matrix
R
k?N
where N = N
W
+ 2N
S
(N
S
is the to-
tal number of entities and relation types). If
there areC connected entities withD relation
types to the candidate answer, its representa-
tion is a 3+C+D or 4+C+D-of-N
S
coded
vector, depending on the path length.
Our hypothesis is that including more informa-
tion about the answer in its representation will lead
to improved results. While it is possible that all
required information could be encoded in the k di-
mensional embedding of the single entity (i), it is
unclear what dimension k should be to make this
possible. For example the embedding of a country
entity encoding all of its citizens seems unrealis-
tic. Similarly, only having access to the path ig-
nores all the other information we have about the
answer entity, unless it is encoded in the embed-
dings of either the entity of the question, the an-
swer or the relations linking them, which might be
quite complicated as well. We thus adopt the sub-
graph approach. Figure 1 illustrates our model.
3.2 Training and Loss Function
As in (Weston et al., 2010), we train our model
using a margin-based ranking loss function. Let
D = {(q
i
, a
i
) : i = 1, . . . , |D|} be the training set
617
?Who did Clooney marry in 1987?? 
Embedding	 ?matrix	 ?W	 ?
G. Clooney K. Preston 
1987 
J. Travolta 
Model 
Honolulu 
Detec?on	 ?of	 ?Freebase	 ?en?ty	 ?in	 ?the	 ?ques?on	 ?
Embedding model 
Freebase subgraph 
Binary	 ?encoding	 ?of	 ?the	 ?subgraph	 ??(a)	 ?
Embedding	 ?of	 ?the	 ?subgraph	 ?g(a)	 ?
Binary	 ?encoding	 ?of	 ?the	 ?ques?on	 ??(q)	 ?
Embedding	 ?of	 ?the	 ?ques?n	 ? f(q)	 ?
Ques?n 	 ?q	 ?
Subgraph	 ?of	 ?a	 ?candidate	 ?answer	 ?a	 ?(here	 ?K.	 ?Preston)	 ?
Score S(q,a) How	 ?the	 ?candidate	 ?answer	 ?fits	 ?the	 ?ques?on	 ?
Dot	 ?product	 ? Embedding	 ?matrix	 ?W	 ?
Figure 1: Illustration of the subgraph embedding model scoring a candidate answer: (i) locate entity in
the question; (ii) compute path from entity to answer; (iii) represent answer as path plus all connected
entities to the answer (the subgraph); (iv) embed both the question and the answer subgraph separately
using the learnt embedding vectors, and score the match via their dot product.
of questions q
i
paired with their correct answer a
i
.
The loss function we minimize is
|D|
?
i=1
?
a??
?
A(a
i
)
max{0,m?S(q
i
, a
i
)+S(q
i
, a?)}, (2)
where m is the margin (fixed to 0.1). Minimizing
Eq. (2) learns the embedding matrix W so that
the score of a question paired with a correct an-
swer is greater than with any incorrect answer a?
by at least m. a? is sampled from a set of incor-
rect candidates
?
A. This is achieved by sampling
50% of the time from the set of entities connected
to the entity of the question (i.e. other candidate
paths), and by replacing the answer entity by a ran-
dom one otherwise. Optimization is accomplished
using stochastic gradient descent, multi-threaded
with Hogwild! (Recht et al., 2011), with the con-
straint that the columns w
i
of W remain within
the unit-ball, i.e., ?
i
, ||w
i
||
2
? 1.
3.3 Multitask Training of Embeddings
Since a large number of questions in our training
datasets are synthetically generated, they do not
adequately cover the range of syntax used in natu-
ral language. Hence, we also multi-task the train-
ing of our model with the task of paraphrase pre-
diction. We do so by alternating the training of
S with that of a scoring function S
prp
(q
1
, q
2
) =
f(q
1
)
>
f(q
2
), which uses the same embedding ma-
trix W and makes the embeddings of a pair of
questions (q
1
, q
2
) similar to each other if they are
paraphrases (i.e. if they belong to the same para-
phrase cluster), and make them different other-
wise. Training S
prp
is similar to that of S except
that negative samples are obtained by sampling a
question from another paraphrase cluster.
We also multitask the training of the embed-
dings with the mapping of the mids of FREEBASE
entities to the actual words of their names, so that
the model learns that the embedding of the mid of
an entity should be similar to the embedding of the
word(s) that compose its name(s).
3.4 Inference
OnceW is trained, at test time, for a given ques-
tion q the model predicts the answer with:
a? = argmax
a
?
?A(q)
S(q, a
?
) (3)
where A(q) is the candidate answer set. This can-
didate set could be the whole KB but this has both
speed and potentially precision issues. Instead, we
create a candidate set A(q) for each question.
We recall that each question contains one identi-
fied FREEBASE entity. A(q) is first populated with
all triples from FREEBASE involving this entity.
This allows to answer simple factual questions
whose answers are directly connected to them (i.e.
1-hop paths). This strategy is denoted C
1
.
Since a system able to answer only such ques-
tions would be limited, we supplement A(q) with
examples situated in the KB graph at 2-hops from
the entity of the question. We do not add all such
quadruplets since this would lead to very large
candidate sets. Instead, we consider the follow-
ing general approach: given that we are predicting
a path, we can predict its elements in turn using
618
Method P@1 F1 F1
(%) (Berant) (Yao)
Baselines
(Berant et al., 2013) ? 31.4 ?
(Bordes et al., 2014b) 31.3 29.7 31.8
(Yao and Van Durme, 2014) ? 33.0 42.0
(Berant and Liang, 2014) ? 39.9 43.0
Our approach
Subgraph & A(q) = C
2
40.4 39.2 43.2
Ensemble with (Berant & Liang, 14) ? 41.8 45.7
Variants
Without multiple predictions 40.4 31.3 34.2
Subgraph & A(q) = All 2-hops 38.0 37.1 41.4
Subgraph & A(q) = C
1
34.0 32.6 35.1
Path & A(q) = C
2
36.2 35.3 38.5
Single Entity & A(q) = C
1
25.8 16.0 17.8
Table 1: Results on the WEBQUESTIONS test set.
a beam search, and hence avoid scoring all can-
didates. Specifically, our model first ranks rela-
tion types using Eq. (1), i.e. selects which rela-
tion types are the most likely to be expressed in
q. We keep the top 10 types (10 was selected on
the validation set) and only add 2-hops candidates
to A(q) when these relations appear in their path.
Scores of 1-hop triples are weighted by 1.5 since
they have one less element than 2-hops quadru-
plets. This strategy, denotedC
2
, is used by default.
A prediction a
?
can commonly actually be
a set of candidate answers, not just one an-
swer, for example for questions like ?Who are
David Beckham?s children??. This is achieved
by considering a prediction to be all the en-
tities that lie on the same 1-hop or 2-hops
path from the entity found in the question.
Hence, all answers to the above question are
connected to david beckham via the same path
(david beckham, people.person.children,
*
).
The feature representation of the prediction is then
the average over each candidate entity?s features
(see Section 3.1), i.e. ?
all
(a
?
) =
1
|a
?
|
?
a
?
j
:a
?
?(a
?
j
)
where a
?
j
are the individual entities in the over-
all prediction a
?
. In the results, we compare to a
baseline method that can only predict single can-
didates, which understandly performs poorly.
4 Experiments
We compare our system in terms of F1 score as
computed by the official evaluation script
2
(F1
(Berant)) but also with a slightly different F1 def-
inition, termed F1 (Yao) which was used in (Yao
and Van Durme, 2014) (the difference being the
way that questions with no answers are dealt with),
2
Available from www-nlp.stanford.edu/software/sempre/
and precision @ 1 (p@1) of the first candidate en-
tity (even when there are a set of correct answers),
comparing to recently published systems.
3
The
upper part of Table 1 indicates that our approach
outperforms (Yao and Van Durme, 2014), (Berant
et al., 2013) and (Bordes et al., 2014b), and per-
forms similarly as (Berant and Liang, 2014).
The lower part of Table 1 compares various ver-
sions of our model. Our default approach uses
the Subgraph representation for answers and C
2
as the candidate answers set. Replacing C
2
by
C
1
induces a large drop in performance because
many questions do not have answers thatare di-
rectly connected to their inluded entity (not in
C
1
). However, using all 2-hops connections as
a candidate set is also detrimental, because the
larger number of candidates confuses (and slows
a lot) our ranking based inference. Our results
also verify our hypothesis of Section 3.1, that a
richer representation for answers (using the local
subgraph) can store more pertinent information.
Finally, we demonstrate that we greatly improve
upon the model of (Bordes et al., 2014b), which
actually corresponds to a setting with the Path rep-
resentation and C
1
as candidate set.
We also considered an ensemble of our ap-
proach and that of (Berant and Liang, 2014). As
we only had access to their test predictions we
used the following combination method. Our ap-
proach gives a score S(q, a) for the answer it pre-
dicts. We chose a threshold such that our approach
predicts 50% of the time (when S(q, a) is above
its value), and the other 50% of the time we use
the prediction of (Berant and Liang, 2014) instead.
We aimed for a 50/50 ratio because both meth-
ods perform similarly. The ensemble improves the
state-of-the-art, and indicates that our models are
significantly different in their design.
5 Conclusion
This paper presented an embedding model that
learns to perform open QA using training data
made of questions paired with their answers and
of a KB to provide a structure among answers, and
can achieve promising performance on the com-
petitive benchmark WEBQUESTIONS.
3
Results of baselines except (Bordes et al., 2014b) have
been extracted from the original papers. For our experiments,
all hyperparameters have been selected on the WEBQUES-
TIONS validation set: k was chosen among {64, 128, 256},
the learning rate on a log. scale between 10
?4
and 10
?1
and
we used at most 100 paths in the subgraph representation.
619
References
Jonathan Berant and Percy Liang. 2014. Semantic
parsing via paraphrasing. In Proceedings of the
52nd Annual Meeting of the Association for Com-
putational Linguistics (ACL?14), Baltimore, USA.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP?13), Seattle, USA.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, Vancouver, Canada. ACM.
Antoine Bordes, Sumit Chopra, and Jason Weston.
2014a. Question answering with subgraph embed-
dings. CoRR, abs/1406.3676.
Antoine Bordes, Jason Weston, and Nicolas Usunier.
2014b. Open question answering with weakly su-
pervised embedding models. In Proceedings of the
7th European Conference on Machine Learning and
Principles and Practice of Knowledge Discovery
in Databases (ECML-PKDD?14), Nancy, France.
Springer.
Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-driven learning for open question
answering. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(ACL?13), Sofia, Bulgaria.
Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2014. Open question answering over curated and
extracted knowledge bases. In Proceedings of 20th
SIGKDD Conference on Knowledge Discovery and
Data Mining (KDD?14), New York City, USA.
ACM.
Oleksandr Kolomiyets and Marie-Francine Moens.
2011. A survey on question answering technology
from an information retrieval perspective. Informa-
tion Sciences, 181(24):5412?5434.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing (EMNLP?13), Seattle, USA,
October.
Thomas Lin, Mausam, and Oren Etzioni. 2012. En-
tity linking at web scale. In Proceedings of the Joint
Workshop on Automatic Knowledge Base Construc-
tion and Web-scale Knowledge Extraction (AKBC-
WEKEX?12), Montreal, Canada.
Benjamin Recht, Christopher R?e, Stephen J Wright,
and Feng Niu. 2011. Hogwild!: A lock-free ap-
proach to parallelizing stochastic gradient descent.
In Advances in Neural Information Processing Sys-
tems (NIPS 24)., Vancouver, Canada.
Christina Unger, Lorenz B?uhmann, Jens Lehmann,
Axel-Cyrille Ngonga Ngomo, Daniel Gerber, and
Philipp Cimiano. 2012. Template-based question
answering over RDF data. In Proceedings of the
21st international conference on World Wide Web
(WWW?12), Lyon, France. ACM.
Jason Weston, Samy Bengio, and Nicolas Usunier.
2010. Large scale image annotation: learning to
rank with joint word-image embeddings. Machine
learning, 81(1).
Xuchen Yao and Benjamin Van Durme. 2014. Infor-
mation extraction over structured data: Question an-
swering with freebase. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL?14), Baltimore, USA.
620
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1822?1827,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
#TAGSPACE: Semantic Embeddings from Hashtags
Jason Weston
Facebook AI Research
jase@fb.com
Sumit Chopra
Facebook AI Research
spchopra@fb.com
Keith Adams
Facebook AI Research
kma@fb.com
Abstract
We describe a convolutional neural net-
work that learns feature representations for
short textual posts using hashtags as a su-
pervised signal. The proposed approach is
trained on up to 5.5 billion words predict-
ing 100,000 possible hashtags. As well as
strong performance on the hashtag predic-
tion task itself, we show that its learned
representation of text (ignoring the hash-
tag labels) is useful for other tasks as well.
To that end, we present results on a docu-
ment recommendation task, where it also
outperforms a number of baselines.
1 Introduction
Hashtags (single tokens often composed of nat-
ural language n-grams or abbreviations, prefixed
with the character ?#?) are ubiquitous on social
networking services, particularly in short textual
documents (a.k.a. posts). Authors use hashtags to
diverse ends, many of which can be seen as labels
for classical NLP tasks: disambiguation (chips
#futurism vs. chips #junkfood); identi-
fication of named entities (#sf49ers); sentiment
(#dislike); and topic annotation (#yoga).
Hashtag prediction is the task of mapping text to
its accompanying hashtags. In this work we pro-
pose a novel model for hashtag prediction, and
show that this task is also a useful surrogate for
learning good representations of text.
Latent representations, or embeddings, are vec-
torial representations of words or documents, tra-
ditionally learned in an unsupervised manner over
large corpora. For example LSA (Deerwester et
al., 1990) and its variants, and more recent neural-
network inspired methods like those of Bengio et
al. (2006), Collobert et al. (2011) and word2vec
(Mikolov et al., 2013) learn word embeddings. In
the word embedding paradigm, each word is rep-
resented as a vector in R
n
, where n is a hyper-
parameter that controls capacity. The embeddings
of words comprising a text are combined using a
model-dependent, possibly learned function, pro-
ducing a point in the same embedding space. A
similarity measure (for example, inner product)
gauges the pairwise relevance of points in the em-
bedding space.
Unsupervised word embedding methods train
with a reconstruction objective in which the em-
beddings are used to predict the original text. For
example, word2vec tries to predict all the words
in the document, given the embeddings of sur-
rounding words. We argue that hashtag predic-
tion provides a more direct form of supervision:
the tags are a labeling by the author of the salient
aspects of the text. Hence, predicting them may
provide stronger semantic guidance than unsuper-
vised learning alone. The abundance of hashtags
in real posts provides a huge labeled dataset for
learning potentially sophisticated models.
In this work we develop a convolutional net-
work for large scale ranking tasks, and apply it
to hashtag prediction. Our model represents both
words and the entire textual post as embeddings as
intermediate steps. We show that our method out-
performs existing unsupervised (word2vec) and
supervised (WSABIE (Weston et al., 2011)) em-
bedding methods, and other baselines, at the hash-
tag prediction task.
We then probe our model?s generality, by trans-
fering its learned representations to the task of per-
sonalized document recommendation: for each of
M users, given N previous positive interactions
with documents (likes, clicks, etc.), predict the
N + 1?th document the user will positively inter-
act with. To perform well on this task, the rep-
resentation should capture the user?s interest in
textual content. We find representations trained
on hashtag prediction outperform representations
from unsupervised learning, and that our convolu-
1822
w1
w2
N ? d
word
lookup 
table
convolution
layer
tanh
layer
max
pooling
tanh
layer
linear
layer
wl
f(w, t)
hashtag
lookup 
table 
t
(l + K   1)? d
l ?H l ?H
H H
d
d
Figure 1: #TAGSPACE convolutional network f(w, t) for scoring a (document, hashtag) pair.
tional architecture performs better than WSABIE
trained on the same hashtag task.
2 Prior Work
Some previous work (Davidov et al., 2010; Godin
et al., 2013; She and Chen, 2014) has addressed
hashtag prediction. Most such work applies to
much smaller sets of hashtags than the 100,000 we
consider, with the notable exception of Ding et al.
(2012), which uses an unsupervised method.
As mentioned in Section 1, many approaches
learn unsupervised word embeddings. In our
experiments we use word2vec (Mikolov et al.,
2013) as a representative scalable model for un-
supervised embeddings. WSABIE (Weston et al.,
2011) is a supervised embedding approach that has
shown promise in NLP tasks (Weston et al., 2013;
Hermann et al., 2014). WSABIE is shallow, linear,
and ignores word order information, and so may
have less modeling power than our approach.
Convolutional neural networks (CNNs), in
which shared weights are applied across the in-
put, are popular in the vision domain and have re-
cently been applied to semantic role labeling (Col-
lobert et al., 2011) and parsing (Collobert, 2011).
Neural networks in general have also been applied
to part-of-speech tagging, chunking, named en-
tity recognition (Collobert et al., 2011; Turian et
al., 2010), and sentiment detection (Socher et al.,
2013). All these tasks involve predicting a limited
(2-30) number of labels. In this work, we make
use of CNNs, but apply them to the task of rank-
ing a very large set of tags. We thus propose a
model and training scheme that can scale to this
class of problem.
3 Convolutional Embedding Model
Our model #TAGSPACE (see Figure 1), like other
word embedding models, starts by assigning a d-
dimensional vector to each of the l words of an
input document w
1
, . . . , w
l
, resulting in a matrix
of size l ? d. This is achieved using a matrix of
N ? d parameters, termed the lookup-table layer
(Collobert et al., 2011), where N is the vocabulary
size. In this work N is 10
6
, and each row of the
matrix represents one of the million most frequent
words in the training corpus.
A convolution layer is then applied to the l ? d
input matrix, which considers all successive win-
dows of text of size K, sliding over the docu-
ment from position 1 to l. This requires a fur-
ther Kd?H weights and H biases to be learned.
To account for words at the two boundaries of the
document we also apply a special padding vector
at both ends. In our experiments K was set to 5
and H was set to 1000. After the convolutional
step, a tanh nonlinearity followed by a max op-
eration over the l ? H features extracts a fixed-
size (H-dimensional) global feature vector, which
is independent of document size. Finally, another
tanh non-linearity followed by a fully connected
linear layer of size H?d is applied to represent the
entire document in the original embedding space
of d-dimensions.
Hashtags are also represented using d-
dimensional embeddings using a lookup-table.
We represent the top 100,000 most frequent tags.
For a given document w we then rank any given
hashtag t using the scoring function:
f(w, t) = e
conv
(w) ? e
lt
(t)
1823
where e
conv
(w) is the embedding of the document
by the CNN just described and e
lt
(t) is the em-
bedding of a candidate tag t. We can thus rank all
candidate hashtags via their scores f(w, t), largest
first.
To train the above scoring function, and hence
the parameters of the model we minimize a rank-
ing loss similar to the one used in WSABIE as
a training objective: for each training example,
we sample a positive tag, compute f(w, t
+
), then
sample random tags
?
t up to 1000 times until
f(w,
?
t) > m + f(w, t
+
), where m is the mar-
gin. A gradient step is then made to optimize the
pairwise hinge loss:
L = max{0,m? f(w, t
+
) + f(w,
?
t)}.
We use m = 0.1 in our experiments. This loss
function is referred to as the WARP loss in (We-
ston et al., 2011) and is used to approximately
optimizing the top of the ranked list, useful for
metrics like precision and recall@k. In particu-
lar, the search for a negative candidate tag means
that more energy is spent on improving the rank-
ing performance of positive labels already near the
top of the ranked list, compared to only randomly
sampling of negatives, which would optimize the
average rank instead.
Minimizing our loss is achieved with parallel
stochastic gradient descent using the hogwild al-
gorithm (Niu et al., 2011). The lookup-table lay-
ers are initialized with the embeddings learned by
WSABIE to expedite convergence. This kind of
?pre-training? is a standard trick in the neural net-
work literature, see e.g. (Socher et al., 2011).
The ranking loss makes our model scalable to
100,000 (or more) hashtags. At each training ex-
ample only a subset of tags have to be computed,
so it is far more efficient than a standard classifi-
cation loss that considers them all.
4 Experiments
4.1 Data
Our experiments use two large corpora of posts
containing hashtags from a popular social net-
work.
1
The first corpus, which we call people,
consists of 201 million posts from individual user
accounts, comprising 5.5 billion words.
The second corpus, which we call pages, con-
sists of 35.3 million page posts, comprising 1.6
1
Both corpora were de-identified during collection.
Posts Words
Dataset (millions) (billions) Top 4 tags
Pages 35.3 1.6
#fitness,
#beauty,
#luxury, #cars
People 201 5.5
#FacebookIs10,
#love, #tbt,
#happy
Table 1: Datasets used in hashtag prediction.
billion words. These posts? authorial voice is a
public entity, such as a business, celebrity, brand,
or product. The posts in the pages dataset are pre-
sumably intended for a wider, more general audi-
ence than the posts in the people dataset. Both are
summarized in Table 1.
Both corpora comprise posts between February
1st and February 17th, 2014. Since we are not at-
tempting a multi-language model, we use a simple
trigram-based language prediction model to con-
sider only posts whose most likely language is En-
glish.
The two datasets use hashtags very differently.
The pages dataset has a fatter head, with popular
tags covering more examples. The people dataset
uses obscure tags more heavily. For example, the
top 100 tags account for 33.9% of page tags, but
only 13.1% of people tags.
4.2 Hashtag prediction
The hashtag prediction task attempts to rank a
post?s ground-truth hashtags higher than hash-
tags it does not contain. We trained models on
both the people and page datasets, and collected
precision at 1, recall at 10, and mean rank for
50,000 randomly selected posts withheld from
training. A further 50,000 withheld posts are
used for selecting hyperparameters. We compare
#TAGSPACE with the following models:
Frequency This simple baseline ignores input
text, always ranking hashtags by their frequency
in the training data.
#words This baseline assigns each tag a static
score based on its frequency plus a large bonus if
it corresponds to a word in the input text. For ex-
ample, on input ?crazy commute this am?, #words
ranks #crazy, #commute, #this and #am
highest, in frequency order.
Word2vec We trained the unsupervised model
of Mikolov et al. (2013) on both datasets, treat-
ing hashtags the same as all other words. To ap-
1824
Crazy commute this am, #nyc, #snow, #puremichigan, #snowday, #snowstorm,
was lucky to even get in to work. #tubestrike, #blizzard, #commute, #snowpocalypse, #chiberia
This can?t go on anymore, #samelove, #equalrights, #equality, #equalityforall, #loveislove,
we need marriage equality now! #lgbt, #marriageequality, #noh8, #gayrights, #gaymarriage
Kevin spacey what a super hottie :) #houseofcards, #hoc, #houseofcardsseason2, #season2, #kevinspacey,
#frankunderwood, #netflix, #suits, #swoon, #hubbahubba
Went shopping today and found a really #mango, #shopping, #heaven, #100happydays, #yummy,
good place to get fresh mango. #lunch, #retailtherapy, #yum, #cravings, #wholefoods
Went running today -- #running, #ouch, #pain, #nopainnogain, #nike
my feet hurt so much! #marathontraining, #sore, #outofshape, #nikeplus, #runnerproblems
Wow, what a goal that was, #arsenal, #coyg, #ozil, #afc, #arsenalfc
just too fast, Mesut Ozil is the best! #lfc, #ynwa, #mesut, #gunners, #ucl
Working really hard on the paper #thestruggle, #smh, #lol, #collegelife, #homework
all last night. #sad, #wtf, #confused, #stressed, #work
The restaurant was too expensive #ripoff, #firstworldproblems, #smh, #fail, #justsaying
and the service was slow. #restaurant, #badservice, #food, #middleclassproblems, #neveragain
The restaurant had great food #dinner, #restaurant, #yum, #food, #delicious
and was reasonably priced. #stuffed, #goodtimes, #foodporn, #yummy, #winning
He has the longest whiskers, #cat, #kitty, #meow, #cats, #catsofinstagram
omg so sweet! #crazycatlady, #cute, #kitten, #catlady, #adorable
Table 2: #TAGSPACE (256 dim) predictions for some example posts.
ply these word embeddings to ranking, we first
sum the embeddings of each word in the text (as
word2vec does), and then rank hashtags by simi-
larity of their embedding to that of the text.
2
WSABIE (Weston et al., 2011) is a supervised
bilinear embedding model. Each word and tag has
an embedding. The words in a text are averaged
to produce an embedding of the text, and hash-
tags are ranked by similarity to the text embed-
ding. That is, the model is of the form:
f(w, t) = w
>
U
>
V t
where the post w is represented as a bag of words
(a sparse vector in R
N
), the tag is a one-hot-vector
in R
N
, and U and V are k ?N embedding matri-
ces. The WARP loss, as described in section 3, is
used for training.
Performance of all these models at hashtag pre-
diction is summarized in Tables 3 and 4. We find
similar results for both datasets. The frequency
and #words baselines perform poorly across the
2
Note that the unsupervised Word2vec embeddings could
be used as input to a supervised classifier, which we did not
do. For a supervised embedding baseline we instead use WS-
ABIE. WSABIE trains word embeddings U and hashtag em-
beddings V in a supervised fashion, whereas Word2vec trains
them both unsupervised. Adding supervision to Word2vec
would effectively do something in-between: U would still be
unsupervised, but V would then be supervised.
board, establishing the need to learn from text.
Among the learning models, the unsupervised
word2vec performs the worst. We believe this
is due to it being unsupervised ? adding super-
vision better optimizes the metric we evaluate.
#TAGSPACE outperforms WSABIE at all dimen-
sionalities. Due to the relatively large test sets,
the results are statistically significant; for example,
comparing #TAGSPACE (64 dim) beats Wsabie (64
dim) for the page dataset 56% of the time, and
draws 23% of the time in terms of the rank met-
ric, and is statistically significant with a Wilcoxon
signed-rank test.
Some example predictions for #TAGSPACE are
given for some constructed examples in Table 2.
We also show nearest word embeddings to the
posts. Training data was collected at the time of
the pax winter storm, explaining predictions for
the first post, and Kevin Spacey appears in the
show ?House of Cards,?. In all cases the hash-
tags reveal labels that capture the semantics of the
posts, not just syntactic similarity of individual
words.
Comparison to Production System We also
compare to a proprietary system in production in
Facebook for hashtag prediction. It trains a lo-
gistic regression model for every hashtag, using
a bag of unigrams, bigrams, and trigrams as the
1825
Method dim P@1 R@10 Rank
Freq. baseline - 1.06% 2.48% 11277
#words baseline - 0.90% 3.01% 11034
Word2Vec 256 1.21% 2.85% 9973
Word2Vec 512 1.14% 2.93% 8727
WSABIE 64 4.55% 8.80% 6921
WSABIE 128 5.25% 9.33% 6208
WSABIE 256 5.66% 10.34% 5519
WSABIE 512 5.92% 10.74% 5452
#TAGSPACE 64 6.69% 12.42% 3569
#TAGSPACE 128 6.91% 12.57% 3858
#TAGSPACE 256 7.37% 12.58% 3820
Table 3: Hashtag test results for people dataset.
Method dim P@1 R@10 Rank
Freq. baseline - 4.20% 1.59% 11103
#words baseline - 2.63% 5.05% 10581
Word2Vec 256 4.66% 8.15% 10149
Word2Vec 512 5.26% 9.33% 9800
WSABIE 64 24.45% 29.64% 2619
WSABIE 128 27.47% 32.94% 2325
WSABIE 256 29.76% 35.28% 1992
WSABIE 512 30.90% 36.96% 1184
#TAGSPACE 64 34.08% 38.96% 1184
#TAGSPACE 128 36.27% 41.42% 1165
#TAGSPACE 256 37.42% 43.01% 1155
Table 4: Hashtag test results for pages dataset.
input features. Unlike the other models we con-
sider here, this baseline has been trained using a
set of approximately 10 million posts. Engineer-
ing constraints prevent measuring mean rank per-
formance. We present it here as a serious effort
at solving the same problem from outside the em-
bedding paradigm. On the people dataset this sys-
tem achieves 3.47% P@1 and 5.33% R@10. On
the pages dataset it obtains 5.97% P@1 and 6.30%
R@10. It is thus outperformed by our method.
However, we note the differences in experimen-
tal setting mean this comparison is perhaps not
completely fair (different training sets). We expect
performance of linear models such as this to be
similar to WSABIE as that has been in the case in
other datasets (Gupta et al., 2014), but at the cost
of more memory usage. Note that models like lo-
gistic regression and SVM do not scale well if you
have millions of hashtags, which we could handle
in our models.
4.3 Personalized document recommendation
To investigate the generality of these learned rep-
resentations, we apply them to the task of recom-
mending documents to users based on the user?s
interaction history. The data for this task comprise
anonymized day-long interaction histories for a
tiny subset of people on a popular social network-
Method dim P@1 R@10 R@50
Word2Vec 256 0.75% 1.96% 3.82%
BoW - 1.36% 4.29% 8.03%
WSABIE 64 0.98% 3.14% 6.65%
WSABIE 128 1.02% 3.30% 6.71%
WSABIE 256 1.01% 2.98% 5.99%
WSABIE 512 1.01% 2.76% 5.19%
#TAGSPACE 64 1.27% 4.56% 9.64%
#TAGSPACE 128 1.48% 4.74% 9.96%
#TAGSPACE 256 1.66% 5.29% 10.69%
WSABIE+ BoW 64 1.61% 4.83% 9.00%
#TAGSPACE+ BoW 64 1.80% 5.90% 11.22%
#TAGSPACE+ BoW 256 1.92% 6.15% 11.53%
Table 5: Document recommendation task results.
ing service. For each of the 34 thousand people
considered, we collected the text of between 5 and
167 posts that she has expressed previous positive
interactions with (likes, clicks, etc.). Given the
person?s trailing n?1 posts, we use our models to
predict the n?th post by ranking it against 10,000
other unrelated posts, and measuring precison and
recall. The score of the n
th
post is obtained by
taking the max similarity over the n? 1 posts. We
use cosine similarity between post embeddings in-
stead of the inner product that was used for hash-
tag training so that the scores are not unduly influ-
enced by document length. All learned hashtag
models were trained on the people dataset. We
also consider a TF-IDF weighted bag-of-words
baseline (BoW). The results are given in Table 5.
Hashtag-based embeddings outperform BoW
and unsupervised embeddings across the board,
and #TAGSPACE outperforms WSABIE. The best
results come from summing the bag-of-words
scores with those of #TAGSPACE.
5 Conclusion
#TAGSPACE is a convolutional neural network
that learns to rank hashtags with a minimum of
task-specific assumptions and engineering. It per-
forms well, beating several baselines and an in-
dustrial system engineered for hashtag prediction.
The semantics of hashtags cause #TAGSPACE to
learn a representation that captures many salient
aspects of text. This representation is general
enough to port to the task of personalized docu-
ment recommendation, where it outperforms other
well-known representations.
Acknowledgements
We thank Ledell Wu and Jeff Pasternak for their
help with datasets and baselines.
1826
References
Yoshua Bengio, Holger Schwenk, Jean-S?ebastien
Sen?ecal, Fr?ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In
Innovations in Machine Learning, pages 137?186.
Springer.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Ronan Collobert. 2011. Deep learning for efficient dis-
criminative parsing. In International Conference on
Artificial Intelligence and Statistics, number EPFL-
CONF-192374.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics:
Posters, COLING ?10, pages 241?249, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Scott C. Deerwester, Susan T Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. JASIS,
41(6):391?407.
Zhuoye Ding, Qi Zhang, and Xuanjing Huang. 2012.
Automatic hashtag recommendation for microblogs
using topic-specific translation model. In COLING
(Posters)?12, pages 265?274.
Fr?ederic Godin, Viktor Slavkovikj, Wesley De Neve,
Benjamin Schrauwen, and Rik Van de Walle. 2013.
Using topic models for twitter hashtag recommen-
dation. In Proceedings of the 22nd international
conference on World Wide Web companion, pages
593?596. International World Wide Web Confer-
ences Steering Committee.
Maya R Gupta, Samy Bengio, and Jason Weston.
2014. Training highly multiclass classifiers. Jour-
nal of Machine Learning Research, 15:1?48.
Karl Moritz Hermann, Dipanjan Das, Jason Weston,
and Kuzman Ganchev. 2014. Semantic frame iden-
tification with distributed word representations. In
Proceedings of ACL.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Feng Niu, Benjamin Recht, Christopher R?e, and
Stephen J Wright. 2011. Hogwild!: A lock-free ap-
proach to parallelizing stochastic gradient descent.
Advances in Neural Information Processing Sys-
tems, 24:693?701.
Jieying She and Lei Chen. 2014. Tomoha: Topic
model-based hashtag recommendation on twitter. In
Proceedings of the companion publication of the
23rd international conference on World wide web
companion, pages 371?372. International World
Wide Web Conferences Steering Committee.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151?161. Association for
Computational Linguistics.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1631?1642.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ?10, pages 384?394,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Jason Weston, Samy Bengio, and Nicolas Usunier.
2011. Wsabie: Scaling up to large vocabulary
image annotation. In Proceedings of the Twenty-
Second international joint conference on Artificial
Intelligence-Volume Volume Three, pages 2764?
2770. AAAI Press.
Jason Weston, Antoine Bordes, Oksana Yakhnenko,
and Nicolas Usunier. 2013. Connecting language
and knowledge bases with embedding models for
relation extraction. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP).
1827
