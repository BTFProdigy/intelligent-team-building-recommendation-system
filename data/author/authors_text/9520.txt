Coling 2008: Companion volume ? Posters and Demonstrations, pages 11?14
Manchester, August 2008
Towards Incremental End-of-Utterance Detection in Dialogue Systems
Michaela Atterer, Timo Baumann, David Schlangen
Institute of Linguistics
University of Potsdam, Germany
{atterer,timo,das}@ling
?
.uni-potsdam.de
Abstract
We define the task of incremental or 0-
lag utterance segmentation, that is, the task
of segmenting an ongoing speech recog-
nition stream into utterance units, and
present first results. We use a combination
of hidden event language model, features
from an incremental parser, and acous-
tic / prosodic features to train classifiers on
real-world conversational data (from the
Switchboard corpus). The best classifiers
reach an F-score of around 56%, improv-
ing over baseline and related work.
1 Introduction
Unlike written language, speech?and hence, au-
tomatic speech transcription?does not come seg-
mented into units. Current spoken dialogue sys-
tems simply wait for the speaker to turn silent to
segment their input. This necessarily reduces their
responsiveness, as further processing can only
even commence a certain duration after the turn
has ended (Ward et al, 2005). Moreover, given
the typically simple domains, such work mostly
does not deal with the problem of segmenting the
turn into utterances, i.e. does not distinguish be-
tween utterance and turn segmentation. However,
as our corpus shows (see below), multi-utterance
turns are the norm in natural dialogues. The work
that does treat intra-turn utterance segmentation
does so in an offline context, namely the post-
processing of automatic transcripts of recorded
speech such as meeting protocols (Fung et al,
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
2007), and relies heavily on right-context pause in-
formation.
In this paper, we define the task of incremental
or 0-lag utterance segmentation, that is, the task of
segmenting an ongoing speech recognition stream
into utterance units using only left-context infor-
mation.1 This work is done in the context of devel-
oping an incremental dialogue system architecture
(as proposed among others by (Aist et al, 2007)),
where, ideally, a considerable part of the analy-
sis has already been done while the speaker still
speaks. The incremental parser and other compo-
nents of such a system need to be reset at turn-
internal utterance-boundaries with as little delay as
possible. Hence it is of vital importance to predict
the end-of-utterance while the last word of a sen-
tence is processed (or even earlier). We investigate
typical features an incremental system can access,
such as partial parse trees and parser internal in-
formation. These experiments are a first important
step towards online endpointing in an incremental
system.
2 Data
We used section 2 of the Switchboard corpus
(Godfrey et al, 1992) for our experiments. Section
3 was used for training the language models and
the parser that we used. Some of the Switchboard
dialogues are of a very low quality. We excluded
those where transcription notes indicated high rate
of problems due to static noise, echo from the other
speaker or background noise. As our parser be-
came very slow for long sentences, we excluded
sentences that were longer than 25 words from the
10-lag here refers to the time where feature extraction
starts. As the modules on which feature extraction is based
require processing time themselves, a complete absence of
prediction delay is of course not possible.
11
analysis (4% of the sentences). We also excluded
back-channel utterances (typically one-word turns)
from the corpus.
Of the remaining corpus we only used the first
100,000 instances to reduce the computational
load for training the classifiers. 80 % of those were
used as a training corpus, and 20 % as a test corpus.
For follow-up experiments that investigated turn-
initial or turn-internal utterance boundaries only
(see below), we used the relevant subsets of the
first 200,000 instances.
3 Feature Extraction
Our features comprise prosodic features, and syn-
tactic features.
Prosodic features are pitch, logarithmized sig-
nal energy and derived features, extracted from the
audio every 10 ms. In order to track changes over
time, we derive features by windowing over past
values of pitch, energy, and energy in voiced re-
gions only, with window sizes ranging from 50 ms
to 5000 ms. We calculate the arithmetic mean and
the range of the values, the mean difference be-
tween values within the window and the relative
position of the minimum and maximum. We also
perform a linear regression and use its slope, the
MSE of the regression and the error of the regres-
sion for the last value in the window.
As classification was done word-wise (final vs.
non-final word), each word was attributed the
prosodic features of the last corresponding 10-ms-
frame.
For the extraction of syntactic features we used
both n-gram models and a parser. The parser
was a modified version of Amit Dubey?s sleepy
parser,2 which can produce syntactic structure in-
crementally online. The n-gram model was a
hidden event model as typically used in the sen-
tence unit detection literature (see e.g. (Fung et
al., 2007)). For the time being, all features based
on word identities are computed on gold stan-
dard transcriptions. We trained n-gram models
both based on words and on words plus POS-
information that was incrementally obtained from
the parser.3 We calculated the log-probability of
trigrams with the last token in the n-gram being
a place-holder for end-of-utterance (i.e. the prob-
2http://homepages.inf.ed.ac.uk/adubey/
software/
3The models were trained using the SRILM-tools (Stol-
cke, 2002) for n = 3 using Chen and Goodman?s modified
Kneser-Ney discounting (Chen and Goodman, 1998).
ability of (I,would,end-of-utterance) or (Thank,you,end-
of-utterance). We also calculated log probabili-
ties for trigrams such as (I, end-of-utterance-1,end-
of-utterance). Thirdly, the log probability was also
computed for a string consisting of 4 word/POS-
pairs followed by an end marker.
Further syntactic features can be roughly di-
vided into two classes: parser-based features,
which are related to internal states of the parser,
and structure-based features which refer to prop-
erties of the syntactic tree. The former try to cap-
ture the expectation of there being more incom-
plete edges towards the beginning of a sentence
than towards the end. We also might expect a rel-
ative decrease in the overall number of edges to-
wards the end of a sentence. Therefore we track a
selection of numbers referring to the various kinds
of edges stored in the chart. Moreover, we utilize
some of the parser?s information about the best in-
termediate edge, and use the category after the dot
of this edge as an estimate for the most probable
category to come next. Furthermore, we use the
forward probability of the best tree as a rounded
log probability.
The structure-based features are simple features
such as the part-of-speech category of the current
word and the number of the word in the current
utterance, and more complex features that try to
(roughly) approximate semantic notions of com-
pleteness by counting the number of verbs or num-
ber of nominal phrases encountered, as we would
usually expect a sentence to be incomplete if we
haven?t heard a verb or nominal phrase yet. For
example, in sentences of the structure (NP) (VP (V
NP)) or (NP) (VP (V NP (N PP))), humans would
typically be aware that the last phrase has probably
been reached during the last noun phrase or prepo-
sitional phrase (cf. (Grosjean, 1983)). However,
the length and internal structure of these phrases
can vary a great deal. We try to capture some of
this variation by features referring to the last non-
terminal seen, the second-to-last non-terminal seen
and the number of words seen since the last non-
terminal. A number of features (the count fea-
tures) are simple features that record the number
of words since the turn or utterance started and the
time elapsed since the utterance started. They are
also subsumed under syntactic features.
We also used dialogue act features like the pre-
vious dialogue act, and the previous dialogue act
of the last speaker. Those currently come from
12
the gold standard. We assume that in a dialogue
system the system would at least have information
about its own dialogue acts.
4 Experimental Settings
We tested a number of classifiers as implemented
in the Weka toolkit (Witten and Frank, 2005),
and found that the JRip-classifier, an implementa-
tion of the RIPPER-algorithm (Cohen, 1995), per-
formed best. A number of attribute selection algo-
rithms also did not result in a significant change of
performance. Therefore, we only report the plain
results by JRip. We also tested the impact each of
our information sources had on the results. The
aim was to find out how important parser, part-of-
speech information and pitch and energy features
are, respectively.
As turn-internal utterance-ends might be more
difficult to detect than those that coincide with
turn-ends, we repeat the experiment with turn-
internal utterances only. Deleting turn-final utter-
ances from our initial 200,000-instance corpus re-
sulted in 128,686 remaining word instances, 80 %
of which were used for training. For a third ex-
periment, where we look at turn-initial utterances,
we use again a subset of those 200,000 word-
instances.
For clarity, we simply use precision/recall for
evaluation; see (Liu and Shriberg, 2007) for a dis-
cussion of other metrics. As a baseline we assume
non-existent utterance segmentation, which results
in a recall of 0 and a precision of 100 %.
5 Results
Precision Recall F
baseline 100 0 0
all features 73.8 45.0 55.9
all syntactic features 74.8 44.0 55.4
word/POS n-gram features 73.4 45.8 56.4
word n-gram features 66.9 34.7 45.7
only count features 59.3 7.7 13.6
prosodic features only 49.5 8.3 14.2
pitch features 100 0 0
energy features 48.2 7.4 12.8
Table 1: Results for end-of-utterance classification
for all utterances.
Tables 1 and 2 show the results for the experi-
mental settings described above. Dialogue act fea-
tures were included in the syntactic features, but
JRip did not use them in its rules eventually. Ta-
ble 1 shows that the overall F-score is best when
n-grams with POS information are used. Adding
a parser, however, increases precision. Prosody
features in general do not seem to have much of
an influence on end-of-utterance prediction in our
data, with energy features contributing more than
pitch features. Table 2 indicates, as expected, that
Precision Recall F
baseline 100 0 0
all features 71.2 40.3 51.4
all syntactic features 72.7 38.2 50.0
word/POS n-gram features 70.5 41.1 51.9
word n-gram features 70.9 26.4 38.5
only count features 60.4 1.0 2.0
prosodic features only 41.7 1.7 3.3
pitch features 100 0 0
energy features 31.6 1.2 2.3
Table 2: Results for end-of-utterance classification
for utterances which are not turn-final.
the end of an utterance is harder to predict when
it is not turn-final. Performance drops compared
to the results shown in Table 1. Note that some
of the performance drop must be attributed to the
use of a different data set. However, the perfor-
mance drop is much more dramatic for the exper-
iments where only prosody is used than for those
where syntax is used. We speculate that the count
features also loose their impact because one-word
utterances like ?Okay? are usually turn-initial.
The results shown in Tables 1 and 2 can be re-
garded as an upper bound for a dialogue system,
because our experiments so far work with gold
standard sentence boundaries for creating syntac-
tic features (e.g., for resetting our parser). Strictly
speaking, they are only realistic for turn-initial ut-
terances. For the remaining 14,737 of our 26,401
utterances, we therefore report a lower bound,
where we use only features that do not have knowl-
edge about the beginning of the sentence (Table 3).
No count and parser-based features were used for
this experiment, only n-gram (word-based without
POS information) and pitch features. The Table
also shows the results for the 11,664 turn-initial
utterances, where we use all features.4 We then
derive the overall performance from the fractions
of initial and non-initial utterances.
Future work aims at putting together a sys-
tem where the parser is restarted using predictions
based on its own output.
4Note that turn-initial utterances can at the same time be
turn-final.
13
Precision Recall F
non-initial 65.0 28.6 39.7
initial 74.5 55.1 63.3
overall 70.4 40.3 51.3
Table 3: Results for end-of-utterance classification
for utterances which are not turn-initial (reduced
feature set), and utterances that are turn-initial (full
feature set) and the derived overall performance.
6 Related Work
(Fuentes et al, 2007) report F-measures of 84%
using prosodic features only, but they use left?
right-windows for feature calculation, where our
processing is truly incremental and more suitable
for real-time usage in a dialogue system. More-
over, they only seem to use one-utterance turns,
which makes the task easier when prosodic fea-
tures are used. In our dialogue corpus (Switch-
board, section 2), however, each turn contains
on average 2.5 utterances, and turn-internal ut-
terances also need to be recognized. (Fung et
al., 2007) reach an F-score of 75.3% , but report
that the best feature was pause-duration?a fea-
ture we don?t use because we want to find out
how well we can predict the end of a sentence
before a pause makes this clear. Similarly, (Fer-
rer et al, 2002) rely largely on pause features.
(Schlangen, 2006) investigates incremental predic-
tion of end-of-utterance and end-of-turn for var-
ious pause-lengths, and achieves an F-score of
35.5% for pause length 0, on which we can im-
prove here.
7 Discussion and Conclusion
We investigated 0-lag end-of-utterance detection
for incremental dialogue systems. In our setup,
we aim to recognise the end of an utterance as
soon as possible, while the potentially last word is
processed, without the help of information about
subsequent silence. We investigate a number of
features an incremental system would be able to
access, such as information from an incremen-
tal parser. We find that remaining (non-pause)
prosodic information is not as helpful as in non-
incremental studies, especially for non-turn-final
utterances. Syntactic information, on the other
hand, increases performance. Future work aims at
more sophisticated prosodic modelling and at test-
ing the impact of using real or simulated speech
recognition output. We also intend to implement
end-of-utterance prediction in the context of a real
incremental system we are building.
8 Acknowledgement
This work was funded by DFG ENP
SCHL845/3-1.
References
Aist, Gregory, James Allen, Ellen Campana, Carlos Gomez-
Gallo, Scott Stoness, Mary Swift, and Michael Tanenhaus.
2007. Incremental understanding in human-computer dia-
logue and experimental evidence for advantages over non-
incremental methods. In Proc. of the 2007 Workshop on
the Semantics and Pragmatics of Dialogue (DECALOG).
Chen, S.F. and J. Goodman. 1998. An empirical study
of smoothing techniques for language modeling. Techni-
cal report, Center for Research in Computing Technology
(Harvard University).
Cohen, William W. 1995. Fast effective rule induction. In
Machine Learning: Proceedings of the Twelfth Interna-
tional Conference.
Ferrer, L., E. Shriberg, and A. Stolcke. 2002. Is the speaker
done yet? Faster and more accurate end-of-utterance de-
tection using prosody in human-computer dialog. In Proc.
Intl. Conf. on Spoken Language Processing, Denver.
Fuentes, Olac, David Vera, and Thamar Solorio. 2007.
A filter-based approach to detect end-of-utterances from
prosody in dialog systems. In Proc. Human Language
Technologies 2007, Rochester, New York.
Fung, J., D. Hakkani-Tur, M. Magimai-Doss, E. Shriberg,
S. Cuendet, and N. Mirghafori. 2007. Prosodic features
and feature selection for multi-lingual sentence segmenta-
tion. In Proc. Interspeech, pages 2585?2588, Antwerp.
Godfrey, John J., E. C. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for research
and development. In Proc. of ICASSP-1992, pages 517?
520, San Francisco, USA, March.
Grosjean, Franc?ois. 1983. How long is the sentence? Pre-
diction and prosody in the on-line processing of language.
Linguistics, 21:501?529.
Liu, Y. and E. Shriberg. 2007. Comparing evaluation metrics
for sentence boundary detection. In Proc. IEEE ICASSP,
Honolulu,USA.
Schlangen, David. 2006. From reaction to prediction: Exper-
iments with computational models of turn-taking. In Proc.
Interspeech 2006, Panel on Prosody of Dialogue Ac ts and
Turn-Taking, Pittsburgh, USA.
Stolcke, Andreas. 2002. SRILM ? an extensible language
modeling toolkit. In Proc. ICSLP 2002.
Ward, Nigel G., Anais G. Rivera, Karen Ward, and David G.
Novick. 2005. Root causes of lost time and user stress in
a simple dialog system. In Proc. of Interspeech, El Paso,
USA.
Witten, Ian H. and Eibe Frank. 2005. Data Mining: Practical
Machine Learning Tools and Techniques. Morgan Kauf-
mann.
14
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 380?388,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Assessing and Improving the Performance of
Speech Recognition for Incremental Systems
Timo Baumann, Michaela Atterer, David Schlangen
Institut fu?r Linguistik
Universita?t Potsdam
Potsdam, Germany
{timo,atterer,das}@ling.uni-potsdam.de
Abstract
In incremental spoken dialogue systems, par-
tial hypotheses about what was said are re-
quired even while the utterance is still ongo-
ing. We define measures for evaluating the
quality of incremental ASR components with
respect to the relative correctness of the par-
tial hypotheses compared to hypotheses that
can optimize over the complete input, the tim-
ing of hypothesis formation relative to the por-
tion of the input they are about, and hypothesis
stability, defined as the number of times they
are revised. We show that simple incremen-
tal post-processing can improve stability dra-
matically, at the cost of timeliness (from 90%
of edits of hypotheses being spurious down to
10% at a lag of 320ms). The measures are
not independent, and we show how system de-
signers can find a desired operating point for
their ASR. To our knowledge, we are the first
to suggest and examine a variety of measures
for assessing incremental ASR and improve
performance on this basis.
1 Introduction
Incrementality, that is, the property of beginning to
process input before it is complete, is often seen as a
desirable property of dialogue systems (e.g., Allen
et al (2001)), as it allows the system to (a) fold
processing time (of modules such as parsers, or di-
alogue managers) into the time taken by the utter-
ance, and (b) react to partial results, for example by
generating back-channel utterances or speculatively
initiating potentially relevant database queries.
Input to a spoken dialogue system normally
passes an automatic speech recognizer (ASR) as a
first processing module, thus the module?s incre-
mentality determines the level of incrementality that
can be reached by the system as a whole. Using
an ASR system incrementally poses interesting chal-
lenges, however. Typically, ASRs use dynamic pro-
gramming and the maximum likelihood hypothesis
to find the word sequence with the lowest expected
likelihood of the sequence containing errors (sen-
tence error). Due to the dynamic programming ap-
proach, what is considered the best hypothesis about
a given stretch of the input signal can change during
the recognition process, as more right context which
can be used as evidence becomes available.
In this paper, we argue that normally used met-
rics for ASR evaluation such as word error rate must
be complemented with metrics specifically designed
for measuring incremental performance, and offer
some such metrics. We show that there are various
subproperties that are not independent of each other,
and that trade-offs are involved if either of those is
to be optimized. Finally, we propose ways to im-
prove incremental performance (as measured by our
metrics) through the use of smoothing techniques.
To our knowledge, incremental evaluation met-
rics of ASR for incremental systems have not yet
been covered in the literature. Most closely related,
Wachsmuth et al (1998) show results for an ASR
which fixes its results after a given time ? and re-
port the corresponding word error rate (WER). This
unfortunately confounds the incremental and non-
incremental properties of their ASR?s performance.
The remainder of this paper is structured as fol-
lows: In section 2, we give an overview of increme-
nality with respect to ASR, and develop our evalua-
380
tion metrics. Section 3 describes the setup and data
that we used in our experiments, and reports and dis-
cusses some basic measures for different variants of
the setup. In section 4 we propose and discuss two
orthogonal methods that improve incremental per-
formance: using right context and using message
smoothing, which show different properties with re-
gard to our measures. Finally, in section 5 we sum
up and point to future directions.
2 Incrementality and Evaluation Measures
for Incremental ASR
In a modular system, an incremental module is one
that generates (partial) responses while input is still
ongoing and makes these available to other mod-
ules (Kilger and Finkler, 1995). ASR modules that
use token passing (Young et al, 1989) can easily
be adapted to output a new, live hypothesis after
processing of every input frame (often that is ev-
ery 10ms). In an incremental system we are able
to get partial results from these hypotheses as soon
as they become available ? or rather as soon as they
can be trusted. As mentioned above, hypotheses are
only tentative, and may be revised when more right
context becomes available. Modules consuming the
output of an incremental ASR hence must be able
to deal with such revisions. There is a first trade-off
here: Depending on how costly revision is for later
modules (which after all may need to revise any hy-
potheses which they themselves based on the now-
revised input), it may be better to reduce the incre-
mentality a bit ? in the sense that partial informa-
tion is produced less often, and hence new words for
example are recognised later ? if that buys stability
(fewer revisions). Also, ignoring some incremen-
tal results that are likely to be wrong may increase
system performance. Defining these notions more
precisely is the aim of this section.
2.1 Relative Correctness
We define a hypothesis at time t (hypt) as consist-
ing of a sequence whypt of words predicted by the
ASR at time t.1 As an example figure 1 shows
1In this paper, we only deal with one-best ASR. We believe
that there are no principled differences when generalising to n-
best hypotheses, but will explore this in detail in future work.
We also abstract away from changes in the hypothesised start
and end times of the words in the sequence. It often happens that
Figure 1: Live ASR hypotheses during incremental
recognition. Edit messages (see section 2.2) are shown
on the right when words are added (?) or revoked (?).
For the word ?zwei? WFC and WFF (see section 2.3) are
shown at the bottom.
a sequence of incrementally produced hypotheses.
(Note that this is an artificial example, showing only
a few illustratory and interesting hypotheses. In a
real recognition system, the hypothesis frequency is
of course much higher, with much repetition of sim-
ilar hypotheses at consecutive frames.)
The question now is how we can evaluate the
quality of a hypothesis at the time t it is produced.
It is reasonable to only expect this hypothesis to say
something (correct or not) about the input up to time
t ? unless we want the ASR to predict, in which case
we want it to make assumptions about times beyond
t (see section 4.1). There are two candidates for the
yardstick against which the partial hypotheses could
be compared: First, one could take the actually spo-
ken words, computing measures such as word error
rate. The other option, which is the one taken here,
is to take as the gold standard the final hypothesis
produced by the ASR when it has all evidence avail-
the ASR?s assumptions about the position of the word bound-
aries change, even if the word sequence stays constant. If, as we
assume here, later modules do not use this timing information,
we can consider two hypotheses that only differ in boundary
placement as identical.
381
able (i.e., when the utterance is complete). This is
more meaningful for our purpose, as it relates the in-
formativity of the partial hypothesis to what can be
expected if the ASR can do all its internal optimisa-
tions, and not to the factually correct sequence that
the ASR might not be able to recognise even with
all information present. This latter problem is al-
ready captured in the conventional non-incremental
performance measures.
In our metrics in this paper, we hence take as gold
standard (wgold) the final, non-incremental hypothe-
sis of the ASR (which, to reiterate this point, might
be factually incorrect, that is, might contain word
errors). We define a module?s incremental response
at time t (whypt) as relatively correct (r-correct), iff
it is equal to the non-incremental hypothesis up to
time t: whypt t = wgoldt. Hence, in figure 1 above,
hypotheses 1, 2, 6, 7, 9 and 12 are r-correct.2 We
call the normalised rate of r-correct responses of a
module its (average) r-correctness.
As defined above, the criterion for r-correctness
is still pretty strict, as it demands of the ASR that
words on the right edge are recognised even from
the first frame on. For example, whyp10 in figure 1
is not r-correct, because wgold10 (that part of wgold
that ends where whyp10 ends) already spans parts of
the word ?drei? which has not yet been picked up
by the incremental recognition. A relaxed notion
of correctness hence is prefix-correctness, which re-
quires only that whypt be a prefix of wgoldt. (Hy-
potheses 3 and 10 in figure 1 are p-correct, as are all
r-correct hypotheses.) It should be noted though that
p-correctness is too forgiving to be used directly as
an optimization target: in the example in figure 1,
a module that only ever produces empty hypotheses
would trivally achieve perfect p-correctness (as this
is always a prefix of wgold).
2.2 Edit Overhead
The measures defined so far capture only static as-
pects of the incremental performance of a module
and do not say anything about the dynamics of the
recognition process. To capture this, we look at
the changes between subsequent partial hypotheses.
There are three ways in which an hypothesis hypt+1
2The timing in hypothesis 7 is not correct ? but this does not
matter to our notion of correctness (see footnote 1).
can be different from hypt: there can be an extension
of the word sequence, a revokation, or a revision of
the last words in the sequence.3 These differences
can be expressed as edit messages, where extending
a sequence by one word would require an add mes-
sage (?), deleting the last word in the sequence a
revoke message (?), and exchange of the last word
would require two messages, one to revoke the old
and one to add the new word.4
Now, an incrementally perfect ASR would only
generate extensions, adding new words at the right
edge; thus, there would be exactly as many edit mes-
sages as there are words in wgold. In reality, there
are typically many more changes, and hence many
spurious edits (see below for characteristic rates in
our data). We call the rate of spurious edits the edit
overhead (EO). For figure 1 above, this is 811 : There
are 11 edits (as shown in the figure), while we?d ex-
pect only 3 (one ? for each word in the final result).
Hence, 8 edits are spurious.
This measure corresponds directly to the amount
of unnecessary activity a consumer of the ASR?s
output performs when it reacts swiftly to words that
may be revoked later on. If the consumer is able to
robustly cope with parallel hypotheses (for example
by building a lattice-like structure), a high EO may
not be problematic, but if revisions are costly for
later modules (or even impossible because action has
already been taken), we would like EO to be as low
as possible. This can be achieved by not sending edit
messages unconditionally as soon as words change
in the ASR?s current hypothesis, using strategies as
outlined in section 4. Obviously, deferring or sup-
pressing messages results in delays, a topic to which
we turn in the following section, where we define
measures for the response time of ASR.
2.3 Timing Measures
So far, our measures capture characteristics about
the complete recognition process. We now turn to
the timing of the recognition of individual words.
For this, we again take the output of the ASR when
all signal is present (i.e., wgold) as the basis. There
3As fourth and most frequent alternative, consecutive hy-
potheses do not change at all.
4Revision could also be seen as a third atomic operation,
as in standard ASR evaluation (then called ?substitution?). To
keep things simple, we only regard two atomic operations.
382
are two things we may be interested in. First, we
may want to know when is the first time that a certain
word appears in the correct position in the sequence
(or equivalently, when its first correct add edit mes-
sage is sent), expressed in relation to its boundaries
in wgold. We measure this event, the first time that
the ASR was right about a word, relative to its gold
beginning. We call the measure word first correct
response (WFC). As a concrete example take hyp7
in figure 1. At this point, the word ?zwei? is first hy-
pothesised. Compared to the beginning of the word
in wgold, this point (t7) has a delay of 1 frame (the
frames are illustrated by the dashed lines).
As explained above, it may very well be the
case that for a brief while another hypothesis, not
r-correct w.r.t. wgold, may be favoured (cf. the word
?zwar? in the example in the figure). Another mea-
sure we hence might also be interested in is when our
word hypothesis starts remaining stable or, in other
words, becomes final. We measure this event rela-
tive to the end of the word in the gold standard. We
call it word first final response (WFF). In our exam-
ple, again for ?zwei?, this is t9, which has a distance
of 0 to the right boundary of the word in wgold.
In principle, we could use both anchor points (the
left vs. the right edge of a word) for either measure
or use a word-relative scale, but for simplicity?s sake
we restrict ourselves to one anchor point each.
Under normal conditions, we expect WFC to be
positive. The better the incremental ASR, the closer
to 0 it will be. WFC is not a measure we can eas-
ily optimize. We would either have to enumerate
the whole language model or use external non-ASR
knowledge to predict continuations of the word se-
quence before the word in question has started. This
would increase EO. In principle, we are rather in-
terested in accepting an increase in WFC, when we
delay messages in order to decrease EO.
WFF however, can reach values below 0. It
converges towards the negative average of word
length as an incremental ASR improves. For non-
incremental ASR it would be positive: the average
distance beween the sentence end and word end.
WFF is a measure we can strive to reduce by sending
fewer (especially fewer wrong) messages.
Another property we might be interested in opti-
mizing is the time it takes from the first correct hy-
pothesis to stabilize to a final hypothesis. We com-
pute this correction time as the difference in time
between WFF and WFC.5 A correction time of 0 in-
dicates that there was no correction, i.e. the ASRwas
immediately correct about a word, something which
we would like to happen as often as possible.
Note that these are measures for each word in
each processed utterance, and we will use distribu-
tional parameters of these timing measures (means
and standard deviations) as metrics for the perfor-
mance of the incremental setups described later.
2.4 Summary of Measures
In this section, we first described measures that eval-
uate the overall correctness of incrementally pro-
duced ASR hypotheses, not taking into account their
sequential nature. We then turned to the dynamics of
how the current hypothesis evolves in a way which
we consider important for a consumer of incremen-
tal ASR, namely the overhead that results from edits
to the hypothesis. Finally, we looked at the timing
of individual messages with regard to first correct
(potentially unstable) occurrence (WFC) and stabil-
ity (WFF). In the next section, we use the measures
defined here to characterize the incremental perfor-
mance of our ASR, before we discuss ways to im-
prove incremental performance in section 4.
3 Setup, Corpora and Base Measurements
We use the large-vocabulary continuous-speech
recognition framework Sphinx-4 (Walker et al,
2004) for our experiments, using the built-in Lex-
Tree decoder, extended by us to provide incremen-
tal results. We built acoustic models for German,
based on a small corpus of spontaneous instructions
in a puzzle building domain,6 and the Kiel corpus
of read speech (IPDS, 1994). We use a trigram lan-
guage model that is based on the puzzle domain tran-
scriptions. As test data we use 85 recordings of two
speakers (unknown to the acoustic model) that speak
sentences similar to those in the puzzle domain.
We do not yet use recognition rescoring to opti-
mize for word error rate, but just the ASR?s best
hypotheses which optimize for low sentence error.
Incremental rescoring mechanisms such as that of
5In figure 1, the correction time for ?zwei? is 9? 7 = 2.
6Available from http://www.voxforge.org/
home/downloads/speech/
383
SER (non-incremental) 68.2%
WER (non-incremental) 18.8%
r-correct (cropped) 30.9%
p-correct (cropped) 53.1%
edit overhead 90.5%
mean word duration 0.378 s
WFC: mean, stddev, median 0.276 s, 0.186 s, 0.230 s
WFF: mean, stddev, median 0.004 s, 0.268 s, ?0.06 s
immediately correct 58.6%
Table 1: Base measurements on our data
Razik et al (2008) to optimize ASR performance are
orthogonal to the approaches presented in section 4
and could well be incorporated to further improve
incremental performance.
The individual recordings in our corpus are fairly
short (5.5 seconds on average) and include a bit of si-
lence at the beginning and end. Obviously, recogniz-
ing silence is much easier than recognizing words.
To make our results more meaningful for continuous
speech, we crop away all ASR hypotheses from be-
fore and after the active recognition process.7 While
this reduces our performance in terms of correctness
(we crop away areas with nearly 100% correctness),
it has no impact on the edit overhead, as the number
of changes in wcurr remains unchanged, and also no
impact on the timing measures as all word bound-
aries remain the same.
3.1 Base Measurements
Table 1 characterises our ASR module (on our data)
in terms of the metrics defined in section 2. Addi-
tionally we state sentence error rate, as the rate of
sentences that contain at least one error, and word
error rate computed in the usual way, as well as
the mean duration of words in our corpus (as non-
incrementally measured for our ASR).
We see that correctness is quite low. This is
mostly due to the jitter that the evolving current hy-
pothesis shows in its last few frames, jumping back
and forth between highly-ranked alternatives. Also,
our ASR only predicts words once there is acoustic
evidence for several phonemes and every phoneme
(being modelled by 3 HMM states) must have a du-
ration of at least 3 frames. Thus, some errors rela-
tive to the final hypothesis occur because the ASR
7In figure 1, hypotheses 1, 2 and 3 would be cropped away.
 40
 50
 60
 70
 80
 90
 100
 0  0.2  0.4  0.6  0.8  1  1.2
pe
rc
en
ta
ge
 o
f w
or
ds
 th
at
 a
re
 fi
na
l
correction time in s
Figure 2: Distribution of correction times (WFF?WFC).
only hypothesizes about words once they already
have a certain duration (and hence preceding hy-
potheses are not r-correct). The difference between
r-correctness and p-correctness (20% in our case)
may be largely attributed to this fact.
The edit overhead of 90.5% means that for ev-
ery neccessary add message, there are nine superflu-
ous (add or revoke) messages. Thus, a consumer of
the ASR output would have to recompute its results
ten times on average. In an incremental system, this
consumer might itself output messages and further
revise decisions as information from other modules
becomes available, leading to a tremendous amount
of changes in the system state. As ASR is the first
module in an incremental spoken dialogue system,
reducing the edit overhead is essential for overall
system performance.
On average, the correct hypothesis about a word
becomes available 276ms after the word has started
(WFC). With a mean word duration of 378ms
this means that information becomes available af-
ter roughly 34 of the word have been spoken. No-
tice though that the median is somewhat lower than
the mean, implying that this time is lower for most
words and much higher for some words. In fact, the
maximum for WFC in our data is 1.38 s.
On average, a word becomes final (i.e. is
not changed anymore) when it has ended
(mean(WFF) = 0.004). Again, the median is
lower, indicating the unnormal distribution of WFF
(more often lower, sometimes much higher).
Of all words, 58.6% were immediately correctly
384
 0
 20
 40
 60
 80
 100
2 5 8 11
LM weight
R-Correctness
P-Correctness
Edit Overhead
WER
Figure 3: Correctness, Edit Overhead and Word Error
Rate (WER) with varied language model weight and un-
altered audio.
hypothesized by the ASR. Figure 2 plots the per-
centage of words with correction times equal to or
lower than the time on the x-axis. While this starts
at the initial 58.6% of words that were immediately
correct, it rises above 90% for a correction time of
320ms and above 95% for 550ms. Inversely this
means that we can be certain to 90% (or 95%) that
a current correct hypothesis about a word will not
change anymore once it has not been revoked for
320ms (or 550ms respectively).
Knowing (or assuming with some certainty) that
a hypothesis is final allows us, to commit ourselves
to this hypothesis. This allows for reduced compu-
tational overhead (as alternative hypotheses can be
abandoned) and is crucial if action is to be taken that
cannot be revoked later on (as for example, initiat-
ing a response from the dialogue system). Figure 2
allows us to choose an operating point for commit-
ment with respect to hypothesis age and certainty.
3.2 Variations of the Setup
In setting up our system we did not yet strive for best
(non-incremental) performance; this would have re-
quired much more training material and parameter
tweaking. We were more interested here in explor-
ing general questions related to incremental ASR,
and in developing approaches to improve incremen-
tal performance (see section 4), which we see as a
problem that is independent from that of improving
performance measures like (overall) accuracy.
To test how independent our measures are on de-
 0
 20
 40
 60
 80
 100
orig -20 -15 -10 -5 0
signal to noise ratio in dB
R-Correctness
P-Correctness
Edit Overhead
WER
Figure 4: Correctness, Edit Overhead and Word Error
Rate (WER) with additive noise (LM weight set to 8).
tails of the specific setting, such as quality of the
audio material and of the language model, we var-
ied these factors systematically, by adding white
noise to the audio and changing the language model
weight relative to the acoustic model. We varied the
noise to produce signal to noise ratios ranging from
hardly audible (?20 dB), through annoying noise
(?10 dB) to barely understandable audio (0 dB).
Figure 3 gives an overview of the ASR-
performance with different LM weights and figure 4
with degraded audio signals. Overall, we see that
r-correctness and EO change little with different
LM and AM performance and correspondigly de-
graded WER. A tendency can be seen that larger LM
weights result in higher correctness and lower EO. A
larger LM weight leads to less influence of acoustic
events which dynamically change hypotheses, while
the static knowledge from the LM becomes more
important. Surprisingly, WER improved with the
addition of slight noise, which we assume is due to
differences in recording conditions between our test
data and the training data of the acoustic model.
In the following experiments as well as in the data
in table 1 above, we use a language model weight of
8 and unaltered audio.
4 Improving Incremental Performance
In the previous section we have shown how a stan-
dard ASR that incrementally outputs partial hy-
potheses after each frame processed performs with
regard to our measures and showed that they remain
385
stable in different acoustic conditions and with dif-
fering LM weights. We now discuss ways of incre-
mentally post-processing ASR hypotheses in order
to improve selected measures.
We particularly look for ways to improve EO;
that is, we want to reduce the amount of wrong hy-
potheses and resulting spurious edits that deterio-
rate later modules? performance, while still being as
quick as possible with passing on relevant hypothe-
ses. We are less concerned with correctness mea-
sures, as they do not capture well the dynamic evo-
lution, which is important for further processing of
the incremental hypothesis. We also discuss trade-
offs that are involved in the optimization decisions.
4.1 Right Context
Allowing the use of some right context is a com-
mon strategy to cope with incremental data. For
example, our ASR already uses this strategy (with
very short right contexts) internally at word bound-
aries to restrict the language model hypotheses to
an acoustically plausible subset (Ortmanns and Ney,
2000). In the experiment described here, we allow
the ASR a larger right context of size ? by taking
into account at time t the output of the ASR up to
time t ? ? only. That is, what the ASR hypothe-
sizes about the interval ]t ? ?, t] is considered to
be too immature and is discarded, and the hypothe-
ses about the input up to t?? have the benefit of a
lookahead up to t. This reduces jitter, which is found
mostly to the very right of the incremental hypothe-
ses. Thus, we expect to reduce the edit overhead in
proportion with ?. On the other hand, allowing the
use of a right context leads to the current hypothe-
sis lagging behind the gold standard. Correspond-
ingly, WFC increases by ?. Obviously, using only
information up to t ? ? has averse effects on cor-
rectness as well, as this measure evaluates the word
sequences up to wgoldt which may already contain
more words (those recognised in ]t ? ?, t]). Thus,
to be more fair and to account for the lag when mea-
suring the module?s correctness, we additionally de-
fine fair r-correctness which restricts the evaluation
up to time t??: whyptt?? = wgoldt??.
Figure 5 details the results for our data with right
context between 1.5 s and ?0.2 s. (The x-axis plots
? as negative values, with 0 being ?now?. Results
for a right context (?) of 1.2 can thus be found 1.2 to
 0
 20
 40
 60
 80
 100
-1.6 -1.4 -1.2 -1 -0.8 -0.6 -0.4 -0.2  0  0.2
right context in s (scale shows larger right contexts towards the left)
(strict) R-Correctness
fair R-Correctness
P-Correctness
Edit Overhead
WER
Figure 5: Correctness (see text), Edit Overhead and
fixed-WER for varying right contexts ?.
the left of 0, at ?1.2.) We see that at least in the fair
measure, fixed lag performs quite well at improving
both the module?s correctness and EO. This is due
to the fact that ASR hypotheses become more and
more stable when given more right context. Still,
even for fairly long lags, many late edits still occur.
To illustrate the effects of a system that does not
support edits of hypotheses, but instead commits
right away, we plot WER that would be reached by a
system that always commits after a right context of
?. As can be seen in the figure, the WER remains
higher than the non-incremental WER (18.8%) even
for fairly large right contexts. Also, the WER plot by
Wachsmuth et al (1998) looks very similar to ours
and likewise shows a sweet spot suitable as an oper-
ating point with a right context of about 800ms.
As expected, the analysis of timing measures
shows an increase with larger right contexts with
their mean values quickly approaching ? (or
??meanword duration for WFF), which are the
lower bounds when using right context. Correspond-
ingly, the percentage of immediately correct hy-
potheses increases with right context reaching 90%
for ? = 580ms and 98% for ? = 1060ms.
Finally, we can extend the concept of right con-
text into negative values, predicting the future, as it
were. By choosing a negative right context, in which
we extrapolate the last hypothesis state by ? into the
future, we can measure the correctness of our hy-
potheses correctly predicting the close future, which
is always the case when the current word is still be-
386
ing spoken. The graph shows that 15% of our hy-
potheses will still be correct 100ms in the future and
10% will still be correct for 170ms. Unfortunately,
there is little way to tell apart hypotheses that will
survive and those which will soon be revised.
4.2 Message Smoothing
In the previous section we reduced wrong edit mes-
sages by avoiding most of the recognition jitter by
allowing the ASR a right context of size ?, which
directly hurt timing measures by roughly the same
amount. In this section, we look at the sequence of
partial hypotheses from the incremental ASR, using
the dynamic properties as cues. We accomplish this
by looking at the edit messages relative to the cur-
rently output word sequence. But instead of sending
them to a consumer directly (updating the external
word sequence), we require that an edit message be
the result of N consecutive hypotheses. To illustrate
the process with N = 2 we return to figure 1. None
of the words ?an?, ?ein? or ?zwar? would ever be
output, because they are only present for one time-
interval each. Edit messages would be sent at the
following times: ?(eins) at t7, ?(zwei) at t10 (only
then is ?zwei? the result of two consecutive hypothe-
ses) and ?(drei) at t13. While no words are revoked
in the example, this still occurs when a revocation is
consecutively hypothesized for N frames.
We get controversial results for this strategy, as
can be seen in figure 6: The edit overhead falls
rapidly, reaching 50% (for each message necessary,
there is one superfluous message) with only 110ms
(and correspondingly increasing WFC by the same
time) and 10% with 320ms. The same thresh-
olds are reached through the use of right context at
530ms and 1150ms respectively as shown in fig-
ure 5. Likewise, the prefix correctness improve-
ments are better than with using right context, but
the r-correctness is poor, even under the ?fair? mea-
sure. We believe this is due to correct hypotheses
being held back too long due to the hypothesis se-
quence being interspersed with wrong hypotheses
(which only last for few consecutive hypotheses)
which reset the counter until the add message (for
the prevalent and potentially correct word) is sent.8
8This could be resolved by using some kind of majority
smoothing instead of requiring a message to be the result of all
consecutive hypotheses. We will investigate this in future work.
 0
 20
 40
 60
 80
 100
-1 -0.8 -0.6 -0.4 -0.2  0
smoothing in s (scale shows larger smoothings towards the left)
(strict) R-Correctness
fair R-Correctness
P-Correctness
Edit Overhead
Figure 6: Correctness and Edit Overhead for varying
smoothing lengths.
5 Conclusions and Further Directions
We have presented the problem of speech recogni-
tion for incremental systems, outlined requirements
for incremental speech recognition and showed mea-
sures that capture how well an incremental ASR per-
forms with regard to these measures. We discussed
the measures and their implications in detail with
our baseline system and showed that the incremen-
tal measures remain stable regardless of the specific
ASR setting used.
Finally, we presented ways for the online post-
processing of incremental results, looking for ways
to improve some of the measures defined, while
hurting the other measures as little as possible.
Specifically, we were interested in generating less
wrong hypotheses at the cost of possible short de-
lays. While using right context shows improvements
with larger delays, using message smoothing seems
especially useful for fast processing. We think these
two approaches could be combined to good effect.
Together with more elaborate confidence handling a
system could quickly generate hypotheses and then
refine the associated confidences over time. We will
explore this in future work.
Acknowledgments
This work was funded by a DFG grant in the Emmy
Noether programme. We wish to thank the anony-
mous reviewers for helpful comments.
387
References
James Allen, George Ferguson, and Amanda Stent. 2001.
An architecture for more realistic conversational sys-
tems. In Proceedings of the Conference on Intelligent
User Interfaces, Santa Fe, USA.
IPDS. 1994. The Kiel Corpus of Read Speech. CD-
ROM.
Anne Kilger and Wolfgang Finkler. 1995. Incremental
generation for real-time applications. Technical Re-
port RR-95-11, DFKI, Saarbru?cken, Germany.
Stefan Ortmanns and Hermann Ney. 2000. Look-ahead
techniques for fast beam search. Computer Speech &
Language, 14:15?32.
Joseph Razik, Odile Mella, Dominique Fohr, and Jean-
Paul Haton. 2008. Frame-Synchronous and Local
ConfidenceMeasures for on-the-fly Automatic Speech
Recognition. In Proceedings of Interspeech 2008.
Sven Wachsmuth, Gernot A. Fink, and Gerhard Sagerer.
1998. Integration of parsing and incremental speech
recognition. In Proceedings of the European Sig-
nal Processing Conference, volume 1, pages 371?375,
Rhodes, Greece.
Willi Walker, Paul Lamere, Philip Kwok, Bhiksha Raj,
Rita Singh, Evandro Gouvea, Peter Wolf, and Joe
Woelfel. 2004. Sphinx-4: A flexible open source
framework for speech recognition. Technical Report
SMLI TR2004-0811, Sun Microsystems Inc.
Steve Young, NH Russell, and JHS Thornton. 1989. To-
ken passing: a simple conceptual model for connected
speech recognition systems. Cambridge University
Engineering Department Technical Report CUED/F-
INFENG/TR, 38.
388
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 30?37,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Incremental Reference Resolution: The Task, Metrics for Evaluation, and
a Bayesian Filtering Model that is Sensitive to Disfluencies
David Schlangen, Timo Baumann, Michaela Atterer
Department of Linguistics
University of Potsdam, Germany
{das|timo|atterer}@ling.uni-potsdam.de
Abstract
In this paper we do two things: a) we dis-
cuss in general terms the task of incre-
mental reference resolution (IRR), in par-
ticular resolution of exophoric reference,
and specify metrics for measuring the per-
formance of dialogue system components
tackling this task, and b) we present a sim-
ple Bayesian filtering model of IRR that
performs reasonably well just using words
directly (no structure information and no
hand-coded semantics): it picks the right
referent out of 12 for around 50% of real-
world dialogue utterances in our test cor-
pus. It is also able to learn to interpret not
only words but also hesitations, just as hu-
mans have shown to do in similar situa-
tions, namely as markers of references to
hard-to-describe entities.
1 Introduction
Like other tasks involved in language comprehen-
sion, reference resolution?that is, the linking of
natural language expressions to contextually given
entities?is performed incrementally by human
listeners. This was shown for example by Tanen-
haus et al (1995) in a famous experiment where
addressees of utterances containing referring ex-
pressions made eye movements towards target ob-
jects very shortly after the end of the first word
that unambiguously specified the referent, even if
that wasn?t the final word of the phrase. In fact, as
has been shown in later experiments (Brennan and
Schober, 2001; Bailey and Ferreira, 2007; Arnold
et al, 2007), such disambiguating material doesn?t
even have to be lexical: under certain circum-
stances, a speaker?s hesitating already seems to be
understood as increasing the likelihood of subse-
quent reference to hard-to-describe entities.
Recently, efforts have begun to build dialogue
systems that make use of incremental processing
as well (Aist et al, 2006; Skantze and Schlangen,
2009). These efforts have so far focused on as-
pects other than resolution of references ((Stoness
et al, 2004) deals with the interaction of reference
and parsing). In this paper, we discuss in gen-
eral terms the task of incremental reference res-
olution (IRR) and specify metrics for evaluating
incremental components for this task. To make
the discussion more concrete, we also describe a
simple Bayesian filtering model of IRR in a do-
main with a small number of possible referents,
and show that it performs better wrt. our metrics
if given information about hesitations?thus pro-
viding computational support for the rationality of
including observables other than words into mod-
els of dialogue meaning.
The remainder of the paper is structured as fol-
lows: We discuss the IRR task in Section 2, and
suitable evaluation metrics in Section 3. In Sec-
tion 4 we describe and analyse the data for which
we present results with our Bayesian model for
IRR in Section 5.
2 Incremental Reference Resolution
To a first approximation, IRR can be modeled as
the ?inverse? as it were of the task of generating re-
ferring expressions (GRE; which is well-studied in
computational linguistics, see e. g. (Dale and Re-
iter, 1995)). Where in GRE words are added that
express features which reduce the size of the set
of possible distractors (with which the object that
the expression is intended to pick out can be con-
fused), in IRR words are encountered that express
features that reduce the size of the set of possible
30
referents. To give a concrete example, for the ex-
pression in (1-a), we could imagine that the logical
representation in (1-b) is built on a word-by-word
basis, and at each step the expression is checked
against the world model to see whether the refer-
ence has become unique.
(1) a. the red cross
b. ?x(red(x) ? cross(x))
To give an example, in a situation where there
are available for reference only one red cross, one
green circle, and two blue squares, we can say
that after ?the red? the referent should have been
found; in a world with two red crosses, we would
need to wait for further restricting information
(e. g. ?. . . on the left?).
This is one way to describe the task, then: a
component for incremental reference resolution
takes expressions as input in a word-by-word fash-
ion and delivers for each new input a set (possibly
a singleton set) as output which collects those dis-
course entities that are compatible with the expres-
sion up to that point. (This description is meant
to be neutral as to whether reference is exophoric,
i. e. directly to entities in the world, or anaphoric,
via previous mentions; we will mainly discuss the
former case, though.)
As we will see below, this does however
not translate directly into a usable metric for
evaluation. While it is easy to identify the
contributions of individual words in simple,
constructed expressions like (1-a), reference in
real conversations is often much more complex,
and is a collaborative process that isn?t confined
to single expressions (Clark and Schaefer, 1987):
referring is a pragmatic action that is not reducible
to denotation. In our corpus (see below), we often
find descriptions as in (2), where the speaker
continuously adds (rather vague) material, typi-
cally until the addressee signals that she identified
the item, or proposes a different way to describe it.
(2) Also das S Teil sieht so aus dass es ein
einzelnes . Teilchen hat . dann . vier am Stu?ck
im rechten Winkel .. dazu nee . nee warte ..
dann noch ein einzelnes das guckt auf der an-
deren Seite raus.
well, the S piece looks so that it has a single . piece .
and then . four together in a 90 degree angle .. and also
. no .. wait .. and then a single piece that sticks out on
the other side.
While it?s difficult to say in the individual case
what the appropriate moment is to settle on a hy-
pothesis about the intended referent, and what the
?correct? time-course of the development of hy-
potheses is, it?s easy to say what we want to be true
in general: we want a referent to be found as early
as possible, with as little change of opinion as pos-
sible during the utterance.1 Hence a model that
finds the correct referent earlier and makes fewer
wrong decisions than a competing one will be con-
sidered better. The metrics we develop in the next
section spell out this idea.
3 Evaluation Metrics for IRR
In previous work, we have discussed metrics for
evaluating the performance of incremental speech
recognition (Baumann et al, 2009). There, our
metrics could rely on time-aligned gold-standard
information against which the incremental results
could be measured. For the reasons discussed
in the previous section, we do not assume that
we have such temporally-aligned information for
evaluating IRR. Our measures described here sim-
ply assume that there is one intention behind the
referring utterances (namely to identify a certain
entity), and that this intention is there from the be-
ginning of the utterance and stays constant.2 This
is not to be understood as the claim that it is rea-
sonable to expect an IRR component to pick out a
referent even if the only part of the utterance that
has already been processed for example is ?now
take the??it just facilitates the ?earlier is better?
ranking discussed above.
We use two kinds of metrics for IRR: posi-
tional metrics, which measure when (which per-
centage into the utterance) a certain event happens,
and edit metrics which capture the ?jumpiness?
of the decision process (how often the component
changes its mind during an utterance).
Figure 1 shows a constructed example that il-
1We leave open here what ?as early as possible? means?
a well-trained model might be able to resolve a reference
before the speaker even deems that possible, and hence ap-
pear to do unnatural (or supernatural?) ?mind reading?. Con-
versely, frequent changes of opinion might be something that
human listeners would exhibit as well (e. g. in their gaze di-
rection). We abstract away from these finer details in our
heuristic.
2Note that our metrics would also work for corpora where
the correct point-of-identification is annotated; this would
simply move the reference point from the beginning of the
utterance to that point. Gallo et al (2007) describe an anno-
tation effort in a simpler domain where entities can easily be
described which would make such information available.
31
X F W F Fsil?model
no?sil?model X ? ?X F
first final
first correct
edit phase
take the
time
gold reference F
words (sil)
F F F
(sil)
F
f
Figure 1: Simple constructed example that illus-
trates the evaluation measures
lustrates these ideas. We assume that reference is
to an object that is internally represented by the
letter F. The example shows two models, no-sil
and sil (what exactly they are doesn?t matter for
now). The former model guesses that reference is
to object X already after the first word, and stays
with this opinion until it encounters the final word,
when it chooses F as most likely referent. (Why
the decision for the items sil is ?-? will be ex-
plained below; here this can be read as ?repetition
of previous decision?.) The other model changes
its mind more often, but also is correct for the first
time earlier and stays correct earlier. Our metrics
make this observation more precise:
? average fc (first correct): how deep into the ut-
terance do we make the first correct guess? (If the
decision component delivers n-best lists instead of
single guesses, ?correct? means here and below ?is
member of n-best list?.)
E. g., if the referent is recognised only after the
final word of the expression, the score for this met-
ric would be 1. In our example it is 2/5 for the
sil-model and 1 for the non-sil model.
? fc applicable: since the previous measure can
only be specified for cases where the correct refer-
ent has been found, we also specify for how many
utterances this is the case.
? average ff (first final): how deep into the utter-
ance do we make the correct guess and don?t sub-
sequently change our mind? This would be 4/5 for
the sil-model in our example and 1 for the no-sil-
model.
? ff applicable: again, the previous measure can
only be given where the final guess of the compo-
nent is correct, so we also need to specify how of-
ten this is the case. Note that whenever ff is appli-
cable, fc is applicable as well, so ff applicable?fc
applicable.
? ed-utt (mean edits per utterance): an IRR mod-
ule may still change its mind even after it has al-
ready made a correct guess. This metric measures
how often the module changes its mind before it
comes back to the right guess (if at all). Since such
decision-revisions (edits) may be costly for later
modules, which possibly need to retract their own
hypotheses that they?ve built based on the output
of this module, ideally this number should be low.
In our example the number of edits between fc
and ff is 2 for the sil-model and 0 for the non-sil
model (because here fc and ff are at the same po-
sition).
? eo (edit overhead): ratio unnecessary edits / nec-
essary edits. (In the ideal case, there is exactly one
edit, from ?no decision? to the correct guess.)
? correctness: how often the model guesses cor-
rectly. This is 3/5 for the sil-model in the example
and 1/5 for the non-sil-model.
? sil-correctness: how often the model guesses
correctly during hesitations. The correctness mea-
sure applied only to certain data-points; we use
this to investigate whether informing the model
about hesitations is helpful.
? adjusted error: some of our IRR models can re-
turn ?undecided? as reply. The correctness mea-
sures defined above would punish this in the same
way as a wrong guess. The adjusted error measure
implements the idea that undecidedness is better
than a wrong guess, at least early in the utterance.
More precisely, it?s defined to be 0 if the guess is
correct, pos / posmax if the reply is ?undecided?
(with pos denoting the position in the utterance),
and 1 if the guess is incorrect. That way uncer-
tainty is not punished in the beginning of the utter-
ance and counted like an error towards its end.
Note that these metrics characterise different as-
pects of the performance of a model. In practi-
cal cases, they may not be independent from each
other, and a system designer will have to decide
which one to optimize. If it is helpful to be in-
formed about a likely referent early, for example
to prepare a reaction, and is not terribly costly to
later have to revise hypotheses, then a low first cor-
rect may be the target. If hypothesis revisions are
costly, then a low edit overhead may be preferred
over a low first correct. (first final and ff applicable,
however, are parameters that are useful for global
optimisation.)
32
Figure 2: The Twelve Pentomino Pieces with their
canonical names (which were not known to the di-
alogue participants). The pieces used in the dia-
logues all had the same colour.
In the remaining sections, we describe a prob-
abilistic model of IRR that we have implemented,
and evaluate it in terms of these metrics. We begin
with describing the data from which we learnt our
model.
4 Data
4.1 Our Corpora
As the basis for training and testing of our model
we used data from three corpora of task-oriented
dialogue that differ in some details of the set-up,
but use the same task: an Instruction Giver (IG) in-
structs an Instruction Follower (IF) on which puz-
zle pieces (from the ?Pentomino? game, see Fig-
ure 2) to pick up. In detail, the corpora were:
? The Pento Naming corpus described in (Siebert
and Schlangen, 2008). In this variant of the task,
IG records instructions for an absent IF; so these
aren?t fully interactive dialogues. The corpus con-
tained 270 utterances out of which we selected
those 143 that contained descriptions of puzzle
pieces (and not of their position on the game-
board).
? Selections from the FTT/PTT corpus described
in (Ferna?ndez et al, 2007), where IF and IG are
connected through an audio-only connection, and
in some dialogues a simplex / push-to-talk one.
We selected all utterances from IG that contained
references to puzzle pieces (286 altogether).
? The third part of our corpus was constructed
specifically for the experiments described here.
We set-up a Wizard of Oz experiment where users
were given the task to describe puzzle pieces for
the ?dialogue system? to pick up. The system
(i. e. the wizard) had available a limited number
of utterances and hence could conduct only a lim-
ited form of dialogue. We collected 255 utter-
ances containing descriptions of puzzle pieces in
this way.
0.
0
0.
1
0.
2
0.
3
0.
4
tile
sl
rt
F I L N P T U V W X Y Z
Figure 3: Silence rate per referent and corpus
(WOz:black, PentoNaming:red, FTT:green)
All utterances were hand-transcribed and the
transcriptions were automatically aligned with the
speech data using the MAUS system (Schiel,
2004); this way, we could automatically identify
pauses during utterances and measure their length.
For some experiments (see below), pauses were
?re-ified? through the addition of silence pseudo-
words (one for each 333ms of silence).
The resulting corpus is not fully balanced in
terms of available material for the various pieces
or contributions by sub-corpora.
4.2 Descriptive Statistics
We were interested to see whether intra-utterance
silences (hesitations) could potentially be used as
an information source in our (more or less) real-
world data in the same way as was shown in
the much more controlled situations described in
the psycholinguistics literature mentioned above
in the introduction (Arnold et al, 2007). Fig-
ure 3 shows the mean ratio of within-utterance si-
lences per word for the different corpora and dif-
ferent referents. We can see that there are clear
differences between the pieces. For example, ref-
erences to the piece whose canonical name is X
contain very few or short hesitations, whereas ref-
erences to Y tend to contain many. We can also
see that the tendencies seem to be remarkably sim-
ilar between corpora, but with relatively stable off-
sets between them, PentoDescr having the longest,
PTT/FTT the shortest silences. We speculate that
this is the result of the differing degrees of inter-
activity (none in PentoDescr, restricted in WOz,
less restricted in PTT, free in FTT) which puts dif-
ferent pressures on speakers to avoid silences. To
balance our data with respect to this difference, we
performed some experiments with adjusted data
33
where silence lengths in PentoDescr were adjusted
by 0.7 and in PTT/FTT by 1.3. This brings the si-
lence rates in the corpora, if plotted in the style of
Figure 3, almost in congruence.
To test whether the differences in silence rate
between utterances referring to different pieces
are significant, we performed an ANOVA and
found a main effect of silence rate, F (11, 672) =
6.2102, p < 8.714?10. A post-hoc t-test reveals
that there are roughly two groups whose members
are not significantly different within-group, but are
across groups: I, L, U, W and X form one group
with relatively low silence rate, F, N, P, T, V, Y, and
Z another with relatively high silence rate. We will
see in the next section whether our model picked
up on these differences.
5 A Bayesian Filtering Model of IRR
To explore incremental reference resolution, and
as part of a larger incremental dialogue system we
are building, we implemented a probabilistic refer-
ence resolver that works in the pentomino domain.
At its base, the resolver has a Bayesian Filtering
model (see e. g. (Thrun et al, 2005)) that with each
new observation (word) computes a belief distri-
bution over the available objects (the twelve puz-
zle pieces); in a second step, a decision for a piece
(or a collection of pieces in the n-best case) is de-
rived from this distribution. This model is incre-
mental in a very natural and direct way: new input
increments are simply treated as new observations
that update the current belief state. Note that this
model does not start with any assumptions about
semantic word classes: whether an observed word
carries information about what is being referred to
will be learnt from data.
5.1 The Belief-Update Model
We use a Bayesian model which treats the in-
tended referent as a latent variable generating a
sequence of observations (w1:n is the sequence of
words w1, w2, . . . , wn):
P (r|w1:n) = ? ? P (wn|r, w1:n?1) ? P (r|w1:n?1)
where
? P (wn|r, w1:n?1) is the likelihood of the new
observation (see below for how we approximate
that); and
? the prior P (r|w1:n?1) at step n is the posterior
of the previous step. Before the first observation is
made (i. e., the first word is seen), the prior is sim-
ply a distribution over the possible referents, P (r).
F I L N P T U V W X Y Z
intended referent:  N
 nimm <sil?0> <sil?1> <sil?2> das teil <sil?0> <sil?1> <sil?2> <sil?3> das aus einer
0.
0
0.
1
0.
2
0.
3
0.
4
Figure 4: Example of Belief Distribution after Ob-
servation
In our experiment, we set this to a uniform distri-
bution, but if there is prior information from other
sources (e. g., because the dialogue state makes
certain pieces more salient), this can be reflected.
? ? is a normalising constant, ensuring that the re-
sult is indeed a probability distribution.
The output of the model is a distribution of be-
lief over the 12 available entities, as shown in Fig-
ure 4. Figure 5 shows in a 3D plot the devel-
opment of the belief state (pieces from front to
back, strength of belief as height of the peaks) over
the course of a whole utterance (with observations
from left to right).
5.2 The Decision Step
We implemented several ways to derive a decision
for a referent from such a distribution:
i) In the arg max approach, at each state the ref-
erent with the highest posterior probability is cho-
sen. For Figure 4, that would be F (and hence,
a wrong decision). As Figure 5 shows (and the
example is quite representative for the model be-
haviour), there often are various local maxima
over the course of an utterance, and hence a model
that takes as its decision always the maximum can
be expected to perform many edits.
ii) In the adaptive threshold approach, we start
with a default decision for a special 13th class,
?undecided?, and a new decision is only made if
the maximal value at the current step is above a
certain threshold, where this threshold is reset ev-
ery time this condition is met. In other words, this
draws a plane into the belief space and only makes
a new decision when a peak rises above this plane
and hence above the previous peak. In effect, this
approach favours strong convictions and reduces
34
utterance #: 230 intended referent:  N
 hast eine lange ule mit drei teilen <sil?0> <sil?1> und eine kurze mit zwei
Z, Y, X, W, V, U, T, P, N, L, I, F
as.matrix(norm.vect[, 1:12])
0.0
0.2
0.4
0.6
0.8
1.0
Figure 5: Belief Update over Course of Utterance
the ?jitter? in the decision making.
In our example from Figure 4, this would mean
that the maximum, F, would only be the decision
if its value was higher than the threshold and there
was no previous guess that was even higher.
iii) The final model implements a threshold n-
best approach, where not just a single piece is se-
lected but all pieces that are above a certain thresh-
old. Assuming that the threshold is 0.1 for exam-
ple this would select F, I, N, Y, and Z?and hence
would include the correct reference in Figure 4.
5.3 Implementation
To learn and query the observation likelihoods
P (wn|r, w1:n?1), we used referent-specific lan-
guage models. More precisely, we computed the
likelihood as P (r, w1:n)/P (r, w1:n?1) (definition
conditional probability), and approximated the
joint probabilities of referent and word sequence
via n-grams with specialised words. E. g., an ut-
terance like ?take the long, narrow piece? refer-
ring to piece I (or tested for reference to this piece)
would be rewritten as ?take I the I long I narrow I
piece I? and presented to the n-gram learner / in-
ference component. (Both taken from the SRI LM
package, (Stolcke, 2002).)
During evaluation of the models, the test utter-
ances are fed word-by-word to the model and the
decision is evaluated against the known intended
referent. Since we were interested in testing
whether disfluencies contained information that
would be learned, for one variant of the system
we also fed pseudo-words for silences and hesi-
tation markers like uhm, numbered by their posi-
tion (i. e., ?take the ..? becomes ?take the sil-1 sil-
2?), to both learning and inference for the silence-
sensitive variant; the silence-ignorant variant sim-
ply repeats the previous decision at such points
and does not update its belief state; this way, it
is guaranteed that both variants generate the same
number of decisions and can be compared directly.
(Cf. the dashes in the ?no-sil-model? in Figure 1
above: those are points where no real computation
is made in the no-sil case.)
5.4 Experiments
All experiments were performed with 10-fold
cross-validation. We always ran both versions, the
one that showed silences to the model and the one
that didn?t. We tested various combinations of lan-
guage model parameters and deciders, of which
the best-performing ones are discussed in the next
section.
5.5 Results
Table 1 shows the results for the different deci-
sion methods and for models where silences are
included as observations and where they aren?t,
and, as a baseline, the result for a resolver that
makes a random decision after each observation.
As we can see, the different decision methods
have different characteristics wrt. individual mea-
sures. The threshold n-best approach performs
best across the board?but of course has a slightly
easier job since it does not need to make unam-
biguous decisions. We will look into the develop-
ment of the n-best lists in a second, but for now
we note that this model is for almost all utterances
correct at least once (97% fc applicable) and if
so, typically very early (after 30% of the utter-
ance). In over half of the cases (54.68%), the fi-
nal decision is correct (i. e. is an n-best list that
contains the correct referent), and similarly for a
good third of all silence observations. Interest-
ingly, silence-correctness is decidedly higher for
the silence model (which does actually make new
decisions during silences and hence based on the
information that the speaker is hesitating) than for
the non-sil model (which at these places only re-
peats the previously made decision). The model
performs significantly bettern than a baseline that
randomly selects n-best lists of the same size (see
rnd-nb in Table 1).
As can be expected, the adaptive threshold ap-
proach is more stable with its decisions, as wit-
nessed by the low edit overhead. The fact that it
changes its decision not as often has an impact on
the other measures, though: in more cases, the
model is correct not even once (fc applicable is
35
n-best rnd-nb adapt max random
Measure / Model w/ h w/o h w/ h w/ h w/o h w/ h w/o h w/ h
fc applicable 97.22% 95.03% 85.38% 63.15% 66.67% 86.55% 82.89% 59.94%
average fc 30.43% 33.73% 29.61% 53.87% 55.25% 46.55% 49.31% 42.60%
ff applicable 54.68% 54.24% 17.54% 48.68% 53.07% 39.77% 40.64% 9.65%
average ff 87.74% 85.01% 97.08% 71.24% 70.89% 96.08% 94.28% 98.44%
edit overhead 93.49% 90.65% 96.65% 69.61% 67.66% 92.57% 89.44% 93.16%
correctness 37.81% 36.81% 23.37% 23.01% 26.61% 17.83% 20.23% 7.83%
sil-correctness 36.60% 31.09% 26.39% 18.71% 22.58% 13.67% 19.34% 8.63%
adjusted error 60.07% 56.96% 76.63% 76.29% 70.90% 82.17% 79.42% 92.16%
Table 1: Results for different decision methods (n-best, adaptive, max arg and random) and for models
with and without silence-observations (w/ h and w/o h, respectively)
lower than for the other two models). But it is
still correct with almost half of its final decisions,
and these come even earlier than for the n-best
model. Silence information does not seem to help
this model; this suggests that the information pro-
vided by knowledge about the fact that the speaker
hesitates is too subtle to push through the thresh-
old in order to change decisions.
The arg max approach fares worst. Since nei-
ther the relative strength of the strongest belief (as
compared to that in the competing pieces) nor the
global strength (have I been more convinced be-
fore?) is taken into account, the model changes
its mind too often, as evidenced by the edit over-
head, and does not settle on the correct referent of-
ten (and if, then late). Again, silence information
does not seem to be helpful for this model.
As a more detailed look at what happens dur-
ing silence sequences, Figure 6 plots the average
change in probability from onset of silence to a
point at 1333ms of silence. (Recall that the un-
derlying Bayesian model is the same for all mod-
els evaluated above, they differ only in how they
derive a decision.) We can see that the gains and
losses are roughly as expected from the analysis of
the corpora: pieces like L and P become more ex-
pected after a silence of that length, pieces like X
less. So the model does indeed seem to learn that
hesitations systematically occur together with cer-
tain pieces. (The reader can convince herself with
the help of Figure 2 that these shapes are indeed
comparatively hard-to-describe; but the interesting
point here is that this categorisation does not have
to be brought to the model but rather is discovered
by it.)
Finally, a look at the distribution and the sizes of
the n-best groupings: the most frequent decision is
F I L N P T U V W X Y Z
?
0.
05
0.
00
0.
05
Figure 6: Average change in probability from on-
set of silence to 1333ms into silence
?undecided? (474 times), followed by the group-
ings F N, N Y, and N Y P (343, 342 and 196, re-
spectively). Here again we find groupings that re-
flect the differences w.r.t. hesitation rate. The av-
erage size of the n-best lists is 2.58 (sd = 1.4).
6 Conclusions and Further Work
We discussed the task of incremental reference
resolution (IRR), in particular with respect to ex-
ophoric reference. From a theoretical perspective,
it might seem easy to specify what the ideal be-
haviour of an IRR component should be, namely
to always produce the set of entities (the exten-
sion) that is compatible with the part of the ex-
pression seen so far. In practice, however, this is
difficult to annotate, for both practical reasons as
well as theoretical (referring is a pragmatic activ-
ity that is not reducible to denotation). The met-
rics we defined for evaluation of IRR components
account for this in that they do not require a gold
36
standard annotation that fixes the dynamics of the
resolution process; they simply make it possible
to quantify the assumption that ?early and with
strong convictions? is best.
We then presented our probabilistic model of
IRR that works directly on word observations
without any further processing (POS tagging,
parsing). It achieves a reasonable success (as mea-
sured with our metrics); for example, in over half
of the cases, the final guess of the model is correct,
and comes before the utterance is over. As an ad-
ditional interesting feature, the model is able to in-
terpret hesitations (silences lifted to pseudo-word
status) in a way shown before only in controlled
psycholinguistic experiments, namely as making
reference to hard-to-describe pieces more likely.3
In future work, we want to explore the model?s
performance on ASR output. It is not clear a
priori that this would degrade performance much,
as it can be expected that the learning components
are quite robust against noise. Connected to
this, we want to explore more complex statis-
tical models, e. g. a hierarchical model where
one level generates parts of the utterance (e. g.
non-referential parts and referential parts) and the
second the actual words. We also want to test how
this approach scales up to worlds with a larger
number of possible referents, where consequently
approximation methods like particle filtering have
to be used. Finally, we will test how the module
contributes to a working dialogue system, where
further decisions (e. g. for clarification requests)
can be built on its output.
Acknowledgments This work was funded by
a grant from DFG in the Emmy Noether Pro-
gramme. We would like to thank the anonymous
reviewers for their detailed comments.
References
G.S. Aist, J. Allen, E. Campana, L. Galescu, C.A.
Gomez Gallo, S. Stoness, M. Swift, and M Tanenhaus.
2006. Software architectures for incremental understand-
ing of human speech. In Proceedings of the Interna-
tional Conference on Spoken Language Processing (IC-
SLP), Pittsburgh, PA, USA, September.
Jennifer E. Arnold, Carla L. Hudson Kam, and Michael K.
Tanenhaus. 2007. If you say thee uh you are describ-
ing something hard: The on-line attribution of disfluency
3It is interesting to speculate whether this could have im-
plications for generation of referring expressions as well. It
might be a good strategy to make your planning problems
observable or even to fake planning problems that are under-
standable to humans.
during reference comprehension. Journal of Experimental
Psychology.
Karl Bailey and F. Ferreira. 2007. The processing of filled
pause disfluencies in the visual world. In R. P. G. von
Gompel, M H. Fischer, W. S. Murray, and R. L. Hill,
editors, Eye Movements: A Window on Mind and Brain,
chapter 22. Elsevier.
Timo Baumann, Michaela Atterer, and David Schlangen.
2009. Assessing and Improving the Performance of
Speech Recognition for Incremental Systems. In Proceed-
ings of NAACL-HLT 2009, Boulder, USA.
Susan E. Brennan and Michael F. Schober. 2001. How lis-
teners compensate for disfluencies in spontaneous speech.
Journal of Memory and Language, 44:274?296.
Herbert H. Clark and Edward F. Schaefer. 1987. Collabo-
rating on contributions to conversations. Language and
Cognitive Processes, 2(1):19?41.
Robert Dale and Ehud Reiter. 1995. Computational interpre-
tations of the gricean maxims in the generation of referring
expressions. Cognitive Science, 19:233?263.
Raquel Ferna?ndez, David Schlangen, and Tatjana Lucht.
2007. Push-to-talk ain?t always bad! comparing differ-
ent interactivity settings in task-oriented dialogue. In Pro-
ceeding of DECALOG (SemDial?07), Trento, Italy, June.
Carlos Go?mez Gallo, Gregory Aist, James Allen, William
de Beaumont, Sergio Coria, Whitney Gegg-Harrison,
Joana Paulo Pardal, and Mary Swift. 2007. Annotating
continuous understanding in a multimodal dialogue cor-
pus. In Proceeding of DECALOG (SemDial07), Trento,
Italy, June.
Florian Schiel. 2004. Maus goes iterative. In Proc. of the
IV. International Conference on Language Resources and
Evaluation, Lisbon, Portugal.
Alexander Siebert and David Schlangen. 2008. A simple
method for resolution of definite reference in a shared vi-
sual context. In Procs of SIGdial, Columbus, Ohio.
Gabriel Skantze and David Schlangen. 2009. Incremental
dialogue processing in a micro-domain. In Proceedings of
EACL 2009, Athens, Greece, April.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings Intl. Conf. Spoken Lan-
guage Processing (ICSLP?02), Denver, Colorado, USA,
September.
Scott C. Stoness, Joel Tetreault, and James Allen. 2004. In-
cremental parsing with reference interaction. In Proceed-
ings of the Workshop on Incremental Parsing at the ACL
2004, pages 18?25, Barcelona, Spain, July.
Michael K. Tanenhaus, Michael J. Spivey-Knowlton, Kath-
llen M. Eberhard, and Julie C. Sedivy. 1995. Intergration
of visual and linguistic information in spoken language
comprehension. Science, 268.
Sebastian Thrun, Wolfram Burgard, and Dieter Fox. 2005.
Probabilistic Robotics. MIT Press, Cambridge, Mas-
sachusetts, USA.
37
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 302?305,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
TELIDA: A Package for Manipulation and Visualization of
Timed Linguistic Data
Titus von der Malsburg, Timo Baumann, David Schlangen
Department of Linguistics
University of Potsdam, Germany
{malsburg|timo|das}@ling.uni-potsdam.de
Abstract
We present a toolkit for manipulating and
visualising time-aligned linguistic data
such as dialogue transcripts or language
processing data. The package comple-
ments existing editing tools by allowing
for conversion between their formats, in-
formation extraction from the raw files,
and by adding sophisticated, and easily ex-
tended methods for visualising the dynam-
ics of dialogue processing. To illustrate
the versatility of the package, we describe
its use in three different projects at our site.
1 Introduction
Manual inspection and visualization of raw data is
often an important first step in the analysis of lin-
guistic data, be that transcripts of conversations or
records of the performance of processing modules.
Dialogue data or speech processing data in gen-
eral are typically temporally aligned, which poses
additional challenges for handling and visualiza-
tion. A number of tools are available for work-
ing with timed data, each with different focus:
as a small selection, Praat (Boersma, 2001) and
Wavesurfer (Sjo?lander and Beskow, 2000) excel at
acoustic analysis and are helpful for transcription
work, Anvil (Kipp, 2001) helps with the analysis
of video material, Exmaralda (Schmidt, 2004) of-
fers a suite of specialized tools for discourse anal-
ysis.
We developed TELIDA (TimEd LInguistic
DAta) to complement the strengths of these tools.
TELIDA comprises (a) a suite of Perl mod-
ules that offer flexible data structures for stor-
ing timed data; tools for converting data in other
formats to and from this format; a command-
line based interface for querying such data, en-
abling for example statistical analysis outside of
the original creators of transcriptions or annota-
tions; and (b) a lightweight but powerful visual-
ization tool, TEDview, that has certain unique fea-
tures, as will be described in Section 2.3. TEL-
IDA is available for download from http://www.
ling.uni-potsdam.de/~timo/code/telida/.
2 Overview of TELIDA
2.1 Data Structures
Like the tools mentioned above, we handle timed
data as discrete labels which span a certain time
and contain some data. To give an example, in a
word-aligned transcription of a recording, a single
word would correspond to one label. Sequences
of (non-overlapping) labels are collected into what
we call alignments. In our example of the word-
aligned transcription, all words from one speaker
might be collected in one alignment.
This so far is a conceptualization that is com-
mon to many tools. In Praat for example, our
alignments would be called a tier. TELIDA adds a
further, novel, abstraction, by treating alignments
as belief states that can have a time (namely that
of their formation) as well. Concretely, an incre-
mental ASR may hypothesize a certain way of an-
alyzing a stretch of sound at one point, but at a
later point might slighlty adapt this analysis; in our
conceptualization, this would be two alignments
that model the same original data, each with a time
stamp. For other applications, timed belief states
may contain other information, e.g. new states of
parse constructions or dialogue manager informa-
tion states. We also allow to store several of such
alignment sequences (= successive belief states) in
parallel, to represent n-best lists.
302
Figure 1: TEDview Showing Annotated Dialogue Data
A document finally can consist of collections
of such alignments that reference the same time-
line, but model different aspects of the base-data.
For example, we may want to store information
about turns, how they decompose into words, and
into phonemes; or, for dual-channel dialogue, have
separate alignments for the different speakers.
2.2 Data Manipulation Tools
In order to process timed linguistic data, we im-
plemented a Perl library and command-line tools,
TGtool and INtool for non-incremental and incre-
mental data respectively. They facilitate handling
(showing, merging, editing, . . . ) and processing
(search-and-replace, hypothesis filtering, . . . ) of
data and interface to TEDview for interactive vi-
sualization.
2.3 TEDview
TEDview is the visualization component of TEL-
IDA. It organizes the different sources of informa-
tion (i.e., alignments or alignment sequences) in
horizontal tracks. Similar as in many of the above-
mentioned tools, time progresses from left to right
in those tracks. The content of tracks consists of
events that are displayed as bars if they have a tem-
poral extent or as diamonds otherwise. TEDview
uses a player metaphor and therefore has a cursor
that marks the current time and a play-mode that
can be used to replay recorded sequences of events
(in real-time or sped-up / slowed-down). Unlike in
other tools, TEDview has a steady cursor (the red
line in the Figures) across which events flow, and
this cursor can be moved, e.g. to give a configura-
tion where no future events are shown.
Information encapsulated by events is displayed
in two different ways:
a) Labels are represented as bars, with the la-
bel information shown as text. (Figure 1 shows a
configuration with only labels.)
b) Events without duration are displayed as di-
amonds at the appropriate time (all other Figures).
Such events can carry a ?payload?; depending on
its type, different display methods are chosen:
? If the payload is an alignment, it is displayed
on the same track, as a sequence of labels.
? In all other cases TEDview determines the
data type of the information and selects an appro-
priate plug-in for displaying it in a separate inspec-
tor window. These data types can be syntax trees,
probability distributions, etc.
To avoid visual clutter, only the information
contained in the diamonds that most recently
passed the cursor are displayed. In this way, TED-
view can elegantly visualize the dynamics of in-
formation state development.
Events can be fed to TEDview either from a file,
in a use case where pre-recorded material is re-
played for analysis, or online, via a network con-
nection, in use cases where processing compo-
nents are monitored or profiled in real-time. The
format used to encode events and their encapsu-
303
Figure 2: TEDview showing different filtering
strategies for incremental ASR: Diamonds corre-
spond to edits of the hypothesis.
lated information is a simple and generic XML
format (which the data manipulation tools can cre-
ate out of other formats, if necessary), i.e. the for-
mat does not make any assumptions as to what the
events represent. For this reason TEDview can be
used to visualize almost any type of discrete tem-
poral data. Intervals can be adorned with display
information, for example to encode further infor-
mation via colouring. Plug-ins for special data-
types can be written in the programming language
Python with its powerful library of extension mod-
ules; this enabled us to implement an inspector for
syntax trees in only 20 lines of code.
3 Use Cases
To illustrate the versatility of the tool, we now de-
scribe how we use it in several projects at our site.
(Technical manuals can be downloaded from the
page listed above.)
3.1 Analysis of Dialogue Data
In the DEAWU project (see e.g. (Schlangen and
Ferna?ndez, 2007)), we used the package to main-
tain transcriptions made in Praat and annotations
made in MMAX2 (Mu?ller and Strube, 2006), and
to visualize these together in a time-aligned view.
As Figure 1 shows, we made heavy use of the
possibility of encoding information via colour. In
the example, there is one track (mac, for mouse
activity) where a numerical value (how much the
mouse travels in a certain time frame) is visual-
ized through the colouring of the interval. In other
tracks other information is encoded through colour
as well. We found this to be of much use in the
?getting to know the data? phase of the analysis of
our experiment. We have also used the tool and
the data in teaching about dialogue structure.
Figure 3: TEDview showing 5-best incremental
ASR hypotheses.
3.2 Analysis of SDS Performance
In another project, we use TELIDA to analyze and
visualize the incremental output of several mod-
ules of a spoken dialogue system we are currently
developing.
In incremental speech recognition, what is con-
sidered the best hypothesis frequently changes as
more speech comes in. We used TEDview to an-
alyze these changes and to develop filtering meth-
ods to reduce the jitter and to reduce edits of the
ASR?s incremental hypothesis (Baumann et al,
2009a). Figure 2 shows incremental hypotheses
and different settings of two filtering strategies.
When evaluating the utility of using n-best ASR
hypotheses, we used TEDview to visualize the
best hypotheses (Baumann et al, 2009b). An in-
teresting result we got from this analysis is that
typically the best hypothesis seems to be more sta-
ble than lower-ranked hypotheses, as can be seen
in Figure 3.
We also evaluated the behaviour of our in-
cremental reference resolution module, which
outputs distributions over possible referents
(Schlangen et al, 2009). We implemented a TED-
view plug-in to show distributions in bar-charts, as
can be seen in Figure 4.
3.3 Analysis of Cognitive Models
In another project, we use TEDview to visualize
the output of an ACT-R (Anderson et al, 2004)
simulation of human sentence parsing developed
by (Patil et al, 2009). This model produces
predictions of parsing costs based on working-
memory load which in turn are used to predict
eye tracking measures in reading. Figure 5 shows
an example where the German sentence ?Den Ton
gab der Ku?nstler seinem Gehilfen? (the artist gives
the clay to his assistant) is being parsed, taking
304
Figure 4: TEDview showing the output of our in-
cremental reference resolution module. Distribu-
tions are shown with a bar-chart plug-in.
about 3 seconds of simulated time. The items in
the channel labeled ?Memory? indicate retrievals
of items from memory, the items in the channel la-
beled ?Parse? indicate that the parser produced a
new hypothesis, and the inspector window on the
right shows the latest of these hypotheses accord-
ing to cursor time. The grey bars finally in the
remaining channels show the activity of the pro-
duction rules. Such visualizations help to quickly
grasp the behaviour of a model, and so greatly aid
development and debugging.
4 Conclusions
We presented TELIDA, a package for the manip-
ulation and visualization of temporally aligned
(linguistic) data. The package enables convenient
handling of dynamic data, especially from incre-
mental processing, but more generally from all
kinds of belief update. We believe that it can be
of use to anyone who is interested in exploring
complex state changes over time, be that in
dialogue annotations or in system performance
profiles.
Acknowledgments This work was funded by
a grant from DFG in the Emmy Noether Pro-
gramme.
References
J.R. Anderson, D. Bothell, M.D. Byrne, S. Douglass,
C. Lebiere, and Y. Qin. 2004. An integrated theory of
the mind. Psychological Review, 111(4):1036?1060.
Timo Baumann, Michaela Atterer, and David Schlangen.
2009a. Assessing and Improving the Performance of
Speech Recognition for Incremental Systems. In Proceed-
ings of NAACL-HLT 2009, Boulder, USA.
Figure 5: TEDview visualizing the dynamics of
an ACT-R simulation, including the current parse-
tree.
Timo Baumann, Okko Bu?, Michaela Atterer, and David
Schlangen. 2009b. Evaluating the Potential Utility of
ASR N-Best Lists for Incremental Spoken Dialogue Sys-
tems. In Proceedings of Interspeech 2009, Brighton, UK.
Paul Boersma. 2001. Praat, a system for doing phonetics by
computer. Glot International, 5(9?10):341?345.
Michael Kipp. 2001. Anvil - a generic annotation tool for
multimodal dialogue. In Proceedings of the 7th Euro-
pean Conference on Speech Communication and Technol-
ogy (Eurospeech), pages 1367?1370, Aalborg, Denmark.
Christoph Mu?ller and Michael Strube. 2006. Multi-level an-
notation of linguistic data with MMAX2. In Corpus Tech-
nology and Language Pedagogy: New Resources, New
Tools, New Methods, pages 197?214. Peter Lang.
Umesh Patil, Marisa Ferrara Boston, John T. Hale, Shravan
Vasishth, and Reinhold Kliegl. 2009. The interaction of
surprisal and working memory cost during reading. In
Proc. of the CUNY sentence processing conference, Davis,
USA.
David Schlangen and Raquel Ferna?ndez. 2007. Speaking
through a noisy channel - experiments on inducing clarifi-
cation behaviour in human-human dialogue. In Proceed-
ings of Interspeech 2007, Antwerp, Belgium.
David Schlangen, Timo Baumann, and Michaela Atterer.
2009. Incremental Reference Resolution: The Task, Met-
rics for Evaluation, and a Bayesian Filtering Model that is
Sensitive to Disfluencies. In Proc. of SigDial 2009, Lon-
don, UK.
Thomas Schmidt. 2004. Transcribing and annotating spoken
language with exmaralda. In Proceedings of the LREC-
Workshop on XML based richly annotated corpora, Lis-
bon 2004, Paris. ELRA. EN.
K. Sjo?lander and J. Beskow. 2000. Wavesurfer?an open
source speech tool. In Sixth International Conference on
Spoken Language Processing, Beijing, China. ISCA.
305
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 514?523,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Joint Satisfaction of Syntactic and Pragmatic Constraints
Improves Incremental Spoken Language Understanding
Andreas Peldszus
University of Potsdam
Department for Linguistics
peldszus@uni-potsdam.de
Okko Bu?
University of Potsdam
Department for Linguistics
okko@ling.uni-potsdam.de
Timo Baumann
University of Hamburg
Department for Informatics
baumann@informatik.uni-hamburg.de
David Schlangen
University of Bielefeld
Department for Linguistics
david.schlangen@uni-bielefeld.de
Abstract
We present a model of semantic processing
of spoken language that (a) is robust against
ill-formed input, such as can be expected
from automatic speech recognisers, (b) re-
spects both syntactic and pragmatic con-
straints in the computation of most likely
interpretations, (c) uses a principled, ex-
pressive semantic representation formalism
(RMRS) with a well-defined model the-
ory, and (d) works continuously (produc-
ing meaning representations on a word-
by-word basis, rather than only for full
utterances) and incrementally (computing
only the additional contribution by the new
word, rather than re-computing for the
whole utterance-so-far).
We show that the joint satisfaction of syn-
tactic and pragmatic constraints improves
the performance of the NLU component
(around 10% absolute, over a syntax-only
baseline).
1 Introduction
Incremental processing for spoken dialogue sys-
tems (i. e., the processing of user input even while
it still may be extended) has received renewed at-
tention recently (Aist et al 2007; Baumann et
al., 2009; Bu? and Schlangen, 2010; Skantze and
Hjalmarsson, 2010; DeVault et al 2011; Purver
et al 2011). Most of the practical work, how-
ever, has so far focussed on realising the poten-
tial for generating more responsive system be-
haviour through making available processing re-
sults earlier (e. g. (Skantze and Schlangen, 2009)),
but has otherwise followed a typical pipeline ar-
chitecture where processing results are passed
only in one direction towards the next module.
In this paper, we investigate whether the other
potential advantage of incremental processing?
providing ?higher-level?-feedback to lower-level
modules, in order to improve subsequent process-
ing of the lower-level module?can be realised as
well. Specifically, we experimented with giving a
syntactic parser feedback about whether semantic
readings of nominal phrases it is in the process of
constructing have a denotation in the given con-
text or not. Based on the assumption that speak-
ers do plan their referring expressions so that they
can successfully refer, we use this information to
re-rank derivations; this in turn has an influence
on how the derivations are expanded, given con-
tinued input. As we show in our experiments, for
a corpus of realistic dialogue utterances collected
in a Wizard-of-Oz setting, this strategy led to an
absolute improvement in computing the intended
denotation of around 10% over a baseline (even
more using a more permissive metric), both for
manually transcribed test data as well as for the
output of automatic speech recognition.
The remainder of this paper is structured as fol-
lows: We discuss related work in the next section,
and then describe in general terms our model and
its components. In Section 4 we then describe the
data resources we used for the experiments and
the actual implementation of the model, the base-
lines for comparison, and the results of our exper-
iments. We close with a discussion and an outlook
on future work.
2 Related Work
The idea of using real-world reference to inform
syntactic structure building has been previously
explored by a number of authors. Stoness et al
(2004, 2005) describe a proof-of-concept imple-
514
mentation of a ?continuous understanding? mod-
ule that uses reference information in guiding a
bottom-up chart-parser, which is evaluated on a
single dialogue transcript. In contrast, our model
uses a probabilistic top-down parser with beam
search (following Roark (2001)) and is evalu-
ated on a large number of real-world utterances
as processed by an automatic speech recogniser.
Similarly, DeVault and Stone (2003) describe a
system that implements interaction between a
parser and higher-level modules (in this case, even
more principled, trying to prove presuppositions),
which however is also only tested on a small, con-
structed data-set.
Schuler (2003) and Schuler et al(2009) present
a model where information about reference is
used directly within the speech recogniser, and
hence informs not only syntactic processing but
also word recognition. To this end, the processing
is folded into the decoding step of the ASR, and
is realised as a hierarchical HMM. While techni-
cally interesting, this approach is by design non-
modular and restricted in its syntactic expressiv-
ity.
The work presented here also has connections
to work in psycholinguistics. Pado? et al(2009)
present a model that combines syntactic and se-
mantic models into one plausibility judgement
that is computed incrementally. However, that
work is evaluated for its ability to predict reading
time data and not for its accuracy in computing
meaning.
3 The Model
3.1 Overview
Described abstractly, the model computes the
probability of a syntactic derivation (and its ac-
companying logical form) as a combination of a
syntactic probability (as in a typical PCFG) and
a semantic or pragmatic plausibility.1 The prag-
matic plausibility here comes from the presuppo-
sition that the speaker intended her utterance to
successfully refer, i. e. to have a denotation in the
current situation (a unique one, in the case of def-
inite reference). Hence, readings that do have a
denotation are preferred over those that do not.
1Note that, as described below, in the actual implemen-
tation the weights given to particular derivations are not real
probabilities anymore, as derivations fall out of the beam and
normalisation is not performed after re-weighting.
The components of our model are described in
the following sections: first the parser which com-
putes the syntactic probability in an incremental,
top-down manner; the semantic construction al-
gorithm which associates (underspecified) logi-
cal forms to derivations; the reference resolution
component that computes the pragmatic plausi-
bility; and the combination that incorporates the
feedback from this pragmatic signal.
3.2 Parser
Roark (2001) introduces a strategy for incremen-
tal probabilistic top-down parsing and shows that
it can compete with high-coverage bottom-up
parsers. One of the reasons he gives for choosing
a top-down approach is that it enables fully left-
connected derivations, where at every process-
ing step new increments directly find their place
in the existing structure. This monotonically en-
riched structure can then serve as a context for in-
cremental language understanding, as the author
claims, although this part is not further developed
by Roark (2001). He discusses a battery of dif-
ferent techniques for refining his results, mostly
based on grammar transformations and on con-
ditioning functions that manipulate a derivation
probability on the basis of local linguistic and lex-
ical information.
We implemented a basic version of his parser
without considering additional conditioning or
lexicalizations. However, we applied left-facto-
rization to parts of the grammar to delay cer-
tain structural decisions as long as possible. The
search-space is reduced by using beam search. To
match the next token, the parser tries to expand
the existing derivations. These derivations are
stored in a priorized queue, which means that the
most probable derivation will always be served
first. Derivations resulting from rule expansions
are kept in the current queue, derivations result-
ing from a successful lexical match are pushed in
a new queue. The parser proceeds with the next
most probable derivation until the current queue
is empty or until a threshhold is reached at which
remaining analyses are pruned. This threshhold
is determined dynamically: If the probability of
the current derivation is lower than the product of
the best derivation?s probability on the new queue,
the number of derivations in the new queue, and a
base beam factor (an initial parameter for the size
of the search beam), then all further old deriva-
515
FormulaIU
CandidateAnalysisIU
TagIU
TextualWordIU
FormulaIU[ [l0:a1:i2]{ [l0:a1:i2] } ] FormulaIU[ [l0:a1:e2]{ [l0:a1:e2] }ARG1(a1,x8),l6:a7:addressee(x8),l0:a1:_nehmen(e2)]
CandidateAnalysisIULD=[s*/s, s/vp, vp/vvimp-v1, m(vvimp)]P=0.49S=[V1, S!]
CandidateAnalysisIULD=[]P=1.00S=[S*,S!]
TagIUvvimp
FormulaIU...
CandidateAnalysisIULD=[s*/s,kon,s*, s/vp, vp/vvimp-v1, m(vvimp)]P=0.14S=[V1, kon, S*, S!]
FormulaIU[ [l0:a1:e2]{ [l18:a19:x14] [l0:a1:e2] }ARG1(a1,x8),l6:a7:addressee(x8),l0:a1:_nehmen(e2),ARG2(a1,x14),BV(a13,x14),RSTR(a13,h21),BODY(a13,h22),l12:a13:_def(),qeq(h21,l18)]
CandidateAnalysisIULD=[v1/np-vz, np/det-n1, m(det)]P=0.2205S=[N1, VZ, S!]
TagIUdet
FormulaIU...
CandidateAnalysisIULD=[v1/np-vz, np/pper, i(det)]P=0.00441S=[pper, VZ, S!]
FormulaIU[ [l0:a1:e2]{ [l29:a30:x14] [l0:a1:e2] }ARG1(a1,x8),l6:a7:addressee(x8),l0:a1:_nehmen(e2),ARG2(a1,x14),BV(a13,x14),RSTR(a13,h21),BODY(a13,h22),l12:a13:_def(),l18:a19:_winkel(x14),qeq(h21,l18)]
CandidateAnalysisIULD=[n1/nn-nz, m(nn)]P=0.06615S=[NZ, VZ, S!]
TagIUnn
FormulaIU...
CandidateAnalysisIULD=[n1/adjp-n1, adjp/adja, i(nn)]P=0.002646S=[adja, N1, VZ, S!]
FormulaIU...
CandidateAnalysisIULD=[n1/nadj-nz, nadj/adja, i(nn)]P=0.000441S=[adja, NZ, VZ, S!]
FormulaIU[ [l0:a1:e2]{ [l42:a43:x44] [l29:a30:x14] [l0:a1:e2] }ARG1(a1,x8),l6:a7:addressee(x8),l0:a1:_nehmen(e2),ARG2(a1,x14),BV(a13,x14),RSTR(a13,h21),BODY(a13,h22),l12:a13:_def(),l18:a19:_winkel(x14),ARG1(a40,x14),ARG2(a40,x44),l39:a40:_in(e41),qeq(h21,l18)]
CandidateAnalysisIULD=[nz/pp-nz, pp/appr-np, m(appr)]P=0.0178605S=[NP, NZ, VZ, S!]
TagIUappr
FormulaIU...
CandidateAnalysisIULD=[nz/advp-nz, advp/adv, i(appr)]P=0.0003969S=[adv, NZ, VZ, S!]
FormulaIU...
CandidateAnalysisIULD=[nz/eps, vz/advp-vz, advp/adv, i(appr)]P=0.00007938S=[adv, VZ, S!]
TagIU$TopOfTags
TextualWordIUnimm TextualWordIUden TextualWordIUwinkel TextualWordIUinTextualWordIU$TopOfWords
Figure 1: An example network of incremental units, including the levels of words, POS-tags, syntactic derivations
and logical forms. See section 3 for a more detailed description.
tions are pruned. Due to probabilistic weighing
and the left factorization of the rules, left recur-
sion poses no direct threat in such an approach.
Additionally, we implemented three robust lex-
ical operations: insertions consume the current
token without matching it to the top stack item;
deletions can ?consume? a requested but actu-
ally non-existent token; repairs adjust unknown
tokens to the requested token. These robust op-
erations have strong penalties on the probability
to make sure they will survive in the derivation
only in critical situations. Additionally, only a
single one of them is allowed to occur between
the recognition of two adjacent input tokens.
Figure 1 illustrates this process for the first few
words of the example sentence ?nimm den winkel
in der dritten reihe? (take the bracket in the third
row), using the incremental unit (IU) model to
represent increments and how they are linked; see
(Schlangen and Skantze, 2009).2 Here, syntactic
2Very briefly: rounded boxes in the Figures represent
IUs, and dashed arrows link an IU to its predecessor on the
same level, where the levels correspond to processing stages.
The Figure shows the levels of input words, POS-tags, syn-
tactic derivations and logical forms. Multiple IUs sharing
derivations (?CandidateAnalysisIUs?) are repre-
sented by three features: a list of the last parser ac-
tions of the derivation (LD), with rule expansions
or (robust) lexical matches; the derivation proba-
bility (P); and the remaining stack (S), where S*
is the grammar?s start symbol and S! an explicit
end-of-input marker. (To keep the Figure small,
we artificially reduced the beam size and cut off
alternatives paths, shown in grey.)
3.3 Semantic Construction Using RMRS
As a novel feature, we use for the representation
of meaning increments (that is, the contributions
of new words and syntactic constructions) as well
as for the resulting logical forms the formalism
Robust Minimal Recursion Semantics (Copestake,
2006). This is a representation formalism that was
originally constructed for semantic underspecifi-
cation (of scope and other phenomena) and then
adapted to serve the purposes of semantics repre-
the same predecessor can be regarded as alternatives. Solid
arrows indicate which information from a previous level an
IU is grounded in (based on); here, every semantic IU is
grounded in a syntactic IU, every syntactic IU in a POS-tag-
IU, and so on.
516
sentations in heterogeneous situations where in-
formation from deep and shallow parsers must be
combined. In RMRS, meaning representations of
a first order logic are underspecified in two ways:
First, the scope relationships can be underspeci-
fied by splitting the formula into a list of elemen-
tary predications (EP) which receive a label ` and
are explicitly related by stating scope constraints
to hold between them (e.g. qeq-constraints). This
way, all scope readings can be compactly repre-
sented. Second, RMRS allows underspecification
of the predicate-argument-structure of EPs. Ar-
guments are bound to a predicate by anchor vari-
ables a, expressed in the form of an argument re-
lation ARGREL(a,x). This way, predicates can
be introduced without fixed arity and arguments
can be introduced without knowing which predi-
cates they are arguments of. We will make use of
this second form of underspecification and enrich
lexical predicates with arguments incrementally.
Combining two RMRS structures involves at
least joining their list of EPs and ARGRELs and
of scope constraints. Additionally, equations be-
tween the variables can connect two structures,
which is an essential requirement for semantic
construction. A semantic algebra for the combi-
nation of RMRSs in a non-lexicalist setting is de-
fined in (Copestake, 2007). Unsaturated semantic
increments have open slots that need to be filled
by what is called the hook of another structure.
Hook and slot are triples [`:a:x] consisting of a
label, an anchor and an index variable. Every vari-
able of the hook is equated with the corresponding
one in the slot. This way the semantic representa-
tion can grow monotonically at each combinatory
step by simply adding predicates, constraints and
equations.
Our approach differs from (Copestake, 2007)
only in the organisation of the slots: In an incre-
mental setting, a proper semantic representation
is desired for every single state of growth of the
syntactic tree. Typically, RMRS composition as-
sumes that the order of semantic combination is
parallel to a bottom-up traversal of the syntactic
tree. Yet, this would require for every incremental
step first to calculate an adequate underspecified
semantic representation for the projected nodes
on the lower right border of the tree and then to
proceed with the combination not only of the new
semantic increments but of the complete tree. For
our purposes, it is more elegant to proceed with
semantic combination in synchronisation with the
syntactic expansion of the tree, i.e. in a top-down
left-to-right fashion. This way, no underspecifica-
tion of projected nodes and no re-interpretation of
already existing parts of the tree is required. This,
however, requires adjustments to the slot structure
of RMRS. Left-recursive rules can introduce mul-
tiple slots of the same sort before they are filled,
which is not allowed in the classic (R)MRS se-
mantic algebra, where only one named slot of
each sort can be open at a time. We thus organize
the slots as a stack of unnamed slots, where mul-
tiple slots of the same sort can be stored, but only
the one on top can be accessed. We then define
a basic combination operation equivalent to for-
ward function composition (as in standard lambda
calculus, or in CCG (Steedman, 2000)) and com-
bine substructures in a principled way across mul-
tiple syntactic rules without the need to represent
slot names.
Each lexical items receives a generic represen-
tation derived from its lemma and the basic se-
mantic type (individual, event, or underspecified
denotations), determined by its POS tag. This
makes the grammar independent of knowledge
about what later (semantic) components will ac-
tually be able to process (?understand?).3 Parallel
to the production of syntactic derivations, as the
tree is expanded top-down left-to-right, seman-
tic macros are activated for each syntactic rule,
composing the contribution of the new increment.
This allows for a monotonic semantics construc-
tion process that proceeds in lockstep with the
syntactic analysis.
Figure 1 (in the ?FormulaIU? box) illustrates
the results of this process for our example deriva-
tion. Again, alternatives paths have been cut to
keep the size of the illustration small. Notice that,
apart from the end-of-input marker, the stack of
semantic slots (in curly brackets) is always syn-
chronized with the parser?s stack.
3.4 Computing Noun Phrase Denotations
Formally, the task of this module is, given a model
M of the current context, to compute the set of
all variable assignments such that M satisfies ?:
G = {g | M |=g ?}. If |G| > 1, we say that ?
refers ambiguously; if |G| = 1, it refers uniquely;
3This feature is not used in the work presented here, but
it could be used for enabling the system to learn the meaning
of unknown words.
517
and if |G| = 0, it fails to refer. This process does
not work directly on RMRS formulae, but on ex-
tracted and unscoped first-order representations of
their nominal content.
3.5 Parse Pruning Using Reference
Information
After all possible syntactic hypotheses at an in-
crement have been derived by the parser and
the corresponding semantic representations have
been constructed, reference resolution informa-
tion can be used to re-rank the derivations. If
pragmatic feedback is enabled, the probability of
every reprentation that does not resolve in the cur-
rent context is degraded by a constant factor (we
used 0.001 in our experiments described below,
determined by experimentation). The degradation
thus changes the derivation order in the parsing
queue for the next input item and increases the
chances of degraded derivations to be pruned in
the following parsing step.
4 Experiments and Results
4.1 Data
We use data from the Pentomino puzzle piece do-
main (which has been used before for example
by (Ferna?ndez and Schlangen, 2007; Schlangen et
al., 2009)), collected in a Wizard-of-Oz study. In
this specific setting, users gave instructions to the
system (the wizard) in order to manipulate (select,
rotate, mirror, delete) puzzle pieces on an upper
board and to put them onto a lower board, reach-
ing a pre-specified goal state. Figure 2 shows an
example configuration. Each participant took part
in several rounds in which the distinguishing char-
acteristics for puzzle pieces (color, shape, pro-
posed name, position on the board) varied widely.
In total, 20 participants played 284 games.
We extracted the semantics of an utterance
from the wizard?s response action. In some cases,
such a mapping was not possible to do (e. g. be-
cause the wizard did not perform a next action,
mimicking a non-understanding by the system),
or potentially unreliable (if the wizard performed
several actions at or around the end of the utter-
ance). We discarded utterances without a clear se-
mantics alignment, leaving 1687 semantically an-
notated user utterances. The wizard of course was
able to use her model of the previous discourse for
resolving references, including anaphoric ones; as
Figure 2: The game board used in the study, as pre-
sented to the player: (a) the current state of the game
on the left, (b) the goal state to be reached on the right.
our study does not focus on these, we have dis-
regarded another 661 utterances in which pieces
are referred to by pronouns, leaving us with 1026
utterances for evaluation. These utterances con-
tained on average 5.2 words (median 5 words;
std dev 2 words).
In order to test the robustness of our method,
we generated speech recognition output using an
acoustic model trained for spontaneous (German)
speech. We used leave-one-out language model
training, i. e. we trained a language model for ev-
ery utterance to be recognized which was based
on all the other utterances in the corpus. Unfor-
tunately, the audio recordings of the first record-
ing day were too quiet for successful recognition
(with a deletion rate of 14%). We thus decided
to limit the analysis for speech recognition out-
put to the remaining 633 utterances from the other
recording days. On this part of the corpus word
error rate (WER) was at 18%.
The subset of the full corpus that we used for
evaluation, with the utterances selected according
to the criteria described above, nevertheless still
only consists of natural, spontaneous utterances
(with all the syntactic complexity that brings) that
are representative for interactions in this type of
domain.
4.2 Grammar and Resolution Model
The grammar used in our experiments was hand-
constructed, inspired by a cursory inspection of
the corpus and aiming to reach good coverage
518
Words Predicates Status
nimm nimm(e) -1
nimm den nimm(e,x) def(x) 0
nimm den Winkel nimm(e,x) def(x) winkel(x) 0
nimm den Winkel in nimm(e,x) def(x) winkel(x) in(x,y) 0
nimm den Winkel in der nimm(e,x) def(x) winkel(x) in(x,y) def(y) 0
nimm den Winkel in der dritten nimm(e,x) def(x) winkel(x) in(x,y) def(y) third(y) 1
nimm den Winkel in der dritten Reihe nimm(e,x) def(x) winkel(x) in(x,y) def(y) third(y) row(y) 1
Table 1: Example of logical forms (flattened into first-order base-language formulae) and reference resolution
results for incrementally parsing and resolving ?nimm den winkel in der dritten reihe?
for a core fragment. We created 30 rules, whose
weights were also set by hand (as discussed be-
low, this is an obvious area for future improve-
ment), sparingly and according to standard intu-
itions. When parsing, the first step is the assign-
ment of a POS tag to each word. This is done by
a simple lookup tagger that stores the most fre-
quent tag for each word (as determined on a small
subset of our corpus).4
The situation model used in reference resolu-
tion is automatically derived from the internal
representation of the current game state. (This
was recorded in an XML-format for each utter-
ance in our corpus.) Variable assignments were
then derived from the relevant nominal predicate
structures,5 consisting of extracted simple pred-
ications, e. g. red(x) and cross(x) for the NP in
a phrase such as ?take the red cross?. For each
unique predicate argument X in these EP struc-
tures (such as as x above), the set of domain ob-
jects that satisfied all predicates of which X was
an argument were determined. For example for
the phrase above, X mapped to all elements that
were red and crosses.
Finally, the size of these sets was determined:
no elements, one element, or multiple elements,
as described above. Emptiness of at least one set
denoted that no resolution was possible (for in-
stance, if no red crosses were available, x?s set
was empty), uniqueness of all sets denoted that
an exact resolution was possible while multiple
elements in at least some sets denoted ambiguity.
This status was then leveraged for parse pruning,
as per Section 3.5.
A more complex example using the scene de-
picted in Figure 2 and the sentence ?nimm den
4A more sophisticated approach has recently been pro-
posed by Beuck et al(2011); this could be used in our setup.
5The domain model did not allow making a plausibility
judgement based on verbal resolution.
winkel in der dritten reihe? (take the bracket in the
third row) is shown in Table 1. The first column
shows the incremental word hypothesis string, the
second the set of predicates derived from the most
recent RMRS representation and the third the res-
olution status (-1 for no resolution, 0 for some res-
olution and 1 for a unique resolution).
4.3 Baselines and Evaluation Metric
4.3.1 Variants / Baselines
To be able to accurately quantify and assess the
effect of our reference-feedback strategy, we im-
plemented different variants / baselines. These all
differ in how, at each step, the reading is deter-
mined that is evaluated against the gold standard,
and are described in the following:
In the Just Syntax (JS) variant, we simply take
single-best derivation, as determined by syntax
alone and evaluate this.
The External Filtering (EF) variant adds in-
formation from reference resolution, but keeps
it separate from the parsing process. Here, we
look at the 5 highest ranking derivations (as de-
termined by syntax alone), and go through them
beginning at the highest ranked, picking the first
derivation where reference resolution can be per-
formed uniquely; this reading is then put up for
evaluation. If there is no such reading, the highest
ranking one will be put forward for evaluation (as
in JS).
Syntax/Pragmatics Interaction (SPI) is the
variant described in the previous section. Here,
all active derivations are sent to the reference res-
olution module, and are re-weighted as described
above; after this has been done, the highest-
ranking reading is evaluated.
Finally, the Combined Interaction and Fil-
tering (CIF) variant combines the previous two
strategies, by using reference-feedback in com-
puting the ranking for the derivations, and then
519
again using reference-information to identify the
most promising reading within the set of 5 highest
ranking ones.
4.3.2 Metric
When a reading has been identified according
to one of these methods, a score s is computed as
follows: s = 1, if the correct referent (according
to the gold standard) is computed as the denota-
tion for this reading; s = 0 if no unique referent
can be computed, but the correct one is part of the
set of possible referents; s = ?1 if no referent
can be computed at all, or the correct one is not
part of the set of those that are computed.
As this is done incrementally for each word
(adding the new word to the parser chart), for an
utterance of length m we get a sequence of m
such numbers. (In our experiments we treat the
?end of utterance? signal as a pseudo-word, since
knowing that an utterance has concluded allows
the parser to close off derivations and remove
those that are still requiring elements. Hence, we
in fact have sequences ofm+1 numbers.) A com-
bined score for the whole utterance is computed
according to the following formula:
su =
m?
n=1
(sn ? n/m)
(where sn is the score at position n). The fac-
tor n/m causes ?later? decisions to count more
towards the final score, reflecting the idea that
it is more to be expected (and less harmful) to
be wrong early on in the utterance, whereas the
longer the utterance goes on, the more pressing
it becomes to get a correct result (and the more
damaging if mistakes are made).6
Note that this score is not normalised by utter-
ance length m; the maximally achievable score
being (m + 1)/2. This has the additional ef-
fect of increasing the weight of long utterances
when averaging over the score of all utterances;
we see this as desirable, as the analysis task be-
comes harder the longer the utterance is.
We use success in resolving reference to eval-
uate the performance of our parsing and semantic
construction component, where more tradition-
ally, metrics like parse bracketing accuracy might
6This metric compresses into a single number some of
the concerns of the incremental metrics developed in (Bau-
mann et al 2011), which can express more fine-grainedly
the temporal development of hypotheses.
be used. But as we are building this module for an
interactive system, ultimately, accuracy in recov-
ering meaning is what we are interested in, and so
we see this not just as a proxy, but actually as a
more valuable metric. Moreover, this metric can
be applied at each incremental step, which is not
clear how to do with more traditional metrics.
4.4 Experiments
Our parser, semantic construction and reference
resolution modules are implemented within the
InproTK toolkit for incremental spoken dialogue
systems development (Schlangen et al 2010). In
this toolkit, incremental hypotheses are modified
as more information becomes available over time.
Our modules support all such modifications (i. e.
also allow to revert their states and output if word
input is revoked).
As explained in Section 4.1, we used offline
recognition results in our evaluation. However,
the results would be identical if we were to use
the incremental speech recognition output of In-
proTK directly.
The system performs several times faster than
real-time on a standard workstation computer. We
thus consider it ready to improve practical end-to-
end incremental systems which perform within-
turn actions such as those outlined in (Bu? and
Schlangen, 2010).
The parser was run with a base-beam factor of
0.01; this parameter may need to be adjusted if a
larger grammar was used.
4.5 Results
Table 2 shows an overview of the experiment re-
sults. The table lists, separately for the manual
transcriptions and the ASR transcripts, first the
number of times that the final reading did not re-
solve at all, or to a wrong entitiy; did not uniquely
resolve, but included the correct entity in its de-
notiation; or did uniquely resolve to the correct
entity (-1, 0, and 1, respectively). The next lines
show ?strict accuracy? (proportion of ?1? among
all results) at the end of utterance, and ?relaxed
accuracy? (which allows ambiguity, i.e., is the set
{0, 1}). incr.scr is the incremental score as de-
scribed above, which includes in the evaluation
the development of references and not just the fi-
nal state. (And in that sense, is the most appro-
priate metric here, as it captures the incremental
behaviour.) This score is shown both as absolute
520
JS EF SPI CIF
tr
an
sc
ri
pt
?1 563 518 364 363
0 197 198 267 268
1 264 308 392 392
str.acc. 25.7% 30.0% 38.2% 38.2%
rel.acc. 44.9% 49.3% 64.2% 64.3%
incr.scr ?1568 ?1248 ?536 ?504
avg.incr.scr ?1.52 ?1.22 ?0.52 ?0.49
re
co
gn
ti
on
?1 362 348 254 255
0 122 121 173 173
1 143 158 196 195
str.acc. 22.6% 25.0% 31.0% 30.8%
rel.acc. 41.2% 44.1% 58.3% 58.1%
incr.scr ?1906 ?1730 ?1105 ?1076
avg.incr.scr ?1.86 ?1.69 ?1.01 ?1.05
Table 2: Results of the Experiments. See text for explanation of metrics.
number as well as averaged for each utterance.
As these results show, the strategy of provid-
ing the parser with feedback about the real-world
utility of constructed phrases (in the form of refer-
ence decisions) improves the parser, in the sense
that it helps the parser to successfully retrieve the
intended meaning more often compared to an ap-
proach that only uses syntactic information (JS)
or that uses pragmatic information only outside
of the main programme: 38.2% strict or 64.2%
relaxed for SPI over 25.7% / 44.9% for JS, an
absolute improvement of 12.5% for strict or even
more, 19.3%, for the relaxed metric; the incre-
mental metric shows that this advantage holds not
only at the final word, but also consistently within
the utterance, the average incremental score for
an utterance being ?0.49 for SPI and ?1.52
for JS. The improvement is somewhat smaller
against the variant that uses some reference infor-
mation, but does not integrate this into the parsing
process (EF), but it is still consistently present.
Adding such n-best-list processing to the output
of the parser+reference-combination (as variant
CIF does) finally does not further improve the
performance noticeably. When processing par-
tially defective material (the output of the speech
recogniser), the difference between the variants
is maintained, showing a clear advantage of SPI,
although performance of all variants is degraded
somewhat.
Clearly, accuracy is rather low for the base-
line condition (JS); this is due to the large num-
ber of non-standard constructions in our sponta-
neous material (e.g., utterances like ?lo?schen, un-
ten? (delete, bottom) which we did not try to cover
with syntactic rules, and which may not even con-
tain NPs. The SPI condition can promote deriva-
tions resulting from robust rules (here, deletion)
which then can refer. In general though state-of-
the art grammar engineering may narrow the gap
between JS and SPI ? this remains to be tested ?
but we see as an advantage of our approach that
it can improve over the (easy-to-engineer) set of
core grammar rules.
5 Conclusions
We have described a model of semantic process-
ing of natural, spontaneous speech that strives
to jointly satisfy syntactic and pragmatic con-
straints (the latter being approximated by the as-
sumption that referring expressions are intended
to indeed successfully refer in the given context).
The model is robust, accepting also input of the
kind that can be expected from automatic speech
recognisers, and incremental, that is, can be fed
input on a word-by-word basis, computing at each
increment only exactly the contribution of the new
word. Lastly, as another novel contribution, the
model makes use of a principled formalism for se-
mantic representation, RMRS (Copestake, 2006).
While the results show that our approach of
combining syntactic and pragmatic information
can work in a real-world setting on realistic
data?previous work in this direction has so far
521
only been at the proof-of-concept stage?there is
much room for improvement. First, we are now
exploring ways of bootstrapping a grammar and
derivation weights from hand-corrected parses.
Secondly, we are looking at making the variable
assignment / model checking function probabilis-
tic, assigning probabilities (degree of strength of
belief) to candidate resolutions (as for example
the model of Schlangen et al(2009) does). An-
other next step?which will be very easy to take,
given the modular nature of the implementation
framework that we have used?will be to integrate
this component into an interactive end-to-end sys-
tem, and testing other domains in the process.
Acknowledgements We thank the anonymous
reviewers for their helpful comments. The work
reported here was supported by a DFG grant in
the Emmy Noether programme to the last author
and a stipend from DFG-CRC (SFB) 632 to the
first author.
References
Gregory Aist, James Allen, Ellen Campana, Car-
los Gomez Gallo, Scott Stoness, Mary Swift, and
Michael K. Tanenhaus. 2007. Incremental under-
standing in human-computer dialogue and experi-
mental evidence for advantages over nonincremen-
tal methods. In Proceedings of Decalog 2007, the
11th International Workshop on the Semantics and
Pragmatics of Dialogue, Trento, Italy.
Timo Baumann, Michaela Atterer, and David
Schlangen. 2009. Assessing and improving the per-
formance of speech recognition for incremental sys-
tems. In Proceedings of the North American Chap-
ter of the Association for Computational Linguis-
tics - Human Language Technologies (NAACL HLT)
2009 Conference, Boulder, Colorado, USA, May.
Timo Baumann, Okko Bu?, and David Schlangen.
2011. Evaluation and optimization of incremen-
tal processors. Dialogue and Discourse, 2(1):113?
141.
Niels Beuck, Arne Ko?hn, and Wolfgang Menzel.
2011. Decision strategies for incremental pos tag-
ging. In Proceedings of the 18th Nordic Con-
ference of Computational Linguistics, NODALIDA-
2011, Riga, Latvia.
Okko Bu? and David Schlangen. 2010. Modelling
sub-utterance phenomena in spoken dialogue sys-
tems. In Proceedings of the 14th International
Workshop on the Semantics and Pragmatics of Dia-
logue (Pozdial 2010), pages 33?41, Poznan, Poland,
June.
Ann Copestake. 2006. Robust minimal recursion se-
mantics. Technical report, Cambridge Computer
Lab. Unpublished draft.
Ann Copestake. 2007. Semantic composition with
(robust) minimal recursion semantics. In Proceed-
ings of the Workshop on Deep Linguistic Process-
ing, DeepLP ?07, pages 73?80, Stroudsburg, PA,
USA. Association for Computational Linguistics.
David DeVault and Matthew Stone. 2003. Domain
inference in incremental interpretation. In Proceed-
ings of ICOS 4: Workshop on Inference in Compu-
tational Semantics, Nancy, France, September. IN-
RIA Lorraine.
David DeVault, Kenji Sagae, and David Traum. 2011.
Incremental Interpretation and Prediction of Utter-
ance Meaning for Interactive Dialogue. Dialogue
and Discourse, 2(1):143?170.
Raquel Ferna?ndez and David Schlangen. 2007. Re-
ferring under restricted interactivity conditions. In
Simon Keizer, Harry Bunt, and Tim Paek, editors,
Proceedings of the 8th SIGdial Workshop on Dis-
course and Dialogue, pages 136?139, Antwerp,
Belgium, September.
Ulrike Pado?, Matthew W Crocker, and Frank Keller.
2009. A probabilistic model of semantic plausi-
bility in sentence processing. Cognitive Science,
33(5):794?838.
Matthew Purver, Arash Eshghi, and Julian Hough.
2011. Incremental semantic construction in a di-
alogue system. In J. Bos and S. Pulman, editors,
Proceedings of the 9th International Conference on
Computational Semantics (IWCS), pages 365?369,
Oxford, UK, January.
Brian Roark. 2001. Robust Probabilistic Predictive
Syntactic Processing: Motivations, Models, and
Applications. Ph.D. thesis, Department of Cogni-
tive and Linguistic Sciences, Brown University.
David Schlangen and Gabriel Skantze. 2009. A gen-
eral, abstract model of incremental dialogue pro-
cessing. In EACL ?09: Proceedings of the 12th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 710?718.
Association for Computational Linguistics, mar.
David Schlangen, Timo Baumann, and Michaela At-
terer. 2009. Incremental reference resolution: The
task, metrics for evaluation, and a bayesian filtering
model that is sensitive to disfluencies. In Proceed-
ings of SIGdial 2009, the 10th Annual SIGDIAL
Meeting on Discourse and Dialogue, London, UK,
September.
David Schlangen, Timo Baumann, Hendrik
Buschmeier, Okko Bu?, Stefan Kopp, Gabriel
Skantze, and Ramin Yaghoubzadeh. 2010. Middle-
ware for Incremental Processing in Conversational
Agents. In Proceedings of SigDial 2010, Tokyo,
Japan, September.
522
William Schuler, Stephen Wu, and Lane Schwartz.
2009. A framework for fast incremental interpre-
tation during speech decoding. Computational Lin-
guistics, 35(3).
William Schuler. 2003. Using model-theoretic se-
mantic interpretation to guide statistical parsing and
word recognition in a spoken language interface. In
Proceedings of the 41st Meeting of the Association
for Computational Linguistics (ACL 2003), Sap-
poro, Japan. Association for Computational Lin-
guistics.
Gabriel Skantze and Anna Hjalmarsson. 2010. To-
wards incremental speech generation in dialogue
systems. In Proceedings of the SIGdial 2010 Con-
ference, pages 1?8, Tokyo, Japan, September.
Gabriel Skantze and David Schlangen. 2009. Incre-
mental dialogue processing in a micro-domain. In
Proceedings of the 12th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL 2009), pages 745?753, Athens,
Greece, March.
Mark Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, Massachusetts.
Scott C. Stoness, Joel Tetreault, and James Allen.
2004. Incremental parsing with reference inter-
action. In Proceedings of the Workshop on In-
cremental Parsing at the ACL 2004, pages 18?25,
Barcelona, Spain, July.
Scott C. Stoness, James Allen, Greg Aist, and Mary
Swift. 2005. Using real-world reference to improve
spoken language understanding. In AAAI Workshop
on Spoken Language Understanding, pages 38?45.
523
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 103?108,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
INPRO_iSS: A Component for Just-In-Time Incremental Speech Synthesis
Timo Baumann
University of Hamburg
Department for Informatics
Germany
baumann@informatik.uni-hamburg.de
David Schlangen
University of Bielefeld
Faculty of Linguistics and Literary Studies
Germany
david.schlangen@uni-bielefeld.de
Abstract
We present a component for incremental
speech synthesis (iSS) and a set of applications
that demonstrate its capabilities. This compo-
nent can be used to increase the responsivity
and naturalness of spoken interactive systems.
While iSS can show its full strength in systems
that generate output incrementally, we also dis-
cuss how even otherwise unchanged systems
may profit from its capabilities.
1 Introduction
Current state of the art in speech synthesis for spoken
dialogue systems (SDSs) is for the synthesis com-
ponent to expect full utterances (in textual form) as
input and to deliver an audio stream verbalising this
full utterance. At best, timing information is returned
as well so that a control component can determine in
case of an interruption / barge-in by the user where
in the utterance this happened (Edlund, 2008; Mat-
suyama et al, 2010).
We want to argue here that providing capabilities
to speech synthesis components for dealing with units
smaller than full utterances can be beneficial for a
whole range of interactive speech-based systems. In
the easiest case, incremental synthesis simply reduces
the utterance-initial delay before speech output starts,
as output already starts when its beginning has been
produced. In an otherwise conventional dialogue sys-
tem, the synthesis module could make it possible
to interrupt the output speech stream (e. g., when a
noise event is detected that makes it likely that the
user will not be able to hear what is being said), and
continue production when the interruption is over. If
other SDS components are adapted more to take ad-
vantage of incremental speech synthesis, even more
flexible behaviours can be realised, such as providing
utterances in installments (Clark, 1996) that prompt
for backchannel signals, which in turn can prompt
different utterance continuations, or starting an utter-
ance before all information required in the utterance
is available (?so, uhm, there are flights to Seoul on uh
. . . ?), signaling that the turn is being held. Another,
less conventional type of speech-based system that
could profit from iSS is ?babelfish-like? simultaneous
speech-to-speech translation.
Research on architectures, higher-level process-
ing modules and lower-level processing modules that
would enable such behaviour is currently underway
(Skantze and Schlangen, 2009; Skantze and Hjal-
marsson, 2010; Baumann and Schlangen, 2011), but
a synthesis component that would unlock the full
potential of such strategies is so far missing. In this
paper, we present such a component, which is capa-
ble of
(a) starting to speak before utterance processing has
finished;
(b) handling edits made to (as-yet unspoken) parts of
the utterance even while a prefix is already being
spoken;
(c) enabling adaptations of delivery parameters such
as speaking rate or pitch;
(d) autonomously making appropriate delivery-
related decisions;
(e) providing information about progress in delivery;
and, last but not least,
(f) running in real time.
Our iSS component is built on top of an exist-
ing non-incremental synthesis component, MaryTTS
(Schr?der and Trouvain, 2003), and on an existing
architecture for incremental processing, INPROTK
(Baumann and Schlangen, 2012).
103
After a discussion of related work (Section 2), we
describe the basic elements of our iSS component
(Section 3) and some demonstrator applications that
we created which showcase certain abilities.1
2 Related Work
Typically, in current SDSs utterances are gener-
ated (either by lookup/template-based generation, or,
less commonly, by concept-to-utterance natural lan-
guage generation (NLG)) and then synthesised in full
(McTear, 2002). There is very little work on incre-
mental synthesis (i.e., one that would work with units
smaller than full utterances). Edlund (2008) outlines
some requirements for incremental speech synthe-
sis: to give constant feedback to the dialogue system
about what has been delivered, to be interruptible
(and possibly continue from that position), and to run
in real time. Edlund (2008) also presents a prototype
that meets these requirements, but is limited to di-
phone synthesis that is performed non-incrementally
before utterance delivery starts. We go beyond this
in processing just-in-time, and also enabling changes
during delivery.
Skantze and Hjalmarsson (2010) describe a sys-
tem that generates utterances incrementally (albeit
in a WOz-enviroment), allowing earlier components
to incrementally produce and revise their hypothesis
about the user?s utterance. The system can automati-
cally play hesitations if by the time it has the turn it
does not know what to produce yet. They show that
users prefer such a system over a non-incremental
one, even though it produced longer dialogues. Our
approach is complementary to this work, as it tar-
gets a lower layer, the realisation or synthesis layer.
Where their system relies on ?regular? speech syn-
thesis which is called on relatively short utterance
fragments (and thus pays for the increase in respon-
siveness with a reduction in synthesis quality, esp.
regarding prosody), we aim to incrementalize the
speech synthesis component itself.
Dutoit et al (2011) have presented an incremental
formulation for HMM-based speech synthesis. How-
ever, their system works offline and is fed by non-
incrementally produced phoneme target sequences.
1The code of the toolkit and its iSS component and the demo
applications discussed below have been released as open-source
at http://inprotk.sourceforge.net.
We aim for a fully incremental speech synthesis com-
ponent that can be integrated into dialogue systems.
There is some work on incremental NLG (Kilger
and Finkler, 1995; Finkler, 1997; Guhe, 2007); how-
ever, that work does not concern itself with the actual
synthesis of speech and hence describes only what
would generate the input to our component.
3 Incremental Speech Synthesis
3.1 Background on Speech Synthesis
Text-to-speech (TTS) synthesis normally proceeds in
a top-down fashion, starting on the utterance level
(for stress patterns and sentence-level intonation) and
descending to words and phonemes (for pronunci-
ation details), in order to make globally optimised
decisions (Taylor, 2009). In that way, target phoneme
sequences annotated with durations and pitch con-
tours are generated, in what is called the linguistic
pre-processing step.
The then following synthesis step proper can be
executed in one of several ways, with HMM-based
and unit-selection synthesis currently being seen as
producing the perceptually best results (Taylor, 2009).
The former works by first turning the target sequence
into a sequence of HMM states; a global optimiza-
tion then computes a stream of vocoding features
that optimize both HMM emission probabilities and
continuity constraints (Tokuda et al, 2000). Finally,
the parameter frames are fed to a vocoder which gen-
erates the speech audio signal. Unit-selection, in
contrast, searches for the best sequence of (variably
sized) units of speech in a large, annotated corpus
of recordings, aiming to find a sequence that closely
matches the target sequence.
As mentioned above, Dutoit et al (2011) have pre-
sented an online formulation of the optimization step
in HMM-based synthesis. Beyond this, two other fac-
tors influenced our decision to follow the HMM-based
approach: (a) HMM-based synthesis nicely separates
the production of vocoding parameter frames from
the production of the speech audio signal, which
allows for more fine-grained concurrent processing
(see next subsection); (b) parameters are partially
independent in the vocoding frames, which makes
it possible to manipulate e. g. pitch independently
(and outside of the HMM framework) without altering
other parameters or deteriorating speech quality.
104
Figure 1: Hierarchic structure of incremental units describ-
ing an example utterance as it is being produced during
utterance delivery.
3.2 System Architecture
Our component works by reducing the aforemen-
tioned top-down requirements. We found that it is
not necessary to work out all details at one level
of processing before starting to process at the next
lower level. For example, not all words of the utter-
ance need to be known to produce the sentence-level
intonation (which itself however is necessary to de-
termine pitch contours) as long as a structural outline
of the utterance is available. Likewise, post-lexical
phonological processes can be computed as long
as a local context of one word is available; vocod-
ing parameter computation (which must model co-
articulation effects) in turn can be satisfied with just
one phoneme of context; vocoding itself does not
need any lookahead at all (aside from audio buffering
considerations).
Thus, our component generates its data structures
incrementally in a top-down-and-left-to-right fashion
with different amounts of pre-planning, using sev-
eral processing modules that work concurrently. This
results in a ?triangular? structure (illustrated in Fig-
ure 1) where only the absolutely required minimum
has to be specified at each level, allowing for later
adaptations with few or no recomputations required.
As an aside, we observe that our component?s ar-
chitecture happens to correspond rather closely to
Levelt?s (1989) model of human speech production.
Levelt distinguishes several, partially independent
processing modules (conceptualization, formulation,
articulation, see Figure 1) that function incrementally
and ?in a highly automatic, reflex-like way? (Levelt,
1989, p. 2).
3.3 Technical Overview of Our System
As a basis, we use MaryTTS (Schr?der and Trou-
vain, 2003), but we replace Mary?s internal data struc-
tures with structures that support incremental spec-
ifications; these we take from an extant incremen-
tal spoken dialogue system architecture and toolkit,
INPROTK (Schlangen et al, 2010; Baumann and
Schlangen, 2012). In this architecture, incremental
processing as the processing of incremental units
(IUs), which are the smallest ?chunks? of information
at a specific level (such as words, or phonemes, as
can be seen in Figure 1). IUs are interconnected to
form a network (e. g. words keep links to their asso-
ciated phonemes, and vice-versa) which stores the
system?s complete information state.
The iSS component takes an IU sequence of
chunks of words as input (from an NLG component).
Crucially, this sequence can then still be modified,
through: (a) continuations, which simply link further
words to the end of the sequence; or (b) replacements,
where elements in the sequence are ?unlinked? and
other elements are spliced in. Additionally, a chunk
can be marked as open; this has the effect of linking
to a special hesitation word, which is produced only
if it is not replaced (by the NLG) in time with other
material.
Technically, the representation levels below the
chunk level are generated in our component by
MaryTTS?s linguistic preprocessing and converting
the output to IU structures. Our component provides
for two modes of operation: Either using MaryTTS?
HMM optimization routines which non-incrementally
solve a large matrix operation and subsequently iter-
atively optimize the global variance constraint (Toda
and Tokuda, 2007). Or, using the incremental algo-
rithm as proposed by Dutoit et al (2011). In our
implementation of this algorithm, HMM emissions
are computed with one phoneme of context in both
directions; Dutoit et al (2011) have found this set-
ting to only slightly degrade synthesis quality. While
the former mode incurs some utterance-initial delay,
switching between alternatives and prosodic alter-
ation can be performed at virtually no lookahead,
while requiring just little lookahead for the truly
incremental mode. The resulting vocoding frames
then are attached to their corresponding phoneme
units. Phoneme units then contain all the information
105
Figure 2: Example application that showcases just-in-time
manipulation of prosodic aspects (tempo and pitch) of the
ongoing utterance.
needed for the final vocoding step, in an accessible
form, which makes possible various manipulations
before the final synthesis step.
The lowest level module of our component is what
may be called a crawling vocoder, which actively
moves along the phoneme IU layer, querying each
phoneme for its parameter frames one-by-one and
producing the corresponding audio via vocoding. The
vocoding algorithm is entirely incremental, making
it possible to vocode ?just-in-time?: only when audio
is needed to keep the sound card buffer full does the
vocoder query for a next parameter frame. This is
what gives the higher levels the maximal amount of
time for re-planning, i. e., to be incremental.
3.4 Quality of Results
As these descriptions should have made clear, there
are some elements in the processing steps in our iSS
component that aren?t yet fully incremental, such as
assigning a sentence-level prosody. The best results
are thus achieved if a full utterance is presented to the
component initially, which is used for computation of
prosody, and of which then elements may be changed
(e. g., adjectives are replaced by different ones) on the
fly. It is unavoidable, though, that there can be some
?breaks? at the seams where elements are replaced.
Moreover, the way feature frames can be modified
(as described below) and the incremental HMM op-
timization method may lead to deviations from the
global optimum. Finally, our system still relies on
Mary?s non-incremental HMM state selection tech-
nique which uses decision trees with non-incremental
features.
However, preliminary evaluation of the compo-
nent?s prosody given varying amounts of lookahead
indicate that degradations are reasonably small. Also,
the benefits in naturalness of behaviour enabled by
iSS may outweigh the drawback in prosodic quality.
4 Interface Demonstrations
We will describe the features of iSS, their implemen-
tation, their programming interface, and correspond-
ing demo applications in the following subsections.
4.1 Low-Latency Changes to Prosody
Pitch and tempo can be adapted on the phoneme
IU layer (see Figure 1). Figure 2 shows a demo in-
terface to this functionality. Pitch is determined by
a single parameter in the vocoding frames and can
be adapted independently of other parameters in the
HMM approach. We have implemented capabilities of
adjusting all pitch values in a phoneme by an offset,
or to change the values gradually for all frames in
the phoneme. (The first feature is show-cased in the
application in Figure 2, the latter is used to cancel
utterance-final pitch changes when a continuation is
appended to an ongoing utterance.) Tempo can be
adapted by changing the phoneme units? durations
which will then repeat (or skip) parameter frames
(for lengthened or shortened phonemes, respectively)
when passing them to the crawling vocoder. Adapta-
tions are conducted with virtually no lookahead, that
is, they can be executed even on a phoneme that is
currently being output.
4.2 Feedback on Delivery
We implemented a fine-grained, hierarchical mech-
anism to give detailed feedback on delivery. A new
progress field on IUs marks whether the IU?s produc-
tion is upcoming, ongoing, or completed. Listeners
may subscribe to be notified about such progress
changes using an update interface on IUs. The appli-
cations in Figures 2 and 4 make use of this interface
to mark the words of the utterance in bold for com-
pleted, and in italic for ongoing words (incidentally,
the screenshot in Figure 4 was taken exactly at the
boundary between ?delete? and ?the?).
4.3 Low-Latency Switching of Alternatives
A major goal of iSS is to change what is being said
while the utterance is ongoing. Forward-pointing
same-level links (SLLs, (Schlangen and Skantze,
2009; Baumann and Schlangen, 2012)) as shown
in Figure 3 allow to construct alternative utterance
paths beforehand. Deciding on the actual utterance
continuation is a simple re-ranking of the forward
106
Figure 3: Incremental units chained together via forward-
pointing same-level links to form an utterance tree.
Figure 4: Example application to showcase just-in-time
selection between different paths in a complex utterance.
SLLs which can be changed until immediately before
the word (or phoneme) in question is being uttered.
The demo application shown in Figure 4 allows the
user to select the path through a fairly complex utter-
ance tree. The user has already decided on the color,
but not on the type of piece to be deleted and hence
the currently selected plan is to play a hesitation (see
below).
4.4 Extension of the Ongoing Utterance
In the previous subsection we have shown how alter-
natives in utterances can be selected with very low
latency. Adding continuations (or alternatives) to
an ongoing utterance incurs some delay (some hun-
dred milliseconds), as we ensure that an appropriate
sentence-level prosody for the alternative (or con-
tinuation) is produced by re-running the linguistic
pre-processing on the complete utterance; we then
integrate only the new, changed parts into the IU
structure (or, if there still is time, parts just before the
change, to account for co-articulation).
Thus, practical applications which use incremen-
tal NLG must generate their next steps with some
lookahead to avoid stalling the output. However, ut-
terances can be marked as non-final, which results in
a special hesitation word being inserted, as explained
below.
4.5 Autonomously Performing Disfluencies
In a multi-threaded, real-time system, the crawling
vocoder may reach the end of synthesis before the
NLG component (in its own thread) has been able
to add a continuation to the ongoing utterance. To
avoid this case, special hesitation words can be in-
serted at the end of a yet unfinished utterance. If the
crawling vocoder nears such a word, a hesitation will
be played, unless a continuation is available. In that
case, the hesitation is skipped (or aborted if currently
ongoing).2
4.6 Type-to-Speech
A final demo application show-cases truly incremen-
tal HMM synthesis taken to its most extreme: A text
input window is presented, and each word that is
typed is treated as a single-word chunk which is im-
mediately sent to the incremental synthesizer. (For
this demonstration, synthesis is slowed to half the
regular speed, to account for slow typing speeds and
to highlight the prosodic improvements when more
right context becomes available to iSS.) A use case
with a similar (but probably lower) level of incre-
mentality could be simultaneous speech-to-speech
translation, or type-to-speech for people with speech
disabilities.
5 Conclusions
We have presented a component for incremental
speech synthesis (iSS) and demonstrated its capa-
bilities with a number of example applications. This
component can be used to increase the responsivity
and naturalness of spoken interactive systems. While
iSS can show its full strengths in systems that also
generate output incrementally (a strategy which is
currently seeing some renewed attention), we dis-
cussed how even otherwise unchanged systems may
profit from its capabilities, e. g., in the presence of
intermittent noise. We provide this component in the
hope that it will help spur research on incremental
natural language generation and more interactive spo-
ken dialogue systems, which so far had to made do
with inadequate ways of realising its output.
2Thus, in contrast to (Skantze and Hjalmarsson, 2010), hesi-
tations do not take up any additional time.
107
References
Timo Baumann and David Schlangen. 2011. Predicting
the Micro-Timing of User Input for an Incremental Spo-
ken Dialogue System that Completes a User?s Ongoing
Turn. In Proceedings of SigDial 2011, pages 120?129,
Portland, USA, June.
Timo Baumann and David Schlangen. 2012. The
INPROTK 2012 release. In Proceedings of SDCTD.
to appear.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press.
Thierry Dutoit, Maria Astrinaki, Onur Babacan, Nico-
las d?Alessandro, and Benjamin Picart. 2011. pHTS
for Max/MSP: A Streaming Architecture for Statistical
Parametric Speech Synthesis. Technical Report 1, nu-
mediart Research Program on Digital Art Technologies,
March.
Jens Edlund. 2008. Incremental speech synthesis. In
Second Swedish Language Technology Conference,
pages 53?54, Stockholm, Sweden, November. System
Demonstration.
Wolfgang Finkler. 1997. Automatische Selbstkorrek-
tur bei der inkrementellen Generierung gesprochener
Sprache unter Realzeitbedingungen. Dissertationen zur
K?nstlichen Intelligenz. infix Verlag.
Markus Guhe. 2007. Incremental Conceptualization for
Language Production. Lawrence Erlbaum Asso., Inc.,
Mahwah, USA.
Anne Kilger and Wolfgang Finkler. 1995. Incremen-
tal Generation for Real-time Applications. Technical
Report RR-95-11, DFKI, Saarbr?cken, Germany.
William J.M. Levelt. 1989. Speaking: From Intention to
Articulation. MIT Press.
Kyoko Matsuyama, Kazunori Komatani, Ryu Takeda,
Toru Takahashi, Tetsuya Ogata, and Hiroshi G. Okuno.
2010. Analyzing User Utterances in Barge-in-able Spo-
ken Dialogue System for Improving Identification Ac-
curacy. In Proceedings of Interspeech, pages 3050?
3053, Makuhari, Japan, September.
Michael McTear. 2002. Spoken Dialogue Technology.
Toward the Conversational User-Interface. Springer,
London, UK.
David Schlangen and Gabriel Skantze. 2009. A General,
Abstract Model of Incremental Dialogue Processing.
In Proceedings of the EACL, Athens, Greece.
David Schlangen, Timo Baumann, Hendrik Buschmeier,
Okko Bu?, Stefan Kopp, Gabriel Skantze, and Ramin
Yaghoubzadeh. 2010. Middleware for Incremental
Processing in Conversational Agents. In Proceedings of
SigDial 2010, pages 51?54, Tokyo, Japan, September.
Marc Schr?der and J?rgen Trouvain. 2003. The German
Text-to-Speech Synthesis System MARY: A Tool for
Research, Development and Teaching. International
Journal of Speech Technology, 6(3):365?377, October.
Gabriel Skantze and Anna Hjalmarsson. 2010. Towards
incremental speech generation in dialogue systems. In
Proceedings of SigDial 2010, pages 1?8, Tokyo, Japan,
September.
Gabriel Skantze and David Schlangen. 2009. Incremental
dialogue processing in a micro-domain. In Proceedings
of EACL 2009, Athens, Greece, April.
Paul Taylor. 2009. Text-to-Speech Synthesis. Cambridge
Univ Press, Cambridge, UK.
Tomoki Toda and Keiichi Tokuda. 2007. A Speech Pa-
rameter Generation Algorithm Considering Global Vari-
ance for HMM-based Speech Synthesis. IEICE Trans-
actions on Information and Systems, 90(5):816?824.
Keiichi Tokuda, Takayoshi Yoshimura, Takashi Ma-
suko, Takao Kobayashi, and Tadashi Kitamura. 2000.
Speech Parameter Generation Algorithms for HMM-
based Speech Synthesis. In Proceedings of ICASSP
2000, pages 1315?1318, Istanbul, Turkey.
108
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 9?16,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Comparing Local and Sequential Models for
Statistical Incremental Natural Language Understanding
Silvan Heintze, Timo Baumann, David Schlangen
Department of Linguistics
University of Potsdam, Germany
firstname.lastname@uni-potsdam.de
Abstract
Incremental natural language understand-
ing is the task of assigning semantic rep-
resentations to successively larger prefixes
of utterances. We compare two types of
statistical models for this task: a) local
models, which predict a single class for
an input; and b), sequential models, which
align a sequence of classes to a sequence
of input tokens. We show that, with some
modifications, the first type of model can
be improved and made to approximate the
output of the second, even though the lat-
ter is more informative. We show on two
different data sets that both types of model
achieve comparable performance (signifi-
cantly better than a baseline), with the first
type requiring simpler training data. Re-
sults for the first type of model have been
reported in the literature; we show that for
our kind of data our more sophisticated
variant of the model performs better.
1 Introduction
Imagine being at a dinner, when your friend Bert
says ?My friend, can you pass me the salt over
there, please??. It is quite likely that you get the
idea that something is wanted of you fairly early
into the utterance, and understand what exactly it
is that is wanted even before the utterance is over.
This is possible only because you form an un-
derstanding of the meaning of the utterance even
before it is complete; an understanding which
you refine?and possibly revise?as the utterance
goes on. You understand the utterance incremen-
tally. This is something that is out of reach for
most current dialogue systems, which process ut-
terances non-incrementally, en bloc (cf. (Skantze
and Schlangen, 2009), inter alia).
Enabling incremental processing in dialogue
systems poses many challenges (Allen et al,
2001; Schlangen and Skantze, 2009); we focus
here on the sub-problem of modelling incremental
understanding?a precondition for enabling truly
interactive behaviour. More specifically, we look
at statistical methods for learning mappings be-
tween (possibly partial) utterances and meaning
representations. We distinguish between two types
of understanding, which were sketched in the first
paragraph above: a) forming a partial understand-
ing, and b) predicting a complete understanding.
Recently, some results have been published on
b), predicting utterance meanings, (Sagae et al,
2009; Schlangen et al, 2009). We investigate
here how well this predictive approach works in
two other domains, and how a simple extension of
techniques (ensembles of slot-specific classifiers
vs. one frame-specific one) can improve perfor-
mance. To our knowledge, task a), computing par-
tial meanings, has so far only been tackled with
symbolic methods (e.g., (Milward and Cooper,
1994; Aist et al, 2006; Atterer and Schlangen,
2009));1 we present here some first results on ap-
proaching it with statistical models.
Plan of the paper: First, we discuss relevant pre-
vious work. We then define the task of incremental
natural language understanding and its two vari-
ants in more detail, also looking at how models
can be evaluated. Finally, we present and discuss
the results of our experiments, and close with a
conclusion and some discussion of future work.
2 Related Work
Statistical natural language understanding is an ac-
tive research area, and many sophisticated mod-
els for this task have recently been published, be
that generative models (e.g., in (He and Young,
2005)), which learn a joint distribution over in-
1We explicitly refer to computation of incremental inter-
pretations here; there is of course a large body of work on
statistical incremental parsing (e.g., (Stolcke, 1995; Roark,
2001)).
9
(Mairesse et al, 2009) 94.50
(He and Young, 2005) 90.3
(Zettlemoyer and Collins, 2007) 95.9
(Meza et al, 2008) 91.56
Table 1: Recent published f-scores for non-
incremental statistical NLU, on the ATIS corpus
put, output and possibly hidden variables; or, more
recently, discriminative models (e.g., (Mairesse et
al., 2009)) that directly learn a mapping between
input and output. Much of this work uses the ATIS
corpus (Dahl et al, 1994) as data and hence is di-
rectly comparable. In Table 1, we list the results
achieved by this work; we will later situate our re-
sults relative to this.
That work, however, only looks at mappings be-
tween complete utterances and semantic represen-
tations, whereas we are interested in the process of
mapping semantic representations to successively
larger utterance fragments. More closely related
then is (Sagae et al, 2009; DeVault et al, 2009),
where a maximum entropy model is trained for
mapping utterance fragments to semantic frames.
(Sagae et al, 2009) make the observation that of-
ten the quality of the prediction does not increase
anymore towards the end of the utterance; that is,
the meaning of the utterance can be predicted be-
fore it is complete.
In (Schlangen et al, 2009), we presented a
model that predicts incrementally a specific as-
pect of the meaning of a certain type of utterance,
namely the intended referent of a referring expres-
sion; the similarity here is that the output is of the
same type regardless of whether the input utter-
ance is complete or not.
(DeVault et al, 2009) discuss how such ?mind
reading? can be used interactionally in a dialogue
system, e.g. for completing the user?s utterance
as an indication of the system?s grounding state.
While these are interesting uses, the approach is
somewhat limited by the fact that it is incremental
only on the input side, while the output does not
reflect how ?complete? (or not) the input is. We
will compare this kind of incremental processing
in the next section with one where the output is
incremental as well, and we will then present re-
sults from our own experiments with both kinds of
incrementality in statistical NLU.
3 Task, Evaluation, and Data Sets
3.1 The Task
We have said that the task of incremental natural
language understanding consists in the assignment
of semantic representations to progressively more
complete prefixes of utterances. This description
can be specified along several aspects, and this
yields different versions of the task, appropriate
for different uses. One question is what the as-
signed representations are, the other is what ex-
actly they are assigned to. We investigate these
questions here abstractly, before we discuss the in-
stantiations in the next sections.
Let?s start by looking at the types of representa-
tions that are typically assigned to full utterances.
A type often used in dialogue systems is the frame,
an attribute value matrix. (The attributes are here
typically called slots.) These frames are normally
typed, that is, there are restrictions on which slots
can (and must) occur together in one frame. The
frames are normally assigned to the utterance as a
whole and not to individual words.
In an incremental setting, where the input
potentially consists of an incomplete utterance,
choosing this type of representation and style of
assignment turns the task into one of prediction of
the utterance meaning. What we want our model
to deliver is a guess of what the meaning of the ut-
terance is going to be, even if we have only seen
a prefix of the utterance so far; we will call this
?whole-frame output? below.2
Another popular representation of semantics in
applied systems uses semantic tags, i.e., markers
of semantic role that are attached to individual
parts of the utterance. Such a style of assignment
is inherently ?more incremental?, as it provides a
way to assign meanings that represent only what
has indeed been said so far, and does not make as-
sumptions about what will be said. The semantic
representation of the prefix simply contains all and
only the tags assigned to the words in the prefix;
this will be called ?aligned output? below. To our
knowledge, the potential of this type of represen-
tation (and the models that create them) for incre-
mental processing has not yet been explored; we
present our first results below.
Finally, there is a hybrid form of representation
and assignment. If we allow the output frames to
?grow? as more input comes in (hence possibly vi-
olating the typing of the frames as they are ex-
pected for full utterances), we get a form of rep-
resentation with a notion of ?partial semantics? (as
2In (Schlangen and Skantze, 2009), this type of incremen-
tal processing is called ?input incremental?, as only the input
is incrementally enriched, while the output is always of the
same type (but may increase in quality).
10
only that is represented for which there is evidence
in what has already been seen), but without direct
association of parts of the representation and parts
of the utterance or utterance prefix.
3.2 Evaluation
Whole-Frame Output A straightforward met-
ric is Correctness, which can take the values 1
(output is exactly as expected) or 0 (output is not
exactly as expected). Processing a test corpus in
this way, we get one number for each utterance
prefix, and, averaging this number, one measure-
ment for the whole corpus.
This can give us a first indication of the gen-
eral quality of the model, but because it weighs
the results for prefixes of all lengths equally, it
cannot tell us much about how well the incremen-
tal processing worked. In actual applications, we
presumably do not expect the model to be correct
from the very first word on, but do expect it to get
better the longer the available utterance prefix be-
comes. To capture this, we define two more met-
rics: first occurrence (FO), as the position (relative
to the eventual length of the full utterance) where
the response was correct first; and final decision
(FD) as the position from which on the response
stayed correct (which consequently can only be
measured if indeed the response stays correct).3
The difference between FO and FD then tells us
something about the stability of hypotheses of the
model.
In some applications, we may indeed only be
able to do further processing with fully correct?
or at least correctly typed?frames; in which case
correctness and FO/FD on frames are appropriate
metrics. However, sometimes even frames that are
only partially correct can be of use, for example if
specific system reactions can be tied to individual
slots. To give us more insight about the quality of a
model in such cases, we need a metric that is finer-
grained than binary correctness. Following (Sagae
et al, 2009), we can conceptualise our task as one
of retrieval of slot/value pairs, and use precision
and recall (and, as their combination, f-score) as
metrics. As we will see, it will be informative to
plot the development of this score over the course
of processing the utterance.
For these kinds of evaluations, we need as a
gold standard only one annotation per utterance,
3These metrics of course can only be computed post-hoc,
as during processing we do not know how long the utterance
is going to be.
namely the final frame.
Aligned Output As sequence alignments have
more structure?there is a linear order between the
tags, and there is exactly one tag per input token?
correctness is a more fine-grained, and hence more
informative, metric here; we define it as the pro-
portion of tags that are correct in a sequence. We
can also use precision and recall here, looking at
each position in the sequence individually: Has
the tag been recalled (true positive), or has some-
thing else been predicted instead (false negative,
and false positive)? Lastly, we can also recon-
struct frames from the tag sequences, where se-
quences of the same tag are interpreted as seg-
menting off the slot value. (And hence, what was
several points for being right or wrong, one for
each tag, becomes one, being either the correct
slot value or not. We will discuss these differences
when we show evaluations of aligned output.)
For this type of evaluation, we need gold-
standard information of the same kind, that is, we
need aligned tag sequences. This information is
potentially more costly to create than the one fi-
nal semantic representation needed for the whole-
frame setting.
Hybrid Output As we will see below, the hy-
brid form of output (?growing? frames) is pro-
duced by ensembles of local classifiers, with one
classifier for each possible slot. How this output
can be evaluated depends on what type of informa-
tion is available. If we only have the final frame,
we can calculate f-score (in the hope that preci-
sion will be better than for the whole-frame clas-
sifier, as such a classifier ensemble can focus on
predicting slots/value pairs for which there is di-
rect evidence); if we do have sequence informa-
tion, we can convert it to growing frames and eval-
uate against that.
3.3 The Data Sets
ATIS As our first dataset, we use the ATIS air
travel information data (Dahl et al, 1994), as pre-
processed by (Meza et al, 2008) and (He and
Young, 2005). That is, we have available for each
utterance a semantic frame as in (1), and also a
tag sequence that aligns semantic concepts (same
as the slot names) and words. One feature to note
here about the ATIS representations is that the slot
values / semantic atoms are just the words in the
utterance. That is, the word itself is its own se-
mantic representation, and no additional abstrac-
11
tion is performed. In this domain, this is likely un-
problematic, as there aren?t many different ways
(that are to be expected in this domain) to refer to
a given city or a day of the week, for example.
(1) ?What flights are there arriving in Chicago after
11pm??
?
?
?
?
GOAL = FLIGHT
TOLOC.CITY NAME = Chicago
ARRIVE TIME.TIME RELATIVE = after
ARRIVE TIME.TIME = 11pm
?
?
?
?
In our experiments, we use the ATIS training
set which contains 4481 utterances, between 1
and 46 words in length (average 11.46; sd 4.34).
The vocabulary consists of 897 distinct words.
There are 3159 distinct frames, 2594 (or 58% of
all frames) of which occur only once. Which of
the 96 possible slots occur in a given frame is
distributed very unevenly; there are some very
frequent slots (like FROMLOC.CITYNAME
or DEPART DATE.DAY NAME) and
some very rare or even unique ones (e.g.,
ARRIVE DATE.TODAY RELATIVE, or
TIME ZONE).
Pentomino The second corpus we use is of ut-
terances in a domain that we have used in much
previous work (e.g., (Schlangen et al, 2009;
Atterer and Schlangen, 2009; Ferna?ndez and
Schlangen, 2007)), namely, instructions for ma-
nipulating puzzle pieces to form shapes. The par-
ticular version we use here was collected in a
Wizard-of-Oz study, where the goal was to instruct
the computer to pick up, delete, rotate or mirror
puzzle tiles on a rectangular board, and drop them
on another one. The user utterances were anno-
tated with semantic frames and also aligned with
tag sequences. We use here a frame representation
where the slot value is a part of the utterance (as
in ATIS), an example is shown in (2). (The cor-
pus is in German; the example is translated here
for presentation.) We show the full frame here,
with all possible slots; unused slots are filled with
?empty?. Note that this representation is some-
what less directly usable in this domain than for
ATIS; in a practical system, we?d need some fur-
ther module (rule-based or statistical) that maps
such partial strings to their denotations, as this
mapping is less obvious here than in the travel do-
main.
(2) ?Pick up the W-shaped piece in the upper right cor-
ner?
?
?
?
?
?
?
?
action = ?pick up?
tile = ?the W-shaped piece
in the upper right corner?
field = empty
rotpar = empty
mirpar = empty
?
?
?
?
?
?
?
The corpus contains 1563 utterances, average
length 5.42 words (sd 2.35), with a vocabulary of
222 distinct words. There are 964 distinct frames,
with 775 unique frames.
In both datasets we use transcribed utterances
and not ASR output, and hence our results present
an upper bound on real-world performance.
4 Local Models: Support Vector Machines
In this section we report the results of our exper-
iments with local classifiers, i.e. models which,
given an input, predict one out of a set of classes as
an answer. Such models are very naturally suited
to the prediction task, where the semantics of the
full utterance is treated as its class, which is to be
predicted on the basis of what possibly is only a
prefix of that utterance. We will also look at a
simple modification, however, which enables such
models to do something that is closer to the task of
computing partial meanings.
4.1 Experimental Setup
For our experiments with local models, we used
the implementations of support vector machines
provided by the WEKA toolkit (Witten and Frank,
2005); as baseline we use a simple majority class
predictor.4
We used the standard WEKA tools to convert
the utterance strings into word vectors. Training
was always done with the full utterance, but test-
ing was done on prefixes of utterances; i.e., a sen-
tence with 5 words would be one instance in train-
ing, but in a testing fold it would contribute 5 in-
stances, one with one word, one with two words,
and so on.5 Because of this special way of testing
the classifiers, and also because of the modifica-
4We tried other classifiers (C4.5, logistic regression, naive
Bayes) as well, and found comparable performance on a de-
velopment set. However, because of the high time costs
(some models needed > 40 hours for training and testing on
modern multi-CPU servers) we do not systematically com-
pare performance and instead focus on SVMs. In any case,
our interest here is not in comparing classification algorithms,
but rather in exploring approaches to the novel problem of
statistical incremental NLU.
5On a development set, we tried training on utterance pre-
fixes, but that degraded performance, presumably due to in-
crease in ambiguous training instances (same beginnings of
what ultimately are very different utterances).
12
tions described below, we had to provide our own
methods for cross-validation and evaluation. For
the larger ATIS data set, we used 10 folds in cross
validation, and for the Pentomino dataset 20 folds.
4.2 Results
To situate our results, we begin by looking at
the performance of the models that predict a full
frame, when given a full utterance; this is the
normal, ?non-incremental? statistical NLU task.6
(3)
classf. metric ATIS Pento
maj correctness 1.07 1.79
maj f-score 35.98 16.15
SVM correctness 16.21 38.77
SVM f-score 68.17 63.23
We see that the results for ATIS are considerably
lower than the state of the art in statistical NLU
(Table 1). This need not concern us too much
here, as we are mostly interested in the dynam-
ics of the incremental process, but it indicates that
there is room for improvement with more sophisti-
cated models and feature design. (We will discuss
an example of an improved model shortly.) We
also see a difference between the corpora reflected
in these results: being exactly right (good correct-
ness) seems to be harder on the ATIS corpus, while
being somewhat right (good f-score) seems to be
harder on the pento corpus; this is probably due to
the different sizes of the search space of possible
frame types (large for ATIS, small for pento).
What we are really interested in, however, is the
performance when given only a prefix of an ut-
terance, and how this develops over the course of
processing successively larger prefixes. We can
investigate this with Figure 1. First, look at the
solid lines. The black line shows the average f-
score at various prefix lengths (in 10% steps) for
the ATIS data, the grey line for the pento corpus.
We see that both lines show a relatively steady in-
cline, meaning that the f-score continues to im-
prove when more of the utterance is seen. This is
interesting to note, as both (DeVault et al, 2009)
and (Atterer et al, 2009) found that in their data,
all that is to be known can often be found some-
what before the end of the utterance. That this
does not work so well here is most likely due to
the difference in domain and the resulting utter-
ances. Utterances giving details about travel plans
6The results for ATIS are based on half of the overall
ATIS data, as cross-validating the model on all data took pro-
hibitively long, presumably due to the large number of unique
frames / classes.
2 4 6 8 10
0.0
0.2
0.4
0.6
0.8
percentiles into utterance
f?sc
ore
l l
l
l
l
l
l
l
l l
l all utterancesshort utterancesnormal utteranceslong utterances
l
l
l
l
l
l
l
l
l l
Figure 1: F-Score by Length of Prefix
are likely to present many important details, and
some of them late into the utterance; cf. (1) above.
The data from (DeVault et al, 2009) seems to be
more conversational in nature, and, more impor-
tantly, presumable the expressible goals are less
closely related to each other and hence can be read
off of shorter prefixes.
As presented so far, the results are not very
helpful for practical applications of incremental
NLU. One thing one would like to know in a prac-
tical situation is how much the prediction of the
model can be trusted for a given partial utterance.
We would like to read this off graphs like those
in the Figure?but of course, normally we cannot
know what percentage of an utterance we have al-
ready seen! Can we trust this averaged curve if we
do not know what length the incoming utterance
will have?
To investigate this question, we have binned the
test utterances into three classes, according to their
length: ?normal?, for utterances that are of aver-
age length? half a standard deviation, and ?short?
for all that are shorter, and ?long? for all that are
longer. The f-score curves for these classes are
shown with the non-solid lines in Figure 1. We
see that for ATIS there is not much variation com-
pared to averaging over all utterances, and more-
over, that the ?normal? class very closely follows
the general curve. On the pento data, the model
seems to be comparably better for short utterances.
In a practical application, one could go with
the assumption that the incoming utterance is go-
ing to be of normal length, and use the ?normal?
13
curve for guidance; or one could devise an ad-
ditional classifier that predicts the length-class of
the incoming utterance, or more generally predicts
whether a frame can already be trusted (DeVault et
al., 2009). We leave this for future work.
As we have seen, the models that treat the se-
mantic frame simply as a class label do not fare
particularly well. This is perhaps not that surpris-
ing; as discussed above, in our corpora there aren?t
that many utterances with exact the same frame.
Perhaps it would help to break up the task, and
train individual classifiers for each slot?7 This
idea can be illustrated with (2) above. There we al-
ready included ?unused? slots in the frame; if we
now train classifiers for each slot, allowing them
to predict ?empty? in cases where a slot is unused,
we can in theory reconstruct any frame from the
ensemble of classifiers. To cover the pento data,
the ensemble is small (there are 5 frames); it is
considerably larger for ATIS, where there are so
many distinct slots.
Again we begin by looking at the performance
for full utterances (i.e., at 100% utterance length),
but this time for constructing the frame from the
reply of the classifier ensemble:
(4)
classf. metric ATIS Pento
maj correctness 0.16 0
maj f-score 33.18 20.24
SVM correctness 52.69 50.48
SVM f-score 86.79 73.15
We see that this approach leads to an impressive
improvement on the ATIS data (83.64 f-score in-
stead of 68.17), whereas the improvement on the
pento data is more modest (73.15 / 63.23).
Figure 2 shows the incremental development of
the f-scores for the reconstructed frame. We see
a similar shape in the curves; again a relatively
steady incline for ATIS and a more dramatic shape
for pento, and again some differences in behaviour
for the different length classes of utterances. How-
ever, by just looking at the reconstructed frame,
we are ignoring valuable information that the slot-
classifier approach gives us. In some applications,
we may already be able to do something useful
with partial information; e.g., in the ATIS domain,
we could look up an airport as soon as a FROM-
LOC becomes known. Hence, we?d want more
fine-grained information, not just about when we
can trust the whole frame, but rather about when
7A comparable approach is used for the non-incremental
case for example by (Mairesse et al, 2009).
2 4 6 8 10
0.0
0.2
0.4
0.6
0.8
percentiles into utterance
f?sc
ore
l l
l
l
l
l
l
l
l
l
l all utterancesshort utterancesnormal utteranceslong utterancesl
l
l
l l
l
l
l
l l
Figure 2: F-Score by Length of Prefix; Slot Clas-
sifiers
we can trust individual predicted slot values. (And
so we move from the prediction task to the partial
representations task.)
To explore this, we look at First Occurrence and
Final Decision for some selected slots in Table 2.
For some slots, the first occurrence (FO) of the
correct value comes fairly early into the utterance
(e.g., for the name of the airline it?s at ca. 60%,
for the departure city at ca. 63%, both with rela-
tively high standard deviation, though) while oth-
ers are found the first time rather late (goal city
at 81%). This conforms well with intuitions about
how such information would be presented in an ut-
terance (?I?d like to fly on Lufthansa from Berlin
to Tokyo?).
We also see that the predictions are fairly stable:
the number of cases where the slot value stays cor-
rect until the end is almost the same as that where
it is correct at least once (FD applicable vs. FO
apl), and the average position is almost the same.
In other words, the classifiers seem to go fairly
reliably from ?empty? (no value) to the correct
value, and then seem to stay there. The overhead
of unnecessary edits (EO) is fairly low for all slots
shown in the table. (Ideally, EO is 0, meaning that
there is no change except the one from ?empty? to
correct value.) All this is good news, as it means
that a later module in a dialogue system can often
begin to work with the partial results as soon as
a slot-classifier makes a non-empty prediction. In
an actual application, how trustworthy the individ-
ual classifiers are would then be read off statistics
14
slot name avg FO stdDev apl avg FD stdDev apl avg EO stdDev apl
AIRLINE NAME 0.5914 0.2690 506 0.5909 0.2698 501 0.5180 0.5843 527
DEPART TIME.PERIOD OF DAY 0.7878 0.2506 530 0.7992 0.2476 507 0.2055 0.5558 579
FLIGHT DAYS 0.4279 0.2660 37 0.4279 0.2660 37 0.0000 0.0000 37
FROMLOC.CITY NAME 0.6345 0.1692 3633 0.6368 0.1692 3554 0.1044 0.4526 3718
ROUND TRIP 0.5366 0.2140 287 0.5366 0.2140 287 0.0104 0.1015 289
TOLOC.CITY NAME 0.8149 0.1860 3462 0.8162 0.1856 3441 0.2348 0.5723 3628
frames 0.9745 0.0811 2382 0.9765 0.0773 2361 0.7963 1.1936 4481
Table 2: FO/FD/EO for some selected slots; averaged over utterances of all lengths
like these, given a corpus from the domain.
To conclude this section, we have shown that
classifiers that predict a complete frame based on
utterance prefixes have a somewhat hard task here
(harder, it seems, than in the corpus used in (Sagae
et al, 2009), where they achieve an f-score of 87
on transcribed utterances), and the prediction re-
sults improve steadily throughout the whole utter-
ance, rather than reaching their best value before
its end. When the task is ?spread? over several
classifiers, with each one responsible for only one
slot, performance improves drastically, and also,
the results become much more ?incremental?. We
now turn to models that by design are more incre-
mental in this sense.
5 Sequential Models: Conditional
Random Fields
5.1 Experimental Setup
We use Conditional Random Fields (Lafferty et
al., 2001) as our representative of the class of se-
quential models, as implemented in CRF++.8 We
use a simple template file that creates features
based on a left context of three words.
Even though sequential models have the poten-
tial to be truly incremental (in the sense that they
could produce a new output when fed a new in-
crement, rather than needing to process the whole
prefix again), CRF++ is targeted at tagging appli-
cations, and expects full sequences. We hence test
in the same way as the SVMs from the previous
section, by computing a new tag sequence for each
prefix. Training again is done only on full utter-
ances / tag sequences.
We compare the CRF results against two base-
lines. The simplest consists of just always choos-
ing the most frequent tag, which is ?O? (for other,
marking material that does not contribute directly
to the relevant meaning of the utterance, such
as ?please? in ?I?d like to return on Monday,
please.?). The other baseline tags each word with
8http://crfpp.sourceforge.net/
2 4 6 8 10
0.5
0.6
0.7
0.8
0.9
1.0
percentiles into utterance
f?sc
ore
l l l l l l l l l l
l all utterancesshort utterancesnormal utteranceslong utterances
l l l l l l l l l
Figure 3: F-Score by Length of Prefix
ATIS Corr. Tag F-Score Frame F-Score
CRF 93.38 82.56 76.10
Maj 85.14 60.86 48.08
O 63.43 00.31 00.31
Pento Corr. Tag F-Score Frame F-Score
CRF 89.19 88.95 76.94
Maj 80.20 80.13 65.94
O 5.90 0.19 0.19
Table 3: Results of CRF models
its most frequent training data tag.
5.2 Results
We again begin by looking at the limiting case, the
results for full utterances (i.e., at the 100%mark).
Table 3 show three sets of results for each cor-
pus. Correctness looks at the proportion of tags
in a sequence that were correct. This measure is
driven up by correct recognition of the dummy
tag ?o?; as we can see, this is quite frequently
correct in ATIS, which drives up the ?always use
O?-baseline. Tag F-Score values the important
tags higher; we see here, though, that the majority
baseline (each word tagged with its most frequent
tag) is surprisingly good. It is solidly beaten for
the ATIS data, though. On the pento data, with
its much smaller tagset (5 as opposed to 95), this
baseline comes very high, but still the learner is
able to get some improvement. The last metric
evaluates reconstructed frames. It is stricter, be-
cause it offers less potential to be right (a sequence
of the same tag will be translated into one slot
value, turning several opportunities to be right into
15
only one).
The incremental dynamics looks quite different
here. Since the task is not one of prediction, we
do not expect to get better with more information;
rather, we start at an optimal point (when nothing
is said, nothing can be wrong), and hope that we
do not amass too many errors along the way. Fig-
ure 3 confirms this, showing that the classifier is
better able to keep the quality for the pento data
than for the ATIS data. Also, there is not much
variation depending on the length of the utterance.
6 Conclusions
We have shown how sequential and local statistical
models can be used for two variants of the incre-
mental NLU task: prediction, based on incomplete
information, and assignment of partial representa-
tions to partial input. We have shown that break-
ing up the prediction task by using an ensemble
of classifiers improves performance, and creates a
hybrid task that sits between prediction and incre-
mental interpretation.
While the objective quality as measured by our
metrics is quite good, what remains to be shown is
how such models can be integrated into a dialogue
system, and how what they offer can be turned into
improvements on interactivity. This is what we are
turning to next.
Acknowledgements Funded by ENP grant from DFG.
References
G.S. Aist, J. Allen, E. Campana, L. Galescu, C.A.
Gomez Gallo, S. Stoness, M. Swift, and M Tanenhaus.
2006. Software architectures for incremental understand-
ing of human speech. In Proceedings of the Interna-
tional Conference on Spoken Language Processing (IC-
SLP), Pittsburgh, PA, USA, September.
James Allen, George Ferguson, and Amanda Stent. 2001.
An architecture for more realistic conversational systems.
In Proceedings of the conference on intelligent user inter-
faces, Santa Fe, USA, June.
Michaela Atterer and David Schlangen. 2009. RUBISC ?
a robust unification-based incremental semantic chunker.
In Proceedings of the 2nd International Workshop on Se-
mantic Representation of Spoken Language (SRSL 2009),
Athens, Greece, March.
Michaela Atterer, Timo Baumann, and David Schlangen.
2009. No sooner said than done? testing incrementality of
semantic interpretations of spontaneous speech. In Pro-
ceedings of Interspeech 2009, Brighton, UK, September.
Deborah A. Dahl, Madeleine Bates, Michael Brown, William
Fisher, Kate Hunicke-Smith, David Pallett, Christine Pao,
Alexander Rudnicky, and Elizabeth Shriberg. 1994. Ex-
panding the scope of the atis task: the atis-3 corpus. In
Proceedings of the workshop on Human Language Tech-
nology, pages 43?48, Plainsboro, NJ, USA.
David DeVault, Kenji Sagae, and David Traum. 2009. Can
i finish? learning when to respond to incremental inter-
pretation results in interactive dialogue. In Proceedings
of the 10th Annual SIGDIAL Meeting on Discourse and
Dialogue (SIGDIAL?09), London, UK, September.
Raquel Ferna?ndez and David Schlangen. 2007. Referring
under restricted interactivity conditions. In Simon Keizer,
Harry Bunt, and Tim Paek, editors, Proceedings of the
8th SIGdial Workshop on Discourse and Dialogue, pages
136?139, Antwerp, Belgium, September.
Yulan He and Steve Young. 2005. Semantic processing us-
ing the hidden vector state model. Computer Speech and
Language, 19(1):85?106.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and
labeling sequence data. In Proc. of ICML, pages 282?289.
F. Mairesse, M. Gasic, F. Jurcicek, S. Keizer, B. Thomson,
K. Yu, and S. Young. 2009. Spoken language understand-
ing from unaligned data using discriminative classification
models. In Proceedings of the 2009 IEEE International
Conference on Acoustics, Speech and Signal Processing,
Taipei, Taiwan, April.
Ivan Meza, Sebastian Riedel, and Oliver Lemon. 2008. Ac-
curate statistical spoken language understanding from lim-
ited development resources. In In Proceedings of ICASSP.
David Milward and Robin Cooper. 1994. Incremental in-
terpretation: Applications, theory, and relationships to dy-
namic semantics. In Proceedings of COLING 1994, pages
748?754, Kyoto, Japan, August.
Brian Roark. 2001. Robust Probabilistic Predictive Syntac-
tic Processing: Motivations, Models, and Applications.
Ph.D. thesis, Department of Cognitive and Linguistic Sci-
ences, Brown University.
Kenji Sagae, Gwen Christian, David DeVault, and David
Traum. 2009. Towards natural language understand-
ing of partial speech recognition results in dialogue sys-
tems. In Short paper proceedings of the North Ameri-
can chapter of the Association for Computational Linguis-
tics - Human Language Technologies conference (NAACL-
HLT?09), Boulder, Colorado, USA, June.
David Schlangen and Gabriel Skantze. 2009. A general, ab-
stract model of incremental dialogue processing. In Pro-
ceedings of the 12th Conference of the European Chapter
of the Association for Computational Linguistics (EACL
2009), pages 710?718, Athens, Greece, March.
David Schlangen, Timo Baumann, and Michaela Atterer.
2009. Incremental reference resolution: The task, met-
rics for evaluation, and a bayesian filtering model that is
sensitive to disfluencies. In Proceedings of SIGdial 2009,
the 10th Annual SIGDIAL Meeting on Discourse and Di-
alogue, London, UK, September.
Gabriel Skantze and David Schlangen. 2009. Incremental
dialogue processing in a micro-domain. In Proceedings
of the 12th Conference of the European Chapter of the
Association for Computational Linguistics (EACL 2009),
pages 745?753, Athens, Greece, March.
Andreas Stolcke. 1995. An efficient probabilistic context-
free parsing algorithm that computes prefix probabilities.
Computational Linguistics, 21(2):165?201.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Practi-
cal machine learning tools and techniques. Morgan Kauf-
mann, San Francisco, USA, 2nd edition.
Luke S. Zettlemoyer and Michael Collins. 2007. Online
learning of relaxed ccg grammars for parsing to logical
form. In Proceedings of EMNLP-CoNLL.
16
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 51?54,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Middleware for Incremental Processing in Conversational Agents
David Schlangen?, Timo Baumann?, Hendrik Buschmeier?, Okko Bu??
Stefan Kopp?, Gabriel Skantze?, Ramin Yaghoubzadeh?
?University of Potsdam ?Bielefeld University ?KTH, Stockholm
Germany Germany Sweden
david.schlangen@uni-potsdam.de
Abstract
We describe work done at three sites on
designing conversational agents capable of
incremental processing. We focus on the
?middleware? layer in these systems, which
takes care of passing around and maintain-
ing incremental information between the
modules of such agents. All implementa-
tions are based on the abstract model of
incremental dialogue processing proposed
by Schlangen and Skantze (2009), and the
paper shows what different instantiations
of the model can look like given specific
requirements and application areas.
1 Introduction
Schlangen and Skantze (2009) recently proposed
an abstract model of incremental dialogue process-
ing. While this model introduces useful concepts
(briefly reviewed in the next section), it does not
talk about how to actually implement such sys-
tems. We report here work done at three different
sites on setting up conversational agents capable
of incremental processing, inspired by the abstract
model. More specifically, we discuss what may
be called the ?middleware? layer in such systems,
which takes care of passing around and maintaining
incremental information between the modules of
such agents. The three approaches illustrate a range
of choices available in the implementation of such
a middle layer. We will make our software avail-
able as development kits in the hope of fostering
further research on incremental systems.1
In the next section, we briefly review the abstract
model. We then describe the implementations cre-
ated at Uni Bielefeld (BF), KTH Stockholm (KTH)
and Uni Potsdam (UP). We close with a brief dis-
cussion of similarities and differences, and an out-
look on further work.
1Links to the three packages described here can be found
at http://purl.org/net/Middlewares-SIGdial2010.
2 The IU-Model of Incremental Processing
Schlangen and Skantze (2009) model incremental
systems as consisting of a network of processing
modules. Each module has a left buffer, a proces-
sor, and a right buffer, where the normal mode of
processing is to take input from the left buffer, pro-
cess it, and provide output in the right buffer, from
where it goes to the next module?s left buffer. (Top-
down, expectation-based processing would work
in the opposite direction.) Modules exchange incre-
mental units (IUs), which are the smallest ?chunks?
of information that can trigger connected modules
into action. IUs typically are part of larger units;
e.g., individual words as parts of an utterance, or
frame elements as part of the representation of an
utterance meaning. This relation of being part of
the same larger unit is recorded through same level
links; the information that was used in creating a
given IU is linked to it via grounded in links. Mod-
ules have to be able to react to three basic situa-
tions: that IUs are added to a buffer, which triggers
processing; that IUs that were erroneously hypothe-
sised by an earlier module are revoked, which may
trigger a revision of a module?s own output; and
that modules signal that they commit to an IU, that
is, won?t revoke it anymore (or, respectively, expect
it to not be revoked anymore).
Implementations of this model then have to re-
alise the actual details of this information flow, and
must make available the basic module operations.
3 Sociable Agents Architecture
BF?s implementation is based on the ?D-Bus? mes-
sage bus system (Pennington et al, 2007), which
is used for remote procedure calls and the bi-
directional synchronisation of IUs, either locally
between processes or over the network. The bus sys-
tem provides proxies, which make the interface of
a local object accessible remotely without copying
data, thus ensuring that any access is guaranteed to
yield up-to-date information. D-Bus bindings exist
for most major programming languages, allowing
51
for interoperability across various systems.
IUs exist as objects implementing a D-Bus in-
terface, and are made available to other modules
by publishing them on the bus. Modules are ob-
jects comprising a main thread and right and left
buffers for holding own IUs and foreign IU proxies,
respectively. Modules can co-exist in one process
as threads or occupy one process each?even dis-
tributed across a network.
A dedicated Relay D-Bus object on the network
is responsible for module administration and up-
date notifications. At connection time, modules
register with the relay, providing a list of IU cat-
egories and/or module names they are interested
in. Category interests create loose functional links
while module interests produce more static ones.
Whenever a module chooses to publish informa-
tion, it places a new IU in its right buffer, while
removal of an IU from the right buffer corresponds
to retraction. The relay is notified of such changes
and in turn invokes a notification callback in all
interested modules synchronising their left buffers
by immediately and transparently creating or re-
moving proxies of those IUs.
IUs consist of the fields described in the abstract
model, and an additional category field which the
relay can use to identify the set of interested mod-
ules to notify. They furthermore feature an optional
custom lifetime, on the expiration of which they
are automatically retracted.
Incremental changes to IUs are simply realised
by changing their attributes: regardless of their lo-
cation in either a right or left buffer, the same setter
functions apply (e.g., set payload). These generate
relay-transported update messages which commu-
nicate the ID of the changed IU. Received update
messages concerning self-owned and remotely-
owned objects are discerned automatically to allow
for special treatment of own IUs. The complete
process is illustrated in Figure 1.
Current state and discussion. Our support for
bi-directional IU editing is an extension to the con-
cepts of the general model. It allows higher-level
modules with a better knowledge of context to re-
vise uncertain information offered by lower levels.
Information can flow both ways, bottom-up and
top-down, thus allowing for diagnostic and causal
networks linked through category interests.
Coming from the field of embodied conversa-
tional agents, and being especially interested in
modelling human-like communication, for exam-
A B
C
IU
IU proxy
Write access
Relay
Data access
Update notification
RBuf LBuf
Interest sets
Figure 1: Data access on the IU proxies is transparently dele-
gated over the D-Bus; module A has published an IU. B and C
are registered in the corresponding interest set, thus receiving
a proxy of this IU in their left buffer. When B changes the IU,
A and C receive update notifications.
ple for on-line production of listener backchannel
feedback, we constantly have to take incremen-
tally changing uncertain input into account. Using
the presented framework consistently as a network
communication layer, we are currently modelling
an entire cognitive architecture for virtual agents,
based on the principle of incremental processing.
The decision for D-Bus as the transportation
layer has enabled us to quickly develop ver-
sions for Python, C++ and Java, and produced
straightforward-to-use libraries for the creation of
IU-exchanging modules: the simplest fully-fledged
module might only consist of a periodically in-
voked main loop callback function and any subset
of the four handlers for IU events (added, removed,
updated, committed).
4 Inpro Toolkit
The InproTK developed at UP offers flexibility on
how tightly or loosely modules are coupled in a
system. It provides mechanisms for sending IU up-
dates between processes via a messaging protocol
(we have used OAA [Cheyer and Martin, 2001], but
other communication layers could also be used) as
well as for using shared memory within one (Java)
process. InproTK follows an event-based model,
where modules create events, for which other mod-
ules can register as Listeners. Module networks are
configured via a system configuration file which
specifies which modules listen to which.
Modules push information to their right, hence
the interface for inter-module communication is
called PushBuffer. (At the moment, InproTK only
implements left-to-right IU flow.) The PushBuffer
interface defines a hypothesis-change method
which a module will call for all its listening mod-
ules. A hypothesis change is (redundantly) charac-
terised by passing both the complete current buffer
state (a list of IUs) as well as the delta between
52
the previous and the current state, leaving listen-
ing modules a choice of how to implement their
internal update.
Modules can be fully event-driven, only trig-
gered into action by being notified of a hypothesis
change, or they can run persistently, in order to cre-
ate endogenous events like time-outs. Event-driven
modules can run concurrently in separate threads or
can be called sequentially by a push buffer (which
may seem to run counter the spirit of incremental
processing, but can be advantageous for very quick
computations for which the overhead of creating
threads should be avoided).
IUs are typed objects, where the base class IU
specifies the links (same-level, grounded-in) that
allow to create the IU network and handles the
assignment of unique IDs. The payload and addi-
tional properties of an IU are specified for the IU?s
type. A design principle here is to make all relevant
information available, while avoiding replication.
For instance, an IU holding a bit of semantic rep-
resentation can query which interval of input data
it is based on, where this information is retrieved
from the appropriate IUs by automatically follow-
ing the grounded-in links. IU networks ground out
in BaseData, which contains user-side input such
as speech from the microphone, derived ASR fea-
ture vectors, camera feeds from a webcam, derived
gaze information, etc., in several streams that can
be accessed based on their timing information.
Besides IU communication as described in the
abstract model, the toolkit also provides a separate
communication track along which signals, which
are any kind of information that is not seen as incre-
mental hypotheses about a larger whole but as infor-
mation about a single current event, can be passed
between modules. This communication track also
follows the observer/listener model, where proces-
sors define interfaces that listeners can implement.
Finally, InproTK also comes with an extensive
set of monitoring and profiling modules which can
be linked into the module network at any point and
allow to stream data to disk or to visualise it online
through a viewing tool (ANON 2009), as well as
different ways to simulate input (e.g., typed or read
from a file) for bulk testing.
Current state and discussion. InproTK is cur-
rently used in our development of an incremental
multimodal conversational system. It is usable in its
current state, but still evolves. We have built and in-
tegrated modules for various tasks (post-processing
of ASR output, symbolic and statistical natural lan-
guage understanding [ANON 2009a,b,c]). The con-
figuration system and the availability of monitoring
and visualisation tools enables us to quickly test
different setups and compare different implementa-
tions of the same tasks.
5 Jindigo
Jindigo is a Java-based framework for implement-
ing and experimenting with incremental dialogue
systems currently being developed at KTH. In
Jindigo, all modules run as separate threads within
a single Java process (although the modules them-
selves may of course communicate with external
processes). Similarly to InproTK, IUs are mod-
elled as typed objects. The modules in the system
are also typed objects, but buffers are not. Instead,
a buffer can be regarded as a set of IUs that are
connected by (typed) same-level links. Since all
modules have access to the same memory space,
they can follow the same-level links to examine
(and possibly alter) the buffer. Update messages
between modules are relayed based on a system
specification that defines which types of update
messages from a specific module go where. Since
the modules run asynchronously, update messages
do not directly invoke methods in other modules,
but are put on the input queues of the receiving
modules. The update messages are then processed
by each module in their own thread.
Jindigo implements a model for updating buffers
that is slightly different than the two previous ap-
proaches. In this approach, IUs are connected by
predecessor links, which gives each IU (words,
widest spanning phrases from the parser, commu-
nicative acts, etc), a position in a (chronologically)
ordered stream. Positional information is reified by
super-imposing a network of position nodes over
the IU network, with the IUs being associated with
edges in that network. These positional nodes then
give us names for certain update stages, and so
revisions can be efficiently encoded by reference
to these nodes. An example can make this clearer.
Figure 2 shows five update steps in the right buffer
of an incremental ASR module. By reference to po-
sitional nodes, we can communicate easily (a) what
the newest committed IU is (indicated in the figure
as a shaded node) and (b) what the newest non-
revoked or active IU is (i.e., the ?right edge? (RE);
indicated in the figure as a node with a dashed line).
So, the change between the state at time t1 and t2
is signalled by RE taking on a different value. This
53
Figure 2: The right buffer of an ASR module, and update
messages at different time-steps.
value (w3) has not been seen before, and so the
consuming module can infer that the network has
been extended; it can find out which IUs have been
added by going back from the new RE to the last
previously seen position (in this case, w2). At t3, a
retraction of a hypothesis is signalled by a return to
a previous state, w2. All consuming modules have
to do now is to return to an internal state linked
to this previous input state. Commitment is repre-
sented similarly through a pointer to the rightmost
committed node; in the figure, that is for example
w5 at t5.
Since information about whether an IU has been
revoked or committed is not stored in the IU it-
self, all IUs can (if desirable) be defined as im-
mutable objects. This way, the pitfalls of having
asynchronous processes altering and accessing the
state of the IUs may be avoided (while, however,
more new IUs have to be created, as compared to
altering old ones). Note also that this model sup-
ports parallel hypotheses as well, in which case the
positional network would turn into a lattice.
The framework supports different types of up-
date messages and buffers. For example, a parser
may incrementally send NPs to a reference reso-
lution (RR) module that has access to a domain
model, in order to prune the chart. Thus, informa-
tion may go both left-to-right and right-to-left. In
the buffer between these modules, the order be-
tween the NPs that are to be annotated is not im-
portant and there is no point in revoking such IUs
(since they do not affect the RR module?s state).
Current state and discussion. Jindigo uses con-
cepts from (Skantze, 2007), but has been rebuilt
from ground up to support incrementality. A range
of modules for ASR, semantic interpretation, TTS,
monitoring, etc., have been implemented within
the framework, allowing us to do experiments
with complete systems interacting with users. We
are currently using the framework to implement a
model of incremental speech production.
6 Discussion
The three implementations of the abstract IU model
presented above show that concrete requirements
and application areas result in different design de-
cisions and focal points.
While BF?s approach is loosely coupled and han-
dles exchange of IUs via shared objects and a me-
diating module, KTH?s implementation is rather
closely coupled and publishes IUs through a single
buffer that lies in shared memory. UP?s approach
is somewhat in between: it abstracts away from the
transportation layer and enables message passing-
based communication as well as shared memory
transparently through one interface.
The differences in the underlying module com-
munication infrastructure affect the way incremen-
tal IU updates are handled in the systems. In BF?s
framework modules holding an IU in one of their
buffers just get notified when one of the IU?s fields
changed. Conversely, KTH?s IUs are immutable
and new information always results in new IUs
being published and a change to the graph repre-
sentation of the buffer?but this allows an efficient
coupling of module states and cheap revoke op-
erations. Again, UP?s implementation lies in the
middle. Here both the whole new state and the delta
between the old and new buffer is communicated,
which leads to flexibility in how consumers can be
implemented, but also potentially to some commu-
nication overhead.
In future work, we will explore if further gener-
alisations can be extracted from the different im-
plementations presented here. For now, we hope
that the reference architectures presented here can
already be an inspiration for further work on incre-
mental conversational systems.
References
Adam Cheyer and David Martin. 2001. The open
agent architecture. Journal of Autonomous Agents
and Multi-Agent Systems, 4(1):143?148, March.
H. Pennington, A. Carlsson, and A. Larsson. 2007.
D-Bus Specification Version 0.12. http://dbus.free-
desktop.org/doc/dbus-specification.html.
David Schlangen and Gabriel Skantze. 2009. A Gen-
eral, Abstract Model of Incremental Dialogue Pro-
cessing. In Proceedings of EACL 2009, Athens,
Greece.
Gabriel Skantze. 2007. Error Handling in Spoken Dia-
logue Systems. Ph.D. thesis, KTH, Stockholm, Swe-
den, November.
54
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 233?236,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Collaborating on Utterances with a Spoken Dialogue System
Using an ISU-based Approach to Incremental Dialogue Management
Okko Bu?, Timo Baumann, David Schlangen
Department of Linguistics
University of Potsdam, Germany
{okko|timo|das}@ling.uni-potsdam.de
Abstract
When dialogue systems, through the
use of incremental processing, are
not bounded anymore by strict, non-
overlapping turn-taking, a whole range of
additional interactional devices becomes
available. We explore the use of one such
device, trial intonation. We elaborate
our approach to dialogue management
in incremental systems, based on the
Information-State-Update approach, and
discuss an implementation in a micro-
domain that lends itself to the use of
immediate feedback, trial intonations and
expansions. In an overhearer evaluation,
the incremental system was judged as sig-
nificantly more human-like and reactive
than a non-incremental version.
1 Introduction
In human?human dialogue, most utterances have
only one speaker.1 However, the shape that an
utterance ultimately takes on is often determined
not just by the one speaker, but also by her ad-
dressees. A speaker intending to refer to some-
thing may start with a description, monitor while
they go on whether the description appears to be
understood sufficiently well, and if not, possibly
extend it, rather than finishing the utterance in the
form that was initially planned. This monitoring
within the utterance is sometimes even made very
explicit, as in the following example from (Clark,
1996):
(1) A: A man called Annegra? -
B: yeah, Allegra
A: Allegra, uh, replied and, uh, . . .
In this example, A makes use of what Sacks and
Schegloff (1979) called a try marker, a ?question-
ing upward intonational contour, followed by a
1Though by far not all; see (Clark, 1996; Purver et al,
2009; Poesio and Rieser, 2010).
brief pause?. As discussed by Clark (1996), this
device is an efficient solution to the problem posed
by uncertainty on the side of the speaker whether
a reference is going to be understood, as it checks
for understanding in situ, and lets the conversation
partners collaborate on the utterance that is in pro-
duction.
Spoken dialogue systems (SDS) typically can-
not achieve the close coupling between produc-
tion and interpretation that is needed for this to
work, as normally the smallest unit on which they
operate is the full utterance (or, more precisely,
the turn). (For a discussion see e.g. (Skantze and
Schlangen, 2009).) We present here an approach
to managing dialogue in an incremental SDS that
can handle this phenomenon, explaining how it is
implemented in system (Section 4) that works in
a micro-domain (which is described in Section 3).
As we will discuss in the next section, this goes be-
yond earlier work on incremental SDS, combining
the production of multimodal feedback (as in (Aist
et al, 2007)) with fast interaction in a semantically
more complex domain (compared to (Skantze and
Schlangen, 2009)).
2 Related Work
Collaboration on utterances has not often been
modelled in SDS, as it presupposes fully incre-
mental processing, which itself is still something
of a rarity in such systems. (There is work on
collaborative reference (DeVault et al, 2005; Hee-
man and Hirst, 1995), but that focuses on written
input, and on collaboration over several utterances
and not within utterances.) There are two systems
that are directly relevant here.
The system described in (Aist et al, 2007) is
able to produce some of the phenomena that we
are interested in here. The set-up is a simple
reference game (as we will see, the domain we
have chosen is very similar), where users can re-
fer to objects shown on the screen, and the SDS
gives continuous feedback about its understand-
233
ing by performing on-screen actions. While we
do produce similar non-linguistic behaviour in our
system, we also go beyond this by producing
verbal feedback that responds to the certainty of
the speaker (expressed by the use of trial intona-
tion). Unfortunately, very little technical details
are given in that paper, so that we cannot compare
the approaches more fully.
Even more closely related is some of our own
previous work, (Skantze and Schlangen, 2009),
where we modeled fast system reactions to deliv-
ery of information in installments in a number se-
quence dictation domain. In a small corpus study,
we found a very pronounced use of trial or in-
stallment intonations, with the first installments of
numbers being bounded by rising intonation, and
the final installment of a sequence by falling into-
nation. We made use of this fact by letting the sys-
tem distinguish these situations based on prosody,
and giving it different reaction possibilities (back-
channel feedback vs. explicit confirmation).
The work reported here is a direct scaling up of
that work. For number sequences, the notion of
utterance is somewhat vague, as there are no syn-
tactic constraints that help demarcate its bound-
aries. Moreover, there is no semantics (beyond
the individual number) that could pose problems
? the main problem for the speaker in that do-
main is ensuring that the signal is correctly identi-
fied (as in, the string could be written down), and
the trial intonation is meant to provide opportuni-
ties for grounding whether that is the fact. Here,
we want to go beyond that and look at utterances
where it is the intended meaning whose recogni-
tion the speaker is unsure about (grounding at level
3 rather than (just) at level 2 in terms of (Clark,
1996).) This difference leads to differences in the
follow up potential: where in the numbers domain,
typical repair follow-ups were repetitions, in se-
mantically more complex domains we can expect
expansions or reformulations.
3 The Puzzle Micro-Domain
To investigate these issues in a controlled set-
ting, we chose a domain that makes complex and
possibly underspecified references likely, and that
also allows a combination of linguistic and non-
linguistic feedback. In this domain, the user?s goal
is to instruct the system to pick up and manipu-
late Tetris-like puzzle pieces, which are shown on
the screen. We recorded human?human as well
as human?(simulated) machine interactions in this
domain, and indeed found frequent use of ?pack-
aging? of instructions, and immediate feedback, as
in (2) (arrow indicating intonation).
(2) IG-1: The cross in the corner? ...
IF-2: erm
IG-3: the red one .. yeah
IF-4: [moves cursor]
IG-5: take that.
We chose these as our target phenomena for the
implementation: intra-utterance hesitations, possi-
bly with trial intonation (as in line 2);2 immediate
execution of actions (line 4), and their grounding
role as display of understanding (?yeah? in line 3).
The system controls the mouse cursor, e.g. moving
it over pieces once it has a good hypothesis about
a reference; other actions are visualised similarly.
4 Implementation
4.1 Overview
Our system is realised as a collection of incre-
mental processing modules in the InproToolKit
(Schlangen et al, 2010), a middle-ware pack-
age that implements some of the features of the
model of incremental processing of (Schlangen
and Skantze, 2009). The modules used in the im-
plementation will be described briefly below.
4.2 ASR, Prosody, Floor Tracker & NLU
For speech recognition, we use Sphinx-4 (Walker
et al, 2004), with our own extensions for incre-
mental speech recognition (Baumann et al, 2009),
and our own domain-specific acoustic model. For
the experiments described here, we used a recog-
nition grammar.
Another module performs online prosodic anal-
ysis, based on pitch change, which is measured in
semi-tone per second over the turn-final word, us-
ing a modified YIN (de Cheveigne? and Kawahara,
2002). Based on the slope of the f0 curve, we clas-
sify pitch as rising or falling.
This information is used by the floor track-
ing module, which notifies the dialogue manager
(DM) about changes in floor status. These sta-
tus changes are classified by simple rules: silence
following rising pitch leads to a timeout signal
2Although we chose to label this ?intra-utterance? here,
it doesn?t matter much for our approach whether one consid-
ers this example to consist of one or several utterances; what
matters is that differences in intonation and pragmatic com-
pleteness have an effect.
234
{< a ( 1 action=A=take; 2 prepare(A) ; 3 U),
( 4 tile=T ; 5 highlight(T) ; 6 U),
( 7 ; 8 execute(A,T) ; 9 U) >
< b (10 action=A=del ;11 prepare(A) ;12 U),
(13 tile=T ;14 highlight(T) ;15 U),
(16 ;17 execute(A,T) ;18 U) >}
Figure 1: Example iQUD
sent to the DM faster (200ms) than silence after
falling pitch (500ms). (Comparable to the rules in
(Skantze and Schlangen, 2009).)
Natural language understanding finally is per-
formed by a unification-based semantic composer,
which builds simple semantic representations out
of the lexical entries for the recognised words; and
a resolver, which matches these representations
against knowledge of the objects in the domain.
4.3 Dialogue Manager and Action Manager
The DM reacts to input from three sides: semantic
material coming from the NLU, floor state signals
from the floor tracker, and notifications about exe-
cution of actions from the action manager.
The central element of the information state
used in the dialogue manager is what we call the
iQUD (for incremental Question under Discus-
sion, as it?s a variant of the QUD of (Ginzburg,
1996)). Figure 1 gives an example. The iQUD
collects all relevant sub-questions into one struc-
ture, which also records what the relevant non-
linguistic actions are (RNLAs; more on this in a
second, but see also (Bu? and Schlangen, 2010),
where we?ve sketched this approach before), and
what the grounding status is of that sub-question.
Let?s go through example (2). The iQUD in
Figure 1 represents the state after the system has
asked ?what shall I do now??. The system an-
ticipates two alternative replies, a take request, or
a delete request; this is what the specification of
the slot value in 1 and 10 in the iQUD indicates.
Now the user starts to speak and produces what is
shown in line 1 in the example. The floor tracker
reacts to the rising pitch and to the silence of ap-
propriate length, and notifies the dialogue man-
ager. In the meantime, the DM has received up-
dates from the NLU module, has checked for each
update whether it is relevant to a sub-question on
the iQUD, and if so, whether it resolves it. In this
situation, the material was relevant to both 4 and
13, but did not resolve it. This is a precondition for
the continuer-questioning rule, which is triggered
by the signal from the floor tracker. The system
then back-channels as in the example, indicating
acoustic understanding (Clark?s level 2), but fail-
ure to operate on the understanding (level 3). (As
an aside, we found that it is far from trivial to find
the right wording for this prompt. We settled on
an ?erm? with level pitch.)
The user then indeed produces more material,
which together with the previously given informa-
tion resolves the question. This is where the RN-
LAs come in: when a sub-question is resolved, the
DM looks into the field for RNLAs, and if there
are any, puts them up for execution to the action
manager. In our case, slots 4 and 13 are both
applicable, but as they have compatible RNLAs,
this does not cause a conflict. When the action
has been performed, a new question is accommo-
dated (not shown here), which can be paraphrased
as ?was the understanding displayed through this
action correct??. This is what allows the user reply
in line 3 to be integrated, which otherwise would
need to be ignored, or even worse, would confuse
a dialogue system. A relevant continuation, on the
other hand, would also have resolved the question.
We consider this modelling of grounding effects
of actions an important feature of our approach.
Similar rules handle other floor tracker events;
not elaborated here for reasons of space. In
our current prototype the rules are hard-coded,
but we are preparing a version where rules and
information-states can be specified externally and
are read in by a rule-engine.
4.4 Overhearer Evaluation
Evaluating the contribution of one of the many
modules in an SDS is notoriously difficult (Walker
et al, 1998). To be able to focus on evaluation of
the incremental dialogue strategies and avoid in-
terference from ASR problems (and more techni-
cal problems; our system is still somewhat frag-
ile), we opted for an overhearer evaluation. (Such
a setting was also used for the test of the incremen-
tal system of (Aist et al, 2007).)
We implemented a non-incremental version of
the system that does not give non-linguistic feed-
back during user utterances and has only one,
fixed, timeout of 800ms (comparable to typical
settings in commercial dialogue systems). Two
of the authors then recorded 30 minutes of inter-
actions with the two versions of the system.We
then identified and discarded ?outlier? interac-
tions, i.e. those with technical problems, or where
235
recognition problems were so severe that a non-
understanding state was entered repeatedly. These
criteria were meant to be fair to both versions
of the system, and indeed we excluded similar
numbers of failed interactions from both versions
(around 10% of interactions in total).
We measured the length of interactions in the
two sets, and found that the interactions in the in-
cremental setting were significantly shorter (t-test,
p< 0.005). This was to be expected, of course,
as the incremental strategies allow faster reactions
(execution time can be folded into the user utter-
ance); other outcomes would have been possible,
though, if the incremental version had systemati-
cally more understanding problems.
We then had 8 subjects (university students,
not involved in the research) watch and directly
judge (questionnaire, Likert-scale replies to ques-
tions about human-likeness, helpfulness, and re-
activity) 34 randomly selected interactions from
either condition. Human-likeness and reactivity
were judged significantly higher for the incremen-
tal version (Wilcoxon rank-sum test; p< 0.05 and
p< 0.005, respectively), while there was no effect
for helpfulness (p= 0.06).
5 Conclusions
We described our incremental micro-domain dia-
logue system, which is capable of reacting to sub-
tle signals from the user about expected feedback,
and is able to produce overlapping non-linguistic
actions, modelling their effect as displays of un-
derstanding. Interactions with the system were
judged by overhearers to be more human-like and
reactive than with a non-incremental variant. We
are currently working on extending and generalis-
ing our approach to incremental dialogue manage-
ment, porting it to other domains.
Acknowledgments Funded by an ENP grant from DFG.
References
Gregory Aist, James Allen, Ellen Campana, Car-
los Gomez Gallo, Scott Stoness, Mary Swift, and
Michael K. Tanenhaus. 2007. Incremental under-
standing in human-computer dialogue and experi-
mental evidence for advantages over nonincremen-
tal methods. In Proceedings of Decalog (Semdial
2007), Trento, Italy.
Timo Baumann, Michaela Atterer, and David
Schlangen. 2009. Assessing and Improving the
Performance of Speech Recognition for Incremental
Systems. In Proceedings of NAACL-HLT 2009,
Boulder, USA.
Okko Bu? and David Schlangen. 2010. Modelling
sub-utterance phenomena in spoken dialogue sys-
tems. In Proceedings of Semdial 2010 (?Pozdial?),
pages 33?41, Poznan, Poland, June.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press, Cambridge.
Alain de Cheveigne? and Hideki Kawahara. 2002. YIN,
a fundamental frequency estimator for speech and
music. Journal of the Acoustical Society of America,
111(4):1917?1930.
David DeVault, Natalia Kariaeva, Anubha Kothari, Iris
Oved, and Matthew Stone. 2005. An information-
state approach to collaborative reference. In Short
Papers, ACL 2005, Michigan, USA, June.
Jonathan Ginzburg. 1996. Interrogatives: Ques-
tions, facts and dialogue. In Shalom Lappin, editor,
The Handbook of Contemporary Semantic Theory.
Blackwell, Oxford.
Peter A. Heeman and Graeme Hirst. 1995. Collabo-
rating on referring expressions. Computational Lin-
guistics, 21(3):351?382.
Massimo Poesio and Hannes Rieser. 2010. Comple-
tions, coordination, and alignment in dialogue. Dia-
logue and Discourse, 1(1):1?89.
Matthew Purver, Christine Howes, Eleni Gre-
goromichelaki, and Patrick Healey. 2009. Split
utterances in dialogue: a corpus study. In Proceed-
ings of the SIGDIAL 2009, pages 262?271, London,
UK, September.
Harvey Sacks and Emanuel A. Schegloff. 1979. Two
preferences in the organization of reference to per-
sons in conversation and their interaction. In George
Psathas, editor, Everyday Language: Studies in Eth-
nomethodology, pages 15?21. Irvington Publishers,
Inc., New York, NY, USA.
David Schlangen and Gabriel Skantze. 2009. A gen-
eral, abstract model of incremental dialogue pro-
cessing. In Proceedings of EACL 2009, pages 710?
718, Athens, Greece, March.
David Schlangen, Timo Baumann, Hendrik
Buschmeier, Okko Bu?, Stefan Kopp, Gabriel
Skantze, and Ramin Yaghoubzadeh. 2010. Middle-
ware for incremental processing in conversational
agents. In Proceedings of SIGDIAL 2010, Tokyo,
Japan.
Gabriel Skantze and David Schlangen. 2009. Incre-
mental dialogue processing in a micro-domain. In
Proceedings of EACL 2009, pages 745?753, Athens,
Greece, March.
Marilyn A. Walker, Diane J. Litman, Candace A.
Kamm, and Alicia Abella. 1998. Evaluating spoken
dialogue agents with PARADISE: Two case studies.
Computer Speech and Language, 12(3).
Willie Walker, Paul Lamere, Philip Kwok, Bhiksha
Raj, Rita Singh, Evandro Gouvea, Peter Wolf, and
Joe Woelfel. 2004. Sphinx-4: A flexible open
source framework for speech recognition. Techni-
cal report, Sun Microsystems Inc.
236
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 120?129,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Predicting the Micro-Timing of User Input for an Incremental
Spoken Dialogue System that Completes a User?s Ongoing Turn
Timo Baumann
Department of Linguistics
Potsdam University
Germany
timo@ling.uni-potsdam.de
David Schlangen
Faculty of Linguistics and Literature
Bielefeld University
Germany
david.schlangen@uni-bielefeld.de
Abstract
We present the novel task of predicting tem-
poral features of continuations of user input,
while that input is still ongoing. We show that
the remaining duration of an ongoing word, as
well as the duration of the next can be predicted
reasonably well, and we put this information to
use in a system that synchronously completes
a user?s speech. While we focus on collabo-
rative completions, the techniques presented
here may also be useful for the alignment of
back-channels and immediate turn-taking in an
incremental SDS, or to synchronously monitor
the user?s speech fluency for other reasons.
1 Introduction
Turn completion, that is, finishing a user?s ongoing ut-
terance, can be considered an ideal test-case of incre-
mental spoken language processing, as it requires that
all levels of language understanding and production
are carried out in real time, without any noticeable
lags and with proper timing and even with the ability
to predict what will come. Spoken dialogue systems,
especially incremental ones, have come a long way
towards reducing lags at turn changes (e. g. (Raux and
Eskenazi, 2009; Skantze and Schlangen, 2009)), or
even predicting upcoming turn changes (Schlangen,
2006; Baumann, 2008; Ward et al, 2010). Com-
pared to regular turn changes, where short pauses or
overlaps occur frequently (Weilhammer and Rabold,
2003), turn completions in natural dialogues are typ-
ically precisely aligned and prosodically highly in-
tegrated with the turn that is being completed (Lo-
cal, 2007). With ever more incremental (and hence
quicker) spoken dialogue systems, the phenomenon
of completion comes into reach for SDSs, and hence
questions of micro-timing become important.
While completing someone else?s turn ? especially
for a computer ? may be considered impolite or even
annoying, being able to do so can be a useful capa-
bility. Some tasks where it might be helpful are
? negotiation training to induce stress in a human
trainee as presented by DeVault et al (2009), or
? pronunciation aids for language learners, in
which hard to pronounce words could be spoken
simultaneously by the system.
A system should certainly not try to complete all
or even many user turns, but having the capability
to do so means that the system has a very efficient
interactional device at its disposal.
Furthermore, monitoring the user?s timing, as is
required for the temporal prediction of turn continua-
tions, can also be used for other conversational tasks
such as producing back-channels that are precisely
aligned to the user?s back-channel inviting cues, to
enable micro-alignment of turn-onsets, or to quickly
react to deviations in the user?s fluency.
In this paper, we concentrate on the temporal as-
pects of turn completion, that is, the prediction of
the precise temporal alignment of a turn continuation
and the technical realization of this timing. We as-
sume the task of predicting the completion itself to
be handled by some other system component. Such
components are indeed under development (see Sec-
tion 2). However, previous work has left out the
question of how the precise timing of turn comple-
tions can be accomplished, which is what we try to
answer here.
The remainder of this paper is structured as fol-
lows: In Section 2 we review literature on turn com-
pletion and related work in spoken dialogue systems,
120
before we explain what exactly our task is in Sec-
tion 3. In Section 4 we present our system?s overall
architecture and the duration modelling technique
that we use, before describing the corpus that we use
in Section 5. In Section 6 we first analyse whether
enough time to output a completion is available suffi-
ciently often, before turning to the question for the
actual sub-tasks of when and how to complete. We
wrap up with concluding remarks and ideas for future
work.
2 Related Work
The general phenomenon of turn completion can
be broken down into cases where the completion
is spoken simultaneously with the original speaker
(turn sharing, (Lerner, 2002)) and where the floor
changes in mid-utterance (collaborative turn se-
quences (Lerner, 2004) or split utterances (Purver
et al, 2009)). In this paper, a differentiation be-
tween the two cases is not important, as we only
deal with the question of when to start speaking
(for the previously non-speaking system) and not the
question of whether the current turn owner will stop
speaking. Moreover, whether the other speaker will
stop is beyond the system?s control. Lerner (2004)
distinguishes turn co-optation, in which a listener
joins in to come first and win the floor, and turn co-
completion, in which the completion is produced in
chorus. Both of these phenomena relate to the cur-
rent speaker?s speech: either to match it, or to beat
it. While we focus on matching in this work, the
methods described similarly apply to co-optation.
As Lerner (2002) notes, attributing this view to
Sacks et al (1974), simultaneous speech in conver-
sation is often treated exclusively as a turn taking
problem in need of repair. This is exactly the point
of view taken by current spoken dialogue systems,
which avoid overlap and interpret al simultaneous
speech as barge-in, regardless of content. However,
Lerner (2002) also notes that simultaneous speech
systematically occurs without being perceived as a
problem, e. g. in greetings, or when saying good bye,
which are relevant sub-tasks in deployed SDSs.
Two corpus studies are available which investi-
gate split utterances and their frequency: Skuplik
(1999) looked at sentence cooperations in a corpus
of task-oriented German (Poesio and Rieser, 2010)
and found 3.4 % of such utterances. Purver et al
(2009) find 2.8 % of utterance boundaries in the BNC
(as annotated by Ferna?ndez and Ginzburg (2002))
to meet their definition of utterances split between
speakers. Thus, while the absolute frequency may
seem low, the phenomenon does seem to occur con-
sistently across different languages and corpora.
Local (2007) describes phonetic characteristics at
utterance splits (he calls the phenomenon turn co-
construction) which distinguish them from regular
turn handovers, namely temporal alignment and close
prosodic integration with the previous speaker?s utter-
ance. In this paper, we focus on the temporal aspects
(both alignment and speech rate) when realizing turn
completions, leaving pitch integration to future work.
Cummins (2009) analyses speech read aloud by
two subjects at the same time (which he calls syn-
chronous speech): Synchrony is slightly better in a
live setting than with a subject synchronizing to a
recording of speech which was itself spoken in syn-
chrony and this is easier than to a recording of uncon-
strained speech. Cummins (2009) also experiments
with reduced stimuli: eliminating f0-contour had no
significant impact on synchrony, while a carrier with-
out segmental information (but including f0-contour)
fared significantly better than speaking to an uninfor-
mative hiss. (The first sentence of each recording was
always left unmodified, allowing subjects to estimate
speech rate even in the HISS condition.) Thus, pitch
information does not seem necessary for the task but
may help in the absence of segmental information.
On a more technical level and as mentioned above,
much work has been put into speeding up end-of-
turn detection and reducing processing lags at turn
changes (Raux and Eskenazi, 2009) and more re-
cently into end-of-turn prediction: Ward et al (2010)
present a model of turn-taking which estimates the
remaining duration of a currently ongoing turn. We
extend the task to predicting the remaining duration
of any currently ongoing word in the turn. Of course,
for this to be possible, words must be recognized
while they are still being uttered. We have previ-
ously shown (Baumann et al, 2009) that this can be
achieved with incremental ASR for the vast major-
ity of words and with an average of 102 ms between
when a word is first recognized and the word?s end.
As mentioned above, our work relies on other in-
cremental components to form a meaningful, turn
121
completing application and such components are be-
ing developed: Incremental understanding is well un-
derway (Sagae et al, 2009; Heintze et al, 2010), as is
decision making on whether full understanding of an
utterance has been reached (DeVault et al, 2009), and
Purver et al (2011) present an incremental semantics
component aimed explicitly at split utterances. In
fact, DeVault et al (2009) provide exactly the coun-
terpart to our work, describing a method that, given
the words of an ongoing utterance, decides when the
point of maximum understanding has been reached
and with what words this utterance is likely to end.
However, in their system demonstration, Sagae et al
(2010) use short silence time-outs to trigger system
responses. Our work eliminates the need for such
time-outs.
Hirasawa et al (1999) present a study where im-
mediate, overlapping back-channel feedback from
the system was found to be inferior to acknowledg-
ing information only after the user?s turn. However,
they disregarded the back-channels? micro-temporal
alignment as explored in this study (presumably pro-
ducing back-channels as early as possible), so their
negative results cannot be taken as demonstrating a
general shortcoming of the interactional strategy.
3 The Task
The general task that our timing component tackles
is illustrated in Figure 1. The component is triggered
into action when an understanding module signals
that (and with what words) a turn should be com-
pleted. At this decision point, our component must
estimate (a) when the current word ends and (b) how
the user will speak the predicted continuation. Ide-
ally, the system will start speaking the continuation
precisely when the next word starts and match the
user?s speech as best as possible. Thus, our compo-
nent must estimate the time between decision point
and ideal onset (which we call holding time) and the
user?s speech rate during the following words.
In order for the system to be able to produce a
continuation (?five six seven? in Figure 1) in time,
of course the decision point must come sufficiently
early (i. e. during ?four?) to allow for a completion
to be output in due time. This important precondition
must be met by-and-large by the employed ASR.
However, it is not a strict requirement: If ASR results
Figure 1: The task: When notified that the ongoing utter-
ance should be completed with ?five six seven? after the
word ?four?, the first three words are used to (a) estimate
the remaining duration of ?four? and to (b) estimate the
speech rate for the completion.
are lagging behind, the timing component?s estimated
holding time should turn negative. Depending on the
estimated lag, a completion can be suppressed or,
if it is small, fairly good completions can still be
realized by shortening the first (few) phonemes of
the completion to be synthesized.
We will now present our overall system before
describing two strategies we developed for solving
the task just described, and further on present the
experiments we conducted with the system and their
results in Sections 5 and 6.
4 System Description
Our system is based on the InproTK toolkit for in-
cremental spoken dialogue systems (Schlangen et
al., 2010) which uses Sphinx-4 (Walker et al, 2004)
and MaryTTS (Schro?der and Trouvain, 2003) as un-
derlying ASR and TTS engines, respectively. The
core of our system is a component that incrementally
receives rich speech recognition input (words, their
durations and a pitch track) from an incremental ASR
and computes the timing of completions.
When receiving a new word from ASR, our com-
ponent queries an understanding component whether
a completion can be predicted, and if so, whether
such a completion should be performed. In order to
not duplicate the work of DeVault et al (2009), we
use a mock implementation of an understanding mod-
ule, which actually knows what words are going to be
spoken (from a transcript file) and aims to complete
after every word spoken.
We have implemented two strategies for the timing
module, which we will describe in turn, after first
discussing a simple baseline approach.
122
Baseline: Speak Immediately A first, very simple
approach for our timing component is to never wait
between the decision point and outputting a comple-
tion right away. We believe that this was the strategy
taken by Hirasawa et al (1999) and we will show in
our evaluation in Section 6.2 that it is not very good.
Strategy 1: Estimating ASR Lookahead In our
ASR-based strategy (illustrated in Figure 2, top) the
system estimates what we call its lookahead rate,
i. e. the average time between when a word is first
recognized by ASR and the word?s end in the signal.
This lookahead is known for the words that have been
recognized so far and the average lookahead can then
be used as an estimate of the remaining duration
of the word that is currently being detected (i. e. its
holding time). Once the currently spoken word is
expected to end, the system should start to speak.
The strategy just described, as well as the baseline
strategy, only solve half of the task, namely, when the
continuation should be started, but not the question
of how to speak, which we will turn to now. Both
sub-tasks can be solved simultaneously by estimating
the speech rate of the current speaker, based on what
she already said so far, and considering this speech
rate when synthesizing a completion. Speech rate
estimation using some kind of duration model thus
forms the second strategy?s main component. For the
purpose of this work, we focus on duration models
in the context of TTS, where they are used to assign
durations to the phones to be uttered. Rule-based
approaches (Klatt, 1979) as well as methods using
machine learning have been used (primarily CART
(Breiman et al, 1984)); for HMM-based speech syn-
thesis, durations can be generated from Gaussian
probability density functions (PDFs) (Yoshimura et
al., 1998). We are not aware of any work that uses
duration models to predict the remaining time of an
ongoing word or utterance.
In our task, we need the duration model to make
estimations based on limited input (instead of pro-
viding plausibility ratings as in most ASR-related
applications). As it turns out, a TTS system in itself
is an excellent duration model because it potentially
ponders all kinds of syntactic, lexical, post-lexical,
phonological and prosodical context when assigning
durations to words and their phones. Also, our task
already involves a TTS system to synthesize the turn
Figure 2: Our strategies to estimate holding time (when to
speak), and speech rate (how to speak; only Strategy 2).
completion ? in our case MaryTTS (Schro?der and
Trouvain, 2003). The durations can be accessed in
symbolic form in MaryTTS, and the system allows
to manipulate this information prior to acoustic syn-
thesis. Depending on which voice is used, MaryTTS
uses machine-learned duration models (CART or
PDFs) or an optimized version of Klatt?s (1979) rules
which have been shown to perform only marginally
worse than the CART-based approach (Brinckmann
and Trouvain, 2003).
Strategy 2: Analysis-by-Synthesis As just de-
scribed, we hence employ the TTS? duration model
in an analysis-by-synthesis approach in this second
strategy, as illustrated in Figure 2 (bottom): When
triggered to complete an ongoing utterance, we query
the TTS for the durations it would assign to a produc-
tion of the predicted full utterance, i. e. the prefix that
was heard plus the predicted continuation of the turn.
In that way, the TTS can take the full utterance into
account when assigning prosodic patterns which may
influence durations. We then compute the factor that
is needed to scale the TTS?s duration of the words
already finished by the user (in the example: ?one
two three?) to the duration of the actual utterance
and apply this scaling factor to the remaining words
in the synthesized completion. We can then read off
the expected duration of the currently spoken word
from the scaled TTS output and, by subtracting the
time that this word is already going on, find out the
holding time. Similarly, the completion of the turn
which is now scaled to match the user?s speech rate
can be fed back to the synthesis system in order to
generate the acoustic waveform which is to be output
to the speakers once the system should start to speak.
123
5 Corpus and Experiment Setup
In order to evaluate the accuracy of the individual
components involved in the specific subtasks, we
conducted a controlled offline experiment. We have
not yet evaluated how actual users of our system
would judge its performance at outputting collabora-
tive completions.
As evaluation corpus we use recordings of the
German version of the story The North Wind and
the Sun (IPA, 1999) from the Kiel Corpus of Read
Speech (IPDS, 1994). The story (including title)
consists of 111 words and is read by 16 speakers,
giving a total of 1776 words in 255 inter-pausal-units
(IPUs), altogether resulting in about 12 minutes of
speech. (In the following, we will equate ?turns? with
IPUs, as our corpus of read speech does not contain
true turns.) Words and phones in our corpus have
a mean/median/std dev duration of 319/290/171 ms
and 78/69/40 ms, respectively.
We assume that every word can be a possible com-
pletion point in a real system, hence we evaluate the
performance of our timing component for all words
in the corpus. (This generalization may have an in-
fluence on our results: real collaborative completions
are sometimes invited by the speaker, probably by
giving cues that might simplify co-completion; if that
is true, the version tackled here is actually harder than
the real task.)
Good turn completions (and good timings) can
probably only be expected in the light of high ASR
performance. We trained a domain-specific language
model (based on the test corpus) and used an acous-
tic model trained for conversational speech which
was not specifically tuned for the task. The resulting
WER is 4.2 %. While our results could hence be con-
sidered too optimistic, Baumann et al (2009) showed
that incremental metrics remained stable in the light
of varying ASR performance. We expect that lower
ASR performance would not radically change pre-
diction quality itself; rather, it would have an impact
on how often continuations could be predicted, since
that is based on correct understanding of the prefix
of the utterance, limiting the amount of data points
for our statistics.
Even though we simulated the understanding and
prediction module, we built in some constraints that
are meant to be representative of real implementa-
tions of such a module: it can only find the right
completion if the previous two words are recognized
correctly and the overall WER is lower than 10 %.
(Coming back to Figure 1, if the system had falsely
recognized ?on two three?, no completion would
take place: Even though the last two words ?two
three? were recognized correctly, the WER between
?on two three? and ?one two three? is too high.) Un-
der this constraint, the timing component generated
data for 1100 IPU-internal and 223 IPU-final words
in our corpus.
The main focus of this paper is turn completion and
completions can only take place if there is something
left to complete (i. e. after turn-internal words). It
is still useful to be able to predict the duration of
turn-final words, though, as this is a prerequisite for
the related task of timing speaker changes. For this
reason, we include both turn-internal and turn-final
words in the analyses in Section 6.2.
In the evaluation, we use the ASR?s word align-
ments from recognition as gold standard (instead of
e. g. hand-labelled timings), which are essentially
equal to output from forced alignment. However,
when evaluating how well our timing component pre-
dicts the following word?s duration, we need that
word to also be correctly recognized by ASR. This
holds for 1045 words in our corpus, for which we
report results in Section 6.3.
6 Results
We evaluate the timing of our system with regards to
whether completions are possible in general, when a
completion should be produced, and what the speech
rate of the completion should be in the subsections
below.
6.1 Availability of Time to Make a Decision
While it is strictly speaking not part of the timing
component, a precondition to being able to speak
just-in-time is to ponder this decision sufficiently
early as outlined above.
Figure 3 shows a statistic of when our ASR first
hypothesizes a correct word relative to the word?s
end (which can be determined post-hoc from the
final recognition result) on the corpus. Most words
are hypothesized before their actual endings, with a
mean of 134 ms (median: 110 ms) ahead. This leaves
124
 0
 5
 10
 15
 20
?
 
-0.48
-0.40
-0.32
-0.24
-0.16
-0.08
 0 ?
 0.08
%
decision point relative to end of word (in seconds)
binned histogram
median (-0.11)
Figure 3: Statistics of when decisions can be first taken
relative to the word?s end (determined post-hoc).
enough lookahead to synthesize a completion and
for some delays that must be taken into account for
input and output buffering in the sound card, which
together take around 50 ms in our system.
Interestingly, lookahead differs widely for the
speakers in our corpus with means between 97 and
237 ms. As can be seen in Figure 3, some words are
only hypothesized after the fact, or at least too late
to account for the inevitable lags, which renders im-
possible successful turn-completions following these
words. However, the timing component should know
when it is too late ? the holding time should be nega-
tive ? and could either not output the completion at
this point or e. g. back off to setting in one or more
phones or syllables later (actually, back off until the
holding time turns positive).
6.2 When to Start Speaking
We evaluate the strategies from Section 4 by com-
paring the predicted holding times with the ideal
holding time, i. e. the time necessary to match the
ASR?s lookahead.
Figure 3 can also be taken as depicting the error
distribution of our baseline strategy to find out when
to start a completion: on average, the completion
will be early by 134 ms if it is uttered immediately
and the distribution is somewhat skewed. An unbi-
ased baseline strategy is obtained by subtracting the
global mean from the holding times. This however re-
quires the mean to be known in advance and is hence
inflexible: the global mean may very well be differ-
ent for other data sets as it already differs between
model
error distribution metrics (in ms)
mean median std dev MAE
baseline: all -134 -110 107 110
baseline ?? 0 23 107 63
ASR-based : all -2 19 105 60
IPU-internal 26 33 82 51
IPU-final -148 -143 87 142
TTS-based : all -3 4 85 45
IPU-internal 12 11 77 41
IPU-final -78 -76 83 79
Table 1: Descriptive statistics of the error distributions
over estimated onset times for different duration models.
speakers in our corpus. The two other strategies? er-
ror distributions are less skewed, so we just report
the distributions? mean, median, and standard devi-
ation,1 as well as the median absolute error (MAE)
for the ASR-based, the TTS-based and the baseline
strategies in Table 1.
As can be seen in Table 1, both strategies are
similarly effective in predicting the average remain-
ing time of a currently uttered word, reducing the
mean error close to zero, a significant improvement
over starting a completion or next turn immediately.
(ANOVA with post-hoc Tukey?s honest significance
differences test.) While our two approaches perform
similarly when comparing the performance for all
words, there actually are differences when looking
separately at IPU-internal and IPU-final words. In
both cases the TTS-based approach has a significantly
lower bias (paired Student?s t-tests, p < 0.01).
The bias of both strategies differs depending on
whether the current word is IPU-internal or -final.
We believe this to be due to final lengthening: phones
are about 40 % longer in IPU-final words. This is not
captured by the ASR-based strategy and the length-
ening may be stronger than what is predicted by the
pronunciation model of the TTS we use.
A low standard deviation of the error distribution
is probably even more important than a low mean
error, as it is variability, or jitter, that makes a system
unpredictable to the user. While there is no signifi-
cant improvement of the ASR-based approach over
the baseline, the TTS-based approach significantly
outperforms the other approaches with a 20 % re-
1We prefer to report mean and std dev for bias and jitter
separately; notice that RMSE=
?
?2 + ?2.
125
task
error distribution metric (in ms)
mean median std dev MAE
TTS-based : duration -5 4 75 45
+ ASR-based : onset 26 33 82 51
= end of word 25 30 100 81
+ TTS-based : onset 12 11 77 41
= end of word 7 10 94 74
Table 2: Descriptive statistics of the error distributions for
the first spoken word of a completion.
duction of jitter down to about the average phone?s
length (Browne-Forsythe?s modified Levene?s test,
p < 0.001).
Regarding human performance in synchronous
speech, Cummins (2002) reports an MAE of 30 ms for
the synchronous condition. However, MAE increased
to 56 ms when synchronizing to an (unsynchronously
read) recording, a value which is in the range of our
results (and with our system relying on similar input).
6.3 How to Speak
As explained in the task description, knowing when
to speak is only one side of the medal, as a turn
completion itself must be integrated with the previ-
ous speech in terms of duration, prosodic shape and
loudness.
Only our TTS-based strategy is capable of out-
putting predictions for a future word; our ASR-based
approach does not provide this information. How-
ever, both duration and onset estimation (the next
onset is identical to the end of the current word as
estimated in Section 6.2) together determine the error
at the word?s end. Hence, we report the error at the
next word?s end for the TTS strategy?s duration esti-
mate combined with both strategies? onset estimates
in Table 2.
Duration prediction for the next word with the
TTS-based strategy works similarly well as for on-
going words (as in Section 6.2), with an MAE of
45 ms (which is again in the range of human perfor-
mance). However, for the next word?s end to occur
when the speaker?s word ends, correct onset estima-
tion is just as important. When we combine onset
estimation with duration prediction, errors add up
and hence the error for the next word?s end is some-
what higher than for either of the tasks alone, with a
standard deviation of 94 ms and an MAE of 74 ms for
the TTS-based model, which again outperforms the
ASR-based model.
So far, we have not evaluated the matching of
prosodic characteristics such as loudness and intona-
tion (nor implemented their prediction). We believe
that simple matching (as we implemented for onset
and speech rate) is not as good a starting point for
these as they are more complex. Instead, we believe
these phenomena to mostly depend on communica-
tive function, e. g. a co-optation having a wide pitch-
range and relatively high loudness regardless of the
current speaker?s speech. Additionally, pitch-range
would have to be incrementally speaker-normalized
which results in some implementation difficulties.2
7 Demo Application: Shadowing
To get a feeling for the complete system and to
demonstrate that our timing component works on
live input, we implemented a shadowing application
which completes ? or rather shadows ? a user utter-
ance word-by-word. Given the prediction for the next
word?s onset time and duration it prepares the output
of that next word while the user is still speaking the
preceding word. As the application expects to know
what the user is going to speak, the user is currently
limited to telling the story of North Wind and the
Sun.
Two examples of shadowings are shown in Ap-
pendix A.3 As can be seen in the screenshots, the
decision points for all words are sufficiently early
before the next word, allowing for the next word?s
output generation to take place. Overall, shadowing
quality is good, with the exception of the second ?die?
in the second example. However, there is an ASR
error directly following (?aus? instead of ?luft?) and
the ASR?s alignment quality for ?sonne die? is al-
ready sub-optimal. Also, notice that the two words
following the ASR error are not shadowed as per our
error recovery strategy outlined in Section 5.
2Edlund and Heldner (2007) report that for a reliable pitch-
range estimation 10 to 20 seconds of voiced speech and hence ?
in our view ? twice the amount of audio is necessary. This would
have reduced our corpus size by too much.
3Audio files of the examples are available at http://www.
ling.uni-potsdam.de/?timo/pub/shadowing/.
126
8 Discussion and Future Work
We described the task of micro-timing, or micro-
aligning a system response (in our case a turn com-
pletion and shadowing a speaker) to the user?s speech
based on incremental ASR output and with both ASR
and symbolic TTS output as duration models to pre-
dict when and how a completion should be uttered.
We have shown first of all, that a completion is pos-
sible after most words, as an incremental ASR in a
small-enough domain can have a sufficient lookahead.
Additionally, we have shown that the TTS-based du-
ration model is better than both the baseline and the
ASR-based model. Both the next word?s onset and
duration can be predicted relatively well (? = 77 ms
and ? = 75 ms, respectively), and within the mar-
gin of human performance in synchronously reading
speech. It is interesting to note here that synchronous
speech is simplified in prosodic characteristics (Cum-
mins, 2002), which presumably facilitates the task.
Errors in speech rate estimation add up, so that the
deviation at the next word?s end is somewhat higher
(? = 94 ms). Deviation will likely increase for longer
completions, underlining the need for an incremen-
tal speech synthesis system which should allow to
instantly adapt output to changes in speech rate, con-
tent, and possibly sentiment of the other speaker.
Clearly, our duration modelling is rather simplistic
and could likely be improved by combining ASR and
TTS knowledge, more advanced (than a purely lin-
ear) mapping when calculating relative speech rate,
integration of phonetic and prosodic features from
the ASR, and possibly more. As currently imple-
mented, improvements to the underlying TTS sys-
tem (e. g. more ?conversational? synthesis) should
automatically improve our model. The TTS-based
approach integrates additional, non-ASR knowledge,
and hence it should be possible to single out those
decision points after which a completion would be es-
pecially error-prone, trading coverage against quality
of results. Initial experiments support this idea and
we would like to extend it to a full error estimation
capability.
We have focused the analysis of incrementally
comparing expected to actual speech rate to the task
of micro-aligning a turn-completion and shadowing a
speaker. However, we believe that this capability can
be used in a broad range of tasks, e. g. in combination
with word-based end-of-turn detection (Atterer et al,
2008) to allow for swift turn taking.4 In fact, precise
micro-alignment of turn handovers could be used for
controlled testing of linguistic/prosodic theory such
as the oscillator model of the timing of turn-taking
(Wilson and Wilson, 2005).
Finally, duration modelling can be used to quickly
detect deviations in speech rate (which may indicate
hesitations or planning problems of the user) as they
happen (rather than post-hoc), allowing to take the
speaker?s fluency into account in understanding and
turn-taking coordination as outlined by Clark (2002).
Acknowledgments
This work was funded by a DFG grant in the Emmy
Noether programme. We wish to thank the anony-
mous reviewers for their very helpful comments.
References
Michaela Atterer, Timo Baumann, and David Schlangen.
2008. Towards incremental end-of-utterance detection
in dialogue systems. In Proceedings of Coling, Manch-
ester, UK.
Timo Baumann, Michaela Atterer, and David Schlangen.
2009. Assessing and improving the performance of
speech recognition for incremental systems. In Pro-
ceedings of NAACL, Boulder, USA.
Timo Baumann. 2008. Simulating spoken dialogue with
a focus on realistic turn-taking. In Proceedings of the
13th ESSLLI Student Session, Hamburg, Germany.
Leo Breiman, Jerome H. Friedman, Richard A. Olshen,
and Charles J. Stone. 1984. Classification and regres-
sion trees. Wadsworth, Monterey.
Caren Brinckmann and Ju?rgen Trouvain. 2003. The role
of duration models and symbolic representation for
timing in synthetic speech. International Journal of
Speech Technology, 6(1):21?31.
Herbert H. Clark. 2002. Speaking in time. Speech Com-
munication, 36(1):5?13.
Fred Cummins. 2002. On synchronous speech. Acoustic
Research Letters Online, 3(1):7?11.
Fred Cummins. 2009. Rhythm as entrainment: The case
of synchronous speech. Journal of Phonetics, 37(1):16?
28.
4Additionally, both our models consistently under-estimate
the duration of IPU-final words. It should be possible to turn this
into a feature by monitoring whether a word actually has ended
when it was predicted to end. If it is still ongoing, this may be
an additional indicator that the word is turn-final.
127
David DeVault, Kenji Sagae, and David Traum. 2009.
Can I finish? Learning when to respond to incremental
interpretation results in interactive dialogue. In Pro-
ceedings of SIGDIAL, London, UK.
Jens Edlund and Mattias Heldner. 2007. Underpinning
/nailon/: Automatic Estimation of Pitch Range and
Speaker Relative Pitch. In Speaker Classification II,
volume 4441 of LNCS, pages 229?242. Springer.
Raquel Ferna?ndez and Jonathan Ginzburg. 2002. Non-
sentential utterances: A corpus-based study. Traitement
automatique des languages, 43(2):13?42.
Silvan Heintze, Timo Baumann, and David Schlangen.
2010. Comparing local and sequential models for sta-
tistical incremental natural language understanding. In
Proceedings of SIGDIAL, Tokyo, Japan.
Jun-ichi Hirasawa, Mikio Nakano, Takeshi Kawabata, and
Kiyoaki Aikawa. 1999. Effects of system barge-in
responses on user impressions. In Proceedings of Eu-
rospeech, Budapest, Hungary.
International Phonetic Association, IPA. 1999. Handbook
of the International Phonetic Association. Cambridge
University Press.
Institut fu?r Phonetik und digitale Sprachverarbeitung,
IPDS. 1994. The Kiel corpus of read speech. CD-
ROM.
Dennis H. Klatt. 1979. Synthesis by rule of segmental
durations in English sentences. Frontiers of Speech
Communication Research, pages 287?299.
Gene H. Lerner. 2002. Turn sharing: The choral co-
production of talk in interaction. In C. Ford, B. Fox,
and S. Thompson, editors, The Language of Turn and
Sequence, chapter 9. Oxford University Press.
Gene H. Lerner. 2004. Collaborative turn sequences. In
Gene H. Lerner, editor, Conversation Analysis: Studies
from the First Generation, Pragmatics & Beyond, pages
225?256. John Benjamins, Amsterdam.
John Local. 2007. Phonetic detail and the organisation of
talk-in-interaction. In Proceedings of the 16th ICPhS,
Saarbru?cken, Germany.
Massimo Poesio and Hannes Rieser. 2010. Completions,
coordination, and alignment in dialogue. Dialogue and
Discourse, 1(1):1?89.
Matthew Purver, Christine Howes, Patrick G. T. Healey,
and Eleni Gregoromichelaki. 2009. Split utterances in
dialogue: a corpus study. In Proceedings of SIGDIAL,
London, UK.
Matthew Purver, Arash Eshghi, and Julian Hough. 2011.
Incremental semantic construction in a dialogue system.
In Proceedings of the 9th IWCS, Oxford, UK.
Antoine Raux and Maxine Eskenazi. 2009. A finite-
state turn-taking model for spoken dialog systems. In
Proceedings of NAACL, Boulder, USA.
Harvey Sacks, Emanuel A. Schegloff, and Gail A. Jeffer-
son. 1974. A simplest systematic for the organization
of turn-taking in conversation. Language, 50:735?996.
Kenji Sagae, Gwen Christian, David DeVault, and David
Traum. 2009. Towards natural language understanding
of partial speech recognition results in dialogue systems.
In Proceedings of NAACL, Boulder, USA.
Kenji Sagae, David DeVault, and David Traum. 2010.
Interpretation of partial utterances in virtual human
dialogue systems. In Proceedings of NAACL.
David Schlangen, Timo Baumann, Hendrik Buschmeier,
Okko Bu?, Stefan Kopp, Gabriel Skantze, and Ramin
Yaghoubzadeh. 2010. Middleware for incremental
processing in conversational agents. In Proceedings of
SIGDIAL, Tokyo, Japan.
David Schlangen. 2006. From reaction to prediction:
Experiments with computational models of turn-taking.
In Proceedings of Interspeech, Pittsburgh, USA.
Marc Schro?der and Ju?rgen Trouvain. 2003. The Ger-
man text-to-speech synthesis system MARY: A tool
for research, development and teaching. International
Journal of Speech Technology, 6(3):365?377.
Gabriel Skantze and David Schlangen. 2009. Incremental
dialogue processing in a micro-domain. In Proceedings
of EACL, Athens, Greece.
Kristina Skuplik. 1999. Satzkooperationen. Defini-
tion und empirische Untersuchung. Technical Report
1999/03, SFB 360, Universita?t Bielefeld.
Willie Walker, Paul Lamere, Philip Kwok, Bhiksha Raj,
Rita Singh, Evandro Gouvea, Peter Wolf, and Joe
Woelfel. 2004. Sphinx-4: A flexible open source
framework for speech recognition. Technical Report
SMLI TR2004-0811, Sun Microsystems Inc.
Nigel Ward, Olac Fuentes, and Alejandro Vega. 2010.
Dialog prediction for a general model of turn-taking.
In Proceedings of Interspeech, Tokyo, Japan.
Karl Weilhammer and Susen Rabold. 2003. Durational as-
pects in turn taking. In Proceedings of the 15th ICPhS,
Barcelona, Spain.
Margaret Wilson and Thomas P. Wilson. 2005. An oscil-
lator model of the timing of turn-taking. Psychonomic
Bulletin & Review, 12(6):957?968.
Takayoshi Yoshimura, Keiichi Tokuda, Takashi Masuko,
Takao Kobayashi, and Tadashi Kitamura. 1998. Du-
ration modeling for HMM-based speech synthesis. In
Proceedings of the 5th ICSLP, Sydney, Australia.
128
Appendix A Examples of Shadowing
Figure 4: Example of shadowing for a file in our corpus (k73nord2). The first line of labels shows the final ASR output,
the second line shows the decision points for each word and the third and fourth lines show the system?s output (planned
output may overlap, hence two lines; in the system, an overlapped portion of a word is replaced by the following word?s
audio).
Figure 5: Example of shadowing with live input (verena2nord2). Notice that ?Luft? is predicted and synthesized
although it is (later) misunderstood by ASR as ?aus?, resulting in a missing shadowing of ?mit? and ?ihren?. In order
to not disturb the speaker, the system?s audio output was muted.
129
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 295?303,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Combining Incremental Language Generation and
Incremental Speech Synthesis for Adaptive Information Presentation
Hendrik Buschmeier1, Timo Baumann3, Benjamin Dosch, Stefan Kopp1, David Schlangen2
1Sociable Agents Group, CITEC and Faculty of Technology, Bielefeld University
2Dialogue Systems Group, Faculty of Linguistics and Literary Studies, Bielefeld University
{hbuschme,bdosch,skopp,david.schlangen}@uni-bielefeld.de
3Natural Language Systems Division, Department of Informatics, University of Hamburg
baumann@informatik.uni-hamburg.de
Abstract
Participants in a conversation are normally re-
ceptive to their surroundings and their inter-
locutors, even while they are speaking and can,
if necessary, adapt their ongoing utterance. Typ-
ical dialogue systems are not receptive and can-
not adapt while uttering. We present combin-
able components for incremental natural lan-
guage generation and incremental speech syn-
thesis and demonstrate the flexibility they can
achieve with an example system that adapts to
a listener?s acoustic understanding problems
by pausing, repeating and possibly rephrasing
problematic parts of an utterance. In an eval-
uation, this system was rated as significantly
more natural than two systems representing the
current state of the art that either ignore the
interrupting event or just pause; it also has a
lower response time.
1 Introduction
Current spoken dialogue systems often produce pre-
scripted system utterances or use templates with vari-
able substitution during language generation. If a
dialogue system uses grammar-based generation at
all, it produces complete utterances that are then syn-
thesised and realised in one big chunk. As systems
become increasingly more conversational, however,
the need arises to make output generation1 more flex-
ible. In particular, capabilities for incrementally gen-
erating output become desirable, for two kinds of
reasons.
(a) In situations where fast system responses are
important, production of output can begin before the
1We will use the term ?output generation? here to cover both
natural language generation and speech synthesis.
content that is to be presented is fully specified ? even
if what is being produced is just a turn-taking signal
(Skantze and Hjalmarsson, 2010).
(b) A system that produces its output incrementally
can react to events happening while it is realising an
utterance. This can be beneficial in domains where
the state of the world that the system relays informa-
tion about can change mid-utterance, so that a need
may arise to adapt while speaking. It should also
improve naturalness by allowing the system to react
to dialogue phenomena such as concurrent feedback
signals from the user (Buschmeier and Kopp, 2011).
We present work towards enabling such capabil-
ities. We have implemented and connected a com-
ponent for incremental natural language genera-
tion (iNLG) that works with specifications of sub-
utterance-sized communicative intentions and a com-
ponent for incremental speech synthesis (iSS) that can
handle sub-utterance-sized input and modifications
to not-yet-spoken parts of the utterance with very low
latencies. To explore whether such an output genera-
tion capability can indeed be advantageous, we have
created a test system that can react to random noise
events that occur during a system utterance by repeat-
ing and modifying the last sub-utterance chunk. In
an evaluation, we found that this system is in general
more reactive than a non-incremental variant and that
humans rate its behaviour to be more natural than
two non-incremental and non-responsive systems.
2 Related Work
Psycholinguistic research has identified incremen-
tality as an important property of human language
production early on and it has been incorporated into
several models (e. g., Kempen and Hoenkamp, 1987;
295
Levelt, 1989). Guhe (2007) presents a computational
model of incremental conceptualisation. However,
work on iNLG itself is rare, partly because NLG re-
search focusses on text (instead of spoken language).
Notable exceptions are the in-depth analysis of
requirements for and properties of incremental gen-
eration by Kilger and Finkler (1995), who describe
the LTAG-based incremental syntactic generator VM-
GEN. It takes incremental input, processes it and pro-
duces output as soon as at least a prefix of the final
sentence is syntactically complete. If VM-GEN no-
tices that it committed itself to a prefix too early, it
can initiate an overt repair. More recently, Skantze
and Hjalmarsson (2010) presented a system that per-
forms incremental generation in the context of a spo-
ken dialogue system. It can already start to produce
output when the user has not yet finished speaking
and only a preliminary interpretation exists. By flexi-
bly changing what to say and by being able to make
self-repairs the system can recover from situations
where it selected and committed on an inadequate
speech plan. Both systems, however, are not able
to flexibly adapt the language that they generate to
changing requirements due to changes in the situation
or changing needs on the side of the user.
Real-time on-the-fly control of speech synthesis
is rare, especially the full integration into a dialogue
system. Matsuyama et al (2010) describe a system
that feeds back to the dialogue system the word at
which it has been interrupted by a barge-in. Edlund
(2008) additionally enables a system to continue at
the point where it was interrupted. He also outlines
some requirements for incremental speech synthe-
sis: to give constant feedback about what has been
delivered, to be interruptible (and possibly continue
from that position), and to run in real time. Edlund?s
system, which uses diphone synthesis, performed
non-incrementally before delivery starts. We go be-
yond this in also enabling changes during delivery
and conducting synthesis steps just-in-time.
Dutoit et al (2011) present an incremental HMM
optimiser which allows to change pitch and tempo
of upcoming phonemes. However, as that system is
fed from a (non-incrementally produced) label file, it
cannot easily be used in an incremental system.
A predecessor of our iSS component (which was
not yet fully incremental on the HMM level) is de-
scribed in detail in (Baumann and Schlangen, 2012a).
3 Incremental and Adaptive NLG
3.1 The SPUD microplanning framework
The NLG component presented here is based on
the SPUD microplanning framework (Stone et al,
2003) and realised in DeVault?s (2008) implemen-
tation ?Java SPUD?. SPUD frames microplannig as
a constraint satisfaction problem, solving the tasks
that are involved in generating a sentence (lexical
and syntactic choice, referring expression generation
and aggregation) in an integrated manner. Genera-
tion starts from a communicative goal that specifies
constraints for the final utterance. The generation pro-
cess is further shaped by (a) general constraints that
model pragmatic properties of language use such as
the Gricean maxims (a principle called ?textual econ-
omy?); (b) specific constraints imposed through the
communicative status of the propositions to be com-
municated (i. e., what knowledge can be presupposed
and what needs to be communicated explicitly); and
(c) linguistic resources (a context-free tree rewriting
formalism based on LTAG; Stone, 2002).
To deal efficiently with the infinite search space
spanned by the linguistic resources, SPUD uses a
heuristic search algorithm to find an utterance that
satisfies the imposed constraints (Stone et al, [2003]
describe the heuristic function). In each search step,
the algorithm expands the ?provisional? utterance by
adding the linguistic resource that maximally reduces
the estimated distance to the final utterance.
If the generation process runs into a dead-end state,
it could in principle deal with the situation by track-
ing back and expanding a different branch. This,
however, is impractical, as it becomes impossible
to project when ? if at all ? generation will finish.
Hence, in that case, SPUD stops without providing a
result, delegating the problem back to the preceding
component in the generation pipeline.
3.2 Partially incremental generation
SPUD generates utterances incrementally in the sense
that the completeness of the provisional utterance
increases monotonically with every step. This, how-
ever, does not mean that the surface structure of pro-
visional utterances is constructed incrementally (i. e.,
from left to right) as well, which would only be pos-
sible, if (a) the search strategy would always expand
the leftmost non-lexicalised node in the provisional
296
Utterance IC
1
IC
2
IC
n
 ?
Utterance
outline
IMPT
1
IMPT
2
IMPT
n
 ?
  MCP
? {U
1
, ?}
? KB
1
? {U
i
, ?}
? KB
2
? {U
k
, ?}
? KB
n
  MPP
 ?state
t
Figure 1: Incremental microplanning consists of two pro-
cesses, micro content planning (MCP) and microplanning-
proper (MPP). The former provides incremental microplan-
ning tasks from an utterance outline to the latter, which
incrementally transforms them into communicative intent
and intonation unit-sized chunks of natural language.
utterance first and if (b) the linguistic resources are
specified (and ordered) in a way that allows left-to-
right expansion of the trees in all possible situations.
In practice, both requirements are difficult to meet
and full word-by-word incrementality in natural lan-
guage microplanning is not within reach in the SPUD
framework. Because of this, we take a slightly more
coarse grained approach to incremental microplan-
ning and choose chunks of the size of intonation
phrases instead of words as our incremental units.
We say that our microplanner does ?partially incre-
mental generation?.
Our incremental microplanner comprises two inter-
acting processes, micro content planning and micro-
planning-proper (MCP and MPP; schematised in Fig-
ure 1), each of which fulfils a distinct task and oper-
ates on different structures.
MCP takes as input utterance outlines that describe
the communicative goal (a set of desired updates Ux)
intended to be communicated in an utterance and the
presuppositions and private knowledge needed to do
so. Importantly, utterance outlines specify how the
communicative goal can be decomposed into an or-
dered list of incremental microplanning-tasks IMPTx.
Each such task comprises (a) a subset of the commu-
nicative goal?s desired updates that belong together
and fit into one intonation unit sized chunk of speech
and (b) knowledge KBx used in generation.
MPP takes one incremental microplanning-task at
a time and uses SPUD to generate the IMPT?s commu-
nicative intent as well as its linguistic surface form
ICx. The communiciative intent is added to a repre-
sentation (?state? in Figure 1) that is shared between
the two processes. While processing the IMPTs of
an utterance outline, MCP can access this representa-
tion, which holds information about all the desired
updates that were achieved before, and thus knows
that a desired update that is shared between subse-
quent IMPTs has already been communicated. MPP
can also take this information into account during
generation. This makes it possible that an utterance
is coherent and adheres to pragmatic principles even
though generation can only take local decisions.
3.3 Adaptive generation
Being able to generate language in sub-utterance
chunks makes it possible to dynamically adapt later
increments of an utterance to changes in the situa-
tion that occur while the utterance is being realised.
Decisions about these adaptations need not be taken
almost until the preceding increment finishes, mak-
ing the generation process very responsive. This is
important to be able to deal with interactive dialogue
phenomena, such as communicative feedback of the
interlocutor (Allwood et al, 1992) or compound con-
tributions (Howes et al, 2011), in a timely manner.
Adaptation may happen in both parts of incremen-
tal microplanning. In MCP, adaptation takes the form
of dynamically changing the choice of which IMPT to
generate next or changing the internal structure of an
IMPT; adaptation in MPP changes the choices the gen-
eration process makes while transforming IMPTs into
communicative intent and surface form. Adaptation
in MCP is triggered top-down, by higher-level pro-
cesses such as dialogue management. Adaptation in
MPP on the other hand depends on the task given and
on the status of the knowledge used during generation.
The details are then governed by global parameter
settings MPP uses during generation.
If there is, for example, reason for the system to
believe that the current increment was not commu-
nicated clearly because of noise in the transmission
channel, the MCP process might delay future IMPTs
and initiate a repair of the current one by re-inserting
it at the beginning of the list of upcoming IMPTs of
this utterance outline. The MPP process? next task
is then to re-generate the same IMPT again. Due to
297
Table 1: Surface forms generated from the same IMPT (de-
sired updates = {hasSubject(event6, ?Vorlesung
Linguistik?)}; KB = {event6}) but with different
levels of verbosity.
Verbosity Generated sub-utterance chunk
0 ?Vorlesung Linguistik?
(lecture Linguistcs)
1 ?Betreff: Vorlesung Linguistik?
(subject: lecture Linguistics)
2 ?mit dem Betreff Vorlesung Linguistik?
(with the subject: lecture Linguistics)
changes in the state information and situation that
influence microplanning, the resulting communica-
tive intent and surface form might then differ from
its previous result.
3.4 Adaptation mechanisms
As a proof of concept, we integrated several adapta-
tion mechanism into our NLG-microplanning system.
The goal of these mechanisms is to respond to a dia-
logue partner?s changing abilities to perceive and/or
understand the information the system wants to con-
vey. Some of the mechanisms operate on the level of
MCP, others on the level of MPP. The mechanisms are
implemented either with the knowledge and its con-
versational status used in generation or by altering
the decision structure of SPUD?s search algorithm?s
heuristic function. Similar to the approach of flexi-
ble NLG described by Walker et al (2007), most of
the mechanism are conditioned upon individual flags,
that in our case depend on a numeric value that repre-
sents the level of understanding the system attributes
to the user. Here we describe the two most relevant
mechanisms used to adapt verbosity and redundancy.
Verbosity The first mechanism aims at influenc-
ing the length of a sub-utterance chunk by making
it either more or less verbose. The idea is that actual
language use of human speakers seldom adheres to
the idealised principle of textual economy. This is
not only the case for reasons of cognitive constraints
during speech production, but also because words
and phrases that do not contribute much to an utter-
ance?s semantics can serve a function, for example by
drawing attention to specific aspects of an utterance
or by giving the listener time to process.
To be able to vary utterance verbosity, we anno-
tated the linguistic resources in our system with val-
ues of their verbosity (these are hand-crafted similar
to the rule?s annotation with production costs). Dur-
ing generation in MPP the values of all linguistic re-
sources used in a (provisional) utterance are added up
and used as one factor in SPUD?s heuristic function.
When comparing two provisional utterances that only
deviate in their verbosity value, the one that is nearer
to a requested verbosity level is chosen. Depend-
ing on this level, more or less verbose constructions
are chosen and it is decided whether sub-utterance
chunks are enriched with additional words. Table 1
shows the sub-utterance chunk ?Betreff: Vorlesung
Linguistik? (subject: lecture Linguistics) generated
with different levels of verbosity.
Redundancy The second adaptation mechanism is
redundancy. Again, redundancy is something that an
ideal utterance does not contain and by design SPUD
penalises the use of redundancy in its heuristic func-
tion. Two provisional utterances being equal, the one
exhibiting less redundancy is normally preferred. But
similar to verbosity, redundancy serves communica-
tive functions in actual language use. It can highlight
important information, it can increase the probability
of the message being understood (Reiter and Sripada,
2002) and it is often used to repair misunderstanding
(Baker et al, 2008).
In incremental microplanning, redundant informa-
tion can be present both within one sub-utterance
chunk (e. g., ?tomorrow, March 26, . . . ? vs. ?tomorrow
. . . ?) or across IMPTs. For the former case, we modi-
fied SPUD?s search heuristic in order to conditionally
either prefer an utterance that contains redundant in-
formation or an utterance that only contains what is
absolutely necessary. In the latter case, redundancy
only becomes an option when later IMPTs enable the
choice of repeating information previously conveyed
and therefore already established as shared knowl-
edge. This is controlled via the internal structure of
an IMPT and thus decided on the level of MCP.
4 Incremental Speech Synthesis
In this section we describe our component for incre-
mental speech synthesis. We extend Edlund?s (2008)
requirements specification cited in Section 2, requir-
ing additionally that an iSS supports changes to as-yet
298
unspoken parts of an ongoing utterance.
We believe that the iSS?s requirements of inter-
ruptability, changeability, responsiveness, and feed-
back are best resolved by a processing paradigm in
which processing takes place just-in-time, i. e., tak-
ing processing steps as late as possible such as to
avoid re-processing if assumptions change. Before
we describe these ideas in detail, we give a short
background on speech synthesis in general.
4.1 Background on speech synthesis
Text-to-speech (TTS) synthesis functions in a top-
down processing approach, starting on the utterance
level and descending onto words and phonemes, in
order to make good decisions (Taylor, 2009). For
example, top-down modelling is necessary to assign
stress patterns and sentence-level intonation which
ultimately lead to pitch and duration contours, and to
model co-articulation effects.
TTS systems start out assigning intonation patterns
to the utterance?s words and then generate a target
phoneme sequence which is annotated with the tar-
gets? durations and pitch contour; all of this is called
the linguistic pre-processing step. The synthesis step
proper can be executed in one of several ways with
HMM-based and unit-selection synthesis currently
producing the perceptually best results.
In HMM-based synthesis, the target sequence is
first turned into a sequence of HMM states. A global
optimisation then determines a stream of vocoding
features that optimise both HMM emission probabili-
ties and continuity constraints (Tokuda et al, 2000).
The stream may also be enhanced to consider global
variance of features (Toda and Tokuda, 2007). The
parameter frames are then fed to a vocoder which
generates the final speech audio signal.
Unit-selection, in contrast, searches for the best
sequence of (variably sized) units of speech in a
large, annotated corpus, aiming to find a sequence
that closely matches the target sequence while having
few and if possible smooth joints between units.
We follow the HMM-based approach for our com-
ponent for the following reasons: (a) even though
only global optimisation is optimal for both tech-
niques, the influence of look-ahead on the continuity
constraints of HMM-based synthesis is linear leading
to a linear loss in optimality with smaller look-aheads
(whereas unit-selection with limited look-ahead may
Figure 2: Hierarchical structure of incremental units de-
scribing an example utterance as it is being produced
during delivery.
jump erratically between completely different unit se-
quences). (b) HMM-based synthesis nicely separates
the production of vocoding parameter frames from
the production of the speech audio signal which al-
lows for fine-grained concurrent processing (see next
subsection). (c) Parameters in the vocoding frames
are partially independent. This allows us to indepen-
dently manipulate, e. g., pitch without altering other
parameters or deteriorating speech quality (in unit-
selection, a completely different unit sequence might
become optimal even for slight changes of pitch).
4.2 Incrementalising speech synthesis
As explained in the previous subsection, speech syn-
thesis is performed top-down, starting at the utterance
and progressing down to the word, target and finally,
in the HMM approach, vocoding parameter and signal
processing levels. It is, however, not necessary that
all details at one level of processing are worked out
before starting to process at the next lower level. To
be precise, some syntactic structure is sufficient to
produce sentence-level intonation, but all words need
not be known. Likewise, post-lexical phonological
processes can be computed as long as a local context
of one word is available and vocoding parameter com-
putation (which must model co-articulation effects)
should in turn be satisfied with about one phoneme of
context. Vocoding itself does not need any lookahead
at all (aside from audio buffering considerations).
Thus, we generate our data structures incremen-
tally in a top-down and left-to-right fashion with dif-
ferent amounts of pre-planning and we do this using
several processing modules that work concurrently.
This results in a ?triangular? structure as shown in
299
Figure 2. At the top stands a pragmatic plan for the
full utterance from which a syntactic plan can be de-
vised. This plan is filled with words, as they become
available. On the vocoding parameter level, only a
few frames into the future have been computed so
far ? even though much more context is already avail-
able. That is, we generate structure just-in-time, only
shortly before it is needed by the next processor. This
holds very similarly for the vocoding step that pro-
duces the speech signal just-in-time.
The just-in-time processing approach, combined
with the increasing temporal granularity of units to-
wards the lower levels has several advantages: (a) lit-
tle utterance-initial processing (only what is neces-
sary to produce the beginning of the signal) allows for
very responsive systems; and (b) changes to the ini-
tial plan result only in a modest processing overhead
because little structure has to be re-computed.
4.3 Technical overview
As a basis, we use MaryTTS (Schr?der and Trouvain,
2003), but replace Mary?s internal data structures
and processing strategies with structures from our
incremental SDS architecture, the INPROTK toolkit
(Schlangen et al, 2010; Baumann and Schlangen,
2012b), which implements the IU model for incre-
mental dialogue processing (Schlangen and Skantze,
2009). The model conceptualises ? and the toolkit
implements ? incremental processing as the process-
ing of incremental units (IUs), which are the smallest
?chunks? of information at a specific level (the boxes
in Figure 2). IUs are interconnected to form a network
(e. g., words keep links to their associated phonemes
and neighbouring words and vice-versa) which repre-
sents the system?s information state.
The component is fed with chunk IUs which con-
tain some words to be synthesised (on their own or
appended to an ongoing utterance). For simplicity,
all units below the chunk level are currently gener-
ated using Mary?s (non-incremental) linguistic pre-
processing capabilities to obtain the target phoneme
sequence. For continuations, the preceding parts of
the utterance are taken into account when generating
prosodic characteristics for the new chunk. Also, our
component is able to revoke and exchange chunks
(or unspoken parts thereof) to change what is to be
spoken; this capability however is not used in the
example system presented in Section 5.
The lowest level module of our component is what
may be called a crawling vocoder, which actively
moves along the phoneme IU layer and executes two
processing steps: (a) for each phoneme it generates
the sequence of HMM parameter frames using a local
optimisation technique (using up to four neighbour-
ing phonemes as context) similar to the one described
by Dutoit et al (2011); and (b) vocoding the HMM
parameters into an audio stream which contains the
actual speech signal.
IUs have a ?progress? field which is set by the
crawling vocoder to one of ?upcoming?, ?ongoing?,
or ?completed?, as applicable. IUs provide a generic
update mechanism to support notification about
progress changes in delivery. The next section de-
scribes how this is used to drive the system.
5 Integrating iNLG and iSS for Adaptive
Information Presentation
Integrating incremental microplanning with incre-
mental speech synthesis in one incremental output
generation architecture allows us to test and explore
how their capabilities act in a coordinated way. As a
first example, we implemented a system that presents
information about events in an appointment database
(e. g., new, conflicting or rescheduled appointments)
and is able to cope with external noise burst events,
as they might for example occur on a bad telephone
line or when using a dialogue system next to a busy
street. The focus is on the incremental capabilities of
the system which enable its adaptive behaviour.
5.1 Component interplay
iNLG and iSS are implemented as IU modules in the
INPROTK architecture. The control flow of the sys-
tem (Figure 3) is managed without special coupling
between the modules, relying only on the left-to-right
processing capabilities of INPROTK combined with
its generic IU update mechanism for transporting
feedback from iSS to iNLG. Both modules can be
(and have been) combined with other IU modules.
To communicate an appointment event, the iNLG
module starts by generating two initial chunk IUs,
the first to be expressed immediately, the second as
additional prosodic context (chunk lengths differ with
an average of about 4 words). The iNLG registers as a
?progress listener? on each chunkIU, which registers
300
Figure 3: Information flow (dashed lines) between iNLG
and iSS components (rounded boxes) and incremental
units (rectangular boxes). The vocoder crawls along with
time and triggers the updates.
as a progress listener on a phonemeIUnear its end.
Shortly before iSS finishes speaking the chunk, iNLG
is thus informed and can generate and send the next
chunk to iSS just-in-time.
If adaptation to noise is needed, iNLG re-generates
and re-sends the previous chunk, taking altered pa-
rameters into account. Again, a subsequent chunk
is immediately pre-generated for additional prosodic
context. This way of generating sub-utterance chunks
ensures that there is always one chunk lookahead to
allow the iSS module to compute an adequate in-
tonation for the current chunk, while maintaining
the single chunk as increment size for the system
and minimising redundant work on the side of iNLG
(this lookahead is not required for iSS; but if it is un-
available, sub-utterance chunks may be inadequately
connected prosodically).
5.2 Responding to a noise event
A third module, the noise detector connects to both
iSS and iNLG. On noise onset, it informs iSS to inter-
rupt the ongoing utterance after the current word (this
works by breaking the links between words so that
the crawling vocoder finishes after the currently ongo-
ing word). Once a noise burst ends, iNLG is informed,
re-generates the interrupted sub-utterance chunk with
the verbosity level decreased by one and the assumed
understanding value increased by one (this degree
of adaptation results in a noticeable difference, it is,
however, not based on empirical study). The values
are then reset, the following chunk is generated and
both chunks are sent to iSS.
It should be noted, that we have not implemented
a real noise source and noise detector. Instead, our
random noise simulator generates bursts of noise of
1000 ms after a random time interval (between 2 and
Table 2: Processing time per processing step before deliv-
ery can begin (in ms; averaged over nine stimuli taking the
median of three runs for each stimulus; calculated from
log messages; code paths preheated for optimisation).
non-incr. incr.
NLG-microplanning 361 52
Synthesis (ling. pre-processing) 217 4472
Synthesis (HMM and vocoding) 1004 21
total response time 1582 519
5 seconds) and directly informs the system 300 ms
after noise starts and ends. We think it is reasonable
to assume that a real noise detector should be able to
give accurate information with a similar delay.
6 Evaluation
6.1 Quantitative evaluation
One important argument in favour of incremental
processing is the possibility of speeding up system
response time, which for non-incremental systems
is the sum of the times taken by all processors to
do their work. An incremental system, in contrast,
can fold large amounts of its processing time into the
ongoing speech output; what matters is the sum of
the onset times of each processor, i. e., the time until
a first output becomes available to the next processor.
Table 2 summarises the runtime for the three major
steps in output production of our system using nine
utterances from our domain. Both NLG and speech
synthesis? onset times are greatly reduced in the in-
cremental system.2 Combined, they reduce system
response time by more than a second. This is mostly
due to the almost complete folding of HMM opti-
misation and vocoding times into the spoken utter-
ance. NLG profits from the fact that at the beginning
of an utterance only two chunks have to be gener-
ated (instead of an average of 6.5 chunks in the non-
incremental system) and that the first chunk is often
very simple.
6.2 Subjective evaluation
To further test whether the system?s behaviour in
noisy situations resembles that of a human speaker
2The iSS component by mistake takes the symbolic pre-
processing step twice. Unfortunately, we found this bug only
after creating the stimuli used in the subjective evaluation.
301
in a similar situation, we let humans rate utterances
produced by the fully incremental, adaptive system
and utterances produced by two non-incremental
and less responsive variants (we have not used non-
incremental TTS in combination with iNLG as another
possible base-line as pretests showed this to sound
very unnatural due to the missing prosodic linkage be-
tween phrases). The participants were to rate whether
they agree to the statement ?I found the behaviour of
the system in this situation as I would expect it from
a human speaker? on a 7-point Likert-scale.
In condition A, full utterances were generated non-
incrementally, synthesised non-incrementally and
played without responding to noise-interruptions in
the channel (as if the system did not notice them).
Utterances in condition B were generated and synthe-
sised as in condition A, but playback responded to the
noisy channel, stopping when the noise was noticed
and continuing when noise ended. For condition C,
utterances were generated with the fully incremental
and adaptive system described in Section 5. Upon
noise detection, speech synthesis is interrupted and,
when the noise ends, iNLG will re-generate the in-
terrupted sub-utterance chunk ? using the adaptation
strategy outlined in Section 5.2. This then triggers
iSS into action and shortly after, the system contin-
ues speaking. Nine system runs, each producing a
different utterance from the calendar domain, were
recorded in each of the three conditions, resulting in
a total of 27 stimuli.
Before the actual stimuli were presented, partici-
pants listened to two example stimuli without noise
interruptions to get an impression of how an aver-
age utterance produced by the system sounds. After
the presentation of these two examples, the 27 stim-
uli were presented in the same random order. Par-
ticipants listened once to each stimulus and rated it
immediately after every presentation.
Twelve PhD-students (3 female, 9 male; mean age
30.5 years; 11 with German as one of their first lan-
guages; none with uncorrected hearing impairment)
from Bielefeld University participated in our study
and listened to and rated the 27 stimuli.
A Friedman rank sum test revealed a highly sig-
nificant difference between the perceived human-
likeness of the three systems (?2 = 151, p < .0001).
Median values of stimulus ratings in the conditions
A, B and C were 2, 2 and 6 respectively, indicat-
ing that the fully incremental system was rated con-
siderably more human-like. This was also shown
through a post-hoc analysis with Wilcoxon signed
rank tests which found no significant difference be-
tween condition A and B (V = 1191.5, p = .91)3.
Conditions A and C, however, differed highly signifi-
cantly (V = 82, p < .0001), as did conditions B and
C (V = 22.5, p < .0001) ? even after applying a Bon-
ferroni correction to correct for a possible cumulation
of ?-errors.
7 Conclusion
We have presented what is ? to the best of our knowl-
edge ? the first integrated component for incremental
NLG and speech synthesis and demonstrated the flex-
ibility that an incremental approach to output gener-
ation for speech systems offers by implementing a
system that can repair understanding problems.
From the evaluation we can conclude that incre-
mental output generation (both iNLG and iSS in iso-
lation or combined) is able to greatly speed up sys-
tem response time and can be used as a means to
speed up system response even in an otherwise non-
incremental system. Furthermore, we showed that the
behaviour of our fully incremental and adaptive sys-
tem was perceived as significantly more human-like
than the non-incremental and the non-incremental
but responsive baseline systems.
The understanding problem that our demonstra-
tor system tackled was of the simplest kind, namely
acoustic non-understanding, objectively detectable
as the presence of noise. In principle, however, the
same mechanisms of stopping and rephrasing can be
used to tackle more subjective understanding prob-
lems as can be signalled by linguistic feedback. Our
incremental output generation component gives us an
ideal basis to explore such problems in future work.
Acknowledgements This research is partially sup-
ported by the Deutsche Forschungsgemeinschaft
(DFG) in the Center of Excellence in ?Cognitive Inter-
action Technology? (CITEC) and through an Emmy
Noether Fellowship to the last author.
3This suggests that it does not matter whether a system re-
sponds to problems in the communication channel by waiting or
totally ignores these problems. Notice, however, that we did not
test recall of the calendar events. In that case, condition B should
outperform A, as some information was clearly inaudible in A.
302
References
Jens Allwood, Joakim Nivre, and Elisabeth Ahls?n. 1992.
On the semantics and pragmatics of linguistic feedback.
Journal of Semantics, 9:1?26.
Rachel Baker, Alastair Gill, and Justine Cassell. 2008.
Reactive redundancy and listener comprehension in
direction-giving. In Proceedings of the 9th SIGdial
Workshop on Discourse and Dialogue, pages 37?45,
Columbus, OH.
Timo Baumann and David Schlangen. 2012a. INPRO_iSS:
A component for just-in-time incremental speech syn-
thesis. In Proceedings of ACL System Demonstrations,
Jeju, South Korea.
Timo Baumann and David Schlangen. 2012b. The
INPROTK 2012 release. In Proceedings of the NAACL-
HLT Workshop on Future directions and needs in the
Spoken Dialog Community: Tools and Data, pages 29?
32, Montr?al, Canada.
Hendrik Buschmeier and Stefan Kopp. 2011. Towards
conversational agents that attend to and adapt to com-
municative user feedback. In Proceedings of the 11th
International Conference on Intelligent Virtual Agents,
pages 169?182, Reykjavik, Iceland.
David DeVault. 2008. Contribution Tracking: Partici-
pating in Task-oriented Dialogue Under Uncertainty.
Ph.D. thesis, Rutgers, The State University of New Jer-
sey, New Brunswick, NJ.
Thierry Dutoit, Maria Astrinaki, Onur Babacan, Nicolas
d?Alessandro, and Benjamin Picart. 2011. pHTS for
Max/MSP: A streaming architecture for statistical para-
metric speech synthesis. Technical Report 1, numediart
Research Program on Digital Art Technologies, Mons,
Belgium.
Jens Edlund. 2008. Incremental speech synthesis. In
Second Swedish Language Technology Conference,
pages 53?54, Stockholm, Sweden, November. System
Demonstration.
Markus Guhe. 2007. Incremental Conceptualization for
Language Production. Lawrence Erlbaum, Mahwah,
NJ.
Christine Howes, Matthew Purver, Patrick G. T. Healey,
Gregory Mills, and Eleni Gregoromichelaki. 2011. On
incrementality in dialogue: Evidence from compound
contributions. Discourse & Dialogue, 2:279?311.
Gerard Kempen and Edward Hoenkamp. 1987. An incre-
mental procedural grammar for sentence formulation.
Cognitive Science, 11:201?258.
Anne Kilger and Wolfgang Finkler. 1995. Incremen-
tal generation for real-time applications. Technical
Report RR-95-11, Deutsches Forschungszentrum f?r
K?nstliche Intelligenz, Saarbr?cken, Germany.
Willem J. M. Levelt. 1989. Speaking: From Intention to
Articulation. The MIT Press, Cambridge, UK.
Kyoko Matsuyama, Kazunori Komatani, Ryu Takeda,
Toru Takahashi, Tetsuya Ogata, and Hiroshi G. Okuno.
2010. Analyzing user utterances in barge-in-able spo-
ken dialogue system for improving identification accu-
racy. In Proceedings of INTERSPEECH 2010, pages
3050?3053, Makuhari, Japan.
Ehud Reiter and Somayajulu Sripada. 2002. Human vari-
ation and lexical choice. Computational Linguistics,
28:545?553.
David Schlangen and Gabriel Skantze. 2009. A general,
abstract model of incremental dialogue processing. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 710?718, Athens, Greece.
David Schlangen, Timo Baumann, Hendrik Buschmeier,
Okko Bu?, Stefan Kopp, Gabriel Skantze, and Ramin
Yaghoubzadeh. 2010. Middleware for incremental
processing in conversational agents. In Proceedings of
SIGdial 2010: the 11th Annual Meeting of the Special
Interest Group in Discourse and Dialogue, pages 51?
54, Tokyo, Japan.
Marc Schr?der and J?rgen Trouvain. 2003. The Ger-
man text-to-speech synthesis system MARY: A tool
for research, development and teaching. International
Journal of Speech Technology, 6:365?377.
Gabriel Skantze and Anna Hjalmarsson. 2010. Towards
incremental speech generation in dialogue systems. In
Proceedings of SIGDIAL 2010: the 11th Annual Meet-
ing of the Special Interest Group on Discourse and
Dialogue, pages 1?8, Tokyo, Japan.
Matthew Stone, Christine Doran, Bonnie Webber, Tonia
Bleam, and Martha Palmer. 2003. Microplanning with
communicative intentions: The SPUD system. Compu-
tational Intelligence, 19:311?381.
Matthew Stone. 2002. Lexicalized grammar 101. In
Proceedings of the ACL-02 Workshop on Effective Tools
and Methodologies for Teaching Natural Language
Processing and Computational Linguistics, pages 77?
84, Philadelphia, PA.
Paul Taylor. 2009. Text-to-Speech Synthesis. Cambridge
Univ Press, Cambridge, UK.
Tomoki Toda and Keiichi Tokuda. 2007. A speech param-
eter generation algorithm considering global variance
for HMM-based speech synthesis. IEICE TRANSAC-
TIONS on Information and Systems, 90:816?824.
Keiichi Tokuda, Takayoshi Yoshimura, Takashi Masuko,
Takao Kobayashi, and Tadashi Kitamura. 2000.
Speech parameter generation algorithms for HMM-
based speech synthesis. In Proceedings of ICASSP
2000, pages 1315?1318, Istanbul, Turkey.
Marylin Walker, Amanda Stent, Fran?ois Mairesse, and
Rashmi Prasad. 2007. Individual and domain adap-
tation in sentence planning for dialogue. Journal of
Artificial Intelligence Research, 30:413?456.
303
NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 29?32,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
The INPROTK 2012 Release
Timo Baumann
Department for Informatics
University of Hamburg, Germany
baumann@informatik.uni-hamburg.de
David Schlangen
Faculty of Linguistics and Literary Studies
Bielefeld University, Germany
david.schlangen@uni-bielefeld.de
Abstract
We describe the 2012 release of our ?Incremen-
tal Processing Toolkit? (INPROTK)1, which
combines a powerful and extensible architec-
ture for incremental processing with compo-
nents for incremental speech recognition and,
new to this release, incremental speech syn-
thesis. These components work fairly domain-
independently; we also provide example imple-
mentations of higher-level components such as
natural language understanding and dialogue
management that are somewhat more tied to a
particular domain. We offer this release of the
toolkit to foster research in this new and excit-
ing area, which promises to help increase the
naturalness of behaviours that can be modelled
in such systems.
1 Introduction
As recent work has shown, incremental (or online)
processing of user input or generation of system
output enables spoken dialogue systems to produce
behaviour that is perceived as more natural than
and preferable to that produced by systems that are
bound by a turn-based processing mode (Aist et
al., 2006; Skantze and Schlangen, 2009; Bu? et al,
2010; Skantze and Hjalmarsson, 2010). There is still
much left to find out about the best ways of mod-
elling these behaviours in such systems, however.
To foster research in this area, we are releasing a
new version of our ?Incremental Processing Toolkit?
(INPROTK), which provides lower-level components
(such as speech recognition and speech synthesis,
1The code of the toolkit and some example applications
have been released as open-source at http://inprotk.
sourceforge.net.
but also a general modular processing architecture)
and allows researchers to concentrate on higher-level
modules (such as natural language understanding and
dialogue modelling; for which we provide example
implementations).2 We describe these components
in the following, pointing out the differences and
extensions to earlier releases (Baumann et al, 2010).
2 An Incremental Processing Architecture
INPROTK realises the IU-model of incremental pro-
cessing (Schlangen and Skantze, 2009; Schlangen
and Skantze, 2011), where incremental systems are
conceptualised as consisting of a network of pro-
cessing modules. Each module has a left buffer, a
processor, and a right buffer, where the normal mode
of processing is to take input from the left buffer, pro-
cess it, and provide output in the right buffer, from
where it goes to the next module?s left buffer. (Top-
down, expectation-based processing would work in
the opposite direction.) Modules exchange incremen-
tal units (IUs), which are the smallest ?chunks? of
information that can trigger connected modules into
action. IUs typically are part of larger units; e.g.,
individual words as parts of an utterance, or frame
elements as part of the representation of an utterance
meaning. This relation of being part of the same
larger unit is recorded through same level links; the
units that were used in creating a given IU are linked
to it via grounded in links. Modules have to be able
to react to three basic situations: that IUs are added
to a buffer, which triggers processing; that IUs that
were erroneously hypothesised by an earlier module
2An alternative to the toolkit described here is jindigo
(Skantze and Hjalmarsson, 2010), http://www.jindigo.
net.
29
are revoked, which may trigger a revision of a mod-
ule?s own output; and that modules signal that they
commit to an IU, that is, won?t revoke it anymore (or,
respectively, expect it to not be revoked anymore).
INPROTK offers flexibility on how tightly or
loosely modules are coupled in a system. It pro-
vides mechanisms for sending IU updates between
processes via a light-weight remote procedure call
protocol,3 as well as for using shared memory within
one (Java) process. INPROTK follows an event-based
model, where modules create events, for which other
modules can register as listeners. Module networks
are configured via a system configuration file which
specifies which modules listen to which.
As opposed to our previous release (Baumann et
al., 2010), INPROTK module communication is now
completely encapsulated in the IUModule class. An
implementing processor is called into action by a
method which gives access both to the edits to IUs
in the left buffer since the last call, and to the list of
IUs directly. The implementing processor must then
notify its right buffer, either about the edits to the
right buffer, or giving the content directly. Modules
can be fully event-driven, only triggered into action
by being notified of a hypothesis change, or they
can run persistently, in order to create endogenous
events like time-outs. Event-driven modules can run
concurrently in separate threads or can be called se-
quentially by another module (which may seem to
run counter the spirit of incremental processing, but
can be advantageous for very quick computations
for which the overhead of creating threads should be
avoided). In the case of separate threads, which run
at different update intervals, the left-buffer view will
automatically be updated to its most recent state.
INPROTK also comes with an extensive set of mon-
itoring and profiling modules which can be linked
into the module network at any point and allow to
stream data to disk or to visualise it online through a
viewing tool (von der Malsburg et al, 2009), as well
as different ways to simulate input (e.g., typed or
read from a file) for debugging. All IUmodules can
also output loggging messages to the viewing tool
directly (to ease graphic debugging of error cases in
multi-threaded applications).
3In an earlier release, we used OAA (Cheyer and Martin,
2001), which however turned out to be too slow.
3 Incremental Speech Recognition
Our speech recognition module is based on the
Sphinx-4 (Walker et al, 2004) toolkit and comes with
acoustic models for German.4 The module queries
the ASR?s current best hypothesis after each frame of
audio and changes its output accordingly, adding or
revoking WordIUs and notifying its listeners. Addi-
tionally, for each of the WordIUs, SyllableIUs and
SegmentIUs are created and bound to the word (and
to the syllable respectively) via the grounded-in hier-
archy. Later modules in the pipeline are thus able to
use this lower-level information (e.g. to disambiguate
meaning based on prosodic aspects of words). For
prosodic processing, we inject additional processors
into Sphinx? acoustic frontend which provide features
for further prosodic processing (pitch, loudness, and
spectral tilt). In this way, IUs are able to access the
precise acoustic data (in raw and processed forms).
An ASR?s current best hypothesis frequently
changes during the recognition process with the ma-
jority of the changes not improving the result. Every
such change triggers all listening modules (and pos-
sibly their listeners), resulting in a lot of unnecessary
processing. Furthermore, changes may actually dete-
riorate results, if a ?good? hypothesis is intermittently
changed for worse. Therefore, we developed hypoth-
esis smoothing approaches (Baumann et al, 2009)
which greatly reduce spurious edits in the output at
the cost of some timeliness: With a lag of 320ms we
reduced the amount of spurious edits to 10% from an
initial 90%. The current implementation of hypothe-
sis smoothing is taylored specifically towards ASR
output, but other input modules (like gesture or facial
expression recognition) could easily be smoothed
with similar methods.
4 Incremental NLU and DM
As mentioned above, the more ?higher-level? com-
ponents in our toolkit are more domain-specific than
the other components, and in any case are proba-
bly exactly those modules which users of the toolkit
may want to substitute with their own. Neverthe-
less, we provide example implementations of a sim-
ple keyword-spotting ?NLU?, as well as statistically
4Models for English, French and other languages
are available from the Sphinx? distribution and from
http://www.voxforge.org.
30
trained ones (Schlangen et al, 2009; Heintze et al,
2010).
We have recently built a somewhat more traditional
NLU component which could be more easily ported
to other domains (by adapting lexicon and grammar).
It consists of a probabilistic, beam-search top-down
parser (following (Roark, 2001)), which produces
a principled semantic representation in the formal-
ism robust minimal recursion semantics (Copestake,
2006). This component is described in more detail in
(Peldszus et al, 2012).
5 Incremental Speech Synthesis
Rounding out the toolkit is our new component for in-
cremental speech synthesis, which has the following
properties:
(a) It makes possible changes to the as-yet unspoken
part of the ongoing utterance,
(b) allows adaptations of delivery parameters such
as speaking rate or pitch with very low latency.
(c) It autonomously makes delivery-related deci-
sions (such as producing hesitations), and
(d) it provides information about delivery status (e. g.
useful in case of barge-ins).
(e) And, last but not least, it runs in real time.
Figure 1 provides a look into the internal data
structures of the component, showing a triangular
structure where on successive levels structure is built
just-in-time (e.g., turning target phoneme sequences
into vocoding parameters) and hence can be changed
with low cost, if necessary. We have evaluated the
component in an application scenario where it proved
to increase perceived naturalness, and have also stud-
ied the tradeoff between look-ahead and prosodic
quality. To this end, Figure 2 plots the deviation of
the prosodic parameters pitch and timing from that
of a non-incremental synthesis of the same utterance
versus the amount of look-ahead, that is, how far into
the current phrase the next phrase becomes known. It
shows that best results are achieved if the next phrase
that is to be synthesized becomes known no later than
one or two words into the current phrase (w0 or w1).
6 Evaluation of Incremental Processors
While not part of the toolkit proper, we think that it
can only be useful for the field to agree on common
evaluation metrics. Incremental processing brings
Figure 1: Hierarchic structure of incremental units describ-
ing an example utterance as it is being produced during
delivery, showing the event-based just-in-time processing
strategy.
0
10
20
30
l l l
l l l
w0 w1 w2 w3 wn?1 wn
l
l
pitch dev.
timing dev.
Figure 2: Deviation of pitch and timing plotted against
lookahead (right context available for incremental synthe-
sis). The more lookahead available, the better the results.
new considerations of dynamics into the assessment
of processing quality, and hence requires additional
metrics compared to non-incremental processing. In
(Baumann et al, 2011) we have proposed a family
of such metrics, and we provide an evaluation frame-
work for analysing incremental ASR performance as
part of our distribution.
7 Conclusions
We have sketched the major features of our ?Incre-
mental Processing Toolkit? INPROTK. While it is far
from offering ?plug-and-play? ease of constructing
incremental dialogue systems, we hope it will prove
useful for other researchers insofar as it offers solu-
tions to the more low-level problems that often are
not one?s main focus, but which need solving any-
ways before more interesting things can be done. We
look forward to what these interesting things may be
that others will build.
31
Acknowledgments
Most of the work decribed in this paper was funded
by a grant from DFG in the Emmy Noether Pro-
gramme.
References
G.S. Aist, J. Allen, E. Campana, L. Galescu, C.A.
Gomez Gallo, S. Stoness, M. Swift, and M Tanen-
haus. 2006. Software architectures for incremental
understanding of human speech. In Proceedings of the
International Conference on Spoken Language Process-
ing (ICSLP), Pittsburgh, PA, USA, September.
Timo Baumann, Michaela Atterer, and David Schlangen.
2009. Assessing and improving the performance of
speech recognition for incremental systems. In Pro-
ceedings of the North American Chapter of the Associa-
tion for Computational Linguistics - Human Language
Technologies (NAACL HLT) 2009 Conference, Boulder,
Colorado, USA, May.
Timo Baumann, Okko Bu?, and David Schlangen. 2010.
InproTK in action: Open-source software for building
german-speaking incremental spoken dialogue systems.
In Proceedings of ESSV 2010, Berlin, Germany.
Timo Baumann, Okko Bu?, and David Schlangen. 2011.
Evaluation and optimization of incremental processors.
Dialogue and Discourse, 2(1):113?141.
Okko Bu?, Timo Baumann, and David Schlangen. 2010.
Collaborating on utterances with a spoken dialogue
system using an isu-based approach to incremental dia-
logue management. In Proceedings of the SIGdial 2010
Conference, pages 233?236, Tokyo, Japan, September.
Adam Cheyer and David Martin. 2001. The open agent
architecture. Journal of Autonomous Agents and Multi-
Agent Systems, 4(1):143?148, March. OAA.
Ann Copestake. 2006. Robust minimal recursion se-
mantics. Technical report, Cambridge Computer Lab.
Unpublished draft.
Silvan Heintze, Timo Baumann, and David Schlangen.
2010. Comparing local and sequential models for sta-
tistical incremental natural language understanding. In
Proceedings of the SIGdial 2010 Conference, pages
9?16, Tokyo, Japan, September.
Andreas Peldszus, Okko Bu?, Timo Baumann, and David
Schlangen. 2012. Joint satisfaction of syntactic and
pragmatic constraints improves incremental spoken lan-
guage understanding. In Proceedings of the Confer-
ence of the European Association for Computational
Linguistics (EACL 2012), Avignon, France, April.
Brian Roark. 2001. Robust Probabilistic Predictive Syn-
tactic Processing: Motivations, Models, and Appli-
cations. Ph.D. thesis, Department of Cognitive and
Linguistic Sciences, Brown University.
David Schlangen and Gabriel Skantze. 2009. A general,
abstract model of incremental dialogue processing. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL 2009), pages 710?718, Athens, Greece,
March.
David Schlangen and Gabriel Skantze. 2011. A gen-
eral, abstract model of incremental dialogue processing.
Dialogue and Discourse, 2(1):83?111.
David Schlangen, Timo Baumann, and Michaela Atterer.
2009. Incremental reference resolution: The task, met-
rics for evaluation, and a bayesian filtering model that
is sensitive to disfluencies. In Proceedings of SIGdial
2009, the 10th Annual SIGDIAL Meeting on Discourse
and Dialogue, London, UK, September.
Gabriel Skantze and Anna Hjalmarsson. 2010. Towards
incremental speech generation in dialogue systems. In
Proceedings of the SIGdial 2010 Conference, pages
1?8, Tokyo, Japan, September.
Gabriel Skantze and David Schlangen. 2009. Incremental
dialogue processing in a micro-domain. In Proceedings
of the 12th Conference of the European Chapter of
the Association for Computational Linguistics (EACL
2009), pages 745?753, Athens, Greece, March.
Titus von der Malsburg, Timo Baumann, and David
Schlangen. 2009. Telida: A package for manipulation
and visualisation of timed linguistic data. In Proceed-
ings of the Poster Session at SIGdial 2009, the 10th
Annual SIGDIAL Meeting on Discourse and Dialogue,
London, UK, September.
Willie Walker, Paul Lamere, Philip Kwok, Bhiksha Raj,
Rita Singh, Evandro Gouvea, Peter Wolf, and Joe
Woelfel. 2004. Sphinx-4: A flexible open source
framework for speech recognition. Technical Report
SMLI TR2004-0811, Sun Microsystems Inc.
32
NAACL-HLT 2012 Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 56?65,
Montre?al, Canada, June 7?8, 2012. c?2012 Association for Computational Linguistics
Generating Situated Assisting Utterances to Facilitate Tactile-Map
Understanding: A Prototype System
Kris Lohmann, Ole Eichhorn, and Timo Baumann
Department of Informatics, University of Hamburg
Vogt-Ko?lln-Stra?e 30
22527 Hamburg, Germany
{lohmann,9eichhor,baumann}@informatik.uni-hamburg.de
Abstract
Tactile maps are important substitutes for vi-
sual maps for blind and visually impaired peo-
ple and the efficiency of tactile-map reading
can largely be improved by giving assisting ut-
terances that make use of spatial language. In
this paper, we elaborate earlier ideas for a sys-
tem that generates such utterances and present
a prototype implementation based on a seman-
tic conceptualization of the movements that the
map user performs. A worked example shows
the plausibility of the solution and the output
that the prototype generates given input derived
from experimental data.
1 Introduction
Humans use maps in everyday scenarios. Especially
for blind and visually impaired people, tactile maps
are helpful accessible substitutes for visual maps
(Espinosa, Ungar, Ochaita, Blades, & Spencer, 1998;
Ungar, 2000). However, tactile maps are less efficient
than visual maps, as they have to be read sequen-
tially. A further problem of physical tactile maps is
restricted availability. While physical tactile maps are
rarely available and costly to produce, modern haptic
human-computer interfaces can be used to present
virtual variants of tactile maps (virtual tactile maps)
providing a similar functionality. For example, the
Sensable Phantom Omni device used in our research
enables a user to feel virtual three-dimensional ob-
jects (see Figure 1). It can be thought of as a reverse
robotic arm that makes virtual haptic perception pos-
sible by generating force feedback. In the context of
the research discussed, these objects are virtual tac-
tile maps. These consist of a virtual plane on which
streets and potential landmarks (such as buildings)
are presented as cavities.
In recent work, Habel, Kerzel, and Lohmann
(2010) have suggested a multi-modal map called
Verbally Assisting Virtual-Environment Tactile Map
(VAVETaM) with the goal to enable more efficient
acquisition of spatial survey (overview) knowledge
for blind and visually impaired people.
VAVETaM extends the approaches towards multi-
modal maps (see Section 2) by generating situated
spatial language. The prototype described reacts to
the user?s exploration movements more like a human
verbally assisting a tactile map reader would do, e.g.,
by describing spatial relations between objects on
the map. The users may explore the map freely, i.e.,
they choose which map objects are of interest and
in which order they explore them. This demands for
situated natural language generation (Roy & Reiter,
2005), which produces timely appropriate assisting
utterances. Previously, the suggested system has not
been implemented.
The goal of this paper is to show that the ideas of
Lohmann, Kerzel, and Habel (2010) and Lohmann,
Eschenbach, and Habel (2011) can be implemented
in a prototype which is able to generate helpful as-
sisting utterances; that is, to show that the language-
generation components of VAVETaM are technically
possible. The remainder of the paper is structured as
follows: We first briefly survey some related work
in Section 2, and then describe the overall structure
of VAVETaM in Section 3. We then present a de-
scription of our system in Section 4 paying special
attention to the input to natural language generation
(Subsection 4.1) and the generation component itself
(Subsection 4.2). We show the appropriateness of the
approach by discussing an example input, the pro-
56
Figure 1: The Sensable Omni Haptic Device and a Visual-
ization of a Virtual Tactile Map.
cesses performed, and the automatically generated
output in Section 5 before we close with concluding
remarks in Section 6.
2 Related Work
To make maps more accessible for visually impaired
people by overcoming drawbacks of uni-modal tac-
tile maps, a number of multi-modal systems that com-
bine haptics and sound have been developed. An
early system is the NOMAD system. It is based on
a traditional physical tactile map, which is placed
on a touch pad. The system allows for the associa-
tion of sound to objects on the map (Parkes, 1988,
1994). The approach to use traditional physical tac-
tile maps as overlays on touch pads has been used
in various systems that were developed subsequently
(e.g., Miele, Landau, & Gilden, 2006; Wang, Li,
Hedgpeth, & Haven, 2009). Overviews of research
on accessible maps for blind and visually impaired
people can be found in Buzzi, Buzzi, Leporini, and
Martusciello (2011) and in De Almeida (Vasconcel-
los) and Tsuji (2005). Other researchers have ad-
vanced the way haptic perception is realized by using
more flexible human-computer-interaction systems
that do not need physical tactile map overlays. For
example, Zeng and Weber (2010) have proposed an
audio-tactile system which is based on a large-scale
braille display and De Felice, Renna, Attolico, and
Distante (2007) presented the Omero system, which
makes use of a virtual haptic interface similar to the
interface used in our research.
Existing systems work on the basis of sounds or
canned texts that are associated to objects or areas
on the map. Sound playback starts when the user
touches a map object or, in some systems, by click-
ing or tapping on it. Yet, when humans are asked to
verbally assist a virtual tactile map explorer, they
produce assisting utterances in which they make
much more use of spatial language and give brief
augmenting descriptions of the objects that are cur-
rently explored and their surroundings (Lohmann et
al., 2011). Based on this, Lohmann and colleagues
suggest which informational content should be in-
cluded in assisting utterances for a tactile-map read-
ing task. Among the types of information that are
suggested for verbal assisting utterances is informa-
tion allowing for identification of objects, e.g., by
stating its name (e.g., ?This is 42nd Avenue?); in-
formation about the spatial relation of objects (?The
church is above the museum?); and talking about
the ends of streets that are explored (?This street is
restricted to the left by the map frame?).
Empirical (Wizard-of-Oz-like) research with 24
blindfolded sighted participants has concerned an
audio-tactile system that makes use of assisting ut-
terances containing the information discussed above
and shown its potential. Different outcome measures,
among them sketch maps and a verbal task, showed
an improved knowledge acquisition with verbal as-
sisting utterances compared to a baseline condition in
which participants verbally only received information
about the names of objects (Lohmann & Habel, forth-
coming). Empirical research with blind and visually
impaired people is ongoing. Data from the ongoing
experiment with blind and visually impaired partic-
ipants is used to show the function of the system in
Section 5.
3 The Structure of VAVETaM
In this section we will recap the overall structure of
VAVETaM as presented by Habel et al (2010) and
Lohmann et al (2010). Figure 2 depicts the relevant
parts of the structure.
The Virtual-Environment Tactile Map (VETM)
57
Audo
Ouput
Vrtual-Envronment
Tactle Map (VETM) 
Generaton of 
Verbal Assstance
(GVA)
MEP Observer
Formulaton & 
Artculaton
Map-Knowledge 
Reasonng (MKR) Components
Informaton Flow
Poston Data
of Haptc Devce
Figure 2: The Interaction of the Generation Components with Other Components of VAVETaM (modified version
following Habel et al, 2010).
knowledge base forms the basis for rendering the
tactile map, for analyzing movements, and for verbal-
izing assistive utterances forming the central knowl-
edge component in the architecture.
Knowledge needed for natural-language genera-
tion is represented in a propositional format which
is linked to knowledge needed for movement classi-
fication and for the haptic presentation of the map.
The latter is stored in a spatial-geometric, coordinate-
based format. The knowledge for assistance gener-
ation is represented using the Referential-Nets for-
malism developed by Habel (1986) and successfully
used by Guhe, Habel, and Tschander (2004) for nat-
ural language generation. Knowledge for verbaliza-
tion is organized by interrelated Referential Objects
(RefOs), which are the potential objects of discourse.
A referential object consists of an identifier for the
object (an arbitrary string, for example pt3), addi-
tional associated information such as the sort of the
object, and associated propositional information that
can be verbalized (such as the name of the object
and relations to other objects, e.g., that the object is
?left of? another object). Important sorts of objects
in the map domain are potential landmarks, regions,
the frame of the map, and tracks and track segments1.
See Lohmann et al (2011) for a discussion of the
propositional layer of the VETM knowledge base.
The Haptic Device provides a stream of position
data. This stream of data is the input to the Map-
Exploratory-Procedures Observer (MEP Observer)
component and its subcomponents which analyzes
the movements the map user performs. By categoriz-
ing the movements and specifying them with identi-
1A track is a structure enabling locomotion, such as a street.
The meaning of the term is similar to the meaning of the term
?path? introduced by Lynch (1960).
fiers of the objects currently explored by the user, a
conceptualization of the user?s movements is created
that is suitable as input to the component dealing
with assisting-utterance generation. For the case of
tactile-map explorations, different circumstances af-
fect which information shall be given via natural
language in an exploration situation: (a) what kind
of information is the user is trying to get (exploration
category), (b) about which object the user is trying
to get information, and (c) what has happened before
(history).
The Map-Knowledge Reasoning (MKR) compo-
nent serves as memory for both the MEP Observer
and the GVA component by keeping track of ver-
bal and haptic information that has been presented
to the user. This component hence helps to avoid
unnecessary verbal repetitions.
The Generation of Verbal Assistance (GVA) com-
ponent, which is at the core of the prototype that
we will present in Section 4, solves the central task
of natural language generation. This component se-
lects the knowledge that is suitable for verbalization
in an exploration situation from the VETM knowl-
edge base and prepares it in a way appropriate for
further output. It sends preverbal messages (PVMs,
see Levelt, 1989), propositional representations of
the semantics of the planned utterance, to the Formu-
lation & Articulation components for the generation
of a surface structure and final utterance.
4 Description of the Prototype
In order to show how an artificial system is able to
generate situated assistance in a well-formed fashion,
we present a prototype implementation of the core
components for natural language generation in the
VAVETaM system.
58
We implemented dummy components in place for
the Map-Knowledge Reasoning (MKR) and MEP
Observer components to allow us to test the natural
language output. The MKR Simulator provides basic
functions sufficient to avoid unnecessary repetitions
of utterances by preventing production of the same
message for a defined time period. An exception
to this rule are those messages that are needed to
identify an object on the map, such as ?This is Dorfs-
tra?e?, which are given every time the user touches
an object.2 The MEP Simulator generates input to
the component as the MEP Observer is planned to
do (see Kerzel & Habel, 2011, for a discussion of a
possible technical realization).
In the following subsection, we will discuss Map-
Exploratory Procedures (MEPs), which are output
by the MEP Observer and form the basic input to
the generation component (GVA), which we then
discuss in Subsection 4.2. Finally, we present the
inner workings of the Formulation & Articulation
components in Subsection 4.3.
4.1 Conceptualization of the User?s Movements
One of the core challenges for situated natural lan-
guage generation is to timely connect the user?s per-
cepts (in the case of virtual-tactile-map exploration
indicated by movements that the user performs with
the device) to symbolic natural language (Roy &
Reiter, 2005). The task to be solved is to have a
well-specified conceptualization of exploration situa-
tions. An exploration situation is constituted by the
kind of movements the user performs, the map ob-
jects the user wants to gain knowledge about (which
constitutes the haptic focus (Lohmann et al, 2011)),
and the haptic exploration and verbalization history.
In the structure of the VAVETaM system, the MEP
Observer fulfills the task of categorizing the user?s
movements and detecting objects in the haptic focus.
Lohmann et al (2011) discuss how Map-Exploratory
Procedures (MEPs), a specialization of Exploratory
Procedures, introduced as categories of general hap-
tic interaction by Lederman and Klatzky (2009), can
be used to categorize the map user?s movements.
MEP types are shown in Table 1.
For example, a trackMEP is, straightforwardly,
2User studies showed that the verbal identification is neces-
sary to recognize the haptic objects.
MEP Type Indication
trackMEP Exploration of a track
or track segment object
landmarkMEP Exploration of a potential
landmark object
regionMEP Exploration of a region object
frameMEP Exploration of a frame object
stopMEP No exploration
Table 1: Map Exploratory Procedures (MEPs).
Dorfstra?e
Potential landmarks
Tracks (ptX) and overlapping
Track Segments (ptsX)
ptX
ptsX
pts55
Blu
me
n
-st
ra?
e
Am
sel
we
g pt5
The user?s movement
Aldi Lidl
Figure 3: Visualization of a Part of a Virtual Tactile Map.
characterized by a track-following movement indi-
cating that the user wants to know something about
a track object. MEPs are (optionally) specified with
identifier(s) that link objects on the propositional
layer of the VETM knowledge base as belonging to
the haptic focus of the MEP.
In this work, we extend the concept to be able to
cope with multiple objects or parts of objects that
can simultaneously be in the user?s haptic focus. The
following example illustrates overlapping haptic foci
(see Figure 3). Consider the track with the name
?Dorfstra?e? being represented as track object pt5 on
the propositional layer of the VETM knowledge base.
If the track pt5 forms a dead end, this dead end can
additionally be represented as a unique track segment
object (pts55). When the user explores the track pt5
from the left to the right, at a certain point, both pt5
and pts55 are in the haptic focus.
Since the user is exploring a track, the movement
is characterized by a trackMEP which is specified
by the objects pt5 and pts55 and either will be in the
primary haptic focus.3 Thus, in this case, pts55 is in
3Notice that the decision whether in fact pt5 or pts55 are
59
GVA Agenda
PVM Construction
GVA Controller ...
Utterance Plans &Agenda Operations
Formulation &Articulation
MKR (Map-Knowledge Reasoner) / MKR SimulatorMEP
Obs
erve
r/M
EPS
imu
lato
r Passive KnowledgeComponents
Active ProcessingComponentsInformation Flow
VETM
Figure 4: The Architecture of the Generation of Verbal Assistance Component.
the secondary haptic focus. It is reasonable to talk
about both, the track and the dead end itself. As a
notational convention, we denote MEPs by their type,
the object in primary focus (if available), and a (possi-
bly empty) list of objects in secondary focus. For the
example above, we write trackMEP(pt5, [pts55]).
4.2 Structure of the Generation Component
The focus of our prototype is on the GVA component
of the VAVETaM system that solves the ?What to
say?? task, the task of determining the content appro-
priate for utterance in an exploration situation (Reiter
& Dale, 2000; De Smedt, Horacek, & Zock, 1996).
This component interacts with different components
introduced above (see Section 3): (1) it receives the
conceptualization of the user?s movements (MEPs
and specifications) from the MEP Observer; (2) it
accesses the propositional layer of the VETM knowl-
edge base in order to retrieve information about the
objects that is suitable for verbalization; (3) it inter-
acts with the MKR component, which keeps track of
the exploration and verbalization history; and (4) it
then sends semantic representations in the form of
preverbal messages (PVMs) to the Formulation &
Articulation components.
The GVA component consists of several subcom-
ponents which are visualized in Figure 4. The GVA
Controller controls the execution of other processes
through controlling the Agenda, which is an ordered
list of preverbal-message representations of utter-
ances.4 Once the GVA component receives a (spec-
ified) MEP describing the user?s movements from
the MEP Observer, it looks up Utterance Plans &
focussed primarily upon is up to the MEP Observer component.
4The term ?Agenda? is used in a similar context in the Colla-
gen system (Rich, Sidner, & Lesh, 2001).
Agenda Operations that specify which information to
express is suitable in the given exploration situation
and where it should be placed on the Agenda. The
PVM Construction component searches an utterance
plan that allows to construct a preverbal message
that contains this information. The top element of
the Agenda is passed on to the Formulation & Ar-
ticulation component as soon as that component has
finished uttering the previous element.
In the current implementation of the GVA, utter-
ance plans are stored as lists of potential messages
and construction rules. For example, with a track-
MEP, associated knowledge is stored that the object
shall first be identified (by either stating the name
associated to that object, e.g., ?Dorfstra?e? or choos-
ing a referring expression that allows for definite
identification). Then, if available, information about
geometric relations such as parallelism with other
linear objects on the map is selected from the VETM
knowledge base, followed by information about spa-
tial relations with other map objects. Subsequently,
the construction of a preverbal message that informs
the user about the extent of the track in the haptic
focus is tried, followed by information about cross-
ings the track has. For each of these construction
rules is tested whether the VETM knowledge base
contains suitable information. If it does, a preverbal
message is generated and added to the Agenda unless
the MKR component rejects the message because
this utterance is inappropriate given the exploration
and verbalization history, which prevents unneces-
sary repetitions of information. For example, if the
user has previously explored the track pt5 and already
received the information that the buildings ?Lidl? and
?Aldi? (cf. Figure 3) are above the track a short time
before, the articulation of this information is pre-
60
Figure 5: Literal Translation of the Template for a German
Identification Message.
vented and the user is given other information (or
none, if no more suitable information is available).
4.3 Formulation and Articulation
In the prototype system presented, formulation is
implemented in a template-based approach (Reiter
& Dale, 2000). The Formulation component uses
a set of sentence templates which consist of partial
lexicalizations and gaps to fill with information for
the exploration situations. Additionally, a lexicon
stores knowledge about natural language expressions
that can be used to express spatial situations. Fig-
ure 5 shows a simple template used for the genera-
tion of identification messages.5 Of the four utter-
ance parts depicted in the left box, one is chosen
randomly enabling some variation in the utterances.
If the MKR component has marked the preverbal
message as a repetition of a previously articulated
utterance, a marker word is placed in the sentence
(here: ?again?). Then, the sentence is completed by
either selecting the name of the object in focus from
the VETM knowledge base or by selecting a referring
expression. The former results in utterances such as
?This is Dorfstra?e?. This text is then synthesized
using text-to-speech (TTS) software.
5 A Worked Example
As described, the development of the component that
conceptualizes the user?s movement is not yet fin-
ished. Therefore, to show the function of the imple-
mentation, we used example inputs that were derived
by manually annotating screen-records from experi-
mental data that was previously collected in Wizard-
of-Oz-like experiments with blindfolded sighted,
blind, and visually impaired people. In these ex-
5Note that the system is implemented in German; the order-
ing of elements indead leads to grammatically correct German
sentences.
periments, participants received pre-recorded verbal
assisting utterances that were selected by the experi-
menter using a custom-built software tool based on a
visualization of the user?s movement on a computer
screen (Lohmann & Habel, forthcoming, and Sec-
tion 2). Using video records of the visualizations
of the user?s movements, the first author manually
annotated the relevant MEPs and their specifications
that, in the VAVETaM structure, the MEP Observer
component should output. These manually annotated
MEPs form the input to test the prototype system.6
In order to exemplify the function of the gener-
ation system, a small part of one of the annotated
inputs is detailed in this section.7 Figure 6 visual-
izes a part of the movement of a visually impaired
map explorer and the corresponding names and iden-
tifiers of the objects used for the specification of the
MEPs in the VETM knowledge base. As the figure
shows, the map explorer touches the track pt3, com-
ing from the left. The track is explored for a while
with small movements. (This position is remained
for a relatively long time, maybe listening to the on-
going utterances.) Then, the map explorer proceeds
to the bottom end of the track before following the
track upwards. Figure 6 shows that the bottom end of
the track is conceptualized as distinct track segment,
track segment pts33, which is part of the track pt3.
pt
3
Am
se
lw
eg
pt
s3
3
Kartenrand [map frame]
Figure 6: Example Movement a Visually Impaired Map
Explorer Performed in an Ongoing Experiment.
The annotated MEPs and their specification of this
small exploration movement are shown in Table 2.
The GVA component and the Formulation & Articu-
lation components generate detailed log files that in-
6Detecting MEPs is an instance of event detection in virtual
haptic environments (Kerzel & Habel, 2011), which showed
its applicability for the task in an early prototype (M. Kerzel,
personal communication).
7We also tested other annotated inputs; this example is repre-
sentative of the behavior of the prototype.
61
Time in Seconds Input to the GVA
. . . . . .
33.0?54.0 trackMEP(pt3)
54.0?57.0 trackMEP(pt3, [pts33])
57.0?57.8 trackMEP(pt3)
. . . . . .
Table 2: Manually Categorized MEPs and Specifications
for the Exploration Depicted in Figure 6.
dicate which information has been selected from the
VETM knowledge base, which preverbal messages
(PVMs) are put onto the Agenda, and how utterances
are articulated. Based on the log files, we detail the
processes performed by the GVA component and the
resulting verbal output in Table 3.
During the user?s long first exploration movement
of the track pt3 from seconds 33 to 54, which is con-
ceptualized by trackMEP(pt3), the GVA component
expresses all the information that is associated with
the track pt3 in the VETM. The first message informs
the user about the identity of the track by stating the
identifying utterance ?This is Amselweg?. Then, the
user is informed about geometric relations of this
track to other tracks. In the present case, information
about parallelism with the track pt4 is available in the
VETM and a corresponding utterance is produced.
Subsequently, the user is informed about the extent
of the track, i.e., where it ends. Then, information
about the intersections the track has is uttered. These
are all assisting utterances that are possible given the
current MEP and the knowledge base.8
Next, the user moves downwards resulting in the
distinct track segment pts33 coming into secondary
focus. All PVMs about the object in primary focus
(pt3) are blocked by the MKR component, as they
have just been uttered. Thus, a message that informs
the user about his or her position on the track segment
is formulated, resulting in a message such as ?Here,
Amselweg is restricted by the map frame?. When the
user leaves the track segment pt33, no further assist-
ing utterances are given as all information associated
with the track pt3 has been expressed recently.
8Note that the order in which information is given is fixed
in the current system as explained in Subsection 4.2. Whether
giving the messages in another order, which is potentially more
flexible, is more helpful, has to be further evaluated.
. . .
33.0?54.0 s
MEP Simulator fires trackMEP(pt3)
GVA receives: trackMEP(pt3)
GVA clears agenda due to MEP change
PVM Construction is able to generate PVMs of class:
Identification, Geometric Relation, Extension, Junc-
tions
PVMs Identification, Geometric Relation, Extension,
Junctions, are put on the Agenda (0 prohibited by Map-
Knowledge Reasoner)
Formulation getting Identification PVM for the RefO
pt3: the following aspects have been chosen by PVM
Construction: name ?Amselweg?
Speechout: ?Dies ist der Amselweg.? [?This is Amsel-
weg.?]
Formulation getting Geometric Relation PVM for the
RefO pt3: the following aspects have been chosen by
the PVM Construction: IS PARALLEL TO with the
arguments [pt3, pt4]
Speechout: ?Parallel zu ihm verla?uft die Blumenstra?e.?
[?. . . which is parallel to Blumenstra?e?]
Formulation getting Extent PVM for the RefO pt3: the
following aspects have been chosen by the PVM Con-
struction: predicate HAS UPPER LIMIT with the ar-
guments [pt3, ptco1]; predicate HAS LOWER LIMIT
with the arguments [pt3, pfr3]
Speechout: ?. . . er muendet nach oben in die Dorfstra?e
und endet unten am Kartenrand.? [?. . . it forms a corner
with Dorfstra?e at the top and at the bottom is restricted
by the map frame.?]
Formulation getting Junctions PVM for the RefO pt3:
the following aspects have been chosen by the PVM
Construction: predicate IS IN TRACK CONFIG with
the arguments [pt3, ptco4]
Speechout: ?Au?erdem hat er eine Kreuzung mit
der Hochstra?e.? [?Furthermore, the street crosses
Hochstra?e.?]
54.0?57.0 s
MEP Simulator changes MEP specification to track-
MEP(pt3, [pts33])
GVA receives: trackMEP(pt3, [pts33])
GVA detects secondary focus change
PVM Construction is able to generate PVMs of class:
Identification
Identification-class PVM is put at the front of the
Agenda (0 prohibited by Map-Knowledge Reasoner)
Formulation getting Identification PVM for the RefO
pts33
. . .
62
. . .
Speechout: ?Hier endet der Amselweg am Kartenrand.?
[?Here, Amselweg is restricted by the map frame.?]
57.0?57.8 s
MEP Simulator changes MEP specification to track-
MEP(pt3)
GVA receives: trackMEP(pt3)
Nothing happens, primary focus not new
. . .
Table 3: The Processes and Output (German and Trans-
lated) of the GVA and the Formulator.
6 Conclusion
We presented a prototype system that generates situ-
ated assisting utterances for tactile-map explorations
to ease tactile map learning. The prototype is based
on an earlier concept. We focussed on the GVA
component in the system, which solves the ?What
to say?? task of natural language generation, taking
into account the situated context. We exemplified
the working of the component in a testing environ-
ment based on a conceptualization of a part of a
real tactile-map exploration, for which it generates
plausible and timely output that is comparable to
assisting utterances that were in previous research
tested in Wizard-of-Oz-like experiments with blind-
folded sighted people and in ongoing experiments
with blind and visually impaired people. Therefore,
we conclude that a generation system working in the
manner described is technically possible. We also
explained in detail the structure and implementation
of MEPs, which are the basis for categorization of the
user?s movements and, with additional specification,
the input to the GVA component.
More fine-grained analysis is needed to gain
knowledge (1) about how much information should
be given via the verbal channel to maximize effi-
ciency, and (2) whether the system can be improved
by using more flexible Utterance Plans.
7 Discussion and Outlook
One problem which became apparent in the experi-
ments and also in preliminary tests of the fully inte-
grated prototype system is the fact that the user?s ex-
ploration movements on the map may be very quick.
In these cases, the information to be delivered may
already be outdated when the assistive utterance con-
veys this information. This is partly due to the Ger-
man word order, as can be seen in Figure 5, which
shows the template for identification messages.
Problems can occur in cases where an utterance
is verbalized shortly before the user starts exploring
another map object. In this case, the exploration situ-
ation changes during articulation. Currently, the com-
ponents concerned with language generation work in
a modularized sequential manner without feedback.
If an utterance was sent to formulation, it cannot not
be changed anymore. Hence, it can happen that as-
sisting utterances and the user?s exploration are not
in all cases timely.
One possible remedy to this problem is to extend
the formulation to work in an incremental fashion
such that it explicitly handles situations in which a
currently articulated utterance is outdated (e.g., an
identification utterance that is no longer valid because
the object to be identified has gone out of focus) and
by altering it to a new utterance of similar structure
(i.e., an identification utterance for a different ob-
ject which just came into the haptic focus). In this
case, it could adapt the ongoing utterance (if it is
still in an early stage of production) to replace the
previous identifying word (e.g., ?Amselweg?) with
the new word (i.e., ?Dorfstra?e?). Of course, this is
only possible if the articulation (text-to-speech syn-
thesis) works in an incremental fashion (i.e., it is able
to change yet unspoken parts of an ongoing utter-
ance). Such work is currently ongoing and we plan
to integrate this functionality in our future work.
Acknowledgments
The research reported in this paper has been partially
supported by DFG (German Science Foundation) in
IRTG 1247 ?Cross-modal Interaction in Natural and
Artificial Cognitive Systems? (CINACS). We thank
the anonymous reviewers for their highly useful com-
mentaries.
References
Buzzi, M. C., Buzzi, M., Leporini, B., & Martu-
sciello, L. (2011). Making visual maps acces-
sible to the blind. Universal Access in Human-
Computer Interaction. Users Diversity, 271?
280.
63
De Almeida (Vasconcellos), R. A., & Tsuji, B.
(2005). Interactive mapping for people who are
blind or visually impaired. In Modern cartog-
raphy series (Vol. 4, pp. 411?431). Elsevier.
De Felice, F., Renna, F., Attolico, G., & Distante,
A. (2007). A haptic/acoustic application to
allow blind the access to spatial information.
In World haptics conference (pp. 310 ? 315).
De Smedt, K., Horacek, H., & Zock, M. (1996).
Architectures for natural language generation:
Problems and perspectives. Trends in Natural
Language Generation An Artificial Intelligence
Perspective, 17?46.
Espinosa, M. A., Ungar, S., Ochaita, E., Blades, M.,
& Spencer, C. (1998). Comparing methods for
introducing blind and visually impaired people
to unfamiliar urban environments. Journal of
Environmental Psychology, 18, 277 ? 287.
Guhe, M., Habel, C., & Tschander, L. (2004). Incre-
mental generation of interconnected preverbal
messages. In T. Pechmann & C. Habel (Eds.),
Multidisciplinary approaches to language pro-
duction (pp. 7?52). Berlin, New York: De
Gruyter.
Habel, C. (1986). Prinzipien der Referentialita?t.
Berlin, Heidelberg, New York: Springer.
Habel, C., Kerzel, M., & Lohmann, K. (2010). Ver-
bal assistance in Tactile-Map explorations: A
case for visual representations and reasoning.
In Proceedings of AAAI workshop on visual
representations and reasoning 2010.
Kerzel, M., & Habel, C. (2011). Monitoring and de-
scribing events for virtual-environment tactile-
map exploration. In M. F. W. A. Galton &
M. Duckham (Eds.), Proceedings of workshop
on ?identifying objects, processes and events?,
10th international conference on spatial infor-
mation theory. Belfast, ME.
Lederman, S., & Klatzky, R. (2009). Haptic per-
ception: A tutorial. Attention, Perception, &
Psychophysics, 71(7), 1439?1459.
Levelt, W. J. M. (1989). Speaking: From intention to
articulation. Cambridge, MA: The MIT Press.
Lohmann, K., Eschenbach, C., & Habel, C. (2011).
Linking spatial haptic perception to linguis-
tic representations: Assisting utterances for
Tactile-Map explorations. In M. Egenhofer,
N. Giudice, R. Moratz, & M. Worboys (Eds.),
Spatial information theory (pp. 328?349).
Berlin, Heidelberg: Springer.
Lohmann, K., & Habel, C. (forthcoming). Extended
verbal assistance facilitates knowledge acqui-
sition of virtual tactile maps. Accepted for
presentation at Spatial Cognition 2012.
Lohmann, K., Kerzel, M., & Habel, C. (2010). Gen-
erating verbal assistance for Tactile-Map ex-
plorations. In I. van der Sluis, K. Bergmann,
C. van Hooijdonk, & M. Theune (Eds.), Pro-
ceedings of the 3rd workshop on multimodal
output generation 2010. Dublin.
Lynch, K. (1960). The image of the city. Cambridge,
MA; London: MIT Press.
Miele, J. A., Landau, S., & Gilden, D. (2006). Talk-
ing TMAP: automated generation of audio-
tactile maps using Smith-Kettlewell?s TMAP
software. British Journal of Visual Impairment,
24(2), 93?100.
Parkes, D. (1988). ?NOMAD?: An audio-tactile
tool for the acquisition, use and management
of spatially distributed information by partially
sighted and blind people. In Proceedings of
the 2nd international conference on maps and
graphics for visually disabled people. Notting-
ham, UK.
Parkes, D. (1994). Audio tactile systems for de-
signing and learning complex environments as
a vision impaired person: static and dynamic
spatial information access. Learning Environ-
ment Technology: Selected Papers from LETA,
94, 219?223.
Reiter, E., & Dale, R. (2000). Building natural lan-
guage generation systems. Cambridge: Cam-
bridge University Press.
Rich, C., Sidner, C. L., & Lesh, N. (2001). Colla-
gen: applying collaborative discourse theory
to human-computer interaction. AI magazine,
22(4), 15?26.
Roy, D., & Reiter, E. (2005). Connecting language
to the world. Artificial Intelligence, 167(1-2),
1?12.
Ungar, S. (2000). Cognitive mapping without visual
experience. In R. Kitchin & S. Freundschuh
(Eds.), Cognitive mapping: Past, present and
future (pp. 221?248). London: Routledge.
Wang, Z., Li, B., Hedgpeth, T., & Haven, T. (2009).
Instant tactile-audio map: enabling access to
64
digital maps for people with visual impairment.
In Proceeding of the 11th international ACM
SIGACCESS conference on computers and ac-
cessibility (pp. 43?50). Pittsburg, PA.
Zeng, L., & Weber, G. (2010). Audio-haptic browser
for a geographical information system. In
K. Miesenberger, W. Zagler, & A. Karschmer
(Eds.), Computers helping people with special
needs, part II (pp. 466?473).
65
Proceedings of the SIGDIAL 2013 Conference, pages 280?283,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Open-ended, Extensible System Utterances Are Preferred,
Even If They Require Filled Pauses
Timo Baumann
Universit?t Hamburg
Department of Informatics
Germany
baumann@informatik.uni-hamburg.de
David Schlangen
University of Bielefeld
Faculty of Linguistics and Literary Studies
Germany
david.schlangen@uni-bielefeld.de
Abstract
In many environments (e. g. sports com-
mentary), situations incrementally unfold
over time and often the future appearance
of a relevant event can be predicted, but not
in all its details or precise timing. We have
built a simulation framework that uses our
incremental speech synthesis component
to assemble in a timely manner complex
commentary utterances. In our evaluation,
the resulting output is preferred over that
from a baseline system that uses a simpler
commenting strategy. Even in cases where
the incremental system overcommits tem-
porally and requires a filled pause to wait
for the upcoming event, the system is pre-
ferred over the baseline.
1 Introduction
In spontaneous speech, speakers often commit tem-
porally, e. g. by starting utterances that they do not
yet know how to complete (Clark, 1996), putting
time pressure on them for the generation of a com-
pletion. While this may be for planning and effi-
ciency reasons, it also enables them to start com-
menting on events for which the outcome is not yet
known. For example when a ball is flying towards
the goal, but it is uncertain yet whether it will hit,
in sports commentary.
To accommodate this incremental behaviour, hu-
man speakers plan their utterances just somewhat
ahead, typically in chunks of major phrases (Levelt,
1989), and remain flexible to change or abandon
the original plan, or to hesitate, e. g. to adapt their
timing. This flexibility is in contrast to speech
output in spoken dialogue systems (SDSs) which
typically generate, synthesize and deliver speech
in units of full utterances that cannot be changed
while ongoing, apart from being aborted or inter-
rupted (Edlund, 2008).
Recently, incremental speech synthesis (iSS) has
been presented (Dutoit et al, 2011; Baumann and
Schlangen, 2012b) which allows to start partial ut-
terances that are then smoothly extended during
verbalization. Incremental spoken output for di-
alogue systems has been shown to improve natu-
ralness (Buschmeier et al, 2012) and Skantze and
Hjalmarsson (2010) have used filled pauses to hold
a turn. Dethlefs et al (2012) present an incremental
NLG strategy to reduce the need for filled pauses
in interactions.
We investigate the impact of incremental spoken
output in a highly dynamic environment, that is,
where the rate of external events is high enough
to allow only few utterances to finish as planned.
As an example, we choose an otherwise simple
commentary domain, where incremental output en-
ables the system to combine multiple events into
one complex commenting utterance that takes into
account predictions about upcoming events. If the
system overcommits to the timing of future events,
it autonomously uses a filled pause until more ma-
terial becomes available.
2 Related Work
A paradigmatic example of a domain that uses
open-ended utterances is sports commentary,
which has received some attention in the NLG
community. For example, Chen and Mooney
(2008) present a system that learns from hand-
annotated data what to comment on. However,
attention seems to have been placed more on
truthfulness of the content, as, judging from videos
provided on their website,1 the formulations
that are produced are rather monotonic (?pink7
dribbles towards the goal. pink7 shoots for the
goal. pink7 passes to...?). More importantly,
the delivery of a produced utterance does not seem
to be temporally tied to the occurrence of the event.
1http://www.cs.utexas.edu/users/ml/clamp/sportscasting
280
Figure 1: The map shown in the CarChase domain,
including the car on one of its itineraries (red; an-
other in blue). At the depicted moment we can
assume that the car will take a turn, but do not
know whether left or right.
Repeatedly, utterances are synthesized long after
the fact that they describe which sometimes has
become obsolete at that point (for example, a goal
is scored while the system still talks about a pass).
Lohmann et al (2011) describe another domain
that can be called highly dynamic: a system that
adds spoken assistance to tactile maps for the vi-
sually impaired. In their settings, users can move
around on a computer representation of a map with
a hand-held haptic force-feedback device. Users
are given spoken advice about the currently tra-
versed streets? names, the relation of streets to each
other, and to other map objects in the user?s vicin-
ity. Such exploratory moves by users can become
rather quick, which in the system they describe
can lead to output that comes late, referring to a
position that has long been left.
3 A Highly Dynamic Commenting Domain
Our example domain combines properties of the
sports commentary and map exploration domains
mentioned above: the CarChase domain depicted
in Figure 1. In the domain, a car drives around
streets on the map and a commentator (supposed to
be observing the scene from above) comments on
where it is driving and what turns it is taking.
The car?s itinerary in our domain simulator is
scripted from a configuration file which assigns
target positions for the car at different points in time
and from which the motion and rotation of the car
is animated. The speed of the car is set so that the
event density is high enough that the setting cannot
be described by simply producing one utterance
per event ? in other words: the domain is highly
dynamic.
time event description ongoing utterance (already realized part in bold,
newly appended continuation in italic)
t1 car on Main Street The car drives along Main Street.
t2 car will likely turn . . .drives along Main Street and then turns ?hes?
t3 car turns right . . .drives along Main Street and then turns right.
Figure 2: Example of incremental utterance pro-
duction as a car drives along a street and turns. The
ongoing utterance is extended as events unfold.
4 A Strategy for Incremental
Commentary
We distinguish three types of events in the do-
main: identification (ID) events trigger the system
to name the street the car is on, turn events fire
when the car is taking a turn. Finally, turn-prep
events fire when it is obvious that the car will turn
but the direction of the turn remains open. These
three event types are shown in Figure 2 at time t1
(ID), t2 (turn-prep), and t3 (turn).
As can be seen in the example in Figure 2, the
turn-prep event enables a system that is able to
incrementally update its ongoing utterance to con-
tinue speaking about the anticipated future (?and
then turns?) without knowing the direction of the
turn. This allows an incremental system to output
efficient utterances that fluently combine multiple
events and avoid repetition. Furthermore, turn-prep
events enable the system to output the direction
of the turn (the most important information) very
shortly after the fact.
A non-incremental system, in contrast, must out-
put individual utterances for every event and utter-
ances can only start after the fact. Furthermore,
a non-incremental system cannot extend ongoing
utterances, rendering turn-prep events useless.
5 Implemented System
The system used for the experiment reported be-
low uses an early version of incremental speech
synthesis as implemented in INPROTK (Baumann
and Schlangen, 2012c), a toolkit for incremental
spoken dialogue processing based on the IU model
(Schlangen and Skantze, 2009). The system al-
lows to extend ongoing utterances, enabling the
281
incremental commenting strategy outlined above.
In addition, we implemented a capability to syn-
thesize a hesitation if no more content is specified,
and to continue as soon as content becomes avail-
able. (Thus, in contrast to (Skantze and Hjalmars-
son, 2010), hesitations do not consume additional
time.) By using hesitations, the system gracefully
accommodates temporal over-commitment (i. e. the
obligation to produce a continuation that is not ful-
filled in time) which may occur, e. g. when the car
drives slower than anticipated and a turn?s direction
is not yet known when the system needs it.
In the preliminary version of iSS used for the ex-
periments, no prosodic integration of continuations
takes place, resulting in prosodic discontinuities;
see (Baumann and Schlangen, 2012a) for a detailed
assessment of prosodic integration in iSS.
As we focus on the merit of iSS in this work, we
did not implement a scene analysis/event detection
nor a NLG component for the task.2 Instead, the
commentary is scripted from the same configura-
tion file that controls the car?s motion on the board.
iSS events lag behind slightly, ensuring that visual
analysis would be possible, and event/text corre-
spondence is close, matching NLG capabilities.
6 Experiment
To evaluate the incremental system, we compared
it to a non-incremental baseline system which is
unable to alter speech incrementally and hence can-
not smoothly extend ongoing partial utterances. In-
stead, the baseline system always produces full
utterances, one per event. To ensure the tempo-
ral proximity of delivery with the causing event
in the baseline system, utterances can be marked
as optional (in which case they are skipped if the
system is still outputting a previous utterance), or
non-optional (in which case an ongoing utterance
is aborted in favour of the new utterance). All ?turn?
events in the domain were marked as optional, all
street ID events as non-optional.
We devised 4 different configurations (including
the itineraries shown in Figure 1), and the timing of
events was varied (by having the car go at different
speeds, or by delaying some events), resulting in 9
scenarios; in 3 of these, the incremental system gen-
erated one or more hesitations. Both systems? out-
put for the 9 scenarios was recorded with a screen-
recorder, resulting in 18 videos that were played in
2However, Lohmann et al (2012) present an incremental
NLG strategy for a similar task.
random order to 9 participants (university students
not involved in the research). Participants were
told that various versions of commentary-generat-
ing systems generated the commentary based on
the running picture in the videos and were then
asked to rate each video on a five-point Likert scale
with regards to how natural (similar to a human)
the spoken commentary was (a) formulated, and
(b) pronounced. In total, this resulted in 81 paired
samples for each question.3
The assumption (and rationale for the second
question) was that the incremental system?s formu-
lations would result in higher formulation ratings,
while we hoped the acoustic and prosodic artefacts
resulting from the coarsely implemented incremen-
tal synthesis would not significantly hurt pronun-
ciation ratings. In order to not draw the subjects?
attention towards incremental aspects, no question
regarding the timeliness of the commentary was
asked for explicitly.
7 Results
The mean ratings for both formulation quality and
pronunciation quality for the incremental and base-
line systems is shown in Figure 3. The median
differences in the ratings of the two conditions is
2 points on the Likert scale for question (a) and
0 points for question (b) (means of 1.66 and 0.51,
respectively), favouring the incremental system.
The sign test shows that the advantage of the incre-
mental system is clearly significant for questions
(a) (68+/9=/4-; p < .0001) and (b) (38+/30=/13-;
p < .0007)4.
Thus, it is safe to say that the production strate-
gies enabled by incremental speech synthesis (i. e.
starting to speak before all evidence is known and
extending the utterance as information becomes
available) allows for formulations in the spoken
commentary that are favoured by human listeners.
Incremental behaviour in the 3 scenarios that
required hesitations was rated significantly worse
than in those scenarios without hesitations for both
questions (t-tests, p < .001 (a) and p < .01 (b)). This
3The experiment was conducted in one language (German)
only, but we believe our results to carry over to other lan-
guages. Specifically, we assume that most or all languages
cater for commenting, and believe that human commenters
universally use their ability to integrate events late in the utter-
ance. However, practices of commenting may work differently
(and differently well) among languages.
4We also conducted a non-paired t-test for question (b), as
the different formulations of the systems might have effects on
pronunciation quality; this test was also significant (p < .0012).
282
very
little
a little
neutral
a bit
very
much
a) formulation b) pronunciation
no hes
no hes 
incremental strategy
baseline strategy
hes
hes
Figure 3: Mean ratings of formulation and pronun-
ciation for the incremental and baseline systems;
the formulation rating differs for utterances with
and without hesitations in the incremental system.
is a clear indication that a system should try to
avoid over-commitment, as users do not accept hes-
itations as inevitable (given that there was simply
no evidence yet where the car would turn, for exam-
ple). However, even in those scenarios that require
filled pauses, the incremental commentary?s for-
mulation is rated as significantly better than the
baseline system?s (sign test, 18+/5=/4-; p < .005)
while there is no effect on pronunciation in these
cases.
8 Discussion & Outlook
The results indicate a clear user preference for open-
ended, extensible utterances that grow as events un-
fold. Furthermore, this preference is stronger than
the negative impact of filled pauses that are needed
to cover temporal over-commitment, and despite
the poor quality of filled pauses in the current sys-
tem, which we plan to improve in the future.
Similarly to spoken commentary in dynamic do-
mains, conversational speech requires revisions and
reactions to events such as listener feedback, or the
absence thereof (Clark, 1996). Thus, we believe
that our results, as well as iSS in general, also apply
to a broad range of conversational SDS tasks.
Finally, synthesis quality appears to be less im-
portant than interaction adequacy: we found no
difference in rating of perceptual quality (?pronun-
ciation?) between the variants, even though in isola-
tion iSS sounded noticeably worse in the prototype.
This result calls for interactive adequacy as an op-
timization target over (isolated) perception ratings
for speech synthesis, and also challenges the use of
canned speech in conversational SDSs, which does
not adapt to the interaction.
Acknowledgements The first author would like
to thank Wolfgang Menzel for fruitful discussions
on the topic, and permanent encouragement.
References
Timo Baumann and David Schlangen. 2012a. Eval-
uating prosodic processing for incremental speech
synthesis. In Procs. of Interspeech, Portland, USA.
Timo Baumann and David Schlangen. 2012b. IN-
PRO_iSS: A component for just-in-time incremen-
tal speech synthesis. In Proceedings of ACL System
Demonstrations, Jeju, Korea.
Timo Baumann and David Schlangen. 2012c. The
INPROTK 2012 release. In Proceedings of SDCTD,
Montr?al, Canada.
Hendrik Buschmeier, Timo Baumann, Benjamin
Dorsch, Stefan Kopp, and David Schlangen. 2012.
Combining incremental language generation and in-
cremental speech synthesis for adaptive information
presentation. In Procs. of SigDial, pages 295?303,
Seoul, Korea.
David L. Chen and Raymond J. Mooney. 2008. Learn-
ing to sportscast: A test of grounded language ac-
quisition. In Proceedings of 25th Int. Conference on
Machine Learning (ICML), Helsinki, Finland.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press.
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012. Optimising incremental generation
for spoken dialogue systems: Reducing the need for
fillers. In Procs. of the Seventh Int. Natural Lan-
guage Generation Conf., pages 49?58, Utica, USA.
Thierry Dutoit, Maria Astrinaki, Onur Babacan, Nico-
las d?Alessandro, and Benjamin Picart. 2011. pHTS
for Max/MSP: A Streaming Architecture for Statis-
tical Parametric Speech Synthesis. Technical Re-
port 1, numediart Research Program on Digital Art
Technologies.
Jens Edlund. 2008. Incremental speech synthesis. In
Second Swedish Language Technology Conference,
pages 53?54, Stockholm, Sweden. System Demo.
William J.M. Levelt. 1989. Speaking: From Intention
to Articulation. MIT Press.
Kris Lohmann, Carola Eschenbach, and Christopher
Habel. 2011. Linking spatial haptic perception to
linguistic representations: assisting utterances for
tactile-map explorations. In Spatial information the-
ory, pages 328?349, Berlin, Heidelberg. Springer.
Kris Lohmann, Ole Eichhorn, and Timo Baumann.
2012. Generating situated assisting utterances to fa-
cilitate tactile-map understanding: A prototype sys-
tem. In Procs. of SLPAT 2012, Montr?al, Canada.
David Schlangen and Gabriel Skantze. 2009. A Gen-
eral, Abstract Model of Incremental Dialogue Pro-
cessing. In Procs. of the EACL, Athens, Greece.
Gabriel Skantze and Anna Hjalmarsson. 2010. To-
wards incremental speech generation in dialogue sys-
tems. In Procs. of SigDial, pages 1?8, Tokyo, Japan.
283
Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 68?72,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Situationally Aware In-Car Information Presentation
Using Incremental Speech Generation: Safer, and More Effective
Spyros Kousidis
1
, Casey Kennington
1,2
, Timo Baumann
4
, Hendrik Buschmeier
2,3
,
Stefan Kopp
2,3
, and David Schlangen
1
1
Dialogue Systems Group,
2
CITEC,
3
Sociable Agents Group ? Bielefeld University
4
Department of Informatics, Natural Language Systems Division ? University of Hamburg
spyros.kousidis@uni-bielefeld.de
Abstract
Holding non-co-located conversations
while driving is dangerous (Horrey and
Wickens, 2006; Strayer et al., 2006),
much more so than conversations with
physically present, ?situated? interlocutors
(Drews et al., 2004). In-car dialogue
systems typically resemble non-co-located
conversations more, and share their
negative impact (Strayer et al., 2013). We
implemented and tested a simple strategy
for making in-car dialogue systems aware
of the driving situation, by giving them
the capability to interrupt themselves
when a dangerous situation is detected,
and resume when over. We show that this
improves both driving performance and
recall of system-presented information,
compared to a non-adaptive strategy.
1 Introduction
Imagine you are driving on a relatively free high-
way at a constant speed and you are talking with the
person next to you. Suddenly, you need to overtake
another car. This requires more attention from you;
you check the mirrors before you change lanes, and
again before you change back. Plausibly, an attent-
ive passenger would have noticed your attention
being focused more on the driving, and reacted to
this by interrupting their conversational contribu-
tion, resuming when back on the original lane.
Using a driving simulation setup, we implemen-
ted a dialogue system that realises this strategy. By
employing incremental output generation, the sys-
tem can interrupt and flexibly resume its output.
We tested the system using a variation of a stand-
ard driving task, and found that it improved both
driving performance and recall, as compared to a
non-adaptive baseline system.
Figure 1: Overview of our system setup: human
controls actions of a virtual car; events are sent to
DM, which controls the speech output.
2 The Setup
2.1 The Situated In-Car System
Figure 1 shows an overview of our system setup,
with its main components: a) the driving simulator
that presents via computer graphics the driving task
to the user; b) the dialogue system, that presents,
via voice output, information to the user (here, cal-
endar entries).
Driving Simulation For the driving simulator,
we used the OpenDS Toolkit,
1
connected to a steer-
ing wheel and a board with an acceleration and
brake pedal, using standard video game hardware.
We developed our own simple driving scenarios
(derived from the ?ReactionTest? task, which is dis-
tributed together with OpenDS) that specified the
driving task and timing of the concurrent speech,
as described below. We modified OpenDS to pass
real-time data (e.g. car position/velocity/events in
the simulation, such as a gate becoming visible
or a lane change) using the mint.tools architec-
ture (Kousidis et al., 2013). In addition, we have
bridged INPROTK (Baumann and Schlangen, 2012)
with mint.tools via the Robotics Service Bus (RSB,
Wienke and Wrede (2011)) framework.
1http://www.opends.eu/
68
Figure 2: Driver?s view during experiment. The
green signal on the signal-bridge indicates the tar-
get lane.
Dialogue System Using INPROTK, we imple-
mented a simple dialogue system. The notion of
?dialogue? is used with some liberty here: the user
did not interact directly with the system but rather
indirectly (and non-intentionally) via driving ac-
tions. Nevertheless, we used the same modularisa-
tion as in more typical dialogue systems by using a
dialoge management (DM) component that controls
the system actions based on the user actions. We
integrated OpenDial (Lison, 2012) as the DM into
INPROTK,
2
though we only used it to make simple,
deterministic decisions (there was no learned dia-
logue policy) based on the state of the simulator
(see below). We used the incremental output gen-
eration capabilities of INPROTK, as described in
(Buschmeier et al., 2012).
3 Experiment
We evaluated the adaptation strategy in a driving
simulation setup, where subjects performed a 30
minute, simulated drive along a straight, five-lane
road, during which they were occasionally faced
with two types of additional tasks: a lane-change
task and a memory task, which aim to measure the
driving performance and the driver?s ability to pay
attention to speech while driving, respectively. The
two tasks occured in isolation or simultaneoulsy.
The Lane-Change Task The driving task we
used is a variant of the well-known lane-change
task (LCT), which is standardised in (ISO, 2010):
It requires the driver to react to a green light posi-
tioned on a signal gate above the road (see Figure 2).
The driver (otherwise instructed to remain in the
middle lane) must move to the lane indicated by
2
OpenDial can be found at http://opendial.
googlecode.com/.
Table 1: Experiment conditions.
Lane Change Presentation mode Abbreviation
Yes CONTROL CONTROL_LANE
Yes ADAPTIVE ADAPTIVE_LANE
Yes NO_TALK NO_TALK_LANE
No CONTROL CONTROL_EMPTY
the green light, remain there until a tone is sounded,
and then return again to the middle lane. OpenDS
gives a success or fail result to this task depending
on whether the target lane was reached within 10
seconds (if at all) and the car was in the middle lane
when the signal became visible. We also added a
speed constraint: the car maintained 40 km/h when
the pedal was not pressed, with a top speed of 70
km/h when fully pressed. During a Lane-change,
the driver was to maintain a speed of 60 km/h, thus
adding to the cognitive load.
The Memory Task We tested the attention of
the drivers to the generated speech using a simple
true-false memory task. The DM generated utter-
ances such as ?am Samstag den siebzehnten Mai
12 Uhr 15 bis 14 Uhr 15 hast du ?gemeinsam Essen
im Westend mit Martin? ? (on Saturday the 17th
of May from 12:15?14:15 you are meeting Mar-
tin for Lunch). Each utterance had 5 information
tokens: day, time, activity, location and partner,
spoken by a female voice. After utterance comple-
tion, and while no driving distraction occurred, a
confirmation question was asked by a male voice,
e.g. ?Richtig oder Falsch? ? Freitag? (Right or
wrong? ? Friday). The subject was then required
to answer true or false by pressing one of two re-
spective buttons on the steering wheel. The token
of the confirmation question was chosen randomly,
although tokens near the beginning of the utterance
(day and time) were given a higher probability of
occurrence. The starting time of the utterance re-
lative to the gate was varied randomly between 3
and 6 seconds before visibility. Figure 3 gives a
schematic overview of the task and describes the
strategy we implemented for interrupting and re-
suming speech, triggered by the driving situation.
3.1 Conditions
Table 1 shows the 4 experiment conditions, de-
noting if a lane change was signalled, and what
presentation strategy was used. Each condition ap-
peared exactly 11 times in the scenario, for a total
of 44 episodes. The order of episodes was randomly
69
t1
t
2
suc
gate
lane t
3
0
1
2
3
4
am Samstag den siebzehn- den siebzehnten Mai ?
am Samstag den siebzehnten Mai um 12 Uhr hast du ?Besprechung mit Peter?
ADAPTIVE
CONTROL
Figure 3: Top view of driving task: as the car moves to the right over time, speech begins at t
1
, the gate with
the lane-change indicator becomes visible at t
2
, where in the adaptive version speech pauses. Successful
lane change is detected at suc; successful change back to the middle lane is detected at lane, and resumes.
(If no change back is detected, the interruption times out at t
3
). All red-dotted lines denote events sent
from OpenDS to the Dialogue Manager.
generated for each subject. With this design, sub-
jects perceive conditions to be entirely random.
3.2 Dependent Variables
The dependent variables for the Memory task
are (a) whether the subject?s answer was correct
(true/false), and (b) the response delay, which is
the time from the end of the clarification ques-
tion to the time the true or false button was
pressed. For the driving task, the dependent vari-
ables are the OpenDS performance measurements
success/failure (as defined above) and reaction time
(time to reach the target lane).
3.3 Procedure
After signing a consent form, subjects were led into
the experiment room, where seat position and audio
level were adjusted, and were given written instruc-
tions. Next, the OpenDS scenario was initiated. The
scenario started with 10 successive lane-change sig-
nal gates without speech, for driving training. An
experimenter provided feedback during training
while the subjects familiarized themselves with the
driving task. Following the training gates came a
clearly-marked ?START? gate, signifying the be-
ginning of the experiment to the subjects (at this
point, the experimenter left). There was a ?FINISH?
gate at the end of the scenario. The whole stretch of
road was 23 km and took approximately 30 minutes
to complete. After the driving task, the subjects
were given a questionnaire, which asked them to
identify the information presentation strategies and
assign a preference.
Table 2: Subjects?
judgement of task
difficulty.
Diff. Freq.
4 (easy) 8
3 7
2 1
1 (hard) 1
Table 3: Subjects? system
preference.
Preference Freq.
ADAPTIVE 3
CONTROL 9
Neither 5
4 Results
In total, 17 subjects (8 male, 9 female, aged 19-
36) participated in the study. All of the subjects
were native German speakers affiliated with AN-
ONYMIZED University. As reported in the post-test
questionnaire, all held a driving license, two had
previous experience with driving simulators and
only one had previous experience with spoken dia-
logue systems. Table 2 shows the subjects? assess-
ment of difficulty, while Table 3 shows their prefer-
ence between the different strategies. Most subjects
found the task relatively easy and either prefer the
speech not to adapt or have no preference.
Memory task The overall percentages of correct
answers to the system?s recall questions (across all
subjects) are shown in Table 4. We see that the sub-
jects? performance in this task is considerably bet-
ter when the system adapts to the driving situation
(ADAPTIVE_LANE condition) rather than speaking
through the lane change (CONTROL_LANE con-
dition). In fact, the performance in the ADAPT-
IVE_LANE condition is closer to the control upper
70
Table 4: Performance in memory task per condi-
tion.
Condition Percentage
CONTROL_EMPTY 169/180 (93.9%)
ADAPTIVE_LANE 156/172 (90.7%)
CONTROL_LANE 150/178 (84.3%)
Table 5: Success in driving task per condition (as
reported by OpenDS).
Condition Success
NOTALK_LANE 175/185 (94.6%)
ADAPTIVE_LANE 165/174 (94.8%)
CONTROL_LANE 165/180 (91.7%)
bound (CONTROL_EMPTY condition). We tested
significance of the results using a generalized lin-
ear mixed model with CONDITION and SUBJECT
as factors, which yields a p-value of 0.027 when
compared against a null model in which only SUB-
JECT is a factor. No significant effects of between-
subjects factors gender, difficulty or preference
were found. In addition, the within-subject variable
time did not have any significant effect (subjects do
not improve in the memory task with time).
The average response delay (from the end of
the recall question to the button press) per condi-
tion across all subjects is shown in Figure 4. Sub-
jects reply slower to the recall questions in the
CONTROL_LANE condition, while their perform-
ance in the ADAPTIVE_LANE condition is indis-
tinguishable from the CONTROL_EMPTY condi-
tion (in which there is no distraction). Addition-
ally, there is a general decreasing trend of response
delay with time, which means that users get ac-
quainted with the task (type of information, format
of question) over time. Both factors (condition
and time) are significant (repeated measures AN-
OVA, 2x2 factorial design, F
condition
= 3.858, p =
0.0359,F
time
= 4.672, p= 0.00662). No significant
effects were found for any of the between-subject
factors (gender, difficulty, preference).
Driving task The success rate in the lane-change
task per condition is shown in Table 5. Here too
we find that the performance is lower in the CON-
TROL_LANE condition, while ADAPTIVE_LANE
does not seem to affect driving performance, when
compared to the NOTALK_LANE condition. The
effect is significant (p = 0.01231) using the same
GLMM approach and factors as above.
ADAPTIVE_LANE CONTROL_EMPTY CONTROL_LANECondition0
500
1000
1500
2000
2500
3000
3500
4000
User
 Res
pons
e De
lay (
ms)
Figure 4: User answer response delay under three
conditions.
5 Discussion, Conclusions, Future Work
We have developed and tested a driving simula-
tion scenario where information is presented by a
spoken dialogue system. Our system has the unique
ability (compared to today?s commercial systems)
to adapt its speech to the driving situation: it in-
terrupts itself when a dangerous situation occurs
and later resumes with an appropriate continuation.
Using this strategy, information presentation had
no impact on driving, and dangerous situations no
impact on information recall. In contrast, a system
that blindly spoke while the driver was distracted
by the lane-change task resulted in worse perform-
ance in both tasks: subjects made more errors in
the memory task and also failed more of the lane-
change tasks, which could prove dangerous in a
real situation.
Interestingly, very few of the subjects preferred
the adaptive version of the system in the post-task
questionnaire. Among the reasons that they gave
for this was their inability to control the interrup-
tions/resumptions of the system. We plan to ad-
dress the issue of control by allowing future ver-
sions of our system to accept user signals, such as
speech or head gestures; it will be interesting to see
whether this will impact driving performance or not.
Further, more sophisticated presentation strategies
(e.g., controlling the complexity of the generated
language in accordance to the driving situation) can
be tested in this framework.
Acknowledgments This research was partly sup-
ported by the Deutsche Forschungsgemeinschaft
(DFG) in the CRC 673 ?Alignment in Communic-
71
ation? and the Center of Excellence in ?Cognit-
ive Interaction Technology? (CITEC). The authors
would like to thank Oliver Eckmeier and Michael
Bartholdt for helping implement the system setup,
as well as Gerdis Anderson and Fabian Wohlge-
muth for assisting as experimenters.
References
Timo Baumann and David Schlangen. 2012. The In-
proTK 2012 release. In NAACL-HLT Workshop on
Future directions and needs in the Spoken Dialog
Community: Tools and Data (SDCTD 2012), pages
29?32, Montr?al, Canada.
Hendrik Buschmeier, Timo Baumann, Benjamin
Dosch, Stefan Kopp, and David Schlangen. 2012.
Combining incremental language generation and in-
cremental speech synthesis for adaptive information
presentation. In Proceedings of the 13th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue, pages 295?303, Seoul, South Korea.
Frank A. Drews, Monisha Pasupathi, and David L.
Strayer. 2004. Passenger and cell-phone conver-
sations in simulated driving. In Proceedings of the
48th Annual Meeting of the Human Factors and Er-
gonomics Society, pages 2210?2212, New Orleans,
USA.
William J. Horrey and Christopher D. Wickens. 2006.
Examining the impact of cell phone conversations
on driving using meta-analytic techniques. Human
Factors, 48:196?205.
ISO. 2010. Road vehicles ? Ergonomic aspects of
transport information and control systems ? Simu-
lated lane change test to assess in-vehicle second-
ary task demand. ISO 26022:2010, Geneva, Switzer-
land.
Spyros Kousidis, Thies Pfeiffer, and David Schlangen.
2013. MINT.tools: Tools and adaptors supporting
acquisition, annotation and analysis of multimodal
corpora. In Interspeech 2013, Lyon, France. ISCA.
Pierre Lison. 2012. Probabilistic dialogue models with
prior domain knowledge. In Proceedings of the 13th
Annual Meeting of the Special Interest Group on Dis-
course and Dialogue, pages 179?188, Seoul, South
Korea.
David L Strayer, Frank A Drews, and Dennis J Crouch.
2006. A comparison of the cell phone driver and the
drunk driver. Human Factors, 48:381?91.
David L Strayer, Joel M Cooper, Jonna Turrill, James
Coleman, and Nate Medeiros. 2013. Measuring
cognitive distraction in the automobile. Technical
report, AAA Foundation for Traffice Safety.
J Wienke and S Wrede. 2011. A middleware for col-
laborative research in experimental robotics. In Sys-
tem Integration (SII), 2011 IEEE/SICE International
Symposium on, pages 1183?1190.
72
