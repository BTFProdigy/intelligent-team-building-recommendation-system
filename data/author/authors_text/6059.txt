R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 155 ? 164, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Chunking Using Conditional Random Fields  
in Korean Texts 
Yong-Hun Lee, Mi-Young Kim, and Jong-Hyeok Lee 
Div. of Electrical and Computer Engineering POSTECH and AITrc, 
San 31, Hyoja-dong, Nam-gu, Pohang, 790-784, R. of Korea 
{yhlee95, colorful, jhlee}@postech.ac.kr 
Abstract. We present a method of chunking in Korean texts using conditional 
random fields (CRFs), a recently introduced probabilistic model for labeling 
and segmenting sequence of data. In agglutinative languages such as Korean 
and Japanese, a rule-based chunking method is predominantly used for its sim-
plicity and efficiency. A hybrid of a rule-based and machine learning method 
was also proposed to handle exceptional cases of the rules. In this paper, we 
present how CRFs can be applied to the task of chunking in Korean texts. Ex-
periments using the STEP 2000 dataset show that the proposed method signifi-
cantly improves the performance as well as outperforms previous systems. 
1   Introduction 
Text chunking is a process to identify non-recursive cores of various phrase types 
without conducting deep parsing of text [3]. Abney first proposed it as an intermedi-
ate step toward full parsing [1]. Since Ramshaw and Marcus approached NP chunking 
using a machine learning method, many researchers have used various machine learn-
ing techniques [2,4,5,6,10,11,13,14]. The chunking task was extended to the CoNLL-
2000 shared task with standard datasets and evaluation metrics, which is now a stan-
dard evaluation task for text chunking [3]. 
Most previous works with relatively high performance in English used machine 
learning methods for chunking [4,13]. Machine learning methods are mainly divided 
into the generative approach and conditional approach. The generative approach relies 
on generative probabilistic models that assign a joint probability p(X,Y) of paired 
input sequence and label sequence, X and Y respectively. It provides straightforward 
understanding of underlying distribution. However, this approach is intractable in 
most domains without strong independence assumptions that each input element is 
independent from the other elements in input sequence, and is also difficult to use 
multiple interacting features and long-range dependencies between input elements. 
The conditional approach views the chunking task as a sequence of classification 
problems, and defines a conditional probability p(Y|X) over label sequence given 
input sequence. A number of conditional models recently have been developed for 
use. They showed better performance than generative models as they can handle 
many arbitrary and overlapping features of input sequence [12]. 
A number of methods are applied to chunking in Korean texts. Unlike English, a 
rule-based chunking method [7,8] is predominantly used in Korean because of its 
well-developed function words, which contain information such as grammatical  
156 Y.-H. Lee, M.-Y. Kim, and J.-H. Lee 
relation, case, tense, modal, etc. Chunking in Korean texts with only simple heuristic 
rules obtained through observation on the text shows a good performance similar to 
other machine learning methods [6]. Park et al proposed a hybrid of rule-based and 
machine learning method to handle exceptional cases of the rules, to improve the 
performance of chunking in Korean texts [5,6]. 
In this paper, we present how CRFs, a recently introduced probabilistic model for 
labeling and segmenting sequence of data [12], can be applied to the task of chunking 
in Korean texts. CRFs are undirected graphical models trained to maximize condi-
tional probabilities of label sequence given input sequence. It takes advantage of gen-
erative and conditional models. CRFs can include many correlated, overlapping fea-
tures, and they are trained discriminatively like conditional model. Since CRFs have 
single exponential model for the conditional probability of entire label sequence given 
input sequence, they also guarantee to obtain globally optimal label sequence. CRFs 
successfully have been applied in many NLP problems such as part-of-speech tagging 
[12], text chunking [13,15] and table extraction from government reports [19]. 
The rest of this paper is organized as follows. Section 2 gives a simple introduction 
to CRFs. Section 3 explains how CRFs is applied to the task of chunking in Korean 
texts. Finally, we present experimental results and draw conclusions.  
2   Conditional Random Fields 
Conditional Random Fields (CRFs) are conditional probabilistic sequence models 
first introduced by Lefferty et al[12]. CRFs are undirected graphical models, which 
can be used to define the joint probability distribution over label sequence given the 
entire input sequence to be labeled, rather than being directed graphical models such 
as Maximum Entropy Markov Models (MEMMs) [11].  It relaxes the strong inde-
pendence assumption of Hidden Markov Models (HMMs), as well as resolves the 
label bias problem exhibited by MEMMs and other non-generative directed graphical 
models such as discriminative Markov models [12]. 
2.1   Fundamentals of CRFs 
CRFs may be viewed as an undirected graphical model globally conditioned on input 
sequence [14]. Let X=x1 x2 x3 ?xn be an input sequence and Y=y1 y2 y3 ?yn a label se-
quence. In the chunking task, X is associated with a sequence of words and Y is asso-
ciated with a sequence of chunk types. If we assume that the structure of a graph 
forms a simple first-order chain, as illustrated in Figure 1, CRFs define the condi-
tional probability of a label sequence Y given an input sequence X by the Hammer-
sley-Clifford theorem [16] as follows: 
??
???
?
= ?? ?
i k
iikk iXyyfXZXYp ),,,(exp)(
1)|( 1?  (1) 
where Z(X) is a normalization factor; fk(yi-1, yi, X, i) is a feature function at positions i 
and i-1 in the label sequence; k?  is  a weight associated with feature kf . 
 Chunking Using Conditional Random Fields in Korean Texts 157 
 
Fig. 1. Graphical structure of chain-structured CRFs 
Equitation 1, the general form of a graph structure for modeling sequential data, 
can be expanded to Equation 2, 
??
???
?
+= ???? ?
i k
ikk
i k
iikk iXysiXyytXZ
XYp ),,(),,,(exp)(
1)|( 1 ??  (2) 
where tk(yi-1, yi, X, i) is a transition feature function of the entire input sequence and the 
labels at positions i and i-1 in the label sequence; sk(yi, X, i) is a state feature function 
of the label at position i and the observed input sequence; and k? and k? are parame-
ters to be estimated from training data. The parameters k? and k?  play similar roles to 
the transition and emission probabilities in HMMs [12]. Therefore, the most probable 
label sequence for input sequence X is Y* which maximizes a posterior probability. 
)|(maxarg* XYPY
Y
?=  (3) 
We can find Y* with dynamic programming using the Viterbi algorithm. 
2.2   Parameter Estimation for CRFs 
Assuming the training data {(X(n), Y(n))} are independently and identically distributed, 
the product of Equation 1 over all training sequences is a likelihood function of the 
parameter ? . Maximum likelihood training chooses parameter values such that the 
log-likelihood is maximized [10]. For CRFs, the log-likelihood )(?L  is given by 
? ??
?
??
???
?
?=
=
?
n
n
i k
nn
i
n
ikk
nn
n
XZiXyyf
XYPL
)(log),,,(
)|(log)(
)()()()(
1
)()(
?
? ?
 (4) 
It is not possible to analytically determine the parameter values that maximize the 
log-likelihood. Instead, maximum likelihood parameters must be identified using an 
iterative technique such as iterative scaling [12] or gradient-based methods [13,14].  
Lafferty et al proposed two iterative scaling algorithms to find parameters for 
CRFs. However, these methods converge into a global maximum very slowly. To 
158 Y.-H. Lee, M.-Y. Kim, and J.-H. Lee 
overcome this problem of slow convergence, several researchers adopted modern 
optimization algorithms such as the conjugate-gradient method or the limited-memory 
BFGS(L-BFGS) method [17]. 
3   Chunking Using Conditional Random Fields in Korean Texts 
We now describe how CRFs are applied to the task of chunking in Korean texts. 
Firstly, we explore characteristics and chunk types of Korean. Then we explain the 
features for the model of chunking in Korean texts using CRFs. The ultimate goal of a 
chunker is to output appropriate chunk tags of a sequence of words with part-of-
speech tags.  
3.1   Characteristics of Korean 
Korean is an agglutinative language, in which a word unit (called an eojeol) is a com-
position of a content word and function word(s). Function words ? postpositions and 
endings ? give much information such as grammatical relation, case, tense, modal, 
etc. Well-developed function words in Korean help with chunking, especially NP and 
VP chunking. For example, when the part-of-speech of current word is one of deter-
miner, pronoun and noun, the following seven rules for NP chunking in Table 1 can 
find most NP chunks in text, with about 89% accuracy [6].  
Table 1. Rules for NP chunking in Korean texts 
No Previous eojeol Chunk tag of current word 
1 determiner I-NP 
2 pronoun I-NP 
3 noun I-NP 
4 noun + possessive postposition I-NP 
5 noun + relative postfix I-NP 
6 adjective + relative ending I-NP 
7 others B-NP 
For this reason, boundaries of chunks are easily found in Korean, compared to 
other languages such as English or Chinese. This is why a rule-based chunking 
method is predominantly used. However, with sophisticated rules, the rule-based 
chunking method has limitations when handling exceptional cases. Park et al pro-
posed a hybrid of the rule-based and the machine learning method to resolve this 
problem [5,6]. Many recent machine learning techniques can capture hidden charac-
teristics for classification. Despite its simplicity and efficiency, the rule-based method 
has recently been outdone by the machine learning method in various classification 
problems. 
3.2   Chunk Types of Korean 
Abney was the first to use the term ?chunk? to represent a non-recursive core of an 
intra-clausal constituent, extending from the beginning of constituent to its head. In 
 Chunking Using Conditional Random Fields in Korean Texts 159 
Korean, there are four basic phrases: noun phrase (NP), verb phrase (VP), adverb 
phrase (ADVP), and independent phrase (IP) [6]. As function words such as postposi-
tion or ending are well-developed, the number of chunk types is small compared to 
other languages such as English or Chinese. Table 2 lists the Korean chunk types, a 
simple explanation and examples of each chunk type. 
Table 2. The Korean chunk types 
No Category Explanation Example 
1 NP Noun Phrase [NP? ???? ???] [???]. ([the beautiful woman] [look]) 
2 VP Verb Phrase [???] [??] [VP???? ??]. ([the roof] [completely] [has fallen in]) 
3 ADVP Adverb Phrase [??] [ADVP ?? ??] [?? ??]. ([a bird] [very high] [is flying]) 
4 IP Independent Phrase [IP ?], [??] [??] [???]. ([wow] [this] [very] [is delicious]) 
Like the CoNLL-2000 dataset, we use three types of chunk border tags, indicating 
whether a word is outside a chunk (O), starts a chunk (B), or continues a chunk (I). 
Each chunk type XP has two border tags: B-XP and I-XP. XP should be one of NP, 
VP, ADVP and IP. There exist nine chunk tags in Korean. 
3.3   Feature Set of CRFs 
One advantage of CRFs is that they can use many arbitrary, overlapping features. So 
we take advantage of all context information of a current word. We use words, part-
of-speech tags of context and combinations of part-of-speech tags to determine the 
chunk tag of the current word,. The window size of context is 5; from left two words 
to right two words. Table 3 summarizes the feature set for chunking in Korean texts. 
Table 3. Feature set for the chunking in Korean texts 
Word POS tag Bi-gram of tags Tri-gram of tags 
wi-2= w 
wi-1= w 
wi= w 
wi+1= w 
wi+2= w 
ti-2= t 
ti-1= t 
ti= t 
ti+1= t 
ti+2= t 
ti-2= t?, ti-1= t 
ti-1= t?, ti= t 
ti= t?, ti+1= t 
ti+1= t?,ti+2= t 
ti-2= t?, ti-1= t?, ti= t 
ti-1= t?, ti= t?, ti+1= t 
ti= t?, ti+1= t?, ti+2= t 
4   Experiments 
In this section, we present experimental results of chunking using CRFs in Korean 
texts and compare the performance with previous systems of Park et al[6]. To make a 
fare comparison, we use the same dataset as Park et al[6]. 
160 Y.-H. Lee, M.-Y. Kim, and J.-H. Lee 
4.1   Data Preparation 
For evaluation of our proposed method, we use the STEP 2000 Korean chunking 
dataset (STEP 2000 dataset)1, which is converted from the parsed KAIST Corpus [9]. 
Table 4. Simple statistics on the STEP 2000 dataset 
Information Value 
POS tags 52 
Words 321,328 
Sentences 12,092 
Chunk tags 9 
Chunks 112,658 
 
 
 
? 
npp B-NP his 
? 
jcm I-NP postposition: possessive 
? 
ncn I-NP book 
? 
jxt I-NP postposition: topic 
?? 
ncpa B-VP destructed 
? 
xsv I-VP be 
? 
ep I-VP pre-final ending : past 
? 
ef I-VP ending : declarative 
. sf O  
Fig. 2. An example of the STEP 2000 dataset 
The STEP 2000 dataset consists of 12,092 sentences. We divide this corpus into 
training data and test data. Training data has 10,883 sentences and test data has 
1,209 sentences, 90% and 10% respectively. Table 4 summarizes characteristics of 
the STEP 2000 dataset. Figure 2 shows an example sentence of the STEP 2000 data-
set and its format is equal to that of CoNLL-2000 dataset. Each line is composed of a 
word, its part-of-speech (POS) tag and a chunk tag. 
4.2   Evaluation Metric 
The standard evaluation metrics for chunking performance are precision, recall and F-
score (F
?=1) [3]. F-score is used for comparisons with other reported results. Each 
equation is defined as follows. 
                                                          
1
 STEP is an abbreviation of Software Technology Enhancement Program. We download this 
dataset from http://bi.snu.ac.kr/~sbpark/Step2000. If you want to know the part-of-speech tags 
used in the STEP 2000 dataset, you can reference KAIST tagset [9]. 
 Chunking Using Conditional Random Fields in Korean Texts 161 
outputinchunksof
chunkscorrectofprecision
#
#
=  (5) 
datatestinchunksof
chunkscorrectof
recall
#
#
=  (6) 
precisionrecall
precisionrecallF
+
??
=
=
2
1?  (7) 
4.3   Experimental Results 
Experiments were performed with C++ implementation of CRFs (FlexCRFs) on 
Linux with 2.4 GHz Pentium IV dual processors and 2.0Gbyte of main memory [18]. 
We use L-BFGS to train the parameters and use a Gaussian prior regularization in 
order to avoid overfitting [20]. 
Table 5. The performance of proposed method 
Chunk tag Precision Recall F-score 
NP 94.23 94.30 94.27 
VP 96.71 96.28 96.49 
ADVP 96.90 97.02 96.96 
IP 99.53 99.07 99.30 
All 95.42 95.31 95.36 
Total number of CRF features is 83,264. As shown in Table 5, the performances of 
most chunk type are 96~100%, very high performance. However, the performance of 
NP chunk type is lowest, 94.27% because the border of NP chunk type is very am-
biguous in case of consecutive nouns. Using more features such as previous chunk tag 
should be able to improve the performance of NP chunk type. 
Table 6. The experimental results of various chunking methods2 
 HMMs DT MBL Rule SVMs Hybrid CRFs 
Precision 73.75 92.29 91.41 91.28 93.63 94.47 95.42 
Recall 76.06 90.45 91.43 92.47 91.48 93.96 95.31 
F-score 74.89 91.36 91.38 91.87 92.54 94.21 95.36 
Park et al reported the performance of various chunking methods [6]. We add the 
experimental results of the chunking methods using HMMs-bigram and CRFs.  
In Table 6, F-score of chunking using CRFs in Korean texts is 97.19%, the highest 
                                                          
2
 Performances of all methods except HMMs and CRFs are cited from the experiment of Park 
et al[6]. They also use the STEP 2000 dataset and similar feature set. Therefore, the compari-
son of performance is reasonable. 
162 Y.-H. Lee, M.-Y. Kim, and J.-H. Lee 
performance of all. It significantly outperforms all others, including machine learning 
methods, rule-based methods and hybrid methods. It is because CRFs have a global 
optimum solution hence overcoming the label bias problem. They also can use many 
arbitrary, overlapping features. 
Figure 3 shows the performance curve on the same test set in terms of the preci-
sion, recall and F-score with respect to the size of training data. In this figure, we can 
see that the performance slowly increases in proportion to the size of training data. 
 
Fig. 3. The performance curve respect to the size of training data 
In the experiment, we can see that CRFs can help improve the performance of 
chunking in Korean texts. CRFs have many promising properties except for the slow 
convergence speed compared to other models. In the next experiment, we have tried 
to analyze the importance of each feature and to make an additional experiment with 
various window sizes and any other useful features. 
5   Conclusion 
In this paper, we proposed a chunking method for Korean texts using CRFs. We ob-
served that the proposed method outperforms other approaches. Experiments on the 
STEP 2000 dataset showed that the proposed method yields an F-score of 95.36%. 
This performance is 2.82% higher than that of SVMs and 1.15% higher than that of 
the hybrid method. CRFs use a number of correlated features and overcome the label 
bias problem. We obtained a very high performance using only small features. Thus, 
if we use more features such as semantic information or collocation, we can obtain a 
better performance. 
From the experiment, we know that the proposed method using CRFs can signifi-
cantly improve the performance of chunking in Korean texts. CRFs are a good frame-
work for labeling an input sequence. In our future work, we will investigate how 
CRFs can be applied to other NLP problems: parsing, semantic analysis and spam 
filtering. Finally, we hope that this work can contribute to the body of research in  
this field. 
 Chunking Using Conditional Random Fields in Korean Texts 163 
Acknowledgements 
This work was supported by the KOSEF through the Advanced Information Technol-
ogy Research Center (AITrc) and by the BK21 Project. 
References 
1. S. Abney: Parsing by chunks. In R. Berwick, S. Abney, and C. Tenny, editors, Principle-
based Parsing. Kluwer Academic Publishers (1991). 
2. L. A. Ramashaw and M. P. Marcus: Text chunking using transformation-based learning. 
Proceedings of the Thired ACL Workshop on Very Large Corpora (1995). 
3. E. F. Tjong Kim Sang and S. Buchholz: Introduction to the CoNLL-2000 shared task: 
Chunking. Proceedings of CoNLL-2000 (2000) 127-132. 
4. T. Kudo and Y. Matsumoto: Chunking with support vector machines. Proceedings of 
NAACL2001, ACL (2001). 
5. Park, S.-B. and Zhang, B.-T.: Combining a Rule-based Method and a k-NN for Chunking 
Korean Text. Proceedings of the 19th International Conference on Computer Processing of 
Oriental Languages (2001) 225-230. 
6. Park, S.-B. and Zhang, B.-T.: Text Chunking by Combining Hand-Crafted Rules and 
Memory-Based Learning. Proceedings of the 41st Annual Meeting of the Association for 
Computational Linguistics (2003) 497-504. 
7. H.-P. Shin: Maximally Efficient Syntactic Parsing with Minimal Resources. Proceedings 
of the Conference on Hangul and Korean Language Information Processing (1999)  
242-244. 
8. M.-Y. Kim, S.-J. Kang and J.-H. Lee: Dependency Parsing by Chunks. Proceedings of the 
27th KISS Spring Conference (1999) 327-329. 
9. J.-T. Yoon and K.-S. Choi: Study on KAIST Corpus, CS-TR-99-139, KAIST CS (1999). 
10. A. L. Berger, S. A. Della Pietra and V. J. Della Pietra: A maximum entropy approach to 
natural language processing. Computational Linguistics, 22(1) (1996) 39-71. 
11. Andrew McCallum, D. Freitag and F. Pereira: Maximum entropy Markov models for in-
formation extraction and segmentation. Proceedings of International Conference on Ma-
chine Learning , Stanford, California (2000) 591-598. 
12. John Lafferty, Andrew McCallum and Fernando Pereira: Conditional Random Fields: 
Probabilistic Models for Segmenting and Labeling Sequence Data. Proceedings of the 18th 
International Conference on Machine Learning (2001) 282-289. 
13. Fei Sha and Fernando Pereira: Shallow Parsing with Conditional Random Fields. Proceed-
ings of Human Language Technology-NAACL, Edmonton, Canada (2003). 
14. Hanna Wallach: Efficient Training of Conditional Random Fields. Thesis. Master of Sci-
ence School of Cognitive Science, Division of Informatics. University of Edinburgh 
(2002). 
15. Yongmei Tan, Tianshun Yao, Qing Chen and Jingbo Zhu: Applying Conditional Random 
Fields to Chinese Shallow Parsing. The 6th International Conference on Intelligent Text 
Processing and Computational Linguistics (CICLing-2005) . LNCS, Vol.3406, Springer, 
Mexico City, Mexico (2005) 167-176. 
16. J. Hammersley and P. Clifford. Markov fields on finite graphs and lattices. Unpublished 
manuscript (1971). 
 
164 Y.-H. Lee, M.-Y. Kim, and J.-H. Lee 
17. D. C. Liu and J. Nocedal: On the limited memory bfgs method for large-scale optimiza-
tion. Mathematic Programming, 45 (1989) 503-528. 
18. Hieu Xuan Phan and Minh Le Nguyen: FlexCRFs: A Flexible Conditional Random Fields 
Toolkit. http:://www.jaist.ac.jp/~hieuxuan/flexcrfs/flexcrfs.html (2004). 
19. D. Pinto, A. McCallum, X. Wei and W. B. Croft: Table extraction using conditional ran-
dom fields. Proceedings of the ACM SIGIR (2003). 
20. S. F. Chen and R. Rosenfeld: A Gaussian prior for smoothing maximum entropy models. 
Technical Report CMU-CS-99-108, Carnegie Mellon University (1999). 
 
The LinGO Redwoods Treebank
Motivation and Preliminary Applications
Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Christopher Manning, Dan Flickinger, and Thorsten Brants
{oe |kristina |manning |dan}@csli.stanford.edu,
shieber@deas.harvard.edu, brants@parc.xerox.com
Abstract
The LinGO Redwoods initiative is a seed activity in the de-
sign and development of a new type of treebank. While sev-
eral medium- to large-scale treebanks exist for English (and
for other major languages), pre-existing publicly available re-
sources exhibit the following limitations: (i) annotation is
mono-stratal, either encoding topological (phrase structure) or
tectogrammatical (dependency) information, (ii) the depth of
linguistic information recorded is comparatively shallow, (iii)
the design and format of linguistic representation in the tree-
bank hard-wires a small, predefined range of ways in which
information can be extracted from the treebank, and (iv) rep-
resentations in existing treebanks are static and over the (often
year- or decade-long) evolution of a large-scale treebank tend
to fall behind the development of the field. LinGO Redwoods
aims at the development of a novel treebanking methodology,
rich in nature and dynamic both in the ways linguistic data can
be retrieved from the treebank in varying granularity and in the
constant evolution and regular updating of the treebank itself.
Since October 2001, the project is working to build the foun-
dations for this new type of treebank, to develop a basic set of
tools for treebank construction and maintenance, and to con-
struct an initial set of 10,000 annotated trees to be distributed
together with the tools under an open-source license.
1 Why Another (Type of) Treebank?
For the past decade or more, symbolic, linguistically ori-
ented methods and statistical or machine learning ap-
proaches to NLP have often been perceived as incompat-
ible or even competing paradigms. While shallow and
probabilistic processing techniques have produced use-
ful results in many classes of applications, they have not
met the full range of needs for NLP, particularly where
precise interpretation is important, or where the variety
of linguistic expression is large relative to the amount
of training data available. On the other hand, deep
approaches to NLP have only recently achieved broad
enough grammatical coverage and sufficient processing
efficiency to allow the use of precise linguistic grammars
in certain types of real-world applications.
In particular, applications of broad-coverage analyti-
cal grammars for parsing or generation require the use of
sophisticated statistical techniques for resolving ambigu-
ities; the transfer of Head-Driven Phrase Structure Gram-
mar (HPSG) systems into industry, for example, has am-
plified the need for general parse ranking, disambigua-
tion, and robust recovery techniques. We observe general
consensus on the necessity for bridging activities, com-
bining symbolic and stochastic approaches to NLP. But
although we find promising research in stochastic pars-
ing in a number of frameworks, there is a lack of appro-
priately rich and dynamic language corpora for HPSG.
Likewise, stochastic parsing has so far been focussed on
information-extraction-type applications and lacks any
depth of semantic interpretation. The Redwoods initia-
tive is designed to fill in this gap.
In the next section, we present some of the motivation
for the LinGO Redwoods project as a treebank develop-
ment process. Although construction of the treebank is
in its early stages, we present in Section 3 some prelim-
inary results of using the treebank data already acquired
on concrete applications. We show, for instance, that
even simple statistical models of parse ranking trained
on the Redwoods corpus built so far can disambiguate
parses with close to 80% accuracy.
2 A Rich and Dynamic Treebank
The Redwoods treebank is based on open-source HPSG
resources developed by a broad consortium of re-
search groups including researchers at Stanford (USA),
Saarbru?cken (Germany), Cambridge, Edinburgh, and
Sussex (UK), and Tokyo (Japan). Their wide distribution
and common acceptance make the HPSG framework and
resources an excellent anchor point for the Redwoods
treebanking initiative.
The key innovative aspect of the Redwoods ap-
proach to treebanking is the anchoring of all linguis-
tic data captured in the treebank to the HPSG frame-
work and a generally-available broad-coverage gram-
mar of English, the LinGO English Resource Grammar
(Flickinger, 2000) as implemented with the LKB gram-
mar development environment (Copestake, 2002). Un-
like existing treebanks, there is no need to define a (new)
form of grammatical representation specific to the tree-
bank. Instead, the treebank records complete syntacto-
semantic analyses as defined by the LinGO ERG and pro-
vide tools to extract different types of linguistic informa-
tion at varying granularity.
The treebanking environment, building on the [incr
tsdb()] profiling environment (Oepen & Callmeier,
2000), presents annotators, one sentence at a time, with
the full set of analyses produced by the grammar. Using
a pre-existing tree comparison tool in the LKB (similar
in kind to the SRI Cambridge TreeBanker; Carter, 1997),
annotators can quickly navigate through the parse for-
est and identify the correct or preferred analysis in the
current context (or, in rare cases, reject all analyses pro-
posed by the grammar). The tree selection tool presents
users, who need little expert knowledge of the underly-
ing grammar, with a range of basic properties that distin-
guish competing analyses and that are relatively easy to
judge. All disambiguating decisions made by annotators
are recorded in the [incr tsdb()] database and thus become
available for (i) later dynamic extraction from the anno-
tated profile or (ii) dynamic propagation into a more re-
cent profile obtained from re-running a newer version of
the grammar on the same corpus.
Important innovative research aspects in this approach
to treebanking are (i) enabling users of the treebank to
extract information of the type they need and to trans-
form the available representation into a form suited to
their needs and (ii) the ability to update the treebank with
an enhanced version of the grammar in an automated
fashion, viz. by re-applying the disambiguating decisions
on the corpus with an updated version of the grammar.
Depth of Representation and Transformation of In-
formation Internally, the [incr tsdb()] database records
analyses in three different formats, viz. (i) as a deriva-
tion tree composed of identifiers of lexical items and con-
structions used to build the analysis, (ii) as a traditional
phrase structure tree labeled with an inventory of some
fifty atomic labels (of the type ?S?, ?NP?, ?VP? et al), and
(iii) as an underspecified MRS (Copestake, Lascarides,
& Flickinger, 2001) meaning representation. While rep-
resentation (ii) will in many cases be similar to the rep-
resentation found in the Penn Treebank, representation
(iii) subsumes the functor ? argument (or tectogrammati-
cal) structure advocated in the Prague Dependency Tree-
bank or the German TiGer corpus. Most importantly,
however, representation (i) provides all the information
required to replay the full HPSG analysis (using the orig-
inal grammar and one of the open-source HPSG process-
ing environments, e.g., the LKB or PET, which already
have been interfaced to [incr tsdb()]). Using the latter ap-
proach, users of the treebank are enabled to extract infor-
mation in whatever representation they require, simply
by reconstructing full analyses and adapting the exist-
ing mappings (e.g., the inventory of node labels used for
phrase structure trees) to their needs. Likewise, the ex-
isting [incr tsdb()] facilities for comparing across compe-
tence and performance profiles can be deployed to evalu-
ate results of a (stochastic) parse disambiguation system,
essentially using the preferences recorded in the treebank
as a ?gold standard? target for comparison.
Automating Treebank Construction Although a pre-
cise HPSG grammar like the LinGO ERG will typically
assign a small number of analyses to a given sentence,
choosing among a few or sometimes a few dozen read-
ings is time-consuming and error-prone. The project is
exploring two approaches to automating the disambigua-
tion task, (i) seeding lexical selection from a part-of-
speech (POS) tagger and (ii) automated inter-annotator
comparison and assisted resolution of conflicts.
Treebank Maintenance and Evolution One of the
challenging research aspects of the Redwoods initiative
is about developing a methodology for automated up-
dates of the treebank to reflect the continuous evolution
of the underlying linguistic framework and of the LinGO
grammar. Again building on the notion of elementary
linguistic discriminators, we expect to explore the semi-
automatic propagation of recorded disambiguating deci-
sions into newer versions of the parsed corpus. While
it can be assumed that the basic phrase structure inven-
tory and granularity of lexical distinctions have stabilized
to a certain degree, it is not guaranteed that one set of
discriminators will always fully disambiguate a more re-
cent set of analyses for the same utterance (as the gram-
mar may introduce new ambiguity), nor that re-playing
a history of disambiguating decisions will necessarily
identify the correct, preferred analysis for all sentences.
A better understanding of the nature of discriminators
and relations holding among them is expected to provide
the foundations for an update procedure that, ultimately,
should be mostly automated, with minimal manual in-
spection, and which can become part of the regular re-
gression test cycle for the grammar.
Scope and Current State of Seeding Initiative The
first 10,000 trees to be hand-annotated as part of the
kick-off initiative are taken from a domain for which the
English Resource Grammar is known to exhibit broad
and accurate coverage, viz. transcribed face-to-face dia-
logues in an appointment scheduling and travel arrange-
ment domain.1 For the follow-up phase of the project, it
is expected to move into a second domain and text genre,
presumably more formal, edited text taken from newspa-
per text or another widely available on-line source. As
of June 2002, the seeding initiative is well underway.
The integrated treebanking environment, combining [incr
tsdb()] and the LKB tree selection tool, has been estab-
lished and has been deployed in a first iteration of anno-
tating the VerbMobil utterances. The approach to parse
selection through minimal discriminators turned out to
be not hard to learn for a second-year Stanford under-
graduate in linguistics, and allowed completion of the
first iteration in less than ten weeks. Table 1 summarizes
the current Redwoods status.
1Corpora of some 50,000 such utterances are readily available from
the VerbMobil project (Wahlster, 2000) and have already been studied
extensively among researchers world-wide.
2Of the four data sets only VM32 has been double-checked by
an expert grammarian and (almost) completely disambiguated to date;
therefore it exhibits an interestingly higher degree of phrasal ambiguity
in the ?active = 1? subset.
total active = 0 active = 1 active > 1 unannotated
corpus ] ?  ? ] ?  ? ] ?  ? ] ?  ? ] ?  ?
VM6 2422 7?7 4?2 32?9 218 8?0 4?4 9?7 1910 7?0 4?0 7?5 80 10?0 4?8 23?8 214 14?9 4?3 287?5
VM13 1984 8?5 4?0 37?9 175 8?5 4?1 9?9 1491 7?2 3?9 7?5 85 9?9 4?5 22?1 233 14?1 4?2 22?1
VM31 1726 6?2 4?5 22?4 164 7?9 4?6 8?0 1360 6?6 4?5 5?9 61 10?1 4?2 14?5 141 13?5 4?7 201?5
VM32 608 7?4 4?3 25?6 51 10?7 4?3 54?4 551 7?9 4?4 19?0 5 12?2 3?9 27?2 1 21?0 6?1 2220?0
Table 1: Redwoods development status as of June 2002: four sets of transcribed and hand-segmented VerbMobil dialogues have
been annotated. The columns are, from left to right, the total number of sentences (excluding fragments) for which the LinGO
grammar has at least one analysis (?]?), average length (???), lexical and structural ambiguity (?? and ???, respectively), followed
by the last four metrics broken down for the following subsets: sentences (i) for which the annotator rejected all analyses (no active
trees), (ii) where annotation resulted in exactly one preferred analysis (one active tree), (iii) those where full disambiguation was
not accomplished through the first round of annotation (more than one active tree), and (iv) massively ambiguous sentences that
have yet to be annotated.2
3 Early Experimental Results
Development of the treebank has just started. Nonethe-
less, we have performed some preliminary experiments
on concrete applications to motivate the utility of the re-
source being developed. In this section, we describe ex-
periments using the Redwoods treebank to build and test
systems for parse disambiguation. As a component, we
build a tagger for the HPSG lexical tags in the treebank,
and report results on this application as well.
Any linguistic system that allows multiple parses
of strings must address the problem of selecting from
among the admitted parses the preferred one. A variety
of approaches for building statistical models of parse se-
lection are possible. At the simplest end, we might look
only at the lexical type sequence assigned to the words
by each parse and rank the parse based on the likelihood
of that sequence. These lexical types ? the preterminals
in the derivation ? are essentially part-of-speech tags, but
encode considerably finer-grained information about the
words. Well-understood statistical part-of-speech tag-
ging technology is sufficient for this approach.
In order to use more information about the parse,
we might examine the entire derivation of the string.
Most probabilistic parsing research ? including, for ex-
ample, work by by Collins (1997), and Charniak (1997)
? is based on branching process models (Harris, 1963).
The HPSG derivations that the treebank makes available
can be viewed as just such a branching process, and
a stochastic model of the trees can be built as a prob-
abilistic context-free grammar (PCFG) model. Abney
(1997) notes important problems with the soundness of
the approach when a unification-based grammar is ac-
tually determining the derivations, motivating the use
of log-linear models (Agresti, 1990) for parse ranking
that Johnson and colleagues further developed (Johnson,
Geman, Canon, Chi, & Riezler, 1999). These models
can deal with the many interacting dependencies and
the structural complexity found in constraint-based or
unification-based theories of syntax.
Nevertheless, the naive PCFG approach has the advan-
tage of simplicity, so we pursue it and the tagging ap-
proach to parse ranking in these proof-of-concept exper-
iments (more recently, we have begun work on building
log-linear models over HPSG signs (Toutanova & Man-
ning, 2002)). The learned models were used to rank
possible parses of unseen test sentences according to the
probabilities they assign to them. We report parse se-
lection performance as percentage of test sentences for
which the correct parse was highest ranked by the model.
(We restrict attention in the test corpus to sentences that
are ambiguous according to the grammar, that is, for
which the parse selection task is nontrivial.) We examine
four models: an HMM tagging model, a simple PCFG, a
PCFG with grandparent annotation, and a hybrid model
that combines predictions from the PCFG and the tagger.
These models will be described in more detail presently.
The tagger that we have implemented is a standard tri-
gram HMM tagger, defining a joint probability distribu-
tion over the preterminal sequences and yields of these
trees. Trigram probabilities are smoothed by linear in-
terpolation with lower-order models. For comparison,
we present the performance of a unigram tagger and an
upper-bound oracle tagger that knows the true tag se-
quence and scores highest the parses that have the correct
preterminal sequence.
The PCFG models define probability distributions
over the trees of derivational types corresponding to the
HPSG analyses of sentences. A PCFG model has parame-
ters ?i, j for each rule Ai ? ? j in the corresponding con-
text free grammar.3 In our application, the nonterminals
in the PCFG Ai are rules of the HPSG grammar used to
build the parses (such as HEAD-COMPL or HEAD-ADJ).
We set the parameters to maximize the likelihood of the
set of derivation trees for the preferred parses of the sen-
tences in a training set. As noted above, estimating prob-
abilities from local tree counts in the treebank does not
provide a maximum likelihood estimate of the observed
data, as the grammar rules further constrain the possible
derivations. Essentially, we are making an assumption of
context-freeness of rule application that does not hold in
the case of the HPSG grammar. Nonetheless, we can still
build the model and use it to rank parses.
3For an introduction to PCFG grammars see, for example, Manning
& Schu?tze (1999).
As previously noted by other researchers (Charniak &
Caroll, 1994), extending a PCFG with grandparent an-
notation improves the accuracy of the model. We imple-
mented an extended PCFG that conditions each node?s
expansion on its parent in the phrase structure tree. The
extended PCFG (henceforth PCFG-GP) has parameters
P(Ak Ai ? ? j |Ak, Ai) . The resulting grammar can be
viewed as a PCFG whose nonterminals are pairs of the
nonterminals of the original PCFG.
The combined model scores possible parses using
probabilities from the PCFG-GP model together with the
probability of the preterminal sequence of the parse tree
according to a trigram tag sequence model. More specif-
ically, for a tree T ,
Score(t) = log(PPCFG-GP(T )) + ? log(PTRIG(tags(T ))
where PTRIG(tags(T )) is the probability of the sequence
of preterminals t1 ? ? ? tn in T according to a trigram tag
model:
PTRIG(t1 ? ? ? tn) =
?n
i=1
P(ti |ti?1, ti?2)
with appropriate treatment of boundaries. The trigram
probabilities are smoothed as for the HMM tagger. The
combined model is relatively insensitive to the relative
weights of the two component models, as specified by ?;
in any case, exact optimization of this parameter was not
performed. We refer to this model as Combined. The
Combined model is not a sound probabilistic model as it
does not define a probability distribution over parse trees.
It does however provide a crude way to combine ancestor
and left context information.
The second column in Table 2 shows the accuracy
of parse selection using the models described above.
For comparison, a baseline showing the expected perfor-
mance of choosing parses randomly according to a uni-
form distribution is included as the first row. The accu-
racy results are averaged over a ten-fold cross-validation
on the data set summarized in Table 1. The data we used
for this experiment was the set of disambiguated sen-
tences that have exactly one preferred parse (comprising
a total of 5312 sentences). Often the stochastic models
we are considering give the same score to several differ-
ent parses. When a model ranks a set of m parses highest
with equal scores and one of those parses is the preferred
parse in the treebank, we compute the accuracy on this
sentence as 1/m.
Since our approach of defining the probability of anal-
yses using derivation trees is different from the tradi-
tional approach of learning PCFG grammars from phrase
structure trees, a comparison of the two is probably in
order. We tested the model PCFG-GP defined over the
corresponding phrase structure trees and its average ac-
curacy was 65.65% which is much lower than the accu-
racy of the same model over derivation trees (71.73%).
This result suggests that the information about grammar
constructions is very helpful for parse disambiguation.
Method Task
tag sel. parse sel.
Random 90.13% 25.81%
Tagger unigram 96.75% 44.15%
trigram 97.87% 47.74%
oracle 100.00% 54.59%
PCFG simple 97.40% 66.26%
grandparent 97.43% 71.73%
combined 98.08% 74.03%
Table 2: Performance of the HMM and PCFG models for the
tag and parse selection tasks (accuracy).
The results in Table 2 indicate that high disambigua-
tion accuracy can be achieved using very simple statisti-
cal models. The performance of the perfect tagger shows
that, informally speaking, roughly half of the information
necessary to disambiguate parses is available in the lexi-
cal types alone. About half of the remaining information
is recovered by our best method, Combined.
An alternative (more primitive) task is the tagging task
itself. It is interesting to know how much the tagging
task can be improved by perfecting parse disambigua-
tion. With the availability of a parser, we can examine the
accuracy of the tag sequence of the highest scoring parse,
rather than trying to tag the word sequence directly. We
refer to this problem as the tag selection problem, by
analogy with the relation between the parsing problem
and the parse selection problem. The first column of Ta-
ble 2 presents the performance of the models on the tag
selection problem. The results are averaged accuracies
over 10 cross-validation splits of the same corpus as the
previous experiment, and show that parse disambigua-
tion using information beyond the lexical type sequence
slightly improves tag selection performance. Note that
in these experiments, the models are used to rank the tag
sequences of the possible parses and not to find the most
probable tag sequence. Therefore tagging accuracy re-
sults are higher than they would be in the latter case.
Since our corpus has relatively short sentences and low
ambiguity it is interesting to see how much the perfor-
mance degrades as we move to longer and more highly
ambiguous sentences. For this purpose, we report in Ta-
ble 3 the parse ranking accuracy of the Combined model
as a function of the number of possible analyses for sen-
tences. Each row corresponds to a set of sentences with
number of possible analyses greater or equal to the bound
shown in the first column. For example, the first row con-
tains information for the sentences with ambiguity ? 2,
which is all ambiguous sentences. The columns show the
total number of sentences in the set, the expected accu-
racy of guessing at random, and the accuracy of the Com-
bined model. We can see that the parse ranking accuracy
is decreasing quickly and more powerful models will be
needed to achieve good accuracy for highly ambiguous
sentences.
Despite several differences in corpus size and compo-
Analyses Sentences Random Combined
? 2 3824 25.81% 74.03%
? 5 1789 9.66% 59.64%
? 10 1027 5.33% 51.61%
? 20 525 3.03% 45.33%
Table 3: Parse ranking accuracy by number of possible parses.
sition, it is perhaps nevertheless useful to compare this
work with other work on parse selection for unification-
based grammars. Johnson et al (1999) estimate a
Stochastic Unification Based Grammar (SUBG) using a
log-linear model. The features they include in the model
are not limited to production rule features but also ad-
junct and argument and other linguistically motivated
features. On a dataset of 540 sentences (total training
and test set) from a Verbmobil corpus they report parse
disambiguation accuracy of 58.7% given a baseline accu-
racy for choosing at random of 9.7%. The random base-
line is much lower than ours for the full data set, but it is
comparable for the random baseline for sentences with
more than 5 analyses. The accuracy of our Combined
model for these sentences is 59.64%, so the accuracies
of the two models seem fairly similar.
4 Related Work
To the best of our knowledge, no prior research has
been conducted exploring the linguistic depth, flexibil-
ity in available information, and dynamic nature of tree-
banks that we have proposed. Earlier work on building
corpora of hand-selected analyses relative to an exist-
ing broad-coverage grammar was carried out at Xerox
PARC, SRI Cambridge, and Microsoft Research. As all
these resources are tuned to proprietary grammars and
analysis engines, the resulting treebanks are not publicly
available, nor have reported research results been repro-
ducible. Yet, especially in light of the successful LinGO
open-source repository, it seems vital that both the tree-
bank and associated processing schemes and stochastic
models be available to the general (academic) public. An
on-going initiative at Rijksuniversiteit Groningen (NL) is
developing a treebank of dependency structures (Mullen,
Malouf, & Noord, 2001), derived from an HPSG-like
grammar of Dutch (Bouma, Noord, & Malouf, 2001).
The general approach resembles the Redwoods initiative
(specifically the discriminator-based method of tree se-
lection; the LKB tree comparison tool was originally de-
veloped by Malouf, after all), but it provides only a sin-
gle stratum of representation, and has no provision for
evolving analyses in tandem with the grammar. Dipper
(2000) presents the application of a broad-coverage LFG
grammar for German to constructing tectogrammatical
structures for the TiGer corpus. The approach is similar
to the Groningen framework, and shares its limitations.
References
Abney, S. P. (1997). Stochastic attribute-value grammars.
Computational Linguistics, 23, 597 ? 618.
Agresti, A. (1990). Categorical data analysis. John Wiley &
Sons.
Bouma, G., Noord, G. van, & Malouf, R. (2001).
Alpino. Wide-coverage computational analysis of Dutch. In
W. Daelemans, K. Sima-an, J. Veenstra, & J. Zavrel (Eds.),
Computational linguistics in the Netherlands (pp. 45 ? 59).
Amsterdam, The Netherlands: Rodopi.
Carter, D. (1997). The TreeBanker. A tool for supervised
training of parsed corpora. In Proceedings of the Workshop
on Computational Environments for Grammar Development
and Linguistic Engineering. Madrid, Spain.
Charniak, E. (1997). Statistical parsing with a context-free
grammar and word statistics. In Proceedings of the Four-
teenth National Conference on Artificial Intelligence (pp.
598 ? 603). Providence, RI.
Charniak, E., & Caroll, G. (1994). Context-sensitive statistics
for improved grammatical language models. In Proceedings
of the Twelth National Conference on Artificial Intelligence
(pp. 742 ? 747). Seattle, WA.
Collins, M. J. (1997). Three generative, lexicalised models for
statistical parsing. In Proceedings of the 35th Meeting of
the Association for Computational Linguistics and the 7th
Conference of the European Chapter of the ACL (pp. 16 ?
23). Madrid, Spain.
Copestake, A. (2002). Implementing typed feature structure
grammars. Stanford, CA: CSLI Publications.
Copestake, A., Lascarides, A., & Flickinger, D. (2001). An
algebra for semantic construction in constraint-based gram-
mars. In Proceedings of the 39th Meeting of the Association
for Computational Linguistics. Toulouse, France.
Dipper, S. (2000). Grammar-based corpus annotation. In
Workshop on linguistically interpreted corpora LINC-2000
(pp. 56 ? 64). Luxembourg.
Flickinger, D. (2000). On building a more efficient grammar
by exploiting types. Natural Language Engineering, 6 (1)
(Special Issue on Efficient Processing with HPSG), 15 ? 28.
Harris, T. E. (1963). The theory of branching processes.
Berlin, Germany: Springer.
Johnson, M., Geman, S., Canon, S., Chi, Z., & Riezler, S.
(1999). Estimators for stochastic ?unification-based? gram-
mars. In Proceedings of the 37th Meeting of the Associa-
tion for Computational Linguistics (pp. 535 ? 541). College
Park, MD.
Manning, C. D., & Schu?tze, H. (1999). Foundations of statis-
tical Natural Language Processing. Cambridge, MA: MIT
Press.
Mullen, T., Malouf, R., & Noord, G. van. (2001). Statistical
parsing of Dutch using Maximum Entropy models with fea-
ture merging. In Proceedings of the Natural Language Pro-
cessing Pacific Rim Symposium. Tokyo, Japan.
Oepen, S., & Callmeier, U. (2000). Measure for mea-
sure: Parser cross-fertilization. Towards increased compo-
nent comparability and exchange. In Proceedings of the 6th
International Workshop on Parsing Technologies (pp. 183 ?
194). Trento, Italy.
Toutanova, K., & Manning, C. D. (2002). Feature selection
for a rich HPSG grammar using decision trees. In Proceed-
ings of the sixth conference on natural language learning
(CoNLL-2002). Taipei.
Wahlster, W. (Ed.). (2000). Verbmobil. Foundations of speech-
to-speech translation. Berlin, Germany: Springer.
The Grammar Matrix: An Open-Source Starter-Kit for the Rapid
Development of Cross-Linguistically Consistent Broad-Coverage Precision
Grammars
Emily M. Bender and Dan Flickinger and Stephan Oepen
Center for the Study of Language and Information
Stanford University
fbender jdan joeg@csli.stanford.edu
Abstract
The grammar matrix is an open-source
starter-kit for the development of broad-
coverage HPSGs. By using a type hierar-
chy to represent cross-linguistic generaliza-
tions and providing compatibility with other
open-source tools for grammar engineering,
evaluation, parsing and generation, it facil-
itates not only quick start-up but also rapid
growth towards the wide coverage necessary
for robust natural language processing and
the precision parses and semantic represen-
tations necessary for natural language under-
standing.
1 Introduction
The past decade has seen the development of
wide-coverage implemented grammars represent-
ing deep linguistic analysis of several languages
in several frameworks, including Head-Driven
Phrase Structure Grammar (HPSG), Lexical-
Functional Grammar (LFG), and Lexicalized Tree
Adjoining Grammar (LTAG). In HPSG, the most ex-
tensive grammars are those of English (Flickinger,
2000), German (Mu?ller & Kasper, 2000), and
Japanese (Siegel, 2000; Siegel & Bender, 2002).
Despite being couched in the same general frame-
work and in some cases being written in the
same formalism and consequently being compati-
ble with the same parsing and generation software,
these grammars were developed more or less inde-
pendently of each other. They each represent be-
tween 5 and 15 person years of research efforts,
and comprise 35?70,000 lines of code. Unfor-
tunately, most of that research is undocumented
and the accumulated analyses, best practices for
grammar engineering, and tricks of the trade are
only available through painstaking inspection of
the grammars and/or consultation with their au-
thors. This lack of documentation holds across
frameworks, with certain notable exceptions, in-
cluding Alshawi (1992), Mu?ller (1999), and Butt,
King, Nin?o, & Segond (1999).
Grammars which have been under development
for many years tend to be very difficult to mine for
information, as they contain layers upon layers of
interacting analyses and decisions made in light of
various intermediate stages of the grammar. As a
result, when embarking on the creation of a new
grammar for another language, it seems almost
easier to start from scratch than to try to model it on
an existing grammar. This is unfortunate?being
able to leverage the knowledge and infrastructure
embedded in existing grammars would greatly ac-
celerate the process of developing new ones. At the
same time, these grammars represent an untapped
resource for the bottom-up exploration of language
universals.
As part of the LinGO consortium?s multi-lingual
grammar engineering effort, we are developing a
?grammar matrix? or starter-kit, distilling the wis-
dom of existing grammars and codifying and doc-
umenting it in a form that can be used as the basis
for new grammars.
In the following sections, we outline the inven-
tory of a first, preliminary version of the grammar
matrix, discuss the interaction of basic construc-
tion types and semantic composition in unification
grammars by means of a detailed example, and
consider extensions to the core inventory that we
foresee and an evaluation methodology for the ma-
trix proper.
2 Preliminary Development of Matrix
We have produced a preliminary version of the
grammar matrix relying heavily on the LinGO
project?s English Resource Grammar, and to a
lesser extent on the Japanese grammar developed
jointly between DFKI Saarbru?cken (Germany) and
YY Technologies (Mountain View, CA). This early
version of the matrix comprises the following com-
ponents:
 Types defining the basic feature geometry and
technical devices (e.g., for list manipulation).
 Types associated with Minimal Recursion Se-
mantics (see, e.g., Copestake, Lascarides, &
Flickinger, 2001), a meaning representation
language which has been shown to be well-
suited for semantic composition in typed fea-
ture structure grammars. This portion of the
grammar matrix includes a hierarchy of rela-
tion types, types and constraints for the prop-
agation of semantic information through the
phrase structure tree, a representation of illo-
cutionary force, and provisions for grammar
rules which make semantic contributions.
 General classes of rules, including deriva-
tional and inflectional (lexical) rules, unary
and binary phrase structure rules, headed and
non-headed rules, and head-initial and head-
final rules. These rule classes include im-
plementations of general principles of HPSG,
like, for example, the Head Feature and Non-
Local Feature Principles.
 Types for basic constructions such as head-
complement, head-specifier, head-subject,
head-filler, and head-modifier rules, coordi-
nation, as well as more specialized classes
of constructions, such as relative clauses and
noun-noun compounding. Unlike in specific
grammars, these types do not impose any or-
dering on their daughters in the grammar ma-
trix.
Included with the matrix are configuration and
parameter files for the LKB grammar engineering
environment (Copestake, 2002).
Although small, this preliminary version of
the matrix already reflects the main goals of
the project: (i) Consistent with other work in
HPSG, semantic representations and in particular
the syntax-semantics interface are developed in de-
tail; (ii) the types of the matrix are each represen-
tations of generalizations across linguistic objects
and across languages; and (iii) the richness of the
matrix and the incorporation of files which connect
it with the LKB allow for extremely quick start-up
as the matrix is applied to new languages.
Since February 2002, this preliminary version of
the matrix has been in use at two Norwegian uni-
versities, one working towards a broad-coverage
reference implementation of Norwegian (NTNU),
the other?for the time being?focused on specific
aspects of clause structure and lexical description
(Oslo University). In the first experiment with
the matrix, at NTNU, basic Norwegian sentences
were parsing and producing reasonable semantics
within two hours of downloading the matrix files.
Linguistic coverage should scale up quickly, since
the foundation supplied by the matrix is designed
not only to provide a quick start, but also to support
long-term development of broad-coverage gram-
mars. Both initiatives have confirmed the utility of
the matrix starter kit and already have contributed
to a series of discussions on cross-lingual HPSG
design aspects, specifically in the areas of argu-
ment structure representations in the lexicon and
basic assumptions about constituent structure (in
one view, Norwegian exhibits a VSO topology in
the main clause). The user groups have suggested
refinements and extensions of the basic inventory,
and it is expected that general solutions, as they are
identified jointly, will propagate into the existing
grammars too.
3 A Detailed Example
As an example of the level of detail involved in
the grammar matrix, in this section we consider
the analysis of intersective and scopal modifica-
tion. The matrix is built to give Minimal Recursion
Semantics (MRS; Copestake et al, 2001; Copes-
take, Flickinger, Sag, & Pollard, 1999; Copestake,
Flickinger, Malouf, Riehemann, & Sag, 1995) rep-
resentations. The two English examples in (1)
exemplify the difference between intersective and
scopal modification:1
(1) a. Keanu studied Kung Fu on a spaceship.
b. Keanu probably studied Kung Fu.
The MRSs for (1a-b) (abstracting away from
agreement information) are given in (2) and (3).
The MRSs are ordered tuples consisting of a top
handle (h1 in both cases), an instance or event vari-
able (e in both cases), a bag of elementary predica-
tions (eps), and a bag of scope constraints (in these
cases, QEQ constraints or ?equal modulo quanti-
fiers?). In a well-formed MRS, the handles can be
1These examples also differ in that probably is a pre-
head modifier while on a spaceship is a post-head modifier.
This word-order distinction cross-cuts the semantic distinc-
tion, and our focus is on the latter, so we won?t consider the
word-order aspects of these examples here.
identified in one or more ways respecting the scope
constraints such that the dependencies between the
eps form a tree. For a detailed description of MRS,
see the works cited above. Here, we will focus on
the difference between the intersective modifier on
(a spaceship) and the scopal modifier probably.
In (2), the ep contributed by on (?on-rel?) shares
its handle (h7) with the ep contributed by the verb
it is modifying (?study-rel?). As such, the two will
always have the same scope; no quantifier can in-
tervene. Further, the second argument of the on-rel
(e) is the event variable of the study-rel. The first
argument, e0, is the event variable of the on-rel and
the third argument, z, is the instance variable of the
spaceship-rel.
(2) h h1, e,
f h1:prpstn-rel(h2), h3:def-np-rel(x, h4, h5),
h6:named-rel(x, ?Keanu?), h7:study-rel(e, x, y),
h8:def-np-rel(y, h9, h10),
h11:named-rel(y, ?Kung Fu?), h7:on-rel(e0, e, z),
h12:a-quant-rel(z, h13, h14),
h15:spaceship-rel(z) g,
f h2 QEQ h7, h4 QEQ h6, h19 QEQ h11,
h13 QEQ h15 g i
In (3), the ep contributed by the scopal modifier
probably (?probably-rel?) has its own handle (h7)
which is not shared by anything. Furthermore, it
takes a handle (h8) rather than the event variable
of the study-rel as its argument. h8 is equal mod-
ulo quantifiers (QEQ) to the handle of the study-rel
(h9), and h7 is equal modulo quantifiers to the ar-
gument of the prpstn-rel (h2). The prpstn-rel is the
ep representing the illocutionary force of the whole
expression. This means that quantifiers associated
with the NPs Keanu and Kung Fu can scope inside
or outside probably.
(3) h h1, e,
f h1:prpstn-rel(h2), h3:def-np-rel(x, h4, h5),
h6:named-rel(x, ?Keanu?),
h7:probably-rel(h8), h9:study-rel(e, x, y),
h10:def-np-rel(y, h11, h12),
h13:named-rel(y, ?Kung Fu?) g,
f h2 QEQ h7, h4 QEQ h6, h8 QEQ h9,
h11 QEQ h13 g i
While the details of modifier placement, which
parts of speech can modify which kinds of phrases,
etc., differ across languages, we believe that all
languages display a distinction between scopal and
intersective modification. Accordingly, the types
isect-mod-phrase := head-mod-phr-simple &
[ HEAD-DTR.SYNSEM.LOCAL
[ CONT [ TOP #hand,
INDEX #index ],
KEYS.MESSAGE 0-dlist ],
NON-HEAD-DTR.SYNSEM.LOCAL
[ CAT.HEAD.MOD <[ LOCAL isect-mod ]>,
CONT.TOP #hand ],
C-CONT.INDEX #index ].
Figure 1: TDL description of isect-mod-phrase
scopal-mod-phrase := head-mod-phr-simple &
[ NON-HEAD-DTR.SYNSEM.LOCAL
[ CAT.HEAD.MOD <[ LOCAL scopal-mod ]>,
CONT.INDEX #index ],
C-CONT.INDEX #index ].
Figure 2: TDL description of scopal-mod-phrase
necessary for describing these two kinds of modi-
fication are included in the matrix.
The types isect-mod-phrase and scopal-mod-
phrase (shown in Figures 1 and 2) encode the in-
formation necessary to build up in a compositional
manner the modifier portions of the MRSs in (2)
and (3).
These types are embedded in the type hierar-
chy of the matrix. Through their supertype head-
mod-phr-simple they inherit information common
to many types of phrases, including the basic fea-
ture geometry, head feature and non-local feature
passing, and semantic compositionality. These
types also have subtypes in the matrix specifying
the two word-order possibilities (pre- or post-head
modifiers), giving a total of four subtypes.2
The most important difference between these
types is in the treatment of the handle of the head
daughter?s semantics, to distinguish intersective
and scopal modification. In isect-mod-phrase, the
top handles (TOP) of the head and non-head (i.e.,
modifier) daughters are identified (#hand). This
allows for MRSs like (2) where the eps contributed
by the head (?study-rel?) and the modifier (?on-rel?)
take the same scope. The type scopal-mod-phrase
bears no such constraint. This allows for MRSs
like (3) where the modifier?s semantic contribution
(?probably-rel?) takes the handle of the head?s se-
mantics (?study-rel?) as its argument, so that the
modifier outscopes the head. In both types of mod-
2All four subtypes are provided on the theory that most
languages will make use of all or most of them.
ifier phrase, a constraint inherited from the super-
type ensures that the handle of the modifier is also
the handle of the whole phrase.
The constraints on the LOCAL value inside
the modifier?s MOD value regulate which lexi-
cal items can appear in which kind of phrase.
Intersective modifiers specify lexically that they
are [ MOD h [ LOCAL isect-mod ] i] and sco-
pal modifiers specify lexically that they are
[ MOD h [ LOCAL scopal-mod ] i].3 These con-
straints exemplify the kind of information that will
be developed in the lexical hierarchy of the matrix.
It is characteristic of broad-coverage grammars
that every particular analysis interacts with many
other analyses. Modularization is an on-going con-
cern, both for maintainability of individual gram-
mars, and for providing the right level of abstrac-
tion in the matrix. For the same reasons, we have
only been able to touch on the highlights of the se-
mantic analysis of modification here, but hope that
this quick tour will suffice to illustrate the extent
of the jump-start the matrix can give in the devel-
opment of new grammars.
4 Future Extensions
The initial version of the matrix, while sufficient to
support some useful grammar work, will require
substantial further development on several fronts,
including lexical representation, syntactic gener-
alization, sociolinguistic variation, processing is-
sues, and evaluation. This first version drew most
heavily from the implementation of the English
grammar, with some further insights drawn from
the grammar of Japanese. Extensions to the ma-
trix will be based on careful study of existing im-
plemented grammars for other languages, notably
German, Spanish and Japanese, as well as feed-
back from those using the first version of the ma-
trix.
For lexical representation, one of the most ur-
gent needs is to provide a language-independent
type hierarchy for the lexicon, at least for major
parts of speech, establishing the mechanisms used
for linking syntactic subcategorization to seman-
tic predicate-argument structure. Lexical rules pro-
vide a second mechanism for expressing general-
3Note that there are no further subtypes of LOCAL values
beyond isect-mod and scopal-mod. Since these grammars do
not make extensive use of subtypes of LOCAL values, they
were available for encoding this distinction. Alternative solu-
tions include positing a new feature.
izations within the lexicon, and offer ready oppor-
tunities for cross-linguistic abstractions for both
inflectional and derivational regularities. Work is
also progressing on establishing a standard rela-
tional database (using PostgreSQL) for storing in-
formation for the lexical entries themselves, im-
proving both scalability and clarity compared to
the current simple text file representation. Form-
based tools will be provided both for constructing
lexical entries and for viewing the contents of the
lexicon.
The primary focus of work on syntactic general-
ization in the matrix is to support more freedom
in word order, for both complements and modi-
fiers. The first step will be a relatively conserva-
tive extension along the lines of Netter (1996), al-
lowing the grammar writer more control over how
a head combines with complements of different
types, and their interleaving with modifier phrases.
Other areas of immediate cross-linguistic interest
include the hierarchy of head types, control phe-
nomena, clitics, auxiliary verbs, noun-noun com-
pounds, and more generally, phenomena that in-
volve the word/phrase distinction, such as noun in-
corporation. A study of the existing grammars for
English, German, Japanese, and Spanish reveals
a high degree of language-specificity for several
of these phenomena, but also suggests promise of
reusable abstractions.
Several kinds of sociolinguistic variation require
extensions to the matrix, including grammaticized
aspects of pragmatics such as politeness and em-
pathy, as well as dialect and register alternations.
The grammar of Japanese provides a starting point
for representations of both empathy and politeness.
Implementations of familiar vs. formal verb forms
in German and Spanish provide further instances
of politeness to help build the cross-linguistic ab-
stractions. Extensions for dialect variation will
build on some exploratory work in adapting the
English grammar to support American, British,
and Australian regionalisms, both lexical and syn-
tactic, while restricting dialect mixture in genera-
tion and associated spurious ambiguity in parsing.
While the development of the matrix will be
built largely on the LKB platform, support will also
be needed for using the emerging grammars on
other processing platforms, and for linking to other
packages for pre-processing the linguistic input.
Several other platforms exist which can efficiently
parse text using the existing grammars, includ-
ing the PET system developed in C++ at Saarland
University (Germany) and the DFKI (Callmeier,
2000); the PAGE system developed in Lisp at the
DFKI (Uszkoreit et al, 1994); the LiLFeS system
developed at Tokyo University (Makino, Yoshida,
Torisawa, & Tsujii, 1998), and a parallel process-
ing system developed in Objective C at Delft Uni-
versity (The Netherlands; van Lohuizen, 2002).
As part of the matrix package, sample configura-
tion files and documentation will be provided for
at least some of these additional platforms.
Existing pre-processing packages can also sig-
nificantly reduce the effort required to develop
a new grammar, particularly for coping with the
morphology/syntax interface. For example, the
ChaSen package for segmenting Japanese input
into words and morphemes (Asahara & Mat-
sumoto, 2000) has been linked to at least the LKB
and PET systems. Support for connecting im-
plementations of language-specific pre-processing
packages of this kind will be preserved and ex-
tended as the matrix develops. Likewise, config-
uration files are included to support generation, at
least within the LKB, provided that the grammar
conforms to certain assumptions about semantic
representation using the Minimal Recursion Se-
mantics framework.
Finally, a methodology is under development for
constructing and using test suites organized around
a typology of linguistic phenomena, using the im-
plementation platform of the [incr tsdb()] profil-
ing package (Oepen & Flickinger, 1998; Oepen
& Callmeier, 2000). These test suites will enable
better communication about current coverage of a
given grammar built using the matrix, and serve as
the basis for identifying additional phenomena that
need to be addressed cross-linguistically within the
matrix. Of course, the development of the typol-
ogy of phenomena is itself a major undertaking
for which a systematic cross-linguistic approach
will be needed, a discussion of which is outside
the scope of this report. But the intent is to seed
this classification scheme with a set of relatively
coarse-grained phenomenon classes drawn from
the existing grammars, then refine the typology as
it is applied to these and new grammars built using
the matrix.
5 Case Studies
One important part of the matrix package will be a
library of phenomenon-based analyses drawn from
the existing grammars and over time from users of
the matrix, to provide working examples of how
the matrix can be applied and extended. Each case
study will be a set of grammar files, simplified for
relevance, along with documentation of the anal-
ysis, and a test suite of sample sentences which
define the range of data covered by the analysis.
This library, too, will be organized around the ty-
pology of phenomena introduced above, but will
also make explicit reference to language families,
since both similarities and differences among re-
lated languages will be of interest in these case
studies. Examples to be included in the first re-
lease of this library include numeral classifiers in
Japanese, subject pro drop in Spanish, partial-VP
fronting in German, and verb diathesis in Norwe-
gian.
6 Evaluation and Evolution
The matrix itself is not a grammar but a collec-
tion of generalizations across grammars. As such,
it cannot be tested directly on corpora from partic-
ular languages, and we must find other means of
evaluation. We envision overall evaluation of the
matrix based on case studies of its performance
in helping grammar engineers quickly start new
grammars and in helping them scale those gram-
mars up. Evaluation in detail will based on au-
tomatable deletion/substitution metrics, i.e., tools
that determine which types from the matrix get
used as is, which get used with modifications, and
which get ignored in various matrix-derived gram-
mars. Furthermore, if the matrix evolves to include
defeasible constraints, these tools will check which
constraints get overridden and whether the value
chosen is indeed common enough to be motivated
as a default value. This evaluation in detail should
be paired with feedback from the grammar engi-
neers to determine why changes were made.
The main goal of evaluation is, of course, to im-
prove the matrix over time. This raises the ques-
tion of how to propagate changes in the matrix to
grammars based on earlier versions. The following
three strategies (meant to be used in combination)
seem promising: (i) segregate changes that are im-
portant to sync to (e.g., changes that affect MRS
outputs, fundamental changes to important anal-
yses), (ii) develop a methodology for communi-
cating changes in the matrix, their motivation and
their implementation to the user community, and
(iii) develop tools for semi-automating resynching
of existing grammars to upgrades of the matrix.
These tools could use the type hierarchy to predict
where conflicts are likely to arise and bring these
to the engineer?s attention, possibly inspired by the
approach under development at CSLI for the dy-
namic maintenance of the LinGO Redwoods tree-
bank (Oepen et al, 2002).
Finally, while initial development of the ma-
trix has been and will continue to be highly cen-
tralized, we hope to provide support for proposed
matrix improvements from the user community.
User feedback will already come in the form of
case studies for the library as discussed in Sec-
tion 5 above, but also potentially in proposals for
modification of the matrix drawing on experiences
in grammar development. In order to provide
users with some cross-linguistic context in which
to develop and evaluate such proposals themselves,
we intend to provide some sample matrix-derived
grammars and corresponding testsuites with the
matrix. A user could thus make a proposed change
to the matrix, run the testsuites for several lan-
guages using the supplied grammars which draw
from that changed matrix, and use [incr tsdb()]
to determine which phenomena have been affected
by the change. It is clear that full automation of
this evaluation process will be difficult, but at least
some classes of changes to the matrix will per-
mit this kind of quick cross-linguistic feedback to
users with only a modest amount of additional in-
frastructure.
7 Conclusion
This project carries linguistic, computational, and
practical interest. The linguistic interest lies in the
HPSG community?s general bottom-up approach
to language universals, which involves aiming for
good coverage of a variety of languages first, and
leaving the task of what they have in common for
later. (Of course, theory building is never purely
data-driven, and there are substantive hypotheses
within HPSG about language universals.) Now
that we have implementations with fairly extensive
coverage for a somewhat typologically diverse set
of languages, it is a good time to take the next step
in this program, working to extract and generalize
what is similar across these existing wide-coverage
grammars. Moreover, the central role of types in
the representation of linguistic generalizations en-
ables the kind of underspecification which is useful
for expressing what is common among related lan-
guages while allowing for the further specializa-
tion which necessarily distinguishes one language
from another.
The computational interest is threefold. First
there is the question of what formal devices the
grammar matrix will require. Should it include
defaults? What about domain union (linearization
theory)? The selection and deployment of formal
devices should be informed by on-going research
on processing schemes, and here the crosslinguis-
tic perspective can be particularly helpful. Where
there are several equivalent analyses of the same
linguistic phenomena (e.g., morphosyntactic am-
biguity or optionality), the choice of analysis can
have processing implications that aren?t necessar-
ily apparent in a single grammar. Second, having
a set of wide-coverage HPSGs with fairly standard-
ized fundamentals could prove interesting for re-
search on stochastic processing and disambigua-
tion, especially if the languages differ in gross ty-
pological features such as word order. Finally,
there are also computational issues involved in
how the grammar matrix would evolve over time
as it is used in new grammars. The matrix en-
ables the developer of a grammar for a new lan-
guage to get a quick start on producing a system
that parses and generates with non-trivial seman-
tics, while also building the foundation for a wide-
coverage grammar of the language. But the matrix
itself may well change in parallel with the devel-
opment of the grammar for a particular language,
so appropriate mechanisms must be developed to
support the merging of enhancements to both.
There is also practical industrial benefit to this
project. Companies that are consumers of these
grammars benefit when grammars of multiple lan-
guages work with the same parsing and generation
algorithms and produce standardized semantic rep-
resentations derived from a rich, linguistically mo-
tivated syntax-semantics interface. More impor-
tantly, the grammar matrix will help to remove one
of the primary remaining obstacles to commercial
deployment of grammars of this type and indeed of
the commercial use of deep linguistic analysis: the
immense cost of developing the resource.
Acknowledgements
Since the grammar matrix draws on prior re-
search and existing grammars, it necessarily re-
flects contributions from many people. Rob
Malouf, Jeff Smith, John Beavers, and Kathryn
Campbell-Kibler have contributed to the LinGO
ERG; Melanie Siegel is the original developer for
the Japanese grammar. Tim Baldwin, Ann Copes-
take, Ivan Sag, Tom Wasow, and other members
of the LinGO Laboratory at CSLI have had a great
deal of influence on the design of the grammatical
analyses and corresponding MRS representations.
Warmest thanks to Lars Hellan and his colleagues
at NTNU and Jan Tore L?nning and his students
at Oslo University for their cooperation, patience,
and tolerance.
References
Alshawi, H. (Ed.). (1992). The Core Language Engine.
Cambridge, MA: MIT Press.
Asahara, M., & Matsumoto, Y. (2000). Extended mod-
els and tools for high-performance part-of-speech
tagger. In Proceedings of the 18th International
Conference on Computational Linguistics (pp. 21 ?
27). Saarbru?cken, Germany.
Butt, M., King, T. H., Nin?o, M.-E., & Segond, F.
(1999). A grammar writer?s cookbook. Stanford,
CA: CSLI Publications.
Callmeier, U. (2000). PET ? A platform for ex-
perimentation with efficient HPSG processing tech-
niques. Natural Language Engineering, 6 (1) (Spe-
cial Issue on Efficient Processing with HPSG), 99 ?
108.
Copestake, A. (2002). Implementing typed feature
structure grammars. Stanford, CA: CSLI Publica-
tions.
Copestake, A., Flickinger, D., Malouf, R., Riehemann,
S., & Sag, I. (1995). Translation using minimal re-
cursion semantics. In Proceedings of the Sixth In-
ternational Conference on Theoretical and Method-
ological Issues in Machine Translation. Leuven,
Belgium.
Copestake, A., Flickinger, D. P., Sag, I. A., & Pol-
lard, C. (1999). Minimal Recursion Semantics. An
introduction. in preparation, CSLI Stanford, Stan-
ford, CA.
Copestake, A., Lascarides, A., & Flickinger, D. (2001).
An algebra for semantic construction in constraint-
based grammars. In Proceedings of the 39th Meet-
ing of the Association for Computational Linguistics.
Toulouse, France.
Flickinger, D. (2000). On building a more efficient
grammar by exploiting types. Natural Language En-
gineering, 6 (1) (Special Issue on Efficient Process-
ing with HPSG), 15 ? 28.
van Lohuizen, M. (2002). Efficient and thread-safe
unification with LinGO. In S. Oepen, D. Flickinger,
J. Tsujii, & H. Uszkoreit (Eds.), Collaborative
language engineering. A case study in efficient
grammar-based processing. Stanford, CA: CSLI
Publications. (forthcoming)
Makino, T., Yoshida, M., Torisawa, K., & Tsujii, J.
(1998). LiLFeS ? towards a practical HPSG parser.
In Proceedings of the 17th International Conference
on Computational Linguistics and the 36th Annual
Meeting of the Association for Computational Lin-
guistics (pp. 807 ? 11). Montreal, Canada.
Mu?ller, S. (1999). Deutsche syntax deklarativ. Head-
Driven Phrase Structure Grammar fu?r das Deutsche.
Tu?bingen, Germany: Max Niemeyer Verlag.
Mu?ller, S., & Kasper, W. (2000). HPSG analysis of
German. In W. Wahlster (Ed.), Verbmobil. Foun-
dations of speech-to-speech translation (Artificial
Intelligence ed., pp. 238 ? 253). Berlin, Germany:
Springer.
Netter, K. (1996). Functional categories in an HPSG
for German. Unpublished doctoral dissertation,
Saarland University, Saarbru?cken, Germany.
Oepen, S., & Callmeier, U. (2000). Measure for mea-
sure: Parser cross-fertilization. Towards increased
component comparability and exchange. In Pro-
ceedings of the 6th International Workshop on Pars-
ing Technologies (pp. 183 ? 194). Trento, Italy.
Oepen, S., & Flickinger, D. P. (1998). Towards sys-
tematic grammar profiling. Test suite technology ten
years after. Journal of Computer Speech and Lan-
guage, 12 (4) (Special Issue on Evaluation), 411 ?
436.
Oepen, S., Toutanova, K., Shieber, S., Manning, C.,
Flickinger, D., & Brants, T. (2002). The LinGO
Redwoods treebank. Motivation and preliminary ap-
plications. In Proceedings of the 19th International
Conference on Computational Linguistics. Taipei,
Taiwan.
Siegel, M. (2000). HPSG analysis of Japanese.
In W. Wahlster (Ed.), Verbmobil. Foundations of
speech-to-speech translation (Artificial Intelligence
ed., pp. 265 ? 280). Berlin, Germany: Springer.
Siegel, M., & Bender, E. M. (2002). Efficient deep
processing of japanese. In Proceedings of the 19th
International Conference on Computational Linguis-
tics. Taipei, Taiwan.
Uszkoreit, H., Backofen, R., Busemann, S., Diagne,
A. K., Hinkelman, E. A., Kasper, W., Kiefer, B.,
Krieger, H.-U., Netter, K., Neumann, G., Oepen, S.,
& Spackman, S. P. (1994). DISCO ? an HPSG-
based NLP system and its application for appoint-
ment scheduling. In Proceedings of the 15th Inter-
national Conference on Computational Linguistics.
Kyoto, Japan.
Parallel Distributed Grammar Engineering for Practical Applications
Stephan Oepen?, Emily M. Bender?, Uli Callmeier?, Dan Flickinger??, Melanie Siegel?
?CSLI Stanford ?YY Technologies ?DFKI GmbH
Stanford (CA) Mountain View (CA) Saarbru?cken (Germany)
?
?
?
oe
bender
dan
?
?
?
@csli.stanford.edu
{
uc
dan
}
@yy.com siegel@dfki.de
Abstract
Based on a detailed case study of paral-
lel grammar development distributed across
two sites, we review some of the require-
ments for regression testing in grammar en-
gineering, summarize our approach to sys-
tematic competence and performance profil-
ing, and discuss our experience with gram-
mar development for a commercial applica-
tion. If possible, the workshop presentation
will be organized around a software demon-
stration.
1 Background
The production of large-scale constraint-based
grammars and suitable processing environments is
a labour- and time-intensive process that, maybe,
has become somewhat of a growth industry over
the past few years, as companies explore products
that incorporate grammar-based language process-
ing. Many broad-coverage grammars have been
developed over several years, sometimes decades,
typically coordinated by a single grammarian who
would often draw on additional contributors (e.g.
the three HPSG implementations developed as part
of the VerbMobil effort, see Flickinger, Copes-
take, & Sag, 2000, Mu?ller & Kasper, 2000, and
Siegel, 2000; or the LFG implementations devel-
oped within the ParGram consortium, Butt, King,
Nin?o, & Segond, 1999).
More recently, we also find genuinely shared
and distributed development of broad-coverage
grammars, and we will use one such initiative as an
example?viz. an open-source HPSG implementa-
tion for Japanese jointly developed between DFKI
Saarbru?cken (Germany) and YY Technologies
(Mountain View, CA)?to demonstrate the techno-
logical and methodological challenges present in
distributed grammar and system engineering.
2 Parallel Distributed Grammar
Development?A Case Study
The Japanese grammar builds on earlier work per-
formed jointly between DFKI and the Computa-
tional Linguistics Department at Saarland Univer-
sity (Germany) within VerbMobil; much like for
the German VerbMobil grammar, two people were
contributing to the grammar in parallel, one build-
ing out syntactic analyses, the other charged with
integrating semantic composition into the syntax.
This relatively strict separation of responsibilities
mostly enabled grammarians to serialize incre-
mental development of the resource: the syntacti-
cian would supply a grammar with extended cov-
erage to the semanticist and, at the onset of the fol-
lowing iteration, start subsequent work on syntax
from the revised grammar.
In the DFKI ? YY cooperation the situation was
quite different. Over a period of eight months,
both partners had a grammarian working on syn-
tax and semantics simultaneously on a day-to-
day basis; both grammarians were submitting
changes to a joint, version-controlled source repos-
itory and usually would start the work day by re-
trieving the most recent revisions. At the same
time, product building and the development of
so-called ?domain libraries? (structured collections
of knowledge about a specific domain that is in-
stantiated from semantic representations delivered
from grammatical analysis) at YY already incorpo-
rated the grammar and depended on it for actual,
customer-specific contracts. Due to a continuous
demand for improvements in coverage and analy-
sis accuracy, the grammar used in the main product
line would be updated from the current develop-
ment version about once or twice a week. Parallel
to work on the Japanese grammar (and simultane-
ous work on grammars for English and Spanish),
both the grammar development environment (the
open-source LKB system; Copestake, 2002) and
the HPSG run-time component powering the YY
linguistic analysis engine (the open-source PET
parser; Callmeier, 2002) continued to evolve, as
did the YY-proprietary mapping of meaning repre-
sentations extracted from the HPSG grammars into
domain knowledge?all central parts of a complex
system of interacting components and constraints.
As has been argued before (see, for exam-
ple, Oepen & Flickinger, 1998), the nature of a
large-scale constraint-based grammar and the sub-
tle interactions of lexical and constructional con-
straints make it virtually impossible to predict how
a change in one part of the grammar affects over-
all system behaviour. A relatively minor repair in
one lexical class, numeral adjectives as in ?three
books were ordered? for instance, will have the po-
tential of breaking the interaction of that class with
the construction deriving named (numeric) entities
from a numeral (e.g. as in ?three is my favourite
number?) or the partitive construction (e.g. as in
?three have arrived already?). A ripple effect of
a single change can thus corrupt the semantics
produced for any of these cases and in the con-
sequence cause failure or incorrect behaviour in
the back-end system. In addition to these qual-
ity assurance requirements on grammatical cover-
age and correctness, the YY application (like most
applications for grammar-based linguistic analy-
sis) utilizes a set of hand-constructed parse rank-
ing heuristics that enables the parser to operate
in best-first search mode and to return only one
reading, i.e. the analysis that is ranked best by the
heuristic component. The parse ranking machin-
ery builds on preferences that are associated with
individual or classes of lexical items and construc-
tions. The set of preferences is maintained in par-
allel to the grammar, in a sense providing a layer
of performance-oriented annotations over the basic
building blocks of the core competence grammar.
Without discussing the details of the parse ranking
approach, it creates an additional element of un-
certainty in assessing grammar changes: since the
preference for a specific analysis results implic-
itly from a series of local preferences (of lexical
items and constructions contributing to the com-
plete derivation), introducing additional elements
(i.e. new local or global ambiguity) into the search
space and subjecting them to the partial ordering
can quickly skew the overall result.
Summing up, the grammar and application engi-
neering example presented here illustrates a num-
ber of highly typical requirements on the engi-
neering environment. First, all grammarians and
system engineers participating in the development
process need to keep frequent, detailed, and accu-
rate records of a large number of relevant parame-
ters, including but not limited to grammatical cov-
erage, correctness of syntactic analyses and cor-
responding semantic forms, parse selection accu-
racy, and overall system performance. Second, as
modifications to the system as a whole are made
daily?and sometimes several times each day?all
developers must be able to assess the impact of
recent changes and track their effects on all rele-
vant parameters; gathering the data and analyzing
it must be simple, fast, and automated as much as
possible. Third, not all modifications (to the gram-
mar or underlying software) will result in ?mono-
tonic? or backwards-compatible effects. A change
in the treatment of optional nominal complements,
for example, may affect virtually all derivation
trees and render a comparison of results at this
level uninformative. At the same time, a primarily
syntactic change of this nature will not cause an ef-
fect in associated meaning representations, so that
a semantic equivalence test over analyses should
be expected to yield an exact match to earlier re-
sults. Hence, the machinery for representation and
comparison of relevant parameters needs to facil-
itate user-level specification of informative tests
and evolution criteria. Finally, the metrics used in
tracking grammar development cannot be isolated
from measurements of system resource consump-
tion and overall performance (specific properties
of a grammar may trigger idiosyncrasies or soft-
ware bugs in a particular version of the process-
ing system); therefore, and to enable exchange of
reference points and comparability of experiments,
grammarians and system developers alike should
use the same, homogenuous set of relevant param-
eters.
3 Integrated Competence and
Performance Profiling
The integrated competence and performance pro-
filing methodology and associated engineering
platform, dubbed [incr tsdb()] (Oepen & Callmeier,
2000)1 and reviewed in the remainder of this sec-
1See ?http://www.coli.uni-sb.de/itsdb/?
for the (draft) [incr tsdb()] user manual, pronunciation rules,
and instructions on obtaining and installing the package.
tion, was designed to meet al of the requirements
identified in the DFKI ? YY case study. Generally
speaking, the [incr tsdb()] environment is an in-
tegrated package for diagnostics, evaluation, and
benchmarking in practical grammar and system
engineering. The toolkit implements an approach
to grammar development and system optimization
that builds on precise empirical data and system-
atic experimentation, as it has been advocated by,
among others, Erbach & Uszkoreit (1990), Erbach
(1991), and Carroll (1994). [incr tsdb()] has been
integrated with, as of June 2002, nine different
constraint-based grammar development and pars-
ing systems (including both environments in use at
YY, i.e. the LKB and PET), thus providing a pre-
standard reference point for a relatively large (and
growing) community of NLP developers. The [incr
tsdb()] environment builds on the following com-
ponents and modules:
? test and reference data stored with annota-
tions in a structured database; annotations
can range from minimal information (unique
test item identifier, item origin, length et al)
to fine-grained linguistic classifications (e.g.
regarding grammaticality and linguistic phe-
nomena presented in an item), as they are rep-
resented in the TSNLP test suites, for example
(Oepen, Netter, & Klein, 1997);
? tools to browse the available data, identify
suitable subsets and feed them through the
analysis component of processing systems
like the LKB and PET, LiLFeS (Makino,
Yoshida, Torisawa, & Tsujii, 1998), TRALE
(Penn, 2000), PAGE (Uszkoreit et al, 1994),
and others;
? the ability to gather a multitude of precise and
fine-grained (grammar) competence and (sys-
tem) performance measures?like the num-
ber of readings obtained per test item, various
time and memory usage statistics, ambigu-
ity and non-determinism metrics, and salient
properties of the result structures?and store
them in a uniform, platform-independent data
format as a competence and performance pro-
file; and
? graphical facilities to inspect the resulting
profiles, analyze system competence (i.e.
grammatical coverage and overgeneration)
and performance (e.g. cpu time and memory
usage, parser search space, constraint solver
'
&
$
%
Parser 3Parser 2Parser 1Grammar 3Grammar 2Grammar 1
TestSet 3TestSet 2TestSet 1
ParallelVirtualMachine
C and Lisp APIRelationalDBMS Batch ControlStatistics UserInterface
ANSI C Common-Lisp Tcl/Tk
Figure 1: Rough sketch of [incr tsdb()] architec-
ture: the core engine comprises the database man-
agement, batch control and statistics component,
and the user interface.
workload, and others) at variable granulari-
ties, aggregate, correlate, and visualize the
data, and compare among profiles obtained
from previous grammar or system versions or
other processing environments.
As it is depicted in Figure 1, the [incr tsdb()]
architecture can be broken down into three major
parts: (i) the underlying database management sys-
tem (DBMS), (ii) the batch control and statistics
kernel (providing a C and Lisp application pro-
gram interface to client systems that can be dis-
tributed across the network), and (iii) the graphi-
cal user interface (GUI). Although, historically, the
DBMS was developed independently and the ker-
nel can be operated without the GUI, the full func-
tionality of the integrated competence and perfor-
mance laboratory?as demonstrated below?only
emerges from the combination of all three com-
ponents. Likewise, the flexibility of a clearly de-
fined API to client systems and its ability to par-
allelize batch processing and distribute test runs
across the network have greatly contributed to the
success of the package. The following paragraphs
review some of the fundamental aspects in more
detail, sketch essential functionality, and comment
on how they have been exploited in the DFKI ? YY
cooperation.
Abstraction over Processors The [incr tsdb()]
environment, by virtue of its generalized pro-
file format, abstracts over specific processing en-
vironments. While grammar engineers in the
DFKI ? YY collaboration regularly use both the
LKB (primarily for interactive development) and
PET (mostly for batch testing and the assessment
of results obtained in the YY production envi-
ronment), usage of the [incr tsdb()] profile anal-
ysis routines in most aspects hides the specifics
of the token processor used in obtaining a profile.
Both platforms interprete the same typed feature
structure formalism, load the same set of gram-
mar source files, and (unless malfunctioning) pro-
duce equivalent results. Using [incr tsdb()], gram-
marians can obtain summary views of grammati-
cal coverage and overgeneration, inspect relevant
subsets of the available data, break down analysis
views according to various aggregation schemes,
and zoom in on specific aggregates or individual
test items as appropriate. Moreover, processing
results obtained from the (far more efficient) PET
parser (that has no visualization or debugging sup-
port built in), once recorded as an [incr tsdb()] pro-
file, can be used in conjunction with the LKB (con-
tingent on the use of identical grammars), thereby
facilitating graphical inspection of parse trees and
semantic formulae.
Parallelization of Test Runs The [incr tsdb()] ar-
chitecture (see Figure 1) separates the batch con-
trol and statistics kernel from what is referred to
as client processors (i.e. parsing systems like the
LKB or PET) through an application program inter-
face (API) and the Parallel Virtual Machine (PVM;
Geist, Bequelin, Dongarra, Manchek, & Sun-
deram, 1994) message-passing protocol layer. The
use of PVM?in connection with task scheduling,
error recovery, and roll-over facilities in the [incr
tsdb()] kernel?enables developers to transparently
parallelize and distribute execution of batch pro-
cessing. At YY, grammarians had a cluster of net-
worked Linux compute servers configured as a sin-
gle PVM instance, so that execution of a test run?
using the efficient PET run-time engine?could be
completed as a matter of a few seconds. The com-
bination of near-instantaneous profile creation and
[incr tsdb()] facilities for quick, semi-automated as-
sessment of relevant changes (see below) enabled
developers to pursue a strongly empiricist style of
grammar engineering, assessing changes and their
effects on actual system behavior in small incre-
ments (often many times per hour).
Structured Comparison One of the facilities
that has proven particularly useful in the dis-
tributed grammar engineering setup outlined in
Section 2 above is the flexible comparison of com-
petence and performance profiles. The [incr tsdb()]
package eases comparison of results on a per-
item basis, using an approach similar to Un?x
diff(1), but generalized for structured data sets.
By selection of a set of parameters for intersec-
tion (and optionally a comparison predicate), the
user interface allows browsing the subset of test
items (and associated results) that fail to match
in the selected properties. One dimension that
grammarians found especially useful in intersect-
ing profiles is on the number of readings assigned
per item?detecting where coverage was lost or
added?and on derivation trees (bracketed struc-
tures labeled with rule names and identifiers of lex-
ical items) associated with each parser analysis?
assessing where analyses have changed. Addition-
ally, using a user-supplied equivalence predicate,
the same technique was regularly used at YY to
track the evolution of meaning representations (as
they form the interface from linguistic analysis into
the back-end knowledge processing engine), both
for all readings and the analysis ranked best by the
parse selection heuristics.
Zooming and Interactive Debugging In
analysing a new competence and performance
profile, grammarians typically start from summary
views (overall grammatical coverage, say), then
single out relevant (or suspicious) subsets of
profile data, and often end up zooming in to
the level of individual test items. For most [incr
tsdb()] analysis views the ?success? criteria can be
varied according to user decisions: in assessing
grammatical coverage, for example, the scoring
function can refer to virtually arbitrary profile
elements?ranging from the most basic coverage
measure (assigning at least one reading) to more
refined or application-specific metrics, the produc-
tion of a well-formed meaning representation, say.
Although the general approach allows output an-
notations on the test data (full or partial constituent
structure descriptions, for example), developers so
far have found the incremental, semi-automated
comparison against earlier results a more adequate
means of regression testing. It would appear
that, especially in an application-driven and
tightly scheduled engineering situation like the
DFKI ? YY partnership, the pace of evolution
and general lack of locality in changes (see the
examples discussed in Section 2) precludes the
construction of a static, ?gold-standard? target for
comparison. Instead, the structured comparison
facilities of [incr tsdb()] enable developers to
incrementally approximate target results and, even
12-sep-2001 (13:24 h) ? 14-feb-2002 (17:14 h)
40
45
50
55
60
65
70
75
80
85
90
95
Grammatical Coverage (Per Cent)
(generated by [incr tsdb()] at 29-jun-2002 (20:49 h))
?
? ???
? ??
?
???
?
?
?
?
?
?
?
?
?
???
??
?
??
?
?
? ?? ??
?
?
?
??
?? ? ?
?
?
?
?
?
?
?
? ???
?
?
????
??
?? ????
?? ?banking?
?? ?trading?
12-sep-2001 (13:24 h) ? 14-feb-2002 (17:14 h)
0
10
20
30
40
50
60
70
80
90
Ambiguity (Average Number of Analyses)
(generated by [incr tsdb()] at 29-jun-2002 (20:59 h))
?
?
? ? ?
?
?? ?? ? ???? ?? ? ?
?
?
?
?
?
? ?
?
??
??? ?
????
?? ?banking?
?? ?trading?
Figure 2: Evolution of grammatical coverage and average ambiguity (number of readings per test item) over
a five-month period; ?banking? and ?trading? are two data sets (of some 700 and 400 sentences, respectively)
of domain data.
in a highly dynamic environment where grammar
and processing environment evolve in parallel,
track changes and identify regression with great
confidence.
4 Looking Back?Quantifying Evolution
Over time, the [incr tsdb()] profile storage accu-
mulates precise data on the grammar development
process. Figure 2 summarizes two aspects of
grammatical evolution compiled over a five-month
period (and representing some 130 profiles that
grammarians put aside for future reference): gram-
matical coverage over two representative samples
of customer data?one for an on-line banking ap-
plication, the other from an electronic stock trad-
ing domain?is contrasted with the development
of global ambiguity (i.e. the average number of
analyses assinged to each test item). As should
be expected, grammatical coverage on both data
sets increases significantly as grammar develop-
ment focuses on these domains (?banking? for the
first three months, ?trading? from there on). While
the collection of available profiles, apparently, in-
cludes a number of data points corresponding to
?failed? experiments (fairly dramatic losses in cov-
erage), the larger picture shows mostly monotonic
improvement in coverage. As a control experi-
ment, the coverage graph includes another data
point for the ?banking? data towards the end of the
reporting period. Two months of focussed devel-
opment on the ?trading? domain have not nega-
tively affected grammatical coverage on the data
set used earlier. Corresponding to the (desirable)
increase in coverage, the graph on the right of Fig-
ure 2 depicts the evolution of grammatical ambi-
guity. As hand-built linguistic grammars put great
emphasis on the precision of grammatical analy-
sis and the exclusion of ungrammatical input, the
overall average of readings assigned to each sen-
tence varies around relatively small numbers. For
the moderately complex email data2 the grammar
often assigns less than ten analyses, rarely more
than a few dozens. However, not surprisingly
the addition of grammatical coverage comes with
a sharp increase in ambiguity (which may indi-
cate overgeneration): the graphs in Figure 2 clearly
show that, once coverage on the ?trading? data was
above eighty per cent, grammarians shifted their
engineering focus on ?tightening? the grammar, i.e.
the elimination of spurious ambiguity and overgen-
eration (see Siegel & Bender, 2002, for details on
the grammar).
Another view on grammar evolution is pre-
sented in Figure 3, depicting the ?size? of the
Japanese grammar over the same five-month de-
velopment cycle. Although measuring the size of
2Quantifying input complexity for Japanese is a non-
trivial task, as the count of the number of input words would
depend on the approach to string segmentation used in a spe-
cific system (the fairly aggressive tokenizer of ChaSen, Asa-
hara & Matsumoto, 2000, in our case); to avoid potential for
confusion, we report input complexity in the (overtly system-
specific) number of lexical items stipulated by the grammar
instead: around 50 and 80, on average, for the ?banking? and
?trading? data sets, respectively (as of February 2002).
12-sep-2001 (13:24 h) ? 14-feb-2002 (17:14 h)
8800
9000
9200
9400
9600
9800
10000
10200
88
90
92
94
96
98
100
102
104
106
Grammar Size
(generated by [incr tsdb()] at 30-jun-2002 (16:09 h))????
???
???
????
??
?????
?
?
?
?
??
?
?
? ???????
?????? ?
??























 





 

?? types
? rules
Figure 3: Evolution of grammar size (in the num-
bers of types, plotted against the left axis, and
grammar rules, plotted against the right axis) over
a five-month period.
computational grammars is a difficult challenge,
for the HPSG framework two metrics suggest them-
selves: the number of types (i.e. the size of the
grammatical ontology) and the number of gram-
mar rules (i.e. the inventory of construction types).
As would be expected, both numbers increase
more or less monotonically over the reporting pe-
riod, where the shift of focus from the ?banking?
into the ?trading? domain is marked with a sharp
increase in (primarily lexical) types. Contrasted
to the significant gains in grammatical coverage
(a relative improvement of more than seventy per
cent on the ?banking? data), the increase in gram-
mar size is moderate, though: around fifteen and
twenty per cent in the number of types and rules,
respectively.
5 Conclusions
At YY and cooperating partners (primarily DFKI
Saarbru?cken and CSLI Stanford), grammarians
(for all languages) as well as developers of both the
grammar development tools and of the production
system all used the competence and performance
profiling environment as part of their daily engi-
neering toolbox. The combination of [incr tsdb()]
facilities to parallelize test run processing and a
break-through in client system efficiency (using
the PET parser; Callmeier, 2002) has created an ex-
perimental development environment where gram-
marians can obtain near-instantaneous feedback on
the effects of changes they explore.
For the Japanese grammar specifically, the
grammar developers at both ends would typically
spend the first ten to twenty minutes of the day ob-
taining fresh profiles for a number of shared test
sets and diagnostic corpora, thereby assessing the
most recent set of changes through empirical anal-
ysis of their effects. In conjunction with a certain
rigor in documentation and communication, it was
the ability of both partners to regularly, quickly,
and semi-automatically monitor the evolution of
the joint resource with great confidence that has
enabled truly parallel development of a single,
shared HPSG grammar across continents. Within
a relatively short time, the partners succeeded
in adapting an existing grammar to a new genre
(email rather than spoken language) and domain
(customer service requests rather than appointment
scheduling), greatly extending grammatical cov-
erage (from initially around forty to above ninety
per cent on representative customer corpora), and
incorporating the grammar-based analysis engine
into a commercial product. And even though in
February 2002, for business reasons, YY decided
to reorganize grammar development for Japanese,
the distributed, parallel grammar development ef-
fort positively demonstrates that methodological
and technological advances in constraint-based
grammar engineering have enabled commercial
development and deployment of broad-coverage
HPSG implementations, a paradigm that until re-
cently was often believed to still lack the maturity
for real-world applications.
Acknowledgements
The DFKI ? YY partnership involved a large group
of people at both sites. We would like to thank
Kirk Oatman, co-founder of YY and first CEO,
and Hans Uszkoreit, Scientific Director at DFKI,
for their initiative and whole-hearted support to
the project; it takes vision for both corporate and
academic types to jointly develop an open-source
resource. Atsuko Shimada (from Saarland Uni-
versity), as part of a two-month internship at YY,
has greatly contributed to the preparation of repre-
sentative data samples, development of robust pre-
processing rules, and extensions to lexical cover-
age. Our colleague and friend Asahara-san (of the
Nara Advanced Institute of Technology, Japan),
co-developer of the open-source ChaSen tokenizer
and morphological analyzer for Japanese, was in-
strumental in the integration of ChaSen into the
YY product and also helped a lot in adapting and
(sometimes) fixing tokenization and morphology.
References
Asahara, M., & Matsumoto, Y. (2000). Extended
models and tools for high-performance part-of-
speech tagger. In Proceedings of the 18th In-
ternational Conference on Computational Lin-
guistics (pp. 21 ? 27). Saarbru?cken, Germany.
Butt, M., King, T. H., Nin?o, M.-E., & Segond, F.
(1999). A grammar writer?s cookbook. Stan-
ford, CA: CSLI Publications.
Callmeier, U. (2002). Preprocessing and encoding
techniques in PET. In S. Oepen, D. Flickinger,
J. Tsujii, & H. Uszkoreit (Eds.), Collabora-
tive language engineering. A case study in ef-
ficient grammar-based processing. Stanford,
CA: CSLI Publications. (forthcoming)
Carroll, J. (1994). Relating complexity to practi-
cal performance in parsing with wide-coverage
unification grammars. In Proceedings of the
32nd Meeting of the Association for Computa-
tional Linguistics (pp. 287 ? 294). Las Cruces,
NM.
Copestake, A. (2002). Implementing typed fea-
ture structure grammars. Stanford, CA: CSLI
Publications.
Erbach, G. (1991). An environment for exper-
imenting with parsing strategies. In J. My-
lopoulos & R. Reiter (Eds.), Proceedings of
IJCAI 1991 (pp. 931 ? 937). San Mateo, CA:
Morgan Kaufmann Publishers.
Erbach, G., & Uszkoreit, H. (1990). Grammar
engineering. Problems and prospects (CLAUS
Report # 1). Saarbru?cken, Germany: Compu-
tational Linguistics, Saarland University.
Flickinger, D., Copestake, A., & Sag, I. A. (2000).
HPSG analysis of English. In W. Wahlster
(Ed.), Verbmobil. Foundations of speech-to-
speech translation (Artificial Intelligence ed.,
pp. 321 ? 330). Berlin, Germany: Springer.
Geist, A., Bequelin, A., Dongarra, J., Manchek, W.
J. R., & Sunderam, V. (Eds.). (1994). PVM ?
parallel virtual machine. A users? guide and tu-
torial for networked parallel computing. Cam-
bridge, MA: The MIT Press.
Makino, T., Yoshida, M., Torisawa, K., & Tsu-
jii, J. (1998). LiLFeS ? towards a practical
HPSG parser. In Proceedings of the 17th In-
ternational Conference on Computational Lin-
guistics and the 36th Annual Meeting of the
Association for Computational Linguistics (pp.
807 ? 11). Montreal, Canada.
Mu?ller, S., & Kasper, W. (2000). HPSG analy-
sis of German. In W. Wahlster (Ed.), Verbmo-
bil. Foundations of speech-to-speech transla-
tion (Artificial Intelligence ed., pp. 238 ? 253).
Berlin, Germany: Springer.
Oepen, S., & Callmeier, U. (2000). Measure for
measure: Parser cross-fertilization. Towards
increased component comparability and ex-
change. In Proceedings of the 6th International
Workshop on Parsing Technologies (pp. 183 ?
194). Trento, Italy.
Oepen, S., & Flickinger, D. P. (1998). Towards
systematic grammar profiling. Test suite tech-
nology ten years after. Journal of Computer
Speech and Language, 12 (4) (Special Issue on
Evaluation), 411 ? 436.
Oepen, S., Netter, K., & Klein, J. (1997). TSNLP
? Test Suites for Natural Language Process-
ing. In J. Nerbonne (Ed.), Linguistic Databases
(pp. 13 ? 36). Stanford, CA: CSLI Publica-
tions.
Penn, G. (2000). Applying constraint handling
rules to HPSG. In Proceedings of the first in-
ternational conference on computational logic
(pp. 51 ? 68). London, UK.
Siegel, M. (2000). HPSG analysis of Japanese. In
W. Wahlster (Ed.), Verbmobil. Foundations of
speech-to-speech translation (Artificial Intelli-
gence ed., pp. 265 ? 280). Berlin, Germany:
Springer.
Siegel, M., & Bender, E. M. (2002). Efficient
deep processing of japanese. In Proceedings
of the 19th International Conference on Com-
putational Linguistics. Taipei, Taiwan.
Uszkoreit, H., Backofen, R., Busemann, S., Di-
agne, A. K., Hinkelman, E. A., Kasper, W.,
Kiefer, B., Krieger, H.-U., Netter, K., Neu-
mann, G., Oepen, S., & Spackman, S. P.
(1994). DISCO ? an HPSG-based NLP
system and its application for appointment
scheduling. In Proceedings of the 15th Interna-
tional Conference on Computational Linguis-
tics. Kyoto, Japan.
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 25?32,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Exploiting Semantic Information for HPSG Parse Selection
Sanae Fujita,? Francis Bond,? Stephan Oepen,? Takaaki Tanaka?
? {sanae,takaaki}@cslab.kecl.ntt.co.jp, ? bond@ieee.org, ? oe@ifi.uio.no
? NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation
? National Institute of Information and Communications Technology (Japan)
? University of Oslo, Department of Informatics (Norway)
Abstract
In this paper we present a framework for
experimentation on parse selection using
syntactic and semantic features. Results
are given for syntactic features, depen-
dency relations and the use of semantic
classes.
1 Introduction
In this paper we investigate the use of semantic in-
formation in parse selection.
Recently, significant improvements have been
made in combining symbolic and statistical ap-
proaches to various natural language processing
tasks. In parsing, for example, symbolic grammars
are combined with stochastic models (Oepen et al,
2004; Malouf and van Noord, 2004). Much of the
gain in statistical parsing using lexicalized models
comes from the use of a small set of function words
(Klein and Manning, 2003). Features based on gen-
eral relations provide little improvement, presum-
ably because the data is too sparse: in the Penn
treebank standardly used to train and test statisti-
cal parsers stocks and skyrocket never appear to-
gether. However, the superordinate concepts capi-
tal (? stocks) and move upward (? sky rocket) fre-
quently appear together, which suggests that using
word senses and their hypernyms as features may be
useful
However, to date, there have been few combina-
tions of sense information together with symbolic
grammars and statistical models. We hypothesize
that one of the reasons for the lack of success is
that there has been no resource annotated with both
syntactic and semantic information. In this paper,
we use a treebank with both syntactic information
(HPSG parses) and semantic information (sense tags
from a lexicon) (Bond et al, 2007). We use this to
train parse selection models using both syntactic and
semantic features. A model trained using syntactic
features combined with semantic information out-
performs a model using purely syntactic information
by a wide margin (69.4% sentence parse accuracy
vs. 63.8% on definition sentences).
2 The Hinoki Corpus
There are now some corpora being built with the
syntactic and semantic information necessary to in-
vestigate the use of semantic information in parse
selection. In English, the OntoNotes project (Hovy
et al, 2006) is combining sense tags with the Penn
treebank. We are using Japanese data from the Hi-
noki Corpus consisting of around 95,000 dictionary
definition and example sentences (Bond et al, 2007)
annotated with both syntactic parses and senses from
the same dictionary.
2.1 Syntactic Annotation
Syntactic annotation in Hinoki is grammar based
corpus annotation done by selecting the best parse
(or parses) from the full analyses derived by a broad-
coverage precision grammar. The grammar is an
HPSG implementation (JACY: Siegel and Bender,
2002), which provides a high level of detail, mark-
ing not only dependency and constituent structure
but also detailed semantic relations. As the gram-
mar is based on a monostratal theory of grammar
(HPSG: Pollard and Sag, 1994), annotation by man-
ual disambiguation determines syntactic and seman-
tic structure at the same time. Using a grammar
25
helps treebank consistency ? all sentences anno-
tated are guaranteed to have well-formed parses.
The flip side to this is that any sentences which the
parser cannot parse remain unannotated, at least un-
less we were to fall back on full manual mark-up of
their analyses. The actual annotation process uses
the same tools as the Redwoods treebank of English
(Oepen et al, 2004).
A (simplified) example of an entry is given in Fig-
ure 1. Each entry contains the word itself, its part
of speech, and its lexical type(s) in the grammar.
Each sense then contains definition and example
sentences, links to other senses in the lexicon (such
as hypernym), and links to other resources, such
as the Goi-Taikei Japanese Lexicon (Ikehara et al,
1997) and WordNet (Fellbaum, 1998). Each content
word of the definition and example sentences is an-
notated with sense tags from the same lexicon.
There were 4 parses for the definition sentence.
The correct parse, shown as a phrase structure tree,
is shown in Figure 2. The two sources of ambigu-
ity are the conjunction and the relative clause. The
parser also allows the conjunction to combine \
densha and 0 hito. In Japanese, relative clauses
can have gapped and non-gapped readings. In the
gapped reading (selected here), 0 hito is the subject
of ?U unten ?drive?. In the non-gapped reading
there is some underspecified relation between the
modifee and the verb phrase. This is similar to the
difference in the two readings of the day he knew
in English: ?the day that he knew about? (gapped)
vs ?the day on which he knew (something)? (non-
gapped). Such semantic ambiguity is resolved by
selecting the correct derivation tree that includes the
applied rules in building the tree (Fig 3).
The semantic representation is Minimal Recur-
sion Semantics (Copestake et al, 2005). We sim-
plify this into a dependency representation, further
abstracting away from quantification, as shown in
Figure 4. One of the advantages of the HPSG sign
is that it contains all this information, making it pos-
sible to extract the particular view needed. In or-
der to make linking to other resources, such as the
sense annotation, easier predicates are labeled with
pointers back to their position in the original sur-
face string. For example, the predicate densha n 1
links to the surface characters between positions 0
and 3:\.
UTTERANCE
NP
VP N
PP V
NP
PP
N CONJ N CASE-P V V
\ ? ? k ?U 2d 0
densha ya jidousha o unten suru hito
train or car ACC drive do person
?U31 ?chauffeur?: ?a person who drives a train or car?
Figure 2: Syntactic View of the Definition of ?U
31 untenshu ?chauffeur?
e2:unknown<0:13>[ARG x5:_hito_n]
x7:densha_n_1<0:3>[]
x12:_jidousha_n<4:7>[]
x13:_ya_p_conj<0:4>[LIDX x7:_densha_n_1,
RIDX x12:_jidousha_n]
e23:_unten_s_2<8:10>[ARG1 x5:_hito_n]
e23:_unten_s_2<8:10>[ARG2 x13:_ya_p_conj]
Figure 4: Simplified Dependency View of the Defi-
nition of ?U31 untenshu ?chauffeur?
2.2 Semantic Annotation
The lexical semantic annotation uses the sense in-
ventory from Lexeed (Kasahara et al, 2004). All
words in the fundamental vocabulary are tagged
with their sense. For example, the wordd& ookii
?big? (of example sentence in Figure 1) is tagged as
sense 5 in the example sentence, with the meaning
?elder, older?.
The word senses are further linked to semantic
classes in a Japanese ontology. The ontology, Goi-
Taikei, consists of a hierarchy of 2,710 semantic
classes, defined for over 264,312 nouns, with a max-
imum depth of 12 (Ikehara et al, 1997). We show
the top 3 levels of the Goi-Taikei common noun on-
tology in Figure 5. The semantic classes are prin-
cipally defined for nouns (including verbal nouns),
although there is some information for verbs and ad-
jectives.
3 Parse Selection
Combining the broad-coverage JACY grammar and
the Hinoki corpus, we build a parse selection model
on top of the symbolic grammar. Given a set of can-
26
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
INDEX ?U3 untenshu
POS noun
SENSE 1
?
?
?
?
?
?
?
?
?
?
?
?
DEFINITION
[
\1 ??1 k?U1 2d04 a person who drives trains and cars
]
EXAMPLE
[
d&(5 C<8b\1 G?U31 Dod6 G%?3 2
I dream of growing up and becoming a train driver
]
HYPERNYM 04 hito ?person?
SEM. CLASS ?292:driver? (? ?4:person?)
WORDNET motorman1
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: Dictionary Entry for ?U31 untenshu ?chauffeur?
frag-np
rel-cl-sbj-gap
hd-complement noun-le
hd-complement v-light
hd-complement
hd-complement case-p-acc-le
noun-le conj-le noun-le vn-trans-le v-light-le
\ ? ? k ?U 2d 0
densha ya jidousha o unten suru hito
train or car ACC drive do person
?U31 ?chauffeur?: ?a person who drives a train or car?
Figure 3: Derivation Tree of the Definition of ?U31 untenshu ?chauffeur?
Phrasal nodes are labeled with identifiers of grammar rules, and (pre-terminal) lexical nodes with class names for types of lexical
entries.
Lvl 0 Lvl 1 Lvl 2 Lvl 3
human
agent organization
facility
c
o
n
c
r
e
t
e
place region
natural place
object animate
inanimate
abstract
thing
mental state
noun action
human activity
event phenomenon
natural phen.
a
b
s
t
r
a
c
t
existence
system
relationship
property
relation state
shape
amount
location
time
Figure 5: Top 3 levels of the GoiTaikei Ontology
didate analyses (for some Japanese string) according
to JACY, the goal is to rank parse trees by their prob-
ability: training a stochastic parse selection model
on the available treebank, we estimate statistics of
various features of candidate analyses from the tree-
bank. The definition and selection of features, thus,
is a central parameter in the design of an effective
parse selection model.
3.1 Syntactic Features
The first model that we trained uses syntactic fea-
tures defined over HPSG derivation trees as summa-
rized in Table 1. For the closely related purpose of
parse selection over the English Redwoods treebank,
Toutanova et al (2005) train a discriminative log-
linear model, using features defined over derivation
trees with non-terminals representing the construc-
tion types and lexical types of the HPSG grammar.
The basic feature set of our parse selection model
for Japanese is defined in the same way (correspond-
ing to the PCFG-S model of Toutanova et al (2005)):
each feature capturing a sub-tree from the deriva-
27
# sample features
1 ?0 rel-cl-sbj-gap hd-complement noun-le?
1 ?1 frag-np rel-cl-sbj-gap hd-complement noun-le?
1 ?2 ? frag-np rel-cl-sbj-gap hd-complement noun-le?
2 ?0 rel-cl-sbj-gap hd-complement?
2 ?0 rel-cl-sbj-gap noun-le?
2 ?1 frag-np rel-cl-sbj-gap hd-complement?
2 ?1 frag-np rel-cl-sbj-gap noun-le?
3 ?1 conj-le ya?
3 ?2 noun-le conj-le ya?
3 ?3  noun-le conj-le ya?
4 ?1 conj-le?
4 ?2 noun-le conj-le?
4 ?3  noun-le conj-le?
Table 1: Example structural features extracted from
the derivation tree in Figure 3. The first column
numbers the feature template corresponding to each
example; in the examples, the first integer value
is a parameter to feature templates, i.e. the depth
of grandparenting (types #1 and#2) or n-gram size
(types #3 and #4). The special symbols ? and 
denote the root of the tree and left periphery of the
yield, respectively.
tion limited to depth one. Table 1 shows example
features extracted from our running example (Fig-
ure 3 above) in our MaxEnt models, where the fea-
ture template #1 corresponds to local derivation sub-
trees. We will refer to the parse selection model us-
ing only local structural features as SYN-1.
3.1.1 Dominance Features
To reduce the effects of data sparseness, feature
type #2 in Table 1 provides a back-off to deriva-
tion sub-trees, where the sequence of daughters is
reduced to just the head daughter. Conversely, to
facilitate sampling of larger contexts than just sub-
trees of depth one, feature template #1 allows op-
tional grandparenting, including the upwards chain
of dominating nodes in some features. In our ex-
periments, we found that grandparenting of up to
three dominating nodes gave the best balance of en-
larged context vs. data sparseness. Enriching our ba-
sic model SYN-1 with these features we will hence-
forth call SYN-GP.
3.1.2 N-Gram Features
In addition to these dominance-oriented features
taken from the derivation trees of each parse tree,
our models also include more surface-oriented fea-
tures, viz. n-grams of lexical types with or without
lexicalization. Feature type #3 in Table 1 defines
n-grams of variable size, where (in a loose anal-
ogy to part-of-speech tagging) sequences of lexical
types capture syntactic category assignments. Fea-
ture templates #3 and #4 only differ with regard to
lexicalization, as the former includes the surface to-
ken associated with the rightmost element of each
n-gram (loosely corresponding to the emission prob-
abilities in an HMM tagger). We used a maximum
n-gram size of two in the experiments reported here,
again due to its empirically determined best overall
performance.
3.2 Semantic Features
In order to define semantic parse selection features,
we use a reduction of the full semantic representa-
tion (MRS) into ?variable-free? elementary depen-
dencies. The conversion centrally rests on a notion
of one distinguished variable in each semantic rela-
tion. For most types of relations, the distinguished
variable corresponds to the main index (ARG0 in the
examples above), e.g. an event variable for verbal re-
lations and a referential index for nominals. Assum-
ing further that, by and large, there is a unique re-
lation for each semantic variable for which it serves
as the main index (thus assuming, for example, that
adjectives and adverbs have event variables of their
own, which can be motivated in predicative usages
at least), an MRS can be broken down into a set of
basic dependency tuples of the form shown in Fig-
ure 4 (Oepen and L?nning, 2006).
All predicates are indexed to the position of the
word or words that introduced them in the input sen-
tence (<start:end>). This allows us to link them
to the sense annotations in the corpus.
3.2.1 Basic Semantic Dependencies
The basic semantic model, SEM-Dep, consists of
features based on a predicate and its arguments taken
from the elementary dependencies. For example,
consider the dependencies for densha ya jidousha-
wo unten suru hito ?a person who drives a train or
car? given in Figure 4. The predicate unten ?drive?
has two arguments: ARG1 hito ?person? and ARG2
jidousha ?car?.
From these, we produce several features (See Ta-
ble 2). One has all arguments and their labels (#20).
We also produce various back offs: #21 introduces
28
# sample features
20 ?0 unten s ARG1 hito n 1 ARG2 ya p conj?
20 ?0 ya p conj LIDX densha n 1 RIDX jidousha n 1?
21 ?1 unten s ARG1 hito n 1?
21 ?1 unten s ARG2 jidousha n 1?
21 ?1 ya p conj LIDX densha n 1?
21 ?1 ya p conj RIDX jidousha n 1?
22 ?2 unten s hito n 1 jidousha n 1?
23 ?3 unten s hito n 1?
23 ?3 unten s jidousha n 1?
. . .
Table 2: Example semantic features (SEM-Dep) ex-
tracted from the dependency tree in Figure 4.
only one argument at a time, #22 provides unlabeled
relations, #23 provides one unlabeled relation at a
time and so on.
Each combination of a predicate and its related
argument(s) becomes a feature. These resemble the
basic semantic features used by Toutanova et al
(2005). We further simplify these by collapsing
some non-informative predicates, e.g. the unknown
predicate used in fragments.
3.2.2 Word Sense and Semantic Class
Dependencies
We created two sets of features based only on the
word senses. For SEM-WS we used the sense anno-
tation to replace each underspecified MRS predicate
by a predicate indicating the word sense. This used
the gold standard sense tags. For SEM-Class, we used
the sense annotation to replace each predicate by its
Goi-Taikei semantic class.
In addition, to capture more useful relationships,
conjunctions were followed down into the left and
right daughters, and added as separate features. The
semantic classes for \1densha ?train? and  ?
1jidousha ?car? are both ?988:land vehicle?,
while ?U1 unten ?drive? is ?2003:motion? and
04 hito ?person?is ?4:human?. The sample features
of SEM-Class are shown in Table 3.
These features provide more specific information,
in the case of the word sense, and semantic smooth-
ing in the case of the semantic classes, as words are
binned into only 2,700 classes.
3.2.3 Superordinate Semantic Classes
We further smooth these features by replacing the
semantic classes with their hypernyms at a given
level (SEM-L). We investigated levels 2 to 5. Pred-
# sample features
40 ?0 unten s ARG1 C4 ARG2 C988?
40 ?1 C2003 ARG1 C4 ARG2 C988?
40 ?1 C2003 ARG1 C4 ARG2 C988?
40 ?0 ya p conj LIDX C988 RIDX C988?
41 ?2 unten s ARG1 C4?
41 ?2 unten s ARG2 C988?
. . .
Table 3: Example semantic class features (SEM-
Class).
icates are binned into only 9 classes at level 2, 30
classes at level 3, 136 classes at level 4, and 392
classes at level 5.
For example, at level 3, the hypernym class
for ?988:land vehicle? is ?706:inanimate?,
?2003:motion? is ?1236:human activity?
and ?4:human? is unchanged. So we used
?706:inanimate? and ?1236:human activity?
to make features in the same way as Table 3.
An advantage of these underspecified semantic
classes is that they are more robust to errors in word
sense disambiguation ? fine grained sense distinc-
tions can be ignored.
3.2.4 Valency Dictionary Compatability
The last kind of semantic information we use is
valency information, taken from the Japanese side
of the Goi-Taikei Japanese-English valency dictio-
nary as extended by Fujita and Bond (2004).This va-
lency dictionary has detailed information about the
argument properties of verbs and adjectives, includ-
ing subcategorization and selectional restrictions. A
simplified entry of the Japanese side for ?U2
dunten-suru ?drive? is shown in Figure 6.
Each entry has a predicate and several case-slots.
Each case-slot has information such as grammatical
function, case-marker, case-role (N1, N2, . . . ) and
semantic restrictions. The semantic restrictions are
defined by the Goi-Taikei?s semantic classes.
On the Japanese side of Goi-Taikei?s valency
dictionary, there are 10,146 types of verbs giving
18,512 entries and 1,723 types of adjectives giving
2,618 entries.
The valency based features were constructed by
first finding the most appropriate pattern, and then
recording how well it matched.
To find the most appropriate pattern, we extracted
candidate dictionary entries whose lemma is the
29
PID:300513
? N1 <4:people> "%" ga
? N2 <986:vehicles> "k" o
? ?U2d unten-suru
Figure 6: ?U2dunten-suru ?N1 drive N2?.
PID is the verb?s Pattern ID
# sample features
31 ?0 High?
31 ?1 300513 High?
31 ?2 2?
31 ?3 R:High?
31 ?4 300513 R:High?
32 ?1 unten s High?
32 ?4 unten s R:High?
33 ?5 N1 C High?
33 ?7 C?
. . .
Table 4: Example semantic features (SP)
same as the predicate in the sentence: for exam-
ple we look up all entries for ?U2d unten-
suru ?drive?. Then, for each candidate pattern, we
mapped its arguments to the target predicate?s ar-
guments via case-markers. If the target predicate
has no suitable argument, we mapped to comitative
phrase. Finally, for each candidate patterns, we cal-
culate a matching score1 and select the pattern which
has the best score.
Once we have the most appropriate pattern,
we then construct features that record how good
the match is (Table 4). These include: the to-
tal score, with or without the verb?s Pattern ID
(High/Med/Low/Zero: #31 0,1), the number of filled
arguments (#31 2), the fraction of filled arguments
vs all arguments (High/Med/Low/Zero: #31 3,4),
the score for each argument of the pattern (#32 5)
and the types of matches (#32 5,7).
These scores allow us to use information about
word usage in an exisiting dictionary.
4 Evaluation and Results
We trained and tested on a subset of the dictionary
definition and example sentences in the Hinoki cor-
pus. This consists of those sentences with ambigu-
ous parses which have been annotated so that the
1The scoring method follows Bond and Shirai (1997), and
depends on the goodness of the matches of the arguments.
number of parses has been reduced (Table 5). That
is, we excluded unambiguous sentences (with a sin-
gle parse), and those where the annotators judged
that no parse gave the correct semantics. This does
not necessarily mean that there is a single correct
parse, we allow the annotator to claim that two or
more parses are equally appropriate.
Corpus # Sents Length Parses/Sent
(Ave) (Ave)
Definitions Train 30,345 9.3 190.1
Test 2,790 10.1 177.0
Examples Train 27,081 10.9 74.1
Test 2,587 10.4 47.3
Table 5: Data of Sets for Evaluation
Dictionary definition sentences are a different
genre to other commonly used test sets (e.g news-
paper text in the Penn Treebank or travel dialogues
in Redwoods). However, they are valid examples
of naturally occurring texts and a native speaker can
read and understand them without special training.
The main differences with newspaper text is that
the definition sentences are shorter, contain more
fragments (especially NPs as single utterances) and
fewer quoting and proper names. The main differ-
ences with travel dialogues is the lack of questions.
4.1 A Maximum Entropy Ranker
Log-linear models provide a very flexible frame-
work that has been widely used for a range of tasks
in NLP, including parse selection and reranking for
machine translation. We use a maximum entropy
/ minimum divergence (MEMD) modeler to train
the parse selection model. Specifically, we use the
open-source Toolkit for Advanced Discriminative
Modeling (TADM:2 Malouf, 2002) for training, us-
ing its limited-memory variable metric as the opti-
mization method and determining best-performing
convergence thresholds and prior sizes experimen-
tally. A comparison of this learner with the use
of support vector machines over similar data found
that the SVMs gave comparable results but were far
slower (Baldridge and Osborne, 2007). Because we
are investigating the effects of various different fea-
tures, we chose the faster learner.
2http://tadm.sourceforge.net
30
Method Definitions Examples
Accuracy Features Accuracy Features
(%) (?1000) (%) (?1000)
SYN-1 52.8 7 67.6 8
SYN-GP 62.7 266 76.0 196
SYN-ALL 63.8 316 76.2 245
SYN baseline 16.4 random 22.3 random
SEM-Dep 57.3 1,189 58.7 675
+SEM-WS 56.2 1,904 59.0 1,486
+SEM-Class 57.5 2,018 59.7 1,669
+SEM-L2 60.3 808 62.9 823
+SEM-L3 59.8 876 62.8 879
+SEM-L4 59.9 1,000 62.3 973
+SEM-L5 60.4 1,240 61.3 1,202
+SP 59.1 1,218 68.2 819
+SEM-ALL 62.7 3,384 69.1 2,693
SYN-SEM 69.5 2,476 79.2 2,126
SEM baseline 20.3 random 22.8 random
Table 6: Parse Selection Results
4.2 Results
The results for most of the models discussed in the
previous section are shown in Table 6. The accuracy
is exact match for the entire sentence: a model gets
a point only if its top ranked analysis is the same as
an analysis selected as correct in Hinoki. This is a
stricter metric than component based measures (e.g.,
labelled precision) which award partial credit for in-
correct parses. For the syntactic models, the base-
line (random choice) is 16.4% for the definitions and
22.3% for the examples. Definition sentences are
harder to parse than the example sentences. This
is mainly because they have fewer relative clauses
and coordinate NPs, both large sources of ambigu-
ity. For the semantic and combined models, multiple
sentences can have different parses but the same se-
mantics. In this case all sentences with the correct
semantics are scored as good. This raises the base-
lines to 20.3 and 22.8% respectively.
Even the simplest models (SYN-1 and SEM-Dep)
give a large improvement over the baseline. Adding
grandparenting to the syntactic model has a large
improvement (SYN-GP), but adding lexical n-grams
gave only a slight improvement over this (SYN-ALL).
The effect of smoothing by superordinate seman-
tic classes (SEM-Class), shows a modest improve-
ment. The syntactic model already contains a back-
off to lexical-types, we hypothesize that the seman-
tic classes behave in the same way. Surprisingly, as
we add more data, the very top level of the seman-
tic class hierarchy performs almost as well as the
+
+ +
+ + + + + + +
+
bc
bc
bc
bc
bc
bc
bc
bc
bc
bc
bc
ld
ld
ld
ld
ld
ld
ld
ld
ld
ld
ld
0 20 40 60 80 100
20
30
40
50
60
70
% of training data (30,345 sentences)
Se
n
t.
A
cc
u
ra
cy SYN-SEM
SEM-ALL
SYN-ALL
Figure 7: Learning Curves (Definitions)
more detailed levels. The features using the valency
dictionary (SP) also provide a considerable improve-
ment over the basic dependencies.
Combining all the semantic features (SEM-ALL)
provides a clear improvement, suggesting that the
information is heterogeneous. Finally, combing the
syntactic and semantic features gives the best results
by far (SYN-SEM: SYN-ALL + SEM-Dep + SEM-Class +
SEM-L2 + SP). The definitions sentences are harder
syntactically, and thus get more of a boost from the
semantics. The semantics still improve performance
for the example sentences.
The semantic class based sense features used here
are based on manual annotation, and thus show an
upper bound on the effects of these features. This
is not an absolute upper bound on the use of sense
information ? it may be possible to improve further
through feature engineering. The learning curves
(Fig 7) have not yet flattened out. We can still im-
prove by increasing the size of the training data.
5 Discussion
Bikel (2000) combined sense information and parse
information using a subset of SemCor (with Word-
Net senses and Penn-II treebanks) to produce a com-
bined model. This model did not use semantic de-
pendency relations, but only syntactic dependen-
cies augmented with heads, which suggests that the
deeper structural semantics provided by the HPSG
parser is important. Xiong et al (2005) achieved
only a very minor improvement over a plain syntac-
tic model, using features based on both the corre-
lation between predicates and their arguments, and
between predicates and the hypernyms of their argu-
ments (using HowNet). However, they do not inves-
tigate generalizing to different levels than a word?s
immediate hypernym.
31
Pioneering work by Toutanova et al (2005) and
Baldridge and Osborne (2007) on parse selection for
an English HPSG treebank used simpler semantic
features without sense information, and got a far less
dramatic improvement when they combined syntac-
tic and semantic information.
The use of hand-crafted lexical resources such as
the Goi-Taikei ontology is sometimes criticized on
the grounds that such resources are hard to produce
and scarce. While it is true that valency lexicons
and sense hierarchies are hard to produce, they are
of such value that they have already been created for
all of the languages we know of which have large
treebanks. In fact, there are more languages with
WordNets than large treebanks.
In future work we intend to confirm that we can
get improved results with raw sense disambiguation
results not just the gold standard annotations and test
the results on other sections of the Hinoki corpus.
6 Conclusions
We have shown that sense-based semantic features
combined with ontological information are effec-
tive for parse selection. Training and testing on
the definition subset of the Hinoki corpus, a com-
bined model gave a 5.6% improvement in parse se-
lection accuracy over a model using only syntactic
features (63.8% ? 69.4%). Similar results (76.2%
? 79.2%) were found with example sentences.
References
Jason Baldridge and Miles Osborne. 2007. Active learning and
logarithmic opinion pools for HPSG parse selection. Natural
Language Engineering, 13(1):1?32.
Daniel M. Bikel. 2000. A statistical model for parsing and
word-sense disambiguation. In Proceedings of the Joint SIG-
DAT Conference on Empirical Methods in Natural Language
Processing and Very Large Corpora, pages 155?163. Hong
Kong.
Francis Bond, Sanae Fujita, and Takaaki Tanaka. 2007. The Hi-
noki syntactic and semantic treebank of Japanese. Language
Resources and Evaluation. (Special issue on Asian language
technology).
Francis Bond and Satoshi Shirai. 1997. Practical and efficient
organization of a large valency dictionary. In Workshop on
Multilingual Information Processing ? Natural Language
Processing Pacific Rim Symposium ?97: NLPRS-97. Phuket.
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A. Sag.
2005. Minimal Recursion Semantics. An introduction. Re-
search on Language and Computation, 3(4):281?332.
Christine Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press.
Sanae Fujita and Francis Bond. 2004. A method of creating
new bilingual valency entries using alternations. In Gilles
Se?rasset, editor, COLING 2004 Multilingual Linguistic Re-
sources, pages 41?48. Geneva.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes: The
90% solution. In Proceedings of the Human Language
Technology Conference of the NAACL, Companion Volume:
Short Papers, pages 57?60. Association for Computational
Linguistics, New York City, USA. URL http://www.
aclweb.org/anthology/N/N06/N06-2015.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi
Ooyama, and Yoshihiko Hayashi. 1997. Goi-Taikei ?
A Japanese Lexicon. Iwanami Shoten, Tokyo. 5 vol-
umes/CDROM.
Kaname Kasahara, Hiroshi Sato, Francis Bond, Takaaki
Tanaka, Sanae Fujita, Tomoko Kanasugi, and Shigeaki
Amano. 2004. Construction of a Japanese semantic lexicon:
Lexeed. In IPSG SIG: 2004-NLC-159, pages 75?82. Tokyo.
(in Japanese).
Dan Klein and Christopher D. Manning. 2003. Accurate un-
lexicalized parsing. In Erhard Hinrichs and Dan Roth, edi-
tors, Proceedings of the 41st Annual Meeting of the Associ-
ation for Computational Linguistics, pages 423?430. URL
http://www.aclweb.org/anthology/P03-1054.pdf.
Robert Malouf. 2002. A comparison of algorithms for maxi-
mum entropy parameter estimation. In CONLL-2002, pages
49?55. Taipei, Taiwan.
Robert Malouf and Gertjan van Noord. 2004. Wide cover-
age parsing with stochastic attribute value grammars. In
IJCNLP-04 Workshop: Beyond shallow analyses - For-
malisms and statistical modeling for deep analyses. JST
CREST. URL http://www-tsujii.is.s.u-tokyo.ac.
jp/bsa/papers/malouf.pdf.
Stephan Oepen, Dan Flickinger, Kristina Toutanova, and
Christoper D. Manning. 2004. LinGO redwoods: A rich and
dynamic treebank for HPSG. Research on Language and
Computation, 2(4):575?596.
Stephan Oepen and Jan Tore L?nning. 2006. Discriminant-
based MRS banking. In Proceedings of the 5th International
Conference on Language Resources and Evaluation (LREC
2006). Genoa, Italy.
Carl Pollard and Ivan A. Sag. 1994. Head Driven Phrase Struc-
ture Grammar. University of Chicago Press, Chicago.
Melanie Siegel and Emily M. Bender. 2002. Efficient deep pro-
cessing of Japanese. In Proceedings of the 3rd Workshop on
Asian Language Resources and International Standardiza-
tion at the 19th International Conference on Computational
Linguistics, pages 1?8. Taipei.
Kristina Toutanova, Christopher D. Manning, Dan Flickinger,
and Stephan Oepen. 2005. Stochastic HPSG parse disam-
biguation using the redwoods corpus. Research on Language
and Computation, 3(1):83?105.
Deyi Xiong, Qun Liu Shuanglong Li and, Shouxun Lin, and
Yueliang Qian. 2005. Parsing the Penn Chinese treebank
with semantic knowledge. In Robert Dale, Jian Su Kam-Fai
Wong and, and Oi Yee Kwong, editors, Natural Language
Processing ? IJCNLP 005: Second International Joint Con-
ference Proceedings, pages 70?81. Springer-Verlag.
32
Proceedings of the 10th Conference on Parsing Technologies, pages 48?59,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Efficiency in Unification-Based N -Best Parsing
Yi Zhang?, Stephan Oepen?, and John Carroll?
?Saarland University, Department of Computational Linguistics, and DFKI GmbH (Germany)
?University of Oslo, Department of Informatics (Norway)
?University of Sussex, Department of Informatics (UK)
Abstract
We extend a recently proposed algorithm for
n-best unpacking of parse forests to deal ef-
ficiently with (a) Maximum Entropy (ME)
parse selection models containing important
classes of non-local features, and (b) forests
produced by unification grammars contain-
ing significant proportions of globally incon-
sistent analyses. The new algorithm empir-
ically exhibits a linear relationship between
processing time and the number of analyses
unpacked at all degrees of ME feature non-
locality; in addition, compared with agenda-
driven best-first parsing and exhaustive pars-
ing with post-hoc parse selection it leads to
improved parsing speed, coverage, and ac-
curacy.?
1 Background?Motivation
Technology for natural language analysis using lin-
guistically precise grammars has matured to a level
of coverage and efficiency that enables parsing of
large amounts of running text. Research groups
working within grammatical frameworks like CCG
(Clark & Curran, 2004), LFG (Riezler et al, 2002),
and HPSG (Malouf & van Noord, 2004; Oepen,
Flickinger, Toutanova, & Manning, 2004; Miyao,
Ninomiya, & Tsujii, 2005) have successfully in-
tegrated broad-coverage computational grammars
with sophisticated statistical parse selection models.
The former delineate the space of possible analy-
ses, while the latter provide a probability distribu-
?The first author warmly acknowledges the guidance of his
PhD advisors, Valia Kordoni and Hans Uszkoreit. We are grate-
ful to Ulrich Callmeier, Berthold Crysmann, Dan Flickinger,
and Erik Velldal for many discussions and their support. We
thank Ron Kaplan, Martin Kay, and Bob Moore for provid-
ing insightful information about related approaches, notably the
XLE and CLE parsers.
tion over competing hypotheses. Parse selection ap-
proaches for these frameworks often use discrimi-
native Maximum Entropy (ME) models, where the
probability of each parse tree, given an input string,
is estimated on the basis of select properties (called
features) of the tree (Abney, 1997; Johnson, Ge-
man, Canon, Chi, & Riezler, 1999). Such features,
in principle, are not restricted in their domain of
locality, and enable the parse selection process to
take into account properties that extend beyond lo-
cal contexts (i.e. sub-trees of depth one).
There is a trade-off in this set-up between the ac-
curacy of the parse selection model, on the one hand,
and the efficiency of the search for the best solu-
tion(s), on the other hand. Extending the context size
of ME features, within the bounds of available train-
ing data, enables increased parse selection accuracy.
However, the interplay of the core parsing algo-
rithm and the probabilistic ranking of alternate (sub-
)hypotheses becomes considerably more complex
and costly when the feature size exceeds the domain
of locality (of depth-one trees) that is characteristic
of phrase structure grammar-based formalisms. One
current line of research focuses on finding the best
balance between parsing efficiency and parse selec-
tion techniques of increasing complexity, aiming to
identify the most probable solution(s) with minimal
effort.
This paper explores a range of techniques, com-
bining a broad-coverage, high-efficiency HPSG
parser with a series of parse selection models with
varying context size of features. We sketch three
general scenarios for the integration: (a) a baseline
sequential configuration, where all results are enu-
merated first, and subsequently ranked; (b) an in-
terleaved but approximative solution, performing a
greedy search for an n-best list of results; and (c) a
two-phase approach, where a complete packed for-
48
est is created and combined with a specialized graph
search procedure to selectively enumerate results in
(globally) correct rank order. Although conceptu-
ally simple, the second technique has not previously
been evaluated for HPSG parsing (to the best of our
knowledge). The last of these techniques, which we
call selective unpacking, was first proposed by Car-
roll & Oepen (2005) in the context of chart-based
generation. However, they only provide an account
of the algorithm for local ME properties and assert
that the technique should generalize to larger con-
texts straightforwardly. This paper describes this
generalization of selective unpacking, in its appli-
cation to parsing, and demonstrates that the move
from features that resemble a context-free domain
of locality to features of, in principle, arbitrary con-
text size can indeed be based on the same algorithm,
but the required extensions are non-trivial.
The structure of the paper is as follows. Sec-
tion 2 summarizes our formalism, grammars used,
parse selection approach, and training and test data.
Section 3 discusses the range of possibilities for
structuring the process of statistical, grammar-based
parsing, and Sections 4 to 6 describe our approach
to efficient n-best parsing. We present experimental
results in Section 7, compare our approach to previ-
ous ones (Section 8), and finally conclude.
2 Overall Set-up
While couched in the HPSG framework, the tech-
niques explored here are applicable to the larger
class of unification-based grammar formalisms. We
make use of the DELPH-IN1 reference formalism,
as implemented by a variety of systems, including
the LKB (Copestake, 2002) and PET (Callmeier,
2002). For the experiments discussed here, we
adapted the open-source PET parsing engine in
conjunction with two publicly available grammars,
the English Resource Grammar (ERG; Flickinger,
2000) and the DFKI German Grammar (GG; Mu?ller
& Kasper, 2000, Crysmann, 2005). Our parse se-
lection models were trained and evaluated on HPSG
treebanks that are distributed with these grammars.
The following paragraphs summarize relevant prop-
erties of the structures manipulated by the parser,
1Deep Linguistic Processing with HPSG, an open-
source repository of grammars and processing tools; see
?http://www.delph-in.net/?.
subjh
hspec
det the le
the
sing noun
n intr le
dog
third sg fin verb
v unerg le
barks
Figure 1: Sample HPSG derivation tree for the sentence the
dog barks. Phrasal nodes are labeled with identifiers of gram-
mar rules, and (pre-terminal) lexical nodes with class names for
types of lexical entries.
followed by relevant background on parse selection.
Figure 1 shows an example ERG derivation tree.
Internal tree nodes are labeled with identifiers of
grammar rules, and leaves with lexical entries. The
derivation tree provides complete information about
the actual HPSG analysis, in the sense that it can be
viewed as a recipe for computing it. Lexical entries
and grammar rules alike are ultimately just feature
structures, complex and highly-structured linguistic
categories. When unified together in the configura-
tion depicted by the derivation tree, the resulting fea-
ture structure yields an HPSG sign, a detailed repre-
sentation of the syntactic and semantic properties of
the input string. Just as the full derivation denotes a
feature structure, so do its sub-trees, and for gram-
mars like the ERG and GG each such structure will
contain hundreds of feature ? value pairs.
Because of the lexicalized nature of HPSG (and
similar frameworks) our parsers search for well-
formed derivations in a pure bottom-up fashion.
Other than that, there are no hard-wired assumptions
about the order of computation, i.e. the specific pars-
ing strategy. Our basic set-up closely mimics that of
Oepen & Carroll (2002), where edges indexed by
sub-string positions in a chart represent the nodes of
the tree, recording both a feature structure (as its cat-
egory label) and the identity of the underlying lexi-
cal entry or rule in the grammar. Multiple edges de-
rived for identical sub-strings can be ?packed? into a
single chart entry in case their feature structures are
compatible, i.e. stand in an equivalence or subsump-
tion relation. By virtue of having each edge keep
back-pointers to its daughter edges?the immediate
sub-nodes in the tree whose combination resulted in
49
the mother edge?the parse forest provides a com-
plete and explicit encoding of all possible results in a
maximally compact form.2 A simple unpacking pro-
cedure is obtained from the cross-multiplication of
all local combinatorics, which is directly amenable
to dynamic programming.
Figure 2 shows a hypothetical forest (on the left),
where sets of edges exhibiting local ambiguity have
been packed into a single ?representative? edge, viz.
the one in each set with one or more incoming dom-
inance arcs. Confirming the findings of Oepen &
Carroll (2002), in our experiments packing under
feature structure subsumption is much more effec-
tive than packing under mere equivalence, i.e. for
each pair of edges (over identical sub-strings) that
stand in a subsumption relation, a technique that
Oepen & Carroll (2002) termed retro-active pack-
ing ensures that the more general of the two edges
remains in the chart. When packing under subsump-
tion, however, some of the cross-product of local
ambiguities in the forest may not be globally con-
sistent. Assume for example that, in Figure 2, edges
6 and 8 subsume 7 and 9 , respectively; combining
7 and 9 into the same tree during unpacking can in
principle fail. Thus, unpacking effectively needs to
deterministically replay unifications, but this extra
expense in our experience is negligible when com-
pared to the decreased cost of constructing the for-
est under subsumption. In Section 3 we argue that
this very property, in addition to increasing parsing
efficiency, interacts beneficially with parse selection
and on-demand enumeration of results in rank order.
Following (Johnson et al, 1999), a conditional
ME model of the probabilities of trees {t1 . . . tn}
for a string s, and assuming a set of feature
functions {f1 . . . fm} with corresponding weights
{?1 . . . ?m}, is defined as:
p(ti|s) =
exp?j ?jfj(ti)
?n
k=1 exp
?
j ?jfj(tk)
(1)
2This property of parse forests is not a prerequisite of the
chart parsing framework. The basic CKY procedure (Kasami,
1965), for example, as well as many unification-based adapta-
tions (e.g. the Core Language Engine; Moore & Alshawi, 1992)
merely record the local category of each edge, which is suffi-
cient for the recognition task and simplifies the search. How-
ever, reading out complete trees from the chart, then, amounts
to a limited form of search, going back to the rules of the gram-
mar itself to (re-)discover decomposition relations among chart
entries.
Type Sample Features
1 ?0 subjh hspec third sg fin verb?
1 ?1 ? subjh hspec third sg fin verb?
1 ?0 hspec det the le sing noun?
1 ?1 subjh hspec det the le sing noun?
1 ?2 ? subjh hspec det the le sing noun?
2 ?0 subjh third sg fin verb?
2 ?0 subjh hspce?
2 ?1 subjh hspec det the le?
2 ?1 subjh hspec sing noun?
3 ?1 n intr le dog?
3 ?2 det the le n intr le dog?
3 ?3  det the le n intr le dog?
Table 1: Examples of structural features extracted from the
derivation tree in Figure 1. The Type column indicates the
template corresponding to each sample feature; the integer that
starts each feature indicates the degree of grandparenting (in the
case of type 1 and 2 features) or n-gram size (type 3 features).
The symbols ? and  denote the root of the tree and left pe-
riphery of the yield, respectively.
Feature functions fj can test for arbitrary structural
properties of analyses ti, and their value typically is
the number of times a specific property is present
in ti. Toutanova, Manning, Flickinger, & Oepen
(2005) propose an inventory of features that per-
form well in HPSG parse selection; currently we re-
strict ourselves to the best-performing of these, of
the form illustrated in Table 1, comprising depth-
one sub-trees (or portions of these) with grammar-
internal identifiers as node labels, plus optionally
a chain of one or more dominating nodes (i.e. lev-
els of grandparents). If a grandparents chain is
present then the feature is non-local. For expository
purposes, Table 1 includes another feature type, n-
grams over leaf nodes of the derivation; in Section 5
below we speculate about the incorporation of these
(and similar) features in our algorithm.
3 Interleaving Parsing and Ranking
At an abstract level, given a grammar and an associ-
ated ME parse selection model, there are three basic
ways of combining them in order to find the single
?best? or small set of n-best results.
The first way is a na??ve sequential set-up, in which
the parser first enumerates the full set of analyses,
computes a score for each using the model, and re-
turns the highest-ranking n results. For carefully
50
1 ?
?
2 3
?
|
?
4 3
?
2 ?
?
5 6
?
|
?
5 7
?
4 ?
?
8 6
?
|
?
8 7
?
|
?
9 6
?
|
?
9 7
?
6 ?
?
10
?
|
?
11
?
Figure 2: Sample forest and sub-node decompositions: ovals in the forest (on the left) indicate packing of edges under subsump-
tion, i.e. edges 4 , 7 , 9 , and 11 are not in the chart proper. During unpacking, there will be multiple ways of instantiating a
chart edge, each obtained from cross-multiplying alternate daughter sequences locally. The elements of this cross-product we call
decomposition, and they are pivotal points both for stochastic scoring and dynamic programming in selective unpacking. The table
on the right shows all non-leaf decompositions for our example packed forest: given two ways of decomposing 6 , there will be
three candidate ways of instantiating 2 and six for 4 , respectively, for a total of nine full trees.
crafted grammars and inputs of average complexity
the approach can perform reasonably well.
Another mode of operation is to organize the
parser?s search according to an agenda (i.e. priority
queue) that assigns numeric scores to parsing moves
(Erbach, 1991). Each such move is an application of
the fundamental rule of chart parsing, combining an
active and a passive edge, and the scores represent
the expected ?figure of merit? (Caraballo & Char-
niak, 1998) of the resulting structure. Assuming a
parse selection model of the type sketched in Sec-
tion 2, we can determine the agenda priority for a
parsing move according to the (unnormalized) ME
score of the derivation (sub-)tree that would result
from its successful execution. Note that, unlike in
probabilistic context-free grammars (PCFGs), ME
scores of partial trees do not necessarily decrease as
the tree size increases; instead, the distribution of
feature weights is in the range (??,+?), centered
around 0, where negative weights intuitively corre-
spond to dis-preferred properties.
This lack of monotonicity in the scores associated
with sub-trees, on the one hand, is beneficial, in that
performing a greedy best-first search becomes prac-
tical: in contrast, with PCFGs and their monoton-
ically decreasing probabilities on larger sub-trees,
once the parser finds the first full tree the chart nec-
essarily has been instantiated almost completely. On
the other hand, the same property prohibits the appli-
cation of exact best-first techniques like A? search,
because there is no reliable future cost estimate; in
this respect, our set-up differs fundamentally from
that of Klein & Manning (2003) and related PCFG
parsing work. Using the unnormalized sum of ME
weights on a partial solution as its agenda score, ef-
fectively, means that sub-trees with low scores ?sink?
to the bottom of the agenda; highly-ranked partial
constituents, in turn, instigate the immediate cre-
ation of larger structures, and ideally the bottom-up
agenda-driven search will greedily steer the parser
towards full analyses with high scores. Given its
heuristic nature, this procedure cannot guarantee
that its n-best list of results corresponds to the glob-
ally correct rank order, but it may in practice come
reasonably close to it. While conceptually simple,
greedy best-first search does not combine easily with
ambiguity packing in the chart: (a) at least when
packing under subsumption, it is not obvious how
to accurately compute the agenda score of packed
nodes, and (b) to the extent that the greedy search
avoids exploration of dis-preferred local ambigu-
ity, the need for packing should be greatly reduced.
Unfortunately, in scoring bottom-up parsing moves,
ME features involving grandparenting are not ap-
plicable, leading to a second potential source of re-
duced parse selection accuracy. In Section 7 below,
we provide an empirical evaluation of both the na??ve
sequential and greedy best-first approaches.
4 Selective Unpacking
Carroll & Oepen (2005) observe that, at least for
grammars like the ERG, the construction of the
parse forest can be very efficient (with observed
polynomial complexity), especially when packing
edges under subsumption. Their selective unpacking
procedure, originally proposed for the forest created
by a chart generator, aims to unpack the n-best set
51
1 procedure selectively-unpack-edge(edge , n) ?
2 results? ??; i? 0;
3 do
4 hypothesis? hypothesize-edge(edge , i); i? i + 1;
5 if (new? instantiate-hypothesis(hypothesis)) then
6 n? n ? 1; results? results ? ?new?;
7 while (hypothesis and n ? 1)
8 return results;
9 procedure hypothesize-edge(edge , i) ?
10 if (edge.hypotheses[i]) return edge.hypotheses[i];
11 if (i = 0) then
12 for each (decomposition in decompose-edge(edge)) do
13 daughters? ? ?; indices? ? ?
14 for each (edge in decomposition.rhs) do
15 daughters? daughters ? ?hypothesize-edge(edge, 0)?;
16 indices? indices ? ?0?;
17 new-hypothesis(edge, decomposition, daughters, indices);
18 if (hypothesis? edge.agenda.pop()) then
19 for each (indices in advance-indices(hypothesis.indices)) do
20 if (indices ? hypothesis.decomposition.indices) then continue
21 daughters? ? ?;
22 for each (edge in hypothesis.decomposition.rhs) each (i in indices) do
23 daughter? hypothesize-edge(edge, i);
24 if (not daughter) then daughters? ??; break
25 daughters? daughters ? ?daughter?;
26 if (daughters) then new-hypothesis(edge, hypothesis.decomposition, daughters, indices)
27 edge.hypotheses[i]? hypothesis;
28 return hypothesis;
29 procedure new-hypothesis(edge , decomposition , daughters , indices) ?
30 hypothesis? new hypothesis(decomposition, daughters, indices);
31 edge.agenda.insert(score-hypothesis(hypothesis), hypothesis);
32 decomposition.indices? decomposition.indices? {indices};
Figure 3: Selective unpacking procedure, enumerating the n best realizations for a top-level result edge from a packed forest. An
auxiliary function decompose-edge() performs local cross-multiplication as shown in the examples in Figure 2. Another utility
function not shown in pseudo-code is advance-indices(), a ?driver? routine searching for alternate instantiations of daughter edges,
e.g. advance-indices(?0 2 1?)? {?1 2 1? ?0 3 1? ?0 2 2?}. Finally, instantiate-hypothesis() is the function that actually builds
result trees, replaying the unifications of constructions from the grammar (as identified by chart edges) with the feature structures
of daughter constituents.
of full trees from the forest, guaranteeing the glob-
ally correct rank order according to the probability
distribution, with a minimal amount of search. The
basic algorithm is a specialized graph search through
the forest, with local contexts of optimization corre-
sponding to packed nodes.
Each such node represents local combinatorics,
and two key notions in the selective unpacking pro-
cedure are the concepts of (a) decomposing an edge
locally into candidate ways of instantiating it, and
of (b) nested contexts of local search for ranked
hypotheses (i.e. uninstantiated edges) about candi-
date subtrees. See Figure 2 for examples of the de-
composition of edges. Given one decomposition?
i.e. a vector of candidate daughters for a particu-
lar rule?there can be multiple ways of instanti-
ating each daughter: a parallel index vector ~I =
?i0 . . . in? serves to keep track of ?vertical? search
among daughter hypotheses, where each index ij
denotes the i-th best instantiation (hypothesis) of
the daughter at position j. If we restrict ME fea-
tures to a depth of one (i.e. without grandparent-
ing), then given the additive nature of ME scores
on complete derivations, it can be guaranteed that
hypothesized trees including an edge e as an im-
mediate daughter must use the best instantiation of
e in their own best instantiation. Assuming a bi-
nary rule, the corresponding hypothesis would use
daughter indices of ?0 0?. The second-best instan-
tiation, in turn, can be obtained from moving to the
second-best hypothesis for one of the elements in the
(right-hand side of the) decomposition, e.g. indices
52
?0 1? or ?1 0? in the binary example. Hypotheses are
associated with ME scores and ordered within each
nested context by means of a local priority queue
(stored in the original representative edge, for con-
venience). Therefore, nested local optimizations re-
sult in a top-down, breadth-first, exact n-best search
through the packed forest, while avoiding exhaustive
cross-multiplication of packed nodes.
Figure 3 shows the unchanged pseudo-code of
Carroll & Oepen (2005). The main function
hypothesize-edge() controls both the ?horizontal? and
?vertical? search, initializing the set of decompo-
sitions and pushing initial hypotheses onto the lo-
cal agenda when called on an edge for the first
time (lines 11 ? 17). For each call, the procedure
retrieves the current next-best hypothesis from the
agenda (line 18), generates new hypotheses by ad-
vancing daughter indices (while skipping over con-
figurations seen earlier) and calling itself recursively
for each new index (lines 19 ? 26), and, finally, ar-
ranging for the resulting hypothesis to be cached for
later invocations on the same edge and i values (line
27). Note that unification (in instantiate-hypothesis())
is only invoked on complete, top-level hypotheses,
as our structural ME features can actually be eval-
uated prior to building each full feature structure.
However, as Carroll & Oepen (2005) suggest, the
procedure could be adapted to perform instantiation
of sub-hypotheses within each local search, should
additional features require it. For better efficiency,
the instantiate-hypothesis() routine applies dynamic
programming (i.e. memoization) to intermediate re-
sults.
5 Generalizing the Algorithm
Carroll & Oepen (2005) offer no solution for selec-
tive unpacking with larger context ME features. Yet,
both Toutanova et al (2005) and our own experi-
ments (described in Section 7 below) suggest that
properties of larger contexts and especially grand-
parenting can greatly improve parse selection ac-
curacy. The following paragraphs outline how to
generalize the basic selective unpacking procedure,
while retaining its key properties: exact n-best enu-
meration with minimal search. Our generalization of
the algorithm distinguishes between ?upward? con-
texts, with grandparenting with dominating nodes as
a representative feature type, and ?downward? exten-
sions, which we discuss for the example of lexical
n-gram features.
A na??ve approach to selective unpacking with
grandparenting might be extending the cross-
multiplication of local ambiguity to trees of more
than depth one. However, with multiple levels of
grandparenting this approach would greatly increase
the combinatorics to be explored, and it would pose
the puzzle of overlapping local contexts of opti-
mization. Choices made among the alternates for
one packed node would interact with other ambi-
guity contexts in their internal nodes, rather than
merely at the leaves of their decompositions. How-
ever, it is sufficient to keep the depth of decompo-
sitions to minimal sub-trees and rather contextual-
ize each decomposition as a whole. Assuming our
sample forest and set of decompositions from Fig-
ure 2, let ?1 4 ? : 6 ??10 ? denote the decomposi-
tion of node 6 in the context of 4 and 1 as its
immediate parents. When descending through the
forest, hypothesize-edge() can, without significant ex-
tra cost, maintain a vector ~P = ?pn . . . p0? of par-
ents of the current node, for n-level grandparenting.
For each packed node, the bookkeeping elements of
the graph search procedure need to be contextual-
ized on ~P , viz. (a) the edge-local priority queue,
(b) the record of index vectors hypothesized already,
and (c) the cache of previous instantiations. Assum-
ing each is stored in an associative array, then all
references to edge.agenda in the original procedure
can be replaced by edge.agenda[~P], and likewise for
other slots. With these extensions in place, the orig-
inal control structure of nested, on-demand creation
of hypotheses and dynamic programming of partial
results can be retained, and for each packed node
with multiple parents ( 6 in our sample forest) there
will be parallel, contextualized partitions of opti-
mization. Thus, extra combinatorics introduced in
this generalized procedure are confined to only such
nodes, which (intuitively at least) appears to estab-
lish the lower bound of added search needed?while
keeping the algorithm non-approximative. Section 7
provides empirical data on the degradation of the
procedure in growing levels of grandparenting and
the number of n-best results to be extracted from the
forest.
Finally, we turn to enlarged feature contexts that
53
capture information from nodes below the elements
of a local decomposition. Consider the example
of feature type 3 in Table 1, n-grams (of vari-
ous size) over properties of the yield of the parse
tree. For now we only consider lexical bi-grams.
For an edge e dominating a sub-string of n words
?wi . . . wi+n?1? there will be n? 1 bi-grams inter-
nal to e, and two bi-grams that interact with wi?1
and wi+n?which will be determined by the left-
and right-adjacent edges to e in a complete tree. The
internal bi-grams are unproblematic, and we can as-
sume that ME weights corresponding to these fea-
tures have been included in the sum of weights as-
sociated to e. Seeing that e may occur in multiple
trees, with different sister edges, the selective un-
packing procedure has to take this variation into ac-
count when evaluating local contexts of optimiza-
tion.
Let xey denote an edge e, with x and y as the
lexical types of its leftmost and rightmost daugh-
ters, respectively. Returning to our sample forest,
assume lexicalizations ? 10? and ? 11 ? (each span-
ning only one word), with ? 6= ?. Obviously, when
decomposing 4 as ?8 6 ?, its ME score, in turn, will
depend on the choice made in the expansion of 6 :
the sequences
?
? 8? ? 6?
?
and
?
? 8? ? 6 ?
?
will dif-
fer in (at least) the scores associated with the bi-
grams ???? vs. ????. Accordingly, when evalu-
ating candidate decompositions of 4 , the number of
hypotheses that need to be considered is doubled;
as an immediate consequence, there can be up to
eight distinct lexicalized variants for the decompo-
sition 1 ??4 3 ? further up in the tree. It may look
as if combinatorics will cross-multiply throughout
the tree?in the worst case returning us to an ex-
ponential number of hypotheses?but this is fortu-
nately not the case: regarding the external bi-grams
of 1 , node 6 no longer participates in its left- or
rightmost periphery, so variation internal to 6 is not
a multiplicative factor at this level. This is essen-
tially the observation of Langkilde (2000), and her
bottom-up factoring of n-gram computation is eas-
ily incorporated into our top-down selective unpack-
ing control structure. At the point where hypothesize-
edge() invokes itself recursively (line 23 in Figure 3),
its return value is now a set of lexicalized alternates,
and hypothesis creation (in line 26) can take into ac-
count the local cross-product of all such alternation.
Including additional properties from non-local sub-
trees (for example higher-order n-grams and head
lexicalization) is a straightforward extension of this
scheme, replacing our per-edge left- and rightmost
periphery symbols with a generalized vector of ex-
ternally relevant, internal properties. In addition
to traditional (head) lexicalization as we have just
discussed it, such extended ?downward? properties
on decompositions?percolated from daughters to
mothers and cross-multiplied as appropriate?could
include metrics of constituent weight too, for exam-
ple to enable the ME model to prefer ?balanced? co-
ordination structures.
However, given that Toutanova et al (2005) ob-
tain only marginally improved parse selection accu-
racy from the inclusion of n-gram (and other lexical)
ME features, we have left the implementation of lex-
icalization and empirical evaluation for future work.
6 Failure Caching and Propagation
As we pointed out at the end of Section 4, during
the unpacking phase, unification is only replayed in
instantiate-hypothesis() on the top-level hypotheses. It
is only at this step that inconsistencies in the local
combinatorics are discovered. However, such a dis-
covery can be used to improve the unpacking rou-
tine by (a) avoiding further unification on hypothe-
ses that have already failed to instantiate, (b) avoid-
ing creating new hypotheses based on failed sub-
hypotheses. This requires some changes to the rou-
tines instantiate-hypothesis() and hypothesize-edge(), as
well as an extra boolean marker for each hypothesis.
The extended instantiate-hypothesis() starts by
checking whether the hypothesis is already marked
as failed. If it is not so marked, the routine recur-
sively instantiates all sub-hypotheses. Any failure
will again lead to instant return. Otherwise, unifica-
tion is used to create a new edge from the outcome of
the sub-hypothesis instantiations. If this unification
fails, the current hypothesis is marked. Moreover,
all its ancestor hypotheses are also marked (by re-
cursively following the pointers to the direct parent
hypotheses) as they are also guaranteed to fail.
Correspondingly, hypothesize-edge() needs to
check the instantiation failure marker to avoid re-
turning hypotheses that are guaranteed to fail. If
a hypothesis coming out of the agenda is already
54
marked as failed, it will be used to create new hy-
potheses (with advance-indices()), but dropped af-
terward. Subsequent hypotheses will be popped
from the agenda until either a hypothesis that is not
marked as failed is returned, or the agenda is empty.
Moreover, hypothesize-edge() also needs to avoid
creating new hypotheses based on failed sub-
hypotheses. When a failed sub-hypothesis is found,
the creation of the new hypothesis is skipped. But
the index vector ~I may not be simply discarded.
Otherwise hypotheses based on advance-indices(~I)
will not be reachable in the search. On the other
hand, simply adding every advance-indices(~I) on to
the pending creation list is not efficient either in the
case where multiple sub-hypotheses fail.
To solve the problem, we compute a failure vec-
tor ~F = ?f0 . . . fn?, where fj is 1 when the sub-
hypothesis at position j is known as failed, and 0
otherwise. If a sub-hypothesis at position j is failed
then all the index vectors having value ij at posi-
tion j must also fail. By putting the result of ~I + ~F
on the pending creation list, we can safely skip the
failed rows of sub-hypotheses, while not losing the
reachability of the others. As an example, suppose
we have a ternary index vector ?3 1 2? for which a
new hypothesis is to be created. By checking the in-
stantiation failure marker of the sub-hypotheses, we
find that the first and the third sub-hypotheses are al-
ready marked. The failure recording vector will then
be ?1 0 1?. By putting ?4 1 3? = ?3 1 2? + ?1 0 1?
on to the pending hypothesis creation list, the failed
sub-hypotheses are skipped.
We evaluate the effects of instantiation failure
caching and propagation below in Section 7.
7 Empirical Results
To evaluate the performance of the selective unpack-
ing algorithm, we carried out a series of empirical
evaluations with the ERG and GG, in combination
with a modified version of the PET parser. When
running the ERG we used as our test set the JH4
section of the LOGON treebank3, which contains
1603 items with an average sentence length of 14.6
words. The remaining LOGON treebank (of around
3The treebank is comprised of several booklets of
edited, instructional texts on backcountry activities in Nor-
way. The data is available from the LOGON web site at
?http://www.emmtee.net?.
Configuration GP Coverage Time (s)
greedy best-first 0 91.6% 3889
exhaustive unpacking 0 84.5% 4673
selective unpacking
0 94.3% 2245
1 94.3% 2529
2 94.3% 3964
3 94.2% 3199
4 94.2% 3502
Table 2: Coverage on the ERG for different configurations, with
fixed resource consumption limits (of 100k passive edges or 300
seconds). In all cases, up to ten ?best? results were searched,
and Coverage shows the percentage of inputs that succeed to
parse within the available resource. Time shows the end-to-end
processing time for each batch.
5 15 25 35
String Length (Number of Input Tokens)
0
1
2
3
4
5
6
(s)
(generated by [incr tsdb()] at 23-mar-2007 (12:44 h))?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
? gready best-first
? exhaustive unpacking
? selective unpacking
? forest creation
Figure 4: Parsing times for different configurations using the
ERG, in all three cases searching for up to ten results, without
the use of grandparenting.
8,000 items) was used in training the various ME
parse disambiguation models. For the experiment
with GG, we designated a 2825-item portion of the
DFKI Verbmobil treebank4 for our tests, and trained
ME models on the remaining 10,000 utterances. At
only 7.4 words, the average sentence length is much
shorter in the Verbmobil data.
We ran seven different configurations of the parser
with different search strategies and (un-)packing
mechanisms:
? Agenda driven greedy n-best parsing using the
ME score without grandparenting features; no
local ambiguity packing;
? Local ambiguity packing with exhaustive un-
packing, without grandparenting features;
4The data in this treebank is taken from transcribed appoint-
ment scheduling dialogues; see ?http://gg.dfki.de/?
for further information on GG and its treebank.
55
1 10 20 30 40 50 60 70 80 90 100
Maximum Number of Trees to Unpack (n)
0.00
0.02
0.04
0.06
0.08
0.10
(s)
? ? ? ? ?
? ?
?? ? ? ?
? ? ?
?
? ? ? ?
? ? ?
?? ? ?
? ? ?
?
?? ?
? ?
? ? ?
?
GP=0
GP=1
GP=2
GP=3
GP=4
Figure 5: Mean times for selective unpacking of all test items
for n-best parsing with the ERG, for varying n and grandpar-
enting (GP) levels
? Local ambiguity packing and selective unpack-
ing for n-best parsing, with 0 through 4 levels
of grandparenting (GP) features.
As a side-effect of differences in efficiency, some
configurations could not complete parsing all sen-
tences given reasonable memory constraints (which
we set at a limit of 100k passive edges or 300 sec-
onds processing time per item). The overall cover-
age and processing time of different configurations
on JH4 are given in Table 2.
The correlation between processing time and cov-
erage is interesting. However, it makes the efficiency
comparison difficult as parser behavior is not clearly
defined when the memory limit is exceeded. To cir-
cumvent this problem, in the following experiments
we average only over those 1362 utterances from
JH4 that complete parsing within the resource limit
in all seven configurations. Nevertheless, it must
be noted that this restriction potentially reduces effi-
ciency differences between configurations, as some
of the more challenging inputs (which typically lead
to the largest differences) are excluded.
Figure 4 compares the processing time of differ-
ent configurations. The difference is much more
significant for longer sentences (i.e. with more than
15 words). If the parser unpacks exhaustively, the
time for unpacking grows with sentence length at a
quickly increasing rate. In such cases, the efficiency
gain with ambiguity packing in the parsing phase
is mostly lost in the unpacking phase. The graph
shows that greedy best-first parsing without packing
outperforms exhaustive unpacking for sentences of
Configuration Exact Match Top Ten
random choice 11.34 43.06
no grandparenting 52.52 68.38
greedy best-first 51.79 69.48
grandparenting[1] 56.83 85.33
grandparenting[2] 56.55 84.14
grandparenting[3] 56.37 84.14
grandparenting[4] 56.28 84.51
Table 3: Parse selection accuracy for various levels of grandpar-
enting. The exact match column shows the percentage of cases
in which the correct tree, according to the treebank, was ranked
highest by the model; conversely, the top ten column indicates
how often the correct tree was among the ten top-ranking re-
sults.
less than 25 words. With sentences longer than 25
words, the packing mechanism helps the parser to
overtake greedy best-first parsing, although the ex-
haustive unpacking time also grows fast.
With the selective unpacking algorithm presented
in the previous sections, unpacking time is reduced,
and grows only slowly as sentence length increases.
Unpacking up to ten results, when contrasted with
the timings for forest creation (i.e. the first parsing
phase) in Figure 4, adds a near-negligible extra cost
to the total time required for both phases. Moreover,
Figure 5 shows that with selective unpacking, as n
is increased, unpacking time grows roughly linearly
for all levels of grandparenting (albeit always with
an initial delay in unpacking the first result).
Table 4 summarizes a number of internal parser
measurements using the ERG with different pack-
ing/unpacking settings. Besides the difference in
processing time, we also see a significant difference
in ?space? between exhaustive and selective un-
packing. Also, the difference in ?unifications? and
?copies? indicates that with our selective unpacking
algorithm, these expensive operations on typed fea-
ture structures are significantly reduced.
In return for increased processing time (and
marginal loss in coverage) when using grandparent-
ing features, Table 3 shows some large improve-
ments in parse selection accuracy (although the pic-
ture is less clear-cut at higher-order levels of grand-
parenting5). A balance point between efficiency
5The models were trained using the open-source TADM pack-
age (Malouf, 2002), using default hyper-parameters for all con-
figurations, viz. a convergence threshold of 10?8, variance of
the prior of 10?4, and frequency cut-off of 5. It is likely that
56
Configuration GP Unifications Copies Space Unpack Total(#) (#) (kbyte) (s) (s)
? 15
greedy best-first 0 1845 527 2328 ? 0.12
words
exhaustive unpacking 0 2287 795 8907 0.01 0.12
selective unpacking
0 1912 589 8109 0.00 0.12
1 1913 589 8109 0.01 0.12
2 1914 589 8109 0.01 0.12
3 1914 589 8110 0.01 0.12
4 1914 589 8110 0.02 0.13
> 15
greedy best-first 0 25233 5602 24646 ? 1.66
words
exhaustive unpacking 0 39095 15685 80832 0.85 1.95
selective unpacking
0 17489 4422 33326 0.03 1.17
1 17493 4421 33318 0.05 1.21
2 17493 4421 33318 0.09 1.25
3 17495 4422 33321 0.13 1.27
4 17495 4422 33320 0.21 1.34
Table 4: Contrasting the efficiency of various (un-)packing settings in use with ERG on short (top) and medium-length (bottom)
inputs; in each configuration, up to ten trees are extracted. Unification and Copies is the count of top-level FS operations, where
only successful unifications require a subsequent copy (when creating a new edge). Unpack and Total are unpacking and total parse
time, respectively.
and accuracy can be made according to application
needs.
Finally, we compare the processing time of the
selective unpacking algorithm with and without in-
stantiation failure caching and propagation (as de-
scribed in Section 4 above). The empirical results
for GG are summarized in Table 5, showing clearly
that the technique reduced unnecessary hypotheses
and instantiation failures. The design philosophy of
the ERG and GG differ. During the first, forest cre-
ation phase, GG suppresses a number of features (in
the HPSG sense, not the ME sense) that can actually
constrain the combinatorics of edges. This move
makes the packed forest more compact, but it im-
plies that unification failures will be more frequent
during unpacking. In a sense, GG thus moves part
of the search for globally consistent derivations into
the second phase, and it is possible for the forest to
contain ?result? trees that ultimately turn out to be
incoherent. Dynamic programming of instantiation
failures makes this approach tractable, while retain-
ing the general breadth-first characteristic of the se-
lective unpacking regime.
further optimization of hyper-parameters for individual config-
urations would moderately improve model performance, espe-
cially for higher-order grandparenting levels with large numbers
of features.
8 Discussion
The approach to n-best parsing described in this pa-
per takes as its point of departure recent work of Car-
roll & Oepen (2005), which describes an efficient al-
gorithm for unpacking n-best trees from a forest pro-
duced by a chart-based sentence generator and con-
taining local ME properties with associated weights.
In an almost contemporaneous study, but in the con-
text of parsing with treebank grammars, Huang &
Chiang (2005) develop a series of increasingly effi-
cient algorithms for unpacking n-best results from
a weighted hypergraph representing a parse forest.
The algorithm of Carroll & Oepen (2005) and the
final one of Huang & Chiang (2005) are essentially
equivalent, and turn out to be reformulations of an
approach originally described by Jime?nez & Marzal
(2000) (although expressed there only for grammars
in Chomsky Normal Form).
In this paper we have considered ME properties
that extend beyond immediate dominance relations,
extending up to 4 levels of grandparenting. Pre-
vious work has either assumed properties that are
restricted to the minimal parse fragments (i.e. sub-
trees of depth one) that make up the packed repre-
sentation (Geman & Johnson, 2002), or has taken a
more relaxed approach by allowing non-local prop-
57
Configuration Unifications Copies Hypotheses Space Unpack Total(#) (#) (#) (kbyte) (ms) (ms)
greedy best-first 5980 1447 ? 9202 ? 400
selective, no caching 5535 1523 1245 27188 70 410
selective, with cache 4915 1522 382 27176 10 350
Table 5: Efficiency effects of the instantiation failure caching and propagation with GG, without grandparenting. All statistics are
averages over the 1941 items that complete within the resource bounds in all three configurations. Unification, Copies, Unpack,
and Total have the same interpretation as in Table 4, and Hypotheses is the average count of hypothesized sub-trees.
erties but without addressing the problem of how to
efficiently extract the top-ranked trees from a packed
forest (Miyao & Tsujii, 2002).
Probably the work closest in spirit to our approach
is that of Malouf & van Noord (2004), who use an
HPSG grammar comparable to the ERG and GG,
non-local ME features, and a two-phase parse for-
est creation and unpacking approach. However, their
unpacking phase uses a beam search to find a good
(single) candidate for the best parse; in contrast?
for ME models containing the types of non-local
features that are most important for accurate parse
selection?we avoid an approximative search and ef-
ficiently identify exactly the n-best parses.
When parsing with context free grammars, a (sin-
gle) parse can be retrieved from a parse forest in
time linear in the length of the input string (Bil-
lot & Lang, 1989). However, as discussed in Sec-
tion 2, when parsing with a unification-based gram-
mar and packing under feature structure subsump-
tion, the cross-product of some local ambiguities
may not be globally consistent. This means that ad-
ditional unifications are required at unpacking time.
In principle, when parsing with a pathological gram-
mar with a high rate of failure, extracting a single
consistent parse from the forest could take exponen-
tial time (see Lang (1994) for a discussion of this is-
sue with respect to Indexed Grammars). In the case
of GG, a high rate of unification failure in unpacking
is dramatically reduced by our instantiation failure
caching and propagation mechanism.
9 Conclusions and Future Work
We have described and evaluated an algorithm for
efficiently computing the n-best analyses from a
parse forest produced by a unification grammar, with
respect to a Maximum Entropy (ME) model con-
taining two classes of non-local features. The al-
gorithm is efficient in that it empirically exhibits a
linear relationship between processing time and the
number of analyses unpacked, at all degrees of ME
feature non-locality. It improves over previous work
in providing the only exact procedure for retrieving
n-best analyses from a packed forest that can deal
with features with extended domains of locality and
with forests created under subsumption. Our algo-
rithm applies dynamic programming to intermediate
results and local failures in unpacking alike.
The experiments compared the new algorithm
with baseline systems representing other possible
approaches to parsing with ME models: (a) a single
phase of agenda-driven parsing with on-line prun-
ing based on intermediate ME scores, and (b) two-
phase parsing with exhaustive unpacking and post-
hoc ranking of complete trees. The new approach
showed better speed, coverage, and accuracy than
the baselines.
Although we have dealt with the non-local ME
features that in previous work have been found to be
the most important for parse selection (i.e. grand-
parenting and n-grams), this does not exhaust the
full range of features that could possibly be useful.
For example, it may be the case that accurately re-
solving some kinds of ambiguities can only be done
with reference to particular parts?or combinations
of parts?of the HPSG feature structures represent-
ing the analysis of a complete constituent. To deal
with such cases we are currently designing an exten-
sion to the algorithms described here which would
add a ?controlled? beam search, in which the size of
the beam was limited by the interval of score adjust-
ments for ME features that could only be evaluated
once the full linguistic structure became available.
This approach would involve a constrained amount
of extra search, but would still produce the exact n-
best trees.
58
References
Abney, S. P. (1997). Stochastic attribute-value grammars. Com-
putational Linguistics, 23, 597 ? 618.
Billot, S., & Lang, B. (1989). The structure of shared forests
in ambiguous parsing. In Proceedings of the 27th Meeting
of the Association for Computational Linguistics (pp. 143 ?
151). Vancouver, BC.
Callmeier, U. (2002). Preprocessing and encoding techniques
in PET. In S. Oepen, D. Flickinger, J. Tsujii, & H. Uszkor-
eit (Eds.), Collaborative language engineering. A case study
in efficient grammar-based processing. Stanford, CA: CSLI
Publications.
Caraballo, S. A., & Charniak, E. (1998). New figures of merit
for best-first probabilistic chart parsing. Computational Lin-
guistics, 24(2), 275 ? 298.
Carroll, J., & Oepen, S. (2005). High-efficiency realization for
a wide-coverage unification grammar. In R. Dale & K. F.
Wong (Eds.), Proceedings of the 2nd International Joint
Conference on Natural Language Processing (Vol. 3651, pp.
165 ? 176). Jeju, Korea: Springer.
Clark, S., & Curran, J. R. (2004). Parsing the WSJ using CCG
and log-linear models. In Proceedings of the 42nd Meeting
of the Association for Computational Linguistics (pp. 104 ?
111). Barcelona, Spain.
Copestake, A. (2002). Implementing typed feature structure
grammars. Stanford, CA: CSLI Publications.
Crysmann, B. (2005). Relative clause extraposition in German.
An efficient and portable implementation. Research on Lan-
guage and Computation, 3(1), 61 ? 82.
Erbach, G. (1991). A flexible parser for a linguistic develop-
ment environment. In O. Herzog & C.-R. Rollinger (Eds.),
Text understanding in LILOG (pp. 74 ? 87). Berlin, Ger-
many: Springer.
Flickinger, D. (2000). On building a more efficient grammar
by exploiting types. Natural Language Engineering, 6 (1),
15 ? 28.
Geman, S., & Johnson, M. (2002). Dynamic programming for
parsing and estimation of stochastic unification-based gram-
mars. In Proceedings of the 40th Meeting of the Association
for Computational Linguistics. Philadelphia, PA.
Huang, L., & Chiang, D. (2005). Better k-best parsing. In
Proceedings of the 9th International Workshop on Parsing
Technologies (pp. 53 ? 64). Vancouver, Canada.
Jime?nez, V. M., & Marzal, A. (2000). Computation of the
n best parse trees for weighted and stochastic context-free
grammars. In Proceedings of the Joint International Work-
shops on Advances in Pattern Recognition (pp. 183 ? 192).
London, UK: Springer-Verlag.
Johnson, M., Geman, S., Canon, S., Chi, Z., & Riezler, S.
(1999). Estimators for stochastic ?unification-based? gram-
mars. In Proceedings of the 37th Meeting of the Association
for Computational Linguistics (pp. 535 ? 541). College Park,
MD.
Kasami, T. (1965). An efficient recognition and syntax al-
gorithm for context-free languages (Technical Report # 65-
758). Bedford, MA: Air Force Cambrige Research Labora-
tory.
Klein, D., & Manning, C. D. (2003). A* parsing. Fast exact
Viterbi parse selection. In Proceedings of the 4th Confer-
ence of the North American Chapter of the ACL. Edmonton,
Canada.
Lang, B. (1994). Recognition can be harder than parsing. Com-
putational Intelligence, 10(4), 486 ? 494.
Langkilde, I. (2000). Forest-based statistical sentence gener-
ation. In Proceedings of the 1st Conference of the North
American Chapter of the ACL. Seattle, WA.
Malouf, R. (2002). A comparison of algorithms for maxi-
mum entropy parameter estimation. In Proceedings of the
6th Conference on Natural Language Learning. Taipei, Tai-
wan.
Malouf, R., & van Noord, G. (2004). Wide coverage parsing
with stochastic attribute value grammars. In Proceedings of
the IJCNLP workshop Beyond Shallow Analysis. Hainan,
China.
Miyao, Y., Ninomiya, T., & Tsujii, J. (2005). Corpus-oriented
grammar development for acquiring a Head-Driven Phrase
Structure Grammar from the Penn Treebank. In K.-Y. Su,
J. Tsujii, J.-H. Lee, & O. Y. Kwong (Eds.), Natural language
processing (Vol. 3248, pp. 684 ? 693). Hainan Island, China.
Miyao, Y., & Tsujii, J. (2002). Maximum entropy estimation
for feature forests. In Proceedings of the Human Language
Technology Conference. San Diego, CA.
Moore, R. C., & Alshawi, H. (1992). Syntactic and semantic
processing. In H. Alshawi (Ed.), The Core Language Engine
(pp. 129 ? 148). Cambridge, MA: MIT Press.
Mu?ller, S., & Kasper, W. (2000). HPSG analysis of German.
In W. Wahlster (Ed.), Verbmobil. Foundations of speech-to-
speech translation (Artificial Intelligence ed., pp. 238 ? 253).
Berlin, Germany: Springer.
Oepen, S., & Carroll, J. (2002). Efficient parsing for
unification-based grammars. In S. Oepen, D. Flickinger,
J. Tsujii, & H. Uszkoreit (Eds.), Collaborative language en-
gineering. A case study in efficient grammar-based process-
ing (pp. 195 ? 225). Stanford, CA: CSLI Publications.
Oepen, S., Flickinger, D., Toutanova, K., & Manning, C. D.
(2004). LinGO Redwoods. A rich and dynamic treebank for
HPSG. Journal of Research on Language and Computation,
2(4), 575 ? 596.
Riezler, S., King, T. H., Kaplan, R. M., Crouch, R., Maxwell III,
J. T., & Johnson, M. (2002). Parsing the Wall Street Journal
using a Lexical-Functional Grammar and discriminative es-
timation techniques. In Proceedings of the 40th Meeting of
the Association for Computational Linguistics. Philadelphia,
PA.
Toutanova, K., Manning, C. D., Flickinger, D., & Oepen, S.
(2005). Stochastic HPSG parse selection using the Red-
woods corpus. Journal of Research on Language and Com-
putation, 3(1), 83 ? 105.
59
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 31?36,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Hybrid Multilingual Parsing with HPSG for SRL
Yi Zhang
Language Technology
DFKI GmbH, Germany
yzhang@coli.uni-sb.de
Rui Wang
Computational Linguistics
Saarland University, Germany
rwang@coli.uni-sb.de
Stephan Oepen
Informatics
University of Oslo, Norway
oe@ifi.uio.no
Abstract
In this paper we present our syntactic and se-
mantic dependency parsing system submitted
to both the closed and open challenges of the
CoNLL 2009 Shared Task. The system ex-
tends the system of Zhang, Wang, & Uszko-
reit (2008) in the multilingual direction, and
achieves 76.49 average macro F1 Score on the
closed joint task. Substantial improvements
to the open SRL task have been observed that
are attributed to the HPSG parses with hand-
crafted grammars. ?
1 Introduction
The CoNLL 2009 shared task (Hajic? et al, 2009)
continues the exploration on learning syntactic and
semantic structures based on dependency notations
in previous year?s shared task. The new addition
to this year?s shared task is the extension to mul-
tiple languages. Being one of the leading compe-
titions in the field, the shared task received sub-
missions from systems built on top of the state-
of-the-art data-driven dependency parsing and se-
mantic role labeling systems. Although it was
originally designed as a task for machine learning
approaches, CoNLL shared tasks also feature an
?open? track since 2008, which encourages the use
of extra linguistic resources to further improve the
?We are indebted to our DELPH-IN colleagues, specifi-
cally Peter Adolphs, Francis Bond, Berthold Crysmann, and
Montserrat Marimon for numerous hours of support in adapt-
ing their grammars and the PET software to parsing the CoNLL
data sets. The first author thanks the German Excellence Clus-
ter of Multimodal Computing and Interaction for the support
of the work. The second author is funded by the PIRE PhD
scholarship program. Participation of the third author in this
work was supported by the University of Oslo, as part of its re-
search partnership with the Center for the Study of Language
and Information at Stanford University. Our deep parsing ex-
perimentation was executed on the TITAN HPC facilities at the
University of Oslo.
performance. This makes the task a nice testbed for
the cross-fertilization of various language process-
ing techniques.
As an example of such work, Zhang et al (2008)
have shown in the past that deep linguistic parsing
outputs can be integrated to help improve the per-
formance of the English semantic role labeling task.
But several questions remain unanswered. First, the
integration only experimented with the semantic role
labeling part of the task. It is not clear whether
syntactic dependency parsing can also benefit from
grammar-based parsing results. Second, the English
grammar used to achieve the improvement is one of
the largest and most mature hand-crafted linguistic
grammars. It is not clear whether similar improve-
ments can be achieved with less developed gram-
mars. More specifically, the lack of coverage of
hand-crafted linguistic grammars is a major concern.
On the other hand, the CoNLL task is also a good
opportunity for the deep processing community to
(re-)evaluate their resources and software.
2 System Architecture
The overall system architecture is shown in Figure 1.
It is similar to the architecture used by Zhang et al
(2008). Three major components were involved.
The HPSG parsing component utilizes several hand-
crafted grammars for deep linguistic parsing. The
outputs of deep parsings are passed to the syntactic
dependency parser and semantic role labeler. The
syntactic parsing component is composed of a mod-
ified MST parser which accepts HPSG parsing re-
sults as extra features. The semantic role labeler is
comprised of a pipeline of 4 sub-components (pred-
icate identification is not necessary in this year?s
task). Comparing to Zhang et al (2008), this archi-
tecture simplified the syntactic component, and puts
more focus on the integration of deep parsing out-
puts. While Zhang et al (2008) only used seman-
31
SyntacticDependencyParsing
MST Parser
ERG
GG
JaCY
SRG
[incr tsdb()]
PET
HPSG Parsing
Argument Identification
Argument Classification
Predicate Classification
SemanticRoleLabeling
MRS
HPSG Syn.
Syn.Dep.
Figure 1: Joint system architecture.
tic features from HPSG parsing in the SRL task, we
added extra syntactic features from deep parsing to
help both tasks.
3 HPSG Parsing for the CoNLL Data
DELPH-IN (Deep Linguistic Processing with
HPSG) is a repository of open-source software and
linguistic resources for so-called ?deep? grammat-
ical analysis.1 The grammars are rooted in rela-
tively detailed, hand-coded linguistic knowledge?
including lexical argument structure and the linking
of syntactic functions to thematic arguments?and
are intended as general-purpose resources, applica-
ble to both parsing and generation. Semantics in
DELPH-IN is cast in the Minimal Recursion Seman-
tics framework (MRS; Copestake, Flickinger, Pol-
lard, & Sag, 2005), essentially predicate ? argument
structures with provision for underspecified scopal
relations. For the 2009 ?open? task, we used the
DELPH-IN grammars for English (ERG; Flickinger,
2000), German (GG; Crysmann, 2005), Japanese
(JaCY; Siegel & Bender, 2002), and Spanish (SRG;
Marimon, Bel, & Seghezzi, 2007). The grammars
vary in their stage of development: the ERG com-
prises some 15 years of continuous development,
whereas work on the SRG only started about five
years ago, with GG and JaCY ranging somewhere
inbetween.
3.1 Overall Setup
We applied the DELPH-IN grammars to the CoNLL
data using the PET parser (Callmeier, 2002) running
1See http://www.delph-in.net for background.
it through the [incr tsdb()] environment (Oepen &
Carroll, 2000), for parallelization and distribution.
Also, [incr tsdb()] provides facilities for (re-)training
the MaxEnt parse selection models that PET uses for
disambiguation.
The two main challenges in applying DELPH-
IN resources to parsing CoNLL data were (a) mis-
matches in basic assumptions, specifically tokeniza-
tion and the inventory of PoS tags provided as part of
the input, and (b) the need to adapt the resources for
new domains and genres?in particular in terms of
parse disambiguation?as the English and Spanish
grammars at least had not been previously applied
to the corpora used in the CoNLL shared task.
The importance of the first of these two aspects
is often underestimated. A detailed computational
grammar, inevitably, comes with its own assump-
tions about tokenization?the ERG, for example, re-
jects the conventional assumptions underlying the
PTB (and derived tools). It opts for an analysis of
punctuation akin to affixation (rather than as stand-
alone tokens), does not break up contracted negated
auxiliaries, and splits hyphenated words like ill-
advised into two tokens (the hyphen being part of
the first component). Thus, a string like Don?t you!
in the CoNLL data is tokenized as the four-element
sequence ?do, n?t, you, !?,2 whereas the ERG analy-
sis has only two leaf nodes: ?don?t, you!?.
Fortunately, the DELPH-IN toolchain recently
incorporated a mechanism called chart mapping
(Adolphs et al, 2008), which allows one to map
flexibly from ?external? input to grammar-internal
assumptions, while keeping track of external token
identities and their contributions to the final analysis.
The February 2009 release of the ERG already had
this machinery in place (with the goal of supporting
extant, PTB-trained PoS taggers in pre-processing
input to the deep parser), and we found that only a
tiny number of additional chart mapping rules was
required to ?fix up? CoNLL-specific deviations from
the PTB tradition. With the help of the original de-
velopers, we created new chart mapping configura-
tions for the German and Japanese grammars (with
17 and 16 such accomodation rules, respectively) in
a similar spirit. All four DELPH-IN grammars in-
2Note that the implied analogy to a non-contracted variant is
linguistically mis-leading, as ?Do not you! is ungrammatical.
32
clude an account of unknown words, based on un-
derspecified ?generic? lexical entries that are acti-
vated from PoS information.
The Japenese case was interesting, in that
the grammar assumes a different pre-processor
(ChaSen, rather than Juman), such that not only to-
ken boundaries but also PoS tags and morphological
features had to be mapped. From our limited ex-
perience to date, we found the chart mapping ap-
proach adequate in accomodating such discrepan-
cies, and the addition of this extra layer of input
processing gave substantial gains in parser cover-
age (see below). For the Spanish data, on the other
hand, we found it impossible to make effective use
of the PoS and morphological information in the
CoNLL data, due to more fundamental discrepan-
cies (e.g. the treatment of enclitics and multi-word
expressions).
3.2 Retraining Disambiguation Models
The ERG includes a domain-specific parse selection
model (for tourism instructions); GG only a stub
model trained on a handful of test sentences. For
use on the CoNLL data, thus, we had to train new
parse selections models, better adapted to the shared
task corpora. Disambiguation in PET is realized by
conditional MaxEnt models (Toutanova, Manning,
Flickinger, & Oepen, 2005), usually trained on full
HPSG treebanks. Lacking this kind of training ma-
terial, we utilized the CoNLL dependency informa-
tion instead, by defining an unlabeled dependency
accuracy (DA) metric for HPSG analyses, essen-
tially quantifying the degree of overlap in head ?
dependent relations against the CoNLL annotations.
Calculating DA for HPSG trees is similar to the
procedure commonly used for extracting bi-lexical
dependencies from phrase structure trees, in a sense
even simpler as HPSG analyses fully determine
headeness. Taking into account the technical com-
plication of token-level mismatches, our DA met-
ric loosely corresponds to the unlabeled attachment
score. To train CoNLL-specific parse selection mod-
els, we parsed the development sections in 500-best
mode (using the existing models) and then mechani-
cally ?annotated? the HPSG analyses with maximum
DA as preferred, all others as dis-preferred. In other
words, this procedure constructs a ?binarized? em-
pirical distribution where estimation of log-linear
Grammar Coverage Time
ERG 80.4% 10.06 s
GG 28.6% 3.41 s
JaCY 42.7% 2.13 s
SRG 7.5% 0.80 s
Table 1: Performance of the DELPH-IN grammars.
model parameters amounts to adjusting conditional
probabilities towards higher DA values.3
Using the [incr tsdb()] MaxEnt experimentation
facilities, we trained new parse selection models
for English and German, using the first 16,000 sen-
tences of the English training data and the full Ger-
man training corpus; seeing that only inputs that (a)
parse successfully and (b) have multiple readings,
with distinct DA values are relevant to this step, the
final models reflect close to 13,000 sentences for En-
glish, and a little more than 4,000 items for German.
Much like in the SRL component, these experiments
are carried out with the TADM software, using ten-
fold cross-validation and exact match ranking accu-
racy (against the binarized training distribution) to
optimize estimation hyper-parameters
3.3 Deep Parsing Features
HPSG parsing coverage and average cpu time per
input for the four languages with DELPH-IN gram-
mars are summarized in Table 1. The PoS-based
unknown word mechanism was active for all gram-
mars but no other robustness measures (which tend
to lower the quality of results) were used, i.e. only
complete spanning HPSG analyses were accepted.
Parse times are for 1-best parsing, using selective
unpacking (Zhang, Oepen, & Carroll, 2007).
HPSG parsing outputs are available in several dif-
ferent forms. We investigated two types of struc-
tures: syntactic derivations and MRS meaningrep-
resentations. Representative features were extracted
from both structures and selectively used in the sta-
tistical syntactic dependency parsing and semantic
role labeling modules for the ?open? challenge.
3We also experimented with using DA scores directly as em-
pirical probabilities in the training distribution (or some func-
tion of DA, to make it fall off more sharply), but none of
these methods seemed to further improve parse selection per-
formance.
33
Deep Semantic Features Similar to Zhang et al
(2008), we extract a set of features from the seman-
tic outputs (MRS) of the HPSG parses. These fea-
tures represent the basic predicate-argument struc-
ture, and provides a simplified semantic view on the
target sentence.
Deep Syntactic Dependency Features A HPSG
derivation is a tree structure. The internal nodes are
labeled with identifiers of grammar rules, and leaves
with lexical entries. The derivation tree provides
complete information about the actual HPSG anal-
ysis, and can be used together with the grammar to
reproduce complete feature structure and/or MRS.
Given that the shared task adopts dependency rep-
resentation, we further map the derivation trees into
token-token dependencies, labeled by corresponding
HPSG rules, by defining a set of head-finding rules
for each grammar. This dependency structure is dif-
ferent from the dependencies in CoNLL dataset, and
provides an alternative HPSG view on the sentences.
We refer to this structure as the dependency back-
bone (DB) of the HPSG anaylsis. A set of features
were extracted from the deep syntactic dependency
structures. This includes: i) the POS of the DB par-
ent from the predicate and/or argument; ii) DB la-
bel of the argument to its parent (only for AI/AC);
iii) labeled path from predicate to argument in DB
(only for AI/AC); iv) POSes of the predicate?s DB
dependents
4 Syntactic Dependency Parsing
For the syntactic dependency parsing, we use the
MST Parser (McDonald et al, 2005), which is a
graph-based approach. The best parse tree is ac-
quired by searching for a spanning tree which max-
imizes the score on either a partially or a fully con-
nected graph with all words in the sentence as nodes
(Eisner, 1996; McDonald et al, 2005). Based on our
experience last year, we use the second order setting
of the parser, which includes features over pairs of
adjacent edges as well as features over single edges
in the graph. For the projective or non-projective
setting, we compare the results on the development
datasets of different languages. According to the
parser performance, we decide to use non-projective
parsing for German, Japanese, and Czech, and use
projective parsing for the rest.
For the Closed Challenge, we first consider
whether to use the morphological features. We find
that except for Czech, parser performs better with-
out morphological features on other languages (En-
glish and Chinese have no morphological features).
As for the other features (i.e. lemma and pos) given
by the data sets, we also compare the gold standard
features and P-columns. For all languages, the per-
formance decreases in the following order: training
with gold standard features and evaluating with the
gold standard features, training with P-columns and
evaluating with P-columns, training with gold stan-
dard features and testing with P-columns. Conse-
quently, in the final submission, we take the second
combination.
The goal of the Open Challenge is to see whether
using external resources can be helpful for the pars-
ing performance. As we mentioned before, our
deep parser gives us both the syntactic analysis of
the input sentences using the HPSG formalism and
also the semantic analysis using MRS as the repre-
sentation. However, for the syntactic dependency
parsing, we only extract features from the syntac-
tic HPSG analyses and feed them into the MST
Parser. Although, when parsing with gold standard
lemma and POS features, our open system outper-
forms the closed system on out-domain tests (for En-
glish), when parsing with P-columns there is no sub-
stantial improvement observed after using the HPSG
features. Therefore, we did not include it in the final
submission.
5 Semantic Role Labeling
The semantic role labeling component used in the
submitted system is similar to the one described
by Zhang et al (2008). Since predicates are indi-
cated in the data, the predicate identification mod-
ule is removed from this year?s system. Argument
identification, argument classification and predicate
classification are the three sub-components in the
pipeline. All of them are MaxEnt-based classifiers.
For parameter estimation, we use the open source
TADM system (Malouf, 2002).
The active features used in various steps of SRL
are fine tuned separately for different languages us-
ing development datasets. The significance of fea-
ture types varies across languages and datasets.
34
ca zh cs en de ja es
SY
N Closed 82.67 73.63 75.58 87.90 84.57 91.47 82.69
ood - - 71.29 81.50 75.06 - -
SR
L
Closed 67.34 73.20 78.28 77.85 62.95 64.71 67.81
ood - - 77.78 67.07 54.87 - -
Open - - - 78.13 (?0.28) 64.31 (?1.36) 65.95 (?1.24) 68.24 (?0.43)
ood - - - 68.11 (?1.04) 58.42 (?3.55) - -
Table 2: Summary of System Performance on Multiple Languages
In the open challenge, two groups of extra fea-
tures from HPSG parsing outputs, as described in
Section 3.3, were used on languages for which we
have HPSG grammars, that is English, German,
Japanese, and Spanish.
6 Result Analysis
The evaluation results of the submitted system are
summarized in Table 2. The overall ranking of
the system is #7 in the closed challenge, and #2
in the open challenge. While the system achieves
mediocre performance, the clear performance dif-
ference between the closed and open challenges of
the semantic role labeler indicates a substantial gain
from the integration of HPSG parsing outputs. The
most interesting observation is that even with gram-
mars which only achieve very limited coverage, no-
ticeable SRL improvements are obtained. Con-
firming the observation of Zhang et al (2008), the
gain with HPSG features is more significant on out-
domain tests, this time on German as well.
The training of the syntactic parsing models for
all seven languages with MST parser takes about
100 CPU hours with 10 iterations. The dependency
parsing takes 6 ? 7 CPU hours. The training and test-
ing of the semantic role labeler is much more effi-
cient, thanks to the use of MaxEnt models and the
efficient parameter estimation software. The train-
ing of all SRL models for 7 languages takes about 3
CPU hours in total. The total time for semantic role
labeling on test datasets is less than 1 hour.
Figure 2 shows the learning curve of the syntactic
parser and semantic role labeler on the Czech and
English datasets. While most of the systems con-
tinue to improve when trained on larger datasets, an
exception was observed with the Czech dataset on
the out-domain test for syntactic accuracy. In most
of the cases, with the increase of training data, the
out-domain test performance of the syntactic parser
and semantic role labeler improves slowly relative
to the in-domain test. For the English dataset, the
SRL learning curve climbs more quickly than those
of syntactic parsers. This is largely due to the fact
that the semantic role annotation is sparser than the
syntactic dependencies. On the Czech dataset which
has dense semantic annotation, this effect is not ob-
served.
7 Conclusion
In this paper, we described our syntactic parsing and
semantic role labeling system participated in both
closed and open challenge of the (Joint) CoNLL
2009 Shared Task. Four hand-written HPSG gram-
mars of a variety of scale have been applied to parse
the datasets, and the outcomes were integrated as
features into the semantic role labeler of the sys-
tem. The results clearly show that the integration of
HPSG parsing results in the semantic role labeling
task brings substantial performance improvement.
The conclusion of Zhang et al (2008) has been re-
confirmed on multiple languages for which we hand-
built HPSG grammars exist, even where grammati-
cal coverage is low. Also, the gain is more signifi-
cant on out-of-domain tests, indicating that the hy-
brid system is more robust to cross-domain varia-
tion.
References
Adolphs, P., Oepen, S., Callmeier, U., Crysmann, B.,
Flickinger, D., & Kiefer, B. (2008). Some fine points
of hybrid natural language parsing. In Proceedings
of the 6th International Conference on Language Re-
sources and Evaluation. Marrakech, Morocco.
Burchardt, A., Erk, K., Frank, A., Kowalski, A., Pado?, S.,
& Pinkal, M. (2006). The SALSA corpus: a German
corpus resource for lexical semantics. In Proceedings
of the 4th International Conference on Language Re-
sources and Evaluation. Genoa, Italy.
35
 60
 65
 70
 75
 80
 85
 90
 0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Ac
cu
ra
cy
 (%
)
Training Corpus Size (English)
Syn
SRL
Syn-ood
SRL-ood
 69
 70
 71
 72
 73
 74
 75
 76
 77
 78
 79
 0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Ac
cu
ra
cy
 (%
)
Training Corpus Size (Czech)
Syn
SRL
Syn-ood
SRL-ood
Figure 2: Learning curves of syntactic dependency parser and semantic role labeler on Czech and English datasets
Callmeier, U. (2002). Preprocessing and encoding tech-
niques in PET. In S. Oepen, D. Flickinger, J. Tsujii, &
H. Uszkoreit (Eds.), Collaborative language engineer-
ing. A case study in efficient grammar-based process-
ing. Stanford, CA: CSLI Publications.
Copestake, A., Flickinger, D., Pollard, C., & Sag, I. A.
(2005). Minimal Recursion Semantics. An introduc-
tion. Journal of Research on Language and Computa-
tion, 3(4), 281 ? 332.
Crysmann, B. (2005). Relative clause extraposition
in German. An efficient and portable implementation.
Research on Language and Computation, 3(1), 61 ?
82.
Flickinger, D. (2000). On building a more efficient gram-
mar by exploiting types. Natural Language Engineer-
ing, 6 (1), 15 ? 28.
Hajic?, J., Ciaramita, M., Johansson, R., Kawahara, D.,
Mart??, M. A., Ma`rquez, L., Meyers, A., Nivre, J., Pado?,
S., S?te?pa?nek, J., Stran?a?k, P., Surdeanu, M., Xue, N.,
& Zhang, Y. (2009). The CoNLL-2009 shared task:
Syntactic and semantic dependencies in multiple lan-
guages. In Proceedings of the 13th Conference on
Computational Natural Language Learning. Boulder,
CO, USA.
Hajic?, J., Panevova?, J., Hajic?ova?, E., Sgall, P., Pa-
jas, P., S?te?pa?nek, J., Havelka, J., Mikulova?, M., &
Z?abokrtsky?, Z. (2006). Prague Dependency Treebank
2.0 (Nos. Cat. No. LDC2006T01, ISBN 1-58563-370-
4). Philadelphia, PA, USA: Linguistic Data Consor-
tium.
Kawahara, D., Kurohashi, S., & Hasida, K. (2002). Con-
struction of a Japanese relevance-tagged corpus. In
Proceedings of the 3rd International Conference on
Language Resources and Evaluation (pp. 2008?2013).
Las Palmas, Canary Islands.
Malouf, R. (2002). A comparison of algorithms for max-
imum entropy parameter estimation. In Proceedings
of the 6th conferencde on natural language learning
(CoNLL 2002) (pp. 49?55). Taipei, Taiwan.
Marimon, M., Bel, N., & Seghezzi, N. (2007). Test suite
construction for a Spanish grammar. In T. H. King &
E. M. Bender (Eds.), Proceedings of the Grammar En-
gineering Across Frameworks workshop (p. 250-264).
Stanford, CA: CSLI Publications.
Oepen, S., & Carroll, J. (2000). Performance profiling for
parser engineering. Natural Language Engineering, 6
(1), 81 ? 97.
Palmer, M., Kingsbury, P., & Gildea, D. (2005). The
Proposition Bank: An Annotated Corpus of Semantic
Roles. Computational Linguistics, 31(1), 71?106.
Palmer, M., & Xue, N. (2009). Adding semantic roles
to the Chinese Treebank. Natural Language Engineer-
ing, 15(1), 143?172.
Siegel, M., & Bender, E. M. (2002). Efficient deep pro-
cessing of Japanese. In Proceedings of the 3rd work-
shop on asian language resources and international
standardization at the 19th international conference
on computational linguistics. Taipei, Taiwan.
Surdeanu, M., Johansson, R., Meyers, A., Ma`rquez, L.,
& Nivre, J. (2008). The CoNLL-2008 shared task on
joint parsing of syntactic and semantic dependencies.
In Proceedings of the 12th Conference on Computa-
tional Natural Language Learning. Manchester, UK.
Taule?, M., Mart??, M. A., & Recasens, M. (2008). An-
Cora: Multilevel Annotated Corpora for Catalan and
Spanish. In Proceedings of the 6th International Con-
ference on Language Resources and Evaluation. Mar-
rakesh, Morroco.
Toutanova, K., Manning, C. D., Flickinger, D., & Oepen,
S. (2005). Stochastic HPSG parse selection using the
Redwoods corpus. Journal of Research on Language
and Computation, 3(1), 83 ? 105.
Zhang, Y., Oepen, S., & Carroll, J. (2007). Efficiency in
unification-based n-best parsing. In Proceedings of the
10th International Conference on Parsing Technolo-
gies (pp. 48 ? 59). Prague, Czech Republic.
Zhang, Y., Wang, R., & Uszkoreit, H. (2008). Hy-
brid Learning of Dependency Structures from Hetero-
geneous Linguistic Resources. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning (CoNLL 2008) (pp. 198?202). Manch-
ester, UK.
36
Using an open-source unification-based system for CL/NLP teaching
Ann Copestake
Computer Laboratory
University of Cambridge
Cambridge, UK
aac@cl.cam.ac.uk
John Carroll
Cognitive and Computing Sciences
University of Sussex
Falmer, Brighton, UK
johnca@cogs.susx.ac.uk
Dan Flickinger
CSLI, Stanford University and
YY Software
Ventura Hall,
Stanford, USA
danf@csli.stanford.edu
Robert Malouf
Alfa Informatica,
University of Groningen,
Postbus 716, 9700 AS Groningen,
The Netherlands
malouf@let.rug.nl
Stephan Oepen
YY Software and
CSLI, Stanford University
110 Pioneer Way
Mountain View, USA
oe@yy.com
Abstract
We demonstrate the open-source LKB
system which has been used to teach the
fundamentals of constraint-based gram-
mar development to several groups of
students.
1 Overview of the LKB system
The LKB system is a grammar development
environment that is distributed as part of the
open source LinGO tools (http://www-
csli.stanford.edu/?aac/lkb.html
and http://lingo.stanford.edu, see
also Copestake and Flickinger, 2000). It is an
open-source grammar development environment
implemented in Common Lisp, distributed not
only as source but also as a standalone application
that can be run on Linux, Solaris and Windows
(see the website for specific requirements). It
will also run under Macintosh Common Lisp,
but for this a license is required. The LKB in-
cludes a parser, generator, support for large-scale
inheritance hierarchies (including the use of
defaults), various tools for manipulating semantic
representations, a rich set of graphical tools
for analyzing and debugging grammars, and
extensive on-line documentation. Grammars of
all sizes have been written using the LKB, for
several languages, mostly within the linguistic
frameworks of Categorial Grammar and Head-
Driven Phrase Structure Grammar. The LKB
system was initially developed in 1991, but has
gone through multiple versions since then. It
is in active use by a considerable number of
researchers worldwide. An introductory book
on implementing grammars in typed feature
structure formalisms using the LKB is near
completion (Copestake, in preparation).
2 Demo outline
Although the LKB has been successfully used for
large-scale grammar development, this demon-
stration will concentrate on its use with relatively
small scale teaching grammars, of a type which
can be developed by students in practical exer-
cises. We will show an English grammar frag-
ment which is linked to a textbook on formal syn-
tax (Sag and Wasow, 1999) to illustrate how the
system may be used in conjunction with more tra-
ditional materials in a relatively linguistically ori-
ented course. We will demonstrate the tools for
analyzing parses and for debugging and also dis-
cuss the way that parse selection mechanisms can
be incorporated in the system. If time permits, we
will show how semantic analyses produced with a
somewhat more complex grammar can be linked
up to a theorem prover and also exploited in se-
mantic transfer for Machine Translation. Exer-
cises where the grammar is part of a larger system
are generally appropriate for advanced courses or
for NLP application courses.
The screen dump in the figure is from a session
working with a grammar fragment for Esperanto.
This shares its basic types and rules with the
English textbook grammar fragment mentioned
above. The windows shown are:
1. The LKB Top interaction window: main
  
 
 
 
Figure 1: Screen dump of the LKB system
menus plus feedback and error messages
2. Type hierarchy window (fragment): the
more general types are on the left. Nodes in
the hierarchy have menus that provide more
information about the types, such as their as-
sociated constraints.
3. Type constraint for the type intrans-verb:
again nodes are clickable for further infor-
mation.
4. Parse tree for La knabo dormas (the boy
sleeps): a larger display for parse trees is
also available, but this scale is useful for
summary information. Menus associated
with trees allow for display of associated se-
mantic information if any is included in the
grammar and for generation. Here the dis-
play shows inflectional rules as well as nor-
mal syntactic rules: hence the VP node un-
der dormas, which corresponds to the stem.
5. In the middle is an emacs window displaying
the source file for the lexicon associated with
this grammar.1 It shows the entry for the lex-
1(We generally use emacs as an editor when teaching,
eme dorm, which, like most lexical entries in
this grammar, just specifies a spelling and a
type (here intrans-verb).
6. Part of the parse chart corresponding to the
tree is shown in the bottom window: nodes
which have knabo as a descendant are high-
lighted. Again, these nodes are active: one
very useful facility associated with them is a
unification checker which allows the gram-
mar writer to establish why a rule did not
apply to a phrase or phrases.
3 Use of the LKB in teaching
Teaching uses of the LKB have included under-
graduate and graduate courses on formal syntax
and on computational linguistics at several sites,
grammar engineering courses at two ESSLLI
summer schools, and numerous student projects
at undergraduate, masters and doctoral levels. An
advantage of the LKB is that students learn to use
a system which is sufficiently heavy duty for more
advanced work, up to the scale at least of research
although this causes some overhead, especially for students
who are only used to word processing programs.
prototypes. This provides them with a good plat-
form on which to build for further research. Feed-
back from the courses we have taught has mostly
been very positive, but we have found a ratio of
six students to one instructor (or teaching assis-
tant) to be the maximum that is workable. One
major reason is that debugging students? gram-
mars and teaching debugging techniques is time-
consuming.
When teaching an introductory course with the
LKB, we start the students off with a very sim-
ple grammar, which they are asked to expand
in specific ways. We introduce various addi-
tional techniques and formal devices (such as in-
flectional and lexical rules, defaults, difference
lists and gaps) gradually during a course. Mate-
rial from our ESSLLI courses, including starting
grammars, exercises and solutions is distributed
via the website. Several other small grammars
developed by students are also distributed as part
of the LKB system and we would welcome fur-
ther contributions. We are hoping to facilitate this
by making it easier for people outside the LinGO
group to add and modify grammars.
Several graduate students have used versions
of the LKB system as part of their thesis work,
for diverse projects including machine transla-
tion and grammar learning. It has been used
in the development of several large grammars,
especially the LinGO English Resource Gram-
mar (ERG), which is itself open-source. Re-
search applications for the ERG include spoken
language machine translation in Verbmobil, gen-
eration for a speech prosthesis, and automated
email response, under development for commer-
cial use. The LKB/ERG combination can be used
by researchers who require a grammar which pro-
vides a detailed semantic analysis and reason-
ably broad coverage, for instance for experiments
on dialogue. The LKB has also been used as
a grammar preprocessor to facilitate experiments
on efficiency using the ERG with other systems
(Flickinger et al 2000).
4 Comparison with other work
There is a long history of the use of fea-
ture structure based systems in teaching, dat-
ing back at least to PATR (Shieber, 1986:
see http://www.ling.gu.se/?li/). The
Alvey Natural Language Tools (Briscoe et al
1987) have been used for teaching at several uni-
versities: Briscoe and Grover developed an ex-
tensive set of teaching examples and exercises,
which is however unpublished. Versions of the
SRI Core Language Engine (Alshawi, 1992) and
of the XTAG grammar (XTAG group, 1995) and
parser have also been used for teaching. Besides
the LKB, typed feature structure environments
have been used at many universities, though un-
like the systems cited above, most have only been
used with small grammars and may not scale
up. Hands on courses using various systems have
been run at many recent summer schools includ-
ing ESSLLI 99 (using the Xerox XLE, see Butt
et al 1999) and ESSLLI 97 and the 1999 LSA
summer school (both using ConTroll, see Hin-
richs and Meurers, 1999). Very little seems to
have been formally published describing expe-
riences in teaching with grammar development
environments, though Bouma (1999) describes
material for teaching a computational linguistics
course that includes exercises using the Hdrug
unification-based enviroment to extend a gram-
mar.
Despite this rich variety of tools, we believe
that the LKB system has a combination of fea-
tures which make it distinctive and give it a useful
niche in teaching. The most important points are
that its availability as open source, combined with
scale and efficiency, allow advanced projects to be
supported as well as introductory courses. As far
as we are aware, it is the only system freely avail-
able with a broad coverage grammar that sup-
ports semantic interpretation and generation. Es-
pecially for more linguistically oriented courses,
the link to the Sag and Wasow textbook is also
important. Similar grammars could be developed
for other systems, but would be less directly com-
parable to the textbook since this assumes a de-
fault formalism which so far is only implemented
in the LKB.
On the other hand, the LKB is not a suitable ba-
sis for a course that involves the students learning
to implement a unifier, parser and so on. The sys-
tem is quite complex (about 120 files and 40,000
lines of Lisp code) and though the vast majority
of this is concerned with non-core functionality,
such as the graphical interfaces, it is still some-
what daunting. This seems an inevitable trade-
off of having a system powerful enough for real
applications (see Bouma (1999) for related dis-
cussion). It is questionable whether the LKB is
entirely satisfactory as a student?s first computa-
tional grammar system, although we have used it
with students who have no prior experience of this
sort: ideally we would suggest starting off with
brief exercises with a pure context-free grammar
to explain the concepts of well-formedness, re-
cursion and so on. We also wouldn?t necessar-
ily advocate using the LKB as a core component
of a first course on formal syntax for linguistic
students, since the specifics of dealing with an
implementation may interfere with understanding
of basic concepts, though it is suitable as a sup-
plement to an initial course or as the basis for a
slightly more advanced course.
We think there is considerable potential for
building materials for courses that allow students
to work with realistic but transparent applications
using the LKB and a large grammar as a compo-
nent. Developing such materials is clearly nec-
essary in order to give students useful practical
experience. It is however very time-consuming,
and most probably will have to be undertaken as
part of a cooperative, open-source development
involving people from several different institu-
tions.
Acknowledgements
This research was partially supported by the Na-
tional Science Foundation, grant number IRI-
9612682. The current versions of the English
grammars associated with the Sag and Wasow
textbook were largely developed by Christopher
Callison-Burch while he was an undergraduate at
Stanford.
References
Alshawi, Hiyan (ed). [1992] The Core Language
Engine, MIT Press, Cambridge, MA.
Bouma, Gosse. [1999] ?A modern computa-
tional linguistics course using Dutch.? In Frank
van Eynde and Ineke Schuurman, editors, CLIN
1998, Papers from the Ninth CLIN Meeting, Am-
sterdam. Rodopi Press.
Briscoe, Ted, Claire Grover, Bran Boguraev
and John Carroll. [1987] ?A formalism and en-
vironment for the development of a large gram-
mar of English?, Proceedings of the 10th Interna-
tional Joint Conference on Artificial Intelligence
(IJCAI-87), Milan, Italy, 703?708.
Butt, Miriam, Anette Frank and Jonas Kuhn.
[1999] ?Development of large scale LFG gram-
mars ? Linguistics, Engineering and Resources?,
http://www.xrce.xerox.com/people/
frank/esslli99-hp/index.html
Copestake, Ann. [in preparation] Implementing
typed feature structure grammars, CSLI Publica-
tions, Stanford.
Copestake, Ann and Dan Flickinger. [2000]
?An open-source grammar development environ-
ment and broad-coverage English grammar us-
ing HPSG?, Second conference on Language Re-
sources and Evaluation (LREC-2000), Athens,
Greece.
Flickinger, Daniel, Stephan Oepen, Hans
Uszkoreit and Jun?ichi Tsujii. [2000] Journal of
Natural Language Engineering. Special Issue on
Efficient Processing with HPSG: Methods, Sys-
tems, Evaluation, 6(1).
Hinrichs, Erhard and Detmar Meurers [1999]
?Grammar Development in Constraint-Based
Formalisms?,
http://www.ling.ohio-state.edu/
?dm/lehre/lsa99/material.html,
see also http://www.sfs.nphil.uni-
tuebingen.de/controll/
Sag, Ivan, and Tom Wasow [1999] Syntactic
Theory: An Introduction, CSLI Publications.
Shieber, Stuart [1986] An Introduction to
Unification-based Approaches to Grammar,
CSLI Publications.
The XTAG Research Group [1995]. ?A Lex-
icalized Tree Adjoining Grammar for English?
IRCS Report 95-03, University of Pennsylvania?
Proceedings of the 43rd Annual Meeting of the ACL, pages 330?337,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
High Precision Treebanking
? Blazing Useful Trees Using POS Information ?
Takaaki Tanaka,? Francis Bond,? Stephan Oepen,? Sanae Fujita?
? {takaaki, bond, fujita}@cslab.kecl.ntt.co.jp
? oe@csli.stanford.edu
? NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation
? Universitetet i Oslo and CSLI, Stanford
Abstract
In this paper we present a quantitative
and qualitative analysis of annotation in
the Hinoki treebank of Japanese, and in-
vestigate a method of speeding annotation
by using part-of-speech tags. The Hinoki
treebank is a Redwoods-style treebank of
Japanese dictionary definition sentences.
5,000 sentences are annotated by three dif-
ferent annotators and the agreement evalu-
ated. An average agreement of 65.4% was
found using strict agreement, and 83.5%
using labeled precision. Exploiting POS
tags allowed the annotators to choose the
best parse with 19.5% fewer decisions.
1 Introduction
It is important for an annotated corpus that the mark-
up is both correct and, in cases where variant anal-
yses could be considered correct, consistent. Con-
siderable research in the field of word sense disam-
biguation has concentrated on showing that the an-
notation of word senses can be done correctly and
consistently, with the normal measure being inter-
annotator agreement (e.g. Kilgariff and Rosenzweig,
2000). Surprisingly, few such studies have been car-
ried out for syntactic annotation, with the notable
exceptions of Brants et al (2003, p 82) for the Ger-
man NeGra Corpus and Civit et al (2003) for the
Spanish Cast3LB corpus. Even such valuable and
widely used corpora as the Penn TreeBank have not
been verified in this way.
We are constructing the Hinoki treebank as part of
a larger project in cognitive and computational lin-
guistics ultimately aimed at natural language under-
standing (Bond et al, 2004). In order to build the ini-
tial syntactic and semantic models, we are treebank-
ing the dictionary definition sentences of the most
familiar 28,000 words of Japanese and building an
ontology from the results.
Arguably the most common method in building a
treebank still is manual annotation, annotators (often
linguistics students) marking up linguistic properties
of words and phrases. In some semi-automated tree-
bank efforts, annotators are aided by POS taggers or
phrase-level chunkers, which can propose mark-up
for manual confirmation, revision, or extension. As
computational grammars and parsers have increased
in coverage and accuracy, an alternate approach has
become feasible, in which utterances are parsed and
the annotator selects the best parse Carter (1997);
Oepen et al (2002) from the full analyses derived
by the grammar.
We adopted the latter approach. There were four
main reasons. The first was that we wanted to de-
velop a precise broad-coverage grammar in tandem
with the treebank, as part of our research into natu-
ral language understanding. Treebanking the output
of the parser allows us to immediately identify prob-
lems in the grammar, and improving the grammar
directly improves the quality of the treebank in a mu-
tually beneficial feedback loop (Oepen et al, 2004).
The second reason is that we wanted to annotate to a
high level of detail, marking not only dependency
and constituent structure but also detailed seman-
tic relations. By using a Japanese grammar (JACY:
Siegel and Bender, 2002) based on a monostratal
theory of grammar (HPSG: Pollard and Sag, 1994)
we could simultaneously annotate syntactic and se-
mantic structure without overburdening the annota-
330
tor. The third reason was that we expected the use
of the grammar to aid in enforcing consistency ?
at the very least all sentences annotated are guaran-
teed to have well-formed parses. The flip side to this
is that any sentences which the parser cannot parse
remain unannotated, at least unless we were to fall
back on full manual mark-up of their analyses. The
final reason was that the discriminants can be used
to update the treebank when the grammar changes,
so that the treebank can be improved along with the
grammar. This kind of dynamic, discriminant-based
treebanking was pioneered in the Redwoods tree-
bank of English (Oepen et al, 2002), so we refer
to it as Redwoods-style treebanking.
In the next section, we give some more details
about the Hinoki Treebank and the data used to eval-
uate the parser (? 2). This is followed by a brief dis-
cussion of treebanking using discriminants (? 3), and
an extension to seed the treebanking using existing
markup (? 4). Finally we present the results of our
evaluation (? 5), followed by some discussion and
outlines for future research.
2 The Hinoki Treebank
The Hinoki treebank currently consists of around
95,000 annotated dictionary definition and example
sentences. The dictionary is the Lexeed Semantic
Database of Japanese (Kasahara et al, 2004), which
consists of all words with a familiarity greater than
or equal to five on a scale of one to seven. This
gives 28,000 words, divided into 46,347 different
senses. Each sense has a definition sentence and
example sentence written using only these 28,000
familiar words (and some function words). Many
senses have more than one sentence in the definition:
there are 81,000 defining sentences in all.
The data used in our evaluation is taken from the
first sentence of the definitions of all words with a
familiarity greater than six (9,854 sentences). The
Japanese grammar JACY was extended until the
coverage was over 80% (Bond et al, 2004).
For evaluation of the treebanking we selected
5,000 of the sentences that could be parsed, and di-
vided them into five 1,000 sentence sets (A?E). Def-
inition sentences tend to vary widely in form de-
pending on the part of speech of the word being de-
fined ? each set was constructed with roughly the
same distribution of defined words, as well as hav-
ing roughly the same length (the average was 9.9,
ranging from 9.5?10.4).
A (simplified) example of an entry (Sense 2 of
  kflaten ?curtain: any barrier to communica-
tion or vision?), and a syntactic view of its parse are
given in Figure 1. There were 6 parses for this def-
inition sentence. The full parse is an HPSG sign,
containing both syntactic and semantic information.
A view of the semantic information is given in Fig-
ure 21.
UTTERANCE
NP
VP N
PP V
NP
DET N CASE-P
    	 	 	


       	 	 	
aru monogoto o kakusu mono
a certain stuff ACC hide thing
Curtain2: ?a thing that hides something?
Figure 1: Syntactic View of the Definition of  

2 kflaten ?curtain?
?h0, x2 {h0 : proposition(h5)
h1 : aru(e1, x1, u0) ?a certain?
h1 : monogoto(x1) ?stuff?
h2 : u def(x1, h1, h6)
h5 : kakusu(e2, x2, x1) ?hide?
h3 : mono(x2) ?thing?
h4 : u def(x2, h3, h7)}?
Figure 2: Semantic View of the Definition of  

2 kflaten ?curtain?
The semantic view shows some ambiguity has
been resolved that is not visible in the purely syn-
tactic view. In Japanese, relative clauses can have
gapped and non-gapped readings. In the gapped
reading (selected here),  mono ?thing? is the sub-
ject of  kakusu ?hide?. In the non-gapped read-
ing there is some unspecified relation between the
thing and the verb phrase. This is similar to the dif-
ference in the two readings of the day he knew in En-
glish: ?the day that he knew about? (gapped) vs ?the
day on which he knew (something)? (non-gapped).
1The semantic representation used is Minimal Recursion Se-
mantics (Copestake et al, Forthcoming). The figure shown here
hides some of the detail of the underspecified scope.
331
Such semantic ambiguity is resolved by selecting the
correct derivation tree that includes the applied rules
in building the tree, as shown in Figure 3. In the next
phase of the Hinoki project, we are concentrating on
acquiring an ontology from these semantic represen-
tations and using it to improve the parse selection
(Bond et al, 2004).
3 Treebanking Using Discriminants
Selection among analyses in our set-up is done
through a choice of elementary discriminants, basic
and mostly independent contrasts between parses.
These are (relatively) easy to judge by annotators.
The system selects features that distinguish between
different parses, and the annotator selects or rejects
the features until only one parse is left. In a small
number of cases, annotation may legitimately leave
more than one parse active (see below). The system
we used for treebanking was the [incr tsdb()] Red-
woods environment2 (Oepen et al, 2002). The num-
ber of decisions for each sentence is proportional
to the log of the number of parses. The number of
decisions required depends on the ambiguity of the
parses and the length of the input. For Hinoki, on av-
erage, the number of decisions presented to the an-
notator was 27.5. However, the average number of
decisions needed to disambiguate each sentence was
only 2.6, plus an additional decision to accept or re-
ject the selected parses3. In general, even a sentence
with 100 parses requires only around 5 decisions and
1,000 parses only around 7 decisions. A graph of
parse results versus number of decisions presented
and required is given in Figure 6.
The primary data stored in the treebank is the
derivation tree: the series of rules and lexical items
the parser used to construct the parse. This, along
with the grammar, can be combined to rebuild the
complete HPSG sign. The annotators task is to se-
lect the appropriate derivation tree or trees. The pos-
sible derivation trees for   2 kflaten ?curtain?
are shown in Figure 3. Nodes in the trees indicate
applied rules, simplified lexical types or words. We
2The [incr tsdb()] system, Japanese and English grammars
and the Redwoods treebank of English are available from the
Deep Linguistic Processing with HPSG Initiative (DELPH-IN:
http://www.delph-in.net/).
3This average is over all sentences, even non-ambiguous
ones, which only require a decision as to whether to accept or
reject.
will use it as an example to explain the annotation
process. Figure 3 also displays POS tag from a sep-
arate tagger, shown in typewriter font.4
This example has two major sources of ambiguity.
One is lexical: aru ?a certain/have/be? is ambigu-
ous between a reading as a determiner ?a certain?
(det-lex) and its use as a verb of possession ?have?
(aru-verb-lex). If it is a verb, this gives rise to
further structural ambiguity in the relative clause, as
discussed in Section 2. Reliable POS tags can thus
resolve some ambiguity, although not all.
Overall, this five-word sentence has 6 parses. The
annotator does not have to examine every tree but is
instead presented with a range of 9 discriminants, as
shown in Figure 4, each local to some segment of
the utterance (word or phrase) and thus presenting a
contrast that can be judged in isolation. Here the first
column shows deduced status of discriminants (typ-
ically toggling one discriminant will rule out oth-
ers), the second actual decisions, the third the dis-
criminating rule or lexical type, the fourth the con-
stituent spanned (with a marker showing segmenta-
tion of daughters, where it is unambiguous), and the
fifth the parse trees which include the rule or lexical
type.
D A
Rules /
Lexical Types
Subtrees /
Lexical items
Parse
Trees
? ? rel-cl-sbj-gap 	
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 53?56,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Re-Usable Tools for Precision Machine Translation?
Jan Tore L?nning? and Stephan Oepen??
?Universitetet i Oslo, Computer Science Institute, Boks 1080 Blindern; 0316 Oslo (Norway)
?Center for the Study of Language and Information, Stanford, CA 94305 (USA)
{ jtl@ifi.uio.no |oe@csli.stanford.edu}
Abstract
The LOGON MT demonstrator assembles
independently valuable general-purpose
NLP components into a machine trans-
lation pipeline that capitalizes on output
quality. The demonstrator embodies an in-
teresting combination of hand-built, sym-
bolic resources and stochastic processes.
1 Background
The LOGON projects aims at building an exper-
imental machine translation system from Norwe-
gian to English of texts in the domain of hiking in
the wilderness (Oepen et al, 2004). It is funded
within the Norwegian Research Council program
for building national infrastructure for language
technology (Fenstad et al, 2006). It is the goal
for the program as well as for the project to in-
clude various areas of language technology as well
as various methods, in particular symbolic and
empirical methods. Besides, the project aims at
reusing available resources and, in turn, producing
re-usable technology.
In spite of significant progress in statistical ap-
proaches to machine translation, we doubt the
long-term value of pure statistical (or data-driven)
approaches, both practically and scientifically. To
ensure grammaticality of outputs as well as fe-
licity of the translation both linguistic grammars
and deep semantic analysis are needed. The ar-
chitecture of the LOGON system hence consists of
a symbolic backbone system combined with vari-
ous stochastic components for ranking system hy-
potheses. In a nutshell, a central research question
in LOGON is to what degree state-of-the-art ?deep?
NLP resources can contribute towards a precision
MT system. We hope to engage the conference
audience in some reflection on this question by
means of the interactive presentation.
2 System Design
The backbone of the LOGON prototype imple-
ments a relatively conventional architecture, orga-
?This demonstration reects the work of a large group
of people whose contributions we gratefully acknowledge.
Please see ?http://www.emmtee.net? for background.
?h1,
{ h1:proposition m(h3),
h4:proper q(x5, h6, h7), h8:named(x5,?Bod??),
h9: populate v(e2, , x5), h9: densely r(e2) },
{ h3 =q h9, h6 =q h8 } ?
Figure 1: Simplied MRS representation for the utterance
?Bod? is densely populated.? The core of the structure is a bag
of elementary predications (EPs), using distinguished han-
dles (?hi? variables) and ?=q? (equal modulo quantier inser-
tion) constraints to underspecify scopal relations. Event- and
instance-type variables (?ej? and ?xk?, respectively) capture
semantic linking among EPs, where we assume a small inven-
tory of thematically bleached role labels (ARG0 ... ARGn).
These are abbreviated through order-coding in the example
above (see ? 2 below for details).
nized around in-depth grammatical analysis in the
source language (SL), semantic transfer of logical-
form meaning representations from the source into
the target language (TL), and full, grammar-based
TL tactical generation.
Minimal Recursion Semantics The three core
phases communicate in a uniform semantic in-
terface language, Minimal Recursion Semantics
(MRS; Copestake, Flickinger, Sag, & Pollard,
1999). Broadly speaking, MRS is a flat, event-
based (neo-Davidsonian) framework for computa-
tional semantics. The abstraction from SL and TL
surface properties enforced in our semantic trans-
fer approach facilitates a novel combination of di-
verse grammatical frameworks, viz. LFG for Nor-
wegian analysis and HPSG for English generation.
While an in-depth introduction to MRS (for MT)
is beyond the scope of this project note, Figure 1
presents a simplified example semantics.
Norwegian Analysis Syntactic analysis of Nor-
wegian is based on an existing LFG resource gram-
mar, NorGram (Dyvik, 1999), under development
on the Xerox Linguistic Environment (XLE) since
around 1999. For use in LOGON, the gram-
mar has been modified and extended, and it has
been augmented with a module of Minimal Re-
cursion Semantics representations which are com-
puted from LFG f-structures by co-description.
In Norwegian, compounding is a productive
morphological process, thus presenting the anal-
ysis engine with a steady supply of ?new? words,
e.g. something like klokkeslettuttrykk meaning ap-
53
Norwegian
Analysis
(LFG) ffPVM
-
NorGram
Lexicon?
Norwegian
SEM-I
6
-
LOGON
Controller ffPVM
- English
Generation
(HPSG)
ERG
Lexicon?
English
SEM-I
6
ffNO? ENTransfer
(MRS)
6PVM ?
GUI 6? WWW6?
Figure 2: Schematic system architecture: the three core pro-
cessing components are managed by a central controller that
passes intermediate results (MRSs) through the translation
pipeline. The Parallel Virtual Machine (PVM) layer facilitates
distribution, parallelization, failure detection, and roll-over.
proximately time-of-day expression. The project
uses its own morphological analyzer, compiled
off a comprehesive computational lexicon of Nor-
wegian, prior to syntactic analysis. One impor-
tant feature of this processor is that it decomposes
compounds in such a way that they can be compo-
sitionally translated downstream.
Current analysis coverage (including well-
formed MRSs) on the LOGON corpus (see below)
is approaching 80 per cent (of which 25 per cent
are ?fragmented?, i.e. approximative analyses).
Semantic Transfer Unlike in parsing and gen-
eration, there is less established common wisdom
in terms of (semantic) transfer formalisms and
algorithms. LOGON follows many of the main
Verbmobil ideas?transfer as a resource-sensitive
rewrite process, where rules replace MRS frag-
ments (SL to TL) in a step-wise manner (Wahlster,
2000)?but adds two innovative elements to the
transfer component, viz. (i) the use of typing for
hierarchical organization of transfer rules and (ii)
a chart-like treatment of transfer-level ambiguity.
The general form of MRS transfer rules (MTRs) is
as a quadruple:
[ CONTEXT : ] INPUT [ ! FILTER ] ? OUTPUT
where each of the four components, in turn, is
a partial MRS, i.e. triplet of a top handle, bag of
EPs, and handle constraints. Left-hand side com-
ponents are unified against an input MRS M and,
when successful, trigger the rule application; ele-
ments of M matched by INPUT are replaced with
the OUTPUT component, respecting all variable
bindings established during unification. The op-
tional CONTEXT and FILTER components serve to
condition rule application (on the presence or ab-
sence of specific aspects of M ), establish bindings
for OUTPUT processing, but do not consume el-
ements of M . Although our current focus is on
?lingo/jan-06/jh1/06-01-20/lkb? Generation Profile
total word distinct overall time
Aggregate items string trees coverage (s)
] ? ? % ?
30 ? i-length < 40 21 33.1 241.5 61.9 36.5
20 ? i-length < 30 174 23.0 158.6 80.5 15.7
10 ? i-length < 20 353 14.3 66.7 86.7 4.1
0 ? i-length < 10 495 4.6 6.0 90.1 0.7
Total 1044 11.6 53.50 86.7 4.3
(generated by [incr tsdb()] at 15-mar-2006 (15:51 h))
Table 1: Central measures of generator performance in re-
lation to input ?complexity?. The columns are, from left to
right, the corpus sub-division by input length, total number
of items, and average string length, ambiguity rate, grammat-
ical coverage, and generation time, respectively.
translation into English, MTRs in principle state
translational correspondence relations and, mod-
ulo context conditioning, can be reversed.
Transfer rules use a multiple-inheritance hier-
archy with strong typing and appropriate feature
constraints both for elements of MRSs and MTRs
themselves. In close analogy to constraint-based
grammar, typing facilitates generalizations over
transfer regularities?hierarchies of predicates or
common MTR configurations, for example?and
aids development and debugging.
An important tool in the constructions of the
transfer rules are the semantic interfaces (called
SEM-Is, see below) of the respective grammars.
While we believe that hand-crafted lexical trans-
fer is a necessary component in precision-oriented
MT, it is also a bottleneck for the development
of the LOGON system, with its pre-existing source
and target language grammars. We have therefore
experimented with the acquistion of transfer rules
by analogy from a bi-lingual dictionary, building
on hand-built transfer rules as a seed set of tem-
plates (Nordga?rd, Nygaard, L?nning, & Oepen,
2006).
English Generation Realization of post-transfer
MRSs in LOGON builds on the pre-existing LinGO
English Resource Grammar (ERG; Flickinger,
2000) and LKB generator (Carroll, Copestake,
Flickinger, & Poznanski, 1999). The ERG al-
ready produced MRS outputs with good coverage
in several domains. In LOGON, it has been refined,
adopted to the new domain, and semantic repre-
sentations revised in light of cross-linguistic ex-
periences from MT. Furthermore, chart generation
efficiency and integration with stochastic realiza-
tion have been substantially improved (Carroll &
Oepen, 2005). Table 1 summarizes (exhaustive)
generator performance on a segment of the LOGON
54
temp loc
at p temp in p temp on p temp
temp abstr
afternoon n day n ? ? ? year n
Figure 3: Excerpt from predicate hierarchies provided by English SEM-I. Temporal, directional, and other usages of prepo-
sitions give rise to distinct, but potentially related, semantic predicates. Likewise, the SEM-I incorporates some ontological
information, e.g. a classication of temporal entities, though crucially only to the extent that is actually grammaticized in the
language proper.
development corpus: realizations average at a lit-
tle less than twelve words in length. After addition
of domain-specific vocabulary and a small amount
of fine-tuning, the ERG provides adequate analyses
for close to ninety per cent of the LOGON reference
translations. For about half the test cases, all out-
puts can be generated in less than one cpu second.
End-to-End Coverage The current LOGON sys-
tem will only produce output(s) when all three
processing phases succeed. For the LOGON target
corpus (see below), this is presently the case in 35
per cent of cases. Averaging over actual outputs
only, the system achieves a (respectable) BLEU
score of 0.61; averaging over the entire corpus, i.e.
counting inputs with processing errors as a zero
contribution, the BLEU score drops to 0.21.
3 Stochastic Components
To deal with competing hypotheses at all process-
ing levels, LOGON incorporates various stochastic
processes for disambiguation. In the following, we
present the ones that are best developed to date.
Training Material A corpus of some 50,000
words of edited, running Norwegian text was gath-
ered and translated by three professional transla-
tors. Three quarters of the material are available
for system development and also serve as training
data for machine learning approaches. Using the
discriminant-based Redwoods approach to tree-
banking (Oepen, Flickinger, Toutanova, & Man-
ning, 2004), a first 5,000 English reference transla-
tions were hand-annotated and released to the pub-
lic.1 In on-going work on adapting the Redwoods
approach to (Norwegian) LFG, we are working to
treebank a sizable text segment (Rose?n, Smedt,
Dyvik, & Meurer, 2005; Oepen & L?nning, 2006).
Parse Selection The XLE analyzer includes sup-
port for stochastic parse selection models, assign-
ing likelihood measures to competing analyses
1See ?http://www.delph-in.net/redwoods/?
for the LinGO Redwoods treebank in its latest release,
dubbed Norwegian Growth.
(Riezler et al, 2002). Using a trial LFG treebank
for Norwegian (of less than 100 annotated sen-
tences), we have adapted the tools for the current
LOGON version and are now working to train on
larger data sets and evaluate parse selection perfor-
mance. Despite the very limited amount of train-
ing so far, the model already appears to pick up
on plausible, albeit crude preferences (as regards
topicalization, for example). Furthermore, to re-
duce fan-out in exhaustive processing, we collapse
analyses that project equivalent MRSs, i.e. syntac-
tic distinctions made in the grammar but not re-
flected in the semantics.
Realization Ranking At an average of more
than fifty English realizations per input MRS (see
Table 1), ranking generator outputs is a vital part
of the LOGON pipeline. Based on a notion of au-
tomatically derived symmetric treebanks, we have
trained comprehensive discriminative, log-linear
models that (within the LOGON domain) achieve
up to 75 per cent exact match accuracy in pick-
ing the most likely realization among compet-
ing outputs (Velldal & Oepen, 2005). The best-
performing models make use of configurational
(in terms of tree topology) as well as of string-
level properties (including local word order and
constituent weight), both with varied domains of
locality. In total, there are around 300,000 features
with non-trivial distribution, and we combine the
MaxEnt model with a traditional language model
trained on a much larger corpus (the BNC). The
latter, more standard approach to realization rank-
ing, when used in isolation only achieves around
50 per cent accuracy, however.
4 Implementation
Figure 2 presents the main components of the LO-
GON prototype, where all component communica-
tion is in terms of sets of MRSs and, thus, can easily
be managed in a distributed and (potentially) par-
allel client ? server set-up. Both the analysis and
generation grammars ?publish? their interface to
transfer?i.e. the inventory and synopsis of seman-
55
tic predicates?in the form of a Semantic Inter-
face specification (?SEM-I?; Flickinger, L?nning,
Dyvik, Oepen, & Bond, 2005), such that trans-
fer can operate without knowledge about gram-
mar internals. In practical terms, SEM-Is are
an important development tool (facilitating well-
formedness testing of interface representations at
all levels), but they also have interesting theoret-
ical status with regard to transfer. The SEM-Is
for the Norwegian analysis and English genera-
tion grammars, respectively, provide an exhaus-
tive enumeration of legitimate semantic predicates
(i.e. the transfer vocabulary) and ?terms of use?,
i.e. for each predicate its set of appropriate roles,
corresponding value constraints, and indication of
(semantic) optionality of roles. Furthermore, the
SEM-I provides generalizations over classes of
predicates?e.g. hierarchical relations like those
depicted in Figure 3 below?that play an impor-
tant role in the organization of MRS transfer rules.
5 Open-Source Machine Translation
Despite the recognized need for translation, there
is no widely used open-source machine translation
system. One of the major reasons for this lack of
success is the complexity of the task. By asso-
ciation to the international open-source DELPH-
IN effort2 and with its strong emphasis on re-
usability, LOGON aims to help build a repository of
open-source precision tools. This means that work
on the MT system benefits other projects, and
work on other projects can improve the MT sys-
tem (where EBMT and SMT systems provide re-
sults that are harder to re-use). While the XLE soft-
ware used for Norwegian analysis remains propri-
etary, we have built an open-source bi-directional
Japanese ? English prototype adaptation of the LO-
GON system (Bond, Oepen, Siegel, Copestake, &
Flickinger, 2005). This system will be available
for public download by the summer of 2006.
References
Bond, F., Oepen, S., Siegel, M., Copestake, A., & Flickinger,
D. (2005). Open source machine translation with DELPH-
IN. In Proceedings of the Open-Source Machine Trans-
lation workshop at the 10th Machine Translation Summit
(pp. 15 ? 22). Phuket, Thailand.
Carroll, J., Copestake, A., Flickinger, D., & Poznanski, V.
(1999). An efcient chart generator for (semi-)lexicalist
grammars. In Proceedings of the 7th European Workshop
on Natural Language Generation (pp. 86 ? 95). Toulouse,
France.
2See ?http://www.delph-in.net? for details, in-
cluding the lists of participating sites and already available
resources.
Carroll, J., & Oepen, S. (2005). High-efciency realization
for a wide-coverage unication grammar. In R. Dale &
K.-F. Wong (Eds.), Proceedings of the 2nd International
Joint Conference on Natural Language Processing (Vol.
3651, pp. 165 ? 176). Jeju, Korea: Springer.
Copestake, A., Flickinger, D., Sag, I. A., & Pollard, C.
(1999). Minimal Recursion Semantics. An introduction.
In preparation, CSLI Stanford, Stanford, CA.
Dyvik, H. (1999). The universality of f-structure. Discov-
ery or stipulation? The case of modals. In Proceedings of
the 4th International Lexical Functional Grammar Con-
ference. Manchester, UK.
Fenstad, J.-E., Ahrenberg, L., Kvale, K., Maegaard, B.,
M?uhlenbock, K., & Heid, B.-E. (2006). KUNSTI. Knowl-
edge generation for Norwegian language technology. In
Proceedings of the 5th International Conference on Lan-
guage Resources and Evaluation. Genoa, Italy.
Flickinger, D. (2000). On building a more efcient grammar
by exploiting types. Natural Language Engineering, 6 (1),
15 ? 28.
Flickinger, D., L?nning, J. T., Dyvik, H., Oepen, S., & Bond,
F. (2005). SEM-I rational MT. Enriching deep grammars
with a semantic interface for scalable machine translation.
In Proceedings of the 10th Machine Translation Summit
(pp. 165 ? 172). Phuket, Thailand.
Nordg	ard, T., Nygaard, L., L?nning, J. T., & Oepen, S.
(2006). Using a bi-lingual dictionary in lexical transfer. In
Proceedings of the 11th conference of the European Asoo-
ciation of Machine Translation. Oslo, Norway.
Oepen, S., Dyvik, H., L?nning, J. T., Velldal, E., Beermann,
D., Carroll, J., Flickinger, D., Hellan, L., Johannessen,
J. B., Meurer, P., Nordg	ard, T., & Ros?en, V. (2004). Som
	a kapp-ete med trollet? Towards MRS-based Norwegian ?
English Machine Translation. In Proceedings of the 10th
International Conference on Theoretical and Methodolog-
ical Issues in Machine Translation. Baltimore, MD.
Oepen, S., Flickinger, D., Toutanova, K., & Manning, C. D.
(2004). LinGO Redwoods. A rich and dynamic treebank
for HPSG. Journal of Research on Language and Compu-
tation, 2(4), 575 ? 596.
Oepen, S., & L?nning, J. T. (2006). Discriminant-based MRS
banking. In Proceedings of the 5th International Con-
ference on Language Resources and Evaluation. Genoa,
Italy.
Riezler, S., King, T. H., Kaplan, R. M., Crouch, R., Maxwell,
J. T., & Johnson, M. (2002). Parsing the Wall Street
Journal using a Lexical-Functional Grammar and discrim-
inative estimation techniques. In Proceedings of the 40th
Meeting of the Association for Computational Linguistics.
Philadelphia, PA.
Ros?en, V., Smedt, K. D., Dyvik, H., & Meurer, P. (2005).
TrePil. Developing methods and tools for multilevel tree-
bank construction. In Proceedings of the 4th Workshop
on Treebanks and Linguistic Theories (pp. 161 ? 172).
Barcelona, Spain.
Velldal, E., & Oepen, S. (2005). Maximum entropy models
for realization ranking. In Proceedings of the 10th Ma-
chine Translation Summit (pp. 109 ? 116). Phuket, Thai-
land.
Wahlster, W. (Ed.). (2000). Verbmobil. Foundations of
speech-to-speech translation. Berlin, Germany: Springer.
56
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 517?525,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Statistical Ranking in Tactical Generation
Erik Velldal
University of Oslo (Norway)
erik.velldal@ifi.uio.no
Stephan Oepen
University of Oslo (Norway)
and CSLI Stanford (CA)
oe@csli.stanford.edu
Abstract
In this paper we describe and evaluate
several statistical models for the task of
realization ranking, i.e. the problem of
discriminating between competing surface
realizations generated for a given input se-
mantics. Three models (and several vari-
ants) are trained and tested: an n-gram
language model, a discriminative maxi-
mum entropy model using structural in-
formation (and incorporating the language
model as a separate feature), and finally an
SVM ranker trained on the same feature
set. The resulting hybrid tactical generator
is part of a larger, semantic transfer MT
system.
1 Introduction
This paper describes the application of several dif-
ferent statistical models for the task of realiza-
tion ranking in tactical generation, i.e. the problem
of choosing among multiple paraphrases that are
generated for a given meaning representation. The
specific realization component we use is the open-
source chart generator of the Linguistic Knowl-
edge Builder (LKB; Carroll, Copestake, Flickinger,
& Poznanski, 1999; Carroll & Oepen, 2005).
Given a meaning representation in the form of
Minimal Recursion Semantics (MRS; Copestake,
Flickinger, Malouf, Riehemann, & Sag, 1995),
the generator outputs English realizations in ac-
cordance with the HPSG LinGO English Resource
Grammar (ERG; Flickinger, 2002).
As an example of generator output, a sub-set
of alternate realizations that are produced for a
single input MRS is shown in Figure 1. For the
two data sets considered in this paper, the aver-
age number of realizations produced by the gen-
erator is 85.7 and 102.2 (the maximum numbers
are 4176 and 3408, respectively). Thus, there is
immediate demand for a principled way of choos-
ing a single output among the generated candi-
dates. For this task we train and test three differ-
ent statistical models: an n-gram language model,
a maximum entropy model (MaxEnt) and a (lin-
ear) support vector machine (SVM). These are
all models that have proved popular within the
NLP community, but it is usually only the first
of these three that has been applied to the task
of ranking in sentence generation. The latter two
models that we present here go beyond the sur-
face information used by the n-gram model, and
are trained on a symmetric treebank with features
defined over the full HPSG analyses of compet-
ing realizations. Furthermore, such discriminative
models are suitable for ?on-line? use within our
generator?adopting the technique of selective un-
packing from a packed forest (Carroll & Oepen,
2005)?which means our hybrid realizer obviates
the need for exhaustive enumeration of candidate
outputs. The present results extend our earlier
work (Velldal, Oepen, & Flickinger, 2004)?and
the related work of Nakanishi, Miyao, & Tsu-
jii (2005)?to an enlarged data set, more feature
types, and additional learners.
The rest of this paper is structured as follows.
Section 2 first gives a general summary of the var-
ious statistical models we will be considering, as
well as the measures used for evaluating them. We
then go on to define the task we are aiming to solve
in terms of treebank data and feature types in Sec-
tion 3. By looking at different variants of the Max-
Ent model we review some results for the relative
contribution of individual features and the impact
of frequency cutoffs for feature selection. Keeping
these parameters constant then, Section 4 provides
an array of empirical results on the relative perfor-
mance of the various approaches.
2 Models
In this section we briefly review the different types
of statistical models that we use for ranking the
output of the generator. We start by describing
the language model, and then go on to review the
framework for discriminative MaxEnt models and
SVM rankers. In the following we will use s and
r to denote semantic inputs and generated realiza-
tions respectively.
517
Remember that dogs must be on a leash.
Remember dogs must be on a leash.
On a leash, remember that dogs must be.
On a leash, remember dogs must be.
A leash, remember that dogs must be on.
A leash, remember dogs must be on.
Dogs, remember must be on a leash.
Table 1: A small example set of generator out-
puts using the ERG. Where the input semantics is
no specified for aspects of information structure
(e.g. requesting foregrounding of a specific entity),
paraphrases include all grammatically legitimate
topicalizations. Other choices involve, for exam-
ple, the optionality of complementizers and rela-
tive pronouns, permutation of (intersective) modi-
fiers, and lexical and orthographic alternations.
2.1 Language Models
The use of n-gram language models is the most
common approach to statistical selection in gen-
eration (Langkilde & Knight, 1998; and White
(2004); inter alios). In order to better assert the
relative performance of the discriminative mod-
els and the structural features we present below,
we also apply a trigram model to the ranking
problem. Using the freely available CMU SLM
Toolkit (Clarkson & Rosenfeld, 1997), we trained
a trigram model on an unannotated version of
the British National Corpus (BNC), containing
roughly 100 million words (using Witten-Bell dis-
counting and back-off). Given such a model p
n
,
the score of a realization r
i
with surface form
w
k
i1
= (w
i1
; : : : ; w
ik
) is then computed as
(1) F (s; r
i
) =
k
X
j=1
p
n
(w
i;j
jw
i;j n
; : : : ; w
i;j 1
)
Given the scoring function F , the best realiza-
tion is selected according to the following decision
function:
(2) r^ = argmax
r
0
2Y(s)
F (s; r
0
)
Although in this case scoring is not conditioned
on the input semantics at all, we still include it to
make the function formulation more general as we
will be reusing it later.
Note that, as the realizations in our symmet-
ric treebank also include punctuation marks, these
are also treated as separate tokens by the language
model (in addition to pseudo-tokens marking sen-
tence boundaries).
2.2 Maximum Entropy Models
Maximum entropy modeling provides a very flex-
ible framework that has been widely used for a
range of tasks in NLP, including parse selection
(e.g. Johnson, Geman, Canon, Chi, & Riezler,
1999; Malouf & Noord, 2004) and reranking for
machine translation (e.g. Och et al, 2004). A
model is specified by a set of real-valued feature
functions that describe properties of the data, and
an associated set of learned weights that determine
the contribution of each feature.
Let us first introduce some notation before we
go on. Let Y(s
i
) = fr
1
; : : : ; r
m
g be the set of re-
alizations licensed by the grammar for a semantic
representation s
i
. Now, let our (positive) training
data be given as X
p
= fx
1
; : : : ; x
N
g where each
x
i
is a pair (s
i
; r
j
) for which r
j
2 Y(s
i
) and r
j
is annotated in the treebank as being a correct re-
alization of s
i
. Note that we might have several
different members of Y(s
i
) that pair up with s
i
in X
p
. In our set-up, this is the case where multi-
ple HPSG derivations for the same input semantics
project identical surface strings.
Given a set of d features (as further described
in Section 3.2), each pair of semantic input s and
hypothesized realization r is mapped to a feature
vector (s; r) 2 <d. The goal is then to find a
vector of weights w 2 <d that optimize the like-
lihood of the training data. A conditional MaxEnt
model of the probability of a realization r given
the semantics s, is defined as
p
w
(rjs) =
e
F
w
(s;r)
Z
w
(s)
(3)
where the function F
w
is simply the sum of the
products of all feature values and feature weights,
given by
(4) F
w
(s; r) =
d
X
i=1
w
i

i
(s; r) = w  (s; r)
The normalization term Z
w
is defined as
(5) Z
w
(s) =
X
r
0
2Y(s)
e
F
w
(s;r)
When we want to find the best realization for a
given input semantics according to a model p
w
, it
is sufficient to compute the score function as in
Equation (4) and then use the decision function
previously given in Equation (2) above. When it
518
comes to estimating1 the parameters w, the pro-
cedure seeks to maximize the (log of) a penalized
likelihood function as in
(6) w^ = argmax
w
logL(w)  
P
d
i=1
w
2
i
2
2
where L(w) is the ?conditionalized? likelihood of
the training data X
p
(Johnson et al, 1999), com-
puted as L(w) =
Q
N
i=1
p
w
(r
i
js
i
). The second
term of the likelihood function in Equation (6) is
a penalty term that is commonly used for reducing
the tendency of log-linear models to over-fit, es-
pecially when training on sparse data using many
features (Chen & Rosenfeld, 1999; Johnson et al,
1999; Malouf & Noord, 2004). More specifically
it defines a zero-mean Gaussian prior on the fea-
ture weights which effectively leads to less ex-
treme values. After empirically tuning the prior
on our ?Jotunheimen? treebank (training and test-
ing by 10-fold cross-validation), we ended up us-
ing 2 = 0:003 for the MaxEnt models applied in
this paper.
2.3 SVM Rankers
In this section we briefly formulate the optimiza-
tion problem in terms of support vector machines.
Our starting point is the SVM approach introduced
in Joachims (2002) for learning ranking functions
for information retrieval. In our case the aim is to
learn a ranking function from a set of preference
relations on sentences generated for a given input
semantics.
In contrast to the MaxEnt approach, the SVM
approach has a geometric rather than probabilistic
view on the problem. Similarly to the the MaxEnt
set-up, the SVM learner will try to learn a linear
scoring function as defined in Equation (4) above.
However, instead of maximizing the probability
of the preferred or positive realizations, we try to
maximize their value for F
w
directly.
Recall our definition of the set of positive train-
ing examples in Section 2.2. Let us here analo-
gously define X
n
= fx
1
; : : : ; x
Q
g to be the neg-
ative counterpart, so that for a given pair x =
(s
i
; r
j
) 2 X
n
, we have that r
j
2 Y(s
i
) but r
j
is
not annotated as a preferred realization of s
i
. Fol-
1We use the TADM open-source package (Malouf, 2002)
for training the models, using its limited-memory variable
metric as the optimization method and experimentally deter-
mine the optimal convergence threshold and variance of the
prior.
lowing Joachims (2002), the goal is to minimize
(7) V (w; ) = 1
2
w  w + C
X

i;j;k
subject to the following constraints,
8ijk s.t. (s
k
; r
i
) 2 X
p
^ (s
k
; r
j
) 2 X
n
:(8)
w  (s
k
; r
i
)  w  (s
k
; r
j
) + 1   
i;j;k
8ijk : 
i;j;k
 0(9)
Joachims (2002) shows that the preference con-
straints in Equation (8) can be rewritten as
(10) w  ((s
k
; r
i
)   (s
k
; r
j
))  1   
i;j;k
so that the optimization problem is equivalent to
training a classifier on the pairwise difference vec-
tors (s
k
; r
i
)   (s
k
; r
j
). The (non-negative)
slack variables 
i;j;k
are commonly used in SVMs
to make it possible to approximate a solution by
allowing some error in cases where a separating
hyperplane can not be found. The trade-off be-
tween maximizing the margin size and minimizing
the training error is governed by the constant C .
Using the SVMlight package by Joachims (1999),
we empirically specified C = 0:005 for the model
described in this paper. Note that, for the ex-
periments reported here, we will only be mak-
ing binary distinctions of preferred/non-preferred
realizations, although the approach presented in
(Joachims, 2002) is formulated for the more gen-
eral case of learning ordinal ranking functions.
Finally, given a linear SVM, we score and se-
lect realizations in the same way as we did with
the MaxEnt model, according to Equations (4) and
(2). Note, however, that it is also possible to use
non-linear kernel functions with this set-up, since
the ranking function can be represented as a linear
combination of the feature vectors as in
(11) w  (s; r
i
) =
X

j;k
(s
j
; r
k
)(s; r
i
)
2.4 Evaluation Measures
The models presented in this paper are evaluated
with respect to two simple metrics: exact match
accuracy and word accuracy. The exact match
measure simply counts the number of times that
the model assigns the highest score to a string that
exactly matches a corresponding ?gold? or refer-
ence sentence (i.e. a sentence that is marked as
preferred in the treebank). This score is discounted
appropriately in the case of ties between preferred
and non-preferred candidates.
519
if several realizations are given the top rank by
the model. We also include the exact match accu-
racy for the five best candidates according to the
models (see the n-best columns of Table 6).
The simple measure of exact match accuracy of-
fers a very intuitive and transparent view on model
performance. However, it is also in some respects
too harsh as an evaluation measure in our setting
since there will often be more than just one of
the candidate realizations that provides a reason-
able rendering of the input semantics. We there-
fore also include WA as similarity-based evalua-
tion metric. This measure is based on the Lev-
ensthein distance between a candidate string and
a reference, also known as edit distance. This is
given by the minimum number of deletions, sub-
stitutions and insertions of words that are required
to transform one string into another. If we let d, s
and i represent the number of necessary deletions,
substitutions and insertions respectively, and let l
be the length of the reference, then WA is defined
as
(12) WA = 1  d + s + i
l
The scores produced by similarity measures such
as WA are often difficult to interpret, but at least
they provide an alternative view on the relative
performance of the different models that we want
to compare. We could also have used several
other similarity measures here, such as the BLEU
score which is a well-established evaluation metric
within MT, but in our experience the various string
similarity measures usually agree on the relative
ranking of the different models.
3 Data Sets and Features
The following sections summarize the data sets
and the feature types used in the experiments.
3.1 Symmetric Treebanks
Conditional parse selection models are standardly
trained on a treebank consisting of strings paired
with their optimal analyses. For our discriminative
realization ranking experiments we require train-
ing corpora that provide the inverse relation. By
assuming that the preferences captured in a stan-
dard treebank can constitute a bidirectional rela-
tion, Velldal et al (2004) propose a notion of sym-
metric treebanks as the combination of (a) a set of
pairings of surface forms and associated seman-
tics; combined with (b) the sets of alternative anal-
Jotunheimen
Bin Items Words Trees Gold Chance
100  n 396 21.7 367.4 20.7 0.083
50  n < 100 246 18.5 73.7 11.5 0.160
10  n < 50 831 14.8 24.2 6.3 0.277
5  n < 10 426 10.1 7.0 3.0 0.436
1 < n < 5 291 11.2 3.3 1.6 0.486
Total 2190 15.1 85.7 8.2 0.287
Rondane
Bin Items Words Trees Gold Chance
100  n 107 21.8 498.4 17.8 0.060
50  n < 100 63 19.1 72.9 12.0 0.162
10  n < 50 244 15.2 23.4 4.9 0.234
5  n < 10 119 11.9 7.2 2.7 0.377
1 < n < 5 101 9.3 3.21 1.5 0.476
Total 634 15.1 102.2 6.8 0.263
Table 2: Some core metrics for the symmetric tree-
banks ?Jotunheimen? (top) and ?Rondane? (bot-
tom). The former data set was used for devel-
opment and cross-validation testing, the latter for
cross-genre held-out testing. The data items are
aggregated relative to their number of realizations.
The columns are, from left to right, the subdivi-
sion of the data according to the number of real-
izations, total number of items scored (excluding
items with only one realization and ones where
all realizations are marked as preferred), aver-
age string length, average number of realizations,
and average number of references. The rightmost
column shows a random choice baseline, i.e. the
probability of selecting the preferred realization
by chance.
yses for each surface form, and (c) sets of alter-
nate realizations of each semantic form. Using
the semantics of the preferred analyses in an ex-
isting treebank as input to the generator, we can
produce all equivalent paraphrases of the original
string. Furthermore, assuming that the original
surface form is an optimal verbalization of the cor-
responding semantics, we can automatically label
the preferred realization(s) by matching the yields
of the generated trees against the original strings
in the ?source? treebank. The result is what we
call a generation treebank, which taken together
with the original parse-oriented pairings constitute
a full symmetrical treebank.
We have successfully applied this technique to
the tourism segments of the LinGO Redwoods
treebank, which in turn is built atop the ERG.2
2See ?http://www.delph-in.net/erg/? for fur-
520
Table 2 summarizes the two resulting data sets,
which are both comprised of instructional texts
on tourist activities, the application domain of the
background MT system.
3.2 Feature Templates
For the purpose of parse selection, Toutanova,
Manning, Shieber, Flickinger, & Oepen (2002)
and Toutanova & Manning (2002) train a dis-
criminative log-linear model on the Redwoods
parse treebank, using features defined over deriva-
tion trees with non-terminals representing the con-
struction types and lexical types of the HPSG
grammar (see Figure 1). The basic feature set
of our MaxEnt realization ranker is defined in the
same way (corresponding to the PCFG-S model of
Toutanova & Manning, 2002), each feature captur-
ing a sub-tree from the derivation limited to depth
one. Table 3 shows example features in our Max-
Ent and SVM models, where the feature template
# 1 corresponds to local derivation sub-trees. To
reduce the effects of data sparseness, feature type
# 2 in Table 3 provides a back-off to derivation
sub-trees, where the sequence of daughters is re-
duced, in turn, to just one of the daughters. Con-
versely, to facilitate sampling of larger contexts
than just sub-trees of depth one, feature template
# 1 allows optional grandparenting, including the
upward chain of dominating nodes in some fea-
tures. In our experiments, we found that grandpar-
enting of up to three dominating nodes gave the
best balance of enlarged context vs. data sparse-
ness.
subjh
hspec
det the le
the
sing noun
n intr le
dog
third sg fin verb
v unerg le
barks
Figure 1: Sample HPSG derivation tree for the
sentence the dog barks. Phrasal nodes are la-
beled with identifiers of grammar rules, and (pre-
terminal) lexical nodes with class names for types
of lexical entries.
In addition to these dominance-oriented fea-
tures taken from the derivation trees of each re-
alization, our models also include more surface-
ther information and download pointers.
Id Sample Features
1 h0 subjh hspec third sg fin verbi
1 h1 4 subjh hspec third sg fin verbi
1 h0 hspec det the le sing nouni
1 h1 subjh hspec det the le sing nouni
1 h2 4 subjh hspec det the le sing nouni
2 h0 subjh third sg fin verbi
2 h0 subjh hspcei
2 h1 subjh hspec det the lei
2 h1 subjh hspec sing nouni
3 h1 n intr le dogi
3 h2 det the le n intr le dogi
3 h3  det the le n intr le dogi
4 h1 n intr lei
4 h2 det the le n intr lei
4 h3  det the le n intr lei
Table 3: Examples of structural features extracted
from the derivation tree in Figure 1. The first col-
umn identifies the feature template corresponding
to each example; in the examples, the first integer
value is a parameter to feature templates, i.e. the
depth of grandparenting (types 1 and 2) or n-gram
size (types 3 and 4). The special symbols 4 and 
denote the root of the tree and left periphery of the
yield, respectively.
oriented features, viz. n-grams of lexical types
with or without lexicalization. Feature type # 3 in
Table 3 defines n-grams of variable size, where
(in a loose analogy to part of speech tagging) se-
quences of lexical types capture syntactic cate-
gory assignments. Feature templates # 3 and # 4
only differ with regard to lexicalization, as the for-
mer includes the surface token associated with the
rightmost element of each n-gram. Unless other-
wise noted, we used a maximum n-gram size of
three in the experiments reported here, again due
to its empirically determined best overall perfor-
mance.
The number of instantiated features produced
by the feature templates easily grows quite large.
For the ?Jotunheimen? data the total number of dis-
tinct feature instantiations is 312,650. For the ex-
periments in this paper we implemented a simple
frequency based cutoff by removing features that
are observed as relevant less than  times. We here
follow the approach of Malouf & Noord (2004)
where relevance of a feature is simply defined as
taking on a different value for any two competing
candidates for the same input. A feature is only
included in training if it is relevant for more than
 items in the training data. Table 4 shows the ef-
fect on the accuracy of the MaxEnt model when
varying the cutoff. We see that a model can be
521
Cutoff Features Accuracy
  312,650 71.18
1 264,455 71.18
2 112,051 70.03
3 66,069 70.28
4 46,139 69.30
5 35,295 67.93
10 16,036 65.36
20 7,563 63.05
50 2,605 59.10
100 889 54.21
200 261 50.11
500 34 34.70
Table 4: The effects of frequency-based feature se-
lection with respect to model size and accuracy.
model configuration match WA
basic model of (Velldal et al, 2004) 63.09 0.904
basic plus partial daughter sequence 64.64 0.910
basic plus grandparenting 67.54 0.923
basic plus lexical type trigrams 68.61 0.921
basic plus all of the above 70.28 0.927
basic plus language model 67.96 0.912
basic plus all of the above 72.28 0.928
Table 5: Performance summaries of best-
performing realization rankers using various fea-
ture configurations, when compared to the set-up
of Velldal et al (2004). These scores where com-
puted using a relevance cutoff of 3 and optimizing
the variance of the prior for individual configura-
tions.
compacted quite aggressively without sacrificing
much in performance. For all models presented
below we use a cutoff of  = 3.
4 Results
In this section we present contrastive results for
the models defined in Section 2 above, evaluated
against the exact match accuracy and word accu-
racy as described in Section 2.4.
As can be seen in Table 6, both the MaxEnt
and SVM learner does a much better job than
the n-gram model at identifying the correct refer-
ence strings. The two discriminative models per-
form very similarly, however, although the Max-
Ent model often seems to do slightly better.
When working with a cross-validation set-up
the difference between the learners can conve-
niently be tested using an approach such as the
cross-validated paired t-test described by Diet-
terich (1998). We also tried this approach using
the Wilcoxon Matched-Pairs Signed-Ranks test as
a non-parametric alternative without the assump-
tion of normality of differences made in the t-test.
However, none of the two tests found that the dif-
ferences between the MaxEnt model and the SVM
model were significant for  = 0:05 (using two-
sided tests).
Note that, due to memory constraints, we only
included a random sample of maximum 50 non-
preferred realizations per item in the training data
used for the SVM ranker. Even so, the SVM
trained on the full ?Jotunheimen? data had a to-
tal of 66,621 example vectors in its training data,
which spawned a total of 639,301 preference con-
straints with respect to the optimization problem
of Equations 8 and 10. We did not try to maxi-
mize performance on the development data by re-
peatedly training with different random samples,
but this might be one way to improve the results.
Although we were only able to present results
using linear kernels for the SVM ranker in this pa-
per, preliminary experiments using a polynomial
kernel seem to give promising results. Due to
memory constraints and long convergence times,
we were only able to train such a model on half
of the ?Jotunheimen? data. However, when testing
on the remaining half, it achieved an exact match
accuracy of 71:03%. This is comparable to the
performance achieved by the linear SVM through
full 10-fold training and testing. Moreover, there
is reason to believe that these results will improve
once we manage to train on the full data set.
In order to assess the effect of increasing the
size of the training set, Figure 3 presents learning
curves for two MaxEnt configurations, viz. the ba-
sic configurational model and the one including all
features but the language model. Each data point
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
1-5 5-10
10-50
50-100
100-4176
Baseline
BNC LM
SVM
MaxEnt
Figure 2: Exact match accuracy scores for the dif-
ferent models. Data items are binned with respect
to the number of distinct realizations.
522
Jotunheimen Rondane
Model accuracy n-best WA accuracy n-best WA
BNC LM 53.24 78.81 0.882 54.19 77.19 0.891
SVM 71.11 84.69 0.922 63.64 83.12 0.906
MaxEnt 72.28 84.59 0.927 64.28 83.60 0.903
Table 6: Performance of the different learners. The results on the ?Jotunheimen? treebank for the discrim-
inative models are averages from 10-fold cross-validation. A model trained on the entire ?Jotunheimen?
data was used when testing on ?Rondane?. Note that the training accuracy of the SVM learner on the
?Jotunheimen? training set is 91.69%, while it?s 92.99% for the MaxEnt model.
 50
 55
 60
 65
 70
 75
 10  20  30  40  50  60  70  80  90  100
a
c
c
u
ra
c
y 
(%
)
training data (%)
Basic
All
Figure 3: Learning curves for two MaxEnt model
configurations (trained without cutoffs). Al-
though there appears to be a saturation effect in
model performance with increasing amounts of
?Jotunheimen? training data, for the richer config-
uration (using all features but the language model)
further enlarging the training data still seems at-
tractive.
corresponds to average exact match performance
for 10-fold cross-validation on ?Jotunheimen?, but
restricting the amount of training data presented to
the learner to between 10 and 100 per cent of the
total. At 60 per cent training data, the two mod-
els already perform at 60:6% and 68:4% accuracy,
and the learning curves are starting to flatten out.
Somewhat remarkably, the richer model including
partial daughter back-off, grandparenting, and lex-
ical type trigrams already outperforms the baseline
model by a clear margin with just a small fraction
of the training data, so the MaxEnt learner appears
to make effective use of the greatly enlarged fea-
ture space.
When testing against the ?Rondane? held-
out set and comparing to performance on the
?Jotunheimen? cross-validation set, we see that the
performance of both the MaxEnt model and the
SVM degrades quite a bit. Of course, some drop
in performance is to be expected as the estimation
parameters had been tuned to this development set.
Furthermore, as can be seen from Table 2, the
baseline is also slightly lower for the ?Rondane?
test set as the average number of realizations is
higher. Also, while basically from the same do-
main, the two text collections differ noticeably
in style: ?Jotunheimen? is based on edited, high-
quality guide books; ?Rondane? has been gathered
from a variety of web sites. Note, however, that
the performance of the BNC n-gram model seems
to be more stable across the different data sets.
In any case we see that, for our realization rank-
ing task, the use of discriminative models in com-
bination with structural features extracted from
treebanks, clearly outperforms the surface ori-
ented, generative n-gram model. This is in spite of
the relatively modest size of the treebanked train-
ing data available to the discriminative models. On
the ?Rondane? test set the reduction in error rate
for the combined MaxEnt model relative to the n-
gram LM, is 22:03%. The error reduction for the
SVM over the LM on ?Rondane? is 20:63%.
Another factor that is likely to be important for
the differences in performance is the fact that the
treebank data is better tuned to the domain of ap-
plication or the test data. The n-gram language
model, on the other hand, was only trained on
the general-domain BNC data. Note, however,
that when testing on ?Rondane?, we also tried to
combine this general-domain model with an ad-
ditional in-domain model trained only on the text
that formed the basis of the ?Jotunheimen? tree-
bank, a total of 5024 sentences. The optimal
weights for linearly combining these two models
were calculated using the interpolation tool in the
CMU toolkit (using the expectation maximization
(EM) algorithm, minimizing the perplexity on a
held out data set of 330 sentences). However,
when applied to the ?Rondane? test set, this in-
523
model error ties correct
BNC LM 253 68 313
MaxEnt (sans LM) 222 63 349
MaxEnt (combined) 225 3 404
Table 7: Exact match error counts for three mod-
els, viz. the BNC LM only, the MaxEnt model by
itself (using all feature types except the LM prob-
ability), and the combined MaxEnt model. The
intermediate column corresponds to ties or partial
errors, i.e. the number of items for which multiple
candidates were ranked at the top, of which some
were actually preferred and some not. Primarily
this latter error type is reduced by including the
LM feature in the MaxEnt universe.
terpolated model failed to improve on the results
achieved by just using the larger general-domain
model alone. This is probably due to the small
amount of domain specific data that we presently
have available for training.
Another observation about our n-gram experi-
ments that is worth a mention is that we found that
ranking realizations according to non-normalized
log probabilities directly resulted in much bet-
ter accuracy than using a length normalized score
such as the geometric mean.
Finally, Table 7 breaks down per-item exact
match errors for three distinct ranking configura-
tions, viz. the BNC LM only, the structural Max-
Ent model only, and the combined MaxEnt model,
which includes the LM probability as an addi-
tional feature; all numbers are for application to
the held-out ?Rondane? test set. Further contrast-
ing the first two of these, the BNC LM yields 129
unique errors, in the sense that the structural Max-
Ent makes the correct predictions on these items,
contrasted to 98 unique errors in the structural
MaxEnt model. When compared to the only 124
errors made equally by both rankers, we conclude
that the different approaches have partially com-
plementary strengths and weaknesses. This ob-
servation is confirmed in the relatively substan-
tial improvement in ranking performance of the
combined model on the ?Rondane? test: The ex-
act match accuracies of the n-gram model, the ba-
sic MaxEnt model and the combined model are
54:19%, 59:43% and 64:28%, respectively.
5 Summary and Outlook
Applying three alternate statistical models to the
realization ranking task, we found that discrimi-
native models with access to structural informa-
tion substantially outperform the traditional lan-
guage model approach. Using comparatively
small amounts of annotated training data, we were
able to boost ranking performance from around
54% to more than 72%, albeit for a limited, rea-
sonably coherent domain and genre. The incre-
mental addition of feature templates into the Max-
Ent model suggests a trend of diminishing return,
most likely due to increasing overlap in the portion
of the problem space captured across templates,
and possibly reflecting limitations in the amount
of training data. The comparison of the Max-
Ent and SVM rankers suggest comparable perfor-
mance on our task, not showing statistically signif-
icant differences. Nevertheless, in terms of scala-
bility when using large data sets, it seems clear
that the MaxEnt framework is a more practical and
manageable alternative, both in terms of training
time and memory requirements.
As further work we would like to try to train an
SVM that takes full advantage of the ranking po-
tential of the set-up described in (Joachims, 2002).
Instead of just making binary (right/wrong) dis-
tinctions, we could grade the realizations in the
training data according to their WA scores toward
the references and try to learn a similar ranking.
So far we have only been able to do preliminary
experiments with this set-up on a small sub-set of
the data. When evaluated with the accuracy mea-
sures used in this paper the results were not as
good as those obtained when training with only
two ranks, however this might very well look dif-
ferent if we evaluate the full rankings (e.g. number
of swapped pairs) instead of just focusing on the
top ranked candidates. Note that it is also possible
to use such graded training data with the MaxEnt
models, by letting the probabilities of the empiri-
cal distribution be based on similarity scores such
as WA instead of frequencies.
Acknowledgments
The work reported here is part of the Norwe-
gian LOGON project on precision MT, and we
are grateful to numerous colleagues; please see
?http://www.emmtee.net? for background.
Furthermore, we warmly acknowledge the sup-
port and productive criticism provided by Dan
Flickinger (the ERG developer), Francis Bond,
John Carrol, and three anonymous reviewers.
524
References
Carroll, J., Copestake, A., Flickinger, D., & Poz-
nanski, V. (1999). An efficient chart generator
for (semi-)lexicalist grammars. In Proceedings
of the 7th European Workshop on Natural Lan-
guage Generation. Toulouse.
Carroll, J., & Oepen, S. (2005). High-efficiency re-
alization for a wide-coverage unification gram-
mar. In R. Dale & K. fai Wong (Eds.), Proceed-
ings of the 2nd International Joint Conference
on Natural Language Processing (Vol. 3651,
pp. 165 ? 176). Jeju, Korea: Springer.
Chen, S. F., & Rosenfeld, R. (1999). A Gaussian
prior for smoothing maximum entropy mod-
els (Tech. Rep.). Carnegie Mellon University.
(Technical Report CMUCS-CS-99-108)
Clarkson, P., & Rosenfeld, R. (1997). Statistical
language modeling using the CMU-Cambridge
Toolkit. In Proceedings of ESCA Eurospeech.
Copestake, A., Flickinger, D., Malouf, R., Riehe-
mann, S., & Sag, I. (1995). Translation using
minimal recursion semantics. In Proceedings
of the Sixth International Conference on The-
oretical and Methodological Issues in Machine
Translation. Leuven, Belgium.
Dietterich, T. G. (1998). Approximate statisti-
cal test for comparing supervised classifica-
tion learning algorithms. Neural Computation,
10(7), 1895?1923.
Flickinger, D. (2002). On building a more effi-
cient grammar by exploiting types. In S. Oepen,
D. Flickinger, J. Tsujii, , & H. Uszkoreit (Eds.),
Collaborative language engineering: A case
study in efficient grammar-based processing
(pp. 1?17). CSLI Press.
Joachims, T. (1999). Making large-scale svm
learning practical. In B. Scho?lkopf, C. Burges,
& A. Smola (Eds.), Advances in kernel methods
- support vector learning. MIT-Press.
Joachims, T. (2002). Optimizing search engines
using clickthrough data. In Proceedings of the
ACM conference on knowledge discovery and
data mining (KDD). ACM.
Johnson, M., Geman, S., Canon, S., Chi, Z., &
Riezler, S. (1999). Estimators for stochastic
?unification-based? grammars. In Proceedings
of the 37th Meeting of the Association for Com-
putational Linguistics (pp. 535 ? 541). College
Park, MD.
Langkilde, I., & Knight, K. (1998). The practical
value of n-grams in generation. In International
natural language generation workshop.
Malouf, R. (2002). A comparison of algorithms for
maximum entropy parameter estimation. In Pro-
ceedings of the 6th Conference on Natural Lan-
guage Learning (pp. 49?55). Taipei, Taiwan.
Malouf, R., & Noord, G. van. (2004). Wide cov-
erage parsing with stochastic attribute value
grammars. In Proceedings of the IJCNLP work-
shop Beyond Shallow Analysis. Hainan, China.
Nakanishi, H., Miyao, Y., & Tsujii, J. (2005).
Probabilistic models for disambiguation of an
HPSG-based chart generator. In Proceedings
of the 9th International Workshop on Pars-
ing Technologies (pp. 93 ? 102). Vancouver,
Canada: Association for Computational Lin-
guistics.
Och, F. J., Gildea, D., Khudanpur, S., Sarkar, A.,
Yamada, K., Fraser, A., Kumar, S., Shen, L.,
Smith, D., Eng, K., Jain, V., Jin, Z., & Radev, D.
(2004). A smorgasbord of features for statistical
machine translation. In Proceedings of the 5th
Conference of the North American Chapter of
the ACL. Boston.
Toutanova, K., & Manning, C. D. (2002). Feature
selection for a rich HPSG grammar using deci-
sion trees. In Proceedings of the 6th Conference
on Natural Language Learning. Taipei, Taiwan.
Toutanova, K., Manning, C. D., Shieber, S. M.,
Flickinger, D., & Oepen, S. (2002). Parse dis-
ambiguation for a rich hpsg grammar. In First
workshop on treebanks and linguistic theories.
Sozopol, Bulgaria.
Velldal, E., Oepen, S., & Flickinger, D. (2004).
Paraphrasing treebanks for stochastic realiza-
tion ranking. In Proceedings of the 3rd work-
shop on Treebanks and Linguistic Theories.
Tu?bingen, Germany.
White, M. (2004). Reining in CCG chart realiza-
tion. In Proceedings of the 3rd International
Conference on Natural Language Generation.
Hampshire, UK.
525
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1379?1387,
Beijing, August 2010
Syntactic Scope Resolution in Uncertainty Analysis
Lilja ?vrelid?? and Erik Velldal? and Stephan Oepen?
? University of Oslo, Department of Informatics
?Universit?t Potsdam, Institut f?r Linguistik
ovrelid@uni-potsdam.de and erikve@ifi.uio.no and oe@ifi.uio.no
Abstract
We show how the use of syntactic struc-
ture enables the resolution of hedge scope
in a hybrid, two-stage approach to un-
certainty analysis. In the first stage, a
Maximum Entropy classifier, combining
surface-oriented and syntactic features,
identifies cue words. With a small set of
hand-crafted rules operating over depen-
dency representations in stage two, we at-
tain the best overall result (in terms of
both combined ranks and average F1) in
the 2010 CoNLL Shared Task.
1 Background?Motivation
Recent years have witnessed an increased interest
in the analysis of various aspects of sentiment in
natural language (Pang & Lee, 2008). The sub-
task of hedge resolution deals with the analysis of
uncertainty as expressed in natural language, and
the linguistic means (so-called hedges) by which
speculation or uncertainty are expressed. Infor-
mation of this kind is of importance for various
mining tasks which aim at extracting factual data.
Example (1), taken from the BioScope corpus
(Vincze, Szarvas, Farkas, M?ra, & Csirik, 2008),
shows a sentence where uncertainty is signaled by
the modal verb may.1
(1) {The unknown amino acid ?may? be used by these
species}.
The topic of the Shared Task at the 2010 Con-
ference for Natural Language Learning (CoNLL)
is hedge detection in biomedical literature?in a
sense ?zooming in? on one particular aspect of the
broader BioNLP Shared Task in 2009 (Kim, Ohta,
Pyysalo, Kano, & Tsujii, 2009). It involves two
subtasks: Task 1 is described as learning to detect
1In examples throughout this paper, angle brackets high-
light hedge cues, and curly braces indicate the scope of a
given cue, as annotated in BioScope.
sentences containing uncertainty; the objective of
Task 2 is learning to resolve the in-sentence scope
of hedge cues (Farkas, Vincze, Mora, Csirik, &
Szarvas, 2010). The organizers further suggest:
This task falls within the scope of semantic analy-
sis of sentences exploiting syntactic patterns [...].
The utility of syntactic information within var-
ious approaches to sentiment analysis in natu-
ral language has been an issue of some debate
(Wilson, Wiebe, & Hwa, 2006; Ng, Dasgupta,
& Arifin, 2006), and the potential contribution of
syntax clearly varies with the specifics of the task.
Previous work in the hedging realm has largely
been concerned with cue detection, i.e. identify-
ing uncertainty cues such as may in (1), which
are predominantly individual tokens (Medlock &
Briscoe, 2007; Kilicoglu & Bergler, 2008). There
has been little previous work aimed at actually
resolving the scope of such hedge cues, which
presumably constitutes a somewhat different and
likely more difficult problem. Morante and Daele-
mans (2009) present a machine-learning approach
to this task, using token-level, lexical informa-
tion only. To this end, CoNLL 2010 enters largely
uncharted territory, and it remains to be seen (a)
whether syntactic analysis indeed is a necessary
component in approaching this task and, more
generally, (b) to what degree the specific task
setup can inform us about the strong and weak
points in current approaches and technology.
In this article, we investigate the contribution
of syntax to hedge resolution, by reflecting on our
experience in the CoNLL 2010 task.2 Our CoNLL
system submission ranked fourth (of 24) on Task 1
and third (of 15) on Task 2, for an overall best av-
erage result (there appears to be very limited over-
lap among top performers for the two subtasks).
2It turns out, in fact, that all the top-performing systems
in Task 2 of the CoNLLShared Task rely on syntactic informa-
tion provided by parsers, either in features for machine learn-
ing or as input to manually crafted rules (Morante, Asch, &
Daelemans, 2010; Rei & Briscoe, 2010).
1379
Sentences Hedged Cues Multi-Word Tokens Cue Tokens
Sentences Cues
Abstracts 11871 2101 2659 364 309634 3056
Articles 2670 519 668 84 68579 782
Total 14541 2620 3327 448 378213 3838
Table 1: Summary statistics for the Shared Task training data.
This article transcends our CoNLL system descrip-
tion (Velldal, ?vrelid, & Oepen, 2010) in several
respects, presenting updated and improved cue de-
tection results (? 3 and ? 4), focusing on the role
of syntactic information rather than on machine
learning specifics (? 5 and ? 6), providing an anal-
ysis and discussion of Task 2 errors (? 7), and gen-
erally aiming to gauge the value of available anno-
tated data and processing tools (? 8). We present
a hybrid, two-level approach for hedge resolution,
where a statistical classifier detects cue words, and
a small set of manually crafted rules operating
over syntactic structures resolve scope. We show
how syntactic information?produced by a data-
driven dependency parser complemented with in-
formation from a ?deep?, hand-crafted grammar?
contributes to the resolution of in-sentence scope
of hedge cues, discussing various types of syn-
tactic constructions and associated scope detec-
tion rules in considerable detail. We furthermore
present a manual error analysis, which reveals re-
maining challenges in our scope resolution rules
as well as several relevant idiosyncrasies of the
preexisting BioScope annotation.
2 Task, Data, and System Basics
Task Definition and Evaluation Metrics
Task 1 is a binary sentence classification task:
identifying utterances as being certain or uncer-
tain. Following common practice, this subtask
is evaluated in terms of precision, recall, and
F1 for the ?positive? class, i.e. uncertain. In
our work, we approach Task 1 as a byproduct
of the full hedge resolution problem, labeling a
sentence as uncertain if it contains at least one
token classified as a hedge cue. In addition to
the sentence-level evaluation for Task 1, we also
present precision, recall, and F1 for the cue-level.
Task 2 comprises two subtasks: cue detection
and scope resolution. The official CoNLL eval-
uation does not tease apart these two aspects of
the problem, however: Only an exact match of
both the cue and scope bracketing (in terms of
substring positions) will be counted as a success,
again quantified in terms of precision, recall, and
F1. Discussing our results below, we report cue
detection and scope resolution performance sepa-
rately, and further put scope results into perspec-
tive against an upper bound based on the gold-
standard cue annotation.
Besides the primary biomedical domain data,
some annotated Wikipedia data was provided
for Task 1, and participating systems are classi-
fied as in-domain (using exclusively the domain-
specific data), cross-domain (combining both
types of training data), or open (utilizing addi-
tional uncertainty-related resources). In our work,
we focus on the interplay of syntax and the more
challenging Task 2; we ignored the Wikipedia
track in Task 1. Despite our using general NLP
tools (see below), our system falls into the most
restrictive, in-domain category.
Training and Evaluation Data The training
data for the CoNLL 2010 Shared Task is taken from
the BioScope corpus (Vincze et al, 2008) and
consists of 14,541 ?sentences? (or other root-level
utterances) from biomedical abstracts and articles
(see Table 1).3 The BioScope corpus provides
annotation for hedge cues as well as their scope.
According to the annotation guidelines (Vincze et
al., 2008), the annotation adheres to a principle
of minimalism when it comes to hedge cues, i.e.
the minimal unit expressing hedging is annotated.
The inverse is true of scope annotations, which ad-
here to a principle of maximal scope?meaning
that scope should be set to the largest syntactic
3As it was known beforehand that evaluation would draw
on full articles only, we put more emphasis on the article
subset of the training data, for example in cross validation
testing and manual diagnosis of errors.
1380
ID FORM LEMMA POS FEATS HEAD DEPREL XHEAD XDEP
1 The the DT _ 4 NMOD 4 SPECDET
2 unknown unknown JJ degree:attributive 4 NMOD 4 ADJUNCT
3 amino amino JJ degree:attributive 4 NMOD 4 ADJUNCT
4 acid acid NN pers:3|case:nom|num:sg|ntype:common 5 SBJ 3 SUBJ
5 may may MD mood:ind|subcat:MODAL|tense:pres|clauseType:decl 0 ROOT 0 ROOT
6 be be VB _ 5 VC 7 PHI
7 used use VBN subcat:V-SUBJ-OBJ|vtype:main|passive:+ 6 VC 5 XCOMP
8 by by IN _ 7 LGS 9 PHI
9 these these DT deixis:proximal 10 NMOD 10 SPECDET
10 species specie NNS num:pl|pers:3|case:obl|common:count|ntype:common 8 PMOD 7 OBL-AG
11 . . . _ 5 P 0 PUNC
Table 2: Stacked dependency representation of example (1), with MaltParser and XLE annotations.
unit possible.
For evaluation purposes, the task organizers
provided newly annotated biomedical articles, fol-
lowing the same general BioScope principles. The
CoNLL 2010 evaluation data comprises 5,003 ad-
ditional utterances (138,276 tokens), of which 790
are annotated as hedged. The data contains a to-
tal of 1033 cues, of which 87 are so-called multi-
word cues (i.e. cues spanning multiple tokens),
comprising 1148 cue tokens altogether.
Stacked Dependency Parsing For syntactic
analysis we employ the open-source MaltParser
(Nivre, Hall, & Nilsson, 2006), a platform for
data-driven dependency parsing. For improved
accuracy and portability across domains and gen-
res, we make our parser incorporate the pre-
dictions of a large-scale, general-purpose LFG
parser?following the work of ?vrelid, Kuhn, and
Spreyer (2009). A technique dubbed parser stack-
ing enables the data-driven parser to learn, not
only from gold standard treebank annotations, but
from the output of another parser (Nivre & Mc-
Donald, 2008). This technique has been shown to
provide significant improvements in accuracy for
both English and German (?vrelid et al, 2009),
and a similar setup employing an HPSG gram-
mar has been shown to increase domain indepen-
dence in data-driven dependency parsing (Zhang
& Wang, 2009). The stacked parser combines
two quite different approaches?data-driven de-
pendency parsing and ?deep? parsing with a hand-
crafted grammar?and thus provides us with a
broad range of different types of linguistic infor-
mation for the hedge resolution task.
MaltParser is based on a deterministic pars-
ing strategy in combination with treebank-induced
classifiers for predicting parse transitions. It sup-
ports a rich feature representation of the parse his-
tory in order to guide parsing and may easily be
extended to take additional features into account.
The procedure to enable the data-driven parser
to learn from the grammar-driven parser is quite
simple. We parse a treebank with the XLE plat-
form (Crouch et al, 2008) and the English gram-
mar developed within the ParGram project (Butt,
Dyvik, King, Masuichi, & Rohrer, 2002). We
then convert the LFG output to dependency struc-
tures, so that we have two parallel versions of the
treebank?one gold standard and one with LFG
annotation. We extend the gold standard treebank
with additional information from the correspond-
ing LFG analysis and train MaltParser on the en-
hanced data set.
Table 2 shows the enhanced dependency rep-
resentation of example (1) above, taken from the
training data. For each token, the parsed data con-
tains information on the word form, lemma, and
part of speech (PoS), as well as on the head and
dependency relation in columns 6 and 7. The
added XLE information resides in the FEATS col-
umn, and in the XLE-specific head and depen-
dency columns 8 and 9. Parser outputs, which in
turn form the basis for our scope resolution rules
discussed in Section 5, also take this same form.
The parser employed in this work is trained on
the Wall Street Journal sections 2 ? 24 of the Penn
Treebank (PTB), converted to dependency format
(Johansson & Nugues, 2007) and extended with
XLE features, as described above. Parsing uses the
arc-eager mode of MaltParser and an SVM with
a polynomial kernel. When tested using 10-fold
cross validation on the enhanced PTB, the parser
achieves a labeled accuracy score of 89.8.
PoS Tagging and Domain Variation Our
parser is trained on financial news, and although
stacking with a general-purpose LFG parser is ex-
1381
pected to aid domain portability, substantial dif-
ferences in domain and genre are bound to neg-
atively affect syntactic analysis (Gildea, 2001).
MaltParser presupposes that inputs have been PoS
tagged, leaving room for variation in preprocess-
ing. On the one hand, we aim to make parser
inputs maximally similar to its training data (i.e.
the conventions established in the PTB); on the
other hand we wish to benefit from specialized re-
sources for the biomedical domain.
The GENIA tagger (Tsuruoka et al, 2005) is
particularly relevant in this respect (as could be
the GENIA Treebank proper4). However, we
found that GENIA tokenization does not match the
PTB conventions in about one out of five sen-
tences (for example wrongly splitting tokens like
?390,926? or ?Ca(2+)?); also in tagging proper
nouns, GENIA systematically deviates from the
PTB. Hence, we adapted an in-house tokenizer
(using cascaded finite-state rules) to the CoNLL
task, run two PoS taggers in parallel, and eclec-
tically combine annotations across the various
preprocessing components?predominantly giv-
ing precedence to GENIA lemmatization and PoS
hypotheses.
To assess the impact of improved, domain-
adapted inputs on our hedge resolution system,
we contrast two configurations: first, running the
parser in the exact same manner as ?vrelid, Kuhn,
and Spreyer (2010), we use TreeTagger (Schmid,
1994) and its standard model for English (trained
on the PTB) for preprocessing; second, we give as
inputs to the parser our refined tokenization and
merged PoS tags, as described above. When eval-
uating the two modes of preprocessing on the ar-
ticles subset of the training data, and using gold-
standard cues, our system for resolving cue scopes
(presented in ? 5) achieves an F1 of 66.31 with
TreeTagger inputs, and 72.30 using our refined to-
kenization and tagger combination. These results
underline the importance of domain adaptation for
accurate syntactic analysis, and in the following
we assume our hybrid in-house setup.
4Although the GENIA Treebank provides syntactic anno-
tation in a form inspired by the PTB, it does not provide func-
tion labels. Therefore, our procedure for converting from
constituency to dependency requires non-trivial adaptation
before we can investigate the effects of retraining the parser
against GENIA.
3 Stage 1: Identifying Hedge Cues
For the task of identifying hedge cues, we devel-
oped a binary maximum entropy (MaxEnt) clas-
sifier. The identification of cue words is used
for (a) classifying sentences as certain/uncertain
(Task 1), and (b) providing input to the syntac-
tic rules that we later apply for resolving the in-
sentence scope of the cues (Task 2). We also re-
port evaluation scores for the sub-task of cue de-
tection in isolation.
As annotated in the training data, it is possible
for a hedge cue to span multiple tokens, e.g. as in
whether or not. The majority of the multi-word
cues in the training data are very infrequent, how-
ever, most occurring only once, and the classifier
itself is not sensitive to the notion of multi-word
cues. Instead, the task of determining whether a
cue word forms part of a larger multi-word cue, is
performed in a separate post-processing step (ap-
plying a heuristic rule targeted at only the most
frequently occurring patterns of multi-word cues
in the training data).
During development, we trained cue classifiers
using a wide variety of feature types, both syn-
tactic and surface-oriented. In the end, however,
we found n-gram-based lexical features to have
the greatest contribution to classifier performance.
Our best-performing classifier so far (see ?Final?
in Table 3) includes the following feature types:
n-grams over forms (up to 2 tokens to the right),
n-grams over base forms (up to 3 tokens left
and right), PoS (from GENIA), subcategorization
frames (from XLE), and phrase-structural coordi-
nation level (from XLE). Our CoNLL system de-
scription includes more details of the various other
feature types that we experimented with (Velldal
et al, 2010).
4 Cue Detection Evaluation
Table 3 summarizes the performance of our Max-
Ent hedge cue classifier in terms of precision, re-
call and F1, computed using the official Shared
Task scorer script. The sentence-level scores cor-
respond to Task 1 of the Shared Task, and the cue-
level scores are based on the exact-match counts
for full hedge cues (possibly spanning multiple to-
kens).
1382
Sentence Level Cue Level
Configuration Prec Rec F1 Prec Rec F1
Baseline, Development 79.25 79.45 79.20 77.37 71.70 74.43
Final, Development 91.39 86.78 89.00 90.18 79.47 84.49
Final, Held-Out 85.61 85.06 85.33 81.97 76.41 79.10
Table 3: Isolated evaluation of the hedge cue classifier.
As the CoNLL test data was known beforehand
to consist of articles only, in 10-fold cross vali-
dation for classifier development we tested exclu-
sively against the articles segment, while always
including all sentences from the abstracts in the
training set. This corresponds to the development
results in Table 3, while the held-out results are
for the official Shared Task evaluation data (train-
ing on all the available training data). A model
using only unigram features serves as a baseline.
5 Stage 2: Resolving Scope
Hedge scope may vary quite a lot depending on
linguistic properties of the cue in question. In our
approach to scope resolution we rely heavily on
syntactic information, taken from the dependency
structures proposed by both MaltParser and XLE,
as well as on various additional features relating
to specific syntactic constructions.
We constructed a small set of heuristic rules
which define the scope for each cue detected in
Stage 1. In developing these rules, we made use
of the information provided by the guidelines for
scope annotation in the BioScope corpus (Vincze
et al, 2008), combined with manual inspection of
the training data in order to further generalize over
the phenomena discussed by Vincze et al (2008)
and work out interactions of constructions for var-
ious types of cues.
The rules take as input a parsed sentence which
has been further tagged with hedge cues. They
operate over the dependency structures and ad-
ditional features provided by the parser. Default
scope is set to start at the cue word and span to
the end of the sentence (modulo punctuation), and
this scope also provides the baseline for the eval-
uation of our rules. In the following, we discuss
broad classes of rules, organized by categories of
hedge cues. As there is no explicit representa-
tion of phrase or clause boundaries in our depen-
dency universe, we assume a set of functions over
dependency graphs, for example finding the left-
or rightmost (direct) dependent of a given node,
or transitively selecting left- or rightmost descen-
dants.
Coordination The dependency analysis of co-
ordination provided by our parser makes the first
conjunct the head of the coordination. For cues
that are coordinating conjunctions (PoS tag CC),
such as or, we define the scope as spanning the
whole coordinate structure, i.e. start scope is set
to the leftmost dependent of the head of the coor-
dination, e.g., roX in (2), and end scope is set to
its rightmost dependent (conjunct), e.g., RNAs in
(2). This analysis provides us with coordinations
at various syntactic levels, such as NP and N (2),
AP and AdvP, or VP (3):
(2) [...] the {roX genes ?or? RNAs} recruit the entire set
of MSL proteins [...]
(3) [...] the binding interfaces are more often {kept ?or?
even reused} rather than lost in the course of
evolution.
Adjectives We distinguish between adjectives
(JJ) in attributive (NMOD) function and adjectives
in predicative (PRD) function. Attributive adjec-
tives take scope over their (nominal) head, with all
its dependents, as in (4) and (5):
(4) The {?possible? selenocysteine residues} are shown
in red, [...]
(5) Extensive analysis of the flanks failed to show any
hallmarks of {?putative? transposons that might be
associated with this RAG1-like protein}, [...]
For adjectives in a predicative function the scope
includes the subject argument of the head verb
(the copula), as well as a (possible) clausal argu-
ment, as in (6). The scope does not, however, in-
clude expletive subjects, as in (7).
1383
(6) Therefore, {the unknown amino acid, if it is encoded
by a stop codon, is ?unlikely? to exist in the current
databases of microbial genomes}.
(7) For example, it is quite {?likely? that there exists an
extremely long sequence that is entirely unique to U}.
Verbs The scope of verbal cues is a bit more
complex and depends on several factors. In our
rules, we distinguish passive usages from active
usages, raising verbs from non-raising verbs, and
the presence or absence of a subject-control em-
bedding context. The scopes of both passive and
raising verbs include the subject argument of their
head verb, as in (8) and (9), unless it is an exple-
tive pronoun, as in (10).
(8) {Interactions determined by high-throughput methods
are generally ?considered? to be less reliable than
those obtained by low-throughput studies} 1314 and
as a consequence [...]
(9) {Genomes of plants and vertebrates ?seem? to be free
of any recognizable Transib transposons} (Figure 1).
(10) It has been {?suggested? that unstructured regions of
proteins are often involved in binding interactions,
particularly in the case of transient interactions} 77.
In the case of subject control involving a hedge
cue, specifically modals, subject arguments are in-
cluded in scopes where the controller heads a pas-
sive construction or a raising verb, as in exam-
ple (1) above, repeated here for convenience:
(11) {The unknown amino acid ?may? be used by these
species}.
In general, the end scope of verbs should ex-
tend over the minimal clause that contains the verb
in question. In terms of dependency structures,
we define the clause boundary as comprising the
chain of descendants of a verb which is not inter-
vened by a token with a higher attachment in the
graph than the verb in question. In example (8)
for instance, the sentence-level conjunction and
marks the end of the clause following the cue con-
sidered.
Prepositions and Adverbs Cues that are tagged
as prepositions (including some complementizers)
take scope over their argument, with all its de-
scendants, (12). Adverbs take scope over their
head with all its (non-subject) syntactic descen-
dants (13).
Configuration F1
BS
P
Default, Gold Cues 45.21
Rules, Gold Cues 72.31
Rules, System Cues 64.77
BS
E Rules, Gold Cues 66.73
Rules, System Cues 55.75
Table 4: Evaluation of scope resolution rules.
(12) {?Whether? the codon aligned to the inframe stop
codon is a nonsense codon or not} was neglected at
this stage.
(13) These effects are {?probably? mediated through the
1,25(OH)2D3 receptor}.
Multi-Word Cues In the case of multi-word
cues, such as indicate that or either ... or, we need
to determine the head of the multi-word unit. We
then set the scope of the whole unit to the scope
of the head token.
As an illustration of rule processing, consider
our running example (11), with its syntactic anal-
ysis as shown in Table 2 above. This example
invokes a variety of syntactic properties, includ-
ing parts of speech, argumenthood, voice etc. Ini-
tially, the scope of the hedge cue is set to default
scope. Then the subject control rule is applied,
which checks the properties of the verbal argu-
ment used, going through a chain of verbal depen-
dents from the modal verb. Since it is marked as
passive in the LFG analysis, the start scope is set to
include the subject of the cue word (the leftmost
descendant in its SBJ dependent).
6 Rule Evaluation
Table 4 summarizes scope resolution performance
(viewed as a an isolated subtask) for various con-
figurations, both against the articles section of the
CoNLL training data (dubbed BSP) and against the
held-out evaluation data (BSE). First of all, we note
that the ?default scope? baseline is quite strong:
unconditionally extending the scope of a cue to
the end of the sentence yields an F1 of 45.21.
Given gold standard cue information, our scope
rules improve on the baseline by 27 points on the
articles section of the data set, for an F1 of 72.31;
with system-assigned hedge cues, our rules still
1384
achieve an F1 of 64.77. Note that scope resolu-
tion scores based on classified cues also yield the
end-to-end system evaluation for Task 2.
The bottom rows of Table 4 show the evaluation
of scope rules on the CoNLL held-out test data. Us-
ing system cues, scope resolution on the held-out
data scores at 55.75 F1. Comparing to the result
on the (articles portion of the) training data, we
observe a substantial drop in performance (of six
points with gold-standard cues, nine points with
system cues). There are several possible explana-
tions for this effect. First of all, there may well
be a certain degree of overfitting of our rules to
the training data. The held-out data may contain
hedging constructions that are not covered by our
current set of scope rules, or annotation of parallel
constructions may in some cases differ in subtle
ways (see ? 7 below). Moreover, scope resolution
performance is of course influenced by cue detec-
tion (see Table 3). The cue-level F1 of our sys-
tem on the held-out data set is 79.10, compared to
84.49 (using cross validation) on the training data.
This drop in cue-level performance appears to af-
fect classification precision far more than recall.
Of course, given that our heuristics for identifying
multi-word cues were based on patterns extracted
from the training data, some loss in the cue-level
score was expected.
7 Error Analysis
To start shedding some light on the significance
of our results, we performed a manual error anal-
ysis on the article portion of the training material
(BSP), with two of the authors (trained linguists)
working in tandem. Using gold-standard cues,
our scope resolution rules fail to exactly replicate
the target annotation in 185 (of 668) cases, corre-
sponding to 72.31 F1 in Table 4 above. Our eval-
uators reviewed and discussed these 185 cases,
classifying 156 (84%) as genuine system errors,
22 (12%) as likely5 annotation errors, and a re-
5In some cases, there is no doubt that annotation is er-
roneous, i.e. in violation of the available annotation guide-
lines (Vincze et al, 2008) or in conflict with otherwise un-
ambiguous patterns. In other cases, however, judgments are
necessarily based on generalizations made by the evaluators,
i.e. assumptions about the underlying system and syntactic
analyses implicit in the BioScope annotations. Furthermore,
selecting items for manual analysis that do not align with the
maining seven cases as involving controversial or
seemingly arbitrary decisions.
The two most frequent classes of system er-
rors pertain (a) to the recognition of phrase and
clause boundaries and (b) to not dealing success-
fully with relatively superficial properties of the
text. Examples (14) and (15) illustrate the first
class of errors, where in addition to the gold-
standard annotation we use vertical bars (?|?) to
indicate scope predictions of our system.
(14) [...] {the reverse complement |mR of m will be
?considered? to be [...]|}
(15) This |{?might? affect the results} if there is a
systematic bias on the composition of a protein
interaction set|.
In our syntax-driven approach to scope resolution,
system errors will almost always correspond to a
failure in determining constituent boundaries, in a
very general sense. However, specifically exam-
ple (15) is indicative of a key challenge in this
task, where adverbials of condition, reason, or
contrast frequently attach within the dependency
domain of a hedge cue, yet are rarely included in
the scope annotation.
Example (16) demonstrates our second fre-
quent class of system errors. One in six items
in the BSP training data contains a sentence-final
parenthesized element or trailing number, as for
example (2), (9), or (10) above; most of these are
bibliographic or other in-text references, which
are never included in scope annotation. Hence,
our system includes a rule to ?back out? from trail-
ing parentheticals; in examples like (16), how-
ever, syntax does not make explicit the contrast
between an in-text reference vs. another type of
parenthetical.
(16) More specifically, {|the bristle and leg phenotypes are
?likely? to result from reduced signaling by Dl| (and
not by Ser)}.
Moving on to apparent annotation errors, the
rules for inclusion (or not) of the subject in
the scope of verbal hedge cues and decisions
on boundaries (or internal structure) of nominals
predictions made by our scope resolution rules is likely to
bias our sample, such that our estimated proportion of 12%
annotation errors cannot be used to project an overall error
rate.
1385
seem problematic?as illustrated in examples (17)
to (22).6
(17) [...] and |this is also {?thought? to be true for the full
protein interaction networks we are modeling}|.
(18) [...] {Neur |?can? promote Ser signaling|}.
(19) |Some of the domain pairs {?seem? to mediate a large
number of protein interactions, thus acting as reusable
connectors}|.
(20) One {|?possible? explanation| is functional
redundancy with the mouse Neur2 gene}.
(21) [...] |redefinition of {one of them is ?feasible?}|.
(22) |The {Bcl-2 family ?appears? to function [...]}|.
Finally, the difficult corner cases invoke non-
constituent coordination, ellipsis, or NP-initial fo-
cus adverbs?and of course interactions of the
phenomena discussed above. Without making the
syntactic structures assumed explicit, it is often
very difficult to judge such items.
8 Reflections ? Outlook
Our combination of stacked dependency parsing
and hand-crafted scope resolution rules proved
adequate for the CoNLL 2010 competition, con-
firming the central role of syntax in this task.
With a comparatively small set of rules (imple-
mented in a few hundred lines of code), con-
structed through roughly two full weeks of ef-
fort (studying BioScope annotations and develop-
ing rules), our CoNLL system achieved an end-to-
end F1 of 55.33 on Task 2.7 The two submis-
sions with better results (at 57.32 and 55.65 F1)
represent groups who have pioneered the hedge
analysis task in previous years (Morante et al,
2010; Rei & Briscoe, 2010). Scores for other ?in-
domain? participants range from 52.24 to 2.15 F1.
6Like in the presentation of system errors, we include
scope predictions of our own rules here too, which we be-
lieve to be correct in these cases. Also in this class of errors,
we find the occasional ?uninteresting? mismatch, for exam-
ple related to punctuation marks and inconsistencies around
parentheses.
7In ? 4 and ? 6 above, we report scores for a slightly im-
proved version of our system, where (after the official CoNLL
submission date) we eliminated a bug related to the treatment
of sentence-initial whitespace in the XML annotations. At an
end-to-end F1 of 55.75, this system would outrank the sec-
ond best performer in Task 2.
Doubtless there is room for straightforward exten-
sion: for example retraining our parser on the GE-
NIA Treebank, further improving the cue classifier,
and refining scope resolution rules in the light of
the error analysis above.
At the same time, we remain mildly am-
bivalent about the long-term impact of some of
the specifics of the 2010 CoNLL task. Shared
tasks (i.e. system bake-offs) have become increas-
ingly popular in past years, and in some sub-
fields (e.g. IE, SMT, or dependency parsing) high-
visibility competitions can shape community re-
search agendas. Hence, even at this early stage, it
seems appropriate to reflect on the possible con-
clusions to be drawn from the 2010 hedge res-
olution task. First, we believe the harsh ?exact
substring match? evaluation metric underestimates
the degree to which current technology can solve
this problem; furthermore, idiosyncratic, string-
level properties (e.g. the exact treatment of punc-
tuation or parentheticals) may partly obscure the
interpretation of methods used and corresponding
system performance.
These effects are compounded by some con-
cerns about the quality of available annotation.
Even though we tried fine-tuning our cross vali-
dation testing to the nature of the evaluation data
(comprising only articles), our system performs
substantially worse on the newly annotated CoNLL
test data, in both stages.8 In our view, the anno-
tation of hedge cues and scopes ideally would be
overtly related to at least some level of syntactic
annotation?as would in principle be possible for
the segment of BioScope drawing on the abstracts
of the GENIA Treebank.
Acknowledgements
We are grateful to the organizers of the 2010
CoNLL Shared Task and creators of the BioScope
resource; first, for engaging in these kinds of com-
munity service, and second for many in-depth dis-
cussions of annotation and task details. We thank
our colleagues at the Universities of Oslo and
Potsdam for their comments and support.
8We are leaving open the possibility to further refine our
system; we have therefore abstained from an error analysis
on the evaluation data so far.
1386
References
Butt, M., Dyvik, H., King, T. H., Masuichi, H., & Rohrer,
C. (2002). The Parallel Grammar Project. In Proceed-
ings of COLING workshop on grammar engineering and
evaluation (pp. 1 ? 7). Taipei, Taiwan.
Crouch, D., Dalrymple, M., Kaplan, R., King, T., Maxwell,
J., & Newman, P. (2008). XLE documentation. Palo Alto,
CA. (Palo Alto Research Center)
Farkas, R., Vincze, V., Mora, G., Csirik, J., & Szarvas, G.
(2010). The CoNLL 2010 Shared Task: Learning to de-
tect hedges and their scope in natural language text. In
Proceedings of the 14th Conference on Natural Language
Learning. Uppsala, Sweden.
Gildea, D. (2001). Corpus variation and parser perfor-
mance. In Proceedings of the 2001 conference on Empir-
ical Methods in Natural Language Processing (pp. 167 ?
202). Pittsburgh, PA.
Johansson, R., & Nugues, P. (2007). Extended constituent-
to-dependency conversion for English. In J. Nivre, H.-
J. Kaalep, & M. Koit (Eds.), Proceedings of NODALIDA
2007 (p. 105-112). Tartu, Estonia.
Kilicoglu, H., & Bergler, S. (2008). Recognizing speculative
language in biomedical research articles: A linguistically
motivated perspective. In Proceedings of the BioNLP
2008 Workshop. Columbus, OH, USA.
Kim, J.-D., Ohta, T., Pyysalo, S., Kano, Y., & Tsujii, J.
(2009). Overview of BioNLP 2009 Shared Task on event
extraction. In Proceedings of the BioNLP 2009 workshop
companion volume for shared task (pp. 1 ? 9). Boulder,
CO: Association for Computational Linguistics.
Medlock, B., & Briscoe, T. (2007). Weakly supervised learn-
ing for hedge classification in scientific literature. In Pro-
ceedings of the 45th Meeting of the Association for Com-
putational Linguistics (pp. 992 ? 999). Prague, Czech Re-
public: Association for Computational Linguistics.
Morante, R., Asch, V. V., & Daelemans, W. (2010).
Memory-based resolution of in-sentence scope of hedge
cues. In Proceedings of the 14th Conference on Natural
Language Learning (pp. 40 ? 47). Uppsala, Sweden.
Morante, R., & Daelemans, W. (2009). Learning the scope
of hedge cues in biomedical texts. In Proceedings of
the BioNLP 2009 Workshop (pp. 28 ? 36). Boulder, Col-
orado.
Ng, V., Dasgupta, S., & Arifin, S. M. N. (2006). Examin-
ing the role of linguistic knowledge sources in the auto-
matic identification and classification of reviews. In Pro-
ceedings of the 21st International Conference on Compu-
tational Linguistics and the 44th Annual Meeting of the
Association for Computational Linguistics. Sydney, Aus-
tralia.
Nivre, J., Hall, J., & Nilsson, J. (2006). MaltParser: A data-
driven parser-generator for dependency parsing. In Pro-
ceedings of the Fifth International Conference on Lan-
guage Resources and Evaluation (p. 2216-2219). Genoa,
Italy.
Nivre, J., & McDonald, R. (2008, June). Integrating graph-
based and transition-based dependency parsers. In Pro-
ceedings of the 46th Meeting of the Association for Com-
putational Linguistics (pp. 950 ? 958). Columbus, Ohio.
?vrelid, L., Kuhn, J., & Spreyer, K. (2009). Improving data-
driven dependency parsing using large-scale LFG gram-
mars. In Proceedings of the 47th Meeting of the Associ-
ation for Computational Linguistics (pp. 37 ? 40). Singa-
pore.
?vrelid, L., Kuhn, J., & Spreyer, K. (2010). Cross-
framework parser stacking for data-driven dependency
parsing. TAL 2010 special issue on Machine Learning
for NLP, 50(3).
Pang, B., & Lee, L. (2008). Opinion mining and senti-
ment analysis. Foundations and Trends in Information
Retrieval, 2(1-2).
Rei, M., & Briscoe, T. (2010). Combining manual rules and
supervised learning for hedge cue and scope detection. In
Proceedings of the 14th Conference on Natural Language
Learning (pp. 56 ? 63). Uppsala, Sweden.
Schmid, H. (1994). Probabilistic part-of-speech tagging us-
ing decision trees. In International conference on new
methods in language processing (p. 44-49). Manchester,
England.
Tsuruoka, Y., Tateishi, Y., Kim, J.-D., Ohta, T., McNaught,
J., Ananiadou, S., et al (2005). Developing a robust
Part-of-Speech tagger for biomedical text. In Advances in
informatics (pp. 382 ? 392). Berlin, Germany: Springer.
Velldal, E., ?vrelid, L., & Oepen, S. (2010). Resolving
speculation: MaxEnt cue classification and dependency-
based scope rules. In Proceedings of the 14th Conference
on Natural Language Learning. Uppsala, Sweden.
Vincze, V., Szarvas, G., Farkas, R., M?ra, G., & Csirik, J.
(2008). The BioScope corpus: Annotation for negation,
uncertainty and their scope in biomedical texts. In Pro-
ceedings of the BioNLP 2008 Workshop. Columbus, OH,
USA.
Wilson, T., Wiebe, J., & Hwa, R. (2006). Recognizing strong
and weak opinion clauses. Computational Intelligence,
22(2), 73 ? 99.
Zhang, Y., & Wang, R. (2009). Cross-domain dependency
parsing using a deep linguistic grammar. In Proceedings
of the 47th Meeting of the Association for Computational
Linguistics. Singapore.
1387
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 90?94, Dublin, Ireland, August 23-29 2014.
RDF Triple Stores and a Custom SPARQL Front-End for
Indexing and Searching (Very) Large Semantic Networks
Milen Kouylekov
?
and Stephan Oepen
??
?
University of Oslo, Department of Informatics
?
Potsdam University, Department of Linguistics
{milen |oe}@ifi.uio.no
Abstract
With growing interest in the creation and search of linguistic annotations that form general graphs
(in contrast to formally simpler, rooted trees), there also is an increased need for infrastructures
that support the exploration of such representations, for example logical-form meaning represen-
tations or semantic dependency graphs. In this work, we lean heavily on semantic technologies
and in particular the data model of the Resource Description Framework (RDF) to represent,
store, and efficiently query very large collections of text annotated with graph-structured repre-
sentations of sentence meaning. Our full infrastructure is available under open-source licensing,
and through this system demonstration we hope to receive feedback on the general approach,
explore its application to additional types of meaning representation, and attract new users and
possibly co-developers.
1 Motivation: The Problem
Much work in the creation and use of language resources has focused on tree-shaped data structures,
1
as are commonly used for the encoding of, for example, syntactic or discourse annotations. Conversely,
there has been less focus on supporting general graphs until recently, but there is growing interest in
graph-structured representations, for example to annotate and process natural language semantics. In
this work, we demonstrate how semantic technologies, and in particular the data model of the Resource
Description Framework (RDF) can be put to use for efficient indexing and search in (very) large-scale
collections of semantic graphs.
We develop a mapping to RDF graphs for a variety of semantic representations, ranging from un-
derspecified logical-form meaning representations to ?pure? bi-lexical semantic dependency graphs, as
exemplified in Figures 1 and 2 below, respectively. Against this uniform data model, we populate off-
the-shelf RDF triple stores with semantic networks comprising between tens of thousands and tens of
millions of analyzed sentences. To lower the technological barrier to exploration of our triple stores, we
implement a compact ?designer? query language for semantic graphs through on-the-fly expansion into
SPARQL. In sum, the combination of standard RDF technologies and specialized query and visualization
interfaces yields a versatile and highly scalable infrastructure for search (and in principle limited forms
of reasoning) over diverse types of graph-structured representations of sentence meaning.
In our view, there is little scientific innovation in this work, but our approach rather demonstrates sub-
stantial design and engineering creativity. Our semantic search infrastructure is built from the combina-
tion of industrial-grade standard technologies (Apache Jena, Lucene, and Tomcat) with an open-source
application for, among others, format conversion, query processing, and visualization implemented in
Java. Thus, the complete tool chain is available freely and across platforms. Its application to additional
types of meaning representation (and possibly other graph-structured layers of linguistic analysis) should
be relatively straightforward, and we thus believe that our infrastructure can be of immediate value to
both providers and consumers of large-scale linguistic annotations that transcend tree structures.
This work is licenced under a Creative Commons Attribution 4.0 International License; page numbers and the proceedings
footer are added by the organizers. http://creativecommons.org/licenses/by/4.0/
1
Formally, trees are a restricted form of graphs, where every node is reachable from a distinguished root node by exactly
one directed path.
90
? h
1
,
h
4
:_a_q(x
6
, h
7
, h
5
), h
8
:_similar_a_to(e
9
, x
6
), h
8
:comp(e
11
, e
9
, ), h
8
:_technique_n_1(x
6
),
h
2
:_almost_a_1(e
12
, h
13
), h
14
:_impossible_a_for(e
3
, h
15
, i
16
),
h
17
:_apply_v_to(e
18
, i
19
, x
6
, x
20
), h
21
:udef_q(x
20
, h
22
, h
23
), h
24
:_other_a_1(e
25
, x
20
), h
24
:_crop_n_1(x
20
),
h
24
:_such+as_p(e
26
, x
20
, x
27
), h
40
:implicit_conj(x
27
, x
33
, x
38
),
h
31
:udef_q(x
33
, h
32
, h
34
), h
35
:_cotton_n_1(x
33
), h
46
:_and_c(x
38
, x
43
, x
47
),
h
41
:udef_q(x
43
, h
42
, h
44
), h
45
:_soybean_u_unknown(x
43
), h
48
:udef_q(x
47
, h
49
, h
50
), h
51
:_rice_n_1(x
47
)
{ h
49
=
q
h
51
, h
42
=
q
h
45
, h
32
=
q
h
35
, h
22
=
q
h
24
, h
15
=
q
h
17
, h
13
=
q
h
14
, h
7
=
q
h
8
, h
1
=
q
h
2
} ?
Figure 1: Example logical form meaning representation (MRS; taken from DeepBank).
2 Technology: Core Components
Our system architecture comprises two core components, viz. (a) the RDF repository, a database storing
semantic networks in RDF triple form, and (b) the Web application, an interface for interactive search
and visualization over the RDF repository.
Representing Semantic Graphs in RDF The RDF data model is based on statements about resources
in the form of subject?predicate?object triples. The subject denotes the resource, and the predicate
denotes traits or aspects of the resource, thus expressing a relationship between the subject and the
object. A database that can store such expression and evaluate queries to them is called a triple store.
In Kouylekov and Oepen (2014), we describe the conversion of different types of semantic struc-
tures into RDF graphs. To date, we have addressed three types of meaning representations, viz. (in de-
creasing complexity) (a) scope-underspecified logical formulas in Minimal Recursion Semantics (MRS;
Copestake et al., 2005); (b) variable-free Elementary Dependency Structures (EDS; Oepen and L?nning,
2006); and (c) bi-lexical dependency graphs as used in Task 8 at SemEval 2014 on Broad-Coverage
Semantic Dependency Parsing (SDP; Oepen et al., 2014; Ivanova et al., 2012). For all three formats, we
draw on (a) gold-standard annotations from DeepBank (Flickinger et al., 2012), a re-annotation of the
venerable Penn Treebank WSJ Corpus (Marcus et al., 1993); and on (b) much larger collections of auto-
matically generated analyses over the full English Wikipedia from the WikiWoods Treecache (Flickinger
et al., 2010).
To store MRS, EDS, and SDP structures, we created small ontologies for each type of representation,
building on a common core of shared ontology elements. In a nutshell, the EDS and SDP ontologies
provide a generic representation of directed graphs with (potentially complex) node and edge labels;
the dependencies proper, i.e. labeled arcs of the graph, are encoded as RDF object properties. The
MRS ontology, on the other hand, distinguishes different types of nodes, corresponding to full predica-
tions vs. individual logical variables vs. hierarchically organized sub-properties of variables. Mapping
the (medium-complexity) EDS graphs from DeepBank and WikiWoods onto RDF, for example, yields
around 12 million and 4.3 billion triples, respectively (for the semantic dependencies of about 37 thou-
sand and 48 million sentences in the two resources).
Web Application The core of our Web application is a search engine that executes SPARQL queries
against the RDF repository. SPARQL is an RDF query language to search triple stores, allowing one to
retrieve and manipulate RDF data. It is fully standardized and considered one of the key technologies
of the Semantic Web. A SPARQL query can consist of triple patterns, conjunctions, disjunctions, and
optional filters and functions. The query processor searches for sets of triples that match the patterns
expressed in the query, binding variables in the query to the corresponding parts of each triple.
A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice.
top
ARG2
ARG3 ARG1
ARG2 _and_cARG1 mweARG1
BV
ARG1 conjARG1
Figure 2: Example bi-lexical semantic dependencies (SDP; taken from DeepBank).
91
PREFIX sdp:<http://wesearch.delph-in.net/rdf/sdp#>
PREFIX dm:<http://wesearch.delph-in.net/rdf/sdp/dm#>
select ?graph
where {
GRAPH ?graph {
?101 sdp:form "quarterly"^^xsd:string .
?x dm:lemma "result"^^xsd:string .
{
?100 dm:pos "vbp"^^xsd:string
UNION ?100 dm:pos "vbg"^^xsd:string
UNION ...
}
?101 dm:arg1 ?x .
{
?100 dm:arg1 ?x UNION ?100 dm:arg2 ?x
UNION ?100 dm:arg3 ?x UNION ?100 dm:arg4
}
FILTER
((!bound(?101) || !bound(?100) || ?101 != ?100)
&& (!bound(?101) || !bound(?x) || ?101 != ?x)
&& (!bound(?100) || !bound(?x) || ?100 != ?x))
}
}
GROUP BY ?graph
ORDER BY ?graph
Figure 3: Core of the auto-generated SPARQL query corresponding to our running example.
Our infrastructure supports the definition of families of ?meta? query languages, to address semantic
structures in a form that is more compact and much better adapted to the specific target format than
SPARQL. An example of such a ?designer? language is the WeSearch Query Language (WQL), which
was used in the context of the SemEval 2014 SDP task.
2
By way of informal introduction, consider the
following example query:
(1) /v
*
[ARG
*
x]
quarterly[ARG1 x]
x:+result
This example is comprised of three predications, one per line. The following characters have operator
status: ?/? (slash), ?
*
? (asterisk), ?[? and ?]? (left and right square bracket), ?:? (colon), and ?+? (plus
sign). This is a near-complete list of operator characters in WQL. Each predication can be composed of
(i) an identifier, followed by a colon if present; (ii) a form pattern; (iii) a lemma pattern, prefixed by a
plus sign, if present; (iv) a part-of-speech (PoS) pattern, prefixed by a slash, if present; and (v) a list of
arguments, enclosed in square brackets, if present. Patterns can make use of Lucene-style wildcards, with
the asterisk matching any number of characters, and a question mark (???) to match a single character.
Thus, our example query searches for a verbal predicate (any PoS tag starting with ?v?), that takes
any form of the lemma ?result? as its argument (in the range ARG
1
. . . ARG
n
), where this argument is
further required to be the ARG
1
of a node labeled ?quaterly?.
The auto-generated SPARQL expression that corresponds to this example query is shown in Figure
3. The query generator replaces the wildcarded PoS pattern by the union of all matching tags (that start
with ?v?, e.g. ??100 dm:pos "vbp" UNION ...? Likewise, the underspecified argument relation
of this predication is replaced by the union of all possible argument types. Finally, the query processor
ensures a one-to-one correspondence between query elements and matching graph elements, i.e. multiple
distinct query components cannot match against the same target (graph component), or vice versa. This
is accomplished in SPARQL through the filter expressions towards the end of the generated query.
2
See http://wesearch.delph-in.net/sdp/ for an on-line demonstration and additional documentation.
92
Figure 4: Screenshot of the interactive search interface, querying (semantic) object control structures.
Figure 4 shows a screenshot from the SemEval SDP user interface, demonstrating how WQL facilitates
concise (and reasonably transparent) search for semantic ?object? control, i.e. a configuration involving
two predicates sharing an argument in a specific assignment of roles.
Within the capabilities of the SPARQL back-end, different dialects of the meta query language can be
implemented in a modular fashion, for example distinguishing different types of nodes and introducing
additional node properties, as in the more complex MRS universe. Our query front-end transforms ?meta?
queries into equivalent SPARQL expressions, and the search interface allows users to inspect the result
of this transformation (and matching results), to possibly refine the search incrementally either at the
?meta? query layer or directly in SPARQL.
3 Demonstration: Indexing and Search
Our proposed interactive demonstration will seek to highlight (a) the flexibility of our infrastructure, i.e.
walk through a series of queries of increasing complexity against different target formats; (b) its scala-
bility, by comparing response times for different types of queries and different target formats over the
large DeepBank and the vast WikiWoods indexes; and (c) the ease of ?behind the scenes? functionality,
showing how additional semantic annotations in various formats can be ingested into the index. As part
of this latter aspect of the demonstration, we will optionally discuss how we apply string-level index-
ing (in Apache Lucene) and basic frequency statistics in query interpretation and optimization, which
jointly with parallelization over ?striped? RDF triple stores can yield greatly reduced response times for
common types of queries to the WikiWoods index. We envision that parts of the demonstration can be
organized in an audience-driven manner, for example taking as input the (possibly informal) charac-
terization of a semantic configuration, collectively transforming it into a query against our DeepBank
or WikiWoods stores, observing linguistic or technical properties of matching results, and refining the
search incrementally.
Our software infrastructure is entirely open-source and (increasingly) modularized and parameterized
to facilitate adaptation to additional types of annotation. Please see the project web page for licensing
and access information, as well as for pointers to a variety of existing on-line demonstrations:




	
http://wesearch.delph-in.net/
93
References
Copestake, A., Flickinger, D., Pollard, C., and Sag, I. A. (2005). Minimal Recursion Semantics. An
introduction. Research on Language and Computation, 3(4), 281 ? 332.
Flickinger, D., Oepen, S., and Ytrest?l, G. (2010). WikiWoods. Syntacto-semantic annotation for En-
glish Wikipedia. In Proceedings of the 7th International Conference on Language Resources and
Evaluation. Valletta, Malta.
Flickinger, D., Zhang, Y., and Kordoni, V. (2012). DeepBank. A dynamically annotated treebank of the
Wall Street Journal. In Proceedings of the 11th International Workshop on Treebanks and Linguistic
Theories (p. 85 ? 96). Lisbon, Portugal: Edi??es Colibri.
Ivanova, A., Oepen, S., ?vrelid, L., and Flickinger, D. (2012). Who did what to whom? A contrastive
study of syntacto-semantic dependencies. In Proceedings of the Sixth Linguistic Annotation Workshop
(p. 2 ? 11). Jeju, Republic of Korea.
Kouylekov, M., and Oepen, S. (2014). Semantic technologies for querying linguistic annotations. An
experiment focusing on graph-structured data. In Proceedings of the 9th International Conference on
Language Resources and Evaluation. Reykjavik, Iceland.
Marcus, M., Santorini, B., and Marcinkiewicz, M. A. (1993). Building a large annotated corpora of
English: The Penn Treebank. Computational Linguistics, 19, 313 ? 330.
Oepen, S., Kuhlmann, M., Miyao, Y., Zeman, D., Flickinger, D., Haji
?
c, J., . . . Zhang, Y. (2014). SemEval
2014 Task 8. Broad-coverage semantic dependency parsing. In Proceedings of the 8th International
Workshop on Semantic Evaluation. Dublin, Ireland.
Oepen, S., and L?nning, J. T. (2006). Discriminant-based MRS banking. In Proceedings of the 5th
International Conference on Language Resources and Evaluation (p. 1250 ? 1255). Genoa, Italy.
94
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 397?408,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Parser Evaluation over
Local and Non-Local Deep Dependencies in a Large Corpus
Emily M. Bender?, Dan Flickinger?, Stephan Oepen?, Yi Zhang?
?Dept of Linguistics, University of Washington, ?CSLI, Stanford University
?Dept of Informatics, Universitetet i Oslo, ?Dept of Computational Linguistics, Saarland University
ebender@uw.edu, danf@stanford.edu, oe@ifi.uio.no, yzhang@coli.uni-sb.de
Abstract
In order to obtain a fine-grained evaluation of
parser accuracy over naturally occurring text,
we study 100 examples each of ten reason-
ably frequent linguistic phenomena, randomly
selected from a parsed version of the En-
glish Wikipedia. We construct a correspond-
ing set of gold-standard target dependencies
for these 1000 sentences, operationalize map-
pings to these targets from seven state-of-the-
art parsers, and evaluate the parsers against
this data to measure their level of success in
identifying these dependencies.
1 Introduction
The terms ?deep? and ?shallow? are frequently used
to characterize or contrast different approaches to
parsing. Inevitably, such informal notions lack a
clear definition, and there is little evidence of com-
munity consensus on the relevant dimension(s) of
depth, let alne agreement on applicable metrics. At
its core, the implied dichotomy of approaches al-
ludes to differences in the interpretation of the pars-
ing task. Its abstract goal, on the one hand, could
be pre-processing of the linguistic signal, to enable
subsequent stages of analysis. On the other hand, it
could be making explicit the (complete) contribution
that the grammatical form of the linguistic signal
makes to interpretation, working out who did what
to whom. Stereotypically, one expects correspond-
ing differences in the choice of interface representa-
tions, ranging from various levels of syntactic anal-
ysis to logical-form representations of semantics.
In this paper, we seek to probe aspects of variation
in automated linguistic analysis. We make the as-
sumption that an integral part of many (albeit not all)
applications of parsing technology is the recovery of
structural relations, i.e. dependencies at the level of
interpretation. We suggest a selection of ten linguis-
tic phenomena that we believe (a) occur with reason-
ably high frequency in running text and (b) have the
potential to shed some light on the depths of linguis-
tic analysis. We quantify the frequency of these con-
structions in the English Wikipedia, then annotate
100 example sentences for each phenomenon with
gold-standard dependencies reflecting core proper-
ties of the phenomena of interest. This gold standard
is then used to estimate the recall of these dependen-
cies by seven commonly used parsers, providing the
basis for a qualitative discussion of the state of the
art in parsing for English.
In this work, we answer the call by Rimell et
al. (2009) for ?construction-focused parser evalua-
tion?, extending and complementing their work in
several respects: (i) we investigate both local and
non-local dependencies which prove to be challeng-
ing for many existing state-of-the-art parsers; (ii) we
investigate a wider range of linguistic phenomena,
each accompanied with an in-depth discussion of
relevant properties; and (iii) we draw our data from
the 50-million sentence English Wikipedia, which
is more varied and a thousand times larger than the
venerable WSJ corpus, to explore a more level and
ambitious playing field for parser comparison.
2 Background
All parsing systems embody knowledge about possi-
ble and probable pairings of strings and correspond-
ing linguistic structure. Such linguistic and proba-
bilistic knowledge can be hand-coded (e.g., as gram-
mar rules) or automatically acquired from labeled or
397
unlabeled training data. A related dimension of vari-
ation is the type of representations manipulated by
the parser. We briefly review some representative
examples along these dimensions, as these help to
position the parsers we subsequently evaluate.1
2.1 Approaches to parsing
Source of linguistic knowledge At one end of this
dimension, we find systems whose linguistic knowl-
edge is encoded in hand-crafted rules and lexical en-
tries; for English, the ParGram XLE system (Rie-
zler et al, 2002) and DELPH-IN English Resource
Grammar (ERG; Flickinger (2000))?each reflect-
ing decades of continuous development?achieve
broad coverage of open-domain running text, for ex-
ample. At the other end of this dimension, we find
fully unsupervised approaches (Clark, 2001; Klein
and Manning, 2004), where the primary source of
linguistic knowledge is co-occurrence patterns of
words in unannotated text. As Haghighi and Klein
(2006) show, augmenting this knowledge with hand-
crafted prototype ?seeds? can bring strong improve-
ments. Somewhere between these poles, a broad
class of parsers take some or all of their linguistic
knowledge from annotated treebanks, e.g. the Penn
Treebank (PTB), which encodes ?surface grammati-
cal analysis? (Marcus et al, 1993). Such approaches
include those that directly (and exclusively) use the
information in the treebank (e.g. Charniak (1997),
Collins (1999), Petrov et al (2006), inter alios) as
well as those that complement treebank structures
with some amount of hand-coded linguistic knowl-
edge (e.g. O?Donovan et al (2004), Miyao et al
(2004), Hockenmaier and Steedman (2007), inter
alios). Another hybrid in terms of its acquisition of
linguistic knowledge is the RASP system of Briscoe
et al (2006), combining a hand-coded grammar over
PoS tag sequences with a probabilistic tagger and
statistical syntactic disambiguation.
Design of representations Approaches to parsing
also differ fundamentally in the style of represen-
tation assigned to strings. These vary both in their
1Additional sources of variation among extant parsing tech-
nologies include (a) the behavior with respect to ungrammatical
inputs and (b) the relationship between probabilistic and sym-
bolic knowledge in the parser, where parsers with a hand-coded
grammar at their core typically also incorporate an automati-
cally trained probabilistic disambiguation component.
formal nature and the ?granularity? of linguistic in-
formation (i.e. the number of distinctions assumed),
encompassing variants of constituent structure, syn-
tactic dependencies, or logical-form representations
of semantics. Parser interface representations range
between the relatively simple (e.g. phrase structure
trees with a limited vocabulary of node labels as in
the PTB, or syntactic dependency structures with a
limited vocabulary of relation labels as in Johansson
and Nugues (2007)) and the relatively complex, as
for example elaborate syntactico-semantic analyses
produced by the ParGram or DELPH-IN grammars.
There tends to be a correlation between the
methodology used in the acquisition of linguistic
knowledge and the complexity of representations: in
the creation of a mostly hand-crafted treebank like
the PTB, representations have to be simple enough
for human annotators to reliably manipulate. Deriv-
ing more complex representations typically presup-
poses further computational support, often involv-
ing some hand-crafted linguistic knowledge?which
can take the form of mappings from PTB-like repre-
sentations to ?richer? grammatical frameworks (as
in the line of work by O?Donovan et al (2004), and
others; see above), or can be rules for creating the
parse structures in the first place (i.e. a computa-
tional grammar), as for example in the treebanks of
van der Beek et al (2002) or Oepen et al (2004).2
In principle, one might expect that richer repre-
sentations allow parsers to capture complex syntac-
tic or semantic dependencies more explicitly. At the
same time, such ?deeper? relations may still be re-
coverable (to some degree) from comparatively sim-
ple parser outputs, as demonstrated for unbounded
dependency extraction from strictly local syntactic
dependency trees by Nivre et al (2010).
2.2 An armada of parsers
Stanford Parser (Klein and Manning, 2003) is a
probabilistic parser which can produce both phrase
structure trees and grammatical relations (syntactic
dependencies). The parsing model we evaluate is the
2A noteworthy exception to this correlation is the annotated
corpus of Zettlemoyer and Collins (2005), which pairs sur-
face strings from the realm of natural language database inter-
faces directly with semantic representations in lambda calculus.
These were hand-written on the basis of database query state-
ments distributed with the original datasets.
398
English factored model which combines the prefer-
ences of unlexicalized PCFG phrase structures and
of lexical dependencies, trained on sections 02?21
of the WSJ portion of the PTB. We chose Stanford
Parser from among the state-of-the-art PTB-derived
parsers for its support for grammatical relations as
an alternate interface representation.
Charniak&Johnson Reranking Parser (Char-
niak and Johnson, 2005) is a two-stage PCFG parser
with a lexicalized generative model for the first-
stage, and a discriminative MaxEnt reranker for the
second-stage. The models we evaluate are also
trained on sections 02?21 of the WSJ. Top-50 read-
ings were used for the reranking stage. The output
constituent trees were then converted into Stanford
Dependencies. According to Cer et al (2010), this
combination gives the best parsing accuracy in terms
of Stanford dependencies on the PTB.
Enju (Miyao et al, 2004) is a probabilistic HPSG
parser, combining a hand-crafted core grammar with
automatically acquired lexical types from the PTB.3
The model we evaluate is trained on the same ma-
terial from the WSJ sections of the PTB, but the
treebank is first semi-automatically converted into
HPSG derivations, and the annotation is enriched
with typed feature structures for each constituent.
In addition to HPSG derivation trees, Enju also pro-
duces predicate argument structures.
C&C (Clark and Curran, 2007) is a statistical
CCG parser. Abstractly similar to the approach of
Enju, the grammar and lexicon are automatically
induced from CCGBank (Hockenmaier and Steed-
man, 2007), a largely automatic projection of (the
WSJ portion of) PTB trees into the CCG framework.
In addition to CCG derivations, the C&C parser can
directly output a variant of grammatical relations.
RASP (Briscoe et al, 2006) is an unlexicalized
robust parsing system, with a hand-crafted ?tag se-
quence? grammar at its core. The parser thus anal-
yses a lattice of PoS tags, building a parse forest
from which the most probable syntactic trees and
sets of corresponding grammatical relations can be
extracted. Unlike other parsers in our mix, RASP
did not build on PTB data in either its PoS tagging
3This hand-crafted grammar is distinct from the ERG, de-
spite sharing the general framework of HPSG. The ERG is not
included in our evaluation, since it was used in the extraction of
the original examples and thus cannot be fairly evaluated.
or syntactic disambiguation components.
MSTParser (McDonald et al, 2005) is a data-
driven dependency parser. The parser uses an edge-
factored model and searches for a maximal span-
ning tree that connects all the words in a sentence
into a dependency tree. The model we evaluate
is the second-order projective model trained on the
same WSJ corpus, where the original PTB phrase
structure annotations were first converted into de-
pendencies, as established in the CoNLL shared task
2009 (Johansson and Nugues, 2007).
XLE/ParGram (Riezler et al, 2002, see also
Cahill et al, 2008) applies a hand-built Lexical
Functional Grammar for English and a stochastic
parse selection model. For our evaluation, we used
the Nov 4, 2010 release of XLE and the Nov 25,
2009 release of the ParGram English grammar, with
c-structure pruning turned off and resource limita-
tions set to the maximum possible to allow for ex-
haustive search. In particular, we are evaluating the
f-structures output by the system.
Each parser, of course, has its own requirements
regarding preprocessing of text, especially tokeniza-
tion. We customized the tokenization to each parser,
by using the parser?s own internal tokenization or
pre-tokenizing to match the parser?s desired input.
The evaluation script is robust to variations in tok-
enization across parsers.
3 Phenomena
In this section we summarize the ten phenomena we
explore and our motivations for choosing them. Our
goal was to find phenomena where the relevant de-
pendencies are relatively subtle, such that more lin-
guistic knowledge is beneficial in order to retrieve
them. Though this set is of course only a sampling,
these phenomena illustrate the richness of structure,
both local and non-local, involved in the mapping
from English strings to their meanings. We discuss
the phenomena in four sets and then briefly review
their representation in the Penn Treebank.
3.1 Long distance dependencies
Three of our phenomena can be classified as involv-
ing long-distance dependencies: finite that-less rel-
atives clauses (?barerel?), tough adjectives (?tough?)
and right node raising (?rnr?). These are illustrated
399
in the following examples:4
(1) barerel: This is the second time in a row Aus-
tralia has lost their home tri-nations? series.
(2) tough: Original copies are very hard to find.
(3) rnr: Ilu?vatar, as his names imply, exists before
and independently of all else.
While the majority of our phenomena involve lo-
cal dependencies, we include these long-distance
dependency types because they are challenging for
parsers and enable more direct comparison with the
work of Rimell et al (2009), who also address right
node raising and bare relatives. Our barerel category
corresponds to their ?object reduced relative? cate-
gory with the difference that we also include adverb
relatives, where the head noun functions as a modi-
fier within the relative clause, as does time in (1). In
contrast, our rnr category is somewhat narrower than
Rimell et al (2009)?s ?right node raising? category:
where they include raised modifiers, we restrict our
category to raised complements.
Part of the difficulty in retrieving long-distance
dependencies is that the so-called extraction site is
not overtly marked in the string. In addition to this
baseline level of complication, these three construc-
tion types present further difficulties: Bare relatives,
unlike other relative clauses, do not carry any lexi-
cal cues to their presence (i.e., no relative pronouns).
Tough adjective constructions require the presence
of specific lexical items which form a subset of a
larger open class. They are rendered more difficult
by two sources of ambiguity: alternative subcatego-
rization frames for the adjectives and the purposive
adjunct analysis (akin to in order to) for the infiniti-
val VP. Finally, right node raising often involves co-
ordination where one of the conjuncts is in fact not
a well-formed phrase (e.g., independently of in (3)),
making it potentially difficult to construct the correct
coordination structure, let alne associate the raised
element with the correct position in each conjunct.
3.2 Non-dependencies
Two of our phenomena crucially look for the lack of
dependencies. These are it expletives (?itexpl?) and
verb-particle constructions (?vpart?):
4All examples are from our data. Words involved in the rel-
evant dependencies are highlighted in italics (dependents) and
boldface (heads).
(4) itexpl: Crew negligence is blamed, and it is sug-
gested that the flight crew were drunk.
(5) vpart: He once threw out two baserunners at
home in the same inning.
The English pronoun it can be used as an ordi-
nary personal pronoun or as an expletive: a place-
holder for when the language demands a subject (or
occasionally object) NP but there is no semantic role
for that NP. The expletive it only appears when it
is licensed by a specific construction (such as ex-
traposition, (4)) or selecting head. If the goal of
parsing is to recover from the surface string the de-
pendencies capturing who did what to whom, exple-
tive it should not feature in any of those dependen-
cies. Likewise, instances of expletive it should be
detected and discarded in reference resolution. We
hypothesize that detecting expletive it requires en-
coding linguistic knowledge about its licensers.
The other non-dependency we explore is between
the particle in verb-particle constructions and the
direct object. Since English particles are almost
always homophonous with prepositions, when the
object of the verb-particle pair follows the par-
ticle, there will always be a competing analysis
which analyses the sequence as V+PP rather than
V+particle+NP. Furthermore, since verb-particle
pairs often have non-compositional semantics (Sag
et al, 2002), misanalyzing these constructions could
be costly to downstream components.
3.3 Phrasal modifiers
Our next category concerns modifier phrases:
(6) ned: Light colored glazes also have softening
effects when painted over dark or bright images.
(7) absol: The format consisted of 12 games, each
team facing the other teams twice.
The first, (?ned?), is a pattern which to our knowl-
edge has not been named in the literature, where a
noun takes the typically verbal -ed ending, is modi-
fied by another noun or adjective, and functions as a
modifier or a predicate. We believe this phenomenon
to be interesting because its unusual morphology is
likely to lead PoS-taggers astray, and because the
often-hyphenated Adj+N-ed constituent has produc-
tive internal structure constraining its interpretation.
The second phrasal modifier we investigate is the
absolutive construction. An absolutive consists of an
400
NP followed by a non-finite predicate (such as could
appear after the copula be). The whole phrase mod-
ifies a verbal projection that it attaches to. Absolu-
tives may be marked with with or unmarked. Here,
we focus on the unmarked type as this lack of lexical
cue can make the construction harder to detect.
3.4 Subtle arguments
Our final three phenomena involve ways in which
verbal arguments can be more difficult to identify
than in ordinary finite clauses. These include de-
tecting the arguments of verbal gerunds (?vger?), the
interleaving of arguments and adjuncts (?argadj?) and
raising/control (?control?) constructions.
(8) vger: Accessing the website without the ?www?
subdomain returned a copy of the main site for
?EP.net?.
(9) argadj: The story shows, through flashbacks, the
different histories of the characters.
(10) control: Alfred ?retired? in 1957 at age 60 but
continued to paint full time.
In a verbal gerund, the -ing form a verb retains
verbal properties (e.g., being able to take NP com-
plements, rather than only PP complements) but
heads a phrase that fills an NP position in the syn-
tax (Malouf, 2000). Since gerunds have the same
morphology as present participle VPs, their role in
the larger clause is susceptible to misanalysis.
The argadj examples are of interest because En-
glish typically prefers to have direct objects directly
adjacent to the selecting verb. Nonetheless, phe-
nomena such as parentheticals and heavy-NP shift
(Arnold et al, 2000), in which ?heavy? constituents
appear further to the right in the string, allow for
adjunct-argument order in a minority of cases. We
hypothesize that the relative infrequency of this con-
struction will lead parsers to prefer incorrect analy-
ses (wherein the adjunct is picked up as a comple-
ment, the complement as an adjunct or the structure
differs entirely) unless they have access to linguis-
tic knowledge providing constraints on possible and
probable complementation patterns for the head.
Finally, we turn to raising and control verbs (?con-
trol?) (e.g., Huddleston and Pullum (2002, ch. 14)).
These verbs select for an infinitival VP complement
and stipulate that another of their arguments (sub-
ject or direct object in the examples we explore) is
identified with the unrealized subject position of the
infinitival VP. Here it is the dependency between
the infinitval VP and the NP argument of the ?up-
stairs? verb which we expect to be particularly sub-
tle. Getting this right requires specific lexical knowl-
edge about which verbs take these complementation
patterns. This lexical knowledge needs to be repre-
sented in such a way that it can be used robustly even
in the case of passives, relative clauses, etc.5
3.5 Penn Treebank representations
We investigated the representation of these 10 phe-
nomena in the PTB (Marcus et al, 1993) in two
steps: First we explored the PTB?s annotation guide-
lines (Bies et al, 1995) to determine how the rele-
vant dependencies were intended to be represented.
We then used Ghodke and Bird?s (2010) Treebank
Search to find examples of the intended annotations
as well as potential examples of the phenomena an-
notated differently, to get a sense of the consistency
of the annotation from both precision and recall per-
spectives. In this study, we take the phrase structure
trees of the PTB to represent dependencies based on
reasonable identification of heads.
The barerel, vpart, and absol phenomena are com-
pletely unproblematic, with their relevant dependen-
cies explicitly and reliably represented. In addition,
the tough construction is reliably annotated, though
one of the dependencies we take to be central is not
directly represented: The missing argument is linked
to a null wh head at the left edge of the comple-
ment of the tough predicate, rather than to its sub-
ject. Two further phenomena (rnr and vger) are es-
sentially correctly represented: the representations
of the dependencies are explicit and mostly but not
entirely consistently applied. Two out of a sample of
20 examples annotated as containing rnr did not, and
two out of a sample of 35 non-rnr-annotated coordi-
nations actually contained rnr. For vger the primary
problem is with the PoS tagging, where the gerund
is sometimes given a nominal tag, contrary to PTB
guidelines, though the structure above it conforms.
The remaining four constructions are more prob-
lematic. In the case of object control, while the guide-
5Distinguishing between raising and control requires fur-
ther lexical knowledge and is another example of a ?non-
dependency? (in the raising examples). We do not draw that
distinction in our annotations.
401
lines specify an analysis in which the shared NP is
attached as the object of the higher verb, the PTB
includes not only structures conforming to that anal-
ysis but also ?small clause? structures, with the latter
obscuring the relationship of the shared argument to
the higher verb. In the case of itexpl, the adjoined
(S(-NONE- *EXP*)) indicating an expletive use of
it is applied consistently for extraposition (as pre-
scribed in the guidelines). However, the set of lex-
ical licensers of the expletive is incomplete. For ar-
gadj we run into the problem that the PTB does not
explicitly distinguish between post-verbal modifiers
and verbal complements in the way that they are at-
tached. The guidelines suggest that the function tags
(e.g., PP-LOC, etc.) should allow one to distinguish
these, but examination of the PTB itself suggests
that they are not consistently applied. Finally, the
ned construction is not mentioned in the PTB guide-
lines nor is its internal structure represented in the
treebank. Rather, strings like gritty-eyed are left un-
segmented and tagged as JJ.
We note that the PTB representations of many of
these phenomena (barerel, tough, rnr, argadj, control,
itexpl) involve empty elements and/or function tags.
Systems that strip these out before training, as is
common practice, will not benefit from the informa-
tion that is in the PTB.
Our purpose here is not to criticize the PTB,
which has been a tremendously important resource
to the field. Rather, we have two aims: The first is
to provide context for the evaluation of PTB-derived
parsers on these phenomena. The second is to high-
light the difficulty of producing consistent annota-
tions of any complexity as well as the hurdles faced
by a hand-annotation approach when attempting to
scale a resource to more complex representations
and/or additional phenomena (though cf. Vadas and
Curran (2008) on improving PTB representations).
4 Methodology
4.1 Data extraction
We processed 900 million tokens of Wikipedia text
using the October 2010 release of the ERG, follow-
ing the work of the WikiWoods project (Flickinger
et al, 2010). Using the top-ranked ERG deriva-
tion trees as annotations over this corpus and sim-
ple patterns using names of ERG-specific construc-
Phenomenon Frequency Candidates
barerel 2.12% 546
tough 0.07% 175
rnr 0.69% 1263
itexpl 0.13% 402
vpart 4.07% 765
ned 1.18% 349
absol 0.51% 963
vger 5.16% 679
argadj 3.60% 1346
control 3.78% 124
Table 1: Relative frequencies of phenomena matches in
Wikipedia, and number of candidate strings vetted.
tions or lexical types, we randomly selected a set
of candidate sentences for each of our ten phenom-
ena. These candidates were then hand-vetted in se-
quence by two annotators to identify, for each phe-
nomenon, 100 examples that do in fact involve the
phenomenon in question and which are both gram-
matical and free of typos. Examples that were ei-
ther deemed overly basic (e.g. plain V+V coordi-
nation, which the ERG treats as rnr) or inappropri-
ately complex (e.g. non-constituent coordination ob-
scuring the interleaving of arguments and adjuncts)
were also discarded at this step. Table 1 summarizes
relative frequencies of each phenomenon in about
47 million parsed Wikipedia sentences, as well as
the total size of the candidate sets inspected. For
the control and tough phenomena hardly any filtering
for complexity was applied, hence these can serve
as indicators of the rate of genuine false positives.
For phenomena that partially overlap with those of
Rimell et al (2009), it appears our frequency es-
timates are comparable to what they report for the
Brown Corpus (but not the WSJ portion of the PTB).
4.2 Annotation format
We annotated up to two dependency triples per phe-
nomenon instance, identifying the heads and depen-
dents by the surface form of the head words in the
sentence suffixed with a number indicating word po-
sition (see Table 2).6 Some strings contain more
than one instance of the phenomenon they illustrate;
in these cases, multiple sets of dependencies are
6As the parsers differ in tokenization strategies, our evalua-
tion script treats these position IDs as approximate indicators.
402
Item ID Phenomenon Polarity Dependency
1011079100200 absol 1 having-2|been-3|passed-4 ARG act-1
1011079100200 absol 1 withdrew-9 MOD having-2|been-3|passed-4
1011079100200 absol 1 carried+on-12 MOD having-2|been-3|passed-4
Table 2: Sample annotations for sentence # 1011079100200: The-0 act-1 having-2 been-3 passed-4 in-5 that-6 year-7
Jessop-8 withdrew-9 and-10 Whitworth-11 carried-12 on-13 with-14 the-15 assistance-16 of-17 his-18 son-19.
Phenomenon Head Type Dependent Distance
Bare relatives gapped predicate in relative ARG2/MOD modified noun 3.0 (8)
(barerel) modified noun MOD top predicate of relative 3.3 (8)
Tough adjectives tough adjective ARG2 to-VP complement 1.7 (5)
(tough) gapped predicate in to-VP ARG2 subject/modifiee of adjective 6.4 (21)
Right Node Raising verb/prep2 ARG2 shared noun 2.8 (9)
(rnr) verb/prep1 ARG2 shared noun 6.1 (12)
Expletive It it-subject taking verb !ARG1 it 1.2 (3)
(itexpl) raising-to-object verb !ARG2 it ?
Verb+particle constructions particle !ARG2 complement 2.7 (9)
(vpart) verb+particle ARG2 complement 3.7 (10)
Adj/Noun2 + Noun1-ed head noun MOD Noun1-ed 2.4 (17)
(ned) Noun1-ed ARG1/MOD Adj/Noun2 1.0 (1.5)
Absolutives absolutive predicate ARG1 subject of absolutive 1.7 (12)
(absol) main clause predicate MOD absolutive predicate 9.8 (26)
Verbal gerunds selecting head ARG[1,2] gerund 1.9 (13)
(vger) gerund ARG2/MOD first complement/modifier of gerund 2.3 (8)
Interleaved arg/adj selecting verb MOD interleaved adjunct 1.2 (7)
(argadj) selecting verb ARG[2,3] displaced complement 5.9 (26)
Control ?upstairs? verb ARG[2,3] ?downstairs? verb 2.4 (23)
(control) ?downstairs? verb ARG1 shared argument 4.8 (17)
Table 3: Dependencies labeled for each phenomenon type, including average and maximum surface distances.
recorded. In addition, some strings evince more than
one of the phenomena we are studying. However,
we only annotate the dependencies associated with
the phenomenon the string was selected to repre-
sent. Finally, in examples with coordinated heads or
dependents, we recorded separate dependencies for
each conjunct. In total, we annotated 2127 depen-
dency triples for the 1000 sentences, including 253
negative dependencies (see below). Table 3 outlines
the dependencies annotated for each phenomenon.
To allow for multiple plausible attachment sites,
we give disjunctive values for heads or dependents
in several cases: (i) with auxiliaries, (ii) with com-
plementizers (that or to, as in Table 2), (iii) in cases
of measure or classifier nouns or partitives, (iv) with
multi-word proper names and (v) where there is
genuine attachment ambiguity for modifiers. As
these sets of targets are disjunctive, these conven-
tions should have the effect of increasing measured
parser performance. 580 (27%) of the annotated de-
pendencies had at least one disjunction.
4.3 Annotation and reconciliation process
The entire data set was annotated independently by
two annotators. Both annotators were familiar with
the ERG, used to identify these sentences in the
WikiWoods corpus, but the annotation was done
without reference to the ERG parses. Before begin-
ning annotation on each phenomenon, we agreed on
which dependencies to annotate. We also communi-
cated with each other about annotation conventions
as the need for each convention became clear. The
annotation conventions address how to handle co-
ordination, semantically empty auxiliaries, passives
and similar orthogonal phenomena.
Once the entire data set was dual-annotated, we
compared annotations, identifying the following
sources of mismatch: typographical errors, incom-
pletely specified annotation conventions, inconsis-
tent application of conventions (101 items, dropping
in frequency as the annotation proceeded), and gen-
uine disagreement about what to annotate, either dif-
ferent numbers of dependencies of interest identified
403
in an item (59 items) or conflicting elements in a de-
pendency (54 items).7 Overall, our initial annotation
pass led to agreement on 79% of the items, and a
higher per-dependency level of agreement. Agree-
ment could be expected to approach 90% with more
experience in applying annotation conventions.
We then reconciled the annotations, using the
comparison to address all sources of difference. In
most cases, we readily agreed which annotation was
correct and which was in error. In a few cases, we
decided that both annotations were plausible alter-
natives (e.g., in terms of alternative attachment sites
for modifiers) and so created a single merged anno-
tation expressing the disjunction of both (cf. ? 4.2).
5 Evaluation
With the test data consisting of 100 items for each of
our ten selected phenomena, we ran all seven pars-
ing systems and recorded their dependency-style
outputs for each sentence. While these outputs
are not directly comparable with each other, we
were able to associate our manually-annotated tar-
get dependencies with parser-specific dependencies,
by defining sets of phenomenon-specific regular ex-
pressions for each parser. In principle, we allow this
mapping to be somewhat complex (and forgiving to
non-contentful variation), though we require that it
work deterministically and not involve specific lexi-
cal information. An example set is given in Fig. 2.
"absol" =>
{?ARG1? => [
?\(ncsubj \W*{W1}\W*_(\d+) \W*{W2}\W*_(\d+) _\)?,
?\(ncmod _ \W*{W2}\W*_(\d+) \W*{W1}\W*_(\d+)\)?],
?ARG? => [
?\(ncsubj \W*{W1}\W*_(\d+) \W*{W2}\W*_(\d+) _\)?,
?\(ncmod _ \W*{W1}\W*_(\d+) \W*{W2}\W*_(\d+)\)?],
?MOD? => [
?\(xmod _ \W*{W1}\W*_(\d+) \W*{W2}\W*_(\d+)\)?,
?\(ncmod _ \W*{W1}\W*_(\d+) \W*{W2}\W*_(\d+)\)?,
?\(cmod _ \W*{W1}\W*_(\d+) \W*{W2}\W*_(\d+)\)?]}
Figure 2: Regexp set to evaluate C&C for absol.
These expressions fit the output that we got from the
C&C parser, illustrated in Fig. 3 with a relevant por-
tion of the dependencies produced for the example
in Table 2. Here the C&C dependency (ncsubj
passed 4 Act 1 ) matches the first target in the
7We do not count typographical errors or incompletely spec-
ified conventions as failures of inter-annotator agreement.
gold-standard (Table 2), but no matching C&C de-
pendency is found for the other two targets.
(xmod _ Act_1 passed_4)
(ncsubj passed_4 Act_1 _)
(ncmod _ withdrew,_9 Jessop_8)
(dobj year,_7 withdrew,_9)
Figure 3: Excerpts of C&C output for item in Table 2.
The regular expressions operate solely on the de-
pendency labels and are not lexically-specific. They
are specific to each phenomenon, as we did not at-
tempt to write a general dependency converter, but
rather to discover what patterns of dependency rela-
tions describe the phenomenon when it is correctly
identified by each parser. Thus, though we did not
hold out a test set, we believe that they would gener-
alize to additional gold standard material annotated
in the same way for the same phenomena.8
In total, we wrote 364 regular expressions to han-
dle the output of the seven parsers, allowing some
leeway in the role labels used by a parser for any
given target dependency. The supplementary mate-
rials for this paper include the test data, parser out-
puts, target annotations, and evaluation script.
Fig. 1 provides a visualization of the results of our
evaluation. Each column of points represents one
dependency type. Dependency types for the same
phenomenon are represented by adjacent columns.
The order of the columns within a phenomenon fol-
lows the order of the dependency descriptions in
Table 3: For each pair, the dependency type with
the higher score for the majority of the parsers is
shown first (to the left). The phenomena them-
selves are also arranged according to increasing (av-
erage) difficulty. itexpl only has one column, as we
annotated just one dependency per instance here.
(The two descriptions in Table 3 reflect different,
mutually-incompatible instance types.) Since ex-
pletive it should not be the semantic dependent of
any head, the targets are generalized for this phe-
nomenon and the evaluation script counts as incor-
8In the case of the XLE, our simplistic regular-expression
approach to the interpretation of parser outputs calls for much
more complex patterns than for the other parsers. This is owed
to the rich internal structure of LFG f-structures and higher
granularity of linguistic analysis, where feature annotations on
nodes as well as reentrancies need to be taken into account.
Therefore, our current results for the XLE admit small amounts
of both over- and under-counting.
404
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
vger
vpart
control
argadj
barerel
rnr tough
ned itexpl
absol
enju
xle
c&j
c&c
stanford
mst
rasp
Figure 1: Individual dependency recall for seven parsers over ten phenomena.
rect any dependency involving referential it.
We observe fairly high recall of the dependencies
for vpart and vger (with the exception of RASP), and
high recall for both dependencies representing con-
trol for five systems. While Enju, Stanford, MST,
and RASP all found between 70 and 85% of the de-
pendency between the adjective and its complement
in the tough construction, only Enju and XLE rep-
resented the dependency between the subject of the
adjective and the gap inside the adjective?s comple-
ment. For the remaining phenomena, each parser
performed markedly worse on one dependency type,
compared to the other. The only exceptions here
are XLE and C&C?s (and to a lesser extent, C&J?s)
scores for barerel. No system scored higher than
33% on the harder of the two dependencies in rnror
absol, and Stanford, MST, and RASP all scored be-
low 25% on the harder dependency in barerel. Only
XLE scored higher than 10% on the second depen-
dency for ned and higher than 50% for itexpl.
6 Discussion
From the results in Fig. 1, it is clear that even the best
of these parsers fail to correctly identify a large num-
ber of relevant dependencies associated with linguis-
tic phenomena that occur with reasonable frequency
in the Wikipedia. Each of the parsers attempts
with some success to analyze each of these phe-
nomena, reinforcing the claim of relevance, but they
vary widely across phenomena. For the two long-
distance phenomena that overlap with those studied
in Rimell et al (2009), our results are comparable.9
Our evaluation over Wikipedia examples thus shows
the same relative lack of success in recovering long-
distance dependencies that they found for WSJ sen-
tences. The systems did better on relatively well-
studied phenomena including control, vger, and vpart,
but had less success with the rest, even though all but
two of those remaining phenomena involve syntac-
tically local dependencies (as indicated in Table 3).
Successful identification of the dependencies in
these phenomena would, we hypothesize, benefit
from richer (or deeper) linguistic information when
parsing, whether it is lexical (tough, control, itexpl,
and vpart), or structural (rnr, absol, vger, argadj, and
barerel), or somewhere in between, as with ned. In
the case of treebank-trained parsers, for the informa-
tion to be available, it must be consistently encoded
in the treebank and attended to during training. As
9Other than Enju, which scores 16 points higher in the eval-
uation of Rimell et al, our average scores for each parser across
the dependencies for these phenomena are within 12 points of
those reported by Rimell et al (2009) and Nivre et al (2010).
405
noted in Sections 2.1 and 3.5, there is tension be-
tween developing sufficiently complex representa-
tions to capture linguistic phenomena and keeping
an annotation scheme simple enough that it can be
reliably produced by humans, in the case of hand-
annotation.
7 Related Work
This paper builds on a growing body of work which
goes beyond (un)labeled bracketing in parser evalua-
tion, including Lin (1995), Carroll et al (1998), Ka-
plan et al (2004), Rimell et al (2009), and Nivre et
al. (2010). Most closely related are the latter two of
the above, as we adopt their ?construction-focused
parser evaluation methodology?.
There are several methodological differences be-
tween our work and that of Rimell et al First, we
draw our evaluation data from a much larger and
more varied corpus. Second, we automate the com-
parison of parser output to the gold standard, and we
distribute the evaluation scripts along with the anno-
tated corpus, enhancing replicability. Third, where
Rimell et al extract evaluation targets on the basis
of PTB annotations, we make use of a linguistically
precise broad-coverage grammar to identify candi-
date examples. This allows us to include both local
and non-local dependencies not represented or not
reliably encoded in the PTB, enabling us to evalu-
ate parser performance with more precision over a
wider range of linguistic phenomena.
These methodological innovations bring two em-
pirical results. The first is qualitative: Where previ-
ous work showed that overall Parseval numbers hide
difficulties with long-distance dependencies, our re-
sults show that there are multiple kinds of reason-
ably frequent local dependencies which are also dif-
ficult for the current standard approaches to pars-
ing. The second is quantitative: Where Rimell et
al. found two phenomena which were virtually un-
analyzed (recall below 10%) for one or two parsers
each, we found eight phenomena which were vir-
tually unanalyzed by at least one system, includ-
ing two unanalyzed by five and one by six. Every
system had at least one virtually unanalyzed phe-
nomenon. Thus we have shown that the dependen-
cies being missed by typical modern approaches to
parsing are more varied and more numerous than
previously thought.
8 Conclusion
We have presented a detailed construction-focused
evaluation of seven parsers over 10 phenomena,
with 1000 examples drawn from English Wikipedia.
Gauging recall of such ?deep? dependencies, in our
view, can serve as a proxy for downstream pro-
cessing involving semantic interpretation of parser
outputs. Our annotations and automated evaluation
script are provided in the supplementary materials,
for full replicability. Our results demonstrate that
significant opportunities remain for parser improve-
ment, and highlight specific challenges that remain
invisible in aggregate parser evaluation (e.g. Parse-
val or overall dependency accuracy). These results
suggest that further progress will depend on train-
ing data that is both more extensive and more richly
annotated than what is typically used today (seeing,
for example, that a large part of more detailed PTB
annotation remains ignored in much parsing work).
There are obvious reasons calling for diversity in
approaches to parsing and for different trade-offs
in, for example, the granularity of linguistic analy-
sis, average accuracy, cost of computation, or ease
of adaptation. Our proposal is not to substitute
construction-focused evaluation on Wikipedia data
for widely used aggregate metrics and reference cor-
pora, but rather to augment such best practices in
the spirit of Rimell et al (2009) and expand the
range of phenomena considered in such evaluations.
Across frameworks and traditions (and in principle
languages), it is of vital importance to be able to
evaluate the quality of parsing (and grammar induc-
tion) algorithms in a maximally informative manner.
Acknowledgments
We are grateful to Tracy King for her assistance in
setting up the XLE system and to three anonymous
reviewers for helpful comments. The fourth author
thanks DFKI and the DFG funded Excellence Clus-
ter on MMCI for their support of the work. Data
preparation on the scale of Wikipedia was made pos-
sible through access to large-scale HPC facilities,
and we are grateful to the Scientific Computing staff
at UiO and the Norwegian Metacenter for Computa-
tional Science.
406
References
Jennifer E. Arnold, Thomas Wasow, Anthony Losongco,
and Ryan Ginstrom. 2000. Heaviness vs. newness:
The effects of structural complexity and discourse sta-
tus on constituent ordering. Language, 76(1):28?55.
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre. 1995. Bracketing guidelines for treebank II
style Penn treebank project. Technical report, Univer-
sity of Pennsylvania.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the COLING/ACL 2006 Interactive Presenta-
tion Sessions, pages 77?80, Sydney, Australia.
Aoife Cahill, John T. Maxwell III, Paul Meurer, Chris-
tian Rohrer, and Victoria Rose?n. 2008. Speeding
up LFG parsing using c-structure pruning. In Coling
2008: Proceedings of the workshop on Grammar En-
gineering Across Frameworks, pages 33?40, Manch-
ester, England, August. Coling 2008 Organizing Com-
mittee.
John Carroll, Ted Briscoe, and Antonio Sanfilippo. 1998.
Parser evaluation: A survey and a new proposal.
In Proceedings of the International Conference on
Language Resources and Evaluation, pages 447?454,
Granada.
Daniel Cer, Marie-Catherine de Marneffe, Daniel Juraf-
sky, and Christopher D. Manning. 2010. Parsing to
Stanford dependencies: Trade-offs between speed and
accuracy. In 7th International Conference on Lan-
guage Resources and Evaluation (LREC 2010), Malta.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL?05),
pages 173?180, Ann Arbor, Michigan.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Proceed-
ings of the Fourteenth National Conference on Artifi-
cial Intelligence, pages 598 ? 603, Providence, RI.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics,
33(4):493?552.
Alexander Clark. 2001. Unsupervised induction
of stochastic context-free grammars using distribu-
tional clustering. In Proceedings of the 5th Confer-
ence on Natural Language Learning, pages 105?112.
Toulouse, France.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Dan Flickinger, Stephan Oepen, and Gisle Ytrest?l.
2010. WikiWoods. Syntacto-semantic annotation for
English Wikipedia. In Proceedings of the 6th Interna-
tional Conference on Language Resources and Evalu-
ation, Valletta, Malta.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language En-
gineering, 6 (1) (Special Issue on Efficient Processing
with HPSG):15 ? 28.
Sumukh Ghodke and Steven Bird. 2010. Fast query for
large treebanks. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 267?275, Los Angeles, California, June.
Association for Computational Linguistics.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
grammar induction. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 881?888, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Dependency
Structures Extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
Rodney Huddleston and Geoffrey K. Pullum. 2002. The
Cambridge Grammar of the English Language. Cam-
bridge University Press, Cambridge, UK.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
In Proceedings of NODALIDA 2007, pages 105?112,
Tartu, Estonia.
Ron Kaplan, Stefan Riezler, Tracy H King, John T
Maxwell III, Alex Vasserman, and Richard Crouch.
2004. Speed and accuracy in shallow and deep
stochastic parsing. In Susan Dumais, Daniel Marcu,
and Salim Roukos, editors, HLT-NAACL 2004: Main
Proceedings, pages 97?104, Boston, Massachusetts,
USA, May. Association for Computational Linguis-
tics.
Dan Klein and Christopher D. Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In Advances in Neural Information Process-
ing Systems 15, pages 3?10, Cambridge, MA. MIT
Press.
Dan Klein and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics, pages 478?485, Barcelona, Spain.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In Proceedings of
IJCAI-95, pages 1420?1425, Montreal, Canada.
Robert Malouf. 2000. Verbal gerunds as mixed cate-
gories in HPSG. In Robert Borsley, editor, The Nature
407
and Function of Syntactic Categories, pages 133?166.
Academic Press, New York.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19:313?330.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-Projective Dependency Pars-
ing using Spanning Tree Algorithms. In Proceedings
of the 2005 Conference on Empirical Methods in Natu-
ral Language Processing, pages 523?530, Vancouver,
Canada.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii.
2004. Corpus-oriented grammar development for ac-
quiring a Head-driven Phrase Structure Grammar from
the Penn Treebank. In Proceedings of the 1st Interna-
tional Joint Conference on Natural Language Process-
ing, pages 684?693, Hainan Island, China.
Joakim Nivre, Laura Rimell, Ryan McDonald, and Carlos
Go?mez Rodr??guez. 2010. Evaluation of dependency
parsers on unbounded dependencies. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics, pages 833?841, Beijing, China.
Ruth O?Donovan, Michael Burke, Aoife Cahill, Josef
Van Genabith, and Andy Way. 2004. Large-scale in-
duction and evaluation of lexical resources from the
penn-ii treebank. In Proceedings of the 42nd Meet-
ing of the Association for Computational Linguistics,
pages 367?374, Barcelona, Spain.
Stephan Oepen, Daniel Flickinger, Kristina Toutanova,
and Christopher D. Manning. 2004. LinGO Red-
woods. A rich and dynamic treebank for HPSG.
Journal of Research on Language and Computation,
2(4):575 ? 596.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 433?440, Sydney, Aus-
tralia.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell III, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proceedings of the 40th Meet-
ing of the Association for Computational Linguistics,
Philadelphia, PA.
Laura Rimell, Stephen Clark, and Mark Steedman. 2009.
Unbounded dependency recovery for parser evalua-
tion. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 813?821, Singapore. Association for Computa-
tional Linguistics.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword expres-
sions. A pain in the neck for NLP. In Alexander Gel-
bukh, editor, Computational Linguistics and Intelli-
gent Text Processing, volume 2276 of Lecture Notes in
Computer Science, pages 189?206. Springer, Berlin,
Germany.
David Vadas and James R. Curran. 2008. Parsing noun
phrase structure with CCG. In Proceedings of ACL-
08: HLT, pages 335?343, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
L. van der Beek, Gosse Bouma, Robert Malouf, and Gert-
jan van Noord. 2002. The Alpino Dependency Tree-
bank. In Mariet Theune, Anton Nijholt, and Hen-
dri Hondorp, editors, Computational Linguistics in the
Netherlands, Amsterdam, The Netherlands. Rodopi.
Luke Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammars. In
Proceedings of the Twenty-First Annual Conference on
Uncertainty in Artificial Intelligence, pages 658?666,
Arlington, Virginia. AUAI Press.
408
Speculation and Negation: Rules, Rankers,
and the Role of Syntax
Erik Velldal?
University of Oslo
Lilja ?vrelid?
University of Oslo
Jonathon Read?
University of Oslo
Stephan Oepen?
University of Oslo
This article explores a combination of deep and shallow approaches to the problem of resolving
the scope of speculation and negation within a sentence, specifically in the domain of biomedical
research literature. The first part of the article focuses on speculation. After first showing how
speculation cues can be accurately identified using a very simple classifier informed only by
local lexical context, we go on to explore two different syntactic approaches to resolving the
in-sentence scopes of these cues. Whereas one uses manually crafted rules operating over depen-
dency structures, the other automatically learns a discriminative ranking function over nodes
in constituent trees. We provide an in-depth error analysis and discussion of various linguistic
properties characterizing the problem, and show that although both approaches perform well
in isolation, even better results can be obtained by combining them, yielding the best published
results to date on the CoNLL-2010 Shared Task data. The last part of the article describes how our
speculation system is ported to also resolve the scope of negation. With only modest modifications
to the initial design, the system obtains state-of-the-art results on this task also.
1. Introduction
The task of providing a principled treatment of speculation and negation is a problem
that has received increased interest within the NLP community during recent years.
This is witnessed not only by this Special Issue, but also by the themes of several recent
shared tasks and dedicated workshops. The Shared Task at the 2010 Conference on Nat-
ural Language Learning (CoNLL) has been of central importance in this respect, where
the topic was speculation detection for the domain of biomedical research literature
? University of Oslo, Department of Informatics, PB 1080 Blindern, 0316 Oslo, Norway.
E-mail: {erikve,liljao,jread,oe}@ifi.uio.no.
Submission received: 5 April 2011; revised submission received: 30 September 2011; accepted for publication:
2 December 2011.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 2
(Farkas et al 2010). This particular area has been the focus of much current research,
triggered by the release of the BioScope corpus (Vincze et al 2008)?a collection of
scientific abstracts, full papers, and clinical reports with manual annotations of words
that signal speculation or negation (so-called cues), as well as of the scopes of these cues
within the sentences. The following examples from BioScope illustrate how sentences
are annotated with respect to speculation. Cues are here shown using angle brackets,
with braces corresponding to their annotated scopes:
(1) {The specific role of the chromodomain is ?unknown?} but chromodomain
swapping experiments in Drosophila {?suggest? that they {?might? be
protein interaction modules}} [18].
(2) These data {?indicate that? IL-10 and IL-4 inhibit cytokine production by
different mechanisms}.
Negation is annotated in the same way, as shown in the following examples:
(3) Thus, positive autoregulation is {?neither? a consequence ?nor? the sole
cause of growth arrest}.
(4) Samples of the protein pair space were taken {?instead of? considering the
whole space} as this was more computationally tractable.
In this article we develop several linguistically informed approaches to automati-
cally identify cues and resolve their scope within sentences, as in the example annota-
tions. Our starting point is the system developed by Velldal, ?vrelid, and Oepen (2010)
for the CoNLL-2010 Shared Task challenge. This system implements a two-stage hybrid
approach for resolving speculation: First, a binary classifier is applied for identifying
cues, and then their in-sentence scope is resolved using a small set of manually defined
rules operating on dependency structures.
In the current article we present several important extensions to the initial system
design of Velldal, ?vrelid, and Oepen (2010): First, in Section 5, we present a simpli-
fied approach to cue classification, greatly reducing the model size and complexity
of our Support Vector Machine (SVM) classifier while at the same time giving better
accuracy. Then, after reviewing the manually defined dependency-based scope rules
(Section 6.1), we show how the scope resolution task can be handled using an alternative
approach based on learning a discriminative ranking function over subtrees of HPSG-
derived constituent trees (Section 6.2). Moreover, by combining this empirical ranking
approach with the manually defined rules (Section 6.3), we are able to obtain the best
published results so far (to the best of our knowledge) on the CoNLL-2010 Shared
Task evaluation data. Finally, in Section 7, we show how our speculation system can be
ported to also resolve the scope of negation. Only requiring modest modifications, the
system also obtains state-of-the-art results on this task. Rather than merely presenting
the implementation details of the new approaches we develop, we also provide in-depth
error analyses and discussion on the linguistic properties of the phenomena of both
speculation and negation.
Before turning to the details of our approach, however, we start by presenting the
relevant data sets and the resources used for pre-processing in Section 2, followed by
a presentation of the various evaluation measures we will use in Section 3. We also
provide a brief review of relevant previous work in Section 4.
370
Velldal et al Rules, Rankers, and the Role of Syntax
2. Data Sets and Preprocessing
Our experiments center on the biomedical abstracts, full papers, and clinical reports of
the BioScope corpus (Vincze et al 2008). This comprises 20,924 sentences (or other root-
level utterances), annotated with respect to both negation and speculation. Some basic
descriptive statistics for the data sets are provided in Table 1. We see that roughly 18% of
the sentences are annotated as uncertain, and 13% contain negations. Note that, for our
speculation experiments, we will be using only the abstracts and the papers for training,
corresponding to the official CoNLL-2010 Shared Task training data. Moreover, we will
be using the Shared Task version of this data, in which certain annotation errors had
been corrected. The Shared Task task organizers also provided a set of newly annotated
biomedical articles for evaluation purposes, constituting an additional 5,003 utterances.
This latter data set (also detailed in Table 1) will be used for held-out testing of our
speculation models. We will be using the following abbreviations when referring to the
various parts of the data: BSA (BioScope abstracts), BSP (full papers), BSE (the held-
out evaluation data), and BSR (clinical reports). Note that, when we get to the negation
task we will be using the original version of the BioScope data. Furthermore, as BSE
does not annotate negation, we instead follow the experimental set-up of Morante and
Daelemans (2009b) for the negation task, reporting 10-fold cross validation on BSA and
held-out testing on BSP and BSR.
2.1 Tokenization
The BioScope data (and other data sets in the CoNLL-2010 Shared Task), are provided
sentence-segmented only, and otherwise non-tokenized. Unsurprisingly, the GENIA
tagger (Tsuruoka et al 2005) has a central role in our pre-processing set-up. We found
that its tokenization rules are not always optimally adapted for the type of text in Bio-
Scope, however. For example, GENIA unconditionally introduces token boundaries for
some punctuation marks that can also occur token-internally, thus incorrectly splitting
tokens like 390,926, methlycobamide:CoM, or Ca(2+). Conversely, GENIA fails to isolate
some kinds of opening single quotes, because the quoting conventions assumed in
BioScope differ from those used in the GENIA Corpus, and it mis-tokenizes LATEX-
style n- and m-dashes. On average, one in five sentences in the CoNLL training data
Table 1
The top three rows summarize the components of the BioScope corpus?abstracts (BSA), full
papers (BSP), and clinical reports (BSR)?annotated for speculation and negation. The bottom
row details the held-out evaluation data (BSE) provided for the CoNLL-2010 Shared Task.
Columns indicate the total number of sentences and their average length, the number of
hedged/negated sentences, the number of cues, and the number of multiword cues. (Note that
BSE is not annotated for negation, and we do not provide speculation statistics for BSR as this
data set will only be used for the negation experiments.
Speculation Negation
Sentences Length Sentences Cues MWCs Sentences Cues MWCs
BSA 11,871 26.1 2,101 2,659 364 1,597 1,719 86
BSP 2,670 25.7 519 668 84 339 376 23
BSR 6,383 7.7 ? ? ? 865 870 8
BSE 5,003 27.6 790 1,033 87 ? ? ?
371
Computational Linguistics Volume 38, Number 2
exhibited GENIA tokenization problems. Our pre-processing approach thus deploys a
cascaded finite-state tokenizer (borrowed and adapted from the open-source English
Resource Grammar: Flickinger [2002]), which aims to implement the tokenization deci-
sions made in the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993)?much
like GENIA, in principle?but more appropriately treating corner cases like the ones
noted here.
2.2 PoS Tagging and Lemmatization
For part-of-speech (PoS) tagging and lemmatization, we combine GENIA (with its
built-in, occasionally deviant tokenizer) and TnT (Brants 2000), which operates on
pre-tokenized inputs but in its default model is trained on financial news from the
Penn Treebank. Our general goal here is to take advantage of the higher PoS accuracy
provided by GENIA in the biomedical domain, while using our improved tokenization
and producing inputs to the parsers that as much as possible resemble the conventions
used in the original training data for the (dependency) parser (the Penn Treebank, once
again).
To this effect, for the vast majority of tokens we can align the GENIA tokeniza-
tion with our own, and in these cases we typically use GENIA PoS tags and lemmas
(i.e., base-forms). For better normalization, we downcase all lemmas except for proper
nouns. GENIA does not make a PoS distinction between proper vs. common nouns
(as assumed in the Penn Treebank), however, and hence we give precedence to TnT
outputs for tokens tagged as nominal by both taggers. Finally, for the small number of
cases where we cannot establish a one-to-one correspondence between GENIA tokens
and our own tokenization, we rely on TnT annotation only.
2.3 A Methodological Caveat
Unsurprisingly, the majority of previous work on BioScope seems to incorporate infor-
mation from the GENIA tagger in one way or another, whether it regards tokenization,
lemmatization, PoS information, or named entity chunking. Using the GENIA tagger for
pre-processing introduces certain dependencies to be aware of, however, as the abstracts
in BioScope are in fact also part of the GENIA corpus (Collier et al 1999) on which the
GENIA tagger is trained. This means that the accuracy of the information provided by
the tagger on this subset of BioScope cannot be expected to be representative of the
accuracy on other texts. Moreover, this effect might of course also carry over to any
downstream components using this information.
For the experiments described in this article, GENIA supplies lemmas for the
n-gram features used by the cue classifiers, as well as PoS tags used in the input to
both the dependency parser and the Head-driven Phrase Structure Grammar (HPSG)
parser (which in turn provide the inputs to our various scope resolution components).
For the HPSG parser, a subset of the GENIA corpus was also used as part of the
training data for estimating an underlying statistical parse selection model, producing
n-best lists of ranked candidate parses (MacKinlay et al 2011). When reporting final
test results on the full papers (BSP or BSE) or the clinical reports (BSR), no such
dependencies between information sources exists. It does mean, however, that we can
reasonably expect to see some extra drop in performancewhen going fromdevelopment
results on data that includes the BioScope abstracts to the test results on these other
data sets.
372
Velldal et al Rules, Rankers, and the Role of Syntax
3. Evaluation Measures
In this section we seek to clarify the type of measures we will be using for evaluating
both the cue detection components (Section 3.1) and the scope resolution components
(Section 3.2). Essentially, we here follow the evaluation scheme established by the
CoNLL-2010 Shared Task on speculation detection, also applying this when evaluating
results for the negation task.
3.1 Evaluation Measures for Cue Identification
For the approaches presented for cue detection in this article (for both speculation and
negation), we will be reporting precision, recall, and F1 for three different levels of
evaluation; the sentence-level, the token-level, and the cue-level. The sentence-level scores
correspond to Task 1 in the CoNLL-2010 Shared Task, that is, correctly identifying
whether a sentence contains uncertainty or not. The scores at the token-level measure
the number of individual tokens within the span of a cue annotation that the classifier
has correctly labeled as a cue. Finally, the stricter cue-level scores measure how well a
classifier succeeds in identifying entire cues (which will in turn provide the input for
the downstream components that later try to resolve the scope of the speculation or
negation within the sentence). A true positive at the cue-level requires that the predicted
cue exactly matches the annotation in its entirety (full multiword cues included).
For assessing the statistical significance of any observed differences in performance,
we will be using a two-tailed sign-test applied to the token-level predictions. This
is a standard non-parametric test for paired samples, which in our setting considers
how often the predictions of two given classifiers differ. Note that we will only be
performing significance testing for the token-level evaluation (unless otherwise stated),
as this is the level that most directly corresponds to the classifier decisions. We will be
assuming a significance level of ? = 0.05, but also reporting actual p-values in cases
where differences are not found to be significant.
3.2 Evaluation Measures for Scope Resolution
When evaluating scope resolution we will be following the methodology of the CoNLL-
2010 Shared Task, also using the scoring software made available by the task organiz-
ers.1 We have modified the software trivially so that it can also be used to evaluate
negation labeling. As pointed out by Farkas et al (2010), this way of evaluating scope is
rather strict: A true positive (TP) requires an exact match for both the entire cue and the
entire scope. On the other hand, a false positive (FP) can be incurred by three different
events; (1) incorrect cue labeling with correct scope boundaries, (2) correct cue labeling
with incorrect scope boundaries, or (3) incorrectly labeled cue and scope. Moreover,
conditions (1) and (2) will give a double penalty, in the sense that they also count as false
negatives (FN) given that the gold-standard cue or scope is missed (Farkas et al 2010).
Finally, false negatives are of course also incurred by cases where the gold-standard
annotations specify a scope but the system makes no such prediction.
Of course, the evaluation scheme outlined here corresponds to an end-to-end eval-
uation of the overall system, where the cue detection performance carries over to the
1 The Java code for computing the scores can be downloaded from the CoNLL-2010 Shared Task Web site:
http://www.inf.u-szeged.hu/rgai/conll2010st/.
373
Computational Linguistics Volume 38, Number 2
scope-level performance. In order to better assess the performance of a scope resolution
component in isolation, we will also report scope results against gold-standard cues. Note
that, when using gold-standard cues, the number of false negatives and false positives
will always be identical, meaning that the scope-level figures for recall, precision, and F1
will all be identical as well, and we will therefore only be reporting the latter in this set-
up. (The reason for this is that, when assuming gold-standard cues, only error condition
(2) can occur, which will in turn always count both a false positive and a false negative,
making the two figures identical.)
Exactly how to define the paired samples that form the basis of the statistical
significance testing is less straightforward for the end-to-end scope-level predictions
than for the cue identification. It is also worth noting that the CoNLL-2010 Shared Task
organizers themselves refrained from including any significance testing when report-
ing the official results. In this article we follow a recall-centered approach: For each
cue/scope pair in the gold standard, we simply note whether it is correctly identified
or not by a given system. The sequence of boolean values that results (FP = 0, TP = 1)
can be directly paired with the corresponding sequence for a different system so that
the sign-test can be applied as above.
Note that our modified scorer for negation is available from our Web page of sup-
plemental materials,2 together with the system output (in XML following the BioScope
DTD) for all end-to-end runs with our final model configurations.
4. Related Work on Speculation Labeling
Although there exists a body of earlier work on identifying uncertainty on the sentence
level, (Light, Qiu, and Srinivasan 2004; Medlock and Briscoe 2007; Szarvas 2008), the
task of resolving the in-sentence scope of speculation cues was first pioneered byMorante
and Daelemans (2009a). In this sense, the CoNLL-2010 Shared Task (Farkas et al 2010)
entered largely uncharted territory and contributed to an increased interest for this task.
Virtually all systems for resolving speculation scope implement a two-stage archi-
tecture: First there is a component that identifies the speculation cues and then there is a
component for resolving the in-sentence scopes of these cues. In this section we provide a
brief review of previous work on this problem, putting emphasis of the best performers
from the two corresponding subtasks of the CoNLL-2010 Shared Task, cue detection
(Task 1) and scope resolution (Task 2).
4.1 Related Work on Identifying Speculation Cues
The top-ranked system for Task 1 in the official CoNLL-2010 Shared Task evaluation
approached cue identification as a sequence labeling problem (Tang et al 2010). Similarly to
the decision-tree approach of Morante and Daelemans (2009a), Tang et al (2010) set out
to label tokens according to a BIO-scheme; indicating whether they are at the Beginning,
Inside, or Outside of a speculation cue. In the ?cascaded? system architecture of Tang et
al. (2010), the predictions of both a Conditional Random Field (CRF) sequence classifier
and an SVM-based Hidden Markov Model (HMM) are both combined in a second CRF.
In terms of the overall approach, namely, viewing the problem as a sequence la-
beling task, Tang et al (2010) are actually representative of the majority of the Shared
Task participants for Task 1 (Farkas et al 2010), including the top three performers on
2 Supplemental materials; http://www.velldal.net/erik/modneg/.
374
Velldal et al Rules, Rankers, and the Role of Syntax
the official held-out data. Many participants instead approached the task as a word-by-
word token classification problem, however. Examples of this approach are the systems of
Velldal, ?vrelid, and Oepen (2010) and Vlachos and Craven (2010), sharing the fourth
rank position (out of 24 submitted systems) for Task 1.
In both the sequence- and token-classification approaches, sentences are labeled
as uncertain if they are found to contain a cue. In contrast to this, a third group of
systems instead label sentences directly, typically using bag-of-words features. Such
sentence classifiers tended to achieve a somewhat lower relative rank in the official Task 1
evaluation (Farkas et al 2010).
4.2 Related Work on Resolving Speculation Scope
As mentioned earlier, the task of resolving the scope of speculation was first introduced
inMorante andDaelemans (2009a), where a system initially designed for negation scope
resolution (Morante, Liekens, and Daelemans 2008) was ported to speculation. Their
general approach treats the scope resolution task in much the same way as the cue
identification task: as a sequence labeling task and using only token-level, lexical infor-
mation. Morante, van Asch, and Daelemans (2010) then extended on this system by also
adding syntactic features, resulting in the top performing system of the CoNLL-2010
Shared Task at the scope-level (corresponding to the second subtask). It is interesting
to note that all the top performers use various types of syntactic information in their
scope resolution systems: The output from a dependency parser (MaltParser) (Morante,
van Asch, and Daelemans 2010; Velldal, ?vrelid, and Oepen 2010), a tag sequence
grammar (RASP) (Rei and Briscoe 2010), as well as constituent analysis in combination
with dependency triplets (Stanford lexicalized parser) (Kilicoglu and Bergler 2010).
The majority of systems perform classification at the token level, using some variant
of machine learning with a BIO classification scheme and a post-processing step to
assemble the full scope (Farkas et al 2010), although several of the top performers
employ manually constructed rules (Kilicoglu and Bergler 2010; Velldal, ?vrelid, and
Oepen 2010) or even combinations of machine learning and rules (Rei and Briscoe 2010).
5. Identifying Speculation Cues
We now turn to look at the details of our own system, starting in this section with
describing a simple yet effective approach to identifying speculation cues. A cue is here
taken to mean the words or phrases that signal the attitude of uncertainty or specula-
tion. As noted by Farkas et al (2010), most hedge cues typically fall in the following cate-
gories; adjectives or adverbs (probable, likely, possible, unsure, etc.), auxiliaries (may,might,
could, etc.), conjunctions (either. . . or, etc.), or verbs of speculation (suggest, suspect, sup-
pose, seem, etc.). Judging by the examples in the Introduction, it might at first seem that
the speculation cues can be identified merely by consulting a pre-compiled list. Most, if
not all, words that can function as cues can also occur as non-cues, however. More than
85% of the cue lemmas observed in the BioScope corpus also have non-cue occurrences.
To give just one example, a hedge detection system needs to correctly discriminate
between the use of appear as a cue in Example (5), and as a non-cue in Example (6):
(5) In 5 patients the granulocytes {?appeared? polyclonal} [. . . ]
(6) The effect appeared within 30 min and returned to basal levels after 2 h.
375
Computational Linguistics Volume 38, Number 2
In the approach of Velldal, ?vrelid, and Oepen (2010), a binary token classifier was
applied in a way that labeled each and every word as cue or non-cue. We will refer to this
mode of classification as word-by-word classification (WbW). The follow-up experi-
ments described by Velldal (2011) showed that comparable results could be achieved
using a filtering approach that ignores words not occurring as cues in the training data.
This greatly reduces both the number of relevant training examples and the number
of features in the model, and in the current article we simplify this ?disambiguation
approach? even further. In terms of modeling framework, we implement our models
as linear SVM classifiers, estimated using the SVMlight toolkit (Joachims 1999). We also
include results for a very simple baseline model, however?to wit, a WbW approach
classifying each word simply based on its observed majority usage as a cue or non-cue
in the training data. Then, as for all our models, if a given sentence is found to contain
a cue, the entire sentence is subsequently labeled uncertain. Before turning to the indi-
vidual models, however, we first describe how we deal with the issue ofmultiword cues.
5.1 Multiword Cues
In the BioScope annotations, it is possible for a speculation cue to span multiple tokens
(e.g., raise an intriguing hypothesis). As seen from Table 1, about 13.5% of the cues in the
training data are such multiword cues (MWCs). The distribution of these cues is very
skewed, however. For instance, although the majority of MWCs are very infrequent
(most of them occurring only once), the pattern indicate that accounts for more than 70%
of the cases alone. Exactly which cases are treated as MWCs often seems somewhat
arbitrary and we have come across several inconsistencies in the annotations. We there-
fore choose to not let the classifiers we develop in this article be sensitive to the notion
of multiword cues. A given word token is considered a cue as long as it falls within
the span of a cue annotation. Multiword cues are instead treated in a separate post-
processing step, applying a small set of heuristic rules that aim to capture only the most
frequently occurring patterns observed in the training data. For example, if we find that
indicate is classified as a cue and it is followed by that, a rule will fire that ensures we
treat these tokens as a single cue. (Note that the rules are only applied to sentences that
have already been labeled uncertain by the classifier.) Table 2 lists the lemma patterns
currently covered by our rules.
5.2 Reformulating the Classification Problem: A Filtered Model
Before detailing our approach, we start with some general observations about the data
and the task. An error analysis of the initial WbW classifier developed by Velldal,
Table 2
Patterns covered by our rules for multiword speculation cues.
cannot {be}? exclude
either .+ or
indicate that
may,? or may not
no {evidence | proof | guarantee}
not {known | clear | evident | understood | exclude}
raise the .* {possibility | question | issue | hypothesis}
whether or not
376
Velldal et al Rules, Rankers, and the Role of Syntax
?vrelid, and Oepen (2010) revealed it was not able to generalize to new speculation
cues beyond those observed during training. On the other hand, only a rather small
fragment of the test cues are actually unseen: Using a 10-fold split for the development
data, the average ratio of test cues that also occur as cues in training is more than 90%.
Another important observation we can take into account is that although it seems
reasonable to assume that anyword occurring as a cue can also occur as a non-cue (recall
that more than 85% of the observed cues also have non-cue occurrences in the training
data), the converse is less likely. Whereas the training data contains a total of approxi-
mately 17,600 unique base forms, only 143 of these ever occur as speculation cues.
As a consequence of these observations, Velldal (2011) proposed that one might
reasonably treat the set of cue words as a near-closed class, at least for the biomedical
data considered in this study. This means reformulating the problem as follows. Instead
of approaching the task as a classification problem defined for all words, we only
consider words that have a base form observed as a speculation cue in the training
material. By restricting the classifier to only this subset of words, we can simplify the
classification problem tremendously. As we shall see, it also has the effect of leveling
out the initial imbalance between negative and positive examples in the data, acting as
a (selective rather than random) downsampling technique.
One reasonable fear here, perhaps, might be that this simplification comes at the
expense of recall, as we are giving up on generalizing our predictions to any previously
unseen cues. As noted earlier, however, the initial WbW model of Velldal, ?vrelid, and
Oepen (2010) already failed to make any such generalizations, and, as we shall see, this
reformulation comes without any loss in performance and actually leads to an increase
in recall compared to a full WbWmodel using the same feature set.
Note that although we will approach the task as a ?disambiguation problem,? it is
not feasible to train separate classifiers for each individual base form. The frequency
distribution of the cue words in the training material is rather skewed with most cues
being very rare?many occurring as a cue only once (? 40%, constituting less than
1.5% of the total number of cue word instances). (Most of these words also have many
additional occurrences in the training data as non-cues, however.) For the majority of
the cue words, then, it seems we cannot hope to gather enough reliable information to
train individual classifiers. Instead, we want to be able to draw on information from
the more frequently occurring cues also when classifying or disambiguating the less
frequent ones. Consequently, we will still train a single global classifier.
Extending on the approach of Velldal (2011), we include a final simple step to reduce
the set of relevant training examples even further. As pointed out in Section 5.1, any
token occurring within a cue annotation is initially regarded as a cue word. Many
multiword cues also include function words, punctuation, and so forth, however. In
order to filter out such spurious but high-frequency ?cues,? we compiled a small stop-
list on the basis of the MWCs in training data (containing just a dozen tokens, namely,
a, an, as, be, for, of, that, the, to, with, ?,?, and ?-?).
5.2.1 Features. After experimenting with a wide range of different features, ?vrelid,
Velldal, and Oepen (2010) concluded that syntactic features appeared unnecessary for
the cue classification task, and that simple sequence-oriented n-gram features recording
immediate lexical context based on lemmas and surface forms is what gave the best
performance.
Initially, the n-gram feature templates we use in the current article record neighbors
for up to three positions left/right of the focus word. For increased generality, we also
include non-lexicalized variants, that is, recording only the neighbors while excluding
377
Computational Linguistics Volume 38, Number 2
the focus word itself. After a grid search across the various configurations of these
features, the best performance was found for a model recording n-grams of lemmas up
to three positions left and right of the focus word, and n-grams of surface forms up to
two positions to the right.
Table 3 shows the performance of the filteringmodel when using this feature config-
uration and testing by 10-fold cross-validation on the training data (BSA and BSP), also
contrasting performance with the majority usage baseline. Achieving a sentence-level
F1 of 92.04 (compared to 89.07 for the baseline), a token-level score of 89.57 (baseline =
86.42), and a cue-level score of 89.11 (baseline = 85.57), it performs significantly better
than the baseline. Applying the sign-test as described in Section 3.1, the token-level
differences were found to be significant for p < 0.05. It is also clear, however, that the
simple baseline appears to be fairly strong.
As discussed previously, part of the motivation for introducing the filtering scheme
is to create a model that is as simple as possible without sacrificing performance. In
addition to the evaluation scores, therefore, it is also worth noting some statistics related
to the classifier and the training data itself. Before looking into the properties of the fil-
tering set-up though, let us start, for the sake of comparison, by considering some prop-
erties of a learning set-up based on full WbW classification like the model of Velldal,
?vrelid, and Oepen (2010), assuming an identical feature configuration as used for the
given filtering model. The row titled WbW in Table 3 lists the development results for
this model, and we see that they are slightly lower than for the filtering model (with the
differences being significant for ? = 0.05). Although precision is slightly higher, recall is
substantially lower. Assuming a 10-fold cross-validation scheme like this, the number of
training examples presented to the WbW learner in each fold averages roughly 340,000,
corresponding to the total number of word tokens. Among these training examples,
the ratio of positive to negative examples (cues vs. non-cues) is roughly 1:100. In other
words, the data is initially very skewed when it comes to class balance. In terms of the
size of the feature set, the average number of distinct feature types per fold, assuming
the given feature configuration, would be roughly 2,600,000 under a WbW set-up.
Turning now to the filtering model, the average number of training examples
presented to the learner in each fold is reduced from roughly 340,000 to just 10,000.
Correspondingly, the average number of distinct feature types is reduced from well
above 2,600,000 to roughly 100,000. The class balance among the tokens given to the
learner is alsomuch less skewed, with positive examples now averaging 30%, compared
to 1% for the WbW set-up. Finally, we observe that the complexity of the model in
terms of how many training examples end up as support vectors (SVs) defining the
separating hyperplane is also considerably reduced: Although the average number of
SVs in each fold corresponds to roughly 14,000 examples for the WbW model, this is
down to roughly 5,000 for the final filtered model. Note that for the SVM regularization
Table 3
Development results for detecting speculationCUES:Averaged 10-fold cross-validation results for
the cue classifiers on both the abstracts and full papers in the BioScope training data (BSA andBSP).
Sentence Level Token Level Cue Level
Model Prec Rec F1 Prec Rec F1 Prec Rec F1
Baseline 91.07 87.21 89.07 91.61 81.85 86.42 90.49 81.16 85.57
WbW 95.01 88.03 91.37 95.29 82.78 88.58 94.65 82.26 88.02
Filtering 94.52 89.72 92.04 94.88 84.86 89.57 94.13 84.60 89.11
378
Velldal et al Rules, Rankers, and the Role of Syntax
parameter C, governing the trade-off between training error and margin size, we will
always be using the default value set by SVMlight. This value is analytically determined
from the training data, and further empirical tuning has in general not led to improve-
ments on our data sets.
5.2.2 The Effect of Data Size. Given how the filtered classifier treats the set of cues as a
closed class, a reasonable concern is its sensitivity to the size of the training set. In order
to further assess this effect, we computed learning curves showing how classifier per-
formance on the development data changes as we incrementally include more training
examples (see Figure 1). For reference we also include learning curves for the word-by-
word classifier using the identical feature configuration, as well as the majority usage
baseline.
As expected, we see that classifier performance steadily improves as more training
data is included. Although additional data would no doubt be beneficial, we reassur-
ingly observe that the curve seems to start gradually flattening out somewhat. If we
instead look at the performance curve for the WbW classifier we find that, while having
roughly the same shape as that of the filtered classifier, although consistently lower, it
nonetheless appears to be more sensitive to the size of the training set. Interestingly,
we see that the baseline model seems to be the one that is least affected by data size. It
actually outperforms the standard WbWmodel for the first three increments, but at the
same time it seems unable to benefit much at all from additional data.
5.2.3 Error Analysis.When looking at the distribution of errors at the cue-level (totaling
just below 700 across the 10-fold run), we find that roughly 74% are false negatives.
Rather than being caused by legitimate cue words being filtered out during training,
however, the FNs mostly pertain to a handful of high-frequency words that are also
highly ambiguous. When sorted according to error frequency, the top four candidates
alone constitute almost half the total number of FNs: or (24% of the FNs), can (10%),
could (7%), and either (6%). Looking more closely at the distribution of these words in
Figure 1
Learning curves showing the effect on token-level F1 for speculation cues when withdrawing
some portion of the training partitions across the 10-fold cycles. The size of the training set is
shown on a logarithmic scale to better see whether improvements are constant for n-fold
increases of data.
379
Computational Linguistics Volume 38, Number 2
the training data, it is easy to see how they pose a challenge for the learner. For example,
whereas or has a total of 1,215 occurrences, only 153 of these are annotated as a cue.
Distinguishing the different usages from each other can sometimes be difficult even for
a human eye, as testified also by the many inconsistencies we observed in the gold-
standard annotation of these cases.
Turning our attention to the other end of the tail, we find that just over 40 (8%) of the
FNs involve tokens for which there is only a single occurrence as a cue in the training
data. In other words, these would first appear to be exactly the tokens that we could
never get right, given our filtering scheme. We find, however, that most of these cases
regard tokens whose one and only appearance as a cue is as part of a multiword cue,
although they typically have a high number of other non-cue occurrences as well. For
example, although number occurs a total of 320 times, its one and only occurrence as
a cue is in the multiword cue address a number of questions. Given that this and several
other equally rare patterns are not currently covered by our MWC rules in the first
place, we would not have been able to get them right even if all the individual tokens
had been classified as cues (recall that a true positive at the cue-level requires an exact
match of the entire span). In total we find that 16% of the cue-level FNs corresponds to
multiword cues.
When looking at the frequency of multiword cues among the false positives, we
find that they only make up roughly 5% of the errors. Furthermore, a manual inspection
reveals that they can all be argued to be instances of annotation errors, in that we believe
these should actually be counted as true positives. Most of them involve indicate that and
not known, as in the following examples (where the cues assigned by our system are not
annotated as cues in BioScope):
(7) In contrast, levels of the transcriptional factor AP-1, which is ?not known?
to be important in B cell Ig production, were reduced by TGF-beta.
(8) Analysis of the nuclear extracts [. . . ] ?indicated that? the composition of
NF-kappa B was similar in neonatal and adult cells.
All in all, the errors in the FP category make up 26% of the total number of errors.
Just as for the FNs, the frequency distribution of the cues involved is quite skewed,
with a handful of highly frequent and highly ambiguous cue words accounting for the
bulk of the errors: The modal could (20%), and the adjectives putative (11%), possible
(6%), potential (6%), and unknown (5%). After manually inspecting the full set of FPs,
however, we find that at least 60% of them should really be counted as true positives.
The following are just a few examples where cues predicted by our classifier are not
annotated as such in BioScope and therefore counted as FPs.
(9) IEF-1, a pancreatic beta-cell type-specific complex ?believed? to regulate
insulin expression, is demonstrated to consist of at least two distinct
species, [. . . ]
(10) We ?hypothesize? that a mutation of the hGR glucocorticoid-binding
domain is the cause [. . . ]
(11) Antioxidants have been ?proposed? to be anti-atherosclerotic agents; [. . . ]
(12) Finally, matDCC might be further stabilized by the addition of roX1 RNA,
which could interact with several of the MSLs and ?perhaps? roX2 RNA
as well.
380
Velldal et al Rules, Rankers, and the Role of Syntax
One interesting source of real FPs concerns ?anti-hedges,? which in the training data
appear with a negation and as part of a multiword cue, for example no proof. During
testing, the classifier will sometimes wrongly predict a word like proof to be a specu-
lation cue, even when it is not negated. Because we already have MWC rules for cases
like this (see Section 5.1) it would be easy to also include a check for ?negative context,?
making sure that such tokens are not classified as cues if the requiredmultiword context
is missing.
Before rounding off this section, a brief look at the BioScope inter-annotator agree-
ment rates may offer some further perspective on the results discussed here. Note that
when creating the BioScope data, the decisions of two independent annotators were
merged by a third expert linguist who resolved any differences. The F1 of each set of
annotations toward the final gold-standard cues are reported by Vincze et al (2008) to
be 83.92 / 92.05 for the abstracts and 81.49 / 90.81 for the full papers. (Recall from Table 3
that our cue-level F1 for the cross-validation runs on the abstracts and papers is 89.11.)
When instead comparing the decisions of the two annotators directly, the F1 is reported
to be 79.12 for the abstracts and 77.60 for the papers.
5.3 Held-Out Results for Identifying Speculation Cues
Table 4 presents the final evaluation of the various cue classifiers developed in this
section, as applied to the held-out BSE test data. In addition to the evaluation results for
our own classifiers, Table 4 also includes the official test results for the system described
by Tang et al (2010). The sequence classifier developed by Tang et al (2010)?combining
a CRF classifier and a large-margin HMM model?obtained the best results for the
official Shared Task evaluation for Task 1 (i.e., sentence-level uncertainty detection), as
well as the highest cue-level scores.
As seen from Table 4, although themodel of Tang et al (2010) still achieves a slightly
higher F1 (81.34) than our filtered disambiguation model for the cue-level, our model
achieves a slightly higher F1 (86.58) for the sentence-level (yielding the best-published
result for this task so far, to the best of our knowledge). The differences are not deemed
statistically significant by a two-tailed sign-test, however (p = 0.37). It is interesting to
note, however, that the two approaches appear to have somewhat different strengths
and weaknesses: Whereas our filtering classifier consistently shows stronger precision
(and theWbWmodel even more so), the model of Tang et al (2010) is stronger on recall.
The sentence-level recall of our filtered classifier is still better than any of the remaining
23 systems submitted for the Shared Task evaluation, however, and, more interestingly,
it improves substantially on the recall of the full WbW classifier.
Table 4
Held-out results for identifying speculation cues: Applying the cue classifiers to the 5,003
sentences in BSE? the biomedical papers provided for the CoNLL-2010 Shared Task evaluation.
Sentence Level Token Level Cue Level
Model Prec Rec F1 Prec Rec F1 Prec Rec F1
Baseline 77.59 81.52 79.51 77.16 72.39 74.70 75.15 72.49 73.80
WbW 89.28 83.29 86.18 87.62 73.95 80.21 86.33 74.21 79.82
Filtering 87.87 85.32 86.58 86.46 76.74 81.31 84.79 77.17 80.80
Tang et al 2010 85.03 87.72 86.36 n/a n/a n/a 81.70 80.99 81.34
381
Computational Linguistics Volume 38, Number 2
We find that, just as for the development data, the reformulation of the cue clas-
sification task as a simple disambiguation problem improves F1 across all evaluation
levels, consistently outperforming the WbW classifiers. When computing a two-tailed
signed-test for the token-level decisions (where the WbW and filtering model achieves
an F1 of 80.21 and 81.31, respectively) the differences are not found to be significant (p =
0.12). As discussed in Section 5.2, however, it is important to bear in mind that the size
and complexity of the filtered ?disambiguation? model is greatly reduced compared
to the WbW model, using a much smaller number of features and relevant training
examples.
While on the topic of model complexity, it is also worth noting that many of the
systems participating in the CoNLL-2010 Shared Task challenge used fairly complex
and resource-heavy feature types, being sensitive to properties of document structure,
grammatical relations, deep syntactic structure, and so forth (Farkas et al 2010). The fact
that comparable or better results can be obtained using a relatively simplistic approach
as developed in this section, with surface-oriented features that are only sensitive to the
immediate lexical context, is an interesting result in its own right. In fact, even the simple
majority usage baseline classifier proves to be surprisingly competitive: Comparing its
sentence-level F1 to those of the official Shared Task evaluation, it actually outranks 7 of
the 24 submitted systems.
A final point that deserves some discussion is the drop in F1 that we observe when
going from the development results to the held-out results. There are several reasons for
this drop. Section 2.3 discussed how certain overfitting effects might be expected from
the GENIA-based pre-processing. In addition to this, it is likely that there are MWC
patterns in the held-out data that were not observed in the training data, and that are
therefore not covered by our MWC rules. Another factor that may have slightly inflated
the development results is the fact that we used a sentence-level rather than a document-
level partitioning of the data for cross-validation.
6. Resolving the Scope of Speculation Cues
Once the speculation cue has been determined using the cue detection system described
here, we go on to determine the scope of the speculation within the sentence. This task
corresponds to Task 2 of the CoNLL-2010 Shared Task. Example (13), which will be
used as a running example throughout this section, shows a scope-resolved BioScope
sentence where speculation is signaled by the modal verb may.
(13) {The unknown amino acid ?may? be used by these species}.
The exact scope will vary quite a lot depending on linguistic properties of the cue
in question, and in our approaches to scope resolution we rely heavily on syntactic
information. We experiment with two different approaches to syntactic analysis; data-
driven dependency parsing and grammar-driven phrase structure parsing. Because
scope determination in BioScope makes reference to subtle and fine-grained linguistic
distinctions (e.g., passivization or subject raising), in both cases we choose parsing
systems that make available comparatively ?deep? syntactic analyses. In the following
we present three different systems; a rule-based approach using dependency structures
(Section 6.1), a data-driven approach using an SVM ranker for selecting appropriate
subtrees in constituent structures (Section 6.2), and finally a hybrid approach combining
the rules and the ranker (Section 6.3).
382
Velldal et al Rules, Rankers, and the Role of Syntax
6.1 A Rule-Based Approach Using Dependency Structures
?vrelid, Velldal, and Oepen (2010) applied a small set of heuristic rules oper-
ating over syntactic dependency structures to define the scope for each cue. In the
following we will provide a detailed description of these rules and the syntactic gen-
eralizations they provide for the scope of speculation (Section 6.1.2). We will evalu-
ate their performance using both gold-standard cues and cues predicted by our cue
classifier (Section 6.1.3), in addition to providing an in-depth manual error analysis
(Section 6.1.5). We start out, however, by presenting some specifics about the processing
of the data; introducing the stacked dependency parser that produces the input to our
rules (Section 6.1.1) and quantifying the effect of using a domain-adapted PoS tagger
(Section 6.1.4).
6.1.1 Stacked Dependency Parsing. For syntactic analysis we use the open-source Malt-
Parser (Nivre, Hall, and Nilsson 2006), a platform for data-driven dependency parsing.
For improved accuracy and portability across domains and genres, we make our parser
incorporate the predictions of a large-scale, general-purpose Lexical-Functional Gram-
mar parser. A technique dubbed parser stacking enables the data-driven parser to learn
from the output of another parser, in addition to gold-standard treebank annotations
(Martins et al 2008; Nivre and McDonald 2008). This technique has been shown to
provide significant improvements in accuracy for both English and German (?vrelid,
Kuhn, and Spreyer 2009), and a similar set-up using an HPSG grammar has been shown
to increase domain independence in data-driven dependency parsing (Zhang andWang
2009). The stacked parser used here is identical to the parser described in ?vrelid,
Kuhn, and Spreyer (2009), except for the preprocessing in terms of tokenization and
PoS tagging, which is performed as detailed in Sections 2.1?2.2. The parser combines
two quite different approaches?data-driven dependency parsing and ?deep? parsing
with a hand-crafted grammar?and thus provides us with a broad range of different
types of linguistic information to draw upon for the speculation resolution task.
MaltParser is based on a deterministic parsing strategy in combination with
treebank-induced classifiers for predicting parse transitions. It supports a rich feature
representation of the parse history in order to guide parsing andmay easily be extended
to take into account additional features. The procedure to enable the data-driven parser
to learn from the grammar-driven parser is quite simple. We parse a treebank with
the XLE platform (Crouch et al 2008) and the English grammar developed within the
ParGram project (Butt et al 2002). We then convert the LFG output to dependency
structures, so that we have two parallel versions of the treebank?one gold-standard
and one with LFG annotation. We extend the gold-standard treebank with additional
information from the corresponding LFG analysis and train MaltParser on the
enhanced data set. For a description of the parse model features and the dependency
substructures proposed by XLE for each word token, see Nivre and McDonald (2008).
For further background on the conversion and training procedures, see ?vrelid, Kuhn,
and Spreyer (2009).
Table 5 shows the enhanced dependency representation for the sentence in Ex-
ample (13). For each token, the parsed data contains information on the word form,
lemma, and PoS, as well as the head and dependency relation (last two columns). The
added XLE information resides in the Features column and in the XLE-specific head and
dependency columns (XHead and XDep). Parser outputs, which in turn form the basis
for our scope resolution rules, also take this same form. The parser used in this work is
trained on the Wall Street Journal Sections 2?24 of the Penn Treebank (PTB), converted
383
Computational Linguistics Volume 38, Number 2
Table 5
Stacked dependency representation of the sentence in Example (13), lemmatized and annotated
with GENIA PoS tags, Malt parses (Head,DepRel), and XLE parses (XHead, XDep), as well as
other morphological and lexical semantic features extracted from the XLE analysis (Features).
Id Form PoS Features XHead XDep Head DepRel
1 The DT _ 4 SPECDET 4 NMOD
2 unknown JJ degree:attributive 4 ADJUNCT 4 NMOD
3 amino JJ degree:attributive 4 ADJUNCT 4 NMOD
4 acid NN pers:3|case:nom|num:sg|ntype:common 3 SUBJ 5 SBJ
5 may MD mood:ind|subcat:MODAL|tense:pres|clauseType:decl 0 ROOT 0 ROOT
6 be VB _ 7 PHI 5 VC
7 used VBN subcat:V-SUBJ-OBJ|vtype:main|passive:+ 5 XCOMP 6 VC
8 by IN _ 9 PHI 7 LGS
9 these DT deixis:proximal 10 SPECDET 10 NMOD
10 species NNS num:pl|pers:3|case:obl|common:count|ntype:common 7 OBL-AG 8 PMOD
11 . . _ 0 PUNC 5 P
to dependency format (Johansson andNugues 2007) and extendedwith XLE features, as
described previously. Parsing uses the arc-eager mode of MaltParser and an SVMwith a
polynomial kernel. When tested using 10-fold cross validation on the enhanced PTB, the
parser achieves a labeled accuracy score of 89.8, which is lower than the current state-
of-the-art for transition-based dependency parsers (to wit, the 91.8 score of Zhang and
Nivre 2011, although not directly comparable given that they test exclusively on WSJ
Section 23), but with the advantage of providing us with the deep linguistic information
from the XLE.
6.1.2 Rule Overview. Our scope resolution rules take as input a parsed sentence that has
been further tagged with speculation cues. We assume the default scope to start at the
cue word and span to the end of the sentence (modulo punctuation), and this scope also
provides the baseline when evaluating our rules.
In developing the rules, we made use of the information provided by the guidelines
for scope annotation in the BioScope corpus (Vincze et al 2008), combined with manual
inspection of the training data in order to further generalize over the phenomena
discussed by Vincze et al (2008) and work out interactions of constructions for various
types of cues. In the following, we discuss broad classes of rules, organized by categories
of speculation cues. An overview is also provided in Table 6, detailing the source of the
syntactic information used by the rule; MaltParser (M) or XLE (X). Note that, as there is
no explicit representation of phrase or clause boundaries in our dependency universe,
we assume a set of functions over dependency graphs, for example, finding the left- or
rightmost (direct) dependent of a given node, or recursively selecting left- or rightmost
descendants.
Coordination. The dependency analysis of coordination provided by our parser makes
the first conjunct the head of the coordination. For cues that are coordinating conjunc-
tions (PoS tag CC), such as or, we define the scope as spanning the whole coordinate
structure, that is, start scope is set to the leftmost dependent of the head of the coordina-
tion, and end scope is set to its rightmost dependent (conjunct). This analysis provides
us with coordinations at various syntactic levels, such as NP and N, AP and AdvP, or
VP as in Example (14):
(14) [...] the binding interfaces are more often {kept ?or? even reused} rather
than lost in the course of evolution.
384
Velldal et al Rules, Rankers, and the Role of Syntax
Table 6
Overview of dependency-based scope rules with information source (MaltParser or XLE),
organized by the triggering PoS of the cue.
PoS Description Source
cc Coordinations scope over their conjuncts M
in Prepositions scope over their argument with its descendants M
jjattr Attributive adjectives scope over their nominal head and its descendants M
jjpred Predicative adjectives scope over referential subjects and clausal arguments, M, X
if present
md Modals inherit subject-scope from their lexical verb and scope over their M, X
descendants
rb Adverbs scope over their heads with its descendants M
vbpass Passive verbs scope over referential subjects and the verbal descendants M, X
vbrais Raising verbs scope over referential subjects and the verbal descendants M, X
* For multiword cues, the head determines scope for all elements
* Back off from final punctuation and parentheses
Adjectives.We distinguish between adjectives (JJ) in attributive (nmod) function and adjec-
tives in predicative (prd) function. Attributive adjectives take scope over their (nominal)
head, with all its dependents, as in Example (15):
(15) The {?possible? selenocysteine residues} are shown in red, [...]
For adjectives in a predicative function the scope includes the subject argument of the
head verb (the copula), as well as a (possible) clausal argument, as in Example (16). The
scope does not, however, include expletive subjects, as in Example (17).
(16) Therefore, {the unknown amino acid, if it is encoded by a stop codon, is
?unlikely? to exist in the current databases of microbial genomes}.
(17) [...] it is quite {?likely? that there exists an extremely long sequence that is
entirely unique to U}.
Verbs. The scope of verbal cues is a bit more complex and depends on several factors.
In our rules, we distinguish passive usages from active usages, raising verbs from non-
raising verbs, and the presence or absence of a subject-control embedding context. The
scopes of both passive and raising verbs include the subject argument of their head
verb, as in Example (18), unless it is an expletive pronoun, as in Example (19).
(18) {Genomes of plants and vertebrates ?seem? to be free of any recognizable
Transib transposons} (Figure 1).
(19) It has been {?suggested? that unstructured regions of proteins are often
involved in binding interactions, particularly in the case of transient
interactions} 77.
In the case of subject control involving a speculation cue, specifically modals, sub-
ject arguments are included in scopes where the controller heads a passive construction
or a raising verb, as in our running Example (13).
385
Computational Linguistics Volume 38, Number 2
In general, the end scope of verbs should extend over the minimal clause that
contains the verb in question. In terms of dependency structures, we define the clause
boundary as comprising the chain of descendants of a verb which is not intervened by
a token with a higher attachment in the graph than the verb in question.
Prepositions and Adverbs. Cues that are tagged as prepositions (including some com-
plementizers) take scope over their argument, with all its descendants, Example (20).
Adverbs take scope over their head with all its (non-subject) syntactic descendants
Example (21).
(20) {?Whether? the codon aligned to the inframe stop codon is a nonsense
codon or not} was neglected [...]
(21) These effects are {?probably? mediated through the 1,25(OH)2D3
receptor}.
Multiword Cues. In the case of multiword cues, such as indicate that or either. . . or, we set
the scope of the unit as a whole to the maximal scope encompassing the scopes of both
units.
As an illustration of processing by the rules, consider our running Example (13),
with its syntactic analysis as shown in Table 5 and the dependency graph depicted
in Figure 2. This example invokes a variety of syntactic properties, including parts of
speech, argumenthood, voice, and so on. Initially, the scope of the speculation cue is
set to default scope. Then the subject control rule is applied, it checks the properties of
the verbal argument used, going through a chain of verbal dependents (VC) from the
modal verb may (indicated in red in Figure 2). Because it is marked as passive in
the LFG analysis (+pass), the start scope is set to include the subject of the cue word
(the leftmost descendant [NMOD] of its SBJ dependent, indicated in green in Figure 2).
6.1.3 Evaluating the Rules. Table 7 summarizes scope resolution performance (viewed as
a subtask in isolation) against both the CoNLL-2010 shared task training data (BSA and
BSP) and held-out evaluation data (BSE), using gold-standard cues. First of all, we note
that the default scope baseline, that is, unconditionally extending the scope of a cue to
the end of the sentence, yields much better results for the abstracts than the full papers.
The main reason is simply that the abstracts contain almost no cases of sentence final
bracketed expressions (e.g., citations and in-text references). Our scope rules improve
on the baseline by only 3.8 percentage points on the BSA data (F1 up from 69.84 to
Figure 2
Dependency representation for Example (13), indicating rule processing of the cue word may.
386
Velldal et al Rules, Rankers, and the Role of Syntax
Table 7
Resolving the scope of gold-standard speculation cues in the development and held-out data
using the dependency rules. For Default, the scope for each cue is always taken to span
rightwards to the end of the sentence.
Data Configuration F1
B
S
A Default 69.84
Dependency Rules 73.67
B
S
P Default 45.21
Dependency Rules 72.31
B
S
E Default 46.95
Dependency Rules 66.60
73.67). For BSP, however, we find that the rules improve on the baseline by as much as
27 points (up from 45.21 to 72.31). Similarly for the papers in the held-out BSE data, the
rules improve the F1 by 19.7 points (F1 up from 46.95 to 66.60).
Comparing to the result on the training data, we observe a substantial drop in
performance on the held-out data. There are several possible explanations for this effect.
First of all, there may well be some degree of overfitting of our rules to the training
data. The held-out data may contain speculation constructions that are not covered
by our current set of scope rules, or annotation of parallel constructions may in some
cases differ in subtle ways (see Section 6.1.5). The overfitting effects caused by the data
dependencies introduced by the various GENIA-based domain adaptation steps, as
described in Section 2.3, must also be taken into account.
6.1.4 PoS Tagging and Domain Variation. As mentioned in Section 6.1.1, an advantage of
stacking with a general-purpose LFG parser is that it can be expected to aid domain
portability. Nonetheless, substantial differences in domain and genre are bound to
negatively affect syntactic analysis (Gildea 2001), and our parser is trained on financial
news. MaltParser presupposes that inputs have been PoS tagged, however, leaving
room for variation in preprocessing. In this article we have aimed, on the one hand,
to make parser inputs conform as much as possible to the conventions established in its
PTB training data, while on the other hand taking advantage of specialized resources
for the biomedical domain.
To assess the impact of improved, domain-adapted inputs on our scope resolution
rules, we contrast two configurations: Running the parser in the exact same manner
as ?vrelid, Kuhn, and Spreyer (2009)?the first configuration uses TreeTagger (Schmid
1994) and its standard model for English (trained on the PTB) for preprocessing. In
the second configuration the parser input is provided by the refined GENIA-based
preprocessing described in Section 2.2. Evaluating the two modes of preprocessing on
the BSP subset of BioScope using gold-standard speculation cues, our scope resolution
rules achieve an F1 of 66.31 when using TreeTagger parser inputs, and 72.31 (see Table 7)
using our GENIA-based tagging and tokenization combination. These results underline
the importance of domain adaptation for accurate syntactic analysis.
6.1.5 Error Analysis. In Section 5.2.3 we discussed BioScope inter-annotator agreement
rates for the cue-level. Focusing only on the cases where the annotators agree with the
final gold-standard cues (as resolved by the chief annotator), Vincze et al (2008) report
387
Computational Linguistics Volume 38, Number 2
the scope-level F1 of the two annotators toward the gold standard to be 66.72 / 89.67 for
BSP. Comparing the decisions of the two annotators directly (i.e., treating one of the
annotations as gold-standard) yields an F1 of 62.50.
Using gold-standard cues, our scope resolution rules fail to exactly replicate the
target annotation in 185 (of 668) cases in the papers portion of the training material
(BSP), corresponding to an F1 of 72.31 as seen in Table 7. Two of the authors, who
are both trained linguists, performed a manual error analysis of these 185 cases. They
classify 156 (84%) as genuine system errors, 22 (12%) as likely3 annotation errors,
and the remaining 7 cases as involving controversial or seemingly arbitrary decisions
(?vrelid, Velldal, and Oepen 2010). Out of the 156 system errors, 85 (55%) were deemed
as resulting from missing or defective rules, and 71 system errors (45%) resulted from
parse errors. The latter were annotated as parse errors even in cases where there was
also a rule error.
The two most frequent classes of system errors pertain to (a) the recognition of
phrase and clause boundaries and (b) not dealing successfully with relatively superficial
properties of the text. Examples (22) and (23) illustrate the first class of errors, where
in addition to the gold-standard annotation we use vertical bars (?|?) to indicate scope
predictions of our system.
(22) [. . . ] {the reverse complement |mR of m will be ?considered? to . . . ]|}
(23) This |{?might? affect the results} if there is a systematic bias on the
composition of a protein interaction set|.
In our syntax-driven approach to scope resolution, system errors will almost always
correspond to a failure in determining constituent boundaries, in a very general sense.
In Example (22), for instance, the parser has failed to correctly locate the head of the
subject. Example (23), however, is specifically indicative of a key challenge in this task,
where adverbials of condition, reason, or contrast frequently attach within the depen-
dency domain of a speculation cue, yet are rarely included in the scope annotation.
For these system errors, the syntactic analysis may well be correct, although additional
information is required to resolve the scope.
Example (24) demonstrates our second frequent class of system errors. One in six
items in the BSP training data contains a sentence-final parenthesized element or trailing
number (e.g., Examples [18] or [19]); most of these are bibliographic or other in-text
references, which are never included in scope annotation. Hence, our system includes a
rule to ?back out? from trailing parentheticals; in cases such as Example (24), however,
syntax does not make explicit the contrast between an in-text reference versus another
type of parenthetical.
(24) More specifically, {|the bristle and leg phenotypes are ?likely? to result
from reduced signaling by Dl| (and not by Ser)}.
3 In some cases, there is no doubt that annotation is erroneous, that is, in violation of the available
annotation guidelines (Vincze et al 2008) or in conflict with otherwise unambiguous patterns. In other
cases, however, judgments are necessarily based on our own generalizations (e.g., assumptions about
syntactic analyses implicit in the BioScope annotations). Furthermore, selecting items for manual analysis
that do not align with the predictions made by our scope resolution rules is likely to bias our sample, such
that our estimated proportion of 12% annotation errors cannot be used to project an overall error rate.
388
Velldal et al Rules, Rankers, and the Role of Syntax
Moving on to apparent annotation errors, the rules for inclusion (or not) of the
subject in the scope of verbal speculation cues and decisions on boundaries (or internal
structure) of nominals seem problematic?as illustrated in Examples (25) and (26).4
(25) [. . . ] and |this is also {?thought? to be true for the full protein interaction
networks we are modeling}|.
(26) [. . . ] |redefinition of {one of them is ?feasible?}|.
Finally, the difficult corner cases invoke non-constituent coordination, ellipsis,
or NP-initial focus adverbs?and of course interactions of the phenomena discussed
herein. Without making the syntactic structures assumed explicit, it is often very diffi-
cult to judge such items.
6.2 A Data-Driven Approach Using an SVM Constituent Ranker
The error analysis indicated that it is often difficult to use dependency paths to define
phenomena that actually correspond to syntactic constituents. Furthermore, we felt that
the factors governing scope resolution would be better expressed in terms of soft con-
straints instead of absolute rules, thus enabling the scope resolver to consider a range
of relevant (potentially competing) contextual properties. In this section we describe
experiments with a novel approach to determining the in-sentence scope of speculation
that, rather than usingmanually defined heuristics operating on dependency structures,
instead uses a data-driven approach, ranking candidate scopes on the basis of constituent
trees. More precisely, our parse trees are licensed by the LinGO English Resource Gram-
mar (ERG; Flickinger [2002]), a general-purpose, wide-coverage grammar couched in
the framework of an HPSG (Pollard and Sag 1987, 1994). The approach rests on two
main assumptions: Firstly, that the annotated scope of a speculation cue corresponds to
a syntactic constituent and secondly, that we can automatically learn a ranking function
that selects the correct constituent.
Our ranking approach to scope resolution is abstractly related to statistical parse
selection, and in particular work on discriminative parse selection for unification based
grammars, such as those by Johnson et al (1999), Riezler et al (2002), Malouf and
van Noord (2004), and Toutanova et al (2005). The overall goal is to learn a function
for ranking syntactic structures, based on training data that annotates which tree(s) are
correct and incorrect for each sentence. In our case, however, rather than discriminating
between complete analyses for a given sentence, we want to learn a ranking function
over candidate subtrees (i.e., constituents) within a parse (or possibly evenwithin several
parses). Figure 3 presents an example derivation tree that represents a complete HPSG
analysis. Starting from the cue and working through the tree bottom?up, there are three
candidate constituents to determine scope (marked in bold), each projecting onto a
substring of the full utterance, and each including at least the cue. Note that in the case
of multiword cues the intersection of each word?s candidates is selected, ensuring that
all cues appear within the scope projected by the candidate constituents.
The training data is then defined as follows. Given a parsed BioScope sentence,
the subtree that corresponds to the annotated scope for a given speculation cue will
4 As in the presentation of system errors, we include scope predictions of our own rules here too, which
we believe to be correct in these cases. Also in this class of errors, we find the occasional ?uninteresting?
mismatch, for example related to punctuation marks and inconsistencies around parentheses.
389
Computational Linguistics Volume 38, Number 2
Figure 3
An example derivation tree. Internal nodes are labeled with ERG rule identifiers; common HPSG
constructions near the top (e.g., subject-head, head-complement, adjunct-head), and lexical rules
(e.g., passivization of verbs or plural formation of nouns) closer to the leaves. The preterminals
are so-called LE types, corresponding to fine-grained parts of speech and reflecting close to a
thousand lexical distinctions.
be labeled as correct. Any other remaining constituents that also span the cue are
labeled as incorrect. We then attempt to learn a linear SVM-based scoring function that
reflects these preferences, using the implementation of ordinal ranking in the SVMlight
toolkit (Joachims 2002). Our definition of the training data, however, glosses over two
important details.
Firstly, the grammar will usually license not just one, but thousands or even hun-
dreds of thousands of different parses for a given sentence which are ranked by an
underlying parse selection model. Some parses may not necessarily contain a subtree
that aligns with the annotated scope.We therefore experiment with defining the training
data relative to n-best lists of available parses. Secondly, the rate of alignment between
annotated scopes and constituents of parsing results indicates the upper-bound per-
formance: For inputs where no constituents align with the correct scope substring,
a correct prediction will not be possible. Searching the n-best parses for alignments
enables additional instances of scope to be presented to the learner, however.
In the following, Section 6.2.1 summarizes the general parsing setup for the ERG, as
well as our rationale for the use of HPSG. Section 6.2.2 provides an empirical assessment
of the degree to which ERG analyses can be aligned with speculation scopes in BioScope
and reviews some frequent sources of alignment failures. After describing our feature
types for representing candidate constituents in Section 6.2.3, Section 6.2.4 details the
tuning of feature configurations and other ranker parameters. Finally, Section 6.2.5
provides an empirical assessment of stand-alone ranker performance, before we discuss
the integration of the dependency rules with the ranking approach in Section 6.3.
6.2.1 Basic Set-up: Parsing Biomedical Text Using the ERG. At some level of abstraction,
the approach to grammatical analysis embodied in the ERG is quite similar to the LFG
parser that was ?stacked? with our data-driven dependency parser in Section 6.1.1?
both are commonly considered comparatively ?deep? (and thus costly) approaches to
syntactic analysis. Judging from the BioScope annotation guidelines, subtle grammat-
ical distinctions are at play when determining scopes, for example, different types of
control verbs, expletives, or passivization (Vincze et al 2008). In contrast to the LFG
framework (with its distinction between so-called constituent and functional struc-
tures), the analyses provided by the ERG offer the convenience of a single syntactic
390
Velldal et al Rules, Rankers, and the Role of Syntax
representation?HPSG derivation trees, as depicted in Figure 3?where all contextual
information that we expect to be relevant for scope resolution is readily accessible.
For parsing biomedical text using the ERG, we build on the same preprocessing
pipeline as described in Section 2. A lattice of tokens annotated with parts of speech
and named entity hypotheses contributed by the GENIA tagger is input to the PET
HPSG parser (Callmeier 2002), a unification-based chart parser that first constructs a
packed forest of candidate analyses and then applies a discriminative parse ranking
model to selectively enumerate an n-best list of top-ranked candidates (Zhang, Oepen,
and Carroll 2007). To improve parse selection for this kind of data, we re-trained the
discriminative model following the approach of MacKinlay et al (2011), combining
gold-standard out-of-domain data from existing ERG treebanks with a fully automated
procedure seeking to take advantage of syntactic annotations in the GENIA Treebank.
Although we have yet to pursue domain adaptation in earnest and have not systemat-
ically optimized the parse selection component for biomedical text, model re-training
contributed about a one-point F1 improvement in stand-alone ranker performance over
the parsed subset of BSP (compare to Table 8).
As the ERG has not previously been adapted to the biomedical domain, unknown
word handling in the parser plays an important role. Here we build on a set of
somewhat underspecified ?generic? lexical entries for common open-class categories
provided by the ERG (thus complementing the 35,000-entry lexicon that comes with the
grammar), which are activated on the basis of PoS and NE annotation from preprocess-
ing. Other than these, there are no robustness measures in the parser, such that syntactic
analysis will fail in a number of cases, to wit, when the ERG is unable to derive a
complete, well-formed syntactic structure for the full input string. In this configuration,
the parser returns at least one derivation for 91.2% of all utterances in BSA, and 85.6%
and 81.4% for BSP and BSE, respectively.
6.2.2 Alignment of Constituents and Scopes. The constituent ranking approach makes ex-
plicit an assumption that is also present at the core of our dependency-based heuristics
(viz., the expectation that scope boundaries align with the boundaries of syntactically
meaningful units). This assumption is motivated by general BioScope annotation prin-
ciples, as Vincze et al (2008) suggest that the ?scope of a keyword can be determined
on the basis of syntax.? To determine the degree to which ERG analyses conform to this
expectation, we computed the ratio of alignment between scopes and constituents (over
parsed sentences) in BioScope, considering various sizes of n-best lists of parses. To
improve alignment we also apply a small number of slackening heuristics. These
rules allow (a) minor adjustments of scope boundaries around punctuation marks
Table 8
Ranker optimization on BSP: Showing ranker performance for various feature type
combinations compared with a random-choice baseline, only considering instances
where the gold-standard scope aligns to a constituent within the 1-best parse.
Features F1
Baseline 26.76
Path 78.10
Path+Surface 79.93
Path+Linguistic 83.72
Path+Surface+Linguistic 85.30
391
Computational Linguistics Volume 38, Number 2
(specifically, utterance-final punctuation is never included in BioScope annotations, yet
the ERG analyzes most punctuation marks as pseudo-affixes on lexical tokens; see Fig-
ure 3). Furthermore, the slackening rules (b) reduce the scope of a constituent to the right
when it includes a citation (see the discussion of parentheticals in Section 6.1.5); (c) re-
duce the scope to the left when the left-most terminal is an adverb and is not the cue; and
(d) ensure that the scope starts with the cue when the cue is a noun. Collectively, these
rules improve alignment (over parsed sentences) in BSP from 74.10% to 80.54%, when
only considering the syntactic analysis ranked most probable by the parse selection
model. Figure 4 further depicts the degree of alignment between speculation scopes and
constituents in the n-best derivations produced by the parser, again after application of
the slackening rules. Alignment when inspecting only the top-ranked parse is 84.37%
for BSA and 80.54% for BSP. Including the top 50-best derivations improves alignment
to 92.21% and 88.93%, respectively. Taken together with an observed parser coverage of
85.6% for BSP, these results mean that for only about 76% of all utterances in BSP can
the ranker potentially identify a constituent matching the gold-standard scope.
To shed some light on the cases where we fail to find an alignment, we manually
inspected all utterances in the BSP segment for which there were (a) syntactic analyses
available from the ERG and (b) no candidate constituents in any of the top-fifty parses
that mapped onto the gold-standard scope (after the application of the slackening rules).
The most interesting cases from this non-alignment analysis are ones judged as ?non-
syntactic? (25% of the total mismatches), which we interpret as violating the assumption
of the annotation guidelines under any possible interpretation of syntactic structure.
Following are select examples in this category:
(27) This allows us to {?address a number of questions?: what proportion of
each organism?s protein interaction network [. . . ] can be attributed to a
known domain-domain interaction}?
(28) As {?suggested? in 18, by making more such data sets available, it will be
possible to [. . . ] determine the most likely human interactions}.
Figure 4
The effect of incrementally including additional derivations from the n-best list when searching
for an alignment between a speculation scope and a constituent. Plots are shown for the BSA and
BSP subsets of the training data.
392
Velldal et al Rules, Rankers, and the Role of Syntax
(29) The {lack of specificity ?might? be attributed to a number of reasons, such
as the absence of other MSL components, the presence of other RNAs
interacting with MOF}, or worse [. . . ].
(30) [. . . ] thereby making {the use of this objective function ? and exploration
of other complex objective functions ? ?possible?}.
Example (27) is representative of a handful of similar cases, where complete sen-
tences are (implicitly or overtly) conjoined, yet the scope annotation encompasses only
part of one of the sentences. Example (28) is in a similar spirit, only in this case a
topicalized prepositional phrase (and hence an integral constituent) is only partially
included in the gold-standard scope. Although our slackening heuristics address a
number of cases of partial noun phrases (with a left scope boundary right before the
head noun or a pre-head attributive adjective), another handful of non-syntactic scopes
are of the type exemplified by Example (29), a class observed earlier already in the error
analysis of our dependency-based scope resolution rules (see Section 6.1.5). Finally,
Example (30) demonstrates one of many linguistically subtle corner cases: The causative
make in standard analyses of the resultative construction takes two arguments, namely,
an NP (the use of this objective function. . . ) and a predicative phrase (possible).
Alongside cases like these, our analysis considered 16% of mismatches owed to
divergent syntactic theories (i.e., structures that in principle can be analyzed in amanner
compatible with the BioScope gold-standard annotations, yet do not form matching
constituents in the ERG analyses). The by far largest class of mismatches was attributed
to parse ranking deficiencies: In close to 40% of cases, the ERG is capable of deriving
a constituent structure compatible with the scope annotations, but no such analysis
was available within the top 50 parses. Somewhat reassuringly, less than 6% of all
mismatches were classified as BioScope annotation errors, whereas a majority of re-
maining mismatches are owed to the recurring issue of parentheticals and bibliographic
references (see examples in Section 6.1.5).
6.2.3 Features of Candidate Scopes. We use three families of features to describe candi-
date constituents. Given our working hypothesis that scopes are aligned with syn-
tactic constituents, the most natural features to use are the location of constituents
within trees. We define these in terms of the paths from speculation cues to can-
didate constituents. For example, the correct candidate in Figure 3 has the feature
v_vp_mdl-p_le\hd-cmp_u_c\sb-hd_mc_c. We include both lexicalized and unlexical-
ized versions of this feature. As traversal from the cue to the candidate can involve
many nodes, we also include a more general version recording only the cue and the
root of the candidate constituent (rather than the full path including all intermediate
nodes). In a similar spirit we also generate bigram features for each path node and its
parent.
In addition to the given path features, we also exploit features describing the
surface properties of scope candidates. These include the enumeration of bigrams of
the preterminal lexical types, the cue position within the candidate (in tertile bins
relative to the candidate length), and the candidate size (in quartile bins relative to the
sentence length). Because punctuation may also be informative for scope resolution, we
also record whether punctuation was present at the end of the terminal preceding the
candidate or at the end of its right-most terminal.
The third family of features is concerned with specific linguistic phenomena de-
scribed in the BioScope annotation guidelines (Vincze et al 2008) or observed when
393
Computational Linguistics Volume 38, Number 2
developing the rules in Section 6.1. These include detection of passivization, subject
control verbs occurring with passivized verbs, subject raising verbs, and predicative
adjectives. Furthermore, these features are only activated when the subject of the con-
struction is not an expletive pronoun, and they are represented by appending the type
of phenomenon observed to the path features described here.
6.2.4 Ranker Optimization. We conducted several experiments designed to find an
optimal configuration of features. Table 8 lists the results of combinations of the fea-
ture families on the BSP data set when using gold-standard cues, reporting 10-fold
cross-validated F1 scores with respect to only the instances where the gold-standard
speculation scope aligns with constituents (i.e., the ?ideal circumstances? for the
ranker). The table also lists results for a random-choice baseline, calculated as the
mean ambiguity of each instance (i.e., the averaged reciprocal of the number of can-
didates). The feature optimization results indicate that each feature family is infor-
mative, and that the best result can be obtained by using all three in conjunction.
The comparatively largest improvement in ranker performance is obtained from the
?rule-like? linguistic feature family, which is noteworthy in two respects: First, our
current system includes only four such features, and second, these features parallel
some of the dependency-based rules of Section 6.1.2?suggesting that subtle syntactic
configurations are an important component also in our data-driven approach to scope
resolution.
As discussed in Section 6.2.2 and depicted in Figure 4, searching the best-ranked
parses can greatly increase the number of aligned constituents and thus improve the
upper-bound potential of the ranker. We therefore experimented with training using
the first aligned constituent in n-best derivations. At the same time we varied the
m-best derivations used during testing, using features from all m derivations. We found
that performance did not vary greatly, but that the best result was achieved when
n = 1 and m = 3 (note, however, that such optimization over n-best lists of ERG parses
will play a much greater role in the hybrid approach to scope resolution developed
in Section 6.3). As explained in Section 5.2.1, all experiments use the SVMlight de-
fault value for the regularization parameter, determined analytically from the training
data.
A cursory error analysis conducted over aligned items in BSP indicated similar
errors to those discussed in connection with the dependency rules (see Section 6.1.5).
There are a number of instances where the predicted scope is correct according to the
BioScope annotation guidelines, but the annotated scope is incorrect. We also note some
instances where the rule-like linguistic features are activated on the correct constituent,
but the ranker nevertheless selects a different candidate. In a strictly rule-based system,
these features would act as hard constraints and yield superior results in these cases.
Therefore, these instances seem a prime source of inspiration for further improvements
to the ranker in future work.
6.2.5 Evaluating the Ranker. Table 9 summarizes the performance of the constituent
ranker (coupledwith the default scope baseline in the case of unparsed items) compared
with the dependency rules, resolving the scope of both gold-standard and predicted
speculation cues. We note that the constituent ranker performs slightly superior to the
dependency rules on BSA but inferior (though well above the default scope baseline)
on BSP and BSE. Applying the sign-test (in the manner described in Section 3.2) to the
scope-level performance of the ranker and the rules on the held-out BSE data (using
gold-standard cues), the differences are found to be statistically significant.
394
Velldal et al Rules, Rankers, and the Role of Syntax
Table 9
Resolving the scope of speculation cues using the dependency rules, the constituent ranker,
and their combination. Whereas table (a) shows results for gold-standard cues, table (b) shows
end-to-end results for the cues predicted by the classifier of Section 5.2. Results are shown both
for the BioScope development data (for which both the scope ranker and the cue classifier is
applied using 10-fold cross-validation) and the CoNLL-2010 Shared Task evaluation data.
Data System F1
B
S
A
Rules 73.67
Ranker 75.48
Combined 79.56
B
S
P
Rules 72.31
Ranker 66.17
Combined 75.15
B
S
A
P Rules 73.40
Ranker 73.61
Combined 78.69
B
S
E
Rules 66.60
Ranker 58.37
Combined 69.60
(a) Resolving Gold-Standard Cues
Data System Prec Rec F1
B
S
A
Rules 72.47 66.42 69.31
Ranker 74.27 68.07 71.04
Combined 77.80 71.31 74.41
B
S
P
Rules 69.87 62.13 65.77
Ranker 62.63 55.69 58.95
Combined 72.05 64.07 67.83
B
S
A
P Rules 71.97 65.56 68.61
Ranker 71.99 65.59 68.64
Combined 76.67 69.85 73.11
B
S
E
Rules 58.95 54.21 56.48
Ranker 51.68 47.53 49.52
Combined 62.00 57.02 59.41
(b) Resolving Predicted Cues
Again we also observe a drop in performance for the results on the held-out data
comparedwith the development data.We attribute this drop partly to overfitting caused
by using GENIA abstracts to adapt the parse ranker to the biomedical domain (see
Section 2.3), but primarily to reduced parser coverage and constituent alignment in the
latter data sets. Improving these aspects should result in substantive gains in ranker
performance. Finally, note that the performance of the default baseline (which is much
better for the abstracts than the full papers of BSP and BSE) also carries over to ranker
performance for the cases where we do not have a parse.
6.3 Combining the Constituent Ranker and the Dependency Rules
Although both the constituent ranker and dependency rules perform well in isolation,
they do not necessarily perform well on the same test items. Consequently, we inves-
tigated the effects of combining their predictions. When ERG parses are available for
a given sentence, the dependency rules may be combined with the information used
by the constituent ranker. We implement this coupling by adding features that record
whether the (slackened) span of a candidate constituent matches the span of the scope
predicted by the rules (either exactly or just at one of its boundaries). When an ERG
parse is not available we simply revert to the prediction of the dependency rules.
Adding the rule prediction features may influence the effectiveness of considering
multiple parses, by compensating for the extra ambiguity. We therefore repeated our
examination of the effects of using the best-ranked parses for training and testing the
ranker. Figure 5 plots the effect on F1 for parsed sentences in BSP when including
constituents from the n-best derivations in training, and from the m-best derivations in
testing.We see that, when activating the dependency prediction features, the constituent
ranker performs best for n = 5 and m = 20.
Looking at the performance summaries of Table 9, we see that the combined ap-
proach consistently outperforms both the dependency rules and the constituent ranker
395
Computational Linguistics Volume 38, Number 2
Figure 5
Cross-validated F1 scores of the ranker combined with the dependency rules over gold cues
for parsed sentences from BSP, varying the maximum number of parse results employed for
training and testing.
in isolation, and the improvements are deemed significant with respect to both of them
(comparing results for BSE using gold-standard cues).
Comparing the combined approach to the plain ranker runs, there are two sources
for the improvements: the addition of the rule prediction features and the fact that we
fall back on using the rule predictions directly (rather than the default scope) when we
do not have an available constituent tree. To isolate the contribution of these factors,
we applied the ranker without the rule-prediction features (as in the initial ranker
set-up), but still using the rules as our fall-back strategy (as in the combined set-up).
Testing on BSP using gold-standard cues this gives an F1 of 69.61, meaning that the
8.98-percentage-point improvement of the combined model over the plain ranker owes
3.44 points to the rule-based fall-back strategy and 5.54 to the new rule-based features.
As discussed in Section 6.2.2, an important premise of the success of our ranking
approach is that scopes align with constituents. Indeed, we find that the performance
of both the ranker in isolation and the combined approach is superior on BSA, which is
the data set that exhibits the greatest proportion of aligned instances. We can therefore
expect that any improvements in our alignment procedure, as well as in the domain-
adapted ERG parse selection model, will also carry through to improve the overall
performance of our subtree ranking.
As a final evaluation of speculation resolution, Table 10 compares the end-to-end
performance of our combined approach with the best end-to-end performer in the
CoNLL-2010 Shared Task. In terms of both precision and recall, our cue classifier using
the combination of constituent ranking and dependency rules for scope resolution
achieves superior performance on BSE comparedwith the system ofMorante, van Asch,
and Daelemans (2010), improving on the overall F1 by more than 2 percentage points.
Whereas the token-level differences for cue classification are found to be significant, the
end-to-end scope-level differences are not (p = 0.39).
7. Porting the Speculation System to Negation
Dealing with negation in natural language has been a long-standing topic and there has
been work attempting to resolve the scope of negation in particular within the area of
sentiment analysis (Moilanen and Pulman 2007), where treatment of negation clearly
constitutes an important subtask and has been shown to provide improved sentiment
396
Velldal et al Rules, Rankers, and the Role of Syntax
Table 10
Final end-to-end results for scope resolution: Held-out testing on BSE, using the cue classifier
described in Section 5.2 while combining the dependency rules and the constituent ranker for
scope resolution. The results are compared to the system with the best end-to-end performance
in the CoNLL-2010 Shared Task (Morante, van Asch, and Daelemans 2010).
Cue Level Scope Level
System Configuration Prec Rec F1 Prec Rec F1
Cue classifier + Scope Rules & Ranking 84.79 77.17 80.80 62.00 57.02 59.41
Morante et al 2010 78.75 74.69 76.67 59.62 55.18 57.32
analysis (Councill, McDonald, and Velikovich 2010). The BioScope corpus (Vincze et al
2008), being annotated with negation as well as speculation, has triggered work on
negation detection in the biomedical domain as well. In this setting, there are a few
previous studies where the same system architecture has been successfully applied for
both speculation and negation. For example, whereas Morante and Daelemans (2009a)
try to resolve the scope of speculation using a system initially developed for negation
(Morante, Liekens, and Daelemans 2008), Zhu et al (2010) develop a system targeting
both tasks. In this section we investigate to what degree our speculation system can be
ported to also deal with negation, hoping that the good results obtained for speculation
will carry over to the negation task at a minimal cost in terms of adaption and modifica-
tion. We start by describing our experiments with porting the cue classifier to negation
in Section 7.1, and then present our modified set of dependency rules for resolving the
scope of the negation cues in Section 7.2. Section 7.3 presents the adaptation of the
constituent ranker, as well as the final end-to-end results when combining the ranker
and the rules, paralleling what we did for speculation. The relation to other relevant
work is discussed as we go along.
Some summarizing statistics for the negation annotations in BioScope were given
in Table 1. Note, however, that the additional evaluation data that we used for held-out
testing of the speculation system, does not contain negation annotations. For this reason,
and in order to be able to compare our results to those obtained in previous studies, we
here follow the partitioning established by Morante and Daelemans (2009b), reporting
10-fold cross-validation (for the cue classifier and the subtree ranker) on the abstracts
(BSA) and using the full papers (BSP) for held-out and cross text-type testing. Note
that for the development results using cross-validation, we partition the data on the
sentence-level, just as in Morante and Daelemans (2009b).
7.1 Identifying Negation Cues
Several previous approaches to detecting negation cues have been based on pre-
compiled lexicons, either alone (Councill, McDonald, and Velikovich 2010) or in combi-
nation with a learner (Morante and Daelemans 2009b). For the purpose of the current
article we wanted to investigate whether the ?filtered classification? approach that we
applied for detecting speculation cues would directly carry over to negation. Drawing
heavily on much of the discussion previously given for the speculation cue classifiers
in Section 5, the small modifications made to implement a classifier for negation cues
are described in Section 7.1.1. We then provide some discussion of the results in Sec-
tion 7.1.2, including comparison to previous work on negation cue detection byMorante
and Daelemans (2009b) and Zhu et al (2010) in Section 7.1.3.
397
Computational Linguistics Volume 38, Number 2
7.1.1 Classifier Description. Apart from re-tuning the feature configuration, the only
modifications that wemade with respect to the speculation classifier regard the rules for
multiword cues (as described for speculation in Section 5.1) and the corresponding stop-
list (Section 5.2). The overall approach, however, is the same:We train and apply a linear
SVM classifier that only considers words whose lemma has been observed as a negation
cue in the training data. Note that roughly 82% of the negation tokens are ambiguous in
the training data, in the sense that they have both cue and non-cue occurrences. Based
on the most frequently occurring MWC patterns observed in the abstracts we defined
post-processing rules to cover the cases shown in Table 11. Furthermore, and again
based on the MWCs, we compiled a small stop-list so that the classifier ignores certain
?spurious? tokens (namely, can, could, notable, of, than, the, with, and ?(?). Although this
of course means that the classifier will never label any such word as a cue, they will
typically be captured by the MWC rules instead.
When re-tuning the feature configuration based on the n-gram templates previously
described in Section 5.2.1, we find that the best performer for negation is the combina-
tion that records lemmas two positions to the left and the right of the target word, and
surface forms one position to the right.
7.1.2 Development Results. The performance of this model, evaluated by 10-fold cross-
validation on the BioScope abstracts, is shown in Table 12. Just as for speculation, we
also contrast the performance with a simple WbW majority usage baseline, classifying
each and every word according to its most frequent usage (cue vs. non-cue) in the
training data. Although this baseline proved to be surprisingly strong for speculation, it
is even stronger for negation: Evaluated at the token-level (though after the application
of the MWC rules) the baseline achieves an F1 of 93.60. Applying the filtering model
further improves this score to 96.00. The differences are found to be statistically signifi-
cant (according to the testing scheme described in Section 3.1), and the filtering classifier
also improves greatly with respect to the sentence-, and cue-level evaluations as well,
in particular with respect to the precision.
Recall that, when looking at the distribution of error types for the token-level
mistakes made by the speculation classifier (see Section 5.2.3), we found that almost 75%
were false negatives. The distribution of error types for the negation cue classifier is very
different: Almost 85% of the errors are false positives. After inspecting the actual cues
involved, we find the same situation as reported by Morante and Daelemans (2009b),
namely, that a very high number of the errors concern cases where not is labeled as a cue
by the classifier but not in the annotations. The same is true for the cue word absence,
and many of these cases appear to be annotation errors.
The class balance among tokens in the BioScope data is extremely skewed, with the
positive examples of negation constituting only 0.5% of the total number of examples.
Table 11
Patterns covered by our post-processing rules for multiword negation cues.
rather than
{can|could} not
no longer
instead of
with the * exception of
neither * nor
{no(t?)|neither} * nor
398
Velldal et al Rules, Rankers, and the Role of Syntax
Table 12
Results for negation cue detection, including the systems of Morante et al (2009b) and Zhu et al
(2010). Whereas the scores for BSA are obtained by 10-fold cross validation, the scores on BSP
and BSR represent held-out testing using a model trained on all the abstracts. The latter scores
thereby serves as a test of generalization performance across different text types within the
same domain.
Sentence Level Token Level Cue Level
Data Model Prec Rec F1 Prec Rec F1 Prec Rec F1
B
S
A
(1
0
-F
o
ld
) Baseline 90.34 98.81 94.37 89.28 98.40 93.60 88.92 97.78 93.14
Filtering 94.19 98.87 96.45 93.46 98.73 96.00 93.19 98.12 95.59
Morante n/a n/a n/a 84.72 98.75 91.20 94.15 90.67 92.38
Zhu n/a n/a n/a 94.35 94.99 94.67 n/a n/a n/a
B
S
P
(H
e
ld
-o
u
t) Baseline 79.48 99.41 88.34 75.96 99.00 85.96 74.55 98.41 84.84
Filtering 86.75 98.53 92.27 85.22 98.25 91.27 84.06 97.62 90.33
Morante n/a n/a n/a 87.18 95.72 91.25 85.55 78.31 81.77
Zhu n/a n/a n/a 87.47 90.48 88.95 n/a n/a n/a
B
S
R
(H
e
ld
-o
u
t) Baseline 96.64 96.42 96.53 96.12 96.01 96.06 95.87 95.98 95.93
Filtering 96.97 96.30 96.64 96.44 95.90 96.17 96.20 95.87 96.03
Morante n/a n/a n/a 97.33 98.09 97.71 96.38 91.62 93.94
Zhu n/a n/a n/a 88.54 86.81 87.67 n/a n/a n/a
In terms of the tokens actually considered by our filtering model, however, the numbers
look much healthier, with the negative examples actually being slightly outweighed
by the positives (just above 50%). Moreover, the average number of distinct n-gram
features instantiated across the 10-folds is approximately 17,500. The small size of the
feature set is of course due to the small number of training examples considered by the
learner: Whereas a WbW approach (like the majority usage baseline) would consider
every token in training data (just below 300,000 in each fold), this number is reduced
by almost 99% for the filtered disambiguation model. In effect, we can conclude that the
proposed approachmanages to combine very good results with very low computational
cost.
Figure 6 shows learning curves for both the word-by-word baseline and the fil-
tering model, plotting token-level F1 against percentages of data included in training.
Compared to the learning curves previously shown for speculation detection (Figure 1),
the curve for the filtering model seems to be somewhat flatter for negation. Looking at
the curve for the WbW unigram baseline, it again seems unable to benefit much from
any additional data after the first few increments.
7.1.3 Comparison to Related Work. To the best of our knowledge, the systems currently
achieving state-of-the-art results for detecting negation cues are those described by
Morante and Daelemans (2009b), Zhu et al (2010), and Councill, McDonald, and
Velikovich (2010). Although the latter work does not offer separate evaluation of the
cue detection scheme in isolation, Morante and Daelemans (2009b) and Zhu et al (2010)
provide cue evaluation for the data splits listed in Table 12; 10-fold cross-validation
experiments (with sentence-level partitioning) on the BioScope abstracts, and held-out
testing on the full papers and the clinical reports (with a model trained on the abstracts).
399
Computational Linguistics Volume 38, Number 2
Figure 6
Learning curves for both baseline and the filtered ?disambiguation? model showing the effect on
token-level negation cue F1 when including larger percentages (shown on a logarithmic scale)
of the training data across the 10-fold cycles on BSA.
The results5 reported by Morante and Daelemans (2009b) and Zhu et al (2010) are
token-level precision, recall, and F1. Having obtained the system output of Morante
and Daelemans (2009b), however, we also computed cue-level scores for their system.
Morante and Daelemans (2009b) identify cues using a small list of unambiguous
cue words compiled from the abstracts in combination with applying a decision tree
classifier to the remaining words. Their features record information about neighboring
word forms, PoS, and chunk information from GENIA. Zhu et al (2010) train an SVM
to classify tokens according to a BIO-scheme using surface-oriented n-gram features in
addition to various syntactic features extracted using the Berkley parser (Petrov and
Klein 2007) trained on the GENIA treebank. Looking at the results in Table 12, we see
that the performance of our cue classifier compares favorably with the systems of both
Morante and Daelemans (2009b) and Zhu et al (2010), achieving a higher cue-level F1
across all data sets (with differences in classifier decisions with respect to Morante and
Daelemans [2009b] being statistically significant for all of them).
For the 10-fold run, the biggest difference concerns token-level precision, where
both the system of Zhu et al (2010) and our own achieves a substantially higher score
than that of Morante and Daelemans (2009b). Turning to the cross-text experiment,
however, the precision of our system and that of Zhu et al (2010) suffers a large
drop, whereas the system of Morante and Daelemans (2009b) actually obtains a higher
precision than for the 10-fold run. These effects are reversed for recall, however, where
our system still maintains the higher score, also resulting in a higher F1. Looking at
the cue-level scores, we find that the precision of our system and that of Morante and
Daelemans (2009b) drops by an equal amount for the BSP cross-text testing. In terms of
recall, however, the cue-level scores of Morante and Daelemans (2009b) suffers a much
larger drop than that of our filtered classifier.
5 As the results reported by Morante and Daelemans (2009b) were inaccurate, we instead refer to values
obtained from personal communication with the authors.
400
Velldal et al Rules, Rankers, and the Role of Syntax
The drop in performance when going from cross-validation to held-out testing
can largely be attributed to the same factors discussed in relation to speculation
cues in Section 5.3 (e.g., GENIA-based pre-processing, sentence-level partitioning in
cross-validation, and unobserved MWCs). In addition, looking at the BioScope inter-
annotator agreement rates for negation cues it is not surprising that we should ob-
serve a drop in results going from BSA to BSP: Measured as the F1 of one of the
annotators with respect to the other, it is reported as 91.46 for BSA, compared with
79.42 for BSP (Vincze et al 2008). Turning to the F1-scores of each annotator with
respect to the final gold standard, the numbers are 91.71/98.05 for BSA and 86.77/91.71
for BSP.
The agreement rates for the clinical reports, on the other hand, are much closer to
those of the abstracts (Vincze et al 2008), and the held-out scores we observe on this data
set are generally also much better, not the least for the simple majority usage baseline.
In general the baseline again proves to be surprisingly competitive, most notably with
respect to recall where it actually outperforms all the other systems for both the cross-
text experiments. (Recall that the baseline scores also reflect the application of the MWC
rules, though.)
7.2 Adapting the Dependency Rules for Resolving Negation Scope
There have been several previous studies on resolving the scope of negation based on
the BioScope corpus. For example, Morante and Daelemans (2009b) present a meta-
learning approach that combines the output from three learners?a memory-based
model, an SVM classifier, and a CRF classifier?using lexical features, such as PoS and
chunk tags. Councill, McDonald, and Velikovich (2010) use a CRF learner with features
based on dependency parsing (e.g., detailing the PoS of the head and the dependency
path to the negation cue).
The annotation of speculation and negation in BioScope was performed using a
common set of principles. It therefore seems reasonable to assume our dependency-
based scope resolution rules for speculation should be general enough to allow porting
to negation with fairly limited efforts. On the other hand, negation is expressed linguis-
tically using quite different syntactic structures from speculation, so it is clear that some
modifications will be necessary as well.
As we recall, the dependency rules for speculation scope are triggered by the PoS
of the cue. Several of the same parts-of-speech (verbs, adverbs) also express negation.
As an initial experiment, therefore, we simply applied the speculation rules to negation
unmodified. As before, taking default scope to start at the cue word and spanning to
the end of the sentence provides us with a baseline system. We find that applying
our speculation scope rules directly to the task of negation scope resolution offers a
fair improvement over the baseline. For BSA and BSP, the default scope achieves F1
scores of 52.24 and 31.12, respectively, and the speculation rules applied directly without
modifications achieve 48.67 and 56.25.
In order to further improve on these results, we introduce a few new rules to
account specifically for negation. The general rule machinery is identical to the specu-
lation scope rules described in Section 6.1: The rules are triggered by the part of speech
of the cue and operate over the dependency representations output by the stacked
dependency parser described in Section 6.1.1. In developing the rules we consulted the
BioScope guidelines (Vincze et al 2008), as well as a descriptive study of negation in the
BioScope corpus (Morante 2010).
401
Computational Linguistics Volume 38, Number 2
Table 13
Additional dependency-based scope rules for negation, with information source (MaltParser or
XLE), organized by PoS of the cue.
PoS Description Source
DT Determiners scope over their head node and its descendants M
NN Nouns scope over their descendants M
NNnone none take scope over entire sentence if subject and otherwise over its descendants M
VB Verbs scope over their descendants M
RBvb Adverbs with verbal head scope over the descendants of the lexical verb M, X
RBother Adverbs scope over the descendants of the head M, X
7.2.1 Rule Overview. The added rules are presented in Table 13 and are described in more
detail subsequently, organized by the triggering PoS of the negation cue.
Determiners. Determiner cue words in BioScope are largely realized by the negative
determiner no. These take scope over their nominal head and its descendants, as seen in
Example (31):
(31) The finding that dexamethasone has {?no? effect on TPA-induced
activation of PKC} suggests [. . . ]
Nouns. Nominal cues take scope over their descendants (i.e., the members of the noun
phrase), as shown in Example (32).
(32) This unresponsiveness occurs because of a {?lack? of expression of the
beta-chain (accessory factor) of the IFN-gamma receptor}, while at the
same time [. . . ]
The negative pronoun none is tagged as a noun by our system, but deviates from regular
nouns in their negation scope: If the pronoun is a subject, it scopes over the remaining
sentence, as in Example (33), whereas in object function it simply scopes over the noun
phrase (Morante 2010). These are therefore treated specifically by our system.
(33) Similarly, {?none? of SCOPE?s component algorithms outperformed the
other ten programs on this data set by a statistically significant margin}.
Adverbs. Adverbs constitute the majority of negation cues and are largely realized by
the lexical item not. Syntactically, however, adverbs are a heterogeneous category. They
may modify a number of different head words and their scope will thus depend largely
on properties of the head. For instance, when an adverb is a nominal modifier, as in
Example (34), it has a narrow scope which includes only the head noun (34) and its
possible conjuncts.
(34) This report directly demonstrates that OTF-2 but {?not? OTF-1} regulates
the DRA gene
Verbal adverbs scope over the clause headed by the verbal head. As shown by Figure 2,
the parser?s analysis of verbal chains has the consequence that preverbal arguments and
modifiers, such as subjects and adverbs, are attached to the finite verb and postverbal
402
Velldal et al Rules, Rankers, and the Role of Syntax
arguments and modifiers are attached to the lexical verb, in cases where there is an
auxiliary. This rule thus locates the lexical verb (e.g., affect in Example [35]), in the
dependency path from the auxiliary head verb and defines scope over the descendants
of this verb. In cases where the lexical verb is passive, the subject is included in the scope
of the adverb, as in Example (36).
(35) IL-1 did {?not? affect the stability of the c-fos and c-jun transcripts}.
(36) {Levels of RNA coding for the receptor were ?not? modulated by exposure
to high levels of ligand}.
7.2.2 Evaluating the Negation Rules. The result of resolving the scope of gold-standard
negation cues using the new set of dependency rules (i.e., the speculation rules extended
with the negation specific rules of Table 13), are presented in Table 14, along with the
performance of the default scope baseline. First of all, we note that the baseline scores
provided by assigning default scope to all cues differ dramatically between the data sets,
ranging from an F1 of 52.24 for BSA, 31.12 for BSP, and 91.43 for BSR. In comparison,
the performance of the rules is fairly stable across BSA and BSP, and for both data sets
they improve substantially on the baseline (up by roughly 18.5 and 34.5 percentage
points on BSA and BSP, respectively). On BSR, however, the default scope baseline is
substantially stronger than for the other data sets, and even performs slightly better
than the rules. Recall from Table 1 that the average sentence length in the clinical reports
is substantially lower (7.7) than for the other data sets (average of 26), a property which
will make the default scope much more likely to succeed.
In order to shed more light on the performance of the rules on BSR, a manual
error analysis was performed, once again by two trained linguists working together. We
found that out of the total of 74 errors, 30 (40.5%) were parse errors, 29 (39.2%) were rule
errors, 8 (10.8%) were annotation errors, and 4 (5.4%) were undecided. Although it is
usually the case that short sentences are easier to parse, the reports contain a substantial
proportion of ungrammatical structures, such as missing subjects, dropped auxiliaries,
and bare noun phrases, as in Example (37), which clearly lead to lower parse quality,
resulting in 40% parse errors. There are also constructions, such as so-called run-on
constructions, as in Example (38), for which there is simply no correct analysis available
Table 14
Scope resolution for gold-standard negation cues across the BioScope sub-corpora.
Data Configuration F1
B
S
A
Default 52.24
Dependency Rules 70.91
Constituent Ranker 68.35
Combined 74.35
B
S
P
Default 31.12
Dependency Rules 65.69
Constituent Ranker 60.90
Combined 70.21
B
S
R
Default 91.43
Dependency Rules 90.86
Constituent Ranker 89.59
Combined 90.74
403
Computational Linguistics Volume 38, Number 2
within the dependency framework (which, for instance, requires that graphs should be
connected). In addition, the annotations of the reports data contain some idiosyncrasies
which the rules fail to reproduce. Twenty-four percent of the errors are found with the
same cue, namely, the adjective negative. The rules make attributive adjectives scope
over their nominal heads, whereas the BSR annotations define the scope to only cover
the cue word itself; see Example (37). The annotation errors were very similar to the
ones observed in the earlier error analysis of Section 6.1.5.
(37) |{?Negative?} chest radiograph|.
(38) |{?No? focal pneumonia}, normal chest radiograph|.
7.3 Adapting the Constituent Ranker for Negation
Adapting the SVM-based discriminative constituent ranker of Section 6.2 to also predict
the scope of negation is a straightforward procedure, requiring only minor modifi-
cations: Firstly, we developed a further slackening heuristic to ensure that predicted
scope does not begin with an auxiliary. Secondly, we augmented the family of linguistic
features to also record the presence of adverb cues with verbal heads (as specified by the
dependency-based scope rules in Table 13). Finally, we repeated the parameter tuning
for training with n-best and testing with m-best parses (as described in Section 6.2.4).
Performing 10-fold cross-validation on BSA using gold-standard negation cues, we
found that the optimal values for the ranker in isolation were n = 10 and m = 1.
When paralleling the combined approach developed in Section 6.3 (adding the rule-
predictions as a feature in the ranker while falling back on rule-predicted scope for
cases where we do not have an ERG parse) the optimal values were found to be n = 15
andm = 5. Examining the coverage of the parser and the alignment of constituents with
negation scope (considering the 50-best parses), we found that the upper-bound of the
constituent ranker (disregarding any fall-back strategy) on the BSA development set is
79.4% (compared to 83.6% for speculation).
Table 14 lists the performance of both the constituent ranker in isolation and the
combined approachwhen resolving the scope of gold-standard negation cues (reporting
10-fold cross-validation results for BSA, while using BSP and BSR for held-out testing).
We see that the dependency rules perform consistently better than the constituent
ranker, although the differences are not found to be statistically significant (the p-values
for BSA, BSP, and BSR are 0.06, 0.11, and 0.25, respectively). The combined approach
again outperforms the dependency rules on both BSA and BSP (and by a much larger
margin than we observed for speculation), however, with the improvements on both
data sets being significant. Just as we observed for the dependency rules in Section 7.2.2,
neither the constituent ranker nor the combined approach are effective in BSR.
7.4 End-to-End Evaluation with Comparison to Related Work
We now turn to evaluating our end-to-end negation system with SVM-based cue
classification and scope resolution using the combination of constituent ranking and
dependency-based rules. To put the evaluation in perspective we also compare our
results against the results of other state-of-the-art approaches to negation detection.
Comparison to previous work is complicated slightly by the fact that different data
splits and evaluation measures have been used across various studies. A commonly
reported measure in the literature on resolving negation scope is the percentage of
404
Velldal et al Rules, Rankers, and the Role of Syntax
correct scopes (PCS) as used by Morante and Daelemans (2009b), and Councill,
McDonald, and Velikovich (2010), among others. Councill, McDonald, and Velikovich
(2010) define PCS as the number of correct spans divided by the number of true spans. It
therefore corresponds roughly to the scope-level recall as reported in the current article.
The PCS notion of a correct scope, however, is less strict than in our set-up (Section 3.2):
Whereas we require an exact match of both the cue and the scope, Councill, McDonald,
and Velikovich (2010) do not include the cue identification in their evaluation.
Moreover, whereas the work of both Morante and Daelemans (2009b) and Councill,
McDonald, and Velikovich (2010) is based on the BioScope corpus, only Morante and
Daelemans (2009b) follow the same set-up assumed in the current article. Councill,
McDonald, and Velikovich (2010), on the other hand, evaluate by 5-fold cross-validation
on the papers alone, reporting a PCS score of 53.7%. When running our negation cue
classifier and constituent ranker (in the hybrid mode using the dependency features)
by 5-fold cross-validation on the papers we achieve a scope-level recall of 68.62 (and an
F1 of 64.50).
Table 15 shows a comparison of our negation scope resolution system with that
of Morante and Daelemans (2009b). Rather than using the PCS measure reported by
Morante and Daelemans (2009b), we have re-scored the output of their system accord-
ing to the CoNLL-2010 shared task scoring scheme, and it should therefore be kept in
mind that the system of Morante and Daelemans (2009b) originally was optimized with
respect to a slightly different metric.
For the cross-validated BSA experiments we find the results of the two systems
to be fairly similar, although the F1 achieved by our system is higher by more than
5 percentage points, mostly due to higher recall. For the cross-text experiments, the
differences are much more pronounced, with the F1 of our system being more than
22 points higher on BSP and more than 17 points higher on BSR. Again, the largest
differences are to be found for recall?even though this is the score that most closely
corresponds to the PCS metric used by Morante and Daelemans (2009b)?but as seen in
Table 15 there are substantial differences in precision as well. The scope-level differences
between the two systems are found to be statistically significant across all the three
BioScope sub-corpora.
Table 15
End-to-end results for our negation system, using the SVM cue classifier and the combination
of subtree ranking and dependency-based rules for scope resolution, comparing with Morante
et al (2009b).
Scope Level
Data Configuration Prec Rec F1
B
S
A
1
0
-F
o
ld Morante et al (2009b) 66.31 65.27 65.79
Cue classifier & Scope Rules + Ranking 69.30 72.89 71.05
B
S
P
H
e
ld
-o
u
t
Morante et al (2009b) 42.49 39.10 40.72
Cue classifier & Scope Rules + Ranking 58.58 68.09 62.98
B
S
R
H
e
ld
-o
u
t
Morante et al (2009b) 74.03 70.54 72.25
Cue classifier & Scope Rules + Ranking 89.62 89.41 89.52
405
Computational Linguistics Volume 38, Number 2
To some degree, some of the differences are to be expected, perhaps, at least with
respect to BSP. For example, the BSP evaluation represents a held-out setting for both
the cue and scope component in themachine learned system ofMorante andDaelemans
(2009b). While also true for our cue classifier and subtree ranker, it is not strictly
speaking the case for the dependency rules, and so the potential effect of any overfitting
during learningmight be less visible. The small set of manually defined rules are general
in nature, targeting the general syntactic constructions expressing negation, as shown
in Table 13. In addition to being based on the BioScope annotation guidelines, however,
both the abstracts and the full papers were consulted for patterns, and the fact that rule
development has included intermediate testing on BSP (although mostly during the
development of the initial set of speculation rules from which the negation rules are
derived) has likely made our system more tailored to the peculiarities of this data set.
When comparing the errors made by our system to those of Morante and Daelemans
(2009b), the most striking example of this is the inclusion of post-processing rules in our
system for ?backing off? from bracketed expressions (as discussed in Section 6.1). Al-
thoughmaking little difference on the abstracts, this has a huge impact when evaluating
the full papers, where bracketed expressions (citations, references to figures and tables,
etc.) are muchmore common, and the system output ofMorante and Daelemans (2009b)
seems to suffer from the lack of such robustness measures. In relation to the clinical
reports, one should bear in mind that, although our combined system outperforms that
of Morante and Daelemans (2009b) by a large margin, this result would still be rivaled
by simply using our default scope baseline, as is clear from Table 14.
The scope results of Zhu et al (2010) are unfortunately not currently directly com-
parable to ours, due to differences in evaluation methodologies. Whereas we perform
an exact match evaluation at the scope-level, as described in Section 3, Zhu et al (2010)
use a much less strict token-level evaluation even for their scopes in their end-to-end
evaluation. Nevertheless, our results appear to be highly competitive, because even
with the strict exact match criterion underlying our scope-level evaluation, our scores
are actually still higher for both the papers and the reports. (Zhu et al [2010] report an
F1 of 78.50 for the 10-fold runs on the abstracts, and 57.22 and 81.41 for held-out testing
on the papers and reports, respectively.)
8. Conclusion
This article has explored several linguistically informed approaches to the problem
of resolving the scope of speculation and negation within sentences. Our point of
departure was the system developed by Velldal, ?vrelid, and Oepen (2010) for the
CoNLL-2010 Shared Task challenge on resolving speculation in biomedical texts, where
a binary maximum entropy cue classifier was used in combination with a small set of
manually crafted scope resolution rules operating over dependency structures. In the
current article we have introduced several major extensions and improvements to this
initial system design.
First we presented a greatly simplified approach to cue identification using a linear
SVM classifier. The classifier only considers features of the immediate lexical context
of a target word, and it only aims to ?disambiguate? words that have already been
observed as speculation cues in the training data. The filtering imposed by this latter
?closed class? assumption greatly reduces the size and complexity of the model while
increasing classifier accuracy, yielding state-of-the-art performance on the CoNLL-2010
Shared Task evaluation data.
406
Velldal et al Rules, Rankers, and the Role of Syntax
We then presented a novel approach to the problem of resolving the scopes of
cues within a sentence. As an alternative to using the manually defined dependency
rules of our initial system, we showed how an SVM-based discriminative ranking
function can be learned for choosing subtrees from HPSG-based constituent structures.
An underlying assumption of the ranking approach is that annotated scopes actually
align with constituents, and we provided in-depth discussion and analysis of this issue.
Furthermore, while both the dependency rules and the constituent ranker achieve
good performance on their own, we showed how even better results can be achieved by
combining the two, as the errors they make are not always overlapping. The combined
approach uses the dependency rules for all cases where we do not have an available
HPSG parse, and for the cases where we do, the scope predicted by the rules is included
as a feature in the constituent ranker model. Together with the reformulation of our cue
classifier, this combined model for scope resolution obtains the best published results
so far on the CoNLL-2010 Shared Task evaluation data (to the best of our knowledge).
Finally, we have showed how all components of our speculation system are easily
ported to also handle the problem of resolving the scope of negation. With only modest
modifications, the system obtains state-of-the-art results also on the negation task. The
system outputs corresponding to the end-to-end experiments with our final model con-
figurations, for both speculation and negation, aremade available online (see footnote 2)
together with the relevant evaluation software.
Acknowledgments
We are grateful to the organizers of the
2010 CoNLL Shared Task and creators of
the BioScope resource; first, for engaging in
these kinds of community service, and
second for many in-depth discussions of
annotation and task details. We also want
to thank Buzhou Tang (HIT Shenzhen
Graduate School) and Roser Morante
(University of Antwerp), together with
their colleagues, for providing us with the
raw XML output of their negation and
speculation systems in order to enable
system comparisons. Andrew MacKinlay
(Melbourne University) and Dan Flickinger
(Stanford University) were of invaluable
help in adapting ERG parse selection to
the biomedical domain. We thank our
colleagues at the University of Oslo for
their comments and support during our
original participation in the 2010 CoNLL
Shared Task, as well as more recently in
preparing this manuscript. Large-scale
experimentation and engineering was
made possible though access to the TITAN
high-performance computing facilities at the
University of Oslo, and we are grateful to
the Scientific Computation staff at UiO, as
well as to the Norwegian Metacenter for
Computational Science. Last but not least,
we are indebted to the anonymous reviewers
for their careful reading and insightful
comments.
References
Brants, Thorsten. 2000. TnT. A statistical
Part-of-Speech tagger. In Proceedings of
the Sixth Conference on Applied Natural
Language Processing, pages 224?231,
Seattle, WA.
Butt, Miriam, Helge Dyvik, Tracy Holloway
King, Hiroshi Masuichi, and Christian
Rohrer. 2002. The Parallel Grammar
Project. In Proceedings of the COLING
Workshop on Grammar Engineering and
Evaluation, pages 1?7, Taipei.
Callmeier, Ulrich. 2002. Preprocessing and
encoding techniques in PET. In Stephan
Oepen, Daniel Flickinger, Jun?ichi Tsujii,
and Hans Uszkoreit, editors, Collaborative
Language Engineering. A Case Study in
Efficient Grammar-based Processing. CSLI
Publications, Stanford, CA, pages 127?143.
Collier, Nigel, Hyun S. Park, Norihiro Ogata,
Yuka Tateishi, Chikashi Nobata, Tomoko
Ohta, Tateshi Sekimizu, Hisao Imai,
Katsutoshi Ibushi, and Jun I. Tsujii. 1999.
The GENIA project: Corpus-based
knowledge acquisition and information
extraction from genome research papers.
In Proceedings of the 9th Conference of the
European Chapter of the ACL, pages 271?272,
Bergen.
Councill, Isaac G., Ryan McDonald, and
Leonid Velikovich. 2010. What?s great
and what?s not: Learning to classify the
scope of negation for improved sentiment
407
Computational Linguistics Volume 38, Number 2
analysis. In Proceedings of the Workshop
on Negation and Speculation in Natural
Language Processing, pages 51?59, Uppsala.
Crouch, Dick, Mary Dalrymple, Ron Kaplan,
Tracy King, John Maxwell, and Paula
Newman. 2008. XLE documentation. Palo
Alto Research Center, Palo Alto, CA.
Farkas, Richard, Veronika Vincze, Gyorgy
Mora, Janos Csirik, and Gy?rgy Szarvas.
2010. The CoNLL 2010 Shared Task:
Learning to detect hedges and their scope
in natural language text. In Proceedings of
the 14th Conference on Natural Language
Learning, pages 1?12, Uppsala.
Flickinger, Dan. 2002. On building a more
efficient grammar by exploiting types.
In Stephan Oepen, Dan Flickinger,
Jun?ichi Tsujii, and Hans Uszkoreit,
editors, Collaborative Language Engineering:
A Case Study in Efficient Grammar-based
Processing. CSLI Publications, Stanford,
CA, pages 1?17.
Gildea, Daniel. 2001. Corpus variation and
parser performance. In Proceedings of the
2001 Conference on Empirical Methods
in Natural Language Processing,
pages 167?202, Pittsburgh, PA.
Joachims, Thorsten. 1999. Making large-scale
SVM learning practical. In Bernhard
Sch?lkopf, Christopher J. C. Burges, and
Alexander J. Smola, editors, Advances in
Kernel Methods: Support Vector Learning.
MIT Press, Cambridge, MA, pages 41?56.
Joachims, Thorsten. 2002. Optimizing
search engines using clickthrough data.
In Proceedings of the Eighth ACM SIGKDD
International Conference on Knowledge
Discovery and Data Mining, pages 133?142,
Alberta.
Johansson, Richard and Pierre Nugues. 2007.
Extended constituent-to-dependency
conversion for English. In Proceedings of
16th Nordic Conference of Computational
Linguistics, pages 105?112, Tartu.
Johnson, Mark, Stuart Geman, Stephen
Canon, Zhiyi Chi, and Stefan Riezler.
1999. Estimators for stochastic
unification-based grammars. In Proceedings
of the 37th Meeting of the Association for
Computational Linguistics, pages 535?541,
College Park, MD.
Kilicoglu, Halil and Sabine Bergler. 2010.
A high-precision approach to detecting
hedges and their scopes. In Proceedings of
the 14th Conference on Natural Language
Learning, pages 70?77, Uppsala.
Light, Marc, Xin Ying Qiu, and Padmini
Srinivasan. 2004. The language of
bioscience: Facts, speculations, and
statements in between. In Proceedings
of the HLT-NAACL 2004 Workshop:
Biolink 2004, Linking Biological Literature,
Ontologies and Databases, pages 17?24,
Boston, MA.
MacKinlay, Andrew, Rebecca Dridan,
Dan Flickinger, Stephan Oepen, and
Timothy Baldwin. 2011. Treeblazing:
Using external treebanks to filter parse
forests for parse selection and treebanking.
In Proceedings of the 5th International Joint
Conference on Natural Language Processing,
pages 246?254, Chiang Mai.
Malouf, Robert and Gertjan van Noord. 2004.
Wide coverage parsing with stochastic
attribute value grammars. In Proceedings
of the IJCNLP Workshop Beyond Shallow
Analysis, Hainan.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English. The
Penn Treebank. Computational Linguistics,
19(2):313?330.
Martins, Andre F. T., Dipanjan Das, Noah A.
Smith, and Eric P. Xing. 2008. Stacking
dependency parsers. In Proceedings
of the 2008 Conference on Empirical
Methods in Natural Language Processing,
pages 157?166, Waikiki, HI.
Medlock, Ben and Ted Briscoe. 2007.
Weakly supervised learning for hedge
classification in scientific literature.
In Proceedings of the 45th Meeting of the
Association for Computational Linguistics,
pages 992?999, Prague.
Moilanen, Karo and Stephen Pulman. 2007.
Sentiment composition. In Proceedings
of the International Conference on Recent
Advances in Natural Language Processing,
pages 378?382, Borovets.
Morante, Roser. 2010. Descriptive analysis
of negation cues in biomedical texts.
In Proceedings of the 7th International
Conference on Language Resources and
Evaluation, pages 1429?1436, Valletta.
Morante, Roser and Walter Daelemans.
2009a. Learning the scope of hedge cues
in biomedical texts. In Proceedings of the
BioNLP 2009 Workshop, pages 28?36,
Boulder, CO.
Morante, Roser and Walter Daelemans.
2009b. A metalearning approach to
processing the scope of negation.
In Proceedings of the 13th Conference on
Natural Language Learning, pages 21?29,
Boulder, CO.
Morante, Roser, Anthony Liekens, and
Walter Daelemans. 2008. Learning the
scope of negation in biomedical texts.
408
Velldal et al Rules, Rankers, and the Role of Syntax
In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language
Processing, pages 715?724, Waikiki, HI.
Morante, Roser, Vincent van Asch, and
Walter Daelemans. 2010. Memory-based
resolution of in-sentence scope of hedge
cues. In Proceedings of the 14th Conference
on Natural Language Learning, pages 40?47,
Uppsala.
Nivre, Joakim, Johan Hall, and Jens Nilsson.
2006. MaltParser: A data-driven
parser-generator for dependency parsing.
In Proceedings of the 5th International
Conference on Language Resources and
Evaluation, pages 2216?2219, Genoa.
Nivre, Joakim and Ryan McDonald.
2008. Integrating graph-based and
transition-based dependency
parsers. In Proceedings of the 46th
Meeting of the Association for
Computational Linguistics,
pages 950?958, Columbus, OH.
?vrelid, Lilja, Jonas Kuhn, and Kathrin
Spreyer. 2009. Cross-framework
parser stacking for data-driven
dependency parsing. TAL special
issue on Machine Learning for NLP,
50(3):109?138.
?vrelid, Lilja, Erik Velldal, and Stephan
Oepen. 2010. Syntactic scope resolution
in uncertainty analysis. In Proceedings
of the 23rd International Conference on
Computational Linguistics, pages 1379?1387,
Beijing.
Petrov, Slav and Dan Klein. 2007.
Improved inference for unlexicalized
parsing. In Proceedings of Human Language
Technologies: The Annual Conference of the
North American Chapter of the Association for
Computational Linguistics, pages 404?411,
Rochester, NY.
Pollard, Carl and Ivan A. Sag. 1987.
Information-Based Syntax and Semantics.
Vol. 1: Fundamentals. CSLI Lecture
Notes # 13. CSLI Press, Stanford, CA.
Pollard, Carl and Ivan A. Sag. 1994.
Head-driven Phrase Structure Grammar.
The University of Chicago Press and
CSLI Publications, Chicago, IL.
Rei, Marek and Ted Briscoe. 2010.
Combining manual rules and supervised
learning for hedge cue and scope
detection. In Proceedings of the 14th
Conference on Natural Language Learning,
pages 56?63, Uppsala.
Riezler, Stefan, Tracy H. King, Ronald M.
Kaplan, Richard Crouch, John T. Maxwell,
and Mark Johnson. 2002. Parsing the Wall
Street Journal using a lexical-functional
grammar and discriminative estimation
techniques. In Proceedings of the
40th Meeting of the Association for
Computational Linguistics, pages 271?278,
Philadelphia, PA.
Schmid, Helmut. 1994. Probabilistic
part-of-speech tagging using decision
trees. In International Conference on
New Methods in Language Processing,
pages 44?49, Manchester.
Szarvas, Gy?rgy. 2008. Hedge classification
in biomedical texts with a weakly
supervised selection of keywords. In
Proceedings of the 46th Meeting of the
Association for Computational Linguistics,
pages 281?289, Columbus, OH.
Tang, Buzhou, Xiaolong Wang, Xuan Wang,
Bo Yuan, and Shixi Fan. 2010. A cascade
method for detecting hedges and their
scope in natural language text. In
Proceedings of the 14th Conference on
Natural Language Learning, pages 13?17,
Uppsala.
Toutanova, Kristina, Christopher D.
Manning, Dan Flickinger, and Stephan
Oepen. 2005. Stochastic HPSG parse
disambiguation using the Redwoods
corpus. Research on Language and
Computation, 3(1):83?105.
Tsuruoka, Yoshimasa, Yuka Tateishi,
Jin-Dong Kim, Tomoko Ohta, John
McNaught, Sophia Ananiadou, and
Jun?ichi Tsujii. 2005. Developing a robust
Part-of-Speech tagger for biomedical text.
In P. Bozanis and E. Houstis, editors,
Advances in Informatics. Springer, Berlin,
pages 382?392.
Velldal, Erik. 2011. Predicting speculation:
A simple disambiguation approach
to hedge detection in biomedical
literature. Journal of Biomedical Semantics,
2(Suppl 5):S7.
Velldal, Erik, Lilja ?vrelid, and Stephan
Oepen. 2010. Resolving speculation:
MaxEnt cue classification and
dependency-based scope rules.
In Proceedings of the 14th Conference on
Natural Language Learning, pages 48?55,
Uppsala.
Vincze, Veronika, Gy?rgy Szarvas, Rich?rd
Farkas, Gy?rgy M?ra, and J?nos Csirik.
2008. The BioScope corpus: Biomedical
texts annotated for uncertainty, negation
and their scopes. BMC Bioinformatics, 9
(Suppl. 11).
Vlachos, Andreas and Mark Craven. 2010.
Detecting speculative language using
syntactic dependencies and logistic
regression. In Proceedings of the 14th
409
Computational Linguistics Volume 38, Number 2
Conference on Natural Language Learning,
pages 18?25, Uppsala.
Zhang, Yi, Stephan Oepen, and John
Carroll. 2007. Efficiency in
unification-based n-best parsing. In
Proceedings of the 10th International
Conference on Parsing Technologies,
pages 48?59, Prague.
Zhang, Yi and Rui Wang. 2009.
Cross-domain dependency parsing
using a deep linguistic grammar.
In Proceedings of the 47th Meeting of the
Association for Computational Linguistics,
pages 378?386, Singapore.
Zhang, Yue and Joakim Nivre. 2011.
Transition-based dependency parsing
with rich non-local features. In Proceedings
of the 49th Meeting of the Association for
Computational Linguistics, pages 188?193,
Portland, OR.
Zhu, Qiaoming, Junhui Li, Hongling
Wang, and Guodong Zhou. 2010.
A unified framework for scope
learning via simplified shallow
semantic parsing. In Proceedings of the
2010 Conference on Empirical Methods
in Natural Language Processing,
pages 714?724, Cambridge, MA.
410
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 378?382,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Tokenization: Returning to a Long Solved Problem
A Survey, Contrastive Experiment, Recommendations, and Toolkit
Rebecca Dridan & Stephan Oepen
Institutt for Informatikk, Universitetet i Oslo
{rdridan |oe}@ifi.uio.no
Abstract
We examine some of the frequently disre-
garded subtleties of tokenization in Penn Tree-
bank style, and present a new rule-based pre-
processing toolkit that not only reproduces the
Treebank tokenization with unmatched accu-
racy, but also maintains exact stand-off point-
ers to the original text and allows flexible con-
figuration to diverse use cases (e.g. to genre-
or domain-specific idiosyncrasies).
1 Introduction?Motivation
The task of tokenization is hardly counted among the
grand challenges of NLP and is conventionally in-
terpreted as breaking up ?natural language text [...]
into distinct meaningful units (or tokens)? (Kaplan,
2005). Practically speaking, however, tokeniza-
tion is often combined with other string-level pre-
processing?for example normalization of punctua-
tion (of different conventions for dashes, say), dis-
ambiguation of quotation marks (into opening vs.
closing quotes), or removal of unwanted mark-up?
where the specifics of such pre-processing depend
both on properties of the input text as well as on as-
sumptions made in downstream processing.
Applying some string-level normalization prior to
the identification of token boundaries can improve
(or simplify) tokenization, and a sub-task like the
disambiguation of quote marks would in fact be hard
to perform after tokenization, seeing that it depends
on adjacency to whitespace. In the following, we
thus assume a generalized notion of tokenization,
comprising all string-level processing up to and in-
cluding the conversion of a sequence of characters
(a string) to a sequence of token objects.1
1Obviously, some of the normalization we include in the to-
kenization task (in this generalized interpretation) could be left
to downstream analysis, where a tagger or parser, for example,
could be expected to accept non-disambiguated quote marks
(so-called straight or typewriter quotes) and disambiguate as
Arguably, even in an overtly ?separating? lan-
guage like English, there can be token-level ambi-
guities that ultimately can only be resolved through
parsing (see ? 3 for candidate examples), and indeed
Waldron et al (2006) entertain the idea of down-
stream processing on a token lattice. In this article,
however, we accept the tokenization conventions
and sequential nature of the Penn Treebank (PTB;
Marcus et al, 1993) as a useful point of reference?
primarily for interoperability of different NLP tools.
Still, we argue, there is remaining work to be done
on PTB-compliant tokenization (reviewed in? 2),
both methodologically, practically, and technologi-
cally. In ? 3 we observe that state-of-the-art tools
perform poorly on re-creating PTB tokenization, and
move on in ? 4 to develop a modular, parameteri-
zable, and transparent framework for tokenization.
Besides improvements in tokenization accuracy and
adaptability to diverse use cases, in ? 5 we further
argue that each token object should unambiguously
link back to an underlying element of the original
input, which in the case of tokenization of text we
realize through a notion of characterization.
2 Common Conventions
Due to the popularity of the PTB, its tokenization
has been a de-facto standard for two decades. Ap-
proximately, this means splitting off punctuation
into separate tokens, disambiguating straight quotes,
and separating contractions such as can?t into ca
and n?t. There are, however, many special cases?
part of syntactic analysis. However, on the (predominant) point
of view that punctuation marks form tokens in their own right,
the tokenizer would then have to adorn quote marks in some
way, as to whether they were split off the left or right periph-
ery of a larger token, to avoid unwanted syntactic ambiguity.
Further, increasing use of Unicode makes texts containing ?na-
tively? disambiguated quotes more common, where it would
seem unfortunate to discard linguistically pertinent information
by normalizing towards the poverty of pure ASCII punctuation.
378
documented and undocumented. In much tagging
and parsing work, PTB data has been used with
gold-standard tokens, to a point where many re-
searchers are unaware of the existence of the orig-
inal ?raw? (untokenized) text. Accordingly, the for-
mal definition of PTB tokenization2 has received lit-
tle attention, but reproducing PTB tokenization au-
tomatically actually is not a trivial task (see ? 3).
As the NLP community has moved to process data
other than the PTB, some of the limitations of the
PTB tokenization have been recognized, and many
recently released data sets are accompanied by a
note on tokenization along the lines of: Tokenization
is similar to that used in PTB, except . . . Most ex-
ceptions are to do with hyphenation, or special forms
of named entities such as chemical names or URLs.
None of the documentation with extant data sets is
sufficient to fully reproduce the tokenization.3
The CoNLL 2008 Shared Task data actually pro-
vided two forms of tokenization: that from the PTB
(which many pre-processing tools would have been
trained on), and another form that splits (most) hy-
phenated terms. This latter convention recently
seems to be gaining ground in data sets like the
Google 1T n-gram corpus (LDC #2006T13) and
OntoNotes (Hovy et al, 2006). Clearly, as one
moves towards a more application- and domain-
driven idea of ?correct? tokenization, a more trans-
parent, flexible, and adaptable approach to string-
level pre-processing is called for.
3 A Contrastive Experiment
To get an overview of current tokenization methods,
we recovered and tokenized the raw text which was
the source of the (Wall Street Journal portion of the)
PTB, and compared it to the gold tokenization in the
syntactic annotation in the treebank.4 We used three
common methods of tokenization: (a) the original
2See http://www.cis.upenn.edu/~treebank/
tokenization.html for available ?documentation? and a
sed script for PTB-style tokenization.
3?vrelid et al (2010) observe that tokenizing with the GE-
NIA tagger yields mismatches in one of five sentences of the
GENIA Treebank, although the GENIA guidelines refer to
scripts that may be available on request (Tateisi & Tsujii, 2006).
4The original WSJ text was last included with the 1995 re-
lease of the PTB (LDC #95T07) and required alignment with
the treebank, with some manual correction so that the same text
is represented in both raw and parsed formats.
Tokenization Differing Levenshtein
Method Sentences Distance
tokenizer.sed 3264 11168
CoreNLP 1781 3717
C&J parser 2597 4516
Table 1: Quantitative view on tokenization differences.
PTB tokenizer.sed script; (b) the tokenizer from the
Stanford CoreNLP tools5; and (c) tokenization from
the parser of Charniak & Johnson (2005). Table 1
shows quantitative differences between each of the
three methods and the PTB, both in terms of the
number of sentences where the tokenization differs,
and also in the total Levenshtein distance (Leven-
shtein, 1966) over tokens (for a total of 49,208 sen-
tences and 1,173,750 gold-standard tokens).
Looking at the differences qualitatively, the most
consistent issue across all tokenization methods was
ambiguity of sentence-final periods. In the treebank,
final periods are always (with about 10 exceptions)
a separate token. If the sentence ends in U.S. (but
not other abbreviations, oddly), an extra period is
hallucinated, so the abbreviation also has one. In
contrast, C&J add a period to all final abbreviations,
CoreNLP groups the final period with a final abbre-
viation and hence lacks a sentence-final period to-
ken, and the sed script strips the period off U.S. The
?correct? choice in this case is not obvious and will
depend on how the tokens are to be used.
The majority of the discrepancies in the sed script
tokenization come from an under-restricted punctu-
ation rule that incorrectly splits on commas within
numbers or ampersands within names. Other than
that, the problematic cases are mostly shared across
tokenization methods, and include issues with cur-
rencies, Irish names, hyphenization, and quote dis-
ambiguation. In addition, C&J make some addi-
tional modifications to the text, lemmatising expres-
sions such as won?t as will and n?t.
4 REPP: A Generalized Framework
For tokenization to be studied as a first-class prob-
lem, and to enable customization and flexibility to
diverse use cases, we suggest a non-procedural,
rule-based framework dubbed REPP (Regular
5See http://nlp.stanford.edu/software/
corenlp.shtml, run in ?strictTreebank3? mode.
379
>wiki
#1
!([? ])([])}?!,;:??]) ([? ]|$) \1 \2 \3
!(?|[? ]) ([[({??])([? ]) \1 \2 \3
#
>1
:[[:space:]]+
Figure 1: Simplified examples of tokenization rules.
Expression-Based Pre-Processing)?essentially a
cascade of ordered finite-state string rewriting rules,
though transcending the formal complexity of regu-
lar languages by inclusion of (a) full perl-compatible
regular expressions and (b) fixpoint iteration over
groups of rules. In this approach, a first phase of
string-level substitutions inserts whitespace around,
for example, punctuation marks; upon completion of
string rewriting, token boundaries are stipulated be-
tween all whitespace-separated substrings (and only
these).
For a good balance of human and machine read-
ability, REPP tokenization rules are specified in a
simple, line-oriented textual form. Figure 1 shows
a (simplified) excerpt from our PTB-style tokenizer,
where the first character on each line is one of four
REPP operators, as follows: (a) ?#? for group forma-
tion; (b) ?>? for group invocation, (c) ?!? for substi-
tution (allowing capture groups), and (d) ?:? for to-
ken boundary detection.6 In Figure 1, the two rules
stripping off prefix and suffix punctuation marks ad-
jacent to whitespace (i.e. matching the tab-separated
left-hand side of the rule, to replace the match with
its right-hand side) form a numbered group (?#1?),
which will be iterated when called (?>1?) until none
of the rules in the group fires (a fixpoint). In this ex-
ample, conditioning on whitespace adjacency avoids
the issues observed with the PTB sed script (e.g. to-
ken boundaries within comma-separated numbers)
and also protects against infinite loops in the group.7
REPP rule sets can be organized as modules, typ-
6Strictly speaking, there are another two operators, for line-
oriented comments and automated versioning of rule files.
7For this example, the same effects seemingly could be ob-
tained without iteration (using greatly more complex rules); our
actual, non-simplified rules, however, further deal with punctu-
ation marks that can function as prefixes or suffixes, as well as
with corner cases like factor(s) or Ca[2+]. Also in mark-up re-
moval and normalization, we have found it necessary to ?parse?
nested structures by means of iterative groups.
ically each in a file of its own, and invoked selec-
tively by name (e.g. ?>wiki? in Figure 1); to date,
there exist modules for quote disambiguation, (rele-
vant subsets of) various mark-up languages (HTML,
LATEX, wiki, and XML), and a handful of robust-
ness rules (e.g. seeking to identify and repair ?sand-
wiched? inter-token punctuation). Individual tok-
enizers are configured at run-time, by selectively ac-
tivating a set of modules (through command-line op-
tions). An open-source reference implementation of
the REPP framework (in C++) is available, together
with a library of modules for English.
5 Characterization for Traceability
Tokenization, and specifically our notion of gener-
alized tokenization which allows text normalization,
involves changes to the original text being analyzed,
rather than just additional annotation. As such, full
traceability from the token objects to the original
text is required, which we formalize as ?character-
ization?, in terms of character position links back to
the source.8 This has the practical benefit of allow-
ing downstream analysis as direct (stand-off) anno-
tation on the source text, as seen for example in the
ACL Anthology Searchbench (Sch?fer et al, 2011).
With our general regular expression replacement
rules in REPP, making precise what it means for a
token to link back to its ?underlying? substring re-
quires some care in the design and implementation.
Definite characterization links between the string
before (I) and after (O) the application of a sin-
gle rule can only be established in certain positions,
viz. (a) spans not matched by the rule: unchanged
text in O outside the span matched by the left-hand
side regex of the rule can always be linked back to
I; and (b) spans caught by a regex capture group:
capture groups represent the same text in the left-
and right-hand sides of a substitution, and so can be
linked back to O.9 Outside these text spans, we can
only make definite statements about characterization
links at boundary points, which include the start and
end of the full string, the start and end of the string
8If the tokenization process was only concerned with the
identification of token boundaries, characterization would be
near-trivial.
9If capture group references are used out-of-order, however,
the per-group linkage is no longer well-defined, and we resort
to the maximum-span ?union? of boundary points (see below).
380
matched by the rule, and the start and end of any
capture groups in the rule.
Each character in the string being processed has
a start and end position, marking the point before
and after the character in the original string. Before
processing, the end position would always be one
greater than the start position. However, if a rule
mapped a string-initial, PTB-style opening double
quote (``) to one-character Unicode ?, the new first
character of the string would have start position 0,
but end position 2. In contrast, if there were a rule
!wo(n?t) will \1 (1)
applied to the string I won?t go!, all characters in the
second token of the resulting string (I will n?t go!)
will have start position 2 and end position 4. This
demonstrates one of the formal consequences of our
design: we have no reason to assign the characters ill
any start position other than 2.10 Since explicit char-
acter links between each I and O will only be estab-
lished at match or capture group boundaries, any text
from the left-hand side of a rule that should appear in
O must be explicitly linked through a capture group
reference (rather than merely written out in the right-
hand side of the rule). In other words, rule (1) above
should be preferred to the following variant (which
would result in character start and end offsets of 0
and 5 for both output tokens):
!won?t will n?t (2)
During rule application, we keep track of charac-
ter start and end positions as offsets between a string
before and after each rule application (i.e. all pairs
?I,O?), and these offsets are eventually traced back
to the original string at the time of final tokenization.
6 Quantitative and Qualitative Evaluation
In our own work on preparing various (non-PTB)
genres for parsing, we devised a set of REPP rules
with the goal of following the PTB conventions.
When repeating the experiment of ? 3 above us-
ing REPP tokenization, we obtained an initial dif-
ference in 1505 sentences, with a Levenshtein dis-
10This subtlety will actually be invisible in the final token
objects if will remains a single token, but if subsequent rules
were to split this token further, all its output tokens would have a
start position of 2 and an end position of 4. While this example
may seem unlikely, we have come across similar scenarios in
fine-tuning actual REPP rules.
tance of 3543 (broadly comparable to CoreNLP, if
marginally more accurate).
Examining these discrepancies, we revealed some
deficiencies in our rules, as well as some peculiari-
ties of the ?raw? Wall Street Journal text from the
PTB distribution. A little more than 200 mismatches
were owed to improper treatment of currency sym-
bols (AU$) and decade abbreviations (?60s), which
led to the refinement of two existing rules. Notable
PTB idiosyncrasies (in the sense of deviations from
common typography) include ellipses with spaces
separating the periods and a fairly large number of
possessives (?s) being separated from their preced-
ing token. Other aspects of gold-standard PTB tok-
enization we consider unwarranted ?damage? to the
input text, such as hallucinating an extra period af-
ter U.S. and splitting cannot (which adds spuri-
ous ambiguity). For use cases where the goal were
strict compliance, for instance in pre-processing in-
puts for a PTB-derived parser, we added an optional
REPP module (of currently half a dozen rules) to
cater to these corner cases?in a spirit similar to the
CoreNLP mode we used in ? 3. With these extra
rules, remaining tokenization discrepancies are con-
tained in 603 sentences (just over 1%), which gives
a Levenshtein distance of 1389.
7 Discussion?Conclusion
Compared to the best-performing off-the-shelf sys-
tem in our earlier experiment (where it is reason-
able to assume that PTB data has played at least
some role in development), our results eliminate two
thirds of the remaining tokenization errors?a more
substantial reduction than recent improvements in
parsing accuracy against the PTB, for example.
Of the remaining differences, over 350 are con-
cerned with mid-sentence period ambiguity, where
at least half of those are instances where a pe-
riod was separated from an abbreviation in the
treebank?a pattern we do not wish to emulate.
Some differences in quote disambiguation also re-
main, often triggered by whitespace on both sides of
quote marks in the raw text. The final 200 or so dif-
ferences stem from manual corrections made during
treebanking, and we consider that these cases could
not be replicated automatically in any generalizable
fashion.
381
References
Charniak, E., & Johnson, M. (2005). Coarse-to-fine
n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics
(pp. 173?180). Ann Arbor, USA.
Hovy, E., Marcus, M., Palmer, M., Ramshaw, L.,
& Weischedel, R. (2006). Ontonotes. The 90%
solution. In Proceedings of the Human Lan-
guage Technology Conference of the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics (pp. 57?60). New York City,
USA.
Kaplan, R. M. (2005). A method for tokenizing
text. Festschrift for Kimmo Koskenniemi on his
60th birthday. In A. Arppe, L. Carlson, K. Lind?n,
J. Piitulainen, M. Suominen, M. Vainio, H. West-
erlund, & A. Yli-Jyr? (Eds.), Inquiries into words,
constraints and contexts (pp. 55 ? 64). Stanford,
CA: CSLI Publications.
Levenshtein, V. (1966). Binary codes capable of cor-
recting deletions, insertions and reversals. Soviet
Physice ? Doklady, 10, 707?710.
Marcus, M. P., Santorini, B., & Marcinkiewicz,
M. A. (1993). Building a large annotated corpus
of English. The Penn Treebank. Computational
Linguistics, 19, 313 ? 330.
?vrelid, L., Velldal, E., & Oepen, S. (2010). Syn-
tactic scope resolution in uncertainty analysis. In
Proceedings of the 23rd international conference
on computational linguistics (pp. 1379 ? 1387).
Beijing, China.
Sch?fer, U., Kiefer, B., Spurk, C., Steffen, J., &
Wang, R. (2011). The ACL Anthology Search-
bench. In Proceedings of the ACL-HLT 2011 sys-
tem demonstrations (pp. 7?13). Portland, Oregon,
USA.
Tateisi, Y., & Tsujii, J. (2006). GENIA anno-
tation guidelines for tokenization and POS tag-
ging (Technical Report # TR-NLP-UT-2006-4).
Tokyo, Japan: Tsujii Lab, University of Tokyo.
Waldron, B., Copestake, A., Sch?fer, U., & Kiefer,
B. (2006). Preprocessing and tokenisation stan-
dards in DELPH-IN tools. In Proceedings of the
5th International Conference on Language Re-
sources and Evaluation (pp. 2263 ? 2268). Genoa,
Italy.
382
Proceedings of the ACL Student Research Workshop, pages 31?37,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Survey on parsing three dependency representations for English
Angelina Ivanova Stephan Oepen Lilja ?vrelid
University of Oslo, Department of Informatics
{angelii |oe |liljao }@ifi.uio.no
Abstract
In this paper we focus on practical is-
sues of data representation for dependency
parsing. We carry out an experimental
comparison of (a) three syntactic depen-
dency schemes; (b) three data-driven de-
pendency parsers; and (c) the influence of
two different approaches to lexical cate-
gory disambiguation (aka tagging) prior to
parsing. Comparing parsing accuracies in
various setups, we study the interactions
of these three aspects and analyze which
configurations are easier to learn for a de-
pendency parser.
1 Introduction
Dependency parsing is one of the mainstream re-
search areas in natural language processing. De-
pendency representations are useful for a number
of NLP applications, for example, machine trans-
lation (Ding and Palmer, 2005), information ex-
traction (Yakushiji et al, 2006), analysis of ty-
pologically diverse languages (Bunt et al, 2010)
and parser stacking (?vrelid et al, 2009). There
were several shared tasks organized on depen-
dency parsing (CoNLL 2006?2007) and labeled
dependencies (CoNLL 2008?2009) and there were
a number of attempts to compare various depen-
dencies intrinsically, e.g. (Miyao et al, 2007), and
extrinsically, e.g. (Wu et al, 2012).
In this paper we focus on practical issues of data
representation for dependency parsing. The cen-
tral aspects of our discussion are (a) three depen-
dency formats: two ?classic? representations for
dependency parsing, namely, Stanford Basic (SB)
and CoNLL Syntactic Dependencies (CD), and
bilexical dependencies from the HPSG English
Resource Grammar (ERG), so-called DELPH-IN
Syntactic Derivation Tree (DT), proposed recently
by Ivanova et al (2012); (b) three state-of-the art
statistical parsers: Malt (Nivre et al, 2007), MST
(McDonald et al, 2005) and the parser of Bohnet
and Nivre (2012); (c) two approaches to word-
category disambiguation, e.g. exploiting common
PTB tags and using supertags (i.e. specialized
ERG lexical types).
We parse the formats and compare accuracies
in all configurations in order to determine how
parsers, dependency representations and grammat-
ical tagging methods interact with each other in
application to automatic syntactic analysis.
SB and CD are derived automatically from
phrase structures of Penn Treebank to accommo-
date the needs of fast and accurate dependency
parsing, whereas DT is rooted in the formal gram-
mar theory HPSG and is independent from any
specific treebank. For DT we gain more expres-
sivity from the underlying linguistic theory, which
challenges parsing with statistical tools. The struc-
tural analysis of the schemes in Ivanova et al
(2012) leads to the hypothesis that CD and DT
are more similar to each other than SB to DT.
We recompute similarities on a larger treebank and
check whether parsing results reflect them.
The paper has the following structure: an
overview of related work is presented in Sec-
tion 2; treebanks, tagsets, dependency schemes
and parsers used in the experiments are introduced
in Section 3; analysis of parsing results is dis-
cussed in Section 4; conclusions and future work
are outlined in Section 5.
2 Related work
Schwartz et al (2012) investigate which depen-
dency representations of several syntactic struc-
tures are easier to parse with supervised ver-
sions of the Klein and Manning (2004) parser,
ClearParser (Choi and Nicolov, 2009), MST
Parser, Malt and the Easy First Non-directional
parser (Goldberg and Elhadad, 2010). The results
imply that all parsers consistently perform better
when (a) coordination has one of the conjuncts as
the head rather than the coordinating conjunction;
31
A , B and C A , B and C A, B and C
Figure 1: Annotation of coordination structure in SB, CD and DT (left to right) dependency formats
(b) the noun phrase is headed by the noun rather
than by determiner; (c) prepositions or subordinat-
ing conjunctions, rather than their NP or clause ar-
guments, serve as the head in prepositional phrase
or subordinated clauses. Therefore we can expect
(a) Malt and MST to have fewer errors on coor-
dination structures parsing SB and CD than pars-
ing DT, because SB and CD choose the first con-
junct as the head and DT chooses the coordinating
conjunction as the head; (b,c) no significant dif-
ferences for the errors on noun and prepositional
phrases, because all three schemes have the noun
as the head of the noun phrase and the preposition
as the head of the prepositional phrase.
Miwa et al (2010) present intristic and extris-
tic (event-extraction task) evaluation of six parsers
(GDep, Bikel, Stanford, Charniak-Johnson, C&C
and Enju parser) on three dependency formats
(Stanford Dependencies, CoNLL-X, and Enju
PAS). Intristic evaluation results show that all
parsers have the highest accuracies with the
CoNLL-X format.
3 Data and software
3.1 Treebanks
For the experiments in this paper we used the Penn
Treebank (Marcus et al, 1993) and the Deep-
Bank (Flickinger et al, 2012). The latter is com-
prised of roughly 82% of the sentences of the first
16 sections of the Penn Treebank annotated with
full HPSG analyses from the English Resource
Grammar (ERG). The DeepBank annotations are
created on top of the raw text of the PTB. Due to
imperfections of the automatic tokenization, there
are some token mismatches between DeepBank
and PTB. We had to filter out such sentences to
have consistent number of tokens in the DT, SB
and CD formats. For our experiments we had
available a training set of 22209 sentences and a
test set of 1759 sentences (from Section 15).
3.2 Parsers
In the experiments described in Section 4 we used
parsers that adopt different approaches and imple-
ment various algorithms.
Malt (Nivre et al, 2007): transition-based de-
pendency parser with local learning and greedy
search.
MST (McDonald et al, 2005): graph-based
dependency parser with global near-exhaustive
search.
Bohnet and Nivre (2012) parser: transition-
based dependency parser with joint tagger that im-
plements global learning and beam search.
3.3 Dependency schemes
In this work we extract DeepBank data in the form
of bilexical syntactic dependencies, DELPH-IN
Syntactic Derivation Tree (DT) format. We ob-
tain the exact same sentences in Stanford Basic
(SB) format from the automatic conversion of the
PTB with the Stanford parser (de Marneffe et al,
2006) and in the CoNLL Syntactic Dependencies
(CD) representation using the LTH Constituent-
to-Dependency Conversion Tool for Penn-style
Treebanks (Johansson and Nugues, 2007).
SB and CD represent the way to convert PTB
to bilexical dependencies; in contrast, DT is
grounded in linguistic theory and captures deci-
sions taken in the grammar. Figure 1 demonstrates
the differences between the formats on the coor-
dination structure. According to Schwartz et al
(2012), analysis of coordination in SB and CD is
easier for a statistical parser to learn; however, as
we will see in section 4.3, DT has more expressive
power distinguishing structural ambiguities illus-
trated by the classic example old men and women.
3.4 Part-of-speech tags
We experimented with two tag sets: PTB tags and
lexical types of the ERG grammar - supertags.
PTB tags determine the part of speech (PoS)
and some morphological features, such as num-
ber for nouns, degree of comparison for adjectives
and adverbs, tense and agreement with person and
number of subject for verbs, etc.
Supertags are composed of part-of-speech, va-
lency in the form of an ordered sequence of
complements, and annotations that encompass
category-internal subdivisions, e.g. mass vs. count
vs. proper nouns, intersective vs. scopal adverbs,
32
or referential vs. expletive pronouns. Example of
a supertag: v np is le (verb ?is? that takes noun
phrase as a complement).
There are 48 tags in the PTB tagset and 1091
supertags in the set of lexical types of the ERG.
The state-of-the-art accuracy of PoS-tagging on
in-domain test data using gold-standard tokeniza-
tion is roughly 97% for the PTB tagset and ap-
proximately 95% for the ERG supertags (Ytrest?l,
2011). Supertagging for the ERG grammar is an
ongoing research effort and an off-the-shelf su-
pertagger for the ERG is not currently available.
4 Experiments
In this section we give a detailed analysis of pars-
ing into SB, CD and DT dependencies with Malt,
MST and the Bohnet and Nivre (2012) parser.
4.1 Setup
For Malt and MST we perform the experiments
on gold PoS tags, whereas the Bohnet and Nivre
(2012) parser predicts PoS tags during testing.
Prior to each experiment with Malt, we used
MaltOptimizer to obtain settings and a feature
model; for MST we exploited default configura-
tion; for the Bohnet and Nivre (2012) parser we
set the beam parameter to 80 and otherwise em-
ployed the default setup.
With regards to evaluation metrics we use la-
belled attachment score (LAS), unlabeled attach-
ment score (UAS) and label accuracy (LACC) ex-
cluding punctuation. Our results cannot be di-
rectly compared to the state-of-the-art scores on
the Penn Treebank because we train on sections
0-13 and test on section 15 of WSJ. Also our re-
sults are not strictly inter-comparable because the
setups we are using are different.
4.2 Discussion
The results that we are going to analyze are pre-
sented in Tables 1 and 2. Statistical significance
was assessed using Dan Bikel?s parsing evaluation
comparator1 at the 0.001 significance level. We
inspect three different aspects in the interpretation
of these results: parser, dependency format and
tagset. Below we will look at these three angles
in detail.
From the parser perspective Malt and MST are
not very different in the traditional setup with gold
1http://nextens.uvt.nl/depparse-wiki/
SoftwarePage#scoring
PTB tags (Table 1, Gold PTB tags). The Bohnet
and Nivre (2012) parser outperforms Malt on CD
and DT and MST on SB, CD and DT with PTB
tags even though it does not receive gold PTB tags
during test phase but predicts them (Table 2, Pre-
dicted PTB tags). This is explained by the fact that
the Bohnet and Nivre (2012) parser implements a
novel approach to parsing: beam-search algorithm
with global structure learning.
MST ?loses? more than Malt when parsing SB
with gold supertags (Table 1, Gold supertags).
This parser exploits context features ?POS tag of
each intervening word between head and depen-
dent? (McDonald et al, 2006). Due to the far
larger size of the supertag set compared to the PTB
tagset, such features are sparse and have low fre-
quencies. This leads to the lower scores of pars-
ing accuracy for MST. For the Bohnet and Nivre
(2012) parser the complexity of supertag predic-
tion has significant negative influence on the at-
tachment and labeling accuracies (Table 2, Pre-
dicted supertags). The addition of gold PTB tags
as a feature lifts the performance of the Bohnet
and Nivre (2012) parser to the level of perfor-
mance of Malt and MST on CD with gold su-
pertags and Malt on SB with gold supertags (com-
pare Table 2, Predicted supertags + gold PTB, and
Table 1, Gold supertags).
Both Malt and MST benefit slightly from the
combination of gold PTB tags and gold supertags
(Table 1, Gold PTB tags + gold supertags). For
the Bohnet and Nivre (2012) parser we also ob-
serve small rise of accuracy when gold supertags
are provided as a feature for prediction of PTB
tags (compare Predicted PTB tags and Predicted
PTB tags + gold supertags sections of Table 2).
The parsers have different running times: it
takes minutes to run an experiment with Malt,
about 2 hours with MST and up to a day with the
Bohnet and Nivre (2012) parser.
From the point of view of the dependency for-
mat, SB has the highest LACC and CD is first-rate
on UAS for all three parsers in most of the con-
figurations (Tables 1 and 2). This means that SB
is easier to label and CD is easier to parse struc-
turally. DT appears to be a more difficult target
format because it is both hard to label and attach
in most configurations. It is not an unexpected re-
sult, since SB and CD are both derived from PTB
phrase-structure trees and are oriented to ease de-
pendency parsing task. DT is not custom-designed
33
Gold PTB tags
LAS UAS LACC
Malt MST Malt MST Malt MST
SB 89.21 88.59 90.95 90.88 93.58 92.79
CD 88.74 88.72 91.89 92.01 91.29 91.34
DT 85.97 86.36 89.22 90.01 88.73 89.22
Gold supertags
LAS UAS LACC
Malt MST Malt MST Malt MST
SB 87.76 85.25 90.63 88.56 92.38 90.29
CD 88.22 87.27 91.17 90.41 91.30 90.74
DT 89.92 89.58 90.96 90.56 92.50 92.64
Gold PTB tags + gold supertags
LAS UAS LACC
Malt MST Malt MST Malt MST
SB 90.321 89.431 91.901 91.842 94.481 93.261
CD 89.591 89.372 92.431 92.772 92.321 92.072
DT 90.691 91.192 91.831 92.332 93.101 93.692
Table 1: Parsing results of Malt and MST on
Stanford Basic (SB), CoNLL Syntactic De-
pendencies (CD) and DELPH-IN Syntactic
Derivation Tree (DT) formats. Punctuation is
excluded from the scoring. Gold PTB tags:
Malt and MST are trained and tested on gold
PTB tags. Gold supertags: Malt and MST
are trained and tested on gold supertags. Gold
PTB tags + gold supertags: Malt and MST are
trained on gold PTB tags and gold supertags.
1 denotes a feature model in which gold PTB
tags function as PoS and gold supertags act
as additional features (in CPOSTAG field); 2
stands for the feature model which exploits
gold supertags as PoS and uses gold PTB tags
as extra features (in CPOSTAG field).
Predicted PTB tags
LAS UAS LACC
Bohnet and Nivre (2012)
SB 89.56 92.36 93.30
CD 89.77 93.01 92.10
DT 88.26 91.63 90.72
Predicted supertags
LAS UAS LACC
Bohnet and Nivre (2012)
SB 85.41 89.38 90.17
CD 86.73 90.73 89.72
DT 85.76 89.50 88.56
Pred. PTB tags + gold supertags
LAS UAS LACC
Bohnet and Nivre (2012)
SB 90.32 93.01 93.85
CD 90.55 93.56 92.79
DT 91.51 92.99 93.88
Pred. supertags + gold PTB
LAS UAS LACC
Bohnet and Nivre (2012)
SB 87.20 90.07 91.81
CD 87.79 91.47 90.62
DT 86.31 89.80 89.17
Table 2: Parsing results of the Bohnet
and Nivre (2012) parser on Stanford Ba-
sic (SB), CoNLL Syntactic Dependencies
(CD) and DELPH-IN Syntactic Deriva-
tion Tree (DT) formats. Parser is trained
on gold-standard data. Punctuation is ex-
cluded from the scoring. Predicted PTB:
parser predicts PTB tags during the test
phase. Predicted supertags: parser pre-
dicts supertags during the test phase. Pre-
dicted PTB + gold supertags: parser re-
ceives gold supertags as feature and pre-
dicts PTB tags during the test phase. Pre-
dicted supertags + gold PTB: parser re-
ceives PTB tags as feature and predicts
supertags during test phase.
34
to dependency parsing and is independent from
parsing questions in this sense. Unlike SB and
CD, it is linguistically informed by the underlying,
full-fledged HPSG grammar.
The Jaccard similarity on our training set is 0.57
for SB and CD, 0.564 for CD and DT, and 0.388
for SB and DT. These similarity values show that
CD and DT are structurally closer to each other
than SB and DT. Contrary to our expectations, the
accuracy scores of parsers do not suggest that CD
and DT are particularly similar to each other in
terms of parsing.
Inspecting the aspect of tagset we conclude that
traditional PTB tags are compatible with SB and
CD but do not fit the DT scheme well, while ERG
supertags are specific to the ERG framework and
do not seem to be appropriate for SB and CD. Nei-
ther of these findings seem surprising, as PTB tags
were developed as part of the treebank from which
CD and SB are derived; whereas ERG supertags
are closely related to the HPSG syntactic struc-
tures captured in DT. PTB tags were designed to
simplify PoS-tagging whereas supertags were de-
veloped to capture information that is required to
analyze syntax of HPSG.
For each PTB tag we collected corresponding
supertags from the gold-standard training set. For
open word classes such as nouns, adjectives, ad-
verbs and verbs the relation between PTB tags
and supertags is many-to-many. Unique one-to-
many correspondence holds only for possessive
wh-pronoun and punctuation.
Thus, supertags do not provide extra level of
detalization for PTB tags, but PTB tags and su-
pertags are complementary. As discussed in sec-
tion 3.4, they contain bits of information that are
different. For this reason their combination re-
sults in slight increase of accuracy for all three
parsers on all dependency formats (Table 1, Gold
PTB tags + gold supertags, and Table 2, Predicted
PTB + gold supertags and Predicted supertags +
gold PTB). The Bohnet and Nivre (2012) parser
predicts supertags with an average accuracy of
89.73% which is significantly lower than state-of-
the-art 95% (Ytrest?l, 2011).
When we consider punctuation in the evalua-
tion, all scores raise significantly for DT and at
the same time decrease for SB and CD for all three
parsers. This is explained by the fact that punctu-
ation in DT is always attached to the nearest token
which is easy to learn for a statistical parser.
4.3 Error analysis
Using the CoNLL-07 evaluation script2 on our test
set, for each parser we obtained the error rate dis-
tribution over CPOSTAG on SB, CD and DT.
VBP, VBZ and VBG. VBP (verb, non-3rd
person singular present), VBZ (verb, 3rd per-
son singular present) and VBG (verb, gerund or
present participle) are the PTB tags that have error
rates in 10 highest error rates list for each parser
(Malt, MST and the Bohnet and Nivre (2012)
parser) with each dependency format (SB, CD
and DT) and with each PoS tag set (PTB PoS
and supertags) when PTB tags are included as
CPOSTAG feature. We automatically collected all
sentences that contain 1) attachment errors, 2) la-
bel errors, 3) attachment and label errors for VBP,
VBZ and VBG made by Malt parser on DT format
with PTB PoS. For each of these three lexical cat-
egories we manually analyzed a random sample
of sentences with errors and their corresponding
gold-standard versions.
In many cases such errors are related to the root
of the sentence when the verb is either treated as
complement or adjunct instead of having a root
status or vice versa. Errors with these groups of
verbs mostly occur in the complex sentences that
contain several verbs. Sentences with coordina-
tion are particularly difficult for the correct attach-
ment and labeling of the VBP (see Figure 2 for an
example).
Coordination. The error rate of Malt, MST and
the Bohnet and Nivre (2012) parser for the coor-
dination is not so high for SB and CD ( 1% and
2% correspondingly with MaltParser, PTB tags)
whereas for DT the error rate on the CPOSTAGS
is especially high (26% with MaltParser, PTB
tags). It means that there are many errors on
incoming dependency arcs for coordinating con-
junctions when parsing DT. On outgoing arcs
parsers also make more mistakes on DT than on
SB and CD. This is related to the difference in
choice of annotation principle (see Figure 1). As
it was shown in (Schwartz et al, 2012), it is harder
to parse coordination headed by coordinating con-
junction.
Although the approach used in DT is harder for
parser to learn, it has some advantages: using SB
and CD annotations, we cannot distinguish the two
cases illustrated with the sentences (a) and (b):
2http://nextens.uvt.nl/depparse-wiki/
SoftwarePage#scoring
35
VBP VBD VBD
The figures show that spending rose 0.1 % in the third quarter <. . .> and was up 3.8 % from a year ago .
root
SB-HD
VP-VP
HD-CMP
MRK-NH
root
SP-HD
HD-CMP Cl-CL
MRK-NH
Figure 2: The gold-standard (in green above the sentence) and the incorrect Malt?s (in red below the
sentence) analyses of the utterance from the DeepBank in DT format with PTB PoS tags
a) The fight is putting a tight squeeze on prof-
its of many, threatening to drive the small-
est ones out of business and straining rela-
tions between the national fast-food chains
and their franchisees.
b) Proceeds from the sale will be used for re-
modelling and reforbishing projects, as well
as for the planned MGM Grand hotel/casino
and theme park.
In the sentence a) ?the national fast-food? refers
only to the conjunct ?chains?, while in the sen-
tence b) ?the planned? refers to both conjuncts and
?MGM Grand? refers only to the first conjunct.
The Bohnet and Nivre (2012) parser succeeds in
finding the correct conjucts (shown in bold font)
on DT and makes mistakes on SB and CD in some
difficult cases like the following ones:
a) <. . .> investors hoard gold and help under-
pin its price <. . .>
b) Then take the expected return and subtract
one standard deviation.
CD and SB wrongly suggest ?gold? and ?help? to
be conjoined in the first sentence and ?return? and
?deviation? in the second.
5 Conclusions and future work
In this survey we gave a comparative experi-
mental overview of (i) parsing three dependency
schemes, viz., Stanford Basic (SB), CoNLL Syn-
tactic Dependencies (CD) and DELPH-IN Syn-
tactic Derivation Tree (DT), (ii) with three lead-
ing dependency parsers, viz., Malt, MST and the
Bohnet and Nivre (2012) parser (iii) exploiting
two different tagsets, viz., PTB tags and supertags.
From the parser perspective, the Bohnet and
Nivre (2012) parser performs better than Malt and
MST not only on conventional formats but also on
the new representation, although this parser solves
a harder task than Malt and MST.
From the dependency format perspective, DT
appeares to be a more difficult target dependency
representation than SB and CD. This suggests that
the expressivity that we gain from the grammar
theory (e.g. for coordination) is harder to learn
with state-of-the-art dependency parsers. CD and
DT are structurally closer to each other than SB
and DT; however, we did not observe sound evi-
dence of a correlation between structural similar-
ity of CD and DT and their parsing accuracies
Regarding the tagset aspect, it is natural that
PTB tags are good for SB and CD, whereas the
more fine-grained set of supertags fits DT bet-
ter. PTB tags and supertags are complementary,
and for all three parsers we observe slight benefits
from being supplied with both types of tags.
As future work we would like to run more ex-
periments with predicted supertags. In the absence
of a specialized supertagger, we can follow the
pipeline of (Ytrest?l, 2011) who reached the state-
of-the-art supertagging accuracy of 95%.
Another area of our interest is an extrinsic eval-
uation of SB, CD and DT, e.g. applied to semantic
role labeling and question-answering in order to
find out if the usage of the DT format grounded
in the computational grammar theory is beneficial
for such tasks.
Acknowledgments
The authors would like to thank Rebecca Dridan,
Joakim Nivre, Bernd Bohnet, Gertjan van Noord
and Jelke Bloem for interesting discussions and
the two anonymous reviewers for comments on
the work. Experimentation was made possible
through access to the high-performance comput-
ing resources at the University of Oslo.
36
References
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and
labeled non-projective dependency parsing. In
EMNLP-CoNLL, pages 1455?1465. ACL.
Harry Bunt, Paola Merlo, and Joakim Nivre, editors.
2010. Trends in Parsing Technology. Springer Ver-
lag, Stanford.
Jinho D Choi and Nicolas Nicolov. 2009. K-best, lo-
cally pruned, transition-based dependency parsing
using robust risk minimization. Recent Advances in
Natural Language Processing V, pages 205?216.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure trees. In
LREC.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05), pages 541?548, Ann Arbor,
Michigan, June. Association for Computational Lin-
guistics.
Daniel Flickinger, Yi Zhang, and Valia Kordoni. 2012.
DeepBank: a Dynamically Annotated Treebank of
the Wall Street Journal. In Proceedings of the
Eleventh International Workshop on Treebanks and
Linguistic Theories, pages 85?96. Edies Colibri.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ?10, pages 742?750, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Angelina Ivanova, Stephan Oepen, Lilja ?vrelid, and
Dan Flickinger. 2012. Who did what to whom?
a contrastive study of syntacto-semantic dependen-
cies. In Proceedings of the Sixth Linguistic Annota-
tion Workshop, pages 2?11, Jeju, Republic of Korea,
July. Association for Computational Linguistics.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
English. In Proceedings of NODALIDA 2007, pages
105?112, Tartu, Estonia, May 25-26.
Dan Klein and Christopher D. Manning. 2004.
Corpus-based induction of syntactic structure: mod-
els of dependency and constituency. In Proceedings
of the 42nd Annual Meeting on Association for Com-
putational Linguistics, ACL ?04, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn Treebank. Com-
putational Linguistics, 19(2):313?330, June.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing, HLT ?05, pages 523?530, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of the
Tenth Conference on Computational Natural Lan-
guage Learning, CoNLL-X ?06, pages 216?220,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun ichi Tsujii. 2010. Evaluating dependency repre-
sentations for event extraction. In Chu-Ren Huang
and Dan Jurafsky, editors, COLING, pages 779?787.
Tsinghua University Press.
Yusuke Miyao, Kenji Sagae, and Jun?ichi Tsujii. 2007.
Towards framework-independent evaluation of deep
linguistic parsers. In Ann Copestake, editor, Pro-
ceedings of the GEAF 2007 Workshop, CSLI Studies
in Computational Linguistics Online, page 21 pages.
CSLI Publications.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav
Marinov, and Erwin Marsi. 2007. MaltParser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Lilja ?vrelid, Jonas Kuhn, and Kathrin Spreyer. 2009.
Cross-framework parser stacking for data-driven de-
pendency parsing. TAL, 50(3):109?138.
Roy Schwartz, Omri Abend, and Ari Rappoport. 2012.
Learnability-based syntactic annotation design. In
Proc. of the 24th International Conference on Com-
putational Linguistics (Coling 2012), Mumbai, In-
dia, December. Coling 2012 Organizing Committee.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2012. A Compara-
tive Study of Target Dependency Structures for Sta-
tistical Machine Translation. In ACL (2), pages 100?
104. The Association for Computer Linguistics.
Akane Yakushiji, Yusuke Miyao, Tomoko Ohta, Yuka
Tateisi, and Jun?ichi Tsujii. 2006. Automatic con-
struction of predicate-argument structure patterns
for biomedical information extraction. In Proceed-
ings of the 2006 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?06,
pages 284?292, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Gisle Ytrest?l. 2011. Cuteforce: deep deterministic
HPSG parsing. In Proceedings of the 12th Interna-
tional Conference on Parsing Technologies, IWPT
?11, pages 186?197, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
37
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 69?78,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Simple Negation Scope Resolution through Deep Parsing:
A Semantic Solution to a Semantic Problem
Woodley Packard
?
, Emily M. Bender
?
, Jonathon Read
?
, Stephan Oepen
??
, and Rebecca Dridan
?
?
University of Washington, Department of Linguistics
?
Teesside University, School of Computing
?
University of Oslo, Department of Informatics
?
Potsdam University, Department of Linguistics
ebender@uw.edu, sweaglesw@sweaglesw.org, j.read@tees.ac.uk, {oe |rdridan}@ifi.uio.no
Abstract
In this work, we revisit Shared Task 1
from the 2012
*
SEM Conference: the au-
tomated analysis of negation. Unlike the
vast majority of participating systems in
2012, our approach works over explicit
and formal representations of proposi-
tional semantics, i.e. derives the notion of
negation scope assumed in this task from
the structure of logical-form meaning rep-
resentations. We relate the task-specific
interpretation of (negation) scope to the
concept of (quantifier and operator) scope
in mainstream underspecified semantics.
With reference to an explicit encoding
of semantic predicate-argument structure,
we can operationalize the annotation deci-
sions made for the 2012
*
SEM task, and
demonstrate how a comparatively simple
system for negation scope resolution can
be built from an off-the-shelf deep parsing
system. In a system combination setting,
our approach improves over the best pub-
lished results on this task to date.
1 Introduction
Recently, there has been increased community in-
terest in the theoretical and practical analysis of
what Morante and Sporleder (2012) call modality
and negation, i.e. linguistic expressions that mod-
ulate the certainty or factuality of propositions.
Automated analysis of such aspects of meaning
is important for natural language processing tasks
which need to consider the truth value of state-
ments, such as for example text mining (Vincze
et al, 2008) or sentiment analysis (Lapponi et al,
2012). Owing to its immediate utility in the cura-
tion of scholarly results, the analysis of negation
and so-called hedges in bio-medical research liter-
ature has been the focus of several workshops, as
well as the Shared Task at the 2011 Conference on
Computational Language Learning (CoNLL).
Task 1 at the First Joint Conference on Lex-
ical and Computational Semantics (
*
SEM 2012;
Morante and Blanco, 2012) provided a fresh, prin-
cipled annotation of negation and called for sys-
tems to analyze negation?detecting cues (affixes,
words, or phrases that express negation), resolv-
ing their scopes (which parts of a sentence are ac-
tually negated), and identifying the negated event
or property. The task organizers designed and
documented an annotation scheme (Morante and
Daelemans, 2012) and applied it to a little more
than 100,000 tokens of running text by the nov-
elist Sir Arthur Conan Doyle. While the task and
annotations were framed from a semantic perspec-
tive, only one participating system actually em-
ployed explicit compositional semantics (Basile et
al., 2012), with results ranking in the middle of
the 12 participating systems. Conversely, the best-
performing systems approached the task through
machine learning or heuristic processing over syn-
tactic and linguistically relatively coarse-grained
representations; see ? 2 below.
Example (1), where ?? marks the cue and {}
the in-scope elements, illustrates the annotations,
including how negation inside a noun phrase can
scope over discontinuous parts of the sentence.
1
(1) {The German} was sent for but professed to
{know} ?nothing? {of the matter}.
In this work, we return to the 2012
*
SEM
task from a deliberately semantics-centered point
of view, focusing on the hardest of the three
sub-problems: scope resolution.
2
Where Morante
and Daelemans (2012) characterize negation as an
?extra-propositional aspect of meaning? (p. 1563),
1
Our running example is a truncated variant of an item
from the Shared Task training data. The remainder of the
original sentence does not form part of the scope of this cue.
2
Resolving negation scope is a more difficult sub-problem
at least in part because (unlike cue and event identification) it
is concerned with much larger, non-local and often discontin-
uous parts of each utterance. This intuition is confirmed by
Read et al (2012), who report results for each sub-problem
using gold-standard inputs; in this setup, scope resolution
showed by far the lowest performance levels.
69
we in fact see it as a core piece of composi-
tionally constructed logical-form representations.
Though the task-specific concept of scope of
negation is not the same as the notion of quan-
tifier and operator scope in mainstream under-
specified semantics, we nonetheless find that re-
viewing the 2012
*
SEM Shared Task annotations
with reference to an explicit encoding of seman-
tic predicate-argument structure suggests a sim-
ple and straightforward operationalization of their
concept of negation scope. Our system imple-
ments these findings through a notion of functor-
argument ?crawling?, using as our starting point
the underspecified logical-form meaning represen-
tations provided by a general-purpose deep parser.
Our contributions are three-fold: Theoretically,
we correlate the structures at play in the Morante
and Daelemans (2012) view on negation with
formal semantic analyses; methodologically, we
demonstrate how to approach the task in terms of
underspecified, logical-form semantics; and prac-
tically, our combined system retroactively ?wins?
the 2012
*
SEM Shared Task. In the following
sections, we review related work (? 2), detail our
own setup (? 3), and present and discuss our ex-
perimental results (? 4 and ? 5, respectively).
2 Related Work
Read et al (2012) describe the best-performing
submission to Task 1 of the 2012
*
SEM Confer-
ence. They investigated two approaches for scope
resolution, both of which were based on syntac-
tic constituents. Firstly, they created a set of 11
heuristics that describe the path from the preter-
minal of a cue to the constituent whose projec-
tion is predicted to match the scope. Secondly
they trained an SVM ranker over candidate con-
stituents, generated by following the path from a
cue to the root of the tree and describing each
candidate in terms of syntactic properties along
the path and various surface features. Both ap-
proaches attempted to handle discontinuous in-
stances by applying two heuristics to the predicted
scope: (a) removing preceding conjuncts from the
scope when the cue is in a conjoined phrase and
(b) removing sentential adverbs from the scope.
The ranking approach showed a modest advan-
tage over the heuristics (with F
1
equal to 77.9
and 76.7, respectively, when resolving the scope
of gold-standard cues in evaluation data). Read et
al. (2012) noted however that the annotated scopes
did not align with the Shared Task?provided con-
stituents for 14% of the instances in the training
data, giving an F
1
upper-bound of around 86.0 for
systems that depend on those constituents.
Basile et al (2012) present the only submission
to Task 1 of the 2012
*
SEM Conference which
employed compositional semantics. Their scope
resolution pipeline consisted primarily of the C&C
parser and Boxer (Curran et al, 2007), which pro-
duce Discourse Representation Structures (DRSs).
The DRSs represent negation explicitly, including
representing other predications as being within the
scope of negation. Basile et al (2012) describe
some amount of tailoring of the Boxer lexicon to
include more of the Shared Task scope cues among
those that produce the negation operator in the
DRSs, but otherwise the system appears to directly
take the notion of scope of negation from the DRS
and project it out to the string, with one caveat: As
with the logical-forms representations we use, the
DRS logical forms do not include function words
as predicates in the semantics. Since the Shared
Task gold standard annotations included such ar-
guably semantically vacuous (see Bender, 2013,
p. 107) words in the scope, further heuristics are
needed to repair the string-based annotations com-
ing from the DRS-based system. Basile et al re-
sort to counting any words between in-scope to-
kens which are not themselves cues as in-scope.
This simple heuristic raises their F
1
for full scopes
from 20.1 to 53.3 on system-predicted cues.
3 System Description
The new system described here is what we call
the MRS Crawler. This system operates over
the normalized semantic representations provided
by the LinGO English Resource Grammar (ERG;
Flickinger, 2000).
3
The ERG maps surface strings
to meaning representations in the format of Mini-
mal Recursion Semantics (MRS; Copestake et al,
2005). MRS makes explicit predicate-argument
relations, as well as partial information about
scope (see below). We used the grammar together
with one of its pre-packaged conditional Maxi-
mum Entropy models for parse ranking, trained
on a combination of encyclopedia articles and
tourism brochures. Thus, the deep parsing front-
end system to our MRS Crawler has not been
3
In our experiments, we use the 1212 release of the ERG,
in combination with the ACE parser (http://sweaglesw
.org/linguistics/ace/). The ERG and ACE are DELPH-
IN resources; see http://www.delph-in.net.
70
? h
1
,
h
4
:_the_q?0:3?(ARG0 x
6
, RSTR h
7
, BODY h
5
), h
8
:_german_n_1?4:10?(ARG0 x
6
),
h
9
:_send_v_for?15:19?(ARG0 e
10
, ARG1 , ARG2 x
6
), h
2
:_but_c?24:27?(ARG0 e
3
, L-HNDL h
9
, R-HNDL h
14
),
h
14
:_profess_v_to?28:37?(ARG0 e
13
, ARG1 x
6
, ARG2 h
15
), h
16
:_know_v_1?41:45?(ARG0 e
17
, ARG1 x
6
, ARG2 x
18
),
h
20
:_no_q?46:53?(ARG0 x
18
, RSTR h
21
, BODY h
22
), h
19
:thing?46:53?(ARG0 x
18
),
h
19
:_of_p?54:56?(ARG0 e
23
, ARG1 x
18
, ARG2 x
24
),
h
25
:_the_q?57:60?(ARG0 x
24
, RSTR h
27
, BODY h
26
), h
28
:_matter_n_of?61:68?(ARG0 x
24
, ARG1 )
{ h
27
=
q
h
28
, h
21
=
q
h
19
, h
15
=
q
h
16
, h
7
=
q
h
8
, h
1
=
q
h
2
} ?
Figure 1: MRS analysis of our running example (1).
adapted to the task or its text type; it is applied
in an ?off the shelf? setting. We combine our
system with the outputs from the best-performing
2012 submission, the system of Read et al (2012),
firstly by relying on the latter for system negation
cue detection,
4
and secondly as a fall-back in sys-
tem combination as described in ? 3.4 below.
Scopal information in MRS analyses delivered
by the ERG fixes the scope of operators?such as
negation, modals, scopal adverbs (including sub-
ordinating conjunctions like while), and clause-
embedding verbs (e.g. believe)?based on their
position in the constituent structure, while leaving
the scope of quantifiers (e.g. a or every, but also
other determiners) free. From these underspec-
ified representations of possible scopal configu-
rations, a scope resolution component can spell
out the full range of fully-connected logical forms
(Koller and Thater, 2005), but it turns out that such
enumeration is not relevant here: the notion of
scope encoded in the Shared Task annotations is
not concerned with the relative scope of quantifiers
and negation, such as the two possible readings of
(2) represented informally below:
5
(2) Everyone didn?t leave.
a. ?(x)?leave(x) ? Everyone stayed.
b. ??(x)leave(x) ? At least some stayed.
However, as shown below, the information about
fixed scopal elements in an underspecified MRS is
sufficient to model the Shared Task annotations.
3.1 MRS Crawling
Fig. 1 shows the ERG semantic analysis for our
running example. The heart of the MRS is a mul-
tiset of elementary predications (EPs). Each ele-
4
Read et al (2012) predicted cues using a closed vocabu-
lary assumption with a supervised classifier to disambiguate
instances of cues.
5
In other words, a possible semantic interpretation of the
(string-based) Shared Task annotation guidelines and data is
in terms of a quantifier-free approach to meaning representa-
tion, or in terms of one where quantifier scope need not be
made explicit (as once suggested by, among others, Alshawi,
1992). From this interpretation, it follows that the notion of
scope assumed in the Shared Task does not encompass inter-
actions of negation operators and quantifiers.
mentary prediction includes a predicate symbol,
a label (or ?handle?, prefixed to predicates with
a colon in Fig. 1), and one or more argument
positions, whose values are semantic variables.
Eventualities (e
i
) in MRS denote states or activ-
ities, while instance variables (x
j
) typically corre-
spond to (referential or abstract) entities. All EPs
have the argument position ARG0, called the dis-
tinguished variable (Oepen and L?nning, 2006),
and no variable is the ARG0 of more than one non-
quantifier EP.
The arguments of one EP are linked to the argu-
ments of others either directly (sharing the same
variable as their value), or indirectly (through so-
called ?handle constraints?, where =
q
in Fig. 1 de-
notes equality modulo quantifier insertion). Thus
a well-formed MRS forms a connected graph. In
addition, the grammar links the EPs to the ele-
ments of the surface string that give rise to them,
via character offsets recorded in each EP (shown
in angle brackets in Fig. 1). For the purposes of
the present task, we take a negation cue as our en-
try point into the MRS graph (as our initial active
EP), and then move through the graph according
to the following simple operations to add EPs to
the active set:
Argument Crawling Add to the scope all EPs
whose distinguished variable or label is an argu-
ment of the active EP; for arguments of type h
k
,
treat any =
q
constraints as label equality.
Label Crawling Add all EPs whose label is iden-
tical to that of the active EP.
Functor Crawling Add all EPs that take the dis-
tinguished variable or label of the active EP as an
argument (directly or via =
q
constraints).
Our MRS crawling algorithm is sketched in
Fig. 2. To illustrate how the rules work, we will
trace their operation in the analysis of example (1),
i.e. traverse the EP graph in Fig. 1.
The negation cue is nothing, from character po-
sition 46 to 53. This leads us to _no_q as our en-
try point into the graph. Our algorithm states that
for this type of cue (a quantifier) the first step is
71
1: Activate the cue EP
2: if the cue EP is a quantifier then
3: Activate EPs reached by functor crawling from the distinguished variable (ARG0) of the cue EP
4: end if
5: repeat
6: for each active EP X do
7: Activate EPs reached by argument crawling or label crawling unless they are co-modifiers of the negation cue.
a
8: Activate EPs reached by functor crawling if they are modal verbs, or one of the following subordinating conjunctions
reached by ARG1: whether, when, because, to, with, although, unless, until, or as.
9: end for
10: until a fixpoint is reached (no additional EPs were activated)
11: Deactivate zero-pronoun EPs (from imperative constructions)
12: Apply semantically empty word handling rules (iterate until a fixpoint is reached)
13: Apply punctuation heuristics
Figure 2: Algorithm for scope detection by MRS crawling
a
Formally: If an EP shares its label with the negation cue, or is a quantifier whose restriction (RSTR) is =
q
equated with the
label of the negation cue, it cannot be in-scope unless its ARG0 is an argument of the negation cue, or the ARG0 of the negation
cue is one of its own arguments. See ? 3.3 for elaboration.
functor crawling (see ? 3.3 below), which brings
_know_v_1 into the scope. We proceed with ar-
gument crawling and label crawling, which pick
up _the_q?0:3? and _german_n_1 as the ARG1.
Further, as the ARG2 of _know_v_1, we reach
thing and through recursive invocation we acti-
vate _of_p and, in yet another level of recursion,
_the_q?57:60? and _matter_n_of. At this point,
crawling has no more links to follow. Thus, the
MRS crawling operations ?paint? a subset of the
MRS graph as in-scope for a given negation cue.
3.2 Semantically Empty Word Handling
Our crawling rules operate on semantic represen-
tations, but the annotations are with reference to
the surface string. Accordingly, we need projec-
tion rules to map from the ?painted? MRS to the
string. We can use the character offsets recorded
in each EP to project the scope to the string. How-
ever, the string-based annotations also include
words which the ERG treats as semantically vacu-
ous. Thus in order to match the gold annotations,
we define a set of heuristics for when to count vac-
uous words as in scope. In (1), there are no se-
mantically empty words in-scope, so we illustrate
these heuristics with another example:
(3) ?I trust that {there is} ?nothing? {of consequence
which I have overlooked}??
The MRS crawling operations discussed above
paint the EPs corresponding to is, thing, of, conse-
quence, I, and overlooked as in-scope (underlined
in (3)). Conversely, the ERG treats the words that,
there, which, and have as semantically empty. Of
these, we need to add all except that to the scope.
Our vacuous word handling rules use the syntac-
tic structure provided by the ERG as scaffolding to
help link the scope information gleaned from con-
tentful words to vacuous words. Each node in the
syntax tree is initially colored either in-scope or
out-of-scope in agreement with the decision made
by the crawler about the lexical head of the corre-
sponding subtree. A semantically empty word is
determined to be in-scope if there is an in-scope
syntax tree node in the right position relative to it,
as governed by a short list of templates organized
by the type of the semantically empty word (par-
ticles, complementizers, non-referential pronouns,
relative pronouns, and auxiliary verbs).
As an example, the rule for auxiliary verbs like
have in our example (3) is that they are in scope
when their verb phrase complement is in scope.
Since overlooked is marked as in-scope by the
crawler, the semantically empty have becomes in-
scope as well. Sometimes the rules need to be
iterated. For example, the main rule for relative
pronouns is that they are in-scope when they fill
a gap in an in-scope constituent; which fills a gap
in the constituent have overlooked, but since have
is the (syntactic) lexical head of that constituent,
the verb phrase is not considered in-scope the first
time the rules are tried.
Similar rules deal with that (complementizers
are in-scope when the complement phrase is an ar-
gument of an in-scope verb, which is not the case
here) and there (non-referential pronouns are in-
scope when they are the subject of an in-scope VP,
which is true here).
72
3.3 Re-Reading the Annotation Guidelines
Our MRS crawling algorithm was defined by look-
ing at the annotated data rather than the annota-
tion guidelines for the Shared Task (Morante et al,
2011). Nonetheless, our algorithm can be seen as
a first pass formalization of the guidelines. In this
section, we briefly sketch how our algorithm cor-
responds to different aspects of the guidelines.
For negated verbs, the guidelines state that ?If
the negated verb is the main verb in the sen-
tence, the entire sentence is in scope.? (Morante
et al, 2011, 17). In terms of our operations de-
fined over semantic representations, this is ren-
dered as follows: all arguments of the negated
verb are selected by argument crawling, all in-
tersective modifiers by label crawling, and func-
tor crawling (Fig. 2, line 8) captures modal auxil-
iaries and non-intersective modifiers. The guide-
lines treat predicative adjectives under a separate
heading from verbs, but describe the same desired
annotations (scope over the whole clause; ibid.,
p. 20). Since these structures are analogous in the
semantic representations, the same operations that
handle negated verbs also handle negated predica-
tive adjectives correctly.
For negated subjects and objects, the guidelines
state that the negation scopes over ?all the clause?
and ?the clause headed by the verb? (Morante et
al., 2011, 19), respectively. The examples given in
the annotation guidelines suggest that these are in
fact meant to refer to the same thing. The negation
cue for a negated nominal argument will appear
as a quantifier EP in the MRS, triggering line 3 of
our algorithm. This functor crawling step will get
to the verb?s EP, and from there, the process is the
same as the last two cases.
In contrast to subjects and objects, negation of
a clausal argument is not treated as negation of the
verb (ibid., p. 18). Since in this case, the negation
cue will not be a quantifier in the MRS, there will
be no functor crawling to the verb?s EP.
For negated modifiers, the situation is somewhat
more complex, and this is a case where our crawl-
ing algorithm, developed on the basis of the anno-
tated data, does not align directly with the guide-
lines as given. The guidelines state that negated at-
tributive adjectives have scope over the entire NP
(including the determiner) (ibid., p. 20) and anal-
ogously negated adverbs have scope over the en-
tire clause (ibid., p. 21). However, the annotations
are not consistent, especially with respect to the
treatment of negated adjectives: while the head
noun and determiner (if present) are typically an-
notated as in scope, other co-modifiers, especially
long, post-nominal modifiers (including relative
clauses) are not necessarily included:
(4) ?A dabbler in science, Mr. Holmes, a picker up
of shells on the shores of {the} great ?un?{known
ocean}.
(5) Our client looked down with a rueful face at {his}
own ?un?{conventional appearance}.
(6) Here was {this} ?ir?{reproachable Englishman}
ready to swear in any court of law that the accused
was in the house all the time.
(7) {There is}, on the face of it, {something}
?un?{natural about this strange and sudden friend-
ship between the young Spaniard and Scott Eccles}.
Furthermore, the guidelines treat relative clauses
as subordinate clauses and thus negation inside a
relative clause is treated as bound to that clause
only, and includes neither the head noun of the
relative clause nor any of its other dependents in
its scope. However, from the perspective of MRS,
a negated relative clause is indistinguishable from
any other negated modifier of a noun. This treat-
ment of relative clauses (as well as the inconsis-
tencies in other forms of co-modification) is the
reason for the exception noted at line 7 of Fig. 2.
By disallowing the addition of EPs to the scope if
they share the label of the negation cue but are not
one of its arguments, we block the head noun?s EP
(and any EPs only reachable from it) in cases of
relative clauses where the head verb inside the rel-
ative clause is negated. It also blocks co-modifiers
like great, own, and the phrases headed by ready
and about in (4)?(7). As illustrated in these exam-
ples, this is correct some but not all of the time.
Having been unable to find a generalization cap-
turing when comodifiers are annotated as in scope,
we stuck with this approximation.
For negation within clausal modifiers of verbs,
the annotation guidelines have further informa-
tion, but again, our existing algorithm has the cor-
rect behavior: The guidelines state that a negation
cue inside of the complement of a subordinating
conjunction (e.g. if ) has scope only over the sub-
ordinate clause (ibid., p. 18 and p. 26). The ERG
treats all subordinating conjunctions as two-place
predicates taking two scopal arguments. Thus,
as with clausal complements of clause-embedding
verbs, the embedding subordinating conjunction
and any other arguments it might have are inac-
cessible, since functor crawling is restricted to a
handful of specific configurations.
73
As is usually the case with exercises in for-
malization, our crawling algorithm generalizes be-
yond what is given explicitly in the annotation
guidelines. For example, all arguments that are
treated as semantically nominal (including PP ar-
guments where the preposition is semantically
null) are treated in the same way as subjects and
objects; similarly, all arguments which are seman-
tically clausal (including certain PP arguments)
are handled the same way as clausal complements.
This is possible because we take advantage of the
high degree of normalization that the ERG accom-
plishes in mapping to the MRS representation.
There are also cases where we are more spe-
cific. The guidelines do not handle coordination in
detail, except to state that in coordinated clauses
negation is restricted to the clause it appears in
(ibid., p. 17?18) and to include a few examples of
coordination under the heading ?ellipsis?. In the
case of VP coordination, our existing algorithm
does not need any further elaboration to pick up
the subject of the coordinated VP but not the non-
negated conjunct, as shown in discussion of (1) in
? 3.1 above. In the case of coordination of negated
NPs, recall that to reach the main portion of the
negated scope we must first apply functor crawl-
ing. The functor crawling procedure has a general
mechanism to transparently continue crawling up
through coordinated structures while blocking fu-
ture crawling from traversing them again.
6
On the other hand, there are some cases in the
annotation guidelines which our algorithm does
not yet handle. We have not yet provided any anal-
ysis of the special cases for save and expect dis-
cussed in Morante et al, 2011, pp. 22?23, and also
do not have a means of picking out the overt verb
in gapping constructions (p. 24).
Finally, we note that even carefully worked out
annotation guidelines such as these are never fol-
lowed perfectly consistently by the human annota-
tors who apply them. Because our crawling algo-
rithm so closely models the guidelines, this puts
our system in an interesting position to provide
feedback to the Shared Task organizers.
3.4 Fall-Back Configurations
The close match between our crawling algorithm
and the annotation guidelines supported by the
mapping to MRS provides for very high precision
6
This allows ate to be reached in We ate bread but no fish.,
while preventing but and bread from being reached, which
they otherwise would via argument crawling from ate.
and recall when the analysis engine produces the
desired MRS.
7
However, the analysis engine does
not always provide the desired analysis, largely
because of idiosyncrasies of the genre (e.g. voca-
tives appearing mid-sentence) that are either not
handled by the grammar or not well modeled in the
parse selection component. In addition, as noted
above, there are a handful of negation cues we do
not yet handle. Thus, we also tested fall-back con-
figurations which use scope predictions based on
MRS in some cases, and scope predictions from
the system of Read et al (2012) in others.
Our first fall-back configuration (Crawler
N
in
Table 1) uses MRS-based predictions whenever
there is a parse available and the cue is one that
our system handles. Sometimes, the analysis
picked by the ERG?s statistical model is not the
correct analysis for the given context. To com-
bat such suboptimal parse selection performance,
we investigated using the probability of the top
ranked analysis (as determined by the parse selec-
tion model and conditioned on the sentence) as a
confidence metric. Our second fall-back configu-
ration (Crawler
P
in Table 1) uses MRS-based pre-
dictions when there is a parse available whose con-
ditional probability is at least 0.5.
8
4 Experiments
We evaluated the performance of our system using
the Shared Task development and evaluation data
(respectively CDD and CDE in Table 1). Since we
do not attempt to perform cue detection, we report
performance using gold cues and also using the
system cues predicted by Read et al (2012). We
used the official Shared Task evaluation script to
compute all scores.
4.1 Data Sets
The Shared Task data consists of chapters from
the Adventures of Sherlock Holmes mystery nov-
els and short stories. As such, the text is carefully
edited turn-of-the-20th-century British English,
9
7
And in fact, the task is somewhat noise-tolerant: some
parse selection decisions are independent of each other, and
a mistake in a part of the analysis far enough away from the
negation cue does not harm performance.
8
This threshold was determined empirically on the devel-
opment data. We also experimented with other confidence
metrics?the probability ratio of the top-ranked and second
parse or the entropy over the probability distribution of the
top 10 parses?but found no substantive differences.
9
In contrast, the ERG was engineered for the analysis of
contemporary American English, and an anecdotal analysis
of parse failures and imperfect top-ranked parses suggests
74
Gold Cues System Cues
Scopes Tokens Scopes Tokens
Set Method Prec Rec F
1
Prec Rec F
1
Prec Rec F
1
Prec Rec F
1
C
D
D
Ranker 100.0 68.5 81.3 84.8 86.8 85.8 91.7 66.1 76.8 79.5 84.9 82.1
Crawler 100.0 53.0 69.3 89.3 67.0 76.6 90.8 53.0 66.9 84.7 65.9 74.1
Crawler
N
100.0 64.9 78.7 89.0 83.5 86.1 90.8 64.3 75.3 82.6 82.1 82.3
Crawler
P
100.0 70.2 82.5 86.4 86.8 86.6 91.2 67.9 77.8 80.0 84.9 82.4
Oracle 100.0 76.8 86.9 91.5 89.1 90.3
C
D
E
Ranker 98.8 64.3 77.9 85.3 90.7 87.9 87.4 61.5 72.2 82.0 88.8 85.3
Crawler 100.0 44.2 61.3 85.8 68.4 76.1 87.8 43.4 58.1 78.8 66.7 72.2
Crawler
N
98.6 56.6 71.9 83.8 88.4 86.1 86.0 54.2 66.5 78.4 85.7 81.9
Crawler
P
98.8 65.5 78.7 86.1 90.4 88.2 87.6 62.7 73.1 82.6 88.5 85.4
Oracle 100.0 70.3 82.6 89.5 93.1 91.3
Table 1: Scope resolution performance of various configurations over each subset of the Shared Task
data. Ranker refers to the system of Read et al (2012); Crawler refers to our current system in isolation,
or falling back to the Ranker prediction either when the sentence is not covered by the parser (Crawler
N
),
or when the parse probability is predicted to be less than 0.5 (Crawler
P
); finally, Oracle simulates best
possible selection among the Ranker and Crawler predictions (and would be ill-defined on system cues).
annotated with token-level information about the
cues and scopes in every negated sentence. The
training set contains 848 negated sentences, the
development set 144, and the evaluation set 235.
As there can be multiple usages of negation in one
sentence, this corresponds to 984, 173, and 264
instances, respectively.
Being rule-based, our system does not require
any training data per se. However, the majority of
our rule development and error analysis were per-
formed against the designated training data. We
used the designated development data for a single
final round of error analysis and corrections. The
system was declared frozen before running with
the formal evaluation data. All numbers reported
here reflect this frozen system.
10
4.2 Results
Table 1 presents the results of our various config-
urations in terms of both (a) whole scopes (i.e. a
true positive is only generated when the predicted
scope matches the gold scope exactly) and (b) in-
scope tokens (i.e. a true positive for every token
the system correctly predicts to be in scope). The
table also details the performance upper-bound for
system combination, in which an oracle selects the
system prediction which scores the greater token-
wise F
1
for each gold cue.
The low recall levels for Crawler can be mostly
that the archaic style in the 2012
*
SEM Shared Task texts
has a strong adverse effect on the parser.
10
The code and data are available from http://www
.delph-in.net/crawler/, for replicability (Fokkens et al,
2013).
attributed to imperfect parser coverage. Crawler
N
,
which falls back just for parse failure brings the
recall back up, and results in F
1
levels closer to
the system of Read et al (2012), albeit still not
quite advancing the state of the art (except over
the development set). Our best results are from
Crawler
P
, which outperforms all other configura-
tions on the development and evaluation sets.
The Oracle results are interesting because they
show that there is much more to be gained in com-
bining our semantics-based system with the Read
et al (2012) syntactically-focused system. Further
analysis of these results to draw out the patterns of
complementary errors and strengths is a promising
avenue for future work.
4.3 Error Analysis
To shed more light on specific strengths and weak-
nesses of our approach, we performed a manual er-
ror analysis of scope predictions by Crawler, start-
ing from gold cues so as to focus in-depth analy-
sis on properties specific to scope resolution over
MRSs. This analysis was performed on CDD, in
order to not bar future work on this task. Of the
173 negation cue instances in CDD, Crawler by it-
self makes 94 scope predictions that exactly match
the gold standard. In comparison, the system of
Read et al (2012) accomplishes 119 exact scope
matches, of which 80 are shared with Crawler; in
other words, there are 14 cue instances (or 8%
of all cues) in which our approach can improve
over the best-performing syntax-based submission
to the original Shared Task.
75
We reviewed the 79 negation instances where
Crawler made a wrong prediction in terms of ex-
act scope match, categorizing the source of failure
into five broad error types:
(1) Annotation Error In 11% of all instances, we
consider the annotations erroneous or inconsistent.
These judgments were made by two of the authors,
who both were familiar with the annotation guide-
lines and conventions observable in the data. For
example, Morante et al (2011) unambiguously
state that subordinating conjunctions shall not be
in-scope (8), whereas relative pronouns should be
(9), and a negated predicative argument to the cop-
ula must scope over the full clause (10):
(8) It was after nine this morning {when we} reached
his house and {found} ?neither? {you} ?nor?
{anyone else inside it}.
(9) ?We can imagine that in the confusion of flight
something precious, something which {he could}
?not? {bear to part with}, had been left behind.
(10) He said little about the case, but from that little we
gathered that he also was not ?dis?{satisfied} at the
course of events.
(2) Parser Failure Close to 30% of Crawler fail-
ures reflect lacking coverage in the ERG parser,
i.e. inputs for which the parser does not make
available an analysis (within certain bounds on
time and memory usage).
11
In this work, we have
treated the ERG as an off-the-shelf system, but
coverage could certainly be straightforwardly im-
proved by adding analyses for phenomena partic-
ular to turn-of-the-20th-century British English.
(3) MRS Inadequacy Another 33% of our false
scope predictions are Crawler-external, viz. owing
to erroneous input MRSs due to imperfect disam-
biguation by the parser or other inadequacies in
the parser output. Again, these judgments (assign-
ing blame outside our own work) were double-
checked by two authors, and we only counted
MRS imperfections that actually involve the cue
or in-scope elements. Here, we could anticipate
improvements by training the parse ranker on in-
domain data or otherwise adapting it to this task.
(4) Cue Selection In close to 9% of all cases,
there is a valid MRS, but Crawler fails to pick out
an initial EP that corresponds to the negation cue.
This first type of genuine crawling failure often re-
lates to cues expressed as affixation (11), as well
11
Overall parsing coverage on this data is about 86%, but
of course all parser failures on sentences containing negation
surface in our error analysis of Crawler in isolation.
Scopes Tokens
Method Prec Rec F
1
Prec Rec F
1
C
D
E
Boxer 76.1 41.0 53.3 69.2 82.3 75.2
Crawler 87.8 43.4 58.1 78.8 66.7 72.2
Crawler
P
87.6 62.7 73.1 82.6 88.5 85.4
Table 2: Comparison to Basile et al (2012).
as to rare usages of cue expressions that predomi-
nantly occur with different categories, e.g. neither
as a generalized quantifier (12):
(11) Please arrange your thoughts and let me know, in
their due sequence, exactly what those events are
{which have sent you out} ?un?{brushed} and un-
kempt, with dress boots and waistcoat buttoned
awry, in search of advice and assistance.
(12) You saw yourself {how} ?neither? {of the inspec-
tors dreamed of questioning his statement}, extraor-
dinary as it was.
(5) Crawler Deficiency Finally, a little more
than 16% of incorrect predictions we attribute to
our crawling rules proper, where we see many
instances of under-coverage of MRS elements
(13, 14) and a few cases of extending the scope too
wide (15). In the examples below, erroneous scope
predictions by Crawler are indicated through un-
derlining. Hardly any of the errors in this category,
however, involve semantically vacuous tokens.
(13) He in turn had friends among the indoor
servants who unite in {their} fear and
?dis?{like of their master}.
(14) He said little about the case, but from that
little we gathered that {he also was} ?not?
{dissatisfied at the course of events}.
(15) I tell you, sir, {I could}n?t move a finger, ?nor?
{get my breath}, till it whisked away and was gone.
5 Discussion and Comparison
The example in (1) nicely illustrates the strengths
of the MRS Crawler and of the abstraction pro-
vided by the deep linguistic analysis made pos-
sible by the ERG. The negated verb in that sen-
tence is know, and its first semantic argument is
The German. This semantic dependency is di-
rectly and explicitly represented in the MRS, but
the phrase expressing the dependent is not adja-
cent to the head in the string. Furthermore, even
a system using syntactic structure to model scope
would be faced with a more complicated task than
our crawling rules: At the level of syntax the de-
pendency is mediated by both verb phrase coordi-
nation and the control verb profess, as well as by
the semantically empty infinitival marker to.
76
The system we propose is very similar in spirit
to that of Basile et al (2012). Both systems map
from logical forms with explicit representations of
scope of negation out to string-based annotations
in the format provided by the Shared Task gold
standard. The main points of difference are in the
robustness of the system and in the degree of tai-
loring of both the rules for determining scope on
the logical form level and the rules for handling se-
mantically vacuous elements. The system descrip-
tion in Basile et al (2012) suggests relatively little
tailoring at either level: aside from adjustments to
the Boxer lexicon to make more negation cues take
the form of the negation operator in the DRS, the
notion of scope is directly that given in the DRS.
Similarly, their heuristic for picking up semanti-
cally vacuous words is string-based and straight-
forward. Our system, on the other hand, models
the annotation guidelines more closely in the def-
inition of the MRS crawling rules, and has more
elaborated rules for handling semantically empty
words. The Crawler alone is less robust than the
Boxer-based system, returning no output for 29%
of the cues in CDE. These factors all point to
higher precision and lower recall for the Crawler
compared to the Boxer-based system. At the to-
ken level, that is what we see. Since full-scope re-
call depends on token-level precision, the Crawler
does better across the board at the full-scope level.
A comparison of the results is shown in Table 2.
A final key difference between our results and
those of Basile et al (2012) is the cascading with
a fall-back system. Presumably a similar system
combination strategy could be pursued with the
Boxer-based system in place of the Crawler.
6 Conclusion and Outlook
Our motivation in this work was to take the design
of the 2012
*
SEM Shared Task on negation analy-
sis at face value?as an overtly semantic problem
that takes a central role in our long-term pursuit of
language understanding. Through both theoreti-
cal and practical reflection on the nature of repre-
sentations at play in this task, we believe we have
demonstrated that explicit semantic structure will
be a key driver of further progress in the analy-
sis of negation. We were able to closely align
two independently developed semantic analyses?
the negation-specific annotations of Morante et al
(2011), on the one hand, and the broad-coverage,
MRS meaning representations of the ERG, on the
other hand. In our view, the conceptual correla-
tion between these two semantic views on nega-
tion analysis reinforces their credibility.
Unlike the rather complex top-performing sys-
tems from the original 2012 competition, our MRS
Crawler is defined by a small set of general rules
that operate over general-purpose, explicit mean-
ing representations. Thus, our approach scores
high on transparency, adaptability, and replicabil-
ity. In isolation, the Crawler provides premium
precision but comparatively low recall. Its limi-
tations, we conjecture, reflect primarily on ERG
parsing challenges and inconsistencies in the tar-
get data. In a sense, our approach pushes a
larger proportion of the task into the parser, mean-
ing (a) there should be good opportunities for
parser adaptation to this somewhat idiosyncratic
text type; (b) our results can serve to offer feed-
back on ERG semantic analyses and parse rank-
ing; and (c) there is a much smaller proportion
of very task-specific engineering. When embed-
ded in a confidence-thresholded cascading archi-
tecture, our system advances the state of the art
on this task, and oracle combination scores sug-
gest there is much remaining room to better ex-
ploit the complementarity of approaches in our
study. In future work, we will seek to better un-
derstand the division of labor between the systems
involved through contrastive error analysis and
possibly another oracle experiment, constructing
gold-standard MRSs for part of the data. It would
also be interesting to try a task-specific adaptation
of the ERG parse ranking model, for example re-
training on the pre-existing treebanks but giving
preference to analyses that lead to correct Crawler
results downstream.
Acknowledgments
We are grateful to Dan Flickinger, the main devel-
oper of the ERG, for many enlightening discus-
sions and continuous assistance in working with
the analyses available from the grammar. This
work grew out of a discussion with colleagues of
the Language Technology Group at the University
of Oslo, notably Elisabeth Lien and Jan Tore L?n-
ning, to whom we are indebted for stimulating co-
operation. Furthermore, we have benefited from
comments by participants of the 2013 DELPH-
IN Summit, in particular Joshua Crowgey, Guy
Emerson, Glenn Slayden, Sanghoun Song, and
Rui Wang.
77
References
Alshawi, H. (Ed.). 1992. The Core Language Engine.
Cambridge, MA, USA: MIT Press.
Basile, V., Bos, J., Evang, K., and Venhuizen, N.
2012. UGroningen. Negation detection with Dis-
course Representation Structures. In Proceedings of
the 1st Joint Conference on Lexical and Computa-
tional Semantics (p. 301 ? 309). Montr?al, Canada.
Bender, E. M. 2013. Linguistic fundamentals for nat-
ural language processing: 100 essentials from mor-
phology and syntax. San Rafael, CA, USA: Morgan
& Claypool Publishers.
Copestake, A., Flickinger, D., Pollard, C., and Sag,
I. A. 2005. Minimal Recursion Semantics. An intro-
duction. Research on Language and Computation,
3(4), 281 ? 332.
Curran, J., Clark, S., and Bos, J. 2007. Linguistically
motivated large-scale NLP with C&C and Boxer.
In Proceedings of the 45th Meeting of the Associa-
tion for Computational Linguistics Demo and Poster
Sessions (p. 33 ? 36). Prague, Czech Republic.
Flickinger, D. 2000. On building a more efficient gram-
mar by exploiting types. Natural Language Engi-
neering, 6 (1), 15 ? 28.
Fokkens, A., van Erp, M., Postma, M., Pedersen,
T., Vossen, P., and Freire, N. 2013. Offspring
from reproduction problems. What replication fail-
ure teaches us. In Proceedings of the 51th Meet-
ing of the Association for Computational Linguistics
(p. 1691 ? 1701). Sofia, Bulgaria.
Koller, A., and Thater, S. 2005. Efficient solving and
exploration of scope ambiguities. In Proceedings of
the 43rd Meeting of the Association for Computa-
tional Linguistics: Interactive Poster and Demon-
stration Sessions (p. 9 ? 12). Ann Arbor, MI, USA.
Lapponi, E., Read, J., and ?vrelid, L. 2012. Repre-
senting and resolving negation for sentiment analy-
sis. In Proceedings of the 2012 ICDM workshop on
sentiment elicitation from natural text for informa-
tion retrieval and extraction. Brussels, Belgium.
Morante, R., and Blanco, E. 2012. *SEM 2012 Shared
Task. Resolving the scope and focus of negation. In
Proceedings of the 1st Joint Conference on Lexical
and Computational Semantics (p. 265 ? 274). Mon-
tr?al, Canada.
Morante, R., and Daelemans, W. 2012. ConanDoyle-
neg. Annotation of negation in Conan Doyle stories.
In Proceedings of the 8th International Conference
on Language Resources and Evaluation. Istanbul,
Turkey.
Morante, R., Schrauwen, S., and Daelemans, W. 2011.
Annotation of negation cues and their scope guide-
lines v1.0 (Tech. Rep. # CTRS-003). Antwerp, Bel-
gium: Computational Linguistics & Psycholinguis-
tics Research Center, Universiteit Antwerpen.
Morante, R., and Sporleder, C. 2012. Modality and
negation. An introduction to the special issue. Com-
putational Linguistics, 38(2), 223 ? 260.
Oepen, S., and L?nning, J. T. 2006. Discriminant-
based MRS banking. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (p. 1250 ? 1255). Genoa, Italy.
Read, J., Velldal, E., ?vrelid, L., and Oepen, S. 2012.
UiO1. Constituent-based discriminative ranking for
negation resolution. In Proceedings of the 1st Joint
Conference on Lexical and Computational Seman-
tics (p. 310 ? 318). Montr?al, Canada.
Vincze, V., Szarvas, G., Farkas, R., M?ra, G., and
Csirik, J. 2008. The BioScope corpus. Biomedical
texts annotated for uncertainty, negation and their
scopes. BMC Bioinformatics, 9(Suppl 11).
78
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 310?318,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UiO1: Constituent-Based Discriminative Ranking for Negation Resolution
Jonathon Read Erik Velldal Lilja ?vrelid Stephan Oepen
University of Oslo, Department of Informatics
{jread,erikve,liljao,oe}@ifi.uio.no
Abstract
This paper describes the first of two systems
submitted from the University of Oslo (UiO)
to the 2012 *SEM Shared Task on resolving
negation. Our submission is an adaption of
the negation system of Velldal et al (2012),
which combines SVM cue classification with
SVM-based ranking of syntactic constituents
for scope resolution. The approach further ex-
tends our prior work in that we also identify
factual negated events. While submitted for
the closed track, the system was the top per-
former in the shared task overall.
1 Introduction
The First Joint Conference on Lexical and Compu-
tational Semantics (*SEM 2012) hosts a shared task
on resolving negation (Morante and Blanco, 2012).
This involves the subtasks of (i) identifying nega-
tion cues, (ii) identifying the in-sentence scope of
these cues, and (iii) identifying negated (and factual)
events. This paper describes a system submitted by
the Language Technology Group at the University of
Oslo (UiO). Our starting point is the negation system
developed by Velldal et al (2012) for the domain of
biomedical texts, an SVM-based system for classi-
fying cues and ranking syntactic constituents to re-
solve cue scopes. However, we extend and adapt
this system in several important respects, such as in
terms of the underlying linguistic formalisms that
are used, the textual domain, handling of morpho-
logical cues and discontinuous scopes, and in that
the current system also identifies negated events.
The data sets used for the shared task include
the following, all based on negation-annotated Co-
nan Doyle (CD) stories (Morante and Daelemans,
2012): a training set of 3644 sentences (hereafter
referred to as CDT), a development set of 787 sen-
tences (CDD), and a held-out evaluation set of 1089
sentences (CDE). We will refer to the combination
of CDT and CDD as CDTD. An example of an an-
notated sentence is shown in (1) below, where the
cue is marked in bold, the scope is underlined, and
the event marked in italics.
(1) There was no answer.
We describe two different system configurations,
both of which were submitted for the closed track
(hence we can only make use of the data provided
by the task organizers). The systems only differ
with respect to how they were optimized. In the
first configuration, (hereafter I), all components in
the pipeline had their parameters tuned by 10-fold
cross-validation across CDTD. The second config-
uration (II) is tuned against CDD using CDT for
training. The rationale for this strategy is to guard
against possible overfitting effects that could result
from either optimization scheme, given the limited
size of the data sets. For the held-out testing all mod-
els are estimated on the entire CDTD.
Unless otherwise noted, all reported scores are
generated using the evaluation script provided by the
organizers, which breaks down performance with re-
spect to cues, events, scope tokens, and two vari-
ants of scope-level exact match (one requiring exact
match of cues and the other only partial cue match).
The latter two scores are identical for our system
hence are not duplicated in this paper. Furthermore,
as we did not optimize for the scope tokens measure
this is only reported for the final evaluation.
Note also that the evaluation actually includes
two variants of the metrics mentioned above; a set
of primary measures with precision computed as
P = TP/(TP + FP ) and a set of so-called B mea-
sures that instead uses P = TP/S, where S is the
310
total number of predictions made by the system. The
reason why S is not identical with TP + FP is
that partial matches are only counted as FNs (and
not FPs) in order to avoid double penalties. We
do not report the B measures for development test-
ing as they were only introduced for the final eval-
uation and hence were not considered in our sys-
tem optimization. We note though, that the relative-
ranking of participating systems for the primary and
B measures is identical, and that the correlation be-
tween the paired lists of scores is nearly perfect
(r = 0.997).
The paper is structured according to the compo-
nents of our system. Section 2 details the process of
identifying instances of negation through the disam-
biguation of known cue words and affixes. Section 3
describes our hybrid approach to scope resolution,
which utilizes both heuristic and data-driven meth-
ods to select syntactic constituents. Section 4 dis-
cusses our event detection component, which first
applies a classifier to filter out non-factual events
and then uses a learned ranking function to select
events among in-scope tokens. End-to-end results
are presented in Section 5.
2 Cue Detection
Cue identification is based on the light-weight clas-
sification scheme presented by Velldal et al (2012).
By treating the set of cue words as a closed class,
Velldal et al (2012) showed that one could greatly
reduce the number of examples presented to the
learner, and correspondingly the number of fea-
tures, while at the same time improving perfor-
mance. This means that the classifier only attempts
to ?disambiguate? known cue words, while ignoring
any words not observed as cues in the training data.
The classifier applied in the current submission
is extended to also handle morphological or affixal
negation cues, such as the prefix cue in impatience,
the infix in carelessness, and the suffix of colourless.
The negation affixes observed in CDTD are; the pre-
fixes un, dis, ir, im, and in; the infix less (we inter-
nally treat this as the suffixes lessly and lessness);
and the suffix less. Of the total set of 1157 cues in
the training and development data, 192 are affixal.
There are, however, a total of 1127 tokens matching
one of the affix patterns above, and while we main-
tain the closed class assumption also for the affixes,
the classifier will need to consider their status as a
cue or non-cue when attaching to any such token, as
in image, recklessness, and bless.
2.1 Features
In the initial formulation of Velldal (2011), an SVM
classifier was applied using simple n-gram features
over words, both full forms and lemmas, to the
left and right of the candidate cues. In addition to
these token-level features, the classifier we apply
here includes features specifically targeting affixal
cues. The first such feature records character n-
grams from both the beginning and end of the base
that an affix attaches to (up to five positions). For
a context like impossible we would record n-grams
such as {possi, poss, . . .} and {sible, ible, . . .}, and
combine this with information about the affix itself
(im) and the token part-of-speech (?JJ?).
For the second type of affix-specific features, we
try to emulate the effect of a lexicon look-up of the
remaining substring that an affix attaches to, check-
ing its status as an independent base form and its
part-of-speech. In order to take advantage of such
information while staying within the confines of the
closed track, we automatically generate a lexicon
from the training data, counting the instances of each
PoS tagged lemma in addition to n-grams of word-
initial characters (again recording up to five posi-
tions). For a given match of an affix pattern, a fea-
ture will then record these counts for the substring it
attaches to. The rationale for this feature is that the
occurrence of a substring such as un in a token such
as underlying should be less likely as a cue given
that the first part of the remaining string (e.g., derly)
would be an unlikely way to begin a word.
It is also possible for a negation cue to span multi-
ple tokens, such as the (discontinuous) pair neither /
nor or fixed expressions like on the contrary. There
are, however, only 16 instances of such multiword
cues (MWCs) in the entire CDTD. Rather than let-
ting the classifier be sensitive to these corner cases,
we cover such MWC patterns using a small set of
simple post-processing heuristics. A small stop-list
is used for filtering out the relevant words from the
examples presented to the classifier (on, the, etc.).
Note that, in terms of training the final classifiers,
CDTD provides us with a total of 1162 positive and
311
Data set Model Prec Rec F1
CDTD
Baseline 92.25 88.50 90.34
ClassifierI 94.99 95.07 95.03
CDD
Baseline 90.68 84.39 87.42
ClassifierII 93.75 95.38 94.56
CDE
Baseline 87.10 92.05 89.51
ClassifierI 91.42 92.80 92.10
ClassifierII 89.17 93.56 91.31
Table 1: Detecting negation cues using the two clas-
sifiers and the majority-usage baseline.
1100 negative training examples, given our closed-
class treatment of cues.
Before we turn to the results, note that the dif-
ference between the two submitted versions of the
classifier (I and II) only concerns the orders of the
n-grams used for the token-level features.1
2.2 Results
Table 1 presents the results for our cue classifier. As
an informed baseline, we also tried classifying each
word based on its most frequent use as a cue or non-
cue in the training data. (Affixal cue occurrences are
counted by looking at both the affix-pattern and the
base it attaches to, basically treating the entire token
as a cue. Tokens that end up being classified as cues
are then matched against the affix patterns observed
during training in order to correctly delimit the an-
notation of the cue.) This simple majority-usage
approach actually provides a fairly strong baseline,
yielding an F1 of 90.34 on CDTD. Compare this to
the F1 of 95.03 obtained by the classifier on the same
data set. However, when applying the models to the
held-out set, with models estimated over the entire
CDTD, the classifier suffers a slight drop in perfor-
mance, leaving the baseline even more competitive:
While our best performing final cue classifier (I)
achieves F1=92.10, the baseline achieves F1=89.51,
and even outperforms four of the ten cue detection
systems submitted for the shared task (three of the
12 shared task submissions use the same classifier).
1Classifier I records the lemma and full form of the target
token, and lemmas two positions left/right. Classifier II records
the lemma, form, and PoS of the target, full forms three posi-
tions to the left and one to the right, PoS one position right/left,
and lemmas three positions to the right. The affixal-specific fea-
tures are the same for both configurations as described above.
S
NP
EX
There
VP
VBD
was
NP
DT
no
NN
answer
.
.
Figure 1: Example parse tree provided in the data,
highlighting our candidate scope constituents.
Inspecting the predictions of the classifier on
CDD, which comprises a total of 173 gold anno-
tated cues, we find that Classifier I mislabels 11
false positives (FPs) and seven false negatives (FNs).
Of the FPs, we find five so-called false negation
cues (Morante et al, 2011), including three in-
stances of the fixed expression none the less. The
others are affixal cues, of which two are clearly
wrong (underworked, universal) while others might
arguably be due to annotation errors (insuperable,
unhappily, endless, listlessly). Among the FNs, two
are due to MWCs not covered by our heuristics (e.g.,
no more), with the remainder concerning affixes.
3 Constituent-Based Scope Resolution
During the development of our scope resolution sys-
tem we have pursued both a rule-based and data-
driven approach. Both are rooted in the assumption
that the scope of negations corresponds to a syntac-
tically meaningful unit. Our starting point here will
be the syntactic analyses provided by the task or-
ganizers (see Figure 1), generated using the rerank-
ing parser of Charniak and Johnson (2005). How-
ever, as alignment between scope annotations and
syntactic units is not straightforward for all cases,
we apply several exception rules that ?slacken? the
requirements for alignment, as discussed in Sec-
tion 3.1. In Sections 3.2 and 3.3 we detail our
rule-based and data-driven approaches, respectively.
Note that the predictions of the rule-based compo-
nent will be incorporated as features in the learned
model, similarly to the set-up described by Read et
al. (2011). Section 3.4 details the post-processing
we apply to handle cases of discontinuous scope, be-
312
fore Section 3.5 finally presents development results
together with a brief error analysis.
3.1 Constituent Alignment and Slackening
In order to test our initial assumption that syntactic
units correspond to scope annotations, we quantify
the alignment of scopes with constituents in CDT,
excluding 97 negations that do not have a scope.
We find that the initial alignment is rather low at
52.42%. We therefore formulate a set of slacken-
ing heuristics, designed to improve on this alignment
by removing certain constituents at the beginning or
end of a scope. First of all, removing constituent-
initial and -final punctuation improves alignment to
72.83%. We then apply the following slackening
rules, with examples indicating the resulting scope
following slackening (not showing events):
- Remove coordination (CC) and following con-
juncts if the coordination is a rightwards sibling
of an ancestor of the cue and it is not directly
dominated by an NP.
(2) Since we have been so unfortunate as to miss him
and have no notion [. . . ]
- Remove S* to the right of cue, if delimited by
punctuation.
(3) ?There is no other claimant, I presume ??
- Remove constituent-initial SBAR.
(4) If it concerned no one but myself I would not
try to keep it from you.?
- Remove punctuation-delimited NPs.
(5) ?But I can?t forget them, Miss Stapleton,? said I.
- Remove constituent-initial RB, CC, UH,
ADVP or INTJ.
(6) And yet it was not quite the last.
The slackening rules are based on a few obser-
vations. First, scope rarely crosses coordination
boundaries (with the exception of nominal coordi-
nation). Second, scope usually does not cross clause
boundaries (indicated by S/SBAR). Furthermore, ti-
tles and other nominals of address are not included
in the scope. Finally, sentence and discourse adver-
bials are often excluded from the scope. Since these
express semantic distinctions, we approximate this
RB//VP/SBAR if SBAR\WH*
RB//VP/S
RB//S
DT/NP if NP/PP
DT//SBAR if SBAR\WHADVP
DT//S
JJ//ADJPVP/S if S\VP\VB*[@lemma="be"]
JJ/NP/NP if NP\PP
JJ//NP
UH
IN/PP
NN/NP//S/SBAR if SBAR\WHNP
NN/NP//S
CC/SINV
Figure 2: Scope resolution heuristics.
notion syntactically using parts-of-speech and con-
stituent category labels expressing adverbials (RB),
coordinations (CC), various types of interjections
(UH, INTJ) and adverbial phrases (ADVP). We may
note here that syntactic categories are not always
sufficient to express semantic distinctions. Preposi-
tional phrases, for instance, are often used to express
the same type of discourse adverbials, but can also
express a range of other distinctions (e.g., tempo-
ral or locative adverbials), which are included in the
scope. So a slackening rule removing initial PPs was
tried but not found to improve overall alignment.
After applying the above slackening rules the
alignment rate for CDT improves to 86.13%. This
also represents an upper-bound on our performance,
as we will not be able to correctly predict a scope
that does not align with a (slackened) constituent.
3.2 Heuristics Operating over Constituents
The alignment of constituents and scopes reveal con-
sistent patterns and we therefore formulate a set of
heuristic rules over constituents. These are based
on frequencies of paths from the cue to the scope-
aligned constituent for the annotations in CDT, as
well as the annotation guidelines (Morante et al,
2011). The rules are formulated as paths over con-
stituent trees and are presented in Figure 2. The
path syntax is based on LPath (Lai and Bird, 2010).
The rules are listed in order of execution, showing
how more specific rules are consulted before more
general ones. We furthermore allow for some ad-
ditional functionality in the interpretation of rules
by enabling simple constraints that are applied to
the candidate constituent. For example, the rule
RB//VP/SBAR if SBAR\WH* will be activated when
the cue is an adverb having some ancestor VP which
has a parent SBAR, where the SBAR must contain a
WH-phrase among its children.
313
In cases where no rule is activated we use a de-
fault scope prediction, which expands the scope to
both the left and the right of the cue until either the
sentence boundary or a punctuation mark is reached.
The rules are evaluated individually in Section 3.5
below and the rule predictions are furthermore em-
ployed as features for the ranker described below.
3.3 Constituent Ranking
Our data-driven approach to scope resolution in-
volves learning a ranking function over candidate
syntactic constituents. The approach has similari-
ties to discriminative parse selection, except that we
here rank subtrees rather than full parses.
When defining the training data, we begin by se-
lecting negations for which the parse tree contains
a constituent that (after slackening) aligns with the
gold scope. We then select an initial candidate by
selecting the smallest constituent that spans all the
words in the cue, and then generate subsequent can-
didates by traversing the path to the root of the
tree (see Figure 1). This results in a mean ambi-
guity of 4.9 candidate constituents per negation (in
CDTD). Candidates whose projection corresponds
to the gold scope are labeled as correct; all others are
labeled as incorrect. Experimenting with a variety of
feature types (listed in Table 2), we use the imple-
mentation of ordinal ranking in the SVMlight toolkit
(Joachims, 2002) to learn a linear scoring function
for preferring correct candidate scopes.
The most informative feature type is the LPath
from cue, which in addition to recording the full
path from the cue to the candidate constituent
(e.g., the path to the correct candidate in Fig-
ure 1 is no/DT/NP/VP/S), also includes delexicalized
(./DT/NP/VP/S), generalized (no/DT//S), and gen-
eralized delexicalized versions (./DT//S).
Note that the rule prediction feature facilitates a
hybrid approach by recording whether the candidate
matches the boundaries of the scope predicted by the
rules of Section 3.2, as well as the degree of overlap.
3.4 Handling Discontinuous Scope
10.3% of the scopes in the training data are what
(Morante et al, 2011) refer to as discontinuous. This
means that the scope contains two or more parts
which are bridged by tokens other than the cue.
Feature types I II
LPath from cue ? ?
LPath from cue bigrams and trigrams ? ?
LPath from cue to left/right boundary ?
LPath to left/right boundary ?
LPath to root ?
Punctuation to left/right ? ?
Rule prediction ?
Sibling bigrams ?
Size in tokens, relative to sentence (%) ? ?
Surface bigrams ? ?
Tree distance from cue ? ?
Table 2: Features used to describe candidate con-
stituents for scope resolution, with indications of
presence in our two system configurations.
(7) I therefore spent the day at my club and did not
return to Baker Street until evening.
(8) There was certainly no physical injury of any kind.
The sentence in (7) exemplifies a common cause
of scopal discontinuity in the data, namely ellipsis
(Morante et al, 2011). Almost all of these are cases
of coordination, as in example (7) where the cue is
found in the final conjunct (did not return [. . . ]) and
the scope excludes the preceding conjunct(s) (there-
fore spent the day at my club). There are also some
cases of adverbs that are excluded from the scope,
causing discontinuity, as in (8), where the adverb
certainly is excluded from the scope.
In order to deal with discontinuous scopes we for-
mulate two simple post-processing heuristics, which
are applied after rules/ranking: (1) If the cue is in
a conjoined phrase, remove the previous conjuncts
from the scope, and (2) remove sentential adverbs
from the scope (where a list of sentential adverbs
was compiled from the training data).
3.5 Results
Our development procedure evaluated all permuta-
tions of feature combinations, searching for opti-
mal parameters using gold-standard cues. Table 2
indicates which features are included in our two
ranker configurations, i.e., tuning by 10-fold cross-
validation on CDTD (I) vs. a train/test-split for
CDT/CDD(II).
Table 3 lists the results of our scope resolution
approaches applied to gold cues. As a baseline, all
314
Data set Model Prec Rec F1
CDTD
Baseline 98.31 33.18 49.61
Rules 100.00 71.37 83.29
RankerI 100.00 73.55 84.76
CDD
Baseline 100.00 36.31 53.28
Rules 100.00 69.64 82.10
RankerII 100.00 70.24 82.52
CDE
Baseline 96.47 32.93 49.10
Rules 98.73 62.65 76.66
RankerI 98.77 64.26 77.86
RankerII 98.75 63.45 77.26
Table 3: Scope resolution for gold cues using the
two versions of the ranker, also listing the perfor-
mance of the rule-based approach in isolation.
cases are assigned the default scope prediction of the
rule-based approach. On CDTD this results in an F1
of 49.61 (P=98.31, R=33.18); compare to the ranker
in Configuration I on the same data set (F1=84.76,
P=100.00, R=73.55). We note that our different op-
timization procedures do not appear to have made
much difference to the learned ranking functions as
both perform similarly on the held-out data, though
suffering a slight drop in performance compared to
the development results. We also evaluate the rules
and observe that this approach achieves similar held-
out results. This is particularly note-worthy given
that there are only fourteen rules plus the default
scope baseline. Note that, as the rankers performed
better than the rules in isolation on both CDTD and
CDD during development, our final system submis-
sions are based on rankers I and II from Table 3.
We performed a manual error analysis of our
scope resolution system (RankerII) on the basis of
CDD (using gold cues). First, we may note that
parse errors are a common sources of scope res-
olution errors. It is well-known that coordina-
tion presents a difficult construction for syntactic
parsers, and we often find incorrectly parsed coordi-
nate structures among the system errors. Since coor-
dination is used both in the slackening rules and the
analysis of discontinuous scopes, these errors have
clear effects on system performance. We may fur-
ther note that discourse-level adverbials, such as in
the second place in example (9) below, are often in-
cluded in the scope assigned by our system, which
they should not be according to the gold annotation.
(9) But, in the second place, why did you not come at once?
There are also quite a few errors related to the scope
of affixal cues, which the ranker often erroneously
assigns a scope that is larger than simply the base
which the affix attaches to.
4 Event Detection
Our event detection component implements two
stages: First we apply a factuality classifier, and
then we identify negated events2 for those contexts
that have been labeled as factual. We detail the two
stages in order below.
4.1 Factuality Detection
The annotation guidelines of Morante et al (2011)
specify that events should only be annotated for
negations that have a scope and that occur in fac-
tual statements. This means that we can view the
*SEM data sets to implicitly annotate factuality and
non-factuality, and take advantage of this to train an
SVM factuality classifier. We take positive exam-
ples to correspond to negations annotated with both
a scope and an event, while negative examples corre-
spond to scope negations with no event. For CDTD,
this strategy gives 738 positive and 317 negative ex-
amples, spread over a total of 930 sentences. Note
that we do not have any explicit annotation of cue
words for these examples. All we have are instances
of negation that we know to be within a factual or
non-factual context, but the indication of factuality
may typically be well outside the annotated nega-
tion scope. For our experiments here, we therefore
use the negation cue itself as a place-holder for the
abstract notion of context that we are really classi-
fying. Given the limited amount of data, we only
optimize our factuality classifier by 10-fold cross-
validation on CDTD (i.e., the same configuration is
used for submissions I and II).
The feature types we use are all variations over
bag-of-words (BoW) features. We include left- and
right-oriented BoW features centered on the nega-
tion cue, recording forms, lemmas, and PoS, and us-
ing both unigrams and bigrams. The features are ex-
2Note that the annotation guidelines use the term event
rather broadly as referring to a process, action, state, or prop-
erty (Morante et al, 2011).
315
Data set Model Prec Rec F1 Acc
CDTD
Baseline 69.95 100.00 82.32 69.95
Classifier 84.51 96.07 89.92 83.98
CDE
Baseline 69.48 100.00 81.99 69.48
Classifier 77.73 95.91 85.86 78.31
Table 4: Results for factuality detection (using gold
negation cues and scopes). Due to the limited train-
ing data for factuality, the classifier is only opti-
mized by 10-fold cross-validation on CDTD.
tracted from the sentence as a whole, as well as from
a local window of six tokens to each side of the cue.
Table 4 provides results for factuality classifica-
tion using gold-standard cues and scopes.3 We also
include results for a baseline approach that simply
considers all cases to be factual, i.e., the majority
class. In this case precision is identical to accuracy
and recall is 100%. For precision and accuracy we
see that the classifier improves substantially over the
baseline on both data sets, although there is a bit of a
drop in performance when going from the 10-fold to
held-out results. There also seem to be some signs
of overfitting, given that roughly 70% of the training
examples end up as support vectors.
4.2 Ranking Events
Having filtered out non-factual contexts, events are
identified by applying a similar approach to that of
the scope-resolving ranker described in Section 3.3.
In this case, however, we rank tokens as candidates
for events. For simplicity in this first round of de-
velopment we make the assumption that all events
are single words. Thus, the system will be unable to
correctly predict the event in the 6.94% of instances
in CDTD that are multi-word.
We select candidate words from all those marked
as being in the scope (including substrings of to-
kens with affixal cues). This gives a mean ambigu-
ity of 7.8 candidate events per negation (in CDTD).
Then, discarding multi-word training examples, we
use SVMlight to learn a ranking function for identi-
fying events among the candidates.
Table 5 shows the features employed, with in-
3As this is not singled out as a separate subtask in the shared
task itself, these are the only scores in the paper not computed
using the script provided by the organizers.
Feature type I II
Contains affixal cue ?
Following lemma ?
Lemma ? ?
LPath to scope constituent ? ?
LPath to scope constituent bigrams ? ?
Part-of-speech ? ?
Position in scope ? ?
Preceding lemma ? ?
Preceding part-of-speech ? ?
Token distance from cue ? ?
Table 5: Features used to describe candidates for
event detection, with indications of presence in our
two system configurations.
Data set Model Prec Rec F1
CDTD RankerI 91.49 90.83 91.16
CDD RankerII 92.11 91.30 91.70
CDE
RankerI 83.73 83.73 83.73
RankerII 84.94 84.95 84.94
Table 6: Event detection for gold scopes and gold
factuality information.
dications as to their presence in our two configu-
rations (after an exhaustive search of feature com-
binations). The most important feature was LPath
to scope constituent. For example, in Figure 1
the scope constituent is the S root of the tree;
the path that describes the correct candidate is
answer/NN/NP/VP/S. As discussed in Section 3.3,
we also record generalized, delexicalized and gener-
alized delexicalized paths.
Table 6 lists the results of the event ranker applied
to gold-standard cues, scopes, and factuality. For a
comparative baseline, we implemented a keyword-
based approach that simply searches in-scope words
for instances of events previously observed in the
training set, sorted according to descending fre-
quency. This baseline achieves F1=29.44 on CDD.
For comparison, the ranker (II) achieves F1=91.70
on the same data set, as seen in Table 6. We also
see that Configuration II appears to generalize best,
with over 1.2 points improvement over the F1 of I.
An analysis of the event predictions for CDD in-
dicates that the most frequent errors (41.2%) are in-
stances where the ranker correctly predicts part of
the event but our single word assumption is invalid.
Another apparent error is that the system fails to
316
Submission I Submission II
Prec Rec F1 Prec Rec F1
Cues 91.42 92.80 92.10 89.17 93.56 91.31
Scopes 87.43 61.45 72.17 83.89 60.64 70.39
Scope Tokens 81.99 88.81 85.26 75.87 90.08 82.37
Events 60.50 72.89 66.12 60.58 75.00 67.02
Full negation 83.45 43.94 57.57 79.87 45.08 57.63
Cues B 89.09 92.80 90.91 86.97 93.56 90.14
Scopes B 59.30 61.45 60.36 56.55 60.64 58.52
Events B 57.62 72.89 64.36 58.60 75.00 65.79
Full negation B 42.18 43.94 43.04 41.90 45.08 43.43
Table 7: End-to-end results on the held-out data.
predict a main verb for the event, and instead pre-
dicts nouns (17.7% of all errors), modals (17.7%) or
prepositions (11.8%).
5 Held-Out Evaluation
Table 7 presents our final results for both system
configurations on the held-out evaluation data (also
including the B measures, as discussed in the intro-
duction). Comparing submission I and II, we find
that the latter has slightly better scores end-to-end.
However, as seen throughout the paper, the picture is
less clear-cut when considering the isolated perfor-
mance of each component. When ranked according
to the Full Negation measures, our submissions were
placed first and second (out of seven submissions in
the closed track, and twelve submissions total). It
is difficult to compare system performance on sub-
tasks, however, as each component will be affected
by the performance of the previous.
6 Conclusions
This paper has presented two closed-track submis-
sions for the *SEM 2012 shared task on negation
resolution. The systems were ranked first and sec-
ond overall in the shared task end-to-end evaluation,
and the submissions only differ with respect to the
data sets used for parameter tuning. There are four
components in the pipeline: (i) An SVM classifier
for identifying negation cue words and affixes, (ii)
an SVM-based ranker that combines empirical evi-
dence and manually-crafted rules to resolve the in-
sentence scope of negation, (iii) a classifier for de-
termining whether a negation is in a factual or non-
factual context, and (iv) a ranker that determines
(factual) negated events among in-scope tokens.
For future work we would like to try training sepa-
rate classifiers for affixal and token-level cues, given
that largely separate sets of features are effective for
the two cases. The system might also benefit from
sources of information that would place it in the
open track. These include drawing information from
other parsers and formalisms, generating cue fea-
tures from an external lexicon, and using additional
training data for factuality detection, e.g., FactBank
(Saur?? and Pustejovsky, 2009).
From observations on CDTD we note that approx-
imately 14% of scopes will be unresolvable as they
are not aligned with constituents (see Section 3.1).
This can perhaps be tackled by ranking tokens as
candidates for left and right scope boundaries (sim-
ilar to the event ranker in the current work). This
would improve the upper-bound to 100% at the ex-
pense of greatly increasing the number of candi-
dates. However, the strong discriminative power of
our current approach can still be incorporated using
constituent-based features.
Acknowledgments
We thank Roser Morante and Eduardo Blanco for
their work in organizing this shared task and com-
mitment to producing quality data. We also thank
the anonymous reviewers for their feedback. Large-
scale experimentation was carried out with the TI-
TAN HPC facilities at the University of Oslo.
317
References
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the Forty-Third Annual Meeting
of the Association for Computational Linguistics, Ann
Arbor, MI.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the Eighth
ACM International Conference on Knowledge Discov-
ery and Data Mining, Alberta.
Catherine Lai and Steven Bird. 2010. Querying linguis-
tic trees. Journal of Logic, Language and Information,
19:53?73.
Roser Morante and Eduardo Blanco. 2012. *SEM 2012
shared task: Resolving the scope and focus of nega-
tion. In Proceedings of the First Joint Conference on
Lexical and Computational Semantics, Montreal.
Roser Morante and Walter Daelemans. 2012.
ConanDoyle-neg: Annotation of negation in Conan
Doyle stories. In Proceedings of the Eighth Interna-
tional Conference on Language Resources and Evalu-
ation, Istanbul.
Roser Morante, Sarah Schrauwen, and Walter Daele-
mans. 2011. Annotation of negation cues and their
scope: Guidelines v1.0. Technical report, Univer-
sity of Antwerp. CLIPS: Computational Linguistics
& Psycholinguistics technical report series.
Jonathon Read, Erik Velldal, Stephan Oepen, and Lilja
?vrelid. 2011. Resolving speculation and negation
scope in biomedical articles using a syntactic con-
stituent ranker. In Proceedings of the Fourth Inter-
national Symposium on Languages in Biology and
Medicine, Singapore.
Roser Saur?? and James Pustejovsky. 2009. Factbank:
a corpus annotated with event factuality. Language
Resources and Evaluation, 43(3):227?268.
Erik Velldal, Lilja ?vrelid, Jonathon Read, and Stephan
Oepen. 2012. Speculation and negation: Rules,
rankers and the role of syntax. Computational Lin-
guistics, 38(2).
Erik Velldal. 2011. Predicting speculation: A simple dis-
ambiguation approach to hedge detection in biomedi-
cal literature. Journal of Biomedical Semantics, 2(5).
318
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 63?72,
Dublin, Ireland, August 23-24, 2014.
SemEval 2014 Task 8:
Broad-Coverage Semantic Dependency Parsing
Stephan Oepen
??
, Marco Kuhlmann
?
, Yusuke Miyao
?
, Daniel Zeman
?
,
Dan Flickinger
?
, Jan Haji
?
c
?
, Angelina Ivanova
?
, and Yi Zhang
?
?
University of Oslo, Department of Informatics
?
Potsdam University, Department of Linguistics
?
Link?ping University, Department of Computer and Information Science
?
National Institute of Informatics, Tokyo
?
Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics
?
Stanford University, Center for the Study of Language and Information
?
Nuance Communications Aachen GmbH
sdp-organizers@emmtee.net
Abstract
Task 8 at SemEval 2014 defines Broad-
Coverage Semantic Dependency Pars-
ing (SDP) as the problem of recovering
sentence-internal predicate?argument rela-
tionships for all content words, i.e. the se-
mantic structure constituting the relational
core of sentence meaning. In this task
description, we position the problem in
comparison to other sub-tasks in compu-
tational language analysis, introduce the se-
mantic dependency target representations
used, reflect on high-level commonalities
and differences between these representa-
tions, and summarize the task setup, partic-
ipating systems, and main results.
1 Background and Motivation
Syntactic dependency parsing has seen great ad-
vances in the past decade, in part owing to rela-
tively broad consensus on target representations,
and in part reflecting the successful execution of a
series of shared tasks at the annual Conference for
Natural Language Learning (CoNLL; Buchholz &
Marsi, 2006; Nivre et al., 2007; inter alios). From
this very active research area accurate and efficient
syntactic parsers have developed for a wide range
of natural languages. However, the predominant
data structure in dependency parsing to date are
trees, in the formal sense that every node in the de-
pendency graph is reachable from a distinguished
root node by exactly one directed path.
This work is licenced under a Creative Commons At-
tribution 4.0 International License. Page numbers and the
proceedings footer are added by the organizers: http://
creativecommons.org/licenses/by/4.0/.
Unfortunately, tree-oriented parsers are ill-suited
for producing meaning representations, i.e. mov-
ing from the analysis of grammatical structure to
sentence semantics. Even if syntactic parsing ar-
guably can be limited to tree structures, this is not
the case in semantic analysis, where a node will
often be the argument of multiple predicates (i.e.
have more than one incoming arc), and it will often
be desirable to leave nodes corresponding to se-
mantically vacuous word classes unattached (with
no incoming arcs).
Thus, Task 8 at SemEval 2014, Broad-Coverage
Semantic Dependency Parsing (SDP 2014),
1
seeks
to stimulate the dependency parsing community
to move towards more general graph processing,
to thus enable a more direct analysis of Who did
What to Whom? For English, there exist several
independent annotations of sentence meaning over
the venerable Wall Street Journal (WSJ) text of the
Penn Treebank (PTB; Marcus et al., 1993). These
resources constitute parallel semantic annotations
over the same common text, but to date they have
not been related to each other and, in fact, have
hardly been applied for training and testing of data-
driven parsers. In this task, we have used three
different such target representations for bi-lexical
semantic dependencies, as demonstrated in Figure 1
below for the WSJ sentence:
(1) A similar technique is almost impossible to apply to
other crops, such as cotton, soybeans, and rice.
Semantically, technique arguably is dependent on
the determiner (the quantificational locus), the mod-
ifier similar, and the predicate apply. Conversely,
the predicative copula, infinitival to, and the vac-
1
See http://alt.qcri.org/semeval2014/
task8/ for further technical details, information on how to
obtain the data, and official results.
63
A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice .
A1 A2
(a) Partial semantic dependencies in PropBank and NomBank.
A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice.
top
ARG2 ARG3 ARG1
ARG2mwe _and_cARG1ARG1
BV
ARG1 implicit_conjARG1
(b) DELPH-IN Minimal Recursion Semantics?derived bi-lexical dependencies (DM).
A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice
top
ARG1
ARG2
ARG1
ARG2
ARG2
ARG1
ARG1 ARG1 ARG1ARG1
ARG1
ARG2
ARG1
ARG2
ARG1
ARG2
ARG1 ARG1 ARG1 ARG2
(c) Enju Predicate?Argument Structures (PAS).
A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice .
RSTR
PAT
EXT
PAT
ACT
RSTR
ADDR
ADDR
ADDR
ADDR
APPS.m
APPS.m
CONJ.m
CONJ.m CONJ.m
top
(d) Parts of the tectogrammatical layer of the Prague Czech-English Dependency Treebank (PCEDT).
Figure 1: Sample semantic dependency graphs for Example (1).
uous preposition marking the deep object of ap-
ply can be argued to not have a semantic contri-
bution of their own. Besides calling for node re-
entrancies and partial connectivity, semantic depen-
dency graphs may also exhibit higher degrees of
non-projectivity than is typical of syntactic depen-
dency trees.
In addition to its relation to syntactic dependency
parsing, the task also has some overlap with Se-
mantic Role Labeling (SRL; Gildea & Jurafsky,
2002). In much previous work, however, target
representations typically draw on resources like
PropBank and NomBank (Palmer et al., 2005; Mey-
ers et al., 2004), which are limited to argument
identification and labeling for verbal and nominal
predicates. A plethora of semantic phenomena?
for example negation and other scopal embedding,
comparatives, possessives, various types of modi-
fication, and even conjunction?typically remain
unanalyzed in SRL. Thus, its target representations
are partial to a degree that can prohibit seman-
tic downstream processing, for example inference-
based techniques. In contrast, we require parsers
to identify all semantic dependencies, i.e. compute
a representation that integrates all content words in
one structure. Another difference to common inter-
pretations of SRL is that the SDP 2014 task defini-
tion does not encompass predicate disambiguation,
a design decision in part owed to our goal to focus
on parsing-oriented, i.e. structural, analysis, and in
part to lacking consensus on sense inventories for
all content words.
Finally, a third closely related area of much cur-
rent interest is often dubbed ?semantic parsing?,
which Kate and Wong (2010) define as ?the task of
mapping natural language sentences into complete
formal meaning representations which a computer
can execute for some domain-specific application.?
In contrast to most work in this tradition, our SDP
target representations aim to be task- and domain-
independent, though at least part of this general-
ity comes at the expense of ?completeness? in the
above sense; i.e. there are aspects of sentence mean-
ing that arguably remain implicit.
2 Target Representations
We use three distinct target representations for se-
mantic dependencies. As is evident in our run-
ning example (Figure 1), showing what are called
the DM, PAS, and PCEDT semantic dependencies,
there are contentful differences among these anno-
tations, and there is of course not one obvious (or
even objective) truth. In the following paragraphs,
64
we provide some background on the ?pedigree? and
linguistic characterization of these representations.
DM: DELPH-IN MRS-Derived Bi-Lexical De-
pendencies These semantic dependency graphs
originate in a manual re-annotation of Sections 00?
21 of the WSJ Corpus with syntactico-semantic
analyses derived from the LinGO English Re-
source Grammar (ERG; Flickinger, 2000). Among
other layers of linguistic annotation, this resource?
dubbed DeepBank by Flickinger et al. (2012)?
includes underspecified logical-form meaning rep-
resentations in the framework of Minimal Recur-
sion Semantics (MRS; Copestake et al., 2005).
Our DM target representations are derived through
a two-step ?lossy? conversion of MRSs, first to
variable-free Elementary Dependency Structures
(EDS; Oepen & L?nning, 2006), then to ?pure?
bi-lexical form?projecting some construction se-
mantics onto word-to-word dependencies (Ivanova
et al., 2012). In preparing our gold-standard
DM graphs from DeepBank, the same conversion
pipeline was used as in the system submission of
Miyao et al. (2014). For this target representa-
tion, top nodes designate the highest-scoping (non-
quantifier) predicate in the graph, e.g. the (scopal)
degree adverb almost in Figure 1.
2
PAS: Enju Predicate-Argument Structures
The Enju parsing system is an HPSG-based parser
for English.
3
The grammar and the disambigua-
tion model of this parser are derived from the Enju
HPSG treebank, which is automatically converted
from the phrase structure and predicate?argument
structure annotations of the PTB. The PAS data
set is extracted from the WSJ portion of the Enju
HPSG treebank. While the Enju treebank is an-
notated with full HPSG-style structures, only its
predicate?argument structures are converted into
the SDP data format for use in this task. Top
nodes in this representation denote semantic heads.
Again, the system description of Miyao et al. (2014)
provides more technical detail on the conversion.
PCEDT: Prague Tectogrammatical Bi-Lexical
Dependencies The Prague Czech-English De-
pendency Treebank (PCEDT; Haji
?
c et al., 2012)
4
is a set of parallel dependency trees over the WSJ
2
Note, however, that non-scopal adverbs act as mere in-
tersective modifiers, e.g. loudly is a predicate in DM, but the
main verb provides the top node in structures like Abrams
sang loudly.
3
See http://kmcs.nii.ac.jp/enju/.
4
See http://ufal.mff.cuni.cz/pcedt2.0/.
id form lemma pos top pred arg1 arg2
#20200002
1 Ms. Ms. NNP ? + _ _
2 Haag Haag NNP ? ? compound ARG1
3 plays play VBZ + + _ _
4 Elianti Elianti NNP ? ? _ ARG2
5 . . . ? ? _ _
Table 1: Tabular SDP data format (showing DM).
texts from the PTB, and their Czech translations.
Similarly to other treebanks in the Prague family,
there are two layers of syntactic annotation: an-
alytical (a-trees) and tectogrammatical (t-trees).
PCEDT bi-lexical dependencies in this task have
been extracted from the t-trees. The specifics of
the PCEDT representations are best observed in the
procedure that converts the original PCEDT data to
the SDP data format; see Miyao et al. (2014). Top
nodes are derived from t-tree roots; i.e. they mostly
correspond to main verbs. In case of coordinate
clauses, there are multiple top nodes per sentence.
3 Graph Representation
The SDP target representations can be character-
ized as labeled, directed graphs. Formally, a se-
mantic dependency graph for a sentence x =
x
1
, . . . , x
n
is a structure G = (V,E, `
V
, `
E
) where
V = {1, . . . , n} is a set of nodes (which are in
one-to-one correspondence with the tokens of the
sentence); E ? V ? V is a set of edges; and `
V
and `
E
are mappings that assign labels (from some
finite alphabet) to nodes and edges, respectively.
More specifically for this task, the label `
V
(i) of a
node i is a tuple consisting of four components: its
word form, lemma, part of speech, and a Boolean
flag indicating whether the corresponding token
represents a top predicate for the specific sentence.
The label `
E
(i? j) of an edge i? j is a seman-
tic relation that holds between i and j. The exact
definition of what constitutes a top node and what
semantic relations are available differs among our
three target representations, but note that top nodes
can have incoming edges.
All data provided for the task uses a column-
based file format (dubbed the SDP data format)
similar to the one of the 2009 CoNLL Shared Task
(Haji
?
c et al., 2009). As in that task, we assume gold-
standard sentence and token segmentation. For
ease of reference, each sentence is prefixed by a
line with just a unique identifier, using the scheme
2SSDDIII, with a constant leading 2, two-digit sec-
tion code, two-digit document code (within each
65
section), and three-digit item number (within each
document). For example, identifier 20200002 de-
notes the second sentence in the first file of PTB
Section 02, the classic Ms. Haag plays Elianti. The
annotation of this sentence is shown in Table 1.
With one exception, our fields (i.e. columns in
the tab-separated matrix) are a subset of the CoNLL
2009 inventory: (1) id, (2) form, (3) lemma, and
(4) pos characterize the current token, with token
identifiers starting from 1 within each sentence. Be-
sides the lemma and part-of-speech information, in
the closed track of our task, there is no explicit
analysis of syntax. Across the three target represen-
tations in the task, fields (1) and (2) are aligned and
uniform, i.e. all representations annotate exactly
the same text. On the other hand, fields (3) and (4)
are representation-specific, i.e. there are different
conventions for lemmatization, and part-of-speech
assignments can vary (but all representations use
the same PTB inventory of PoS tags).
The bi-lexical semantic dependency graph over
tokens is represented by two or more columns start-
ing with the obligatory, binary-valued fields (5)
top and (6) pred. A positive value in the top
column indicates that the node corresponding to
this token is a top node (see Section 2 below). The
pred column is a simplification of the correspond-
ing field in earlier tasks, indicating whether or not
this token represents a predicate, i.e. a node with
outgoing dependency edges. With these minor dif-
ferences to the CoNLL tradition, our file format can
represent general, directed graphs, with designated
top nodes. For example, there can be singleton
nodes not connected to other parts of the graph,
and in principle there can be multiple tops, or a
non-predicate top node.
To designate predicate?argument relations, there
are as many additional columns as there are pred-
icates in the graph (i.e. tokens marked + in the
pred column); these additional columns are called
(7) arg1, (8) arg2, etc. These colums contain
argument roles relative to the i-th predicate, i.e. a
non-empty value in column arg1 indicates that
the current token is an argument of the (linearly)
first predicate in the sentence. In this format, graph
reentrancies will lead to a token receiving argument
roles for multiple predicates (i.e. non-empty arg
i
values in the same row). All tokens of the same sen-
tence must always have all argument columns filled
in, even on non-predicate words; in other words,
all lines making up one block of tokens will have
the same number n of fields, but n can differ across
DM PAS PCEDT
(1) # labels 51 42 68
(2) % singletons 22.62 4.49 35.79
(3) # edge density 0.96 1.02 0.99
(4) %
g
trees 2.35 1.30 56.58
(5) %
g
projective 3.05 1.71 53.29
(6) %
g
fragmented 6.71 0.23 0.56
(7) %
n
reentrancies 27.35 29.40 9.27
(8) %
g
topless 0.28 0.02 0.00
(9) # top nodes 0.9972 0.9998 1.1237
(10) %
n
non-top roots 44.71 55.92 4.36
Table 2: Contrastive high-level graph statistics.
sentences, depending on the count of graph nodes.
4 Data Sets
All three target representations are annotations of
the same text, Sections 00?21 of the WSJ Cor-
pus. For this task, we have synchronized these
resources at the sentence and tokenization levels
and excluded from the SDP 2014 training and test-
ing data any sentences for which (a) one or more of
the treebanks lacked a gold-standard analysis; (b) a
one-to-one alignment of tokens could not be estab-
lished across all three representations; or (c) at least
one of the graphs was cyclic. Of the 43,746 sen-
tences in these 22 first sections of WSJ text, Deep-
Bank lacks analyses for close to 15%, and the Enju
Treebank has gaps for a little more than four per-
cent. Some 500 sentences show tokenization mis-
matches, most owing to DeepBank correcting PTB
idiosyncrasies like ?G.m.b, H.?, ?S.p, A.?, and
?U.S., .?, and introducing a few new ones (Fares
et al., 2013). Finally, 232 of the graphs obtained
through the above conversions were cyclic. In total,
we were left with 34,004 sentences (or 745,543
tokens) as training data (Sections 00?20), and 1348
testing sentences (29,808 tokens), from Section 21.
Quantitative Comparison As a first attempt at
contrasting our three target representations, Table 2
shows some high-level statistics of the graphs com-
prising the training data.
5
In terms of distinctions
5
These statistics are obtained using the ?official? SDP
toolkit. We refer to nodes that have neither incoming nor
outgoing edges and are not marked as top nodes as singletons;
these nodes are ignored in subsequent statistics, e.g. when
determining the proportion of edges per node (3) or the per-
centages of rooted trees (4) and fragmented graphs (6). The
notation ?%
n
? denotes (non-singleton) node percentages, and
?%
g
? percentages over all graphs. We consider a root node any
(non-singleton) node that has no incoming edges; reentrant
nodes have at least two incoming edges. Following Sagae and
Tsujii (2008), we consider a graph projective when there are
no crossing edges (in a left-to-right rendering of nodes) and no
roots are ?covered?, i.e. for any root j there is no edge i? k
66
Directed Undirected
DM PAS PCEDT DM PAS PCEDT
DM ? .6425 .2612 ? .6719 .5675
PAS .6688 ? .2963 .6993 ? .5490
PCEDT .2636 .2963 ? .5743 .5630 ?
Table 3: Pairwise F
1
similarities, including punctu-
ation (upper right diagonals) or not (lower left).
drawn in dependency labels (1), there are clear dif-
ferences between the representations, with PCEDT
appearing linguistically most fine-grained, and PAS
showing the smallest label inventory. Unattached
singleton nodes (2) in our setup correspond to
tokens analyzed as semantically vacuous, which
(as seen in Figure 1) include most punctuation
marks in PCEDT and DM, but not PAS. Further-
more, PCEDT (unlike the other two) analyzes some
high-frequency determiners as semantically vacu-
ous. Conversely, PAS on average has more edges
per (non-singleton) nodes than the other two (3),
which likely reflects its approach to the analysis of
functional words (see below).
Judging from both the percentage of actual trees
(4), the proportions of projective graphs (5), and the
proportions of reentrant nodes (7), PCEDT is much
more ?tree-oriented? than the other two, which at
least in part reflects its approach to the analysis
of modifiers and determiners (again, see below).
We view the small percentages of graphs without
at least one top node (8) and of graphs with at
least two non-singleton components that are not
interconnected (6) as tentative indicators of general
well-formedness. Intuitively, there should always
be a ?top? predicate, and the whole graph should
?hang together?. Only DM exhibits non-trivial (if
small) degrees of topless and fragmented graphs,
and these may indicate imperfections in the Deep-
Bank annotations or room for improvement in the
conversion from full MRSs to bi-lexical dependen-
cies, but possibly also exceptions to our intuitions
about semantic dependency graphs.
Finally, in Table 3 we seek to quantify pairwise
structural similarity between the three representa-
tions in terms of unlabeled dependency F
1
(dubbed
UF in Section 5 below). We provide four variants
of this metric, (a) taking into account the direc-
tionality of edges or not and (b) including edges
involving punctuation marks or not. On this view,
DM and PAS are structurally much closer to each
other than either of the two is to PCEDT, even more
such that i < j < k.
so when discarding punctuation. While relaxing
the comparison to ignore edge directionality also
increases similarity scores for this pair, the effect
is much more pronounced when comparing either
to PCEDT. This suggests that directionality of se-
mantic dependencies is a major source of diversion
between DM and PAS on the one hand, and PCEDT
on the other hand.
Linguistic Comparison Among other aspects,
Ivanova et al. (2012) categorize a range of syntac-
tic and semantic dependency annotation schemes
according to the role that functional elements take.
In Figure 1 and the discussion of Table 2 above, we
already observed that PAS differs from the other
representations in integrating into the graph aux-
iliaries, the infinitival marker, the case-marking
preposition introducing the argument of apply (to),
and most punctuation marks;
6
while these (and
other functional elements, e.g. complementizers)
are analyzed as semantically vacuous in DM and
PCEDT, they function as predicates in PAS, though
do not always serve as ?local? top nodes (i.e. the se-
mantic head of the corresponding sub-graph): For
example, the infinitival marker in Figure 1 takes the
verb as its argument, but the ?upstairs? predicate
impossible links directly to the verb, rather than to
the infinitival marker as an intermediate.
At the same time, DM and PAS pattern alike
in their approach to modifiers, e.g. attributive ad-
jectives, adverbs, and prepositional phrases. Un-
like in PCEDT (or common syntactic dependency
schemes), these are analyzed as semantic predi-
cates and, thus, contribute to higher degrees of
node reentrancy and non-top (structural) roots.
Roughly the same holds for determiners, but here
our PCEDT projection of Prague tectogrammatical
trees onto bi-lexical dependencies leaves ?vanilla?
articles (like a and the) as singleton nodes.
The analysis of coordination is distinct in the
three representations, as also evident in Figure 1.
By design, DM opts for what is often called
the Mel?
?
cukian analysis of coordinate structures
(Mel?
?
cuk, 1988), with a chain of dependencies
rooted at the first conjunct (which is thus consid-
ered the head, ?standing in? for the structure at
large); in the DM approach, coordinating conjunc-
tions are not integrated with the graph but rather
contribute different types of dependencies. In PAS,
the final coordinating conjunction is the head of the
6
In all formats, punctuation marks like dashes, colons, and
sometimes commas can be contentful, i.e. at times occur as
both predicates, arguments, and top nodes.
67
employee stock investment plans
compound compound compound
employee stock investment plans
ARG1
ARG1
ARG1
employee stock investment plans
ACT
PAT REG
Figure 2: Analysis of nominal compounding in DM, PAS, and PCEDT, respectively .
structure and each coordinating conjunction (or in-
tervening punctuation mark that acts like one) is a
two-place predicate, taking left and right conjuncts
as its arguments. Conversely, in PCEDT the last
coordinating conjunction takes all conjuncts as its
arguments (in case there is no overt conjunction, a
punctuation mark is used instead); additional con-
junctions or punctuation marks are not connected
to the graph.
7
A linguistic difference between our representa-
tions that highlights variable granularities of anal-
ysis and, relatedly, diverging views on the scope
of the problem can be observed in Figure 2. Much
noun phrase?internal structure is not made explicit
in the PTB, and the Enju Treebank from which
our PAS representation derives predates the brack-
eting work of Vadas and Curran (2007). In the
four-way nominal compounding example of Fig-
ure 2, thus, PAS arrives at a strictly left-branching
tree, and there is no attempt at interpreting seman-
tic roles among the members of the compound ei-
ther; PCEDT, on the other hand, annotates both the
actual compound-internal bracketing and the as-
signment of roles, e.g. making stock the PAT(ient)
of investment. In this spirit, the PCEDT annota-
tions could be directly paraphrased along the lines
of plans by employees for investment in stocks. In
a middle position between the other two, DM dis-
ambiguates the bracketing but, by design, merely
assigns an underspecified, construction-specific de-
pendency type; its compound dependency, then,
is to be interpreted as the most general type of de-
pendency that can hold between the elements of
this construction (i.e. to a first approximation either
an argument role or a relation parallel to a prepo-
sition, as in the above paraphrase). The DM and
PCEDT annotations of this specific example hap-
pen to diverge in their bracketing decisions, where
the DM analysis corresponds to [...] investments
in stock for employees, i.e. grouping the concept
7
As detailed by Miyao et al. (2014), individual con-
juncts can be (and usually are) arguments of other predicates,
whereas the topmost conjunction only has incoming edges in
nested coordinate structures. Similarly, a ?shared? modifier of
the coordinate structure as a whole would take as its argument
the local top node of the coordination in DM or PAS (i.e. the
first conjunct or final conjunction, respectively), whereas it
would depend as an argument on all conjuncts in PCEDT.
employee stock (in contrast to ?common stock?).
Without context and expert knowledge, these de-
cisions are hard to call, and indeed there has been
much previous work seeking to identify and anno-
tate the relations that hold between members of a
nominal compound (see Nakov, 2013, for a recent
overview). To what degree the bracketing and role
disambiguation in this example are determined by
the linguistic signal (rather than by context and
world knowledge, say) can be debated, and thus the
observed differences among our representations in
this example relate to the classic contrast between
?sentence? (or ?conventional?) meaning, on the one
hand, and ?speaker? (or ?occasion?) meaning, on
the other hand (Quine, 1960; Grice, 1968). In
turn, we acknowledge different plausible points of
view about which level of semantic representation
should be the target representation for data-driven
parsing (i.e. structural analysis guided by the gram-
matical system), and which refinements like the
above could be construed as part of a subsequent
task of interpretation.
5 Task Setup
Training data for the task, providing all columns in
the file format sketched in Section 3 above, together
with a first version of the SDP toolkit?including
graph input, basic statistics, and scoring?were
released to candidate participants in early Decem-
ber 2013. In mid-January, a minor update to the
training data and optional syntactic ?companion?
analyses (see below) were provided, and in early
February the description and evaluation of a sim-
ple baseline system (using tree approximations and
the parser of Bohnet, 2010). Towards the end of
March, an input-only version of the test data was
released, with just columns (1) to (4) pre-filled; par-
ticipants then had one week to run their systems on
these inputs, fill in columns (5), (6), and upwards,
and submit their results (from up to two different
runs) for scoring. Upon completion of the testing
phase, we have shared the gold-standard test data,
official scores, and system results for all submis-
sions with participants and are currently preparing
all data for general release through the Linguistic
Data Consortium.
68
DM PAS PCEDT
LF LP LR LF LM LP LR LF LM LP LR LF LM
Peking 85.91 90.27 88.54 89.40 26.71 93.44 90.69 92.04 38.13 78.75 73.96 76.28 11.05
Priberam 85.24 88.82 87.35 88.08 22.40 91.95 89.92 90.93 32.64 78.80 74.70 76.70 09.42
Copenhagen-
80.77 84.78 84.04 84.41 20.33 87.69 88.37 88.03 10.16 71.15 68.65 69.88 08.01
Malm?
Potsdam 77.34 79.36 79.34 79.35 07.57 88.15 81.60 84.75 06.53 69.68 66.25 67.92 05.19
Alpage 76.76 79.42 77.24 78.32 09.72 85.65 82.71 84.16 17.95 70.53 65.28 67.81 06.82
Link?ping 72.20 78.54 78.05 78.29 06.08 76.16 75.55 75.85 01.19 60.66 64.35 62.45 04.01
DM PAS PCEDT
LF LP LR LF LM LP LR LF LM LP LR LF LM
Priberam 86.27 90.23 88.11 89.16 26.85 92.56 90.97 91.76 37.83 80.14 75.79 77.90 10.68
CMU 82.42 84.46 83.48 83.97 08.75 90.78 88.51 89.63 26.04 76.81 70.72 73.64 07.12
Turku 80.49 80.94 82.14 81.53 08.23 87.33 87.76 87.54 17.21 72.42 72.37 72.40 06.82
Potsdam 78.60 81.32 80.91 81.11 09.05 89.41 82.61 85.88 07.49 70.35 67.33 68.80 05.42
Alpage 78.54 83.46 79.55 81.46 10.76 87.23 82.82 84.97 15.43 70.98 67.51 69.20 06.60
In-House 75.89 92.58 92.34 92.46 48.07 92.09 92.02 92.06 43.84 40.89 45.67 43.15 00.30
Table 4: Results of the closed (top) and open tracks (bottom). For each system, the second column (LF)
indicates the averaged LF score across all target representations), which was used to rank the systems.
Evaluation Systems participating in the task
were evaluated based on the accuracy with which
they can produce semantic dependency graphs for
previously unseen text, measured relative to the
gold-standard testing data. The key measures for
this evaluation were labeled and unlabeled preci-
sion and recall with respect to predicted dependen-
cies (predicate?role?argument triples) and labeled
and unlabeled exact match with respect to complete
graphs. In both contexts, identification of the top
node(s) of a graph was considered as the identifi-
cation of additional, ?virtual? dependencies from
an artificial root node (at position 0). Below we
abbreviate these metrics as (a) labeled precision,
recall, and F
1
: LP, LR, LF; (b) unlabeled precision,
recall, and F
1
: UP, UR, UF; and (c) labeled and
unlabeled exact match: LM, UM.
The ?official? ranking of participating systems, in
both the closed and the open tracks, is determined
based on the arithmetic mean of the labeled depen-
dency F
1
scores (i.e. the geometric mean of labeled
precision and labeled recall) on the three target rep-
resentations (DM, PAS, and PCEDT). Thus, to be
considered for the final ranking, a system had to
submit semantic dependencies for all three target
representations.
Closed vs. Open Tracks The task was sub-
divided into a closed track and an open track, where
systems in the closed track could only be trained
on the gold-standard semantic dependencies dis-
tributed for the task. Systems in the open track, on
the other hand, could use additional resources, such
as a syntactic parser, for example?provided that
they make sure to not use any tools or resources
that encompass knowledge of the gold-standard
syntactic or semantic analyses of the SDP 2014
test data, i.e. were directly or indirectly trained or
otherwise derived from WSJ Section 21.
This restriction implies that typical off-the-shelf
syntactic parsers had to be re-trained, as many data-
driven parsers for English include this section of
the PTB in their default training data. To simplify
participation in the open track, the organizers pre-
pared ready-to-use ?companion? syntactic analyses,
sentence- and token-aligned to the SDP data, in
two formats, viz. PTB-style phrase structure trees
obtained from the parser of Petrov et al. (2006) and
Stanford Basic syntactic dependencies (de Marn-
effe et al., 2006) produced by the parser of Bohnet
and Nivre (2012).
6 Submissions and Results
From 36 teams who had registered for the task,
test runs were submitted for nine systems. Each
team submitted one or two test runs per track. In
total, there were ten runs submitted to the closed
track and nine runs to the open track. Three teams
submitted to both the closed and the open track.
The main results are summarized and ranked in
Table 4. The ranking is based on the average LF
score across all three target representations, which
is given in the LF column. In cases where a team
submitted two runs to a track, only the highest-
ranked score is included in the table.
69
Team Track Approach Resources
Link?ping C extension of Eisner?s algorithm for DAGs, edge-factored
structured perceptron
?
Potsdam C & O graph-to-tree transformation, Mate companion
Priberam C & O model with second-order features, decoding with dual decom-
position, MIRA
companion
Turku O cascade of SVM classifiers (dependency recognition, label
classification, top recognition)
companion,
syntactic n-grams,
word2vec
Alpage C & O transition-based parsing for DAGs, logistic regression, struc-
tured perceptron
companion,
Brown clusters
Peking C transition-based parsing for DAGs, graph-to-tree transforma-
tion, parser ensemble
?
CMU O edge classification by logistic regression, edge-factored struc-
tured SVM
companion
Copenhagen-Malm? C graph-to-tree transformation, Mate ?
In-House O existing parsers developed by the organizers grammars
Table 5: Overview of submitted systems, high-level approaches, and additional resources used (if any).
In the closed track, the average LF scores across
target representations range from 85.91 to 72.20.
Comparing the results for different target represen-
tations, the average LF scores across systems are
85.96 for PAS, 82.97 for DM, and 70.17 for PCEDT.
The scores for labeled exact match show a much
larger variation across both target representations
and systems.
8
In the open track, we see very similar trends.
The average LF scores across target representations
range from 86.27 to 75.89 and the corresponding
scores across systems are 88.64 for PAS, 84.95
for DM, and 67.52 for PCEDT. While these scores
are consistently higher than in the closed track,
the differences are small. In fact, for each of the
three teams that submitted to both tracks (Alpage,
Potsdam, and Priberam) improvements due to the
use of additional resources in the open track do not
exceed two points LF.
7 Overview of Approaches
Table 5 shows a summary of the systems that sub-
mitted final results. Most of the systems took
a strategy to use some algorithm to process (re-
stricted types of) graph structures, and apply ma-
chine learning like structured perceptrons. The
methods for processing graph structures are clas-
sified into three types. One is to transform graphs
into trees in the preprocessing stage, and apply con-
ventional dependency parsing systems (e.g. Mate;
Bohnet, 2010) to the converted trees. Some sys-
tems simply output the result of dependency pars-
ing (which means they inherently lose some depen-
8
Please see the task web page at the address indicated
above for full labeled and unlabeled scores.
dencies), while the others apply post-processing
to recover non-tree structures. The second strat-
egy is to use a parsing algorithm that can directly
generate graph structures (in the spirit of Sagae &
Tsujii, 2008; Titov et al., 2009). In many cases
such algorithms generate restricted types of graph
structures, but these restrictions appear feasible for
our target representations. The last approach is
more machine learning?oriented; they apply classi-
fiers or scoring methods (e.g. edge-factored scores),
and find the highest-scoring structures by some de-
coding method.
It is difficult to tell which approach is the best;
actually, the top three systems in the closed and
open tracks selected very different approaches. A
possible conclusion is that exploiting existing sys-
tems or techniques for dependency parsing was
successful; for example, Peking built an ensemble
of existing transition-based and graph-based depen-
dency parsers, and Priberam extended an existing
dependency parser. As we indicated in the task de-
scription, a novel feature of this task is that we have
to compute graph structures, and cannot assume
well-known properties like projectivity and lack of
reentrancies. However, many of the participants
found that our representations are mostly tree-like,
and this fact motivated them to apply methods that
have been well studied in the field of syntactic de-
pendency parsing.
Finally, we observe that three teams participated
in both the closed and open tracks, and all of them
reported that adding external resources improved
accuracy by a little more than one point. Systems
with (only) open submissions extensively use syn-
tactic features (e.g. dependency paths) from exter-
nal resources, and they are shown effective even
70
with simple machine learning models. Pre-existing,
tree-oriented dependency parsers are relatively ef-
fective, especially when combined with graph-to-
tree transformation. Comparing across our three
target representations, system scores show a ten-
dency PAS> DM> PCEDT, which can be taken as
a tentative indicator of relative levels of ?parsabil-
ity?. As suggested in Section 4, this variation most
likely correlates at least in part with diverging de-
sign decisions, e.g. the inclusion of relatively local
and deterministic dependencies involving function
words in PAS, or the decision to annotate contex-
tually determined speaker meaning (rather than
?mere? sentence meaning) in at least some construc-
tions in PCEDT.
8 Conclusions and Outlook
We have described the motivation, design, and out-
comes of the SDP 2014 task on semantic depen-
dency parsing, i.e. retrieving bi-lexical predicate?
argument relations between all content words
within an English sentence. We have converted to
a common format three existing annotations (DM,
PAS, and PCEDT) over the same text and have put
this to use for the first time in training and testing
data-driven semantic dependency parsers. Building
on strong community interest already to date and
our belief that graph-oriented dependency parsing
will further gain importance in the years to come,
we are preparing a similar (slightly modified) task
for SemEval 2015. Candidate modifications and
extensions will include cross-domain testing and
evaluation at the level of ?complete? predications
(in contrast to more lenient per-dependency F
1
used
this year). As optional new sub-tasks, we plan on
offering cross-linguistic variation and predicate (i.e.
semantic frame) disambiguation for at least some of
the target representations. To further probe the role
of syntax in the recovery of semantic dependency
relations, we will make available to participants
a wider selection of syntactic analyses, as well as
add a third (idealized) ?gold? track, where syntactic
dependencies are provided directly from available
syntactic annotations of the underlying treebanks.
Acknowledgements
We are grateful to ?eljko Agi
?
c and Bernd Bohnet
for consultation and assistance in preparing our
baseline and companion parses, to the Linguistic
Data Consortium (LDC) for support in distributing
the SDP data to participants, as well as to Emily M.
Bender and two anonymous reviewers for feedback
on this manuscript. Data preparation was supported
through access to the ABEL high-performance com-
puting facilities at the University of Oslo, and we
acknowledge the Scientific Computing staff at UiO,
the Norwegian Metacenter for Computational Sci-
ence, and the Norwegian tax payers. Part of this
work has been supported by the infrastructural fund-
ing by the Ministry of Education, Youth and Sports
of the Czech Republic (CEP ID LM2010013).
References
Bohnet, B. (2010). Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (p. 89 ? 97). Beijing, China.
Bohnet, B., & Nivre, J. (2012). A transition-based
system for joint part-of-speech tagging and labeled
non-projective dependency parsing. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Conference on
Natural Language Learning (p. 1455 ? 1465). Jeju
Island, Korea.
Buchholz, S., & Marsi, E. (2006). CoNLL-X shared
task on multilingual dependency parsing. In Pro-
ceedings of the 10th Conference on Natural Lan-
guage Learning (p. 149 ? 164). New York, NY,
USA.
Copestake, A., Flickinger, D., Pollard, C., & Sag, I. A.
(2005). Minimal Recursion Semantics. An introduc-
tion. Research on Language and Computation, 3(4),
281 ? 332.
de Marneffe, M.-C., MacCartney, B., & Manning, C. D.
(2006). Generating typed dependency parses from
phrase structure parses. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (p. 449 ? 454). Genoa, Italy.
Fares, M., Oepen, S., & Zhang, Y. (2013). Machine
learning for high-quality tokenization. Replicating
variable tokenization schemes. In Computational lin-
guistics and intelligent text processing (p. 231 ? 244).
Springer.
Flickinger, D. (2000). On building a more efficient
grammar by exploiting types. Natural Language En-
gineering, 6 (1), 15 ? 28.
Flickinger, D., Zhang, Y., & Kordoni, V. (2012). Deep-
Bank. A dynamically annotated treebank of the Wall
Street Journal. In Proceedings of the 11th Interna-
tional Workshop on Treebanks and Linguistic Theo-
ries (p. 85 ? 96). Lisbon, Portugal: Edi??es Colibri.
Gildea, D., & Jurafsky, D. (2002). Automatic labeling
of semantic roles. Computational Linguistics, 28,
71
245 ? 288.
Grice, H. P. (1968). Utterer?s meaning, sentence-
meaning, and word-meaning. Foundations of Lan-
guage, 4(3), 225 ? 242.
Haji?c, J., Ciaramita, M., Johansson, R., Kawahara, D.,
Mart?, M. A., M?rquez, L., . . . Zhang, Y. (2009).
The CoNLL-2009 Shared Task. syntactic and seman-
tic dependencies in multiple languages. In Proceed-
ings of the 13th Conference on Natural Language
Learning (p. 1 ? 18). Boulder, CO, USA.
Haji?c, J., Haji?cov?, E., Panevov?, J., Sgall, P., Bojar,
O., Cinkov?, S., . . . ?abokrtsk?, Z. (2012). An-
nouncing Prague Czech-English Dependency Tree-
bank 2.0. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(p. 3153 ? 3160). Istanbul, Turkey.
Ivanova, A., Oepen, S., ?vrelid, L., & Flickinger, D.
(2012). Who did what to whom? A contrastive study
of syntacto-semantic dependencies. In Proceedings
of the Sixth Linguistic Annotation Workshop (p. 2 ?
11). Jeju, Republic of Korea.
Kate, R. J., & Wong, Y. W. (2010). Semantic pars-
ing. The task, the state of the art and the future. In
Tutorial abstracts of the 20th Meeting of the Associ-
ation for Computational Linguistics (p. 6). Uppsala,
Sweden.
Marcus, M., Santorini, B., & Marcinkiewicz, M. A.
(1993). Building a large annotated corpora of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19, 313 ? 330.
Mel?
?
cuk, I. (1988). Dependency syntax. Theory and
practice. Albany, NY, USA: SUNY Press.
Meyers, A., Reeves, R., Macleod, C., Szekely, R.,
Zielinska, V., Young, B., & Grishman, R. (2004).
Annotating noun argument structure for NomBank.
In Proceedings of the 4th International Conference
on Language Resources and Evaluation (p. 803 ?
806). Lisbon, Portugal.
Miyao, Y., Oepen, S., & Zeman, D. (2014). In-house:
An ensemble of pre-existing off-the-shelf parsers. In
Proceedings of the 8th International Workshop on
Semantic Evaluation. Dublin, Ireland.
Nakov, P. (2013). On the interpretation of noun com-
pounds: Syntax, semantics, and entailment. Natural
Language Engineering, 19(3), 291 ? 330.
Nivre, J., Hall, J., K?bler, S., McDonald, R., Nilsson,
J., Riedel, S., & Yuret, D. (2007). The CoNLL 2007
shared task on dependency parsing. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Conference on
Natural Language Learning (p. 915 ? 932). Prague,
Czech Republic.
Oepen, S., & L?nning, J. T. (2006). Discriminant-
based MRS banking. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (p. 1250 ? 1255). Genoa, Italy.
Palmer, M., Gildea, D., & Kingsbury, P. (2005). The
Proposition Bank. A corpus annotated with semantic
roles. Computational Linguistics, 31(1), 71 ? 106.
Petrov, S., Barrett, L., Thibaux, R., & Klein, D. (2006).
Learning accurate, compact, and interpretable tree
annotation. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th Meeting of the Association for Computational
Linguistics (p. 433 ? 440). Sydney, Australia.
Quine, W. V. O. (1960). Word and object. Cambridge,
MA, USA: MIT press.
Sagae, K., & Tsujii, J. (2008). Shift-reduce depen-
dency DAG parsing. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics (p. 753 ? 760). Manchester, UK.
Titov, I., Henderson, J., Merlo, P., & Musillo, G.
(2009). Online graph planarisation for synchronous
parsing of semantic and syntactic dependencies. In
Proceedings of the 21st International Joint Confer-
ence on Artifical Intelligence (p. 1562 ? 1567).
Vadas, D., & Curran, J. (2007). Adding Noun Phrase
Structure to the Penn Treebank. In Proceedings of
the 45th Meeting of the Association for Computa-
tional Linguistics (p. 240 ? 247). Prague, Czech Re-
public.
72
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 335?340,
Dublin, Ireland, August 23-24, 2014.
In-House: An Ensemble of Pre-Existing Off-the-Shelf Parsers
Yusuke Miyao
?
, Stephan Oepen
??
, and Daniel Zeman
?
?
National Institute of Informatics, Tokyo
?
University of Oslo, Department of Informatics
?
Potsdam University, Department of Linguistics
?
Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics
yusuke@nii.ac.jp, oe@ifi.uio.no, zeman@ufal.mff.cuni.cz
Abstract
This submission to the open track of
Task 8 at SemEval 2014 seeks to connect
the Task to pre-existing, ?in-house? pars-
ing systems for the same types of target
semantic dependency graphs.
1 Background and Motivation
The three target representations for Task 8 at
SemEval 2014, Broad-Coverage Semantic Depen-
dency Parsing (SDP; Oepen et al., 2014), are
rooted in language engineering efforts that have
been under continuous development for at least
the past decade. The gold-standard semantic de-
pendency graphs used for training and testing in
the Task result from largely manual annotation, in
part re-purposing and adapting resources like the
Penn Treebank (PTB; Marcus et al., 1993), Prop-
Bank (Palmer et al., 2005), and others. But the
groups who prepared the SDP target data have also
worked in parallel on automated parsing systems
for these representations.
Thus, for each of the target representations,
there is a pre-existing parser, often developed in
parallel to the creation of the target dependency
graphs, viz. (a) for the DM representation, the
parser of the hand-engineered LinGO English Re-
source Grammar (ERG; Flickinger, 2000); (b) for
PAS, the Enju parsing system (Miyao, 2006), with
its probabilistic HPSG acquired through linguis-
tic projection of the PTB; and (c) for PCEDT,
the scenario for English analysis within the Treex
framework (Popel and ?abokrtsk?, 2010), com-
bining data-driven dependency parsing with hand-
engineered tectogrammatical conversion. At least
This work is licenced under a Creative Commons At-
tribution 4.0 International License; page numbers and the
proceedings footer are added by the organizers. http://
creativecommons.org/licenses/by/4.0/
for DM and PAS, these parsers have been exten-
sively engineered and applied successfully in a
variety of applications, hence represent relevant
points of comparison. Through this ?in-house?
submission (of our ?own? parsers to our ?own?
task), we hope to facilitate the comparison of dif-
ferent approaches submitted to the Task with this
pre-existing line of parser engineering.
2 DM: The English Resource Grammar
Semantic dependency graphs in the DM target rep-
resentation, DELPH-IN MRS-Derived Bi-Lexical
Dependencies, stem from a two-step ?reduc-
tion? (simplification) of the underspecified logical-
form meaning representations output natively by
the ERG parser, which implements the linguis-
tic framework of Head-Driven Phrase Structure
Grammar (HPSG; Pollard and Sag, 1994). Gold-
standard DM training and test data for the Task
were derived from the manually annotated Deep-
Bank Treebank (Flickinger et al., 2012), which
pairs Sections 00?21 of the venerable PTB Wall
Street Journal (WSJ) Corpus with complete ERG-
compatible HPSG syntactico-semantic analyses.
DeepBank as well as the ERG rely on Minimal Re-
cursion Semantics (MRS; Copestake et al., 2005)
for meaning representation, such that the exact
same post-processing steps could be applied to the
parser outputs as were used in originally reducing
the gold-standard MRSs from DeepBank into the
SDP bi-lexical semantic dependency graphs.
Parsing Setup The ERG parsing system is a hy-
brid, combining (a) the hand-built, broad-coverage
ERG with (b) an efficient chart parser for uni-
fication grammars and (c) a conditional proba-
bility distribution over candidate analyses. The
parser most commonly used with the ERG, called
PET (Callmeier, 2002),
1
constructs a complete,
1
The SDP test data was parsed using the 1212 release
of the ERG, using PET and converter versions from what
335
subsumption-based parse forest of partial HPSG
derivations (Oepen and Carroll, 2000), and then
extracts from the forest n-best lists (in globally
correct rank order) of complete analyses according
to a discriminative parse ranking model (Zhang et
al., 2007). For our experiments, we trained the
parse ranker on Sections 00?20 of DeepBank and
otherwise used the default, non-pruning develop-
ment configuration, which is optimized for accu-
racy. In this setup, ERG parsing on average takes
close to ten seconds per sentence.
Post-Parsing Conversion After parsing, MRSs
are reduced to DM bi-lexical semantic dependen-
cies in two steps. First, Oepen and L?nning
(2006) define a conversion to variable-free Ele-
mentary Dependency Structures (EDS), which (a)
maps each predication in the MRS logical-form
meaning representation to a node in a dependency
graph and (b) transforms argument relations rep-
resented by shared logical variables into directed
dependency links between graph nodes. This first
step of the conversion is ?mildly? lossy, in that
some scope-related information is discarded; the
EDS graph, however, will contain the same num-
ber of nodes and the same set of argument de-
pendencies as there are predications and semantic
role assignments in the original MRS. In particu-
lar, the EDS may still reflect non-lexical semantic
predications introduced by grammatical construc-
tions like covert quantifiers, nominalization, com-
pounding, or implicit conjunction.
2
Second, in another conversion step that is not
information-preserving, the EDS graphs are fur-
ther reduced into strictly bi-lexical form, i.e. a set
of directed, binary dependency relations holding
exclusively between lexical units. This conversion
is defined by Ivanova et al. (2012) and seeks to
(a) project some aspects of construction seman-
tics onto word-to-word dependencies (for example
introducing specific dependency types for com-
pounding or implicit conjunction) and (b) relate
the linguistically informed ERG-internal tokeniza-
tion to the conventions of the PTB.
3
Seeing as both
is called the LOGON SVN trunk as of January 2014; see
http://moin.delph-in.net/LogonTop for detail.
2
Conversely, semantically vacuous parts of the original
input (e.g. infinitival particles, complementizers, relative pro-
nouns, argument-marking prepositions, auxiliaries, and most
punctuation marks) were not represented in the MRS in the
first place, hence have no bearing on the conversion.
3
Adaptations of tokenization encompass splitting ?multi-
word? ERG tokens (like such as or ad hoc), as well as ?hiding?
ERG token boundaries at hyphens or slashes (e.g. 77-year-
conversion steps are by design lossy, DM seman-
tic dependency graphs present a true subset of the
information encoded in the full, original MRS.
3 PAS: The Enju Parsing System
Enju Predicate?Argument Structures (PAS) are
derived from the automatic HPSG-style annota-
tion of the PTB, which was primarily used for the
development of the Enju parsing system
4
(Miyao,
2006). A notable feature of this parser is that the
grammar is not developed by hand; instead, the
Enju HPSG-style treebank is first developed, and
the grammar (or, more precisely, the vast major-
ity of lexical entries) is automatically extracted
from the treebank (Miyao et al., 2004). In this
?projection? step, PTB annotations such as empty
categories and coindexation are used for deriv-
ing the semantic representations that correspond
to HPSG derivations. Its probabilistic model for
disambiguation is also trained using this treebank
(Miyao and Tsujii, 2008).
5
The PAS data set is an extraction of predicate?
argument structures from the Enju HPSG tree-
bank. The Enju parser outputs results in ?ready-
to-use? formats like phrase structure trees and
predicate?argument structures, as full HPSG anal-
yses are not friendly to users who are not famil-
iar with the HPSG theory. The gold-standard PAS
target data in the Task was developed using this
function; the conversion program from full HPSG
analyses to predicate?argument structures was ap-
plied to the Enju Treebank.
Predicate?argument structures (PAS) represent
word-to-word semantic dependencies, such as se-
mantic subject and object. Each dependency type
is represented with two elements: the type of the
predicate, such as verb and adjective, and the ar-
gument label, such as ARG1 and ARG2.
6
old), which the PTB does not split.
4
See http://kmcs.nii.ac.jp/enju/.
5
Abstractly similar to the ERG, the annotations of the
Enju treebank instantiate the linguistic theory of HPSG.
However, the two resources have been developed indepen-
dently and implementation details are quite different. The
most significant difference is that the Enju HPSG treebank is
developed by linguistic projection of PTB annotations, and
the Enju parser derived from the treebank; conversely, the
ERG was predominantly manually crafted, and it was later
applied in the DeepBank re-annotation of the WSJ Corpus.
6
Full details of the predicate?argument structures in the
Enju HPSG Treebank, are available in two documents linked
from the Enju web site (see above), viz. the Enju Output
Specification Manual and the XML Format Documentation.
336
Parsing Setup Basically we used the publicly
available package of the Enju parser ?as is? (see the
above web site). We did not change default pars-
ing parameters (beam width, etc.) and features.
However, the release version of the Enju parser is
trained with the HPSG treebank corresponding to
the Penn Treebank WSJ Sections 2?21, which in-
cludes the test set of the Task (Section 21). There-
fore, we re-trained the Enju parser using Sections
0?20, and used this re-trained parser in preparing
the PAS semantic dependency graphs in this en-
semble submission.
Post-Parsing Conversion The dependency for-
mat of the Enju parser is almost equivalent to what
is provided as the PAS data set in this shared task.
Therefore, the post-parsing conversion for the PAS
data involves only formatting, viz. (a) format con-
version into the tabular file format of the Task; and
(b) insertion of dummy relations for punctuation
tokens ignored in the output of Enju.
7
4 PCEDT: The Treex Parsing Scenario
The Prague Czech-English Dependency Treebank
(PCEDT; Haji
?
c et al., 2012)
8
is a set of parallel de-
pendency trees over the same WSJ texts from the
Penn Treebank, and their Czech translations. Sim-
ilarly to other treebanks in the Prague family, there
are two layers of syntactic annotation: analytical
(a-trees) and tectogrammatical (t-trees). Unlike
for the other two representations used in the Task,
for PCEDT there is no pre-existing parsing system
designed to deliver the full scale of annotations
of the SDP gold-standard data. The closest avail-
able match is a parsing scenario implemented in
the Treex natural language processing framework.
Parsing Setup Treex
9
(Popel and ?abokrtsk?,
2010) is a modular, open-source framework origi-
nally developed for transfer-based machine trans-
lation. It can accomplish any NLP-related task
by sequentially applying to the same piece of data
various blocks of code. Blocks operate on a com-
mon data structure and are chained in scenarios.
Some early experiments with scenarios for tec-
togrammatical analysis of English were described
by Klime? (2007). It is of interest that they report
7
The Enju parser ignores tokens tagged as ?.?, while
the PAS representation includes them with dummy relations;
thus, missing periods are inserted in post-processing by com-
parison to the original PTB token sequence.
8
See http://ufal.mff.cuni.cz/pcedt2.0/.
9
See http://ufal.mff.cuni.cz/treex/.
U.S. should regulate X more stringently than  Y
CPR
PAT
PRED
ACT
PAT
MANN
CPR
PAT
PRED
ACT
PAT
MANN CPR
Figure 1: PCEDT asserts two copies of the token
regulate (shown here as ?regulate? and ??, under-
lined). Projecting t-nodes onto the original tokens,
required by the SDP data format, means that the
 node will be merged with regulate. The edges
going to and from  will now lead to and from reg-
ulate (see the dotted arcs), which results in a cycle.
To get rid of the cycle, we skip  and connect di-
rectly its children, as shown in the final SDP graph
below the sentence.
an F
1
score of assigning functors (dependency la-
bels in PCEDT terminology) of 70.3%; however,
their results are not directly comparable to ours.
Due to the modular nature of Treex, there are
various conceivable scenarios to get the t-tree of
a sentence. We use the default scenario that con-
sists of 48 blocks: two initial blocks (reading the
input), one final block (writing the output), two
A2N blocks (named entity recognition), twelve
W2A blocks (dependency parsing at the analytical
layer) and 31 A2T and T2T blocks (creating the
t-tree based on the a-tree).
Most blocks are highly specialized in one par-
ticular subtask (e.g. there is a block just to make
sure that quotation marks are attached to the root
of the quoted subtree). A few blocks are respon-
sible for the bulk of the work. The a-tree is con-
structed by a block that contains the MST Parser
(McDonald et al., 2005), trained on the CoNLL
2007 English data (Nivre et al., 2007), i.e. Sec-
tions 2?11 of the PTB, converted to dependencies.
The annotation style of CoNLL 2007 differs from
PCEDT 2.0, and thus the unlabeled attachment
score of the analytical parser is only 66%.
Obviously one could expect better results if we
retrained the MST Parser directly on the PCEDT
a-trees, and on the whole training data. The only
reason why we did not do so was lack of time.
Our results thus really demonstrate what is avail-
able ?off-the-shelf?; on the other hand, the PCEDT
component of our ensemble fails to set any ?upper
bound? of output quality, as it definitely is not bet-
337
John brought and ate ripe apples and pears
ACT
CONJ
CONJ
PRED.m PRED.m
RSTR
PAT.m PAT.m
PAT
TOP TOP
PAT
PAT
ACT
ACT
CONJ.m CONJ.m
RSTR
RSTR
PAT
CONJ.m CONJ.m
Figure 2: Coordination in PCEDT t-tree (above)
and in the corresponding SDP graph (below).
ter informed than the other systems participating
in the Task.
Functor assignment is done heuristically, based
on POS tags and function words. The primary
focus of the scenario was on functors that could
help machine translation, thus it only generated
25 different labels (of the total set of 65 labels in
the SDP gold-standard data)
10
and left about 12%
of all nodes without functors. Precision peaks at
78% for ACT(or) relations, while the most fre-
quent error type (besides labelless dependencies)
is a falsely proposed RSTR(iction) relation. Both
ACT and RSTR are among the most frequent de-
pendency types in PCEDT.
Post-Parsing Conversion Once the t-tree has
been constructed, it is converted to the PCEDT
target representation of the Task, using the same
conversion code that was used to prepare the gold-
standard SDP data.
11
SDP graphs are defined over surface tokens but
the set of nodes of a t-tree need not correspond
one-to-one to the set of tokens. For example, there
are no t-nodes for punctuation and function words
(except in coordination); these tokens are rendered
as semantically vacuous in SDP, i.e. they do not
participate in edges. On the other hand, t-trees can
contain generated nodes, which represent elided
words and do not correspond to any surface to-
10
The system was able to output the following functors (or-
dered in the descending order of their frequency in the sys-
tem output): RSTR, PAT, ACT, CONJ.member, APP, MANN,
LOC, TWHEN, DISJ.member, BEN, RHEM, PREC, ACMP,
MEANS, ADVS.member, CPR, EXT, DIR3, CAUS, COND,
TSIN, REG, DIR2, CNCS, and TTILL.
11
In the SDP context, the target representation derived
from the PCEDT is called by the same name as the origi-
nal treebank; but note that the PCEDT semantic dependency
graphs only encode a subset of the information annotated at
the tectogrammatical layer of the full treebank.
DM PAS PCEDT
LF LM LF LM LF LM
Priberam .8916 .2685 .9176 .3783 .7790 .1068
In-House .9246 .4807 .9206 .4384 .4315 .0030
UF UM UF UM UF UM
Priberam .9032 .2990 .9281 .3924 .8903 .3071
In-House .9349 .5230 .9317 .4429 .6919 .0148
Table 1: End-to-end ?in-house? parsing results.
ken. Most generated nodes are leaves and, thus,
can simply be omitted from the SDP graphs. Other
generated nodes are copies of normal nodes and
they are linked to the same token to which the
source node is mapped. As a result, one token can
appear at several different positions in the tree; if
we project these occurrences into one node, the
graph will contain cycles. We decided to remove
all generated nodes causing cycles. Their chil-
dren are attached to their parents and inherit the
functor of the generated node (Figure 1). The con-
version procedure also removes cycles caused by
more fine-grained tokenization of the t-layer.
Furthermore, t-trees use technical edges to cap-
ture paratactic constructions where the relations
are not ?true? dependencies. The conversion pro-
cedure extracts true dependency relations: Each
conjunct is linked to the parent or to a shared child
of the coordination. In addition, there are also
links from the conjunction to the conjuncts and
they are labeled CONJ.m(ember). These links pre-
serve the paratactic structure (which can even be
nested) and the type of coordination. See Figure 2
for an example.
5 Results and Reflections
Seeing as our ?in-house? parsers are not directly
trained on the semantic dependency graphs pro-
vided for the Task, but rather are built from ad-
ditional linguistic resources, we submitted results
from the parsing pipelines sketched in Sections 2
to 4 above to the open SDP track. Table 1
summarizes parser performance in terms of la-
beled and unlabeled F
1
(LF and UF)
12
and full-
sentence exact match (LM and UM), comparing
to the best-performing submission (dubbed Prib-
eram; Martins and Almeida, 2014) to this track.
Judging by the official SDP evaluation metric, av-
erage labeled F
1
over the three representations,
our ensemble ranked last among six participating
12
Our ensemble members exhibit comparatively small dif-
ferences in recall vs. precision.
338
teams; in terms of unlabeled average F
1
, the ?in-
house? submission achieved the fourth rank.
As explained in the task description (Oepen et
al., 2014), parts of the WSJ Corpus were excluded
from the SDP training and testing data because
of gaps in the DeepBank and Enju treebanks, and
to exclude cyclic dependency graphs, which can
sometimes arise in the DM and PCEDT conver-
sions. For these reasons, one has to allow for the
possibility that the testing data is positively bi-
ased towards our ensemble members.
13
But even
with this caveat, it seems fair to observe that the
ERG and Enju parsers both are very competitive
for the DM and PAS target representations, respec-
tively, specifically so when judged in exact match
scores. A possible explanation for these results
lies in the depth of grammatical information avail-
able to these parsers, where DM or PAS seman-
tic dependency graphs are merely a simpliefied
view on the complete underlying HPSG analyses.
These parsers have performed well in earlier con-
trastive evaluation too (Miyao et al., 2007; Bender
et al., 2011; Ivanova et al., 2013; inter alios).
Results for the Treex English parsing scenario,
on the other hand, show that this ensemble mem-
ber is not fine-tuned for the PCEDT target rep-
resentation; due to the reasons mentioned above,
its performance even falls behind the shared task
baseline. As is evident from the comparison of
labeled vs. unlabeled F
1
scores, (a) the PCEDT
parser is comparatively stronger at recovering se-
mantic dependency structure than at assigning la-
bels, and (b) about the same appears to be the case
for the best-performing Priberam system (on this
target representation).
Acknowledgements
Data preparation and large-scale parsing in the
DM target representation was supported through
access to the ABEL high-performance computing
facilities at the University of Oslo, and we ac-
knowledge the Scientific Computing staff at UiO,
the Norwegian Metacenter for Computational Sci-
ence, and the Norwegian tax payers. This project
has been supported by the infrastructural funding
13
There is no specific evidence that the WSJ sentences ex-
cluded in the Task for technical issues in either of the under-
lying treebanks or conversion procedures would be compara-
tively much easier to parse for other submissions than for the
members of our ?in-house? ensemble, but unlike other sys-
tems these parsers ?had a vote? in the selection of the data,
particularly so for the DM and PAS target representations.
by the Ministry of Education, Youth and Sports of
the Czech Republic (CEP ID LM2010013).
References
Bender, E. M., Flickinger, D., Oepen, S., and
Zhang, Y. (2011). Parser evaluation over local
and non-local deep dependencies in a large cor-
pus. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Pro-
cessing (p. 397 ? 408). Edinburgh, Scotland,
UK.
Callmeier, U. (2002). Preprocessing and encoding
techniques in PET. In S. Oepen, D. Flickinger,
J. Tsujii, and H. Uszkoreit (Eds.), Collabora-
tive language engineering. A case study in effi-
cient grammar-based processing (p. 127 ? 140).
Stanford, CA: CSLI Publications.
Copestake, A., Flickinger, D., Pollard, C., and
Sag, I. A. (2005). Minimal Recursion Seman-
tics. An introduction. Research on Language
and Computation, 3(4), 281 ? 332.
Flickinger, D. (2000). On building a more ef-
ficient grammar by exploiting types. Natural
Language Engineering, 6 (1), 15 ? 28.
Flickinger, D., Zhang, Y., and Kordoni, V. (2012).
DeepBank. A dynamically annotated treebank
of the Wall Street Journal. In Proceedings of the
11th International Workshop on Treebanks and
Linguistic Theories (p. 85 ? 96). Lisbon, Portu-
gal: Edi??es Colibri.
Haji
?
c, J., Haji
?
cov?, E., Panevov?, J., Sgall, P.,
Bojar, O., Cinkov?, S., . . . ?abokrtsk?, Z.
(2012). Announcing Prague Czech-English De-
pendency Treebank 2.0. In Proceedings of the
8th International Conference on Language Re-
sources and Evaluation (p. 3153 ? 3160). Istan-
bul, Turkey.
Ivanova, A., Oepen, S., Dridan, R., Flickinger, D.,
and ?vrelid, L. (2013). On different approaches
to syntactic analysis into bi-lexical dependen-
cies. An empirical comparison of direct, PCFG-
based, and HPSG-based parsers. In Proceedings
of the 13th International Conference on Parsing
Technologies (p. 63 ? 72). Nara, Japan.
Ivanova, A., Oepen, S., ?vrelid, L., and
Flickinger, D. (2012). Who did what to whom?
339
A contrastive study of syntacto-semantic depen-
dencies. In Proceedings of the Sixth Linguistic
Annotation Workshop (p. 2 ? 11). Jeju, Republic
of Korea.
Klime?, V. (2007). Transformation-based tec-
togrammatical dependency analysis of English.
In V. Matou?ek and P. Mautner (Eds.), Text,
speech and dialogue 2007, LNAI 4629 (p. 15 ?
22). Berlin / Heidelberg, Germany: Springer.
Marcus, M., Santorini, B., and Marcinkiewicz,
M. A. (1993). Building a large annotated cor-
pora of English: The Penn Treebank. Computa-
tional Linguistics, 19, 313 ? 330.
Martins, A. F. T., and Almeida, M. S. C. (2014).
Priberam. A turbo semantic parser with second
order features. In Proceedings of the 8th In-
ternational Workshop on Semantic Evaluation.
Dublin, Ireland.
McDonald, R., Pereira, F., Ribarov, K., and Haji
?
c,
J. (2005). Non-projective dependency parsing
using spanning tree algorithms. In Proceedings
of the Human Language Technology Conference
and Conference on Empirical Methods in Nat-
ural Language Processing (p. 523 ? 530). Van-
couver, British Columbia, Canada.
Miyao, Y. (2006). From linguistic theory to
syntactic analysis. Corpus-oriented grammar
development and feature forest model. Doc-
toral Dissertation, University of Tokyo, Tokyo,
Japan.
Miyao, Y., Ninomiya, T., and Tsujii, J. (2004).
Corpus-oriented grammar development for ac-
quiring a Head-Driven Phrase Structure Gram-
mar from the Penn Treebank. In Proceedings of
the 1st International Joint Conference on Natu-
ral Language Processing (p. 684 ? 693).
Miyao, Y., Sagae, K., and Tsujii, J. (2007).
Towards framework-independent evaluation of
deep linguistic parsers. In Proceedings of
the 2007 Workshop on Grammar Engineering
across Frameworks (p. 238 ? 258). Palo Alto,
California.
Miyao, Y., and Tsujii, J. (2008). Feature forest
models for probabilistic HPSG parsing. Com-
putational Linguistics, 34(1), 35 ? 80.
Nivre, J., Hall, J., K?bler, S., McDonald, R., Nils-
son, J., Riedel, S., and Yuret, D. (2007). The
CoNLL 2007 shared task on dependency pars-
ing. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Conference on Natural
Language Learning (p. 915 ? 932). Prague,
Czech Republic.
Oepen, S., and Carroll, J. (2000). Ambiguity
packing in constraint-based parsing. Practical
results. In Proceedings of the 1st Meeting of the
North American Chapter of the Association for
Computational Linguistics (p. 162 ? 169). Seat-
tle, WA, USA.
Oepen, S., Kuhlmann, M., Miyao, Y., Zeman, D.,
Flickinger, D., Haji
?
c, J., . . . Zhang, Y. (2014).
SemEval 2014 Task 8. Broad-coverage seman-
tic dependency parsing. In Proceedings of the
8th International Workshop on Semantic Evalu-
ation. Dublin, Ireland.
Oepen, S., and L?nning, J. T. (2006).
Discriminant-based MRS banking. In Proceed-
ings of the 5th International Conference on
Language Resources and Evaluation (p. 1250 ?
1255). Genoa, Italy.
Palmer, M., Gildea, D., and Kingsbury, P. (2005).
The Proposition Bank. A corpus annotated with
semantic roles. Computational Linguistics,
31(1), 71 ? 106.
Pollard, C., and Sag, I. A. (1994). Head-Driven
Phrase Structure Grammar. Chicago, USA:
The University of Chicago Press.
Popel, M., and ?abokrtsk?, Z. (2010). TectoMT.
Modular NLP framework. Advances in Natural
Language Processing, 293 ? 304.
Zhang, Y., Oepen, S., and Carroll, J. (2007).
Efficiency in unification-based n-best parsing.
In Proceedings of the 10th International Con-
ference on Parsing Technologies (p. 48 ? 59).
Prague, Czech Republic.
340
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 48?55,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Resolving Speculation: MaxEnt Cue Classification and
Dependency-Based Scope Rules?
Erik Velldal? and Lilja ?vrelid?? and Stephan Oepen?
? University of Oslo, Department of Informatics (Norway)
?Universit?t Potsdam, Institut f?r Linguistik (Germany)
erikve@ifi.uio.no and ovrelid@uni-potsdam.de and oe@ifi.uio.no
Abstract
This paper describes a hybrid, two-level
approach for resolving hedge cues, the
problem of the CoNLL-2010 shared task.
First, a maximum entropy classifier is ap-
plied to identify cue words, using both
syntactic- and surface-oriented features.
Second, a set of manually crafted rules,
operating on dependency representations
and the output of the classifier, is applied
to resolve the scope of the hedge cues
within the sentence.
1 Introduction
The CoNLL-2010 shared task1 comprises two
sub-tasks. Task 1 is described as learning to detect
sentences containing uncertainty, while the object
of Task 2 is learning to resolve the in-sentence
scope of hedge cues (Farkas et al, 2010). Parallel-
ing this two-fold task definition, the architecture
of our system naturally decomposes into two main
steps. First, a maximum entropy (MaxEnt) classi-
fier is applied to automatically detect cue words.
For Task 1, a given sentence is labeled as uncer-
tain if it contains a word classified as a cue. For
Task 2, we then go on to determine the scope of
the identified cues using a set of manually crafted
rules operating on dependency representations.
For both Task 1 and Task 2, our system partic-
ipates in the stricter category of ?closed? or ?in-
domain? systems. This means that we do not
use any additional uncertainty-annotated material
beyond the supplied training data, consisting of
14541 sentences from biomedical abstracts and ar-
ticles (see Table 2). In the official ranking of re-
?We are grateful to our colleagues at the University of
Oslo and the University of Potsdam, for many useful discus-
sions, constructive critique, and encouragment. We specifi-
cally thank Woodley Packard for careful proof-reading.
1The CoNLL-2010 shared task website: http://www.
inf.u-szeged.hu/rgai/conll2010st/.
sults, and considering systems in all categories to-
gether (closed/open/cross-domain), our system is
ranked 4 out of 24 for Task 1 and 3 out of 15 for
Task 2, resulting in highest average rank (and F1)
overall. We detail the implementation of the cue
classifier and the syntactic rules in Sections 3 and
4, respectively. Results for the held-out testing are
provided in Section 5. First, however, the next sec-
tion describes the various resources that we used
for pre-processing the CoNLL data sets, to prepare
the input to our hedge analysis systems.
2 Architecture and Set-Up
2.1 Preprocessing
To ease integration of annotations across system
components, we converted the XML training data
to plain-text files, with stand-off annotation linked
to the raw data by virtue of character start and end
positions (dubbed characterization in the follow-
ing). Thus, hedge cues, scope boundaries, tok-
enization, Part-of-Speech (PoS) assignments, etc.
are all represented in a uniform fashion: as po-
tentially overlapping annotations on sub-strings of
the raw input.
The GENIA tagger (Tsuruoka et al, 2005) takes
an important role in our pre-processing set-up.
However, maybe somewhat surprisingly, we found
that its tokenization rules are not always opti-
mally adapted for the BioScope corpus. GENIA
unconditionally introduces token boundaries for
some punctuation marks that can also occur token-
internally. For example, it wrongly splits tokens
like ?3,926.50?, ?methlycobamide:CoM?,
or ?Ca(2+)?. Conversely, GENIA fails to isolate
some kinds of opening single quotes, because the
quoting conventions assumed in BioScope differ
from those used in the GENIA Corpus; furthermore,
it mis-tokenizes LATEX-style n- and m-dashes.
On average, one in five sentences in the CoNLL
training data exhibited GENIA tokenization prob-
48
ID FORM LEMMA POS FEATS HEAD DEPREL XHEAD XDEP
1 The the DT _ 4 NMOD 4 SPECDET
2 unknown unknown JJ degree:attributive 4 NMOD 4 ADJUNCT
3 amino amino JJ degree:attributive 4 NMOD 4 ADJUNCT
4 acid acid NN pers:3|case:nom|num:sg|ntype:common 5 SBJ 3 SUBJ
5 may may MD mood:ind|subcat:MODAL|tense:pres|clauseType:decl|passive:- 0 ROOT 0 ROOT
6 be be VB _ 5 VC 7 PHI
7 used use VBN subcat:V-SUBJ-OBJ|vtype:main|passive:+ 6 VC 5 XCOMP
8 by by IN _ 7 LGS 9 PHI
9 these these DT deixis:proximal 10 NMOD 10 SPECDET
10 species specie NNS num:pl|pers:3|case:obl|common:count|ntype:common 8 PMOD 7 OBL-AG
11 . . . _ 5 P 0 PUNC
Table 1: Enhanced dependency representation of the example sentence The unknown amino acid may
be used by these species with GENIAPoS-tags (POS), Malt parses (HEAD, DEPREL) and XLE parses
(XHEAD, XDEP).
lems. Our pre-processing approach therefore de-
ploys a home-grown, cascaded finite-state tok-
enizer (borrowed and adapted from the open-
source English Resource Grammar; Flickinger
(2000)), which aims to implement the tokeniza-
tion decisions made in the Penn Treebank (Mar-
cus et al, 1993) ? much like GENIA, in principle
? but properly treating corner cases like the ones
above. Synchronized via characterization, this to-
kenization is then enriched with the output of no
less than two PoS taggers, as detailed in the next
section.
2.2 PoS Tagging and Lemmatization
For PoS tagging and lemmatization, we combine
GENIA (with its built-in, occasionally deviant to-
kenizer) and TnT (Brants, 2000), which operates
on pre-tokenized inputs but in its default model
is trained on financial news from the Penn Tree-
bank. Our general goal here is to take advantage
of the higher PoS accuracy provided by GENIA in
the biomedical domain, while using our improved
tokenization and producing inputs to the parsing
stage (see Section 2.3 below) that as much as pos-
sible resemble the conventions used in the original
training data for the parser ? the Penn Treebank,
once again.
To this effect, for the vast majority of tokens we
can align the GENIA tokenization with our own,
and in these cases we typically use GENIA PoS
tags and lemmas (i.e. base forms). For better nor-
malization, we downcase base forms for all parts
of speech except proper nouns. However, GENIA
does not make a PoS distinction between proper
and common nouns, as in the Penn Treebank, and
hence we give precedence to TnT outputs for to-
kens tagged as nominal by both taggers. Finally,
for the small number of cases where we cannot es-
tablish a one-to-one alignment from an element in
our own tokenization to a GENIA token, we rely on
TnT annotation only. In the merging of annotations
across components, and also in downstream pro-
cessing we have found it most convenient to op-
erate predominantly in terms of characterization,
i.e. sub-strings of the raw input that need not align
perfectly with token boundaries.
2.3 Dependency Parsing with LFG Features
For syntactic parsing we employ a data-driven de-
pendency parser which incorporates the predic-
tions from a large-scale LFG grammar. A tech-
nique of parser stacking is employed, which en-
ables a data-driven parser to learn from the out-
put of another parser, in addition to gold stan-
dard treebank annotations (Nivre and McDonald,
2008). This technique has been shown to pro-
vide significant improvements in accuracy for both
English and German (?vrelid et al, 2009), and
a similar approach employing an HPSG grammar
has been shown to increase domain independence
in data-driven dependency parsing (Zhang and
Wang, 2009). For our purposes, we decide to use a
parser which incorporates analyses from two quite
different parsing approaches ? data-driven depen-
dency parsing and ?deep? parsing with a hand-
crafted grammar ? providing us with a range of
different types of linguistic features which may be
used in hedge detection.
We employ the freely available MaltParser
(Nivre et al, 2006), which is a language-
independent system for data-driven dependency
parsing.2 It is based on a deterministic pars-
ing strategy in combination with treebank-induced
classifiers for predicting parse transitions. It sup-
ports a rich feature representation of the parse his-
tory in order to guide parsing and may easily be
extended to take into account new features of the
2See http://maltparser.org.
49
Sentences Hedged Cues Multi-Word Tokens Cue Tokens
Sentences Cues
Abstracts 11871 2101 2659 364 309634 3056
Articles 2670 519 668 84 68579 782
Total 14541 2620 3327 448 378213 3838
Table 2: Some descriptive figures for the shared task training data. Token-level counts are based on the
tokenization described in Section 2.1.
parse history.
Parser stacking The procedure to enable the
data-driven parser to learn from the grammar-
driven parser is quite simple. We parse a treebank
with the XLE platform (Crouch et al, 2008) and the
English grammar developed within the ParGram
project (Butt et al, 2002). We then convert the
LFG output to dependency structures, so that we
have two parallel versions of the treebank ? one
gold standard and one with LFG-annotation. We
extend the gold standard treebank with additional
information from the corresponding LFG analysis
and train the data-driven dependency parser on the
enhanced data set. See ?vrelid et al (2010) for
details of the conversion and training of the parser.
Table 1 shows the enhanced dependency rep-
resentation of the English sentence The unknown
amino acid may be used by these species, taken
from the training data. For each token, the parsed
data contains information on the surface form,
lemma, and PoS tag, as well as on the head and de-
pendency relation in columns 6 and 7. The depen-
dency analysis suggested by XLE is contained in
columns 8 and 9, whereas additional XLE informa-
tion, such as morphosyntactic properties like num-
ber and voice, as well as more semantic properties
detailing, e.g., subcategorization frames, seman-
tic conceptual categories such as human, time and
location, etc., resides in the FEATS column. The
parser outputs, which in turn form the basis for our
scope resolution rules discussed in Section 4, also
take this same form.
The parser employed in this work is trained
on the Wall Street Journal sections 2 ? 24 of the
Penn Treebank, converted to dependency format
(Johansson and Nugues, 2007) and extended with
XLE features, as described above. Parsing is per-
formed using the arc-eager mode of MaltParser
(Nivre, 2003) and an SVM with a polynomial ker-
nel. When tested using 10-fold cross-validation on
this data set, the parser achieves a labeled accuracy
score of 89.8 (?vrelid et al, 2010).
3 Identifying Hedge Cues
For the task of identifying hedge cues, we devel-
oped a binary maximum entropy (MaxEnt) classi-
fier. The identification of cue words is used for (i)
classifying sentences as certain/uncertain (Task 1),
and (ii) providing input to the syntactic rules that
we later apply for resolving the in-sentence scope
of the cues (Task 2). We also report evaluation
scores for the sub-task of cue detection in isola-
tion.
As annotated in the training data, it is possible
for a hedge cue to span multiple tokens, e.g. as in
whether or not. The majority of the multi-word
cues in the training data are very infrequent, how-
ever, most occurring only once, and the classifier
itself is not sensitive to the notion of multi-word
cues. A given word token in the training data is
simply considered to be either a cue or a non-cue,
depending on whether it falls within the span of a
cue annotation. The task of determining whether
a cue word forms part of a larger multi-word cue,
is performed by a separate post-processing step,
further described in Section 3.2.
3.1 Maximum Entropy Classification
In the MaxEnt framework, each training exam-
ple ? in our case a paired word and label ?wi, yi?
? is represented as a feature vector f(wi, yi) =
fi ? <d. Each dimension or feature function fij
can encode arbitrary properties of the data. The
particular feature functions we are using for the
cue identification are described under Section 3.4
below. For model estimation we use the TADM3
software (Malouf, 2002). For feature extraction
and model tuning, we build on the experimen-
tation environment developed by Velldal (2008)
(in turn extending earlier work by Oepen et al
3Toolkit for Advanced Discriminative Modeling; avail-
able from http://tadm.sourceforge.net/.
50
(2004)). Among other things, its highly optimized
feature handling ? where the potentially expen-
sive feature extraction step is performed only once
and then combined with several levels of feature
caching ? make it computationally feasible to per-
form large-scale ?grid searches? over different con-
figurations of features and model parameters when
using many millions of features.
3.2 Multi-Word Cues
After applying the classifier, a separate post-
processing step aims to determine whether tokens
identified as cue words belong to a larger multi-
word cue. For example, when the classifier has
already identified one or more of the tokens in a
phrase such as raises the possibility to be part of a
hedge cue, a heuristic rule (viz. basically lemma-
level pattern-matching, targeted at only the most
frequently occurring multi-word cues in the train-
ing data) makes sure that the tokens are treated as
part of one and the same cue.
3.3 Model Development, Data Sets and
Evaluation Measures
While the training data made available for the
shared task consisted of both abstracts and full
articles from the BioScope corpus (Vincze et al,
2008), the test data were pre-announced to consist
of biomedical articles only. In order to make the
testing situation during development as similar as
possible to what could be expected for the held-out
testing, we only tested on sentences taken from the
articles part of the training data. When developing
the classifiers we performed 10-fold training and
testing over the articles, while always including all
sentences from the abstracts in the training set as
well. Table 2 provides some basic descriptive fig-
ures summarizing the training data.
As can be seen in Table 3, we will be report-
ing precision, recall and F-scores for three dif-
ferent levels of evaluation for the cue classifiers:
the sentence-level, token-level and cue-level. The
sentence-level scores correspond to Task 1 of the
shared task, i.e. correctly identifying sentences as
being certain or uncertain. A sentence is labeled
uncertain if it contains at least one token classi-
fied as a hedge cue. The token-level scores indi-
cate how well the classifiers succeed in identify-
ing individual cue words (this score does not take
into account the heuristic post-processing rules for
finding multi-word cues). Finally, the cue-level
scores are based on the exact-match counts for full
 70
 75
 80
 85
 90
 10  20  30  40  50  60  70  80  90  100
Token level F1Sentence level F1
Figure 1: Learning curves showing, for both
token- and sentence-level F-scores, the effect of
incrementally including a larger percentage of
training data into the 10-fold cycles. (As described
also for the other development results, while we
are training on both the articles and the abstracts,
we are testing only on the articles.)
hedge cues (possibly spanning multiple tokens).
These latter scores are computed using the official
shared task scorer script.
3.4 Feature Types
We trained cue classifiers using a wide vari-
ety of feature types, both syntactic and surface-
oriented. However, to better assess the contri-
bution of the different features, we first trained
two baseline models using only features defined
for non-normalized surface forms as they occur in
the training data. The most basic baseline model
(Baseline 1) included only unigram features. The
behavior of this classifier is similar to what we
would expect from simply compiling a list of cue
words from the training data, based on the major-
ity usage of each word as cue or non-cue. Base-
line 2 additionally included 2 words to the left and
3 to the right of the focus word (after first perform-
ing a search for the optimal spans of n-grams up
to 5). As shown in Table 3, this model achieved
a sentence-level F1 of 87.14 and a token-level F1
of 81.97. The corresponding scores for Baseline 1
are 79.20 and 69.59.
A general goal in our approach to hedge analy-
sis is to evaluate the contribution of syntactic in-
formation, both in cue detection and scope resolu-
tion. After applying the parser described in Sec-
tion 2.3, we extracted a range of classifier features
on the basis of the dependency structures (both as
51
proposed by the stacked MaltParser and converted
from XLE) as well as the deep grammar (XLE). Ad-
ditionally we defined various features on the basis
of base forms and PoS information provided by the
GENIA pre-processing (see Section 2.2).
For a quick overview, the feature types we ex-
perimented with include the following:
GENIA features n-gram features over the base
forms and PoS tags from the GENIA information
described in Section 2.2.
Dependency features A range of features ex-
tracted from dependency structures produced by
MaltParser and XLE (see Section 2.3), designed
to capture the syntactic properties and environ-
ment of a token: deprel ? dependency rela-
tion (Malt and XLE), deppath ? dependency
path to root, deppattern ? ordered set of co-
dependents/siblings, including focus token (Malt),
lextriple/postriple ? lexicalized and unlexicalized
dependency triplet for token (Malt), coord ? bi-
nary feature expressing coordination (XLE), co-
ordLevel ? phrase-structural level of coordination
(XLE).
Lexical parser features Other features con-
structed on the basis of the parser output: subcat
? subcategorization frame for verbs (XLE), adv-
Type ? type of adverbial, e.g. sentence, VP (XLE),
adjType ? adjectival function, e.g. attributive vs.
predicative (XLE)
When added to Baseline 2 in isolation, most of
these features resulted in a boost in classifier per-
formance. For the dependency-based features, the
contribution was more pronounced for lexicalized
versions of the features. This also points to the
fact that lexical information seems to be the key
for the task of cue identification, where the model
using only n-grams over surface forms proved a
strong baseline. As more feature types were added
to the classifier together, we also saw a clear trend
of diminishing returns, in that many of the fea-
tures seemed to contribute overlapping informa-
tion. After several rounds of grid-search over dif-
ferent feature configurations, the best-performing
classifier (as used for the shared task) used only
the following feature types: n-grams over surface
forms (including up to 2 tokens to the right), n-
grams over base forms (up to 3 tokens left and
right), PoS of the target word, ?subcat?, ?coord?,
and ?coordLevel?. The ?subcat? feature contains
information taken from XLE regarding the subcat-
egorization requirements of a verb in a specific
context, e.g., whether a verb is modal, takes an
expletive subject etc., whereas the coordination
features signal coordination (?coord?) and detail
the phrase-structural level of coordination (?co-
ordLevel?), e.g., NP, VP, etc. This defines the fea-
ture set used for the model referred to as final in
Table 3.
Recall that for Baseline 2, the F-score is 87.14
for the sentence-level evaluation and 81.97 for the
token-level. For our best and final feature config-
uration, the corresponding F-scores are 89.00 and
83.42, respectively. At both the sentence-level and
the token-level, the differences in classifier per-
formance were found to be statistically significant
at p < 0.005, using a two-tailed sign-test. Af-
ter also applying the heuristic rules for detecting
multi-word cues, the cue-level F-score for our final
model is 84.60, compared to 82.83 for Baseline 2.
3.5 The Effect of Data Size
In order to asses the effect of the size of the train-
ing set, we computed learning curves showing
how classifier performance changes as more train-
ing data is added. Starting with only 10% of the
training data included in the 10-fold cycle, Fig-
ure 1 shows the effect on both token level and
sentence-level F-scores as we incrementally in-
clude larger portions of the available training data.
Unsurprisingly, we see that the performance of
the classifier is steadily improving up to the point
where 100% of the data is included, and by extrap-
olating from the curves shown in Figure 1 it seems
reasonable to assume that this improvement would
continue if more data were available. We there-
fore tried to further increase the size of the training
set by also using the hedge-annotated clinical re-
ports that form part of the BioScope corpus. This
provided us with an additional 855 hedged sen-
tences. However, the classifiers did not seem able
to benefit from the additional training examples,
and across several feature configurations perfor-
mance was found to be consistently lower (though
not significantly so). The reason is probably that
the type of text is quite different ? the clinical re-
ports have a high ratio of fragments and also shows
other patterns of cue usage, being somewhat more
jargon-based. This seems to underpin the findings
of previous studies that hedge cue learners appear
quite sensitive to text type (Morante and Daele-
52
Sentence Level Token Level Cue Level
Model Prec Rec F1 Prec Rec F1 Prec Rec F1
Baseline 1 79.25 79.45 79.20 77.71 63.41 69.59 77.37 71.70 74.43
Baseline 2 86.83 87.54 87.14 86.86 77.69 81.97 85.34 80.21 82.69
Final 91.39 86.78 89.00 91.20 76.95 83.42 90.18 79.47 84.49
Table 3: Averaged 10-fold cross-validation results on the articles in the official shared task training data,
always including the abstracts in the training portion. The model listed as final includes features such
as n-grams over surface forms and base forms (both left and right), PoS, subcategorization frames, and
phrase-structural coordination level. The feature types are further described in Section 3.4.
PoS Description Source
CC Coordinations scope over their conjuncts M
IN Prepositions scope over their arguments with its descendants M
JJattr Attributive adjectives scope over their nominal head and its descendants M
JJpred Predicative adjectives scope over referential subjects and clausal arguments, if any M, X
MD Modals inherit subj-scope from their lexical verb and scope over their descendants M, X
RB Adverbs scope over their heads with its descendants M
VBpass Passive verbs scope over referential subjects and the verbal descendants M, X
VBrais Raising verbs scope over referential subjects and the verbal descendants M, X
* For multi-word cues, the head determines scope for all elements
* Back off from final punctuation and parentheses
Table 4: Overview of dependency-based scope rules with information source (MaltParser or XLE), orga-
nized by PoS of the cue.
mans, 2009).
4 Resolving Cue Scope
In our approach to scope resolution we rely heav-
ily on syntactic information, taken from the depen-
dency structures proposed by both MaltParser and
XLE, as well as various additional features from
the XLE parses relating to specific syntactic con-
structions.
4.1 Scope Rules
We construct a small set of heuristic rules which
define the scope for each cue detected in Stage
1. In the construction of these rules, we made use
of the information provided by the guidelines for
scope annotation in the BioScope corpus (Vincze
et al, 2008) as well as manual inspection of the
training data in order to arrive at reasonable scope
hypotheses for various types of cues.
The rules take as input a parsed sentence which
has been tagged with hedge cues and operate over
the dependency structures and additional features
provided by the parser. Default scope is set to
start at the cue word and span to the end of the
sentence (not including final puctuation), and this
scope also provides the baseline for the evaluation
of our rules. Table 4 provides an overview of the
rules employed for scope resolution.
In the case of multi-word cues, such as indicate
that, and either ... or, which share scope, we need
to determine the head of the multi-word unit. We
then set the scope of the whole unit to the scope of
the head token.
As an example, the application of the rules in
Table 4 to the sentence with the parsed output
in Table 1 correctly determine the scope of the
cue may as shown in example (1), using a variety
of syntactic cues regarding part-of-speech, argu-
menthood, voice, etc. First, the scope of the sen-
tence is set to default scope. Then the MD rule is
applied, which checks the properties of the lexical
verb used, located through a chain of verbal de-
pendents from the modal verb. Since it is passive
(passive:+), initial scope is set to include the
cue?s subject (SBJ) argument with all its descen-
dants (The unknown amino acid).
53
Task 1 Task 2 Cue Detection
Prec Rec F1 Prec Rec F1 Prec Rec F1
85.48 84.94 85.21 56.71 54.02 55.33 81.20 76.31 78.68
Table 6: Evaluation results for the official held-out testing.
Scope Prec Rec F1
Default w/gold cues 45.21 45.21 45.21
Rules w/gold cues 72.31 72.31 72.31
Rules w/classified cues 68.56 61.38 64.77
Table 5: Evaluation of the scope resolution rules
on the training articles, using both gold standard
cues and predicted cues. For the row labeled De-
fault, the scope for each cue is always taken to
span rightward to the end of the sentence. In the
rows labeled Rules, the scopes have been resolved
using the dependency-based rules.
(1) (The unknown amino acid <may> be used
by these species).
4.2 Rule Evaluation
Table 5 shows the evaluation of the set of scope
rules on the articles section of the data set, using
gold standard cues.4 This gives us an indication of
the performance of the rules, isolated from errors
in cue detection.
First of all, we may note that the baseline is
a strong one: choosing to extend the scope of a
cue to the end of the sentence provides an F-score
of 45.21. Given gold standard cue information,
the set of scope rules improves on the baseline by
27 percentage points on the articles section of the
data set, giving us an F-score of 72.31. Comparing
to the evaluation using classified cues (the bottom
row of Table 5), we find that the use of automati-
cally assigned cues causes a drop in performance
of 7.5 percentage points, to a result of 64.77.
5 Held-Out Testing
Table 6 presents the final results as obtained on
the held-out test data, which constitute the official
4This evaluation was carried out using the official scorer
script of the CoNLL shared task. When cue information is
kept constant, as in our case, the values for false positives
and false negatives will be identical, hence the precision and
recall values will always be identical as well.
results for our system in the CoNLL-2010 shared
task. The held-out test set comprises biomedical
articles with a total of 5003 sentences (790 of them
hedged).
For Task 1 we obtain an F-score of 85.21. The
corresponding result for the training data, which is
reported as ?Sentence Level? in Table 3, is 89.00.
Although we experience a slight drop in perfor-
mance (3.8 percentage points), the system seems
to generalize quite well to unseen data when it
comes to the detection of sentence-level uncer-
tainty.
For Task 2, the result on the held-out data set is
an F-score of 55.33, with quite balanced values for
precision and recall, 56.7 and 54.0, respectively. If
we compare this to the end-to-end evaluation on
the training data, provided in the bottom row of
Table 5, we find a somewhat larger drop in perfor-
mance (9.5 percentage points), from an F-score of
64.77 to the held-out 55.3. There are several pos-
sible reasons for this drop. First of all, there might
be a certain degree of overfitting of our system to
the training data. The held-out data may contain
hedging constructions that are not covered by our
set of scope rules. Moreover, the performance of
the scope rules is also influenced by the cue de-
tection, which is reported in the final columns of
Table 6. The cue-level performance of our system
on the held-out data set is 78.68, whereas the same
evaluation on the training data is 84.49. We find
that it is the precision, in particular, which suffers
in the application to the held-out data set. A pos-
sible strategy for future work is to optimize both
components of the Task 2 system, the cue detec-
tion and the scope rules, on the entire training set,
instead of just on the articles.
6 Conclusions ? Outlook
We have described a hybrid, two-level approach
for resolving hedging in biomedical text, as sub-
mitted for the stricter track of ?closed? or ?in-
domain? systems in the CoNLL-2010 shared task.
For the task of identifying hedge cues, we train
a MaxEnt classifier, which, for the held-out test
54
data, achieves an F-score of 78.68 on the cue-level
and 85.21 on the sentence-level (Task 1). For the
task of resolving the in-sentence scope of the iden-
tified cues (Task 2), we apply a set of manually
crafted rules operating on dependency representa-
tions, resulting in an end-to-end F-score of 55.33
(based on exact match of both cues and scopes). In
the official shared task ranking of results, and con-
sidering systems in all tracks together, our system
is ranked 4 out of 24 for Task 1 and 3 out of 15 for
Task 2, resulting in the highest average rank over-
all. For future work we aim to further improve the
cue detection, in particular with respect to multi-
word cues, and also continue to refine the scope
rules. Instead of defining the scopal rules only at
the level of dependency structure, one could also
have rules operating on constituent structure ? per-
haps even combining alternative resolution candi-
dates using a statistical ranker.
References
Thorsten Brants. 2000. TnT. A statistical Part-of-
Speech tagger. In Proceedings of the Sixth Con-
ference on Applied Natural Language Processing,
pages 224?231.
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The
Parallel Grammar Project. In Proceedings of COL-
ING Workshop on Grammar Engineering and Eval-
uation, pages 1?7.
Dick Crouch, Mary Dalrymple, Ron Kaplan, Tracy
King, John Maxwell, and Paula Newman. 2008.
XLE documentation. Palo Alto Research Center.
Rich?rd Farkas, Veronika Vincze, Gy?rgy M?ra, J?nos
Csirik, and Gy?rgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010): Shared
Task, pages 1?12.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering, 6 (1):15?28.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
English. In Joakim Nivre, Heiki-Jaan Kaalep,
and Mare Koit, editors, Proceedings of NODALIDA
2007, pages 105?112.
Robert Malouf. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In Pro-
ceedings of the 6th Conference on Natural Language
Learning, pages 49?55.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English. The Penn Treebank. Computa-
tional Linguistics, 19:313?330.
Roser Morante and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts. In
Proceedings of the BioNLP 2009 Workshop, pages
28?36.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proceedings of the 46th Meeting of the
Association for Computational Linguistics, pages
950?958.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
MaltParser: A data-driven parser-generator for de-
pendency parsing. In Proceedings of the Fifth In-
ternational Conference on Language Resources and
Evaluation, pages 2216?2219.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
Eighth International Workshop on Parsing Tech-
nologies, pages 149?160.
Stephan Oepen, Daniel Flickinger, Kristina Toutanova,
and Christopher D. Manning. 2004. LinGO Red-
woods. A rich and dynamic treebank for HPSG.
Journal of Research on Language and Computation,
2(4):575?596.
Lilja ?vrelid, Jonas Kuhn, and Kathrin Spreyer. 2009.
Improving data-driven dependency parsing using
large-scale LFG grammars. In Proceedings of the
47th Meeting of the Association for Computational
Linguistics, pages 37?40.
Lilja ?vrelid, Jonas Kuhn, and Kathrin Spreyer. 2010.
Cross-framework parser stacking for data-driven de-
pendency parsing. TAL 2010 special issue on Ma-
chine Learning for NLP, 50(3).
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun?ichi Tsujii. 2005. Developing a robust Part-
of-Speech tagger for biomedical text. In Advances
in Informatics, pages 382?392. Springer, Berlin,
Germany.
Erik Velldal. 2008. Empirical Realization Ranking.
Ph.D. thesis, University of Oslo, Institute of Infor-
matics, Oslo.
Veronika Vincze, Gy?rgy Szarvas, Rich?rd Farkas,
Gy?rgy M?ra, and J?nos Csirik. 2008. The Bio-
Scope corpus: Annotation for negation, uncertainty
and their scope in biomedical texts. In Proceedings
of the BioNLP 2008 Workshop.
Yi Zhang and Rui Wang. 2009. Cross-domain depen-
dency parsing using a deep linguistic grammar. In
Proceedings of the 47th Meeting of the Association
for Computational Linguistics, Singapore.
55
Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 88?97,
Jeju, Republic of Korea, 10 July 2012. c?2012 Association for Computational Linguistics
Towards an ACL Anthology Corpus with Logical Document Structure
An Overview of the ACL 2012 Contributed Task
Ulrich Sch?fer
DFKI Language Technology Lab
Campus D 3 1
D-66123 Saarbr?cken, Germany
ulrich.schaefer@dfki.de
Jonathon Read, Stephan Oepen
Department of Informatics
Universitetet i Oslo
0316 Oslo, Norway
{jread |oe}@ifi.uio.no
Abstract
The ACL 2012 Contributed Task is a com-
munity effort aiming to provide the full ACL
Anthology as a high-quality corpus with rich
markup, following the TEI P5 guidelines?
a new resource dubbed the ACL Anthology
Corpus (AAC). The goal of the task is three-
fold: (a) to provide a shared resource for ex-
perimentation on scientific text; (b) to serve
as a basis for advanced search over the ACL
Anthology, based on textual content and cita-
tions; and, by combining the aforementioned
goals, (c) to present a showcase of the benefits
of natural language processing to a broader au-
dience. The Contributed Task extends the cur-
rent Anthology Reference Corpus (ARC) both
in size, quality, and by aiming to provide tools
that allow the corpus to be automatically ex-
tended with new content?be they scanned or
born-digital.
1 Introduction?Motivation
The collection of the Association for Computational
Linguistics (ACL) Anthology began in 2002, with
3,100 scanned and born-digital1 PDF papers. Since
then, the ACL Anthology has become the open ac-
cess collection2 of scientific papers in the area of
Computational Linguistics and Language Technol-
ogy. It contains conference and workshop proceed-
ings and the journal Computational Linguistics (for-
merly the American Journal of Computational Lin-
guistics). As of Spring 2012, the ACL Anthol-
1The term born-digital means natively digital, i.e. prepared
electronically using typesetting systems like LATEX, OpenOffice,
and the like?as opposed to digitized (or scanned) documents.
2
http://aclweb.org/anthology
ogy comprises approximately 23,000 papers from 46
years.
Bird et al (2008) started collecting not only the
PDF documents, but also providing the textual con-
tent of the Anthology as a corpus, the ACL Anthol-
ogy Reference Corpus3 (ACL-ARC). This text ver-
sion was generated fully automatically and in differ-
ent formats (see Section 2.2 below), using off-the-
shelf tools and yielding somewhat variable quality.
The main goal was to provide a reference cor-
pus with fixed releases that researchers could use
and refer to for comparison. In addition, the vision
was formulated that manually corrected ground-
truth subsets could be compiled. This is accom-
plished so far for citation links from paper to paper
inside the Anthology for a controlled subset. The
focus thus was laid on bibliographic and bibliomet-
ric research and resulted in the ACL Anthology Net-
work (Radev et al, 2009) as a public, manually cor-
rected citation database.
What is currently missing is an easy-to-process
XML variant that contains high-quality running text
and logical markup from the layout, such as section
headings, captions, footnotes, italics etc. In prin-
ciple this could be derived from LATEX source files,
but unfortunately, these are not available, and fur-
thermore a considerable amount of papers have been
typeset with various other word processing software.
Here is where the ACL 2012 Contributed Task
starts: The idea is to combine OCR and PDFBox-
like born-digital text extraction methods and re-
assign font and logical structure information as part
of a rich XML format. The method would rely on
OCR exclusively only in cases where no born-digital
3
http://acl-arc.comp.nus.edu.sg
88
PDFs are available?in case of the ACL Anthology
mostly papers published before the year 2000. Cur-
rent results and status updates will always be acces-
sible through the following address:




	
http://www.delph-in.net/aac/
We note that manually annotating the ACL An-
thology is not viable. In a feasibility study we took
a set of five eight-page papers. After extracting
the text using PDFBox4 we manually corrected the
output and annotated it with basic document struc-
ture and cross-references; this took 16 person-hours,
which would suggest a rough estimate of some 25
person-years to manually correct and annotate the
current ACL Anthology. Furthermore, the ACL An-
thology grows substantially every year, requiring a
sustained effort.
2 State of Affairs to Date
In the following, we briefly review the current status
of the ACL Anthology and some of its derivatives.
2.1 ACL Anthology
Papers in the current Anthology are in PDF format,
either as scanned bitmaps or digitally typeset with
LATEX or word processing software. Older scanned
papers were often created using type writers, and
sometimes even contained hand-drawn graphics.
2.2 Anthology Reference Corpus (ACL-ARC)
In addition to the PDF documents, the ACL-ARC
also contains (per page and per paper)
? bitmap files (in the PNG file format)
? plain text in ?normal? reading order
? formatted text (in two columns for most of the
papers)
? XML raw layout format containing position in-
formation for each word, grouped in lines, with
font information, but no running text variant.
The latter three have been generated using OCR
software (OmniPage) operating on the bitmap files.
4
http://pdfbox.apache.org
However, OCR methods tend to introduce charac-
ter and layout recognition errors, from both scanned
and born-digital documents.
The born-digital subset of the ACL-ARC (mostly
papers that appeared in 2000 or later) also contains
PDFBox plain text output. However, this is not
available for approximately 4% of the born-digital
PDFs due to unusual font encodings. Note though,
that extracting text from PDFs in normal reading
order is not a trivial task (Berg et al, 2012), and
many errors exist. Furthermore, the plain text is
not dehyphenated, necessitating a language model
or lexicon-based lookup for post-processing.
2.3 ACL Anthology Network
The ACL Anthology Network (Radev et al, 2009)
is based on the ACL-ARC text outputs. It addition-
ally contains manually-corrected citation graphs, au-
thor and affiliation data for most of the Anthology
(papers until 2009).
2.4 Publications with the ACL Anthology as a
Corpus
We did a little survey in the ACL Anthology of pa-
pers reporting on having used the ACL Anthology as
corpus/dataset. The aim here is to get an overview
and distribution of the different NLP research tasks
that have been pursued using the ACL Anthology as
dataset. There are probably other papers outside the
Anthology itself, but these have not been looked at.
The pioneers working with the Anthology as cor-
pus are Ritchie et al (2006a, 2006b). They did work
related to citations which also forms the largest topic
cluster of papers applying or using Anthology data.
Later papers on citation analysis, summarization,
classification, etc. are Qazvinian et al (2010), Abu-
Jbara & Radev (2011), Qazvinian & Radev (2010),
Qazvinian & Radev (2008), Mohammad et al
(2009), Athar (2011), Sch?fer & Kasterka (2010),
and Dong & Sch?fer (2011).
Text summarization research is performed in
Qazvinian & Radev (2011) and Agarwal et al
(2011a, 2011b).
The HOO (?Help our own?) text correction shared
task (Dale & Kilgarriff, 2010; Zesch, 2011; Ro-
zovskaya et al, 2011; Dahlmeier et al, 2011) aims
at developing automated tools and techniques that
89
assist authors, e.g. non-native speakers of English,
in writing (better) scientific publications.
Classification/Clustering related publications are
Muthukrishnan et al (2011) and Mao et al (2010).
Keyword extraction and topic models based on
Anthology data are addressed in Johri et al (2011),
Johri et al (2010), Gupta & Manning (2011), Hall
et al (2008), Tu et al (2010) and Daudaravic?ius
(2012). Reiplinger et al (2012) use the ACL An-
thology to acquire and refine extraction patterns for
the identification of glossary sentences.
In this workshop several authors have used the
ACL Anthology to analyze the history of compu-
tational linguistics. Radev & Abu-Jbara (2012) ex-
amine research trends through the citing sentences
in the ACL Anthology Network. Anderson et al
(2012) use the ACL Anthology to perform a people-
centered analysis of the history of computational
linguistics, tracking authors over topical subfields,
identifying epochs and analyzing the evolution of
subfields. Sim et al (2012) use a citation analysis to
identify the changing factions within the field. Vo-
gel & Jurafsky (2012) use topic models to explore
the research topics of men and women in the ACL
Anthology Network. Gupta & Rosso (2012) look
for evidence of text reuse in the ACL Anthology.
Most of these and related works would benefit
from section (heading) information, and partly the
approaches already used ad hoc solutions to gather
this information from the existing plain text ver-
sions. Rich text markup (e.g. italics, tables) could
also be used for linguistic, multilingual example ex-
traction in the spirit of the ODIN project (Xia &
Lewis, 2008; Xia et al, 2009).
3 Target Text Encoding
To select encoding elements we adopt the TEI P5
Guidelines (TEI Consortium, 2012). The TEI en-
coding scheme was developed with the intention of
being applicable to all types of natural language, and
facilitating the exchange of textual data among re-
searchers across discipline. The guidelines are im-
plemented in XML; we currently use inline markup,
but stand-off annotations have also been applied
(Ban?ski & Przepi?rkowski, 2009).
We use a subset of the TEI P5 Guidelines as
not all elements were deemed necessary. This pro-
cess was made easier through Roma5, an online
tool that assists in the development of TEI valida-
tors. We note that, while we initially use a simpli-
fied version, the schemas are readily extensible. For
instance, Przepi?rkowski (2009) demonstrates how
constituent and dependency information can be en-
coded following the guidelines, in a manner which
is similar to other prominent standards.
A TEI corpus is typically encoded as a sin-
gle XML document, with several text elements,
which in turn contain front (for abstracts), body
and back elements (for acknowledgements and bib-
liographies). Then, sections are encoded using div
elements (with xml:ids), which contain a heading
(head) and are divided into paragraphs (p). We
aim for accountability when translating between for-
mats; for example, the del element records deletions
(such as dehyphenation at line breaks).
An example of a TEI version of an ACL Anthol-
ogy paper is depicted in Figure 1 on the next page.
4 An Overview of the Contributed Task
The goal of the ACL 2012 Contributed Task is to
provide a high-quality version of the textual content
of the ACL Anthology as a corpus. Its rich text
XML markup will contain information on logical
document structure such as section headings, foot-
notes, table and figure captions, bibliographic ref-
erences, italics/emphasized text portions, non-latin
scripts, etc.
The initial source are the PDF documents of the
Anthology, processed with different text extraction
methods and tools that output XML/HTML. The in-
put to the task itself then consists of two XML for-
mats:
? PaperXML from the ACL Anthology Search-
bench6 (Sch?fer et al, 2011) provided
by DFKI Saarbr?cken, of all approximately
22,500 papers currently in the Anthology (ex-
cept ROCLING which are mostly in Chi-
nese). These were obtained by running a com-
mercial OCR program and applying logical
markup postprocessing and conversion to XML
(Sch?fer & Weitz, 2012).
5
http://www.tei-c.org/Roma/
6
http://aclasb.dfki.de
90
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 aclarc.tei.xsd" xml:lang="en">
<teiHeader>
<fileDesc>
<titleStmt>
<title>Task-oriented Evaluation of Syntactic Parsers and Their Representations</title>
<author>
Yusuke Miyao? Rune S?tre? Kenji Sagae? Takuya Matsuzaki? Jun?ichi Tsujii??*
?Department of Computer Science, University of Tokyo, Japan
?School of Computer Science, University of Manchester, UK
National Center for Text Mining, UK
{yusuke,rune.saetre,sagae,matuzaki,tsujii}@is.s.u-tokyo.ac.jp
</author>
</titleStmt>
<publicationStmt>
<publisher>Association for Computational Linguistics</publisher>
<pubPlace> Columbus, Ohio, USA</pubPlace>
<date>June 2008</date>
</publicationStmt>
<sourceDesc> [. . . ] </sourceDesc>
</fileDesc>
<encodingDesc> [. . . ] </encodingDesc>
</teiHeader>
<text>
<front>
<div type="abs">
<head>Abstract</head>
<p> [. . . ] </p>
</div>
</front>
<body>
<div xml:id="SE1">
<head>Introduction</head>
<p>
Parsing technologies have improved considerably in
the past few years, and high-performance syntactic
parsers are no longer limited to PCFG-based frame<del type="lb">-</del>
works (<ref target="#BI6">Charniak, 2000</ref>;
[. . . ]
</p>
</div>
</body>
<back>
<div type="ack">
<head>Acknowledgements</head>
<p> [. . . ] </p>
</div>
<div type="bib">
<head>References</head>
<listBibl>
<bibl xml:id="BI1">
D. M. Bikel. 2004. Intricacies of Collins? parsing model.
<hi rend="italic">Computational Linguistics</hi>, 30(4):479?511.
</bibl>
[. . . ]
</listBibl>
<pb n="54"/>
</div>
</back>
</text>
</TEI>
Figure 1: An example of a TEI-compliant version of an ACL Anthology document P08-1006. Some elements are
truncated ([. . . ]) for brevity.
91
? TEI P5 XML generated by PDFExtract. For pa-
pers from after 1999, an additional high-quality
extraction step took place, applying state-of
the art word boundary and layout recognition
methods directly to the native, logical PDF
structure (Berg et al, 2012). As no charac-
ter recognition errors occur, this will form the
master format for textual content if available.
Because both versions are not perfect, a large, ini-
tial part of the Contributed Task requires automat-
ically adding missing or correcting markup, using
information from OCR where necessary (e.g. for ta-
bles). Hence, for most papers from after 1999 (cur-
rently approx. 70% of the papers), the Contributed
Task can make use of both representations simulta-
neously.
The role of paperXML in the Contributed Task is
to serve as fall-back source (1) for older, scanned
papers (mostly published before the year 2000), for
which born-digital PDF sources are not available,
or (2) for born-digital PDF papers on which the
PDFExtract method failed, or (3) for document parts
where PDFExtract does not output useful markup
such as currently for tables, cf. Section 4.2 below.
A big advantage of PDFExtract is its ability to ex-
tract the full Unicode character range without char-
acter recognition errors, while the OCR-based ex-
traction methods in our setup are basically limited
to Latin1 characters to avoid higher recognition er-
ror rates.
We proposed the following eight areas as possible
subtasks towards our goal.
4.1 Subtask 1: Footnotes
The first task addresses identification of footnotes,
assigning footnote numbers and text, and generating
markup for them in TEI P5 style. For example:
We first determine lexical heads of nonterminal
nodes by using Bikel's implementation of
Collins' head detection algorithm
<note place="foot" n="9">
<hi rend="monospace">http://www.cis.upenn.edu/
~dbikel/software.html</hi>
</note>
(<ref target="#BI1">Bikel, 2004</ref>;
<ref target="#BI11">Collins, 1997</ref>).
Footnotes are handled to some extent in PDFEx-
tract and paperXML, but the results require refine-
ment.
4.2 Subtask 2: Tables
Task 2 identifies figure/table references in running
text and links them to their captions. The latter
will also have to be distinguished from running text.
Furthermore, tables will have to be identified and
transformed into HTML style table markup. This
is currently not generated by PDFExtract, but the
OCR tool used for paperXML generation quite re-
liably recognizes tables and transforms tables into
HTML. Thus, a preliminary solution would be to in-
sert missing table content in PDFExtract output from
the OCR results. In the long run, implementing table
handling in PDFExtract would be desirable.
<ref target="#TA3">Table 3</ref> shows the
time for parsing the entire AImed corpus,...
<figure xml:id="TA3">
<head>Table 3: Parsing time (sec.)</head>
<!-- TEI table content markup here -->
</figure>
4.3 Subtask 3: Bibliographic Markup
The purpose of this task is to identify citations in
text and link them to the bibliographic references
listed at the end of each paper. In TEI markup, bibli-
ographies are contained in listBibl elements. The
contents of listBibl can range from formatted text
to moderately-structured entries (biblStruct) and
fully-structured entries (biblFull). For example:
We follow the PPI extraction method of
<ref target="#BI39">S?tre et al (2007)</ref>,
which is based on SVMs ...
<div type="bib">
<head>References</head>
<listBibl>
<bibl xml:id="BI39">
R. S?tre, K. Sagae, and J. Tsujii. 2007.
Syntactic features for protein-protein
interaction extraction. In
<hi rend="italic">LBM 2007 short papers</hi>.
</bibl>
</listBibl>
</div>
A citation extraction and linking tool that is
known to deliver good results on ACL Anthology
papers (and even comes with CRF models trained
on this corpus) is ParsCit (Councill et al, 2008). In
this volume, Nhat & Bysani (2012) provide an im-
plementation for this task using ParsCit and discuss
possible further improvements.
92
4.4 Subtask 4: De-hyphenation
Both paperXML and PDFExtract output contain soft
hyphenation indicators at places where the original
paper contained a line break with hyphenation. In
paperXML, they are represented by the Unicode soft
hyphen character (in contrast to normal dashes that
also occur). PDFExtract marks hyphenation from
the original text using a special element. How-
ever, both tools make errors: In some cases, the hy-
phens are in fact hard hyphens. The idea of this
task is to combine both sources and possibly ad-
ditional information, as in general the OCR pro-
gram used for paperXML more aggressively pro-
poses de-hyphenation than PDFExtract. Hyphen-
ation in names often persists in paperXML and
therefore remains a problem that will have to be ad-
dressed as well. For example:
In this paper, we present a comparative
eval<del type="lb">-</del>uation of syntactic
parsers and their output
represen<del type="lb">-</del>tations based on
different frameworks:
4.5 Subtask 5: Remove Garbage such as
Leftovers from Figures
In both paperXML and PDFExtract output, text
remains from figures, illustrations and diagrams.
This occurs more frequently in paperXML than in
PDFExtract output because text in bitmap figures
undergoes OCR as well. The goal of this subtask
is to recognize and remove such text.
Bitmaps in born-digital PDFs are embedded ob-
jects for PDFExtract and thus can be detected and
encoded within TEI P5 markup and ignored in the
text extraction process:
<figure xml:id="FI3">
<graphic url="P08-1006/FI3.png" />
<head>
Figure 3: Predicate argument structure
</head>
</figure>
4.6 Subtask 6: Generate TEI P5 Markup for
Scanned Papers from paperXML
Due to the nature of the extraction process, PDFEx-
tract output is not available for older, scanned pa-
pers. These are mostly papers from before 2000, but
also e.g. EACL 2003 papers. On the other hand, pa-
perXML versions exist for almost all papers of the
ACL Anthology, generated from OCR output. They
still need to be transformed to TEI P5, e.g. using
XSLT. The paperXML format and transformation to
TEI P5 is discussed in Sch?fer & Weitz (2012) in
this volume.
4.7 Subtask 7: Add Sentence Splitting Markup
Having a standard for sentence splitting with unique
sentence IDs per paper to which everyone can refer
to later could be important. The aim of this task is to
add sentence segmentation to the target markup. It
should be based on an open source tokenizer such as
JTok, a customizable open source tool7 that was also
used for the ACL Anthology Searchbench semantic
index pre-processing, or the Stanford Tokenizer8.
<p><s>PPI extraction is an NLP task to identify
protein pairs that are mentioned as interacting
in biomedical papers.</s> <s>Because the number
of biomedical papers is growing rapidly, it is
impossible for biomedical researchers to read
all papers relevant to their research; thus,
there is an emerging need for reliable IE
technologies, such as PPI identification.
</s></p>
4.8 Subtask 8: Math Formulae
Many papers in the Computational Linguistics area,
especially those dealing with statistical natural lan-
guage processing, contain mathematical formulae.
Neither paperXML nor PDFExtract currently pro-
vide a means to deal with these.
A math formula recognition is a complex task, in-
serting MathML9 formula markup from an external
tool (formula OCR, e.g. from InftyReader10) could
be a viable solution.
For example, the following could become the tar-
get format of MathML embedded in TEI P5, for
?? > 0 3 f (x) < 1:
<mrow>
<mo> there exists </mo>
<mrow>
<mrow>
<mi> &#916; <!--GREEK SMALL DELTA--></mi>
<mo> &gt; </mo>
<mn> 0 </mn>
7
http://heartofgold.opendfki.de/repos/trunk/
jtok; LPGL license
8
http://nlp.stanford.edu/software/tokenizer.
shtml; GPL V2 license
9
http://www.w3.org/TR/MathML/
10
http://sciaccess.net/en/InftyReader/
93
</mrow>
<mo> such that </mo>
<mrow>
<mrow>
<mi> f </mi>
<mo> &#2061; <!--FUNCTION APPL.--></mo>
<mrow>
<mo> ( </mo>
<mi> x </mi>
<mo> ) </mo>
</mrow>
</mrow>
<mo> &lt; </mo>
<mn> 1 </mn>
</mrow>
</mrow>
</mrow>
An alternative way would be to implement math
formula recognition directly in PDFExtract using
methods known from math OCR, similar to the page
layout recognition approach.
5 Discussion?Outlook
Through the ACL 2012 Contributed Task, we have
taken a (small, some might say) step further towards
the goal of a high-quality, rich-text version of the
ACL Anthology as a corpus?making available both
the original text and logical document structure.
Although many of the subtasks sketched above
did not find volunteers in this round, the Contributed
Task, in our view, is an on-going, long-term com-
munity endeavor. Results to date, if nothing else,
confirm the general suitability of (a) using TEI P5
markup as a shared target representation and (b) ex-
ploiting the complementarity of OCR-based tech-
niques (Sch?fer & Weitz, 2012), on the one hand,
and direct interpretation of born-digital PDF files
(Berg et al, 2012), on the other hand. Combin-
ing these approaches has the potential to solve the
venerable challenges that stem from inhomogeneous
sources in the ACL Anthology?e.g. scanned, older
papers and digital newer papers, generated from a
broad variety of typesetting tools.
However, as of mid-2012 there still is no ready-to-
use, high-quality corpus that could serve as a shared
starting point for the range of Anthology-based NLP
activities sketched in Section 1 above. In fact, we
remain slightly ambivalent about our recommenda-
tions for utilizing the current state of affairs and ex-
pected next steps?as we would like to avoid much
work getting underway with a version of the corpus
that we know is unsatisfactory. Further, obviously,
versioning and well-defined release cycles will be a
prerequisite to making the corpus useful for compa-
rable research, as discussed by Bird et al (2008).
In a nutshell, we see two possible avenues for-
ward. For the ACL 2012 Contributed Task, we col-
lected various views on the corpus data (as well as
some of the source code used in its production) in a
unified SVN repository. Following the open-source,
crowd-sourcing philosophy, one option would be to
make this repository openly available to all inter-
ested parties for future development, possibly aug-
menting it with support infrastructure like, for ex-
ample, a mailing list and shared wiki.
At the same time, our experience from the past
months suggests that it is hard to reach sufficient
momentum and critical mass to make substantial
progress towards our long-term goals, while con-
tributions are limited to loosely organized volun-
teer work. A possibility we believe might overcome
these limitations would be an attempt at formaliz-
ing work in this spirit further, for example through a
funded project (with endorsement and maybe finan-
cial support from organizations like the ACL, ICCL,
AFNLP, ELRA, or LDC).
A potential, but not seriously contemplated ?busi-
ness model? for the ACL Anthology Corpus could be
that only groups providing also improved versions
of the corpus would get access to it. This would
contradict the community spirit and other demands,
viz. that all code should be made publicly available
(as open source) that is used to produce the rich-text
XML for new papers added to the Anthology. To de-
cide on the way forward, we will solicit comments
and expressions of interest during ACL 2012, in-
cluding of course from the R50 workshop audience
and participants in the Contributed Task. Current
results and status updates will always be accessible
through the following address:




	
http://www.delph-in.net/aac/
The ACL publication process for conferences and
workshops already today supports automated collec-
tion of metadata and uniform layout/branding. For
future high-quality collections of papers in the area
of Computational Linguistics, the ACL could think
94
about providing extended macro packages for con-
ferences and journals that generate rich text and doc-
ument structure preserving (TEI P5) XML versions
as a side effect, in addition to PDF generation. Tech-
nically, it should be possible in both LATEX and (for
sure) in word processors such as OpenOffice or MS
Word. It would help reducing errors induced by
the tedious PDF-to-XML extraction this Contributed
Task dealt with.
Finally, we do think that it will well be possible to
apply the Contributed Task ideas and machinery to
scientific publications in other areas, including the
envisaged NLP research and existing NLP applica-
tions for search, terminology extraction, summariza-
tion, citation analysis, and more.
6 Acknowledgments
The authors would like to thank the ACL, the work-
shop organizer Rafael Banchs, the task contributors
for their pioneering work, and the NUS group for
their support. We are indebted to Rebecca Dridan
for helpful feedback on this work.
The work of the first author has been funded
by the German Federal Ministry of Education and
Research, projects TAKE (FKZ 01IW08003) and
Deependance (FKZ 01IW11003). The second and
third authors are supported by the Norwegian Re-
search Council through the VerdIKT programme.
References
Abu-Jbara, A., & Radev, D. (2011). Coherent
citation-based summarization of scientific papers.
In Proceedings of the 49th annual meeting of the
association for computational linguistics: Human
language techologies (pp. 500?509). Portland,
OR.
Agarwal, N., Reddy, R. S., Gvr, K., & Ros?, C. P.
(2011a). Scisumm: A multi-document summa-
rization system for scientific articles. In Proceed-
ings of the ACL-HLT 2011 system demonstrations
(pp. 115?120). Portland, OR.
Agarwal, N., Reddy, R. S., Gvr, K., & Ros?, C. P.
(2011b). Towards multi-document summarization
of scientific articles: Making interesting compar-
isons with SciSumm. In Proceedings of the work-
shop on automatic summarization for different
genres, media, and languages (pp. 8?15). Port-
land, OR.
Anderson, A., McFarland, D., & Jurafsky, D.
(2012). Towards a computational history of the
ACL:1980?2008. In Proceedings of the ACL-
2012 main conference workshop: Rediscovering
50 years of discoveries. Jeju, Republic of Korea.
Athar, A. (2011). Sentiment analysis of citations us-
ing sentence structure-based features. In Proceed-
ings of the ACL 2011 student session (pp. 81?87).
Portland, OR.
Ban?ski, P., & Przepi?rkowski, A. (2009). Stand-off
TEI annotation: the case of the National Corpus
of Polish. In Proceedings of the third linguistic
annotation workshop (pp. 64?67). Suntec, Singa-
pore.
Berg, ?. R., Oepen, S., & Read, J. (2012). To-
wards high-quality text stream extraction from
PDF. Technical background to the ACL 2012
Contributed Task. In Proceedings of the ACL-
2012 main conference workshop on Rediscover-
ing 50 Years of Discoveries. Jeju, Republic of
Korea.
Bird, S., Dale, R., Dorr, B., Gibson, B., Joseph, M.,
Kan, M.-Y., Lee, D., Powley, B., Radev, D., &
Tan, Y. F. (2008). The ACL Anthology Reference
Corpus: A reference dataset for bibliographic re-
search in computational linguistics. In Proceed-
ings of the sixth international conference on lan-
guage resources and evaluation (LREC-08). Mar-
rakech, Morocco.
Councill, I. G., Giles, C. L., & Kan, M.-Y. (2008).
ParsCit: An open-source CRF reference string
parsing package. In Proceedings of LREC-2008
(pp. 661?667). Marrakesh, Morocco.
Dahlmeier, D., Ng, H. T., & Tran, T. P. (2011). NUS
at the HOO 2011 pilot shared task. In Proceedings
of the generation challenges session at the 13th
european workshop on natural language genera-
tion (pp. 257?259). Nancy, France.
Dale, R., & Kilgarriff, A. (2010). Helping Our Own:
Text massaging for computational linguistics as a
new shared task. In Proceedings of the 6th inter-
national natural language generation conference.
Trim, Co. Meath, Ireland.
95
Daudaravic?ius, V. (2012). Applying collocation seg-
mentation to the ACL Anthology Reference Cor-
pus. In Proceedings of the ACL-2012 main con-
ference workshop: Rediscovering 50 years of dis-
coveries. Jeju, Republic of Korea.
Dong, C., & Sch?fer, U. (2011). Ensemble-style
self-training on citation classification. In Pro-
ceedings of 5th international joint conference on
natural language processing (pp. 623?631). Chi-
ang Mai, Thailand.
Gupta, P., & Rosso, P. (2012). Text reuse with
ACL: (upward) trends. In Proceedings of the
ACL-2012 main conference workshop: Rediscov-
ering 50 years of discoveries. Jeju, Republic of
Korea.
Gupta, S., & Manning, C. (2011). Analyzing the
dynamics of research by extracting key aspects of
scientific papers. In Proceedings of 5th interna-
tional joint conference on natural language pro-
cessing (pp. 1?9). Chiang Mai, Thailand.
Hall, D., Jurafsky, D., & Manning, C. D. (2008).
Studying the history of ideas using topic models.
In Proceedings of the 2008 conference on empir-
ical methods in natural language processing (pp.
363?371). Honolulu, Hawaii.
Johri, N., Ramage, D., McFarland, D., & Jurafsky,
D. (2011). A study of academic collaborations
in computational linguistics using a latent mix-
ture of authors model. In Proceedings of the 5th
ACL-HLT workshop on language technology for
cultural heritage, social sciences, and humanities
(pp. 124?132). Portland, OR.
Johri, N., Roth, D., & Tu, Y. (2010). Experts?
retrieval with multiword-enhanced author topic
model. In Proceedings of the NAACL HLT 2010
workshop on semantic search (pp. 10?18). Los
Angeles, California.
Mao, Y., Balasubramanian, K., & Lebanon, G.
(2010). Dimensionality reduction for text using
domain knowledge. In COLING 2010: Posters
(pp. 801?809). Beijing, China.
Mohammad, S., Dorr, B., Egan, M., Hassan, A.,
Muthukrishan, P., Qazvinian, V., Radev, D., & Za-
jic, D. (2009). Using citations to generate surveys
of scientific paradigms. In Proceedings of human
language technologies: The 2009 annual confer-
ence of the north american chapter of the associa-
tion for computational linguistics (pp. 584?592).
Boulder, Colorado.
Muthukrishnan, P., Radev, D., & Mei, Q. (2011). Si-
multaneous similarity learning and feature-weight
learning for document clustering. In Proceedings
of textgraphs-6: Graph-based methods for natu-
ral language processing (pp. 42?50). Portland,
OR.
Nhat, H. D. H., & Bysani, P. (2012). Linking ci-
tations to their bibliographic references. In Pro-
ceedings of the ACL-2012 main conference work-
shop: Rediscovering 50 years of discoveries. Jeju,
Republic of Korea.
Przepi?rkowski, A. (2009). TEI P5 as an XML stan-
dard for treebank encoding. In Proceedings of the
eighth international workshop on treebanks and
linguistic theories (pp. 149?160). Milano, Italy.
Qazvinian, V., & Radev, D. R. (2008). Scientific
paper summarization using citation summary net-
works. In Proceedings of the 22nd international
conference on computational linguistics (COL-
ING 2008) (pp. 689?696). Manchester, UK.
Qazvinian, V., & Radev, D. R. (2010). Identi-
fying non-explicit citing sentences for citation-
based summarization. In Proceedings of the 48th
annual meeting of the association for computa-
tional linguistics (pp. 555?564). Uppsala, Swe-
den.
Qazvinian, V., & Radev, D. R. (2011). Learning
from collective human behavior to introduce di-
versity in lexical choice. In Proceedings of the
49th annual meeting of the association for com-
putational linguistics: Human language techolo-
gies (pp. 1098?1108). Portland, OR.
Qazvinian, V., Radev, D. R., & Ozgur, A. (2010).
Citation summarization through keyphrase ex-
traction. In Proceedings of the 23rd international
conference on computational linguistics (COL-
ING 2010) (pp. 895?903). Beijing, China.
Radev, D., & Abu-Jbara, A. (2012). Rediscovering
ACL discoveries through the lens of ACL Anthol-
ogy Network citing sentences. In Proceedings of
96
the ACL-2012 main conference workshop: Redis-
covering 50 years of discoveries. Jeju, Republic
of Korea.
Radev, D., Muthukrishnan, P., & Qazvinian, V.
(2009). The ACL Anthology Network corpus. In
Proceedings of the 2009 workshop on text and
citation analysis for scholarly digital libraries.
Morristown, NJ, USA.
Radev, D. R., Muthukrishnan, P., & Qazvinian, V.
(2009). The ACL Anthology Network. In Pro-
ceedings of the 2009 workshop on text and cita-
tion analysis for scholarly digital libraries (pp.
54?61). Suntec City, Singapore.
Reiplinger, M., Sch?fer, U., & Wolska, M. (2012).
Extracting glossary sentences from scholarly ar-
ticles: A comparative evaluation of pattern boot-
strapping and deep analysis. In Proceedings of the
ACL-2012 main conference workshop: Rediscov-
ering 50 years of discoveries. Jeju, Republic of
Korea.
Ritchie, A., Teufel, S., & Robertson, S. (2006a).
Creating a test collection for citation-based IR ex-
periments. In Proceedings of the human language
technology conference of the NAACL, main con-
ference (pp. 391?398). New York City.
Ritchie, A., Teufel, S., & Robertson, S. (2006b).
How to find better index terms through cita-
tions. In Proceedings of the workshop on how can
computational linguistics improve information re-
trieval? (pp. 25?32). Sydney, Australia.
Rozovskaya, A., Sammons, M., Gioja, J., & Roth,
D. (2011). University of illinois system in HOO
text correction shared task. In Proceedings of the
generation challenges session at the 13th euro-
pean workshop on natural language generation
(pp. 263?266). Nancy, France.
Sch?fer, U., & Kasterka, U. (2010). Scientific au-
thoring support: A tool to navigate in typed cita-
tion graphs. In Proceedings of the NAACL HLT
2010 workshop on computational linguistics and
writing: Writing processes and authoring aids
(pp. 7?14). Los Angeles, CA.
Sch?fer, U., Kiefer, B., Spurk, C., Steffen, J., &
Wang, R. (2011). The ACL Anthology Search-
bench. In Proceedings of the ACL-HLT 2011 sys-
tem demonstrations (pp. 7?13). Portland, OR.
Sch?fer, U., & Weitz, B. (2012). Combining OCR
outputs for logical document structure markup.
Technical background to the ACL 2012 Con-
tributed Task. In Proceedings of the ACL-2012
main conference workshop on Rediscovering 50
Years of Discoveries. Jeju, Republic of Korea.
Sim, Y., Smith, N. A., & Smith, D. A. (2012).
Discovering factions in the computational linguis-
tics community. In Proceedings of the ACL-
2012 main conference workshop: Rediscovering
50 years of discoveries. Jeju, Republic of Korea.
TEI Consortium. (2012, February). TEI P5: Guide-
lines for electronic text encoding and interchange.
(http://www.tei-c.org/Guidelines/P5)
Tu, Y., Johri, N., Roth, D., & Hockenmaier, J.
(2010). Citation author topic model in expert
search. In COLING 2010: Posters (pp. 1265?
1273). Beijing, China.
Vogel, A., & Jurafsky, D. (2012). He said, she said:
Gender in the ACL anthology. In Proceedings of
the ACL-2012 main conference workshop: Redis-
covering 50 years of discoveries. Jeju, Republic
of Korea.
Xia, F., Lewis, W., & Poon, H. (2009). Language
ID in the context of harvesting language data off
the web. In Proceedings of the 12th conference
of the european chapter of the ACL (EACL 2009)
(pp. 870?878). Athens, Greece.
Xia, F., & Lewis, W. D. (2008). Repurposing the-
oretical linguistic data for tool development and
search. In Proceedings of the third international
joint conference on natural language processing:
Volume-i (pp. 529?536). Hyderabad, India.
Zesch, T. (2011). Helping Our Own 2011: UKP
lab system description. In Proceedings of the
generation challenges session at the 13th euro-
pean workshop on natural language generation
(pp. 260?262). Nancy, France.
97
Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 98?103,
Jeju, Republic of Korea, 10 July 2012. c?2012 Association for Computational Linguistics
Towards High-Quality Text Stream Extraction from PDF
Technical Background to the ACL 2012 Contributed Task
?yvind Raddum Berg, Stephan Oepen, and Jonathon Read
Department of Informatics, Universitetet i Oslo
{oyvinrb |oe |jread}@ifi.uio.no
Abstract
Extracting textual content and document
structure from PDF presents a surprisingly
(depressingly, to some, in fact) difficult chal-
lenge, owing to the purely display-oriented de-
sign of the PDF document standard. While a
variety of lower-level PDF extraction toolk-
its exist, none fully support the recovery of
original text (in reading order) and relevant
structural elements, even for so-called born-
digital PDFs, i.e. those prepared electronically
using typesetting systems like LATEX, OpenOf-
fice, and the like. This short paper summarizes
a new tool for high-quality extraction of text
and structure from PDFs, combining state-of-
the-art PDF parsing, font interpretation, layout
analysis, and TEI-compliant output of text and
logical document markup.?
1 Introduction?Motivation
To view a collection of scholarly articles like the
ACL Anthology as a structured knowledge base sub-
stantially transcends a na?ve notion of a corpus as
a mere collection of running text. Research litera-
ture is the result of careful editing and typesetting
and, thus, is organized around its complex internal
structure. Relevant structural elements can comprise
both geometric (e.g. pages, columns, blocks, or ta-
bles) and logical units (e.g. titles, abstracts, head-
ings, paragraphs, or citations)?where (ideally) ge-
ometric and logical document structure play hand
in hand to a degree that can make it hard to draw
clear dividing lines in some cases (e.g. in itemized
or numbered lists).
To date, the dominant standard for electronic doc-
ument archival is Portable Document Format (PDF),
?We are indebted to Rebecca Dridan, Ulrich Sch?fer, and the
ACL workshop reviewers for helpful feedback on this work.
originally created as a proprietary format by Adobe
Systems Incorporated in the early 1990s and sub-
sequently made an open ISO standard (which was
officially adopted in 2008 and embraced by Adobe
through a public license that grants royalty-free us-
age). PDF is something of a composite standard,
unifying at least three basic technologies:
1. A subset of the PostScript page ?programming?
language, dropping constructs like loops and
branches, but including all graphical operations
to draw layout elements, text, and images.
2. A font embedding system which allows a doc-
ument to ?carry along? a broad variety of fonts
(in various formats), as may be needed to en-
sure display just as the document was designed.
3. A structured storage system, which organizes
various data objects?for example images and
fonts?inside a PDF document.
All data objects in a PDF file are represented in
a visually-oriented way, as a sequence of operators
which?when interpreted by a PDF renderer?will
draw the document on a page canvas. This is a nat-
ural approach considering the design roots of PDF
as a PostScript successor and its original central role
in desktop publishing applications; but the implica-
tions of such visually-centered design are unfortu-
nate for the task of recovering textual content and
logical document structure.
Interpretation of PDF operators will provide one
with all the individual characters, as well as their
formatting and position on the page. However, they
generally do not convey information about higher
level text units such as tokens, lines, or columns?
information about boundaries between such units is
only available implicitly through whitespace, i.e. the
98
mere absence of textual or graphical objects. Fur-
thermore, data fragments comprising content text on
a page may consist of individual characters, parts of
a word, whole lines, or any combination thereof?
as dictated by font properties and kerning require-
ments. Complicating text extraction from PDF fur-
ther, there are no rules governing the order in which
content is encoded in the document. For example, to
produce a page with a two-column layout, the page
could be drawn by first drawing the first lines of the
left and right columns, then the second lines, etc.
Obtaining text in logical reading order, however, ob-
viously requires that the text in the left column be
processed before the one on the right, so a na?ve ap-
proach to text extraction based on the sequencing of
objects in the PDF file might produce undesirable
results.
Since the standard is now open and free for any-
one to use, we are fortunate to have several ma-
ture, open-source libraries to handle low-level pars-
ing and manipulation of objects in PDF documents.
For this project, we build on Apache PDFBox1, for
its maturity, relatively active support, and interface
flexibility. Originally as an MSc project in Com-
puter Science (Berg, 2011), we have developed a pa-
rameterizable toolkit for high-quality text and struc-
ture extraction from born-digital PDFs, which we
dub PDFExtract.2 In this application, we seek to ap-
proximate this structure by using all the visual clues
and information we have available.
The data presented in a PDF file consists of
streams of objects; by placing hardly any signifi-
cance on the order of elements within these streams,
and more on the visual result obtained by (virtu-
ally) ?rendering? PDF operations, the task of text and
structure extraction is shifted slightly?from what
traditionally amounts to stream-processing, and to-
wards a point of view related to computer vision.
This view, in fact, essentially corresponds to the
same problem tackled by OCR software, though
without the need to perform actual character recog-
nition. Some of the key elements of PDFExtract,
thus, build on related OCR techniques and adapt
and extend these to the PDF processing task. The
process of ?understanding? a PDF document in this
1See http://pdfbox.apache.org/ for details.
2See http://github.com/elacin/PDFExtract/.
context is called document layout analysis, a task
which is commonly treated as two sequential sub-
processes. First, a page image is subjected to geo-
metric layout analysis; the result of this first stage
then serves as input for a subsequent step of logi-
cal layout analysis and content extraction. The fol-
lowing sections briefly review core aspects of the
design and implementation of PDFExtract, ranging
from low-level whitespace detection (?2), over ge-
ometric and logical layout analysis (?3 and ?5, re-
spectively), to aspects of font handling (?4).
2 Whitespace Detection
As a prerequisite to all subsequent analysis, seg-
ment boundaries between tokens, lines, columns,
and other blocks of content need to be made ex-
plicit. Such boundaries are predominantly repre-
sented through whitespace, which is not overtly rep-
resented among the data objects in PDF files. The
approach to whitespace detection and page segmen-
tation in PDFExtract is an extension of the frame-
work proposed by Breuel (2002) (originally in the
context of OCR).
The first step here is to find a cover of the back-
ground whitespace of a document in terms of maxi-
mal empty rectangles. This is accomplished in a top-
down procedure, using a whole page as its starting
point, and working in a way abstractly analogous to
quicksort or branch and bound algorithms. Whites-
pace rectangles are identified in order of decreasing
?quality? (as determined by size, shape, position, and
relations to actual page content), which means that
the result will in general be globally optimal?in the
sense that no other (equal-sized) sequence of cover-
ing rectangles would yield a larger total quality sum.
Figure 1 illustrates the main idea of the algorithm,
which starts from a bound (initially the page at large)
and a set of non-empty rectangles, called obstacles.
If the set is empty, it means that the bound is a max-
imal rectangle with respect to other obstacles (sur-
rounding the bound). If, as in Figure 1, there are
obstacles, the bound needs to be further subdivided.
To this end, we choose one obstacle as a pivot, which
ideally is centered somewhere around the middle of
the bound. As no maximal rectangle can contain ob-
stacles, in particular not the pivot, there are four pos-
sibilities for the solution of the maximal whitespace
99
(a) (b) (c) (d)
Figure 1: Schematic example of one iteration of the whitespace covering algorithm. In (a) we see some obstacles (in
blue) contained within a bounding rectangle; in (b) one of them is chosen as as pivot (in red); and (c) and (d) show
how the original bound is divided into four smaller rectangles (in grey) around the pivot.
rectangle problem?one for each side of the pivot.
The areas of these four sub-bounds are computed, a
list of intersecting obstacles is computed for each of
them, and they are processed in turn.
As originally proposed by Breuel (2002), the
basic procedure proved applicable to born-digital
PDFs, though leaving room for improvements both
in terms of the quality of results and run-time perfor-
mance. Some deficiencies that were observed in pro-
cessing documents from the ACL Anthology (and
other samples of scholarly literature) are exempli-
fied in Figure 2, relating to smallish, ?stray? whites-
pace rectangles in the middle of otherwise contigu-
ous segments (top row in Figure 2), challenges re-
lated to relative differences in line spacing (middle),
and spurious vertical boundaries introduced by so-
called rivers, i.e. accidental alignment of horizon-
tal spacing across lines (bottom). Besides adjust-
ments to the rectangle ?quality? function, the prob-
lems were addressed by (a) allowing a small degree
Figure 2: Select challenges to whitespace covering ap-
proach: stray whitespace inbetween groups of text (top);
inter- vs. intra-paragraph spacing (middle); and ?rivers?
leading to spurious vertical boundaries (bottom).
of overlap between whitespace rectangles and obsta-
cles, (b) a strong preference for contiguous areas of
whitespace (thus making the procedure work from
the page borders inwards), (c) variable lower bounds
on the height and width of whitespace rectangles,
computed dynamically from font properties of sur-
rounding text, and (d) a small number of special-
ized heuristic rules, to block unwanted whitespace
rectangles in select configurations. Berg (2011) pro-
vides full details for these adaptations, as well as for
algorithmic optimizations and parameterization that
enable run-time throughputs of tens of pages per cpu
second.
3 Determining Page Layout
The high-level goal in analyzing page layout is to
produce a hierarchical representation of a page in
terms of blocks of homogenous content, thus mak-
ing explicit relevant spatial relationship between
them. In the realm of OCR, this task is often re-
ferred to as geometric layout analysis (see, for ex-
ample, (Cattoni et al, 1998)), whereas the term
(de)boxing has at times been used in the context of
text stream extraction from PDFs. In the following
paragraphs, we will focus on column boundary de-
tection, but PDFExtract essentially applies the same
general techniques to the identification of other rel-
evant inter-segment boundaries.
While whitespace rectangles are essential to col-
umn boundary identification, there is of course no
guarantee for the existence of one rectangle which
were equivalent to a whole column boundary. First,
as a natural consequence of the whitespace detection
procedure, horizontal rectangles can ?interrupt? can-
didate colum boundaries. Second, there may well be
typographic imperfections causing gaps in the iden-
tified whitespace (as exemplified in the top of Fig-
100
Figure 3: Select challenges to column identification: text
elements protruding into the margin (top) and gaps in
whitespace rectangle coverage (often owed to processing
bounds imposed for premium performance).
ure 3), or it can be the case that geometric constraints
or computational limits imposed on the whitespace
cover algorithm result in ?missing? whitespace rect-
angles (in the bottom of Figure 3). Whereas the orig-
inal design of Breuel (2002) makes no provisions for
these cases, PDFExtract adapts a revised, three-step
approach to column detection, viz. (a) extracting an
initial set of candidate boundaries; (b) heuristically
expanding column boundary candidates vertically;
and (c) combining logically equivalent boundaries
and filtering unwarranted ones. Here, both steps (a)
and (b) assume geometric constraints on the aspect
ratio of candidate column boundaries, as well as on
the existence and relative proportions of surround-
ing non-whitespace content. Again, please see Berg
(2011) for further background on these steps.
With column boundaries in place, PDFExtract
proceeds to the identification of blocks of content
(which may correspond to, for example, logical
paragraphs, headings, displayed equations, tables, or
graphical elements). This step, essentially, is real-
ized through a recursive ?flooding? function, form-
ing connected blocks from adjacent, non-whitespace
PDF data objects where there are no intervening
whitespace rectangles. Regions that (by content
or font properties) can be identified as (parts of)
mathematical equations receive special attention at
this stage, allowing limited amounts of horizon-
tally separating whitespace to be ignored for block
formation. In a similar spirit, line segmentation
(i.e. grouping of vertically aligned data objects) is
performed block-wise?sorting content within each
block by Y-coordinates and determining baselines
and inter-line spacing in a single downwards pass.
The final key component in geometric layout
analysis is the recovery of reading order (recalling
that PDFs do not provide reliable sequencing infor-
mation for data objects). PDFExtract adapts one
of the two techniques suggested by Breuel (2003),
viz. topological sorting of lines (which can include
single-line blocks, where no block-internal line seg-
mentation was detected) based on (a) relations of hi-
erarchical nesting and (b) relative geometric posi-
tions. PDFExtract was tested against a set of some
100 diverse PDF documents (from different sources
of scholarly literature, a range of distinct PDF gener-
ators, quite variable layout, and multiple languages),
and its topological content sorting (detailed further
in Berg, 2011) was found to give very satisfactory
results in terms of reading order recovery.
4 Font Handling and Word Segmentation
Many of the steps of geometric layout analysis out-
lined above depend on accurate coordinate informa-
tion for glyphs, which turned out an unforeseen low-
level challenge in our approach of building PDFEx-
tract on top of Apache PDFBox. Figure 4 (on the
left) shows a problematic example of ?raw? glyph
placement information. Several factors contribute to
incorrect glyph positioning, including the sheer va-
riety of font types supported in PDFs, missing in-
formation about non-standard, embedded fonts, and
design limitations and bugs in PDFBox. To work
around common issues, PDFExtract includes a cou-
ple of patches to PDFBox internals as well as spe-
cialized code for different types of font embedding
in PDF to perform boundary box computation, po-
sition offsetting, and and mapping to Unicode code
points. The (much improved though not quite per-
fect) result of these adjustments, when applied to
our running example, is depicted in the middle of
Figure 4.
With the ultimate goal of creating a high-quality
101
(a) (b) (c)
Figure 4: Examples of font-related challenges (before and after correction) and word segmentation.
(structured) text corpus from ACL Anthology doc-
uments, word segmentation naturally is a mission-
critical component of PDFExtract. Seeing that inter-
word whitespace is more often than not omitted
from PDF data objects, word segmentation?much
like other sub-tasks in geometric layout analysis?
operates in terms of display positions. Deter-
mining whether the distance between two adjacent
glyphs represents a word-separating whitespace or
not, might sound simple?but in practice it proved
difficult to devise a generic solution that performs
well across differences in fonts and sizes (and corre-
sponding variation in kerning, i.e. intra-word spac-
ing), deals with both high-quality and poor typog-
raphy, and is somewhat robust to remaining inac-
curacies in glyph positions . PDFExtract arrived at
a novel algorithm that approximates character text
spacing (as could be set by the PDF Tc operator) by
averaging a selection of the smaller character dis-
tances within a line. The resulting average charac-
ter spacing is subsequently used to normalize hori-
zontal distances, i.e. subtract line-specific character
spacing from every distance on that line?to ideally
center character distances around zero, while leav-
ing word distances larger (they will also be relatively
much larger than before in comparison). The iden-
tification of word boundaries itself, accordingly, be-
comes straightforward, comparing normalized dis-
tances to a percentage of the local font size. The
results of this process are shown for our example in
the right of Figure 4.
5 (Preliminary) Logical Layout Analysis
In our view, thorough geometric layout analysis is
an important prequisite of logical layout analysis.
Hence, the emphasis of Berg (2011) was with re-
spect to the geometric analysis. However, what fol-
lows is an overview of the preliminary procedure in
PDFExtract to determine logical document structure
from geometric layout and typographic information.
The process begins by collating a set of text styles
(i.e. unique combinations of font type and size).
Then, various heuristics govern the assignment of
styles to logical roles:
Body text Choose whichever style occurs most fre-
quently (in terms of the number of characters).
Title Choose the header-like block on the first page
that has the largest font size.
Abstract If one of the first pages has a single-line
block with a style which is bigger or bolder
than body text, and contains the word abstract,
it is chosen as an abstract header. All body text
until the next heading is the abstract text.
Footnote Search for blocks on the lower part of
the page that are smaller than body text; check
that they start with a number or other footnote-
indicating symbol.
Sections Identify section header styles by compil-
ing a list of styles that are either larger than or
have some emphasis on the body text style, and
have instances with evidence of section num-
bering (e.g. 1.1, (1a)). Infer the nesting level
of each section header style from its order of
occurrence in the document; a section head-
ing will always appear earlier than a subsection
heading, for instance.
Having identified the different components in the
document, these are used to create a logical hierar-
chical representation following the TEI P5 Guide-
lines (TEI Consortium, 2012) as introduced by
Sch?fer et al (2012). Title, abstract, floaters, and
figures are separated from the main text. The body
of the document is then collated into a tree of section
elements, with headers and body text. Body text is
collected by combining consecutive text blocks that
102
have identical styles, before inferring paragraphs on
the basis of indented initial lines. Dehyphenation is
tackled using a combination of a lexicon and a set of
orthographic rules.
6 Discussion?Outlook
PDFExtract provides a fresh and open-source take
on the problem of high-quality content and struc-
ture extraction from born-digital PDFs. Unlike ex-
isting initiatives (e.g. the basic TextExtraction
class of PDFBox or the pdftotext command line util-
ity from the Poppler library3), PDFExtract discards
sequencing information available in the so-called
PDF text stream, but instead applies and adapts tech-
niques from OCR?notably a whitespace covering
algorithm, column, block, and line detection, recov-
ery of reading order based on line-oriented topolog-
ical sort, and improved word segmentation taking
advantage of specialized PDF font interpretation.
While very comprehensive in terms of its geometric
layout analysis, PDFExtract to date only make avail-
able a limited range of logical layout analysis func-
tionality (and output into TEI-compliant markup),
albeit also in this respect more so than pre-existing
PDF text stream extraction approaches.
For the ACL 2012 Contributed Task on Redis-
covering 50 Years of Discoveries (Sch?fer et al,
2012), PDFExtract outputs for the born-digital sub-
set of the ACL Anthology are a component of the
?starter package? offered to participants, in the hope
that content and structure derived from OCR tech-
niques (Sch?fer & Weitz, 2012) and those extracted
directly from embedded content in the PDFs will
complement each other. As discussed in more detail
by Sch?fer et al (2012), the two approaches have
in part non-overlapping strengths and weaknesses,
such that aligning content elements that correspond
to each other across the two universes could yield a
multi-dimensional, ideally both more complete and
more accurate perspective. PDFExtract is a recent
development and remains subject to refinement and
extension. Beyond a limited quantitative and qual-
itative evaluation review by Berg (2011), the exact
quality levels of text and document structure that it
makes available (as well as relevant factors of varia-
tion, across different types of documents in the ACL
3See http://poppler.freedesktop.org/.
Anthology) remains to be determined empirically.
We make available the full package, accompanied
by some technical documentation (Berg, 2011), as
well as a sample of gold-standard TEI-compliant
target outputs) in the hope that it may serve as the
basis for future work towards the ACL Anthology
Corpus?both at our own sites (i.e. the University
of Oslo and DFKI Saarbr?cken) and collaborating
partners. We would enthusiastically welcome addi-
tional collaborators in this enterprise and will seek
to provide any reasonable assistance required for the
deployment and extension of PDFExtract.
References
Berg, ?. R. (2011). High precision text extraction
from PDF documents. MSc Thesis, University of
Oslo, Department of Informatics, Oslo, Norway.
Breuel, T. (2002). Two geometric algorithms for
layout analysis. In Proceedings of the 5th work-
shop on Document Analysis Systems (pp. 687?
692). Princeton, USA.
Breuel, T. (2003). Layout analysis based on text line
segment hypotheses. In Third international work-
shop on Document Layout Interpretation and its
Applications. Edinburgh, Scotland.
Cattoni, R., Coianiz, T., & Messelodi, S. (1998). Ge-
ometric layout analysis techniques for document
image understanding. A review (ITC-irst Techni-
cal Report TR#9703-09). Trento, Italy.
Sch?fer, U., Read, J., & Oepen, S. (2012). Towards
an ACL Anthology corpus with logical document
structure. An overview of the ACL 2012 con-
tributed task. In Proceedings of the ACL-2012
main conference workshop on Rediscovering 50
Years of Discoveries. Jeju, Republic of Korea.
Sch?fer, U., & Weitz, B. (2012). Combining OCR
outputs for logical document structure markup.
Technical background to the ACL 2012 Con-
tributed Task. In Proceedings of the ACL-2012
main conference workshop on Rediscovering 50
Years of Discoveries. Jeju, Republic of Korea.
TEI Consortium. (2012, February). TEI P5: Guide-
lines for electronic text encoding and interchange.
(http://www.tei-c.org/Guidelines/P5)
103
