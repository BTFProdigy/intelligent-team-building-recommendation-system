Jurilinguistic Engineering in Cantonese Chinese: 
An N-gram-based Speech to Text Transcription System 
B K T'sou, K K Sin, S W K Chan, T B Y Lai, C Lun, K T Ko, G K K Chan, L Y L Cheung 
Language hfformation Sciences Research Centre 
City University of Itong Kong 
Tat Chee Avenue, Kowloon 
Hong Kong SAR, China 
Email: rlbtsou @nxmail.cityu.edu.hk 
Abstract  
A Cantonese Chinese transcription system to 
automatically convert stenograph code to 
Chinese characters ix reported. The major 
challenge in developing such a system is the 
critical homocode problem because of 
homonymy. The statistical N-gram model is 
used to compute the best combination of 
characters. Supplemented with a 0.85 million 
character corpus of donmin-specific training 
data and enhancement measures, the bigram 
and trigrmn implementations achieve 95% 
and 96% accuracy respectively, as compared 
with 78% accuracy in the baseline model. The 
system perforlnance is comparable with other 
adwmced Chinese Speech-to-Text input 
applications under development. The system 
meets an urgent need o1' the .ludiciary ot: post- 
1997 Hong Kong. 
Keyword: Speech to Text, Statistical 
Modelling, Cantonese, Chinese, Language 
Engineering 
1. Introduct ion 
British rule in Hong Kong lnade English the only 
official language in the legal domain for over a 
Century. After the reversion of Hong Kong 
sovereignty to China in 1997, legal bilingualism 
has brought on an urgent need to create a 
Computer-Aided Transcription (CAT) system for 
Cantonese Chinese to produce and maintain the 
massive legally tenable records of court 
proceedings conducted in the local majority 
language (T'sou, 1993, Sin and T'sou, 1994, Lun 
et al, 1995). With the support fl'om the Hong 
Kong Judiciary, we have developed a 
transcription system for converting stenograph 
code to Chinese characters. 
CAT has been widely used for English for 
many years and awlilable R~r Mandarin Chinese, 
but none has existed for Cantonese. Althongh 
Cantonese is a Chinese dialect, Cantonese and 
Mandarin differ considerably in terms of 
phonological struclure, phouotactics, word 
morphology, vocabulary and orthogral)hy. Mutual 
intelligibility between the two dialects is generally 
very low. For example, while Cantonese has lnole 
than 700 distinct syllables, Mandarin has only 
about 400. Cantonese has 6 tone contours and 
Mandarin only 4. As for vocabulary, 16.5% of the 
words in a 1 million character corpus of court 
proceedings in Canlonese cannot be found in a 
corlms consisting of 30 million character 
newspaper texts in Modern Written Chinese 
(T'sou el al, 1997). For orthography, Mainhmd 
China uses the Simplified Chinese character set, 
and Hong Kong uses the Traditional set l~lus 
4,702 special local Cantonese Chinese characters 
(Hong Kong Government, 1999). Such 
differences between Cantonese and Mandarin 
necessitate the Jtnilinguistic Engineering 
undertaking to develop an independent Cantonese 
CAT system for the local language nvironment. 
The major challenge in developing a 
Cantonese CAT system lies in the conversion of 
phonologically-based stenograph code into 
Chinese text. Chinese is a logographic language. 
Each character or logograph represents a syllable. 
While the total inventory of Cantonese syllable 
types is about 720, them am at least 14,000 
Chinese character types. The limited syllabary 
creates many homophones in the language (T'sou, 
1976). In a one million character corlms of court 
proceedings, 565 distinct syllable types were 
found, representing 2,922 distinct character types. 
Of the 565 syllable types, 470 have 2 or morn 
homophonous characters. In the extreme case, zi 
represents 35 homophonous character types. 
1121 
Coverage ofl lomophonous and Non-homophous Characters 
100.00% ?'o o~ 4:~ ~ ~os 4:e 
80.00% 
6 
60.00% 
# 40.00% 
c~ 20.00% 
0.00% 
? No. of High Freq. Characters %~ t~ p 
\[EJ HoInophonous Charactcrs \[\] Non-honaophonous chaliactcrs\] 
Figure I. Covcragc of Homonymous Characters 
These 470 syllables represent 2,810 homophonous 
character types which account for 94.7% of the 
text, as shown in Figure \]. The homocode 
problem nmst be properly resolved to ensure 
successful conversion. 
2. Computer-Aided Transcription (CAT) 
~dJ l  
t 
? I 
co 2F2L . 
Stcnoma~h code I i 
Slzg?2 E 
I Transcription \[ | 
/ ! EIt?illC ; | 
h Trigranl ,, Big,an, i ~ '9 
=, E ) 
! 
Proofreading 
\[ i 
' I 
' Bigram / Trip, ram 
i Statistical Dala 
Chinese Text i l l :~!  
Figure 2. Automatic Transcription Process 
Figure 2 outlines the transcription process in the 
Cantonese CAT system. Following typical 
courtroom CAT systems, our process is divided 
into three major stages. In Stage 1, simultaneous 
to a litigant speaking, a stenographer inputs 
speech, i.e. a sequence of transcribed syllables or 
stenograph codes, via a stenograph code generator. 
Each stenograph code basically stands for a 
syllable. In Stage 2, the transcription software 
converts the sequence of stenograph codes \[Sl . . . . .  
s,,} into the original character text {q . . . . .  c,,}. 
This procedure requires the conversion 
component to be tightly bound to the phonology 
and orthography of a specific language. To 
specifically address homonymy in Cantonese, the 
conversion procedure in our system is supported 
by bigram and trigram statistical data derived 
from domain-specific training. In Stage 3, manual 
editing of the transcribed texts corrects errors 
from typing mistakes or his-transcription. 
3. System Architecture 
3.1 Statistical Formulation 
To resolve massive ambiguity in speech to text 
conversion, the N-gram model is used to 
determine the most probable character sequence 
{q . . . . .  ck} given the input stenograph code 
sequence {s~ . . . . .  Sk}. The conditional probability 
(1) is to be maximized. 
(1) P(q . . . . .  c~l sl . . . . .  sk) 
where {q . . . . .  c~} stands for a sequence of N 
characters, and {sl . . . . .  sk} for a sequence of k 
input stenograph codes. 
The co-occurrence frequencies necessary for 
computation are acquired through training. 
However, a huge amount of data is needed to 
generate reliable statistical estimates for (1) if 
N > 3. Consequently, N-gram probability is 
approximated by bigram or trigram estimates. 
First, rewrite (1) as (2) using Bayes' rule. 
P(c, ..... c )xP(s, ..... s lc, ..... c , )  
(2) 
P(s, ..... s k ) 
As the value of P(s I . . . . .  st) remains unchanged 
for any choice of {q . . . . .  ct}, one needs only to 
maximize the numerator in (2), i.e. (3). 
(3) P(cl ..... Ck) X P(sl ..... s,\[cl ..... ck) 
(3) can then be approximated by (4) or (5) using 
bigram and trigram models respectively. 
(4) FL=,...., (P(c,lq_,) x P(s,.Iq)) 
(5) ?P(silci)) 
The transcription program is to compute the best 
sequence of {q . . . . .  c,} so as to maximize (4) or 
(5). The advantage of the approximations in (4) 
and (5) is that P(s,lc,), P(c,lc,.,) and P(c,lc,_2c,_,) 
can be readily estimated using a training corpus of 
manageable size. 
3.2 Viterbi Algorithm 
The Viterbi algorithm (Viterbi, 1967) is 
implemented toefficiently compute the maxinmm 
value of (4) and (5) for different choices of 
1122 
character sequences. Instead o1' exhaustively 
computing tile values for all possible character 
sequences, the algorithm only keeps track of the 
probability of the best character sequence 
terminating in each possible character candidate 
for a stenograph code. 
In the trigram implelnentatiou, size limitation 
in the training cortms makes it impossible to 
estimate all possible P(c i lc i_2ci . i )  because some 
{ci_2, ci_l, q} may never occur there. Following 
Jelinek (1990), P(cil ci.2ci_ i ) is approximated by 
the summation of weighted lrigram, bigram and 
unigram estimates in (6). 
(6) p(c i I ci_ 2 ci_ 1) 
f(ci_ 2 ci- I ci ) f(ci_ 1 c i ) f(c i ) 
= w 3 X + w 2 X + w 1 X -  
f(ci_ 2 ci_ 1 ) f(ci_ I ) Z f(cj ) 
where (i) w,, w2, w-s _> 0 are weights, (ii) 
wl-l-w2-{-H; 3 = 1, and (iii) Z f(q) is the stun of 
frequencies of all characters. Typically lhe best 
results can be obtained if w:~, the weight for 
trigram, is significantly greater than the olher two 
weights so that the trigram probability has 
dominant effect in the probability expression. In 
our tests, we sot wl=0.01, w2=0.09, aud u;3=0.9. 
The Viterbi algorithm substantially reduces the 
computational complexity flom O(m") to O(m.-~n) 
and O(nr~n) using bigram and trigram estimation 
rc:spectively where n is the number of stenograph 
code tokens in a sentence, and m is tile upper 
bound of the number of homophonous characters 
for a stenograph code. 
To maximize the transcription accuracy, we 
also refine the training corpus to ensure that the 
bigram and trigram statistical models reflect the 
comtroom lauguage closely. This is done by 
enlarging tile size of tile training corpus and by 
compiling domain-specific text corpora. 
3.,3 Special Encoding 
After some initial trial tests, error analysis was 
conducted to investigate the causes of the mis- 
transcribed characters. It showed that a noticeable 
amount of errors were due to high failure rate in 
the mtriewtl of seine characters in the 
transcription. The main reason is that high 
fiequency characters are more likely to interfere 
with the correct retrieval of other relatively lower 
frequency homophouous characters. For example, 
Cantonese, hal ('to be') and hal ('at') are 
homophouous in terms of seglnental makeup. 
Their absolute fiequcucies in our training corpus 
are 8,695 and 1,614 respectively. Because of the 
large fi'equency discrepancy, the latter was mis- 
transcribed as tile former 44% of the times in a 
trial test. 32 such high fi'equency characters were 
found to contribute to about 25% of all 
transcription errors. To minimize the interference, 
special encoding, which resulted flom shallow 
linguistic processing, is applied to the 32 
characters o that each of them is assigned a 
unique stenograph code. This was readily 
accepted by the court stenographers. 
4. hnplementation and Results 
4.1 Compilation of Corpora 
In our expreriments, authentic Chinese court 
proceedings from the Hong Kong Judiciary were 
used fox tile compilation of the training and 
testing corpora for the CAT prototypes. To ensure 
that tile training data is comparable with tile data 
to be transcribed, the training corpus should be 
large enough to obtain reliable estimates for 
P(silc,.), P(cilci j) and P(cilci_2ci_l).  in our trials, 
we quickly approached the point of diminishing 
return when the size of the training corpus reaches 
about 0.85 million characters. (See Section 4.2.2.) 
To further enhance training, the system also 
exploited stylistic and lexical variations across 
different legal domains, e.g. tra\[.'fic, assauh,  and 
f raud  offences. Since different case types show 
distinct domain-specific legal vocabulary or usage, 
simply integrating all texts in a single training 
corpus may obscure the characteristics o1' specific 
language domains, thus degrading the modelling. 
Hence domain-specific training corpora were also 
compiled to enhance performance. 
Two sets of data were created for testing and 
comparison: Gener ic  Coqms (GC) and Domain -  
,specific Cmpus  (DC). Whereas GC consists of 
texts representing various legal case types, DC is 
restricted to traffic offence cases. Each set 
consists of a training corpus of 0.85 million 
characters and a testing corpus of 0.2 million 
characters. The training corpus consists of 
Chinese characters along with the corresponding 
stenograph codes, and tile testing corpus consists 
solely of stenograph codes of the Chinese texts. 
4.2 Experimental Results 
For ewfluation, several prototypes were set up to 
1123 
test how different factors affected transcription 
accuracy. They included (i) use of bigram vs. 
trigram models, (ii) the size of the training 
corpora, (iii) domain-specific training, and (iv) 
special encoding. To measure conversion 
accuracy, the output text was compared with the 
original Chinese text in each test on a character by 
character basis, and the percentage of correctly 
transcribed characters was computed. Five sets of 
experiments are reported below. 
4.2.1 Bigram vs. Trigram 
Three prototypes were developed: the Bigram 
Prototype, CA Tva2, the Trigram Prototype, CA Tva.~, 
and the Baseline Prototype, CATo. CATva2 and 
CATvA.~ implelnent he conversion engines using 
the bigram and trigram Viterbi algorithm 
respectively. CA7o, was set up to serve as an 
experimental control. Instead of implementing the 
N-gram model, conversion is accomplished by 
selecting the highest fiequency item out of the 
homophonous character set for each stenograph 
code. GC was used throughout the three 
experiments. The training and testing data sets are 
0.85 and 0.20 million characters respectively. The 
results are summarized in Table 1. 
Corpus GC GC GC 
Accuracy 78.0% 92.4% 93.6% 
Table 1. Different N-gram Models 
The application of the bigram and trigram models 
offers about 14% and 15% improvement in 
accuracy over Control Prototype, CATo. 
4.2.2 Size of Training Corpora 
In this set of tests, the size of the training corpora 
was varied to determine the impact of the training 
corpus size on accuracy. The sizes tested are 0.20, 
0.35, 0.50, 0.63, 0.73 and 0.85 million characters. 
Each corpus is a proper subset of the immediately 
larger corpus so as to ensure the comparability of 
he trainin texts. CATvA 2 was used in the tests. 
Training Corpus GC GC GC 
Accurac~ 89.5% 91.2% 91.8% 
Training Corpus GC GC GC 
Accuracy 92.1% 92.3 % 92.4 % 
Table 2. Variable Training Data Size 
The results in Table 2 show that increasing the 
size of the training corpus enhances the accuracy 
incrementally. However, the point of diminishing 
return is reached when the size reaches 0.85 
million characters. We also tried doubling the 
corpus size to 1.50 million characters. It only 
yields 0.8% gain over the 0.85 million character 
corpus. 
4.2.3 Use of Domain-specific Training 
This set of tests evaluates the effectiveness of 
domain-specific training. Data fi'oln the two 
corpora, GC and DC, are utilized in the training of 
the bigram and trigram prototypes. The size of 
each training set is 0.85 million characters. The 
same set of 0.2 million character testing data from 
DC is used in all four conversion tests. Without 
increasing the size of the training data, setups with 
domain-specific training consistently ield about 
2% improvement. A more comprehensive set of 
corpora including Tra.lfic, Assault, and Robbeo~ is
bein )iled and will be re )ortcd in future. 
Prototypes 
Training Data 
CATvA; CATvA3 CATvaz CATvA3 
GC GC DC DC 
Testing Data DC DC DC DC 
Accuracy 92.6% 92.8% 94.7% 94.8% 
Table 3. Application of Domain-Specificity 
4.2.4 Special Encoding 
Following shallow linguistic processing, special 
encoding assigns unique codes to 32 characters to 
reduce confusion with other characters. Another 
round of tests was repeated, identical to the 
CATvA2 and CATvA 3 tests in Section 4.2.1, except 
for the use of special encoding. The use of 
training and testing corpora have 0.85 and 0.20 
million characters respective 
S ~ i ~  I;:~ :::NOfA~I~Ii~: 
Prototypes CATw~ CATvA~ CATw2 CATvA3 
Corpus GC GC GC GC 
Accuracy 92.4% 93.6% 94.7% 95.6% 
Table 4. Application of Special Encoding 
Table 4 shows that the addition of special 
encoding consistently offers about 2% increase in 
accuracy. Special encoding and hence shallow 
linguistic processing provide the most significant 
improvement in accuracy. 
4.2.5 Incorporation of Domain-Specificity and 
Special Eneoding 
As discussed above, both domain-specific training 
and special encoding raise the accuracy of 
transcription. The last set of tests deals with the 
integration of the two features. Special encoding 
1124 
is utilized in the training and testing data of DC 
which have 0.85 and 0.20 million characters 
respectively. 
~raining/Testing, Data DC DC 
I S. Encoding Applied Applied / zcuracy 95.4 % 96.2 % 
Table 5. Integration of D. Specificity and S. Encoding 
Recall that Domain-Specificity and Special 
Encoding each offers 2% improvelnent. Table 5 
shows that combining BOTH features offer about 
3% improvement over tests without them. (See 
non-domain-specific tests in Section 4.2.3) 
The 96.2% accuracy achieved by CATvA 3 
represents the best performance of our system. 
The result is conaparable with other relevant 
advanced systems for speech to text conversion. 
For example, Lee (1999) reported 94% accuracy 
in a Chinese speech to text transcription system 
under developlnent with very large training 
corpus. 
5. Conclusion 
We have created a Cantonese Chinese CAT 
system which uses the phonologically-based 
stenograph machine. The system delivers 
encouragingly accurate transcription in a language 
which has many hon\]ol~honous characters. To 
resolve problematic ambiguity in the conversion 
fi'on-i a I)honologically-based code to the 
logograt)hic Chinese characters, we made use of 
lhe N-gram statistical model. The Viterbi 
algorithm has enabled us to identify the most 
probable sequence of characters from the sels of 
possible homophonous characters. With the 
additional use of special encoding and domain- 
specific training, the Cantonese CAT system has 
attained 96% transcription accuracy. The success 
of the Jurilinguistic Engineering project can 
further enhance the efforts by the Hong Kong 
Judiciary to conduct trials in the language of the 
majority population. Further improvement to the 
system will include (i) more domain-specific 
training and testing across different case types, (2) 
firm-tuning for the optimal weights in the trigram 
formula, and (3) optilnizing the balance between 
training corpus size and shallow linguistic 
processing. 
Acknowledgement  
Support for the research reported here is provided 
mainly through the Research Grants Council of 
Hong Kong under Competitive Earmarked 
Research Grant (CERG) No. 9040326. 
References 
Hong Kong Govennnent. 1999. Hong Kong 
Szq~plemenmry Character Set. hfformation 
Technology Services Department & Official 
Languages Agency. 
Jelinek, F. 1990. "Self-organized Language Modeling 
lbr Speech Recognition." In A. Waibel and K.F. 
Lee, (eds.). Readings in Speech Recognition. San 
Mateo: CA: Morgan Kaufmann Publishers. 
Lee, K. F. 1999. "Towards a Multimedia, Multimodal, 
Multilingual Computer." Paper presented on behall' 
of Microsoft Research Institute, China in the 5th 
Natural Language Processing Pacil'ic Rim 
Symposium held in Belling, China, November 5-7, 
1999. 
Lun, S., K. K. Sin, B. K. T'sou and T. A. Cheng. 1997. 
"l)iannao Fuzhu Yueyu Suii Fangan." (The 
Cantonese Shorthand System for Computer-Aided 
Transcription) (in Chinese) Proceedings o.f the 5th 
lntermttional Confere,ce on Cantonese and Other 
Yue Dialects. B. H. Zhan (ed). Guangzhou: Jinan 
University Press. pp. 217--227. 
Sin, K. K. and B. K. T'sou. 1994. "Hong Kong 
Courtroon\] Language: Some Issues on Linguistics 
and Language Technology." Paper presented at lhe 
Third International Conference on Chinese 
Linguistics. Hong Kong. 
T'sou, B. K. 1976. "Homophony and Internal Change 
in Chinese." Computational Analyses of Asian and 
African Lauguages 3, 67--86. 
T'sou, t3. K. 1993. "Some Issues on Law and Language 
in the ltong Kong Special Administrative Region 
(HKSAR) of China." Language, Law attd Equality: 
Proceedings of the 3rd International Conference of 
the International Acaden G of Language Law (IALL). 
K. Prinsloo et al Pretoria (cds.): University of South 
Africa. pp. 314-331. 
T'sou, B. K., H. L. Lin, G. Liu, T. Chan, J. Hu, C. H. 
Chew, and J. K. P. Tse. 1997. "A Synchronous 
Chinese Language Corpus fi'om Different Speech 
Communities: Construction and Applications." 
Computational Linguistics and Chinese Language 
Ptwcessing 2:91-- 104. 
Viterbi, A. J. 1967. "Error Bounds for Convolution 
Codes and an Asymptotically Optimal l)ecoding 
Algorithm." IEEE 7'ransactions on htformation 
TheoG 13: 260--269. 
1125 
	
						
			Morpheme-based Derivation of  
Bipolar Semantic Orientation of Chinese Words  
Raymond W.M. Yuen, Terence Y.W. Chan, Tom B.Y. Lai, O.Y. Kwong, Benjamin K.Y. T'sou  
Language Information Sciences Research Centre, the City University of Hong Kong 
83 Tat Chee Avenue, Hong Kong  
{ wmyuen, dcywchan, cttomlai, rlolivia, rlbtsou}@cityu.edu.hk 
 
Abstract 
The evaluative character of a word is called its 
semantic orientation (SO). A positive SO 
indicates desirability (e.g. Good, Honest) and 
a negative SO indicates undesirability (e.g., 
Bad, Ugly). This paper presents a method, 
based on Turney (2003), for inferring the SO 
of a word from its statistical association with 
strongly-polarized words and morphemes in 
Chinese. It is noted that morphemes are much 
less numerous than words, and that also a 
small number of fundamental morphemes may 
be used in the modified system to great 
advantage. The algorithm was tested on 1,249 
words (604 positive and 645 negative) in a 
corpus of 34 million words, and was run with 
20 and 40 polarized words respectively, giving 
a high precision (79.96% to 81.05%), but a 
low recall (45.56% to 59.57%). The algorithm 
was then run with 20 polarized morphemes, or 
single characters, in the same corpus, giving a 
high precision of 80.23% and a high recall of 
85.03%. We concluded that morphemes in 
Chinese, as in any language, constitute a dis-
tinct sub-lexical unit which, though small in 
number, has greater linguistic significance 
than words, as seen by the significant en-
hancement of results with a much smaller 
corpus than that required by Turney. 
1. Introduction 
The semantic orientation (SO) of a word indicates 
the direction in which the word deviates from the 
norm for its semantic group or lexical field (Lehrer, 
1974). Words that encode a desirable state (e.g., 
beautiful) have a positive SO, while words that 
represent undesirable states (e.g. absurd) have a 
negative SO (Hatzivassiloglou and Wiebe, 2000). 
Hatzivassiloglou and Mckeown (1997) used the 
words ?and?, ?or?, and ?but? as linguistic cues to 
extract adjective pairs. Turney (2003) assessed the 
SO of words using their occurrences near strongly-
polarized words like ?excellent? and ?poor? with 
accuracy from 61% to 82%, subject to corpus size. 
Turney?s algorithm requires a colossal corpus 
(hundred billion words) indexed by the AltaVista 
search engine in his experiment. Undoubtedly, 
internet texts have formed a very large and easily-
accessible corpus. However, Chinese texts in 
internet are not segmented so it is not cost-
effective to use them. 
This paper presents a general strategy for 
inferring SO for Chinese words from their 
association with some strongly-polarized 
morphemes. The modified system of using 
morphemes was proved to be more effective than  
strongly-polarized words in a much smaller corpus.  
Related work and potential applications of SO 
are discussed in section 2. 
Section 3 illustrates one of the methods of 
Turney?s model for inferring SO, namely, 
Pointwise Mutual Information (PMI), based on the 
hypothesis that the SO of a word tends to 
correspond to the SO of its neighbours. 
The experiment with polarized words is 
presented in section 4. The test set includes 1,249 
words (604 positive and 645 negative). In a corpus 
of 34 million word tokens, 410k word types, the 
algorithm is run with 20 and 40 polarized words, 
giving a precision of 79.96% and 81.05%, and a 
recall  of 45.56% and 59.57%, respectively. 
The system is further modified by using 
polarized morphemes in section 5. We first 
evaluate the distinction of Chinese morphemes to 
justify why the modification can probably give 
simpler and better results, and then introduce a 
more scientific selection of polarized morphemes. 
A high precision of 80.23% and a greatly increased 
recall of 85.03% are yielded. 
In section 6, the algorithm is run with 14, 10 and 
6 morphemes, giving a precision of 79.15%, 
79.89% and 75.65%, and a recall of 79.50%, 
73.26% and 66.29% respectively. It shows that the 
algorithm can be also effectively run with 6 to 10 
polarized morphemes in a smaller corpus. 
The conclusion and future work are discussed in 
section 7. 
2. Related Work and Applications 
Hatzivassiloglou and Mckeown (1997) presented a 
method for automatically assigning a + or ? 
orientation label to adjectives known to have some 
SO by the linguistic constraints on the use of 
adjectives in conjunctions. For example, ?and? 
links adjectives that have the same SO, while ?but? 
links adjectives that have opposite SO. They 
devised an algorithm based on such constraints to 
evaluate 1,336 manually-labeled adjectives (657 
positive and 679 negative) with 97% accuracy in a 
corpus of 21 million words. 
Turney (2003) introduced a method for 
automatically inferring the direction and intensity 
of the SO of a word from its statistical association 
with a set of positive and negative paradigm words, 
i.e., strongly-polarized words. The algorithm was 
evaluated on 3,596 words (1,614 positive and 
1,982 negative) including adjectives, adverbs, 
nouns, and verbs. An accuracy of 82.8% was 
attained in a corpus of hundred billion words. 
SO can be used to classify reviews (e.g., movie 
reviews) as positive or negative (Turney, 2002), 
and applied to subjectivity analysis such as 
recognizing hostile messages, classifying emails, 
mining reviews (Wiebe et al, 2001). The first step 
of those applications is to recognize that the text is 
subjective and then the second step, naturally, is to 
determine the SO of the subjective text. Also, it 
can be used to summarize argumentative articles 
like editorials of news media. A summarization 
system would benefit from distinguishing 
sentences intended to present factual materials 
from those intended to present opinions, since 
many summaries are meant to include only facts. 
3. SO from Association-PMI 
Turney (2003) examined SO-PMI (Pointwise 
Mutual Information) and SO-LSA (Latent 
Semantic Analysis). SO-PMI will be our focus in 
the following parts. PMI is defined as:  
 
PMI(word1, word2)=log2( )()(
)&(
21
21
wordpwordp
wordwordp ) 
 
where p(word1 & word2) is the probability that 
word1 and word2 co-occur. If the words are 
statistically independent, the probability that they 
co-occur is given by the product p(word1) p(word2). 
The ratio between p(word1 & word2) and p(word1) 
p(word2) is a measure of the degree of statistical 
dependence between the words. The SO of a given 
word is calculated from the strength of its 
association with a set of positive words, minus the 
strength of its association with a set of negative 
words. Thus the SO of a word, word, is calculated 
by SO-PMI as follows: 
SO-PMI(word) = 

?Pwordspword
pwordwordPMI ),(  - 
?Nwordsnword
nwordwordPMI ),(  
 
where Pwords is a set of 7 positive paradigm 
words (good, nice, excellent, positive, fortunate, 
correct, and superior) and Nwords is a set of 7 
negative paradigm words (bad, nasty, poor, 
negative, unfortunate, wrong, and inferior). Those 
14 words were chosen by intuition and based on 
opposing pairs (good/bad, excellent/poor, etc.). 
The words are rather insensitive to context, i.e., 
?excellent? is positive in almost all contexts. 
A word, word, is classified as having a positive 
SO when SO-PMI(word) is positive and a negative 
SO when SO-PMI(word) is negative.  
Turney (2003) used the Alta Vista Advanced 
search engine with a NEAR operator, which 
constrains the search to documents that contain the 
words within ten words of one another, in either 
order. Three corpora were tested. AV-ENG is the 
largest corpus covering 350 million web pages 
(English only) indexed by Alta Vista. The medium 
corpus is a 2% subset of AV-ENG corpus called 
AV-CA (Canadian domain only). The smallest 
corpus TASA is about 0.5% of AV-CA and 
contains various short documents. 
One of the lexicons used in Turney?s experiment 
is the GI lexicon (Stone et al, 1966), which 
consists of 3,596 adjectives, adverbs, nouns, and 
verbs, 1,614 positive and 1,982 negative. 
Table 1 shows the precision of SO-PMI with the 
GI lexicon in the three corpora. 
Precision Percent of 
full test set 
Size of 
test set AV-ENG AV-CA TASA 
100% 3596 82.84% 76.06% 61.26% 
75% 2697 90.66% 81.76% 63.92% 
50% 1798 95.49% 87.26% 47.33% 
25% 899 97.11% 89.88% 68.74% 
Approx. no. of 
words 1x10
11
 2x109 1x107 
Table 1: The precision of SO-PMI with the GI 
lexicon  
 
The strength (absolute value) of the SO was 
used as a measure of confidence that the words 
will be correctly classified. Test set words were 
sorted in descending order of the absolute value of 
their SO and the top ranked words (the highest 
confidence words) were then classified. For 
example, the second row (starting with 75%) in 
table 1 shows the precision when the top 75% were 
classified and the last 25% (with lowest confidence) 
were ignored. We will employ this measure of 
confidence in the following experiments.  
Turney concluded that SO-PMI requires a large 
corpus (hundred billion words), but it is simple, 
easy to implement, unsupervised, and it is not 
restricted to adjectives.  
4. Experiment with Chinese Words 
In the following experiments, we applied Turney?s 
method to Chinese. The algorithm was run with 20 
and then 40 paradigm words for comparison. The 
experiment details include: 
NEAR Operator: it was applied to constrain 
the search to documents that contain the words 
within ten words of one another, in either order. 
Corpus: the LIVAC synchronous corpus (Tsou 
et al, 2000, http://www.livac.org) was used. It 
covers 9-year news reports of Chinese 
communities including Hong Kong, Beijing and 
Taiwan, and we used a sub-corpus with about 34 
million word tokens and 410k word types.  
Test Set Words: a combined set of two 
dictionaries of polarized words (Guo, 1999, Wang, 
2001) was used to evaluate the results. While 
LIVAC is an enormous Chinese corpus, its size is 
still far from the hundred-billion-word corpus used 
by Turney. It is likely that some words in the 
combined set are not used in the 9-year corpus. To 
avoid a skewed recall, the number of test set words 
used in the corpus is given in table 2. In other 
words, the recall can be calculated by the total 
number of words used in the corpus, but not by 
that recorded in the dictionaries. The difference 
between two numbers is just 100. 
Polarity Total no. of the 
test set words 
Words used in 
the 9-year corpus 
Positive 629 604 
Negative 721 645 
Total 1350 1249 
Table 2: Number of the test set words  
 
Paradigm words: The paradigm words were 
chosen using intuition and based on opposing pairs, 
as Turney (2003) did. The first experiment was 
conducted with 10 positive and 10 negative 
paradigm words, as follows,  
Pwords: (honest), (clever), (sufficient), 
(lucky), (right), (excellent), 
(prosperous), (kind), (brave), (humble) 
Nwords: (hypocritical), (foolish), 
(deficient), (unlucky), (wrong), (adverse), 
(unsuccessful), (violent), (cowardly), 
(arrogant) 
The experiment was then repeated by increasing 
the number of paradigm words to 40. The 
paradigm words added are: 
Pwords: (mild), (favourable), 
(successful), (positive), (active), 
(optimistic), (benign), (attentive), 
(promising), (incorrupt) 
Nwords: (radical), (unfavourable), 
(failed), (negative), (passive), 
(pessimistic), (malignant), (inattentive), 
(indifferent), (corrupt) 
4.1 Results 
Tables 3 and 4 show the precision and recall of 
SO-PMI by two sets of paradigm words.  
% of test set 100% 75% 50% 25% 
Size of test set 1249 937 625 312 
Extracted Set 569 427 285 142 
Precision 79.96% 86.17% 86.99% 90.16% 
Recall 45.56% 
Table 3: Precision and Recall of the SO-PMI of the 
20 paradigm word test set 
% of  test set 100% 75% 50% 25% 
Size of test set 1249 937 625 312 
Extracted Set 744 558 372 186 
Precision 81.05% 86.02% 88.71% 94.09% 
Recall 59.57% 
Table 4: Precision and Recall of the SO-PMI of the 
40 paradigm word test set 
 
The results of both sets gave a satisfactory 
precision of 80% even in 100% confidence. 
However, the recall was just 45.56% under the 20-
word condition, and rose to 59.57% under the 40-
word condition. The 15% rise was noted. 
To further improve the recall performance, we 
experimented with a modified algorithm based on 
the distinct features of Chinese morphemes.  
5. Experiment with Chinese Morphemes 
Taking morphemes to be smallest linguistic 
meaningful unit, Chinese morphemes are mostly 
monosyllabic and single characters, although there 
are some exceptional poly-syllabic morphemes like 
 (grape),  (coffee), which are mostly 
loanwords. In the following discussion, we 
consider morphemes to be monosyllabic and 
represented by single characters. 
It is observed that many poly-syllabic words 
with the same SO incorporate a common set of 
morphemes. The fact suggests the possibility of 
using paradigm morphemes instead of words.  
Unlike English, the constituent morphemes of a 
Chinese word are often free-standing monosyllabic 
words. It is note-worthy that words in ancient 
Chinese were much more mono-morphemic than 
modern Chinese. The evolution from monosyllabic 
word to disyllabic word may have its origin in the 
phonological simplification which has given rise to 
homophony, and which has affected the efficacy of 
communication. To compensate for this, many 
more related disyllabic words have appeared in 
modern Chinese (Tsou, 1976). There are three 
basic constructions for deriving disyllabic words in 
Chinese, including:  
(1) combination of synonyms or near 
synonyms ( , warm, genial, =warm, mild, 
=warm, genial) 
(2) combination of semantically related 
morphemes ( , =affair, =circumstances) 
(3) The affixation of minor suffixes which 
serve no primary grammatical function ( , 
=village, =zi, suffix) 
The three processes for deriving disyllabic 
morphemes in Chinese outlined here should be 
viewed as historical processes. The extent to which 
such processes may be realized by native speakers 
to be productive synchronically bears further 
exploration. Of the three processes, the first two, 
i.e., synonym and near-synonym compounding, are 
used frequently by speakers for purposes of 
disambiguation. In view of this development, the 
evolution from monosyllabic words in ancient 
Chinese to disyllabic words in modern Chinese 
does not change the inherent meaning of the 
morphemes (words in ancient Chinese) in many 
cases. The SO of a word often conforms to that of 
its morphemes.  
In English, there are affixal morphemes like dis-, 
un- (negation prefix), or ?less (suffix meaning 
short-age), -ful (suffix meaning ?to have a property 
of?), we can say ?careful? or ?careless? to expand 
the meaning of ?care?. However, it is impossible to 
construct a word like ?*ful-care?, ?*less-care?. 
However, in Chinese, the position of a morpheme 
in many disyllabic words is far more flexible in the 
formation of synonym and near-synonym 
compound words. For instance, ? ?(honor) is a 
part of two similar word ? ? (honor-bright) and 
? ?(outstanding-honor). Morphemes in Chinese 
are like a ?zipped file? of the same file types. When 
it unzips, all the words released have the same SO. 
5.1 Probability of Constituent Morphemes 
of Words with the Same SO 
Most morphemes can contribute to positive or 
negative words, regardless of their inherent 
meaning. For example, ? ? (luck) has inherently a 
positive meaning, but it can construct both positive 
word ? ? (lucky) or a negative word ? ? 
(unlucky). Thus it is not easy to define the 
paradigm set simply by intuition. But we can 
assign a probability value for a morpheme in 
forming polarized words on the basis of corpus 
data. 
The first step is to come up with possible 
paradigm morphemes by intuition in a large set of 
polarized words. With the LIVAC synchronous 
corpus, the types and tokens of the words 
constructed by the selected morphemes can easily 
be extracted. The word types, excluding proper 
nouns, are then manually-labeled as negative, 
neutral or positive. Then to obtain the probability 
that a polar morpheme generates words with the 
same SO, the tokens of the polarized word types 
carrying the morpheme are divided by the tokens 
of all word types carrying the morpheme. For 
example, given a negative morpheme, m1, the 
probability that it appears in negative words in 
token, P(m1, -ve) is given by: 
 
1m Carrying  WordtypesAll of Tokens
1m Carrying rdtypesNegativeWo of Tokens
 
 
Positive morphemes can be done likewise. Ten 
negative morphemes and ten positive morphemes 
were chosen as in table 5. Their values of 
P(morpheme, orientation) are all above 0.95. 
 +ve Morpheme -ve Morpheme 
1  (gift) (hurt) 
2  (win)  
3  (good) (doubt) 
4  (secure) (difficult) 
5  (rich) (rush) 
6  (health)  
7  (happy) (explode) 
8  (honor) (ban) 
9 (hardworking) (collapse) 
10 (smooth) (reject) 
Derived Types 7383 2048 
Tokens 247249 166335 
Table 5: Selected positive and negative 
morphemes 
 
Those morphemes were extracted from a 5-year 
subset of the LIVAC corpus. A morpheme, free to 
construct new words, may construct hundreds of 
words but those words with extremely low 
frequency can be regarded as ?noise?. The ?noise? 
may be ?creative use? or even incorrect use. Thus, 
the number of ready-to-label word types formed 
from a particular morpheme was limited to 50, but 
it must cover 80% of the tokens of all word types 
carrying the morpheme in the corpus (i.e., 80% 
dominance). For example, if the morpheme m1 
constructs 120 word types with 10,000 tokens, and 
the first 50 high-frequency words can reach 8,000 
tokens, then the remaining 70 low-frequency word 
types, or noise, are discarded. Otherwise, the 
number of sampled words would be expanded to a 
number (over 50) fulfilling 80% dominance. 
5.2 Results and Evaluation 
In table 6, the precision of 80.23% is slightly better 
than 79.96% of the 20-word condition, and just 1% 
lower than that of the 40-word condition. However, 
the recall drastically increases from 45.56%, or 
59.57% under the 40-word condition, to 85.03%. 
In other words, the algorithm run with 20 Chinese 
paradigm morphemes resulted not only in high 
precision but also much higher recall than Chinese 
paradigm words in the same corpus. 
% of test set 100% 75% 50% 25% 
Size of test set 1249 937 625 312
Extracted Set 1062 797 531 266
Precision 80.23% 85.44% 90.96% 96.61%
Recall 85.03% 
Table 6: Precision and Recall of SO-PMI of the 20 
paradigm morpheme test set 
 
Since the morphemes were chosen from a subset 
of the corpus for evaluation, we repeated the 
experiment in a separate 1-year corpus (2001-
2002). The results in table 7 reflect a similar 
pattern in the two corpora ? both words and 
morphemes can get high precision, but morphemes 
can double the recall of words. 
 40 Words 20 Morphemes 
Size of test set 1065 
Extracted Set 333 671 
Precision (Full Set) 75.38% 73.62% 
Recall 31.27% 63.00% 
Table 7: Precision (full test set only) and Recall of 
SO-PMI of 40 paradigm words and 20 paradigm 
morphemes in 1-year corpus 
 
It is assumed that a smaller corpus easily leads 
to the algorithm?s low recall because many low-
frequency words in the test set barely associate 
with the paradigm words. To examine the 
assumption, the results were further analyzed with 
the frequency of the test set words. First, the 
occurrence of the test set words in the 9-year 
corpus was counted, then the median of the 
frequency, 44 in this case, was taken. The results 
were divided into two sections from the median 
value, and the recall of two sections was calculated 
respectively, as in table 8.  
?
Table 8: Morpheme-based and word-based recall 
of high-frequency and low-frequency words  
 
The results showed that high-frequency words 
could be largely extracted by the algorithm with 
both morphemes (99.80% recall) and words 
(89.45% recall). However, paradigm words gave 
26.55% recall of low-frequency words, whereas 
paradigm morphemes gave 67.66%. They showed 
that morphemes outperform words in the retrieval 
of low-frequency words. 
Colossal corpora like Turney?s hundred-billion-
word corpus can compensate for the low 
performance of paradigm words in low-frequency 
words. Such a large corpus has been easily-
accessible since the emergence of internet, but it is 
not cost-effective to use the Chinese texts from the 
internet because those texts are not segmented. 
Another way of compensation is the expansion of 
paradigm words, but doubling the number of 
paradigm words just raised the recall from 45.56% 
to 59.57%, as shown in section 4. The supervised 
cost is not reasonable if the number of paradigm 
words is further expanded. 
Morphemes, or single characters in Chinese, 
naturally occur more frequently than words in an 
article, so 20 morphemes can be more discretely-
distributed over texts than 20 or even 40 words. 
The results show that some morphemes always 
retain their inherent SO when becoming 
constituents in other derived words. Such 
morphemes are like a zipped file of the same SO, 
when the algorithm is run with 20 paradigm 
morphemes, it is actually run by thousands of 
paradigm words. Consequently, the recall could 
double while the high precision was not affected.  
It may be argued that the labour cost of defining 
the SO of 20 morphemes is not sufficiently low 
either. The following experiments will demonstrate 
that decreasing the number of morphemes can also 
give satisfactory results. 
6. Experiment with different number of 
morphemes 
The following experiments were done respectively 
by decreasing the number of morphemes, i.e., 14 
and 10 morphemes, chosen from table 5. The 
algorithm was then run with 3 groups of 6 different 
morphemes, in which the morphemes were 
different, and the combination of morphemes in 
each group was random. The morphemes in each 
group are shown in table 9. Other conditions for 
the experiments were unchanged. 
6.1 Results and Evaluation 
Table 10 shows the results with different number 
of morphemes, and table 11 shows those for 
different groups of 6 morphemes. For convenient 
comparison, the tables only show the results of the 
full test set, i.e., no threshold filtering. 
It is shown that the recall falls as the number of 
morphemes is reduced. However, even the average 
recall 66.29% under the 6-morpheme condition is 
still higher than that under the 40-word condition 
(59.57%). In section 5, it was evaluated that low 
recall could be attributed to the low frequency of 
test set words. Therefore, 6 to 10 morphemes are 
already ideal for deducing the SO of high-
frequency words.  
 Number of morphemes used 
Morpheme 20 14 10 6 (Gp1) 
6 
(Gp2) 
6 
(Gp3)
P 
 (gift) 1   1   
P 
 (good) 1 1 1 1   
P 
 (happy) 1 1  1   
P 
 (rich) 1 1   1  
P 
 (honor) 1 1 1  1  
P (smooth) 1 1 1  1  
P 
 (win) 1     1 
P 
 (secure) 1     1 
P 
 (health) 1 1 1   1 
P 
 (hardworking) 1 1 1    
N (doubt) 1 1 1 1   
N (explode) 1 1  1   
N (ban) 1 1 1 1   
N (rash) 1 1 1  1  
N (greedy) 1 1 1  1  
N (difficult) 1 1 1  1  
N (hurt) 1 1    1 
N (rush) 1     1 
N (collapse) 1     1 
N (reject) 1      
Table 9: Morphemes selected for different 
experimental sets, P=+ve, N=-ve, 1=?selected?, 
Gp= Group 
 Number of morphemes used 
No of morphemes 20 14 10 
Size of test set 1249 1249 1249 
Extracted Set 1062 993 915 
Precision (%) 80.23 79.15 79.89 
Recall (%) 85.03 79.50 73.26 
Table 10: Precision and Recall of SO-PMI of the 
test set words with different no. of morphemes 
Group of 
Morphemes 
Group 1 Group 2 Group 3 Average 
Size of test set 1249 1249 1249 1249 
Extracted Set 837 776 871 828
Precision (%) 79.69 78.48 68.77 75.65
Recall (%) 67.01 62.13 69.74 66.29
Table 11: Precision and Recall of SO-PMI of the 
test set words with 3 different groups of 6 
morphemes 
 
The precision remains high from 20 morphemes 
to 6 morphemes, but from table 10 the precision 
varies with different sets of morphemes. Group 3 
gave the lowest precision of 68.77%, whereas 
other groups gave a high precision close to 80%. 
The limited space of this paper cannot allow a 
detailed investigation into the reasons for this 
result, only some suggestions can be made. 
The precision may be related to the dominant 
lexical types of the words constructed by the 
morphemes and those of the test set words. Lexical 
types should be carefully considered in the 
algorithm for Chinese because Chinese is an 
isolating language - no form change. For example, 
the word ? ? (recover) can appear in different 
positions of a sentence, such as the following 
examples extracted from the corpus:  
(1)? ? (...American 
economy is gradually recovering?) 
(2) ?
(?most people is now pessimistic about the 
economy recovery) 
(3) ?
(?decelerates the recovery, but also makes the 
future unpredictable.) 
English allows different forms of ?recovery, like 
?recovery?, ?recovering?, ?recovered? but Chinese 
does not. Lexical types are thus an important factor 
for the precision performance. Another way of 
solving the problems of lexical types is the 
automatic extraction of meaningful units 
(Danielsson, 2003). Simply, meaningful units are 
some frequently-used patterns which consist of two 
or more words. It is useful to automatically extract 
the meaningful units with SO in future. 
Syntactic markers like negation, and creative 
uses like ironical expression of adding quotation 
marks can also affect the precision. Here is an 
example from the corpus: 
(?HONEST BUSINESSMAN?). The quotation 
mark (? ? in English) is to actually express the 
opposite meaning of words within the mark, i.e., 
HONEST means DISHONEST in this case. Such 
markers should further be handled, just as with the 
use of ?so-called?. 
6 Conclusion and Future Work 
This paper presents an algorithm based on 
Turney?s model (2003) for inferring SO of Chinese 
words from their association with strongly-
polarized Chinese morphemes. The algorithm was 
run with 20 and 40 strongly-polarized Chinese 
words respectively in a corpus of 34 million words, 
giving a high precision of 79.96% and 81.05%, but 
a low recall of 45.56% and 59.57%. The algorithm 
was then run with 20 Chinese polarized 
morphemes, or single characters, in the same 
corpus, giving a high precision of 80.23% and an 
even high recall of 85.03%. The algorithm was 
further run with just 14, 10 and 6 morphemes, 
giving a precision of 79.15%, 79.89% and 75.65%, 
and a recall of 79.50%, 73.26% and 66.29% 
respectively.  
Thus, conveniently defined morphemes in 
Chinese enhance the effectiveness of the algorithm 
by simplifying processing and yielding better 
results even in a smaller corpus compared with 
what Turney (2003) used. Just 6 to 10 morphemes 
can give satisfactory results in a smaller corpus. 
The efficient application of Turney?s algorithm 
with help of colossal corpus like hundred-billion-
word corpus is matched by the ready availability of 
internet texts. However, the same convenience is 
not available to Chinese because of the heavy cost 
of word segmentation. 
The efficient application of Turney?s algorithm 
with help of colossal corpus like hundred-billion-
word corpus is matched by the ready availability of 
internet texts. However, the same convenience is 
not available to Chinese because of the heavy cost 
of word segmentation. 
In our experiment, all syntactic markers are 
ignored. Better results can be expected if syntactic 
markers are taken into consideration. An obvious 
example is negation (not, never) which can 
counteract the polarity of a word. In future, we will 
try to handle negation and other syntactic markers. 
The lists of the probability of morphemes 
forming polarized words in section 5.2 can be 
handled by the concept of decision list (Yarowsky, 
2000) which has not been applied in this paper for 
simplification. In the future, decision lists can be 
employed to systematically include the loaded 
features of morphemes. 
The experiment can be conducted with different 
sets of paradigm morphemes, and on corpora of 
different sizes. With the LIVAC synchronous 
corpus (Tsou et al, 2000), it should be possible to 
compare the SO of some words in different 
communities like Beijing, Hong Kong and Taipei. 
The data would be valuable for cultural studies if 
the SO of some words fluctuates in different 
communities.  
SO from association can be also applied to the 
judgment of news articles like editorials on 
celebrities. Given a celebrity name or organization 
name, we can calculate, using SO-PMI, the 
strength of SO of the ?given word?, i.e., the name. 
Then we would be able to tell whether the news 
about the target is positive or negative. For 
example, we tried to calculate the SO-PMI of the 
name ?George W Bush?, the U.S. President, with 
thousands of polarized Chinese words in the 
corpus, it was found that the SO-PMI of ?Bush? 
was about -200 from January to February, 2003, 
and plunged to -500 from March to April, 2003, 
when U.S. launched an ?unauthorized war? against 
Iraq. Such useful applications will be further 
investigated in future. 
References  
DANIELSSON, P. 2003. Automatic Extraction of 
Meaningful Units from Corpora. International 
Journal of Corpus Linguistics, 8(1), 109-127. 
GUO XIAN-ZHEN, ZHANG WEI, LIU JIN, WANG 
LING-LING. 1999. ChangYong BaoBianYi CiYu 
XiangJie CiDian ( ). 
Commercial Press, Beijing.  
HATZIVASSILOGLOU, V., AND MCKEOWN, K.R. 
1997. Predicting the Semantic Orientation of 
Adjectives. Proceedings of the 35th Annual Meeting 
of the Association for Computational Linguistics and 
the 8th Conference of the European Chapter of the 
ACL, Madrid, Spain, 174-181. 
HATZIVASSILOGLOU, V. AND WIEBE, J.M. 2000. 
Effects of Adjective Orientation and Grad-ability on 
Sentence Subjectivity. Proceedings of 18th 
International Conference on Computational 
Linguistics (Coling?00), Saarbr?cken, Germany. 
LEHRER, A. 1974. Semantic Fields and Lexical 
Structure. North Holland, Amsterdam and New York. 
STONE, P.J., DUNPHY, D. C., SMITH, M. S., AND 
OGILVIE, D. M. 1966. The General Inquirer: A 
Computer Approach to Content Analysis. MIT Press, 
Cambridge, MA. 
TSOU, B.K. 1976. Homophony and Internal Change in 
Chinese. Computational Analyses of Asian and 
African Languages 3, 67-86. 
TSOU, B.K., TSOI, W.F., LAI, T.B.Y., HU, J. AND 
CHAN, S.W.K. 2000. LIVAC, A Chinese 
Synchronous Corpus, and Some Applications. 
Proceedings of the ICCLC International Conference 
on Chinese Language Computing. Chicago, 233-238. 
TURNEY, P.D. 2002. Thumbs up or Thumbs down? 
Semantic Orientation Applied to Unsupervised 
Classification of Reviews. Proceedings of the 
Association for Computational Linguistics 40th 
Anniversary Meeting, University of Pennsylvania, 
Philadelphia, PA, USA. 
TURNEY, P.D. & LITTMAN, M.L. 2003. Measuring 
Praise and Criticism: Inference of Semantic 
Orientation from Association. ACM Transactions on 
Information System (TOIS), 21(4), pp315-346. 
WANG GUO-ZHANG. 2001. A Dictionary of Chinese 
Praise and Blame Words (
). Sinolingua, Beijing.  
WIEBE, J.M., BRUCE, R., BELL, M. MARTIN, M., 
AND WILSON, T. 2001. A Corpus Study of 
Evaluative and Speculative Language. Proceedings 
of the Second ACL SIG on Dialogue Work-shop on 
Discourse and Dialogue. Aalborg, Denmark.  
YAROWSKY, D. 2000. Hierarchical Decision Lists for 
Word Sense Disambiguation. Computers and the 
Humanities, 34(1-2). 
 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 457?464
Manchester, August 2008
Extending a Thesaurus with Words from Pan-Chinese Sources 
Oi Yee Kwong?? and Benjamin K. Tsou? 
?Department of Chinese, Translation and Linguistics 
?Language Information Sciences Research Centre 
City University of Hong Kong 
Tat Chee Avenue, Kowloon, Hong Kong 
{rlolivia, rlbtsou}@cityu.edu.hk 
 Abstract 
In this paper, we work on extending a 
Chinese thesaurus with words distinctly 
used in various Chinese communities.  
The acquisition and classification of such 
region-specific lexical items is an impor-
tant step toward the larger goal of con-
structing a Pan-Chinese lexical resource.  
In particular, we extend a previous study 
in three respects: (1) to improve auto-
matic classification by removing dupli-
cated words from the thesaurus, (2) to 
experiment with classifying words at the 
subclass level and semantic head level, 
and (3) to further investigate the possible 
effects of data heterogeneity between the 
region-specific words and words in the 
thesaurus on classification performance.  
Automatic classification was based on 
the similarity between a target word and 
individual categories of words in the the-
saurus, measured by the cosine function.  
Experiments were done on 120 target 
words from four regions.  The automatic 
classification results were evaluated 
against a gold standard obtained from 
human judgements.  In general accuracy 
reached 80% or more with the top 10 (out 
of 80+) and top 100 (out of 1,300+) can-
didates considered at the subclass level 
and semantic head level respectively, 
provided that the appropriate data sources 
were used. 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
 
1 Introduction 
A unique problem in Chinese language process-
ing arises from the extensive lexical variations 
among major Chinese speech communities.  Al-
though different communities (e.g. Beijing, Hong 
Kong, Taipei and Singapore) often share a large 
core lexicon, lexical variations could occur in at 
least two ways.  On the one hand, even the same 
word forms shared by various communities could 
be used with different meanings.  For instance, 
the word?? (ju1wu1)1 refers to general hous-
ing in Mainland China but specifically to housing 
under the Home Ownership Scheme in Hong 
Kong.  On the other hand, there are substantially 
different lexical items used for lexicalizing 
common or region-specific concepts.  For exam-
ple, while the word ?? (zhu4fang2) is similarly 
used as ??  to mean general housing in 
Mainland China, it is rarely seen in the Hong 
Kong context; and ?? (xia4gang3) is specific, 
if not exclusive, to Mainland China for referring 
to a special concept of unemployment. 
Existing Chinese lexical resources are often 
based on language use in one particular region 
and are therefore not comprehensive enough to 
capture the substantial regional variation as an 
important part of the lexical knowledge, which 
will be useful and critical for many NLP applica-
tions, including natural language understanding, 
information retrieval, and machine translation. 
Tsou and Kwong (2006) proposed a compre-
hensive Pan-Chinese lexical resource, using a 
large and unique synchronous Chinese corpus as 
an authentic source of lexical variation among 
various Chinese speech communities.  They also 
studied the feasibility of taking an existing Chi-
nese thesaurus as leverage and classifying new 
words from various Chinese communities with 
respect to the classificatory structure therein 
(Kwong and Tsou, 2007).  They used the catego-
                                                 
1 The transcriptions in brackets are based on Hanyu Pinyin. 
457
ries at the subclass level of the Tongyici Cilin (?
????, abbreviated as Cilin hereafter) for the 
task.  The classification was done by comparing 
the similarity of a target word (i.e. the word to be 
classified) and individual categories of words in 
the thesaurus based on a feature vector of co-
occurring words in a corpus.  Since words in the 
thesaurus are mostly based on lexical items used 
in Mainland China, and the target words come 
from various Chinese communities, a major issue 
in the classification task is thus the heterogeneity 
of the data sources.  It was hypothesized that the 
datasets from which the features were extracted 
(for the target words and words in the thesaurus 
respectively) may affect the performance of 
automatic classification.  The experimental re-
sults supported the hypothesis in part, and the 
actual effect varied with datasets from individual 
regions.  Moreover, there is room to improve the 
overall accuracy for the method to be useful in 
practice, and it appears that the duplicated words 
in the thesaurus might have skewed the similarity 
measurement to a certain extent. 
The current study thus attempts to extend this 
previous study in three respects: (1) to improve 
automatic classification by removing duplicated 
words from the thesaurus, (2) to experiment with 
classifying words at the subclass level and se-
mantic head level (a finer level), and (3) to fur-
ther investigate the possible effects of data het-
erogeneity between the region-specific words 
and words in the thesaurus on classification per-
formance. 
In Section 2, we will briefly review related 
work and the background of the current study.  In 
Sections 3 and 4, we will describe the materials 
used and the experimental setup respectively.  
Results will be presented in Section 5 and dis-
cussed in Section 6, followed by a conclusion in 
Section 7. 
2 Related Work 
To build a semantic lexicon, one has to identify 
the relation between words within a semantic 
hierarchy, and to group similar words together 
into a class.  Previous work on automatic meth-
ods for building semantic lexicons could be di-
vided into two main groups.  One is automatic 
thesaurus acquisition, that is, to identify syno-
nyms or topically related words from corpora 
based on various measures of similarity (e.g. 
Riloff and Shepherd, 1997; Lin, 1998; Caraballo, 
1999; Thelen and Riloff, 2002; You and Chen, 
2006).  
Another line of research, which is more 
closely related to the current study, is to extend 
existing thesauri by classifying new words with 
respect to their given structures (e.g. Tokunaga et 
al., 1997; Pekar, 2004).  An early effort along 
this line is Hearst (1992), who attempted to iden-
tify hyponyms from large text corpora, based on 
a set of lexico-syntactic patterns, to augment and 
critique the content of WordNet.  Ciaramita 
(2002) compared several models in classifying 
nouns with respect to a simplified version of 
WordNet and signified the gain in performance 
with morphological features.  For Chinese, Tseng 
(2003) proposed a method based on morphologi-
cal similarity to assign a Cilin category to un-
known words from the Sinica corpus which were 
not in the Chinese Electronic Dictionary and 
Cilin; but somehow the test data were taken from 
Cilin, and therefore could not really demonstrate 
the effectiveness with unknown words found in 
the Sinica corpus. 
Kwong and Tsou (2007) attempted to classify 
words distinctly used in Beijing, Hong Kong, 
Singapore, and Taiwan, with respect to the Cilin 
classificatory structure.  They brought up the is-
sue of data heterogeneity in the task.  In general, 
automatic classification of words via similarity 
measurement between two words, or between a 
word and a class of words, was often done on 
words from a similar data source, with the as-
sumption that the feature vectors under compari-
son are directly comparable.  In the Pan-Chinese 
context, however, the words to be classified 
come from corpora collected from various Chi-
nese speech communities, but the words in the 
thesaurus are often based on usages found in a 
particular community, such as Mainland China in 
the case of Cilin.  It is thus questionable whether 
the words in Cilin would appear in comparable 
contexts in texts from other places, thus affecting 
the similarity measurement.  In view of this het-
erogeneous nature of the data, they experimented 
with extracting feature vectors for the Cilin 
words from different datasets and found that the 
classification of words from Taipei was most 
affected in this regard. 
In general, up to 85% accuracy was reached 
with the top 15 candidates for classification at 
the Cilin subclass level.  This performance, how-
ever, should be improved for the method to be 
useful in practice.  It is observed that Cilin, as 
most other thesauri, does not have a mutually 
exclusive classification.  Many words appear in 
more than one category (at various levels).  Such 
duplication may affect the similarity comparison 
458
between a target word and words in a category.  
The current study thus attempts to avoid this con-
founding factor by removing duplicated words 
from Cilin for the comparison of similarity, and 
to extend the classification to a finer level. 
3 Materials 
3.1 The Tongyici Cilin 
The Tongyici Cilin (?????) (Mei et al, 
1984) is a Chinese synonym dictionary, or more 
often known as a Chinese thesaurus in the tradi-
tion of the Roget?s Thesaurus for English.  The 
Roget?s Thesaurus has about 1,000 numbered 
semantic heads, more generally grouped under 
higher level semantic classes and subclasses, and 
more specifically differentiated into paragraphs 
and semicolon-separated word groups.  Similarly, 
some 70,000 Chinese lexical items are organized 
into a hierarchy of broad conceptual categories in 
Cilin.  Its classification consists of 12 top-level 
semantic classes, 94 subclasses, 1,428 semantic 
heads and 3,925 paragraphs.  It was first pub-
lished in the 1980s and was based on lexical us-
ages mostly of post-1949 Mainland China.  In the 
current study, we will focus on the subclass level 
and semantic head level.  Some example sub-
classes and semantic heads are shown in Table 1. 
We classify words with respect to the subclass 
level and semantic head level (that is, second and 
third levels in the Cilin organisation).  Moreover, 
we skip class K and class L as the former con-
tains mostly function words and the latter longer 
expressions.  We are thus considering 88 sub-
classes and 1,356 semantic heads in this study. 
Within classes A to J, there are 7,517 words 
which were found to appear in more than one 
category.  Upon removing these entries, 44,588 
words were used in the similarity comparison for 
the current study.  
 
 
Class Subclasses Semantic Heads 
A ? (Human) Aa ? Ae ?? (Occupation)  Af?? 
(Identity) ? An 
Aa01 ? Ae10 ?? ?? ?? ?? (com-
mander, soldier) ? An07 
B ? (Things) Ba ? Bb ??? (Shape) ? Bi ?? 
(Animal)? Bm ?? (Material)?Bq ?? 
(Clothing) ? Br 
Ba01 ? Bm08 ? ? (coal, carbon) ? Bn03 
?? (room) ? Br14 
C ????? (Time and 
Space) 
Ca ?? (Time)  Cb ?? (Space) Ca01 ? Ca18 ? (year) ? Cb28 ?? (loca-
tion) ? Cb30 
D ???? 
(Abstract entities) 
Da ?? ?? (Condition) ? Df ?? (Ide-
ology) ? Di ?? ?? (Society) Dj ?? 
(Economics) ? Dm ?? (Organization) 
Dn ?? ?? (Quantity) 
Da01 ? Di10 ?? ?? (group, party) ? 
Dj04 ?? ?? ?? ?? (capital, interest)  
Dj05 ?? ?? (currency, invoice) ? Dm01 
?? (government) ? Dn10 
E ?? 
(Characteristics) 
Ea ? Ed ?? (Property)? Ef Ea01 ? Ed03 ? ? (goodness, badness) ? 
Ef14 
F ?? (Action) Fa ? Fd ???? (Body action) Fa01 ? Fb01 ? ? (run) ? Fd09 
G ???? (Psycho-
logical activities) 
Ga ? Gb ???? (Psychological activi-
ties)? Gc 
Ga01 ? Gb01 ?? ?? ?? ?? (imag-
ine, think) ? Gc04 
H ?? 
(Activities) 
Ha ? He ???? (Economic activi-
ties) ? Hd ?? (Production) ? Hf ??
?? (Transportation) Hg ???? (Scien-
tific research)? Hi ?? (Social contact) 
Hj ?? (Livelihood) 
Ha01 ? Hc09 ?? ?? ?? ?? ?? (in 
charge, administer, lead) ? He03 ? ? (buy, 
sell) ? Hg01 ?? ?? ?? (teach, 
demo) ? Hj12 ? ?? ?? ?? (do, coop-
erate, try) ? Hn13 
I ????? (Phe-
nomenon and state) 
Ia ? If ?? (Circumstance)  Ig ?? 
(Process)? Ih 
Ia01 ? Ig01 ?? ?? (begin, end) ? Ih05 
?? ?? ?? (increase, decrease) ? Ih13 
J ?? 
(Association) 
Ja ?? (Liaison)  Jb ?? (Similarity and 
Difference) Jc ?? (Matching) ? Je 
Ja01 ? Jc01 ?? ?? ?? (adapt, 
match) ? Je14 
Table 1  Some Examples of Cilin Subclasses and Semantic Heads 
 
3.2 The LIVAC Synchronous Corpus 
LIVAC (http://www.livac.org) stands for Lin-
guistic Variation in Chinese Speech Communi-
ties.  It is a synchronous corpus developed and 
dynamically maintained by the Language Infor-
mation Sciences Research Centre of the City 
University of Hong Kong since 1995 (Tsou and 
Lai, 2003).  The corpus consists of newspaper 
articles collected regularly and synchronously 
from six Chinese speech communities, namely 
Hong Kong, Beijing, Taipei, Singapore, Shang-
459
hai, and Macau.  Texts collected cover a variety 
of domains, including front page news stories, 
local news, international news, editorials, sports 
news, entertainment news, and financial news.  
Up to December 2007, the corpus has already 
accumulated over 250 million character tokens 
which, upon automatic word segmentation and 
manual verification, yielded about 1.2 million 
word types. 
For the present study, we made use of subcor-
pora consisting of the financial news sections 
collected over the 9-year period 1995-2004 from 
Beijing (BJ), Hong Kong (HK), Singapore (SG), 
and Taipei (TW).  Table 2 shows the sizes of the 
subcorpora. 
 
Region Size of Financial Subcorpus 
(rounded to nearest 1K) 
 Word Token Word Type 
BJ 232K 20K 
HK 970K 38K 
SG 621K 28K 
TW 254K 22K 
Table 2  Sizes of Individual Subcorpora 
 
3.3 Test Data 
Kwong and Tsou (2006) observed that among the 
unique lexical items found from the individual 
subcorpora, only about 30-40% are covered by 
Cilin, but not necessarily in the expected senses.  
In other words, Cilin could in fact be enriched 
with over 60% of the unique items from various 
regions. 
In the current study, we sampled the most fre-
quent 30 words distinctly and predominantly 
used in each of the BJ, HK, SG, and TW subcor-
pus.  Classification was based on their similarity 
with each of the Cilin subclasses and semantic 
heads, compared by the cosine measure, as dis-
cussed in Section 4.2. 
4 Experiments 
4.1 Setting the Gold Standard 
Three linguistics undergraduate students and one 
research student on computational linguistics 
from the City University of Hong Kong were 
asked to assign what they would consider to be 
the most appropriate Cilin category (at the sub-
class and semantic head level) to each of the 120 
target words. 
All human judges reported difficulties in vari-
ous degrees in assigning Cilin categories to the 
target words.  The major problem came from the 
regional specificity and thus the unfamiliarity of 
the judges with the respective lexical items and 
contexts.  For example, all judges reported prob-
lem with the term ?? (zi4cuo1), one of the tar-
get words from Singapore referring to ???? 
(zi4cuo1gu3shi4, CLOB in the Singaporean 
stock market), which is specific to Singapore. 
Notwithstanding the difficulty, the inter-
annotator agreement, as measured by Kappa, was 
found to be 0.6870 at the subclass level and 
0.5971 at the semantic head level. 
We took a ?loose? approach to form the gold 
standard, which includes all categories (at the 
subclass level and semantic head level respec-
tively) assigned by one or more judges.  Auto-
matic classification will be considered ?correct? 
if any of these categories is matched. 
4.2 Automatic Classification 
Each target word was compared to all Cilin cate-
gories and automatically classified to the cate-
gory which is most similar to it.  The Cilin data 
was first pre-processed to remove duplicated 
words. 
We compute the similarity by the cosine be-
tween the two corresponding feature vectors con-
taining all co-occurring content words in a cor-
pus within a window of ?5 words (excluding 
many general adjectives and adverbs, and num-
bers and proper names were all ignored).  The 
feature vector of a Cilin category is based on the 
union of the features from all individual mem-
bers in the category. 
The cosine of two feature vectors vv  and wv  is 
computed as 
 
wv
wvwv vv
vvvv ?=),cos(  
 
The feature vector of a given target word is 
extracted from the respective subcorpus from 
which the target word was found (called the tar-
get subcorpus hereafter).  To study the data het-
erogeneity effect, we experimented with two 
conditions for the extraction of feature vectors 
for Cilin words: from the target subcorpus or 
from the BJ subcorpus which is assumed to be 
representative of usages in Mainland China. 
All automatic classification results were 
evaluated against the gold standard based on hu-
460
Sub-tw-tw
0
10
20
30
40
50
60
70
80
90
100
0 5 10 15
Top N Candidates
Ac
cu
ra
cy
 (%
)
Baseline With Duplicates No Duplicates
Sub-hk-hk
0
10
20
30
40
50
60
70
80
90
100
0 5 10 15
Top N Candidates
Ac
cu
ra
cy
 (%
)
Baseline With Duplicates No Duplicates
Sub-sg-sg
0
10
20
30
40
50
60
70
80
90
100
0 5 10 15
Top N Candidates
Ac
cu
ra
cy
 (%
)
Baseline With Duplicates No Duplicates
Sub-bj-bj
0
10
20
30
40
50
60
70
80
90
100
0 5 10 15
Top N Candidates
Ac
cu
ra
cy
 (%
)
Baseline With Duplicates No Duplicates
Sub-bj-bj
0
10
20
30
40
50
60
70
80
90
100
0 5 10 15
Top N Candidates
Ac
cu
ra
cy
 (%
)
Baseline Sub-bj-bj
Sub-tw-tw and Sub-tw-bj
0
10
20
30
40
50
60
70
80
90
100
0 5 10 15
Top N Candidates
Ac
cu
ra
cy
 (%
)
Baseline Sub-tw-tw Sub-tw-bj
Sub-hk-hk and Sub-hk-bj
0
10
20
30
40
50
60
70
80
90
100
0 5 10 15
Top N Candidates
Ac
cu
ra
cy
 (%
)
Baseline Sub-hk-hk Sub-hk-bj
Sub-sg-sg and Sub-sg-bj
0
10
20
30
40
50
60
70
80
90
100
0 5 10 15
Top N Candidates
Ac
cu
ra
cy
 (%
)
Baseline Sub-sg-sg Sub-sg-bj
man judgements as discussed in Section 4.1.  
Classification performance is measured based on 
the correctness of the top N candidates. 
4.3 Baseline 
A simple baseline measure was obtained by rank-
ing the subclasses in descending order of the 
number of words they cover.  It was assumed 
that the bigger the subclass size, the more likely 
it covers a new term.  The top N candidates in 
this ranking were checked against the gold stan-
dard as above. 
5 Results 
In the following discussion, we will use labels in 
the form of <Cat>-<Target>-<CilinFeatSource> 
to refer to the various testing conditions, where 
Cat refers to the category type, Target to the 
originating source of the target words, and Cilin-
FeatSource to the source from which the feature 
vectors for the Cilin words were extracted.  Thus 
the label Sub-hk-hk means classification of HK 
target words at the Cilin subclass level, with fea-
ture vectors for target words and Cilin words ex-
tracted from the HK subcorpus; and the label 
Head-tw-bj means classification of TW target 
words at the Cilin semantic head level, with fea-
ture vectors for the target words extracted from 
the TW subcorpus and those for the Cilin words 
extracted from the BJ subcorpus. 
5.1 Pre-processing of Cilin 
Figure 1 shows the comparison of classification 
accuracy for words from the four regions at the 
subclass level before and after duplicates in Cilin 
were removed.  All feature vectors were ex-
tracted from the respective target corpora. 
 
Figure 1  Effect of Pre-processing Cilin 
It can be seen from Figure 1 that removing 
duplicated words in Cilin could improve the clas-
sification of words from all regions at the sub-
class level. 
5.2 Data Heterogeneity Effect 
As explained earlier, since the words to be classi-
fied come from various Chinese speech commu-
nities, but the words in Cilin are mostly based on 
usages found in Mainland China, it is uncertain 
whether the words in Cilin would appear in com-
parable contexts in texts from other places, for 
the similarity measurement to be effective.  
Hence, we experimented with two conditions for 
extracting feature vectors for the Cilin words.  
While the features for a target word to be classi-
fied are extracted from the respective target sub-
corpus, the features for the Cilin words are ex-
tracted either from the target subcorpus or from 
the BJ subcorpus.  Figure 2 shows the data het-
erogeneity effect on the classification of target 
words from various regions at the subclass level.  
 
Figure 2  Data Heterogeneity Effect 
 
The data heterogeneity effect is most notice-
able for the TW words.  Extracting features for 
the Cilin words from the BJ subcorpus always 
gives better classification results for the TW 
words, than if the features were extracted from 
the TW subcorpus.  The difference between Sub-
hk-hk and Sub-hk-bj, and that between Sub-sg-sg 
and Sub-sg-bj, however, is not as great.  This 
suggests that the lexical difference is particularly 
significant between BJ and TW. 
5.3 Fine-grainedness of Classification 
The semantic head level is more fine-grained 
than the subclass level, and is expected to be 
461
Head-bj-bj
0
10
20
30
40
50
60
70
80
90
100
0 10 20 30 40 50 60 70 80 90 100 110
Top N Candidates
Ac
cu
ra
cy
 (%
)
Head-bj-bj
Head-tw-tw and Head-tw-bj
0
10
20
30
40
50
60
70
80
90
100
0 10 20 30 40 50 60 70 80 90 100 110
Top N Candidates
Ac
cu
ra
cy
 (%
)
Head-tw-tw Head-tw-bj
Head-hk-hk and Head-hk-bj
0
10
20
30
40
50
60
70
80
90
100
0 10 20 30 40 50 60 70 80 90 100 110
Top N Candidates
Ac
cu
ra
cy
 (%
)
Head-hk-hk Head-hk-bj
Head-sg-sg and Head-sg-bj
0
10
20
30
40
50
60
70
80
90
100
0 10 20 30 40 50 60 70 80 90 100 110
Top N Candidates
Ac
cu
ra
cy
 (%
)
Head-sg-sg Head-sg-bj
more difficult for classification.  Figure 3 shows 
the results of classification at the semantic head 
level, with the effect of data heterogeneity. 
 
Figure 3  Semantic Head Level Classification 
 
It is observed from Figures 2 and 3 that data 
heterogeneity affects the classification of TW 
words at both the subclass and semantic head 
level.  In both cases, features for Cilin words ex-
tracted from BJ subcorpus work better than those 
from TW subcorpus.  A somewhat opposite ef-
fect was observed for SG target words, especially 
beyond the top 5 to 10 candidates.  There is not 
much difference for the HK target words. 
The classification at the semantic head level is 
expectedly less precise than that at the subclass 
level.  At the subclass level, 80% or more accu-
racy could be reached with the top 10 candidates 
considered, whereas the top 50 candidates or 
more would be needed to reach a similar level of 
accuracy at the semantic head level.  This is nev-
ertheless encouraging in view of the total number 
of categories at the semantic head level. 
6 Discussions 
6.1 Overall Classification Accuracy 
From the results reported in the last section, it 
can be seen that removing the duplicated words 
in Cilin could help improve the classification 
accuracy at all conditions.  This is because some 
words, which appear in more than one category 
at the subclass or semantic head level, might 
skew the similarity measured between a target 
word and a given category.  An example will be 
discussed in Section 6.3. 
In general, the top 10 candidates could lead to 
over 80% accuracy at the subclass level (much 
improved from previous results before removing 
duplicates in Cilin, where it usually took the top 
15 candidates to reach about 80% accuracy).  At 
the semantic head level, the top 50 candidates 
could lead to over 70% accuracy for HK and TW 
words and to 80% or more for BJ and SG words.  
The accuracy, nevertheless, is also dependent on 
the datasets from which features were extracted, 
as shown in Sections 5.2 and 5.3 above and fur-
ther discussed below. 
6.2 Regional Variation 
The various Chinese speech communities might 
differ not only in the lexical items they use, but 
also in the way they use the lexical items in 
common.  The demand on cross-cultural knowl-
edge thus poses a challenge for building a Pan-
Chinese lexical resource manually.  Cilin, for 
instance, is quite biased in language use in 
Mainland China, and it requires experts with 
knowledge of a wide variety of Chinese terms to 
be able to manually classify lexical items specific 
to other Chinese speech communities.  It is there-
fore even more important to devise robust ways 
for automatic classification of words from vari-
ous regions. 
The data heterogeneity effect is quite different 
for the classification of SG words and TW words, 
but apparently not very significant for HK words.  
Beyond the top 5 to 10 candidates, features ex-
tracted from the SG subcorpus for Cilin words 
seem to have an advantage.  This suggests that 
although the SG subcorpus shares those words in 
Cilin, the context in which they are used might 
be slightly different from their use in Mainland 
China.  Thus extracting their contextual features 
from the SG subcorpus might better reflect their 
usage and make them more comparable with the 
target words from SG.  For the TW words, on the 
contrary, features for Cilin words extracted from 
the BJ subcorpus always have an advantage over 
those extracted from the TW subcorpus.  As 
Kwong and Tsou (2006) observed, Beijing and 
Taipei data share the least number of lexical 
items among the four regions under investigation.  
Words in Cilin therefore might not have the ap-
propriate contextual feature vectors extracted 
from the TW subcorpus. 
6.3 Analysis for Individual Words 
In order to study the actual effect of various ex-
perimental conditions on the classification of 
individual target words, we also worked out the 
change in the ranking (?r) of the correct category 
for each target word.  A negative ?r thus corre-
462
sponds to an improvement in the classification as 
the new ranking of the correct category is smaller 
(earlier) than the old one.  Table 3 shows some 
examples with improvement in this regard.  The 
Rank column refers to the rank of the correct 
category in Sub-{bj,hk,tw,sg}-bj, ?r(D) is the 
change after duplicates were removed from Cilin, 
and ?r(H) is the change from Sub-x-x conditions. 
 
No. Word (Region) Rank ?r(D) ?r(H) 
1 ??? (BJ) 1 -6 - 
2 ??? (BJ) 3 -15 - 
3 ?? (BJ) 1 -1 - 
4 ?? (BJ) 1 -2 - 
5 ?? (BJ) 3 -2 - 
6 ?? (HK) 4 -4 -6 
7 ?? (HK) 2 -3 -6 
8 ??? (HK) 1 -6 -1 
9 ?? (HK) 1 -8 -5 
10 ??  (HK) 2 -4 -5 
11 ?? (SG) 1 -3 -4 
12 ?? (SG) 1 -12 -1 
13 ??? (SG) 1 -3 -1 
14 ??? (SG) 2 -2 -4 
15 ??? (SG) 2 -7 1 
16 ?? (TW) 3 -1 -13 
17 ??? (TW) 2 -3 -7 
18 ?? (TW) 1 0 -3 
19 ?? (TW) 6 -3 -20 
20 ?? (TW) 3 -6 -5 
Table 3  Ranking Change for Individual Words2 
 
Take the example of the BJ target word ??
? (xin4xi1hua4, informationize).  Before dupli-
cated words were removed from Cilin, the most 
appropriate subclass (Ih:Change) ranked 7th in 
the automatic classification.  Upon the removal 
of duplicated words, subclass Ih ranked first in 
the results.  The words shared by other top rank-
ing subclasses (e.g. Je:influence, Da:condition, 
etc.) such as ? (jia1, increase), ? (tui1, push), 
?? (ti2gao1, raise), etc., may have skewed the 
similarity comparison by introducing many 
common co-occurring words which are not par-
ticularly characteristic of any subclass. 
For the TW target word ?? (tou2xin4, in-
vestment trust), the appropriate subclass 
                                                 
2 English gloss: 1-informationize, 2-re-employed, 3-
unemployed, 4-resist drought, 5-quality check, 6-general 
trend of stock market, 7-buy in stocks, 8-H stock, 9-interest 
rate, 10-sell (stocks), 11-Singaporean dollar, 12-Malaysian 
stocks, 13-closing price, 14-rights issue, 15-holding space 
rate, 16-investment trust, 17-growth rate, 18-financial hold-
ings, 19-over-bought, 20-bank. 
(Dm:organization) soared from the 16th to the 
3rd when features were extracted for the Cilin 
words from the BJ subcorpus instead of the TW 
subcorpus.  It was observed that both vectors 
have a large part in common, but the one ex-
tracted from TW subcorpus contained many 
more spurious features which might not be char-
acteristic of the subclass, thus affecting the simi-
larity score. 
It is also apparent that region-specific but 
common concepts like ??? (xie3zi4lou2, of-
fice), ??  (zu3wu1, apartment), and ?? 
(si1zhai2, private residence), etc., are more ad-
versely affected when features for Cilin words 
were extracted from the BJ subcorpus instead of 
the respective target subcorpora, while other 
more core financial concepts could often take 
advantage of the former.  Thus it appears that the 
domain and concept specificity could also affect 
the effectiveness of the method. 
6.4 Future Directions 
There is room to improve the results at both the 
subclass and semantic head level.  More qualita-
tive analysis is needed for the data heterogeneity 
effect.  The category size, and as pointed out 
above, the domain and concept specificity are 
also worth further investigation.  The latter will 
thus involve the classification of words from 
other special domains like sports, as well as those 
from the general domain. 
One problem we need to address in the next 
step is the class imbalance problem as Cilin cate-
gories could differ considerably in size, which 
will affect the number of features and subsequent 
classification.  For this we plan to try the k near-
est neighbours approach.  In addition, the fea-
tures might need to be constrained, as simple co-
occurrence might be too coarse for distinguishing 
the subtle characteristics among Cilin categories. 
7 Conclusion 
We have worked on extending a Chinese thesau-
rus with words distinctly used in various Chinese 
communities.  Classification results have im-
proved as duplicated words in Cilin were re-
moved.  In view of the demand on cross-cultural 
knowledge for building a Pan-Chinese lexical 
resource manually, it is particularly important to 
devise robust ways for automatic acquisition of 
such a resource.  Automatic classification of 
words with respect to an existing classificatory 
structure with proper datasets for feature extrac-
tion should be a prominent direction in this re-
463
gard.  Further investigation is needed to better 
understand the interaction among data heteroge-
neity, category size, feature selection, and the 
domain and concept specificity of the words. 
Acknowledgements 
The work described in this paper was supported 
by a grant from the Research Grants Council of 
the Hong Kong Special Administrative Region, 
China (Project No. CityU 1317/03H).  The au-
thors would like to thank Jingbo Zhu for useful 
discussions on an earlier draft of this paper, and 
the anonymous reviewers for their comments on 
the submission. 
References 
Caraballo, S.A. (1999)  Automatic construction of a 
hypernym-labeled noun hierarchy from text.  In 
Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics (ACL?99), 
Maryland, USA, pp.120-126. 
Ciaramita, M. (2002)  Boosting automatic lexical ac-
quisition with morphological information.  In Pro-
ceedings of the ACL?02 Workshop on Unsuper-
vised Lexical Acquisition, Philadelphia, USA, 
pp.17-25. 
Hearst, M. (1992)  Automatic Acquisition of Hypo-
nyms from Large Text Corpora.  In Proceedings of 
the 14th International Conference on Computa-
tional Linguistics (COLING-92), Nantes, France, 
pp.539-545. 
Kwong, O.Y. and Tsou, B.K. (2006)  Feasibility of 
Enriching a Chinese Synonym Dictionary with a 
Synchronous Chinese Corpus.  In T. Salakoski, F. 
Ginter, S. Pyysalo and T. Pahikkala (Eds.), Ad-
vances in Natural Language Processing: Proceed-
ings of FinTAL 2006. Lecture Notes in Artificial 
Intelligence, Vol.4139, pp.322-332, Springer-
Verlag. 
Kwong, O.Y. and Tsou, B.K. (2007)  Extending a 
Thesaurus in the Pan-Chinese Context.  In Pro-
ceedings of the 2007 Joint Conference on Empiri-
cal Methods in Natural Language Processing and 
Computational Natural Language Learning 
(EMNLP-CoNLL-2007), Prague, pp.325-333. 
Lin, D. (1998)  Automatic Retrieval and Clustering of 
Similar Words.  In Proceedings of the 36th Annual 
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on 
Computational Linguistics (COLING-ACL?98), 
Montreal, Canada, pp.768-774. 
Mei et al ??????????????? (1984)  
??????? (Tongyici Cilin).  ????? 
(Commercial Press) / ???????. 
Pekar, V. (2004)  Linguistic Preprocessing for Distri-
butional Classification of Words.  In Proceedings 
of the COLING2004 Workshop on Enhancing and 
Using Electronic Dictionaries, Geneva. 
Riloff, E. and Shepherd, J. (1997)  A corpus-based 
approach for building semantic lexicons.  In Pro-
ceedings of the Second Conference on Empirical 
Methods in Natural Language Processing, Provi-
dence, Rhode Island, pp.117-124. 
Thelen, M. and Riloff, E. (2002)  A Bootstrapping 
Method for Learning Semantic Lexicons using Ex-
traction Pattern Contexts.  In Proceedings of the 
2002 Conference on Empirical Methods in Natural 
Language Processing (EMNLP 2002), Philadelphia, 
USA. 
Tokunaga, T., Fujii, A., Iwayama, M., Sakurai, N. and 
Tanaka, H. (1997)  Extending a thesaurus by clas-
sifying words.  In Proceedings of the ACL Work-
shop on Automatic Information Extraction and 
Building of Lexical Semantic Resources for NLP 
Applications, Madrid, pp.16-21. 
Tseng, H. (2003)  Semantic Classification of Chinese 
Unknown Words.  In the Proceedings of the ACL-
2003 Student Research Workshop, Companion 
Volume to the Proceedings of the 41st Annual 
Meeting of the Association for Computational Lin-
guistics, Sapporo, Japan. 
Tsou, B.K. and Kwong, O.Y. (2006)  Toward a Pan-
Chinese Thesaurus.  In Proceedings of the Fifth In-
ternational Conference on Language Resources 
and Evaluation (LREC 2006), Genoa, Italy. 
Tsou, B.K. and Lai, T.B.Y. ??????? (2003)  
????????????.  In B. Xu, M. Sun 
and G. Jin ?????????? (Eds.), ???
???????????  (Issues in Chinese 
Language Processing).  ???????? , 
pp.147-165 
You, J-M. and Chen, K-J. (2006)  Improving Context 
Vector Models by Feature Clustering for Auto-
matic Thesaurus Construction.  In Proceedings of 
the Fifth SIGHAN Workshop on Chinese Language 
Processing, COLING-ACL 2006, Sydney, Austra-
lia, pp.1-8. 
 
 
464
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1137?1144
Manchester, August 2008
Active Learning with Sampling by Uncertainty and Density for Word 
Sense Disambiguation and Text Classification  
Jingbo Zhu  Huizhen Wang  Tianshun Yao 
Natural Language Processing Laboratory 
Northeastern University 
Shenyang, Liaoning, P.R.China 110004 
zhujingbo@mail.neu.edu.cn 
wanghuizhen@mail.neu.edu.cn 
Benjamin K Tsou 
Language Information Sciences 
Research Centre 
City University of Hong Kong 
HK, P.R.China 
rlbtsou@cityu.edu.hk 
 
Abstract 
This paper addresses two issues of active 
learning. Firstly, to solve a problem of 
uncertainty sampling that it often fails by 
selecting outliers, this paper presents a 
new selective sampling technique, sam-
pling by uncertainty and density (SUD), 
in which a k-Nearest-Neighbor-based 
density measure is adopted to determine 
whether an unlabeled example is an out-
lier. Secondly, a technique of sampling 
by clustering (SBC) is applied to build a 
representative initial training data set for 
active learning. Finally, we implement a 
new algorithm of active learning with 
SUD and SBC techniques. The experi-
mental results from three real-world data 
sets show that our method outperforms 
competing methods, particularly at the 
early stages of active learning.  
1 Introduction 
Creating a large labeled training corpus is expen-
sive and time-consuming in some real-world ap-
plications (e.g. word sense annotation), and is 
often a bottleneck to build a supervised classifier 
for a new application or domain. Our study aims 
to minimize the amount of human labeling ef-
forts required for a supervised classifier (e.g. for 
automated word sense disambiguation) to 
achieve a satisfactory performance by using ac-
tive learning.  
Among the techniques to solve the knowledge 
bottleneck problem, active learning is a widely 
used framework in which the learner has the abil-
ity to automatically select the most informative 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
unlabeled examples for human annotation. The 
ability of the active learner can be referred to as 
selective sampling. Uncertainty sampling (Lewis 
and Gale, 1994) is a popular selective sampling 
technique, and has been widely studied in natural 
language processing (NLP) applications such as 
word sense disambiguation (WSD) (Chen et al, 
2006; Chan and Ng, 2007), text classification 
(TC) (Lewis and Gale, 1994; Zhu et al, 2008), 
statistical syntactic parsing (Tang et al, 2002), 
and named entity recognition (Shen et al, 2004).  
Actually the motivation behind uncertainty 
sampling is to find some unlabeled examples 
near decision boundaries, and use them to clarify 
the position of decision boundaries. However, 
uncertainly sampling often fails by selecting out-
liers (Roy and McCallum, 2001; Tang et al, 
2002). These selected outliers (i.e. unlabeled ex-
amples) have high uncertainty, but can not pro-
vide much help to the learner. To solve the out-
lier problem, we proposed in this paper a new 
method, sampling by uncertainty and density 
(SUD), in which a K-Nearest-Neighbor-based 
density (KNN-density) measure is used to deter-
mine whether an unlabeled example is an outlier, 
and a combination strategy based on KNN-
density measure and uncertainty measure is de-
signed to select the most informative unlabeled 
examples for human annotation at each learning 
iteration.  
The second effort we made is to study how to 
build a representative initial training data set for 
active learning. We think building a more repre-
sentative initial training data set is very helpful 
for active learning. In previous studies on active 
learning, the initial training data set is generally 
generated at random, based on an assumption 
that random sampling will be likely to build the 
initial training set with same prior data distribu-
tion as that of whole corpus. However, this situa-
tion seldom occurs in real-world applications due 
to the small size of initial training set used. In 
1137
this paper, we utilize an approach, sampling by 
clustering (SBC), to selecting the most represen-
tative examples to form initial training data set 
for active learning. To do it, the whole unlabeled 
corpus should be first clustered into predefined 
number of clusters (i.e. the predefined size of the 
initial training data set). The example closest to 
the centroid of each cluster will be selected to 
augment initial training data set, which is consid-
ered as the most representative case.  
Finally, we describe an implementation of ac-
tive learning with SUD and SBC techniques. Ex-
perimental results of active learning for WSD 
and TC tasks show that our proposed method 
outperforms competing methods, particularly at 
the early stages of active learning process. It is 
noteworthy that these proposed techniques are 
easy to implement, and can be easily applied to 
several learners, such as Maximum Entropy 
(ME), na?ve Bayes (NB) and Support Vector 
Machines (SVMs). 
2 Active Learning Process 
In this work, we are interested in uncertainty 
sampling (Lewis and Gale, 1994) for pool-based 
active learning, in which an unlabeled example x 
with maximum uncertainty is selected for human 
annotation at each learning cycle. The maximum 
uncertainty implies that the current classifier (i.e. 
the learner) has the least confidence on its classi-
fication of this unlabeled example.  
Actually active learning is a two-stage process 
in which a small number of labeled samples and 
a large number of unlabeled examples are first 
collected in the initialization stage, and a closed-
loop stage of query and retraining is adopted.  
Procedure: Active Learning Process 
Input: initial small training set L, and pool of unla-
beled data set U 
Use L to train the initial classifier C  
Repeat 
1. Use the current classifier C to label all unla-
beled examples in U 
2. Use uncertainty sampling technique to select 
m2  most informative unlabeled examples, and 
ask oracle H for labeling 
3. Augment L with these m new examples, and 
remove them from U 
4. Use L to retrain the current classifier C 
Until the predefined stopping criterion SC is met. 
Figure 1. Active learning with uncertainty sam-
pling technique 
                                                 
2 A batch-based sample selection labels the top-m most 
informative unlabeled examples at each learning cycle to 
decrease the number times the learner is retrained. 
3 Uncertainty Measures 
In real-world applications, only limited size of 
training sample set can be provided to train a 
supervised classifier. Due to manual efforts in-
volved, such brings up a considerable issue: what 
is the best subset of examples to annotate. In the 
uncertainty sampling scheme, the unlabeled ex-
ample with maximum uncertainty is viewed as 
the most informative case. The key point of un-
certainty sampling is how to measure the uncer-
tainty of an unlabeled example x. 
3.1 Entropy Measure 
The well-known entropy is a popular uncertainty 
measurement widely used in previous studies on 
active learning (Tang et al, 2002; Chen et al 
2006; Zhu and Hovy, 2007): 
?
?
?=
Yy
xyPxyPxH )|(log)|()(             (1) 
where P(y|x) is the a posteriori probability. We 
denote the output class y?Y={y1, y2, ?, yk}. H is 
the uncertainty measurement function based on 
the entropy estimation of the classifier?s 
posterior distribution. 
In the following comparison experiments, the 
uncertainty sampling based on entropy criterion 
is considered as the baseline method, also called 
traditional uncertainty sampling.  
3.2 Density*Entropy Measure 
To analyze the outlier problem of traditional un-
certainty sampling, we first give an example to 
explain our motivation. 
 
Figure 2. An example of two points A and B with 
maximum uncertainty at the ith learning iteration 
 
As mentioned in Section 1, the motivation be-
hind uncertainty sampling is to find some unla-
beled examples near decision boundaries, and 
assume that these examples have the maximum 
uncertainty. Fig. 2 shows two unlabeled exam-
ples A and B with maximum uncertainty at the ith 
1138
learning cycle. Roughly speaking, there are three 
unlabeled examples near or similar to B, but, 
none for A. We think example B has higher rep-
resentativeness than example A, and A is likely 
to be an outlier. We think adding B to the train-
ing set will help the learner more than A.  
The motivation of our study is that we prefer 
not only the most informative example in terms 
of uncertainty measure, but also the most repre-
sentative example in terms of density measure. 
The density measure can be evaluated based on 
how many examples there are similar or near to it. 
An example with high density degree is less 
likely to be an outlier.  
In most real-world applications, because the 
scale of unlabeled corpus would be very large, 
Tang et al (2002) and Shen et al (2004) evalu-
ated the density of an example within a cluster. 
Unlike their work 3 , we adopt a new approach, 
called K-Nearest-Neighbor-based density (KNN-
density) measure, to evaluating the density of an 
unlabeled example x. Given a set of K (i.e. =20 
used in our experiments) most similar examples 
S(x)={s1, s2, ?, sK} of the example x,  the KNN-
density DS(.) of example x is defined as: 
K
sx
xDS xSs
i
i
?
?= )(
),cos(
)(                     (2) 
As discussed above, we prefer to select exam-
ples with maximum uncertainty and highest den-
sity for human annotation. We think getting their 
labels can help the learner greatly. To do it, we 
proposed a new method, sampling by uncertainty 
and density (SUD), in which entropy-based un-
certainty measure and KNN-density measure are 
considered simultaneously.  
In SUD scheme, a new uncertainty measure, 
called density*entropy measure4 , is defined as: 
)()()( xHxDSxDSH ?=                 (3) 
4 Initial Training Set Generation 
As shown in Fig. 1, only a small number of train-
ing samples are provided at the beginning of ac-
tive learning process. In previous studies on ac-
tive learning, the initial training set is generally 
generated by random sampling from the whole 
unlabeled corpus. However, random sampling 
technique can not guarantee selecting a most rep-
                                                 
3 We also tried their cluster-based density measure, but per-
formance was essentially degraded.  
4 We also tried other ways like ?*DS(x)+(1-?) H(x) 
measure used in previous studies, but it seems to be random. 
Actually it is very difficult to determine an appropriate? 
value for a specific task.  
resentative subset, because the size of initial 
training set is generally too small (e.g. 10). We 
think selecting some representative examples to 
form initial training set can help the active 
learner.  
In this section we utilize an approach, sam-
pling by clustering (SBC), to selecting the most 
representative examples to form initial training 
data set. In the SBC scheme, the whole unlabeled 
corpus has been first clustered into a predefined 
number of clusters (i.e. the predefined size of the 
initial training set). The example closest to the 
centroid of each cluster will be selected to aug-
ment initial training set, which is viewed as the 
most representative case.  
We use the K-means clustering algorithm 
(Duda and Hart, 1973) to cluster examples in the 
whole unlabeled corpus. In the following K-
means clustering algorithm, the traditional cosine 
measure is adopted to estimate the similarity be-
tween two examples, that is 
ji
ji
ji ww
ww
ww ?
?=),cos(                     (4) 
where wi and wj are the feature vectors of the ex-
amples i and j.  
To summarize the SBC-based initial training 
set generation algorithm, let U={U1, U2, ?, UN} 
be the set of unlabeled examples to be clustered, 
and k be the predefined size of initial training 
data set. In other words, SBC technique selects k 
most representative unlabeled examples from U 
to generate the initial training data set. The SBC-
based initial training set generation procedure is 
summarized as follows: 
SBC-based Initial Training Set Generation 
Input: U, k 
Phrase 1: Cluster the corpus U into k clusters 
? j(j=1,?,k) by using K-means clustering algo-
rithm as follows: 
1. Initialization. Randomly choosing k exam-
ples as the centroid ?j(j=1,?,k) for initial 
clusters ? j(j=1,?,k), respectively.  
2. Re-partition {U1, U2, ?, UN} into k clus-
ters ?  j(j=1,?,k), where 
}.),,cos(),cos(:{ jtUUU tijiij ??=? ??  
3. Re-estimate the centroid ?j for each clus-
ters ? j, that is: 
m
U
jiU
i
j
?
??=? , where m is the size of ?  j.
4. Repeat Step 2 and Step 3 until the algo-
rithm converges. 
1139
Phrase 2: Select the example uj closest to the 
centroid?j for each cluster j to augment ini-
tial training data set ?, where 
?
]},1[,),,cos(),cos(:{ kjUuUuu ijjijjj ???=? ??
Return?; 
 
The computation complexity of the K-means 
clustering algorithm is O(NdkT), where d is the 
number of features and T is the number of itera-
tions. In practice, we can define the stopping cri-
terion (i.e. shown in Step 4) of K-means cluster-
ing algorithm that relative change of the total 
distortion is smaller than a threshold.  
5 Active Learning with SUD and SBC  
Procedure: Active Learning with SUD and SBC 
Input: Pool of unlabeled data set U; k is the prede-
fined size of initial training data set 
Initialization.  
z Evaluate the density of each unlabeled example 
in terms of KNN-density measure; 
z Use SBC technique to generate the small initial 
training data set of size k. 
Use L to train the initial classifier C  
Repeat 
1. Use the current classifier C to label all unla-
beled examples in U 
2. Use uncertainty sampling technique in terms 
of density*entropy measure to select m most 
informative unlabeled examples, and ask ora-
cle H for labeling, namely SUD scheme.  
3. Augment L with these m new examples, and 
remove them from U 
4. Use L to retrain the current classifier C 
Until the predefined stopping criterion SC is met. 
Figure 3. Active learning with SUD and SBC  
 
Fig. 3 shows the algorithm of active learning 
with SUD and SBC techniques. Actually there 
are some variations. For example, if the initial 
training data set is generated by SBC, and en-
tropy-based uncertainty measure is used, it is 
active learning with SBC. Similarly, if the initial 
training data set is generated at random, and the 
density*entropy uncertainty measure is used, it is 
active learning with SUD. If both SBC and SUD 
techniques are not used, we call it (traditional) 
uncertainty sampling as baseline method.  
6 Evaluation 
In the following comparison experiments, we 
evaluate the effectiveness of various active learn-
ing methods for WSD and TC tasks on three pub-
licly available real-world data sets. 
6.1 Deficiency Measure 
To compare various active learning methods, 
deficiency is a statistic developed to compare 
performance of active learning methods globally 
across the learning curve, which has been used in 
previous studies (Schein and Unga, 2007). The 
deficiency measure can be defined as: 
?
?
=
=
?
?= n
t tn
n
t tn
n
REFaccREFacc
ALaccREFacc
REFALDef
1
1
))()((
))()((
),( (5) 
where acct is the average accuracy at tth learning 
iteration. REF is the baseline active learning 
method, and AL is the active learning variant of 
the learning algorithm of REF, e.g. active learn-
ing with SUD and SBC. n refers to the evaluation 
stopping points (i.e. the number of learned ex-
amples). Smaller deficiency value (i.e. <1.0) in-
dicates AL method is better than REF method. 
Conversely, a larger value (i.e. >1.0) indicates a 
negative result. 
In the following comparison experiments, we 
evaluate the effectiveness of six active learning 
methods, including random sampling (random), 
uncertainty sampling (uncertainty), SUD, ran-
dom sampling with SBC (random+SBC), uncer-
tainty sampling with SBC (uncertainty+SBC), 
and SUD with SBC (SUD+SBC). ?+SBC? indi-
cates initial training data set generated by SBC 
technique. Otherwise, initial training set is gen-
erated at random. To evaluate deficiency of each 
method, the REF method (i.e. the baseline 
method) defined in Equation (5) refers to (tradi-
tional) uncertainty sampling. 
6.2 Experimental Settings 
We utilize a maximum entropy (ME) model 
(Berger et al, 1996) to design the basic classifier 
for WSD and TC tasks. The advantage of the ME 
model is the ability to freely incorporate features 
from diverse sources into a single, well-grounded 
statistical model. A publicly available ME tool-
kit 5  was used in our experiments. To build the 
ME-based classifier for WSD, three knowledge 
sources are used to capture contextual informa-
tion: unordered single words in topical context, 
POS of neighboring words with position infor-
mation, and local collocations, which are the 
same as the knowledge sources used in (Lee and 
Ng, 2002). In the design of text classifier, the 
maximum entropy model is also utilized, and no 
feature selection technique is used. 
                                                 
5See  http://homepages.inf.ed.ac.uk/s0450736/maxent_ 
toolkit.html 
1140
In the following comparison experiments, the 
algorithm starts with a initial training set of 10 
labeled examples, and make 10 queries after each 
learning iteration. A 10 by 10-fold cross-
validation was performed. All results reported 
are the average of 10 trials in each active 
learning process.  
6.3 Data Sets 
Three publicly available natural data sets have 
been used in the following active learning com-
parison experiments. Interest data set is used for 
WSD tasks. Comp2 and WebKB data sets are 
used for TC tasks.  
The Interest data set developed by Bruce and 
Wiebe (1994) has been previously used for WSD 
(Ng and Lee, 1996). This data set consists of 
2369 sentences of the noun ?interest? with its 
correct sense manually labeled. The noun 
?interest? has six different senses in this data set.  
The Comp2 data set consists of comp.graphics 
and comp.windows.x categories from News-
Groups,  which has been previously used in ac-
tive learning for TC (Roy and McCallum, 2001; 
Schein and Ungar, 2007). 
The WebKB dataset was widely used in TC 
research. Following previous studies (McCallum 
and Nigam, 1998), we use the four most popu-
lous categories: student, faculty, course and pro-
ject, altogether containing 4199 web pages. In 
the preprocessing step, we remove those words 
that occur merely once without using stemming. 
The resulting vocabulary has 23803 words. 
 
Data sets Interest Comp2 WebKB 
Accuracy 0.908 0.90 0.91 
Table 1. Average accuracy of supervised learning 
on each data set when all examples have been 
learned. 
6.4 Active Learning for WSD Task 
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0  50  100  150  200  250  300
A
cc
ur
ac
y
Number of Learned Examples
Active Learning for WSD on Interest
random
random + SBC
uncertainty
uncertainty + SBC
SUD
SUD + SBC
 
Figure 4. Active learning curve for WSD on In-
terest data set 
 
 
Random Random+SBC Uncertainty 
1.926 1.886 NA 
Uncertainty+SBC SUD SUD+SBC 
0.947 0.811 0.758 
Table 2. Average deficiency achieved by various 
active learning methods on Interest data set. The 
stopping point is 300.  
 
Fig. 4 depicts performance curves of various ac-
tive learning methods for WSD task on Interest 
data set. Among these six methods, random sam-
pling method shows the worst performance. SUD 
method constantly outperforms uncertainty sam-
pling. As discussed above, SUD method prefers 
not only the most uncertainty examples, but also 
the most representative examples. In the SUD 
scheme, the factor of KNN-density can effec-
tively avoid selecting the outliers that often cause 
uncertainty sampling to fail.  
It is noteworthy that using SBC to generate 
initial training data set can improve random (-
0.04 deficiency), uncertainty (-0.053 deficiency) 
and SUD (-0.053 deficiency) methods, respec-
tively. If the initial training data set is generated 
at random, the initial accuracy is only 55.6%. 
Interestingly, SBC achieves 62.2% initial accu-
racy, and makes 6.6% accuracy performance im-
provement. However, SBC only makes perform-
ance improvement for each method at the early 
stages of active learning. After 50 unlabeled ex-
amples have been learned, it seems that SBC has 
very little contribution to random, uncertainty 
and SUD methods. Table 2 shows that the best 
method is SUD with SBC (0.758 deficiency), 
followed by SUD method. 
6.5 Active Learning for TC Tasks 
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0  50  100  150
A
cc
ur
ac
y
Number of Learned Examples
Active Learning for Text Classification on Comp2
uncertainty
uncertainty + SBC
SUD
SUD + SBC
 
Figure 5. Active learning curve for text classifi-
cation on Comp2 data set 
 
Uncertainty Uncertainty+SBC SUD SUD+SBC
NA 0.409 0.588 0.257 
Table 3. Average deficiency achieved by various 
active learning methods on Comp2 data set. The 
stopping point is 150.  
1141
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0  50  100  150
A
cc
ur
ac
y
Number of Learned Examples
Active Learning for Text Classification on WebKB
uncertainty
uncertainty + SBC
SUD
SUD + SBC
 
Figure 6. Active learning curve for text classifi-
cation on WebKB data set 
 
Uncertainty Uncertainty+SBC SUD SUD+SBC 
NA 0.669 0.748 0.595 
Table 4. Average deficiency achieved by various 
active learning methods on WebKB data set. The 
stopping point is 150.  
 
Fig. 5 and 6 show the effectiveness of various 
active learning methods for text classification 
tasks. Since random sampling performs poorly as 
shown in Fig. 4, it is not further shown in Fig. 5 
and 6. We only compare uncertainty sampling 
and our proposed methods for both text classifi-
cation tasks.  
Similarly, SUD method constantly outper-
forms uncertainty sampling on two data sets. 
SBC greatly improves uncertainty sampling (i.e. 
0.591 and 0.331 deficiencies degraded) and SUD 
method (i.e. 0.331 and 0.153 deficiencies de-
graded), respectively. Interestingly, unlike WSD 
task shown in Fig. 4, Table 3 and 4 show that 
uncertainty sampling with SBC outperforms our 
SUD method for text classification on both data 
sets. The reason is that SBC makes about 15% 
initial accuracy improvement on Comp2 data set, 
and about 23% initial accuracy improvement on 
WebKB data set. Such improvements indicate 
that selecting high representative initial training 
set is very necessary and helpful for active learn-
ing. Table 3 and 4 show that the best active 
learning method for TC task is SUD with SBC, 
following by uncertainty sampling with SBC 
method. It is noteworthy that on WebKB uncer-
tainty sampling with SBC (0.669 deficiency) 
achieves only slight better performance than 
SUD method (0.748 deficiency) as shown in Ta-
ble 4, simply because SBC only introduce good 
performance improvement at the early stages. 
Actually on WebKB SUD method achieves 
slight better performance than uncertainty sam-
pling with SBC after about 50 unlabeled exam-
ples have been learned. 
7 Related Work 
In recent years active learning has been widely 
studied in various natural language processing 
(NLP) tasks, such as word sense disambiguation 
(Chen et al, 2006; Zhu and Hovy, 2007), text 
classification (TC) (Lewis and Gale, 1994; 
McCallum and Nigam, 1998), named entity 
recognition (NER) (Shen et al, 2004), chunking 
(Ngai and Yarowsky, 2000), information 
extraction (IE) (Thompson et al, 1999), and 
statistical parsing (Tang et al, 2002). 
In addition to uncertainty sampling, there is 
another popular selective sampling scheme, 
Query-by-committee (Engelson and Dagan, 
1999), which generates a committee of classifiers 
(always more than two classifiers) and selects the 
next unlabeled example by the principle of 
maximal disagreement among these classifiers. A 
method similar to committee-based sampling is 
co-testing proposed by Muslea et al (2000), 
which trains two learners individually on two 
compatible and uncorrelated views that should be 
able to reach the same classification accuracy. In 
practice, however, these conditions of view se-
lection are difficult to meet in real-world applica-
tions. Cohn et al (1996) and Roy and McCallum 
(2001) proposed a method that directly optimizes 
expected future error on future test examples. 
However, the computational complexity of their 
methods is very high.  
There are some similar previous studies (Tang 
et al, 2002; Shen et al, 2004) in which the rep-
resentativeness criterion in active learning is 
considered. Unlike our sampling by uncertainty 
and density technique, Tang et al (2002) adopted 
a sampling scheme of most uncertain per cluster 
for NLP parsing, in which the learner selects the 
sentence with the highest uncertain score from 
each cluster, and use the density to weight the 
selected examples while we use density informa-
tion to select the most informative examples. Ac-
tually the scheme of most uncertain per cluster 
still can not solve the outlier problem faced by 
uncertainty sampling technique. Shen et al 
(2004) proposed an approach to selecting exam-
ples based on informativeness, representativeness 
and diversity criteria. In their work, the density 
of an example is evaluated within a cluster, and 
multiple criteria have been linearly combined 
with some coefficients. However, it is difficult to 
automatically determine sufficient coefficients in 
real-world applications. Perhaps there are differ-
ent appropriate coefficients for various applica-
tions.  
1142
8 Discussion 
For batch mode active learning, we found some-
times there is a redundancy problem that some 
selected examples are identical or similar. Such 
situation would reduce the representativeness of 
selected examples. To solve this problem, we 
tried the sampling scheme of ?most uncertain per 
cluster? (Tang et al, 2002) to select the most 
informative examples. We think selecting exam-
ples from each cluster can alleviate the redun-
dancy problem. However, this sampling scheme 
works poorly for WSD and TC on the three data 
sets, compared to traditional uncertainty sam-
pling. From the clustering results, we found these 
resulting clusters are very imbalanced. It makes 
sense that more informative examples are con-
tained in a bigger cluster. In this work, we only 
use SUD technique to select the most informative 
examples for active learning. We plan to study 
how combining SBC and SUD techniques can 
enhance the selection of the most informative 
examples in the future work. 
Furthermore, we think that a misclassified 
unlabeled example may convey more 
information than a correctly classified unlabeled 
example which is closer to the decision boundary. 
But there is a difficulty that the true label of each 
unlabeled example is unknown. To use misclassi-
fication information to select the most informa-
tive examples, we should study how to automati-
cally determine whether an unlabeled example 
has been misclassified. For example, we can 
make an assumption that an unlabeled example 
may be misclassified if this example was previ-
ously ?outside? and is now ?inside?. We will 
study this issue in the future work.  
Actually these proposed techniques can be 
easily applied for committee-based sampling for 
active learning. However, to do so, we should 
adopt a new uncertainty measurement such as 
vote entropy to measure the uncertaity of each 
unlabled example in committee-based sampling 
scheme. 
9 Conclusion and Future Work 
In this paper, we have addressed two issues of 
active learning, involving the outlier problem of 
traditional uncertainty sampling, and initial train-
ing data set generation. To solve the outlier prob-
lem of traditional uncertainly sampling, we pro-
posed a new method of sampling by uncertainty 
and density (SUD) in which KNN-density meas-
ure and uncertainty measure are combined to-
gether to select the most informative unlabeled 
example for human annotation at each learning 
cycle. We employ a method of sampling by clus-
tering (SBC) to generate a representative initial 
training data set. Experimental results on three 
evaluation data sets show that our combined 
SUD with SBC method achieved the best per-
formance compared to other competing methods, 
particularly at the early stages of active learning 
process. In future work, we will focus on the re-
dundancy problem faced by batch mode active 
learning, and how to make use of misclassified 
information to select the most useful examples 
for human annotation. 
Acknowledgments 
This work was supported in part by the National 
863 High-tech Project (2006AA01Z154) and the 
Program for New Century Excellent Talents in 
University (NCET-05-0287). 
References 
Berger Adam L., Vincent J. Della Pietra, Stephen A. 
Della Pietra. 1996. A maximum entropy approach 
to natural language processing. Computational 
Linguistics 22(1):39?71. 
Bruce Rebecca and Janyce Wiebe. 1994. Word sense 
disambiguation using decomposable models. Pro-
ceedings of the 32nd annual meeting on Associa-
tion for Computational Linguistics, pp. 139-146. 
Chan Yee Seng and Hwee Tou Ng. 2007. Domain 
adaptation with active learning for word sense dis-
ambiguation. Proceedings of the 45th annual meet-
ing on Association for Computational Linguistics, 
pp. 49-56 
Chen Jinying, Andrew Schein, Lyle Ungar and 
Martha Palmer. 2006. An empirical study of the 
behavior of active learning for word sense disam-
biguation. Proceedings of the main conference on 
Human Language Technology Conference of the 
North American Chapter of the Association of 
Computational Linguistics, pp. 120-127 
Cohn David A., Zoubin Ghahramani and Michael I. 
Jordan. 1996. Active learning with statistical mod-
els. Journal of Artificial Intelligence Research, 4, 
129?145. 
Duda Richard O. and Peter E. Hart. 1973. Pattern 
classification and scene analysis. New York: 
Wiley. 
Engelson S. Argamon and I. Dagan. 1999. Commit-
tee-based sample selection for probabilistic classi-
fiers. Journal of Artificial Intelligence Research 
(11):335-360. 
1143
Lee Yoong Keok and Hwee Tou Ng. 2002. An em-
pirical evaluation of knowledge sources and learn-
ing algorithm for word sense disambiguation. In 
Proceedings of the ACL-02 conference on Empiri-
cal methods in natural language processing, pp. 41-
48 
Lewis David D. and William A. Gale. 1994. A se-
quential algorithm for training text classifiers. In 
Proceedings of the 17th annual international ACM 
SIGIR conference on Research and development in 
information retrieval, pp. 3-12 
McCallum Andrew and Kamal Nigam. 1998. A com-
parison of event models for na?ve bayes text classi-
fication. In AAAI-98 workshop on learning for text 
categorization. 
Muslea Ion, Steven Minton and Craig A. Knoblock. 
2000. Selective sampling with redundant views. In 
Proceedings of the Seventeenth National Confer-
ence on Artificial Intelligence and Twelfth Confer-
ence on Innovative Applications of Artificial Intel-
ligence, pp. 621-626. 
Ng Hwee Tou and Hian Beng Lee. 1996. Integrating 
multiple knowledge sources to disambiguate word 
sense: an exemplar-based approach. In Proceed-
ings of the Thirty-Fourth Annual Meeting of the 
Association for Computational Linguistics, pp. 40-
47 
 Ngai Grace and David Yarowsky. 2000. Rule writing 
or annotation: cost-efficient resource usage for 
based noun phrase chunking. In Proceedings of the 
38th Annual Meeting of the Association for Com-
putational Linguistics, pp. 117-125 
Roy Nicholas and Andrew McCallum. 2001. Toward 
optimal active learning through sampling estima-
tion of error reduction. In Proceedings of the 
Eighteenth International Conference on Machine 
Learning, pp. 441-448 
Schein Andrew I. and Lyle H. Ungar. 2007. Active 
learning for logistic regression: an evaluation. 
Machine Learning 68(3): 235-265 
Schohn Greg and David Cohn. 2000. Less is more: 
Active learning with support vector machines. In 
Proceedings of the Seventeenth International Con-
ference on Machine Learning, pp. 839-846 
Shen Dan, Jie Zhang, Jian Su, Guodong Zhou and 
Chew-Lim Tan. 2004. Multi-criteria-based active 
learning for named entity recognition. In Proceed-
ings of the 42nd Annual Meeting on Association 
for Computational Linguistics. 
Tang Min, Xiaoqiang Luo and Salim Roukos. 2002. 
Active learning for statistical natural language 
parsing. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics, 
pp. 120-127 
Thompson Cynthia A., Mary Elaine Califf and Ray-
mond J. Mooney. 1999. Active learning for natural 
language parsing and information extraction. In 
Proceedings of the Sixteenth International Confer-
ence on Machine Learning, pp. 406-414 
Zhu Jingbo and Eduard Hovy. 2007. Active learning 
for word sense disambiguation with methods for 
addressing the class imbalance problem. In Pro-
ceedings of the 2007 Joint Conference on Empiri-
cal Methods in Natural Language Processing and 
Computational Natural Language Learning, pp. 
783-790 
Zhu Jingbo, Huizhen Wang and Eduard Hovy. 2008. 
Learning a stopping criterion for active learning 
for word sense disambiguation and text classifica-
tion. In Proceedings of the Third International Joint 
Conference on Natural Language Processing, pp. 
366-372 
1144
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 325?333, Prague, June 2007. c?2007 Association for Computational Linguistics
Extending a Thesaurus in the Pan-Chinese Context 
Oi Yee Kwong and Benjamin K. Tsou 
Language Information Sciences Research Centre 
City University of Hong Kong 
Tat Chee Avenue, Kowloon, Hong Kong 
{rlolivia,rlbtsou}@cityu.edu.hk 
 
Abstract 
In this paper, we address a unique problem 
in Chinese language processing and report 
on our study on extending a Chinese the-
saurus with region-specific words, mostly 
from the financial domain, from various 
Chinese speech communities.  With the 
larger goal of automatically constructing a 
Pan-Chinese lexical resource, this work 
aims at taking an existing semantic classi-
ficatory structure as leverage and incorpo-
rating new words into it.  In particular, it is 
important to see if the classification could 
accommodate new words from heterogene-
ous data sources, and whether simple simi-
larity measures and clustering methods 
could cope with such variation.  We use the 
cosine function for similarity and test it on 
automatically classifying 120 target words 
from four regions, using different datasets 
for the extraction of feature vectors.  The 
automatic classification results were evalu-
ated against human judgement, and the per-
formance was encouraging, with accuracy 
reaching over 85% in some cases.  Thus 
while human judgement is not straightfor-
ward and it is difficult to create a Pan-
Chinese lexicon manually, it is observed 
that combining simple clustering methods 
with the appropriate data sources appears 
to be a promising approach toward its 
automatic construction. 
1 Introduction 
Large-scale semantic lexicons are important re-
sources for many natural language processing 
(NLP) tasks.  For a significant world language 
such as Chinese, it is especially critical to capture 
the substantial regional variation as an important 
part of the lexical knowledge, which will be useful 
for many NLP applications, including natural lan-
guage understanding, information retrieval, and 
machine translation.  Existing Chinese lexical re-
sources, however, are often based on language use 
in one particular region and thus lack the desired 
comprehensiveness. 
Toward this end, Tsou and Kwong (2006) pro-
posed a comprehensive Pan-Chinese lexical re-
source, based on a large and unique synchronous 
Chinese corpus as an authentic source for lexical 
acquisition and analysis across various Chinese 
speech communities.  To allow maximum versatil-
ity and portability, it is expected to document the 
core and universal substances of the language on 
the one hand, and also the more subtle variations 
found in different communities on the other.  Dif-
ferent Chinese speech communities might share 
lexical items in the same form but with different 
meanings.  For instance, the word ?? refers to 
general housing in Mainland China but specifically 
to housing under the Home Ownership Scheme in 
Hong Kong; and while the word ?? is similar to 
?? to mean general housing in Mainland China, 
it is rarely seen in the Hong Kong context. 
Hence, the current study aims at taking an exist-
ing Chinese thesaurus, namely the Tongyici Cilin 
????? , as leverage and extending it with 
lexical items specific to individual Chinese speech 
communities.  In particular, the feasibility depends 
on the following issues:  (1) Can lexical items from 
various Chinese speech communities, that is, from 
such heterogeneous sources, be classified as effec-
tively with methods shown to work for clustering 
325
closely related words from presumably the same, 
or homogenous, source?  (2) Could existing se-
mantic classificatory structures accommodate con-
cepts and expressions specific to individual Chi-
nese speech communities? 
Measuring similarity will make sense only if the 
feature vectors of the two words under comparison 
are directly comparable.  There is usually no prob-
lem if both words and their contextual features are 
from the same data source.  Since Tongyici Cilin 
(or simply Cilin hereafter) is based on the vocabu-
lary used in Mainland China, it is not clear how 
often these words will be found in data from other 
places, and even if they are found, how well the 
feature vectors extracted could reflect the expected 
usage or sense.  Our hypothesis is that it will be 
more effective to classify new words from 
Mainland China with respect to Cilin categories, 
than to do the same on new words from regions 
outside Mainland China.  Furthermore, if this hy-
pothesis holds, one would need to consider sepa-
rate mechanisms to cluster heterogeneous region-
specific words in the Pan-Chinese context. 
Thus in the current study we sampled 30 target 
words specific to each of Beijing, Hong Kong, 
Singapore, and Taipei, from the financial domain; 
and used the cosine similarity function to classify 
them into one or more of the semantic categories in 
Cilin.  The automatic classification results were 
compared with a simple baseline method, against 
human judgement as the gold standard.  In general, 
an accuracy of up to 85% could be reached with 
the top 15 candidates considered.  It turns out that 
our hypothesis is supported by the Taipei test data, 
whereas the data heterogeneity effect is less obvi-
ous in Hong Kong and Singapore test data, though 
the effect on individual test items varies. 
In Section 2, we will briefly review related work 
and highlight the innovations of the current study.  
In Sections 3 and 4, we will describe the materials 
used and the experimental setup respectively.  Re-
sults will be presented and discussed with future 
directions in Section 5, followed by a conclusion in 
Section 6. 
2 Related Work 
To build a semantic lexicon, one has to identify the 
relation between words within a semantic 
hierarchy, and to group similar words together into 
a class.  Previous work on automatic methods for 
building semantic lexicons could be divided into 
two main groups.  One is automatic thesaurus 
acquisition, that is, to identify synonyms or 
topically related words from corpora based on 
various measures of similarity (e.g. Riloff and 
Shepherd, 1997; Thelen and Riloff, 2002).  For 
instance, Lin (1998) used dependency relation as 
word features to compute word similarities from 
large corpora, and compared the thesaurus created 
in such a way with WordNet and Roget classes.  
Caraballo (1999) selected head nouns from 
conjunctions and appositives in noun phrases, and 
used the cosine similarity measure with a bottom-
up clustering technique to construct a noun 
hierarchy from text.  Curran and Moens (2002) 
explored a new similarity measure for automatic 
thesaurus extraction which better compromises 
with the speed/performance tradeoff.  You and 
Chen (2006) used a feature clustering method to 
create a thesaurus from a Chinese newspaper 
corpus. 
Another line of research, which is more closely 
related with the current study, is to extend existing 
thesauri by classifying new words with respect to 
their given structures (e.g. Tokunaga et al, 1997; 
Pekar, 2004).  An early effort along this line is 
Hearst (1992), who attempted to identify hypo-
nyms from large text corpora, based on a set of 
lexico-syntactic patterns, to augment and critique 
the content of WordNet.  Ciaramita (2002) com-
pared several models in classifying nouns with re-
spect to a simplified version of WordNet and signi-
fied the gain in performance with morphological 
features.  For Chinese, Tseng (2003) proposed a 
method based on morphological similarity to as-
sign a Cilin category to unknown words from the 
Sinica corpus which were not in the Chinese Elec-
tronic Dictionary and Cilin; but somehow the test 
data were taken from Cilin, and therefore could not 
really demonstrate the effectiveness with unknown 
words found in the Sinica corpus. 
The current work attempts to classify new words 
with an existing thesaural classificatory structure.  
However, the usual practice in past studies is to 
test with a portion of data from the thesaurus itself 
and evaluate the results against the original classi-
fication of those words.  This study is thus differ-
ent in the following ways: (1) The test data (i.e. the 
target words to be classified) were not taken from 
the thesaurus, but extracted from corpora and these 
words were unknown to the thesaurus.  (2) The 
326
target words were not limited to nouns.  (3) Auto-
matic classification results were compared with a 
baseline method and with the manual judgement of 
several linguistics students constituting the gold 
standard.  (4) In view of the heterogeneous nature 
of the Pan-Chinese context, we experimented with 
extracting feature vectors from different datasets. 
3 Materials 
3.1 The Tongyici Cilin 
The Tongyici Cilin (?????) (Mei et al, 1984) 
is a Chinese synonym dictionary, or more often 
known as a Chinese thesaurus in the tradition of 
the Roget?s Thesaurus for English.  The Roget?s 
Thesaurus has about 1,000 numbered semantic 
heads, more generally grouped under higher level 
semantic classes and subclasses, and more 
specifically differentiated into paragraphs and 
semicolon-separated word groups.  Similarly, some 
70,000 Chinese lexical items are organized into a 
hierarchy of broad conceptual categories in Cilin.  
Its classification consists of 12 top-level semantic 
classes, 94 subclasses, 1,428 semantic heads and 
3,925 paragraphs.  It was first published in the 
1980s and was based on lexical usages mostly of 
post-1949 Mainland China.  The Appendix shows 
some example subclasses.  In the following 
discussion, we will mainly refer to the subclass 
level and semantic head level. 
3.2 The LIVAC Synchronous Corpus 
LIVAC (http://www.livac.org) stands for Linguis-
tic Variation in Chinese Speech Communities.  It is 
a synchronous corpus developed and dynamically 
maintained by the Language Information Sciences 
Research Centre of the City University of Hong 
Kong since 1995 (Tsou and Lai, 2003).  The cor-
pus consists of newspaper articles collected regu-
larly and synchronously from six Chinese speech 
communities, namely Hong Kong, Beijing, Taipei, 
Singapore, Shanghai, and Macau.  Texts collected 
cover a variety of domains, including front page 
news stories, local news, international news, edito-
rials, sports news, entertainment news, and finan-
cial news.  Up to December 2006, the corpus has 
already accumulated over 200 million character 
tokens which, upon automatic word segmentation 
and manual verification, amount to over 1.2 mil-
lion word types. 
For the present study, we made use of the sub-
corpora collected over the 9-year period 1995-2004 
from Beijing (BJ), Hong Kong (HK), Singapore 
(SG), and Taipei (TW).  In particular, we made use 
of the financial news sections in these subcorpora, 
from which we extracted feature vectors for com-
paring similarity between a given target word and a 
thesaurus class, which is further explained in Sec-
tion 4.3.  Table 1 shows the sizes of the subcorpora. 
3.3 Test Data 
Instead of using a portion of Cilin as the test data, 
we extracted unique lexical items from the various 
subcorpora above, and classified them with respect 
to the Cilin classification. 
Kwong and Tsou (2006) observed that among 
the unique lexical items found from the individual 
subcorpora, only about 30-40% are covered by 
Cilin, but not necessarily in the expected senses.  
In other words, Cilin could in fact be enriched with 
over 60% of the unique items from various regions. 
In the current study, we sampled the most fre-
quent 30 words from each of these unique item 
lists for testing.  Classification was based on their 
similarity with each of the Cilin subclasses, com-
pared by the cosine measure, as discussed in Sec-
tion 4.3. 
 
 
Subcorpus Size of Financial News Sections 
(rounded to nearest 1K) 
 Word Token Word Type 
BJ 232K 20K 
HK 970K 38K 
SG 621K 28K 
TW 254K 22K 
Table 1  Sizes of Individual Subcorpora 
 
4 Experiments 
4.1 Human Judgement 
Three undergraduate linguistics students and one 
research student on computational linguistics from 
the City University of Hong Kong were asked to 
do the task.  The undergraduate students were 
raised in Hong Kong and the research student in 
Mainland China.  They were asked to assign what 
they consider the most appropriate Cilin category 
(up to the semantic head level, i.e. third level in the 
327
Cilin structure) to each of the 120 target words.  
The inter-annotator agreement was measured by 
the Kappa statistic (Siegel and Castellan, 1988), at 
both the subclass and semantic head levels.  Re-
sults on the human judgement are discussed in Sec-
tion 5.1. 
4.2 Creating Gold Standard 
The ?gold standard? was set at both the subclass 
level and semantic head level.  For each level, we 
formed a ?strict? standard for which we considered 
all categories assigned by at least two judges to a 
word; and a ?loose? standard for which we consid-
ered all categories assigned by one or more judges.  
For evaluating the automatic classification in this 
study, however, we only experimented with the 
loose standard at the subclass level. 
4.3 Automatic Classification 
Each target word was automatically classified with 
respect to the Cilin subclasses based on the similar-
ity between the target word and each subclass. 
We compute the similarity by the cosine be-
tween the two corresponding feature vectors.  The 
feature vector of a given target word contains all 
its co-occurring content words in the corpus within 
a window of ?5 words (excluding many general 
adjectives and adverbs, and numbers and proper 
names were all ignored).  The feature vector of a 
Cilin subclass is based on the union of the features 
(i.e. co-occurring words in the corpus) from all 
individual members in the subclass. 
The cosine of two feature vectors is computed as 
 
wv
wvwv vv
vvvv ?=),cos(  
 
In view of the difference in the feature space of a 
target word and a whole class of words, and thus 
the potential difference in the number of occur-
rence of individual features, we experimented with 
two versions of the cosine measurement, namely 
binary vectors and real-valued vectors. 
In addition, as mentioned in previous sections, 
we also experimented with the following condi-
tions: whether feature vectors for the Cilin sub-
classes were extracted from the subcorpus where a 
given target word originates, or from the Beijing 
subcorpus which is assumed to be representative of 
language use in Mainland China.  All automatic 
classification results were evaluated against the 
gold standard based on human judgement. 
4.4 Baseline 
To evaluate the effectiveness of the automatic clas-
sification, we adopted a simple baseline measure 
by ranking the 94 subclasses in descending order 
of the number of words they cover.  In other words, 
assuming the bigger the subclass size, the more 
likely it covers a new term, thus we compared the 
top-ranking subclasses with the classifications ob-
tained from the automatic method using the cosine 
measure. 
5 Results and Discussion 
5.1 Response from Human Judges 
All human judges reported difficulties in various 
degrees in assigning Cilin categories to the target 
words.  The major problem comes from the re-
gional specificity and thus the unfamiliarity of the 
judges with the respective lexical items and con-
texts.  For instance, students grown up in Hong 
Kong were most familiar with the Hong Kong data, 
and slightly less so with the Beijing data, but more 
often had the least ideas for the Taipei and Singa-
pore data.  The research student from Mainland 
China had no problem with Beijing data and the 
lexical items in Cilin, but had a hard time figuring 
out the meaning for words from Hong Kong, 
Taipei and Singapore.  For example, all judges re-
ported problem with the term ??, one of the tar-
get words from Singapore referring to ???? 
(CLOB in the Singaporean stock market), which is 
really specific to Singapore. 
The demand on cross-cultural knowledge thus 
poses a challenge for building a Pan-Chinese 
lexical resource manually.  Cilin, for instance, is 
quite biased in language use in Mainland China, 
and it requires experts with knowledge of a wide 
variety of Chinese terms to be able to manually 
classify lexical items specific to other Chinese 
speech communities.  It is therefore even more 
important to devise robust ways for automatic 
acquisition of such a resource. 
Notwithstanding the difficulty, the inter-
annotator agreement was quite satisfactory.  At the 
subclass level, we found K=0.6870.  At the seman-
tic head level, we found K=0.5971.  Both figures 
are statistically significant. 
328
5.2 Gold Standard 
As mentioned, we set up a loose standard and a 
strict standard at both the subclass and semantic 
head level.  In general, the judges managed to 
reach some consensus in all cases, except for two 
words from Singapore.  For these two cases, we 
considered all categories assigned by any of the 
judges for both standards. 
The gold standards were verified by the authors.  
Although in several cases the judges did not reach 
complete agreement with one another, we found 
that their decisions reflected various possible per-
spectives to classify a given word with respect to 
the Cilin classification; and the judges? assign-
ments, albeit varied, were nevertheless reasonable 
in one way or another. 
5.3 Evaluating Automatic Classification 
In the following discussion, we will refer to the 
various testing conditions for each group of target 
words with labels in the form of Cos-<Vector 
Type>-<Target Words>-<Cilin Feature Source>.  
Thus the label Cos-Bin-hk-hk means testing on 
Hong Kong target words with binary vectors and 
extracting features for the Cilin words from the 
Hong Kong subcorpus; and the label Cos-RV-sg-bj 
means testing on Singapore target words with real-
valued vectors and extracting features for the Cilin 
words from the Beijing subcorpus.  For each target 
word, we evaluated the automatic classification 
(and the baseline ranking) by matching the human 
decisions with the top N candidates.  If any of the 
categories suggested by the human judges is cov-
ered, the automatic classification is considered ac-
curate.  The results are shown in Figure 1 for test 
data from individual regions. 
Overall speaking, the results are very encourag-
ing, especially in view of the number of categories 
(over 90) we have at the subclass level.  An accu-
racy of 80% or more is obtained in general if the 
top 15 candidates were considered, which is much 
higher than the baseline result in all cases.  Table 2 
shows some examples with appropriate classifica-
tion within the Top 3 candidates.  The two-letter 
codes in the ?Top 3? column in Table 2 refer to the 
subclass labels, and the code in bold is the one 
matching human judgement. 
In terms of the difference between binary vec-
tors and real-valued vectors in the similarity meas-
urement, the latter almost always gave better re-
sults.  This was not surprising as we expected by 
using real-valued vectors we could be less affected 
by the potential huge difference in the feature 
space and the number of occurrence of the features 
for a Cilin subclass and a target word. 
As for extracting features for Cilin subclasses 
from the Beijing subcorpus or other subcorpora, 
the difference is more obvious for the Singapore 
and Taipei target words.  We will discuss the re-
sults for each group of target words in detail below. 
5.4  Performance on Individual Sources 
Target words from Beijing were expected to have a 
relatively higher accuracy because they are ho-
mogenous with the Cilin content.  It turned out, 
however, the accuracy only reached 73% with top 
15 candidates and 83% with top 20 candidates 
even under the Cos-RV-bj-bj condition.  Words 
like ?? (SARS), ?? (save water), ??? (in-
dustrialize / industrialization), ??? (passing rate) 
and ?? (multi-level marketing) could not be suc-
cessfully classified. 
Results were surprisingly good for target words 
from the Hong Kong subcorpus.  Under the Cos-
RV-hk-hk condition, the accuracy was 87% with 
top 15 candidates and even over 95% with top 20 
candidates considered.  Apart from this high accu-
racy, another unexpected observation is the lack of 
significant difference between Cos-RV-hk-hk and 
Cos-RV-hk-bj.  One possible reason is that the 
relatively larger size of the Hong Kong subcorpus 
might have allowed enough features to be ex-
tracted even for the Cilin words.  Nevertheless, the 
similar results from the two conditions might also 
suggest that the context in which Cilin words are 
used might be relatively similar in the Hong Kong 
subcorpus and the Beijing subcorpus, as compared 
with other communities.  
Similar trends were observed from the Singa-
pore target words.  Looking at Cos-RV-sg-sg and 
Cos-RV-sg-bj, it appears that extracting feature 
vectors for the Cilin words from the Singapore 
subcorpus leads to better performance than extract-
ing them from the Beijing subcorpus.  It suggests 
that although the Singapore subcorpus shares those 
words in Cilin, the context in which they are used 
might be slightly different from their use in 
Mainland China.  Thus extracting their contextual 
features from the Singapore subcorpus might better 
reflect their usage and makes it more comparable 
329
Classification Accuracy for HK Data
0
10
20
30
40
50
60
70
80
90
100
0 5 10 15
Top N
Acc %
Cos-Bin-hk Cos-Bin-bj Cos-RV-hk Cos-RV-bj Baseline
Classification Accuracy for BJ Data
0
10
20
30
40
50
60
70
80
0 5 10 15
Top N
Acc %
Cos-Bin Cos-RV Baseline
`
Classification Accuracy for SG Data
0
10
20
30
40
50
60
70
80
90
100
0 5 10 15
Top N
Acc %
Cos-Bin-sg Cos-Bin-bj Cos-RV-sg Cos-RV-bj Baseline
Classification Accuracy for TW Data
0
10
20
30
40
50
60
70
80
90
100
0 5 10 15
Top N
Acc %
Cos-Bin-tw Cos-Bin-bj Cos-RV-tw Cos-RV-bj Baseline
with the unique target words from Singapore.  
Such possible difference in contextual features 
with shared lexical items between different Chi-
nese speech communities would require further 
investigation, and will form part of our future work 
as discussed below.  Despite the above observation 
from the accuracy figures, the actual effect, how-
ever, seems to vary on individual lexical items.  
Table 3 shows some examples of target words 
which received similar (with white cells) and very 
different (with shaded cells) classification respec-
tively under the two conditions.  It appears that the 
region-specific but common concepts like ??? 
(office), ??  (apartment), ??  (private resi-
dence), which relate to building or housing, were 
affected most. 
Taipei data, on the contrary, seems to be more 
affected by the different testing conditions.  Cos-
Bin-tw-bj and Cos-RV-tw-bj produced similar re-
sults, and both conditions showed better results 
than Cos-RV-tw-tw.  This supports our hypothesis 
that the effect of data heterogeneity is so apparent 
that it is much harder to classify target words 
unique to Taipei with respect to the Cilin catego-
ries.  In addition, as Kwong and Tsou (2006) ob-
served, Beijing and Taipei data share the least 
number of lexical items, among the four regions 
under investigation.  Hence, words in Cilin might 
not have the appropriate contextual feature vectors 
extracted from the Taipei subcorpus. 
The different results for individual regions 
might be partly due to the endocentric and exocen-
tric nature of influence in lexical innovation (e.g. 
Tsou, 2001) especially with respect to the financial 
domain and the history of capitalism in individual 
regions.  This factor is worth further investigation. 
 
 
 
Figure 1  Classification Results with Top N Candidates 
 
330
No. Region Word Top 3 
1 BJ ???? Di  Gb  Df 
2 BJ ?? Bq  Ae  Hd 
3 BJ ?? Bm  Hi  Hd 
4 BJ ?? Hj  Di  Hd 
5 BJ ?? Aa  If  Ae 
6 HK ?? Da  Cb  Bi 
7 HK ?? Bb  Jc  Hi 
8 HK ?? Dj  Da  Hi 
9 HK ?? Bi  Dj  Dn 
10 HK ??? Bi  Dj  Gb 
11 SG ?? Ca  Dm  Hi 
12 SG ?? Ig  He  Dj 
13 SG ?? Dm  Dj  Hi 
14 SG ?? Dm  Dj  He 
15 SG ?? Hi  Hg  Af 
16 TW ?? Dm  Hd  Hi 
17 TW ?? Jb  Dn  Dj 
18 TW ?? Ja  Ca  He 
19 TW ??? Hf  Dj  Dm 
20 TW ?? Dj  Ed  Ca 
Table 2  Examples of Correct Classification (Top 3)1 
  
5.5 General Discussions and Future Work 
As mentioned in a previous section, the test data in 
this study were not taken from the thesaurus itself, 
but were unknown words to the thesaurus.  They 
were extracted from corpora, and were not limited 
to nouns.  We found in this study that the simple 
cosine measure, which used to be applied for clus-
tering contextually similar words from homoge-
nous sources, performs quite well in general for 
classifying these unseen words with respect to the 
Cilin subclasses.  The automatic classification re-
sults were compared with the manual judgement of 
several linguistics students.  In addition to provid-
ing a gold standard for evaluating the automatic 
classification results in this study, the human 
                                                 
1 English gloss: 1-restoring agricultural lands for affore-
station, 2-material, 3-coal mine, 4-to seize (an opportu-
nity), 5-unemployed, 6-sales performance, 7-broadband, 
8-red chip, 9-interest rate, 10-property stocks, 11-
financial year, 12-sell short, 13-proposal, 14-sell, 15-
brigadier general, 16-financial holdings, 17-individual 
stocks, 18-property market, 19-cash card, 20-stub. 
judgement on the one hand proves that the Cilin 
classificatory structure could accommodate region-
specific lexical items; but on the other hand also 
suggests how difficult it would be to construct such 
a Pan-Chinese lexicon manually as rich cultural 
and linguistic knowledge would be required.  
Moreover, we started with Cilin as the established 
semantic classification and attempted to classify 
words specific to Beijing, Hong Kong, Singapore, 
and Taipei respectively.  The heterogeneity of 
sources did not seem to hamper the similarity 
measure on the whole, provided appropriate data-
sets are used for feature extraction, although the 
actual effect seemed to vary on individual lexical 
items. 
 
No. Source Word Ranking of 
1st appropriate class 
   Cos-RV-hk-hk, 
etc. 
Cos-RV-hk-bj, 
etc. 
1 HK ?? 1 1 
2 HK ?? 1 1 
3 HK ?? 1 1 
4 HK ?? 2 10 
5 HK ?? 19 5 
6 HK ??? 13 30 
7 SG ??? 2 2 
8 SG ?? 2 1 
9 SG ??? 5 4 
10 SG ?? 1 12 
11 SG ??? 1 9 
12 SG ?? 8 26 
13 TW ?? 1 1 
14 TW ?? 4 3 
15 TW ?? 5 1 
16 TW ?? 18 4 
17 TW ??? 12 5 
18 TW ??? 8 2 
Table 3  Different Impact on Individual Items2 
 
Despite the encouraging results with the top 15 
candidates in the current study, it is desirable to 
improve the accuracy for the system to be useful in 
                                                 
2 English gloss: 1-sales performance, 2-broadband, 3-
red chip, 4-add (supply to market), 5-low level, 6-office, 
7-financial year, 8-sell short, 9-rights issue, 10-
apartment, 11-holding space rate, 12-private residence, 
13-stub, 14-individual stocks, 15-financial holdings, 16-
investment trust, 17-growth rate, 18-cash card. 
331
practice.  Hence our next step is to expand the test 
data size and to explore alternative methods such 
as using a nearest neighbour approach.  In addition, 
we plan to further the investigation in the follow-
ing directions.  First, we will experiment with the 
automatic classification at the Cilin semantic head 
level, which is much more fine-grained than the 
subclasses.  The fine-grainedness might make the 
task more difficult, but at the same time the more 
specialized grouping might pose less ambiguity for 
classification.  Second, we will further experiment 
with classifying words from other special domains 
like sports, as well as the general domain.  Third, 
we will study the classification in terms of the part-
of-speech of the target words, and their respective 
requirements on the kinds of features which give 
best classification performance. 
The current study only dealt with presumably 
Modern Standard Chinese in different communities, 
and it could potentially be expanded to handle 
various dialects within a common resource, even-
tually benefiting speech lexicons and applications 
at large. 
6 Conclusion 
In this paper, we have reported our study on a 
unique problem in Chinese language processing, 
namely extending a Chinese thesaurus with new 
words from various Chinese speech communities, 
including Beijing, Hong Kong, Singapore and 
Taipei.  The critical issues include whether the ex-
isting classificatory structure could accommodate 
concepts and expressions specific to various Chi-
nese speech communities, and whether the differ-
ence in textual sources might pose difficulty in us-
ing conventional similarity measures for the auto-
matic classification.  Our experiments, using the 
cosine function to measure similarity and testing 
with various sources for extracting contextual vec-
tors, suggest that the classification performance 
might depend on the compatibility between the 
words in the thesaurus and the sources from which 
the target words are drawn.  Evaluated against hu-
man judgement, an accuracy of over 85% was 
reached in some cases, which were much higher 
than the baseline and were very encouraging in 
general.  While human judgement is not straight-
forward and it is difficult to create a Pan-Chinese 
lexicon manually, combining simple classification 
methods with the appropriate data sources seems to 
be a promising approach toward its automatic 
construction. 
Acknowledgements 
The work described in this paper was supported by 
a grant from the Research Grants Council of the 
Hong Kong Special Administrative Region, China 
(Project No. CityU 1317/03H). 
Appendix 
The following table shows some examples of the 
Cilin subclasses: 
 
Class Subclasses 
A ? (Human) Aa ? Ae ?? (Occupation)  Af
?? (Identity) ? An 
B ? (Things) Ba ? Bb ??? (Shape) ? Bi ?
? (Animal)? Bm ?? (Mate-
rial)?Bq ?? (Clothing) ? Br 
C ????? 
(Time and Space) 
Ca ?? (Time)  Cb ?? (Space) 
D ???? 
(Abstract entities) 
Da ?? ?? (Condition) ? Df 
?? (Ideology) ? Di ?? ?? 
(Society) Dj ?? (Economics) ? 
Dm ?? (Organization) Dn ?? 
?? (Quantity) 
E ?? 
(Characteristics) 
Ea ? Ed ?? (Property)? Ef 
F ?? (Action) Fa ? Fd 
G ???? 
(Psychological 
activities) 
Ga ? Gb ???? (Psychologi-
cal activities)? Gc 
H ?? 
(Activities) 
Ha ? He ???? (Economic 
activities) ? Hd ?? (Produc-
tion) ? Hf ???? (Transporta-
tion) Hg ???? (Scientific re-
search)? Hi ?? (Social contact) 
Hj ?? (Livelihood) 
I ????? 
(Phenomenon and 
state) 
Ia ? If ?? (Circumstance)  Ig ?
? (Process)? Ih 
J ?? 
(Association) 
Ja ?? (Liaison)  Jb ?? (Simi-
larity and Difference) Jc ?? 
(Matching) ? Je 
K ?? 
(Auxiliary words) 
Ka ? Kf 
L ?? 
(Respectful ex-
pressions) 
 
 
332
References 
Caraballo, S.A. (1999)  Automatic construction of a 
hypernym-labeled noun hierarchy from text.  In Pro-
ceedings of the 37th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?99), Mary-
land, USA, pp.120-126. 
Ciaramita, M. (2002)  Boosting automatic lexical acqui-
sition with morphological information.  In Proceed-
ings of the ACL?02 Workshop on Unsupervised Lexi-
cal Acquisition, Philadelphia, USA, pp.17-25. 
Curran, J.R. and Moens, M. (2002)  Improvements in 
Automatic Thesaurus Extraction.  In Proceedings of 
the ACL?02 Workshop on Unsupervised Lexical Ac-
quisition, Philadelphia, USA, pp.59-66. 
Hearst, M. (1992)  Automatic Acquisition of Hyponyms 
from Large Text Corpora.  In Proceedings of the 14th 
International Conference on Computational Linguis-
tics (COLING-92), Nantes, France, pp.539-545. 
Kwong, O.Y. and Tsou, B.K. (2006)  Feasibility of En-
riching a Chinese Synonym Dictionary with a Syn-
chronous Chinese Corpus.  In T. Salakoski, F. Ginter, 
S. Pyysalo and T. Pahikkala (Eds.), Advances in 
Natural Language Processing: Proceedings of Fin-
TAL 2006. Lecture Notes in Artificial Intelligence, 
Vol.4139, pp.322-332, Springer-Verlag. 
Lin, D. (1998)  Automatic Retrieval and Clustering of 
Similar Words.  In Proceedings of the 36th Annual 
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics (COLING-ACL?98), Montreal, 
Canada, pp.768-774. 
Mei et al ??????????????? (1984)  
???????  (Tongyici Cilin).  ????? 
(Commerical Press) / ???????. 
Pekar, V. (2004)  Linguistic Preprocessing for Distribu-
tional Classification of Words.  In Proceedings of the 
COLING2004 Workshop on Enhancing and Using 
Electronic Dictionaries, Geneva. 
Riloff, E. and Shepherd, J. (1997)  A corpus-based ap-
proach for building semantic lexicons.  In Proceed-
ings of the Second Conference on Empirical Methods 
in Natural Language Processing, Providence, Rhode 
Island, pp.117-124. 
Siegel, S. and Castellan, N.J. (1988)  Nonparametric 
Statistics for the Behavioral Sciences (2nd Ed.).  
McGraw-Hill. 
Thelen, M. and Riloff, E. (2002)  A Bootstrapping 
Method for Learning Semantic Lexicons using Ex-
traction Pattern Contexts.  In Proceedings of the 2002 
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2002), Philadelphia, 
USA. 
Tokunaga, T., Fujii, A., Iwayama, M., Sakurai, N. and 
Tanaka, H. (1997)  Extending a thesaurus by classi-
fying words.  In Proceedings of the ACL Workshop 
on Automatic Information Extraction and Building of 
Lexical Semantic Resources for NLP Applications, 
Madrid, pp.16-21. 
Tseng, H. (2003)  Semantic Classification of Chinese 
Unknown Words.  In the Proceedings of the ACL-
2003 Student Research Workshop, Companion Vol-
ume to the Proceedings of the 41st Annual Meeting of 
the Association for Computational Linguistics, Sap-
poro, Japan. 
Tsou, B.K. (2001)  Language Contact and Lexical Inno-
vation.  In M. Lackner, I. Amelung and J. Kurtz 
(Eds.), New Terms for New Ideas: Western Knowl-
edge and Lexical Change in Late Imperial China.  
Berlin: Brill. 
Tsou, B.K. and Kwong, O.Y. (2006)  Toward a Pan-
Chinese Thesaurus.  In Proceedings of the Fifth In-
ternational Conference on Language Resources and 
Evaluation (LREC 2006), Genoa, Italy. 
Tsou, B.K. and Lai, T.B.Y. ??????? (2003)  ?
???????????.  In B. Xu, M. Sun and G. 
Jin ?????????? (Eds.), ??????
????????  (Issues in Chinese Language 
Processing).  ????????, pp.147-165 
You, J-M. and Chen, K-J. (2006)  Improving Context 
Vector Models by Feature Clustering for Automatic 
Thesaurus Construction.  In Proceedings of the Fifth 
SIGHAN Workshop on Chinese Language Processing, 
COLING-ACL 2006, Sydney, Australia, pp.1-8. 
 
 
333
115
116
117
118
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 292 ? 301, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Using Multiple Discriminant Analysis Approach  
for Linear Text Segmentation 
Zhu  Jingbo1, Ye Na 1, Chang Xinzhi 1, Chen Wenliang 1, and Benjamin K Tsou2 
1 Natural Language Processing Laboratory, 
Institute of Computer Software and Theory, Northeastern University, Shenyang, P.R. China 
{zhujingbo, chenwl}@mail.neu.edu.cn 
{yena, changxz}@ics.neu.edu.cn 
2 Language Information Sciences Research Centre, 
City University of Hong Kong, HK 
rlbtsou@cityu.edu.hk 
Abstract. Research on linear text segmentation has been an on-going focus in 
NLP for the last decade, and it has great potential for a wide range of 
applications such as document summarization, information retrieval and text 
understanding. However, for linear text segmentation, there are two critical 
problems involving automatic boundary detection and automatic determination 
of the number of segments in a document. In this paper, we propose a new 
domain-independent statistical model for linear text segmentation. In our 
model, Multiple Discriminant Analysis (MDA) criterion function is used to 
achieve global optimization in finding the best segmentation by means of the 
largest word similarity within a segment and the smallest word similarity 
between segments. To alleviate the high computational complexity problem 
introduced by the model, genetic algorithms (GAs) are used. Comparative 
experimental results show that our method based on MDA criterion functions 
has achieved higher Pk measure (Beeferman) than that of the baseline system 
using TextTiling algorithm. 
1   Introduction 
Typically a document is concerned with more than one subject, and most texts consist 
of long sequences of paragraphs with very little structural demarcation. The goal of 
linear text segmentation is to divide a document into topically-coherent sections, each 
corresponding to a relevant subject. Linear text segmentation has been applied in 
document summarization, information retrieval, and text understanding. For example, 
in recent years, passage-retrieval techniques based on linear text segmentation, are 
becoming increasingly popular in information retrieval as relevant text passages often 
provide better answers than complete document texts in response to user queries[1]. 
In recent years, many techniques have been applied to linear text segmentation. 
Some have used linguistic information[2,3,4,5,6,9] such as cue phrases, punctuation 
marks, prosodic features, reference, and new words occurrence. Others have used 
statistical methods[7,8,10,11,12,13,14,15] such as those based on word co-
occurrence, lexical cohesion relations, semantic network, similarity between adjacent 
parts of texts, similarity between all parts of a text, dynamic programming algorithm, 
and HMM model.   
 Using Multiple Discriminant Analysis Approach for Linear Text Segmentation 293 
In linear text segmentation study, there are two critical problems involving 
automatic boundary detection and automatic determination of the number of segments 
in a document. Some efforts have focused on using similarity between adjacent parts 
of a text to solve topic boundary detection. In fact, the similarity threshold is very 
hard to set, and it is very difficult to identify exactly topic boundaries only according 
to similarity between adjacent parts of a text. Other works have focused on the 
similarity between all parts of a text. Reynar[7] and Choi[13] used dotplots technique 
to perform linear text segmentation which can be seen as a form of approximate and 
local optimization. Yaari[16] has used agglomerative clustering to perform 
hierarchical segmentation. Others[10,17,18,19] used dynamic programming to 
perform exact and global optimization in which some prior parameters are needed. 
These parameters can be obtained via uninformative prior probabilities[18], or 
estimated from training data[19]. 
In this paper, we propose a new statistical model for linear text segmentation, 
which uses Multiple Discriminant Analysis (MDA) method to define a global 
criterion function for document segmentation. Our method focuses on within-segment 
word similarity and between-segment word similarity. This process can achieve 
global optimization in addressing the two aforementioned problems of linear text 
segmentation. Our method is domain-independent and does not use any training data. 
In section 2, we introduce Multiple Discriminant Analysis (MDA) criterion 
functions in detail. In section 3, our statistical model of linear text segmentation is 
proposed. A new MDA criterion function revised by adding penalty factor is further 
discussed in section 4. Comparative experimental results are given in Section 5. At 
last, we address conclusions and future work in section 6. 
2   MDA Criterion Function 
In statistical pattern classification, MDA approach is commonly used to find effective 
linear transformations[20,21]. The MDA approach seeks a projection that best 
separates the data in a least-squares sense. As shown in Figure 1, using MDA method 
we could get the greatest separation over data space when average within-class 
distance is the smallest, and average between-class distance is the largest.  
Similarly, if we consider a document as data space, and a segment as a class, the 
basic idea of our approach for linear text segmentation is to find best segmentation of 
a document(greatest separation over data space) by focusing on within-segment word 
similarity and between-segment word similarity. It is clear that the smaller the 
average within-class distance or the average between-class distance, the larger the 
within-segment word similarity or the between-segment word similarity, and vice 
versa. In other words, we want to find the best segmentation of a document in which 
within-segment word similarity is the largest, and between-segment word similarity is 
the smallest. To achieve this goal, we introduce a criterion function to evaluate the 
segmentation of a document and assign a score to it. In this paper, we adopt the MDA 
approach to define a global criterion function of document segmentation, and called 
as MDA criterion function, which is described below. 
294 J. Zhu et al 
 
Fig. 1. When average within-class distance is the smallest, and average between-class distance 
is the largest, the greatest separation over data space is shown 
Let W=w1w2?wt be a text consisting of t words, and let S=s1s2?sc be a 
segmentation of W consisting of c segments. We define W as data space, S as 
segmentation distribution over data space W. Because the lengths of paragraphs or 
sentences can be highly irregular, unbalanced comparisons can result in text 
segmentation process. Thus we adopt the block method that is used in the TextTiling 
algorithm[2,3], but we replace lexical word with block. In our model, we group 
blocksize words into a block which can be represented by a d-dimensional vector. In 
practice, we find that the value of blocksize=100 works well for many Chinese 
documents. Then W =w1w2?wt can be redefined as B=b1b2?bk. As illustrated in 
Figure 1, a cross point can be defined as a d-dimensional block vector. 
In this paper, we introduce MDA criterion function Jd in the following form[20] 
( )( ) ( )
B
d
W
tr SJ s
tr S
=  (1) 
Where tr(A) is the trace of matrix A. SW and SB are within-segment scatter matrix and 
between-segment scatter matrix, respectively. SW is defined by  
1
1 ( )( )
i
c
t
W i i i
i b si
S P b m b m
n
= ?
= ? ?? ?  (2) 
Where b stands for blocks belonging to segment si, Pi is the a priori probability of 
segment si, and is defined to be the ratio of blocks in segment si divided by the total 
number of blocks of the document, ni is the number of blocks in the segment si, mi is 
the d-dimensional block mean of the segment si given by 
1
.
i
i
b si
m b
n ?
= ?  (3) 
D1 labeled class ?1 
D2 labeled class ?2 
D3 labeled class ?3 
 Using Multiple Discriminant Analysis Approach for Linear Text Segmentation 295 
Suppose that a total mean vector m is defined by 
1
1 1 c
i i
B i
m b n m
n n
=
= =? ?  (4) 
In equation (1), between-segment scatter matrix SB is defined by 
1
( )( )
c
t
B i i i
i
S P m m m m
=
= ? ??  (5) 
3   Statistical Model for Linear Text Segmentation 
Using the same definitions of text W, segmentation S and blocks B in section 2, we 
first discuss the statistical model for linear text segmentation. The key of statistical 
model for text segmentation is to find the segmentation with maximum-probability. 
This can be turned into another task of finding segmentation with highest Jd score 
equally. The most likely segmentation is given by 
$ arg max P( | ) arg max ( , )def d
S S
S S W J W S= =  (6) 
As mentioned above, because paragraph or sentence length can be highly irregular, 
it leads to unbalanced comparisons in text segmentation process. So W =w1w2?wn 
could be redefined as B=b1b2?bk, and the most likely segmentation is given by 
? arg max P( | ) arg max ( , )def d
S S
S S B J B S= =  (7) 
 The computational complexity for achieving the above solution is O(2k), where k 
is the number of blocks in a document. To alleviate the high computational 
complexity problem, we adopt the genetic algorithms (GAs)[22]. GAs provides a 
learning method motivated by an analogy to biological evolution. Rather than 
searching from general-to-specific hypotheses, or from simple-to-complex, GAs 
generate successor hypotheses by repeatedly mutating and recombining parts of the 
best currently known hypotheses. GAs have most commonly been applied to 
optimization problems outside machine learning, and are especially suited to tasks 
in which hypotheses are complex. 
By adopting this methodology, we derive the following text segmentation 
algorithm, as illustrated in Figure 2. In this paper, we focus our study on paragraph-
level linear text segmentation, in which the potential boundary mark between 
segments can be placed only between adjacent paragraphs.  
 
 
296 J. Zhu et al 
Given a text W and blocks B, Kmax is the total number of paragraphs in the text. 
Initialization: Sbest = {}, Jd(B,Sbest)=0.0 
Segmentation: 
For k = 2 to Kmax 
Begin 
1) Use genetic algorithms and equation (7) to find the best segmentation S 
of k segments. 
 2)  If Jd(B,Sbest) < Jd(B,S) Then 
      Begin 
  Sbest = S and Jd(B,Sbest) = Jd(B,S). 
              Endif 
Endfor 
Output the best segmentation Sbest. 
Fig. 2. MDA-based text segmentation algorithm 
4   Penalty Factor 
In the text segmentation process, adjacent boundary adjustment should be 
considered in cases when there are some very close adjacent but incorrect segment 
boundaries. In experiments we find that in these cases some single-sentence 
paragraphs are wrongly recognized as isolated segments. To solve the problem, we 
propose a penalty factor (PF) to prevent assignment of very short segment 
boundaries (such as a single-sentence segment) by adjusting very close adjacent 
boundaries, and therefore improve the performance of linear text segmentation 
system.  
Suppose that we get a segmentation S=s1s2?sc of the input document, let L be the 
length of the document, Li be the length of the segment si. We know L=L1+L2+?+Lc. 
We define penalty factor as 
1
c
i
i
LPF
L
=
= ?  (8) 
As can be seen, short-length segments would result in smaller penalty factor. We 
use penalty factor to revise the Jd scores of segmentations. To incorporate the penalty 
factor PF, our MDA criterion function Jd can be rewritten as 
1
( )( ) ( ) ( )
c
i B
d PF d
i W
L tr SJ x PF J x
L tr S?
=
= ? = ??  (9) 
In the following experiments, we will evaluate effectiveness of using the two MDA 
criterion functions Jd and Jd-PF for linear text segmentation. 
 Using Multiple Discriminant Analysis Approach for Linear Text Segmentation 297 
5   Experimental Results 
5.1   Evaluation Methods 
Precision and recall statistics are conventional means of evaluating the performance 
of classification algorithms. For the segmentation task, recall measures the fraction of 
actual boundaries that an automatic segmenter correctly identifies, and precision 
measures the fraction of boundaries identified by an automatic segmenter that are 
actual boundaries. The shortcoming is that every inaccurately estimated segment 
boundary is penalized equally whether it is near or far from a true segment boundary.  
To overcome the shortcoming of precision and recall, we use a measure called Pk, 
proposed by Beeferman et al[8]. Pk method measures the proportion of sentences 
which are wrongly predicted to belong in the same segment or sentences which are 
wrongly predicted to belong in different segments. More formally, given two 
segmentations ref(true segmentation) and hyp(hypothetical segmentation) for a 
document of n sentences, Pk is formally defined by 
1
( , ) ( , )( ( , ) ( , ))k ref hyp
i j n
P ref hyp D i j i j i j? ? ?
? ? ?
= ??  (10) 
Where ?ref(i,j) is an indicator function whose value is 1 if sentences i and j belong in 
the same segment in the true segmentation, and 0 otherwise. Similarly, ?hyp(i,j) is an 
indicator function which evaluates to 1 if sentences i and j belong in the same 
segment in the hypothetical segmentation, and 0 otherwise. The operator between 
?ref(i,j) and ?hyp(i,j) in the above formula is the XNOR function on its two operands. 
The function D
?
 is a distance probability distribution over the set of possible distances 
between sentences chosen randomly from the document, and will in general depend 
on certain parameters ? such as the average spacing between sentences. In equation 
(10), D
?
was defined as an exponential distribution with mean 1/?, a parameter that we 
fix at the approximate mean document length for the domain[8]. 
( , ) i jD i j e ?? ?? ? ?=  (11) 
Where ?
?
is a normalization chosen so that D
?
 is a probability distribution over the 
range of distance it can accept. From the above formulation, we could find one 
weakness of the metric: there is no principled way of specifying the distance 
distribution D
?
. In the following experiments, we use Pk as performance measure, 
where the mean segment length in the test data was 1/?=11 sentences. 
5.2   Quantitative Results 
We mainly focus our work on paragraph-level linear text segmentation techniques. 
The Hearst?s TextTiling algorithm[2,3] is a simple and domain-independent technique 
for linear text segmentation, which segments at the paragraph level. Topic boundaries 
are determined by changes in the sequence of similarity scores. This algorithm uses a 
simple cutoff function to determine automatically the number of boundaries.  
298 J. Zhu et al 
In our experiments, we use the TextTiling algorithm to provide the baseline 
system, and use the Pk measure to evaluate and compare the performance of the 
TextTiling and our method. Our data set - NEU_TS, is collected manually, and it 
consists of 100 Chinese documents, all from 2004-2005 Chinese People?s Daily 
newspaper. The number of segments per document varies from five to eight. The 
average number of paragraphs per document is 25.8 paragraphs. To build the ground 
truth for NEU_TS data set, five trained graduate students in our laboratory who are 
working on the analysis of Chinese document are asked to provide judgment on the 
segmentation of every Chinese document. We first use the toolkit CipSegSDK[23] for 
document preprocessing, including word segmentation, but with the removal of  
stopwords from all documents.  
1)   Experiment 1 
In the first experiment, we assume the number of segments of an input document is 
known in advance. We use the NEU_TS data set and the Pk measure to evaluate and 
compare the performance of TextTiling and our method. The purpose of this 
experiment is to compare the performance of boundary detection techniques of 
TextTiling algorithm and our model using MDA criterion functions.  
Table 1.  Pk value with known number of document segments 
Measure TextTiling algorithm MDA method 
using Jd 
MDA method 
using Jd-PF 
Pk value 0.825 0.869 0.905 
In the TextTiling algorithm, topic boundaries are determined by changes in the 
sequence of similarity scores. The boundaries are determined by locating the 
lowermost portions of valleys in the resulting plot. Therefore, it is not a global 
evaluation method. However, in our model, MDA criterion function provides a global 
evaluation method to text segmentation; it selects the best segmentation with the 
largest within-segment word similarity and the smallest between-segment word 
similarity. Results shown in Table 1 indicated that our boundary detection techniques 
based on two MDA criterion functions perform better than the TextTiling algorithm, 
and MDA criterion function Jd-PF works the best.  
Table 2. Pk value with unknown number of document segments 
Measure TextTiling algorithm MDA method 
using Jd 
MDA method 
using Jd-PF 
Pk value 0.808 0.831 0.87 
2)   Experiment 2 
In this experiment, we assume the number of segments of a document is unknown in 
advance. In other words, Texttiling algorithm and our model should determine the 
number of segments of a document automatically. Similar to Experiment 1, the same 
 Using Multiple Discriminant Analysis Approach for Linear Text Segmentation 299 
data set is used and the Pk measure is calculated for both TextTiling and our method 
using MDA criterion functions Jd and Jd-PF. The comparative results are shown  
in Table 2. 
As mentioned above, how to determine the number of segments to be assigned to a 
document is a difficult problem. Texttiling algorithm uses a simple cutoff function 
method to determine the number of segments and it is sensitive to the patterns of 
similarity scores[2,3]. The cutoff function is defined as a function of the average and 
standard deviations of the depth scores for the text under analysis. A boundary is drawn 
only if the depth score exceeds the cutoff value. We think that the simple cutoff function 
method is hard to achieve global optimization when solving these two key problems of 
linear text segmentation process. In our model, two MDA criterion functions Jd and Jd-PF 
are used to determine the number of segments and boundary detection by maximizing Jd 
score of segmentations. Once the maximum-score segmentation is found, the number of 
segments of the document is produced automatically. Experimental results show that 
our MDA criterion functions are superior to the TextTiling?s cutoff function in terms of 
automatic determination of the number of segments. It is also shown that the MDA 
criterion function Jd-PF revised with Penalty Factor works better than Jd. In 
implementation, we have adopted genetic algorithms (GAs) to alleviate the 
computational complexity of MDA, and have obtained good results. 
6   Conclusions and Future Work 
In this paper, we studied and proposed a new domain-independent statistical model 
for linear text segmentation in which multiple discriminant analysis(MDA) approach  
is used as global criterion function for document segmentation. We attempted to 
achieve global optimization in solving the two fundamental problems of text 
segmentation involving automatic boundary detection and automatic determination of 
number of segments of a document, by focusing on within-segment word similarity 
and between-segment word similarity. We also applied genetic algorithms(GAs) to 
reduce the high computational complexity of MDA based method. Experimental 
results show that our method based on MDA criterion functions outperforms the 
TextTiling algorithm.  
The solution to the high computational complexity problem will continue to be 
studied by using other effective optimization algorithm or near optimal solutions. In the 
next stage we plan to combine MDA criterion functions with other algorithms such as 
clustering to improve the performance of our text segmentation system, and apply the 
text segmentation technique to other text processing task, such as information retrieval 
and document summarization. 
Acknowledgements 
We thank Keh-Yih Su and Matthew Ma for discussions related to this work. This 
research was supported in part by the National Natural Science Foundation of China 
& Microsoft Asia Research Centre(No. 60203019), the Key Project of Chinese 
Ministry of Education(No. 104065), and the National Natural Science Foundation of 
China(No. 60473140). 
300 J. Zhu et al 
References 
1. Gerard Salton, Amit Singhal, Chris Buckley, and Mandar Mitra.: Automatic text 
decomposition using text segments and text themes. In proceedings of the seventh ACM 
conference on Hypertext, Bethesda, Maryland, United States (1996) 53-65 
2. Hearst, M.A.: Multi-paragraph segmentation of expository text. In proceedings of the 32th 
Annual Meeting of the Association for Computational Linguistics, Las Cruces, New 
Mexico (1994) 9-16 
3. Hearst, M.A.: TextTiling: segmenting text into multi-paragraph subtopic passages. 
Computational Linguistics, Vol.23, No.1 (1997) 33-64 
4. Youmans, G.: A new tool for discourse analysis: The vocabulary management profile. 
Language, Vol.67, No.4 (1991) 763-789 
5. Morris, J. and Hirst, G.: Lexical cohesion computed by thesauri relations as an indicator of 
the structure of text. Computational Linguistics, Vol.17, No.1 (1991) 21-42 
6. Kozima, H.: Text segmentation based on similarity between words. In proceedings of the 
31th Annual Meeting of the Association for Computational Linguistics, Student Session 
(1993) 286-288 
7. Reynar, J.C.: An automatic method of finding topic boundaries. In proceedings of the 32 
nd Annual Meeting of the Association for Computational Linguistics, Student Session, Las 
Cruces, New Mexico (1994) 331-333 
8. Beeferman, D., Berger, A., and Lafferty, J.: Text segmentation using exponential models. 
In proceedings of the Second Conference on Empirical Methods in Natural Language 
Processing, pages, Providence, Rhode Island (1997) 35-46 
9. Passoneau, R. and Litman, D.J.: Intention-based segmentation: Human reliability and 
correlation with linguistic cues. In proceedings of the 31st Meeting of the Association for 
Computational Linguistics (1993) 148-155 
10. Jay M. Ponte and Bruce W. Croft.: Text segmentation by topic. In proceeding of the first 
European conference on research and advanced technology for digital libraries. U.Mass. 
Computer Science Technical Report TR97-18 (1997) 
11. Reynar, J.C.: Statistical models for topic segmentation. In proceedings of the 37th Annual 
Meeting of the Association for Computational Linguistics (1999) 357-364 
12. Hirschberg, J. and Grosz, B.: Intentional features of local and global discourse. In 
proceedings of the Workshop on Spoken Language Systems (1992) 441-446 
13. Freddy Y. Y. Choi.: Advances in domain independent linear text segmentation. In Proc. of 
NAACL-2000 (2000) 
14. Choi, F.Y.Y., Wiemer-Hastings, P. & Moore, J.: Latent semantic analysis for text 
segmentation. In proceedings of the 6th Conference on Empirical Methods in Natural 
Language Processing (2001) 109-117. 
15. Blei, D.M. and Moreno, P.J.: Topic segmentation with an aspect hidden Markov model. 
Tech. Rep. CRL 2001-07, COMPAQ Cambridge Research Lab (2001) 
16. Yaari, Y.: Segmentation of expository texts by hierarchical agglomerative clustering. In 
proceedings of the conference on recent advances in natural language processing (1997) 
59-65 
17. Heinonen, O.: Optimal multi-paragraph text segmentation by dynamic programming. In 
proceedings of 17th international conference on computational linguistics (1998) 1484-
1486. 
18. Utiyama, M., and Isahara, H.: A statistical model for domain-independent text 
segmentation. In proceedings of the 9th conference of the European chapter of the 
association for computational linguistics (2001) 491-498 
 Using Multiple Discriminant Analysis Approach for Linear Text Segmentation 301 
19. A Kehagias, P Fragkou, V Petridis.: Linear Text Segmentation using a Dynamic 
Programming Algorithm. In proceedings of 10th Conference of European chapter of the 
association for computational linguistics (2003) 
20. R. Duda, P. Hart, and D. Stork.: Pattern Classification. Second Edition, John Wiley & 
Sons (2001) 
21. Julius T.Tol and Rafael C. Gonzaiez.: Pattern recognition principles. Addison-Wesley 
Publishing Company (1974) 
22. Tom M.Mitchell.: Machine Learning. McGraw-Hill (1997) 
23. Yao Tianshun, Zhu Jingbo, Zhang li, and Yang Ying.: Natural language processing-
research on making computers understand human languages. Tsinghua university  
press (2002) 
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 804 ? 814, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Semantic Role Tagging for Chinese at the Lexical Level 
Oi Yee Kwong and Benjamin K. Tsou 
Language Information Sciences Research Centre, City University of Hong Kong, 
Tat Chee Avenue, Kowloon, Hong Kong 
{rlolivia, rlbtsou}@cityu.edu.hk 
Abstract. This paper reports on a study of semantic role tagging in Chinese, in 
the absence of a parser.  We investigated the effect of using only lexical infor-
mation in statistical training; and proposed to identify the relevant headwords in 
a sentence as a first step to partially locate the corresponding constituents to be 
labelled.  Experiments were done on a textbook corpus and a news corpus, rep-
resenting simple data and complex data respectively.  Results suggested that in 
Chinese, simple lexical features are useful enough when constituent boundaries 
are known, while parse information might be more important for complicated 
sentences than simple ones. Several ways to improve the headword identifica-
tion results were suggested, and we also plan to explore some class-based tech-
niques for the task, with reference to existing semantic lexicons. 
1   Introduction 
As the development of language resources progresses from POS-tagged corpora to 
syntactically annotated treebanks, the inclusion of semantic information such as 
predicate-argument relations is becoming indispensable.  The expansion of the Penn 
Treebank into a Proposition Bank [11] is a typical move in this direction.  Lexical 
resources also need to be enhanced with semantic information (e.g. [5]).  In fact the 
ability to identify semantic role relations correctly is essential to many applications 
such as information extraction and machine translation; and making available re-
sources with this kind of information would in turn facilitate the development of such 
applications. 
Large-scale production of annotated resources is often labour-intensive, and thus 
needs automatic labelling to streamline the work.  The task can essentially be per-
ceived as a two-phase process, namely to recognise the constituents bearing some 
semantic relationship to the target verb in a sentence, and then to label them with the 
corresponding semantic roles. 
In their seminal proposal, Gildea and Jurafsky approached the task using various 
features such as headword, phrase type, and parse tree path [6].  Such features have 
remained the basic and essential features in subsequent research, irrespective of the 
variation in the actual learning components.  In addition, parsed sentences are often 
required, for extracting the path features during training and providing the argument 
boundaries during testing.  The parse information is deemed important for the per-
formance of role labelling [7, 8]. 
More precisely, in semantic role labelling, parse information is rather more critical 
for the identification of boundaries for candidate constituents than for the extraction 
 Semantic Role Tagging for Chinese at the Lexical Level 805 
of training data.  Its limited function in training, for instance, is reflected in the low 
coverage reported (e.g. [21]).  However, given the imperfection of existing automatic 
parsers, which are far from producing gold standard parses, many thus resort to shal-
low syntactic information from simple chunking, though results often turn out to be 
less satisfactory than with full parses. 
This limitation is even more pertinent for the application of semantic role labelling 
to languages which do not have sophisticated parsing resources.  In the case of Chi-
nese, for example, there is considerable variability in its syntax-semantics interface; 
and when one has more nested and complex sentences such as those from news arti-
cles, it becomes more difficult to capture the sentence structures by typical examples. 
It is therefore worthwhile to investigate alternatives to the role labelling task for 
Chinese under the parsing bottleneck, both in terms of the features used and the short-
cut or compromise to at least partially pin down the relevant constituents.  A series of 
related questions deserve consideration here: 
1. how much could we achieve with only parse-independent features in the role la-
belling process; 
2. with constituent boundaries unknown in the absence of parse information, could 
we at least identify the headwords in the relevant constituents to be tagged; and 
3. whether the unknown boundary problem varies with the nature of the dataset, 
e.g., will the degradation in performance from known boundaries to unknown 
boundaries be more serious for complicated sentences than for simple  
sentences. 
So in the current study we experiment on the use of parse-independent features for 
semantic role labelling in Chinese, for locating the headwords of the constituents 
corresponding to arguments to be labelled.  We will also compare the results on two 
training and testing datasets. 
In Section 2, related work will be reviewed.  In Section 3, the data used in the cur-
rent study will be introduced.  Our proposed method will be explained in Section 4, 
and the experiment reported in Section 5.  Results and future work will be discussed 
in Section 6, followed by conclusions in Section 7. 
2   Related Work 
The definition of semantic roles falls on a continuum from abstract ones to very spe-
cific ones.  Gildea and Jurafsky [6], for instance, used a set of roles defined according 
to the FrameNet model [2], thus corresponding to the frame elements in individual 
frames under a particular domain to which a given verb belongs.  Lexical entries (in 
fact not limited to verbs, in the case of FrameNet) falling under the same frame will 
share the same set of roles.  Gildea and Palmer [7] defined roles with respect to indi-
vidual predicates in the PropBank, without explicit naming.  To date PropBank and 
FrameNet are the two main resources in English for training semantic role labelling 
systems. 
The theoretical treatment of semantic roles is also varied in Chinese.  In practice, 
for example, the semantic roles in the Sinica Treebank mark not only verbal argu-
ments but also modifier-head relations within individual constituents, following a 
806 O.Y. Kwong and B.K. Tsou 
head-driven principle [4].  In our present study, we use a set of more abstract semantic 
roles, which are generalisable to most Chinese verbs and are not dependent on par-
ticular predicates.  They will be further introduced in Section 3. 
The major concerns in automatic semantic role labelling include the handling of al-
ternations (as in ?the window broke? and ?John broke the window?, where in both 
cases ?the window? should be tagged as ?patient? despite its appearance in different 
positions in the sentences), and generalisation to unseen constituents and predicates.  
For the latter, clustering and semantic lexicons or hierarchies have been used (e.g. 
[6]), or similar argument structures are assumed for near-synonyms and verbs under 
the same frame (e.g. [11]). 
Approaches in automatic semantic role labelling are mostly statistical, typically 
making use of a number of features extracted from parsed training sentences.  In 
Gildea and Jurafsky [6], the features studied include phrase type (pt), governing cate-
gory (gov), parse tree path (path), position of constituent with respect to the target 
predicate (position), voice (voice), and headword (h).  The labelling of a constituent 
then depends on its likelihood to fill each possible role r given the features and the 
target predicate t, as in the following, for example: 
),,,,,|( tvoicepositiongovpthrP    
Subsequent studies exploited a variety of implementation of the learning compo-
nent, including Maximum Entropy (e.g. [1, 12]), Support Vector Machines (e.g. [9, 
16]), etc.  Transformation-based approaches were also used (e.g. [10, 19]).  Swier and 
Stevenson [17] innovated with an unsupervised approach to the problem, using a 
bootstrapping algorithm, and achieved 87% accuracy. 
While the estimation of the probabilities could be relatively straightforward, the 
key often lies in locating the candidate constituents to be labelled.  A parser of some 
kind is needed.  Gildea and Hockenmaier [8] compared the effects of Combinatory 
Categorial Grammar (CCG) derivations and traditional Treebank parsing, and found 
that the former performed better on core arguments, probably due to its ability to 
capture long range dependencies, but comparable for all arguments.  Gildea and 
Palmer [7] compared the effects of full parsing and shallow chunking; and found that 
when constituent boundaries are known, both automatic parses and gold standard 
parses resulted in about 80% accuracy for subsequent automatic role tagging, but 
when boundaries are unknown, results with automatic parses dropped to 57% preci-
sion and 50% recall.  With chunking only, performance further degraded to below 
30%.  Problems mostly arise from arguments which correspond to more than one 
chunk, and the misplacement of core arguments. 
A couple of evaluation exercises for semantic role labelling were organized re-
cently, such as the shared task in CoNLL-2004 using PropBank data [3], and the one 
in SENSEVAL-3 using the FrameNet dataset [15].  Most systems in SENSEVAL-3 
used a parser to obtain full syntactic parses for the sentences, whereas systems par-
ticipating in the CoNLL task were restricted to using only shallow syntactic informa-
tion.  Results reported in the former tend to be higher.  Although the dataset may be a 
factor affecting the labelling performance, it nevertheless reinforces the usefulness of 
full syntactic information. 
 Semantic Role Tagging for Chinese at the Lexical Level 807 
According to Carreras and M?rquez [3], for English, the state-of-the-art results 
reach an F1 measure of slightly over 83 using gold standard parse trees and about 77 
with real parsing results.  Those based on shallow syntactic information is about 60. 
The usefulness of parse information for semantic role labelling would be especially 
interesting in the case of Chinese, given the flexibility in its syntax-semantics interface 
(e.g. the object after ? ?eat? could refer to the Patient as in ??? ?eat apple?, Loca-
tion as in ??? ?eat canteen?, Duration as in ??? ?eat three years?, etc.).  In the 
absence of sophisticated parsing resources, however, we attempt to investigate how 
well one could simply use a set of parse-independent features and backward guess the 
likelihood of headwords to partially locate the candidate constituents to be labelled. 
3   The Data 
3.1   Materials 
As mentioned in the introduction, we attempted to investigate the difference between 
labelling simple sentences and complex ones.  For this purpose, sentences from pri-
mary school textbooks were taken as examples for simple data, while sentences from 
a large corpus of newspaper texts were taken as complex examples. 
Two sets of primary school Chinese textbooks popularly used in Hong Kong were 
taken for reference.  The two publishers were Keys Press [22] and Modern Education 
Research Society Ltd [23].  Texts for Primary One to Six were digitised, segmented 
into words, and annotated with parts-of-speech (POS).  The two sets of textbooks 
amount to a text collection of about 165K character tokens and upon segmentation 
about 109K word tokens (about 15K word types).  There were about 2,500 transitive 
verb types, with frequency ranging from 1 to 926. 
The complex examples were taken from a subset of the LIVAC synchronous cor-
pus1 [13, 18].   The subcorpus consists of newspaper texts from Hong Kong, including 
local news, international news, financial news, sports news, and entertainment news, 
collected in 1997-98.  The texts were segmented into words and POS-tagged, amount-
ing to about 1.8M character tokens and upon segmentation about 1M word tokens 
(about 47K word types).  There were about 7,400 transitive verb types, with fre-
quency ranging from 1 to just over 6,300. 
3.2   Training and Testing Data 
For the current study, a set of 41 transitive verbs common to the two corpora (hereaf-
ter referred to as textbook corpus and news corpus), with frequency over 10 and over 
50 respectively, was sampled.   
Sentences in the corpora containing the sampled verbs were extracted.  Constituents 
corresponding to semantic roles with respect to the target verbs were annotated by a 
trained annotator, whose annotation was verified by another.  In this study, we worked 
with a set of 11 predicate-independent abstract semantic roles.  According to the Dic-
tionary of Verbs in Contemporary Chinese (Xiandai Hanyu Dongci Dacidian, ???
??????) [14], our semantic roles include the necessary arguments for most 
                                                          
1
 http://www.livac.org 
808 O.Y. Kwong and B.K. Tsou 
verbs such as Agent and Patient, or Goal and Location in some cases; and some op-
tional arguments realised by adjuncts, such as Quantity, Instrument, and Source.  Some 
examples of semantic roles with respect to a given predicate are shown in Fig. 1. 
 
Fig. 1. Examples of semantic roles with respect to a given predicate 
Altogether 980 sentences covering 41 verb types in the textbook corpus were anno-
tated, resulting in 1,974 marked semantic roles (constituents); and 2,122 sentences 
covering 41 verb types in the news corpus were annotated, resulting in 4,933 marked 
constituents2. 
The role labelling system was trained on 90% of the sample sentences from the 
textbook corpus and the news corpus separately; and tested on the remaining 10% of 
the respective corpora.   
4   Automatic Role Labelling 
The automatic labelling was based on the statistical approach in Gildea and Jurafsky 
[6].  In Section 4.1, we will briefly mention the features employed in the training 
process.  Then in Sections 4.2 and 4.3, we will explain our approach for locating 
headwords in candidate constituents associated with semantic roles, in the absence of 
parse information. 
4.1   Training 
In this study, our probability model was based mostly on parse-independent features 
extracted from the training sentences, namely: 
                                                          
2
  These figures only refer to the samples used in the current study.  In fact over 35,000 sen-
tences in the LIVAC corpus have been semantically annotated, covering about 1,500 verb 
types and about 80,000 constituents were marked. 
? ?? ?? ?? ? ?? ??
Next week school hold tell story contest 
Time Agent Target Patient 
Example: (Next week, the school will hold a story-telling contest.) 
?? ? ?? 
??
??
? ?
(-pl) write essay always feel not anything 
Experiencer Target Theme 
Example: (Students always feel there is nothing to write about for their essays.) 
?  
?
time 
??
? 
can 
Time 
Student write 
 Semantic Role Tagging for Chinese at the Lexical Level 809 
Headword (head): The headword from each constituent marked with a semantic role 
was identified.  For example, in the second sentence in Fig. 1, ?? (school) is the 
headword in the constituent corresponding to the Agent of the verb ?? (hold), and 
?? (contest) is the headword of the noun phrase corresponding to the Patient. 
Position (posit): This feature shows whether the constituent being labelled appears 
before or after the target verb.  In the first example in Fig. 1, the Experiencer and 
Time appear on the left of the target, while the Theme is on its right. 
POS of headword (HPos): Without features provided by the parse, such as phrase 
type or parse tree path, the POS of the headword of the labelled constituent could 
provide limited syntactic information. 
Preposition (prep): Certain semantic roles like Time and Location are often realised 
by prepositional phrases, so the preposition introducing the relevant constituents 
would be an informative feature. 
Hence for automatic labelling, given the target verb t, the candidate constituent, 
and the above features, the role r which has the highest probability for P(r | head, 
posit, HPos, prep, t) will be assigned to that constituent.  In this study, however, we 
are also testing with the unknown boundary condition where candidate constituents 
are not available in advance, hence we attempt to partially locate them by identifying 
their headwords to start with.  Our approach is explained in the following sections. 
4.2   Locating Candidate Headwords 
In the absence of parse information, and with constituent boundaries unknown, we 
attempt to partially locate the candidate constituents by trying to identify their corre-
sponding headwords first.  Sentences in our test data were segmented into words and 
POS-tagged.  We thus divide the recognition process into two steps, locating the 
headword of a candidate constituent first, and then expanding from the headword to 
determine its boundaries. 
Basically, if we consider every word in the same sentence as the target verb (both 
to its left and to its right) a potential headword for a candidate constituent, what we 
need to do is to find out the most probable words in the sentence to match against 
individual semantic roles.  We start with a feature set with more specific distributions, 
and back off to feature sets with less specific distributions.  Hence in each round we 
look for 
)|(maxarg setfeaturerP
r
 
for every candidate word.  Ties are resolved by giving priority to the word nearest to 
the target verb in the sentence. 
Fig. 2 shows an example illustrating the procedures for locating candidate head-
words.  The target verb is ?? (discover).  In the first round, using features head, 
posit, HPos, and t, ?? (time) and ?? (problem) were identified as Time and Pa-
tient respectively.  In the fourth subsequent round, backing off with features posit and 
HPos, ?? (we) was identified as a possible Agent.  In this round a few other words 
were identified as potential Patients.  However, since Patient was already located in 
810 O.Y. Kwong and B.K. Tsou 
the previous round, those come up in this round are not considered.  So in the end the 
headwords identified for the test sentence are ?? (we) for Agent, ?? (problem) 
for Patient and ?? (time) for Time. 
 
Fig. 2. Example illustrating the procedures for locating candidate headwords 
4.3   Constituent Boundary 
Upon the identification of headwords for potential constituents, the next step is to 
expand from these headwords for constituent boundaries.  Although we are not doing 
this step in the current study, it can potentially be done via some finite state tech-
niques, or better still, with shallow syntactic processing like simple chunking if  
available. 
5   The Experiment 
5.1   Testing 
The system was trained and tested on the textbook corpus and the news corpus  
respectively.  The testing was done under the ?known constituent? and ?unknown 
constituent? conditions.  The former essentially corresponds to the known-boundary 
condition in related studies; whereas in the unknown-constituent condition, which we 
will call ?headword location? condition hereafter, we tested our method of locating 
candidate headwords as explained above in Section 4.2.  In this study, every noun, 
verb, adjective, pronoun, classifier, and number within the test sentence containing 
the target verb was considered a potential headword for a candidate constituent  
Sentence: 
?????????????????????????????????????? 
During revision, we discover a lot of problems which we have not thought of or cannot be 
solved, then we go and ask father. 
Candidate  Round 1 ? Round 4  Final Result 
Headwords 
 
?? (revision)    Patient 
?? (time)  Time     ----       Time 
?? (we)    Agent       Agent 
?? (normally) 
?? (think)    Patient 
? (can) 
?? (solve)    Patient 
?? (problem)  Patient     ----       Patient 
? (go)     Patient 
? (ask)    Patient 
?? (father)    Patient 
 Semantic Role Tagging for Chinese at the Lexical Level 811 
corresponding to some semantic role.  The performance was measured in terms of the 
precision (defined as the percentage of correct outputs among all outputs), recall (de-
fined as the percentage of correct outputs among expected outputs), and F1 score 
which is the harmonic mean of precision and recall. 
5.2   Results 
The results are shown in Table 1, for testing on both the textbook corpus and the news 
corpus under the known constituent condition and the headword location condition. 
Table 1. Results on two datasets for known constituents and headword location 
 Textbook Data News Data 
 Precision Recall F1 Precision Recall F1 
Known Constituent 93.85 87.50 90.56 90.49 87.70 89.07 
Headword Location 46.12 61.98 52.89 38.52 52.25 44.35 
Under the known constituent condition, the results were good on both datasets, 
with an F1 score of about 90.  This is comparable or even better to the results reported 
in related studies for known boundary condition.  The difference is that we did not use 
any parse information in the training, not even phrase type.  Our results thus suggest 
that for Chinese, even without more complicated syntactic information, simple lexical 
information might already be useful in semantic role tagging. 
Comparison of the known constituent condition with the headword location condi-
tion shows that performance for the latter has expectedly dropped.  However, the 
degradation was less serious with simple sentences than with complex ones, as is seen 
from the higher precision and recall for textbook data than for news data under the 
headword location condition.  What is noteworthy here is that recall apparently dete-
riorated less seriously than precision.  In the case of news data, for instance, we were 
able to maintain over 50% recall but only obtained about 39% precision.  The surpris-
ingly low precision is attributed to a technical inadequacy in the way we break ties.  
In this study we only make an effort to eliminate multiple tagging of the same role to 
the same target verb in a sentence on either side of the target verb, but not if they 
appear on both sides of the target verb.  This should certainly be dealt with in future 
experiments.  The differential degradation of performance between textbook data and 
news data also suggests the varied importance of constituent boundaries to simple 
sentences and complex ones, and hence possibly their varied requirements for full 
parse information for the semantic labelling task. 
6   Discussion 
According to Carreras and M?rquez [3], the state-of-the-art results for semantic role 
labelling systems based on shallow syntactic information is about 15 lower than 
those with access to gold standard parse trees, i.e., around 60.  Our experimental 
results for the headword location condition, with no syntactic information available 
812 O.Y. Kwong and B.K. Tsou 
at all, give an F1 score of 52.89 and 44.35 respectively for textbook data and news 
data. This further degradation in performance is nevertheless within expectation, 
but whether this is also a result of the difference between English and Chinese  
remains to be seen. 
In response to the questions raised in the introduction, firstly, the results for the 
known constituent condition (F1 of 90.56 and 89.07 for textbook data and news data 
respectively) have shown that even if we do not use parse-dependent features such as 
governing category and parse tree path, results are not particularly affected.  In other 
words, lexical features are already very useful as long as the constituent boundaries 
are given.  Secondly, in the absence of parse information, the results of identifying the 
relevant headwords in order to partially locate candidate constituents were not as 
satisfactory as one would like to see.  One possible way to improve the results, as 
suggested above, would be to improve the handling of ties.  Other possibilities includ-
ing a class-based method could also be used, as will be discussed below.  Thirdly, 
results for news data degraded more seriously than textbook data from the known 
constituent condition to the headword location condition.  This suggests that complex 
sentences in Chinese are more affected by the availability of full parse information.  
To a certain extent, this might be related to the relative flexibility in the syntax-
semantics interface of Chinese; hence when a sentence gets more complicated, there 
might be more intervening constituents and the parse information would be useful to 
help identify the relevant ones in semantic role labelling.   
In terms of future development, apart from improving the handling of ties in our 
method, as mentioned in the previous section, we plan to expand our work in several 
respects, the major part of which is on the generalization to unseen headwords and 
unseen predicates.  As is with other related studies, the examples available for training 
for each target verb are very limited; and the availability of training data is also insuf-
ficient in the sense that we cannot expect them to cover all target verb types.  Hence it 
is very important to be able to generalize the process to unseen words and predicates.  
To this end, we will experiment with a semantic lexicon like Tongyici Cilin (???
??, a Chinese thesaurus) in both training and testing, which we expect to improve 
the overall performance. 
Another area of interest is to look at the behaviour of near-synonymous predicates 
in the tagging process.  Many predicates may be unseen in the training data, but while 
the probability estimation could be generalized from near-synonyms as suggested by a 
semantic lexicon, whether the similarity and subtle differences between near-
synonyms with respect to the argument structure and the corresponding syntactic 
realisation could be distinguished would also be worth studying.  Related to this is the 
possibility of augmenting the feature set with semantic features.  Xue and Palmer 
[20], for instance, looked into new features such as syntactic frame, lexicalized con-
stituent type, etc., and found that enriching the feature set improved the labelling 
performance. 
Another direction of future work is on the location of constituent boundaries upon 
the identification of the headword.  As mentioned earlier on, this could probably be 
tackled by some finite state techniques or with the help of simple chunkers. 
 Semantic Role Tagging for Chinese at the Lexical Level 813 
7   Conclusion 
The study reported in this paper has thus tackled the unknown constituent boundary 
condition in semantic role labelling for Chinese, by attempting to locate the corre-
sponding headwords first.  We experimented with both simple and complex data.  
Using only parse-independent features, our results on known boundary condition are 
comparable to those reported in related studies.  Although the results for headword 
location condition were not as good as state-of-the-art performance with shallow 
syntactic information, we have nevertheless suggested some possible ways to improve 
the results.  We have further observed that the influence of full syntactic information 
is more serious for complex data than simple data, which might be a consequence of 
the characteristic syntax-semantics interface of Chinese.  As a next step, we plan to 
explore some class-based techniques for the task, with reference to existing  
semantic lexicons. 
Acknowledgements 
This work is supported by Competitive Earmarked Research Grants (CERG) of the 
Research Grants Council of Hong Kong under grant Nos. CityU1233/01H and 
CityU1317/03H. 
References 
1. Baldewein, U., Erk, K., Pad?, S. and Prescher, D. (2004)  Semantic Role Labelling With 
Chunk Sequences.  In Proceedings of the Eighth Conference on Computational Natural 
Language Learning (CoNLL-2004), Boston, Massachusetts, pp.98-101. 
2. Baker, C.F., Fillmore, C.J. and Lowe, J.B. (1998)  The Berkeley FrameNet Project.  In 
Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics 
and the 17th International Conference on Computational Linguistics (COLING-ACL ?98), 
Montreal, Quebec, Canada, pp.86-90. 
3. Carreras, X. and M?rquez, L. (2004)  Introduction to the CoNLL-2004 Shared Task: Se-
mantic Role Labeling.  In Proceedings of the Eighth Conference on Computational Natu-
ral Language Learning (CoNLL-2004), Boston, Massachusetts, pp.89-97. 
4. Chen, F-Y., Tsai, P-F., Chen, K-J. and Huang, C-R. (1999)  Sinica Treebank (?????
???????). Computational Linguistics and Chinese Language Processing, 4(2): 
87-104. 
5. Fellbaum, C., Palmer, M., Dang, H.T., Delfs, L. and Wolf, S. (2001)  Manual and Auto-
matic Semantic Annotation with WordNet.  In Proceedings of the NAACL-01 SIGLEX 
Workshop on WordNet and Other Lexical Resources, Invited Talk, Pittsburg, PA. 
6. Gildea, D. and Jurafsky, D. (2002)  Automatic Labeling of Semantic Roles.  Computa-
tional Linguistics, 28(3): 245-288. 
7. Gildea, D. and Palmer, M. (2002)  The Necessity of Parsing for Predicate Argument Rec-
ognition.  In Proceedings of the 40th Meeting of the Association for Computational Lin-
guistics (ACL-02), Philadelphia, PA. 
8. Gildea, D. and Hockenmaier, J. (2003)  Identifying Semantic Roles Using Combinatory 
Categorial Grammar.  In Proceedings of the 2003 Conference on Empirical Methods in 
Natural Language Processing, Sapporo, Japan. 
814 O.Y. Kwong and B.K. Tsou 
9. Hacioglu, K., Pradhan, S., Ward, W., Martin, J.H. and Jurafsky, D. (2004)  Semantic Role 
Labeling by Tagging Syntactic Chunks.  In Proceedings of the Eighth Conference on 
Computational Natural Language Learning (CoNLL-2004), Boston, Massachusetts, 
pp.110-113. 
10. Higgins, D. (2004)  A transformation-based approach to argument labeling.  In Proceed-
ings of the Eighth Conference on Computational Natural Language Learning (CoNLL-
2004), Boston, Massachusetts, pp.114-117. 
11. Kingsbury, P. and Palmer, M. (2002)  From TreeBank to PropBank.  In Proceedings of the 
Third Conference on Language Resources and Evaluation (LREC-02), Las Palmas, Ca-
nary Islands, Spain. 
12. Kwon, N., Fleischman, M. and Hovy, E. (2004)  SENSEVAL Automatic Labeling of Se-
mantic Roles using Maximum Entropy Models.  In Proceedings of the Third International 
Workshop on the Evaluation of Systems for the Semantic Analysis of Text (SENSEVAL-3), 
Barcelona, Spain, pp.129-132. 
13. Kwong, O.Y. and Tsou, B.K. (2003) Categorial Fluidity in Chinese and its Implications 
for Part-of-speech Tagging. In Proceedings of the Research Note Session of the 10th Con-
ference of the European Chapter of the Association for Computational Linguistics, Buda-
pest, Hungary, pp.115-118. 
14. Lin, X., Wang, L. and Sun, D. (1994)  Dictionary of Verbs in Contemporary Chinese.  
Beijing Language and Culture University Press. 
15. Litkowski, K.C. (2004) SENSEVAL-3 Task: Automatic Labeling of Semantic Roles.  In 
Proceedings of the Third International Workshop on the Evaluation of Systems for the Se-
mantic Analysis of Text (SENSEVAL-3), Barcelona, Spain, pp.9-12. 
16. Moldovan, D., Girju, R., Olteanu, M. and Fortu, O. (2004)  SVM Classification of Frame-
Net Semantic Roles.  In Proceedings of the Third International Workshop on the Evalua-
tion of Systems for the Semantic Analysis of Text (SENSEVAL-3), Barcelona, Spain, 
pp.167-170. 
17. Swier, R.S. and Stevenson, S. (2004)  Unsupervised Semantic Role Labelling.  In Pro-
ceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, 
Barcelona, Spain, pp.95-102. 
18. Tsou, B.K., Tsoi, W.F., Lai, T.B.Y., Hu, J. and Chan, S.W.K. (2000)  LIVAC, A Chinese 
Synchronous Corpus, and Some Applications.  In Proceedings of the ICCLC International 
Conference on Chinese Language Computing, Chicago, pp. 233-238. 
19. Williams, K., Dozier, C. and McCulloh, A. (2004)  Learning Transformation Rules for 
Semantic Role Labeling.  In Proceedings of the Eighth Conference on Computational 
Natural Language Learning (CoNLL-2004), Boston, Massachusetts, pp.134-137. 
20. Xue, N. and Palmer, M. (2004)  Calibrating Features for Semantic Role Labeling.  In Pro-
ceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, 
Barcelona, Spain, pp.88-94. 
21. You, J-M. and Chen, K-J. (2004)  Automatic Semantic Role Assignment for a Tree Struc-
ture.  In Proceedings of the 3rd SigHAN Workshop on Chinese Language Processing, 
ACL-04, Barcelona, pp.109-115. 
22. ?????? Qisi Zhongguo Yuwen.  Primary 1-6, 24 volumes, 2004.  Hong Kong: Keys 
Press. 
23. ?????? Xiandai Zhongguo Yuwen.  Primary 1-6, 24 volumes, 2004.  Hong Kong: 
Modern Education Research Society Ltd. 
Mining Discourse Markers for Chinese Textual Summarization 
Samuel W. K. Chan I, Tom B. Y. Lai 2, W. J. Gao 3, Benjamin K. T'sou 4 
J 24Languag e Information Sciences Research Centre 
City Un!versity of Hong Kong 
Tat CheeAvenue, Kowloon Tong, 
Hong Kong SAR, China 
3 North Eastern University, China 
Iswkchan@cs.cityu.edu.hk, {2cttomlai, 4rlbtsou} @cpccux0.cityu.edu.hk, 3wjgao@ramm.neu.edu.cn 
Abstract 
Discourse markers foreshadow the message 
thrust of texts and saliently guide their 
rhetorical structure which are important for 
content filtering and text abstraction. This 
paper reports on efforts to automatically 
identify and classify discourse markers in 
Chinese texts using heuristic-based and 
corpus-based ata-mining methods, as an 
integral part of automatic text 
summarization via rhetorical structure and 
Discourse Markers. Encouraging results are 
reported. 
1 Introduction 
Discourse is understood to refer to any form of 
language-based communication involving multiple 
sentences or utterances. The most important forms 
of discourse of interest o computerized natural 
.language processing are text and dialogue. While 
discourse such as written text normally appears to 
? be a linear sequence of clauses and sentences, it 
has "long been recognized by linguists that these 
clauses and sentences tend to cluster together into 
units, called discourse segments, that are related 
pragmatically toform a hierarchical structure. 
Discourse analysis goes beyond the levels of 
syntactic and semantic analysis, which typically 
treats each sentence as an isolated, independent 
unit. The function of discourse analysis is to 
divide a text into discourse segments, and to 
recognize and re-construct the discourse structure 
of the text as intended by its author. Results of 
discourse analysis can be used to solve many 
important NLP problems such as anaphoric 
reference (Hirst 1981), tense and aspect analysis 
(Hwang and Schubert 1992), intention recognition 
(Grosz and Sidner 1986; Litman and Allen 1990), 
or'can be directly applied to computational NLP 
applications uch as text abstraction (Ono et al 
1994; T'sou et al 1996) and text generation 
(McKeown 1985; Lin et al 1991). 
Automatic text abstraction has received 
considerable attention (see Paice (1990) for a 
comprehensive r view). While some statistical 
approaches have had some success in extracting 
one or more sentences which can serve as a 
summary (Brandow et al 1995; Kupiec et al 1995; 
Salton et al 1997), summarization i  general has 
remained an elusive task. McKeown and Radev 
(1995) develop a system SUMMONS to 
summarize full text input using templates 
produced by the message understanding systems, 
developed under ARPA human language 
technology. Unlike previous approaches, their 
system summarizes a series of news articles on the 
same event, producing a paragraph consisting of 
one or more sentences. Endres-Niggemeyer et al 
(1995) uses a blackboard system architecture with 
co-operating object-oriented agents and a dynamic 
text representation which borrows its conceptual 
relations from Rhetorical Structure Theory (RST) 
(Mann and Thompson 1986). Furthermore, 
connectionist models of discourse summarization 
have also attracted a lot of attention (Aretoulaki et 
al. 1998). The main underlying principles are the 
distributed encoding of concepts and the 
simulation of human association with a large 
amount of processing nodes. What is crucial in 
this approach is to provide a subconceptual l yer 
in the linguistic reasoning. 
As in Paice (1990), summarization 
techniques in text analysis are severely impaired 
by the absence of a generally accepted iscourse 
11 
model and the use of superstructural schemes is 
promising for abstracting text. Johnson et al (1993) 
describes a text processing system that can 
identify anaphors o that they may be utilized to 
enhance sentence selection. It is based on the 
assumption that sentences which contain non- 
anaphoric noun phrases and introduce key 
concepts into the text are worthy of inclusion in an 
abstract. Ono et al (1994), T'sou et al (1992) and 
Marcu (1997) focus on discourse structure in 
summarization using the Rhetorical Structure 
Theory (RST). The theory has been exploited in a. 
number of computational systems (e.g. Hovy 
1993). The main idea is to build a discourse tree 
where each node of the tree represents a RST 
relation. Summarization is achieved by trimming 
unimportant sentences on the basis of the relative 
saliency or rhetorical relations. On the other hand, 
cohesion can also provide context o aid in the 
resolution of ambiguity as well as in text 
summarization (Halliday and Hasan 1976; Morris 
and Hirst 1991; Hearst 1997). Mani et al (1998) 
describes a method based on text coherence which 
models text in terms of macro-level relations 
between clauses or sentences tohelp determine the 
overall argumentative structure of the text. They 
examine the extent to which cohesion and 
coherence can each be used to establish saliency of 
textual units. 
The SIFAS (S,yntactic Marker based Eull- 
Text Abstration System) system has been designed 
and implemented to use discourse markers in the 
automatic summarization of Chinese. Section 2 
provides an introduction to discourse markers in 
Chinese. An overview of SIFAS is presented in 
Section 3. In Section 4, we describe a coding 
scheme for tagging every discourse marker 
appearing in the SIFAS corpus. In Section 5, we 
introduce a heuristic-based algorithm for 
automatic tagging of discourse markers. In Section 
6, we describe the application of the C4.5 
algorithm to the same task. In Section 7, we 
present he evaluation results of applying the two 
algorithms to corpus tagging, followed by a 
conclusion. 
2 Chinese Discourse Markers 
Among all kinds of information that may be found 
in a piece of discourse, discourse markers (also 
known as discourse connectives, clue words 
(Reichman 1978; Siegel et al 1994) or cue phrases 
(Grosz et al 1986; Litman 1996) are regarded as 
the major linguistic deviceavailable for a writer to 
structure a discourse. Discourse markers are 
expressions which signal a sequential relationship 
between the current basic message and the 
previous discourse. Schiffrin (1987) is concerned 
with elements which mark sequentially dependent 
units of discourse. She examines discourse 
markers in interview data, looking specifically at 
their distribution and their particular 
interpretation(s). She proposes that these markers 
typically serve three functions: (i) they index 
adjacent utterances to the speaker, the hearer, or 
both; (ii) they index adjacent utterances to prior 
and/or subsequent discourse; (iii) they work as 
contextual coordinates for utterances by locating 
them on one or more planes of her discourse 
model. 
Discourse markers also figure prominently in
Chinese which has a tendency to delay topic 
introduction (Kaplan 1996; Kirkpatrick 1993). 
Hinds (1982) and Kong (1998) also maintain that 
the Chinese tendency of delayed topic introduction 
is heavily influenced by the qi cheng zhuan he 
canonical structure (a Chinese rhetorical pattern). 
In a study examining rhetorical structure in 
Chinese, Kirkpatrick (1993) found that several 
major patterns, favored and considered to be good 
style by native Chinese writers, are hinted at by 
Chinese discourse markers. Although the effect of 
discourse markers in other languages might not be 
too prominent, here is a great necessity to study 
discourse markers in Chinese in order to capture 
the major associated rhetorical patterns in Chinese 
texts. While the full semantic understanding in
Chinese texts is obviously much more difficult to 
accomplish, the approach using text mining 
techniques in identifying discourse markers and 
associated rhetorical structures in a sizeable 
Chinese corpus will be certainly beneficial to any 
language processing, such as summarization and 
knowledge xtraction i  Chinese. 
In Chinese, two distinct classes of discourse 
markers are useful for identification and 
interpretation of the discourse structure of a 
Chinese text: pr imary discourse markers and 
secondary discourse markers (T'sou et al 1999). 
Discourse markers can be either words or phrases. 
Table 1 provides a sample listing of various 
12 
I 
I 
I 
I 
i 
I 
I 
rhetorical relations and examples considered in 
this research. 
\[Discourse Type 
Sufficiency 
Necessity 
Causality 
Deduction 
?\[dversativity 
Concession 
Conjunction 
Disjunction 
Progression 
Table 
Discourse 
Primary Marker 
ruguo 'if', name 'then' 
zhiyou 'only if', cai 'only \[hen' 
?inwei 'because', suoyi 'therefore' 
iiran 'given that', name 'then' 
suiran 'although', danshi 'but' 
"ishi 'even if', rengran 'still' 
chule 'except', j ianzhi 'also' 
huozhe 'or', huozhe 'or' 
~udan 'not only', erqie 'but also' 
/ 
Examples of Discourse Markers 
Markers 
Discourse Type 
Summary 
Contrast 
fflustration 
Specification 
Generalization 
Digression 
rtemization 
Paraphrasing 
Equivalence 
Enquiry 
ludgment 
Secondary Marker  
zong er yan zhi 'in one word' 
~hishi shang 'in fact' 
liru 'for example' 
tebie shi 'in particular' 
dati er yan 'in general' 
wulun ruhe 'anyway'  
shouxian 'first', qici "next" 
huan ju hua shuo 'in other words' 
zhengru 'just as' 
nandao ('does it mean... ')  
kexi 'unfortunately' 
and Associated Rhetorical Relations in Chinese 
It may be noted that our analysis of Chinese 
has yielded about 150 discourse markers, and that 
on the average, argumentative t xt (e.g. editorials) 
in Chinese shows more than one third of the 
discourse segments to contain discourse markers. 
While primary discourse markers can be paired 
discontinuous constituents, with each marker 
attached to one of the two utterances or 
propositions, the socondary discourse markers 
tend to be unitary constituents only. In the case of 
primary discourse markers, it is quite common that 
one member of the pair is deleted, unless for 
emphasis. The deletion of both discourse markers 
ts also possible. The recovery process therefore 
faces considerable challenge ven when concerned 
? with the deletion of only one member of the paired 
discourse markers. Since these discourse markers 
'have no unique lexical realization, there is also the 
need for disambiguation i  a homocode problem. 
Moreover, primary discourse markers can 
also be classified as simple adverbials, as is the 
case in English: 
(I) Even though a child, John is so tall that 
he has problem getting half-fare. 
(2) Even though a child, (because) John is 
tall, so he has problem getting half-fare. 
In (1), so is usually classified as an adverb 
within a sentence, but in (2) so is recognized as 
marking a change in message thrust at the 
discourse level. 
In the deeper linguistic analysis the two so's 
may be related, for they refer to a situation 
involving excessive height with implied 
consequence which may or may not be stated. In 
terms of the surface syntactic structure, so in (1) 
can occur in a simple (exclamatory) sentence (e.g. 
"John is so tall!"), but so in (2) must occur in the 
context of complex sentences. Our concern in this 
project is to identify so in the discourse sense as in 
(2) in contrast to so used as an adverb in the 
sentential sense as in (1). Similar difficulties are 
found in Chinese, as discussed in Section 7. 
3 SIFAS System Architecture 
From the perspective of discourse analysis, the 
study of discourse markers basically involves four 
distinct but fundamental issues: 1) the occurrence 
and the frequency of occurrence of discourse 
markers (Moser and Moore 1995), 2) determining 
whether a candidate linguistic item is a discourse 
marker (identification / disambiguation) 
(Hirschberg and Litman 1993; Siegel and 
McKeown 1994), 3) determination or selection of 
the discourse function of an identified discourse 
marker (Moser and Moore 1995), and 4) the 
coverage capabilities (in terms of levels of 
embedding) among rhetorical relations, as well as 
among individual discourse markers. Discussion 
of these problems for Chinese compound 
sentences can be found in Wang et al (1994). 
Previous attempts to address the above 
problems in Chinese text have usually been based 
on the investigators' intuition and knowledge, or 
on a small number of constructed examples. In our 
current research, we adopt heuristics-based 
13 
corpus-based 
learning to discover the correlation between 
various linguistic features and different aspects of 
approaches, and use machine discourse marker usage. Our research framework 
i 
Statistical Analysis i 
Discourse Analysis 
Text Abstraction i 
i Natural Language 1 
\[ Understanding 
\[ 
! 
I 
Analysis 
& 
Application 
is shown in Figure I. 
Raw Corpus . 
...... (Editorials) 
...... ~ ' " !  ....... ~,~uto Tagging & 
Proofreading 
Segmented 
Corpus " 
~'~-~ ~ J  Word 
~.-~.._~.: . . . .  Segmentation 
Discourse 
Marker & 
Rhetorical 
Relation Tagged 
Corpus 
Feature 
.. ~ ~. .  Extractiov 
Feature 
Database 
; i i 1 
f f  ~ i 
j 
i Dicti?naries \] ,~ ,, 
Heuristics I / .......... ;-
i Induced Rules ! \ ........... ~ i 
? "- '~- ~ ML & 
Evaluation 
xq 
=_. 
OQ 
K 
?-) 
- i  
?tl 
t-' ?D 
,< 
Figure 1 Framework for Corpus-based Study of Discourse Marker Usage in Chinese Text 
Data in the segmented corpus are divided 
into two sets of texts, namely, the training set and 
:the test set, each of which includes 40 editorials in 
:our present research. Texts in the training set are 
. manually and semi-automatically tagged to reflect where, 
the  properties of every Candidate Discourse DMi: 
Marker (CDM). Texts in the test set are 
automatically tagged and proofread. Different 
algorithms, depending on the features being RRi: 
investigated, are derived to automatically extract 
the interesting features to form a feature database. RPi: 
Machine learning algorithms are then applied to 
the feature database to generate linguistic rules 
(decision trees) reflecting the characteristics of 
various discourse markers and the relevant CT~: 
rhetorical relations. For every induced rule (or a 
combination of them), its performance is evaluated 
by tagging the discourse markers appearing in th- 
test set of the corpus. 
4 A Framework for Tagging MN~: 
Discourse Markers 
The following coding scheme is designed to 
encode all and only Real Discourse Markers RN~: 
(RDM) appearing in the SIFAS corpus. We 
describe the i th discourse marker with a 7-tuple 
RDMi, 
RDMi=< DMI, RR/,  RPI, CTi, MNi ,  RNI ,  
> 
the lexical item of the Discourse Marker, 
or the value 'NULL'.  
the Rhetorical Relation in which DMi is 
one of the constituting markers. 
the Relative Position of DM;. The value 
of RPi can be either 'Front' or 'Back' 
denoting the relative posit ion of the 
marker in the rhetorical relation RRi. 
the Connection Type of RRi. The value 
of CT~ can be either 'Inter" or ' Intra', 
which indicates that the DM~ functions as 
a discourse marker in an inter-sentence 
relation or an Intra-sentence relation. 
the Discourse Marker Sequence Number. 
The value of MNi is assigned 
sequentially from the beginning of the 
processed text to the end. 
the Rhetorical Relation Sequence 
Number. The value of RNi is assigned 
14 
I 
I 
I 
I 
I 
l 
sequentially to the corresponding 
rhetorical relation RR; in the text. 
OTi: the Order Type of RR;. The value of OTi 
can be 1, -1 or 0, denoting respectively 
the normal order, reverse order or 
irrelevance of the premise-consequence 
ordering of RRI. 
For Apparent Discourse Markers (ADM) that do 
not function as real discourse markers in a text, a 
different 3-tuple coding scheme is used to encode 
them: 
ADM~ = < LIi, *, SNi > where, 
LIi: the Lexical Item of the ADM. 
SNi: the Sequence Number of the ADM. 
To illustrate the above coding scheme 
consider the following examples of encoded 
sentences where every CDM has been tagged to be 
either a 7-tuple or a 3-tuple. 
Example 1 
<vouvu ('because').Causalitv. Front. lntra. 2. 2. 
/> Zhu Pei ('Jospin') zhengfu ('government') 
taidu ('attitude') qiangying ('adamant'), chaoye 
('government-public') duikang ('confrontation') 
yue-yan-yue ('more-develop-more') -lie 
('strong'), <NULL. Causality. Back. Intra, O. 2. 
/> gongchao ('labour unrest') <vi ('with'). * 
:1> liaoyuan ('bum-plain') zhi ('gen') shi 
'tendency' xunshu 'quick' poji 'spread to' ge 
('every') hang ('profession') ge ('every') ye 
, ('trade'). 
'As a result of the adamant attitude of the 
Jospin administration, confrontation between 
the government and the public is becoming 
w.orse and worse. Labour unrest has spread 
quickly to all industrial sectors.' 
From the above tagging, we can immediately 
obtain the discourse structure that the two clauses 
encapsulated by the two discourse markers youyu 
(with sequence number 2) and NULL (with 
sequence number 0). They have formed a causality 
relation (with sequence number 2). We denote this 
as a binary relation 
Causality(FrontClause(2), BaekClause(2)) 
where FrontClause(n) denotes the discourse 
segment that is encapsulated by the Front 
discourse marker of the corresponding rhetorical 
relation whose sequence number is n. 
15 
BackClause(n) can be defined similarly. Note that 
although yi is a CDM, it does not function as a 
discourse indicator in this sentence. Therefore, it is " 
encoded as an apparent discourse marker. 
Example 2 
<dan ('however'). Adversativitv. Back. Inter. 
17. 14. 1> <ruguo 'if'. Su_~ciencv. Front. Inter, 
18. 15. 1> Zhu Pei ('Jospin') zhengfu 
('government') cici ('this time') zai ('at') 
gongchao ('labour unrest') mianqian ('in the 
face of') tuique ('back down'), <NULL. 
Su.~ciencv. Back. Inter. O. 15. 1> houguo 
('result') <geng.('more'). *. 3> shi bukan ('is 
unbearable') shexian ('imagine'). 
'However, if the Jospin administration backs 
down in the face of the labour unrest, the result 
will be terrible.' 
From the above tagging, we can obtain the 
following discourse structure with embedding 
relations: 
A dversativity ( &F (14 ), 
Sufficiency(F rontClause(15), 
BackClause(15))) 
where &F(n) denotes the Front discourse segment 
of an inter-sentence rhetorical relation whose 
sequence number is n. We can define &B(n) 
similarly. 
5 Heuristic-based Tagging of 
Discourse Markers 
In the previous section, we have introduced a
coding, scheme for CDMs, and have explained 
how to automatically derive the discourse 
structure from sentences with tagged discourse 
markers. Now, the problem we have to resolve is: 
Is there an algorithm that will tag the markers 
according to the above encoding scheme? 
To derive such an algorithm,-even an 
imperfect one, it is necessary that we have 
knowledge of the usage patterns and statistics of 
discourse markers in unrestricted texts. This is 
exactly what project SIFAS intends to achieve as 
explained in Section 3. Instead of completely 
relying on a human encoder to encode all the 
training texts in the SIFAS corpus, we have 
experimented with a simple algorithm using a 
small number of heuristic rules to automatically 
encode the CDMs. The algorithm is a 
straightforward matching algorithm for rhetorical 
relations based recognition of their constituent 
discourse markers as specified in the Rhetorical 
Relation Dictionary (T'sou et al 1999). The 
following principles are adopted by the heuristic- 
based algorithm to resolve ambiguous ituations 
encountered in the process of matching discourse 
markers: 
(1) Principle of Greediness: When matching a
pair of CDMs for a rhetorical relation, 
priority is given to the first matched relation 
from the left. 
(2) Principle of Locality: When matching a pair 
of CDMs for a rhetorical relation, priority is 
given to the relation where the distance 
between its constituent CDMs is shortest. 
(3) Principle of Explicitness: When matching a
pair of CDMs for a rhetorical relation, 
priority is given to the relation that has both 
CDMs explicitly present. 
(4) Principle of Superiority: When matching a
pair of CDMs for a rhetorical relation, 
priority is given to the inter-sentence relation 
whose back discourse marker matches the 
first CDM of a sentence. 
(5) Principle of Back-Marker Preference: this 
principle is applicable only to rhetorical 
relations where either the front or the back 
marker is absent. In such cases, priority is 
given to the relation with the back marker 
present. 
' Application of the above principles to 
process a text is in the order shown, with the 
? exception that the principle of greediness is 
applied whenever none of the other principles can 
be, used to resolve an ambiguous ituation. The 
following pseudo code realizes principles 1, 2 and 
3: 
I := l ;  
whi le I < NumberOfCDMsInTheSentence  do 
beg in  
for J := l  to NumberOfCDMsInTheSentencen  - 
I do 
i f  ((not CDMs\ [ J \ ] .Tagged)  and (not 
CDMs\ [ J+ I \ ] .Tagged)}  then 
Match ing(CDMs\ [ J \ ] ,  CDMs\ [ J+ I \ ] )  ; 
I := I + 1 ; 
end ; 
The following code realizes principles 1,4 and 5: 
16 
for I :=l  to NumberOfCDMs lnTheSentence  do 
begin 
if (not CDMs\ [ I \ ] .Tagged)  then  
Match ing(NULL ,  CDMs\[ I \ ] )  ; 
i f  (not CDMs\ [ I \ ] .Tagged)  then  
Match ing(CDMs\ [ I \ ] ,  NULL) ; 
end ; 
In the above pseudo codes, CDMs\[\] denotes 
the array holding the candidate discourse markers, 
and the Boolean variable Tagged is used to 
indicate whether a CDM has been tagged. 
Furthermore, the procedure Matching0 is to 
examine whether the first word or phrase 
appearing in a sentence is an inter-sentence 
CDMs\[I\]. 
6 Mining Discourse Marker Using 
Machine Learning 
Data mining techniques constitute a field 
dedicated to the development of computational 
methods underlying learning processes and they 
have been applied in various disciplines in text 
processing, such as finding associations in a 
collection of texts (Feldman and Hirsh 1997) and 
mining online text (Knight 1999). In this section, 
we focus on the problem of discourse marker 
disambiguation using decision trees obtained by 
machine learning techniques. Our novel approach 
in mining Chinese discourse markers attempts to 
apply the C4.5 learning algorithm, as introduced 
by Quinlan (1993), in the context of non-tabular, 
unstructured ata. A decision tree consists of 
nodes and branches connecting the nodes. The 
nodes located at the bottom of the tree are called 
leaves, and indicate classes. The top node in the 
tree is called the root, and contains all the training 
examples that are to be divided into classes. In 
order to minimize the branches in the tree, the best 
attribute is selected and used in the test at the root 
node of the tree. A descendant of the root node is 
then created for each possible value of this 
attribute, and the training examples are sorted to 
the appropriate descendant node. The entire 
process is then repeated using the training 
examples associated with each descendant ode to 
select he best attribute for testing at that point in 
the tree. A statistical property, called information 
gain, is used to measure how well a given attribute 
differentiates the training examples according to 
their target classificatory scheme and to select he 
I 
I 
I 
I 
| 
I 
I 
I 
! 
I 
I 
I 
I 
I 
II. 
I 
I- 
II 
I 
most suitable candidate attribute at each step while 
expanding the tree. 
The attributes we use in this research include 
the candidate discourse marker itself, two words 
immediately to the left of the CDM, and two 
words immediately to the right of the CDM. The 
attribute names are F2, F1, CDM, B1,  B2, 
respectively. All these five attributes are discrete. 
The following are two examples: 
? ",", dan 'but', youyu 'since', Xianggang 
'Hong Kong', de 'of', T. 
? zhe 'this', yi 'also', zhishi 'is only', 
Xianggang 'Hong Kong', de 'of', F. 
where "T" denotes the CDM youyu as a discourse 
marker in the given context, and "F" denotes that 
zhishi is not a discourse marker. 
In building up a decision-tree in our 
application of C4.5 to the mining of discourse 
markers, entropy, first of all, is used to measure 
the homogeneity of the examples. For any possible 
candidate A chosen as an attribute in classifying 
the training data S, Gain(S, A) information gain, 
relative to a data set S is defined. This information 
gain measures the expected reduction in entropy 
and defines one branch for the possible subset Si 
of the training examples. For each subset Si, a new 
test is then chosen for any further split. If Si 
satisfies a stopping criterion, such as all the 
element in S~ belong to one class, the decision tree 
is formed with all the leaf nodes associated with 
the most frequent class in S. C4.5 uses arg 
max(Gain(S, A)) or arg max(Gain Ratio(S, A)) as 
defined in the following to construct the minimal 
decision tree. 
c 
Entropy(S) = -~_ -  p, log 2 p~ (Eqn. I) 
i=1 
Gain(S,A) = Entropy(S)- ~ -~'Entropy(S~) isl 
(Eqn. 2) 
Gain Ratio - Gain(S,A) (Eqn. 3) 
Splitlnformation( S, A) 
? ? j is, t.  s,i where Splitlnformation=-2./--,Jog 2 ~, Si is 
!S! iS! 
subset of S for which A has value vt 
In our text mining, according to the number 
of times a CDM occurs in the 80 tagged editorials, 
we select 75 CDMs with more than 10 occurrences. 
To avoid decision trees being over-fitted or trivial, 
for F2, F1, B1 and B2, only values of attributes 
with frequency more than 15 in the corpus are 
used in building the decision trees. We denote all 
values of attributes with frequency less than 15 as 
'Other'.  If a CDM is the first, the second or the 
last word of a sentence, values of F2, F1, or B2 
will be null, we denote a null-value as "*". The 
following are two other examples: 
? "*", "*", zheyang 'thus', ",", Other, T. 
? "*", "*", zheyang 'thus', Other, de 'of', F. 
7 Evaluation 
7.1 Evaluation of Heuristic-based 
Algorithm 
In order to evaluate the effectiveness of the 
heuristic-based algorithm, we randomly selected 
40 editorials from Ming Pao, a Chinese newspaper 
of Hong Kong, to form our test data. Only 
editorials are chosen because they are mainly 
argumentative texts and their lengths are relatively 
uniform. 
The steps of evaluation consist of: 1) tagging 
all of the test data using the heuristic-based 
algorithm, and 2) proofreading, correcting and 
recording all the tagging errors by a human 
encoder. The resulting statistics include, for each 
editorial in the test data, the number of lexical 
items (#Lltms), the number of sentences (#Sens), 
the number of discourse markers (#Mrkrs), and the 
number of sentences containing at least one 
discourse marker (#CSens). Table 2 shows the 
minimum, maximum and average values of these 
characteristics. The ratio of the average number of 
discourse markers to the average number of lexical 
items is 4.37%, and the ratio of the average 
number of sentences 
discourse marker to 
sentences i  62.66%. 
#Lltms 
MIN 466 
MAX 1082 
AVERAGE 676.25 
containing at least one 
the average number of 
#Mrkrs #Sens #CSens 
14 11 6 
52 45 26 
29.58 22.15 13.88 
Table 2 Characteristics of the Test Data 
Our evaluation is based on counting the 
number of discourse markers that are correctly 
17 
tagged. For incorrectly tagged iscourse markers, 
we classify them according to the types of errors 
that we have introduced in T'sou et al (1999). We 
define two evaluation metrics as follows: Gross 
Accuracy (GA) is defined to be the percentage of
correctly tagged discourse markers to the total 
number of discourse markers while Relation- 
Matching Accuracy (RMA) is defined to be the 
percentage of correctly tagged discourse markers 
to the total number of discourse markers minus 
those errors caused by non-markers and 
unrecorded markers. The results for our testing. 
data have GA = 68.89% and RMA = 95.07%. 
Since the heuristic-based algorithm does 
not assume any knowledge of the statistics and 
behavioral patterns of discourse markers, our GA 
demonstrates the usefulness of the algorithm in 
alleviating the burden of human encoders in 
developing a sufficiently large-corpus for the 
purpose of studying the usage of discourse 
markers. 
In our experiment, most errors come from 
tagging non-discourse markers as discourse 
markers (T'sou et al 1999). This is due to the fact 
that, similar to the question of cue phrase 
polysemy (Hirschberg and Litman 1993), many 
Chinese discourse markers have both discourse 
senses and alternate sentential senses in different 
;utterances. For example: 
? ... Zhe ('this') buguo shi ('only is') yi ('one') 
ge ('classifier') wanxiao ('joke') 
...('This is only a joke'.) (sentential sense) 
? ...Buguo ('however'), wo (T)  bu ('neg') 
zheyang ('thus') renwei ('consider') 
? . . ( 'But  I don't think so.') (discourse sense) 
7.2 Evaluation of Decision Tree 
Algorithm (with C4.5) 
In Section 6, we discuss how machine learning 
techniques have been applied to the problem of 
discourse marker disambiguation in Chinese. 
In our experiment, there are a total of 2627 
cases. In our decision tree construction, we use 75 
percent of the total cases as a training set, and the 
remaining 25 percent of cases as a test set. Many 
decision trees can be generated by adjusting the 
parameters in the learning algorithm. Many 
decision trees generated in our experiment have an 
accuracy around 80% for both the training set and 
the test set. Figure 2 shows one of the possible 
decision trees in our experiment. The last branch 
of the decision tree 
F1 = danshi 'but' 
I CDM in {ru 'if', reng 'still', geng 'even more', que 
'however' }:F (6/0) 
I CDM in {chule 'except', youyu 'since', ruo 'if"} : T 
(4/0) 
can be explained as: 
if (F1 = danshi 'but') then 
if (CDM in {ru 'if', reng 'still', geng "even more', que 
'however' }) then dassify as F 
else 
if (CDM in {chule 'except',youyu 'since', ru0 'if '  }) 
then classify as T 
Decision Tree: (Size = 38, Items = 1971, Errors = 282) 
F1 in {di, ye, yi} : F (25/5) 
F i  in (,shi, ;} : T (712/131) 
F1 = Other: 
F I  = danshi :
I CDM in {ru, reng, geng, que} : F (7/10) 
I CDM in {chule, youyu, ruo} : T (4/0) 
Evaluation on trainine data from Data. Data (1971 cases~: 
Classified results: 
T F I <" Classified 
937 125 \] C lass :T  
157 752 C.lass : F Errors : 282 (14.3%) 
Evaluation on testin~ data from Data. Test (656 cases): 
T F ~ f i e d  
293 62 \[Class : T 
68 233 IClass : F Errors : 130 (19.8%) 
1 
Figure 2 An Example of Decision Trees 
The two numbers in the brackets denote the 
number of cases covered by the branch and the 
number of cases being misclassified respectively: 
The results of our experiment will be elaborated 
on in future, when we shall also explore the 
application of machine learning techniques to 
recognizing rhetorical relations on the basis of 
discourse markers, and extracting important 
sentences from Chinese text. 
8. Conclusion 
We discuss in this paper the use of discourse 
markers in Chinese text summarization. Discourse 
structure trees with nodes representing RST 
(Rhetorical Structure Theory) relations are built 
and summarization is achieved by trimming 
18 
unimportant sentences on the basis of the relative 
saliency or rhetorical relations. In order to study 
discourse markers for use in the automatic 
summarization f Chinese, we have designed and 
implemented the SIFAS system. We investigate 
the relationships between various linguistic 
features and different aspects of discourse marker 
usage on naturally occurring text. An encoding 
scheme that captures the essential features of 
discourse marker usage is introduced. A heuristic- 
based algorithm for automatic tagging of discourse 
markers is designed to alleviate the burden of a 
human encoder in developing a large corpus of 
encoded texts and to discover potential problems 
in automatic discourse marker tagging. A study on 
applying machine learning techniques todiscourse 
marker disambiguation is also conducted. C4.5 is 
used to generate decision tree classifiers. Our 
results indicate that machine learning is a 
promising approach to improving the accuracy of 
discourse marker tagging. 
9 Acknowledgement  , 
Support for the research reported here is 
provided through the Research Grants Council 
of Hong Kong under Competitive Earmarked 
Research Grants (CERG) No. 9040067, 
9040233 and 9040326. 
10 References 
~Aretoulaki M., Scheler G. and Brauer W. (1998) 
"Connectionist Modeling of Human Event 
Memorization Processes with Application to 
Automatic Text Summarization." In 
Proceedings of AAAI Spring Symposium on 
Intelligent Text Summarization, Stanford, pp. 
148-150. 
Brandow R., Mitze K. and Rau L. F. (1995) 
"Automatic Condensation of Electronic 
Publications by Sentence Selection." 
Information Processing and Management, 
31(5): 675-685. 
Endres-Niggemeyer B., Maier E. and Sigel A. 
(1995) "How to Implement a Naturalistic 
Model of Abstracting: Four Core Working 
Steps of an Expert Abstractor." Information 
Processing and Management, 31(5): 631-674. 
19 
Feldman R. and Hirsh H. (1997). "Finding 
associations in collections of text." In R.S. 
Michalski I. Bratko and Kubat M. (Eds.)," 
Machine Learning and Data Mining: Methods 
and Applications, pp. 224-240. Wiley. 
Grosz B.J. and Sidner C. (1986) "Attention, 
Intention, and the Structure of Discourse," 
Computational Linguistics 12(3): 175-204. 
Halliday M. A. K. and Hasan R. (1976) Cohesion 
in English, Longman. 
Hearst M. A. (1997) "Texttiling: Segmenting Text 
into Multi-paragraph Subtopic Passages." 
Computational Linguistics, 23(1):33-64. 
Hinds J. (1982) "Inductive, deductive, quasi- 
.inductive: Expository writing in Japanese, 
Korean, Chinese, and Thai." In U. Connor and 
A.M. Johns (Eds.). Coherence in Writing, pp. 
89-109. TESOL publisher. 
Hirschberg J. and Litman D. (1993) "Empirical 
Studies on the Disambiguation f Cue Phrases." 
Computational Linguistics 19(3): 501-530. 
Hirst G. (1981) "Discourse Oriented Anaphoral 
Resolution in Natural Language Understanding: 
A Review." Computational Linguistics 7(2): 
85-98. 
Hovy E. (1993) "Automated Discourse Generation 
using Discourse Structure Relations." Artificial 
Intelligence 63: 341-385. 
Hwang C. H. and Schubert L. K. (1992) "Tense 
Trees as the 'Fine Structure' of Discourse." In 
Proc. 30 th Annual Meeting, Assoc. for 
Computational Linguistics, pp. 232-240. 
Johnson F. C., Paice C. D., Black W. J. and Neal 
A. P. (1993) "'The Application of Linguistic 
Processing to Automatic Abstract Generation." 
Journal of Document and Text Management I:
215-241. 
Kaplan R. B. (1996) "Cultural though patterns in 
intercultural education." Language Learning, 
l&2: 1-20. 
Kirkpatrick A. (1993) "Information sequencing in
modem standard Chinese in a genre of extended 
spoken discourse." Text 13(3): 423-453. 
Kong K.C.C. (1998) "Are simple business request 
letters really simple? A comparison of Chinese 
and English business request letters." Text 18(1 ) :  
103-141. 
Knight K. (1999) "Mining online text." 
Communications of the A CM 42(11): 58-61. 
Kupiec J., Pedersen J., and Chen F. (1995) "A 
Trainable Document Summarizer." In 
Proceedings of the lff h Annual International 
ACM SIGIR Conference on Research and 
Development in Information Retrieval, Seattle, 
pp. 68-73. 
Lin H. L., T'sou B. K., H. C. Ho, Lai T., Lun C., 
C. K. Choi and C.Y. Kit. (1991) "Automatic 
Chinese Text Generation Based on Inference 
Trees.'" In Proc. of ROCL1NG Computational 
Linguistic Conference IV, Taipei, pp. 215-236. 
Litman D. J. and Allen J. (1990) "Discourse 
Processing and Commonsense Plans." In Cohen 
et al(ed.) Intentions in Communications, pp. 
365-388. 
Litman D. J. (1996) "Cue Phrase Classification 
Using Machine Learning." Journal of Artificial 
Intelligence Research 5: 53-94. 
Mani I., Bloedorn E. and B. Gates (1998) "Using 
Cohesion and Coherence Models for Text 
Summarization." In Proceedings of AAAI 
Spring Symposium on Intelligent Text 
Summarization, Stanford, pp. 69-76. 
Mann W. C. and Thompson S. A (1988) 
"Rhetorical Structure Theory: Towards a 
Functional Theory of Text Organization." Text 
8(3): 243-281. 
Marcu D. (1997) "From Discourse Structures to 
Text Summaries." In Proceedings of the 
A CL/EA CL '97 Workshop on Intelligent 
: Scalable Text Summarization, Spain, pp. 82-88. 
McKeown K. and Radev D. (1995) "Summaries of 
Multiple News Articles." In Proceedings of the 
18 'h Annual International A CM S1GIR 
Conference on Research and Development in
Information Retrieval, Seattle, pp. 74-82. 
McKeown K. R. (1985) "Discourse Strategies for 
Generating Natural-Language T xt." Artificial 
Intelligence 27(1): 1-41. 
Morris J. and Hirst G. (1991) "Lexical Cohesion 
Computed by Thesaural Relations as an 
Indicator of the Structure of Text." 
Computational Linguistics 17(1): 21-48. 
Moser M. and Moore J. D. (1995) "Investigating 
Cue Selection and Placement in Tutorial 
Discourse.'" In Proceedings of ACL'95, pp. 
130-135. 
Ono K., Sumita K. and S. Miike. (1994) "Abstract 
Generation based on Rhetorical Structure 
Extraction." In Proceedings of International 
Conference on Computational Linguistics, 
Japan, pp. 344-348. 
Paice C. D. (1990) "Constructing Literature 
Abstracts by Computer: Techniques and 
Prospects." Information Processing and" 
Management 26(1): 171-186. 
Quinlan J. Ross (1993)"C4.5 Programs for 
Machine Learning." San Mateo, CA: Morgan 
Kaufmann. 
Reichman R. (1978) "Conversational Coherence." 
Cognitive Science 2(4): 283-328. 
Salton G., Singhal A., Mitra M. and Buckley C. 
(1997) "Automatic Text Structuring and 
Summarization." Information Processing and 
Management 33(2): 193-207. 
Schiffrin D. (1987) Discourse Markers. 
' Cambridge: Cambridge University Press. 
Siegel E. V. and McKeown K. R. (1994) 
"Emergent Linguistic Rules from Inducing 
Decision Trees: Disambiguating Discourse Clue 
Words." In Proceedings of AAAI, pp. 820-826. 
T'sou B. K., Ho H. C., Lai B. Y., Lun C. and Lin 
H. L. (1992) "A Knowledge-based Machine- 
aided System for Chinese Text Abstraction." In 
Proceedings of International Conference on 
Computational Linguistics, France, pp, 1039- 
1042. 
T'sou B. K., Gao W. J., Lin H. L., Lai T. B. Y. 
and Ho H. C. (1999) "Tagging Discourse 
Markers: Towards a Corpus based Study of 
Discourse Marker Usage in Chinese Text" In 
Proceedings of the 18th International 
Conference on Computer Processing of 
Oriental Languages, March 1999, Japan, pp. 
391-396. 
T'sou B. K., Lin H. L., Ho H. C., Lai T. and Chan 
T. (1996) "Automated Chinese Full-text 
Abstraction Based on Rhetorical Structure 
Analysis." Computer Processing of Oriental 
Languages 10(2): 225-238. 
Wang W. X., Zhang X. C., Lu M. Y. and Cheng H. 
Y. (1994) "Xian Dai Han Yu Fu Ju Xian Jie (A 
New Analysis of Complex Sentences in Modern 
Standard Chinese)", Hua Dong S.hi Fan Da Xue 
Chu Ban She, 1994. 
20 
! 
I 
i 
i 
! 
I 
I 
I 
I 
I 
i 
I 
I 
i 
i 
I 
I 
I 
I 
Enhancement of a Chinese Discourse Marker Tagger with C4.5 
Benjamin K. T'sou l, Torn B. Y. Lai 2, Samuel W. K. Chan 3, Weijun Gao 4, Xuegang Zhan 5
23Languag e Information Sciences Research Centre 
City University of Hong Kong 
Tat Chee Avenue, Kowloon 
Hong Kong SAR, China 
Northeastern U iversity, China 
{ ~rlbtsou, 2ettomlai} @uxmail.cityu.edu.hk, 3swkchan@cs.cityu.edu.hk, 
4wj gao@mail.neu.edu.cn, Szxg@ics.cs.neu.edu.cn 
Abstract 
Discourse markers are complex 
discontinuous linguistic expressions which 
are used to explicitly signal the discourse 
structure of a text. This paper describes 
efforts to improve an automatic tagging 
system which identifies and classifies 
discourse markers in Chinese texts by 
applying machine learning (ML) to the 
disambiguation f discourse markers, as an 
integral part of automatic text summarization 
via rhetorical structure. Encouraging results 
are reported. 
Keywords: discourse marker, Chinese 
corpus, rhetorical relation, automatic tagging, 
machine learning 
1 Introduction 
Discourse refers to any form of 
language-based communication involving 
multiple sentences or utterances. The most 
important forms of discourse of interest o 
Natural Language Processing (NLP) are text 
and dialogue. The function of discourse 
analysis is to divide a text into discourse 
segments, and to recognize and re-construct 
the discourse structure of the text as intended 
by its author. 
Automatic text abstraction has received 
considerable attention (Paice 1990). Various 
systems have been developed (Chan et al 
2000). Ono et al (1994), T'sou et al (1992) 
and Marcu (1997) focus on discourse 
structure in summarization using the 
Rhetorical Structure Theory (RST, Mann and 
Thompson 1986). The theory has been 
exploited in a number of computational 
systems (e.g. Hovy 1993). The main idea is 
to build a discourse tree where each node of 
the tree represents an RST relation. 
Summarization is achieved by trimming 
lmimportant sentences on the basis of the 
relative saliency or rhetorical relations. 
The SIFAS (Syntactic Marker based 
Full-Text Abstraction System) system has 
been implemented to use discourse markers 
in the automatic summarization of Chinese 
(T'sou et al 1999). In this paper, we report 
our efforts to improve the SIFAS tagging 
system by applying machine learning 
techniques to disambiguation of discourse 
markers. C4.5 (Quirdan, 1993) is used in our 
system. 
2 Manual Tagging Process 
To tag the discourse markers, the 
following coding scheme is designed to 
encode Real Discourse Markers (RDM) 
appearing in the SIFAS corpus (T'sou et al 
1998). We describe the z ~h discourse marker 
with a 7-tuple RDM; 
RDMi=< DM i, RRi, RPi, CTi, MNi, 
RNi, OT i >, where 
38 
DMi 
RP i : 
: 
MN~ : 
RNi : 
OT~ : 
the lexical item of the 
Discourse Marker, or the 
value'NULL'. 
the Rhetorical Relation in 
which DIVI~ is a constituent 
marker. 
the Relative Position of DMi. 
the Connection Type of RRi. 
the Discourse Marker 
Sequence Number. 
the Rhetorical Relation 
Sequence Number. 
the Order Type of RR~. The 
value of OTi can be 1, -1 or 0, 
denoting respectively the 
normal order, reverse order or 
irrelevance of the premise- 
consequence ordering of RR i . 
For apparent discourse markers that do 
not function as a real discourse marker in a 
text, a different coding scheme is used to 
encode them. We describe the i th apparent 
discourse marker using a 3-Tuple ADM~: 
ADMi =< LIi, *, SNi >, where 
LIi : the Lexical Item of the 
apparent discourse marker. 
SNi : the Sequence Number of the 
apparent discourse marker. 
In Chinese, discourse markers can be 
either words or phrases. To tag the SIFAS 
corpus, all discourse markers are organized 
into a discourse marker pair-rhetorical 
relation correspondence table. Part of the 
table is shown Table 1. 
To construct an automatic tagging 
system, let us first examine the sequential 
steps in the tagging process of a human 
tagger. 
S1. Written Chinese consists of rurming texts 
without word delimiters; the first step is 
is to segment the text into Chinese word 
sequences. 
$2. On the basis of a discourse marker list, 
we identify those words in the text 
which appear on the list as Candidate 
Discourse Markers (CDMs). 
$3. To winnow Real Discourse Markers 
(RDMs) and Apparent Discourse 
Markers (ADMs) from the CDMs, and 
encode the ADMs with a 3-tuple. 
$4. To encode the RDM with a 7-tuple 
according to a Discourse Marker Pair- 
Rhetorical Relation correspondence 
table. 
Relat- 
ion 
Adver- 
sativity 
Adver- 
sativity 
Causa- 
nty 
Causa- 
lity 
Front Back Con- 
nection 
Type 
Inter 
Intra 
Intra 1 
Intra -1 
Table 1 Discourse Marker Pair- 
Rhetorical Relation Table 
Order 
Type 
3 Automat ic  Tagg ing  Process  
The identification of candidate discourse 
markers is based on a discourse marker list, 
which now contains 306 discourse markers 
plus a NULL marker. The markers are 
extracted from newspaper editorials of Hong 
Kong, Mainland China, Taiwan and 
Singapore. These markers constitute 480 
distinct discontinuous pairs that correspond 
to 25 rhetorical relations. In actual usage, 
some discourse marker pairs designate 
multiple rhetorical relations according to 
context. Some pairs can represent both 
INTER-sentence and INTRA-sentence 
relations. Thus the correspondence b tween 
the discourse marker pairs and the rhetorical 
relations is not single-valued. Some 
discourse marker pairs correspond to more 
than one rhetorical relation or connection 
type. We have 504 correspondences between 
the discourse marker pairs and the rhetorical 
relations. 
39 
In practice, one discontinuous 
constituent member of a marker pair is often 
omitted. We use the NULL marker to 
indicate the omission. In the 504 
correspondences, 244 of them are double 
constituent marker pairs, 260 are single 
constituent markers (i.e. One of the markers 
is NULL). And in the 244 double constituent 
markers, only 3 are not single-valued 
correspondences (one of" which is an 
INTER/INTRA relation, and can easily be 
distinguished.). Thus the tagging of the 244 
double constituent markers is basically a 
table searching process. But for the 260 
single constituent markers, the identity of the 
NULL marker is often difficult o determine. 
The SIFAS tagging system works in two 
modes: automatic and interactive (semi- 
automatic). The automatic tagging procedure 
is as follows: 
1. Data preparation: Input data files are 
modified according to the required 
format. 
2. Word segmentation: Because there are 
no delimiters between Chinese words in 
a text, words have to be extracted 
through asegmentation process. 
3. CDM identification 
4. Full-Marker RDM recognition 
5. ADM identification (first pass, 
deterministic) 
6. CDM feature xtraction 
7. ADM identification (2nd pass, via ML) 
8. Tagging NuLL-marker CDM pairs (via 
ML) 
9.ADM and RDM sequencing, proof- 
reading, training data generation, and 
statistics 
The following principles are adopted by 
the tagging algorithm to resolve ambiguity in 
the process of matching discontinuous 
discourse markers: 
1. the principle of greediness: When 
matching a pair of discourse markers for 
a rhetorical relation, priority is given to 
the first matched relation from the left. 
2.the principle of locality: When 
matching apair of discourse markers for 
a rhetorical relation, priority is given to 
the relation where the distance between 
its constituent markers is shortest. 
3.the principle of explicitness: When 
matching a pair of discourse markers for 
a rhetorical relation, priority is given to 
the relation where both markers are 
explicitly presented. 
4. the principle of superiority: When 
matching a pair of discourse markers for 
a rhetorical relation, priority is given to 
the inter-sentence r lation whose back 
discourse marker matched with the first 
word of a sentence. 
5. the principle of Back-marker 
preference: This is applicable only to 
rhetorical relations where either the 
front or the back marker is absent, or to 
a NULL marker. In such cases, priority 
is given to the relation with the back 
marker present. 
Steps 1 to 6 and the five principles 
underlie the original naive tagger of the 
SIFAS system (T'sou et al 1998), which also 
contains the system framework. 
4 Improvement 
4.1 Problems 
Many Chinese discourse markers have 
both discourse senses and alternate sentential 
senses in different context. For a human 
tagger, steps $3 and $4 in section 2 are not 
difficult because he/she can identify an 
ADM/RDM based on his/her text 
comprehension. However, for an automatic 
process, it is quite difficult o distinguish an 
ADM from an RDM if no syntactic/semantic 
information is available. 
Another problem is the location of 
NULL-Marker described above. Our earlier 
statistics howed some characteristics in the 
distance measured by punctuation marks. 
Statistics from 80 tagged editorials how that 
most of the relations are INTRA-Sentence 
relations (about 93%), about 70% of the 
INTRA RDM pairs have NULL markers. 
Most of these RDM pairs are separated by 
ONE comma (62%). These statistics how 
40 
the importance of the problems of 
positioning the NULL markers. 
The naive tagger partially solved the 
CDM discrimination and NULL marker 
location problems. Our experiment shows 
that about 45% of the ADMs can be 
correctly identified, and about 60% of the 
NULL markers can be correctly located one 
comma/period away from the current RDM. 
This leaves much room for improvement. 
One solution is to add a few rules 
according to previous tatistics. The original 
naive tagger did not assume any knowledge 
of the statistics and behavioral patterns of 
discourse markers. From the error analysis, 
we extracted some additional rules to guide 
the classification and matching of the 
discourse markers. For example, one of the 
rules we extracted is: 
"A matching pair must be separated by 
at least two words or by punctuation 
marks". Using this rule, the following 
full marker matching error is avoided. 
< ~ ~ >< ~ ~ >< ~ x ~ >< 
./~ ,conjunction,Front, Intra,5,5,1>< ~ >< ~,  
conjunction, Back, Intra, 6,5,1><~>, <~t~><7~ 
?~x~><~><t$ i~>,  <~,* ,7xf f~>< 
X><~x<~><~x~><~x~_~ 
><~>0 
Another solution is to use 
? syntactic/semantic information through 
machine learning. 
4.2 C4.5 
Most empirical learning systems are 
given a set of pre-classified cases, each 
described by a vector of attribute values, and 
construct from them a mapping from 
attribute values to classes. C4.5 is one such 
system that learns decision-tree classifiers. It
uses a divide-and-conquer approach to 
growing decision trees. The current version 
of C4.5 is C5.0 for Unix and See5 for 
Windows. 
Let attributes be denoted A={a~, a2, ..., 
a,,J, cases be denoted D={d 1, d2, ..., d J ,  and 
classes be denoted C={c, c 2, ..., cJ. For a 
set of cases D, a test 1q is a split of D based 
on attribute at. It splits D into mutually 
exclusive subsets D~, D 2, ..., D r These 
subsets of cases are single-class collections 
of cases. 
If a test T is chosen, the decision tree 
for D consists of a node identifying the test 
T ,  and one branch for each possible subset 
D~. For each subset D~, a new test is then 
chosen for further split. If D~ satisfies a 
stopping criterion, the tree for Dr is a leaf 
associated with the most frequent class in D~. 
One reason for stopping is that cases in D~ 
belong to one class. 
C4.5 uses arg max(gain(D,1)) or arg 
max(gain ratio(D,T)) to choose tests for 
split: 
k 
Info(D) = -~p(c , ,D)  * log2(p(c,,D)) 
i=I 
Split(D,T) = _L ID ,  I .  log2(~-~) 
i=l IDI 
Gain(D,T) = Info(D)- "J"'~.~'. Di I.  Info(Di) 
i=l I DI 
Gain ratio(D, T) = gain(D, T) / Split(D, T) 
where, p(c~,D) denotes the proportion of 
cases in D that belong to the i th class. 
4.3 Application of C4.5 
Since using semantic information 
requires a comprehensive thesaurus, which is 
unavailable at present, we only use syntactic 
information through machine learning. 
The attributes used in the original 
SIFAS system include the candidate 
discourse marker itself, two words 
immediately to the left of the CDM, and two 
words immediately to the right of the CDM. 
The attribute names are F2, F1, CDM, B1, 
B2, respectively (T'sou et al 1999). SIFAS 
only uses the Part Of Speech attribute of the 
neighboring words. This reflects to some 
degree the syntactic characteristics of the 
CDM. 
To reflect the distance characteristics, 
we add two other attributes: the number of 
discourse delimiters (commas, semicolons 
for INTRA-sentence relation, periods and 
41 
exclamation marks for INTER-sentence 
relation) before and after the current CDM, 
denoted Fcom and Boom, respectively. For 
the location of the NULL marker, we still 
add an actual number of delirniters Acorn. 
The order of these attributes is: CDM, 
F1, F2, B1, B2, Fcom, Boom Acorn for Null 
marker location, and CDM, F1, F2, B1, B2, 
Fcom, Bcom, IsRDM for CDM classification, 
where IsRDM is a Boolean value. 
The following are two examples of 
cases: 
9~: _N. ,?,q,a,a,7,1,1 for NULL marker 
location 
N~,d,?,u,?,l ,0,F for CDM classificati 
on 
where "?" denotes that no corresponding 
word is at the position (beginning or end of 
sentence); a, d, q, and u are part-of-speech 
symbols in our segmentation dictionary, 
representing adjective, adverb, classifier, and 
auxiliary, respectively. 
The following are two examples of the 
rules generated by the C4.5. The first is a 
CDM classification rule, and the other is a 
NULL marker location rule. 
Rule 5: (11/1, lift 2.2) 
CDM = 
B1 =v 
Fcom > 0 
class T \[0.846\] 
which can be explained as: if the word after 
the CDM "~:" is a verb, and there is one 
comma in the sentence, before "~J:~:", then 
"~:" is an RDM. 
Rule 22: (1, lift 3.4) 
B2 = p 
Fcom > 1 
class 2 \[0.667\] 
which can be explained as: if the second 
word after the RDM is a preposition, and 
there is more then one commas before the 
current RDM, then the location of the NULL 
marker is two commas away from the RDM. 
4.4 Objects in the SIFAS system 
The objects in the new SIFAS tagging 
system are listed below. 
1. Dictionary Editor: for the update of 
word segmentation dictionary and the 
rhetorical relation table. 
2. Data Manager: for the modification of 
the input data (editorial texts) to 
conform with the required format. 
3. Word Segmenter: for the segmentation 
of the original texts, and the recognition 
of CDMs. 
4. RDM Tagger: The initial identification 
of RDMs is a table searching process. 
All those full-marker pairs are identified 
as rhetorical relations according to the 
principles described above. For those 
Null-marker pairs, the location of the 
Null maker is left to the rule interpreter. 
5. ADM Tagger: The identification of 
ADMs is also a table searching process, 
because, without other 
syntactic/semantic information, the only 
way to identify ADMs from the CDMs 
is to find out that the CDM cannot form 
a valid pair with any other CDMs 
(including the NULL marker) to 
correspond to a rhetorical relation. 
6. CDM Feature Extractor: For those 
untagged CDMs, the classification is 
carried out through C4.5. The Feature 
Extractor extracts yntactic information 
about he current CDM and send it to the 
Rule Interpreter (see below). 
7. Rule Interpreter: C4.5 takes feature data 
file as the input to construct a classifier, 
and the rules formed are stored in an 
output file. The rule interpreter eads 
this output file and applies the rules to 
classify the CDMs. In our system, The 
Rule Interpreter functions as a NULL 
Marker Locator and a CDM classifier. 
8. Sequencer: for the rearrangement of
RDM and ADM order number. In the 
rearranging process, the Sequencer also 
extracts statistical information for 
analysis. 
9. Interaction Recorder: for the recording 
of user interaction information for 
42 
statistics use. 
10. Data Retriever: for data retrieval and 
browsing. 
5 Evaluation 
In order to evaluate the effectiveness of 
the tagging system in terms of the percentage 
of discourse markers that can be tagged 
correctly, we have chosen 80 tagged 
editorials from Ming Pao, a Chinese 
newspaper of Hong Kong, in the duration 
from December 1995 to January 1996 to 
form a training data set. Then we randomly 
selected 20 editorials from Mainland China 
and Hong Kong newspapers for the system 
to tag automatically, and then manually 
checked the results. 
The total CDMs in the training data set 
is 4764, in which 2116 are RDMs and 2648 
are ADMs. The distribution of INTER- 
sentence r lations, INTRA-sentence r lations, 
and NULL marker pairs is shown below. 
Total 
Relations 
Inter- 
Sentence 
Relations 
Intra- 
Sentence 
Relations 
Relations 
with 
NULL 
marker 
pair 
1589 98 1491 1062 
100% 6.17% 93.83% 66.83% 
Table 2 Distribution of INTER-/INTRA- 
sentence relations, 
and NULL marker pairs 
Our evaluation is based on counting the 
number of discourse markers that are 
correctly and incorrectly tagged. 
The total CDMs in the test data set is 
1134, in which 563 are RDMs and 571 are 
ADMs. The distribution of INTER-sentence 
relations, INTRA-sentence relations, and 
NULL marker pairs in the test data set is 
shown in Table 3. 
Total 
Relations 
Inter- 
Sentence 
Relations 
Intra- 
Sentence 
Relations 
Relations 
with 
NULL 
marker 
pair 
424 23 401 285 
100% 5.42% 94.58% 67.22% 
Table 3 Distribution of INTER-/INTRA- 
sentence relations, and NULL marker 
pairs in testing data set 
451 399 11 1 65 3 
Table 4 Test Results 
From the test results shown in Table 4, 
we can see that most of the errors are caused 
by the misclassification f the CDMs. An 
example of Other errors is shown below. 
The following sentence is from an editorial 
of People's Daily. 
< ~ ~17 >< ~ ~.~ >< ~ ~ > , 
<NULL,sufficienc y, Front, Intra, O,81,1x -- ~ ~ff \[\] 
><~ ><~. ~lJ><~.~.>< \[\] ~>,  <~><~iA><-- 
+~ \[ \ ]><~><)~lJ>, <~><~iA><~>< 
~,*,80><~ \[\]><1~><--~52">, <~~>< 
~,sufticiency, Back,Intra,81,81,1 < ~ ~ x :~ x 
~.><~><~>,  <~ ~><~ :~ ><~,*,SZ><~ l~J, 
><~>? 
In the above sentence, the first "R"  is 
matched with the NULL marker, but the 
second "R" is left as an ADM. This causes 
an "Other error" and an "ADM/RDM 
classification error". 
The Gross Accuracy (GA) as defined in 
T'sou et al (1999) is: 
GA = correctly tagged discourse 
markers / total number of discourse markers 
= 95.38% 
This greatly improves the performance 
compared with the original GA = 68.89%. 
The overgeneration problem (tagged 415, 
actual 424) is caused by the mismatch of 
CDMs as RDM pairs, or by the 
43 
misclassification of CDMs as RDMs. 
Following are two examples. 
< ~\[I ~ ,sufficieney, Front, Intra,54,54,1x ~ ~1"\] >_< 
~\[1- ~><~,* ,56x~xf f  ~ >, <~A.x~x~ :t: 
>< ~ > , < ~1~. ,sufficieney, Back,Intra,57,54,1>_< 
,*,58><~ x~ ~><~><:~ ~x~ ~x 
><~I l~ . l><~x(~ ~ x~,* ,59><~ A.x~ ~ 
><:t:~><--~>? 
In this example, "~tl ~"  could have 
matched <:~,*.55>, < ~,*,56>, or<~,*,58>. 
Only the <:~,*,55> and the <~,*,58> can be 
eliminated from the candidates according to 
the "simple rules" mentioned in section 4.1. 
The system has to choose from <~,*,56> and 
<}J~,*,57> to match with "~zn~'. Luckily, 
the system has given a right choice here. 
< --  ~" ~ \[\] >< ~ ,conjunction,Front, Intra,46, 
46,1><~~><~><~r~> , <NULL, 
conjunction,Front, Intra,0,49,1 ><-- f" ~ \[\] ><)~ 
>< ~,~ ,conjunction,Back, Intra,47,46,1>< 
~i~,*,48>< ~ ~><1~ \]\]~><~E ~><~><~ >< 
~ \[\] A .><~><~ ~ ><th~> , < 
,eonjunction,Baek,Intra,49,49,1>< ~ ~tJ ~ 
><?x~ ~><\[ \ ]  ~><~n><~,*,50><l~# 
\[\]><~ I x~x\ [ \ ]  g,~><~ ~>< ~ ><Lib>. 
The two "~" are misclassified as RDMs, 
and causes a mismatch of RDM pair. Such 
errors are difficult to avoid for an automatic 
system. Without further syntactic/semantic 
analysis, we can only hope for the ML 
algorithm to give us a solution from more 
training data. 
6 Conc lus ion  
In order to study discourse markers for 
use in the automatic summarization of 
Chinese text, we have designed and 
implemented the SIFAS system. In this 
paper, we have focused on the problems of 
NULL marker location and the classification 
of RDMs and ADMs. A study on applying 
machine learning techniques to discourse 
marker disambiguation is conducted. C4.5 is 
used to generate decision tree classifiers. Our 
results indicate that machine learning is an 
effective approach to improving the accuracy 
of discourse marker tagging. For interactive 
use of the system, if we set a threshold for 
the rule precision and only display those low 
precision rules for interactive selection, we 
can greatly speed up the semi-automatic 
tagging process. 
7 References  
Chart S., Lai T., Gao W. J. and T'sou B. K. 
(2000) "Mining Discourse Markers for 
Chinese Textual Summarization." In 
Proceedings of the Sixth Applied Natural 
Language Processing Conference and the 
North American Chapter of the 
Association for Computational Linguistics. 
Workshop on Automatic Summarization, 
Seattle, Washington, 29 April to 3 May, 
2000. 
Grosz B.J. and Sidner C. (1986) "Attention, 
Intention, and the Structure of Discourse," 
Computational Linguistics 12(3): 175-204. 
Hirst G. (1981) "Discourse Oriented 
Anaphoral Resolution in Natural Language 
Understanding: A Review." Computational 
Linguistics 7(2): 85-98. 
Hovy E. (1993) "Automated Discourse 
Generation using Discourse Structure 
Relations." Artificial Intelligence 63: 341- 
385. 
Hwang C. H. and Schubert L. K. (1992) 
"Tense Trees as the 'Fine Structure' of 
Discourse." In Proc. 30th Annual Meeting, 
Assoc. for Computational Linguistics, pp. 
232-240. 
Lin H. L., T'sou B. K., H. C. Ho, Lai T., Lun 
C., C. K. Choi and C.Y. Kit. (1991) 
"Automatic Chinese Text Generation 
Based on Inference Trees." In Proe. of 
ROCLING Computational Linguistic 
Conference IV, Taipei, pp. 215-236. 
Litman D. J. and Allen J. (1990) "Discourse 
Processing and Commonsense Plans." In 
Cohen et al(ed.) Intentions in 
Communications, pp. 365-388. 
Mann W. C. and Thompson S. A (1988) 
"Rhetorical Structure Theory: Towards a 
Functional Theory of Text Organization." 
44 
? Text 8(3): 243-281. 
Marcu D. (1997) "From Discourse Structures 
to Text Summaries." In Proceedings of the 
ACL/EACL'97 Workshop on Intelligent 
Scalable Text Summarization, Spain, pp. 
82-88. 
McKeown K. and Radev D. (1995) 
"Summaries of Multiple News Articles." 
In Proceedings of the 18th Annual 
International ACM SIGIR Conference on 
Research and Development in Information 
Retrieval, Seattle, pp. 74-82. 
Ono K., Surnita K. and S. Miike. (1994) 
"Abstract Generation based on Rhetorical 
Structure Extraction." In Proceedings of 
International Conference on 
Computational Linguistics, Japan, pp. 344- 
348. 
Paice C. D. (1990) "Constructing Literature 
Abstracts by Computer: Techniques and 
Prospects." Information Processing and 
Management 26(1): 171-186. 
Qulnlan J. Ross (1993) "C4.5 Programs for 
Machine Learning." San Mateo, CA: 
Morgan Kaufmann. 
T'sou B. K., Ho H. C., Lai B. ?., Lun C. and 
Lin H. L. (1992) "A Knowledge-based 
Machine-aided System for Chinese Text 
Abstraction." In Proceedings of 
International Conference on 
Computational Linguistics, France, pp. 
1039-1042. 
T'sou B. K., Gao W. J., Lin H. L., Lai T. B. 
Y. and Ho H. C. (1999) "Tagging 
Discourse Markers: Towards a Corpus 
based Study of Discourse Marker Usage in 
Chinese Text" In Proceedings of the 18th 
International Conference on Computer 
Processing of Oriental Languages, March 
1999, Japan, pp. 391-396. 
T'sou B. K., Lin H. L., Ho H. C., Lai T. and 
Chan T. (1996) "Automated Chinese Full- 
text Abstraction Based on Rhetorical 
Structure Analysis." Computer Processing 
of Oriental Languages 10(2): 225-238. 
Tsou, B.K., et al, 1998: ~1~,  ~ ,  
i ~ ~ 1 - ~ 3 - ~ ~  ~ ~ " ,  
ICCIP'98, Beijing, Nov. 18-20, 1998. 
45 
 	
 Some Considerations on Guidelines for  
Bilingual Alignment and Terminology Extraction 
 
Lawrence Cheung, Tom Lai, Robert Luk?, Oi Yee Kwong, King Kui Sin, Benjamin K. Tsou 
 
Language Information Sciences Research Centre 
City University of Hong Kong 
Tat Chee Avenue, Kowloon, Hong Kong  
{rlylc, cttomlai, rlolivia, ctsinkk, rlbtsou}@cityu.edu.hk  
?Department of Computing 
Hong Kong Polytechnic University  
Hung Hom, Kowloon, Hong Kong 
csrluk@comp.polyu.edu.hk 
 
Abstract  
Despite progress in the development of 
computational means, human input is still 
critical in the production of consistent and 
useable aligned corpora and term banks. This 
is especially true for specialized corpora and 
term banks whose end-users are often 
professionals with very stringent 
requirements for accuracy, consistency and 
coverage. In the compilation of a high quality 
Chinese-English legal glossary for ELDoS 
project, we have identified a number of issues 
that make the role human input critical for 
term alignment and extraction. They include 
the identification of low frequency terms, 
paraphrastic expressions, discontinuous units, 
and maintaining consistent term granularity, 
etc. Although manual intervention can more 
satisfactorily address these issues, steps must 
also be taken to address intra- and 
inter-annotator inconsistency.  
 
Keyword: legal terminology, bilingual 
terminology, bilingual alignment, 
corpus-based linguistics 
1. Introduction 
Multilingual terminology is an important 
language resource for a range of natural language 
processing tasks such as machine translation and 
cross-lingual information retrieval. The 
compilation of multilingual terminology is often 
time-consuming and involves much manual 
labour to be of practical use. Aligning texts of 
typologically different languages such as Chinese 
and English is even more challenging because of 
the significant differences in lexicon, syntax, 
semantics and styles. The discussion in the paper 
is based on issues arising from the extraction of 
bilingual legal terms from aligned 
Chinese-English legal corpus in the 
implementation of a bilingual a text retrieval 
system for the Judiciary of the Hong Kong Special 
Administrative Region (HKSAR) Government.  
 Much attention in computational 
terminology has been directed to the development 
of algorithms for extraction from parallel texts. 
For example, Chinese-English (Wu and Xia 1995), 
Swedish-English-Polish (Borin 2000), and 
Chinese-Korean (Huang and Choi 2000). Despite 
considerable progress, bilingual terminology so 
generated is often not ready for immediate and 
practical use. Machine extraction is often the first 
step of terminology extraction and must be used in 
conjunction with rigorous and well-managed 
manual efforts which are critical for the 
production of consistent and useable multilingual 
terminology. However, there has been relatively 
little discussion on the significance of human 
intervention. The process is far from being 
straightforward because of the different purposes 
of alignment, the requirements of target users and 
the corpus type. Indeed, there remain many 
problematical issues that will not be easy to be 
resolved satisfactorily by computational means in 
the near future, especially when typologically 
different languages are involved, and must require 
considerable manual intervention. Unfortunately, 
such critical manual input has often been treated as 
an obscure process. As with other human cognitive 
process (T?sou et al 1998), manual terminology 
markup is not a straightforward task and many 
issues deserve closer investigation. 
 In this paper, we will present some 
significant issues for Chinese-English alignment 
 and term extraction for the construction of a 
bilingual legal glossary. Section 2 describes the 
background of the associated bilingual alignment 
project. Section 3 discusses the necessity of 
manual input in bilingual alignment, and some 
principles adopted in the project to address these 
issues. Section 4 provides an outline for further 
works to improve terminology management, 
followed by a conclusion in Section 5. 
2. High Quality Terminology Alignment 
and Extraction 
2.1 Bilingual Legal Terminology in Hong 
Kong 
The implementation of a bilingual legal system in 
Hong Kong as a result of the return of 
sovereignty to China in 1997 has given rise to a 
need for the creation and standardization of 
Chinese legal terminology of the Common Law 
on par with the English one. The standardization 
of legal terminology will not only facilitate the 
mandated wider use of Chinese among legal 
professionals in various legal practices such as 
trials and production of legal documentation 
involving bilingual laws and judgments, but also 
promote greater consistency of semantic 
reference of terminology to minimize ambiguity 
and to avoid confusion of interpretation in legal 
argumentation.  
 In the early 90?s, Hong Kong law drafters 
and legal translation experts undertook the 
unprecedented task of translating Hong Kong 
Laws, which are based on the Common Law 
system, from English into Chinese. In the 
process, many new Chinese legal terms for the 
Common Law were introduced. On this basis, an 
English-Chinese Glossary of legal terms and a 
Chinese-English Glossary were published in 1995 
and 1999 respectively. The legal terminology was 
vetted by the high level Bilingual Laws Advisory 
Committee (BLAC) of Hong Kong. The 
glossaries which contain about 30,000 basic 
entries have become an important reference for 
Chinese legal terms in Hong Kong. The Bilingual 
Legal Information System (BLIS) developed by 
the Department of Justice, HKSAR provides 
simple keyword search for the glossaries and 
laws that are available in both Chinese and 
English. Nevertheless, the glossaries are far from 
being adequate for many different types of legal 
documentation, e.g. contracts, court judgments, 
etc. One major limitation of the BLIS glossary is 
its restricted coverage of legal terminology in the 
Laws of Hong Kong, within a basically 
prescriptive context as when the laws were studied 
at the time of its promulgation. There are other 
important bilingual references (Li and Poon 1998, 
Yiu and Au-Yeung 1992, Yiu and Cheung 1996) 
which focus more on the translation of Common 
Law concepts. These are almost exclusively 
nominal expressions. 
 In 2000, the City University of Hong 
Kong, in cooperation with the Judiciary, HKSAR, 
initiated a research project to develop a bilingual 
text retrieval system, Electronic Legal 
Documentation/Corpus System (ELDoS), which is 
supported by a bilingually aligned corpus of 
judgments. The purpose of the on-going project is 
twofold. First, the aligned legal corpus enables the 
retrieval of legal terms used in authentic contexts 
where the essence and spirit of the laws are tested 
(and contested) in reality, explicated and 
elaborated on, as an integral part of the evolving 
and defining body of important precedent cases 
unique to the Common Law tradition. Second, the 
corpus covers judgment texts involving 
interpretation of different language styles and 
vocabulary from Hong Kong laws. The alignment 
markup also serves as the basis for the compilation 
of a high-quality bilingual legal term bank. To 
complete the task within the tight timeframe, a 
team of annotators highly trained in law and 
language are involved in alignment markup and 
related editing. 
2.2 Need for Human Input 
The legal professionals which are the target users 
of ELDoS have very stringent demands on 
terminology in terms of accuracy, coverage and 
consistency. Aligned texts and extracted terms 
must therefore be carefully and thoroughly 
verified manually to minimize errors. 
Furthermore, many studies on terminology 
alignment and extraction deal predominantly with 
nominal expressions. Since the project aims to 
provide comprehensive information on the 
manifestations of legal vocabulary in Chinese and 
English texts, the retrieval system should not 
restrict users to nominal expressions but should 
also provide reference to many other phenomena 
such as alternation of part-of-speech (POS) (e.g. 
noun-verb alternation) inherent in bilingual texts, 
as will be seen in Section 3.  
 The availability of bilingual corpora has 
made it possible to construct representative term 
 banks. Nonetheless, current alignment and term 
extraction technology are still considered 
insufficient to meet the requirements for high 
quality terminology extraction. In ELDoS project, 
many issues are difficult to be handled 
satisfactorily by the computer in the foreseeable 
future. Although human input is essential for high 
quality term bank construction, the practice of 
manual intervention is not straightforward. 
Indeed, the manual efforts to correct the errors 
can be substantial, and the associated cost should 
not be underestimated. The annotator must first 
go through the entire texts to spot the errors and 
terms left out by the machines. In this process, 
both the source and target materials have to be 
consulted. The annotator must also ensure the 
consistency of the output. As a result, guidelines 
should be set up to streamline the process. 
3. Aspects of Terminology Alignment 
The approach adopted for the manual annotation 
of alignment markup and the maintenance of term 
bank in the ELDoS project will be described. 
Additional caution has been taken in the 
coordination of a team of annotators.  
3.1 Term Frequency 
An important reason for manual intervention in 
bilingual term alignment is the relatively poor 
recall rate for low frequency terms. Many 
extraction algorithms make use of statistical 
techniques to identify multi-word strings that 
frequently co-occur (Wu and Xia 1995; Kwong 
and Tsou 2001). These methods are less effective 
for locating low frequency terms. Of the 16,000 
terms extracted from ELDoS bilingual corpora, 
about 62% occur only once in about 80 
judgments. For high quality alignment and 
extraction, failure to include these low frequency 
terms would be totally unacceptable.  
3.2 Correspondence of Aligned Units 
Because of the different grammatical requirement 
and language style, a term in the source language 
often differs in different ways from the 
corresponding manifestations in the target 
language. These differences could be alternation 
of POS and the use of paraphrastic expressions. 
Although many term banks avoid such variations 
and focus primarily on equivalent nominals or 
verbs, the correspondence of terms between two 
typologically different languages is often more 
complicated. For example, the English nominal 
(?fulfilment?) is more naturally translated into 
Chinese as a verb (?l??, ?????, ????). 
More examples can be found in Table 1. 
 
Alternation of POS  
English  Chinese POS alternation 
The accused 
o+ 
det + adj ~ noun 
hold 
*? 
verb ~ noun 
fulfillment  
?
 
noun ~ verb 
administration  
?D 
noun ~ verb 
repudiation  
l? 
noun ~ neg + verb 
Table 1. Alternation of POS  
 
In some cases, there are simply no equivalent 
words in the target language. Paraphrasing or 
circumlocution may be necessary. Such 
correspondence is far less consistent and obvious 
to be identified by the computer.  
 
Paraphrasing/Circumlocution 
English Chinese 
The judge entered judgment in 
favour of the respondents in 
respect of their claim for arrears 
of wages, and severance payment. 
t?o31
?2??KDI
??9? 
In our view,? z??a?
 
?evidenced by the Defendant's 
letter ? 
?7:+3?}
???1$Ym
??S??? 
Table 2.  Examples of paraphrasing 
 
Because of language differences, legal terms can 
be contextually realized as anaphors in the target 
language. Examples of such correspondence 
would be useful for legal drafting and translation. 
Again, such anaphoric relations are more 
accurately handled by humans. 
 
Anaphoric Relation 
English Chinese 
He was subsequently 
charged?  
9?3??s? 
Liu JA dealt with that 
application on 14 March 
1996 and dismissed it. 
B9?t?W?
?1996?3?14?A?
 ?-??9?? 
Enforcement of a 
Convention award may 
also be refused if the 
award is in respect of a 
matter which is not capable 
of settlement by arbitration.
???*????1
??"lh?X*
?????M???
N+Proceedings of the ACL-SIGLEX Workshop on Deep Lexical Acquisition, pages 1?9,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Data Homogeneity and Semantic Role Tagging in Chinese 
Oi Yee Kwong and Benjamin K. Tsou 
Language Information Sciences Research Centre 
City University of Hong Kong 
Tat Chee Avenue, Kowloon, Hong Kong 
{rlolivia, rlbtsou}@cityu.edu.hk 
 
 
Abstract 
This paper reports on a study of semantic 
role tagging in Chinese in the absence of a 
parser.  We tackle the task by identifying 
the relevant headwords in a sentence as a 
first step to partially locate the corre-
sponding constituents to be labelled.  We 
also explore the effect of data homogene-
ity by experimenting with a textbook cor-
pus and a news corpus, representing 
simple data and complex data respectively.  
Results suggest that while the headword 
location method remains to be improved, 
the homogeneity between the training and 
testing data is important especially in 
view of the characteristic syntax-
semantics interface in Chinese.  We also 
plan to explore some class-based tech-
niques for the task with reference to exist-
ing semantic lexicons, and to modify the 
method and augment the feature set with 
more linguistic input. 
1 Introduction 
As the development of language resources pro-
gresses from POS-tagged corpora to syntactically 
annotated treebanks, the inclusion of semantic in-
formation such as predicate-argument relations 
becomes indispensable.  The expansion of the Penn 
Treebank into a Proposition Bank (Kingsbury and 
Palmer, 2002) is a typical move in this direction.  
Lexical resources also need to be enhanced with 
semantic information (e.g. Fellbaum et al, 2001).  
The ability to identify semantic role relations cor-
rectly is essential to many applications such as in-
formation extraction and machine translation; and 
making available resources with this kind of in-
formation would in turn facilitate the development 
of such applications. 
Large-scale production of annotated resources 
is often labour intensive, and thus calls for auto-
matic labelling to streamline the process.  The task 
is essentially done in two phases, namely recognis-
ing the constituents bearing some semantic rela-
tionship to the target verb in a sentence, and then 
labelling them with the corresponding semantic 
roles. 
In their seminal proposal, Gildea and Jurafsky 
(2002) approached the task using various features 
such as headword, phrase type, and parse tree path.  
While such features have remained the basic and 
essential features in subsequent research, parsed 
sentences are nevertheless required, for extracting 
the path features during training and providing the 
argument boundaries during testing.  The parse 
information is deemed important for the perform-
ance of role labelling (Gildea and Palmer, 2002; 
Gildea and Hockenmaier, 2003). 
More precisely, parse information is rather 
more critical for the identification of boundaries of 
candidate constituents than for the extraction of 
training data.  Its limited function in training, for 
instance, is reflected in the low coverage reported 
(e.g. You and Chen, 2004).  As full parses are not 
always accessible, many thus resort to shallow syn-
tactic information from simple chunking, even 
though results often turn out to be less satisfactory 
than with full parses. 
This limitation is even more pertinent for the 
application of semantic role labelling to languages 
which do not have sophisticated parsing resources.  
In the case of Chinese, for example, there is con-
1
siderable variability in its syntax-semantics inter-
face; and when one comes to more nested and 
complex sentences such as those from news arti-
cles, it becomes more difficult to capture the sen-
tence structures by typical examples. 
Thus in the current study, we approach the 
problem in Chinese in the absence of parse infor-
mation, and attempt to identify the headwords in 
the relevant constituents in a sentence to be tagged 
as a first step.  In addition, we will explore the ef-
fect of training on different datasets, simple or 
complex, to shed light on the relative importance 
of parse information for indicating constituent 
boundaries in semantic role labelling. 
In Section 2, related work will be reviewed.  In 
Section 3, the data used in the current study will be 
introduced.  Our proposed method will be ex-
plained in Section 4, and the experiment reported 
in Section 5.  Results and future work will be dis-
cussed in Section 6, followed by conclusions in 
Section 7. 
 
2 Related Work 
The definition of semantic roles falls on a contin-
uum from abstract ones to very specific ones.  
Gildea and Jurafsky (2002), for instance, used a set 
of roles defined according to the FrameNet model 
(Baker et al, 1998), thus corresponding to the 
frame elements in individual frames under a par-
ticular domain to which a given verb belongs.  
Lexical entries (in fact not limited to verbs, in the 
case of FrameNet) falling under the same frame 
will share the same set of roles.  Gildea and Palmer 
(2002) defined roles with respect to individual 
predicates in the PropBank, without explicit nam-
ing.  To date PropBank and FrameNet are the two 
main resources in English for training semantic 
role labelling systems, as in the CoNLL-2004 
shared task (Carreras and M?rquez, 2004) and 
SENSEVAL-3 (Litkowski, 2004). 
The theoretical treatment of semantic roles is 
also varied in Chinese.  In practice, for example, 
the semantic roles in the Sinica Treebank mark not 
only verbal arguments but also modifier-head rela-
tions (You and Chen, 2004).  In our present study, 
we go for a set of more abstract semantic roles 
similar to the thematic roles for English used in 
VerbNet (Kipper et al, 2002).  These roles are 
generalisable to most Chinese verbs and are not 
dependent on particular predicates.  They will be 
further introduced in Section 3. 
Approaches in automatic semantic role label-
ling are mostly statistical, typically making use of 
a number of features extracted from parsed training 
sentences.  In Gildea and Jurafsky (2002), the fea-
tures studied include phrase type (pt), governing 
category (gov), parse tree path (path), position of 
constituent with respect to the target predicate (po-
sition), voice (voice), and headword (h).  The la-
belling of a constituent then depends on its 
likelihood to fill each possible role r given the fea-
tures and the target predicate t, as in the following, 
for example: 
 
),,,,,|( tvoicepositiongovpthrP    
 
Subsequent studies exploited a variety of im-
plementation of the learning component.  Trans-
formation-based approaches were also used (e.g. 
see Carreras and M?rquez (2004) for an overview 
of systems participating in the CoNLL shared task).  
Swier and Stevenson (2004) innovated with an un-
supervised approach to the problem, using a boot-
strapping algorithm, and achieved 87% accuracy. 
While the estimation of the probabilities could 
be relatively straightforward, the trick often lies in 
locating the candidate constituents to be labelled.  
A parser of some kind is needed.  Gildea and 
Palmer (2002) compared the effects of full parsing 
and shallow chunking; and found that when con-
stituent boundaries are known, both automatic 
parses and gold standard parses resulted in about 
80% accuracy for subsequent automatic role tag-
ging, but when boundaries are unknown, results 
with automatic parses dropped to 57% precision 
and 50% recall.  With chunking only, performance 
further degraded to below 30%.  Problems mostly 
arise from arguments which correspond to more 
than one chunk, and the misplacement of core ar-
guments.  Sun and Jurafsky (2004) also reported a 
drop in F-score with automatic syntactic parses 
compared to perfect parses for role labelling in 
Chinese, despite the comparatively good results of 
their parser (i.e. the Collins parser ported to Chi-
nese).  The necessity of parse information is also 
reflected from recent evaluation exercises.  For 
instance, most systems in SENSEVAL-3 used a 
parser to obtain full syntactic parses for the sen-
tences, whereas systems participating in the 
CoNLL task were restricted to use only shallow 
2
syntactic information.  Results reported in the for-
mer tend to be higher.  Although the dataset may 
be a factor affecting the labelling performance, it 
nevertheless reinforces the usefulness of full syn-
tactic information. 
According to Carreras and M?rquez (2004), for 
English, the state-of-the-art results reach an F1 
measure of slightly over 83 using gold standard 
parse trees and about 77 with real parsing results.  
Those based on shallow syntactic information is 
about 60. 
In this work, we study the problem in Chinese, 
treating it as a headword identification and label-
ling task in the absence of parse information, and 
examine how the nature of the dataset could affect 
the role tagging performance. 
3 The Data 
3.1 Materials 
In this study, we used two datasets: sentences from 
primary school textbooks were taken as examples 
for simple data, while sentences from a large cor-
pus of newspaper texts were taken as complex ex-
amples. 
Two sets of primary school Chinese textbooks 
popularly used in Hong Kong were taken for refer-
ence.  The two publishers were Keys Press and 
Modern Education Research Society Ltd.  Texts 
for Primary One to Six were digitised, segmented 
into words, and annotated with parts-of-speech 
(POS).  This results in a text collection of about 
165K character tokens and upon segmentation 
about 109K word tokens (about 15K word types).  
There were about 2,500 transitive verb types, with 
frequency ranging from 1 to 926. 
The complex examples were taken from a sub-
set of the LIVAC synchronous corpus1 (Tsou et al, 
2000; Kwong and Tsou, 2003).   The subcorpus 
consists of newspaper texts from Hong Kong, in-
cluding local news, international news, financial 
news, sports news, and entertainment news, col-
lected in 1997-98.  The texts were segmented into 
words and POS-tagged, resulting in about 1.8M 
character tokens and upon segmentation about 1M 
word tokens (about 47K word types).  There were 
about 7,400 transitive verb types, with frequency 
ranging from 1 to just over 6,300. 
                                                          
                                                          
1 http://www.livac.org 
3.2 Training and Testing Data 
For the current study, a set of 41 transitive verbs 
common to the two corpora (hereafter referred to 
as textbook corpus and news corpus), with fre-
quency over 10 and over 50 respectively, was 
sampled.   
Sentences in the corpora containing the sam-
pled verbs were extracted.  Constituents corre-
sponding to semantic roles with respect to the 
target verbs were annotated by a trained human 
annotator and the annotation was verified by an-
other.  In this study, we worked with a set of 11 
predicate-independent abstract semantic roles.  
According to the Dictionary of Verbs in Contem-
porary Chinese (Xiandai Hanyu Dongci Dacidian, 
????????? ? Lin et al, 1994), our se-
mantic roles include the necessary arguments for 
most verbs such as agent and patient, or goal and 
location in some cases; and some optional argu-
ments realised by adjuncts, such as quantity, in-
strument, and source.  Some examples of semantic 
roles with respect to a given predicate are shown in 
Figure 1. 
Altogether 980 sentences covering 41 verb 
types in the textbook corpus were annotated, re-
sulting in 1,974 marked semantic roles (constitu-
ents); and 2,122 sentences covering 41 verb types 
in the news corpus were annotated, resulting in 
4,933 marked constituents2. 
The role labelling system was trained on 90% 
of the sample sentences from the textbook corpus 
and the news corpus separately; and tested on the 
remaining 10% of both corpora.   
4 Automatic Role Labelling 
The automatic labelling was based on the statistical 
approach in Gildea and Jurafsky (2002).  In Sec-
tion 4.1, we will briefly mention the features used 
in the training process.  Then in Sections 4.2 and 
4.3, we will explain our approach for locating 
headwords in candidate constituents associated 
with semantic roles, in the absence of parse infor-
mation. 
2 These figures only refer to the samples used in the current 
study.  In fact over 35,000 sentences in the LIVAC corpus 
have been semantically annotated, covering about 1,500 verb 
types and about 80,000 constituents were marked. 
3
4.1 Training 
In this study, our probability model was based 
mostly on parse-independent features extracted 
from the training sentences, namely: 
 
Headword (head): The headword from each con-
stituent marked with a semantic role was identified.  
For example, in the second sentence in Figure 1, 
?? (school) is the headword in the constituent 
corresponding to the agent of the verb ?? (hold), 
and ?? (contest) is the headword of the noun 
phrase corresponding to the patient. 
 
Position (posit): This feature shows whether the 
constituent being labelled appears before or after 
the target verb.  In the first example in Figure 1, 
the experiencer and time appear on the left of the 
target, while the theme is on its right. 
 
POS of headword (HPos): Without features pro-
vided by the parse, such as phrase type or parse 
tree path, the POS of the headword of the labelled 
constituent could provide limited syntactic infor-
mation. 
 
Preposition (prep): Certain semantic roles like 
time and location are often realised by preposi-
tional phrases, so the preposition introducing the 
relevant constituents would be an informative fea-
ture. 
 
Hence for automatic labelling, given the target 
verb t, the candidate constituent, and the above 
features, the role r which has the highest probabil-
ity for P(r | head, posit, HPos, prep, t) will be as-
signed to that constituent.  In this study, however, 
we are also testing with the unknown boundary 
condition where candidate constituents are not 
available in advance.  To start with, we attempt to 
partially locate them by identifying their head-
words first, as explained in the following sections.  
 
 
 
Figure 1  Examples of semantic roles with respect to a given predicate 
 
 
4.2 Locating Candidate Headwords 
In the absence of parse information, and with con-
stituent boundaries unknown, we attempt to par-
tially locate the candidate constituents by 
identifying their corresponding headwords first.  
Sentences in our test data were segmented into 
words and POS-tagged.  We thus divide the recog-
nition process into two steps, locating the head-
word of a candidate constituent first, and then 
expanding from the headword to determine its 
boundaries. 
Student 
? ?? ?? ?? ? ?? ?? 
Next week school hold tell story contest 
Time Agent Target Patient 
Example: (Next week, the school will hold a story-telling contest.) 
?? ? ?? ?? ?? ? ? 
(-pl) write essay always feel (neg) anything 
Experiencer Target Theme 
Example: (Students always feel there is nothing to write about for their essays.) 
?   ?
time 
?? ? 
can 
Time 
write 
4
Basically, if we consider every word in the 
same sentence with the target verb (both to its left 
and to its right) a potential headword for a candi-
date constituent, what we need to do is to find out 
the most probable words in the sentence to match 
against individual semantic roles.  We start with a 
feature set with more specific distributions, and 
back off to feature sets with less specific distribu-
tions3.  Hence in each round we look for 
 
)|(maxarg setfeaturerP
r
 
 
for every candidate word.  Ties are resolved by 
giving priority to the word nearest to the target 
verb in the sentence. 
Figure 2 shows an example illustrating the pro-
cedures for locating candidate headwords.  The 
target verb is ?? (discover).  In the first round, 
using features head, posit, HPos, and t, ?? (time) 
and ?? (problem) were identified as Time and 
Patient respectively.  In the fourth subsequent 
round, backing off with features posit and HPos, 
?? (we) was identified as a possible Agent.  In 
this round a few other words were identified as 
potential Patients.  However, they would not be 
considered since Patient was already located in a 
previous round.  So in the end the headwords iden-
tified for the test sentence are ?? for Agent, ?
? for Patient and ?? for Time. 
4.3 Constituent Boundary 
Upon the identification of headwords for potential 
constituents, the next step is to expand from these 
headwords for constituent boundaries.  Although 
we are not doing this step in the current study, it 
can potentially be done via some finite state tech-
niques, or better still, with shallow syntactic proc-
essing like simple chunking if available. 
                                                          
3 In this experiment, we back off in the following order: 
P(r|head, posit, HPos, prep t), P(r|head, posit, t), P(r | head, t), 
P(r | HPos, posit, t), P(r | HPos, t).  However, the prep feature 
becomes obsolete when constituent boundaries are unknown. 
5 The Experiment 
5.1 Testing 
The system was trained on the textbook corpus and 
the news corpus separately, and tested on both cor-
pora (the data is homogeneous if the system is 
trained and tested on materials from the same 
source).  The testing was done under the ?known 
constituent? condition and ?unknown constituent? 
condition.  The former essentially corresponds to 
the known-boundary condition in related studies; 
whereas in the unknown-constituent condition, 
which we will call ?headword location? condition 
hereafter, we tested our method of locating candi-
date headwords as explained above in Section 4.2.  
In this study, every noun, verb, adjective, pronoun, 
classifier, and number within the test sentence con-
taining the target verb was considered a potential 
headword for a candidate constituent correspond-
ing to some semantic role.  The performance was 
measured in terms of the precision (defined as the 
percentage of correct outputs among all outputs), 
recall (defined as the percentage of correct outputs 
among expected outputs), and F1 score which is the 
harmonic mean of precision and recall. 
5.2 Results 
The results are shown in Tables 1 and 2, for train-
ing on homogeneous dataset and different dataset 
respectively, and testing under the known constitu-
ent condition and the headword location condition. 
When trained on homogeneous data, the results 
were good on both datasets under the known con-
stituent condition, with an F1 score of about 90.  
This is comparable or even better to the results re-
ported in related studies for known boundary con-
dition.  The difference is that we did not use any 
parse information in the training, not even phrase 
type.  When trained on a different dataset, however, 
the accuracy was maintained for textbook data, but 
it decreased for news data, for the known constitu-
ent condition. 
For the headword location condition, the per-
formance in general was expectedly inferior to that 
for the known constituent condition.  Moreover, 
this degradation seemed to be quite consistent in 
most cases, regardless of the nature of the training 
set.  In fact, despite the effect of training set on 
news data, as mentioned above, the degradation 
5
Sentence: 
?????????????????????????????????????
During revision, we discover a lot o
? 
f problems which we have not thought of or cannot be 
solved, then we go and ask father. 
Candidate  Round 1       ? Round 4    Final Result 
eadwords 
n) 
H
 
?? (revisio    Patient
?? (time)  Time            ----       Time 
?? (we)    Agent       Agent 
k)    Patient
?? (normally) 
?? (thin
? (can) 
?? (solve)    Patient
?? (problem)  Patient    ----       Patient 
? (go)     Patient
? (ask)     Patient
from known constituent to headword location is 
nevertheless the least fo
?? (father)    Patient
r news data when trained 
on 
remature at this stage, given the considerable dif-
 
 
 
Figure 2  Example illustrating the procedures for locating candidate headwords 
  
 
Tex ata News a 
different materials.   
Hence the effect of training data is only obvious 
in the news corpus.  In other words, both sets of 
training data work similarly well with textbook test 
data, but the performance on news test data is 
worse when trained on textbook data.  This is un-
derstandable as the textbook data contain fewer 
examples and the sentence structures are usually 
much simpler than those in newspapers.  Hence the 
system tends to miss many secondary roles like 
location and time, which are not sufficiently repre-
sented in the textbook corpus.  The conclusion that 
training on news data gives better result might be 
ference in the corpus size of the two datasets.  
Nevertheless, the deterioration of results on text-
book sentences, even when trained on news data, is 
simply reinforcing the importance of data homoge-
neity, if nothing else.  More on data homogeneity 
will be discussed in the next section. 
p
In addition, the surprisingly low precision under 
the headword location condition is attributable to a 
technical inadequacy in the way we break ties.  In 
this study we only make an effort to eliminate mul-
tiple tagging of the same role to the same target 
verb in a sentence on either side of the target verb, 
but not if they appear on both sides of the target 
verb.  This should certainly be dealt with in future 
experiments. 
 
 
 
 
 tbook D Dat
 P  Precisionrecision Recall F1 Recall F1
Known Constituent 93.85 87.50 90.56 90.49 87.70 89.07 
Headword Location 46.12 61.98 52.89 38.52 52.25 44.35 
 
Table 1  Results for Training on Homogeneous Datasets 
 
 
6
 
Tex ata News a  tbook D Dat
 P  Precisionrecision Recall F1 Recall F1
Known Constituent 91.85 88.02 89.86 80.30 66.80 72.93 
Headword Location 38.87 57.29 46.32 37.89 42.01 39.84 
 
Table 2  Results for Training on Different Datasets 
 
6 Discussion 
 
hen
cuss this below in relation to 
n?, duration as in 
??
d the parse information 
wo
verb ?? , being very 
pol
he design 
he feature set should benefit 
 m nalysis and input. 
 
6.1 Role of Parse Information 
According to Carreras and M?rquez (2004), the 
state-of-the-art results for semantic role labelling 
systems based on shallow syntactic information is 
about 15 lower than those with access to gold stan-
dard parse trees, i.e., around 60.  With homogene-
ous training and testing data, our experimental 
results for the headword location condition, with 
no syntactic information available at all, give an F1 
score of 52.89 and 44.35 respectively for textbook 
data and news data.  Such results are in line with 
and comparable to those reported for the unknown 
boundary condition with automatic parses in 
Gildea and Palmer (2002), for instance.  Moreover, 
when they used simple chunks instead of full 
parses, the performance resulted in a drop to below 
50% precision and 35% recall with relaxed scoring,
ce their conclusion on the necessity of a parser. 
The more degradation in performance observed 
in the news data is nevertheless within expectation, 
and it suggests that simple and complex data seem 
to have varied dependence on parse information.  
We will further dis
data homogeneity. 
6.2 Data Homogeneity 
The usefulness of parse information for semantic 
role labelling is especially interesting in the case of 
Chinese, given the flexibility in its syntax-
semantics interface (e.g. the object after ? ?eat? 
could refer to the patient as in ??? ?eat apple?, 
location as in ??? ?eat cantee
? ?eat three years?, etc.).   
  As reflected from the results, the nature of 
training data is obviously more important for the 
news data than the textbook data; and the main 
reason might be the failure of the simple training 
data to capture the many complex structures of the 
news sentences, as we suggested earlier.  The rela-
tive flexibility in the syntax-semantics interface of 
Chinese is particularly salient; hence when a sen-
tence gets more complicated, there might be more 
intervening constituents an
uld be useful to help identify the relevant ones 
in semantic role labelling. 
With respect to the data used in the experiment, 
we tried to explore the complexity in terms of the 
average sentence length and number of semantic 
role patterns exhibited.  For the news data, the av-
erage sentence length is around 59.7 characters 
(syllables), and the number of semantic role pat-
terns varies from 4 (e.g. ?? ?to plan?) to as many 
as 25 (e.g. ?? ?to proceed with some action?), 
with an average of 9.5 patterns per verb.  On the 
other hand, the textbook data give an average sen-
tence length of around 39.7 characters, and the 
number of semantic role patterns only varies from 
1 (e.g. ?? ?to decide?) to 11 (e.g. ?? ?to hold 
some event?), with an average of 5.1 patterns per 
verb.  Interestingly, the 
ymorphous in news texts, only shows 5 differ-
ent patterns in textbooks. 
Thus the nature of the dataset for semantic role 
labelling is worth further investigation.  T
of the method and t
from ore linguistic a
6.3 Future Work 
In terms of future development, apart from improv-
ing the handling of ties in our method, as men-
tioned above, we plan to expand our work in 
several respects.  The major part would be on the 
generalization to unseen headwords and unseen 
predicates.  As is with other related studies, the 
examples available for training for each target verb 
are very limited; and the availability of training 
data is also insufficient in the sense that we cannot 
expect them to cover all target verb types.  Hence 
7
it is very important to be able to generalize the 
process to unseen words and predicates.  To this 
end we will experiment with a semantic lexicon 
like Tongyici Cilin (?????, a Chinese the-
sau
re of 
Chinese, we intend to improve our method and 
re linguistic consideration. 
 
 
 semantic lexicons, 
and to modify the method and augment the feature 
set with more linguistic input. 
This work is supported by Competitive Earmarked 
Research Grants (CERG) of the Research Grants 
Hong Kong under grant Nos. 
R
Ba
 the 
Ca troduction to the 
Fe
Resources, Invited Talk, Pittsburg, PA. 
Gi D. and Palmer, M. (2002)  The Necessity of 
Gi kenmaier, J. (2003)  Identifying Se-
Kw
r Part-of-speech 
Tagging. In Proceedings of the Research Note Ses-
sion of the 10th Conference of the European Chapter 
rus) in both training and testing, which we ex-
pect to improve the overall performance. 
Another area of interest is to look at the behav-
iour of near-synonymous predicates in the tagging 
process.  Many predicates may be unseen in the 
training data, but while the probability estimation 
could be generalized from near-synonyms as sug-
gested by a semantic lexicon, whether the similar-
ity and subtle differences between near-synonyms 
with respect to the argument structure and the cor-
responding syntactic realisation could be distin-
guished would also be worth studying.  Related to 
this is the possibility of augmenting the feature set.  
Xue and Palmer (2004), for instance, looked into 
new features such as syntactic frame, lexicalized 
constituent type, etc., and found that enriching the 
feature set improved the labelling performance.  In 
particular, given the importance of data homogene-
ity as observed from the experimental results, and 
the challenges posed by the characteristic natu
feature set with mo
7 Conclusion 
The study reported in this paper has thus tackled 
semantic role labelling in Chinese in the absence of 
parse information, by attempting to locate the cor-
responding headwords first.  We experimented 
with both simple and complex data, and have ex-
plored the effect of training on different datasets. 
Using only parse-independent features, our results 
under the known boundary condition are compara-
ble to those reported in related studies.  The head-
word location method can be further improved. 
More importantly, we have observed the impor-
tance of data homogeneity, which is especially sa-
lient given the relative flexibility of Chinese in its 
syntax-semantics interface.  As a next step, we 
plan to explore some class-based techniques for the 
task with reference to existing
 
Acknowledgements 
Council of 
CityU1233/01H and CityU1317/03H. 
 
eferences 
ker, C.F., Fillmore, C.J. and Lowe, J.B. (1998)  The 
Berkeley FrameNet Project.  In Proceedings of
36th Annual Meeting of the Association for Computa-
tional Linguistics and the 17th International Confer-
ence on Computational Linguistics (COLING-
ACL ?98), Montreal, Quebec, Canada, pp.86-90. 
rreras, X. and M?rquez, L. (2004)  In
CoNLL-2004 Shared Task: Semantic Role Labeling.  
In Proceedings of the Eighth Conference on Compu-
tational Natural Language Learning (CoNLL-2004), 
Boston, Massachusetts, pp.89-97. 
llbaum, C., Palmer, M., Dang, H.T., Delfs, L. and 
Wolf, S. (2001)  Manual and Automatic Semantic 
Annotation with WordNet.  In Proceedings of the 
NAACL-01 SIGLEX Workshop on WordNet and 
Other Lexical 
Gildea, D. and Jurafsky, D. (2002)  Automatic Labeling 
of Semantic Roles.  Computational Linguistics, 28(3): 
245-288. 
ldea, 
Parsing for Predicate Argument Recognition.  In Pro-
ceedings of the 40th Meeting of the Association for 
Computational Linguistics (ACL-02), Philadelphia, 
PA. 
ldea, D. and Hoc
mantic Roles Using Combinatory Categorial Gram-
mar.  In Proceedings of the 2003 Conference on 
Empirical Methods in Natural Language Processing, 
Sapporo, Japan. 
Kingsbury, P. and Palmer, M. (2002)  From TreeBank 
to PropBank.  In Proceedings of the Third Confer-
ence on Language Resources and Evaluation (LREC-
02), Las Palmas, Canary Islands, Spain. 
Kipper, K., Palmer, M. and  Rambow, O. (2002)  Ex-
tending PropBank with VerbNet Semantic Predicates.  
In Proceedings of the AMTA-2002 Workshop on Ap-
plied Interlinguas, Tiburon, CA. 
ong, O.Y. and Tsou, B.K. (2003) Categorial Fluidity 
in Chinese and its Implications fo
8
of the Association for Computational Linguistics, 
Budapest, Hungary, pages 115-118. 
n, X., Wang, L. and Sun, D. Li (1994)  Dictionary of 
Li
uation of 
(
Sun
apter of the Association for Computa-
Swi
Ts
ings of 
Xu
2004 
Yo
.109-115. 
? ongguo Yuwen.  Primary 1-6, 
24 volumes, 2004.  Hong Kong: Modern Education 
Research Society Ltd. 
 
Verbs in Contemporary Chinese.  Beijing Language 
and Culture University Press. 
tkowski, K.C. (2004) SENSEVAL-3 Task: Automatic 
Labeling of Semantic Roles.  In Proceedings of the 
Third International Workshop on the Eval
Systems for the Semantic Analysis of Text 
SENSEVAL-3), Barcelona, Spain, pp.9-12. 
, H. and Jurafsky, D. (2004)  Shallow Semantic 
Parsing of Chinese.  In Proceedings of the Human 
Language Technology Conference of the North 
American Ch
tional Linguistics (HLT-NAACL 2004), Boston, 
pp.249-256. 
er, R.S. and Stevenson, S. (2004)  Unsupervised 
Semantic Role Labelling.  In Proceedings of the 
2004 Conference on Empirical Methods in Natural 
Language Processing, Barcelona, Spain, pp.95-102. 
ou, B.K., Tsoi, W.F., Lai, T.B.Y., Hu, J. and Chan, 
S.W.K. (2000)  LIVAC, A Chinese Synchronous 
Corpus, and Some Applications.  In Proceed
the ICCLC International Conference on Chinese 
Language Computing, Chicago, pp. 233-238. 
e, N. and Palmer, M. (2004)  Calibrating Features for 
Semantic Role Labeling.  In Proceedings of the 
Conference on Empirical Methods in Natural Lan-
guage Processing, Barcelona, Spain, pp.88-94. 
u, J-M. and Chen, K-J. (2004)  Automatic Semantic 
Role Assignment for a Tree Structure.  In Proceed-
ings of the 3rd SigHAN Workshop on Chinese Lan-
guage Processing, ACL-04, Barcelona, pp
?????? Qisi Zhongguo Yuwen.  Primary 1-6, 24 
volumes, 2004.  Hong Kong: Keys Press. 
????? Xiandai Zh
9
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 9?16,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Regional Variation of Domain-Specific Lexical Items: Toward a Pan-
Chinese Lexical Resource 
 
 
Oi Yee Kwong and Benjamin K. Tsou 
Language Information Sciences Research Centre 
City University of Hong Kong 
Tat Chee Avenue, Kowloon, Hong Kong 
{rlolivia,rlbtsou}@cityu.edu.hk 
 
  
 
Abstract 
This paper reports on an initial and nec-
essary step toward the construction of a 
Pan-Chinese lexical resource.  We inves-
tigated the regional variation of lexical 
items in two specific domains, finance 
and sports; and explored how much of 
such variation is covered in existing Chi-
nese synonym dictionaries, in particular 
the Tongyici Cilin.  The domain-specific 
lexical items were obtained from subsec-
tions of a synchronous Chinese corpus, 
LIVAC.  Results showed that 20-40% of 
the words from various subcorpora are 
unique to the individual communities, 
and as much as 70% of such unique items 
are not yet covered in the Tongyici Cilin.  
The results suggested great potential for 
building a Pan-Chinese lexical resource 
for Chinese language processing.  Our 
next step would be to explore automatic 
means for extracting related lexical items 
from the corpus, and to incorporate them 
into existing semantic classifications. 
1 Introduction 
Many cities have underground railway systems.  
Somehow one takes the tube in London but the 
subway in New York.  In a more recent edition 
of the Roget?s Thesaurus (Kirkpatrick, 1987), 
subway, tube, underground railway and metro 
are found in the same semicolon-separated group 
under head 624 Way.  Similarly if one looks up 
WordNet (http://wordnet.princeton.edu; Miller et 
al., 1990), the synset to which subway belongs 
also contains the words metro, tube, under-
ground, and subway system; and it is further in-
dicated that ?in Paris the subway system is called 
the ?metro? and in London it is called the ?tube? 
or the ?underground??.  Such regional lexical 
variation is also found in Chinese.  For instance, 
the subway system in Hong Kong, known as the 
Mass Transit Railway or MTR, is called ?? in 
Chinese.  The subway systems in Beijing and 
Shanghai, as well as the one in Singapore, are 
also known as ??, but that in Taipei is known 
as ??.  Their counterpart in Japan is written as 
??? in Kanji.  Such regional variation, as part 
of lexical knowledge, is important and useful for 
many natural language applications, including 
natural language understanding, information re-
trieval, and machine translation.  Unfortunately, 
existing Chinese lexical resources often lack 
such comprehensiveness. 
To fill this gap, Tsou and Kwong (2006) pro-
posed a comprehensive Pan-Chinese lexical re-
source, based on a large and unique synchronous 
Chinese corpus as an authentic basis for lexical 
acquisition and analysis across various Chinese 
speech communities.  For a significant world 
language like Chinese, a useful lexical resource 
should have maximum versatility and portability.  
It is not sufficient to target at one particular 
community speaking the language and thus cover 
only language usage observed from that particu-
lar community.  Instead, such a lexical resource 
should document the core and universal sub-
stances of the language on the one hand, and also 
the more subtle variations found in different 
communities on the other.  As is evident from the 
above example on the variation of subway, re-
gional variation should be captured for the lexi-
cal resource to be useful in a wide range of ap-
plications. 
In this study, we investigate and compare the 
regional variation of lexical items from two spe-
9
cific domains, finance and sports, as an initial 
and necessary step toward the more important 
undertaking of building a Pan-Chinese lexical 
resource.  In addition, we make use of an exist-
ing Chinese synonym dictionary, the Tongyici 
Cilin (Mei et al, 1984) as leverage, and explore 
its coverage of such variation and thus the poten-
tial for enriching it.  The lexical items under 
study were obtained from a synchronous Chinese 
corpus, LIVAC, which will be further introduced 
in Section 4.  Corpus data from four Chinese 
speech communities were compared with respect 
to their commonality and uniqueness, and also 
against Cilin for their coverage.  Results showed 
that 20-40% of the words extracted from the cor-
pus are unique to the individual communities, 
and as much as 70% of such unique items are not 
yet covered in Cilin.  It therefore suggests that 
the synchronous corpus is a rich source for 
mining region-specific lexical items, and there is 
great potential for building a Pan-Chinese lexical 
resource for Chinese language processing. 
In Section 2, we will briefly review existing 
resources and related work.  Then in Section 3, 
we will briefly outline the design and architec-
ture of the Pan-Chinese lexical resource pro-
posed by Tsou and Kwong (2006).  In Section 4, 
we will further describe the Chinese synonym 
dictionary and the synchronous Chinese corpus 
used in this study.  The comparison of their lexi-
cal items will be discussed in Section 5.  Future 
directions will be presented in Section 6, fol-
lowed by a conclusion. 
2 Existing Resources and Related Work 
The construction and development of large 
lexical resources is relying more and more on 
corpus-based approaches, not only as a result of 
the increased availability of large corpora, but 
also for the authoritativeness and authenticity 
allowed by the approach.  The Collins 
COBUILD English Dictionary (Sinclair, 1987) is 
amongst the most well-known lexicographic fruit 
based on large corpora. 
For natural language applications, much of the 
information in conventional dictionaries targeted 
at human readers must be made explicit.  Lexical 
resources for computer use thus need consider-
able manipulation, customisation, and supple-
mentation (e.g. Calzolari, 1982).  WordNet 
(Miller et al, 1990), grouping words into synsets 
and linking them up with relational pointers, is 
probably the first broad coverage general compu-
tational lexical database.  In view of the intensive 
time and effort required in resource building, 
some researchers have taken an alternative route 
by extracting information from existing machine-
readable dictionaries and corpora semi-
automatically (e.g. Vossen et al, 1989; Riloff 
and Shepherd, 1999; Lin et al 2003). 
Compared to the development of thesauri and 
lexical databases, and research into semantic 
networks for major languages such as English, 
similar work for the Chinese language is less 
mature.  This gap was partly due to the lack of 
authoritative Chinese corpora as a basis for 
analysis, but has been gradually reduced with the 
recent availability of large Chinese corpora in-
cluding the LIVAC synchronous corpus (Tsou 
and Lai, 2003) used in this work and further de-
scribed below, the Sinica Corpus (Chen et al, 
1996), the Chinese Penn Treebank (Xia et al, 
2000), and the like. 
An important issue which is seldom addressed 
in the construction of Chinese lexical databases 
is the problem of versatility and portability.  For 
a language such as Chinese which is spoken in 
many different communities, different linguistic 
norms have emerged as a result of the individual-
istic evolution and development of the language 
within a particular community and culture.  Such 
variations are seldom adequately reflected in ex-
isting lexical resources, which often only draw 
reference from one particular source.  For in-
stance, Tongyici Cilin (?????) (Mei et al, 
1984) is a thesaurus containing some 70,000 
Chinese lexical items in the tradition of the Ro-
get?s Thesaurus for English, that is, in a hierar-
chy of broad conceptual categories.  First pub-
lished in the 1980s, it was based exclusively on 
Chinese as used in post-1949 Mainland China.  
Thus for the subway example above, the closest 
word group found is ??, ?? (train) only, let 
alone the subway itself and its regional variations. 
With the recent availability of large corpora, 
especially synchronous ones, to construct an au-
thoritative and timely lexical resource for Chi-
nese is less distant than it was in the past.  A 
large synchronous corpus provides authentic ex-
amples of the language as used in a variety of 
locations.  It thus enables us to attempt a com-
prehensive and in-depth analysis of the core 
common language in constructing a lexical re-
source; and to incorporate useful information 
relating to location-sensitive linguistic variations. 
10
3 Proposal of a Pan-Chinese Thesaurus 
The Pan-Chinese lexicon proposed by Tsou and 
Kwong (2006) is expected to capture not only the 
core senses of lexical items but also senses and 
uses specific to individual Chinese speech 
communities. 
The lexical database will be organised into a 
core database and a supplementary one.  The 
core database will contain the core lexical infor-
mation for word senses and usages which are 
common to most Chinese speech communities, 
whereas the supplementary database will contain 
the language uses specific to individual commu-
nities, including ?marginal? and ?sublanguage? 
uses. 
A network structure will be adopted for the 
lexical items.  The nodes could be sets of near-
synonyms or single lexical items (in which case 
synonymy will be one type of links).  The links 
will not only represent the paradigmatic semantic 
relations but also syntagmatic ones (such as se-
lectional restrictions). 
We thus begin by investigating in depth the 
regional variation of lexical items, especially 
domain-specific words, among several Chinese 
speech communities.  In addition, we explore the 
potential of enriching existing resources as a start.  
In the following section, we will discuss the 
Tongyici Cilin and the synchronous Chinese cor-
pus used in this study in greater details. 
4 Materials and Method 
4.1 The Tongyici Cilin 
The Tongyici Cilin (?????) (Mei et al, 
1984) is a Chinese synonym dictionary, or more 
often known as a Chinese thesaurus in the tradi-
tion of the Roget?s Thesaurus for English.  The 
Roget?s Thesaurus has about 1,000 numbered 
semantic heads, more generally grouped under 
higher level semantic classes and subclasses, and 
more specifically differentiated into paragraphs 
and semicolon-separated word groups.  Similarly, 
some 70,000 Chinese lexical items are organized 
into a hierarchy of broad conceptual categories in 
the Tongyici Cilin.  Its classification consists of 
12 top-level semantic classes, 94 sub-classes, 
1,248 semantic heads and 3,925 paragraphs. 
4.2 The LIVAC Synchronous Corpus 
LIVAC (http://www.livac.org) stands for Lin-
guistic Variation in Chinese Speech Communi-
ties.  It is a synchronous corpus developed by the 
Language Information Sciences Research Centre 
of the City University of Hong Kong since 1995 
(Tsou and Lai, 2003).  The corpus consists of 
newspaper articles collected regularly and syn-
chronously from six Chinese speech communi-
ties, namely Hong Kong, Beijing, Taipei, Singa-
pore, Shanghai, and Macau.  Texts collected 
cover a variety of domains, including front page 
news stories, local news, international news, edi-
torials, sports news, entertainment news, and fi-
nancial news.  Up to December 2005, the corpus 
has already accumulated about 180 million char-
acter tokens which, upon automatic word seg-
mentation and manual verification, amount to 
over 900K word types. 
For the present study, we make use of the sub-
corpora collected over the 9-year period 1995-
2004 from Hong Kong (HK), Beijing (BJ), 
Taipei (TW), and Singapore (SG).  In particular, 
we focus on the financial news and sports news 
to investigate the commonality and uniqueness of 
the lexical items used in these specific domains 
in the various communities.  We also evaluate 
the adequacy of the Tongyici Cilin in terms of its 
coverage of such domain-specific terms espe-
cially from the Pan-Chinese perspective, and 
thus assess the room for its enrichment with the 
synchronous corpus.  Table 1 shows the sizes of 
the subcorpora used for this study. 
 
Subcorpus Overall 
(rounded to nearest 0.01M) 
Financial News 
(rounded to nearest 1K) 
Sports News 
(rounded to nearest 1K) 
 Word Token Word Type Word Token Word Type Word Token Word Type 
HK 14.39M 0.22M 970K 38K 1041K 39K 
BJ 11.70M 0.19M 232K 20K 443K 28K 
TW 12.32M 0.20M 254K 22K 657K 33K 
SG 13.22M 0.21M 621K 28K 998K 34K 
Table 1  Sizes of individual subcorpora 
 
11
4.3 Procedures 
Word-frequency lists were generated from the 
financial and sports subcorpora from each indi-
vidual community.  For each resulting list, the 
steps below were followed to remove irrelevant 
items and retain only the potentially useful con-
tent words: 
(a) Remove all numbers and non-Chinese words. 
(b) Remove all proper names, including those 
annotated as personal names, geographical 
names, and organisation names.  Proper 
names have been annotated in the corpora 
during the process of word segmentation. 
(c) Remove function words. 
(d) Remove lexical items with frequency 5 or 
below. 
The numbers of remaining items in each sub-
corpus after the above steps are listed in Tables 2 
and 3 for the two domains respectively.  The 
lexical items retained, which are expected to con-
tain a substantial amount of content words, are 
potentially useful for the current study.  The lists 
in each domain (from the various subcorpora) 
were compared in terms of the items they share 
and those unique to individual communities.  
Their unique items were also compared against 
the Tongyici Cilin to investigate its adequacy 
and explore how it might be enriched with the 
synchronous corpus. 
 
Subcorpus All After (a) After (b) After (c) After(d) 
HK 37,525 27,937 20,422 17,162 5,238 
BJ 20,025 17,361 14,460 12,134 2,791 
TW 22,142 19,428 16,316 13,496 3,088 
SG 28,193 22,829 16,863 13,822 3,836 
Table 2  Number of word types remaining after various data cleaning steps for the financial domain 
 
Subcorpus All After (a) After (b) After (c) After(d) 
HK 39,190 35,720 25,289 21,502 6,316 
BJ 27,971 26,049 19,799 16,598 3,878 
TW 32,706 30,231 20,361 17,248 4,601 
SG 34,040 31,974 19,995 16,780 5,120 
Table 3  Number of word types remaining after various data cleaning steps for the sports domain 
 
5 Results and Discussion 
5.1 Lexical Items from LIVAC 
The four subcorpora of the financial domain 
differ considerably in their sizes, and slightly less 
so for the sports domain.  Despite this, we ob-
served for both domains from Tables 2 and 3 that 
in general about 40-50% of all word types are 
numbers, non-Chinese words, proper names, and 
function words.  Of the remaining items, about 
20-30% have frequency greater than 5.  These 
several thousand word types from each subcor-
pus are expected to be amongst the more interest-
ing items and form the ?candidate sets? for fur-
ther investigation. 
5.2 Commonality among Various Regions  
Comparing the candidate sets from various sub-
corpora, which reflect the use of Chinese in vari-
ous Chinese speech communities, Tables 4 and 5 
show the sizes of the intersection sets among 
different places for the two domains respectively. 
The intersection set for all four places contains 
slightly more than 1,000 lexical items in the fi-
nancial domain.  A quick skim through these 
common lexical items suggests that they contain, 
on the one hand, the many general concepts in 
the financial domain (e.g. ?? company, ?? 
market, ?? bank, ?? invest / investment, ?
? business, ?? develop / development, ?? 
corporation, ?? stock shares, ?? shareholder, 
?? capital, etc.); and on the other hand, many 
reportage and cognitive verbs often used in news 
articles (e.g. ?? express, ?? reckon, ?? 
appear, ?? reflect, etc.). 
In the sports domain, more than 1,700 lexical 
items were found in all of the four subcorpora.  
Like its financial counterpart, we found many 
general concepts at the top of the list (e.g. ?? 
player, ?? team, ?? match, ?? competi-
12
tion, ?? league, ?? coach, ?? opponent, 
?? champion, etc.). 
The numbers of overlaps in Tables 4 suggest 
that lexical items used in Mainland China (as 
evident from BJ data) seem to have the least in 
common with the rest.  For instance, compared to 
the overlap amongst all four regions (i.e. 1,039), 
the overlap has increased most when BJ was not 
included in the comparison; and when we com-
pare any two regions, the overlap between BJ 
and TW is smallest.  Nevertheless, such unique-
ness of BJ data is less apparent in the sports do-
main.  In particular, the difference between 
HK/BJ and BJ/TW is even slightly less than that 
in the financial domain. 
If we look at the individual regions, HK ap-
parently shares most (about 50%) with SG, and 
vice versa (about 68%), in the financial domain.  
At the same time, BJ also shares more with HK 
than with the other two regions, and so does TW.  
But surprisingly, BJ has over 60% overlap with 
SG and about 55% with TW in the sports domain.  
The overlaps of TW with HK and with BJ differ 
by more than 20% in the finance domain, but 
only by about 10% in the sports domain.  All 
these patterns might suggest lexical items in the 
financial domain are more versatile and have 
more varied focus in different communities, 
whereas those in the sports domain reflect the 
more common interests of different places.   
 
Regions Overlap Proportion to individual lists (%) 
  HK BJ TW SG 
HK / BJ / TW / SG 1039 19.84 37.23 33.65 27.09 
HK / BJ / TW 1126 21.50 40.34 36.46  
HK / BJ / SG 1327 25.33 47.55  34.59 
HK / TW / SG 1581 30.18  51.20 41.21 
BJ / TW / SG 1092  39.13 35.36 28.47 
HK / BJ 1609 30.72 57.65   
HK / TW 1912 36.50  61.92  
HK / SG 2607 49.77   67.96 
BJ / TW 1250  44.79 40.48  
BJ / SG 1505  53.92  39.23 
TW / SG 1795   58.13 46.79 
Table 4  Commonality amongst various regions for the financial domain 
 
Regions Overlap Proportion to individual lists (%) 
  HK BJ TW SG 
HK / BJ / TW / SG 1668 26.41 43.01 36.25 32.58 
HK / BJ / TW 1782 28.21 45.95 38.73  
HK / BJ / SG 2047 32.41 52.78  39.98 
HK / TW / SG 2249 35.61  48.88 43.93 
BJ / TW / SG 1864  48.07 40.51 36.41 
HK / BJ 2318 36.70 59.77   
HK / TW 2693 42.64  58.53  
HK / SG 3305 52.33   64.55 
BJ / TW 2124  54.77 46.16  
BJ / SG 2554  65.86  49.88 
TW / SG 2709   58.88 52.91 
Table 5  Commonality amongst various regions for the sports domain 
 
5.3 Uniqueness of Various Regions 
Next we compared the lists with respect to what 
they have unique to themselves.  Table 6 shows 
the numbers of unique items found in each list, 
together with examples from the most frequent 
20 unique items in each case. 
Again, taking the size difference among the 
candidate sets into account, about 40% of the 
lexical items found in HK data are unique to the 
region, which re-echoes the versatility and wide 
13
coverage of interests of HK data.  This is espe-
cially evident when compared to only about 20% 
of the candidate sets for SG are unique to Singa-
pore.1 
Looking at the unique lexical items found in 
individual regions, it is not difficult to see the 
region-specific lexicalisation of certain concepts.  
For instance, in terms of housing, ?? (housing 
under the Home Ownership Scheme) is a specific 
kind of housing in Hong Kong, ?? is a specific 
term in Singapore (as seen in SG data), whereas 
housing is generally expressed as ??  in 
Mainland China (as seen in BJ data).  Similarly, 
?? (HK) and ?? (BJ) both refer to training, 
but may relate to different practice in the two 
communities.  Such regional variation lends 
strong support to the importance of a Pan-
Chinese lexical resource. 
The lists of unique items also suggest the vari-
ous focus and orientation in different Chinese 
speech communities.  For example, while Hong 
Kong pays much attention to the real estate mar-
ket and stock market, Mainland China may be 
focusing more on the basic needs like water, 
farming, poverty alleviation, etc., and Singapore 
is relatively more concerned with local affairs 
like port management.  The passion for baseball, 
among other more popular sports like soccer, is 
most obvious from the unique lexical items 
found in TW data. 
5.4 Comparison with Tongyici Cilin 
As mentioned earlier, the Tongyici Cilin contains 
some 70,000 lexical items under 12 broad se-
mantic classes, 94 subclasses, and 1,428 heads.  
It was first published in the 1980s and was based 
on lexical usages mostly of post-1949 Mainland 
China.  In this section, we discuss the results ob-
tained from comparing the unique lexical items 
found from individual subcorpora with Cilin, 
which are shown in Table 7. 
On the one hand, Cilin?s collection of words 
may be considerably dated and obviously will 
not include new concepts and neologisms arising 
in the last two decades.  On the other hand, the 
data in LIVAC come from newspaper materials 
in the 1990s.  So overall speaking, for each of the 
unique word lists, much less than 50% are cov-
ered in Cilin. 
                                                 
1 Upon further analysis, on average about 60% of these 
?unique? items were actually found in one or more of the 
other regions, but with frequency 5 or below.  Since the 
difference in frequency is quite large for most items, we can 
reasonably treat them as unique to a particular community. 
Nevertheless, there is still an apparent gap be-
tween Cilin?s coverage of the unique items from 
various places.  About 40% of the unique items 
found in BJ for both domains are covered; but 
for other places, the coverage is more often less 
than 30% in either or both domains.  Again, this 
could be considered a result of Cilin?s bias to-
ward lexical usages in Mainland China. 
In addition, while almost 40% of the unique 
items in BJ data are found in Cilin, many of 
these unique items covered are amongst the most 
frequent items.  On the contrary, even though 
about 560 unique items in HK data are also 
found in Cilin, only 3 out of the 20 most frequent 
items are amongst them.  In addition, the appar-
ent coverage does not necessarily suggest the 
correct match of word senses.  For instance, ?? 
is found under head Bn1 together with other 
items like ??, ??, etc., all of which only re-
fer to the general concept of housing, instead of 
the housing specifically under the Home Owner-
ship Scheme as known in Hong Kong.   Also, 
coverage of words like ??, ?? and ?? in 
the sports domain does not match their actual 
usages which refer to team names.  A more inter-
esting example might be ??, which is used in 
the basketball context in TW data, and in no way 
refers to the literal ?hot pot? sense. 
Results from the above comparisons thus sup-
port that (1) different Chinese speech communi-
ties have their distinct usage of Chinese lexical 
items, in terms of both form and sense; (2) such 
variation is found in different domains, such as 
the financial and sports domain; (3) existing 
lexical resources, the Tongyici Cilin in particular 
as in our current study, should be enriched and 
enhanced by capturing lexical usages from a va-
riety of Chinese speech communities, to repre-
sent the lexical items from a Pan-Chinese per-
spective; and (4) lexical items obtained from the 
synchronous Chinese corpus can supplement the 
existing content of the Tongyici Cilin, with more 
contemporarily lexicalised concepts, as well as 
variant expressions of similar and related con-
cepts from various Chinese speech communities. 
Hence it remains for us to further investigate 
how the related lexical items obtained from the 
synchronous corpus should be grouped and in-
corporated into the semantic classification of ex-
isting lexical resources; and to further explore 
how they might be extracted in a large scale by 
automatic means.  These will definitely be 
amongst the most important future directions as 
discussed in the next section. 
14
6 Future Work 
In the current study, we have investigated the 
regional variation of lexical items from the fi-
nancial and sports domain, and the coverage of 
the Tongyici Cilin for such variation.  The results 
suggested great potential for building a Pan-
Chinese lexical resource for Chinese language 
processing.  Our next step would thus be to fur-
ther investigate more automatic means for ex-
tracting the near-synonymous or closely related 
items from the various subcorpora.  To this end, 
we would explore algorithms like those used in 
Lin et al (2003).  Of similar importance is the 
mechanism for grouping the related lexical items 
and incorporating them into the semantic classi-
fications of existing lexical resources.  In this 
regard we will proceed with further in-depth 
analysis of the classificatory structures of indi-
vidual resources and fit in our Pan-Chinese ar-
chitecture. 
Apart from the Tongyici Cilin, there are other 
existing Chinese lexical resources such as 
HowNet (Dong and Dong, 2000), SUMO and 
Chinese WordNet (Huang et al, 2004), as well 
as other synonym dictionaries from which we 
might draw reference to build up our Pan-
Chinese lexical resource. 
7 Conclusion 
In this paper, we have investigated the regional 
variation of lexical items in two specific domains 
from a synchronous Chinese corpus, and ex-
plored their coverage in a Chinese synonym dic-
tionary.  Results are encouraging in the sense 
that 20-40% of the candidate words from various 
subcorpora are unique to the individual commu-
nities, and as much as 70% of such unique items 
are not yet covered in the Tongyici Cilin.  It 
therefore suggests great importance and potential 
for a Pan-Chinese lexical resource which we aim 
to construct.  The synchronous corpus is a valu-
able resource for mining the region-specific 
expressions while existing synonym dictionaries 
might provide a ready-made semantic classifica-
tory structure.  Our next step would be to explore 
automatic means for extracting related lexical 
items from the corpus, and to incorporate them 
into existing semantic classifications. 
Acknowledgements 
This work is supported by Competitive Ear-
marked Research Grant (CERG) of the Research 
Grants Council of Hong Kong under grant No. 
CityU1317/03H.  The authors would like to 
thank the anonymous reviewers for comments. 
References 
Calzolari, N. (1982)  Towards the organization of 
lexical definitions on a database structure. In E. 
Hajicova (Ed.), COLING ?82 Abstracts, Charles 
University, Prague, pp.61-64. 
Caraballo, S.A. (1999)  Automatic construction of a 
hypernym-labeled noun hierarchy.  In Proceedings 
of the 37th Annual Meeting of the Association for 
Computational Linguistics (ACL?99), College Park, 
Maryland, pp.120-126. 
Chen, K-J., Huang, C-R., Chang, L-P. and Hsu, H-L. 
(1996)  Sinica Corpus: Design Methodology for 
Balanced Corpora.  In Proceedings of the 11th Pa-
cific Asia Conference on Language, Information, 
and Computation (PACLIC 11), Seoul, Korea, 
pp.167-176. 
Dong, Z. and Dong, Q. (2000)  HowNet.  
http://www.keenage.com. 
Huang, C-R., Chang, R-Y. and Lee, S-B. (2004)  
Sinica BOW (Bilingual Ontological Wordnet): In-
tegration of Bilingual WordNet and SUMO.  In 
Proceedings of the 4th International Conference on 
Language Resources and Evaluation (LREC2004), 
Lisbon, Portugal. 
Kirkpatrick, B. (1987)  Roget?s Thesaurus of English 
Words and Phrases.  Penguin Books. 
Lin, D., Zhao, S., Qin, L. and Zhou, M. (2003)  Iden-
tifying Synonyms among Distributionally Similar 
Words.  In Proceedings of the 18th Joint Interna-
tional Conference on Artificial Intelligence (IJCAI-
03), Acapulco, pp.1492-1493 . 
Mei et al ??????????????? (1984)  
??????? (Tongyici Cilin).  ????? 
(Commerical Press) / ???????. 
Miller, G.A., Beckwith, R., Fellbaum, C., Gross, D. 
and Miller, K.J. (1990)  Introduction to WordNet: 
An online lexical database.  International Journal 
of Lexicography, 3(4):235-244. 
Riloff, E. and Shepherd, J. (1999)  A corpus-based 
bootstrapping algorithm for semi-automated se-
mantic lexicon construction.  Natural Language 
Engineering, 5(2):147-156. 
Sinclair, J. (1987)  Collins COBUILD English Lan-
guage Dictionary.  London, UK: HarperCollins. 
Tsou, B.K. and Kwong, O.Y. (2006)  Toward a Pan-
Chinese Thesaurus.  In Proceedings of the Fifth 
International Conference on Language Resources 
and Evaluation (LREC 2006), Genoa, Italy. 
Tsou, B.K. and Lai, T.B.Y. ??????? (2003)  
????????????.  In B. Xu, M. Sun 
15
and G. Jin ?????????? (Eds.), ???
???????????  (Issues in Chinese 
Language Processing).  ???????? , 
pp.147-165. 
Vossen, P., Meijs, W. and den Broeder, M. (1989)  
Meaning and structure in dictionary definitions.  In 
B. Boguraev and T. Briscoe (Eds.), Computational 
Lexicography for Natural Language Processing.  
Essex, UK: Longman Group. 
Xia, F., Palmer, M., Xue, N., Okwrowski, M.E., 
Kovarik, J., Huang, S., Kroch, T. and Marcus, M. 
(2000)  Developing Guidelines and Ensuring Con-
sistency for Chinese Text Annotation.  In Proceed-
ings of the 2nd International Conference on Lan-
guage Resources and Evaluation (LREC-2000), 
Athens, Greece. 
 
Region Unique Items and Examples (Financial) Unique Items and Examples (Sports) 
HK 2105 (40.19%) 
?? ??? ?? ?? 
?? ?? ??? ?? 
?? ??  ?? ?? 
?? ??  ?? ??  
?? ??  ?? ?? 
2410 (38.16%) 
?? ?? ?? ?? 
?? ?? ?? ?? 
?? ?? ?? ??? 
?? ?? 12? ?? 
??? ?? ?? ?? 
BJ 933 (33.43%) 
?? ??  ? ?? 
???? ??  ?? ?? 
?? ??  ?? ?? 
?? ??  ?? ??  
??  ??  ??? ??  
907 (23.39%) 
??? ?? ??? ??? 
?? ?? ???? ??? 
??? ??? ?? ?? 
??? ??? ?? ?? 
??? ?? ??? ?? 
TW 891 (28.85%) 
?? ??  ??? ?? 
?? ??  ??? ??? 
??? ???  ?? ?? 
?? ??  ? ?? 
?? ??  ?? ??  
1302 (28.30%) 
?? ?? ?? ?? 
??? ?? ?? ?? 
?? ?? ?? ?? 
?? ?? ?? ?? 
??? ?? ?? ??? 
SG 890 (23.20%) 
?? ??  ?? ?? 
?? ??  ??? ?? 
?? ??  ?? ?? 
?? ???  ?? ??? 
???? ??  ?? ??   
1044 (20.39%) 
???? ???? ??? ????? 
?? ?? ??? 76?? 
?? ???? ???? ??? 
?? ?? ?? ??? 
??? ??? ?? ?? 
Table 6  Uniqueness of individual subcorpora 
 
Region Financial Sports 
 Found in Cilin Not in Cilin Found in Cilin Not in Cilin 
HK 560 (26.60%) 
??  ??  ??  ??  ?? 
??  ??  ??  ??  ?? 
1545 
(73.40%) 
884 (36.68%) 
??  ??  ??  ??  ?? 
??  ??  ?  ?  ?? 
1526 
(62.32%) 
BJ 369 (39.55%) 
??  ??  ??  ??  ? 
??  ??  ??  ??  ?? 
564 
(60.45%) 
355 (39.14%) 
???  ??  ???  ??  ???  
??  ??  ???  ??  ?? 
552 
(60.86%) 
TW 265 (29.74%) 
??  ??  ???  ??  ?? 
??  ??  ?  ??  ?? 
626 
(70.26%) 
 
354 (27.19%) 
??  ??  ??  ??  ?? 
??  ??  ??  ??  ?? 
948 
(72.81%) 
SG 333 (37.42%) 
??  ????  ??  ??  ??  
??  ??  ??  ??  ?? 
557 
(62.58%) 
281 (26.91%) 
??  ???  ???  ??  ?? 
???  ???  ???  ???  ??? 
763 
(73.08%) 
Table 7  Coverage of the Tongyici Cilin for the unique lexical items in individual subcorpora 
16
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 292?295,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
CityU-DAC: Disambiguating Sentiment-Ambiguous Adjectives within 
Context 
 
Bin LU and Benjamin K. TSOU 
Department of Chinese, Translation and Linguistics &  
Language Information Sciences Research Centre 
City University of Hong Kong 
{lubin2010, rlbtsou}@gmail.com 
  
 
Abstract 
This paper describes our system 
participating in task 18 of SemEval-2010, 
i.e. disambiguating Sentiment-
Ambiguous Adjectives (SAAs). To 
disambiguating SAAs, we compare the 
machine learning-based and lexicon-
based methods in our submissions: 1) 
Maximum entropy is used to train  
classifiers based on the annotated 
Chinese data from the NTCIR opinion 
analysis tasks, and the clause-level and 
sentence-level classifiers are compared; 
2) For the lexicon-based method, we first 
classify the adjectives into two classes: 
intensifiers (i.e. adjectives intensifying 
the intensity of context) and suppressors 
(i.e. adjectives decreasing the intensity of 
context), and then use the polarity of 
context to get the SAAs? contextual 
polarity based on a sentiment lexicon. 
The results show that the performance of 
maximum entropy is not quite high due 
to little training data; on the other hand, 
the lexicon-based method could improve 
the precision by considering the polarity 
of context. 
1 Introduction 
In recent years, sentiment analysis, which mines 
opinions from information sources such as news, 
blogs, and product reviews, has drawn much 
attention in the NLP field (Hatzivassiloglou and 
McKeown, 1997; Pang et al, 2002; Turney, 
2002; Hu and Liu, 2004; Pang and Lee, 2008). It 
has many applications such as social media 
monitoring, market research, and public 
relations.  
Some adjectives are neutral in sentiment 
polarity out of context, but they could show 
positive, neutral or negative meaning within 
specific context. Such words can be called 
dynamic sentiment-ambiguous adjectives 
(SAAs). However, SAAs have not been 
intentionally tackled in the researches of 
sentiment analysis, and usually have been 
discarded or ignored by most previous work. Wu 
et al, (2008) presents an approach of combining 
collocation information and SVM to 
disambiguate SAAs, in which the collocation-
based method was first used to disambiguate 
adjectives within the context of collocation (i.e. a 
sub-sentence marked by comma), and then the 
SVM algorithm was explored for those instances 
not covered by the collocation-based method. 
According to their experiments, their supervised 
algorithm achieves encouraging performance. 
The task 18 at SemEval-2010 is intended to 
create a benchmark dataset for disambiguating 
SAAs. Given only 100 trial sentences, but not 
provided with any official training data, 
participants are required to tackle this problem 
data by unsupervised approaches or use their 
own training data. The task consists of 14 SAAs, 
which are all high-frequency words in Mandarin 
Chinese. They are ?|big, ?|small, ?|many, ?
|few, ?|high, ?|low, ?|thick, ?|thin, ?|deep, 
?|shallow, ?|heavy, ?|light, ??|huge, ??
|grave. This task deals with Chinese SAAs, but 
the disambiguating techniques should be 
language-independent. Please refer to (Wu and 
Jin, 2010) for more descriptions of the task. 
In our participating system, the annotated 
Chinese data from the NTCIR opinion analysis 
tasks is used as training data with the help of a 
combined sentiment lexicon. A machine 
learning-based method (namely maximum 
entropy) and the lexicon-based method are 
compared in our submissions. The results show 
that the performance of maximum entropy is not 
quite high due to little training data; on the other 
hand, the lexicon-based method could improve 
292
the precision by considering the context of 
SAAs. In Section 2, we briefly describe data 
preparation of sentiment lexicon and training 
data. Our approaches for disambiguating SAAs 
are given in Section 3. The experiment and 
results are presented in Section 4, followed by a 
conclusion in Section 5. 
2 Data Preparation 
2.1 Sentiment Lexicon 
Several traditional Chinese resources of polar 
words/phrases are collected, including NTU 
Sentiment Dictionary1, The Lexicon of Chinese 
Positive Words (Shi and Zhu, 2006), The Lexicon 
of Chinese Negative Words (Yang and Zhu, 2006) 
0, and CityU?s sentiment-bearing word/phrase 
list (Lu et al 2008), which were manually 
marked in the political news data by trained 
annotators (Benjamin and Lu, 2008). Sentiment-
bearing items marked with the SENTIMENT_KW 
tag (SKPI), including only positive and negative 
items but not neutral ones, were also 
automatically extracted from the Chinese sample 
data of NTCIR-6 OAPT (Seki et al, 2007). All 
these polar item lexicons were combined, and the 
combined polar item lexicon consists of 13,437 
positive items and 18,365 negative items, a total 
of 31,802 items.  
2.2 Training Data 
The training data is extracted from the Chinese 
sample and test data from the NTCIR opinion 
analysis task, including NTCIR-6 (Seki et al, 
2007), NTCIR-7 (Seki et al, 2008) and NTCIR-8 
(Seki et al, 2010). The NTCIR opinion analysis 
tasks provide an opportunity to evaluate the 
techniques used by different participants based 
on a common evaluation framework in Chinese 
(simplified and traditional), Japanese and 
English.  
For data from NTCIR-6 and NTCIR-7, three 
annotators manually marked the polarity of each 
opinionated sentence, and the lenient polarity is 
used here as the gold standard (please refer to 
Seki et al, 2008 for explanation of lenient and 
strict standard). For each opinionated sentence 
from NTCIR-8, only two annotators marked and 
the strict polarity is used as the gold standard. 
The traditional Chinese sentences are transferred 
into simplified Chinese. In total, there are about 
12K opinionated sentences annotated with 
polarity, out of which about 9K are marked as 
                                                          
1 http://nlg18.csie.ntu.edu.tw:8080/opinion/index.html  
positive or negative, and others neutral. All the 
9K sentences plus the 100 sentences from the 
trial data are used as the sentence-level training 
data. 
Meanwhile, we also try to get the clause-level 
training data since the context of collocation 
within sub-sentences are quite crucial for 
disambiguating SAAs according to Wu et al 
(2008). From the 9K positive/ negative sentences 
above, we automatically extract the clause for 
each occurrence of SAAs.  
Note the polarity for a whole sentence is not 
necessarily the same with that of the clause 
containing SAAs. Consider the sentence ? ?? 
? ?? ? ?? ? ? ?? ?? ?? ?? 
(In the current large circumstance of the world, 
China and Russia support each other). The 
polarity of the whole sentence is positive, while 
the clause ??????????(In the current 
large circumstance of the world) containing a 
SAA ? (large) is neutral, and the polarity lies in 
the second part of the whole sentence, i.e. ?? 
?? (support each other). 
Thus, we manually checked the polarity of 
clauses containing SAAs. Due to time limitation, 
we only checked 465 clauses. Plus the clauses 
extracted from 100 trial sentences, the final 
clause-level training data consist of 565 
positive/negative clauses containing SAAs. 
3 Our Approach for Disambiguating 
SAAs 
To disambiguating SAAs, we use the maximum 
entropy algorithm and the sentiment lexicon-
based method, and also combine them together. 
3.1 The Maximum Entropy-based Method 
Maximum entropy classification (MaxEnt) is a 
technique which has proven effective in a 
number of natural language processing 
applications (Berger et al, 1996). Le Zhang?s 
maximum entropy tool2 is used for classification. 
The Chinese sentences are segmented into 
words using a production segmentation system. 
Unigrams of words are used as basic features for 
classification. Bigrams are also tried, but does 
not show improvement, and thus are not 
described in details here. 
3.2 The Lexicon-based Method 
For the lexicon-based method, we first classify 
the 14 adjectives into two classes: intensifiers 
                                                          
2 http:// homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html 
293
and suppressors. Intensifiers refer to adjectives 
intensifying the intensity of context, including ?
|big, ? |many, ? |high, ? |thick, ? |deep, ?
|heavy, ??|huge, ??|grave, while suppressors 
refer to adjectives decreasing the intensity of 
context, including ?|small, ?|few, ?|low, ?
|thin, ?|shallow, ?|light. 
Meanwhile, the collocation nouns are also 
classified into two classes: positive and negative. 
Positive nouns include ? ? |quality, ? ?
|standard, ? ? |level, ? ? |benefit, ? ?
|achievement, etc. Negative nouns include ??
|pressure, ?? |disparity, ?? |problem, ??
|risk, ??|pollution etc.  
The hypothesis here is that intensifiers will 
receive the polarity of their collocations while 
suppressors will get the opposite polarity of their 
collocations. For example, ?? |achievement 
could be collocated with one of the following 
intensifiers: ?|big, ?|many or ?|high, and the 
adjectives just receive the polarity of ??
|achievement, which is positive. Meanwhile, ?
?|pollution could be collocated with one of the 
following suppressors: ?|small, ?|few, ?|low, 
and the adjectives just receive the opposite 
polarity of??|pollution, which is also positive. 
 Based on this hypothesis, we could get the 
polarity of SAAs through theirs collocation 
nouns within the clauses containing SAAs. The 
context of SAAs is a sub-sentence marked by 
comma. The sentiment lexicon mentioned in 
Section 2.1 is used to find polarity of collocation 
nouns. 
3.3 Combining Maximum Entropy and 
Lexicon  
To combine the two methods above, the lexicon-
based method is first used to disambiguate the 
sentiment of SAAs, and the context of 
collocation is a sub-sentence marked by comma. 
Then for those instances that are not covered by 
the lexicon-based method, the maximum entropy 
algorithm is explored. 
4 Experiment and Results 
The dataset contains two parts: some sentences 
were extracted from Chinese Gigaword (LDC 
corpus: LDC2005T14), and other sentences were 
gathered through the search engine like Google. 
Firstly, these sentences were automatically 
segmented and POS-tagged, and then the 
ambiguous adjectives were manually annotated 
with the correct sentiment polarity within the 
sentence context. Two annotators annotated the 
sentences double blindly, and the third annotator 
checks the annotation. All the data of 2,917 
sentences is provided as the test set, and 
evaluation is performed in terms of micro 
accuracy and macro accuracy.  
We submitted 4 runs: run 1 is based on the 
sentence-level MaxEnt classifier; run 2 on the 
clause-level MaxEnt classifier; run 3 is got by 
combining the lexicon-based method and the 
sentence-level MaxEnt classifier; and run 4 by 
combining the lexicon-based method and the 
clause-level MaxEnt classifier. The official 
scores for the 4 runs are shown in Table 2. 
Table 2. Results of 4 Runs 
Run Micro Acc. (%) Macro Acc. (%)
1 61.98 67.89 
2 62.63 60.85 
3 71.55 75.54 
4 72.47 69.80 
From Table 2, we can observe that: 
1) Compared the highest scores achieved by 
other teams, the performance of maximum 
entropy (run 1 and 2) is not quite high due to 
little training data;  
2) By integrating the lexicon-based method 
and maximum entropy (run 3 and 4), we improve 
the accuracy by considering the context of SAAs;  
3) The sentence-level maximum entropy 
classifier shows better macro accuracy, and 
clause-level one better micro accuracy. 
In addition to the official scores, we also 
evaluate the performance of the lexicon-based 
method alone. The micro and macro accuracy are 
respectively 0.847 and 0.835665, showing that 
the lexicon-based method is more accurate than 
the maximum entropy algorithm (run 1 and 2). 
But it only covers 1,436 (49%) of 2,917 test 
instances.  
Because the data from the NTCIR opinion 
analysis task is not specifically annotated for this 
task, and the manually checked clauses are less 
than 600, the performance of our system is not 
quite high compared to the highest performance 
achieved by other teams. 
5 Conclusion 
To disambiguating SAAs, we compare machine 
learning-based and lexicon-based methods in our 
submissions: 1) Maximum entropy is used to 
train classifiers based on the annotated Chinese 
data from the NTCIR opinion analysis tasks, and  
the clause-level and sentence-level classifiers are 
294
compared; 2) For the lexicon-based method, we 
first classify the adjectives into two classes: 
intensifiers (i.e. adjectives intensifying the 
intensity of context) and suppressors (i.e. 
adjectives decreasing the intensity of context), 
and then use the polarity of context to get the 
SAAs? contextual polarity. The results show that 
the performance of maximum entropy is not 
quite high due to little training data; on the other 
hand, the lexicon-based method could improve 
the precision by considering the context of 
SAAs. 
 
References 
Adam L. Berger, Stephen A. Della Pietra, and Vincent 
J. Della Pietra. 1996. A maximum entropy 
approach to natural language processing. 
Computational Linguistics, 22(1):39-71. 
Vasileios Hatzivassiloglou and Kathleen McKeown. 
1997. Predicting the Semantic Orientation of 
Adjectives. Proceedings of ACL-97. 174-181. 
Minqing Hu and Bing Liu. 2004. Mining Opinion 
Features in Customer Reviews. In Proceedings of 
the 19th National Conference on Artificial 
Intelligence, pp. 755-760.  
Bin Lu, Benjamin K. Tsou and Oi Yee Kwong. 2008. 
Supervised Approaches and Ensemble Techniques 
for Chinese Opinion Analysis at NTCIR-7. In 
Proceedings of the Seventh NTCIR Workshop 
(NTCIR-7). pp. 218-225. Tokyo, Japan. 
Bo Pang and Lillian Lee. 2008. Opinion mining and 
sentiment analysis, Foundations and Trends in 
Information Retrieval, Now Publishers. 
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 
2002. Thumbs up? Sentiment classification using 
machine learning techniques. In Proceedings of 
EMNLP 2002, pp.79?86. 
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun, 
Hsin-His Chen, Noriko Kando. 2007. Overview of 
Opinion Analysis Pilot Task at NTCIR-6. Proc. of 
the Seventh NTCIR Workshop. Japan. 2007.6. 
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun, 
Hsin-His Chen, Noriko Kando and Chin-Yew Lin. 
2008. Overview of Multilingual Opinion Analysis 
Task at NTCIR-7. Proc. of the Seventh NTCIR 
Workshop. Japan. Dec. 2008. 
Yohei Seki, Lun-Wei Ku, Le Sun, Hsin-His Chen, 
Noriko Kando. 2010. Overview of Multilingual 
Opinion Analysis Task at NTCIR-8. Proc. of the 
Seventh NTCIR Workshop. Japan. June, 2010. 
Jilin Shi and Yinggui Zhu. 2006. The Lexicon of 
Chinese Positive Words (?????). Sichuan 
Lexicon Press. 
Benjamin K. Tsou and Bin Lu. 2008. A Political 
News Corpus in Chinese for Opinion Analysis. In 
Proceedings of the Second International Workshop 
on Evaluating Information Access (EVIA2008). pp. 
6-7. Tokyo, Japan. 
Peter D. Turney. 2002. Thumbs up or thumbs down? 
Semantic orientation applied to unsupervised 
classification of reviews, In Proceedings of ACL-
02, Philadelphia, Pennsylvania, 417-424. 
Yunfang Wu, Miao Wang, Peng Jin and Shiwen Yu. 
2008. Disambiguate sentiment ambiguous 
adjectives. In Proceedings of  IEEE International 
Conference on Natural Language Processing and 
Knowledge Engineering (NLP-KE?08). 
Yunfang Wu, and Peng Jin. 2010. SemEval-2010 
Task 18: Disambiguate sentiment ambiguous 
adjectives. In Proceedings of SemEval-2010. 
Ruifeng Xu, Kam-Fai Wong and Yunqing Xia. 2008. 
Coarse-Fine Opinion Mining - WIA in NTCIR-7 
MOAT Task. In Proceedings of the Seventh NTCIR 
Workshop (NTCIR-7). Tokyo, Japan, Dec. 16-19. 
Ling Yang and Yinggui Zhu. 2006. The Lexicon of 
Chinese Negative Words (?????). Sichuan 
Lexicon Press. 
 
 
295
Mining Large-scale Parallel Corpora from Multilingual Patents: 
An English-Chinese example and its application to SMT 
Bin Lu?, Benjamin K. Tsou??, Tao Jiang?, Oi Yee Kwong?, and Jingbo Zhu? 
?Department of Chinese, Translation & Linguistics, City University of Hong Kong 
?Research Centre on Linguistics and Language Information Sciences,  
Hong Kong Institute of Education 
?ChiLin Star Corp., Southern Software Park, Zhuhai, China 
?Natural Language Processing Lab, Northeastern University, Shenyang, China 
{lubin2010, rlbtsou, jiangtaoster}@gmail.com, 
rlolivia@cityu.edu.hk, zhujingbo@mail.neu.edu.cn 
 
Abstract 
In this paper, we demonstrate how to 
mine large-scale parallel corpora with 
multilingual patents, which have not 
been thoroughly explored before. We 
show how a large-scale English-Chinese 
parallel corpus containing over 14 
million sentence pairs with only 1-5% 
wrong can be mined from a large amount 
of English-Chinese bilingual patents. To 
our knowledge, this is the largest single 
parallel corpus in terms of sentence pairs. 
Moreover, we estimate the potential for 
mining multilingual parallel corpora 
involving English, Chinese, Japanese, 
Korean, German, etc., which would to 
some extent reduce the parallel data 
acquisition bottleneck in multilingual 
information processing. 
1 Introduction 
Multilingual data are critical resources for 
building many applications, such as machine 
translation (MT) and cross-lingual information 
retrieval. Many parallel corpora have been built, 
such as the Canadian Hansards (Gale and 
Church, 1991), the Europarl corpus (Koehn, 
2005), the Arabic-English and English-Chinese 
parallel corpora used in the NIST Open MT 
Evaluation.  
However, few parallel corpora exist for many 
language pairs, such as Chinese-Japanese, 
Japanese-Korean, Chinese- French or 
Japanese-German. Even for language pairs with 
several parallel corpora, such as Chinese-English 
and Arabic-English, the size of parallel corpora 
is still a major limitation for SMT systems to 
achieve higher performance. 
In this paper, we present a way which could, to 
some extent, reduce the parallel data acquisition 
bottleneck in multilingual language processing.  
Based on multilingual patents, we show how an 
enlarged English-Chinese parallel corpus 
containing over 14 million high-quality sentence 
pairs can be mined from a large number of 
comparable patents harvested from the Web. To 
our knowledge, this is the largest single parallel 
corpus in terms of parallel sentences. Some SMT 
experiments are also reported. Moreover, we 
investigate the potential to get large-scale 
parallel corpora for languages beyond the 
Canadian Hansards, Europarl and UN news used 
in NIST MT Evaluation by estimating the 
quantity of multilingual patents involving 
English, Chinese, Japanese, Korean, German, 
etc.  
Related work is introduced in Section 2. 
Patents, PCT patents, multilingual patents are 
described in Section 3. Then an English-Chinese 
parallel corpus, its mining process and 
application to SMT are introduced in Section 4, 
followed by the quantity estimation of 
multilingual patents involving other language 
pairs in Section 5. We discuss the results in 
Section 6, and conclude in Section 7. 
2 Related Work 
Parallel sentences could be extracted from 
parallel documents or comparable corpora. 
Different approaches have been proposed to 
align sentences in parallel documents consisting 
of the same content in different languages based 
on the following information: a) the sentence 
length in bilingual sentences (Brown et al 1991; 
Gale and Church, 1991); b) lexical information 
in bilingual dictionaries (Ma, 2006); c) statistical 
translation model (Chen, 1993), or the composite 
of more than one approach (Simard and 
Plamondon, 1998; Moore, 2002).  
To overcome the lack of parallel documents, 
comparable corpora are also used to mine 
parallel sentences, which raises further 
challenges since the bilingual contents are not 
strictly parallel. For instance, Zhao and Vogel 
(2002) investigated the mining of parallel 
sentences for Web bilingual news. Munteanu and 
Marcu (2005) presented a method for 
discovering parallel sentences in large Chinese, 
Arabic, and English comparable, non-parallel 
corpora based on a maximum entropy classifier. 
Cao et al, (2007) and Lin et al, (2008) proposed 
two different methods utilizing the parenthesis 
pattern to extract term translations from bilingual 
web pages. Jiang et al (2009) presented an 
adaptive pattern-based method which produced 
Chinese-English bilingual sentences and terms  
with over 80% accuracy. 
Only a few papers were found on the related 
work in the patent domain. Higuchi et al (2001) 
used the titles and abstracts of 32,000 
Japanese-English bilingual patents to extract 
bilingual terms. Utiyama and Isahara (2007) 
mined about 2 million parallel sentences by 
using two parts in the description section of 
Japanese-English comparable patents. Lu et al 
(2009) derived about 160K parallel sentences 
from Chinese-English comparable patents by 
aligning sentences and filtering alignments with 
the combination of different quality measures. 
Another closely related work is the 
English-Chinese parallel corpus (Lu et al, 
2010), which is largely extended by this work, in 
which both the number of patents and that of 
parallel sentences are augmented by about 
100%, and more SMT experiments are given. 
Moreover, we show the potential for mining 
parallel corpora from multilingual patents 
involving other languages. 
For statistical machine translation (SMT), 
tremendous strides have been made in two 
decades, including Brown et al (1993), Och and 
Ney (2004) and Chiang (2007). For the MT 
evaluation, NIST (Fujii et al, 2008; 2010) has 
been organizing open evaluations for years, and 
the performance of the participants has been 
improved rapidly.  
3 Patents and Multilingual Patents 
A patent is a legal document representing ?an 
official document granting the exclusive right to 
make, use, and sell an invention for a limited 
period? (Collins English Dictionary1). A patent 
application consists of different sections, and we 
focus on the text, i.e. only title, abstract, claims 
and description.  
3.1 PCT Patents 
Since the invention in a patent is only protected 
in the filing countries, a patent applicant who 
wishes to protect his invention outside the 
original country should file patents in other 
countries, which may involve other languages. 
The Patent Cooperation Treaty (PCT) system 
offers inventors and industry an advantageous 
route for obtaining patent protection 
internationally. By filing one ?international? 
patent application under the PCT via the World 
Intellectually Property Organization (WIPO), 
protection of an invention can be sought 
simultaneously (i.e. the priority date) in each of a 
large number of countries. 
The number of PCT international applications 
                                                          
1 Retrieved March 2010, from 
http://www.collinslanguage.com/ 
filed is more than 1.7 million 2 . A PCT 
international application may be filed in any 
language accepted by the relevant receiving 
office, but must be published in one of the 
official publication languages (Arabic, Chinese, 
English, French, German, Japanese, Korean, 
Russian and Spanish). Other highly used 
languages for filing include Italian, Dutch, 
Finnish, Swedish, etc. Table 1 3  shows the 
number of PCT applications for the most used 
languages of filing and publication.  
 Lang. of Filing 
Share 
(%) 
Lang. of 
Publication 
Share 
(%) 
English 895K 52.1 943K 54.9 
Japanese 198K 11.5 196K 11.4 
German 185K 10.8 184K 10.7 
French 55K 3.2 55K 3.2 
Korean 24K 1.4 3K4 0.2 
Chinese 24K 1.4 24K 1.4 
Other 336K 19.6 313K 18.2 
Total 1.72M 100 1.72M 100 
Table 1. PCT Application Numbers for Languages of 
Publication and Filing 
From Table 1, we can observe that English, 
Japanese and German are the top 3 languages in 
terms of PCT applications, and English accounts 
for over 50% of applications in terms of 
language of both publication and filing.  
3.2 Multilingual Patents 
A PCT application does not necessarily mean a 
multilingual patent. An applicant who has 
decided to proceed further with his PCT 
international application must fulfill the 
requirements for entry into the PCT national 
phase at the patent offices of countries where he 
seeks protection. For example, a Chinese 
company may first file a Chinese patent in China 
                                                          
2 Retrieved Apr., 2010 from 
http://www.wipo.int/pctdb/en/. The data below involving 
PCT patents comes from the website of WIPO. 
3 The data in this and other tables in the following sections 
involving PCT patents comes from the website of WIPO. 
4  Korean just became one of the official publication 
languages for the PCT system since 2009, and thus the 
number of PCT patents with Korean as language of 
publication is small. 
patent office and then file its international 
application also in Chinese under the PCT. Later 
on, it may have the patent translated into English 
and file it in USA patent office, which means the 
patent becomes bilingual. If the applicant 
continues to file it in Japan with Japanese, it 
would be trilingual. Even more, it would be 
quadrilingual or involve more languages when it 
is filed in other countries with more languages. 
Such multilingual patents are considered 
comparable (or noisy parallel) because they are 
not parallel in the strict sense but still closely 
related in terms of information conveyed 
(Higuchi et al, 2001; Lu et al, 2009). 
4 A Large English-Chinese Parallel 
Corpus Mined from Bilingual Patents 
In this section, we introduce the English-Chinese 
bilingual patents harvested from the Web and the 
method to mine parallel sentences from them. 
SMT experiments on the final parallel corpus are 
also described. 
4.1 Harvesting English-Chinese Bilingual 
Patents 
The official patent office in China is the State 
Intellectual Property Office (SIPO). In early 
2009,  by searching on its website, we found 
about 200K Chinese patents previously filed as 
PCT applications in English and crawled their 
bibliographical data, titles, abstracts and the 
major claim from the Web, and then other claims 
and descriptions were also added. Since some 
contents are in the image format, the images 
were OCRed and the texts recognized were 
manually verified. 
All PCT patent applications are filed through 
WIPO. With the Chinese patents mentioned 
above, the corresponding English patents were 
searched from the website of WIPO by the PCT 
publication numbers to obtain relevant sections 
of the English PCT applications, including 
bibliographical data, title, abstract, claims and 
description. About 80% (160K) out of the 
Chinese patents found their corresponding 
English ones. Some contents of the English 
patents were OCRed by WIPO. 
We automatically split the patents into 
individual sections according to the respective 
tags inside the patents, and segmented each 
section sentences according to punctuations. The 
statistics of each section for Chinese and 
English patents are shown in Table 2. 
Chinese English 
Sections 
#Char #Sent #Word #Sent 
Title 2.7M 157K 1.6M 157K 
Abstract 33M 596K 20M 784K 
Claim 367M 6.8M 217M 7.4M 
Desc. 2,467M 48.8M 1,353M 54.0M 
Total 2,870M 56.2M 1,591M 62.3M 
Table 2. Statistics of Comparable Patents 
4.2 Mining Parallel Sentences from 
Bilingual Patents 
The sentences in each section of Chinese patents 
were aligned with those in the corresponding 
section of the corresponding English patents to 
find parallel sentences after the Chinese 
sentences were segmented into words. 
Since the comparable patents are not strictly 
parallel, the individual alignment methods 
mentioned in Section 2 would be not effective: 1) 
the length-based method is not accurate since it 
does not consider content similarity; 2) the 
bilingual dictionary-based method cannot deal 
with new technical terms in the patents; 3) the 
translation model-based method would need 
training data to get a translation model. Thus, in 
this study we combine these three methods to 
mine high-quality parallel sentences from 
comparable patents. 
We first use a bilingual dictionary to 
preliminarily align the sentences in each section 
of the comparable patents. The dictionary-based 
similarity score dP  of a sentence pair is 
computed based on a bilingual dictionary as 
follows (Utiyama and Isahara, 2003):  
2/??
)deg()deg(
),(
),(
ce
Sw Sw ec
ec
ecd ll
ww
ww
SSp cc ee
+
=
? ?
? ?
?
 
where cw  and ew  are respectively the 
word types in Chinese sentence cS  and 
English sentence eS ; cl  and el  respectively 
denote the lengths of cS  and eS  in terms of 
the number of words; and ),( ec ww?  = 1 if 
cw  and ew  is a translation pair in the 
bilingual dictionary or are the same string, 
otherwise 0; and 
?
?
=
ee Sw
ecc www ),()deg( ?
?
?
=
ce Sw
ece www ),()deg( ? . 
For the bilingual dictionary, we combine three 
ones: namely, LDC_CE_DIC2.0 5  constructed 
by LDC, bilingual terms in HowNet and the 
bilingual lexicon in Champollion (Ma, 2006). 
We then remove sentence pairs using length 
filtering and ratio filtering: 1) for length filtering, 
if a sentence pair has more than 100 words in the 
English sentence or more than 333 characters in 
the Chinese one, it is removed; 2) for length ratio 
filtering, we discard the sentence pairs with 
Chinese-English length ratio outside the range of 
0.8 to 1.8. The parameters here are set 
empirically. 
We further filter the parallel sentence 
candidates by learning an IBM Model-1 on the 
remaining aligned sentences and compute the 
translation similarity score tP  of sentence 
pairs by combining the translation probability 
value of both directions (i.e. Chinese->English 
and English->Chinese) based on the trained 
IBM-1 model (Moore, 2002; Chen, 2003; Lu et 
al, 2009). It is computed as follows: 
ec
ecce
ect ll
)S(SPlog)S(SPlog
SSp
+
+
=
)|()|(
),(
 
where )SS(P ce | denotes the probability 
that a translator will produce eS  in English 
when presented with cS  in Chinese, and vice 
versa for )|(S ec SP . Sentence pairs with 
                                                          
5 http://projects.ldc.upenn.edu/Chinese/LDC_ch.htm 
similarity score tP  lower than a predefined 
threshold are filtered out as wrong aligned 
sentences. 
Table 3 shows the sentence numbers and the 
percentages of sentences kept in each step above 
with respect to all sentence pairs. In the first row 
of Table 3, 1.DICT denotes the first step of using 
the bilingual dictionary to align sentences; 2. FL 
denotes the length and ratio filtering; 3. TM 
refers to the third and final step of using 
translation models to filter sentence pairs. 
 1. DICT 2.FL 3. TM (final) 
Abstr. 503K 352K  (70%) 
166K  
(33%) 
Claims 6.0M 4.3M (72.1%) 
2.0M 
(33.4%) 
Desc. 38.6M 26.8M (69.4%) 
12.1M 
(31.3%) 
Total6 45.1M 31.5M (69.8%) 
14.3M 
(31.7%) 
Table 3. Numbers of Sentence Pairs 
Both the 31.5M parallel sentences after the 
second step FL and the final 14.3M after the third 
step TM are manually evaluated by randomly 
sampling 100 sentence pairs for each section. 
The evaluation metric follows the one in Lu et al 
(2009), which classifies each sentence pair into 
Correct, Partially Correct or Wrong. The results 
of manual evaluation are shown in Table 4. 
 Section Correct Partially Correct Wrong 
Abstr. 85% 7% 8% 
Claims 83% 10% 7% 2. FL 
Desc. 69% 15% 15% 
Abstr. 97% 2% 1% 
Claims 92% 3% 5% 3. TM  (final) 
Desc. 89% 8% 3% 
Table 4. Manual Evaluation of the Corpus 
From Table 4, we can see that: 1) In the final 
corpus, the percentages of correct parallel 
sentences are quite high, and the wrong 
percentages are no higher than 5%; 2) Without 
                                                          
6 Here the total number does not include the number of 
titles, which are directly treated as parallel. 
the final step of TM, the accuracies of 31.5M 
sentence pairs are between 69%-85%, and the 
percentages of wrong pairs are between 
7%-15%; 3) The abstract section shows the 
highest correct percentage, while the description 
section shows the lowest. 
Thus, we could conclude that the mined 14M 
parallel sentences are of high quality with only 
1%-5% wrong pairs, and our combination of 
bilingual dictionaries and translation models for 
mining parallel sentences are quite effective. 
4.3 Chinese-English Statistical Machine 
Translation 
A Chinese-English SMT system is setup using 
Moses (Koehn, 2007). We train models  based 
on different numbers of parallel sentences mined 
above. The test set contains 548 sentence pairs 
which are randomly selected and different from 
the training data. The sizes of the training data 
and BLEU scores for the models are shown in 
Table 5. 
System BLEU (%) #Sentence Pairs for training 
Model-A 17.94 300K 
Model-B 19.96 750K 
Model-C 20.09 1.5M 
Model-D 20.98 3M 
Model-E 22.60 6M 
Table 5. SMT Experimental Results 
From Table 5, we can see that the BLEU 
scores are improving steadily when the training 
data increases. When the training data is 
enlarged by 20 times from 300K to 6M, the 
BLEU score increases to 22.60 from 17.94, 
which is quite a significant improvement. We 
show the translations of one Chinese sample 
sentence in Table 6 below. 
CN  
Sent. 
?? ?? ?? ??? ?? ? ? ?? ? 
? ? 
Ref. 
the main shaft of the electric motor 
extends into the working cavity of the 
compressor shell , 
Model-A the motor main shaft into the compressor the chamber 
Model-B motor shaft into the compressor housing . the working chamber 
Model-C motor shaft into the compressor housing . the working chamber 
Model-D 
motor spindle extends into the 
compressor housing . the working 
chamber 
Model-E motor spindle extends into the working chamber of the compressor housing , 
Table 6. Translations of One Chinese Sentence 
From Table 6, we can see the translations 
given by Model-A to Model-C are lack of the 
main verb, the one given by Model-D has an 
ordering problem for the head noun and the 
modifier, and the one given by Model-E seems 
better than the others and its content is already 
quite similar to the reference despite the lexical 
difference. 
5 Multilingual Corpora for More 
Languages 
In this section, we describe the potential of 
building large-scale parallel corpora for more 
languages, especially Asian languages by using 
the 1.7 million PCT patent applications and their 
national correspondents. By using PCT 
applications as the pivot, we can build 
multilingual parallel corpora from multilingual 
patents, which would greatly enlarge parallel 
data we could obtain. 
The patent applications filed in one country 
should be in the official language(s) of the 
country, e.g. the applications filed in China 
should be in Chinese, those in Japan be in 
Japanese, and so on. In Table 7, the second 
column shows the total numbers of patent 
applications in different countries which were 
previously filed as PCT ones; and the third 
column shows the total numbers of applications 
in different countries, which were previously 
filed as PCT ones with English as language of 
publication. 
National Phase 
Country7 ALL 
English as Lang. 
of Publication 
                                                          
7 For the national phase of the PCT System, the statistics 
are based on data supplied to WIPO by national and 
Japan 424K 269K 
China 307K 188K 
Germany 32K 10K 
R. Korea 236K 134K 
China & Japan 189K 130K 
China & R. Korea 154K 91K 
Japan & R. Korea 158K 103K 
China & Japan  
& R. Korea 106K 73K 
Table 7. Estimated Numbers of Multilingual 
Patents 
The number of the Chinese-English bilingual 
patents (CE) in Table 7 is about 188K, which is 
consistent with the number of 160K found in 
Section 4.1 since the latter contains only the 
applications up to early 2009. Based on Table 7, 
we estimate below the rough sizes of bilingual 
corpora, trilingual corpora, and even 
quadrilingual corpora for different languages. 
1) Bilingual Corpora with English as one 
language 
Compared to CE (188K), the 
Japanese-English bilingual corpus (269K) could 
be 50% larger in terms of bilingual patents, the 
Korean-English one (134K) could be about 30% 
smaller, and the German-English one (10K) 
would be much smaller. 
2) Bilingual Corpora for Asian Languages  
The Japanese-Chinese bilingual corpus 
(189K) could be comparable to CE (188K) in 
terms of bilingual applications, the Chinese- 
Korean one (154K) could be about 20% smaller, 
and the Japanese-Korean one (158K) is quite 
similar to the Chinese-Korean one. 
3)  Trilingual Corpora 
In addition to bilingual corpora, we can also 
build trilingual corpora from trilingual patents. It 
is quite interesting to note that the trilingual 
corpora  could be quite large even compared to 
the bilingual corpora.  
The trilingual corpora for Chinese, Japanese 
and English (130K) could be only 30% smaller 
than CE in terms of patents. The trilingual corpus 
                                                                                      
regional patent Offices, received at WIPO often 6 months or 
more after the end of the year concerned, i.e. the numbers 
are not up-to-date . 
for Chinese, Korean and English (91K) and that 
for Japanese, Korean and English (103K) are 
also quite large. The number of the trilingual 
patents for the Asian languages of Chinese, 
Japanese and Korean (106K) is about 54% of 
that of CE. 
4) Quadrilingual Corpora 
The number of the quadrilingual patents for 
Chinese, Japanese, Korean and English (73K) is 
about 38% of that of CE. From these figures, we 
could say that a large proportion of the PCT 
applications published in English later have been 
filed in all the three Asian countries: China, 
Japan, and R. Korea. 
6 Discussion 
The websites from which the Chinese and 
English patents were downloaded were quite 
slow to access, and were occasionally down 
during access. To avoid too much workload for 
the websites, the downloading speed had been 
limited. Some large patents would cost much 
time for the websites to respond and had be 
specifically handled. It took considerable efforts 
to obtain these comparable patents.  
In addition our English-Chinese corpus mined 
in this study is at least one order of magnitude 
larger, we give some other differences between 
ours and those introduced in Section 2 (Higuchi 
et al, 2001; Utiyama and Isahara, 2007; Lu et al 
2009)  
1) Their bilingual patents were identified by 
the priority information in the US patents, and 
could not be easily extended to language pairs 
without English; while our method using PCT 
applications as the pivot could be easily 
extended to other language pairs as illustrated in 
Section 5. 
2) The translation process is different: their 
patents were filed in USA Patent Office in 
English by translating from Japanese or Chinese, 
while our patents were first filed in English as a 
PCT application, and later translated into 
Chinese. The different translation processes may 
have different characteristics. 
Since the PCT and multilingual patent 
applications increase rapidly in recent years as 
discussed in Section 3, we could expect more 
multilingual patents to enlarge the large-scale 
parallel corpora with the new applications and 
keep them up-to-date with new technical terms. 
On the other hand, patents are usually translated 
by patent agents or professionals, we could 
expect high quality translations from 
multilingual patents. We have been planning to 
build trilingual and quadrilingual corpora from 
multilingual patents. 
One possible limitation of patent corpora is 
that the sentences are all from technical domains 
and written in formal style, and thus it is 
interesting to know if the parallel sentences 
could improve the performance of SMT systems  
on NIST MT evaluation corpus containing news 
sentences and web sentences.  
7 Conclusion 
In this paper, we show how a large high-quality 
English-Chinese parallel corpus can be mined 
from a large amount of comparable patents 
harvested from the Web, which is the largest 
single parallel corpus in terms of the  number of 
parallel sentences. Some sampled parallel 
sentences are available at 
http://www.livac.org/smt/parpat.html, and more 
parallel sentences would be publicly available to 
the research community. 
With 1.7 million PCT patent applications and 
their corresponding national ones, there are 
considerable potentials of constructing 
large-scale high-quality parallel corpora for 
languages. We give an estimation on the sizes of 
multilingual parallel corpora which could be 
obtained from multilingual patents involving 
English, Chinese, Japanese, Korean, German, 
etc., which would to some extent reduce the 
parallel data acquisition bottleneck in 
multilingual information processing. 
Acknowledgements 
We wish to thank Mr. Long Jiang from 
Microsoft Research Asia and anonymous 
reviewers for their valuable comments. 
References 
Brown, Peter F., Jennifer C. Lai, and Robert L. 
Mercer. 1991. Aligning sentences in parallel 
corpora. In Proceedings of ACL. pp.169-176. 
Brown, Peter F., Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1993. 
Mathematics of statistical machine translation: 
Parameter estimation. Computational Linguistics, 
19(2), 263-311. 
Cao, Guihong, Jianfeng Gao and Jianyun Nie. 2007. 
A System to Mine Large-scale Bilingual 
Dictionaries from  Monolingual Web Pages. In 
Proceedings of MT Summit. pp. 57-64. 
Chen, Stanley F. 1993. Aligning sentences in 
bilingual corpora using lexical information. In 
Proceedings of ACL. pp. 9-16. 
Chiang, David. 2007. Hierarchical phrase-based 
translation. Computational Linguistics, 33(2), 
201?228. 
Fujii, Atsushi, Masao Utiyama, Mikio Yamamoto, 
and Takehito Utsuro. 2008. Overview of the patent 
translation task at the NTCIR-7 workshop. In 
Proceedings of the NTCIR-7 Workshop. pp. 
389-400. Tokyo, Japan. 
Fujii, Atsushi, Masao Utiyama, Mikio Yamamoto, 
Takehito Utsuro, Terumasa Ehara, Hiroshi 
Echizen-ya and Sayori Shimohata. 2010. 
Overview of the patent translation task at the 
NTCIR-8 workshop. In Proceedings of the 
NTCIR-8 Workshop. Tokyo, Japan. 
Gale, William A., and Kenneth W. Church. 1991. A 
program for aligning sentences in bilingual 
corpora. In Proceedings of ACL. pp.79-85. 
Higuchi, Shigeto, Masatoshi Fukui, Atsushi Fujii, and 
Tetsuya Ishikawa. PRIME: A System for 
Multi-lingual Patent Retrieval. In Proceedings of 
MT Summit VIII, pp.163-167, 2001. 
Koehn, Philipp. 2005. Europarl: A parallel corpus for 
statistical machine translation. In Proceedings of 
MT Summit X. 
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, et al 2007. Moses: Open source 
toolkit for statistical machine translation. In 
Proceedings of ACL Demo Session. pp. 177-180. 
Lin, Dekang, Shaojun Zhao, Benjamin V. Durme and 
Marius Pasca. 2008. Mining Parenthetical 
Translations from the Web by Word Alignment. In 
Proceedings of ACL-08. pp. 994-1002. 
Jiang, Long, Shiquan Yang, Ming Zhou, Xiaohua Liu, 
and Qingsheng Zhu. 2009. Mining Bilingual Data 
from the Web with Adaptively Learnt Patterns. In 
Proceedings of ACL-IJCNLP. pp. 870-878. 
Lu, Bin, Benjamin K. Tsou, Jingbo Zhu, Tao Jiang, 
and Olivia Y. Kwong. 2009. The Construction of 
an English-Chinese Patent Parallel Corpus. MT 
Summit XII 3rd Workshop on Patent Translation. 
Lu, Bin, Tao Jiang, Kapo Chow and Benjamin K. 
Tsou. 2010. Building a Large English-Chinese 
Parallel Corpus from Comparable Patents and its 
Experimental Application to SMT. LREC 
Workshop on Building and Using Comparable 
Corpora. Malta. May, 2010. 
Ma, Xiaoyi. 2006. Champollion: A Robust Parallel 
Text Sentence Aligner. In Proceedings of the 5th 
International Conference on Language Resources 
and Evaluation (LREC). Genova, Italy. 
Moore, Robert C. 2002. Fast and Accurate Sentence 
Alignment of Bilingual Corpora. In Proceedings of 
AMTA. pp.135-144. 
Munteanu, Dragos S., and Daniel Marcu. 2005. 
Improving Machine Translation Performance by 
Exploiting Non-parallel Corpora. Computational 
Linguistics, 31(4), 477?504. 
Och, Franz J., and Hermann Ney. 2004. The 
Alignment Template Approach to Machine 
Translation. Computational Linguistics, 30(4), 
417-449. 
Simard, Michel, and Pierre Plamondon. 1998. 
Bilingual Sentence Alignment: Balancing 
Robustness and Accuracy. Machine Translation, 
13(1), 59-80. 
Utiyama, Masao, and Hitoshi Isahara. 2007. A 
Japanese-English patent parallel corpus. In 
Proceeding of MT Summit XI. pp. 475?482. 
Zhao, Bing, and Stephen Vogel. 2002. Adaptive 
Parallel Sentences Mining from Web Bilingual 
News Collection. In Proceedings of Second IEEE 
International Conference on Data Mining 
(ICDM?02). 
