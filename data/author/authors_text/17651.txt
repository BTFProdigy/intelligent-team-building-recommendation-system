Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 770?779,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Graph-based Semi-Supervised Model for Joint Chinese Word
Segmentation and Part-of-Speech Tagging
Xiaodong Zeng? Derek F. Wong? Lidia S. Chao? Isabel Trancoso?
?Department of Computer and Information Science, University of Macau
?INESC-ID / Instituto Superior Te?cnico, Lisboa, Portugal
nlp2ct.samuel@gmail.com, {derekfw, lidiasc}@umac.mo,
isabel.trancoso@inesc-id.pt
Abstract
This paper introduces a graph-based semi-
supervised joint model of Chinese word
segmentation and part-of-speech tagging.
The proposed approach is based on a
graph-based label propagation technique.
One constructs a nearest-neighbor simi-
larity graph over all trigrams of labeled
and unlabeled data for propagating syn-
tactic information, i.e., label distribution-
s. The derived label distributions are re-
garded as virtual evidences to regular-
ize the learning of linear conditional ran-
dom fields (CRFs) on unlabeled data. An
inductive character-based joint model is
obtained eventually. Empirical results on
Chinese tree bank (CTB-7) and Microsoft
Research corpora (MSR) reveal that the
proposed model can yield better result-
s than the supervised baselines and other
competitive semi-supervised CRFs in this
task.
1 Introduction
Word segmentation and part-of-speech (POS) tag-
ging are two critical and necessary initial proce-
dures with respect to the majority of high-level
Chinese language processing tasks such as syn-
tax parsing, information extraction and machine
translation. The traditional way of segmentation
and tagging is performed in a pipeline approach,
first segmenting a sentence into words, and then
assigning each word a POS tag. The pipeline ap-
proach is very simple to implement, but frequently
causes error propagation, given that wrong seg-
mentations in the earlier stage harm the subse-
quent POS tagging (Ng and Low, 2004). The join-
t approaches of word segmentation and POS tag-
ging (joint S&T) are proposed to resolve these t-
wo tasks simultaneously. They effectively allevi-
ate the error propagation, because segmentation
and tagging have strong interaction, given that
most segmentation ambiguities cannot be resolved
without considering the surrounding grammatical
constructions encoded in a POS sequence (Qian
and Liu, 2012).
In the past years, several proposed supervised
joint models (Ng and Low, 2004; Zhang and
Clark, 2008; Jiang et al, 2009; Zhang and Clark,
2010) achieved reasonably accurate results, but the
outstanding problem among these models is that
they rely heavily on a large amount of labeled data,
i.e., segmented texts with POS tags. However, the
production of such labeled data is extremely time-
consuming and expensive (Jiao et al, 2006; Jiang
et al, 2009). Therefore, semi-supervised join-
t S&T appears to be a natural solution for easily in-
corporating accessible unlabeled data to improve
the joint S&T model. This study focuses on using
a graph-based label propagation method to build
a semi-supervised joint S&T model. Graph-based
label propagation methods have recently shown
they can outperform the state-of-the-art in sever-
al natural language processing (NLP) tasks, e.g.,
POS tagging (Subramanya et al, 2010), knowl-
edge acquisition (Talukdar et al, 2008), shallow
semantic parsing for unknown predicate (Das and
Smith, 2011). As far as we know, however, these
methods have not yet been applied to resolve
the problem of joint Chinese word segmentation
(CWS) and POS tagging.
Motivated by the works in (Subramanya et al,
2010; Das and Smith, 2011), for structured prob-
lems, graph-based label propagation can be em-
ployed to infer valuable syntactic information (n-
gram-level label distributions) from labeled data
to unlabeled data. This study extends this intui-
tion to construct a similarity graph for propagating
trigram-level label distributions. The derived label
distributions are regarded as prior knowledge to
regularize the learning of a sequential model, con-
ditional random fields (CRFs) in this case, on both
770
labeled and unlabeled data to achieve the semi-
supervised learning. The approach performs the
incorporation of the derived labeled distributions
by manipulating a ?virtual evidence? function as
described in (Li, 2009). Experiments on the da-
ta from the Chinese tree bank (CTB-7) and Mi-
crosoft Research (MSR) show that the proposed
model results in significant improvement over oth-
er comparative candidates in terms of F-score and
out-of-vocabulary (OOV) recall.
This paper is structured as follows: Section
2 points out the main differences with the re-
lated work of this study. Section 3 reviews the
background, including supervised character-based
joint S&T model based on CRFs and graph-based
label propagation. Section 4 presents the details of
the proposed approach. Section 5 reports the ex-
periment results. The conclusion is drawn in Sec-
tion 6.
2 Related Work
Prior supervised joint S&T models present ap-
proximate 0.2% - 1.3% improvement in F-score
over supervised pipeline ones. The state-of-the-
art joint models include reranking approaches (Shi
and Wang, 2007), hybrid approaches (Nakagawa
and Uchimoto, 2007; Jiang et al, 2008; Sun,
2011), and single-model approaches (Ng and Low,
2004; Zhang and Clark, 2008; Kruengkrai et al,
2009; Zhang and Clark, 2010). The proposed ap-
proach in this paper belongs to the single-model
type.
There are few explorations of semi-supervised
approaches for CWS or POS tagging in previ-
ous works. Xu et al (2008) described a Bayesian
semi-supervised CWS model by considering the
segmentation as the hidden variable in machine
translation. Unlike this model, the proposed ap-
proach is targeted at a general model, instead of
one oriented to machine translation task. Sun and
Xu (2011) enhanced a CWS model by interpolat-
ing statistical features of unlabeled data into the
CRFs model. Wang et al (2011) proposed a semi-
supervised pipeline S&T model by incorporating
n-gram and lexicon features derived from unla-
beled data. Different from their concern, our em-
phasis is to learn the semi-supervised model by
injecting the label information from a similarity
graph constructed from labeled and unlabeled da-
ta.
The induction method of the proposed approach
also differs from other semi-supervised CRFs al-
gorithms. Jiao et al (2006), extended by Mann
and McCallum (2007), reported a semi-supervised
CRFs model which aims to guide the learning
by minimizing the conditional entropy of unla-
beled data. The proposed approach regularizes the
CRFs by the graph information. Subramanya et
al. (2010) proposed a graph-based self-train style
semi-supervised CRFs algorithm. In the proposed
approach, an analogous way of graph construction
intuition is applied. But overall, our approach dif-
fers in three important aspects: first, novel feature
templates are defined for measuring the similari-
ty between vertices. Second, the critical property,
i.e., sparsity, is considered among label propaga-
tion. And third, the derived label information from
the graph is smoothed into the model by optimiz-
ing a modified objective function.
3 Background
3.1 Supervised Character-based Model
The character-based joint S&T approach is oper-
ated as a sequence labeling fashion that each Chi-
nese character, i.e., hanzi, in the sequence is as-
signed with a tag. To perform segmentation and
tagging simultaneously in a uniform framework,
according to Ng and Low (2004), the tag is com-
posed of a word boundary part, and a POS part,
e.g., ?B NN? refers to the first character in a word
with POS tag ?NN?. In this paper, 4 word bound-
ary tags are employed: B (beginning of a word),
M (middle part of a word), E (end of a word) and
S (single character). As for the POS tag, we shal-
l use the 33 tags in the Chinese tree bank. Thus,
the potential composite tags of joint S&T consist
of 132 (4?33) classes.
The first-order CRFs model (Lafferty et al,
2001) has been the most common one in this
task. Given a set of labeled examples Dl =
{(xi, yi)}li=1, where xi = x1ix2i ...xNi is the se-
quence of characters in the ith sentence, and yi =
y1i y2i ...yNi is the corresponding label sequence.
The goal is to learn a CRFs model in the form,
p(yi|xi; ?) =
1
Z(xi; ?)
exp{
N?
j=1
K?
k=1
?kfk(yj?1i , y
j
i , xi, j)}
(1)
where Z(xi; ?) is the partition function that nor-
malizes the exponential form to be a probability
distribution, and fk(yj?1i , yji , xi, j). In this study,
771
the baseline feature templates of joint S&T are
the ones used in (Ng and Low, 2004; Jiang et al,
2008), as shown in Table 1. ? = {?1?2...?K} ?
RK are the weight parameters to be learned. In su-
pervised training, the aim is to estimate the ? that
maximizes the conditional likelihood of the train-
ing data while regularizing model parameters:
L(?) =
l?
i=1
log p(yi|xi; ?)?R(?) (2)
R(?) can be any standard regularizer on parame-
ters, e.g., R(?) =? ? ? /2?2, to limit overfitting
on rare features and avoid degeneracy in the case
of correlated features. This objective function can
be optimized by the stochastic gradient method or
other numerical optimization methods.
Type Font Size
Unigram Cn(n = ?2,?1, 0, 1, 2)
Bigram CnCn+1(n = ?2,?1, 0, 1)
Date, Digit and
Alphabetic Letter
T (C?2)T (C?1)T (C0)
T (C1)T (C2)
Table 1: The feature templates of joint S&T.
3.2 Graph-based Label Propagation
Graph-based label propagation, a critical subclass
of semi-supervised learning (SSL), has been wide-
ly used and shown to outperform other SSL meth-
ods (Chapelle et al, 2006). Most of these algo-
rithms are transductive in nature, so they cannot
be used to predict an unseen test example in the fu-
ture (Belkin et al, 2006). Typically, graph-based
label propagation algorithms are run in two main
steps: graph construction and label propagation.
The graph construction provides a natural way to
represent data in a variety of target domains. One
constructs a graph whose vertices consist of la-
beled and unlabeled examples. Pairs of vertices
are connected by weighted edges which encode
the degree to which they are expected to have the
same label (Zhu et al, 2003). Popular graph con-
struction methods include k-nearest neighbors (k-
NN) (Bentley, 1980; Beygelzimer et al, 2006),
b-matching (Jebara et al, 2009) and local recon-
struction (Daitch et al, 2009). Label propaga-
tion operates on the constructed graph. The pri-
mary objective is to propagate labels from a few
labeled vertices to the entire graph by optimiz-
ing a loss function based on the constraints or
properties derived from the graph, e.g., smooth-
ness (Zhu et al, 2003; Subramanya et al, 2010;
Talukdar et al, 2008), or sparsity (Das and Smith,
2012). State-of-the-art label propagation algo-
rithms include LP-ZGL (Zhu et al, 2003), Ad-
sorption (Baluja et al, 2008), MAD (Talukdar
and Crammer, 2009) and Sparse Inducing Penal-
ties (Das and Smith, 2012).
4 Method
The emphasis of this work is on building a joint
S&T model based on two different kinds of data
sources, labeled and unlabeled data. In essence,
this learning problem can be treated as incorporat-
ing certain gainful information, e.g., prior knowl-
edge or label constraints, of unlabeled data into
the supervised model. The proposed approach em-
ploys a transductive graph-based label propagation
method to acquire such gainful information, i.e.,
label distributions from a similarity graph con-
structed over labeled and unlabeled data. Then,
the derived label distributions are injected as vir-
tual evidences for guiding the learning of CRFs.
Algorithm 1 semi-supervised joint S&T induction
Input:
Dl = {(xi, yi)}li=1 labeled sentences
Du = {(xi)}l+ui=l+1 unlabeled sentencesOutput:
?: a set of feature weights
1: Begin
2: {G} = construct graph (Dl,Du)
3: {q0} = init labelDist ({G})
4: {q} = propagate label ({G}, {q0})
5: {?} = train crf (Dl ? Du, {q})
6: End
The model induction includes the following
steps (see Algorithm 1): firstly, given labeled
and unlabeled data, i.e., Dl = {(xi, yi)}li=1
with l labeled sentences and Du = {(xi)}l+ui=l+1with u unlabeled sentences, a specific similarity
graph G representing Dl and Du is constructed
(construct graph). The vertices (Section 4.1) in
the constructed graph consist of all trigrams that
occur in labeled and unlabeled sentences, and edge
weights between vertices are computed using the
cosine distance between pointwise mutual infor-
mation (PMI) statistics. Afterwards, the estimated
label distributions q0 of vertices in the graph G are
randomly initialized (init labelDist). Subsequently,
772
the label propagation procedure (propagate label)
is conducted for projecting label distributions q
from labeled vertices to the entire graph, using
the algorithm of Sparse-Inducing Penalties (Das
and Smith, 2012) (Section 4.2). The final step
(train crf) of the induction is incorporating the in-
ferred trigram-level label distributions q into CRFs
model (Section 4.3).
4.1 Graph Construction
In most graph-based label propagation tasks, the
final effect depends heavily on the quality of
the graph. Graph construction thus plays a cen-
tral role in graph-based label propagation (Zhu et
al., 2003). For character-based joint S&T, unlike
the unstructured learning problem whose vertices
are formed directly by labeled and unlabeled in-
stances, the graph construction is non-trivial. Das
and Petrov (2011) mentioned that taking individu-
al characters as the vertices would result in various
ambiguities, whereas the similarity measurement
is still challenging if vertices corresponding to en-
tire sentences.
This study follows the intuitions of graph con-
struction from Subramanya et al (2010) in which
vertices are represented by character trigrams oc-
curring in labeled and unlabeled sentences. For-
mally, given a set of labeled sentences Dl, and un-
labeled onesDu, whereD , {Dl,Du}, the goal is
to form an undirected weighted graph G = (V,E),
where V is defined as the set of vertices which
covers all trigrams extracted from Dl and Du.
Here, V = Vl ? Vu, where Vl refers to trigrams
that occurs at least once in labeled sentences and
Vu refers to trigrams that occur only in unlabeled
sentences. The edges E ? Vl ? Vu, connect all
the vertices. This study makes use of a symmet-
ric k-NN graph (k = 5) and the edge weights are
measured by a symmetric similarity function (E-
quation (3)):
wi,j =
{
sim(xi, xj) if j ? K(i) or i ? K(j)
0 otherwise
(3)
where K(i) is the set of the k nearest neighbors of
xi(|K(i) = k, ?i|) and sim(xi, xj) is a similari-
ty measure between two vertices. The similarity
is computed based on the co-occurrence statistic-
s over the features in Table 2. Most features we
adopted are selected from those of (Subramanya
et al, 2010). Note that a novel feature in the last
row encodes the classes of surrounding character-
s, where four types are defined: number, punctu-
ation, alphabetic letter and other. It is especially
helpful for the graph to make connections with tri-
grams that may not have been seen in labeled data
but have similar label information. The pointwise
mutual information values between the trigram-
s and each feature instantiation that they have in
common are summed to sparse vectors, and their
cosine distances are computed as the similarities.
Description Feature
Trigram + Context x1x2x3x4x5
Trigram x2x3x4
Left Context x1x2
Right Context x4x5
Center Word x3
Trigram - Center Word x2x4
Left Word + Right Context x2x4x5
Right Word + Left Context x1x2x3
Type of Trigram: number,
punctuation, alphabetic letter
and other
t(x2)t(x3)t(x4)
Table 2: Features employed to measure the sim-
ilarity between two vertices, in a given tex-
t ?x1x2x3x4x5?, where the trigram is ?x2x3x4?.
The nature of the similarity graph enforces that
the connected trigrams with high weight appearing
in different texts should have similar syntax con-
figurations. Thus, the constructed graph is expect-
ed to provide additional information that cannot
be expressed directly in a sequence model (Subra-
manya et al, 2010). One primary benefit of this
property is on enriching vocabulary coverage. In
other words, the new features of various trigram-
s only occurring in unlabeled data can be discov-
ered. As the excerpt in Figure 1 shows, the trigram
????? (Tianjin port) has no any label informa-
tion, as it only occurs in unlabeled data, but for-
tunately its neighborhoods with similar syntax in-
formation, e.g., ????? (Shanghai port), ???
?? (Guangzhou port), can assist to infer the cor-
rect tag ?M NN?.
4.2 Label Propagation
In order to induce trigram-level label distributions
from the graph constructed by the previous step,
a label propagation algorithm, Sparsity-Inducing
Penalties, proposed by Das and Smith (2012), is
employed. This algorithm is used because it cap-
tures the property of sparsity that only a few labels
773
Figure 1: An excerpt from the similarity graph
over trigrams on labeled and unlabeled data.
are typically associated with a given instance. In
fact, the sparsity is also a common phenomenon
among character-based CWS and POS tagging.
The following convex objective is optimized on
the similarity graph in this case:
argmin
q
l?
j=1
? qj ? rj ?2
+?
l+u?
i=1,k?N (i)
wik ? qi ? qk ?2 +?
l+u?
i=1
? qi ?2
s.t. qi ? 0, ?i ? V
(4)
where rj denotes empirical label distributions of
labeled vertices, and qi denotes unnormalized es-
timate measures in every vertex. The wik refers to
the similarity between the ith trigram and the kth
trigram, and N (i) is a set of neighbors of the ith
trigram. ? and ? are two hyperparameters whose
values are discussed in Section 5. The squared-
loss criterion1 is used to formulate the objective
function. The first term in Equation (4) is the seed
match loss which penalizes the estimated label dis-
tributions qj , if they go too far away from the em-
pirical labeled distributions rj . The second term
is the edge smoothness loss that requires qi should
be smooth with respect to the graph, such that two
vertices connected by an edge with high weight
should be assigned similar labels. The final term
is a regularizer to incorporate the prior knowledge,
e.g., uniform distributions used in (Talukdar et al,
2008; Das and Smith, 2011). This study applies
the squared norm of q to encourage sparsity per
vertex. Note that the estimated label distribution
1It can be seen as a multi-class extension of quadratic cost
criterion (Bengio et al, 2006) or as a variant of the objective
in (Zhu et al, 2003). An entropic distance measure could also
be used, e.g., KL-divergence (Subramanya et al, 2010; Das
and Smith, 2012).
qi in Equation (4) is relaxed to be unnormalized,
which simplifies the optimization. Thus, the objec-
tive function can be optimized by L-BFGS-B (Zhu
et al, 1997), a generic quasi-Newton gradient-
based optimizer. The partial derivatives of Equa-
tion (4) are computed for each parameter of q and
then passed on to the optimizer that updates them
such that Equation (4) is maximized.
4.3 Semi-Supervised CRFs Training
The trigram-level label distributions inferred in the
propagation step can be viewed as a kind of valu-
able ?prior knowledge? to regularize the learning
on unlabeled data. The final step of the induc-
tion is thus to incorporate such prior knowledge
into CRFs. Li (2009) generalizes the use of vir-
tual evidence to undirected graphical models and,
in particular, to CRFs for incorporating external
knowledge. By extending the similar intuition, as
illustrated in Figure 2, we modify the structure of
a regular linear-chain CRFs on unlabeled data for
smoothing the derived label distributions, where
virtual evidences, i.e., q in our case, are donated
by {v1, v2, . . . , vT }, in parallel with the state vari-
ables {y1, y2, . . . , yT }. The modified CRFs model
allows us to flexibly define the interaction between
estimated state values and virtual evidences by po-
tential functions. Therefore, given labeled and un-
labeled data, the learning objective is defined as
follows:
L(?) +
l+u?
i=l+1
Ep(yi|xi,vi;?g)[log p(yi, vi|xi; ?)]
(5)
where the conditional probability in the second
term is denoted as
p(yi, vi|xi; ?) =
1
Z ?(xi; ?)
exp{
N?
j=1
K?
k=1
?kfk(yj?1i , y
j
i , xi, j)
+?
N?
t=1
s(yti , vti)}
(6)
The first term in Equation (5) is the same as E-
quation (2), which is the traditional CRFs learn-
ing objective function on the labeled data. The
second term is the expected conditional likelihood
of unlabeled data. It is directed to maximize the
conditional likelihood of hidden states with the
derived label distributions on unlabeled data, i.e.,
p(y, v|x), where y and v are jointly modeled but
774
the probability is still conditional on x. Here,
Z ?(x; ?) is the partition function of normalization
that is achieved by summing the numerator over
both y and v. A virtual evidence feature function
of s(yti , vti) with pre-defined weight ? is defined
to regularize the conditional distributions of states
over the derived label distributions. The learning
is impacted by the derived label distributions as E-
quation (7): firstly, if the trigram xt?1i xtixt+1i at
current position does have no corresponding de-
rived label distributions (vti = null), the value of
zero is assigned to all state hypotheses so that the
posteriors would not affected by the derived infor-
mation. Secondly, if it does have a derived label
distribution, since the virtual evidence in this case
is a distribution instead of a specific label, the la-
bel probability in the distribution under the current
state hypothesis is assigned. This means that the
values of state variables are constrained to agree
with the derived distributions.
s(yti , vti) =
{
qxt?1i xtixt+1i (y
t
i) if vti 6= null
0 else
(7)
The second term in Equation (5) can be op-
timized by using the expectation maximization
(EM) algorithm in the same fashion as in the
generative approach, following (Li, 2009). One
can iteratively optimize the Q function Q(?) =?
y p(yi|xi; ?g) log p(yi, vi|xi; ?), in which ?g is
the model estimated from the previous iteration.
Here the gradient of the Q function can be mea-
sured by:
?Q(?)
??k
=
?
t
?
yt?1i ,yti
fk(yt?1i , yti , xi, t).
(p(yt?1i , yti |xi, vi; ?)? p(yt?1i , yti |xi; ?))
(8)
The forward-backward algorithm is used to mea-
sure p(yt?1i , yti |xi, vi; ?) and p(yt?1i , yti |xi; ?).
Thus, the objective function Equation (5) is op-
timized as follows: for the instances i = 1, 2, ..., l,
the parameters ? are learned as the supervised
manner; for the instances i = l+1, l+2, ..., u+ l,
in the E-step, the expected value of Q function is
computed, based on the current model ?g. In the
M-step, the posteriors are fixed and updated ? that
maximizes Equation (5).
Figure 2: Modified linear-chain CRFs integrating
virtual evidences on unlabeled data.
5 Experiment
5.1 Setting
The experimental data are mainly taken from the
Chinese tree bank (CTB-7) and Microsoft Re-
search (MSR)2. CTB-7 consists of over one mil-
lion words of annotated and parsed text from Chi-
nese newswire, magazine news, various broadcast
news and broadcast conversation programs, web
newsgroups and weblogs. It is a segmented, POS
tagged3 and fully bracketed corpus. The train, de-
velopment and test sets4 from CTB-7 and their
corresponding statistics are reported in Table 3.
To satisfy the characteristic of the semi-supervised
learning problem, the train set, i.e., the labeled da-
ta, is formed by a relatively small amount of an-
notated texts sampled from CTB-7. For the un-
labeled data in this experiment, a greater amount
of texts is extracted from CTB-7 and MSR, which
contains 53,108 sentences with 2,418,690 charac-
ters.
The performance measurement indicators for
word segmentation and POS tagging (joint S&T)
are balance F-score, F = 2PR/(P+R), the harmon-
ic mean of precision (P) and recall (R), and out-
of-vocabulary recall (OOV-R). For segmentation,
a token is regarded to be correct if its boundaries
match the ones of a word in the gold standard.
For the POS tagging, it is correct only if both the
boundaries and the POS tags are perfect matches.
The experimental platform is implemented
based on two toolkits: Mallet (McCallum and
Kachites, 2002) and Junto (Talukdar and Pereira,
2010). Mallet is a java-based package for s-
tatistical natural language processing, which in-
cludes the CRFs implementation. Junto is a graph-
2It can be download at: www.sighan.org/bakeoff2005.
3There is a total of 33 POS tags in CTB-7.
4The extracted sentences in train, development and test set
were assigned with the composite tags as described in Section
3.1.
775
based label propagation toolkit that provides sev-
eral state-of-the-art algorithms.
Data #Sent #Word #Char #OOV
Train 17,968 374,697 596,360
Develop 1,659 46,637 79,283 0.074
Test 2,037 65,219 104,502 0.089
Table 3: Training, development and testing data.
5.2 Baseline and Proposed Models
In the experiment, the baseline supervised pipeline
and joint S&T models are built only on the train
data. The proposed model will also be compared
with the semi-supervised pipeline S&T model de-
scribed in (Wang et al, 2011). In addition, two
state-of-the-art semi-supervised CRFs algorithms,
Jiao?s CRFs (Jiao et al, 2006) and Subramanya?s
CRFs (Subramanya et al, 2010), are also used to
build joint S&T models. The corresponding set-
tings of the above candidates are listed below:
? Baseline I: a supervised CRFs pipeline S&T
model. The feature templates are from Zhao
et al (2006) and Wu et al (2008).
? Wang?s model: a semi-supervised CRFs
pipeline S&T model. The same feature tem-
plates in (Wang et al, 2011) are used, i.e.,
?+n-gram+cluster+lexicon?.
? Baseline II: a supervised CRFs joint S&T
model. The feature templates introduced in
Section 3.1 are used.
? Jiao?s model: a semi-supervised CRFs joint
S&T model trained using the entropy regular-
ization (ER) criteria (Jiao et al, 2006). The
optimization method proposed by Mann and
McCallum (2007) is applied.
? Subramanya?s model: a self-train style
semi-supervised CRFs joint S&T model
based on the same parameters used in (Sub-
ramanya et al, 2010).
? Our model: several parameters in our model
are needed to tune based on the development
set, e.g., ?, ? and ?.
In all the CRFs models above, the Gaussian reg-
ularizer and stochastic gradient descent method
are employed.
5.3 Main Results
This experiment yielded a similarity graph that
consists of 462,962 trigrams from labeled and un-
labeled data. The majority (317,677 trigrams) oc-
curred only in unlabeled data. Based on the de-
velopment data, the hyperparameters of our mod-
el were tuned among the following settings: for
the graph propagation, ? ? {0.2, 0.5, 0.8} and
? ? {0.1, 0.3, 0.5, 0.8}; for the CRFs training,
? ? {0.1, 0.3, 0.5, 0.7, 0.9}. The best performed
joint settings are ? = 0.5, ? = 0.3 and ? = 0.7.
With the chosen set of hyperparameters, the test
data was used to measure the final performance.
Model Segmentation POS TaggingF1 OOV-R F1 OOV-R
Baseline I 94.27 60.12 91.08 51.72
Wang?s 95.17 63.10 91.64 53.29
Baseline II 95.14 61.52 91.61 52.29
Jiao?s 95.58 63.05 92.11 53.27
Subramanya?s 96.30 67.12 92.46 57.15
Our model 96.85 68.09 92.89 58.36
Table 4: The performance of segmentation and
POS tagging on testing data.
Table 4 summarizes the performance of seg-
mentation and POS tagging on the test data, in
comparison with the other five models. First-
ly, as expected, for the two supervised baselines,
the joint model outperforms the pipeline one, e-
specially on segmentation. It obtains 0.92% and
2.32% increase in terms of F-score and OOV-R
respectively. This outcome verifies the commonly
accepted fact that the joint model can substantially
improve the pipeline one, since POS tags provide
additional information to word segmentation (Ng
and Low, 2004). Secondly, it is also noticed that
all four semi-supervised models are able to benefit
from unlabeled data and greatly improve the re-
sults with respect to the baselines. On the whole,
for segmentation, they achieve average improve-
ments of 1.02% and 6.8% in F-score and OOV-R;
whereas for POS tagging, the average increments
of F-sore and OOV-R are 0.87% and 6.45%. An
interesting phenomenon is found among the com-
parisons with baselines that the supervised joint
model (Baseline II) is even competitive with semi-
supervised pipeline one (Wang et al, 2011). This
illustrates the effects of error propagation in the
pipeline approach. Thirdly, in what concerns the
semi-supervised approaches, the three joint S&T
models, i.e., Jiao?s, Subramanya?s and our mod-
el, are superior to the pipeline model, i.e., Wang?s
776
model. Moreover, the two graph-based approach-
es, i.e., Subramanya?s and our model, outperform
the others. Most importantly, the boldface num-
bers in the last row illustrate that our model does
achieve the best performance. Overall, for word
segmentation, it obtains average improvements of
1.43% and 8.09% in F-score and OOV-R over oth-
ers; for POS tagging, it achieves average improve-
ments of 1.09% and 7.73%.
0 10,000 20,000 30,000 40,000 50,000
94.0
94.5
95.0
95.5
96.0
96.5
97.0
97.5
 Wang's
 Jiao's
 Subramanya's
 Our
F
-
s
c
o
r
e
Number of unlabeled sentences
0 10,000 20,000 30,000 40,000 50,000
91.0
91.5
92.0
92.5
93.0
93.5
 Wang's
 Jiao's
 Subramanya's
 Our
F
-
s
c
o
r
e
Number of unlabeled sentences
0 10,000 20,000 30,000 40,000 50,000
60.0
62.5
65.0
67.5
70.0
 Wang's
 Jiao's
 Subramanya's
 Our
O
O
V
-
R
Number of unlabeled sentences
0 10,000 20,000 30,000 40,000 50,000
51.0
52.5
54.0
55.5
57.0
58.5
 Wang's
 Jiao's
 Subramanya's
 Our
O
O
V
-
R
Number of unlabeled sentences
Figure 3: The learning curves of semi-supervised
models on unlabeled data, where left graphs are
segmentation and the right ones are tagging.
5.4 Learning Curve
An additional experiment was conducted to inves-
tigate the impact of unlabeled data for the four
semi-supervised models. Figure 3 illustrates the
curves of F-score and OOV-R for segmentation
and tagging respectively, as the unlabeled data
size is progressively increased in steps of 6,000
sentences. It can be clearly observed that al-
l curves of our model are able to mount up steadi-
ly and achieve better gains over others consistent-
ly. The most competitive performance of the oth-
er three candidates is achieved by Subramanya?s
model. This strongly reveals that the knowledge
derived from the similarity graph does effectively
strengthen the model. But in Subramanya?s mod-
el, when the unlabeled size ascends to approxi-
mately 30,000 sentences the curves become nearly
asymptotic. The semi-supervised pipeline model,
Wang?s model, presents a much slower growth on
all curves over the others and also begins to over-
fit with large unlabeled data sizes (>25,000 sen-
tences). The figure also shows an erratic fluctu-
ation of Jiao?s model. Since this approach aims
at minimizing conditional entropy over unlabeled
data and encourages finding putative labelings for
unlabeled data, it results in a data-sensitive mod-
el (Li et al, 2009).
5.5 Analysis & Discussion
A statistical analysis of the segmentation and tag-
ging results of the supervised joint model (Base-
line II) and our model is carried out to comprehend
the influence of the graph-based semi-supervised
behavior. For word segmentation, the most signif-
icant improvement of our model is mainly concen-
trated on two kinds of words which are known for
their difficulties in terms of CWS: a) named enti-
ties (NE), e.g., ????? (Tianjin port) and ???
?? (free tax zone); and b) Chinese numbers (CN),
e.g., ?????? (eight hundred and fifty million)
and ???????? (seventy two percent). Very
often, these words do not exist in the labeled data,
so the supervised model is hard to learn their fea-
tures. Part of these words, however, may occur in
the unlabeled data. The proposed semi-supervised
approach is able to discover their label information
with the help of a similarity graph. Specifically, it
learns the label distributions from similar words
(neighborhoods), e.g., ????? (Shanghai port),
????? (protection zone), ?????? (nine
hundred and seventy million). The statistics in Ta-
ble 5 demonstrate significant error reductions of
50.44% and 48.74% on test data, corresponding to
NE and CN respectively.
Type #word #baErr #gbErr ErrDec%
NE 471 226 112 50.44
CN 181 119 61 48.74
Table 5: The statistics of segmentation error for
named entities (NE) and Chinese numbers (CN)
in test data. #baErr and #gbErr denote the count
of segmentations by Baseline II and our model;
ErrDec% denotes the error reduction.
On the other hand, to better understand the tag-
ging results, we summarize the increase and de-
crease of the top five common tagging error pat-
terns of our model over Baseline II for the cor-
rectly segmented words, as shown in Table 6. The
error pattern is defined by ?A?B? that refers the
true tag of ?A? is annotated by a tag of ?B?. The
obvious improvement brought by our model oc-
curs with the tags ?NN?, ?CD?, ?NR?, ?JJ? and
?NR?, where errors are reduced 60.74% on aver-
777
Pattern #baErr ? Pattern #baErr ?
NN?VV 58 38 NN?NR 13 6
CD?NN 41 27 IJ?ON 9 5
NR?VV 29 17 VV?NN 4 3
JJ?NN 18 11 NR?NN 1 3
NR?VA 19 10 JJ?AD 1 2
Table 6: The statistics of POS tagging error pat-
terns in test data. #baErr denote the count of tag-
ging error by Baseline II, while ? and ? denotes
the number of error reduced or increased by our
model.
age. More impressively, there is a large portion of
fixed error pattern instances stemming from OOV
words. Meanwhile, it is also observed that the dis-
ambiguation of error patterns in the right portion
of the table slightly suffers from our approach. In
reality, it is impossible and unrealistic to request
a model to be ?no harms but only benefits? under
whatever circumstances.
6 Conclusion
This study introduces a novel semi-supervised ap-
proach for joint Chinese word segmentation and
POS tagging. The approach performs the semi-
supervised learning in the way that the trigram-
level distributions inferred from a similarity graph
are used to regularize the learning of CRFs model
on labeled and unlabeled data. The empirical re-
sults indicate that the similarity graph information
and the incorporation manner of virtual evidences
present a positive effect to the model induction.
Acknowledgments
The authors are grateful to the Science and Tech-
nology Development Fund of Macau and the Re-
search Committee of the University of Macau
for the funding support for our research, un-
der the reference No. 017/2009/A and RG060/09-
10S/CS/FST. The authors also wish to thank the
anonymous reviewers for many helpful comments.
References
Shumeet Baluja, Rohan Seth, D. Sivakumar, Yushi
Jing, Jay Yagnik, Shankar Kumar, Deepak Ravich,
and Mohamed Aly. 2008. Video suggestion and
discovery for youtube: taking random walks through
the view graph. In Proceedings of WWW, pages 895-
904, Beijing, China.
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani.
2006. Manifold regularization. Journal of machine
learning research, 7:2399?2434.
Yoshua Bengio, Olivier Delalleau, and Nicolas Le
Roux. 2006. Label propogation and quadratic crite-
rion. MIT Press.
Jon Louis Bentley. 1980. Multidimensional divide-and
-conquer. Communications of the ACM, 23(4):214 -
229.
Alina Beygelzimer, Sham Kakade, and John Langford.
2006. Cover trees for nearest neighbor. In Proceed-
ings of ICML, pages 97-104, New York, USA
Olivier Chapelle, Bernhard Scho? lkopf, and Alexander
Zien. 2006. Semi-supervised learning. MIT Press.
Samuel I. Daitch, Jonathan A. Kelner, and Daniel A.
Spielman. 2009. Fitting a graph to vector data. In
Proceedings of ICML, 201-208, NY, USA.
Dipanjan Das and Noah A. Smith. 2011. Semi-
supervised framesemantic parsing for unknown
predicates. In Proceedings of ACL, pages 1435-
1444, Portland, Oregon, USA.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
Part-of-Speech Tagging with Bilingual Graph-based
Projections. In Proceedings of ACL, pages 1435-
1444, Portland, Oregon, USA.
Dipanjan Das and Noah A. Smith. 2012. Graph-based
lexicon expansion with sparsity-inducing penalties.
In Proceedings of NAACL, pages 677-687, Montre?al,
Canada.
Tony Jebara, Jun Wang, and Shih-Fu Chang. 2009.
Graph construction and b-matching for semi-
supervised learning. In Proceedings of ICML, 441-
448, New York, USA.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Liu.
2008. A Cascaded Linear Model for Joint Chinese
Word Segmentation and Part-of-Speech Tagging. In
Proceedings of ACL, pages 897-904, Columbus, O-
hio.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic Adaptation of Annotation Standards: Chi-
nese Word Segmentation and POS Tagging ? A Case
Study. In Proceedings of he ACL and the 4th IJC-
NLP of the AFNLP, pages 522?530, Suntec, Singa-
pore.
Feng Jiao, Shaojun Wang, and Chi-Hoon Lee. 2006.
Semi-supervised conditional random fields for im-
proved sequence segmentation and labeling. In In
Proceedings of ACL, pages 209?216, Sydney, Aus-
tralia.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hy-
brid model for joint Chinese word segmentation and
POS tagging. In Proceedings of ACL and IJCNLP
of the AFNLP, pages 513- 521, Suntec, Singapore
August.
778
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Field: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In Proceedings of ICML, pages 282-
289, Williams College, USA.
Xiao Li. 2009. On the use of virtual evidence in con-
ditional random fields. In Proceedings of EMNLP,
pages 1289-1297, Singapore.
Xiao Li, Ye-Yi Wang, and Alex Acero. 2009. Extract-
ing structured information from user queries with
semi-supervised conditional random fields In Pro-
ceedings of ACM SIGIR, pages 572-579, Boston,
USA.
Gideon S. Mann and Andrew McCallum. 2007. Ef-
ficient computation of entropy gradient for semi-
supervised conditional random fields. In Proceed-
ings of NAACL, pages 109-112, New York, USA.
McCallum and Andrew Kachites. 2002. MALLET: A
Machine Learning for Language Toolkit. Software
at http://mallet.cs.umass.edu.
Tetsuji Nakagawa and Kiyotaka Uchimoto. 2007. A
hybrid approach to word segmentation and POS tag-
ging. In Proceedings of ACL Demo and Poster Ses-
sion, pages 217?220, Prague, Czech Republic.
Hwee Tou Ng and Jin Kiat Low 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proceedings of
EMNLP, Barcelona, Spain.
Xian Qian and Yang Liu. 2012. Joint Chinese Word
Segmentation, POS Tagging and Parsing. In Pro-
ceedings of EMNLP-CoNLL, pages 501-511, Jeju Is-
land, Korea.
Yanxin Shi and Mengqiu Wang. 2007. A dual-layer
CRF based joint decoding method for cascade seg-
mentation and labelling tasks. In Proceedings of IJ-
CAI, Hyderabad, India.
Amarnag Subramanya, Slav Petrov, and Fernando
Pereira. 2010. Efficient graph-based semi-
supervised learning of structured tagging models.
In Proceedings of EMNLP, pages 167-176, Mas-
sachusetts, USA.
Weiwei Sun. 2011. A Stacked Sub-Word Model
for Joint Chinese Word Segmentation and Part-of-
Speech Tagging. In Proceedings of ACL, pages
1385?1394, Portland, Oregon.
Weiwei Sun and Jia Xu. 2011. Enhancing Chinese
word segmentation using unlabeled data. In Pro-
ceedings of EMNLP, pages 970-979, Scotland, UK.
Partha Pratim Talukdar, Joseph Reisinger, Marius Pas-
ca, Deepak Ravichandran, Rahul Bhagat, and Fer-
nando Pereira. 2008. Weakly Supervised Acquisi-
tion of Labeled Class Instances using Graph Ran-
dom Walks. In Proceedings of EMNLP, pages 582-
590, Hawaii, USA.
Partha Pratim Talukdar and Koby Crammer. 2009.
New Regularized Algorithms for Transductive
Learning. In Proceedings of ECML-PKDD, pages
442 - 457, Bled, Slovenia.
Partha Pratim Talukdar and Fernando Pereira. 2010.
Experiments in graph-based semi-supervised learn-
ing methods for class-instance acquisition. In Pro-
ceedings of ACL, pages 1473-1481, Uppsala, Swe-
den.
Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Torisa-
wa. 2011. Improving Chinese word segmentation
and POS tagging with semi-supervised methods us-
ing large auto-analyzed data. In Proceedings of IJC-
NLP, pages 309?317, Chiang Mai, Thailand.
Yu-Chieh Wu Jie-Chi Yang, and Yue-Shi Lee. 2008.
Description of the NCU Chinese Word Segmenta-
tion and Part-of-Speech Tagging for SIGHAN Bake-
off. In Proceedings of the SIGHAN Workshop on
Chinese Language Processing, pages 161-166, Hy-
derabad, India.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Herman-
n Ney. 2008. Bayesian semi-supervised chinese
word segmentation for statistical machine transla-
tion. In Proceedings of COLING, pages 1017-1024,
Manchester, UK.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and POS tagging using a single percep-
tron. In Proceedings of EMNLP, pages 888-896,
Columbus, Ohio.
Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and POS-tagging using
a single discriminative model. In Proceedings of
EMNLP, pages 843-852, Massachusetts, USA.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2006. Effective tag set selection in Chinese
word segmentation via conditional random field
modeling. In Proceedings of PACLIC, pages 87-94,
Wuhan, China.
Xiaojin Zhu, Zoubin Ghahramani, and John Laffer-
ty. 2003. Semi-supervised learning using Gaussian
fields and harmonic functions. In Proceedings of
ICML, pages 912?919, Washington DC, USA.
Ciyou Zhu, Richard H. Byrd, Peihuang Lu, and Jorge
Nocedal. 1997. L-BFGS-B: Fortran subroutines for
large scale bound constrained optimization. ACM
Transactions on Mathematical Software, 23:550-
560.
779
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 171?176,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Co-regularizing character-based and word-based models for
semi-supervised Chinese word segmentation
Xiaodong Zeng? Derek F. Wong? Lidia S. Chao? Isabel Trancoso?
?Department of Computer and Information Science, University of Macau
?INESC-ID / Instituto Superior Te?cnico, Lisboa, Portugal
nlp2ct.samuel@gmail.com, {derekfw, lidiasc}@umac.mo,
isabel.trancoso@inesc-id.pt
Abstract
This paper presents a semi-supervised
Chinese word segmentation (CWS) ap-
proach that co-regularizes character-based
and word-based models. Similarly to
multi-view learning, the ?segmentation
agreements? between the two differen-
t types of view are used to overcome the
scarcity of the label information on unla-
beled data. The proposed approach train-
s a character-based and word-based mod-
el on labeled data, respectively, as the ini-
tial models. Then, the two models are con-
stantly updated using unlabeled examples,
where the learning objective is maximiz-
ing their segmentation agreements. The a-
greements are regarded as a set of valuable
constraints for regularizing the learning of
both models on unlabeled data. The seg-
mentation for an input sentence is decod-
ed by using a joint scoring function com-
bining the two induced models. The e-
valuation on the Chinese tree bank reveals
that our model results in better gains over
the state-of-the-art semi-supervised mod-
els reported in the literature.
1 Introduction
Chinese word segmentation (CWS) is a critical
and a necessary initial procedure with respect to
the majority of high-level Chinese language pro-
cessing tasks such as syntax parsing, informa-
tion extraction and machine translation, since Chi-
nese scripts are written in continuous characters
without explicit word boundaries. Although su-
pervised CWS models (Xue, 2003; Zhao et al,
2006; Zhang and Clark, 2007; Sun, 2011) pro-
posed in the past years showed some reasonably
accurate results, the outstanding problem is that
they rely heavily on a large amount of labeled da-
ta. However, the production of segmented Chi-
nese texts is time-consuming and expensive, since
hand-labeling individual words and word bound-
aries is very hard (Jiao et al, 2006). So, one can-
not rely only on the manually segmented data to
build an everlasting model. This naturally pro-
vides motivation for using easily accessible raw
texts to enhance supervised CWS models, in semi-
supervised approaches. In the past years, however,
few semi-supervised CWS models have been pro-
posed. Xu et al (2008) described a Bayesian semi-
supervised model by considering the segmentation
as the hidden variable in machine translation. Sun
and Xu (2011) enhanced the segmentation result-
s by interpolating the statistics-based features de-
rived from unlabeled data to a CRFs model. An-
other similar trial via ?feature engineering? was
conducted by Wang et al (2011).
The crux of solving semi-supervised learning
problem is the learning on unlabeled data. In-
spired by multi-view learning that exploits redun-
dant views of the same input data (Ganchev et
al., 2008), this paper proposes a semi-supervised
CWS model of co-regularizing from two dif-
ferent views (intrinsically two different models),
character-based and word-based, on unlabeled da-
ta. The motivation comes from that the two types
of model exhibit different strengths and they are
mutually complementary (Sun, 2010; Wang et al,
2010). The proposed approach begins by train-
ing a character-based and word-based model on
labeled data respectively, and then both models
are regularized from each view by their segmen-
tation agreements, i.e., the identical outputs, of
unlabeled data. This paper introduces segmenta-
tion agreements as gainful knowledge for guiding
the learning on the texts without label information.
Moreover, in order to better combine the strengths
of the two models, the proposed approach uses a
joint scoring function in a log-linear combination
form for the decoding in the segmentation phase.
171
2 Segmentation Models
There are two classes of CWS models: character-
based and word-based. This section briefly re-
views two supervised models in these categories,
a character-based CRFs model, and a word-based
Perceptrons model, which are used in our ap-
proach.
2.1 Character-based CRFs Model
Character-based models treat word segmentation
as a sequence labeling problem, assigning label-
s to the characters in a sentence indicating their
positions in a word. A 4 tag-set is used in this
paper: B (beginning), M (middle), E (end) and
S (single character). Xue (2003) first proposed
the use of CRFs model (Lafferty et al, 2001) in
character-based CWS. Let x = (x1x2...x|x|) ? X
denote a sentence, where each character and y =
(y1y2...y|y|) ? Y denote a tag sequence, yi ? T
being the tag assigned to xi. The goal is to achieve
a label sequence with the best score in the form,
p?c(y|x) =
1
Z(x; ?c)
exp{f(x, y) ? ?c} (1)
where Z(x; ?c) is a partition function that normal-
izes the exponential form to be a probability distri-
bution, and f(x, y) are arbitrary feature functions.
The aim of CRFs is to estimate the weight param-
eters ?c that maximizes the conditional likelihood
of the training data:
??c = argmax
?c
l?
i=1
log p?c(yi|xi)? ???c?22 (2)
where ???c?22 is a regularizer on parameters to
limit overfitting on rare features and avoid degen-
eracy in the case of correlated features. In this
paper, this objective function is optimized by s-
tochastic gradient method. For the decoding, the
Viterbi algorithm is employed.
2.2 Word-based Perceptrons Model
Word-based models read a input sentence from left
to right and predict whether the current piece of
continuous characters is a word. After one word
is identified, the method moves on and searches
for a next possible word. Zhang and Clark (2007)
first proposed a word-based segmentation mod-
el using a discriminative Perceptrons algorithm.
Given a sentence x, let us denote a possible seg-
mented sentence as w ? w, and the function that
enumerates a set of segmentation candidates as
GEN:w = GEN(x) for x. The objective is to
maximize the following problem for all sentences:
??w = argmax
w=GEN(x)
|w|?
i=1
?(x,wi) ? ?w (3)
where it maps the segmented sentencew to a glob-
al feature vector ? and denotes ?w as its cor-
responding weight parameters. The parameter-
s ?w can be estimated by using the Perceptron-
s method (Collins, 2002) or other online learning
algorithms, e.g., Passive Aggressive (Crammer et
al., 2006). For the decoding, a beam search decod-
ing method (Zhang and Clark, 2007) is used.
2.3 Comparison Between Both Models
Character-based and word-based models present
different behaviors and each one has its own
strengths and weakness. Sun (2010) carried out a
thorough survey that includes theoretical and em-
pirical comparisons from four aspects. Here, two
critical properties of the two models supporting
the co-regularization in this study are highlight-
ed. Character-based models present better predic-
tion ability for new words, since they lay more
emphasis on the internal structure of a word and
thereby express more nonlinearity. On the oth-
er side, it is easier to define the word-level fea-
tures in word-based models. Hence, these models
have a greater representational power and conse-
quently better recognition performance for in-of-
vocabulary (IV) words.
3 Semi-supervised Learning via
Co-regularizing Both Models
As mentioned earlier, the primary challenge of
semi-supervised CWS concentrates on the unla-
beled data. Obviously, the learning on unlabeled
data does not come for ?free?. Very often, it is
necessary to discover certain gainful information,
e.g., label constraints of unlabeled data, that is in-
corporated to guide the learner toward a desired
solution. In our approach, we believe that the seg-
mentation agreements (? 3.1) from two differen-
t views, character-based and word-based models,
can be such gainful information. Since each of the
models has its own merits, their consensuses signi-
fy high confidence segmentations. This naturally
leads to a new learning objective that maximizes
segmentation agreements between two models on
unlabeled data.
172
This study proposes a co-regularized CWS
model based on character-based and word-based
models, built on a small amount of segmented sen-
tences (labeled data) and a large amount of raw
sentences (unlabeled data). The model induction
process is described in Algorithm 1: given labeled
dataset Dl and unlabeled dataset Du, the first t-
wo steps are training a CRFs (character-based) and
Perceptrons (word-based) model on the labeled
data Dl , respectively. Then, the parameters of
both models are continually updated using unla-
beled examples in a learning cycle. At each iter-
ation, the raw sentences in Du are segmented by
current character-based model ?c and word-based
model ?w. Meanwhile, all the segmentation agree-
ments A are collected (? 3.1). Afterwards, the
agreements A are used as a set of constraints to
bias the learning of CRFs (? 3.2) and Perceptron
(? 3.3) on the unlabeled data. The convergence
criterion is the occurrence of a reduction of seg-
mentation agreements or reaching the maximum
number of learning iterations. In the final segmen-
tation phase, given a raw sentence, the decoding
requires both induced models (? 3.4) in measuring
a segmentation score.
Algorithm 1 Co-regularized CWS model induction
Require: n labeled sentencesDl;m unlabeled sentencesDu
Ensure: ?c and ?w
1: ?0c ? crf train(Dl)
2: ?0w ? perceptron train(Dl)
3: for t = 1...Tmax do
4: At ? agree(Du, ?t?1c , ?t?1w )
5: ?tc ? crf train constraints(Du,At, ?t?1c )
6: ?tw ? perceptron train constraints(Du,At, ?t?1w )
7: end for
3.1 Agreements Between Two Models
Given a raw sentence, e.g., ?????????
?????(I am watching the opening ceremony
of the Olympics in Beijing.)?, the two segmenta-
tions shown in Figure 1 are the predictions from
a character-based and word-based model. The
segmentation agreements between the two mod-
els correspond to the identical words. In this ex-
ample, the five words, i.e. ?? (I)?, ??? (Bei-
jing)?, ?? (watch)?, ???? (opening ceremony)?
and ??(.)?, are the agreements.
3.2 CRFs with Constraints
For the character-based model, this paper fol-
lows (Ta?ckstro?m et al, 2013) to incorporate the
segmentation agreements into CRFs. The main
idea is to constrain the size of the tag sequence
lattice according to the agreements for achieving
simplified learning. Figure 2 demonstrates an ex-
ample of the constrained lattice, where the bold
node represents that a definitive tag derived from
the agreements is assigned to the current charac-
ter, e.g., ?? (I)? has only one possible tag ?S?
because both models segmented it to a word with
a single character. Here, if the lattice of all admis-
sible tag sequences for the sentence x is denoted
as Y(x), the constrained lattice can be defined by
Y?(x, y?), where y? refers to tags inferred from the
agreements. Thus, the objective function on unla-
beled data is modeled as:
???c = argmax
?c
m?
i=1
log p?c(Y?(xi, y?i)|xi)? ???c?22
(4)
It is a marginal conditional probability given by
the total probability of all tag sequences consistent
with the constrained lattice Y?(x, y?). This objec-
tive can be optimized by using LBFGS-B (Zhu et
al., 1997), a generic quasi-Newton gradient-based
optimizer.
Figure 1: The segmentations given by character-
based and word-based model, where the words in
?2? refer to the segmentation agreements.
Figure 2: The constrained lattice representation
for a given sentence, ????????????
???.
3.3 Perceptrons with Constraints
For the word-based model, this study incorporates
segmentation agreements by a modified parame-
ter update criterion in Perceptrons online training,
as shown in Algorithm 2. Because there are no
?gold segmentations? for unlabeled sentences, the
output sentence predicted by the current model is
compared with the agreements instead of the ?an-
swers? in the supervised case. At each parameter
173
update iteration k, each raw sentence xu is decod-
ed with the current model into a segmentation zu.
If the words in output zu do not match the agree-
ments A(xu) of the current sentence xu, the pa-
rameters are updated by adding the global feature
vector of the current training example with the a-
greements and subtracting the global feature vec-
tor of the decoder output, as described in lines 3
and 4 of Algorithm 2.
Algorithm 2 Parameter update in word-based model
1: for k = 1...K, u = 1...m do
2: calculate zu = argmax
w=GEN(x)
?|w|
i=1 ?(xu, wi) ? ?k?1w
3: if zu 6= A(xu)
4: ?kw = ?k?1w + ?(A(xu))? ?(zu)
5: end for
3.4 The Joint Score Function for Decoding
There are two co-regularized models as results of
the previous induction steps. An intuitive idea is
that both induced models are combined to conduct
the segmentation, for the sake of integrating their
strengths. This paper employs a log-linear inter-
polation combination (Bishop, 2006) to formulate
a joint scoring function based on character-based
and word-based models in the decoding:
Score(w) = ? ? log(p?c(y|x))
+(1? ?) ? log(?(x,w) ? ?w) (5)
where the two terms of the logarithm are the s-
cores of character-based and word-based model-
s, respectively, for a given segmentation w. This
composite function uses a parameter ? to weight
the contributions of the two models. The ? value
is tuned using the development data.
4 Experiment
4.1 Setting
The experimental data is taken from the Chinese
tree bank (CTB). In order to make a fair compar-
ison with the state-of-the-art results, the versions
of CTB-5, CTB-6, and CTB-7 are used for the e-
valuation. The training, development and testing
sets are defined according to the previous works.
For CTB-5, the data split from (Jiang et al, 2008)
is employed. For CTB-6, the same data split as
recommended in the CTB-6 official document is
used. For CTB-7, the datasets are formed accord-
ing to the way in (Wang et al, 2011). The cor-
responding statistic information on these data s-
plits is reported in Table 1. The unlabeled data in
our experiments is from the XIN CMN portion of
Chinese Gigaword 2.0. The articles published in
1991-1993 and 1999-2004 are used as unlabeled
data, with 204 million words.
The feature templates in (Zhao et al, 2006)
and (Zhang and Clark, 2007) are used in train-ing
the CRFs model and Perceptrons model, respec-
tively. The experimental platform is implement-
ed based on two popular toolkits: CRF++ (Kudo,
2005) and Zpar (Zhang and Clark, 2011).
Data #Sent-train
#Sent-
dev
#Sent-
test
OOV-
dev
OOV-
test
CTB-5 18,089 350 348 0.0811 0.0347
CTB-6 23,420 2,079 2,796 0.0545 0.0557
CTB-7 31,131 10,136 10,180 0.0549 0.0521
Table 1: Statistics of CTB-5, CTB-6 and CTB-7
data.
4.2 Main Results
The development sets are mainly used to tune the
values of the weight factor ? in Equation 5. We
evaluated the performance (F-score) of our model
on the three development sets by using differen-
t ? values, where ? is progressively increased in
steps of 0.1 (0 < ? < 1.0). The best performed
settings of ? for CTB-5, CTB-6 and CTB-7 on de-
velopment data are 0.7, 0.6 and 0.6, respectively.
With the chosen parameters, the test data is used
to measure the final performance.
Table 2 shows the F-score results of word seg-
mentation on CTB-5, CTB-6 and CTB-7 testing
sets. The line of ?ours? reports the performance
of our semi-supervised model with the tuned pa-
rameters. We first compare it with the supervised
?baseline? method which joints character-based
and word-based model trained only on the training
set1. It can be observed that our semi-supervised
model is able to benefit from unlabeled data and
greatly improves the results over the supervised
baseline. We also compare our model with two
state-of-the-art semi-supervised methods of Wang
?11 (Wang et al, 2011) and Sun ?11 (Sun and X-
u, 2011). The performance scores of Wang ?11 are
directly taken from their paper, while the results of
Sun ?11 are obtained, using the program provided
by the author, on the same experimental data. The
1The ?baseline? uses a different training configuration so
that the ? values in the decoding are also need to be tuned on
the development sets. The tuned ? values are {0.6, 0.6, 0.5}
for CTB-5, CTB-6 and CTB-7.
174
bold scores indicate that our model does achieve
significant gains over these two semi-supervised
models. This outcome can further reveal that us-
ing the agreements from these two views to regu-
larize the learning can effectively guide the mod-
el toward a better solution. The third compari-
son candidate is Hatori ?12 (Hatori et al, 2012)
which reported the best performance in the litera-
ture on these three testing sets. It is a supervised
joint model of word segmentation, POS tagging
and dependency parsing. Impressively, our model
still outperforms Hatori ?12 on all three datasets.
Although there is only a 0.01 increase on CTB-5,
it can be seen as a significant improvement when
considering Hatori ?12 employs much richer train-
ing resources, i.e., sentences tagged with syntactic
information.
Method CTB-5 CTB-6 CTB-7
Ours 98.27 96.33 96.72
Baseline 97.58 94.71 94.87
Wang ?11 98.11 95.79 95.65
Sun ?11 98.04 95.44 95.34
Hatori ?12 98.26 96.18 96.07
Table 2: F-score (%) results of five CWS models
on CTB-5, CTB-6 and CTB-7.
5 Conclusion
This paper proposed an alternative semi-
supervised CWS model that co-regularizes a
character- and word-based model by using their
segmentation agreements on unlabeled data. We
perform the agreements as valuable knowledge
for the regularization. The experiment results
reveal that this learning mechanism results in a
positive effect to the segmentation performance.
Acknowledgments
The authors are grateful to the Science and Tech-
nology Development Fund of Macau and the Re-
search Committee of the University of Macau for
the funding support for our research, under the ref-
erence No. 017/2009/A and MYRG076(Y1-L2)-
FST13-WF. The authors also wish to thank the
anonymous reviewers for many helpful comments.
References
Christopher M. Bishop. 2006. Pattern recognition and
machine learning.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP, pages 1-8, Philadelphia, USA.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of ma-chine
learning research, 7:551-585.
Kuzman Ganchev, Joao Graca, John Blitzer, and Ben
Taskar. 2008. Multi-View Learning over Struc-
tured and Non-Identical Outputs. In Proceedings of
CUAI, pages 204-211, Helsinki, Finland.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2012. Incremental Joint Approach
to Word Segmentation, POS Tagging, and Depen-
dency Parsing in Chinese. In Proceedings of ACL,
pages 1045-1053, Jeju, Republic of Korea.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Liu.
2008. A Cascaded Linear Model for Joint Chinese
Word Segmentation and Part-of-Speech Tagging. In
Proceedings of ACL, pages 897-904, Columbus, O-
hio.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic Adaptation of Annotation Standards: Chi-
nese Word Segmentation and POS Tagging - A Case
Study. In Proceedings of ACL and the 4th IJCNLP
of the AFNLP, pages 522-530, Suntec, Singapore.
Feng Jiao, Shaojun Wang and Chi-Hoon Lee. 2006.
Semi-supervised conditional random fields for im-
proved sequence segmentation and labeling. In Pro-
ceedings of ACL and the 4th IJCNLP of the AFNLP,
pages 209-216, Strouds-burg, PA, USA.
Taku Kudo. 2005. CRF++: Yet another CRF toolkit.
Software available at http://crfpp.sourceforge. net.
John Lafferty, Andrew McCallum, and Fernando Pe-
reira. 2001. Conditional Random Field: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In Proceedings of ICML, pages 282-
289, Williams College, USA.
Weiwei Sun. 2001. Word-based and character-based
word segmentation models: comparison and com-
bination. In Proceedings of COLING, pages 1211-
1219, Bejing, China.
Weiwei Sun. 2011. A stacked sub-word model for
joint Chinese word segmentation and part-of-speech
tagging. In Proceedings of ACL, pages 1385-1394,
Portland, Oregon.
Weiwei Sun and Jia Xu. 2011. Enhancing Chinese
word segmentation using unlabeled data. In Pro-
ceedings of EMNLP, pages 970-979, Scotland, UK.
Oscar Ta?ckstro?m, Dipanjan Das, Slav Petrov, Ryan M-
cDonald, and Joakim Nivre. 2013. Token and Type
Constraints for Cross-Lingual Part-of-Speech Tag-
ging. In Transactions of the Association for Compu-
tational Linguistics, 1:1-12.
175
Kun Wang, Chengqing Zong, and Keh-Yih Su. 2010.
A Character-Based Joint Model for Chinese Word
Segmentation. In Proceedings of COLING, pages
1173-1181, Bejing, China.
Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Torisawa.
2011. Improving Chinese word segmentation and
POS tagging with semi-supervised methods using
large auto-analyzed data. In Proceedings of IJC-
NLP, pages 309-317, Hyderabad, India.
Jia Xu, Jianfeng Gao, Kristina Toutanova and Her-
mann Ney. 2008. Bayesian semi-supervised chinese
word segmentation for statistical machine transla-
tion. In Proceedings of COLING, pages 1017-1024,
Manchester, UK.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. Computational Linguistics and
Chinese Language Processing, 8(1):29-48.
Yue Zhang and Stephen Clark. 2007. Chinese segmen-
tation using a word-based perceptron algorithm. In
Proceedings of ACL, pages 840-847, Prague, Czech
Republic.
Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and POS-tagging using
a single discriminative model. In Proceedings of
EMNLP, pages 843-852, Massachusetts, USA.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105-151.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2006. Effective tag set selection in Chinese
word segmentation via conditional random field
modeling. In Proceedings of PACLIC, pages 87-94,
Wuhan, China.
Ciyou Zhu, Richard H. Byrd, Peihuang Lu, and Jorge
Nocedal. 2006. L-BFGS-B: Fortran subroutines for
large scale bound constrained optimization. ACM
Transactions on Mathematical Software, 23:550-
560.
176
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1360?1369,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Toward Better Chinese Word Segmentation for SMT via Bilingual
Constraints
Xiaodong Zeng
?
Lidia S. Chao
?
Derek F. Wong
?
Isabel Trancoso
?
Liang Tian
?
?
NLP
2
CT Lab / Department of Computer and Information Science, University of Macau
?
INESC-ID / Instituto Superior T?enico, Lisboa, Portugal
nlp2ct.samuel@gmail.com, {lidiasc, derekfw}@umac.mo,
isabel.trancoso@inesc-id.pt,tianliang0123@gmail.com
Abstract
This study investigates on building a
better Chinese word segmentation mod-
el for statistical machine translation. It
aims at leveraging word boundary infor-
mation, automatically learned by bilin-
gual character-based alignments, to induce
a preferable segmentation model. We
propose dealing with the induced word
boundaries as soft constraints to bias the
continuous learning of a supervised CRF-
s model, trained by the treebank data (la-
beled), on the bilingual data (unlabeled).
The induced word boundary information
is encoded as a graph propagation con-
straint. The constrained model induction
is accomplished by using posterior reg-
ularization algorithm. The experiments
on a Chinese-to-English machine transla-
tion task reveal that the proposed model
can bring positive segmentation effects to
translation quality.
1 Introduction
Word segmentation is regarded as a critical pro-
cedure for high-level Chinese language process-
ing tasks, since Chinese scripts are written in con-
tinuous characters without explicit word bound-
aries (e.g., space in English). The empirical works
show that word segmentation can be beneficial to
Chinese-to-English statistical machine translation
(SMT) (Xu et al, 2005; Chang et al, 2008; Zhao
et al, 2013). In fact most current SMT models
assume that parallel bilingual sentences should be
segmented into sequences of tokens that are meant
to be ?words? (Ma and Way, 2009). The practice
in state-of-the-art MT systems is that Chinese sen-
tences are tokenized by a monolingual supervised
word segmentation model trained on the hand-
annotated treebank data, e.g., Chinese treebank
(CTB) (Xue et al, 2005). These models are con-
ducive to MT to some extent, since they common-
ly have relatively good aggregate performance and
segmentation consistency (Chang et al, 2008).
But one outstanding problem is that these mod-
els may leave out some crucial segmentation fea-
tures for SMT, since the output words conform to
the treebank segmentation standard designed for
monolingually linguistic intuition, rather than spe-
cific to the SMT task.
In recent years, a number of works (Xu et al,
2005; Chang et al, 2008; Ma and Way, 2009;
Xi et al, 2012) attempted to build segmentation
models for SMT based on bilingual unsegment-
ed data, instead of monolingual segmented data.
They proposed to learn gainful bilingual knowl-
edge as golden-standard segmentation supervi-
sions for training a bilingual unsupervised mod-
el. Frequently, the bilingual knowledge refers to
the mappings of an individual English word to one
or more consecutive Chinese characters, generat-
ed via statistical character-based alignment. They
leverage such mappings to either constitute a Chi-
nese word dictionary for maximum-matching seg-
mentation (Xu et al, 2004), or form labeled data
for training a sequence labeling model (Paul et al,
2011). The prior works showed that these models
help to find some segmentations tailored for SMT,
since the bilingual word occurrence feature can be
captured by the character-based alignment (Och
and Ney, 2003). However, these models tend to
miss out other linguistic segmentation patterns as
monolingual supervised models, and suffer from
the negative effects of erroneously alignments to
word segmentation.
This paper proposes an alternative Chinese
Word Segmentation (CWS) model adapted to the
SMT task, which seeks not only to maintain the
advantages of a monolingual supervised model,
having hand-annotated linguistic knowledge, but
also to assimilate the relevant bilingual segmenta-
1360
tion nature. We propose leveraging the bilingual
knowledge to form learning constraints that guide
a supervised segmentation model toward a better
solution for SMT. Besides the bilingual motivat-
ed models, character-based alignment is also em-
ployed to achieve the mappings of the successive
Chinese characters and the target language word-
s. Instead of directly merging the characters in-
to concrete segmentations, this work attempts to
extract word boundary distributions for character-
level trigrams (types) from the ?chars-to-word?
mappings. Furthermore, these word boundaries
are encoded into a graph propagation (GP) expres-
sion, in order to widen the influence of the induced
bilingual knowledge among Chinese texts. The G-
P expression constrains similar types having ap-
proximated word boundary distributions. Crucial-
ly, the GP expression with the bilingual knowledge
is then used as side information to regularize a
CRFs (conditional random fields) model?s learn-
ing over treebank and bitext data, based on the
posterior regularization (PR) framework (Ganchev
et al, 2010). This constrained learning amounts to
a jointly coupling of GP and CRFs, i.e., integrating
GP into the estimation of a parametric structural
model.
This paper is structured as follows: Section 2
points out the main differences with the related
works of this study. Section 3 presents the de-
tails of the proposed segmentation model. Section
4 reports the experimental results of the proposed
model for a Chinese-to-English MT task. The con-
clusion is drawn in Section 5.
2 Related Work
In the literature, many approaches have been pro-
posed to learn CWS models for SMT. They can
be put into two categories, monolingual-motivated
and bilingual-motivated. The former primarily op-
timizes monolingual supervised models according
to some predefined segmentation properties that
are manually summarized from empirical MT e-
valuations. Chang et al (2008) enhanced a CRF-
s segmentation model in MT tasks by tuning the
word granularity and improving the segmentation
consistence. Zhang et al (2008) produced a bet-
ter segmentation model for SMT by concatenat-
ing various corpora regardless of their differen-
t specifications. Distinct from their behaviors,
this work uses automatically learned constraints
instead of manually defined ones. Most impor-
tantly, the constraints have a better learning guid-
ance since they originate from the bilingual texts.
On the other hand, the bilingual-motivated CWS
models typically rely on character-based align-
ments to generate segmentation supervisions. Xu
et al (2004) proposed to employ ?chars-to-word?
alignments to generate a word dictionary for max-
imum matching segmentation in SMT task. The
works in (Ma and Way, 2009; Zhao et al, 2013)
extended the dictionary extraction strategy. Ma
and Way (2009) adopted co-occurrence frequency
metric to iteratively optimize ?candidate words?
extract from the alignments. Zhao et al (2013) at-
tempted to find an optimal subset of the dictionary
learned by the character-based alignment to maxi-
mize the MT performance. Paul et al (2011) used
the words learned from ?chars-to-word? align-
ments to train a maximum entropy segmentation
model. Rather than playing the ?hard? uses of
the bilingual segmentation knowledge, i.e., direct-
ly merging ?char-to-word? alignments to words
as supervisions, this study extracts word bound-
ary information of characters from the alignments
as soft constraints to regularize a CRFs model?s
learning.
The graph propagation (GP) technique provides
a natural way to represent data in a variety of tar-
get domains (Belkin et al, 2006). In this tech-
nique, the constructed graph has vertices consist-
ing of labeled and unlabeled examples. Pairs of
vertices are connected by weighted edges encod-
ing the degree to which they are expected to have
the same label (Zhu et al, 2003). Many recent
works, such as by Subramanya et al (2010), Das
and Petrov (2011), Zeng et al (2013; 2014) and
Zhu et al (2014), proposed GP for inferring the la-
bel information of unlabeled data, and then lever-
age these GP outcomes to learn a semi-supervised
scalable model (e.g., CRFs). These approaches are
referred to as pipelined learning with GP. This s-
tudy also works with a similarity graph, encoding
the learned bilingual knowledge. But, unlike the
prior pipelined approaches, this study performs a
joint learning behavior in which GP is used as a
learning constraint to interact with the CRFs mod-
el estimation.
One of our main objectives is to bias CRF-
s model?s learning on unlabeled data, under a
non-linear GP constraint encoding the bilingual
knowledge. This is accomplished by the poste-
rior regularization (PR) framework (Ganchev et
1361
al., 2010). PR performs regularization on poste-
riors, so that the learned model itself remains sim-
ple and tractable, while during learning it is driven
to obey the constraints through setting appropriate
parameters. The closest prior study is constrained
learning, or learning with prior knowledge. Chang
et al (2008) described constraint driven learning
(CODL) that augments model learning on unla-
beled data by adding a cost for violating expec-
tations of constraint features designed by domain
knowledge. Mann and McCallum (2008) and M-
cCallum et al (2007) proposed to employ gener-
alized expectation criteria (GE) to specify prefer-
ences about model expectations in the form of lin-
ear constraints on some feature expectations.
3 Methodology
This work aims at building a CWS model adapted
to the SMT task. The model induction is shown in
Algorithm 1. The input data requires two type-
s of training resources, segmented Chinese sen-
tences from treebank D
c
l
and parallel unsegment-
ed sentences of Chinese and foreign language D
c
u
and D
f
u
. The first step is to conduct character-
based alignment over bitexts D
c
u
and D
f
u
, where
every Chinese character is an alignment target.
Here, we are interested on n-to-1 alignment pat-
terns, i.e., one target word is aligned to one or
more source Chinese characters. The second step
aims to collect word boundary distributions for al-
l types, i.e., character-level trigrams, according to
the n-to-1 mappings (Section 3.1). The third step
is to encode the induced word boundary informa-
tion into a k-nearest-neighbors (k-NN) similarity
graph constructed over the entire set of types from
D
c
l
and D
c
u
(Section 3.2). The final step trains a
discriminative sequential labeling model, condi-
tional random fields, on D
c
l
and D
c
u
under bilin-
gual constraints in a graph propagation expression
(Section 3.3). This constrained learning is carried
out based on posterior regularization (PR) frame-
work (Ganchev et al, 2010).
3.1 Word Boundaries Learned from
Character-based Alignments
The gainful supervisions toward a better segmen-
tation solution for SMT are naturally extracted
from MT training resources, i.e., bilingual parallel
data. This study employs an approximated method
introduced in (Xu et al, 2004; Ma and Way, 2009;
Chung and Gildea, 2009) to learn bilingual seg-
Algorithm 1 CWS model induction with bilingual
constraints
Require:
Segmented Chinese sentences from treebank
D
c
l
; Parallel sentences of Chinese and foreign
language D
c
u
and D
f
u
Ensure:
?: the CRFs model parameters
1: D
c?f
? char align bitext (D
c
u
,D
f
u
)
2: r ? learn word bound (D
c?f
)
3: G ? encode graph constraint (D
c
l
,D
c
u
, r)
4: ? ? pr crf graph (D
c
l
,D
c
u
,G)
mentation knowledge. This relies on statistical
character-based alignment: first, every Chinese
character in the bitexts is divided by a white s-
pace so that individual characters are regarded as
special ?words? or alignment targets, and second,
they are connected with English words by using
a statistical word aligner, e.g., GIZA++ (Och and
Ney, 2003). Note that the aligner is restricted to
use an n-to-1 alignment pattern. The primary idea
is that consecutive Chinese characters are grouped
to a candidate word, if they are aligned to the same
foreign word. It is worth mentioning that prior
works presented a straightforward usage for can-
didate words, treating them as golden segmenta-
tions, either dictionary units or labeled resources.
But this study treats the induced candidate word-
s in a different way. We propose to extract the
word boundary distributions
1
for character-level
trigrams (type)
2
, as shown in Figure 1, instead of
the very specific words. There are two main rea-
sons to do so. First, it is a more general expression
which can reduce the impact amplification of er-
roneous character alignments. Second, boundary
distributions can play more flexible roles as con-
straints over labelings to bias the model learning.
The type-level word boundary extraction is for-
mally described as follows. Given the ith sen-
tence pair ?x
c
i
, x
f
i
,A
c?f
i
? of the aligned bilin-
gual corpus D
c?f
, the Chinese sentence x
c
i
con-
sisting of m characters {x
c
i,1
, x
c
i,2
, ..., x
c
i,m
}, and
the foreign language sentence x
f
i
, consisting of
1
The distribution is on four word boundary labels indi-
cating the character positions in a word, i.e., B (begin), M
(middle), E (end) and S (single character).
2
A word boundary distribution corresponds to the center
character of a type. In fact, it aims at reducing label ambi-
guities to collect boundary information of character trigrams,
rather than individual characters (Altun et al, 2006).
1362
n words {x
f
i,1
, x
f
i,2
, ..., x
f
i,n
}, A
c?f
i
represents a
set of alignment pairs a
j
= ?C
j
, x
f
i,j
? that de-
fines connections between a few Chinese char-
acters C
j
= {x
c
i,j
1
, x
c
i,j
2
, ..., x
c
i,j
k
} and a sin-
gle foreign word x
f
i,j
. For an alignment a
j
=
?C
j
, x
f
i,j
?, only the sequence of characters C
j
=
{x
c
i,j
1
, x
c
i,j
2
, ..., x
c
i,j
k
} ?d ? [1, k?1], j
d+1
? j
d
=
1 constitutes a valid candidate word. For the w-
hole bilingual corpus, we assign each character
in the candidate words with a word boundary tag
T ? {B,M,E, S}, and then count across the en-
tire corpus to collect the tag distributions r
i
=
{r
i,t
; t ? T} for each type x
c
i,j?1
x
c
i,j
x
c
i,j+1
.
???
??
Beijin
g   Ol
ympu
s
Chara
cter-b
ased a
lignm
ent
???
??
BE 
   B   
M   E
Beijin
g   Ol
ympu
s
Word
 boun
daries
??? ??? ? Type-level W
ord 
bound
ary di
stribu
tions
BeiP
ing S
hi ???
BeiJi
ng Re
n ??? BeiJing
 Di ???
Quan
Yun H
ui ???
BeiJi
ng Sh
i ???
0.8 0
.6
0.3
0.2
0.9
AoYu
n Hui ??? 0.2
Figure 1: An example of similarity graph over
character-level trigrams (types).
3.2 Constraints Encoded by Graph
Propagation Expression
The previous step contributes to generate bilingual
segmentation supervisions, i.e., type-level word
boundary distributions. An intuitive manner is to
directly leverage the induced boundary distribu-
tions as label constraints to regularize segmenta-
tion model learning, based on a constrained learn-
ing algorithm. This study, however, makes further
efforts to elevate the positive effects of the bilin-
gual knowledge via the graph propagation tech-
nique. We adopt a similarity graph to encode
the learned type-level word boundary distribution-
s. The GP expression will be defined as a PR con-
straint in Section 3.3 that reflects the interactions
between the graph and the CRFs model. In other
words, GP is integrated with estimation of para-
metric structural model. This is greatly different
from the prior pipelined approaches (Subramanya
et al, 2010; Das and Petrov, 2011; Zeng et al,
2013), where GP is run first and its propagated
outcomes are then used to bias the structural mod-
el. This work seeks to capture the GP benefits dur-
ing the modeling of sequential correlations.
In what follows, the graph setting and propa-
gation expression are introduced. As in conven-
tional GP examples (Das and Smith, 2012), a sim-
ilarity graph G = (V,E) is constructed over N
types extracted from Chinese training data, includ-
ing treebank D
c
l
and bitexts D
c
u
. Each vertex V
i
has a |T |-dimensional estimated measure v
i
=
{v
i,t
; t ? T} representing a probability distribu-
tion on word boundary tags. The induced type-
level word boundary distributions r
i
= {r
i,t
; t ?
T} are empirical measures for the corresponding
M graph vertices. The edges E ? V
i
?V
j
connect
all the vertices. Scores between pairs of graph ver-
tices (types), w
ij
, refer to the similarities of their
syntactic environment, which are computed fol-
lowing the method in (Subramanya et al, 2010;
Das and Petrov, 2011; Zeng et al, 2013). The
similarities are measured based on co-occurrence
statistics over a set of predefined features (intro-
duced in Section 4.1). Specifically, the point-wise
mutual information (PMI) values, between ver-
tices and each feature instantiation that they have
in common, are summed to sparse vectors, and
their cosine distances are computed as the sim-
ilarities. The nature of this similarity graph en-
forces that the connected types with high weight-
s appearing in different texts should have similar
word boundary distributions.
The quality (smoothness) of the similarity graph
can be estimated by using a standard propagation
function, as shown in Equation 1. The square-loss
criterion (Zhu et al, 2003; Bengio et al, 2006) is
used to formulate this function:
P(v) =
T
?
t=1
(
M
?
i=1
(v
i,t
? r
i,t
)
2
+?
N
?
j=1
N
?
i=1
w
ij
(v
i,t
? v
j,t
)
2
+ ?
N
?
i=1
(v
i,t
)
2
)
(1)
The first term in this equation refers to seed match-
es that compute the distances between the estimat-
ed measure v
i
and the empirical probabilities r
i
.
The second term refers to edge smoothness that
measures how vertices v
i
are smoothed with re-
spect to the graph. Two types connected by an
edge with high weight should be assigned similar
word boundary distributions. The third term, a `
2
norm, evaluates the distribution sparsity (Das and
1363
Smith, 2012) per vertex. Typically, the GP process
amounts to an optimization process with respect
to parameter v such that Equation 1 is minimized.
This propagation function can be used to reflect
the graph smoothness, where the higher the score,
the lower the smoothness.
3.3 PR Learning with GP Constraint
Our learning problem belongs to semi-supervised
learning (SSL), as the training is done on treebank
labeled data (X
L
,Y
L
) = {(x
1
, y
1
), ..., (x
l
, y
l
)},
and bilingual unlabeled data (X
U
) = {x
1
, ..., x
u
}
where x
i
= {x
1
, ..., x
m
} is an input word se-
quence and y
i
= {y
1
, ..., y
m
}, y ? T is its corre-
sponding label sequence. Supervised linear-chain
CRFs can be modeled in a standard conditional
log-likelihood objective with a Gaussian prior:
L(?) = p
?
(y
i
|x
i
)?
???
2
2?
(2)
The conditional probabilities p
?
are expressed as a
log-linear form:
p
?
(y
i
|x
i
) =
exp(
m
?
k=1
?
T
f(y
k?1
i
, y
k
i
, x
i
))
Z
?
(x
i
)
(3)
Where Z
?
(x
i
) is a partition function that normal-
izes the exponential form to be a probability dis-
tribution, and f(y
k?1
i
, y
k
i
, x
i
) are arbitrary feature
functions.
In our setting, the CRFs model is required
to learn from unlabeled data. This work em-
ploys the posterior regularization (PR) frame-
work
3
(Ganchev et al, 2010) to bias the CRFs
model?s learning on unlabeled data, under a con-
straint encoded by the graph propagation expres-
sion. It is expected that similar types in the graph
should have approximated expected taggings un-
der the CRFs model. We follow the approach in-
troduced by (He et al, 2013) to set up a penalty-
based PR objective with GP: the CRFs likelihood
is modified by adding a regularization term, as
shown in Equation 4, representing the constraints:
R
U
(?, q) = KL(q||p
?
) + ?P(v) (4)
Rather than regularize CRFs model?s posteriors
p
?
(Y|x
i
) directly, our model uses an auxiliary
distribution q(Y|x
i
) over the possible labelings
3
The readers are refered to the original paper of Ganchev
et al (2010).
Y for x
i
, and penalizes the CRFs marginal log-
likelihood by a KL-divergence term
4
, represent-
ing the distance between the estimated posteriors
p and the desired posteriors q, as well as a penal-
ty term, formed by the GP function. The hy-
perparameter ? is used to control the impacts of
the penalty term. Note that the penalty is fired
if the graph score computed based on the expect-
ed taggings given by the current CRFs model is
increased vis-a-vis the previous training iteration.
This nature requires that the penalty term P(v)
should be formed as a function of posteriors q over
CRFs model predictions
5
, i.e., P(q). To state this,
a mappingM : ({1, ..., u}, {1, ...,m})? V from
words in the corpus to vertices in the graph is de-
fined. We can thus decompose v
i,t
into a function
of q as follows:
v
i,t
=
u?
a=1
m?
b=1;
M(a,b)=V
i
T?
c=1
?
y?Y
1(y
b
= t, y
b?1
= c)q(y|x
a
)
u?
a=1
m?
b=1
1(M(a, b) = V
i
)
(5)
The final learning objective combines the CRF-
s likelihood with the PR regularization term:
J (?, q) = L(?) + R
U
(?, q). This joint objec-
tive, over ? and q, can be optimized by an expecta-
tion maximization (EM) style algorithm as report-
ed in (Ganchev et al, 2010). We start from ini-
tial parameters ?
0
, estimated by supervised CRFs
model training on treebank data. The E-step is to
minimize R
U
(?, q) over the posteriors q that are
constrained to the probability simplex. Since the
penalty term P(v) is a non-linear form, the opti-
mization method in (Ganchev et al, 2010) via pro-
jected gradient descent on the dual is inefficient
6
.
This study follows the optimization method (He et
al., 2013) that uses exponentiated gradient descent
(EGD) algorithm. It allows that the variable up-
date expression, as shown in Equation 6, takes a
multiplicative rather than an additive form.
q
(w+1)
(y|x
i
) = q
(w)
(y|x
i
) exp(??
?R
?q
(w)
(y|x
i
)
)
(6)
where the parameter ? controls the optimization
rate in the E-step. With the contributions from
4
The form of KL term: KL(q||p) =
?
q?Y
q(y) log
q(y)
p(y)
.
5
The original PR setting also requires that the penalty ter-
m should be a linear (Ganchev et al, 2010) or non-linear (He
et al, 2013) function on q.
6
According to (He et al, 2013), the dual of quadratic pro-
gram implies an expensive matrix inverse.
1364
the E-step that further encourage q and p to agree,
the M-step aims to optimize the objective J (?, q)
with respect to ?. The M-step is similar to the stan-
dard CRFs parameter estimation, where the gradi-
ent ascent approach still works. This EM-style ap-
proach monotonically increases J (?, q) and thus
is guaranteed to converge to a local optimum.
E-step: q
(t+1)
= argmin
q
R
U
(?
(t)
, q
(t)
)
M-step: ?
(t+1)
= argmax
?
L(?)
+?
u
?
i=1
?
y?Y
q
(t+1)
(y|x
i
) log p
?
(y|x
i
)
(7)
4 Experiments
4.1 Data and Setup
The experiments in this study evaluated the per-
formances of various CWS models in a Chinese-
to-English translation task. The influence of
the word segmentation on the final translation
is our main investigation. We adopted three
state-of-the-art metrics, BLEU (Papineni et al,
2002), NIST (Doddington et al, 2000) and ME-
TEOR (Banerjee and Lavie, 2005), to evaluate the
translation quality.
The monolingual segmented data, train
TB
, is
extracted from the Penn Chinese Treebank (CTB-
7) (Xue et al, 2005), containing 51,447 sentences.
The bilingual training data, train
MT
, is formed
by a large in-house Chinese-English parallel cor-
pus (Tian et al, 2014). There are in total 2,244,319
Chinese-English sentence pairs crawled from on-
line resources, concentrated in 5 different domains
including laws, novels, spoken, news and miscel-
laneous
7
. This in-house bilingual corpus is the
MT training data as well. The target-side lan-
guage model is built on over 35 million mono-
lingual English sentences, train
LM
, crawled from
online resources. The NIST evaluation campaign
data, MT-03 and MT-05, are selected to comprise
the MT development data, dev
MT
, and testing da-
ta, test
MT
, respectively.
For the settings of our model, we adopted the
standard feature templates introduced by Zhao et
al. (2006) for CRFs. The character-based align-
ment for achieving the ?chars-to-word? mappings
is accomplished by GIZA++ aligner (Och and
Ney, 2003). For the GP, a 10-NNs similarity graph
7
The in-house corpus has been manually validated, in a
long process that exceeded 500 hours.
was constructed
8
. Following (Subramanya et al,
2010; Zeng et al, 2013), the features used to
compute similarities between vertices were (Sup-
pose given a type ?w
2
w
3
w
4
? surrounding contexts
?w
1
w
2
w
3
w
4
w
5
?): unigram (w
3
), bigram (w
1
w
2
,
w
4
w
5
, w
2
w
4
), trigram (w
2
w
3
w
4
, w
2
w
4
w
5
,
w
1
w
2
w
4
), trigram+context (w
1
w
2
w
3
w
4
w
5
) and
character classes in number, punctuation, alpha-
betic letter and other (t(w
2
)t(w
3
)t(w
4
)). There
are four hyperparameters in our model to be tuned
by using the development data (dev
MT
) among
the following settings: for the graph propagation,
? ? {0.2, 0.5, 0.8} and ? ? {0.1, 0.3, 0.5, 0.8};
for the PR learning, ? ? {0 ? ?
i
? 1} and ? ?
{0 ? ?
i
? 1} where the step is 0.1. The best per-
formed joint settings, ? = 0.5, ? = 0.5, ? = 0.9
and ? = 0.8, were used to measure the final per-
formance.
The MT experiment was conducted based on
a standard log-linear phrase-based SMT model.
The GIZA++ aligner was also adopted to obtain
word alignments (Och and Ney, 2003) over the
segmented bitexts. The heuristic strategy of grow-
diag-final-and (Koehn et al, 2007) was used to
combine the bidirectional alignments for extract-
ing phrase translations and reordering tables. A
5-gram language model with Kneser-Ney smooth-
ing was trained with SRILM (Stolcke, 2002) on
monolingual English data. Moses (Koehn et al,
2007) was used as decoder. The Minimum Error
Rate Training (MERT) (Och, 2003) was used to
tune the feature parameters on development data.
4.2 Various Segmentation Models
To provide a thorough analysis, the MT experi-
ments in this study evaluated three baseline seg-
mentation models and two off-the-shelf models,
in addition to four variant models that also employ
the bilingual constraints. We start from three base-
line models:
? Character Segmenter (CS): this model sim-
ply divides Chinese sentences into sequences
of characters.
? Supervised Monolingual Segmenter (SM-
S): this model is trained by CRFs on treebank
training data (train
TB
). The same feature
templates (Zhao et al, 2006) are used. The
standard four-tags (B, M, E and S) were used
8
We evaluated graphs with top k (from 3 to 20) nearest
neighbors on development data, and found that the perfor-
mance converged beyond 10-NNs.
1365
as the labels. The stochastic gradient descent
is adopted to optimize the parameters.
? Unsupervised Bilingual Segmenter (UBS):
this model is trained on the bitexts (trainMT)
following the approach introduced in (Ma
and Way, 2009). The optimal set of the mod-
el parameter values was found on dev
MT
to
be k = 3, t
AC
= 0.0 and t
COOC
= 15.
The comparison candidates also involve two pop-
ular off-the-shelf segmentation models:
? Stanford Segmenter: this model, trained by
Chang et al (2008), treats CWS as a binary
word boundary decision task. It covers sev-
eral features specific to the MT task, e.g., ex-
ternal lexicons and proper noun features.
? ICTCLAS Segmenter: this model, trained
by Zhang et al (2003), is a hierarchical
HMM segmenter that incorporates parts-of-
speech (POS) information into the probabili-
ty models and generates multiple HMM mod-
els for solving segmentation ambiguities.
This work also evaluated four variant models
9
that perform alternative ways to incorporate the
bilingual constraints based on two state-of-the-art
graph-based SSL approaches.
? Self-training Segmenters (STS): two vari-
ant models were defined by the approach re-
ported in (Subramanya et al, 2010) that us-
es the supervised CRFs model?s decodings,
incorporating empirical and constraint infor-
mation, for unlabeled examples as additional
labeled data to retrain a CRFs model. One
variant (STS-NO-GP) skips the GP step, di-
rectly decoding with type-level word bound-
ary probabilities induced from bitexts, while
the other (STS-GP-PL) runs the GP at first
and then decodes with GP outcomes. The
optimal hyperparameter values were found to
be: STS-NO-GP (? = 0.8) and ? = 0.6) and
STS-GP-PL (? = 0.5, ? = 0.3, ? = 0.8 and
? = 0.6).
? Virtual Evidences Segmenters (VES): T-
wo variant models based on the approach
in (Zeng et al, 2013) were defined. The type-
level word boundary distributions, induced
9
Note that there are two variant models working with GP.
To be fair, the same similarity graph settings introduced in
this paper were used.
by the character-based alignment (VES-NO-
GP), and the graph propagation (VES-GP-
PL), are regarded as virtual evidences to bias
CRFs model?s learning on the unlabeled da-
ta. The optimal hyperparameter values were
found to be: VES-NO-GP (? = 0.7) and
VES-GP-PL (? = 0.5, ? = 0.3 and ? = 0.7).
4.3 Main Results
Table 1 summarizes the final MT performance on
the MT-05 test data, evaluated with ten different
CWS models. In what follows, we summarized
four major observations from the results. First-
ly, as expected, having word segmentation does
help Chinese-to-English MT. All other nine CWS
models outperforms the CS baseline which does
not try to identify Chinese words at all. Second-
ly, the other two baselines, SMS and UBS, are on
a par with each other, showing less than 0.36 av-
erage performance differences on the three eval-
uation metrics. This outcome validated that the
models, trained by either the treebank or the bilin-
gual data, performed reasonably well. But they
only capture partial segmentation features so that
less gains for SMT are achieved when compar-
ing to other sophisticated models. Thirdly, we no-
tice that the two off-the-shelf models, Stanford and
ICTCLAS, just brought minor improvements over
the SMS baseline, although they are trained us-
ing richer supervisions. This behaviour illustrates
that the conventional optimizations to the mono-
lingual supervised model, e.g., accumulating more
supervised data or predefined segmentation prop-
erties, are insufficient to help model for achiev-
ing better segmentations for SMT. Finally, high-
lighting the five models working with the bilingual
constraints, most of them can achieve significant
gains over the other ones without using the bilin-
gual constraints. This strongly demonstrates that
bilingually-learned segmentation knowledge does
helps CWS for SMT. The models working with G-
P, STS-GP-PL, VES-GP-PL and ours outperform
all others. We attribute this to the role of GP in
assisting the spread of bilingual knowledge on the
Chinese side. Importantly, it can be observed that
our model outperforms STS-GP, VES-GP, which
greatly supports that joint learning of CRFs and
GP can alleviate the error transfer by the pipelined
models. This is one of the most crucial findings
in this study. Overall, the boldface numbers in the
last row illustrate that our model obtains average
improvements of 1.89, 1.76 and 1.61 on BLEU,
1366
NIST and METEOR over others.
Models BLEU NIST METEOR
CS 29.38 59.85 54.07
SMS 30.05 61.33 55.95
UBS 30.15 61.56 55.39
Stanford 30.40 61.94 56.01
ICTCLAS 30.29 61.26 55.72
STS-NO-GP 31.47 62.35 56.12
STS-GP-PL 31.94 63.20 57.09
VES-NO-GP 31.98 62.63 56.59
VES-GP-PL 32.04 63.49 57.34
Our Model 32.75 63.72 57.64
Table 1: Translation performances (%) on MT-05
testing data by using ten different CWS models.
4.4 Analysis & Discussion
This section aims to further analyze the three pri-
mary observations concluded in Section 4.3: i)
word segmentation is useful to SMT; ii) the tree-
bank and the bilingual segmentation knowledge
are helpful, performing segmentation of differen-
t nature; and iii) the bilingual constraints lead to
learn segmentations better tailored for SMT.
The first observation derives from the compar-
isons between the CS baseline and other model-
s. Our results, showing the significant CWS ben-
efits to SMT, are consistent with the works re-
ported in the literature (Xu et al, 2004; Chang
et al, 2008). In our experiment, two additional
evidences found in the translation model are pro-
vided to further support that NO tokenization of
Chinese (i.e., the CS model?s output) could har-
m the MT system. First, the SMT phrase extrac-
tion, i.e., building ?phrases? on top of the char-
acter sequences, cannot fully capture all meaning-
ful segmentations produced by the CS model. The
character based model leads to missing some use-
ful longer phrases, and to generate many meaning-
less or redundant translations in the phrase table.
Moreover, it is affected by translation ambiguities,
caused by the cases where a Chinese character has
very different meanings in different contextual en-
vironments.
The second observation shifts the emphasis to
SMS and UBS, based on the treebank and the
bilingual segmentation, respectively. Our result-
s show that both segmentation patterns can bring
positive effects to MT. Through analyzing both
models? segmentations for train
MT
and test
MT
,
we attempted to get a closer inspection on the seg-
mentation preferences and their influence on MT.
Our first finding is that the segmentation consen-
suses between SMS and UBS are positive to MT.
There have about 35% identical segmentations
produced by the two models. If these identical
segmentations are removed, and the experiments
are rerun, the translation scores decrease (on av-
erage) by 0.50, 0.85 and 0.70 on BLEU, NIST
and METEOR, respectively. Our second finding
is that SMS exhibits better segmentation consis-
tency than UBS. One representative example is the
segmentations for ???? (lonely)?. All the out-
puts of SMS were ?????, while UBS generat-
ed three ambiguous segmentations, ??(alone) ?
?(double zero)?, ???(lonely) ?(zero)? and
??(alone) ?(zero) ?(zero)?. The segmentation
consistency of SMS rests on the high-quality tree-
bank data and the robust CRFs tagging mod-
el. On the other hand, the advantage of UB-
S is to capture the segmentations matching the
aligned target words. For example, UBS grouped
??(country) ?(border) ?(between)? to a word
????(international)?, rather than two word-
s ???(international) ?(between)? (as given by
SMS), since these three characters are aligned to
a single English word ?international?. The above
analysis shows that SMS and UBS have their own
merits and combining the knowledge derived from
both segmentations is highly encouraged.
The third observation concerns the great im-
pact of the bilingual constraints to the segmenta-
tion models in the MT task. The use of the bilin-
gual constraints is the prime objective of this s-
tudy. Our first contribution for this purpose is
on using the word boundary distributions to cap-
ture the bilingual segmentation supervisions. This
representation contributes to reduce the negative
impacts of erroneous ?chars-to-word? alignments.
The ambiguous types (having relatively uniform
boundary distribution), caused by alignment er-
rors, cannot directly bias the model tagging pref-
erences. Furthermore, the word boundary distri-
butions are convenient to make up the learning
constraints over the labelings among various con-
strained learning approaches. They have success-
fully played in three types of constraints for our
experiments: PR penalty (Our model), decoding
constraints in self-training (STS) and virtual evi-
dences (VES). The second contribution is the use
of GP, illustrated by STS-GP-PL, VES-GP-PL and
1367
Our model. The major effect is to multiply the im-
pacts of the bilingual knowledge through the sim-
ilarity graph. The graph vertices (types)
10
, with-
out any supervisions, can learn the word bound-
ary information from their similar types (neigh-
borhoods) having the empirical boundary prob-
abilities. The segmentations given by the three
GP models show about 70% positive segmenta-
tion changes, affected by the unlabeled graph ver-
tices, with respect to the ones given by the NO-
GP models, STS-NO-GP and VES-NO-GP. In our
opinion, the learning mechanism of our approach,
joint coupling of GP and CRFs, rather than the
pipelined one as the other two models, contributes
to maximizing the graph smoothness effects to the
CRFs estimation so that the error propagation of
the pipelined approaches is alleviated.
5 Conclusion
This paper proposed a novel CWS model for the
SMT task. This model aims to maintain the lin-
guistic segmentation supervisions from treebank
data and simultaneously integrate useful bilingual
segmentations induced from the bitexts. This ob-
jective is accomplished by three main steps: 1)
learn word boundaries from character-based align-
ments; 2) encode the learned word boundaries into
a GP constraint; and 3) training a CRFs model, un-
der the GP constraint, by using the PR framework.
The empirical results indicate that the proposed
model can yield better segmentations for SMT.
Acknowledgments
The authors are grateful to the Science and
Technology Development Fund of Macau and
the Research Committee of the University of
Macau (Grant No. MYRG076 (Y1-L2)-FST13-
WF and MYRG070 (Y1-L2)-FST12-CS) for the
funding support for our research. The work of
Isabel Trancoso was supported by national funds
through FCT-Fundac??ao para a Ci?ecia e a Tecnolo-
gia, under project PEst-OE/EEI/LA0021/2013.
The authors also wish to thank the anonymous re-
viewers for many helpful comments.
10
This experiment yielded a similarity graph that consists
of 11,909,620 types from train
TB
and train
MT
, where there
have 8,593,220 (72.15%) types without any empirical bound-
ary distributions.
References
Yasemin Altun, David McAllester, and Mikhail Belkin.
2006. Maximum margin semi-supervised learning
for structured variables. Advances in Neural Infor-
mation Processing Systems, 18:33.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
correlation with human judgments. In Proceedings
of the ACL Workshop on Intrinsic and Extrinsic E-
valuation Measures for Machine Translation and/or
Summarization, pages 65?72. Association for Com-
putational Linguistics.
Yoshua Bengio, Olivier Delalleau, and Nicolas
Le Roux. 2006. Label propagation and quadrat-
ic criterion. Semi-Supervised Learning, pages 193?
216.
Pi-Chuan Chang, Michel Galley, and Christopher D
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of WMT, pages 224?232. Association for
Computational Linguistics.
Tagyoung Chung and Daniel Gildea. 2009. Unsuper-
vised tokenization for machine translation. In Pro-
ceedings of EMNLP, pages 718?726. Association
for Computational Linguistics.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of ACL, pages 600?609.
Association for Computational Linguistics.
Dipanjan Das and Noah A Smith. 2012. Graph-based
lexicon expansion with sparsity-inducing penalties.
In Proceedings of NAACL, pages 677?687. Associa-
tion for Computational Linguistics.
George R. Doddington, Mark A. Przybocki, Alvin F.
Martin, and Douglas A. Reynolds. 2000. The nist
speaker recognition evaluation?overview, methodol-
ogy, systems, results, perspective. Speech Commu-
nication, 31(2):225?254.
Kuzman Ganchev, J?oao Grac?a, Jennifer Gillenwater,
and Ben Taskar. 2010. Posterior regularization for
structured latent variable models. The Journal of
Machine Learning Research, 11:2001?2049.
Luheng He, Jennifer Gillenwater, and Ben Taskar.
2013. Graph-based posterior regularization for
semi-supervised structured prediction. In Proceed-
ings of CoNLL, page 38. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertol-
di, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of ACL on Interactive Poster and Demon-
stration Sessions, pages 177?180. Association for
Computational Linguistics.
1368
Yanjun Ma and Andy Way. 2009. Bilingually motivat-
ed domain-adapted word segmentation for statistical
machine translation. In Proceedings of EACL, pages
549?557. Association for Computational Linguistic-
s.
Gideon S. Mann and Andrew McCallum. 2008.
Generalized expectation criteria for semi-supervised
learning of conditional random fields. In Proceed-
ings of ACL, pages 870?878. Association for Com-
putational Linguistics.
Andrew McCallum, Gideon Mann, and Gregory
Druck. 2007. Generalized expectation criteri-
a. Computer Science Technical Note, University of
Massachusetts, Amherst, MA.
Franz Josef Och and Hermann Ney. 2003. A systemat-
ic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of A-
CL, pages 160?167. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic e-
valuation of machine translation. In Proceedings of
ACL, pages 311?318. Association for Computation-
al Linguistics.
Michael Paul, Finch Andrew, and Sumita Eiichiro.
2011. Integration of multiple bilingually-trained
segmentation schemes into statistical machine trans-
lation. IEICE Transactions on Information and Sys-
tems, 94(3):690?697.
Andreas Stolcke. 2002. SRILM-an extensible lan-
guage modeling toolkit. In Proceedings of Inter-
speech.
Amarnag Subramanya, Slav Petrov, and Fernando
Pereira. 2010. Efficient graph-based semi-
supervised learning of structured tagging models. In
Proceedings of EMNLP, pages 167?176. Associa-
tion for Computational Linguistics.
Liang Tian, Derek F. Wong, Lidia S. Chao, Paulo
Quaresma, Francisco Oliveira, Shuo Li, Yiming
Wang, and Yi Lu. 2014. UM-Corpus: A large
English-Chinese parallel corpus for statistical ma-
chine translation. In Proceedings of LREC. Euro-
pean Language Resources Association.
Ning Xi, Guangchao Tang, Xinyu Dai, Shujian Huang,
and Jiajun Chen. 2012. Enhancing statistical ma-
chine translation with character alignment. In Pro-
ceedings of ACL, pages 285?290. Association for
Computational Linguistics.
Jia Xu, Richard Zens, and Hermann Ney. 2004. Do
we need Chinese word segmentation for statistical
machine translation? In Proceedings of the Third
SIGHAN Workshop on Chinese Language Learning,
pages 122?128. Association for Computational Lin-
guistics.
Jia Xu, Evgeny Matusov, Richard Zens, and Hermann
Ney. 2005. Integrated Chinese word segmentation
in statistical machine translation. In Proceedings of
IWSLT, pages 216?223. Association for Computa-
tional Linguistics.
Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, and Is-
abel Trancoso. 2013. Graph-based semi-supervised
model for joint Chinese word segmentation and part-
of-speech tagging. In Proceedings of ACL, pages
770?779. Association for Computational Linguistic-
s.
Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, Is-
abel Trancoso, Liangye He, and Qiuping Huang.
2014. Lexicon expansion for latent variable gram-
mars. Pattern Recognition Letters, 42:47?55.
Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun
Liu. 2003. HHMM-based Chinese lexical analyzer
ICTCLAS. In Proceedings of the Second SIGHAN
Workshop on Chinese Language Processing, pages
184?187. Association for Computational Linguistic-
s.
Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.
2008. Improved statistical machine translation by
multiple Chinese word segmentation. In Proceed-
ings of WMT, pages 216?223. Association for Com-
putational Linguistics.
Hai Zhao, Chang-Ning Huang, and Mu Li. 2006. An
improved Chinese word segmentation system with
conditional random field. In Proceedings of the Fifth
SIGHAN Workshop on Chinese Language Process-
ing. Association for Computational Linguistics.
Hai Zhao, Masao Utiyama, Eiichiro Sumita, and Bao-
Liang Lu. 2013. An empirical study on word seg-
mentation for Chinese machine translation. In Com-
putational Linguistics and Intelligent Text Process-
ing, pages 248?263. Springer.
Xiaojin Zhu, Zoubin Ghahramani, and John Laffer-
ty. 2003. Semi-supervised learning using gaussian
fields and harmonic functions. In Proceedings of
ICML, volume 3, pages 912?919.
Ling Zhu, Derek F. Wong, and Lidia S. Chao. 2014.
Unsupervised chunking based on graph propagation
from bilingual corpus. The Scientific World Journal,
2014(401943):10.
1369
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 365?372,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Quality Estimation for Machine Translation Using the Joint Method 
of Evaluation Criteria and Statistical Modeling 
 
 
Aaron Li-Feng Han 
hanlifengaaron@gmail.com 
Yi Lu 
mb25435@umac.mo 
Derek F. Wong 
derekfw@umac.mo 
   
Lidia S. Chao 
lidiasc@umac.mo 
Liangye He 
wutianshui0515@gmail.com 
Junwen Xing 
mb15470@umac.mo 
    
Natural Language Processing & Portuguese-Chinese Machine Translation Laboratory 
Department of Computer and Information Science 
University of Macau, Macau S.A.R. China 
 
  
 
Abstract 
This paper is to introduce our participation in 
the WMT13 shared tasks on Quality Estima-
tion for machine translation without using ref-
erence translations. We submitted the results 
for Task 1.1 (sentence-level quality estima-
tion), Task 1.2 (system selection) and Task 2 
(word-level quality estimation). In Task 1.1, 
we used an enhanced version of BLEU metric 
without using reference translations to evalu-
ate the translation quality. In Task 1.2, we uti-
lized a probability model Na?ve Bayes (NB) as 
a classification algorithm with the features 
borrowed from the traditional evaluation met-
rics. In Task 2, to take the contextual infor-
mation into account, we employed a discrimi-
native undirected probabilistic graphical mod-
el Conditional random field (CRF), in addition 
to the NB algorithm. The training experiments 
on the past WMT corpora showed that the de-
signed methods of this paper yielded promis-
ing results especially the statistical models of 
CRF and NB. The official results show that 
our CRF model achieved the highest F-score 
0.8297 in binary classification of Task 2. 
 
1 Introduction 
Due to the fast development of Machine transla-
tion, different automatic evaluation methods for 
the translation quality have been proposed in re-
cent years. One of the categories is the lexical 
similarity based metric. This kind of metrics in-
cludes the edit distance based method, such as 
WER (Su et al, 1992), Multi-reference WER 
(Nie?en et al, 2000), PER (Tillmann et al, 
1997), the works of (Akiba, et al, 2001), 
(Leusch et al, 2006) and (Wang and Manning, 
2012); the precision based method, such as 
BLEU (Papineni et al, 2002), NIST (Doddington, 
2002), and SIA (Liu and Gildea, 2006); recall 
based method, such as ROUGE (Lin and Hovy 
2003); and the combination of precision and re-
call, such as GTM (Turian et al, 2003), METE-
OR (Lavie and Agarwal, 2007), BLANC (Lita et 
al., 2005), AMBER (Chen and Kuhn, 2011), 
PORT (Chen et al, 2012b), and LEPOR (Han et 
al., 2012). 
Another category is the using of linguistic fea-
tures. This kind of metrics includes the syntactic 
similarity, such as the POS information used by 
TESLA (Dahlmeier et al, 2011), (Liu et al, 
2010) and (Han et al, 2013), phrase information 
used by (Povlsen, et al, 1998) and (Echizen-ya 
and Araki, 2010), sentence structure used by 
(Owczarzak et al, 2007); the semantic similarity, 
such as textual entailment used by (Mirkin et al, 
2009) and (Castillo and Estrella, 2012), Syno-
nyms used by METEOR (Lavie and Agarwal, 
2007), (Wong and Kit, 2012), (Chan and Ng, 
2008); paraphrase used by (Snover et al, 2009). 
The traditional evaluation metrics tend to 
evaluate the hypothesis translation as compared 
to the reference translations that are usually of-
fered by human efforts. However, in the practice, 
there is usually no golden reference for the trans-
lated documents, especially on the internet works. 
How to evaluate the quality of automatically 
translated documents or sentences without using 
the reference translations becomes a new chal-
lenge in front of the NLP researchers. 
365
ADJ ADP ADV CONJ DET NOUN NUM PRON PRT VERB X . 
ADJ PREP, 
PREP/DEL 
ADV, 
NEG 
CC, 
CCAD, 
CCNEG, 
CQUE, 
CSUBF, 
CSUBI, 
CSUBX 
ART NC, 
NMEA, 
NMON, 
NP, 
PERCT,  
UMMX 
CARD, 
CODE, 
QU 
DM, 
INT, 
PPC, 
PPO, 
PPX, 
REL 
SE VCLIger, 
VCLIinf, 
VCLIfin, 
VEadj, 
VEfin, 
VEger, 
VEinf, 
VHadj, 
VHfin, 
VHger, 
VHinf, 
VLadj, 
VLfin, 
VLger, 
VLinf, 
VMadj, 
VMfin, 
VMger, 
VMinf, 
VSadj, 
VSfin, 
VSger, 
VSinf 
ACRNM, 
ALFP, 
ALFS, 
FO, ITJN, 
ORD, 
PAL, 
PDEL, 
PE, PNC, 
SYM 
BACKSLASH, 
CM, COLON, 
DASH, DOTS, 
FS, LP, QT, 
RP, SEMICO-
LON, SLASH 
Table 1: Developed POS mapping for Spanish and universal tagset 
 
2 Related Works 
Gamon et al (2005) perform a research about 
reference-free SMT evaluation method on sen-
tence level. This work uses both linear and non-
linear combinations of language model and SVM 
classifier to find the badly translated sentences. 
Albrecht and Hwa (2007) conduct the sentence-
level MT evaluation utilizing the regression 
learning and based on a set of weaker indicators 
of fluency and adequacy as pseudo references. 
Specia and Gimenez (2010) use the Confidence 
Estimation features and a learning mechanism 
trained on human annotations. They show that 
the developed models are highly biased by diffi-
culty level of the input segment, therefore they 
are not appropriate for comparing multiple sys-
tems that translate the same input segments. Spe-
cia et al (2010) discussed the issues between the 
traditional machine translation evaluation and the 
quality estimation tasks recently proposed. The 
traditional MT evaluation metrics require refer-
ence translations in order to measure a score re-
flecting some aspects of its quality, e.g. the 
BLEU and NIST. The quality estimation ad-
dresses this problem by evaluating the quality of 
translations as a prediction task and the features 
are usually extracted from the source sentences 
and target (translated) sentences. They also show 
that the developed methods correlate better with 
human judgments at segment level as compared 
to traditional metrics. Popovi? et al (2011) per-
form the MT evaluation using the IBM model 
one with the information of morphemes, 4-gram 
POS and lexicon probabilities. Mehdad et al 
(2012) use the cross-lingual textual entailment to 
push semantics into the MT evaluation without 
using reference translations. This evaluation 
work mainly focuses on the adequacy estimation. 
Avramidis (2012) performs an automatic sen-
tence-level ranking of multiple machine transla-
tions using the features of verbs, nouns, sentenc-
es, subordinate clauses and punctuation occur-
rences to derive the adequacy information. Other 
descriptions of the MT Quality Estimation tasks 
can be gained in the works of (Callison-Burch et 
al., 2012) and (Felice and Specia, 2012). 
3 Tasks Information  
This section introduces the different sub-tasks we 
participated in the Quality Estimation task of 
WMT 13 and the methods we used.  
3.1 Task 1-1 Sentence-level QE 
Task 1.1 is to score and rank the post-editing 
effort of the automatically translated English-
Spanish sentences without offering the reference 
translation. 
Firstly, we develop the English and Spanish 
POS tagset mapping as shown in Table 1. The 75 
Spanish POS tags yielded by the Treetagger 
(Schmid, 1994) are mapped to the 12 universal 
tags developed in (Petrov et al, 2012). The Eng-
lish POS tags are extracted from the parsed sen-
tences using the Berkeley parser (Petrov et al, 
2006). 
Secondly, the enhanced version of BLEU 
(EBLEU) formula is designed with the factors of 
modified length penalty (   ), precision, and 
recall, the   and   representing the lengths of 
hypothesis (target) sentence and source sentence 
respectively. We use the harmonic mean of pre-
cision and recall, i.e.  (       ). We assign 
the weight values     and    , i.e. higher 
weight value is assigned to precision, which is 
different with METEOR (the inverse values). 
 
       
          (?      ( (       ))) (1) 
 
     {
   
 
            
   
 
            
 (2) 
 
    
                    
                                
 (3) 
 
    
                    
                                
 (4) 
366
Lastly, the scoring for the post-editing effort 
of the automatically translated sentences is per-
formed on the extracted POS sequences of the 
source and target languages. The evaluated per-
formance of EBLEU on WMT 12 corpus is 
shown in Table 2 using the Mean-Average-Error 
(MAE), Root-Mean-Squared-Error (RMSE).  
 
 Precision Recall MLP EBLEU 
MAE 0.17 0.19 0.25 0.16 
RMSE 0.22 0.24 0.30 0.21 
Table 2: Performance on the WMT12 corpus 
The official evaluation scores of the testing re-
sults on WMT 13 corpus are shown in Table 3. 
The testing results show similar scores as com-
pared to the training scores (the MAE score is 
around 0.16 and the RMSE score is around 0.22), 
which shows a stable performance of the devel-
oped model EBLEU. However, the performance 
of EBLEU is not satisfactory currently as shown 
in the Table 2 and Table 3. This is due to the fact 
that we only used the POS information as lin-
guistic feature. This could be further improved 
by the combination of lexical information and 
other linguistic features such as the sentence 
structure, phrase similarity, and text entailment. 
 
 MAE RMSE DeltaAvg 
Spearman 
Corr 
EBLEU 16.97 21.94 2.74 0.11 
Baseline 
SVM 
14.81 18.22 8.52 0.46 
Table 3: Performance on the WMT13 corpus 
3.2 Task 1-2 System Selection 
Task 1.2 is the system selection task on EN-ES 
and DE-EN language pairs. Participants are re-
quired to rank up to five alternative translations 
for the same source sentence produced by multi-
ple translation systems.  
Firstly, we describe the two variants of 
EBLEU method for this task. We score the five 
alternative translation sentences as compared to 
the source sentence according to the closeness of 
their POS sequences. The German POS is also 
extracted using Berkeley parser (Petrov et al, 
2006). The mapping of German POS to universal 
POS tagset is using the developed one in the 
work of (Petrov et al, 2012). When we convert 
the absolute scores into the corresponding rank 
values, the variant EBLEU-I means that we use 
five fixed intervals (with the span from 0 to 1) to 
achieve the alignment as shown in Table 4. 
[1,0.4) [0.4, 0.3) [0.3, 0.25) [0.25, 0.2) [0.2, 0] 
5 4 3 2 1 
Table 4: Convert absolute scores into ranks 
 
The alignment work from absolute scores to 
rank values shown in Table 4 is empirically de-
termined. We have made a statistical work on the 
absolute scores yielded by our metrics, and each 
of the intervals shown in Table 4 covers the simi-
lar number of sentence scores. 
On the other hand, in the metric EBLEU-A, 
?A? means average. The absolute sentence edit 
scores are converted into the five rank values 
with the same number (average number). For 
instance, if there are 1000 sentence scores in to-
tal then each rank level (from 1 to 5) will gain 
200 scores from the best to the worst. 
Secondly, we introduce the NB-LPR model 
used in this task. NB-LPR means the Na?ve 
Bayes classification algorithm using the features 
of Length penalty (introduced in previous sec-
tion), Precision, Recall and Rank values. NB-
LPR considers each of its features independently. 
Let?s see the conditional probability that is also 
known as Bayes? rule. If the  ( | )  is given, 
then the  ( | ) can be calculated as follows: 
 
  ( | )  
 ( | ) ( )
 ( )
 (5) 
 
Given a data point identified as 
 (          ) and the classifications 
 (          ), Bayes? rule can be applied to 
this statement: 
 
  (  |          )  
 (         |  ) (  )
 (         )
 (6) 
 
As in many practical applications, parameter 
estimation for NB-LPR model uses the method 
of maximum likelihood. For details of Na?ve 
Bayes algorithm, see the works of (Zhang, 2004) 
and (Harrington, 2012). 
Thirdly, the SVM-LPR model means the sup-
port vector machine classification algorithm us-
ing the features of Length penalty, Precision, 
Recall and Rank values, i.e. the same features as 
in NB-LPR. SVM solves the nonlinear classifica-
tion problem by mapping the data from a low 
dimensional space to a high dimensional space 
using the Kernel methods. In the projected high 
dimensional space, the problem usually becomes 
a linear one, which is easier to solve. SVM is 
also called maximum interval classifier because 
it tries to find the optimized hyper plane that 
367
separates different classes with the largest mar-
gin, which is usually a quadratic optimization 
problem. Let?s see the formula below, we should 
find the points with the smallest margin to the 
hyper plane and then maximize this margin. 
 
          {    (      ( 
    ))  
 
? ?
}
 (7) 
 
where   is normal to the hyper plane, || || is 
the Euclidean norm of  , and | | || ||  is the 
perpendicular distance from the hyper plane to 
the origin. For details of SVM, see the works of 
(Cortes and Vapnik, 1995) and (Burges, 1998). 
 
EN-ES 
NB-LPR SVM-LPR 
MAE RMSE Time MAE RMSE Time 
.315 .399 .40s .304 .551 60.67s 
DE-EN 
NB-LPR SVM-LPR 
MAE RMSE Time MAE RMSE Time 
.318 .401 .79s .312 .559 111.7s 
Table 5: NB-LPR and SVM-LPR training 
In the training stage, we used all the officially 
released data of WMT 09, 10, 11 and 12 for the 
EN-ES and DE-EN language pairs. We used the 
WEKA (Hall et al, 2009) data mining software 
to implement the NB and SVM algorithms. The 
training scores are shown in Table 5. The NB-
LPR performs lower scores than the SVM-LPR 
but faster than SVM-LPR. 
 
 DE-EN EN-ES 
Methods 
Tau(ties 
penalized) 
|Tau|(ties 
ignored) 
Tau(ties 
penalized) 
|Tau|(ties 
ignored) 
EBLEU-I -0.38 -0.03 -0.35 0.02 
EBLEU-A N/A N/A -0.27 N/A 
NB-LPR -0.49 0.01 N/A 0.07 
Baseline  -0.12 0.08 -0.23 0.03 
Table 6: QE Task 1.2 testing scores 
The official testing scores are shown in Table 
6. Each task is allowed to submit up to two sys-
tems and we submitted the results using the 
methods of EBLEU and NB-LPR. The perfor-
mance of NB-LPR on EN-ES language pair 
shows higher Tau score (0.07) than the baseline 
system score (0.03) when the ties are ignored. 
Because of the number limitation of submitted 
systems for each task, we did not submit the 
SVM-LPR results. However, the training exper-
iments prove that the SVM-LPR model performs 
better than the NB-LPR model though SVM-
LPR takes more time to run. 
3.3 Task 2 Word-level QE 
Task 2 is the word-level quality estimation of 
automatically translated news sentences from 
English to Spanish without given reference trans-
lations. Participants are required to judge each 
translated word by assigning a two- or multi-
class labels. In the binary classification, a good 
or a bad label should be judged, where ?bad? 
indicates the need for editing the token. In the 
multi-class classification, the labels include 
?keep?, ?delete? and ?substitute?. In addition to 
the NB method, in this task, we utilized a dis-
criminative undirected probabilistic graphical 
model, i.e. Conditional Random Field (CRF). 
CRF is early employed by Lefferty (Lefferty 
et al, 2001) to deal with the labeling problems of 
sequence data, and is widely used later by other 
researchers. As the preparation for CRF defini-
tion, we assume that   is a variable representing 
the input sequence, and   is another variable rep-
resenting the corresponding labels to be attached 
to  . The two variables interact as conditional 
probability  ( | )  mathematically. Then the 
definition of CRF: Let a graph model   (   ) 
comprise a set   of vertices or nodes together 
with a set   of edges or lines and      |  
  , such that   is indexed by the vertices of  , 
then (   ) shapes a CRF model. This set meets 
the following form:  
 
   ( | )      
(?     (   |   )       ?     (   |   )     )
 (8) 
 
where   and   represent the data sequence and 
label sequence respectively;    and    are the 
features to be defined;    and    are the parame-
ters trained from the datasets. We used the tool 
CRF++1 to implement the CRF algorithm. The 
features we used for the NB and CRF are shown 
in Table 7. We firstly trained the CRF and NB 
models on the officially released training corpus 
(produced by Moses and annotated by computing 
TER with some tweaks). Then we removed the 
truth labels in the training corpus (we call it 
pseudo test corpus) and labeled each word using 
the derived training models. The test results on 
the pseudo test corpus are shown in Table 8, 
                                                 
1 https://code.google.com/p/crfpp/ 
368
which specifies CRF performs better than NB 
algorithm. 
 
     (    ) 
Unigram, from antecedent 4th 
to subsequent 3rd token 
       
 (    ) 
Bigram, from antecedent 2nd 
to subsequent 2nd token 
      
Jump bigram, antecedent and 
subsequent token 
          
 (    ) 
Trigram, from antecedent 2nd 
to subsequent 2nd token 
Table 7: Developed features 
 
Binary 
CRF NB 
Training Accuracy Training Accuracy 
Itera=108 
Time=2.48s 
0.944 Time=0.59s 0.941 
Multi-classes 
CRF NB 
Training Accuracy Training Accuracy 
Itera=106 
Time=3.67s 
0.933 Time=0.55s 0.929 
Table 8: Performance on pseudo test corpus 
The official testing scores of Task 2 are shown 
in Table 9. We include also the results of other 
participants (CNGL and LIG) and their ap-
proaches. 
 
 Binary Multiclass 
Methods Pre Recall F1 Acc 
CNGL-
dMEMM 
0.7392 0.9261 0.8222 0.7162 
CNGL-
MEMM 
0.7554 0.8581 0.8035 0.7116 
LIG-All N/A N/A N/A 0.7192 
LIG-FS 0.7885 0.8644 0.8247 0.7207 
LIG-
BOOSTING 
0.7779 0.8843 0.8276 N/A 
NB 0.8181 0.4937 0.6158 0.5174 
CRF 0.7169 0.9846 0.8297 0.7114 
Table 9: QE Task 2 official testing scores 
The results show that our method CRF yields 
a higher recall score than other systems in binary 
judgments task, and this leads to the highest F1 
score (harmonic mean of precision and recall). 
The recall value reflects the loyalty to the truth 
data. The augmented feature set designed in this 
paper allows the CRF to take the contextual in-
formation into account, and this contributes 
much to the recall score. On the other hand, the 
accuracy score of CRF in multiclass evaluation is 
lower than LIG-FS method. 
4 Conclusions 
This paper describes the algorithms and features 
we used in the WMT 13 Quality Estimation tasks. 
In the sentence-level QE task (Task 1.1), we de-
velop an enhanced version of BLEU metric, and 
this shows a potential usage for the traditional 
evaluation criteria. In the newly proposed system 
selection task (Task 1.2) and word-level QE task 
(Task 2), we explore the performances of several 
statistical models including NB, SVM, and CRF, 
of which the CRF performs best, the NB per-
forms lower than SVM but much faster than 
SVM. The official results show that the CRF 
model yields the highest F-score 0.8297 in binary 
classification judgment of word-level QE task. 
Acknowledgments 
The authors are grateful to the Science and 
Technology Development Fund of Macau and 
the Research Committee of the University of 
Macau for the funding support for our research, 
under the reference No. 017/2009/A and 
RG060/09-10S/CS/FST. The authors also wish to 
thank the anonymous reviewers for many helpful 
comments. 
References  
Akiba, Yasuhiro, Kenji Imamura, and Eiichiro Sumita. 
2001. Using Multiple Edit Distances to Automati-
cally Rank Machine Translation Output. In Pro-
ceedings of the MT Summit VIII, Santiago de 
Compostela, Spain. 
Albrecht, Joshua, and Rebecca Hwa. 2007. Regres-
sion for sentence-level MT evaluation with pseudo 
references. ACL. Vol. 45. No. 1. 
Avramidis, Eleftherios. 2012. Comparative quality 
estimation: Automatic sentence-level ranking of 
multiple machine translation outputs. In Proceed-
ings of 24th International Conference on 
Computational Linguistics (COLING), pages 
115?132, Mumbai, India. 
Burges, Christopher J. C. 1998. A Tutorial on Support 
Vector Machines for Pattern Recognition. J. Data 
Min. Knowl. Discov. Volume 2 Issue 2, June 
1998, 121-167. Kluwer Academic Publishers 
Hingham, MA, USA. 
Callison-Burch, Chris, Philipp Koehn, Christof Monz, 
Matt Post, Radu Soricut, and Lucia Specia. 2012. 
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh 
369
Workshop on Statistical Machine Translation, 
pages 10?51, Montr?al, Canada, June. 
Castillo, Julio and Paula Estrella. 2012. Semantic 
Textual Similarity for MT evaluation, Proceed-
ings of the 7th Workshop on Statistical Ma-
chine Translation (WMT2012), pages 52?58, 
Montre a?l, Canada, June 7-8. Association for 
Computational Linguistics. 
Chan, Yee Seng and Hwee Tou Ng. 2008. MAXSIM: 
A maximum similarity metric for machine transla-
tion evaluation. In Proceedings of ACL 2008: 
HLT, pages 55?62. Association for Computational 
Linguistics. 
Chen, Boxing and Roland Kuhn. 2011. Amber: A 
modified bleu, enhanced ranking metric. In Pro-
ceedings of the Sixth Workshop on Statistical 
Machine translation of the Association for 
Computational Linguistics(ACL-WMT), pages 
71-77, Edinburgh, Scotland, UK. 
Chen, Boxing, Roland Kuhn and Samuel Larkin. 2012. 
PORT: a Precision-Order-Recall MT Evaluation 
Metric for Tuning, Proceedings of the 50th An-
nual Meeting of the Association for Computa-
tional Linguistics, pages 930?939, Jeju, Republic 
of Korea, 8-14 July. 
Cortes, Corinna and Vladimir Vapnik. 1995. Support-
Vector Networks, J. Machine Learning, Volume 
20, issue 3, pp 273-297. Kluwer Academic Pub-
lishers, Boston. Manufactured in The Netherlands. 
Dahlmeier, Daniel, Chang Liu, and Hwee Tou Ng. 
2011. TESLA at WMT2011: Translation evalua-
tion and tunable metric. In Proceedings of the 
Sixth Workshop on Statistical Machine Trans-
lation, Association for Computational Linguis-
tics (ACL-WMT), pages 78-84, Edinburgh, Scot-
land, UK. 
Doddington, George. 2002. Automatic evaluation of 
machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the sec-
ond international conference on Human Lan-
guage Technology Research (HLT '02). Morgan 
Kaufmann Publishers Inc., San Francisco, CA, 
USA, 138-145. 
Echizen-ya, Hiroshi and Kenji Araki. 2010. Automat-
ic evaluation method for machine translation using 
noun-phrase chunking. In Proceedings of ACL 
2010, pages 108?117. Association for Computa-
tional Linguistics. 
Gamon, Michael, Anthony Aue, and Martine Smets. 
2005. Sentence-level MT evaluation without refer-
ence translations: Beyond language modeling. 
Proceedings of EAMT. 
Hall, Mark, Eibe Frank, Geoffrey Holmes, Bernhard 
Pfahringer, Peter Reutemann, Ian H. Witten. 2009. 
The WEKA data mining software: An update. 
SIGKDD Explorations, 11. 
Han, Aaron Li-Feng, Derek F. Wong and Lidia S. 
Chao. 2012. LEPOR: A Robust Evaluation Metric 
for Machine Translation with Augmented Factors. 
Proceedings of the 24th International Confer-
ence on Computational Linguistics (COLING 
2012: Posters), Mumbai, India. 
Han, Aaron Li-Feng, Derek F. Wong, Lidia S. Chao, 
Liangye He, Yi Lu, Junwen Xing and Xiaodong 
Zeng. 2013. Language-independent Model for Ma-
chine Translation Evaluation with Reinforced Fac-
tors. Proceedings of the 14th International 
Conference of Machine Translation Summit 
(MT Summit 2013), Nice, France. 
Harrington, Peter. 2012. Classifying with probability 
theory: na?ve bayes. Machine Learning in Ac-
tion, Part 1 Classification. Page 61-82. Publisher: 
Manning Publications. April. 
Lafferty, John, McCallum Andrew, and Pereira C.N. 
Ferando. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling se-
quence data. In Proceeding of 18th Internation-
al Conference on Machine Learning. 282-289. 
Lavie, Alon and Abhaya Agarwal. 2007. METEOR: 
An Automatic Metric for MT Evaluation with High 
Levels of Correlation with Human Judgments, 
Proceedings of the ACL Second Workshop on 
Statistical Machine Translation, pages 228-231, 
Prague, June. 
Leusch, Gregor, Nicola Ueffing, and Hermann Ney. 
2006. CDer: Efficient MT Evaluation Using Block 
Movements. In Proceedings of the 13th Confer-
ence of the European Chapter of the Associa-
tion for Computational Linguistics (EACL-06), 
241-248. 
Lin, Chin-Yew and Eduard Hovy. 2003. Automatic 
Evaluation of Summaries Using N-gram Co-
occurrence Statistics. In Proceedings of 2003 
Language Technology Conference (HLT-
NAACL 2003), Edmonton, Canada, May 27 - June 
1. 
Lita, Lucian Vlad, Monica Rogati and Alon Lavie. 
2005. BLANC: Learning Evaluation Metrics for 
MT, Proceedings of Human Language Tech-
nology Conference and Conference on Empir-
ical Methods in Natural Language Processing 
(HLT/EMNLP), pages 740?747, Vancouver, Oc-
tober. Association for Computational Linguistics. 
Liu, Chang, Daniel Dahlmeier and Hwee Tou Ng. 
2010. TESLA: Translation evaluation of sentences 
370
with linear-programming-based analysis. In Pro-
ceedings of the Joint Fifth Workshop on Statis-
tical Machine Translation and MetricsMATR. 
Liu, Ding and Daniel Gildea. 2006. Stochastic itera-
tive alignment for machine translation evaluation. 
Sydney. ACL06. 
Mariano, Felice and Lucia Specia. 2012. Linguistic 
Features for Quality Estimation. Proceedings of 
the 7th Workshop on Statistical Machine 
Translation, pages 96?103. 
Mehdad, Yashar, Matteo Negri, and Marcello Federi-
co. 2012. Match without a referee: evaluating MT 
adequacy without reference translations. Proceed-
ings of the Seventh Workshop on Statistical 
Machine Translation. Association for Compu-
tational Linguistics. 
Mirkin, Shachar, Lucia Specia, Nicola Cancedda, Ido 
Dagan, Marc Dymetman, and Idan Szpektor. 2009. 
Source-Language Entailment Modeling for Trans-
lating Unknown Terms, Proceedings of the Joint 
Conference of the 47th Annual Meeting of the 
ACL and the 4th International Joint Confer-
ence on Natural Language Processing of the 
AFNLP, pages 791?799, Suntec, Singapore, 2-7. 
ACL and AFNLP. 
Nie?en, Sonja, Franz Josef Och, Gregor Leusch, and 
Hermann Ney. 2000. A Evaluation Tool for Ma-
chine Translation: Fast Evaluation for MT Re-
search. In Proceedings of the 2nd International 
Conference on Language Resources and Eval-
uation (LREC-2000). 
Owczarzak, Karolina, Josef van Genabith and Andy 
Way. 2007. Labelled Dependencies in Machine 
Translation Evaluation, Proceedings of the ACL 
Second Workshop on Statistical Machine 
Translation, pages 104-111, Prague. 
Papineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a method for automat-
ic evaluation of machine translation. 
In Proceedings of the 40th Annual Meeting on 
Association for Computational Linguis-
tics (ACL '02). Association for Computational 
Linguistics, Stroudsburg, PA, USA, 311-318. 
Petrov, Slav, Leon Barrett, Romain Thibaux, and Dan 
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the 
21st International Conference on Computa-
tional Linguistics and the 44th annual meeting 
of the Association for Computational Linguis-
tics (ACL-44). Association for Computational 
Linguistics, Stroudsburg, PA, USA, 433-440. 
Popovic, Maja, David Vilar, Eleftherios Avramidis, 
Aljoscha Burchardt. 2011. Evaluation without ref-
erences: IBM1 scores as evaluation metrics. In 
Proceedings of the Sixth Workshop on Statisti-
cal Machine Translation, Association for 
Computational Linguistics (ACL-WMT), pages 
99-103, Edinburgh, Scotland, UK. 
Povlsen, Claus, Nancy Underwood, Bradley Music, 
and Anne Neville. 1998. Evaluating Text-Type 
Suitability for Machine Translation a Case Study 
on an English-Danish System. Proceedings of the 
First Language Resources and Evaluation 
Conference, LREC-98, Volume I. 27-31. Grana-
da, Spain. 
Schmid, Helmut. 1994. Probabilistic Part-of-Speech 
Tagging Using Decision Trees. In Proceedings of 
International Conference on New Methods in 
Language Processing, Manchester, UK. 
Snover, Matthew G., Nitin Madnani, Bonnie Dorr, 
and Richard Schwartz. 2009. TER-Plus: paraphrase, 
semantic, and alignment enhancements to Transla-
tion Edit Rate. J. Machine Tranlslation, 23: 117-
127. 
Specia, Lucia and Gimenez, J. 2010. Combining Con-
fidence Estimation and Reference-based Metrics 
for Segment-level MT Evaluation. The Ninth 
Conference of the Association for Machine 
Translation in the Americas (AMTA). 
Specia, Lucia, Dhwaj Raj, and Marco Turchi. 2010. 
Machine Translation Evaluation Versus Quality 
Estimation. Machine Translation, 24:39?50. 
Su, Keh-Yih, Wu Ming-Wen and Chang Jing-Shin. 
1992. A New Quantitative Quality Measure for 
Machine Translation Systems. In Proceedings of 
the 14th International Conference on Compu-
tational Linguistics, pages 433?439, Nantes, 
France, July. 
Tillmann, Christoph, Stephan Vogel, Hermann Ney, 
Arkaitz Zubiaga, and Hassan Sawaf. 1997. Accel-
erated DP Based Search For Statistical Translation. 
In Proceedings of the 5th European Confer-
ence on Speech Communication and Technol-
ogy (EUROSPEECH-97). 
Turian, Joseph P., Luke Shen, and I. Dan Melamed. 
2003. Evaluation of Machine Translation and its 
Evaluation. In Machine Translation Summit IX, 
pages 386?393. International Association for Ma-
chine Translation. 
Wang, Mengqiu and Christopher D. Manning. 2012. 
SPEDE: Probabilistic Edit Distance Metrics for 
MT Evaluation, WMT2012, 76-83. 
Wong, Billy T. M. and Chunyu Kit. 2012. Extending 
Machine Translation Evaluation Metrics with Lex-
ical Cohesion to Document Level. Proceedings of 
the 2012 Joint Conference on Empirical 
371
Methods in Natural Language Processing and 
Computational Natural Language Learning, 
pages 1060?1068, Jeju Island, Korea, 12?14 July. 
Association for Computational Linguistics. 
Zhang, Harry. 2004. The Optimality of Naive Bayes. 
Proceedings of the Seventeenth International 
Florida Artificial Intelligence Research Socie-
ty Conference, Miami Beach, Florida, USA. 
AAAI Press. 
372
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 414?421,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
A Description of Tunable Machine Translation Evaluation Systems in 
WMT13 Metrics Task 
 
 
Aaron L.-F. Han 
hanlifengaaron@gmail.com 
Derek F. Wong 
derekfw@umac.mo 
Lidia S. Chao 
lidiasc@umac.mo 
   
Yi Lu 
mb25435@umac.mo 
Liangye He 
wutianshui0515@gmail.com 
Yiming Wang 
mb25433@umac.mo 
Jiaji Zhou 
mb25473@uamc.mo 
    
Natural Language Processing & Portuguese-Chinese Machine Translation Laboratory 
Department of Computer and Information Science 
University of Macau, Macau S.A.R. China 
 
  
 
Abstract 
This paper is to describe our machine transla-
tion evaluation systems used for participation 
in the WMT13 shared Metrics Task. In the 
Metrics task, we submitted two automatic MT 
evaluation systems nLEPOR_baseline and 
LEPOR_v3.1. nLEPOR_baseline is an n-gram 
based language independent MT evaluation 
metric employing the factors of modified sen-
tence length penalty, position difference penal-
ty, n-gram precision and n-gram recall. 
nLEPOR_baseline measures the similarity of 
the system output translations and the refer-
ence translations only on word sequences. 
LEPOR_v3.1 is a new version of LEPOR met-
ric using the mathematical harmonic mean to 
group the factors and employing some linguis-
tic features, such as the part-of-speech infor-
mation. The evaluation results of WMT13 
show LEPOR_v3.1 yields the highest average-
score 0.86 with human judgments at system-
level using Pearson correlation criterion on 
English-to-other (FR, DE, ES, CS, RU) lan-
guage pairs. 
1 Introduction 
Machine translation has a long history since the 
1950s (Weaver, 1955) and gains a fast develop-
ment in the recent years because of the higher 
level of computer technology. For instances, Och 
(2003) presents Minimum Error Rate Training 
(MERT) method for log-linear statistical ma-
chine translation models to achieve better trans-
lation quality; Menezes et al (2006) introduce a 
syntactically informed phrasal SMT system for 
English-to-Spanish translation using a phrase 
translation model, which is based on global reor-
dering and the dependency tree; Su et al (2009) 
use the Thematic Role Templates model to im-
prove the translation; Costa-juss? et al (2012) 
develop the phrase-based SMT system for Chi-
nese-Spanish translation using a pivot language. 
With the rapid development of Machine Transla-
tion (MT), the evaluation of MT has become a 
challenge in front of researchers. However, the 
MT evaluation is not an easy task due to the fact 
of the diversity of the languages, especially for 
the evaluation between distant languages (Eng-
lish, Russia, Japanese, etc.). 
2 Related works 
The earliest human assessment methods for ma-
chine translation include the intelligibility and 
fidelity used around 1960s (Carroll, 1966), and 
the adequacy (similar as fidelity), fluency and 
comprehension (improved intelligibility) (White 
et al, 1994). Because of the expensive cost of 
manual evaluations, the automatic evaluation 
metrics and systems appear recently. 
The early automatic evaluation metrics in-
clude the word error rate WER (Su et al, 1992) 
and position independent word error rate PER 
(Tillmann et al, 1997) that are based on the Le-
venshtein distance. Several promotions for the 
MT and MT evaluation literatures include the 
ACL?s annual workshop on statistical machine 
translation WMT (Koehn and Monz, 2006; Calli-
son-Burch et al, 2012), NIST open machine 
translation (OpenMT) Evaluation series (Li, 
2005) and the international workshop of spoken 
language translation IWSLT, which is also orga-
nized annually from 2004 (Eck and Hori, 2005; 
414
Paul, 2008, 2009; Paul, et al, 2010; Federico et 
al., 2011). 
BLEU (Papineni et al, 2002) is one of the 
commonly used evaluation metrics that is de-
signed to calculate the document level precisions. 
NIST (Doddington, 2002) metric is proposed 
based on BLEU but with the information weights 
added to the n-gram approaches. TER (Snover et 
al., 2006) is another well-known MT evaluation 
metric that is designed to calculate the amount of 
work needed to correct the hypothesis translation 
according to the reference translations. TER in-
cludes the edit categories such as insertion, dele-
tion, substitution of single words and the shifts of 
word chunks. Other related works include the 
METEOR (Banerjee and Lavie, 2005) that uses 
semantic matching (word stem, synonym, and 
paraphrase), and (Wong and Kit, 2008), (Popovic, 
2012), and (Chen et al, 2012) that introduces the 
word order factors, etc. The traditional evalua-
tion metrics tend to perform well on the language 
pairs with English as the target language. This 
paper will introduce the evaluation models that 
can also perform well on the language pairs that 
with English as source language. 
3 Description of Systems 
3.1 Sub Factors 
Firstly, we introduce the sub factor of modified 
length penalty inspired by BLEU metric. 
 
    {
   
 
            
                  
   
 
            
 (1) 
 
In the formula,    means sentence length 
penalty that is designed for both the shorter or 
longer translated sentence (hypothesis translation) 
as compared to the reference sentence. Parame-
ters   and   represent the length of candidate 
sentence and reference sentence respectively. 
Secondly, let?s see the factors of n-gram pre-
cision and n-gram recall. 
 
    
              
                           
 (2) 
 
    
              
                          
  (3) 
 
The variable                represents the 
number of matched n-gram chunks between hy-
pothesis sentence and reference sentence. The n-
gram precision and n-gram recall are firstly cal-
culated on sentence-level instead of corpus-level 
that is used in BLEU (  ). Then we define the 
weighted n-gram harmonic mean of precision 
and recall (WNHPR). 
 
          (?       (        
 
    (4) 
 
Thirdly, it is the n-gram based position differ-
ence penalty (NPosPenal). This factor is de-
signed to achieve the penalty for the different 
order of successfully matched words in reference 
sentence and hypothesis sentence. The alignment 
direction is from the hypothesis sentence to the 
reference sentence. It employs the  -gram meth-
od into the matching period, which means that 
the potential matched word will be assigned 
higher priority if it also has nearby matching. 
The nearest matching will be accepted as a back-
up choice if there are both nearby matching or 
there is no other matched word around the poten-
tial pairs. 
 
                 (5) 
 
     
 
          
?      
         
    (6) 
 
                             (7) 
 
The variable           means the length of 
the hypothesis sentence; the variables 
          and           represent the posi-
tion number of matched words in hypothesis sen-
tence and reference sentence respectively.  
3.2 Linguistic Features 
The linguistic features could be easily employed 
into our evaluation models. In the submitted ex-
periment results of WMT Metrics Task, we used 
the part of speech information of the words in 
question. In grammar, a part of speech, which is 
also called a word class, a lexical class, or a lexi-
cal category, is a linguistic category of lexical 
items. It is generally defined by the syntactic or 
morphological behavior of the lexical item in 
question. The POS information utilized in our 
metric LEPOR_v3.1, an enhanced version of 
LEPOR (Han et al, 2012), is extracted using the 
Berkeley parser (Petrov et al, 2006) for English, 
German, and French languages, using COM-
POST Czech morphology tagger (Collins, 2002) 
for Czech language, and using TreeTagger 
(Schmid, 1994) for Spanish and Russian lan-
guages respectively. 
415
Ratio 
other-to-English English-to-other 
CZ-EN DE-EN ES-EN FR-EN EN-CZ EN-DE EN-ES EN-FR 
HPR:LP:NPP(word) 7:2:1 3:2:1 7:2:1 3:2:1 7:2:1 1:3:7 3:2:1 3:2:1 
HPR:LP:NPP(POS) NA 3:2:1 NA 3:2:1 7:2:1 7:2:1 NA 3:2:1 
    (      1:9 9:1 1:9 9:1 9:1 9:1 9:1 9:1 
    (     NA 9:1 NA 9:1 9:1 9:1 NA 9:1 
        NA 1:9 NA 9:1 1:9 1:9 NA 9:1 
Table 1. The tuned weight values in LEPOR_v3.1 system 
 
System 
Correlation Score with Human Judgment 
other-to-English English-to-other Mean 
score CZ-EN DE-EN ES-EN FR-EN EN-CZ EN-DE EN-ES EN-FR 
LEPOR_v3.1 0.93 0.86 0.88 0.92 0.83 0.82 0.85 0.83 0.87 
nLEPOR_baseline 0.95 0.61 0.96 0.88 0.68 0.35 0.89 0.83 0.77 
METEOR 0.91 0.71 0.88 0.93 0.65 0.30 0.74 0.85 0.75 
BLEU 0.88 0.48 0.90 0.85 0.65 0.44 0.87 0.86 0.74 
TER 0.83 0.33 0.89 0.77 0.50 0.12 0.81 0.84 0.64 
Table 2. The performances of nLEPOR_baseline and LEPOR_v3.1 systems on WMT11 corpora 
 
3.3 The nLEPOR_baseline System 
The nLEPOR_baseline system utilizes the simple 
product value of the factors: modified length 
penalty, n-gram position difference penalty, and 
weighted n-gram harmonic mean of precision 
and recall. 
 
                           (8) 
 
The system level score is the arithmetical 
mean of the sentence level evaluation scores. In 
the experiments of Metrics Task using the 
nLEPOR_baseline system, we assign N=1 in the 
factor WNHPR, i.e. weighted unigram harmonic 
mean of precision and recall. 
3.4 The LEPOR_v3.1 System 
The system of LEPOR_v3.1 (also called as 
hLEPOR) combines the sub factors using 
weighted mathematical harmonic mean instead 
of the simple product value. 
 
        
                   
   
  
 
          
         
 
    
   
 (9) 
 
Furthermore, this system takes into account 
the linguistic features, such as the POS of the 
words. Firstly, we calculate the hLEPOR score 
on surface words            (the closeness of 
the hypothesis translation and the reference 
translation). Then, we calculate the hLEPOR 
score on the extracted POS sequences 
          (the closeness of the corresponding 
POS tags between hypothesis sentence and refer-
ence sentence). The final score             is 
the combination of the two sub-scores 
           and          . 
 
             
 
       
(              
               (10) 
 
4 Evaluation Method 
In the MT evaluation task, the Spearman rank 
correlation coefficient method is usually used by 
the authoritative ACL WMT to evaluate the cor-
relation of different MT evaluation metrics. So 
we use the Spearman rank correlation coefficient 
  to evaluate the performances of 
nLEPOR_baseline and LEPOR_v3.1 in system 
level correlation with human judgments. When 
there are no ties,   is calculated using: 
 
     
 ?  
 
 (     
  (11) 
 
The variable    is the difference value be-
tween the ranks for         and   is the number 
of systems. We also offer the Pearson correlation 
coefficient information as below. Given a sample 
of paired data (X, Y) as (      ,         , the 
Pearson correlation coefficient is: 
 
     
? (      (      
 
   
?? (       
 
   
?? (     )
 
     
 (12) 
416
where    and    specify the mean of discrete 
random variable X and Y respectively. 
 
Directions 
EN-
FR 
EN-
DE 
EN-
ES 
EN-
CS 
EN-
RU 
Av 
LEPOR_v3.1 .91 .94 .91 .76 .77 .86 
nLEPOR_baseline .92 .92 .90 .82 .68 .85 
SIMP-
BLEU_RECALL 
.95 .93 .90 .82 .63 .84 
SIMP-
BLEU_PREC 
.94 .90 .89 .82 .65 .84 
NIST-mteval-
inter 
.91 .83 .84 .79 .68 .81 
Meteor .91 .88 .88 .82 .55 .81 
BLEU-mteval-
inter 
.89 .84 .88 .81 .61 .80 
BLEU-moses .90 .82 .88 .80 .62 .80 
BLEU-mteval .90 .82 .87 .80 .62 .80 
CDER-moses .91 .82 .88 .74 .63 .80 
NIST-mteval .91 .79 .83 .78 .68 .79 
PER-moses .88 .65 .88 .76 .62 .76 
TER-moses .91 .73 .78 .70 .61 .75 
WER-moses .92 .69 .77 .70 .61 .74 
TerrorCat .94 .96 .95 na na .95 
SEMPOS na na na .72 na .72 
ACTa .81 -.47 na na na .17 
ACTa5+6 .81 -.47 na na na .17 
Table 3. System-level Pearson correlation scores 
on WMT13 English-to-other language pairs 
5 Experiments 
5.1 Training 
In the training stage, we used the officially re-
leased data of past WMT series. There is no Rus-
sian language in the past WMT shared tasks. So 
we trained our systems on the other eight lan-
guage pairs including English to other (French, 
German, Spanish, Czech) and the inverse transla-
tion direction. In order to avoid the overfitting 
problem, we used the WMT11 corpora as train-
ing data to train the parameter weights in order to 
achieve a higher correlation with human judg-
ments at system-level evaluations. For the 
nLEPOR_baseline system, the tuned values of   
and   are 9 and 1 respectively for all language 
pairs except for (   ,    ) for CS-EN lan-
guage pair. For the LEPOR_v3.1 system, the 
tuned values of weights are shown in Table 1. 
The evaluation scores of the training results on 
WMT11 corpora are shown in Table 2. The de-
signed methods have shown promising correla-
tion scores with human judgments at system lev-
el, 0.87 and 0.77 respectively for 
nLEPOR_baseline and LEPOR_v3.1 of the mean 
score on eight language pairs. As compared to 
METEOR, BLEU and TER, we have achieved 
higher correlation scores with human judgments.  
5.2 Testing 
In the WMT13 shared Metrics Task, we also 
submitted our system performances on English-
to-Russian and Russian-to-English language 
pairs. However, since the Russian language did 
not appear in the past WMT shared tasks, we 
assigned the default parameter weights to Rus-
sian language for the submitted two systems. The 
officially released results on WMT13 corpora are 
shown in Table 3, Table 4 and Table 5 respec-
tively for system-level and segment-level per-
formance on English-to-other language pairs. 
 
Directions 
EN-
FR 
EN-
DE 
EN-
ES 
EN-
CS 
EN-
RU 
Av 
SIMP-
BLEU_RECALL 
.92 .93 .83 .87 .71 .85 
LEPOR_v3.1 .90 .9 .84 .75 .85 .85 
NIST-mteval-
inter 
.93 .85 .80 .90 .77 .85 
CDER-moses .92 .87 .86 .89 .70 .85 
nLEPOR_baseline .92 .90 .85 .82 .73 .84 
NIST-mteval .91 .83 .78 .92 .72 .83 
SIMP-
BLEU_PREC 
.91 .88 .78 .88 .70 .83 
Meteor .92 .88 .78 .94 .57 .82 
BLEU-mteval-
inter 
.92 .83 .76 .90 .66 .81 
BLEU-mteval .89 .79 .76 .90 .63 .79 
TER-moses .91 .85 .75 .86 .54 .78 
BLEU-moses .90 .79 .76 .90 .57 .78 
WER-moses .91 .83 .71 .86 .55 .77 
PER-moses .87 .69 .77 .80 .59 .74 
TerrorCat .93 .95 .91 na na .93 
SEMPOS na na na .70 na .70 
ACTa5+6 .81 -.53 na na na .14 
ACTa .81 -.53 na na na .14 
Table 4. System-level Spearman rank correlation 
scores on WMT13 English-to-other language 
pairs 
 
Table 3 shows LEPOR_v3.1 and 
nLEPOR_baseline yield the highest and the sec-
ond highest average Pearson correlation score 
0.86 and 0.85 respectively with human judg-
ments at system-level on five English-to-other 
language pairs. LEPOR_v3.1 and 
417
nLEPOR_baseline also yield the highest Pearson 
correlation score on English-to-Russian (0.77) 
and English-to-Czech (0.82) language pairs re-
spectively. The testing results of LEPOR_v3.1 
and nLEPOR_baseline show better correlation 
scores as compared to METEOR (0.81), BLEU 
(0.80) and TER-moses (0.75) on English-to-other 
language pairs, which is similar with the training 
results.  
On the other hand, using the Spearman rank 
correlation coefficient, SIMPBLEU_RECALL 
yields the highest correlation score 0.85 with 
human judgments. Our metric LEPOR_v3.1 also 
yields the highest Spearman correlation score on 
English-to-Russian (0.85) language pair, which 
is similar with the result using Pearson correla-
tion and shows its robust performance on this 
language pair.  
 
Directions 
EN-
FR 
EN-
DE 
EN-
ES 
EN-
CS 
EN-
RU Av 
SIMP-
BLEU_RECALL 
.16 .09 .23 .06 .12 .13 
Meteor .15 .05 .18 .06 .11 .11 
SIMP-
BLEU_PREC 
.14 .07 .19 .06 .09 .11 
sentBLEU-moses .13 .05 .17 .05 .09 .10 
LEPOR_v3.1 .13 .06 .18 .02 .11 .10 
nLEPOR_baseline .12 .05 .16 .05 .10 .10 
dfki_logregNorm-
411 
na na .14 na na .14 
TerrorCat .12 .07 .19 na na .13 
dfki_logregNormS
oft-431 
na na .03 na na .03 
Table 5. Segment-level Kendall?s tau correlation 
scores on WMT13 English-to-other language 
pairs 
 
However, we find a problem in the Spearman 
rank correlation method. For instance, let two 
evaluation metrics MA and MB with their evalu-
ation scores   ??????                   and  
  ???? ??                   respectively reflecting 
three MT systems  
 ??            . Before the calculation of cor-
relation with human judgments, they will be 
converted into   ??????  ?          and   ???? ??  ?          
with the same rank sequence using Spearman 
rank method; thus, the two evaluation systems 
will get the same correlation with human judg-
ments whatever are the values of human judg-
ments. But the two metrics reflect different re-
sults indeed: MA gives the outstanding score 
(0.95) to M1 system and puts very low scores 
(0.50 and 0.45) on the other two systems M2 and 
M3 while MB thinks the three MT systems have 
similar performances (scores from 0.74 to 0.77). 
This information is lost using the Spearman rank 
correlation methodology. 
The segment-level performance of 
LEPOR_v3.1 is moderate with the average Ken-
dall?s tau correlation score 0.10 on five English-
to-other language pairs, which is due to the fact 
that we trained our metrics at system-level in this 
shared metrics task. Lastly, the officially released 
results on WMT13 other-to-English language 
pairs are shown in Table 6, Table 7 and Table 8 
respectively for system-level and segment-level 
performance.  
 
Directions 
FR-
EN 
DE-
EN 
ES-
EN 
CS-
EN 
RU-
EN Av 
Meteor .98 .96 .97 .99 .84 .95 
SEMPOS .95 .95 .96 .99 .82 .93 
Depref-align .97 .97 .97 .98 .74 .93 
Depref-exact .97 .97 .96 .98 .73 .92 
SIMP-
BLEU_RECALL 
.97 .97 .96 .94 .78 .92 
UMEANT .96 .97 .99 .97 .66 .91 
MEANT .96 .96 .99 .96 .63 .90 
CDER-moses .96 .91 .95 .90 .66 .88 
SIMP-
BLEU_PREC 
.95 .92 .95 .91 .61 .87 
LEPOR_v3.1 .96 .96 .90 .81 .71 .87 
nLEPOR_baseline .96 .94 .94 .80 .69 .87 
BLEU-mteval-
inter 
.95 .92 .94 .90 .61 .86 
NIST-mteval-inter .94 .91 .93 .84 .66 .86 
BLEU-moses .94 .91 .94 .89 .60 .86 
BLEU-mteval .95 .90 .94 .88 .60 .85 
NIST-mteval .94 .90 .93 .84 .65 .85 
TER-moses .93 .87 .91 .77 .52 .80 
WER-moses .93 .84 .89 .76 .50 .78 
PER-moses .84 .88 .87 .74 .45 .76 
TerrorCat .98 .98 .97 na na .98 
Table 6. System-level Pearson correlation scores 
on WMT13 other-to-English language pairs 
 
METEOR yields the highest average correla-
tion scores 0.95 and 0.94 respectively using 
Pearson and Spearman rank correlation methods 
on other-to-English language pairs. The average 
performance of nLEPOR_baseline is a little bet-
ter than LEPOR_v3.1 on the five language pairs 
of other-to-English even though it is also moder-
ate as compared to other metrics. However, using 
418
the Pearson correlation method, 
nLEPOR_baseline yields the average correlation 
score 0.87 which already wins the BLEU (0.86) 
and TER (0.80) as shown in Table 6. 
 
Directions 
FR-
EN 
DE-
EN 
ES-
EN 
CS-
EN 
RU-
EN Av 
Meteor .98 .96 .98 .96 .81 .94 
Depref-align .99 .97 .97 .96 .79 .94 
UMEANT .99 .95 .96 .97 .79 .93 
MEANT .97 .93 .94 .97 .78 .92 
Depref-exact .98 .96 .94 .94 .76 .92 
SEMPOS .94 .92 .93 .95 .83 .91 
SIMP-
BLEU_RECALL 
.98 .94 .92 .91 .81 .91 
BLEU-mteval-
inter 
.99 .90 .90 .94 .72 .89 
BLEU-mteval .99 .89 .89 .94 .69 .88 
BLEU-moses .99 .90 .88 .94 .67 .88 
CDER-moses .99 .88 .89 .93 .69 .87 
SIMP-
BLEU_PREC 
.99 .85 .83 .92 .72 .86 
nLEPOR_baseline .95 .95 .83 .85 .72 .86 
LEPOR_v3.1 .95 .93 .75 0.8 .79 .84 
NIST-mteval .95 .88 .77 .89 .66 .83 
NIST-mteval-inter .95 .88 .76 .88 .68 .83 
TER-moses .95 .83 .83 0.8 0.6 
0.8
0 
WER-moses .95 .67 .80 .75 .61 .76 
PER-moses .85 .86 .36 .70 .67 .69 
TerrorCat .98 .96 .97 na na .97 
Table 7. System-level Spearman rank correlation 
scores on WMT13 other-to-English language 
pairs 
 
Once again, our metrics perform moderate at 
segment-level on other-to-English language pairs 
due to the fact that they are trained at system-
level. We notice that some of the evaluation met-
rics do not submit the results on all the language 
pairs; however, their performance on submitted 
language pair is sometimes very good, such as 
the dfki_logregFSS-33 metric with a segment-
level correlation score 0.27 on German-to-
English language pair. 
6 Conclusion 
This paper describes our participation in the 
WMT13 Metrics Task. We submitted two sys-
tems nLEPOR_baseline and LEPOR_v3.1. Both 
of the two systems are trained and tested using 
the officially released data. LEPOR_v3.1 yields 
the highest Pearson correlation average-score 
0.86 with human judgments on five English-to-
other language pairs, and nLEPOR_baseline 
yields better performance than LEPOR_v3.1 on 
other-to-English language pairs. Furthermore, 
LEPOR_v3.1 shows robust system-level perfor-
mance on English-to-Russian language pair, and 
nLEPOR_baseline shows best system-level per-
formance on English-to-Czech language pair us-
ing Pearson correlation criterion. As compared to 
nLEPOR_baseline, the experiment results of 
LEPOR_v3.1 also show that the proper use of 
linguistic information can increase the perfor-
mance of the evaluation systems. 
 
Directions 
FR-
EN 
DE-
EN 
ES-
EN 
CS-
EN 
RU-
EN Av 
SIMP-
BLEU_RECALL 
.19 .32 .28 .26 .23 .26 
Meteor .18 .29 .24 .27 .24 .24 
Depref-align .16 .27 .23 .23 .20 .22 
Depref-exact .17 .26 .23 .23 .19 .22 
SIMP-
BLEU_PREC 
.15 .24 .21 .21 .17 .20 
nLEPOR_baseline .15 .24 .20 .18 .17 .19 
sentBLEU-moses .15 .22 .20 .20 .17 .19 
LEPOR_v3.1 .15 .22 .16 .19 .18 .18 
UMEANT .10 .17 .14 .16 .11 .14 
MEANT .10 .16 .14 .16 .11 .14 
dfki_logregFSS-
33 
na .27 na na na .27 
dfki_logregFSS-
24 
na .27 na na na .27 
TerrorCat .16 .30 .23 na na .23 
Table 8. Segment-level Kendall?s tau correlation 
scores on WMT13 other-to-English language 
pairs 
Acknowledgments 
The authors wish to thank the anonymous re-
viewers for many helpful comments. The authors 
are grateful to the Science and Technology De-
velopment Fund of Macau and the Research 
Committee of the University of Macau for the 
funding support for our research, under the refer-
ence No. 017/2009/A and RG060/09-
10S/CS/FST.  
References  
Banerjee, Satanjeev and Alon Lavie. 2005. METEOR: 
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In 
Proceedings of the 43th Annual Meeting of the 
419
Association of Computational Linguistics 
(ACL- 05), pages 65?72, Ann Arbor, US, June. 
Association of Computational Linguistics. 
Callison-Burch, Chris, Philipp Koehn, Christof Monz, 
and Omar F. Zaidan. 2011. Findings of the 2011 
Workshop on Statistical Machine Translation. 
In Proceedings of the Sixth Workshop on Sta-
tistical Machine Translation (WMT '11). Asso-
ciation for Computational Linguistics, Stroudsburg, 
PA, USA, 22-64. 
Callison-Burch, Chris, Philipp Koehn, Christof Monz, 
Matt Post, Radu Soricut, and Lucia Specia. 2012. 
Findings of the 2012Workshop on Statistical Ma-
chine Translation. In Proceedings of the Seventh 
Workshop on Statistical Machine Translation, 
pages 10?51, Montreal, Canada. Association for 
Computational Linguistics. 
Carroll, John B. 1966. An Experiment in Evaluating 
the Quality of Translations, Mechanical Transla-
tion and Computational Linguistics, vol.9, 
nos.3 and 4, September and December 1966, page 
55-66, Graduate School of Education, Harvard 
University. 
Chen, Boxing, Roland Kuhn and Samuel Larkin. 2012. 
PORT: a Precision-Order-Recall MT Evaluation 
Metric for Tuning, Proceedings of the 50th An-
nual Meeting of the Association for Computa-
tional Linguistics, pages 930?939, Jeju, Republic 
of Korea, 8-14 July 2012. 
Collins, Michael. 2002. Discriminative Training 
Methods for Hidden Markov Models: Theory and 
Experiments with Perceptron Algorithms. In Pro-
ceedings of the ACL-02 conference on Empiri-
cal methods in natural language processing, 
Volume 10 (EMNLP 02), pages 1-8. Association 
for Computational Linguistics, Stroudsburg, PA, 
USA. 
Costa-juss?, Marta R., Carlos A. Henr?quez and Ra-
fael E. Banchs. 2012. Evaluating Indirect Strategies 
for Chinese-Spanish Statistical Machine Transla-
tion. Journal of artificial intelligence research, 
Volume 45, pages 761-780. 
Doddington, George. 2002. Automatic evaluation of 
machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the sec-
ond international conference on Human Lan-
guage Technology Research (HLT '02). Morgan 
Kaufmann Publishers Inc., San Francisco, CA, 
USA, 138-145. 
Eck, Matthias and Chiori Hori. 2005. Overview of the 
IWSLT 2005 Evaluation Campaign. Proceedings 
of IWSLT 2005. 
Federico, Marcello, Luisa Bentivogli, Michael Paul, 
and Sebastian Stiiker. 2011. Overview of the 
IWSLT 2011 Evaluation Campaign. In Proceed-
ings of IWSLT 2011, 11-27. 
Han, Aaron Li-Feng, Derek F. Wong and Lidia S. 
Chao. 2012. LEPOR: A Robust Evaluation Metric 
for Machine Translation with Augmented Factors. 
Proceedings of the 24th International Confer-
ence on Computational Linguistics (COLING 
2012: Posters), Mumbai, India. 
Koehn, Philipp and Christof Monz. 2006. Manual and 
Automatic Evaluation of Machine Translation be-
tween European Languages. Proceedings of the 
ACLWorkshop on Statistical Machine Trans-
lation, pages 102?121, New York City. 
Li, A. (2005). Results of the 2005 NIST machine 
translation evaluation. In Machine Translation 
Workshop. 
Menezes, Arul, Kristina Toutanova and Chris Quirk. 
2006. Microsoft Research Treelet Translation Sys-
tem: NAACL 2006 Europarl Evaluation, Proceed-
ings of the ACLWorkshop on Statistical Ma-
chine Translation, pages 158-161, New York 
City. 
Och, Franz Josef. 2003. Minimum Error Rate Train-
ing for Statistical Machine Translation.  In Pro-
ceedings of the 41st Annual Meeting of the As-
sociation for Computational Linguistics (ACL-
2003). pp. 160-167. 
Papineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a method for automat-
ic evaluation of machine translation. 
In Proceedings of the 40th Annual Meeting on 
Association for Computational Linguis-
tics (ACL '02). Association for Computational 
Linguistics, Stroudsburg, PA, USA, 311-318. 
Paul, Michael. 2008. Overview of the IWSLT 2008 
Evaluation Campaign. Proceeding of IWLST 
2008, Hawaii, USA. 
Paul, Michael. 2009. Overview of the IWSLT 2009 
Evaluation Campaign. In Proc. of IWSLT 2009, 
Tokyo, Japan, pp. 1?18. 
Paul, Michael, Marcello Federico and Sebastian 
Stiiker. 2010. Overview of the IWSLT 2010 Eval-
uation Campaign, Proceedings of the 7th Inter-
national Workshop on Spoken Language 
Translation, Paris, December 2nd and 3rd, 2010, 
page 1-25. 
Petrov, Slav, Leon Barrett, Romain Thibaux, and Dan 
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the 
21st International Conference on Computa-
tional Linguistics and the 44th annual meeting 
of the Association for Computational Linguis-
420
tics (ACL-44). Association for Computational 
Linguistics, Stroudsburg, PA, USA, 433-440. 
Popovic, Maja. 2012. Class error rates for evaluation 
of machine translation output. Proceedings of the 
7th Workshop on Statistical Machine Transla-
tion, pages 71?75, Canada. 
Schmid, Helmut. 1994. Probabilistic Part-of-Speech 
Tagging Using Decision Trees. In Proceedings of 
International Conference on New Methods in 
Language Processing, Manchester, UK. 
Snover, Matthew, Bonnie J. Dorr, Richard Schwartz, 
Linnea Micciulla, and John Makhoul. 2006. A 
Study of Translation Edit Rate with Targeted Hu-
man Annotation. In Proceedings of the 7th Con-
ference of the Association for Machine Trans-
lation in the Americas (AMTA-06), pages 223?
231, USA. Association for Machine Translation in 
the Americas. 
Su, Hung-Yu and Chung-Hsien Wu. 2009. Improving 
Structural Statistical Machine Translation for Sign 
Language With Small Corpus Using Thematic 
Role Templates as Translation Memory, IEEE 
TRANSACTIONS ON AUDIO, SPEECH, AND 
LANGUAGE PROCESSING, VOL. 17, NO. 7. 
Su, Keh-Yih, Wu Ming-Wen and Chang Jing-Shin. 
1992. A New Quantitative Quality Measure for 
Machine Translation Systems. In Proceedings of 
the 14th International Conference on Compu-
tational Linguistics, pages 433?439, Nantes, 
France. 
Tillmann, Christoph, Stephan Vogel, Hermann Ney, 
Arkaitz Zubiaga, and Hassan Sawaf. 1997. Accel-
erated DP Based Search For Statistical Translation. 
In Proceedings of the 5th European Confer-
ence on Speech Communication and Technol-
ogy (EUROSPEECH-97). 
Weaver, Warren. 1955. Translation. In William Locke 
and A. Donald Booth, editors, Machine Transla-
tion of Languages: Fourteen Essays. John 
Wiley & Sons, New York, pages 15?23. 
White, John S., Theresa O?Connell, and Francis 
O?Mara. 1994. The ARPA MT evaluation method-
ologies: Evolution, lessons, and future approaches. 
In Proceedings of the Conference of the Asso-
ciation for Machine Translation in the Ameri-
cas (AMTA 1994). pp193-205. 
Wong, Billy and Chunyu Kit. 2008. Word choice and 
word position for automatic MT evaluation. In 
Workshop: MetricsMATR of the Association for 
Machine Translation in the Americas (AMTA), 
short paper, Waikiki, Hawai?I, USA. Association 
for Machine Translation in the Americas. 
 
421
Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 82?87,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Experiments with POS-based restructuring and alignment-based re-
ordering for statistical machine translation 
 
 
Shuo Li, Derek F. Wong and Lidia S. Chao 
Department of Computer and Information Science 
University of Macau, Macau S.A.R., China. 
leevis1987@gmail.com, {derekfw, lidiasc}@umac.mo 
 
Abstract 
This paper presents the methods which are 
based on the part-of-speech (POS) and auto 
alignment information to improve the quality 
of machine translation result and the word 
alignment. We utilize different types of POS 
tag to restructure source sentences and use an 
alignment-based reordering method to im-
prove the alignment. After applying the reor-
dering method, we use two phrase tables in the 
decoding part to keep the translation perfor-
mance. Our experiments on Korean-Chinese 
show that our methods can improve the align-
ment and translation results. Since the pro-
posed approach reduces the size of the phrase 
table, multi-tables are considered. The combi-
nation of all these methods together would get 
the best translation result. 
1 Introduction 
Translating between two morphological different 
languages is more difficult in the descriptions by 
Koehn (2005). In Statistical Machine Translation 
(SMT) system, the surface word in a morpholog-
ically poor language is difficult to be generated 
from a morphologically richer language. Take 
the example of Korean and Chinese, their mor-
phologies are different from European languages. 
Korean is a kind of subject-object-verb (SOV) 
language while Chinese is subject-verb-object 
(SVO) language which is a little similar to Eng-
lish. This leads to a problem of word order: de-
spite the automatic word alignment tool GIZA++ 
(Och and Ney, 2003) is widely applied, there are 
still many generated misaligned language pairs 
among these two languages. 
In Korean, a functional word may have differ-
ent morphologies under different conditions. The 
verb and adjective usually end with suffixes in a 
sentence to represent different meanings (Li et al, 
2012). On the other hand, alignment mistakes are 
often generated when many Korean words with 
different morphologies are aligned with the same 
Chinese tokens in Korean-Chinese translation. 
We applied a simple but efficient approach by 
utilizing different part-of-speech (POS) infor-
mation to restructure Korean, after restructuring, 
many Korean words share the same Chinese 
meaning with different morphologies can be re-
stored to their original forms. In particular, we 
expect to reduce the problem of misalignment 
due to the verb and adjective variations. Besides 
word restructuring, an alignment?based word 
reordering method which would improve the 
alignment result indirectly was applied in our 
experiment. This method is simple but effective 
and language-independent by modifying some 
alignment files. The lack of the off-the-shelf Ko-
rean-Chinese corpus is also an important prob-
lem. Most of these corpora are not open source 
for users, so it is hard for people applying Kore-
an-Chinese corpus in the experiments like Euro-
parl (Koehn, 2005), we built a small size corpus 
by ourselves in a short time to do the experi-
ments based on the proposed methods. A script is 
developed for crawling parallel corpus of some 
specific websites. 
In this paper, section 2 will review previous 
related works. In section 3, the POS-based re-
structuring method and alignment-based reorder-
ing approaches to improve the quality of align-
ment will be introduced. Experimental results 
and the analysis will be given in the following 
section 4. Finally, section 5 is the conclusion. 
2 Related work 
Several studies have been proposed to use POS 
tags and morphological information to enrich 
82
languages to tackle some problems in SMT: Li et 
al. (2009) proposed an approach focused on us-
ing pre-processing and post-processing methods, 
such as reordering the source sentences in a Chi-
nese-Korean phrase-based SMT using syntactic 
information. Lee et al (2010) transformed the 
syntactic relations of Chinese SVO patterns and 
inserted the corresponding transferred relations 
as pseudo words to solve the problem of word 
order. In order to reduce the morpheme-level 
translation ambiguity in an English-Chinese 
SMT system, Wu et al (2008) grouped the mor-
phemes into morpheme phrase and used the do-
main information for translation candidate selec-
tion. A contraction separation for Spanish in a 
Spanish-English SMT system was proposed in 
(Gispert and Mari?o, 2008). Habash et al (2009) 
proposed methods to tackle the Arabic enclitics. 
The experiment in Stymne et al (2008) described 
that using POS information to split the com-
pounds in a morphologically rich language 
(German nouns and adjectives) gave an effect for 
translation output. Holmqvist et al (2009) also 
reported that using POS-based and morphology-
based sequence model would give an improve-
ment to the translation quality between English 
and German in WMT09 shared task.  
In accordance with adding richer information 
to the training model, reordering the source lan-
guage text to make it more similar to the target 
side is confirmed to be another kind of method to 
improve the word alignment. Collins et al (2005) 
employed the forms of syntactic analysis and 
hand?written rules on the corpus, Xia and 
McCord (2004) extracted the rules from a paral-
lel text automatically. A statistical machine pre-
reordering method which addressed the reorder-
ing problems as a translation from the source 
sentence to a monotonized source sentence was 
proposed by Costa-juss? and Fonollosa (2006). 
Visweswariah et al (2011) proposed a method 
which learns a model that can directly reorder 
source side text from a small parallel corpus with 
high quality word alignment, but this is hard for 
people to get such a high-quality aligned parallel 
corpus. Ma et al (2007) packed some words to-
gether with the help of the existing statistical 
word aligner, which simplify the task of automat-
ic word alignment by packing consecutive words 
together.  
These approaches are integrated with morpho-
logical information in the translation and decod-
ing model. Our approach is inspired by the ap-
proach proposed by Lee et al(2006) which added 
POS information; reordered the word sequence 
in the source corpus; deleted case particle and 
final ending words in Korean; appended the ex-
ternal dictionary in the training step between Ko-
rean and English. In the experiment reported by 
Li et al (2012), these pre-processing methods on 
the Korean to Chinese translation system took 
advantage of POS in their additional factored 
translation model. In these studies, POS infor-
mation was reported that it would improve the 
translation quality, but their taxonomy of POS 
tag is sole and less. On the word alignment side, 
we try to implement the idea proposed by 
Holmqvist et al (2012), which was reported as a 
simple, language-independent reordering method 
to improve the quality of word alignment. But 
their method did not consider the problem that 
the probability and the amount would be changed 
when updated with an improved word alignment. 
The accuracy of alignment would be improved 
but the size of phrase-table would be less than 
the original one because there are more sure 
alignments generated. The probabilities of word 
and phrase also have the same problem.  
Our works are based on the integration of 
these two methods. We utilized POS information 
and applied a richer taxonomy of POS tags in the 
restructuring of Korean, applied reordering 
method on Korean-Chinese, and combined the 
POS-based restructuring and alignment-based 
reordering together in the experiment. 
3 POS-based restructuring and align-
ment-based reordering 
The POS information is helpful when dealing 
with morphologically rich languages. In the 
morphological analysis, the Korean POS tagger 
involves the analytical task to identify the stem 
and suffixes of Korean, followed by assigning 
corresponding POS tags to both the morphemes 
and extracted stems. As described in Li et al 
(2012), Korean is considered as a highly aggluti-
native language: the verbs, adjectives and ad-
verbs are able to attach with affixes and particles. 
We considered that different category of POS tag 
would lead to different results of the translation. 
The more complex of tag would get a better re-
sult of alignment and the quality of translation. 
The method of processing Chinese POS is simi-
lar to Korean, which applies a more complex 
POS tag category from a Chinese POS tagger. 
Another simple but effective and language-
independent reordering method which
83
Figure 1. Different types of POS tag of Korean 
 
improves the quality of automatic word align-
ment is applied on Korean-Chinese. The method 
is implemented by modifying the alignment file 
in Moses (Koehn et al, 2007), which needs two 
runs of GIZA++. After this step, an improved 
word alignment is generated potentially. Then, 
we combine the restructuring and reordering to-
gether to compare the superposed quality of 
these two methods. 
3.1 POS-based restructuring 
Because Korean is a kind of morphologically 
rich language, most of Korean verbs, adjectives 
and adverbs can be taken as the compound words 
like Germany. For example, the negative verb 
??? ?? (do not go)? should be restored to its 
original form ??? (go)? and the negative verb 
suffix ?? ?? (do not)?. Another example is 
the future tense verb ???? (will go)? is the 
combination of original stem ?? (go)? and suf-
fix with future tense ??? (will)?. With the help 
of POS tagger, we can restructure the Korean 
with the 22 tags category instead of 9 tags in (Li 
et al, 2012). Here is an example of Korean re-
structuring in Figure 1, POS tagger can be de-
tected the compound word and analyze its com-
bination (tagged with ?+?). The taxonomy with 
22 tags is more specific than 9 tags, when tag-
ging a noun, 22 tags will use NC (normal noun) 
instead of N (noun, pronoun, numeral) in 9 tags. 
When dealing with the compound verb ???? 
(in order to avoid)?, ??? (avoid) +??? is 
more reasonable than ???+??, because 
???? represents ?in order to? in corresponding 
Chinese grammar. Then the tags were removed 
and restructured to a new sentence. After restruc-
turing, the length of original sentence increased 
from 5 to 9 (9 tags) and 10 (22 tags). Based on 
previous relative simple tags, more complex tax-
onomy gives a deeper analysis of the sentence 
which would influence the alignment and the 
lexical possibility between the source and target 
language.  
3.2 Alignment-based reordering 
The aim of utilizing the alignment information is 
to make the order in source text same as the tar-
get text. It is believed that statistical word align-
ment methods perform better on translation with 
similar word orders.  
The method needs two runs of word alignment, 
in the first run of GIZA++: the alignment infor-
mation is acquired based on the original order. 
Then the source text is reordered by the order of 
the target text based on the information in the 
first alignment. Next, the reordered source text 
and the original target text are applied on the 
second run of GIZA++, which means this new 
parallel corpus includes the word with more sim-
ilar order than before. After this step, a new 
alignment file would be generated, which covers 
potential improved word alignment with the re-
ordered source language. Finally, the order of 
source text in the new alignment file is restored 
in its original order but kept with its new align-
ment information. 
 
 
 
 
 
 
 
 
 
 
 
Figure 2. The alignment 
 
The algorithm processes the corpus and the 
alignment results in a single direction: source 
side (Holmqvist et al, 2012). As an example, in 
the Korean-Chinese single direction in Figure 2, 
each Korean word aligns with the corresponding 
Chinese word, but Chinese is different. There are 
cross alignments between ????, ????, 
?????, ?????, ???? and ????, ???, 
???? ???? ???? ??? ???. 
9 tags: 
????/N ??/N+?/X+?/E ????/N ??/N+?/J ??/P+?/E ./S 
22 tags: 
????/NC ??/NC+?/XS+?/EC ????/NC ??/NC+?/JC ??/PV+??/EC ./SF 
9 tags deleted: 
???? ?? ? ? ???? ?? ? ?? ? . 
22 tags deleted: 
???? ?? ? ? ???? ?? ? ?? ?? . 
First alignment: 
Bill?  ??  ????  ??  ??  ???  ???  ??  
 
 
?? 1? 2?? 3 6 7 8 ?? null ?? null ? 4?? 5 
(Bill is very quiet, try to encourage him to speak) 
Second alignment (reordered): 
Bill?  ??  ????  ???  ???  ??  ??  ?? 
 
 
?? 1? 2?? 3 4 ?? 5 6 ?? null ? 7?? 8 
84
????. Moreover, the alignment of these words 
is not totally correct. 
Before the second run of GIZA++, the original 
Korean is reordered to the alignment of the Chi-
nese side, so a new Korean sentence is generated 
by the Chinese order. After the second alignment, 
in Figure 2, there are no cross points and the ad-
ditional correct alignment between ?????, 
???? and ???? is generated, the misalign-
ment is decreased.  
 
 
 
 
 
 
Figure 3. The improved alignment  
 
In Figure 3, the new alignment information 
(new) is kept in the restored file. The crossing 
alignment still exists but it is more correct than 
the previous one (old). Based on this alignment, 
the establishment of word alignment, the estima-
tion of lexical translation table and the extraction 
of phrase table are changed.  
We assume that after applying our method, the 
size of the extracted words would increase be-
cause more alignments are generated at the end 
of the second run of GIZA++. Another assump-
tion is that the size of the phrase table would de-
crease if two languages share such a different 
word order, because additional alignments would 
result in some cross alignments but the phrase 
extraction algorithm could not extract them. 
Based on two assumptions, we utilize multiple 
models in the decoding stage. This approach was 
proposed in (Koehn and Schroeder, 2007; Axel-
rod et al, 2011) which passes phrase and reor-
dering tables in parallel. We used our modified 
tables (small size) as the main tables, and the 
baseline tables (big size) as the additional table 
when decoding. This can guarantee that if a 
phrase in testing sentence does not occur in the 
modified tables, the decoder would find the 
phrase in the original table. This method is effec-
tive in avoiding translation mistakes if our meth-
od harms the result.  
4 Experimental results 
We apply our methods on Korean-Chinese 
phrase-based statistical machine translation sys-
tems. The system is built based on Moses, and 
our reordering method is applied at the second 
step among the nine steps during the training in 
Moses. An additional combination of the POS-
based restructuring and alignment-based reorder-
ing is considered in our experiment.  
4.1 Corpus and system information 
The Korean-Chinese (KOR-CHN) corpus is 
crawled from the Internet by our script1 and we 
limited the length of sentence to be under 25 
words. We use 990 sentences as the testing cor-
pus. On the other hand, we use a monolingual 
corpus of 600k Chinese sentences to build a Chi-
nese 3-gram language model. ICTCLAS 2 is an 
open source Chinese segmenter applied to delim-
iter the word boundaries and label with proper 
POS tags, while the Korean text is processed by 
the Korean POS tagger, HanNanum 3. Table 1 
shows the average information of each corpus. 
All the experiment was trained without tuning.  
 
 Token Avg. 
Length 
Sentence 
CHN 664,290 7.36 90,237 
KOR 539,903 5.98 
KOR (9) 969,445 10.74 
KOR (22) 1,010,117 11.19 
 
Table 1. Summary of training corpora 
 
4.2 Korean-Chinese machine translation 
The Korean-Chinese translation system contains 
a reordering model in the translation model. The 
reordering model is trained as the default setting 
from the training corpus itself. The ?grow-diag-
final-and? symmetrization heuristic is applied in 
two directions word alignment. As described in 
the previous section, we restructured the Korean 
by POS tagger and applied our reordering ap-
proach to the translation system. Since the re-
structured Korean can be considered as a new 
corpus, it could be applied to our reordering 
method. 
According to the study of Holmqvist et al 
(2012), when dealing with the morphologically 
different language pair, reordering the morpho-
logically richer side performs better. In the ex-
periments, Korean was reordered and the exper-
imental result is shown in Table 2. 
From the results, applying more POS tags on 
the morphological analysis of Korean got a better 
performance and our reordering method im-
1 http://nlp2ct.sftw.umac.mo/views/tools/WebpageCrawler  
2 http://ictclas.nlpir.org/ 
3 http://semanticweb.kaist.ac.kr/home/index.php/HanNanum 
Bill? ?? ???? ?? ?? ??? ??? ?? 
 
 
?? 1? 2?? 3 6 ?? 7 8 ?? null ? 4?? 5 (new) 
?? 1? 2?? 3 6 7 8 ?? null ?? null ? 4?? 5  (old) 
                                                 
85
proved the translation result from 14.98 to 15.50 
in BLEU (Papineni et al, 2002). The combina-
tion of POS and reordering methods based on the 
multiple phrase tables and reordering tables got 
the best performance with BLEU score 17.35. 
 
Corpus KOR-
CHN 
BLEU 
Baseline 14.98 
POS-based (9 tags) 16.61 
POS-based (22 tags) 16.92 
Alignment-based 15.50 
POS (9 tags) + Alignment 16.71 
POS (22 tags) + Alignment 17.03 
POS (22 tags) + Alignment + two tables 17.35 
 
Table 2. The translation results 
 
4.3 Analysis and discussion 
After the modification of the alignment file, the 
changes of size of the lexical file and the tables 
(phrase and reordering) file are shown in Table 3. 
 
Tables KOR-CHN 
baseline 
KOR-CHN 
modified 
Word 12.39 MB 12.52 MB 
Phrase 19.41 MB 19.04 MB 
Reordering 10.01 MB 9.83 MB 
 
Table 3. The size changes of the word and 
phrase tables 
 
From the table we found that the lexical ex-
traction is bigger than the original system, but 
the size of phrase tables and the reordering tables 
decreased slightly. The result of these changes 
shows that our assumption is reasonable: our 
method can improve the quality of automatic 
alignment, but the phrase extracted from the cor-
pus would decrease. The more word alignment 
points were generated by using our method, the 
more words would be extracted. But this will 
bring some cross alignments when dealing with 
two morphological different languages. 
5 Conclusion 
In this paper, we presented some pre-
processing methods to deal with Korean, which 
is a morphological rich language. POS-based 
restructuring restores most of the Korean verbs, 
adjectives and adverbs to their original format. It 
is shown that the POS tag set with a richer tax-
onomy gives a higher translation result. Moreo-
ver, two runs of automatic alignment information 
got better results on the morphologically richer 
side. All of these methods can be combined to-
gether and improve the final translation. Finally, 
using two tables instead of one modified table in 
the decoding part will guarantee the translation 
quality if the reordering model harms the transla-
tion result.  
 
Acknowledgments 
 
This work is partially supported by the Research 
Committee of University of Macau, and Science 
and Technology Development Fund of Macau 
under the grants RG060/09-10S/CS/FST, and 
057/2009/A2. 
References  
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 
2011. Domain adaptation via pseudo in-domain da-
ta selection. In Proceedings of the Conference 
on Empirical Methods in Natural Language 
Processing. pages 355?362, Edinburgh, Scotland, 
UK. 
Michael Collins, Philipp Koehn, and Ivona Kucerov?. 
2005. Clause restructuring for statistical machine 
translation. In Proceedings of the 43rd Annual 
Meeting on Association for Computational 
Linguistics. pages 531?540, Ann Arbor, Michigan. 
Marta Ruiz Costa-juss? and Jos? A. R. Fonollosa. 
2006. Statistical machine reordering. In Proceed-
ings of the 2006 Conference on Empirical 
Methods in Natural Language Processing, 
pages 70?76, Sydney, Australia. 
Adri? de Gispert and Jos? B. Mari?o. 2008. On the 
impact of morphology in English to Spanish statis-
tical MT. Speech Communication. Pages 
50:1034?1046. 
Maria Holmqvist, Sara Stymne, Jody Foo, and Lars 
Ahrenberg. 2009. Improving alignment for SMT 
by reordering and augmenting the training corpus. 
In Proceedings of the Fourth Workshop on 
Statistical Machine Translation, Athens, 
Greece. 
Maria Holmqvist, Sara Stymne, Lars Ahrenberg, and 
Magnus Merkel. 2012. Alignment-based reordering 
for SMT. In Proceedings of the Eight Interna-
tional Conference on Language Resources and 
Evaluation (LREC?12). pages 3436?3440, Istan-
bul, Turkey. 
86
Nizar Habash, Owen Rambow, and Ryan Roth. 2009. 
MADA+TOKAN: A toolkit for arabic tokenization, 
diacritization, morphological disambiguation, pos 
tagging, stemming and lemmatization. In Pro-
ceedings of the 2nd International Conference 
on Arabic Language Resources and Tools 
(MEDAR), pages 102?109, Cairo, Egypt. 
Philipp Koehn. 2005. Europarl: A parallel corpus for 
statistical machine translation. In Machine Trans-
lation Summit X, pages 79-86, Phuket, Thailand. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, 
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin, and Evan Herbst. 2007. Moses: Open 
source toolkit for statistical machine translation. In 
Proceedings of the 45th Annual Meeting of the 
ACL, Demonstration session, Prague, Czech 
Republic. 
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine 
translation. In Proceedings of the Second Work-
shop on Statistical Machine Translation, pag-
es 224?227, Prague, Czech Republic. 
Shuo Li, Derek F. Wong, and Lidia S. Chao. 2012. 
Korean-Chinese statistical translation model. In 
Machine Learning and Cybernetics (ICMLC), 
2012 International Conference, 2:767-772, Xian, 
Shannxi, China. 
Jin-Ji Li, Jungi Kim, Dong-Il Kim, and Jong-Hyeok 
Lee. 2009. Chinese syntactic reordering for ade-
quate generation of Korean verbal phrases in Chi-
nese-to-Korean SMT. In Proceedings of the 
Fourth Workshop on Statistical Machine 
Translation, pages 190?196, Athens, Greece. 
Jae-Hee Lee, Seung-Wook Lee, Gumwon Hong, 
Young-Sook Hwang, Sang-Bum Kim, and Hae-
Chang Rim. 2010. A post-processing approach to 
statistical word alignment reflecting alignment ten-
dency between part-of-speeches. In Proceedings 
of the 23rd International Conference on Com-
putational Linguistics: Posters, pages 623?629, 
Beijing, China. 
Jonghoon Lee, Donghyeon Lee, and Gary Geunbae 
Lee. 2006. Improving phrase-based Korean-
English statistical machine translation. In Pro-
ceedings of the Ninth International Confer-
ence on Spoken Language Processing, Pitts-
burgh, Pennsylvania. 
Yanjun Ma, Nicolas Stroppa, and Andy Way. 2007. 
Bootstrapping word alignment via word packing. 
In Proceedings of Annual Meeting-association 
for Computational Linguistic, pages 304-311, 
Prague, Czech Republic. 
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment 
models. Computational Linguistics, 29(1):19?51. 
Kishore Papineni, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a method for automat-
ic evaluation of machine translation. In Proceed-
ings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics. pages 311?
318, Philadelphia, USA. 
Sara Stymne, Maria Holmqvist, and Lars Ahrenberg. 
2008. Effects of morphological analysis in transla-
tion between German and English. In Proceedings 
of the Third Workshop on Statistical Machine 
Translation, pages 135?138, Columbus, Ohio, 
USA. 
Karthik Visweswariah, Rajakrishnan Rajkumar, 
Ankur. Gandhe, Ananthakrishnan Ramanathan, and 
Jiri Navratil. 2011. A word reordering model for 
improved machine translation. In Proceedings of 
the Conference on Empirical Methods in Nat-
ural Language Processing, pages 486?496, Ed-
inburgh, Scotland, UK. 
Xianchao Wu, Naoaki Okazaki, Takashi Tsunakawa, 
and Jun?ichi Tsujii. 2008. Improving English-to-
Chinese translation for technical terms using mor-
phological information. In Proceedings of the 
8th AMTA Conference, Hawaii, USA. 
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical MT system with automatically learned re-
write patterns. In Proceedings of the 20th Inter-
national Conference on Computational Lin-
guistics, pages 508, Geneva, Switzerland. 
 
 
87
Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 34?42,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
UM-Checker: A Hybrid System for English Grammatical Error Cor-
rection 
 
 
Junwen Xing, Longyue Wang, Derek F. Wong, Lidia S. Chao, Xiaodong Zeng 
Natural Language Processing & Portuguese-Chinese Machine Translation Laboratory,  
Department of Computer and Information Science, 
University of Macau, Macau S.A.R., China 
nlp2ct.{vincent, anson}@gmail.com,  
{derekfw, lidiasc}@umac.mo, nlp2ct.samuel@gmail.com 
 
  
 
Abstract 
This paper describes the NLP2CT Grammati-
cal Error Detection and Correction system for 
the CoNLL 2013 shared task, with a focus on 
the errors of article or determiner (ArtOrDet), 
noun number (Nn), preposition (Prep), verb 
form (Vform) and subject-verb agreement 
(SVA). A hybrid model is adopted for this spe-
cial task. The process starts with spell-
checking as a preprocessing step to correct any 
possible erroneous word. We used a Maxi-
mum Entropy classifier together with manual-
ly rule-based filters to detect the grammatical 
errors in English. A language model based on 
the Google N-gram corpus was employed to 
select the best correction candidate from a 
confusion matrix. We also explored a graph-
based label propagation approach to overcome 
the sparsity problem in training the model. Fi-
nally, a number of deterministic rules were 
used to increase the precision and recall. The 
proposed model was evaluated on the test set 
consisting of 50 essays and with about 500 
words in each essay. Our system achieves the 
5
th
 and 3
rd
 F1 scores on official test set among 
all 17 participating teams based on gold-
standard edits before and after revision, re-
spectively.  
1 Introduction 
With the increasing number of people all over 
the world who study English as their second lan-
guage1, grammatical errors in writing often oc-
curs due to cultural diversity, language habits, 
education background, etc. Thus, there is a sub-
stantial and increasing need of using computer 
                                                 
    1 A well-known fact is that the most popular language 
chosen as a first foreign language is English. 
techniques to improve the writing ability for sec-
ond language learners. Grammatical error correc-
tion is the task of automatically detecting and 
correction erroneous word usage and ill-formed 
grammatical constructions in text (Dahlmeier et 
al., 2012). 
In recent decades, this special task has gained 
more attention by some organizations such as the 
Helping Our Own (HOO) challenge (Dale and 
Kilgarriff, 2010; Dale et al, 2012). Although the 
performance of grammatical error correction sys-
tems has been improved, it is still mostly limited 
to dealing with the determiner and preposition 
error types with a very low recall and precision. 
This year, the CoNLL-2013 shared task extends 
to include a more comprehensive list of error 
types, as shown in Table 1. 
To take on this challenge, this paper proposes 
pipe-line architecture in combination with sever-
al error detection and correction models based on 
a hybrid approach. As a preprocessing step we 
firstly employ a spelling correction to correct the 
misspelled words. To correct the grammatical 
errors, a hybrid system is designed that integrat-
ed with Maximum Entropy (ME) classifier, de-
terministic filter and N-gram language model 
scorer, each of which is constructed as an indi-
vidual model. According to the phenomena of 
the problems, we use different combinations of 
the models trained on specific data to tackle the 
corresponding types of errors. For instance, Prep 
and Nn have a strong inter-relation with the 
words (surface) that are preceding and following 
the active word. This can be detected and recov-
ered by using a language model. On the other 
hand, SVA is more complicated and it is more 
effective to determine the mistakes by using the 
linguistic and grammatical rules. The correction
34
Error Type Description Example 
Vform 
Replacement The solution can be obtain (obtained) by using technology. 
Insertion 
However, the world has always beyond our imagination and ? (has) 
never let us down. 
Deletion It also indicates that the economy has been (?) dramatically grown. 
SVA 
Subject-verb-
Agreement 
My brothers is (are) nutritionists. 
ArtOrDet 
Replacement 
The leakage of these (this) confidential information can be a sensitive 
issue to personal, violation of freedom and breakdown of safety. 
Insertion The survey was done by ? (the) United Nations. 
Deletion 
The air cargo of the (?) Valujet plane was on fire after the plane had 
taken off. 
Nn Noun number He receives two letter (letters). 
Prep 
Replacement They work under (in) a conductive environment. 
Insertion 
Definitely, there are point of view that agree ? (with) the technology 
but also the voices of objection. 
Deletion 
Today, the surveillance technology has become almost manifest to (?) 
wherever we go. 
 
Table 1: The error types with descriptions and examples. 
 
components are combined into a pipeline of cor-
rection steps to form an end-to-end correction 
system. Different types of corrections may inter-
act with each other. Therefore, only for each fo-
cus word in a sentence will pass the filter and 
predict by the system. 
Take the sentence for example, ?The patent 
applications do not need to be censored.?, if the 
word ?applications? is changed to ?application? 
(Nn error) by a correction module, then the fol-
lowing auxiliary verb ?do? should be revised to 
?does? (SVA error) accordingly. That is, if a mis-
take is introduced by a component in the prior 
step, subsequent analyses are most likely affect-
ed negatively. To avoid the errors propagated 
into further components, we proposed to deploy 
the analytical (pipelined) components in the or-
der of Nn, ArtOrDet, Vform, SVA and Prep. 
For non-native language learners, over 90% 
usage of prepositions and articles are correctly 
used, which makes the errors very sparse (Ro-
zovskaya and Roth, 2010c) in a text, and about 
10% error is not ?sparse? by the way. This factor 
severely restricts the improvement of data-driven 
systems. Different from the previous methods to 
overcome error sparsity, we explored a graph-
based label propagation method that makes use 
of the prediction on large amount of unlabeled 
data. The predicted data are then used to 
resample our training data. This semi-supervised 
method may fix a skewed label distribution in the 
training set and is helpful to enhance the models.  
The paper is organized as follows. We firstly 
review and discuss the related work. The data 
used to construct the models is described in Sec-
tion 3. Section 4 discusses the proposed model 
based on semi-supervised learning, and the over-
all hybrid system is given in Section 5. The 
methods of grammatical error detection and cor-
rection are detailed in Section 6, followed by an 
evaluation, discussion and a conclusion to end 
the paper. 
2 Related Work 
The issues of grammatical error correction have 
been discussed from different perspectives for 
several decades. In this section, we briefly re-
view some related methods. 
The use of machine learning methods to tackle 
this problem has shown a promising perfor-
mance. These methods are normally created 
based on a large corpus of well-formed native 
English texts (Tetreault and Chodorow 2008; 
Tetreault et al, 2010) or annotated non-native 
data (Gamon, 2010; Han et al, 2010). Although 
the manually error-tagged text is much more ex-
pensive, it has shown improvements over the 
models trained solely on well-formed native text 
(Kochmar et al, 2012). Additionally, both gener-
ative and discriminative classifiers were widely 
used. Among them, Maximum Entropy was gen-
erally used (Rozovskaya and Roth, 2011; 
Sakaguchi et al, 2012; Quan et al, 2012) and 
obtained a good result for preposition and article 
correction using a large feature set. Naive Bayes 
35
were also applied to recognize or correct the er-
rors in speech or texts (Lynch et al, 2012). How-
ever, only using classifiers always cannot give a 
satisfied performance. Thus, grammar rules and 
probabilistic language model can be used as a 
simple but effective assistant for correction of 
spelling (Kantrowitz et al 2003) and grammati-
cal errors (Dahlmeier et al, 2012; Lynch et al, 
2012; Quan et al, 2012; Rozovskaya et al, 
2012). 
3 Data Set 
The training data is the NUS Corpus of Learner 
English (NUCLE) that provided by the National 
University of Singapore (Dahlmeier et al, 2013). 
The NUCLE contains more than one million 
words (1,400 essays) and has been annotated 
with error-tags and correction-labels. There are 
27 categories of errors, with 45,106 errors in to-
tal. In this CoNLL-2013 shared task, five types 
of errors (around 32% of the total errors) are 
concerned. Figure 1 shows the statistics infor-
mation of error types. 
 
 
 
Figure 1. The distribution of different error types in 
the training set. 
 
As the distribution of different errors respects 
the real environment, there is a serious problem 
hidden in it. Roughly estimated, the ratio be-
tween the correct and error classes in NUCLE is 
around 100:1, or even more. The imbalance 
problem may be heavily harmful to machine 
learning methods. Therefore, researchers (Ro-
zovskaya et al, 2012; Dahlmeier et al, 2012) 
provided several approaches such as reducing 
correct instances to deal with error sparsity. In-
stead of downsampling the data, we try to up-
sample error instances. Different from UI system 
(Rozovskaya et al, 2012) which simulates learn-
ers to make mistakes artificially, we propose a 
semi-supervised learning method that makes use 
of a large amount of unlabeled data which is easy 
to collect. In practice, semi-supervised learning 
requires less human effort and gives higher accu-
racy in creating a model.  
4 Error Examples Expansion Using 
Graph-Based Label Propagation  
As mentioned before, the corpus contains a low 
amount of error examples, which results in a 
high sparsity in the label distribution. In reality, 
the balance between the error and correct data is 
crucial for training a robust grammar detection 
models. Our experiment results demonstrate that 
too many correct data lead to unfavorable error 
detection rate. In order to resolve this obstacle, 
this paper introduces to using external data 
sources, i.e., a large amount of easily accessible 
raw texts, to automatically achieve more labeled 
example for training a stronger model. This pa-
per employs transductive graph-based semi-
supervised learning approach. 
4.1 Graph-Based Label Propagation 
Graph-based label propagation is one of the criti-
cal subclasses of SSL. Graph-based label propa-
gation methods have recently shown they can 
outperform the state-of-the-art in several natural 
language processing (NLP) tasks, e.g., POS tag-
ging (Subramanya et al, 2010), knowledge ac-
quisition (Talukdar et al, 2008), shallow seman-
tic parsing for unknown predicate (Das and 
Smith, 2011).  This study uses graph SSL to en-
rich training data, mainly the examples with in-
correct tag, from raw texts.  
This approach constructs a k nearest-neighbor 
(k-nn) similarity graph over the labeled and un-
labeled data in the first step. The vertices in the 
constructed graph consist of all instances (feature 
vector) that occur in labeled and unlabeled text, 
and edge weights between vertices are computed 
using their Euclidean distance. Pairs of vertices 
are connected by weighted edges which encode 
the degree to which they are expected to have the 
same label (Zhu, 2003). In the second step, label 
propagation operates on the constructed graph. 
The primary objective is to propagate labels from 
a few labeled vertices to the unlabeled ones by 
optimizing a loss function based on the con-
straints or properties derived from the graph, e.g. 
smoothness (Zhu et al, 2003; Subramanya and 
Bilmes, 2008; Talukdar et al, 2009), or sparsity 
(Das and Smith, 2012). This paper uses propaga-
tion method (MAD) in (Talukdar et al, 2009).  
Vform
9%
SVA
10%
ArtOrDet
42%
Nn
24%
P ep
15%
36
  
 
Figure 2. Workflow of our proposed system. 
4.2 Implementation 
In this paper, the labeled data is taken from NU-
CLE corpus. They are regarded as the ?seed? 
data, including 93,000 correct and 1,200 incor-
rect instances. The unlabeled data is collected 
from the English side of news magazine corpus 
(LDC2005T10). Based on that, a 5-NN similarity 
graph is constructed. With the graph and the 
properties of the labeled data derived from the 
NUCLE, the MAD algorithm is used to propa-
gate the error-tag (label) from labeled vertices to 
the unlabeled vertices. Afterwards, the unlabeled 
examples with incorrect tag are added into the 
original training data for training. 
5 System Description 
This section describes the details of our system, 
including preprocessing of training set, confusion 
set generating, classifier training and language 
models building. The grammatical error correc-
tion procedure is shown in Figure 2. 
5.1 Preprocessing 
As mentioned in Section 3, there is a large 
amount (68%) of other error types which may 
result in new errors or confuse the system with 
wrong information in correction. In order to 
make the best use of the corpus, it needs to filter 
all errors not covered by the CoNLL 2013 shared 
task, and then generate a separate corpus for each 
error type. Therefore, we recovered other irrele-
vant errors accordingly. For each error type, we 
also recover other 4 types of errors, and then we 
got a pure training data set which only includes 
one error type.  
For the misspelled problem, we used an open 
source toolkit (JMySpell2) which allows us to 
use the dictionaries form OpenOffice. JMySpell 
                                                 
    2 Available at https://kenai.com/projects/jmyspell. 
gives a list of suggestion candidate words, and 
we select the first one to replace the original 
word.  
5.2 Confusion Set Generating 
Confusion sets include the correction candidates 
which are used to modify the wrong places of a 
sentence. We generated a confusion set for each 
type of error correction component.  
The confusion set for Nn, Vform and SVA was 
built on Penn Treebank3. The format can be de-
scribed as that each prototype word follows all 
possible combinations with Part-Of-Speech (POS) 
and variants. For instance, the format of the word 
?look? in confusion set should looks like ?look 
look#VB look#VBP looking#VBG looks#VBZ 
looked#VBN look#NN looks#NNS?. The proto-
type ?look? and POS are the constraints for 
choosing the correct candidate. In order to quick-
ly find the candidates according to each detected 
error place, we indexed the confusion set in Lu-
cene4 which is another open source toolkit with a 
high-performance, full-featured text search en-
gine library. 
For ArtOrDet and Prep, the confusion sets are 
manually created because the possible modifica-
tions are not so many which are discussed in 
Section 6.1 and 6.2. 
5.3 Maximum Entropy Classifier 
The machine learning algorithm we used to train 
the detection models is Maximum Entropy (ME), 
which can classify the data by giving a probabil-
ity distribution. It is similar to multiclass logistic 
regression models, but much more profitable 
with sparse explanatory feature vectors. For ME 
classifier, the feature of text data is suitable for 
training the model, so we choose it as our detec-
tion classifier.  
                                                 
    3 Available at http://www.cis.upenn.edu/~treebank/. 
    4 Available at http://lucene.apache.org/. 
Source
Text
Rule-based 
Filter
ArtOrDet
LM Scorer
Nn
ME
classifier
LM Scorer
Vform
SVA
ME
classifier
Rule-based 
Filter
Rule-based 
Filter
Rule-based 
Filter
Prep
LM Scorer
Hybrid System
Correct
Text
37
We employed Stanford Classifier5 which is a 
Java implementation of maximum entropy 
(Manning & Klein, 2003).  
5.4 N-gram Language Model 
The probabilistic language model is constructed 
on Google Web 1T 5-gram corpus (Brants and 
Franz, 2006) by using the SRILM toolkit 
(Stolcke, 2002). All generated modification can-
didates are scored by it and only candidates that 
strictly increase than a threshold can be kept.  
The normalized language model score is de-
fined as 
1 log Pr( )lmscore ss?
                (1) 
in which s is the corrected sentence and |s| is the 
sentence length in tokens (Dahlmeier et al, 
2012). 
6 Grammatical Error Correction 
6.1 Article and Determiner 
The component for ArtOrDet task integrates with 
the language model and rule-based techniques. 
Language models are constructed to select the 
best candidate from a confusion set of possible 
article choices {a, the, an, ?}, given the pre-
corrected sentence. Each Noun Phrase (NP) in 
the test sentence will be pre-corrected as correc-
tion candidates. However, only using a language 
model to determine the best correction will often 
result in a low precision, because a certain 
amount of correct usages of ArtOrDet are mis-
judged. 
In order to avoid this problem, we proposed a 
voting method based on multiple language mod-
els. We integrated two separate language models: 
one was converted from the large Google corpus 
(general LM) and the other one was constructed 
from a small in-domain corpus (in-domain LM). 
Additionally, the in-domain corpus involves two 
parts. One is the training data which has been 
totally corrected according to the gold answer. 
The other one includes the sentences which are 
similar to the test set. We extracted them from 
some well-formed native English corpora such as 
English News Magazine of LDC2005T106 using 
term frequency-inverse document frequency (TF-
IDF) as the similarity score. Each document Di is 
                                                 
    5 Available at 
http://nlp.stanford.edu/software/classifier.shtml. 
    
6
 Available at http://www.ldc.upenn.edu/Catalog/catalog 
Entry.jsp?catalogId=LDC2005T10. 
represented as a vector (wi1, wi2,?, win), and n is 
the size of the vocabulary. So wij is calculated as 
follows: 
 )log( jijij idftfw ??  (2) 
where tfij is term frequency (TF) of the j-th word 
in the vocabulary in the document Di, and idfj is 
the is the inverse document frequency (IDF) of 
the j-th word calculated. The similarity between 
two sentences is then defined as the cosine of the 
angle between two vectors.  
Each candidate sentence will be scored by 
these two LMs and compared with a threshold. 
Only if both of the LMs agree, the modification 
will be kept. We believe this method could filter 
a lot of wrong modification and improve the pre-
cision. 
6.2 Preposition 
For Prep error type, we used the same method as 
ArtOrDet. The only difference is confusion ma-
trix. Our system corrects the unnecessary, miss-
ing and unwanted errors for the five most fre-
quently prepositions which are in, for, to, of and 
on. While developing our system, we found that 
adding more prepositions did not increase per-
formance in our experiments. Thus the confusion 
set is {in, for, to, of, on, ?}. 
6.3 Noun Number 
A single noun in the sentence that is hard to dis-
tinguish whether it is singular or plural, so we 
treat a noun phrase as a observe subject. Our 
strategy of correcting noun number error is to use 
a filter contains rule-based and machine learning 
method. It can filter a part of nouns that absolute-
ly right, and the rest of nouns will be detected by 
the language model generated by SRILM7. 
The rule-based filter of our system contains 
several criteria. It can detect the noun phrase by 
article, i.e. it can simply find out that the noun is 
singular which with an article of ?a? or ?an?. 
The determiner and cardinal number also will be 
taken into consider by the rule-based model such 
as ?I have three apple.?, then system can find out 
the ?apple? should be ?apples?. The correct noun 
will keep the original one, and the incorrect noun 
will be replaced with a new candidate. 
After the first level filtering by the rules, the 
rest of noun phrases are indeterminacy by system. 
Therefore, we use a ME classifier for further fil-
tering. We use lexical, POS and dependency 
                                                 
    7 http://www.speech.sri.com/projects/srilm/. 
38
parse information as features. The features are 
listed in Table 2.  
In previous steps, most of the error can be de-
tected, but also it may give a lot of wrong sug-
gests, in order to reduce this situation, we use N-
gram language model scorer to evaluate on the 
candidates and choose the highest probability 
one. 
 
Feature Example 
Observer word 
Word (w0) resource 
POS (p0) NN 
First word in NP 
Word (wNP-1st) a 
POS (pNP-1st) DT 
Dependency Relation det 
Previous word before observed word 
Word (w-1) good 
POS (p-1) JJ 
Word after observed word 
Word (w1) and 
POS (p1) CC 
Head word of observed word 
Word (whead) water 
POS (phead) NN 
Dependency relation rcomd 
Word Combination 
w0 + wNP-1st resource + a 
w0 + w-1 resource + good 
w0 + w1 resource + and 
w0 + whead resource + water 
wNP-1st + whead a + water 
POS Combination 
p0 + pNP-1st NN + DT 
p0 + p-1 NN + JJ 
p0 + p1 NN + CC 
p0 + phead NN + NN 
pNP-1st + phead DT + NN 
 
Table 2: Features for Nn and the example: ?An exam-
ple is water which is a good resource and is plentiful.? 
6.4 Verb Form 
Determining the correct form of a verb in Eng-
lish is complex, involving a relatively wide range 
of choices. A verb can have many forms, such as 
base, gerund, preterite, past participle and so on. 
To detect the tense of verb error is much more 
related to the semantics level than syntax level. 
Therefore, it is hard to extract a common feature 
for training model. We chose to separate it into 
several problems and use rule-based model to do 
the Vform correction. 
For auxiliary verbs, there are three categories, 
one is modal verbs (do, can, may, will, might, 
should, must, need and dare), the other is the 
form of ?be? and ?have?. In a verb phrase, nor-
mally modals precede ?have? and ?be?, and 
?have? proceed ?be?, then we can get the order-
ing like this: Modal, Have, Be. Auxiliary verbs 
can incorporate with other verbs, and have dif-
ferent combination. Based on the previous study 
of the core language engine (Alshawi, 1992), we 
define the rules that contain the type of verb, 
which tense of verbs can be used with, and their 
entries in the lexicon. For example: 
 
(can (aux (modal) (vform pres)  (COMPFORM bare)) 
 
This means ?can? is a modal verb, it can be 
used with a verb that in the present tense, when 
?can? used alone with the main verb should as 
complement the base (bare) form. In here, the 
COMPFORM attribute is the entry condition in 
the grammar.  
6.5 Subject-Verb Agreement 
The basic principle of Subject-Verb Agreement 
is singular subjects need singular verbs; plural 
subjects need plural verbs, such as following sen-
tences: 
My brother is a nutritionist. 
My sisters are dancers. 
Therefore, the subject of the sentence is the 
key point. To decide whether the verb is singular 
or plural should look into the context and find 
out the POS of the subject. We utilize the exist-
ing information given by NUCLE to extract the 
subject of the verb. For example, the sentence 
?Statistics show that the number are continuing 
to grow with the existing population explosion.? 
Figure 3 shows the parse tree of this sentence. 
 
Figure 3. Parse tree of the example sentence. 
Root
S1
NP1
VP1
VBP1NNPS
SBAR
IN1
S2
NP2 VP2
?DT2 NN2 VBP2
arenumberthe
that
showStatistics
.
.
?
39
Through Figure 3, the observed words are 
?show? and ?are?, the subjects are ?statistics? 
and ?number? respectively that we can conclude 
?statistics? should use plural verb and ?number? 
should use singular verb ?is? instead of ?are?. 
The other features extracted for training are 
listed in Table 3. 
 
Feature Example 
Observer word 
Word (w0) are 
POS (p0) VBP 
Subject NP 
First word (wNP-1st) the 
POS of first word (pNP-1st) DT 
Head word (wNP-head) number 
POS of head word (pNP-head) NN 
Previous word before observed word 
Word (w-1) number 
POS (p-1) NN 
NP after observed word 
First word (wNPa-1st) the 
POS of first word (pNPa-1st) DT 
Head word (wNPa-head) explosion 
POS of head word (pNPa-head) NN 
Word combination 
w0 + wNP-1st are + the 
w0 + wNP-head are + number 
w0 + w-1 are + number 
w0 + wNPa-1st are + the 
w0 + wNPa-head are + explosion 
POS combination 
p0 + pNP-1st VBP + DT 
p0 + pNP-head VBP + NN 
p0 + p-1 VBP + NN 
p0 + pNPa-1st VBP + DT 
p0 + pNPa-head VBP + NN 
 
Table 3: Features for SVA and the example: ?Statis-
tics show that the number are continuing to grow with 
the existing population explosion.? 
 
The purpose of extracting the noun phrase af-
ter the observed word is in the situation of the 
subject is after the verb, such as ?Where are my 
scissors??, ?scissors? is the subject of this sen-
tence. 
7 Evaluation and Discussion 
The evaluation is provided by the organizer and 
generated by M2 scorer (Dahlmeier & Ng, 2012). 
The result consists of precision, recall and F-
score. Our grammatical error correction system 
has proposed 1,011 edits. The evaluation result 
of our system output for the CoNLL-2013 test 
data is shown in Table 4. 
 
Results Precision Recall F-score 
Before 
Revision 
0.2849 0.1753 0.2170 
After  
Revision 
0.3712 0.2366 0.2890 
 
Table 4: Evaluation result of Precision, Recall and F-
score. 
 
Error Type Error # Correct # % 
ArtOrDet 690 145 21.01 
Nn 396 92 23.23 
Vform 122 8 6.55 
SVA 124 37 29.83 
Prep 311 6 1.93 
 
Table 5: Detail information of evaluation result (Be-
fore Revision). 
 
Error Type Error # Correct # % 
ArtOrDet 725 177 24.42 
Nn 484 132 27.27 
Vform 151 16 10.60 
SVA 138 47 34.06 
Prep 325 9 2.77 
 
Table 6: Detail information of evaluation result (After 
Revision). 
 
The data in table 5 and 6 are the detailed in-
formation for each error type which was calcu-
lated by us, the table 5 is the data before revision, 
and the table 6 is that after revision. Second col-
umn is the amount of the gold edits, and the third 
column is the amount of our correct edits, and 
the last column is the percentage of correct edits. 
We analyzed the results in detail, and found sev-
eral critical reasons of causing low recall. Firstly, 
the five error types are associated relatively, if 
one is modified, it may cause a chain reaction, 
such as the article will affect the noun number, 
and the noun number will cause the SVA errors. 
Some Nn errors still cannot be detected or given 
a wrong correction by our system, which de-
creases the precision and recall of SVA. Another 
reason is our system does not perform well in 
Vform and Prep error correction. In our output, 
just a few errors have been revised. This means 
the quantity of correction rules is not enough that 
cannot cover all the linguistic phenomena. For 
40
instance, the situation of missing verb or unnec-
essary verb cannot be detected. On the other 
hand, the hybrid method of our system has fil-
tered some wrong suggestion candidates that im-
prove the precision. 
8 Conclusion 
We have presented the hybrid system for English 
grammatical error correction. It achieves a 28.9% 
F1-score on the official test set. We believe that if 
we find more appropriate features, our system 
can still be improved and achieve a better per-
formance. 
 
Acknowledgments 
 
The authors are grateful to the Science and 
Technology Development Fund of Macau and 
the Research Committee of the University of 
Macau for the funding support for our research, 
under the reference No. 017/2009/A and 
MYRG076(Y1-L2)-FST13-WF. The authors also 
wish to thank the anonymous reviewers for many 
helpful comments as well as Liangye He, Yuchu 
Lin and Jiaji Zhou who give us a lot of help. 
References  
Hiyan Alshawi. 1992. The core language engine. The 
MIT Press. 
Jon Louis Bentley. 1980. Multidimensional divide-
and-conquer. Communications of the ACM, 
23:214?229. 
Alina Beygelzimer, Sham Kakade, and John Lang-
ford. 2006. Cover trees for nearest neighbor. In: 
Proceedings of the 23rd International Confer-
ence on Machine Learning, pp. 97?104. 
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram Version 1. Linguistic Data Consortium, 
Philadelohia, PA. 
Olivier Chapelle, Bernhard Sch?lkopf, Alexander 
Zien, and others. 2006. Semi-supervised learning. 
MIT press Cambridge. 
Daniel Dahlmeier, Hwee Tou Ng, and Eric Jun Feng 
Ng. 2012. NUS at the HOO 2012 Shared Task. In: 
Proceedings of the Seventh Workshop on 
Building Educational Applications Using 
NLP, pp. 216?224. 
Daniel Dahlmeier & Hwee Tou Ng, and Siew Mei 
Wu (2013). Building a Large Annotated Corpus of 
Learner English: The NUS Corpus of Learner Eng-
lish. To appear in Proceedings of the 8th Work-
shop on Innovative Use of NLP for Building 
Educational Applications (BEA 2013). Atlanta, 
Georgia, USA. 
Daniel Dahlmeier, and Hwee Tou Ng (2012). Better 
Evaluation for Grammatical Error Correction. 
Proceedings of the 2012 Conference of the 
North American Chapter of the Association 
for Computational Linguistics (NAACL 2012), 
pp. 568 ? 572. 
Robert Dale, Ilya Anisimoff, and George Narroway. 
2012. HOO 2012: A report on the preposition and 
determiner error correction shared task. In: Pro-
ceedings of the Seventh Workshop on Building 
Educational Applications Using NLP, pp. 54?
62. 
Robert Dale and Adam Kilgarriff. 2011. Helping our 
own: The HOO 2011 pilot shared task. In: 
Proceedings of the 13th European Workshop 
on Natural Language Generation, pp. 242?249. 
Dipanjan Das and Noah A. Smith 2012. Graph-based 
lexicon expansion with sparsity-inducing penalties. 
In: Proceedings of the 2012 Conference of the 
North American Chapter of the Association 
for Computational Linguistics: Human Lan-
guage Technologies, pp. 677?687. 
Michael Gamon. 2010. Using mostly native data to 
correct errors in learners? writing: a meta-classifier 
approach. In: Human Language Technologies: 
The 2010 Annual Conference of the North 
American Chapter of the Association for 
Computational Linguistics, pp. 163?171. 
Andrew B. Goldberg and Xiaojin Zhu. 2006. Seeing 
stars when there aren?t many stars: graph-based 
semi-supervised learning for sentiment categoriza-
tion. In: Proceedings of the First Workshop on 
Graph Based Methods for Natural Language 
Processing, pp. 45?52. 
Na-Rae Han, Joel Tetreault, Soo-Hwa Lee, and Jin-
Young Ha. 2010. Using an error-annotated learner 
corpus to develop an ESL/EFL error correction 
system. In: Proceedings of LREC, pp. 763?770. 
Mark Kantrowitz. 2003. Method and apparatus for 
analyzing affect and emotion in text. U.S. Patent 
No. 6,622,140. 
Ekaterina Kochmar. 2011. Identification of a writer?s 
native language by error analysis. Master?s thesis, 
University of Cambridge. 
Gerard Lynch, Erwan Moreau, and Carl Vogel. 2012. 
A Naive Bayes classifier for automatic correction 
of preposition and determiner errors in ESL text. 
In: Proceedings of the Seventh Workshop on 
Building Educational Applications Using 
NLP, pp. 257?262. 
41
Christopher Manning and Dan Klein. 2003. Optimiza-
tion, Maxent Models, and Conditional Estimation 
without Magic. Tutorial at HLT-NAACL 2003 
and ACL 2003. 
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian 
Hadiwinoto, and Joel Tetreault (2013). The 
CoNLL-2013 Shared Task on Grammatical Error 
Correction. To appear in Proceedings of the Sev-
enteenth Conference on Computational Natu-
ral Language Learning. 
Li Quan, Oleksandr Kolomiyets, and Marie-Francine 
Moens. 2012. KU Leuven at HOO-2012: a hybrid 
approach to detection and correction of determiner 
and preposition errors in non-native English text. 
In: Proceedings of the Seventh Workshop on 
Building Educational Applications Using 
NLP, pp. 263?271. 
Juan Ramos. 2003. Using tf-idf to determine word 
relevance in document queries. In: Proceedings of 
the First Instructional Conference on Machine 
Learning. 
Alla Rozovskaya and Dan Roth. 2010. Training para-
digms for correcting errors in grammar and usage. 
In: Human Language Technologies: The 2010 
Annual Conference of the North American 
Chapter of the Association for Computational 
Linguistics, pp. 154?162. 
Alla Rozovskaya, Mark Sammons, and Dan Roth. 
2012. The UI system in the HOO 2012 shared task 
on error correction. In: Proceedings of the Sev-
enth Workshop on Building Educational Ap-
plications Using NLP, pp. 272?280. 
Keisuke Sakaguchi, Yuta Hayashibe, Shuhei Kondo, 
Lis Kanashiro, Tomoya Mizumoto, Mamoru Ko-
machi, and Yuji Matsumoto. 2012. NAIST at the 
HOO 2012 Shared Task. In: Proceedings of the 
Seventh Workshop on Building Educational 
Applications Using NLP, pp. 281?288. 
Andreas Stolcke and others. 2002. SRILM-an exten-
sible language modeling toolkit. In: Proceedings 
of the International Conference on Spoken 
Language Processing, pp. 901?904. 
Partha Pratim Talukdar and Koby Crammer. 2009. 
New regularized algorithms for transductive learn-
ing. In: Machine Learning and Knowledge 
Discovery in Databases. Springer, pp. 442?457. 
Joel Tetreault, Jennifer Foster, and Martin Chodorow. 
2010. Using parse features for preposition selection 
and error detection. In: Proceedings of the Acl 
2010 Conference Short Papers, pp. 353?358. 
Joel R. Tetreault and Martin Chodorow. 2008. The 
ups and downs of preposition error detection in 
ESL writing. In: Proceedings of the 22nd Inter-
national Conference on Computational Lin-
guistics Volume 1, pp. 865?872. 
 
42
Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 83?90,
Baltimore, Maryland, 26-27 July 2014. c?2014 Association for Computational Linguistics
Factored Statistical Machine Translation for Grammatical Error 
Correction 
 
 
Yiming Wang, Longyue Wang, Derek F. Wong, Lidia S. Chao, Xiaodong Zeng, Yi Lu 
Natural Language Processing & Portuguese-Chinese Machine Translation Laboratory,  
Department of Computer and Information Science, 
University of Macau, Macau S.A.R., China 
{wang2008499,vincentwang0229}@gmail.com,derekfw@umac.mo, 
lidiasc@umac.mo,nlp2ct.samuel@gmail.com,mb25435@umac.mo 
 
  
 
Abstract 
This paper describes our ongoing work on 
grammatical error correction (GEC). Focusing 
on all possible error types in a real-life 
environment, we propose a factored statistical 
machine translation (SMT) model for this task. 
We consider error correction as a series of 
language translation problems guided by 
various linguistic information, as factors that 
influence translation results. Factors included 
in our study are morphological information, i.e. 
word stem, prefix, suffix, and Part-of-Speech 
(PoS) information. In addition, we also 
experimented with different combinations of 
translation models (TM), phrase-based and 
factor-based, trained on various datasets to 
boost the overall performance. Empirical 
results show that the proposed model yields an 
improvement of 32.54% over a baseline 
phrase-based SMT model. The system 
participated in the CoNLL 2014 shared task 
and achieved the 7
th
 and 5
th
 F0.5 scores
1 on the 
official test set among the thirteen 
participating teams. 
 
1 Introduction 
The task of grammatical error detection and 
correction (GEC) is to make use of 
computational methods to fix the mistakes in a 
written text. It is useful in two aspects. For a 
non-native English learner it may help to 
improve the grammatical quality of the written 
text. For a native speaker the tool may help to 
remedy mistakes automatically. Automatic 
                                                          
1  These two rankings are based on gold-standard edits 
without and with alternative answers, respectively. 
correction of grammatical errors is an active 
research topic, aiming at improving the writing 
process with the help of artificial intelligent 
techniques. Second language learning is a user 
group of particular interest. 
Recently, Helping Our Own (HOO) and 
CoNLL held a number of shared tasks on this 
topic (Dale et al., 2012, Ng et al., 2013, Ng et al., 
2014). Previous studies based on rules (Sidorov 
et al., 2013), data-driven methods (Berend et al., 
2013, Yi et al., 2013) and hybrid methods (Putra 
and Szab?, 2013, Xing et al., 2013) have shown 
substantial gains for some frequent error types 
over baseline methods. Most proposed methods 
share the commonality that a sub-model is built 
for a specific type of error, on top of which a 
strategy is applied to combine a number of these 
individual models. Also, detection and correction 
are often split into two steps. For example, Xing 
et al. (2013) presented the UM-Checker for five 
error types in the CoNLL 2013 shared task. The 
system implements a cascade of five individual 
detection-and-correction models for different 
types of error. Given an input sentence, errors are 
detected and corrected one-by-one by each sub-
model at the level of its corresponding error type.  
The specifics of an error type are fully 
considered in each sub-model, which is easier to 
realize for a single error type than for multiple 
types in a single model. In addition, dividing the 
error detection and correction into two steps 
alleviates the application of machine learning 
classifiers. However, an approach that considers 
error types individually may have negative 
effects: 
? This approach assumes independence 
between each error type. It ignores the 
interaction of neighboring errors. Results 
(Xing et al., 2013) have shown that 
83
consecutive errors of multiple types tend to 
hinder solving these errors individually. 
? As the number of error types increases, the 
complexities of analyzing, designing, and 
implementing the model increase, in 
particular when combinatorial errors are 
taken into account. 
? Looking for an optimal model combination 
becomes complex. A simple pipeline 
approach would result in interference and the 
generation of new errors, and hence to 
propagating those errors to the subsequent 
processes. 
? Separating the detection and correction tasks 
may result in more errors. For instance, once 
a candidate is misidentified as an error, it 
would be further revised and turned into an 
error by the correction model. In this 
scenario the model risks losing precision. 
In the shared task of this year (Ng et la., 
2014), two novelties are introduced: 1) all types 
of errors present in an essay are to be detected 
and corrected (i.e., there is no restriction on the 
five error types of the 2013 shared task); 2) the 
official evaluation metric of this year adopts F0.5, 
weighting precision twice as much as recall. This 
requires us to explore an alternative universal 
joint model that can tackle various kinds of 
grammatical errors as well as join the detection 
and correction processes together. Regarding 
grammatical error correction as a process of 
translation has been shown to be effective (Ehsan 
and Faili, 2013, Mizumoto et al., 2011, 
Yoshimoto et al., 2013, Yuan and Felice, 2013). 
We treat the problematic sentences and golden 
sentences as pairs of source and target sentences. 
In SMT, a translation model is trained on a 
parallel corpus that consists of the source 
sentences (i.e. sentences that may contain 
grammatical errors) and the targeted translations 
(i.e. the grammatically well-formed sentences). 
The challenge is that we need a large amount of 
these parallel sentences for constructing such a 
data-driven SMT system. Some researches 
(Brockett et al., 2006, Yuan and Felice, 2013) 
explore generating artificial errors to resolve this 
sparsity problem. Other studies (Ehsan and Faili, 
2013, Yoshimoto et al., 2013, Yuan and Felice, 
2013) focus on using syntactic information (such 
as PoS or tree structure) to enhance the SMT 
models.  
In this paper, we propose a factored SMT 
model by taking into account not only the surface 
information contained in the sentence, but also 
morphological and syntactic clues (i.e., word 
stem, prefix, suffix and finer PoS information). 
To counter the sparsity problem we do not use 
artificial or manual approaches to enrich the 
training data. Instead we apply factored and 
transductive learning techniques to enhance the 
model on a small dataset. In addition, we also 
experimented with different combinations of 
translation models (TM), phrase- and factor-
based, that are trained on different datasets to 
boost the overall performance. Empirical results 
show that the proposed model yields an 
improvement of 32.54% over a baseline phrase-
based SMT model. 
The remainder of this paper is organized as 
follows: Section 2 describes our proposed 
methods. Section 3 reports on the design of our 
experiments. We discuss the result, including the 
official shared task results, in Section 4,. We 
summarize our conclusions in Section 5. 
2 Methodology 
In contrast with phrase-based translation models, 
factored models make use of additional linguistic 
clues to guide the system such that it generates 
translated sentences in which morphological and 
syntactic constraints are met (Koehn and Hoang, 
2007). The linguistic clues are taken as factors in 
a factored model; words are represented as 
vectors of factors rather than as a single token. 
This requires us to pre-process the training data 
to factorize all words. In this study, we explore 
the use of various types of morphological 
information and PoS as factors. For each possible 
factor we build an individual translation model. 
The effectiveness of all factors is analyzed by 
comparing the performance of the corresponding 
models on the grammatical error correction task. 
Furthermore, two approaches are proposed to 
combine those models. One adopts the model 
cascading method based on transductive learning. 
The second approach relies on learning and 
decoding multiple factors learning. The details of 
each approach are discussed in the following 
sub-sections. 
2.1 Data Preparation 
In order to construct a SMT model, we convert 
the training data into a parallel corpus where the 
problematic sentences that ought to be corrected 
are regarded as source sentences, while the 
reference sentences are treated as the 
corresponding target translations. We discovered 
that a number of sentences is absent at the target 
side due to incorrect annotations in the golden 
84
data. We removed these unparalleled sentences 
from the data. Secondly, the initial 
capitalizations of sentences are converted to their 
most probable casing using the Moses truecaser2. 
URLs are quite common in the corpus, but they 
are not useful for learning and even may cause 
the model to apply unnecessary correction on it. 
Thus, we mark all of the ULRs with XML 
markups, signaling the SMT decoder not to 
analyze an URL and output it as is.  
2.2 Model Construction 
In this study we explore four different factors: 
prefix, suffix, stem, and PoS. This linguistic 
information not only helps to capture the local 
constraints of word morphologies and the 
interaction of adjacent words, but also helps to 
prevent data sparsity caused by inflected word 
variants and insufficient training data.  
Word stem: Instead of lemmas, we prefer  
word stemming as one of the factors, considering 
that stemming does not requires deep 
morphological analysis and is easier to obtain. 
Second, during the whole error detection and 
correction process, stemming information is used 
as auxiliary information in addition to the 
original word form. Third, for grammatical error 
correction using word lemmas or word stems in 
factored translation model shows no significant 
difference. This is because we are translating text 
of the same language, and the translation of this 
factor, stem or lemma, is straightforwardly 
captured by the model. Hence, we do not rely on 
the word lemma. In this work, we use the 
English Porter stemmer (Porter, 1980) for 
generating word stems.  
Prefix: The second type of morphological 
information we explored is the word prefix. 
Although a prefix does not present strong 
evidence to be useful to the grammatical error 
correction, we include it in our study in order to 
fully investigate all types of morphological 
information. We believe the prefix can be an 
important factor in the correction of initial 
capitalization, e.g. ?In this era, engineering 
designs?? should be changed to ?In this era, 
engineering designs?? In model construction, 
we take the first three letters of a word as its 
prefix. If the length of a word is less than three, 
we use the word as the prefix factor. 
Suffix: Suffix, one of the important factors, 
helps to capture the grammatical agreements 
between predicates and arguments within a 
                                                          
2 After decoding, we will de-truecase all these words. 
sentence. Particularly the endings of plural nouns 
and inflected verb variants are useful for the 
detection of agreement violations that shown up 
in word morphologies. Similar to how we 
represent the prefix, we are interested in the last 
three characters of a word.  
 Examples 
Sentence 
this card contains biometric data to 
add security and reduce the risk of 
falsification 
Original 
POS 
DT NN BVZ JJ NNS TO VB NN 
CC VB DT NN IN NN 
Specific 
POS 
DT NN VBZ JJ NNS TO_to VB 
NN CC VB DT_the NN IN_of 
NN 
Table 1: Example of modified PoS. 
According to the description of factors, Figure 
1 illustrates the forms of various factors 
extracted from a given example sentence.  
Surface 
constantly combining ideas will 
result in better solutions being 
formulated 
Prefix con com ide wil res in bet sol bei for 
Suffix tly ing eas ill ult in ter ons ing ted 
Stem 
constantli combin idea will result in 
better solut be formul 
Specific 
POS 
RB VBG NNS MD VB IN JJR NNS 
VBG VBN 
Figure 1: The factorized sentence. 
PoS: Part-of-Speech tags denote the morpho-
syntactic category of a word. The use of PoS 
sequences enables us to some extent to recover 
missing determiners, articles, prepositions, as 
well as the modal verb in a sentence. Empirical 
studies (Yuan and Felice, 2013) have 
demonstrated that the use of this information can 
greatly improve the accuracy of the grammatical 
error correction. To obtain the PoS, we adopt the 
Penn Treebank tag set (Marcus et al., 1993), 
which contains 45 PoS tags. The Stanford parser 
(Klein and Manning, 2002) is used to extract the 
PoS information. Inspired by Yuan and Felice 
(2013), who used preposition-specific tags to fix 
the problem of being unable to distinguish 
between prepositions and obtained good 
performance, we create specific tags both for 
determiners (i.e., a, an, the) and prepositions. 
Table 1 provides an example of this modification, 
where prepositions, TO and IN, and determiner, 
85
DT, are revised to TO_to, IN_of and DT_the, 
respectively. 
2.3 Model Combination 
In addition to the design of different factored 
translation models, two model combination 
strategies are designed to treat grammatical error 
correction problem as a series of translation 
processes, where an incorrect sentence is 
translated into the correct one. In both 
approaches we pipeline two translation models, 
    and    . In the first approach, we derive 
four combinations of different models that 
trained on different sources.  
? In case I,    
  and    
  are both factored 
models but trained on different factors, e.g. 
for     
 training on ?surface + factori? and 
    
  on ?surface + factori?j?. Both models 
use the same training sentences, but different 
factors.  
? In case II,     
  is trained on sentences that 
paired with the output from the previous 
model,     
 , and the golden correct sentences. 
We want to create a second model that can 
also tackle the new errors introduced by the 
first model. 
? In case III, similar to case II, the second 
translation model,    
  is replaced by a 
phrase-based translation model.  
? In case IV, the quality of training data is 
considered vital to the construction of a good 
translation model. The present training dataset 
is not large enough. To complement this, the 
second model,     
 , is trained on an enlarged 
data set, by combining the training data of 
both models, i.e. the original parallel data 
(official incorrect and correct sentence pairs) 
and the supplementary parallel data 
(sentences output from the first model,     
 , 
and the correct sentences). Note that we do 
not de-duplicate sentences.  
In all cases, the testing process is carried out 
as follows. The test set is translated by the first 
translation model,     
 . The output from the first 
model is then fed into the second translation 
model,     
 . The output of the second model is 
used as the final corrections. 
The second combination approach is to make 
use of multiple factors for model construction. 
The question is whether multiple factors when 
used together may improve the correction results. 
In this setting we combine two factors together 
with the word surface form to build a multi-
factored translation model. All pairs of factors 
are used, e.g. stem and PoS. The decoding 
sequence is as follows: translate the input stems 
into target stems; translate the PoS; and generate 
the surface form given the factors of stem and 
PoS. 
3 Experiment Setup  
3.1 Dataset 
We pre-process the NUCLE corpus (Dahlmeier 
et al., 2013) as described in Section 2 for training 
different translation models. We use both the 
official golden sentences and additional 
WMT2014 English monolingual data3 to train an 
in-domain and a general-domain language model 
(LM), respectively. These language models are 
linearly interpolated in the decoding phase. We 
also randomly select a number of sentence pairs 
from the parallel corpus as a development set and 
a test set, disjoint from the training data. Table 2 
summarizes the statistics of all the datasets.  
Corpus Sentences Tokens 
Parallel 
Corpus 
55,503 
1,124,521 / 
1,114,040 
Additional 
Monolingual 
85,254,788 2,033,096,800 
Dev. Set 500 10,532 / 10,438 
Test Set 900 18,032 / 17,906 
Table 2: Statistics of used corpora. 
The experiments were carried out with 
MOSES 1.04 (Philipp Koehn et al., 2007). The 
translation and the re-ordering model utilizes the 
?grow-diag-final? symmetrized word-to-word 
alignments created with GIZA++5 (Och and Ney, 
2003) and the training scripts of MOSES. A 5-
gram LM was trained using the SRILM toolkit6 
(Stolcke et al., 2002), exploiting the improved 
modified Kneser-Ney smoothing (Kneser and 
Ney, 1995), and quantizing both probabilities 
and back-off weights. For the log-linear model 
training, we take minimum-error-rate training 
(MERT) method as described in (Och, 2003). 
The result is evaluated by M2 Scorer (Dahlmeier 
and Ng, 2012) computing precision, recall and 
F0.5.  
                                                          
3 http://www.statmt.org/wmt14/translation-task.html. 
4 http://www.statmt.org/moses/. 
5 http://code.google.com/p/giza-pp/. 
6 http://www.speech.sri.com/projects/srilm/. 
86
In total, one baseline system, five individual 
systems, and four combination systems are 
evaluated in this study. The baseline system 
(Baseline) is trained on the words-only corpus 
using a phrase-based translation model. For the 
individual systems we adopt the factored 
translation model that are trained respectively on 
1) surface and stem factors (Sys+stem), 2) surface 
and suffix factors (Sys+suf), 3) surface and prefix 
factors (Sys+pref), 4) surface and PoS factors 
(Sys+PoS), and 5) surface and modified-PoS 
factors (Sys+MPoS). The combination systems 
include: 1) the combination of ?factored + 
phrase-based? and ?factored + factored? for 
models cascading; and 2) the factors of surface, 
stem and modified-PoS (Sys+stem+MPoS) are 
combined for constructing a correction system 
based on a multi-factor model. 
4 Results and Discussions 
We report our results in terms of the precision, 
recall and F0.5 obtained by each of the individual 
models and combined models.  
4.1 Individual Model 
Table 3 shows the absolute measures for the 
baseline system, while the other individual 
models are listed with values relative to the 
baseline.  
Model Precision  Recall  F0.5 
Baseline 25.58 3.53 11.37 
Sys+stem -14.84 +13.00 +0.18 
Sys+suf -14.57 +14.77 +0.60 
Sys+pref -15.74 +12.20 -0.77 
Sys+PoS -11.63 +9.79 +2.45 
Sys+MPoS -10.25 +10.60 +3.70 
Table 3: Performance of various models. 
The baseline system has the highest precision 
score but the lowest recall. Nearly all individual 
models except Sys+pref show improvements in the 
correction result (F0.5) over the baseline. Overall, 
Sys+MPoS achieves the best result for the 
grammatical error correction task. It shows a 
significant improvement over the other models 
and outperforms the baseline model by 3.7 F0.5 
score. The Sys+stem and Sys+suf models obtain an 
improvement of 0.18 and 0.60 in F0.5 scores, 
respectively, compared to the baseline. Although 
the differences are not significant, it confirms our 
hypothesis that morphological clues do help to 
improve error correction. The F0.5 score of 
Sys+pref is the lowest among the models including 
the baseline, showing a drop of 0.77 in F0.5 score 
against the baseline. One possible reason is that 
few errors (in the training corpus) involve word 
prefixes. Thus, the prefix does not seem to be a 
suitable factor for tackling the GEC problem. 
Type 
Sys+stem 
(%) 
Sys+suf 
(%) 
Sys+MPoS 
(%) 
Error 
Num. 
Vt 17.07 12.20 12.20 41 
ArtOrDet 37.65 36.47 29.41 85 
Nn 33.33 19.61 23.53 51 
Prep 10.26 10.26 12.82 39 
Wci 9.10 10.61 6.10 66 
Rloc- 15.20 13.92 10.13 79 
Table 4: The capacity of different models in 
handling six frequent error types. 
We analyze the capacities of the models on 
different types of errors. Sys+PoS and Sys+MPoS are 
built by using the PoS and modified PoS. Both of 
them yield an improvement in F0.5 score. Overall, 
Sys+MPoS produces more accurate results than 
Sys+pref. Therefore, we specifically compare and 
evaluate the best three models, Sys+stem, Sys+suf 
and Sys+MPoS. Table 4 presents evaluation scores 
of these models for the six most frequent error 
types, which take up a large part of the training 
and test data. Among them, Sys+stem displays a 
powerful capacity to handle determiner and 
noun/number agreement errors, up to 37.65% 
and 33.33%. Sys+suf shows the ability to correct 
determiner errors at 36.47%; Sys+MPoS yields a 
similar performance to Sys+suf. All three 
individual models exhibit a relatively high 
capacity to handle determiner errors. The likely 
reason is that this mistake constitutes the largest 
portion in training data and test set, giving the 
learning models many examples to capture this 
problem well. In the case of preposition errors, 
Sys+MPoS demonstrates a better performance. This, 
once again, confirms the result (Yuan and Felice, 
2013) that the modified PoS factor is effective 
for every preposition word. For these six error 
types, the individual models show a weak 
capacity to handle the word collocation or idiom 
error category (Wci). Although Sys+MPoS 
achieves the highest F0.5 score in the overall 
evaluation, it only achieves 6.10% in handling 
this error type. The likely reason is that idioms 
are not frequent in the training data, and also that 
in most of the cases they contain out-of-
vocabulary words never seen in training data. 
4.2 Model Combination 
We intend to further boost the overall 
performance of the correction system by 
87
combining the strengths of individual models 
through model combination, and compare against 
the baseline. The systems compared here cover 
three pipelined models and a multi-factored 
model, as described earlier in Section 3. The 
combined systems include: 1) CSyssuf+phrase: the 
combination of Sys+suf and the baseline phrase-
based translation model; 2) CSyssuf+suf: we 
combine two similar factored models with suffix 
factors, Sys+suf, which is trained on the same 
corpus; and 3) TSyssuf+phrase: similar to 
CSyssuf+phrase, but the training data for the second 
phrase-based model is augmented by adding the 
output sentences from the previous model (paired 
with the correct sentences). Our intention is to 
enlarge the size of the training data. The 
evaluation results are presented in Table 5. 
Model Precision Recall F0.5 
Baseline 25.58 3.53 11.37 
CSyssuf+phrase -14.70 +14.61 +0.45 
CSyssuf+suf -15.04 +14.13 +0.09 
TSyssuf+phrase -14.76 +14.61 +0.40 
Sys+stem+MPoS -15.87 +11.72 -0.90 
Table 5: Evaluation results of combined models. 
In Table 5 we observe that Sys+stem+MPoS hurts 
performance and shows a drop of 0.9% in F0.5 
score. Both the CSyssuf+phrase and CSyssuf+suf 
show minor improvements over the baseline 
system. Even when we enrich the training data 
for the second model in TSyssuf+phrase, it cannot 
help in boosting the overall performance of the 
system. One of the problems we observe is that, 
with this combination structure, new incorrect 
sentences are introduced by the model at each 
step. The errors are propagated and accumulated 
to the final result. Although CSyssuf+phrase and 
CSyssuf+suf produce a better F0.5 score over the 
baseline, they are not as good as the individual 
models, Sys+PoS and Sys+MPoS, which are trained 
on PoS and modified-PoS, respectively. 
4.3 The Official Result 
After fully evaluating the designed individual 
models as well as the integrated ones, we adopt 
Sys+MPoS as our designated system for this 
grammatical error correction task. The official 
test set consists of 50 essays, and 2,203 errors. 
Table 6 shows the final result obtained by our 
submitted system.  
Table 7 details the correction rate of the five 
most frequent error types obtained by our system. 
The result suggests that the proposed system has 
a better ability in handling the verb, article and 
determiner error than other error types. 
Criteria Result Alt. Result 
P 0.3127 0.4317 
R 0.1446 0.1972 
F0.5 0.2537 0.3488 
Table 6: The official correction results of our 
submitted system. 
Type Error Correct % 
Vt 203/201 21/22 10.34/10.94 
V0 57/54 9/9 15.79/16.67 
Vform 156/169 11/18 7.05/10.65 
ArtOrDet 569/656 84/131 14.76/19.97 
Nn 319/285 31/42 9.72/10.91 
Table 7: Detailed error information of evaluation 
system (with alternative result). 
5 Conclusion 
This paper describes our proposed grammatical 
error detection and correction system based on a 
factored statistical machine translation approach. 
We have investigated the effectiveness of models 
trained with different linguistic information 
sources, namely morphological clues and 
syntactic PoS information. In addition, we also 
explore some ways to combine different models 
in the system to tackle the correction problem. 
The constructed models are compared against the 
baseline model, a phrase-based translation model. 
Results show that PoS information is a very 
effective factor, and the model trained with this 
factor outperforms the others. One difficulty of 
this year?s shared task is that participants have to 
tackle all 28 types of errors, which is five times 
more than last year. From the results, it is 
obvious there are still many rooms for improving 
the current system. 
Acknowledgements 
The authors are grateful to the Science and 
Technology Development Fund of Macau and 
the Research Committee of the University of 
Macau for the funding support for their research, 
under the Reference nos. MYRG076 (Y1-L2)-
FST13-WF and MYRG070 (Y1-L2)-FST12-CS. 
The authors also wish to thank the anonymous 
reviewers for many helpful comments with 
special thanks to Antal van den Bosch for his 
generous help on this manuscript. 
  
88
References  
 G?bor Berend, Veronika Vincze, Sina Zarriess, 
and Rich?rd Farkas. 2013. LFG-based 
Features for Noun Number and Article 
Grammatical Errors. CoNLL-2013. 
Chris Brockett, William B. Dolan, and Michael 
Gamon. 2006. Correcting ESL errors using 
phrasal SMT techniques. Proceedings of the 
21st International Conference on 
Computational Linguistics and the 44th 
annual meeting of the Association for 
Computational Linguistics pages 249?256. 
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei 
Wu. 2013. Building a Large Annotated 
Corpus of Learner English: The NUS Corpus 
of Learner English. Proceedings of the Eighth 
Workshop on Innovative Use of NLP for 
Building Educational Applications. pages 22-
31. 
Robert Dale, Ilya Anisimoff, and George 
Narroway. 2012. HOO 2012: A report on the 
preposition and determiner error correction 
shared task. Proceedings of the Seventh 
Workshop on Building Educational 
Applications Using NLP pages 54?62. 
Nava Ehsan, and Heshaam Faili. 2013. 
Grammatical and context-sensitive error 
correction using a statistical machine 
translation framework. Software: Practice and 
Experience. Wiley Online Library. 
D. Klein, and C. D. Manning. 2002. Fast exact 
inference with a factored model for natural 
language parsing. Advances in neural 
information processing systems. 
Reinhard Kneser, and Hermann Ney. 1995. 
Improved backing-off for m-gram language 
modeling. Acoustics, Speech, and Signal 
Processing, 1995. ICASSP-95., 1995 
International Conference on Vol. 1, pages 
181?184. 
P. Koehn, and H. Hoang. 2007. Factored 
translation models. Proceedings of the Joint 
Conference on Empirical Methods in Natural 
Language Processing and Computational 
Natural Language Learning (EMNLP-CoNLL) 
Vol. 868, pages 876?876. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, 
Chris Callison-Burch, Marcello Federico, 
Nicola Bertoldi, Brooke Cowan, et al. 2007. 
Moses: Open source toolkit for statistical 
machine translation. Proceedings of the 45th 
Annual Meeting of the ACL on Interactive 
Poster and Demonstration Sessions pages 
177?180. 
M. P. Marcus, M. A. Marcinkiewicz, and B. 
Santorini. 1993. Building a large annotated 
corpus of English: The Penn Treebank. 
Computational linguistics. MIT Press. 
Tomoya Mizumoto, Mamoru Komachi, Masaaki 
Nagata, and Yuji Matsumoto. 2011. Mining 
Revision Log of Language Learning SNS for 
Automated Japanese Error Correction of 
Second Language Learners. IJCNLP pages 
147?155. 
Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, 
Christian Hadiwinoto, Raymond Hendy 
Susanto, and Bryant Christopher. 2014. The 
conll-2014 shared task on grammatical error 
correction. Proceedings of CoNLL. Baltimore, 
Maryland, USA. 
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, 
Christian Hadiwinoto, and Joel Tetreault. 
2013. The conll-2013 shared task on 
grammatical error correction. Proceedings of 
CoNLL. 
Franz Josef Och. 2003. Minimum Error Rate 
Training in Statistical Machine Translation, 
160?167. 
Franz Josef Och, and Hermann Ney. 2003. A 
systematic comparison of various statistical 
alignment models. Computational linguistics. 
MIT Press. 
Martin F. Porter. 1980. An algorithm for suffix 
stripping. Program: electronic library and 
information systems. MCB UP Ltd. 
Desmond Darma Putra, and Lili Szab?. 2013. 
UdS at the CoNLL 2013 Shared Task. 
CoNLL-2013. 
Grigori Sidorov, Anubhav Gupta, Martin Tozer, 
Dolors Catala, Angels Catena, and Sandrine 
Fuentes. 2013. Rule-based System for 
Automatic Grammar Correction Using 
Syntactic N-grams for English Language 
Learning (L2). CoNLL-2013. 
Andreas Stolcke, and others. 2002. SRILM-an 
extensible language modeling toolkit. 
INTERSPEECH. 
Junwen Xing, Longyue Wang, Derek F. Wong, 
Lidia S. Chao, and Xiaodong Zeng. 2013. 
UM-Checker: A Hybrid System for English 
Grammatical Error Correction. Proceedings of 
the Seventeenth Conference on Computational 
Natural Language Learning: Shared Task, 
34?42. Sofia, Bulgaria: Association for 
Computational Linguistics. Retrieved from 
http://www.aclweb.org/anthology/W13-3605 
Bong-Jun Yi, Ho-Chang Lee, and Hae-Chang 
Rim. 2013. KUNLP Grammatical Error 
Correction System For CoNLL-2013 Shared 
89
Task. CoNLL-2013. 
Ippei Yoshimoto, Tomoya Kose, Kensuke 
Mitsuzawa, Keisuke Sakaguchi, Tomoya 
Mizumoto, Yuta Hayashibe, Mamoru 
Komachi, et al. 2013. NAIST at 2013 CoNLL 
grammatical error correction shared task. 
CoNLL-2013. 
Zheng Yuan, and Mariano Felice. 2013. 
Constrained grammatical error correction 
using Statistical Machine Translation. 
CoNLL-2013. 
  
90
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 233?238,
Baltimore, Maryland USA, June 26?27, 2014. c?2014 Association for Computational Linguistics
Domain Adaptation for Medical Text Translation Using Web Re-
sources
Yi Lu, Longyue Wang, Derek F. Wong, Lidia S. Chao, Yiming Wang, Francisco Oliveira
Natural Language Processing & Portuguese-Chinese Machine Translation Laboratory,
Department of Computer and Information Science,
University of Macau, Macau, China
takamachi660@gmail.com, vincentwang0229@hotmail.com,
derekfw@umac.mo, lidiasc@umac.mo, wang2008499@gmail.com, 
olifran@umac.mo
Abstract
This paper describes adapting statistical 
machine translation (SMT) systems to 
medical domain using in-domain and 
general-domain data as well as web-
crawled in-domain resources. In order to 
complement the limited in-domain corpo-
ra, we apply domain focused web-
crawling approaches to acquire in-
domain monolingual data and bilingual 
lexicon from the Internet. The collected 
data is used for adapting the language 
model and translation model to boost the 
overall translation quality. Besides, we 
propose an alternative filtering approach
to clean the crawled data and to further 
optimize the domain-specific SMT sys-
tem. We attend the medical summary
sentence unconstrained translation task of 
the Ninth Workshop on Statistical Ma-
chine Translation (WMT2014). Our sys-
tems achieve the second best BLEU 
scores for Czech-English, fourth for 
French-English, English-French language 
pairs and the third best results for re-
minding pairs.
1 Introduction
In this paper, we report the experiments carried 
out by the NLP2CT Laboratory at University of 
Macau for WMT2014 medical sentence transla-
tion task on six language pairs: Czech-English 
(cs-en), French-English (fr-en), German-English 
(de-en) and the reverse direction pairs (i.e., en-cs, 
en-fr and en-de). 
As data in specific domain are usually rela-
tively scarce, the use of web resources to com-
plement the training resources provides an effec-
tive way to enhance the SMT systems (Resnik 
and smith, 2003; Espl?-Gomis and Forcada, 2010; 
Pecina et al., 2011; Pecina et al., 2012; Pecina et 
al., 2014). In our experiments, we not only use 
all available training data provided by the
WMT2014 standard translation task 1 (general-
domain data) and medical translation task2 (in-
domain data), but also acquire addition in-
domain bilingual translations (i.e. dictionary) and 
monolingual data from online sources.
First of all, we collect the medical terminolo-
gies from the web. This tiny but significant par-
allel data are helpful to reduce the out-of-
vocabulary words (OOVs) in translation models. 
In addition, the use of larger language models 
during decoding is aided by more efficient stor-
age and inference (Heafield, 2011). Thus, we 
crawl more in-domain monolingual data from the 
Internet based on domain focused web-crawling
approach. In order to detect and remove out-
domain data from the crawled data, we not only 
explore text-to-topic classifier, but also propose 
an alternative filtering approach combined the 
existing one (text-to-topic classifier) with per-
plexity. After carefully pre-processing all the 
available training data, we apply language model 
adaptation and translation model adaptation us-
ing various kinds of training corpora. Experi-
mental results show that the presented approach-
es are helpful to further boost the baseline system.
The reminder of this paper is organized as fol-
lows. In Section 2, we detail the workflow of 
web resources acquisition. Section 3 describes 
the pre-processing steps for the corpora. Section 
5 presents the baseline system. Section 6 reports 
the experimental results and discussions. Finally, 
                                                
1 http://www.statmt.org/wmt14/translation-task.html.
2 http://www.statmt.org/wmt14/medical-task/.
233
the submitted systems and the official results are 
reported in Section 7.
2 Domain Focused Web-Crawling
In this section, we introduce our domain focused 
web-crawling approaches on acquisition of in-
domain translation terminologies and monolin-
gual sentences. 
2.1 Bilingual Dictionary
Terminology is a system of words used to name 
things in a particular discipline. The in-domain 
vocabulary size directly affects the performance 
of domain-specific SMT systems. Small size of 
in-domain vocabulary may result in serious 
OOVs problem in a translation system. Therefore, 
we crawl medical terminologies from some 
online sources such as dict.cc3, where the vocab-
ularies are divided into different subjects. We 
obtain the related bilingual entries in medicine 
subject by using Scala build-in XML parser and 
XPath. After cleaning, we collected 28,600, 
37,407, and 37,600 entries in total for cs-en, de-
en, and fr-en respectively.
2.2 Monolingual Data
The workflow for acquiring in-domain resources 
consists of a number of steps such as domain 
identification, text normalization, language iden-
tification, noise filtering, and post-processing as 
well as parallel sentence identification.
Firstly we use an open-source crawler, Com-
bine4, to crawl webpages from the Internet. In 
order to classify these webpages as relevant to 
the medical domain, we use a list of triplets 
<term, relevance weight, topic class> as the 
basic entries to define the topic. Term is a word 
or phrase. We select terms for each language 
from the following sources: 
? The Wikipedia title corpus, a WMT2014 of-
ficial data set consisting of titles of medical 
articles. 
? The dict.cc dictionary, as is described in Sec-
tion 2.1.
? The DrugBank corpus, which is a WMT2014 
official data set on bioinformatics and 
cheminformatics.
For the parallel data, i.e. Wikipedia and dict.cc 
dictionary, we separate the source and target text 
into individual text and use either side of them
for constructing the term list for different lan-
                                                
3 http://www.dict.cc/.
4 http://combine.it.lth.se/.
guages. Regarding the DrugBank corpus, we di-
rectly extract the terms from the ?name? field. 
The vocabulary size of collected text for each 
language is shown in Table 1.
EN CS DE FR
Wikipedia Titles 12,684 3,404 10,396 8,436
dict.cc 29,294 16,564 29,963 22,513
DrugBank 2,788
Total 44,766 19,968 40,359 30,949
Table 1: Size of terms used for topic definition.
Relevance weight is the score for each occur-
rence of the term, which is assigned by its length, 
i.e., number of tokens. The topic class indicates 
the topics. In this study, we are interested in 
medical domain, the topic class is always marked 
with ?MED? in our topic definition. 
The topic relevance of each document is cal-
culated5 as follows:
  ? ?      
   
  
   
 
   (1)
where is the amount of terms in the topic defi-
nition;   
 is the weight of term  ;   
 is the 
weight of term at location  .    is the number of 
occurrences of term  at  position. In implemen-
tation, we use the default values for setting and
parameters. Another input required by the crawl-
er is a list of seed URLs, which are web sites that 
related to medical topic. We limit the crawler 
from getting the pages within the http domain 
guided by the seed links. We acquired the list 
from the Open Directory Project6, which is a re-
pository maintained by volunteer editors. Totally, 
we collected 12,849 URLs from the medicine
category.
Text normalization is to convert the text of 
each HTML page into UTF-8 encoding accord-
ing to the content_charset of the header. In addi-
tion, HTML pages often consist of a number of 
irrelevant contents such as the navigation links, 
advertisements disclaimers, etc., which may neg-
atively affect the performance of SMT system. 
Therefore, we use the Boilerpipe tool 
(Kohlsch?tter et al., 2010) to filter these noisy
data and preserve the useful content that is 
marked by the tag, <canonicalDocument>. The 
resulting text is saved in an XML file, which will 
be further processed by the subsequent tasks. For 
language identification, we use the language-
detection7 toolkit to determine the possible lan-
                                                
5
http://combine.it.lth.se/documentation/DocMain/node6.html.
6 http://www.dmoz.org/Health/Medicine/.
7 https://code.google.com/p/language-detection/.
234
guage of the text, and discard the articles which 
are in the right language we are interested.
2.3 Data Filtering
The web-crawled documents (described in Sec-
tion 2.2) may consist a number of out-domain 
data, which would harm the domain-specific lan-
guage and translation models. We explore and 
propose two filtering approaches for this task. 
The first one is to filter the documents based on 
their relative score, Eq. (1). We rank all the doc-
uments according to their relative scores and se-
lect top K percentage of entire collection for fur-
ther processing. 
Second, we use a combination method, which 
takes both the perplexity and relative score into 
account for the selection. Perplexity-based data 
selection has shown to be a powerful mean on 
SMT domain adaptation (Wang et al., 2013; 
Wang et al., 2014; Toral, 2013; Rubino et al., 
2013; Duh et al., 2013). The combination method 
is carried out as follows: we first retrieve the 
documents based on their relative scores. The 
documents are then split into sentences, and
ranked according to their perplexity using Eq. (2)
(Stolcke et al., 2002). The used language model 
is trained on the official in-domain data. Finally, 
top N percentage of ranked sentences are consid-
ered as additional relevant in-domain data. 
    ( )        
 ( )
    (2)
where  is a input sentence or document,  ( ) is 
the probability of  -gram segments estimated 
from the training set.     is the number of 
tokens of an input string.
3 Pre-processing
Both official training data and web-crawled re-
sources are processed using the Moses scripts8, 
this includes the text tokenization, truecasing and 
length cleaning. For trusecasing, we use both the 
target side of parallel corpora and monolingual 
data to train the trucase models. We consider the 
target system is intended for summary translation, 
the sentences tend to be short in length. We re-
move sentence pairs which are more than 80 
words at length in either sides of the parallel text.
In addition to these general data filtering steps,
we introduce some extra steps to pre-process the 
training data. The first step is to remove the du-
plicate sentences. In data-driven methods, the 
more frequent a term occurs, the higher probabil-
                                                
8 http://www.statmt.org/moses/?n=Moses.Baseline.
ity it biases. Duplicate data may lead to unpre-
dicted behavior during the decoding. Therefore, 
we keep only the distinct sentences in monolin-
gual corpus. By taking into account multiple 
translations in parallel corpus, we remove the 
duplicate sentence pairs. We also use a biomedi-
cal sentence splitter9 (Rune et al., 2007) to split 
sentences in monolingual corpora. The statistics 
of the data are provided in Table 2.
4 Baseline System
We built our baseline system on an optimized 
level. It is trained on all official in-domain train-
ing corpora and a portion of general-domain data. 
We apply the Moore-Lewis method (Moore and 
Lewis, 2010) and modified Moore-Lewis method 
(Axelrod et al., 2011) for selecting in-domain 
data from the general-domain monolingual and 
parallel corpora, respectively. The top M per-
centages of ranked sentences are selected as a 
pseudo in-domain data to train an additional LM
and TM. For LM, we linearly interpolate the ad-
ditional LM with in-domain LM. For TM, the 
additional model is log-linearly interpolated with 
the in-domain model using the multi-decoding 
method described in (Koehn and Schroeder, 
2007). Finally, LM adaptation and TM adapta-
tion are combined to further improve the transla-
tion quality of baseline system.
5 Experiments and Results
The official medical summary development sets 
(dev) are used for tuning and evaluating the 
comparative systems. The official medical sum-
mary test sets (test) are only used in our final 
submitted systems.
The experiments were carried out with the 
Moses 1.010 (Koehn et al., 2007). The translation 
and the re-ordering model utilizes the ?grow-
diag-final? symmetrized word-to-word align-
ments created with MGIZA++11 (Och and Ney, 
2003; Gao and Vogel, 2008) and the training 
scripts from Moses. A 5-gram LM was trained 
using the SRILM toolkit12 (Stolcke et al., 2002), 
exploiting improved modified Kneser-Ney 
smoothing, and quantizing both probabilities and 
back-off weights. For the log-linear model train-
ing, we take the minimum-error-rate training 
(MERT) method as described in (Och, 2003).
                                                
9 http://www.nactem.ac.uk/y-matsu/geniass/.
10 http://www.statmt.org/moses/.
11 http://www.kyloo.net/software/doku.php/mgiza:overview.
12 http://www.speech.sri.com/projects/srilm/.
235
In the following sub-sections, we describe the
results of baseline systems, which are trained on 
the official corpora. We also present the en-
hanced systems that make use of the web-
crawled bilingual dictionary and monolingual 
data as the additional training resources. Two
variants of enhanced system are constructed 
based on different filtering criteria.
5.1 Baseline System
The baseline systems is constructed based on the 
combination of TM adaptation and LM adapta-
tion, where the corresponding selection thresh-
olds ( ) are manually tuned. Table 3 shows the 
BLEU scores of baseline systems as well as the
threshold values of for general-domain mono-
lingual corpora and parallel corpora selection, 
respectively.
By looking into the results, we find that en-cs 
system performs poorly, because of the limited 
in-domain parallel and monolingual corpora 
(shown in Table 2). While the fr-en and en-fr 
systems achieve the best scores, due the availa-
bility of the high volume training data. We ex-
periment with different values of ={0, 25, 50, 
75, 100} that indicates the percentages of sen-
tences out of the general corpus used for con-
structing the LM adaptation and TM adaptation. 
After tuning the parameter  , we find that
BLEU scores of different systems peak at differ-
ent values of . LM adaptation can achieve the 
best translation results for cs-en, en-fr and de-en 
pairs when  =25, en-cs and en-de pairs when 
 =50, and fr-en pair when  =75. While TM 
adaptation yields the best scores for en-fr and en-
de pairs at  =25 and cs-en and fr-en pairs at 
 =50, de-en pair when =75 and en-cs pair at 
 =100.
Lang. Pair BLEU
Mono.
(M%)
Parallel
(M%)
en-cs 17.57 50% 100%
cs-en 31.29 25% 50%
en-fr 38.36 25% 25%
fr-en 44.36 75% 50%
en-de 18.01 50% 25%
de-en 32.50 25% 75%
Table 3: BLEU scores of baseline systems for 
different language pairs.
5.2 Based on Relevance Score Filtering
As described in Section 2.3, we use the relevance
score to filter out the non-in-domain documents. 
Once again, we evaluate different values of 
Data Set Lang. Sent. Words Vocab. Ave. Len. Sites Docs
In-domain 
Parallel Data
cs/en 1,770,421
9,373,482/
10,605,222
134,998/
156,402
5.29/
5.99
de/en 3,894,099
52,211,730/
58,544,608
1,146,262/
487,850
13.41/
15.03
fr/en 4,579,533
77,866,237/
68,429,649
495,856/
556,587
17.00/
14.94
General-
domain 
Parallel Data
cs/en 12,426,374
180,349,215/
183,841,805
1,614,023/
1,661,830
14.51/
14.79
de/en 4,421,961
106,001,775/
112,294,414
1,912,953/
919,046
23.97/
25.39
fr/en 36,342,530
1,131,027,766/
953,644,980
3,149,336/
3,324,481
31.12/
26.24
In-domain 
Mono. Data
cs 106,548 1,779,677 150,672 16.70
fr 1,424,539 53,839,928 644,484 37.79
de 2,222,502 53,840,304 1,415,202 24.23
en 7,802,610 199430649 1,709,594 25.56
General-
domain 
Mono. Data
cs 33,408,340 567,174,266 3,431,946 16.98
fr 30,850,165 780,965,861 2,142,470 25.31
de 84,633,641 1,548,187,668 10,726,992 18.29
en 85,254,788 2,033,096,800 4,488,816 23.85
Web-crawled 
In-domain 
Mono. Data
en 8,448,566 280,211,580 3,047,758 33.16 26 1,601
cs 44,198 1,280,326 137,179 28.96 4 388
de 473,171 14,087,687 728,652 29.77 17 968
fr 852,036 35,339,445 718,141 41.47 10 683
Table 2: Statistics summary of corpora after pre-processing.
236
 ={0, 25, 50, 75, 100} that represents the per-
centages of crawled documents we used for 
training the LMs. In Table 4, we show the abso-
lute BLEU scores of the evaluated systems, listed 
with the optimized thresholds, and the relative 
improvements (?%) in compared to the baseline 
system. The size of additional training data (for 
LM) is displayed at the last column.
Lang. 
Pair
Docs
( %)
BLEU
? 
(%)
Sent.
en-cs 50 17.59 0.11 31,065 
en-de 75 18.52 2.83 435,547 
en-fr 50 39.08 1.88 743,735 
cs-en 75 32.22 2.97 7,943,931
de-en 25 33.50 3.08 4,951,189
fr-en 100 45.45 2.46 8,448,566
Table 4: Evaluation results for systems that 
trained on relevance-score-filtered documents.
The relevance score filtering approach yields 
an improvement of 3.08% of BLEU score for de-
en pair that is the best result among the language 
pairs. On the other hand, en-cs pair obtains a 
marginal gain. The reason is very obvious that 
the training data is very insufficient. Empirical 
results of all language pairs expect fr-en indicate
that data filtering is the necessity to improve the 
system performance.
5.3 Based on Moore-Lewis Filtering
In this approach, we need to determine the values 
of two parameters, top  documents and top  
sentences, where  ={100, 75, 50} and  ={75, 
50, 25},    . When  =100, it is a conven-
tional perplexity-based data selection method, i.e. 
no document will be filtered. Table 5 shows the 
combination of different  and  that gives the 
best translation score for each language pair. We 
provide the absolute BLEU for each system, to-
gether with relative improvements (?%) that 
compared to the baseline system.
Lang.  
Pair
Docs
( %)
Target 
Size ( %)
BLEU ? (%)
en-cs 50 25 17.69 0.68
en-de 100 50 18.03 0.11
en-fr 100 50 38.73 0.96
cs-en 100 25 32.20 2.91
de-en 100 25 33.10 1.85
fr-en 100 25 45.22 1.94
Table 5: Evaluation results for systems that 
trained on combination filtering approach.
In this shared task, we have a quality and 
quantity in-domain monolingual training data for 
English. All the systems that take English as the 
target translation always outperform the other
reverse pairs. Besides, we found the systems 
based on the perplexity data selection method
tend to achieve a better scores in BLEU.
6 Official Results and Conclusions
We described our study on developing uncon-
strained systems in the medical translation task 
of 2014 Workshop on Statistical Machine Trans-
lation. In this work, we adopt the web crawling 
strategy for acquiring the in-domain monolingual 
data.  In detection the domain data, we exploited 
Moore-Lewis data selection method to filter the 
collected data in addition to the build-in scoring 
model provided by the crawler toolkit. However, 
after investigation, we found that the two meth-
ods are very competitive to each other.
The systems we submitted to the shared task 
were built using the language models and trans-
lation models that yield the best results in the 
individual testing. The official test set is convert-
ed into the recased and detokenized SGML for-
mat. Table 9 presents the official results of our 
submissions for every language pair.
Lang. 
Pair
BLEU of Combined 
systems
Official 
BLEU
en-cs 23.16 (+5.59) 22.10
cs-en 36.8 (+5.51) 37.40
en-fr 40.34 (+1.98) 40.80
fr-en 45.79 (+1.43) 43.80
en-de 19.36 (+1.35) 18.80
de-en 34.17 (+1.67) 32.70
Table 6: BLEU scores of the submitted systems 
for the medical translation task in six language 
pairs.
Acknowledgments
The authors are grateful to the Science and 
Technology Development Fund of Macau and 
the Research Committee of the University of 
Macau for the funding support for their research, 
under the Reference nos. MYRG076 (Y1-L2)-
FST13-WF and MYRG070 (Y1-L2)-FST12-CS.
References 
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 
2011. Domain adaptation via pseudo in-domain da-
ta selection. In Proceedings of EMNLP, pages 355-
362.
237
K. Duh, G. Neubig, K. Sudoh, H. Tsukada. 2013. Ad-
aptation data selection using neural language mod-
els: Experiments in machine translation. In Pro-
ceedings of the Annual Meeting of the Association 
for Computational Linguistics, pages, 678?683.
M. Espl?-Gomis and M. L. Forcada. 2010. Combining 
Content-Based and URL-Based Heuristics toHar-
vest Aligned Bitexts from Multilingual Sites with 
Bitextor. The Prague Bulletin of Mathemathical 
Lingustics, 93:77?86.
Qin Gao and Stephan Vogel. 2008. Parallel Imple-
mentations of Word Alignment Tool. Software En-
gineering, Testing, and Quality Assurance for Nat-
ural Language Processing, pp. 49-57.
Kenneth Heafield. 2011. KenLM: Faster and smaller 
language model queries. In Proceedings of the 
Sixth Workshop on Statistical Machine Translation, 
pages 187-197.
Papineni, Kishore, Salim Roukos, ToddWard, and-
Wei-Jing Zhu. 2002. BLEU: a method for automat-
ic evaluation of machine translation. In 40th Annu-
al Meeting on Association for Computational Lin-
guistics, ACL ?02, pages 311?318, Philadelphia, 
USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran et al. 
2007. Moses: Open source toolkit for statistical 
machine translation. In Proceedings of ACL, pages
177-180.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine 
translation. In Proceedings of the 2nd ACL Work-
shop on Statistical Machine Translation, pages 
224-227.
Christian Kohlsch?tter, Peter Fankhauser, and Wolf-
gang Nejdl. 2010. Boilerplate detection using shal-
low text features. In Proceedings of the 3rd ACM
International Conference on Web Search and Data
Mining, pages 441-450.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In 
Proceedings of ACL: Short Papers, pages 220-224.
Franz Josef Och. 2003. Minimum error rate training 
in statistical machine translation. Proceedings of 
ACL, pp. 160-167.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment 
models. Computational Linguistics, 29:19-51.
P. Pecina, A. Toral, A. Way, V. Papavassiliou, P. 
Prokopidis, and M. Giagkou. 2011. Towards Using 
WebCrawled Data for Domain Adaptation in Sta-
tistical Machine Translation. In Proceedings of the 
15th Annual Conference of the European Associta-
tion for Machine Translation, pages 297-304.
P. Pecina, A. Toral, V. Papavassiliou, P. Prokopidis, J. 
van Genabith,  and R. I. C. Athena. 2012. Domain 
adaptation of statistical machine translation using 
web-crawled resources: a case study. In Proceed-
ings of the 16th Annual Conference of the Europe-
an Association for Machine Translation, pp. 145-
152.
P. Pecina, O. Du?ek, L. Goeuriot, J. Haji?, J. Hla-
v??ov?, G. J. Jones, and Z. Ure?ov?. 2014. Adapta-
tion of machine translation for multilingual infor-
mation retrieval in the medical domain. Artificial 
intelligence in medicine, pages 1-25.
Philip Resnik and Noah A. Smith. 2003. The Web as 
a parallel corpus. Computational Linguistics, 
29:349?380
Raphael Rubino, Antonio Toral, Santiago Cort?s 
Va?llo, Jun Xie, Xiaofeng Wu, Stephen Doherty, 
and Qun Liu. 2013. The CNGL-DCU-Prompsit 
translation systems for WMT13. In Proceedings of 
the Eighth Workshop on Statistical Machine Trans-
lation, pages 213-218.
S?tre Rune, Kazuhiro Yoshida, Akane Yakushiji, 
Yusuke Miyao, Yuichiro Matsubayashi and Tomo-
ko Ohta. 2007. AKANE System: Protein-Protein 
Interaction Pairs in BioCreAtIvE2 Challenge, PPI-
IPS subtask. In Proceedings of the Second BioCre-
ative Challenge Evaluation Workshop, pp. 209-212.
Andreas Stolcke. 2002. SRILM-an extensible lan-
guage modeling toolkit. Proceedings of the Inter-
national Conference on Spoken Language Pro-
cessing, pp. 901-904.
Antonio Toral. 2013. Hybrid selection of language 
model training data using linguistic information 
and perplexity. In ACL Workshop on Hybrid Ma-
chine Approaches to Translation.
Longyue Wang, Derek F. Wong, Lidia S. Chao, Yi Lu, 
and Junwen Xing. 2014. A Systematic Com-
parison of Data Selection Criteria for SMT Domain 
Adaptation. The Scientific World Journal, vol. 
2014, Article ID 745485, 10 pages.
Longyue Wang, Derek F. Wong, Lidia S. Chao, Yi Lu, 
Junwen Xing. 2013. iCPE: A Hybrid Data Selec-
tion Model for SMT Domain Adaptation. Chinese 
Computational Linguistics and Natural Language 
Processing Based on Naturally Annotated Big Da-
ta. Springer Berlin Heidelberg. pages, 280-290
238
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 254?259,
Baltimore, Maryland USA, June 26?27, 2014. c?2014 Association for Computational Linguistics
Combining Domain Adaptation Approaches for Medical Text Transla-
tion 
 
Longyue Wang, Yi Lu, Derek F. Wong, Lidia S. Chao, Yiming Wang, Francisco Oliveira 
Natural Language Processing & Portuguese-Chinese Machine Translation Laboratory, 
Department of Computer and Information Science, 
University of Macau, Macau, China 
vincentwang0229@hotmail.com,  
{mb25435, derekfw, lidiasc, mb25433, olifran}@umac.mo 
 
 
Abstract 
This paper explores a number of simple 
and effective techniques to adapt statisti-
cal machine translation (SMT) systems in 
the medical domain. Comparative exper-
iments are conducted on large corpora for 
six language pairs. We not only compare 
each adapted system with the baseline, 
but also combine them to further improve 
the domain-specific systems. Finally, we 
attend the WMT2014 medical summary 
sentence translation constrained task and 
our systems achieve the best BLEU 
scores for Czech-English, English-
German, French-English language pairs 
and the second best BLEU scores for re-
minding pairs. 
 
1. Introduction 
This paper presents the experiments conducted 
by the NLP2CT Laboratory at the University of 
Macau for WMT2014 medical sentence transla-
tion task on six language pairs: Czech-English 
(cs-en), French-English (fr-en), German-English 
(de-en) and the reverse direction pairs, i.e., en-cs, 
en-fr and en-de.  
By comparing the medical text with common 
text, we discovered some interesting phenomena 
in medical genre. We apply domain-specific 
techniques in data pre-processing, language 
model adaptation, translation model adaptation, 
numeric and hyphenated words translation.  
Compared to the baseline systems (detailed in 
Section 2 & 3), the results of each method show 
reasonable gains. We combine individual ap-
proach to further improve the performance of our 
systems. To validate the robustness and lan-
guage-independency of individual and combined 
systems, we conduct experiments on the official 
training data (detailed in Section 3) in all six lan-
guage pairs. We anticipate the numeric compari-
son (BLEU scores) on these individual and com-
bined domain adaptation approaches that could 
be valuable for others on building a real-life do-
main-specific system. 
The reminder of this paper is organized as fol-
lows. In Section 2, we detail the configurations 
of our experiments as well as the baseline sys-
tems. Section 3 presents the specific pre-
processing for medical data. In Section 4 and 5, 
we describe the language model (LM) and trans-
lation model (TM) adaptation, respectively. Be-
sides, the techniques for numeric and hyphenated 
words translation are reported in Section 6 and 7. 
Finally, the performance of design systems and 
the official results are reported in Section 8. 
2. Experimental Setup 
All available training data from both WMT2014 
standard translation task1 (general-domain data) 
and medical translation task 2  (in-domain data) 
are used in this study. The official medical sum-
mary development sets (dev) are used for tuning 
and evaluating all the comparative systems. The 
official medical summary test sets (test) are only 
used in our final submitted systems.  
The experiments were carried out with the 
Moses 1.03 (Koehn et al., 2007). The translation 
and the re-ordering model utilizes the ?grow-
diag-final? symmetrized word-to-word align-
ments created with MGIZA++4 (Och and Ney, 
                                                 
1 http://www.statmt.org/wmt14/translation-task.html. 
2 http://www.statmt.org/wmt14/medical-task/. 
3 http://www.statmt.org/moses/. 
4 http://www.kyloo.net/software/doku.php/mgiza:overview. 
254
2003; Gao and Vogel, 2008) and the training 
scripts from Moses. A 5-gram LM was trained 
using the SRILM toolkit5 (Stolcke et al., 2002), 
exploiting improved modified Kneser-Ney 
smoothing, and quantizing both probabilities and 
back-off weights. For the log-linear model train-
ing, we take the minimum-error-rate training 
(MERT) method as described in (Och, 2003). 
3. Task Oriented Pre-processing 
A careful pre-processing on training data is sig-
nificant for building a real-life SMT system. In 
addition to the general data preparing steps used 
for constructing the baseline system, we intro-
duce some extra steps to pre-process the training 
data. 
The first step is to remove the duplicate sen-
tences. In data-driven methods, the more fre-
quent a term occurs, the higher probability it bi-
ases. Duplicate data may lead to unpredicted be-
havior during the decoding. Therefore, we keep 
only the distinct sentences in monolingual cor-
pus. By taking into account multiple translations 
in parallel corpus, we remove the duplicate sen-
tence pairs. The second concern in pre-
processing is symbol normalization. Due to the 
nature of medical genre, symbols such as num-
bers and punctuations are commonly-used to pre-
sent chemical formula, measuring unit, terminol-
ogy and expression. Fig. 1 shows the examples 
of this case. These symbols are more frequent in 
medical article than that in the common texts. 
Besides, the punctuations of apostrophe and sin-
gle quotation are interchangeably used in French 
text, e.g. ?l?effet de l'inhibition?. We unify it by 
replacing with the apostrophe. In addition, we 
observe that some monolingual training subsets 
(e.g., Gene Regulation Event Corpus) contain 
sentences of more than 3,000 words in length. To 
avoid the long sentences from harming the true-
case model, we split them into sentences with a 
sentence splitter6 (Rune et al., 2007) that is opti-
mized for biomedical texts. On the other hand, 
we consider the target system is intended for 
summary translation, the sentences tend to be 
short in length. For instance, the average sen-
tence lengths in development sets of cs, fr, de 
and en are around 15, 21, 17 and 18, respective-
ly. We remove sentence pairs which are more 
than 80 words at length. In order to that our ex-
periments are reproducible, we give the detailed 
                                                 
5 http://www.speech.sri.com/projects/srilm/. 
6 http://www.nactem.ac.uk/y-matsu/geniass/. 
statistics of task oriented pre-processed training 
data in Table 2. 
1,25-OH 
47 to 80% 
10-20 ml/kg 
A&E department 
Infective endocarditis (IE) 
Figure 1. Examples of the segments with sym-
bols in medical texts. 
To validate the effectiveness of the pre-
processing, we compare the SMT systems 
trained on original data 7 (Baseline1) and task-
oriented-processed data (Baseline2), respective-
ly. Table 1 shows the results of the baseline sys-
tems. We found all the Baseline2 systems outper-
form the Baseline1 models, showing that the sys-
tems can benefit from using the processed data. 
For cs-en and en-cs pairs, the BLEU scores im-
prove quite a lot. For other language pairs, the 
translation quality improves slightly.  
By analyzing the Baseline2 results (in Table 1) 
and the statistics of training corpora (in Table 2), 
we can further elaborate and explain the results. 
The en-cs system performs poorly, because of 
the short average length of training sentences, as 
well as the limited size of in-domain parallel and 
monolingual corpora. On the other hand, the fr-
en system achieves the best translation score, as 
we have sufficient training data. The translation 
quality of cs-en, en-fr, fr-en and de-en pairs is 
much higher than those in the other pairs. Hence, 
Baseline2 will be used in the subsequent compar-
isons with the proposed systems described in 
Section 4, 5, 6 and 7. 
Lang. Pair Baseline1 Baseline2 Diff. 
en-cs 12.92 17.57 +4.65 
cs-en 20.85 31.29 +10.44 
en-fr 38.31 38.36 +0.05 
fr-en 44.27 44.36 +0.09 
en-de 17.81 18.01 +0.20 
de-en 32.34 32.50 +0.16 
Table 1: BLEU scores of two baseline systems 
trained on original and processed corpora for 
different language pairs. 
4. Language Model Adaptation 
The use of LMs (trained on large data) during 
decoding is aided by more efficient storage and 
inference (Heafield, 2011). Therefore, we not 
                                                 
7 Data are processed according to Moses baseline tutorial: 
http://www.statmt.org/moses/?n=Moses.Baseline. 
255
Data Set Lang. Sent. Words Vocab. Ave. Len. 
In-domain  
Parallel Data 
cs/en 1,770,421 
9,373,482/ 
10,605,222 
134,998/ 
156,402 
5.29/ 
5.99 
de/en 3,894,099 
52,211,730/ 
58,544,608 
1,146,262/ 
487,850 
13.41/ 
15.03 
fr/en 4,579,533 
77,866,237/ 
68,429,649 
495,856/ 
556,587 
17.00/ 
14.94 
General-domain  
Parallel Data 
cs/en 12,426,374 
180,349,215/ 
183,841,805 
1,614,023/ 
1,661,830 
14.51/ 
14.79 
de/en 4,421,961 
106,001,775/ 
112,294,414 
1,912,953/ 
919,046 
23.97/ 
25.39 
fr/en 36,342,530 
1,131,027,766/ 
953,644,980 
3,149,336/ 
3,324,481 
31.12/ 
26.24 
In-domain  
Mono. Data 
cs 106,548 1,779,677 150,672 16.70 
fr 1,424,539 53,839,928 644,484 37.79 
de 2,222,502 53,840,304 1,415,202 24.23 
en 7,802,610 199430649 1,709,594 25.56 
General-domain  
Mono. Data 
cs 33,408,340 567,174,266 3,431,946 16.98 
fr 30,850,165 780,965,861 2,142,470 25.31 
de 84,633,641 1,548,187,668 10,726,992 18.29 
en 85,254,788 2,033,096,800 4,488,816 23.85 
Table 2: Statistics summary of corpora after pre-processing. 
only use the in-domain training data, but also the 
selected pseudo in-domain data 8  from general-
domain corpus to enhance the LMs (Toral, 2013; 
Rubino et al., 2013; Duh et al., 2013). Firstly, 
each sentence s in general-domain monolingual 
corpus is scored using the cross-entropy differ-
ence method in (Moore and Lewis, 2010), which 
is calculated as follows: 
 ( ) ( ) ( )I Gscore s H s H s? ? (1) 
where H(s) is the length-normalized cross-
entropy. I and G are the in-domain and general-
domain corpora, respectively. G is a random sub-
set (same size as the I) of the general-domain 
corpus. Then top N percentages of ranked data 
sentences are selected as a pseudo in-domain 
subset to train an additional LM. Finally, we lin-
early interpolate the additional LM with in-
domain LM.  
We use the top N% of ranked results, where 
N={0, 25, 50, 75, 100} percentages of sentences 
out of the general corpus. Table 3 shows the ab-
solute BLEU points for Baseline2 (N=0), while 
the LM adapted systems are listed with values 
relative to the Baseline2. The results indicate that 
LM adaptation can gain a reasonable improve-
ment if the LMs are trained on more relevant 
data for each pair, instead of using the whole 
training data. For different systems, their BLEU 
                                                 
8 Axelrod et al. (2011) names the selected data as pseudo in-
domain data. We adopt both terminologies in this paper. 
scores peak at different values of N. It gives the 
best results for cs-en, en-fr and de-en pairs when 
N=25, en-cs and en-de pairs when N=50, and fr-
en pair when N=75. Among them, en-cs and en-
fr achieve the highest BLEU scores. The reason 
is that their original monolingual (in-domain) 
data for training the LMs are not sufficient. 
When introducing the extra pseudo in-domain 
data, the systems improve the translation quality 
by around 2 BLEU points. While for cs-en, fr-en 
and de-en pairs, the gains are small. However, it 
can still achieve a significant improvement of 
0.60 up to 1.12 BLEU points. 
Lang. N=0 N=25 N=50 N=75 N=100 
en-cs 17.57 +1.66 +2.08 +1.72 +2.04 
cs-en 31.29 +0.94 +0.60 +0.66 +0.47 
en-fr 38.36 +1.82 +1.66 +1.60 +0.08 
fr-en 44.36 +0.91 +1.09 +1.12 +0.92 
en-de 18.01 +0.57 +1.02 -4.48 -4.54 
de-en 32.50 +0.60 +0.50 +0.56 +0.38 
Table 3: BLEU scores of LM adapted systems. 
5. Translation Model Adaptation 
As shown in Table 2, general-domain parallel 
corpora are around 1 to 7 times larger than the 
in-domain ones. We suspect if general-domain 
corpus is broad enough to cover some in-domain 
sentences. To observe the domain-specificity of 
general-domain corpus, we firstly evaluate sys-
tems trained on general-domain corpora. In Ta-
256
ble 4, we show the BLEU scores of general-
domain systems9 on translating the medical sen-
tences. The BLEU scores of the compared sys-
tems are relative to the Baseline2 and the size of 
the used general-domain corpus is relative to the 
corresponding in-domain one. For en-cs, cs-en, 
en-fr and fr-en pairs, the general-domain parallel 
corpora we used are 6 times larger than the orig-
inal ones and we obtain the improved BLEU 
scores by 1.72 up to 3.96 points. While for en-de 
and de-en pairs, the performance drops sharply 
due to the limited training corpus we used. 
Hence we can draw a conclusion: the general-
domain corpus is able to aid the domain-specific 
translation task if the general-domain data is 
large and broad enough in content.  
Lang. Pair BLEU Diff. Corpus 
en-cs 21.53 +3.96 
+601.89% 
cs-en 33.01 +1.72 
en-fr 41.57 +3.21 
+693.59% 
fr-en 47.33 +2.97 
en-de 16.54 -1.47 
+13.63% 
de-en 27.35 -5.15 
Table 4: The BLEU scores of systems trained on 
general-domain corpora. 
Taking into account the performance of gen-
eral-domain system, we explore various data se-
lection methods to derive the pseudo in-domain 
sentence pairs from general-domain parallel cor-
pus for enhancing the TMs (Wang et al., 2013; 
Wang et al., 2014). Firstly, sentence pair in cor-
responding general-domain corpora is scored by 
the modified Moore-Lewis (Axelrod et al., 
2011), which is calculated as follows: 
 ? ?
g g
( ) ( ) ( )
( ) ( )
I src G src
I t t G t t
score s H s H s
H s H s
? ?
? ?
? ?
? ?? ?? ?
 (2) 
which is similar to Eq. (1) and the only differ-
ence is that it considers the both the source (src) 
and target (tgt) sides of parallel corpora. Then 
top N percentage of ranked sentence pairs are 
selected as a pseudo in-domain subset to train an 
individual translation model. The additional 
model is log-linearly interpolated with the in-
domain model (Baseline2) using the multi-
decoding method described in (Koehn and 
Schroeder, 2007). 
Similar to LM adaptation, we use the top N% 
of ranked results, where N={0, 25, 50, 75, 100} 
percentages of sentences out of the general cor-
                                                 
9  General-domain systems are trained only on genera-
domain training corpora (i.e., parallel, monolingual). 
pus. Table 5 shows the absolute BLEU points for 
Baseline2 (N=0), while for the TM adapted sys-
tems we show the values relative to the Base-
line2. For different systems, their BLEU peak at 
different N. For en-fr and en-de pairs, it gives the 
best translation results at N=25. Regarding cs-en 
and fr-en pairs, the optimal performance is 
peaked at N=50. While the best results for de-en 
and en-cs pairs are N=75 and N=100 respective-
ly. Besides, performance of TM adapted system 
heavily depends on the size and (domain) broad-
ness of the general-domain data. For example, 
the improvements of en-de and de-en systems are 
slight due to the small general-domain corpora. 
While the quality of other systems improve about 
3 BLEU points, because of their large and broad 
general-domain corpora.  
Lang. N=0 N=25 N=50 N=75 N=100 
en-cs 17.57 +0.84 +1.53 +1.74 +2.55 
cs-en 31.29 +2.03 +3.12 +3.12 +2.24 
en-fr 38.36 +3.87 +3.66 +3.53 +2.88 
fr-en 44.36 +1.29 +3.36 +1.84 +1.65 
en-de 18.01 +0.02 -0.13 -0.07 0 
de-en 32.50 -0.12 +0.06 +0.31 +0.24 
Table 5: BLEU scores of TM adapted systems. 
6. Numeric Adaptation 
As stated in Section 3, numeric occurs frequently 
in medical texts. However, numeric expression in 
dates, time, measuring unit, chemical formula are 
often sparse, which may lead to OOV problems 
in phrasal translation and reordering. Replacing 
the sparse numbers with placeholders may pro-
duce more reliable statistics for the MT models.  
Moses has support using placeholders in train-
ing and decoding. Firstly, we replace all the 
numbers in monolingual and parallel training 
corpus with a common symbol (a sample phrase 
is illustrated in Fig. 2). Models are then trained 
on these processed data. We use the XML 
markup translation method for decoding.  
Original: Vitamin D 1,25-OH  
Replaced: Vitamin D @num@, @num@-OH 
Figure 2. Examples of placeholders. 
Table 6 shows the results on this number ad-
aptation approach as well as the improvements 
compared to the Baseline2. The method im-
proves the Baseline2 systems by 0.23 to 0.40 
BLEU scores. Although the scores increase 
slightly, we still believe this adaptation method is 
significant for medical domain. The WMT2014 
medical task only focuses on the summary of 
257
medical text, which may contain fewer chemical 
expression in compared with the full article. As 
the used of numerical instances increases, place-
holder may play a more important role in domain 
adaptation.  
Lang. Pair BLEU (Dev) Diff. 
en-cs 17.80 +0.23 
cs-en 31.52 +0.23 
en-fr 38.72 +0.36 
fr-en 44.69 +0.33 
en-de 18.41 +0.40 
de-en 32.88 +0.38 
Table 6: BLEU scores of numeric adapted sys-
tems. 
7. Hyphenated Word Adaptation 
Medical texts prefer a kind of compound words, 
hyphenated words, which is composed of more 
than one word. For instance, ?slow-growing? and 
?easy-to-use? are composed of words and linked 
with hyphens. These hyphenated words occur 
quite frequently in medical texts. We analyze the 
development sets of cs, fr, en and de respective-
ly, and observe that there are approximately 
3.2%, 11.6%, 12.4% and 19.2% of sentences that 
contain one or more hyphenated words. The high 
ratio of such compound words results in Out-Of-
Vocabulary words (OOV) 10 , and harms the 
phrasal translation and reordering. However, a 
number of those hyphenated words still have 
chance to be translated, although it is not precise-
ly, when they are tokenized into individual 
words.  
Algorithm: Alternative-translation Method 
Input: 
1. A sentence, s, with M hyphenated words 
2. Translation lexicon 
Run: 
1. For i = 1, 2, ?, M 
2.   Split the ith hyphenated word (Ci) into 
Pi 
3.   Translate  Pi into Ti 
4.   If (Ti are not OOVs): 
5.      Put alternative translation Ti in XML 
6.    Else: keep Ci unchanged 
Output: 
Sentence, s?, embedded with alternative 
translations for all Ti. 
End 
Table 7: Alternative-translation algorithm. 
                                                 
10 Default tokenizer does not handle the hyphenated words. 
To resolve this problem, we present an alter-
native-translation method in decoding. Table 7 
shows the proposed algorithm. 
In the implementation, we apply XML markup 
to record the translation (terminology) for each 
compound word. During the decoding, a hyphen-
ated word delimited with markup will be re-
placed with its corresponding translation. Table 8 
shows the BLEU scores of adapted systems ap-
plied to hyphenated translation. This method is 
effective for most language pairs. While the 
translation systems for en-cs and cs-en do not 
benefit from this adaptation, because the hy-
phenated words ratio in the en and cs dev are 
asymmetric. Thus, we only apply this method for 
en-fr, fr-en, de-en and en-de pairs. 
Lang. Pair BLEU (Dev) Diff. 
en-cs 16.84 -0.73 
cs-en 31.23 -0.06 
en-fr 39.12 +0.76 
fr-en 45.02 +0.66 
en-de 18.64 +0.63 
de-en 33.01 +0.51 
Table 8: BLEU scores of hyphenated word 
adapted systems. 
3. Final Results and Conclusions 
According to the performance of each individual 
domain adaptation approach, we combined the 
corresponding models for each language pair. In 
Table 10, we show the BLEU scores and its in-
crements (compared to the Baseline2) of com-
bined systems in the second column. The official 
test set is converted into the recased and deto-
kenized SGML format. The official results of our 
submissions are given in the last column of Table 
9. 
Lang. 
Pair 
BLEU of Com-
bined systems 
Official 
BLEU 
en-cs 23.66 (+6.09) 22.60 
cs-en 38.05 (+6.76) 37.60 
en-fr 42.30 (+3.94) 41.20 
fr-en 48.25 (+3.89) 47.10 
en-de 21.14 (+3.13) 20.90 
de-en 36.03 (+3.53) 35.70 
Table 9: BLEU scores of the submitted systems 
for the medical translation task. 
This paper presents a set of experiments con-
ducted on all available training data for six lan-
guage pairs. We explored various domain adap-
tation approaches for adapting medical transla-
258
tion systems. Compared with other methods, lan-
guage model adaptation and translation model 
adaptation are more effective. Other adapted 
techniques are still necessary and important for 
building a real-life system. Although all individ-
ual methods are not fully additive, combining 
them together can further boost the performance 
of the overall domain-specific system. We be-
lieve these empirical approaches could be valua-
ble for SMT development. 
Acknowledgments 
The authors are grateful to the Science and 
Technology Development Fund of Macau and 
the Research Committee of the University of 
Macau for the funding support for their research, 
under the Reference nos. MYRG076 (Y1-L2)-
FST13-WF and MYRG070 (Y1-L2)-FST12-CS. 
The authors also wish to thank the colleagues in 
CNGL, Dublin City University (DCU) for their 
helpful suggestion and guidance on related work. 
Reference 
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 
2011. Domain adaptation via pseudo in-domain da-
ta selection. In Proceedings of EMNLP, pages 355-
362. 
K. Duh, G. Neubig, K. Sudoh, H. Tsukada. 2013. Ad-
aptation data selection using neural language mod-
els: Experiments in machine translation. In Pro-
ceedings of the Annual Meeting of the Association 
for Computational Linguistics, pages, 678?683. 
Qin Gao and Stephan Vogel. 2008. Parallel imple-
mentations of word alignment tool. Software Engi-
neering, Testing, and Quality Assurance for Natu-
ral Language Processing, pages 49-57. 
Kenneth Heafield. 2011. KenLM: Faster and smaller 
language model queries. In Proceedings of the 
Sixth Workshop on Statistical Machine Transla-
tion, pages 187-197. 
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine 
translation. In Proceedings of the 2nd ACL Work-
shop on Statistical Machine Translation, pages 
224-227. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Ber-
toldi, Brooke Cowan, Wade Shen, Christine Moran 
et al. 2007. Moses: open source toolkit for statisti-
cal machine translation. In Proceedings of ACL, 
pages 177-180. 
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In 
Proceedings of ACL: Short Papers, pages 220-224. 
S?tre Rune, Kazuhiro Yoshida, Akane Yakushiji, 
Yusuke Miyao, Yuichiro Matsubayashi and Tomo-
ko Ohta. 2007. AKANE system: protein-protein in-
teraction pairs in BioCreAtIvE2 challenge, PPI-IPS 
subtask. In Proceedings of the Second BioCreative 
Challenge Evaluation Workshop, pages 209-212.  
Raphael Rubino, Antonio Toral, Santiago Cort?s 
Va?llo, Jun Xie, Xiaofeng Wu, Stephen Doherty, 
and Qun Liu. 2013. The CNGL-DCU-Prompsit 
translation systems for WMT13. In Proceedings of 
the Eighth Workshop on Statistical Machine Trans-
lation, pages 213-218. 
Andreas Stolcke and others. 2002. SRILM-An exten-
sible language modeling toolkit. In Proceedings of 
the International Conference on Spoken Language 
Processing, pages 901-904. 
Antonio Toral. 2013. Hybrid selection of language 
model training data using linguistic information 
and perplexity. In ACL Workshop on Hybrid Ma-
chine Approaches to Translation. 
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment 
models. Computational Linguistics, 29:19-51. 
Franz Josef Och. 2003. Minimum error rate training 
in statistical machine translation. In Proceedings of 
the 41st Annual Meeting on Association for Com-
putational Linguistics, pages 160-167. 
Longyue Wang, Derek F. Wong, Lidia S. Chao, Yi 
Lu, and Junwen Xing. 2014 ?A Systematic Com-
parison of Data Selection Criteria for SMT Domain 
Adaptation,? The Scientific World Journal, vol. 
2014, Article ID 745485, 10 pages. 
Longyue Wang, Derek F. Wong, Lidia S. Chao, Yi 
Lu, Junwen Xing. 2013. iCPE: A Hybrid Data Se-
lection Model for SMT Domain Adaptation. Chi-
nese Computational Linguistics and Natural Lan-
guage Processing Based on Naturally Annotated 
Big Data. Springer Berlin Heidelberg. pages, 280-
290. 
 
259
