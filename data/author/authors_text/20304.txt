Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 428?432,
Dublin, Ireland, August 23-24, 2014.
NILC USP: An Improved Hybrid System for Sentiment Analysis in
Twitter Messages
Pedro P. Balage Filho, Lucas Avanc?o, Thiago A. S. Pardo, Maria G. V. Nunes
Interinstitutional Center for Computational Linguistics (NILC)
Institute of Mathematical and Computer Sciences, University of S?ao Paulo
S?ao Carlos - SP, Brazil
{balage, taspardo, gracan}@icmc.usp.br avanco@usp.br
Abstract
This paper describes the NILC USP sys-
tem that participated in SemEval-2014
Task 9: Sentiment Analysis in Twitter, a
re-run of the SemEval 2013 task under the
same name. Our system is an improved
version of the system that participated in
the 2013 task. This system adopts a hybrid
classification process that uses three clas-
sification approaches: rule-based, lexicon-
based and machine learning. We sug-
gest a pipeline architecture that extracts
the best characteristics from each classi-
fier. In this work, we want to verify how
this hybrid approach would improve with
better classifiers. The improved system
achieved an F-score of 65.39% in the Twit-
ter message-level subtask for 2013 dataset
(+ 9.08% of improvement) and 63.94% for
2014 dataset.
1 Introduction
Twitter is an important platform of social com-
munication. The analysis of the Twitter messages
(tweets) offers a new possibility to understand so-
cial behavior. Understanding the sentiment con-
tained in such messages showed to be very impor-
tant to understand user behavior and also to as-
sist market analysis (Java et al., 2007; Kwak et al.,
2010).
Sentiment analysis, the area in charge of study-
ing how sentiments and opinions are expressed in
texts, is usually associated with text classification
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
tasks. Sentiment classifiers are commonly cate-
gorized in two basic approaches: lexicon-based
and machine learning approaches (Taboada et al.,
2011). A lexicon-based classifier uses a lexicon
to provide the polarity, or semantic orientation, of
each word or phrase in the text. A machine learn-
ing classifier uses features (usually the vocabulary
in the texts) obtained from labeled examples to
classify the texts according to their polarity.
In this paper, we present a hybrid system for
sentiment classification in Twitter messages. Our
system combines the lexicon-based and machine
learning approaches, as well as uses simple rules
to aid in the process. Our system participated in
SemEval-2014 Task 9: Sentiment Analysis in Twit-
ter (Rosenthal et al., 2014), a re-run for the Se-
mEval 2013 task under the same name (Nakov et
al., 2013). The task goal was to determine the sen-
timent contained in tweets. The task included two
sub-tasks: a expression-level classification (Task
A) and a message-level classification (Task B).
Our system participated only in Task B, where, for
a given message, it should classify it as positive,
negative, or neutral.
The system presented is an improved version of
the system submitted for Semeval 2013. Our pre-
vious system had demonstrated that a hybrid ap-
proach could achieve good results (F-measure of
56.31%), even if we did not use the state-of-the-
art algorithms for each approach (Balage Filho and
Pardo, 2013). In this way, this work aims to ver-
ify how much this hybrid system could improve in
relation to the previous one by including modifica-
tions on both lexicon-based and machine learning
approaches.
428
2 Related work
The analysis of Tweets has gained lots of interest
recently. One evidence is the expressive number
of participants in the SemEval-2013 Task 2: Sen-
timent Analysis in Twitter (Nakov et al., 2013).
There were a total of 149 submissions from 44
teams. The best performing system on twitter
dataset for task B was reported by Mohammad et
al. (2013) with an F-mesaure of 69.02%. Their
system used a machine learning approach and a
very rich feature set. They showed that the best
results were achieved using a built-in positive and
negative lexicon and a bag-of-words as features.
Other important system in Semeval 2013 was
reported by Malandrakis et al. (2013). The authors
presented a hybrid system for twitter sentiment
analysis combining two approaches: a hierarchi-
cal model based on an affective lexicon and a lan-
guage modeling approach. The system achieved
an F-mesaure of 60.14%.
Most work in sentiment analysis uses either ma-
chine learning or lexicon-based techniques. How-
ever, few studies have shown promising results
with the hybrid approach. K?onig and Brill (2006)
proposed a hybrid classifier that uses human rea-
soning over automatically discovered text patterns
to complement machine learning. Prabowo and
Thelwall (2009) evaluated the effectiveness of dif-
ferent classifiers. Their study showed that the use
of multiple classifiers in a hybrid manner could
improve the effectiveness of sentiment analysis.
3 System Architecture
Our system is described as a pipeline solution of
four main processes: normalization, rule-based
classification, lexicon-based classification and ma-
chine learning classification. This is the same ar-
chitecture presented by our system in 2013.
This pipeline architecture works as a back-off
model. In this model, each classifier tries to clas-
sify the tweets by using the underlying approach.
If a certain degree of confidence is achieved, the
classifier will provide the final sentiment class for
the message. Otherwise, the next classifier will
continue the classification task. The last possibil-
ity is the machine learning classifier, responsible
to deliver the class when the previous two could
not achieve the confidence level. We decided to
use this back-off model instead of a voting system,
for example, due to the high precision achieved for
the rule-based and the lexicon-based classifiers.
The aim of this pipeline architecture is to im-
prove the classification process. In Balage Filho
and Pardo (2013), we have shown that this hybrid
classification approach may outperform the indi-
vidual approaches.
In the following subsections, we detail the com-
ponents of our system. In the next section, we ex-
plain how the confidence level was determined.
3.1 Normalization and Rule-based Classifier
The normalization module is responsible for nor-
malizing and tagging the texts. This module per-
forms the following operations:
? Hashtags, urls and mentions are transformed
into codes;
? Emoticons are grouped into representative
categories (such as ?happy?, ?sad?, ?laugh?)
and are converted to particular codes;
? Part-of-speech tagging is performed by using
the Ark-twitter NLP (Owoputi et al., 2013)
The rule-based classifier is designed to provide
rules that better impact the precision than the re-
call. In our 2014 system, we decided to use the
same rule-based classifier from the 2013 system.
The rules in this classifier only verify the pres-
ence of emoticons in the text. Empirically, we
evidenced that the use of emoticons indicates the
actual polarity of the message. In this module,
we consider the number of positive and negative
emoticons found in the text to determine its clas-
sification.
3.2 Lexicon-based Classifier
The lexicon-based classifier is based on the idea
that the polarity of a text can be given by the sum
of the individual polarity values of each word or
phrase present in the text. For this, a sentiment lex-
icon identifies polarity words and assigns polarity
values to them (known as semantic orientations).
In the 2013 system, we had used SentiStrength
lexicon (Thelwall et al., 2010). In 2014, we
improved our lexicon-based classifier by using
a larger sentiment lexicon. We used the senti-
ment lexicon provided by Opinion-Lexicon (Hu
and Liu, 2004) and a list of sentiment hashtags
provided by the NRC Hashtag Sentiment Lexicon
(Mohammad et al., 2013). For dealing with nega-
tion, we used a handcrafted list of negative words.
429
In our algorithm, the semantic orientations of
each individual word in the text are added up.
In this approach, the algorithm searches for each
word in the lexicon and only the words that were
found are returned. We associate the value +1 to
the positive words, and -1 to the negative words.
If a polarity word is negated, its value is inverted.
This lexicon-based classifier assumes the signal of
the final score as the sentiment class (positive or
negative) and the score zero as neutral.
3.3 Machine Learning Classifier
The machine learning classifier uses labeled ex-
amples to learn how to classify new instances.
The features used for this 2014 system were com-
pletely changed from 2013 system. We inspired
our machine learning module in the work reported
by Mohammad et al. (2013). The features used by
the classifier are:
1. unigrams, bigrams and trigrams
2. the presence of negation
3. the presence of three or more characters in
the words
4. the sequence of three or more punctuation
marks
5. the number of words with all letters in upper-
case
6. the total number of each tag present in the
text
7. the number of positive words computed by
the lexicon-based method
8. the number of negative words computed by
the lexicon-based method
We use a Linear Kernel SVM classifier provided
by the python sckit-learn library with C=0.005
1
.
4 Hybrid Approach and Tuning
The organization from SemEval-2014 Task 9: Sen-
timent Analysis in Twitter provided four datasets
for the task: a training dataset (TrainSet) with
9675 messages directly retrieved from Twitter; a
development dataset (DevSet), with 1654 mes-
sages; the testing dataset from 2013 run, which
was not used; and the testing dataset for 2014
1
Available at http://scikit-learn.org/
with 8987 messages. The 2014 testing dataset was
composed of 5 different sources:
? Twitter2013: Twitter test data from 2013 run
? SMS2013: SMS test data from 2013 run
? Twitter2014: 2000 tweets
? LiveJournal2014: 2000 sentences from Live-
Journal blogs
? Twitter2014Sarcasm: 100 tweets that contain
sarcasm
As we said in the previous section, our system is
a pipeline of classifiers where each classifier may
assign a sentiment class if it achieves a particu-
lar confidence threshold score. This confidence
score is a fixed value set for each system in or-
der to have a decision boundary. This decision
was made by inspecting the results obtained for the
development set. Tables 1 and 2 shows how the
rule-based and lexicon-based classifiers perform
for the development dataset in terms of score. The
score obtained by the rule-based classifier consists
of the difference between the number of positive
emoticons and the number of negative emoticons
found in the messages. The score obtained by the
lexicon-based classifier represents the total seman-
tic orientation obtained by the algorithm by adding
up the semantic orientation for their lexicon.
Inspecting Table 1, for the best threshold, we
adjusted the rule-based classifier boundary to de-
cide when the score is different from zero. For
values greater than zero, the classifier will assign
the positive class and, for values below zero, the
classifier will assign the negative class. For values
equal to zero, the classifier will call the lexicon-
based classifier.
Table 1: Correlation between the rule-based clas-
sifier scores and the gold standard classes in the
DevSet
Rule-based Gold Standard Class
classifier score Negative Neutral Positive
-1 22 3 3
0 311 709 495
1 7 26 73
2 0 0 2
3 to 6 0 1 2
Inspecting Table 2, for the best threshold, we
adjusted the lexicon-based classifier to assign the
430
positive class when the total score is greater than
1 and negative class when the total score is below
-2. For any other values, the classifier will call the
machine learning classifier.
Table 2: Correlation between the lexicon-based
classifier score and the gold standard classes in the
devset
Lexicon-based Gold Standard Class
classifier scores Negative Neutral Positive
-7 to -4 2 0 0
-3 10 4 0
-2 48 18 7
-1 111 99 35
0 108 432 178
1 48 143 210
2 11 39 104
3 to 5 3 4 47
As the machine learning classifier is responsible
for the final stage, we did not have to decide any
threshold for this classifier. However, we empiri-
cally identified a bias toward the positive class (the
negative class was barely chosen). In order to cor-
rect this problem, we setup the machine learning
classifier to decide for the negative class whenever
the SVM score for this class is bigger than -0.4.
Next section shows the results achieved for the Se-
meval test dataset.
5 Results
Table 3 shows the results obtained by each individ-
ual classifier and by the hybrid classifier for the
Twitter2014 messages in the testset. In the task,
the systems were evaluated with the average F-
score obtained for positive and negative classes.
Table 3: Average F-score (positive and negative)
obtained by each classifier and the hybrid ap-
proach for the Twitter2014 testset
Classifier Twitter2014 Testset
Rule-based 14.03
Lexicon-Based 47.55
Machine Learning 63.36
Hybrid Approach 63.94
Table 4 shows the improvement of the system
over the 2013 run. Unlike last year, we notice that
the performance of this hybrid system is very close
to the performance of the machine-learning.
Table 4: Comparison of the average F-score (pos-
itive and negative) obtained by each classifier and
the hybrid approach for the Twitter2013 testset for
2013 and 2014 versions
Classifier 2013 system 2014 system
Rule-based 14.37 13.31
Lexicon-Based 44.87 46.80
Machine Learning 49.99 63.75
Hybrid Approach 56.31 65.39
Table 5 shows the scores for each source in the
testset. Last column shows our system rank among
the 50 systems that participated in the competition.
For the entire testing dataset, our algorithm had
503 (5%) examples classified by the rule-based
classifier, 3204 (36%) by the lexicon-based classi-
fier and 5280 (59%) by the machine learning clas-
sifier.
6 Conclusion
We described our improved hybrid classification
system used for Semeval-2014 Task 9: Sentiment
Analysis in Twitter. This work showed that this
hybrid classifier can be improved as its modules
are too. However, we noticed that, improving the
lexicon and machine learning modules, the overall
score tends towards the machine learning score.
The source code produced for the experiment is
available at https://github.com/pedrobalage.
Acknowledgments
We would like to thank the organizers for their
work in constructing the dataset and in the over-
seeing of the task. We also would like to
thank FAPESP and SAMSUNG for supporting
this work.
References
Pedro Balage Filho and Thiago Pardo. 2013.
NILC USP: A Hybrid System for Sentiment Analy-
sis in Twitter Messages. In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
Volume 2: Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
pages 568?572, Atlanta, Georgia, USA, June. Asso-
ciation for Computational Linguistics.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
431
Table 5: Results for Twitter TestSet
TestSet Source Majority Baseline Our Score Best Result Our Rank
Twitter2013 29.2 65.39 72.12 15th
SMS2013 19.0 61.35 70.28 16th
Twitter2014 34.6 63.94 70.96 19th
LiveJournal2014 27.2 69.02 74.84 18th
Twitter2014Sarcasm 27.7 42.06 58.16 34th
Akshay Java, Xiaodan Song, Tim Finin, and Belle
Tseng. 2007. Why we twitter: understanding mi-
croblogging usage and communities. In Proceed-
ings of the 9th WebKDD and 1st SNA-KDD 2007
workshop on Web mining and social network anal-
ysis, WebKDD/SNA-KDD ?07, pages 56?65, New
York, NY, USA. ACM.
Arnd Christian K?onig and Eric Brill. 2006. Reducing
the human overhead in text categorization. In Pro-
ceedings of the 12th ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, KDD ?06, pages 598?603, New York, NY, USA.
ACM.
Haewoon Kwak, Changhyun Lee, Hosung Park, and
Sue Moon. 2010. What is twitter, a social network
or a news media? In Proceedings of the 19th inter-
national conference on World wide web, WWW ?10,
pages 591?600, New York, NY, USA. ACM.
Nikolaos Malandrakis, Abe Kazemzadeh, Alexan-
dros Potamianos, and Shrikanth Narayanan. 2013.
SAIL: A hybrid approach to sentiment analysis. In
Second Joint Conference on Lexical and Computa-
tional Semantics (*SEM), Volume 2: Proceedings
of the Seventh International Workshop on Seman-
tic Evaluation (SemEval 2013), pages 438?442, At-
lanta, Georgia, USA, June. Association for Compu-
tational Linguistics.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the State-of-
the-Art in Sentiment Analysis of Tweets. In Second
Joint Conference on Lexical and Computational Se-
mantics (*SEM), Volume 2: Proceedings of the Sev-
enth International Workshop on Semantic Evalua-
tion (SemEval 2013), pages 321?327, Atlanta, Geor-
gia, USA, June. Association for Computational Lin-
guistics.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 312?
320, Atlanta, Georgia, USA, June. Association for
Computational Linguistics.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 380?390, Atlanta, Georgia, June. Association
for Computational Linguistics.
Rudy Prabowo and Mike Thelwall. 2009. Sentiment
analysis: A combined approach. Journal of Infor-
metrics, 3(2):143?157.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment Analysis in Twitter. In Preslav Nakov and
Torsten Zesch, editors, Proceedings of the 8th In-
ternational Workshop on Semantic Evaluation, Se-
mEval ?14, Dublin, Ireland.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
Based Methods for Sentiment Analysis. Computa-
tional Linguistics, 37(2):267?307, June.
Mike Thelwall, Kevan Buckley, Georgios Paltoglou,
Di Cai, and Arvid Kappas. 2010. Sentiment in
short strength detection informal text. Journal of the
American Society for Information Science and Tech-
nology, 61(12):2544?2558, December.
432
Felix Bildhauer & Roland Sch?fer (eds.), Proceedings of the 9th Web as Corpus Workshop (WaC-9) @ EACL 2014, pages 22?28,
Gothenburg, Sweden, April 26 2014. c?2014 Association for Computational Linguistics
Some issues on the normalization of a corpus of products reviews in 
Portuguese 
 
Magali S. Duran 
NILC-ICMC 
University of S?o Paulo 
Brazil 
 magali.duran@gmail.com 
Lucas V. Avan?o 
NILC-ICMC 
University of S?o Paulo 
Brazil  
avanco89@gmail.com 
Sandra M. Alu?sio 
NILC-ICMC 
University of S?o Paulo 
Brazil 
sandra@icmc.usp.br 
 
Thiago A. S. Pardo 
NILC-ICMC 
University of S?o Paulo 
Brazil 
taspardo@icmc.usp.br 
Maria G. V. Nunes 
NILC-ICMC 
University of S?o Paulo 
Brazil 
gracan@icmc.usp.br 
 
Abstract 
This paper describes the analysis of different 
kinds of noises in a corpus of products 
reviews in Brazilian Portuguese. Case 
folding, punctuation, spelling and the use of 
internet slang are the major kinds of noise we 
face. After noting the effect of these noises 
on the POS tagging task, we propose some 
procedures to minimize them. 
1. Introduction 
 
Corpus normalization has become a common 
challenge for everyone interested in processing a 
web corpus. Some normalization tasks are 
language and genre independent, like boilerplate 
removal and deduplication of texts. Others, like 
orthographic errors correction and internet slang 
handling, are not.  
Two approaches to web corpus normalization 
have been discussed in Web as a Corpus (WAC)  
literature. One of them is to tackle the task as a 
translation problem, being the web texts the 
source language and the normalized texts the 
target language (Aw et al., 2006; Contractor et 
al., 2010; Schlippe et al., 2013). Such approach 
requires a parallel corpus of original and 
normalized texts of reasonable size for training a 
system with acceptable accuracy. The other 
approach is to tackle the problem as a number of 
sub problems to be solved in sequence 
(Ringlstetter et al., 2006; Bildhauer & Sch?fer, 
2013; Sch?fer et al., 2013). 
The discussion we engage herein adopts the 
second approach and is motivated by the  
demand of preprocessing a Brazilian Portuguese 
web corpus constituted of products reviews for 
the specific purpose of building an opinion 
mining classifier and summarizer. Our project 
also includes the task of adding a layer of 
semantic role labeling to the corpus. The roles 
will be assigned to nodes of the syntactic trees 
and, therefore, SRL subsumes the existence of 
layers of morphosyntactic and syntactic 
annotations. The annotated corpus will be used 
as training corpus for a SRL classifier. The aim 
of SRL classifier, on its turn, is to provide deep 
semantic information that may be used as 
features by the opinion miner. If the text is not 
normalized, the POS tagger does not perform 
well and compromise the parsing result, which, 
as consequence, may generate defective trees, 
compromising the assignment of role labels to 
their nodes. 
In fact, mining opinions from a web corpus is 
a non-trivial NLP task which often requires some 
language processing, such as POS tagging and 
parsing. Most of taggers and parsers are made to 
handle error-free texts; therefore they may 
jeopardize the application results when they face 
major noises. What constitutes a major noise and 
which noise may be removed or corrected in 
such a corpus is the challenge we are facing in 
this project. 
22
 2. Related Work 
 
Depending on the point of view, there are 
several studies that face problems similar to 
those faced by us. The general issue is: how to 
convert a non-standard text into a standard one? 
By non-standard text we mean a text produced 
by people that have low literacy level or by 
foreign language learners or by speech-to-text 
converters, machine translators or even by  
digitization process. Also included in this class 
are the texts produced in special and informal 
environments such as the web. Each one of these 
non-standard texts has its own characteristics. 
They may differ in what concerns spelling, non-
canonical use of case, hyphen, apostrophe, 
punctuation, etc. Such characteristics are seen as 
?noise? by NLP tools trained in well written texts 
that represent what is commonly known as 
standard language. Furthermore, with the 
widespread use of web as corpus, other types of 
noise need to be eliminated, as for example 
duplication of texts and boilerplates.  
The procedures that aim to adapt texts to 
render them more similar to standard texts are 
called normalization. Some normalization 
procedures like deduplication and boilerplate 
removal are less likely to cause destruction of 
relevant material. The problem arises when the 
noise category contains some forms that are 
ambiguous to other forms of the standard 
language. For example, the words ?Oi? and 
?Claro? are the names of two Brazilian mobile 
network operators, but they are also common 
words (?oi? = hi; ?claro? = clear). Cases like 
these led Lita et al. (2003) to consider case 
normalization as a problem of word sense 
disambiguation. Proper nouns which are derived 
from common nouns (hence, distinguished only 
by case) are one of the challenges for case 
normalization reported by Manning et al. (2008). 
Similar problem is reported by Bildhauer and 
Sch?fer (2013) regarding dehyphenation, that is, 
the removal of hyphens used in typeset texts and 
commonly found in digitized texts. In German, 
there are many hyphenated words and the 
challenge is to remove noisy hyphens without 
affecting the correct ones. There are situations, 
however, in which both the corrected and the 
original text are desired. For example, social 
media corpora are plain of noises that express 
emotions, a rich material for sentiment analysis. 
For these cases, the non-destructive strategy 
proposed by Bildhauer and Sch?fer (2013), 
keeping the corrected form as an additional 
annotation layer, may be the best solution.  
 
3. Corpus of Products Reviews 
 
To build the corpus of products reviews, we 
have crawled  a products reviews database of one 
of the most traditional online services in Brazil, 
called Buscap?, where customers post their 
comments about several products. The comments 
are written in a free format within a template 
with three sections: Pros, Cons, and Opinion. We 
gathered 85,910 reviews, totaling 4,088,718 
tokens and 90,513 types. After removing stop 
words, numbers and punctuation, the frequency 
list totaled 63,917 types. 
Customers have different levels of literacy 
and some reviews are very well written whereas 
others present several types of errors. In addition, 
some reviewers adopt a standard language style, 
whereas others incorporate features that are 
typical of the internet informality, like abusive 
use of abbreviations, missing or inadequate 
punctuation; a high percentage of named entities 
(many of which are misspelled); a high 
percentage of foreign words; the use of internet 
slang; non-conventional use of uppercase; 
spelling errors and missing of diacritic signals. 
A previous work (Hartmann et al. 2014) 
investigated the nature and the distribution of the 
34,774 words of the corpus Buscap? not 
recognized by Unitex, a Brazilian Portuguese 
lexicon (Muniz et. al. 2005). The words for 
which only the diacritic signals were missing 
(3,652 or 10.2%) have been automatically 
corrected. Then, all the remaining words with 
more than 2 occurrences (5775) were classified 
in a double-blind annotation task, which obtained 
0,752 of inter-annotator agreement (Kappa 
statistics, Carletta, 1996). The results obtained 
are shown in Table 1.  
 
Table 1. Non-Recognized Words with more 
than 2 occurrences in the corpus 
Common Portuguese misspelled words 44% 
Acronyms 5% 
Proper Nouns 24% 
Abbreviations 2% 
Internet Slang 4% 
Foreign words used in Portuguese 8% 
Units of Measurement 0% 
Other problems  13% 
Total 100% 
23
The study reported herein aims to investigate 
how some of these problems occur in the corpus 
and to what extent they may affect POS tagging. 
Future improvements remain to be done in the 
specific tools that individually tackle these 
problems.  
 
4. Methodology  
 
As the same corpus is to be used for different 
subtasks ? semantic role labeling, opinion 
detection, classification and summarization ? the 
challenge is to normalize the corpus but also 
keep some original occurrences that may be 
relevant for such tasks. Maintaining two or more 
versions of the corpus is also being considered. 
To enable a semi-automatic qualitative and 
quantitative investigation, a random 10-reviews 
sample (1226 tokens) of the original corpus was 
selected and POS tagged by the MXPOST tagger 
which was trained on MAC-Morpho, a 1.2 
million tokens corpus of Brazilian Portuguese 
newspaper articles (Alu?sio et al., 2003).  
It is worthwhile to say that the sampling did 
not follow statistical principles. In fact, we 
randomly selected 10 texts (1226 tokens from a 
corpus of 4,088,718 tokens), which we 
considered a reasonable portion of text to 
undertake the manual tasks required by the first 
diagnosis experiments. Our aim was to explore 
tendencies and not to have a precise statistical 
description of the percentage of types of errors in 
the corpus. Therefore, the probabilities of each 
type of error may not reflect those of the entire 
corpus.  
We manually corrected the POS tagged 
version to evaluate how many tags were 
correctly assigned. The precision of MXPOST in 
our sample is 88.74%, while its better precision, 
of 96.98%, has been obtained in its training 
corpus. As one may see, there was a decrease of 
8.49% in performance, which is expected in such 
change of text genre. 
In the sequence, we created four manually 
corrected versions of the sample, regarding each 
of the following normalization categories: 
spelling (including foreign words and named 
entities); case use; punctuation; and use of 
internet slang. This step produced four golden 
corpus samples which were used for separate 
evaluations. The calculation of the difference 
between the original corpus sample and each of 
the golden ones led us to the following 
conclusions.  
The manual corrections of the sample were 
made by a linguist who followed some rules  
established in accordance with the project goals 
and the MXPOST annotation guidelines1. As a 
result, only the punctuation correction allowed 
some subjective decisions; the other kinds of 
correction were very objective. 
 
5. Results of diagnosing experiments 
 
Regarding to spelling, 2 foreign words, 3 
named entities and 19 common words were 
detected as misspelled. A total of 24 (1.96%) 
words have been corrected. There are 35 words 
(2.90%) for which the case have been changed (6 
upper to lower and 29 in the reverse direction). 
Punctuation has showed to be a relevant 
issue: 48 interventions (deletions, insertions or 
substitutions) have been made to turn the texts 
correct, representing 3.92% of the sample. 
Regarding internet slang, only 3 occurrences 
(0.24%) were detected in the sample, what 
contradicted our expectation that such lexicon 
would have a huge impact in our corpus. 
However due to the size of our sample, this may 
have occurred by chance.  
The precision of the POS tagged sample has 
been compared with the ones of the POS tagged 
versions of golden samples. The results showed 
us the impact of the above four normalization 
categories on the tagger performance.  
We have verified that there was improvement 
after the correction of each category, reducing 
the POS tagger errors as shown in Table 2. When 
we combine all the categories of correction 
before tagging the sample, the cumulative result 
is an error reduction of 19.56%.  
 
Table 2. Improvement of the tagger precision 
in the sample 
Case Correction + 15.94% 
Punctuation Correction + 4.34% 
Spelling + 2.90% 
Internet Slang Convertion + 1.45% 
Cumulative Error Reduction 19.56% 
 
These first experiments revealed that case 
correction has major relevance in the process of 
normalizing our corpus of products reviews. It is 
important to note that case information is largely 
                                                          
1
 Available at 
http://www.nilc.icmc.usp.br/lacioweb/manuais.htm 
24
used as feature by Named Entities Recognizers 
(NER), POS taggers and parsers. 
To evaluate whether the case use distribution 
is different from that of a corpus of well written 
texts, we compared the statistics of case use in 
our corpus with those of a newspaper corpus 
(http://www.linguateca.pt/CETENFolha/), as 
shown in Table 3. 
 
Table 3. Percentage of case use in newspaper 
and products reviews corpus genres 
CORPUS Newspaper 
 
Products 
Reviews 
Uppercase words 6.41% 5.30% 
Initial uppercase 
words 
20.86% 7.30% 
Lowercase words 70.79% 85.37% 
 
The differences observed led us to conclude 
that the tendency observed in our sample (proper 
names and acronyms written in lower case) is 
probably a problem for the whole corpus.  
To confirm such conclusion, we searched in 
the corpus the 1,339 proper nouns identified in 
our previous annotation task. They occurred 
40,009 times with the case distribution shown in 
Table 4. 
 
Table 4. Case distribution of Proper Nouns 
Initial uppercase words 15,148 38% 
Uppercase words 7,392 18% 
Lower case words 17,469 44% 
Total 40,009 100% 
 
The main result of these experiments is the 
evidence that the four kind of errors investigated 
do affect POS tagging. In the next section we 
will detail the procedures envisaged to provide 
normalization for each one of the four categories 
of errors.  
 
6. Towards automatic normalization 
procedures 
 
After diagnosing the needs of text 
normalization of our corpus, we started to test 
automatic procedures to meet them. The 
processing of a new genre always poses a 
question: should we normalize the new genre to 
make it similar to the input expected by available 
automatic tools or should we adapt the existing 
tools to process the new genre? This is not a 
question of choice, indeed. We argue that both 
movements are needed. Furthermore, the 
processing of a new genre is an opportunity not 
only to make genre-adaptation, but also to 
improve general purpose features of NLP tools. 
 
6.1 Case normalization: truecasing 
 
In NLP the problem of case normalization is 
usually called ?truecasing? (Lita et al, 2003, 
Manning et al., 2008). The challenge is to decide 
when uppercase should be changed into lower 
case and when lower case should be changed into 
upper case. In brief, truecasing is the process of 
correcting case use in badly-cased or non-cased 
text. 
The problem is particularly relevant in two 
scenarios; speech recognition and informal web 
texts. 
We prioritized the case normalization for two 
reasons: first, badly-cased text seems to be a 
generalized problem in the genre of products 
reviews and, second, it is important to make case 
normalization before using a spell checker. This 
is crucial to ?protect? Named Entities from 
spelling corrections because when non-
recognized lowercase words are checked by 
spellers, there is the risk of wrong correction. 
Indeed, the more extensive is the speller lexicon, 
the greater is the risk of miscorrection. 
The genre under inspection presents a 
widespread misuse of case. By one side, lower 
case is used in place of uppercase in the initial 
letter of proper names. On the other side, upper 
case is used to emphasize any kind of word.  
Our first tentative to tackle the problem of 
capitalization was to submit the samples to a 
Named Entity Recognizer. We chose Rembrandt2 
(Cardoso, 2012), a Portuguese NER that 
enhances both lexical knowledge extracted from 
Wikipedia and statistical knowledge.  
The procedure was: 1) to submit the sample 
to Rembrandt; 2) to capitalize the recognized 
entities written in lower case; 3) to change all the 
words capitalized, except the named entities, to 
lower case. Then we tagged the sample with 
MXPOST to evaluate the effect on POS tagging 
accuracy.  
The number of errors of POS tagging 
increased (149) when compared to the one of the 
sample without preprocessing (138). The 
                                                          
2
 The Portuguese named entity recognition is made by 
system Rembrandt (http://xldb.di.fc.ul.pt/Rembrandt/) 
25
explanation for this is that among the words not 
recognized as named entities there were 
capitalized named entities which were lost by 
this strategy. 
Next we tried a new version of this same 
experiment: we only changed into lower case the 
words not recognized as named entities that were 
simultaneously recognized by Unitex. The results 
were slightly better (143 errors) compared to the 
first version of the experiment, but still worse 
than those of the sample without preprocessing.  
Our expectation was to automatically 
capitalize the recognized entities written in lower 
case. In both experiments, however, no word was 
changed from lower to upper case because all the 
entities recognized by the NER were already 
capitalized.  
The sample contains 57 tokens of named 
entities (corresponding to proper nouns and 
acronyms) from which 24 were written in lower 
case. The NER recognized 22 of the 57 or 18 of 
the 38 types of named entities (a performance of 
47.4%). Unfortunately the NER is strongly based 
on the presence of capitalized initial letters and 
was of no aid in the procedure we tested. 
We argue that a finite list of known proper 
nouns and acronyms, although useful for 
improving evaluation figures, is of limited use 
for an application such as an opinion miner. In 
real scenarios this constitutes an open class and 
new entities shall be recognized as well.  
We observed that many of the named entities 
found in the reviews relate to the product being 
reviewed and to the company that produces it. 
Then we realized an advantage of the source 
from which we have crawled the reviews: the 
customers are only allowed to review products 
that have been previously registered in the site 
database. The register of the name of the product 
is kept in our corpus as metadata for each review. 
This situation gave us the opportunity to 
experiment another strategy: to identify named 
entities of each review in its respective metadata 
file. We first gathered all the words annotated as 
Proper Nouns and Acronyms in our previous 
annotation task3. Then we search for the matches. 
The result is promising: from 1,334 proper nouns 
and from 271 acronyms, respectively 676 
                                                          
3
 Confusion matrix of our double annotated data show 
that annotators diverged in what concerns Proper Nouns and 
Acronyms. For our purposes, however, all of them are 
named entities and need to be capitalized, so that this kind 
of disagreement did not affect the use we have made of the 
annotated words. 
(50.67%) and 44 (16.23%) were found in the 
metadata. Adding both types of named entities, 
we have a match of 44.85% (720 of 1605). This 
is pretty good mainly because the named entities 
recognized are precisely the names of products 
for which opinions will be mined. 
However, we still need to solve the 
recognition of the other named entities in order 
to support the truecasing strategies.  
Following Lita et al. (2003) and Beaufays and 
Strope (2013), we are considering using a 
language model. Lita et al. developed a truecaser 
for news articles, a genre more ?stable? than 
products reviews. Beaufays and Strope, on their 
turn, developed a truecaser to tackle texts 
generated from speech recognition. Language 
modeling may be a good approach to our 
problem because many named entities of 
products domain do not sound as Portuguese 
words. For example, they frequently have the 
consonants k, y and w, which are only used in 
proper names in Portuguese. Other approaches to 
truecasing reported in the literature include finite 
state transducers automatically built from 
language models and maximum entropy models 
(Batista et al. 2008). 
 
6.2 Punctuation problems 
 
Many reviews have no punctuation at all. 
This prevents processing the text by most of NLP 
tools which processes sentences. Some 
grammatical rules may be used to correct the use 
of comma, but the problem is more complex in 
what concerns full stop. We are now training a 
machine learning based program with a corpus of 
well written texts by using features related to n-
grams. We aim at building a sentence 
segmentation tool which does not depend on the 
presence of punctuation or case folding, since 
these are major noises in the corpus.  
 
6.3 Spelling correction 
 
The common Portuguese words in the corpus 
which were not recognized by Unitex have been 
spell checked. Manual analysis is being 
undertaken to determine whether the word has 
been accurately corrected or not. Early results 
evidenced opportunity to extend Unitex and to 
improve our spellers with more phonetic rules in 
order to suggest more adequate alternatives. As 
we have already mentioned, product reviewers 
have several levels of literacy and those of lower 
level frequently swap the consonant letters that 
26
conveys the same phonetic value. For example, 
in Portuguese the letters ?s?, ?c?, ?xc? ?ss? and 
??? can have the same sound: /s/. Therefore, it is 
a common mistake to employ one instead of the 
other. These rules shall be incorporated in spell 
checker. In addition, there are many words which 
were correctly spelled, but were not part of 
Unitex or of the speller?s dictionary or both. 
Both lexicons will be extended with the missing 
words. 
In the same way, the foreign words of current use 
in Brazilian Portuguese shall be incorporated in 
the spell checkers in order to improve their 
suggestions of correction. As a matter of fact, 
foreign words are frequently misspelled. For 
example, ?touchscreen? appeared as 10 different 
spelling forms in our corpus with more than 2 
occurrences (?toch escreen?, ?touch screem?, 
?touch sreen?, ?touche?, ?touch scream?, 
?touchscream?, ?touchscreem?, ?touch-screen?, 
?touchsren?, ?touch screen"). 
 
6.4 Internet slang normalization 
 
Internet slang is a class that combines: 1) 
words written in a different way and 
abbreviations of recurrent expressions, for which 
there is an equivalent in the standard language 
(in this case the procedure is to substitute one for 
another); 2) repeated letters and punctuation (e.g. 
!!!!!!!!!!!!, and ameiiiiiiiiiiiiiiiiiiiiiii, in which the 
word "amei" = ?love? is being emphasized), 
which may be normalized by eliminating 
repetitions; and 3) sequences of letters related to 
emotion expression, like emoticons (e.g. ?:)?, 
?:=(?), laughing (e.g. rsrsrsrs, heheheh, 
kkkkkkkk), which for some purposes shall be 
eliminated and for others shall not. The 
procedures relating to internet slang will be 
implemented carefully  to allow the user to 
activate each one of the three procedures 
separately, depending on his/her interest in 
preserving emotion expression or not.  
  
7. Final Remarks 
 
This preliminary investigation about the 
needs of text normalization for the genre of 
products reviews led us to deep understand our 
challenges and to envisage some solutions.  
We have opened some avenues for future 
works and established an agenda for the next 
steps towards corpus normalization.  
 
Acknowledgments  
This research work is being carried on as part of 
an academic agreement between University of 
S?o Paulo and Samsung Eletr?nica da Amaz?nia 
Ltda.  
References  
Alu?sio, S. M.; Pelizzoni, J. M.; Marchi, A. R.; 
Oliveira, L. H.; Manenti, R.; Marquivaf?vel, V. 
(2003). An account of the challenge of tagging a 
reference corpus of Brazilian Portuguese. In: 
Proceedings of PROPOR?2003. Springer Verlag, 
2003, pp. 110-117. 
Aw, A.; Zhang, M.; Xiao, J.; Su, J. (2006).  A Phrase-
based Statistical Model for SMS Text 
Normalization. In: Proceedings of the COLING-
2006 .ACL, Sydney, 2006, pp. 33?40. 
Batista, F.; Caseiro, D. A.;  Mamede, N. J.; Trancoso, 
I. (2008). Recovering Capitalization and 
Punctuation Marks for Automatic Speech 
Recognition: Case Study for the Portuguese 
Broadcast News, Speech Communication, vol. 50, 
n. 10, pages 847-862, doi: 
10.1016/j.specom.2008.05.008, October 2008 
Beaufays, F.; Strope, B. (2013) Language Model 
Capitalization. In:  2013 IEEE International 
Conference on Acoustics, Speech and Signal 
Processing (ICASSP), p. 6749 ? 6752. 
Bildhauer, F.; Sch?fer, R. (2013) Token-level noise in 
large Web corpora and non-destructive 
normalization for linguistic applications. In: 
Proceedings of Corpus Analysis with Noise in the 
Signal (CANS 2013) . 
Cardoso, N. (2012). Rembrandt - a named-entity 
recognition framework. In: Proceedings of the 
Eight International Conference on Language 
Resources and Evaluation (LREC'12). May, 23-25, 
Istanbul, Turkey. 
Carletta, J.: Assessing Agreement on Classification 
Tasks: The Kappa Statistic. Computational 
Linguistics, vol. 22, n. 2, pp. 249--254. (1996) 
Contractor, D.; Tanveer A.; Faruquie; L.; 
Subramaniam, V. (2010). Unsupervised cleansing 
of noisy text. Coling 2010: Poster Volume, pages 
189?196, Beijing, August 2010. 
Hartmann, N. S.; Avan?o. L.; Balage, P. P.; Duran, 
M. S.; Nunes, M. G. V.; Pardo, T.; Alu?sio, S. 
(2014). A Large Opinion Corpus in Portuguese - 
Tackling Out-Of-Vocabulary Words. In: 
Proceedings of the Ninth International Conference 
27
on Language Resources and Evaluation (LREC 
2014). Forthcoming. 
Lita, L., Ittycheriah, A., Roukos, S. & Kambhatla,N. 
(2003), Truecasing, In: Proceedings of the 41st 
Annual Meeting of the Association for 
Computational Linguistics, Japan. 
Manning, C. D., Raghavan, P., & Sch?tze, H. (2008). 
Introduction to information retrieval (Vol. 1). 
Cambridge: Cambridge university press. 
Muniz, M.C.M.; Nunes, M.G.V.; Laporte. E. (2005) 
"UNITEX-PB, a set of flexible language resources 
for Brazilian Portuguese", Proceedings of the 
Workshop on Technology of Information and 
Human Language (TIL), S?o Leopoldo (Brazil): 
Unisinos. 
Ringlstetter, C.; Schulz, K. U. and Mihov, S. (2006). 
Orthographic Errors in Web Pages: Toward 
Cleaner Web Corpora. In: Computational 
Linguistics Volume 32, Number 3, p. 295-340.  
Sch?fer, R.; Barbaresi, A.; Bildhauer, F. (2013) The 
Good, the Bad, and the Hazy: Design Decisions in 
Web Corpus Construction. In:  Proceedings of the 
8th Web as Corpus Workshop (WAC-8). 
Schlippe, T.; Zhu, C.; Gebhardt J.;, Schultz, T.(2013). 
Text Normalization based on Statistical Machine 
Translation and Internet User Support. In: 
Proceedings of The 38th International Conference 
on Acoustics, Speech, and Signal Processing  
(ICASSP-2013) p. 8406 ? 841. 
 
 
28
