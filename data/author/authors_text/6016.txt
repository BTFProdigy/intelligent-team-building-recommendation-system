Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 393?400
Manchester, August 2008
The Effect of Syntactic Representation on Semantic Role Labeling
Richard Johansson and Pierre Nugues
Lund University, Sweden
{richard, pierre}@cs.lth.se
Abstract
Almost all automatic semantic role label-
ing (SRL) systems rely on a preliminary
parsing step that derives a syntactic struc-
ture from the sentence being analyzed.
This makes the choice of syntactic repre-
sentation an essential design decision. In
this paper, we study the influence of syn-
tactic representation on the performance
of SRL systems. Specifically, we com-
pare constituent-based and dependency-
based representations for SRL of English
in the FrameNet paradigm.
Contrary to previous claims, our results
demonstrate that the systems based on de-
pendencies perform roughly as well as
those based on constituents: For the ar-
gument classification task, dependency-
based systems perform slightly higher on
average, while the opposite holds for the
argument identification task. This is re-
markable because dependency parsers are
still in their infancy while constituent pars-
ing is more mature. Furthermore, the re-
sults show that dependency-based seman-
tic role classifiers rely less on lexicalized
features, which makes them more robust
to domain changes and makes them learn
more efficiently with respect to the amount
of training data.
1 Introduction
The role-semantic paradigm has a long and rich
history in linguistics, and the NLP community
has recently devoted much attention to develop-
ing accurate and robust methods for performing
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
role-semantic analysis automatically (Gildea and
Jurafsky, 2002; Litkowski, 2004; Carreras and
M?rquez, 2005; Baker et al, 2007). It is widely
conjectured that an increased SRL accuracy will
lead to improvements in certain NLP applica-
tions, especially template-filling systems. SRL has
also been used in prototypes of more advanced
semantics-based applications such as textual en-
tailment recognition.
It has previously been shown that SRL systems
need a syntactic structure as input (Gildea and
Palmer, 2002; Punyakanok et al, 2008). An im-
portant consideration is then what information this
input should represent. By habit, most systems for
automatic role-semantic analysis have used Penn-
style constituents (Marcus et al, 1993) produced
by Collins? (1997) or Charniak?s (2000) parsers.
The influence of the syntactic formalism on SRL
has only been considered in a few previous arti-
cles. For instance, Gildea and Hockenmaier (2003)
reported that a CCG-based parser gives improved
results over the Collins parser.
Dependency syntax has only received little at-
tention for the SRL task, despite a surge of inter-
est in dependency parsing during the last few years
(Buchholz and Marsi, 2006). Early examples of
dependency-based SRL systems, which used gold-
standard dependency treebanks, include ?abokrt-
sk? et al (2002) and Hacioglu (2004). Two stud-
ies that compared the respective performances of
constituent-based and dependency-based SRL sys-
tems (Pradhan et al, 2005; Swanson and Gor-
don, 2006), both using automatic parsers, reported
that the constituent-based systems outperformed
the dependency-based ones by a very wide mar-
gin. However, the figures reported in these studies
can be misleading since the comparison involved a
10-year-old rule-based dependency parser versus a
state-of-the-art statistical constituent parser. The
recent progress in statistical dependency parsing
gives grounds for a new evaluation.
393
In addition, there are a number of linguistic mo-
tivations why dependency syntax could be bene-
ficial in an SRL context. First, complex linguis-
tic phenomena such as wh-word extraction and
topicalization can be transparently represented by
allowing nonprojective dependency links. These
links also justify why dependency syntax is of-
ten considered superior for free-word-order lan-
guages; it is even very questionable whether the
traditional constituent-based SRL strategies are vi-
able for such languages. Second, grammatical
function such as subject and object is an integral
concept in dependency syntax. This concept is in-
tuitive when reasoning about the link between syn-
tax and semantics, and it has been used earlier in
semantic interpreters such as Absity (Hirst, 1983).
However, except from a few tentative experiments
(Toutanova et al, 2005), grammatical function is
not explicitly used by current automatic SRL sys-
tems, but instead emulated from constituent trees
by features like the constituent position and the
governing category. More generally, these lin-
guistic reasons have made a number of linguists
argue that dependency structures are more suit-
able for explaining the syntax-semantics interface
(Mel?c?uk, 1988; Hudson, 1984).
In this work, we provide a new evaluation of
the influence of the syntactic representation on se-
mantic role labeling in English. Contrary to previ-
ously reported results, we show that dependency-
based systems are on a par with constituent-based
systems or perform nearly as well. Furthermore,
we show that semantic role classifiers using a de-
pendency parser learn faster than their constituent-
based counterparts and therefore need less train-
ing data to achieve similar performances. Finally,
dependency-based role classifiers are more robust
to vocabulary change and outperform constituent-
based systems when using out-of-domain test sets.
2 Statistical Dependency Parsing for
English
Except for small-scale efforts, there is no depen-
dency treebank of significant size for English. Sta-
tistical dependency parsers of English must there-
fore rely on dependency structures automatically
converted from a constituent corpus such as the
Penn Treebank (Marcus et al, 1993).
Typical approaches to conversion of constituent
structures into dependencies are based on hand-
constructed head percolation rules, an idea that has
its roots in lexicalized constituent parsing (Mager-
man, 1994; Collins, 1997). The head rules cre-
ated by Yamada and Matsumoto (2003) have been
used in almost all recent work on statistical depen-
dency parsing of English (Nivre and Scholz, 2004;
McDonald et al, 2005).
Recently, Johansson and Nugues (2007) ex-
tended the head percolation strategy to incorporate
long-distance links such as wh-movement and top-
icalization, and used the full set of grammatical
function tags from Penn in addition to a number of
inferred tags (in total 57 function tags). A depen-
dency parser based on this syntax was used in the
best-performing system in the SemEval-2007 task
on Frame-semantic Structure Extraction (Baker et
al., 2007), and the conversion method (in two dif-
ferent forms) was used for the English data in the
CoNLL Shared Tasks of 2007 and 2008.
3 Automatic Semantic Role Labeling
with Constituents and Dependencies
To study the influence of syntactic representation
on SRL performance, we developed a framework
that could be easily parametrized to process either
constituent or dependency input1. This section de-
scribes its implementation. As the role-semantic
paradigm, we used FrameNet (Baker et al, 1998).
3.1 Systems
We built SRL systems based on six different
parsers. All parsers were trained on the Penn Tree-
bank, either directly for the constituent parsers or
through the LTH constituent-to-dependency con-
verter (Johansson and Nugues, 2007). Our systems
are identified as follows:
LTH. A dependency-based system using the LTH
parser (Johansson and Nugues, 2008).
Malt. A dependency-based system using
MaltParser (Nivre et al, 2007).
MST. A dependency-based system using
MSTParser (McDonald et al, 2005).
C&J. A constituent-based system using the
reranking parser (the May 2006 version) by
Charniak and Johnson (2005).
Charniak. A constituent-based system using
Charniak?s parser (Charniak, 2000).
Collins. A constituent-based system using
Collins? parser (Collins, 1997).
1Our implementation is available for download at
http://nlp.cs.lth.se/fnlabeler.
394
MaltParser is an incremental greedy classifier-
based parser based on SVMs, while the LTH parser
and MSTParser use exact edge-factored search
with a linear model trained using online margin-
based structure learning. MaltParser and MST-
Parser have achieved state-of-the-art results for a
wide range of languages in the 2006 and 2007
CoNLL Shared Tasks on dependency parsing, and
the LTH parser obtained the best result in the 2008
CoNLL Shared Task on joint syntactic and seman-
tic parsing. Charniak?s and Collins? parsers are
widely used constituent parsers for English, and
the C&J parser is the best-performing freely avail-
able constituent parser at the time of writing ac-
cording to published figures. Charniak?s parser
and the C&J parser come with a built-in part-of-
speech tagger; all other systems used the Stanford
tagger (Toutanova et al, 2003).
Following Gildea and Jurafsky (2002), the SRL
problem is traditionally divided into two subtasks:
identifying the arguments and labeling them with
semantic roles. Although state-of-the-art SRL sys-
tems use sophisticated statistical models to per-
form these two tasks jointly (e.g. Toutanova et
al., 2005, Johansson and Nugues, 2008), we im-
plemented them as two independent support vector
classifiers to be able to analyze the impact of syn-
tactic representation on each task separately. The
features used by the classifiers are traditional, al-
though the features for the dependency-based clas-
sifiers needed some adaptation. Table 1 enumer-
ates the features, which are described in more de-
tail in Appendix A. The differences in the fea-
ture sets reflect the structural differences between
constituent and dependency trees: The constituent-
only features are based on phrase tags and the
dependency-only features on grammatical func-
tions labels.
3.2 Dependency-based Argument
Identification
The argument identification step consists of find-
ing the arguments for a given predicate. For
constituent-based SRL, this problem is formulated
as selecting a subset of the constituents in a parse
tree. This is then implemented in practice as a
binary classifier that determines whether or not a
given constituent is an argument. We approached
the problem similarly in the dependency frame-
work, applying the classifier on dependency nodes
rather than constituents. In both cases, the identi-
Argument Argument
Features identification classification
TARGETLEMMA C,D C,D
FES C,D C,D
TARGETPOS C,D C,D
VOICE C,D C,D
POSITION C,D C,D
ARGWORD/POS C,D C,D
LEFTWORD/POS C,D C,D
RIGHTWORD/POS C,D C,D
PARENTWORD/POS C,D
C-SUBCAT C C
C-PATH C C
PHRASETYPE C C
GOVCAT C C
D-SUBCAT D D
D-PATH D D
CHILDDEPSET D D
PARENTHASOBJ D
RELTOPARENT D
FUNCTION D
Table 1: Classifier features. The features used by
the constituent-based and the dependency-based
systems are marked C and D, respectively.
fication step was preceded by a pruning stage that
heuristically removes parse tree nodes unlikely to
represent arguments (Xue and Palmer, 2004).
To score the performance of the argument iden-
tifier, traditional evaluation procedures treat the
identification as a bracketing problem, meaning
that the entities scored by the evaluation procedure
are labeled snippets of text; however, it is ques-
tionable whether this is the proper way to evalu-
ate a task whose purpose is to find semantic re-
lations between logical entities. We believe that
the same criticisms that have been leveled at the
PARSEVAL metric for constituent structures are
equally valid for the bracket-based evaluation of
SRL systems. The inappropriateness of the tra-
ditional metric has led to a number of alternative
metrics (Litkowski, 2004; Baker et al, 2007).
We have stuck to the traditional bracket-based
scoring metric for compatibility with previous re-
sults, but since it represents the arguments as la-
beled spans, a conversion step is needed when us-
ing dependencies. Algorithm 1 shows how the
spans are constructed from the argument depen-
dency nodes. For each argument node, the algo-
rithm computes the yield Y , the set of dependency
nodes to include in the bracketing. This set is then
partitioned into contiguous parts, which are then
converted into spans. In most cases, the yield is
just the subtree dominated by the argument node.
However, if the argument dominates the predi-
395
cate, then the branch containing the predicate is
removed. Also, FrameNet alows arguments to co-
incide with the predicate; in this case, the yield is
just the predicate node.
Algorithm 1 Span creation from argument depen-
dency nodes.
input Predicate node p, argument node a
if a does not dominate p
Y ? {n; a dominates n}
else if p = a
Y ? {p}
else
c? the child of a that dominates p
Y ? {n; a dominates n} \ {n; c dominates n}
end if
S ? partition of Y into contiguous subsets
return {(min-index s,max-index s); s ? S}
that we have been relying onthe ideas
ROOT?FRAG
VCSBJ VC CLR
PMOD
NMOD
NMOD
Figure 1: Example of a dependency tree containing
a predicate relyingwith three arguments: the ideas,
we, and on . . . that.
To illustrate Algorithm 1, consider Figure 1. In
this sentence, the predicate relying has three argu-
ments: the ideas, we, and on . . . that. The simplest
of them is we, which does not dominate its predi-
cate and which is not discontinuous. A more com-
plex case is the discontinuous argument headed by
on, where the yield {on, that} is partitioned into
two subsets that result in two separate spans. Fi-
nally, the dependency node ideas dominates the
predicate. In this case, the algorithm removes the
subtree headed by have, so the remaining yield is
{the, ideas}.
4 Experiments
We carried out a number of experiments to com-
pare the influence of the syntactic representation
on different aspects of SRL performance. We
used the FrameNet example corpus and running-
text corpus, from which we randomly sampled a
training and test set. The training set consisted
of 134,697 predicates and 271,560 arguments, and
the test set of 14,952 predicates and 30,173 argu-
ments. This does not include null-instantiated ar-
guments, which were removed from the training
and test sets.
4.1 Argument Identification
Before evaluating the full automatic argument
identification systems, we studied the effect of
the span creation from dependency nodes (Algo-
rithm 1). To do this, we measured the upper-bound
recall of argument identification using the con-
ventional span-based evaluation metric. We com-
pared the quality of pruned spans (Algorithm 1)
to unpruned spans (a baseline method that brack-
ets the full subtree). Table 2 shows the re-
sults of this experiment. The figures show that
proper span creation is essential when the tradi-
tional metrics are used: For all dependency-based
systems, the upper-bound recall increases signif-
icantly. However, the dependency-based systems
generally have lower figures for the upper-bound
recall than constituent-based ones.
System Pruned Unpruned
LTH 83.9 82.1
Malt 82.1 78.3
MST 80.4 77.1
C&J 85.3
Charniak 83.4
Collins 81.8
Table 2: Upper-bound recall for argument identifi-
cation.
Our first experiment investigated how the syn-
tactic representation influenced the performance
of the argument identification step. Table 3
shows the result of this evaluation. As can be
seen, the constituent-based systems outperform the
dependency-based systems on average. However,
the picture is not clear enough to draw any firm
conclusion about a fundamental structural differ-
ence. There are also a number of reasons to be cau-
tious: First, the dependency parsers were trained
on a treebanks that had been automatically cre-
ated from a constituent treebank, which probably
results in a slight decrease in annotation quality.
Second, dependency parsing is still a developing
field, while constituent parsing is more mature.
The best constituent parser (C&J) is a reranking
parser utilizing global features, while the depen-
dency parsers use local features only; we believe
396
that a reranker could be used to improve the de-
pendency parsers as well.
System P R F1
LTH 79.7 77.3 78.5
Malt 77.4 73.8 75.6
MST 73.9 71.9 72.9
C&J 81.4 77.3 79.2
Charniak 79.8 75.0 77.3
Collins 78.4 72.9 75.6
Table 3: Argument identification performance.
Differences between parsers using the same syn-
tactic formalism are also considerable, which sug-
gests that the attachment accuracy is probably the
most important parameter when choosing a parser
for this task.
4.2 Argument Classification
To evaluate the argument classification accuracies,
we provided the systems with gold-standard argu-
ments, which were then automatically classified.
Table 4 shows the results.
System Accuracy
LTH 89.6
Malt 88.5
MST 88.1
C&J 88.9
Charniak 88.5
Collins 88.3
Table 4: Semantic role classification accuracy.
Here, the situation is different: the best
dependency-based system make 6.3% fewer errors
than the best constituent-based one, a statistically
significant difference at the 99.9% level according
to a McNemar test. Again, there are no clear differ-
ences that can be attributed to syntactic formalism.
However, this result is positive, because it shows
clearly that SRL can be used in situations where
only dependency parsers are available.
On the other hand, it may seem paradoxical
that the rich set of grammatical functions used by
the dependency-based systems did not lead to a
clearer difference between the groups, despite the
linguistic intuition that this feature should be use-
ful for argument classification. Especially for for
the second- and third-best systems (Malt and MST
versus Charniak and Collins), the performance fig-
ures are almost identical. However, all systems
use lexical features of the argument, and given
enough training data, one may say that the gram-
matical function is implicitly encoded in these fea-
tures. This suggests that lexical features are more
important for constituent-based systems than for
dependency-based ones.
4.3 Robustness of SRL Classifiers
In this section, we test the hypothesis that the
SRL systems based on dependency syntax rely less
heavily on lexical features. We also investigate
two parameters that are influenced by lexicaliza-
tion: domain sensitivity and the amount of training
data required by classifiers.
Tests of Unlexicalized Models
To test the hypothesis about the reliance on lex-
icalization, we carried out a series of experiments
where we set aside the lexical features of the argu-
ment. Table 5 shows the results.
As expected, there is a sharp drop in perfor-
mance for all systems, but the results are very
clear: When no argument lexical features are avail-
able, the dependency-based systems have a supe-
rior performance. The difference between MST
and C&J constitutes an error reduction of 6.9% and
is statistically significant at the 99.9% level.
System Accuracy
LTH 83.0
Malt 81.9
MST 81.7
C&J 80.3
Charniak 80.0
Collins 79.8
Table 5: Accuracy for unlexicalized role classi-
fiers. Dependency-based systems make at least
6.9% fewer errors.
Training Set Size
Since the dependency-based systems rely less
on lexicalization, we can expect them to have a
steeper learning curve. To investigate this, we
trained semantic role classifiers using training sets
of varying sizes and compared the average clas-
sification accuracies of the two groups. Fig-
ure 2 shows the reduction in classification error
of the dependency-based group compared to the
constituent-based group (again, all systems were
lexicalized). For small training sets, the differ-
ences are large; the largest observed error reduc-
397
tion was 5.4% with a training set of 25,000 in-
stances. When the training set size increases, the
difference between the groups decreases. The plot
is consistent with our hypothesis that the gram-
matical function features used by the dependency-
based systems make generalization easier for sta-
tistical classifiers. We interpret the flatter learning
curves for constituent-based systems as a conse-
quence of lexicalization ? these systems need more
training data to use lexical information to capture
grammatical function information implicitly.
105
1
1.5
2
2.5
3
3.5
4
4.5
5
5.5
6
Training set size
Er
ro
r r
ed
uc
tio
n
Figure 2: Error reduction of average dependency-
based systems as a function of training set size.
Out-of-domain Test Sets
We finally conducted an evaluation of the se-
mantic role classification accuracies on an out-of-
domain test set: the FrameNet-annotated Nuclear
Threat Initiative texts from SemEval task (Baker
et al, 2007). Table 6 shows the results. This cor-
pus contained 9,039 predicates and 15,343 argu-
ments. The writing style is very different from
the FrameNet training data, and the annotated data
contain several instances of predicates and frames
unseen in the training set. We thus see that all sys-
tems suffer severely from domain sensitivity, but
we also see that the dependency-based systems are
more resilient ? the difference between MST and
C&J is statistically significant at the 97.5% level
and corresponds to an error reduction of 2%. The
experiment reconfirms previous results (Carreras
and M?rquez, 2005) that the argument classifica-
tion part of SRL systems is sensitive to domain
changes, and Pradhan et al (2008) argued that an
important reason for this is that the lexical fea-
tures are heavily domain-dependent. Our results
are consistent with this hypothesis, and suggest
that the inclusion of grammatical function features
is an effective way to mitigate this sensitivity.
System Accuracy
LTH 71.1
Malt 70.1
MST 70.1
C&J 69.5
Charniak 69.3
Collins 69.3
Table 6: Classification accuracy on the NTI texts.
Dependency-based systems make 2% fewer errors.
5 Discussion
We have described a set of experiments that in-
vestigate the relation between syntactic represen-
tation and semantic role labeling performance,
specifically focusing on a comparison between
constituent- and dependency-based SRL systems.
A first conclusion is that our dependency-based
systems perform more or less as well as the more
mature constituent-based systems: For the argu-
ment classification task, dependency-based sys-
tems are slightly better on average, while the
constituent-based systems perform slightly higher
in argument identification.
This result contrasts with previously published
comparisons, which used less accurate depen-
dency parsers, and shows that semantic analyz-
ers can be implemented for languages where con-
stituent parsers are not available. While traditional
constituent-based SRL techniques have so far been
applied to languages characterized by simple mor-
phology and rigid word order, such as English and
Chinese, we think that dependency-based SRL can
be particularly useful for languages with a free
word order.
For dependency-based systems, the conversion
from parse tree nodes to argument spans, which
are needed to use the traditional span-based evalu-
ation method, is less trivial than in the constituent
case. To make a comparison feasible, we imple-
mented an algorithm for span creation from ar-
gument nodes. However, the fundamental prob-
lem lies in evaluation ? the field needs to design
new evaluation procedures that use some sort of
link-based scoring method. The evaluation met-
rics used in the SemEval task on Frame-semantic
398
Structure Extraction and the 2008 CoNLL Shared
Task are examples of steps in the right direction.
Our second main result is that for argument
classification, dependency-based systems rely less
heavily on lexicalization, and we suggest that this
is because they use features based on grammatical
function labels. These features make the learning
curve steeper when training the classifier, and im-
prove robustness to domain changes.
A Features Used by the Classifiers
The following subsections itemize the features
used by the systems. All examples are given with
respect to the sentence she gave the horse an apple.
The constituent and dependency trees are shown
in Figure 3. For this sentence, the predicate is
gave, which has the FrameNet frame GIVING. It
has three arguments: she, which has the DONOR
role; the horse, the RECIPIENT; and an apple, the
THEME.
NP NP NP
VP
S
gave the horse an appleshe
gave the horse an appleshe
SBJ
ROOT?S
NMODNMOD
IOBJ
OBJ
Figure 3: Examples of parse trees.
A.1 Common Features
The following features are used by both the
constituent-based and the dependency-based se-
mantic analyzers. Head-finding rules (Johansson
and Nugues, 2007) were applied when heads of
constituents were needed.
TARGETLEMMA. The lemma of the target word
itself, e.g. give.
FES. For a given frame, the set of available frame
elements listed in FrameNet. For instance, for
give in the GIVING frame, we have 12 frame
elements: DONOR, RECIPIENT, THEME, . . .
TARGETPOS. Part-of-speech tag for the target
word.
VOICE. For verbs, this feature is Active or Pas-
sive. For other types of words, it is not de-
fined.
POSITION. Position of the head word of the argu-
ment with respect to the target word: Before,
After, or On.
ARGWORD and ARGPOS. Lexical form and
part-of-speech tag of the head word of the ar-
gument.
LEFTWORD and LEFTPOS. Form and part-of-
speech tag of the leftmost dependent of the
argument head.
RIGHTWORD and RIGHTPOS. Form and part-
of-speech tag of the rightmost dependent of
the argument head.
PARENTWORD and PARENTPOS. Form and
part-of-speech tag of the parent node of the
target.
A.2 Features Used by the Constituent-based
Analyzer Only
C-SUBCAT. Subcategorization frame: corre-
sponds to the phrase-structure rule used to ex-
pand the phrase around the target. For give in
the example, this feature is VP?VB NP NP.
C-PATH. A string representation of the path
through the constituent tree from the target
word to the argument constituent. For in-
stance, the path from gave to she is ?VP-?S-
?NP.
PHRASETYPE. Phrase type of the argument con-
stituent, e.g. NP for she.
GOVCAT. Governing category: this feature is ei-
ther S or VP, and is found by starting at the ar-
gument constituent and moving upwards until
either a VP or a sentence node (S, SINV, or
SQ) is found. For instance, for she, this fea-
ture is S, while for the horse, it is VP. This
can be thought of as a very primitive way of
distinguishing subjects and objects.
A.3 Features Used by the Dependency-based
Analyzer Only
D-SUBCAT. Subcategorization frame: the
grammatical functions of the dependents
concatenated. For gave, this feature is
SBJ+IOBJ+OBJ.
D-PATH. A string representation of the path
through the dependency tree from the target
node to the argument node. Moving upwards
through verb chains is not counted in this path
string. In the example, the path from gave to
she is ?SBJ.
399
CHILDDEPSET. The set of grammatical func-
tions of the direct dependents of the target
node. For instance, for give, this is { SBJ,
IOBJ, OBJ }.
PARENTHASOBJ. Binary feature that is set to
true if the parent of the target has an object.
RELTOPARENT. Dependency relation between
the target node and its parent.
FUNCTION. The grammatical function of the ar-
gument node. For direct dependents of the
target, this feature is identical to the D-PATH.
References
Baker, Collin F., Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceedings
of COLING/ACL-1998.
Baker, Collin, Michael Ellsworth, and Katrin Erk. 2007. Se-
mEval task 19: Frame semantic structure extraction. In
Proceedings of SemEval-2007.
Buchholz, Sabine and Erwin Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proceedings
of the CoNLL-X.
Carreras, Xavier and Llu?s M?rquez. 2005. Introduction to
the CoNLL-2005 shared task: Semantic role labeling. In
Proceedings of CoNLL-2005.
Charniak, Eugene and Mark Johnson. 2005. Coarse-to-fine
n-best parsing and MaxEnt discriminative reranking. In
Proceedings of ACL.
Charniak, Eugene. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL-2000.
Collins, Michael. 1997. Three generative, lexicalised models
for statistical parsing. In Proceedings of ACL/EACL-1997.
Gildea, Daniel and Julia Hockenmaier. 2003. Identifying
semantic roles using combinatory categorial grammar. In
Proceedings of EMNLP-2003.
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguistics,
28(3):245?288.
Gildea, Daniel and Martha Palmer. 2002. The necessity of
syntactic parsing for predicate argument recognition. In
Proceedings of the ACL-2002.
Hacioglu, Kadri. 2004. Semantic role labeling using depen-
dency trees. In Proceedings of COLING-2004.
Hirst, Graeme. 1983. A foundation for semantic interpreta-
tion. In Proceedings of the ACL-1983.
Hudson, Richard. 1984. Word Grammar. Blackwell.
Johansson, Richard and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In Pro-
ceedings of NODALIDA 2007.
Johansson, Richard and Pierre Nugues. 2008. Dependency-
based syntactic?semantic analysis with PropBank and
NomBank. In Proceedings of CoNLL?2008.
Litkowski, Ken. 2004. Senseval-3 task: Automatic labeling
of semantic roles. In Proceedings of Senseval-3.
Magerman, David M. 1994. Natural language parsing as sta-
tistical pattern recognition. Ph.D. thesis, Stanford Univer-
sity.
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: the Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
McDonald, Ryan, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency parsers.
In Proceedings of ACL-2005.
Mel?c?uk, Igor A. 1988. Dependency Syntax: Theory and
Practice. State University Press of New York.
Nivre, Joakim and Mario Scholz. 2004. Deterministic depen-
dency parsing of English text. In Proceedings of COLING-
2004.
Nivre, Joakim, Johan Hall, Jens Nilsson, Atanas Chanev,
G?ls?en Eryig?it, Sandra K?bler, Svetoslav Marinov, and Er-
win Marsi. 2007. MaltParser: A language-independent
system for data-driven dependency parsing. Natural Lan-
guage Engineering, 13(2):95?135.
Pradhan, Sameer, Wayne Ward, Kadri Hacioglu, James Mar-
tin, and Daniel Jurafsky. 2005. Semantic role labeling
using different syntactic views. In Proceedings of ACL-
2005.
Pradhan, Sameer, Wayne Ward, and James H. Martin. 2008.
Towards robust semantic role labeling. Computational
Linguistics, 34(2):289?310.
Punyakanok, Vasin, Dan Roth, and Wen-tau Yih. 2008. The
importance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics, 34(2):257?287.
Swanson, Reid and Andrew S. Gordon. 2006. A comparison
of alternative parse tree paths for labeling semantic roles.
In Proceedings of COLING/ACL-2006.
Toutanova, Kristina, Dan Klein, Christopher Manning, and
Yoram Singer. 2003. Feature-rich part-of-speech tagging
with a cyclic dependency network. In Proceedings of HLT-
NAACL-2003.
Toutanova, Kristina, Aria Haghighi, and Christopher D. Man-
ning. 2005. Joint learning improves semantic role label-
ing. In Proceedings of ACL-2005.
?abokrtsk?, Zdene?k, Petr Sgall, and Sa?o D?eroski. 2002.
A machine learning approach to automatic functor assign-
ment in the Prague dependency treebank. In Proceedings
of LREC-2002.
Xue, Nianwen and Martha Palmer. 2004. Calibrating features
for semantic role labeling. In Proceedings of EMNLP-
2004.
Yamada, Hiroyasu and Yuji Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In Pro-
ceedings of IWPT-2003.
400
Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1134?1138,
Prague, June 2007. c?2007 Association for Computational Linguistics
Incremental Dependency Parsing Using Online Learning
Richard Johansson and Pierre Nugues
Department of Computer Science, Lund University, Sweden
{richard, pierre}@cs.lth.se
Abstract
We describe an incremental parser that
was trained to minimize cost over sen-
tences rather than over individual parsing ac-
tions. This is an attempt to use the advan-
tages of the two top-scoring systems in the
CoNLL-X shared task.
In the evaluation, we present the perfor-
mance of the parser in the Multilingual task,
as well as an evaluation of the contribution
of bidirectional parsing and beam search to
the parsing performance.
1 Introduction
The two best-performing systems in the CoNLL-X
shared task (Buchholz and Marsi, 2006) can be clas-
sified along two lines depending on the method they
used to train the parsing models. Although the
parsers are quite different, their creators could re-
port near-tie scores. The approach of the top sys-
tem (McDonald et al, 2006) was to fit the model
to minimize cost over sentences, while the second-
best system (Nivre et al, 2006) trained the model to
maximize performance over individual decisions in
an incremental algorithm. This difference is a nat-
ural consequence of their respective parsing strate-
gies: CKY-style maximization of link score and in-
cremental parsing.
In this paper, we describe an attempt to unify the
two approaches: an incremental parsing strategy that
is trained to maximize performance over sentences
rather than over individual parsing actions.
2 Parsing Method
2.1 Nivre?s Parser
We used Nivre?s algorithm (Nivre et al, 2006),
which is a variant of the shift?reduce parser. Like
the regular shift?reduce, it uses a stack S and a list
of input words W , and builds the parse tree incre-
mentally using a set of parsing actions (see Table 1).
It can be shown that Nivre?s parser creates projec-
tive and acyclic graphs and that every projective de-
pendency graph can be produced by a sequence of
parser actions. In addition, the worst-case number of
actions is linear with respect to the number of words
in the sentence.
2.2 Handling Nonprojective Parse Trees
While the parsing algorithm produces projective
trees only, nonprojective arcs can be handled using
a preprocessing step before training the model and a
postprocessing step after parsing the sentences.
The projectivization algorithm (Nivre and Nils-
son, 2005) iteratively moves each nonprojective arc
upward in the tree until the whole tree is projective.
To be able to recover the nonprojective arcs after
parsing, the projectivization operation replaces the
labels of the arcs it modifies with traces indicating
which links should be moved and where attach to at-
tach them (the ?Head+Path? encoding). The model
is trained with these new labels that makes it pos-
sible to carry out the reverse operation and produce
nonprojective structures.
2.3 Bidirectional Parsing
Shift-reduce is by construction a directional parser,
typically applied from left to right. To make bet-
ter use of the training set, we applied the algorithm
in both directions as Johansson and Nugues (2006)
and Sagae and Lavie (2006) for all languages except
Catalan and Hungarian. This, we believe, also has
the advantage of making the parser less sensitive to
whether the language is head-initial or head-final.
We trained the model on projectivized graphs
from left to right and right to left and used a vot-
ing strategy based on link scores. Each link was as-
signed a score (simply by using the score of the la
or ra actions for each link). To resolve the conflicts
1134
Table 1: Nivre?s parser transitions where W is the initial word list; I , the current input word list; A, the
graph of dependencies; and S, the stack. (n?, n) denotes a dependency relations between n? and n, where n?
is the head and n the dependent.
Actions Parser actions Conditions
Initialize ?nil, W, ??
Terminate ?S, nil, A?
Left-arc ?n|S, n?|I,A? ? ?S, n?|I, A ? {(n?, n)}? ??n??(n??, n) ? A
Right-arc ?n|S, n?|I,A? ? ?n?|n|S, I,A ? {(n, n?)}? ??n??(n??, n?) ? A
Reduce ?n|S, I,A? ? ?S, I, A? ?n?(n?, n) ? A
Shift ?S, n|I, A? ? ?n|S, I,A?
between the two parses in a manner that makes the
tree projective, single-head, rooted, and cycle-free,
we applied the Eisner algorithm (Eisner, 1996).
2.4 Beam Search
As in our previous parser (Johansson and Nugues,
2006), we used a beam-search extension to Nivre?s
original algorithm (which is greedy in its original
formulation). Each parsing action was assigned a
score, and the beam search allows us to find a bet-
ter overall score of the sequence of actions. In
this work, we used a beam width of 8 for Catalan,
Chinese, Czech, and English and 16 for the other
languages.
3 Learning Method
3.1 Overview
We model the parsing problem for a sentence x as
finding the parse y? = arg maxy F (x, y) that max-
imizes a discriminant function F . In this work, we
consider linear discriminants of the following form:
F (x, y) = w ??(x, y)
where ?(x, y) is a numeric feature representation
of the pair (x, y) and w a vector of feature weights.
Learning F in this case comes down to assigning
good weights in the vector w.
Machine learning research for similar prob-
lems have generally used margin-based formula-
tions. These include global batch methods such
as SVMstruct (Tsochantaridis et al, 2005) as well
as online methods such as the Online Passive-
Aggressive Algorithm (OPA) (Crammer et al,
2006). Although the batch methods are formulated
very elegantly, they do not seem to scale well to
the large training sets prevalent in NLP contexts ?
we briefly considered using SVMstruct but train-
ing was too time-consuming. The online methods
on the other hand, although less theoretically ap-
pealing, can handle realistically sized data sets and
have successfully been applied in dependency pars-
ing (McDonald et al, 2006). Because of this, we
used the OPA algorithm throughout this work.
3.2 Implementation
In the online learning framework, the weight vector
is constructed incrementally. At each step, it com-
putes an update to the weight vector based on the
current example. The resulting weight vector is fre-
quently overfit to the last examples. One way to
reduce overfitting is to use the average of all suc-
cessive weight vectors as the result of the training
(Freund and Schapire, 1999).
Algorithm 1 shows the algorithm. It uses an
?aggressiveness? parameter C to reduce overfitting,
analogous to the C parameter in SVMs. The algo-
rithm also needs a cost function ?, which describes
how much a parse tree deviates from the gold stan-
dard. In this work, we defined ? as the sum of link
costs, where the link cost was 0 for a correct depen-
dency link with a correct label, 0.5 for a correct link
with an incorrect label, and 1 for an incorrect link.
The number of iterations was 5 for all languages.
For a sentence x and a parse tree y, we defined
the feature representation by finding the sequence
??S1, I1? , a1? , ??S2, I2? , a2? . . . of states and their
corresponding actions, and creating a feature vector
for each state/action pair. The discriminant function
was thus written
?(x, y) ?w =
?
i
?(?Si, Ii? , ai) ?w
where ? is a feature function that assigns a feature
1135
Algorithm 1 The Online PA Algorithm
input Training set T = {(xt, yt)}Tt=1
Number of iterations N
Regularization parameter C
Cost function ?
Initialize w to zeros
repeat N times
for (xt, yt) in T
let y?t = arg maxy F (xt, y) +
?
?(yt, y)
let ?t = min
(
C, F (xt,y?t)?F (xt,yt)+
?
?(yt,y?t)
??(x,yt)??(x,y?t)?2
)
w ? w + ?t(?(x, yt)??(x, y?t))
return waverage
vector to a state ?Si, Ii? and the action ai taken in
that state. Table 2 shows the feature sets used in
? for all languages. In principle, a kernel could
also be used, but that would degrade performance
severely. Instead, we formed a new vector by com-
bining features pairwisely ? this is equivalent to us-
ing a quadratic kernel.
Since the history-based feature set used in the
parsing algorithm makes it impossible to use inde-
pendence to factorize the scoring function, an ex-
act search to find the best-scoring action sequence
(arg maxy in Algorithm 1) is not possible. How-
ever, the beam search allows us to find a reasonable
approximation.
4 Results
Table 3 shows the results of our system in the Mul-
tilingual task.
4.1 Compared to SVM-based Local Classifiers
We compared the performance of the parser with
a parser based on local SVM classifiers (Johansson
and Nugues, 2006). Table 4 shows the performance
of both parsers on the Basque test set. We see that
what is gained by using a global method such as
OPA is lost by sacrificing the excellent classifica-
tion performance of the SVM. Possibly, better per-
formance could be achieved by using a large-margin
batch method such as SVMstruct.
Table 2: Feature sets.
ar ca cs el en eu hu it tr zh
Fine POS top ? ? ? ? ? ? ? ? ? ?
Fine POS top-1 ? ? ? ? ? ? ?
Fine POS list ? ? ? ? ? ? ? ? ? ?
Fine POS list-1 ? ? ? ? ? ? ? ? ? ?
Fine POS list+1 ? ? ? ? ? ? ? ? ? ?
Fine POS list+2 ? ? ? ? ? ? ? ? ? ?
Fine POS list+3 ? ? ? ? ? ?
POS top ? ? ? ? ? ? ? ? ? ?
POS top-1 ?
POS list ? ? ? ? ? ? ? ? ? ?
POS list-1 ? ? ? ? ? ?
POS list+1 ? ? ? ? ? ? ? ? ? ?
POS list+2 ? ? ? ? ? ? ? ?
POS list+3 ? ? ? ? ? ? ? ?
Features top ? ? ? ? ? ? ? ?
Features list ? ? ? ? ? ? ? ?
Features list-1 ? ? ? ? ?
Features list+1 ? ? ? ? ? ? ?
Features list+2 ? ? ? ? ?
Word top ? ? ? ? ? ? ? ? ?
Word top-1 ? ?
Word list ? ? ? ? ? ? ? ? ? ?
Word list-1 ? ? ? ? ? ?
Word list+1 ? ? ? ?
Lemma top ? ? ? ? ? ?
Lemma list ? ? ? ? ?
Lemma list-1 ? ?
Relation top ? ?
Relation top left ? ? ? ? ?
Relation top right ? ? ? ? ?
Relation list right ?
Word top left ?
Word top right ?
Word list left ?
POS top left ? ?
POS top right ? ? ?
POS list left ? ? ? ? ? ? ? ?
Features top right ?
Features first left ? ?
Table 3: Summary of results.
Languages Unlabeled Labeled
Arabic 80.91 71.76
Basque 80.41 75.08
Catalan 88.34 83.33
Chinese 81.30 76.30
Czech 77.39 70.98
English 81.43 80.29
Greek 79.58 72.77
Hungarian 75.53 71.31
Italian 81.55 77.55
Turkish 84.80 78.46
Average result 81.12 75.78
Table 4: Accuracy by learning method.
Learning Method Accuracy
OPA 75.08
SVM 75.53
1136
4.2 Beam Width
To investigate the influence of the beam width on the
performance, we measured the accuracy of a left-to-
right parser on a development set for Basque (15%
of the training data) as a function of the width. Ta-
ble 5 shows the result. We see clearly that widening
the beam considerably improves the figures, espe-
cially in the lower ranges.
Table 5: Accuracy by beam width.
Width Accuracy
2 72.01
4 74.18
6 75.05
8 75.30
12 75.49
4.3 Direction
We also investigated the contribution of the bidirec-
tional parsing. Table 6 shows the result of this exper-
iment on the Basque development set (the same 15%
as in 4.2). The beam width was 2 in this experiment.
Table 6: Accuracy by parsing direction.
Direction Accuracy
Left to right 72.01
Right to left 71.02
Bidirectional 74.48
Time did not allow a full-scale experiment, but
for all languages except Catalan and Hungarian, the
bidirectional parsing method outperformed the uni-
directional methods when trained on a 20,000-word
subset. However, the gain of using bidirectional
parsing may be more obvious when the treebank is
small. For all languages except Czech, left-to-right
outperformed right-to-left parsing.
5 Discussion
The paper describes an incremental parser that we
trained to minimize the cost over sentences, rather
than over parsing actions as is usually done. It
was trained using the Online Passive-Aggressive
method, a cost-sensitive online margin-based learn-
ing method, and shows reasonable performance and
received above-average scores for most languages.
The performance of the parser (relative the other
teams) was best for Basque and Turkish, which were
two of the smallest treebanks. Since we found that
the optimal number of iterations was 5 for Basque
(the smallest treebank), we used this number for all
languages since we did not have time to investigate
this parameter for the other languages. This may
have had a detrimental effect for some languages.
We think that some of the figures might be squeezed
slightly higher by optimizing learning parameters
and feature sets.
This work shows that it was possible to combine
approaches used by Nivre?s and McDonald?s parsers
in a single system. While the parser is outperformed
by a system based on local classifiers, we still hope
that the parsing and training combination described
here opens new ways in parser design and eventually
leads to the improvement of parsing performance.
Acknowledgements
This work was made possible because of the anno-
tated corpora that were kindly provided to us: (Hajic?
et al, 2004; Aduriz et al, 2003; Mart? et al, 2007;
Chen et al, 2003; B?hmov? et al, 2003; Marcus et
al., 1993; Johansson and Nugues, 2007; Prokopidis
et al, 2005; Csendes et al, 2005; Montemagni et al,
2003; Oflazer et al, 2003)
References
A. Abeill?, editor. 2003. Treebanks: Building and Using
Parsed Corpora. Kluwer.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. Diaz de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency treebank.
In Proc. of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT), pages 201?204.
A. B?hmov?, J. Hajic?, E. Hajic?ov?, and B. Hladk?. 2003.
The PDT: a 3-level annotation scenario. In Abeill?
(Abeill?, 2003), chapter 7, pages 103?127.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In CoNLL-X.
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica treebank: Design criteria,
representational issues and implementation. In Abeill?
(Abeill?, 2003), chapter 13, pages 231?248.
1137
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Schwartz,
and Y. Singer. 2006. Online passive-aggressive algo-
rithms. JMLR, 2006(7):551?585.
D. Csendes, J. Csirik, T. Gyim?thy, and A. Kocsor. 2005.
The Szeged Treebank. Springer.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proceedings of
ICCL.
Y. Freund and R. E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37(3):277?296.
J. Hajic?, O. Smr?, P. Zem?nek, J. ?naidauf, and E. Be?ka.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools,
pages 110?117.
R. Johansson and P. Nugues. 2006. Investigating multi-
lingual dependency parsing. In CoNLL-X.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
M. A. Mart?, M. Taul?, L. M?rquez, and M. Bertran.
2007. CESS-ECE: A multilingual and multilevel
annotated corpus. Available for download from:
http://www.lsi.upc.edu/?mbertran/cess-ece/.
R. McDonald, K. Lerman, and F. Pereira. 2006. Multi-
lingual dependency parsing with a two-stage discrimi-
native parser. In CoNLL-X.
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari,
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli,
M. Massetani, R. Raffaelli, R. Basili, M. T. Pazienza,
D. Saracino, F. Zanzotto, N. Nana, F. Pianesi, and
R. Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeill? (Abeill?, 2003), chap-
ter 11, pages 189?210.
J. Nivre and J. Nilsson. 2005. Pseudo-projective depen-
dency parsing. In Proceedings of ACL-05.
J. Nivre, J. Hall, J. Nilsson, G. Eryig?it, and S. Marinov.
2006. Labeled pseudo-projective dependency parsing
with support vector machines. In CoNLL-X.
K. Oflazer, B. Say, D. Zeynep Hakkani-T?r, and G. T?r.
2003. Building a Turkish treebank. In Abeill?
(Abeill?, 2003), chapter 15, pages 261?277.
P. Prokopidis, E. Desypri, M. Koutsombogera, H. Papa-
georgiou, and S. Piperidis. 2005. Theoretical and
practical issues in the construction of a Greek depen-
dency treebank. In Proc. of the 4th Workshop on Tree-
banks and Linguistic Theories (TLT), pages 149?160.
K. Sagae and A. Lavie. 2006. Parser combination by
reparsing. In Proceedings of the HLT-NAACL.
I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Al-
tun. 2005. Large margin methods for structured and
interdependent output variables. JMLR, 6:1453?1484.
1138
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 69?78,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Dependency-based Semantic Role Labeling of PropBank
Richard Johansson and Pierre Nugues
Lund University, Sweden
{richard, pierre}@cs.lth.se
Abstract
We present a PropBank semantic role label-
ing system for English that is integrated with
a dependency parser. To tackle the problem
of joint syntactic?semantic analysis, the sys-
tem relies on a syntactic and a semantic sub-
component. The syntactic model is a projec-
tive parser using pseudo-projective transfor-
mations, and the semantic model uses global
inference mechanisms on top of a pipeline of
classifiers. The complete syntactic?semantic
output is selected from a candidate pool gen-
erated by the subsystems.
We evaluate the system on the CoNLL-
2005 test sets using segment-based and
dependency-based metrics. Using the
segment-based CoNLL-2005 metric, our
system achieves a near state-of-the-art F1
figure of 77.97 on the WSJ+Brown test set,
or 78.84 if punctuation is treated consistently.
Using a dependency-based metric, the F1
figure of our system is 84.29 on the test
set from CoNLL-2008. Our system is the
first dependency-based semantic role labeler
for PropBank that rivals constituent-based
systems in terms of performance.
1 Introduction
Automatic semantic role labeling (SRL), the task
of determining who does what to whom, is a use-
ful intermediate step in NLP applications perform-
ing semantic analysis. It has obvious applications
for template-filling tasks such as information extrac-
tion and question answering (Surdeanu et al, 2003;
Moschitti et al, 2003). It has also been used in
prototypes of NLP systems that carry out complex
reasoning, such as entailment recognition systems
(Haghighi et al, 2005; Hickl et al, 2006). In addi-
tion, role-semantic features have recently been used
to extend vector-space representations in automatic
document categorization (Persson et al, 2008).
The NLP community has recently devoted much
attention to developing accurate and robust methods
for performing role-semantic analysis automatically,
and a number of multi-system evaluations have been
carried out (Litkowski, 2004; Carreras andM?rquez,
2005; Baker et al, 2007; Surdeanu et al, 2008).
Following the seminal work of Gildea and Juraf-
sky (2002), there have been many extensions in ma-
chine learning models, feature engineering (Xue and
Palmer, 2004), and inference procedures (Toutanova
et al, 2005; Surdeanu et al, 2007; Punyakanok et
al., 2008).
With very few exceptions (e.g. Collobert and
Weston, 2007), published SRL methods have used
some sort of syntactic structure as input (Gildea and
Palmer, 2002; Punyakanok et al, 2008). Most sys-
tems for automatic role-semantic analysis have used
constituent syntax as in the Penn Treebank (Marcus
et al, 1993), although there has also been much re-
search on the use of shallow syntax (Carreras and
M?rquez, 2004) in SRL.
In comparison, dependency syntax has received
relatively little attention for the SRL task, despite
the fact that dependency structures offer a more
transparent encoding of predicate?argument rela-
tions. Furthermore, the few systems based on de-
pendencies that have been presented have generally
performed much worse than their constituent-based
69
counterparts. For instance, Pradhan et al (2005) re-
ported that a system using a rule-based dependency
parser achieved much inferior results compared to a
system using a state-of-the-art statistical constituent
parser: The F-measure on WSJ section 23 dropped
from 78.8 to 47.2, or from 83.7 to 61.7 when using
a head-based evaluation. In a similar vein, Swanson
and Gordon (2006) reported that parse tree path fea-
tures extracted from a rule-based dependency parser
are much less reliable than those from a modern con-
stituent parser.
In contrast, we recently carried out a de-
tailed comparison (Johansson and Nugues, 2008b)
between constituent-based and dependency-based
SRL systems for FrameNet, in which the results of
the two types of systems where almost equivalent
when using modern statistical dependency parsers.
We suggested that the previous lack of progress in
dependency-based SRL was due to low parsing ac-
curacy. The experiments showed that the grammat-
ical function information available in dependency
representations results in a steeper learning curve
when training semantic role classifiers, and it also
seemed that the dependency-based role classifiers
were more resilient to lexical problems caused by
change of domain.
The recent CoNLL-2008 Shared Task (Surdeanu
et al, 2008) was an attempt to show that SRL can be
accurately carried out using only dependency syn-
tax. However, these results are not easy to compare
to previously published results since the task defini-
tions and evaluation metrics were different.
This paper compares the best-performing sys-
tem in the CoNLL-2008 Shared Task (Johans-
son and Nugues, 2008a) with previously published
constituent-based SRL systems. The system carries
out joint dependency-syntactic and semantic anal-
ysis. We first describe its implementation in Sec-
tion 2, and then compare the system with the best
system in the CoNLL-2005 Shared Task in Section
3. Since the outputs of the two systems are differ-
ent, we carry out two types of evaluations: first by
using the traditional segment-based metric used in
the CoNLL-2005 Shared Task, and then by using
the dependency-based metric from the CoNLL-2008
Shared Task. Both evaluations require a transforma-
tion of the output of one system: For the segment-
based metric, we have to convert the dependency-
based output to segments; and for the dependency-
based metric, a head-finding procedure is needed to
select heads in segments. For the first time for a sys-
tem using only dependency syntax, we report results
for PropBank-based semantic role labeling of En-
glish that are close to the state of the art, and for
some measures even superior.
2 Syntactic?Semantic Dependency
Analysis
The training corpus that we used is the dependency-
annotated Penn Treebank from the 2008 CoNLL
Shared Task on joint syntactic?semantic analysis
(Surdeanu et al, 2008). Figure 1 shows a sentence
annotated in this framework. The CoNLL task in-
volved semantic analysis of predicates from Prop-
Bank (for verbs, such as plan) and NomBank (for
nouns, such as investment); in this paper, we report
the performance on PropBank predicates only since
we compare our system with previously published
PropBank-based SRL systems.
Chrysler plans new investment in Latin America
plan.01
LOC PMODNMODNMODOBJ
A0
investment.01
A1A0 A2
SBJ
ROOT
Figure 1: An example sentence annotated with syntactic
and semantic dependency structures.
We model the problem of constructing a syntac-
tic and a semantic graph as a task to be solved
jointly. Intuitively, syntax and semantics are highly
interdependent and semantic interpretation should
help syntactic disambiguation, and joint syntactic?
semantic analysis has a long tradition in deep-
linguistic formalisms. Using a discriminative model,
we thus formulate the problem of finding a syntactic
tree y?syn and a semantic graph y?sem for a sentence
x as maximizing a function Fjoint that scores the
complete syntactic?semantic structure:
?y?syn, y?sem? = arg max
ysyn,ysem
Fjoint(x, ysyn, ysem)
The dependencies in the feature representation used
to compute Fjoint determine the tractability of the
70
rerankingPred?argLinguisticconstraintsSensedisambig. Argumentidentification Argumentlabeling
dependencySyntacticparsing
Global semantic model
Syntactic?semantic
reranking
Semantic pipeline
Figure 2: The architecture of the syntactic?semantic analyzer.
search procedure needed to perform the maximiza-
tion. To be able to use complex syntactic features
such as paths when predicting semantic structures,
exact search is clearly intractable. This is true even
with simpler feature representations ? the problem
is a special case of multi-headed dependency analy-
sis, which is NP-hard even if the number of heads is
bounded (Chickering et al, 1994).
This means that we must resort to a simplifica-
tion such as an incremental method or a rerank-
ing approach. We chose the latter option and thus
created syntactic and semantic submodels. The
joint syntactic?semantic prediction is selected from
a small list of candidates generated by the respective
subsystems. Figure 2 shows the architecture.
2.1 Syntactic Submodel
We model the process of syntactic parsing of a
sentence x as finding the parse tree y?syn =
argmaxysyn Fsyn(x, ysyn) that maximizes a scoring
function Fsyn. The learning problem consists of fit-
ting this function so that the cost of the predictions is
as low as possible according to a cost function ?syn.
In this work, we consider linear scoring functions of
the following form:
Fsyn(x, ysyn) = ?syn(x, ysyn) ?w
where ?syn(x, ysyn) is a numeric feature represen-
tation of the pair (x, ysyn) andw a vector of feature
weights. We defined the syntactic cost ?syn as the
sum of link costs, where the link cost was 0 for a
correct dependency link with a correct label, 0.5 for
a correct link with an incorrect label, and 1 for an
incorrect link.
A widely used discriminative framework for fit-
ting the weight vector is the max-margin model
(Taskar et al, 2003), which is a generalization of
the well-known support vector machines to gen-
eral cost-based prediction problems. Since the large
number of training examples and features in our
case make an exact solution of the max-margin op-
timization problem impractical, we used the on-
line passive?aggressive algorithm (Crammer et al,
2006), which approximates the optimization process
in two ways:
? The weight vector w is updated incrementally,
one example at a time.
? For each example, only the most violated con-
straint is considered.
The algorithm is a margin-based variant of the per-
ceptron (preliminary experiments show that it out-
performs the ordinary perceptron on this task). Al-
gorithm 1 shows pseudocode for the algorithm.
Algorithm 1 The Online PA Algorithm
input Training set T = {(xt, yt)}Tt=1
Number of iterations N
Regularization parameter C
Initialize w to zeros
repeat N times
for (xt, yt) in T
let y?t = argmaxy F (xt, y) + ?(yt, y)
let ?t = min
(
C, F (xt,y?t)?F (xt,yt)+?(yt,y?t)??(x,yt)??(x,y?t)?2
)
w ? w + ?t(?(x, yt)??(x, y?t))
return waverage
We used a C value of 0.01, and the number of
iterations was 6.
2.1.1 Features and Search
The feature function?syn is a factored represen-
tation, meaning that we compute the score of the
complete parse tree by summing the scores of its
parts, referred to as factors:
?(x, y) ?w =
?
f?y
?(x, f) ?w
71
We used a second-order factorization (McDonald
and Pereira, 2006; Carreras, 2007), meaning that
the factors are subtrees consisting of four links: the
governor?dependent link, its sibling link, and the
leftmost and rightmost dependent links of the depen-
dent.
This factorization allows us to express useful fea-
tures, but also forces us to adopt the expensive
search procedure by Carreras (2007), which ex-
tends Eisner?s span-based dynamic programming al-
gorithm (1996) to allow second-order feature depen-
dencies. This algorithm has a time complexity of
O(n4), where n is the number of words in the sen-
tence. The search was constrained to disallow mul-
tiple root links.
To evaluate the argmax in Algorithm 1 during
training, we need to handle the cost function ?syn in
addition to the factor scores. Since the cost function
?syn is based on the cost of single links, this can
easily be integrated into the factor-based search.
2.1.2 Handling Nonprojective Links
Although only 0.4% of the links in the training
set are nonprojective, 7.6% of the sentences con-
tain at least one nonprojective link. Many of these
links represent long-range dependencies ? such as
wh-movement ? that are valuable for semantic pro-
cessing. Nonprojectivity cannot be handled by
span-based dynamic programming algorithms. For
parsers that consider features of single links only, the
Chu-Liu/Edmonds algorithm can be used instead.
However, this algorithm cannot be generalized to the
second-order setting ?McDonald and Pereira (2006)
proved that this problem is NP-hard, and described
an approximate greedy search algorithm.
To simplify implementation, we instead opted for
the pseudo-projective approach (Nivre and Nilsson,
2005), in which nonprojective links are lifted up-
wards in the tree to achieve projectivity, and spe-
cial trace labels are used to enable recovery of the
nonprojective links at parse time. The use of trace
labels in the pseudo-projective transformation leads
to a proliferation of edge label types: from 69 to 234
in the training set, many of which occur only once.
Since the running time of our parser depends on the
number of labels, we used only the 20 most frequent
trace labels.
2.2 Semantic Submodel
Our semantic model consists of three parts:
? A SRL classifier pipeline that generates a list of
candidate predicate?argument structures.
? A constraint system that filters the candidate
list to enforce linguistic restrictions on the
global configuration of arguments.
? A global reranker that assigns scores to
predicate?argument structures in the filtered
candidate list.
Rather than training the models on gold-standard
syntactic input, we created an automatically parsed
training set by 5-fold cross-validation. Training
on automatic syntax makes the semantic classifiers
more resilient to parsing errors, in particular adjunct
labeling errors.
2.2.1 SRL Pipeline
The SRL pipeline consists of classifiers for pred-
icate disambiguation, argument identification, and
argument labeling. For the predicate disambigua-
tion classifiers, we trained one subclassifier for each
lemma. All classifiers in the pipeline were L2-
regularized linear logistic regression classifiers, im-
plemented using the efficient LIBLINEAR package
(Lin et al, 2008). For multiclass problems, we used
the one-vs-all binarization method, which makes it
easy to prevent outputs not allowed by the PropBank
frame.
Since our classifiers were logistic, their output
values could be meaningfully interpreted as prob-
abilities. This allowed us to combine the scores
from subclassifiers into a score for the complete
predicate?argument structure. To generate the can-
didate lists used by the global SRL models, we ap-
plied beam search based on these scores using a
beam width of 4.
The argument identification classifier was pre-
ceded by a pruning step similar to the constituent-
based pruning by Xue and Palmer (2004).
The features used by the classifiers are listed in
Table 1, and are described in Appendix A. We se-
lected the feature sets by greedy forward subset se-
lection.
72
Feature PredDis ArgId ArgLab
PREDWORD ?
PREDLEMMA ?
PREDPARENTWORD/POS ?
CHILDDEPSET ? ? ?
CHILDWORDSET ?
CHILDWORDDEPSET ?
CHILDPOSSET ?
CHILDPOSDEPSET ?
DEPSUBCAT ?
PREDRELTOPARENT ?
PREDPARENTWORD/POS ?
PREDLEMMASENSE ? ?
VOICE ? ?
POSITION ? ?
ARGWORD/POS ? ?
LEFTWORD/POS ?
RIGHTWORD/POS ? ?
LEFTSIBLINGWORD/POS ?
PREDPOS ? ?
RELPATH ? ?
VERBCHAINHASSUBJ ? ?
CONTROLLERHASOBJ ?
PREDRELTOPARENT ? ?
FUNCTION ?
Table 1: Classifier features in predicate disambiguation
(PredDis), argument identification (ArgId), and argument
labeling (ArgLab).
2.2.2 Linguistically Motivated Global
Constraints
The following three global constraints were used
to filter the candidates generated by the pipeline.
CORE ARGUMENT CONSISTENCY. Core argu-
ment labels must not appear more than once.
DISCONTINUITY CONSISTENCY. If there is a la-
bel C-X, it must be preceded by a label X.
REFERENCE CONSISTENCY. If there is a label R-
X and the label is inside an attributive relative
clause, it must be preceded by a label X.
2.2.3 Predicate?Argument Reranker
Toutanova et al (2005) have showed that a global
model that scores the complete predicate?argument
structure can lead to substantial performance gains.
We therefore created a global SRL classifier using
the following global features in addition to the fea-
tures from the pipeline:
CORE ARGUMENT LABEL SEQUENCE. The com-
plete sequence of core argument labels. The
sequence also includes the predicate and voice,
for instance A0+break.01/Active+A1.
MISSING CORE ARGUMENT LABELS. The set of
core argument labels declared in the PropBank
frame that are not present in the predicate?
argument structure.
Similarly to the syntactic submodel, we trained
the global SRL model using the online passive?
aggressive algorithm. The cost function ? was
defined as the number of incorrect links in the
predicate?argument structure. The number of iter-
ations was 20 and the regularization parameter C
was 0.01. Interestingly, we noted that the global
SRL model outperformed the pipeline even when
no global features were added. This shows that the
global learning model can correct label bias prob-
lems introduced by the pipeline architecture.
2.3 Syntactic?Semantic Reranking
As described previously, we carried out reranking
on the candidate set of complete syntactic?semantic
structures. To do this, we used the top 16 trees from
the syntactic module and applied a linear model:
Fjoint(x, ysyn, ysem) = ?joint(x, ysyn, ysem) ?w
Our baseline joint feature representation?joint con-
tained only three features: the log probability of the
syntactic tree and the log probability of the seman-
tic structure according to the pipeline and the global
model, respectively. This model was trained on the
complete training set using cross-validation. The
probabilities were obtained using the multinomial
logistic function (?softmax?).
We carried out an initial experiment with a more
complex joint feature representation, but failed to
improve over the baseline. Time prevented us from
exploring this direction conclusively.
3 Comparisons with Previous Results
To compare our results with previously published
results in SRL, we carried out an experiment com-
paring our system to the top system (Punyakanok et
al., 2008) in the CoNLL-2005 Shared Task. How-
ever, comparison is nontrivial since the output of
the CoNLL-2005 systems was a set of labeled seg-
ments, while the CoNLL-2008 systems (including
ours) produced labeled semantic dependency links.
To have a fair comparison of our link-based sys-
tem against previous segment-based systems, we
73
carried out a two-way evaluation: In the first eval-
uation, the dependency-based output was converted
to segments and evaluated using the segment scorer
from CoNLL-2005, and in the second evaluation, we
applied a head-finding procedure to the output of a
segment-based system and scored the result using
the link-based CoNLL-2008 scorer.
It can be discussed which of the two metrics is
most correlated with application performance. The
traditional metric used in the CoNLL-2005 task
treats SRL as a bracketing problem, meaning that
the entities scored by the evaluation procedure are
labeled snippets of text; however, it is questionable
whether this is the proper way to evaluate a task
whose purpose is to find semantic relations between
logical entities. We believe that the same criticisms
that have been leveled at the PARSEVAL metric
for constituent structures are equally valid for the
bracket-based evaluation of SRL systems. The in-
appropriateness of the traditional metric has led to
a number of alternative metrics (Litkowski, 2004;
Baker et al, 2007; Surdeanu et al, 2008).
3.1 Segment-based Evaluation
To be able to score the output of a dependency-based
SRL system using the segment scorer, a conversion
step is needed. Algorithm 2 shows how a set of seg-
ments is constructed from an argument dependency
node. For each argument node, the algorithm com-
putes the yield Y of the argument node, i.e. the set of
dependency nodes to include in the bracketing. This
set is then partitioned into contiguous parts, from
which the segments are computed. In most cases,
the yield is just the subtree dominated by the argu-
ment node. However, if the argument dominates the
predicate, then the branch containing the predicate
is removed.
Table 2 shows the performance figures of our
system on the WSJ and Brown corpora: preci-
sion, recall, F1-measure, and complete proposition
accuracy (PP). These figures are compared to the
best-performing system in the CoNLL-2005 Shared
Task (Punyakanok et al, 2008), referred to as Pun-
yakanok in the table, and the best result currently
published (Surdeanu et al, 2007), referred to as Sur-
deanu. To validate the sanity of the segment cre-
ation algorithm, the table also shows the result of ap-
plying segment creation to gold-standard syntactic?
Algorithm 2 Segment creation from an argument
dependency node.
input Predicate node p, argument node a
if a does not dominate p
Y ? {n : a dominates n}
else
c? the child of a that dominates p
Y ? {n : a dominates n} \ {n : c dominates n}
end if
S ? partition of Y into contiguous subsets
return {(min-index s,max-index s) : s ? S}
WSJ P R F1 PP
Our system 82.22 77.72 79.90 57.24
Punyakanok 82.28 76.78 79.44 53.79
Surdeanu 87.47 74.67 80.56 51.66
Gold standard 97.38 96.77 97.08 93.20
Brown P R F1 PP
Our system 68.79 61.87 65.15 32.34
Punyakanok 73.38 62.93 67.75 32.34
Surdeanu 81.75 61.32 70.08 34.33
Gold standard 97.22 96.55 96.89 92.79
WSJ+Brown P R F1 PP
Our system 80.50 75.59 77.97 53.94
Punyakanok 81.18 74.92 77.92 50.95
Surdeanu 86.78 72.88 79.22 49.36
Gold standard 97.36 96.75 97.05 93.15
Table 2: Evaluation with unnormalized segments.
semantic trees. We see that the two conversion pro-
cedures involved (constituent-to-dependency con-
version by the CoNLL-2008 Shared Task organizers,
and our dependency-to-segment conversion) work
satisfactorily although the process is not completely
lossless.
During inspection of the output, we noted that
many errors arise from inconsistent punctuation at-
tachment in PropBank/Treebank. We therefore nor-
malized the segments to exclude punctuation at the
beginning or end of a segment. The results of this
evaluation is shown in Table 3. This table does not
include the Surdeanu system since we did not have
74
access to its output.
WSJ P R F1 PP
Our system 82.95 78.40 80.61 58.65
Punyakanok 82.67 77.14 79.81 54.55
Gold standard 97.85 97.24 97.54 94.34
Brown P R F1 PP
Our system 70.84 63.71 67.09 36.94
Punyakanok 74.29 63.71 68.60 34.08
Gold standard 97.46 96.78 97.12 93.41
WSJ+Brown P R F1 PP
Our system 81.39 76.44 78.84 55.77
Punyakanok 81.63 75.34 78.36 51.84
Gold standard 97.80 97.18 97.48 94.22
Table 3: Evaluation with normalized segments.
The results on the WSJ test set clearly show
that dependency-based SRL systems can rival
constituent-based systems in terms of performance
? it clearly outperforms the Punyakanok system, and
has a higher recall and complete proposition accu-
racy than the Surdeanu system. We interpret the high
recall as a result of the dependency syntactic repre-
sentation, which makes the parse tree paths simpler
and thus the arguments easier to find.
For the Brown test set, on the other hand, the
dependency-based system suffers from a low pre-
cision compared to the constituent-based systems.
Our error analysis indicates that the domain change
caused problems with prepositional attachment for
the dependency parser ? it is well-known that prepo-
sitional attachment is a highly lexicalized problem,
and thus sensitive to domain changes. We believe
that the reason why the constituent-based systems
are more robust in this respect is that they utilize a
combination strategy, using inputs from two differ-
ent full constituent parsers, a clause bracketer, and
a chunker. However, caution is needed when draw-
ing conclusions from results on the Brown test set,
which is only 7,585 words, compared to the 59,100
words in the WSJ test set.
3.2 Dependency-based Evaluation
It has previously been noted (Pradhan et al, 2005)
that a segment-based evaluation may be unfavorable
to a dependency-based system, and that an evalua-
tion that scores argument heads may be more indica-
tive of its true performance. We thus carried out an
evaluation using the evaluation script of the CoNLL-
2008 Shared Task. In this evaluation method, an ar-
gument is counted as correctly identified if its head
and label are correct. Note that this is not equivalent
to the segment-based metric: In a perfectly identi-
fied segment, we may still pick out the wrong head,
and if the head is correct, we may infer an incorrect
segment. The evaluation script also scores predicate
disambiguation performance; we did not include this
score since the 2005 systems did not output predi-
cate sense identifiers.
Since CoNLL-2005-style segments have no in-
ternal tree structure, it is nontrivial to extract a
head. It is conceivable that the output of the parsers
used by the Punyakanok system could be used to
extract heads, but this is not recommendable be-
cause the Punyakanok system is an ensemble sys-
tem and a segment does not always exactly match
a constituent in a parse tree. Furthermore, the
CoNLL-2008 constituent-to-dependency conversion
method uses a richer structure than just the raw con-
stituents: empty categories, grammatical functions,
and named entities. To recreate this additional infor-
mation, we would have to apply automatic systems
and end up with unreliable results.
Instead, we thus chose to find an upper bound
on the performance of the segment-based system.
We applied a simple head-finding procedure (Algo-
rithm 3) to find a set of head nodes for each seg-
ment. Since the CoNLL-2005 output does not in-
clude dependency information, the algorithm uses
gold-standard dependencies and intersects segments
with the gold-standard segments. This will give us
an upper bound, since if the segment contains the
correct head, it will always be counted as correct.
The algorithm looks for dependencies leaving the
segment, and if multiple outgoing edges are found,
a couple of simple heuristics are applied. We found
that the best performance is achieved when selecting
only one outgoing edge. ?Small clauses,? which are
split into an object and a predicative complement in
the dependency framework, are the only cases where
we select two heads.
Table 4 shows the results of the dependency-
based evaluation. In the table, the output of the
75
Algorithm 3 Finding head nodes in a segment.
input Argument segment a
if a overlaps with a segment in the gold standard
a? intersection of a and gold standard
F ? {n : governor of n outside a}
if |F | = 1
return F
remove punctuation nodes from F
if |F | = 1
return F
if F = {n1, n2, . . .} where n1 is an object and n2 is
the predicative part of a small clause
return {n1, n2}
if F contains a node n that is a subject or an object
return {n}
else
return {n}, where n is the leftmost node in F
dependency-based system is compared to the seman-
tic dependency links automatically extracted from
the segments of the Punyakanok system.
WSJ P R F1 PP
Our system 88.46 83.55 85.93 61.97
Punyakanok 87.25 81.59 84.32 58.17
Brown P R F1 PP
Our system 77.67 69.63 73.43 41.32
Punyakanok 80.29 68.59 73.98 37.28
WSJ+Brown P R F1 PP
Our system 87.07 81.68 84.29 59.22
Punyakanok 86.94 80.21 83.45 55.39
Table 4: Dependency-based evaluation.
In this evaluation, the dependency-based system
has a higher F1-measure than the Punyakanok sys-
tem on both test sets. This suggests that the main ad-
vantage of using a dependency-based semantic role
labeler is that it is better at finding the heads of
semantic arguments, rather than finding segments.
The results are also interesting in comparison to
the multi-view system described by Pradhan et al
(2005), which has a reported head F1 measure of
85.2 on the WSJ test set. The figure is not exactly
compatible with ours, however, since that system
used a different head extraction mechanism.
4 Conclusion
We have described a dependency-based system1 for
semantic role labeling of English in the PropBank
framework. Our evaluations show that the perfor-
mance of our system is close to the state of the
art. This holds regardless of whether a segment-
based or a dependency-based metric is used. In-
terestingly, our system has a complete proposition
accuracy that surpasses other systems by nearly 3
percentage points. Our system is the first semantic
role labeler based only on syntactic dependency that
achieves a competitive performance.
Evaluation and comparison is a difficult issue
since the natural output of a dependency-based sys-
tem is a set of semantic links rather than segments,
as is normally the case for traditional systems. To
handle this situation fairly to both types of systems,
we carried out a two-way evaluation: conversion of
dependencies to segments for the dependency-based
system, and head-finding heuristics for segment-
based systems. However, the latter is difficult since
no structure is available inside segments, and we
had to resort to computing upper-bound results using
gold-standard input; despite this, the dependency-
based system clearly outperformed the upper bound
of the performance of the segment-based system.
The comparison can also be slightly misleading
since the dependency-based system was optimized
for the dependency metric and previous systems for
the segment metric.
Our evaluations suggest that the dependency-
based SRL system is biased to finding argument
heads, rather than argument text snippets, and this
is of course perfectly logical. Whether this is an ad-
vantage or a drawback will depend on the applica-
tion ? for instance, a template-filling system might
need complete segments, while an SRL-based vector
space representation for text categorization, or a rea-
soning application, might prefer using heads only.
In the future, we would like to further investigate
whether syntactic and semantic analysis could be in-
tegrated more tightly. In this work, we used a sim-
1Our system is freely available for download at
http://nlp.cs.lth.se/lth_srl.
76
plistic loose coupling by means of reranking a small
set of complete structures. The same criticisms that
are often leveled at reranking-based models clearly
apply here too: The set of tentative analyses from the
submodules is too small, and the correct analysis is
often pruned too early. An example of a method to
mitigate this shortcoming is the forest reranking by
Huang (2008), in which complex features are evalu-
ated as early as possible.
A Classifier Features
Features Used in Predicate Disambiguation
PREDWORD, PREDLEMMA. The lexical form and
lemma of the predicate.
PREDPARENTWORD and PREDPARENTPOS.
Form and part-of-speech tag of the parent node
of the predicate.
CHILDDEPSET, CHILDWORDSET, CHILD-
WORDDEPSET, CHILDPOSSET, CHILD-
POSDEPSET. These features represent the set
of dependents of the predicate using combina-
tions of dependency labels, words, and parts of
speech.
DEPSUBCAT. Subcategorization frame: the con-
catenation of the dependency labels of the pred-
icate dependents.
PREDRELTOPARENT. Dependency relation be-
tween the predicate and its parent.
Features Used in Argument Identification and
Labeling
PREDLEMMASENSE. The lemma and sense num-
ber of the predicate, e.g. give.01.
VOICE. For verbs, this feature is Active or Passive.
For nouns, it is not defined.
POSITION. Position of the argument with respect
to the predicate: Before, After, or On.
ARGWORD and ARGPOS. Lexical form and part-
of-speech tag of the argument node.
LEFTWORD, LEFTPOS, RIGHTWORD, RIGHT-
POS. Form/part-of-speech tag of the left-
most/rightmost dependent of the argument.
LEFTSIBLINGWORD, LEFTSIBLINGPOS.
Form/part-of-speech tag of the left sibling of
the argument.
PREDPOS. Part-of-speech tag of the predicate.
RELPATH. A representation of the complex gram-
matical relation between the predicate and the
argument. It consists of the sequence of de-
pendency relation labels and link directions in
the path between predicate and argument, e.g.
IM?OPRD?OBJ?.
VERBCHAINHASSUBJ. Binary feature that is set
to true if the predicate verb chain has a subject.
The purpose of this feature is to resolve verb
coordination ambiguity as in Figure 3.
CONTROLLERHASOBJ. Binary feature that is true
if the link between the predicate verb chain and
its parent is OPRD, and the parent has an ob-
ject. This feature is meant to resolve control
ambiguity as in Figure 4.
FUNCTION. The grammatical function of the argu-
ment node. For direct dependents of the predi-
cate, this is identical to the RELPATH.
I
SBJ
eat drinkyouand
COORD SBJCONJ
ROOT
SBJ COORD
ROOT
drinkandeatI
CONJ
Figure 3: Coordination ambiguity: The subject I is in an
ambiguous position with respect to drink.
I to
IMSBJ
want sleephim
OBJOPRD
ROOT
IM
sleepI
SBJ
want
ROOT
to
OPRD
Figure 4: Subject/object control ambiguity: I is in an am-
biguous position with respect to sleep.
77
References
Collin Baker, Michael Ellsworth, and Katrin Erk. 2007.
SemEval task 19: Frame semantic structure extraction.
In Proceedings of SemEval-2007.
Xavier Carreras and Llu?s M?rquez. 2004. Introduction
to the CoNLL-2004 shared task: Semantic role label-
ing. In Proceedings of CoNLL-2004.
Xavier Carreras and Llu?s M?rquez. 2005. Introduction
to the CoNLL-2005 shared task: Semantic role label-
ing. In Proceedings of CoNLL-2005.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceedings of
CoNLL-2007.
David M. Chickering, Dan Geiger, and David Hecker-
man. 1994. Learning Bayesian networks: The com-
bination of knowledge and statistical data. Technical
Report MSR-TR-94-09, Microsoft Research.
Ronan Collobert and Jason Weston. 2007. Fast semantic
extraction using a novel neural network architecture.
In Proceedings of ACL-2007.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Schwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. JMLR, 2006(7):551?585.
Jason M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proceed-
ings of ICCL.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Daniel Gildea and Martha Palmer. 2002. The necessity
of syntactic parsing for predicate argument recogni-
tion. In Proceedings of the ACL-2002.
Aria Haghighi, Andrew Y. Ng, and Christopher D. Man-
ning. 2005. Robust textual inference via graph match-
ing. In Proceedings of EMNLP-2005.
Andrew Hickl, Jeremy Bensley, John Williams, Kirk
Roberts, Bryan Rink, and Ying Shi. 2006. Recogniz-
ing textual entailment with LCC?s GROUNDHOG sys-
tems. In Proceedings of the Second PASCAL Recog-
nizing Textual Entailment Challenge.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-2008.
Richard Johansson and Pierre Nugues. 2008a.
Dependency-based syntactic?semantic analysis with
PropBank and NomBank. In Proceedings of the
Shared Task Session of CoNLL-2008.
Richard Johansson and Pierre Nugues. 2008b. The effect
of syntactic representation on semantic role labeling.
In Proceedings of COLING-2008.
Chih-Jen Lin, Ruby C. Weng, and S. Sathiya Keerthi.
2008. Trust region Newton method for large-scale lo-
gistic regression. JMLR, 2008(9):627?650.
Ken Litkowski. 2004. Senseval-3 task: Automatic label-
ing of semantic roles. In Proceedings of Senseval-3.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of EACL-2006.
Alessandro Moschitti, Paul Mora?rescu, and Sanda
Harabagiu. 2003. Open domain information extrac-
tion via automatic semantic labeling. In Proceedings
of FLAIRS.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proceedings of ACL-2005.
Jacob Persson, Richard Johansson, and Pierre Nugues.
2008. Text categorization using predicate?argument
structures. Submitted.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin, and Daniel Jurafsky. 2005. Semantic role la-
beling using different syntactic views. In Proceedings
of ACL-2005.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2):257?287.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In Proceedings of
ACL-2003.
Mihai Surdeanu, Llu?s M?rquez, Xavier Carreras, and
Pere R. Comas. 2007. Combination strategies for se-
mantic role labeling. Journal of Artificial Intelligence
Research, 29:105?151.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu?s M?rquez, and Joakim Nivre. 2008. The
CoNLL?2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
CoNLL?2008.
Reid Swanson and Andrew S. Gordon. 2006. A compari-
son of alternative parse tree paths for labeling semantic
roles. In Proceedings of COLING/ACL-2006.
Ben Taskar, Carlos Guestrin, and Daphne Koller. 2003.
Max-margin Markov networks. In Proceedings of
NIPS-2003.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic role
labeling. In Proceedings of ACL-2005.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings of
EMNLP-2004.
78
A Machine Learning Approach to Extract Temporal Information from
Texts in Swedish and Generate Animated 3D Scenes
Anders Berglund Richard Johansson Pierre Nugues
Department of Computer Science, LTH
Lund University
SE-221 00 Lund, Sweden
d98ab@efd.lth.se, {richard, pierre}@cs.lth.se
Abstract
Carsim is a program that automatically
converts narratives into 3D scenes. Carsim
considers authentic texts describing road
accidents, generally collected from web
sites of Swedish newspapers or transcribed
from hand-written accounts by victims of
accidents. One of the program?s key fea-
tures is that it animates the generated scene
to visualize events.
To create a consistent animation, Carsim
extracts the participants mentioned in a
text and identifies what they do. In this
paper, we focus on the extraction of tem-
poral relations between actions. We first
describe how we detect time expressions
and events. We then present a machine
learning technique to order the sequence
of events identified in the narratives. We
finally report the results we obtained.
1 Extraction of Temporal Information
and Scene Visualization
Carsim is a program that generates 3D scenes from
narratives describing road accidents (Johansson et
al., 2005; Dupuy et al, 2001). It considers au-
thentic texts, generally collected from web sites
of Swedish newspapers or transcribed from hand-
written accounts by victims of accidents.
One of Carsim?s key features is that it animates
the generated scene to visualize events described
in the narrative. The text below, a newspaper arti-
cle with its translation into English, illustrates the
goals and challenges of it. We bracketed the enti-
ties, time expressions, and events and we anno-
tated them with identifiers, denoted respectively
oi, tj , and ek:
En {bussolycka}e1 i s?dra Afghanistan
kr?vdee2 {p? torsdagen}t1 {20
d?dsoffer}o1 . Ytterligare {39
personer}o2 skadadese3 i olyckane4.
Busseno3 {var p? v?g}e5 fr?n Kanda-
har mot huvudstaden Kabul n?r deno4
under en omk?rninge6 k?rdee7
av v?gbanano5 och voltadee8,
meddeladee9 general Salim Khan,
bitr?dande polischef i Kandahar.
TT-AFP & Dagens Nyheter, July 8,
2004
{20 persons}o1 diede2 in a {bus
accident}e1 in southern Afghanistan
{on Thursday}t1. In addition, {39
persons}o2 {were injured}e3 in the
accidente4.
The buso3 {was on its way}e5 from
Kandahar to the capital Kabul when
ito4 {drove off}e7 the roado5 while
overtakinge6 and {flipped over}e8,
saide9 General Salim Khan, assistant
head of police in Kandahar.
The text above, our translation.
To create a consistent animation, the program
needs to extract and understand who the partici-
pants are and what they do. In the case of the ac-
cident above, it has to:
1. Detect the involved physical entities o3, o4,
and o5.
2. Understand that the pronoun o4 refers to o3.
3. Detect the events e6, e7, and e8.
385
4. Link the participants to the events using se-
mantic roles or grammatical functions and in-
fer the unmentioned vehicle that is overtaken.
5. Understand that the order of the events is e6-
e7-e8.
6. Detect the time expression t1 to anchor tem-
porally the animation.
In this paper, we describe how we address tasks
3, 5, and 6 within the Carsim program, i.e., how
we detect, interpret, and order events and how we
process time expressions.
2 Previous Work
Research on the representation of time, events,
and temporal relations dates back the beginning
of logic. It resulted in an impressive number of
formulations and models. In a review of contem-
porary theories and an attempt to unify them, Ben-
nett and Galton (2004) classified the most influen-
tial formalisms along three lines. A first approach
is to consider events as transitions between states
as in STRIPS (Fikes and Nilsson, 1971). A sec-
ond one is to map events on temporal intervals
and to define relations between pairs of intervals.
Allen?s (1984) 13 temporal relations are a widely
accepted example of this. A third approach is to
reify events, to quantify them existentially, and
to connect them to other objects using predicates
based on action verbs and their modifiers (David-
son, 1967). The sentence John saw Mary in Lon-
don on Tuesday is then translated into the logical
form: ?[Saw(, j,m)?Place(, l)?T ime(, t)].
Description of relations between time, events,
and verb tenses has also attracted a considerable
interest, especially in English. Modern work on
temporal event analysis probably started with Re-
ichenbach (1947), who proposed the distinction
between the point of speech, point of reference,
and point of event in utterances. This separation
allows for a systematic description of tenses and
proved to be very powerful.
Many authors proposed general principles to
extract automatically temporal relations between
events. A basic observation is that the tempo-
ral order of events is related to their narrative or-
der. Dowty (1986) investigated it and formulated a
Temporal Discourse Interpretation Principle to in-
terpret the advance of narrative time in a sequence
of sentences. Lascarides and Asher (1993) de-
scribed a complex logical framework to deal with
events in simple past and pluperfect sentences.
Hitzeman et al (1995) proposed a constraint-
based approach taking into account tense, aspect,
temporal adverbials, and rhetorical structure to an-
alyze a discourse.
Recently, groups have used machine learn-
ing techniques to determine temporal relations.
They trained automatically classifiers on hand-
annotated corpora. Mani et al (2003) achieved
the best results so far by using decision trees to
order partially events of successive clauses in En-
glish texts. Boguraev and Ando (2005) is another
example of it for English and Li et al (2004) for
Chinese.
3 Annotating Texts with Temporal
Information
Several schemes have been proposed to anno-
tate temporal information in texts, see Setzer and
Gaizauskas (2002), inter alia. Many of them were
incompatible or incomplete and in an effort to rec-
oncile and unify the field, Ingria and Pustejovsky
(2002) introduced the XML-based Time markup
language (TimeML).
TimeML is a specification language whose
goal is to capture most aspects of temporal rela-
tions between events in discourses. It is based
on Allen?s (1984) relations and a variation of
Vendler?s (1967) classification of verbs. It de-
fines XML elements to annotate time expressions,
events, and ?signals?. The SIGNAL tag marks sec-
tions of text indicating a temporal relation. It
includes function words such as later and not.
TimeML also features elements to connect entities
using different types of links, most notably tem-
poral links, TLINKs, that describe the temporal re-
lation holding between events or between an event
and a time.
4 A System to Convert Narratives of
Road Accidents into 3D Scenes
4.1 Carsim
Carsim is a text-to-scene converter. From a nar-
rative, it creates a complete and unambiguous 3D
geometric description, which it renders visually.
Carsim considers authentic texts describing road
accidents, generally collected from web sites of
Swedish newspapers or transcribed from hand-
written accounts by victims of accidents. One of
the program?s key features is that it animates the
generated scene to visualize events.
386
The Carsim architecture is divided into two
parts that communicate using a frame representa-
tion of the text. Carsim?s first part is a linguistic
module that extracts information from the report
and fills the frame slots. The second part is a vir-
tual scene generator that takes the structured rep-
resentation as input, creates the visual entities, and
animates them.
4.2 Knowledge Representation in Carsim
The Carsim language processing module reduces
the text content to a frame representation ? a tem-
plate ? that outlines what happened and enables a
conversion to a symbolic scene. It contains:
? Objects. They correspond to the physical en-
tities mentioned in the text. They also include
abstract symbols that show in the scene. Each
object has a type, that is selected from a pre-
defined, finite set. An object?s semantics is
a separate geometric entity, where its shape
(and possibly its movement) is determined by
its type.
? Events. They correspond intuitively to an ac-
tivity that goes on during a period in time
and here to the possible object behaviors. We
represent events as entities with a type taken
from a predefined set, where an event?s se-
mantics will be a proposition paired with a
point or interval in time during which the
proposition is true.
? Relations and Quantities. They describe spe-
cific features of objects and events and how
they are related to each other. The most obvi-
ous examples of such information are spatial
information about objects and temporal in-
formation about events. Other meaningful re-
lations and quantities include physical prop-
erties such as velocity, color, and shape.
5 Time and Event Processing
We designed and implemented a generic com-
ponent to extract temporal information from the
texts. It sits inside the natural language part of
Carsim and proceeds in two steps. The first step
uses a pipeline of finite-state machines and phrase-
structure rules that identifies time expressions, sig-
nals, and events. This step also generates a feature
vector for each element it identifies. Using the
vectors, the second step determines the temporal
relations between the extracted events and orders
them in time. The result is a text annotated using
the TimeML scheme.
We use a set of decision trees and a machine
learning approach to find the relations between
events. As input to the second step, the decision
trees take sequences of events extracted by the
first step and decide the temporal relation, possi-
bly none, between pairs of them. To run the learn-
ing algorithm, we manually annotated a small set
of texts on which we trained the trees.
5.1 Processing Structure
We use phrase-structure rules and finite state ma-
chines to mark up events and time expressions. In
addition to the identification of expressions, we of-
ten need to interpret them, for instance to com-
pute the absolute time an expression refers to. We
therefore augmented the rules with procedural at-
tachments.
We wrote a parser to control the processing flow
where the rules, possibly recursive, apply regular
expressions, call procedures, and create TimeML
entities.
5.2 Detection of Time Expressions
We detect and interpret time expressions with a
two-level structure. The first level processes in-
dividual tokens using a dictionary and regular ex-
pressions. The second level uses the results from
the token level to compute the meaning of multi-
word expressions.
Token-Level Rules. In Swedish, time expres-
sions such as en tisdagseftermiddag ?a Tuesday
afternoon? use nominal compounds. To decode
them, we automatically generate a comprehensive
dictionary with mappings from strings onto com-
pound time expressions. We decode other types
of expressions such as 2005-01-14 using regular
expressions
Multiword-Level Rules. We developed a
grammar to interpret the meaning of multiword
time expressions. It includes instructions on how
to combine the values of individual tokens for ex-
pressions such as {vid lunchtid}t1 {en tisdagefter-
middag}t2 ?{at noon}t1 {a Tuesday afternoon}t2?.
The most common case consists in merging the to-
kens? attributes to form a more specific expression.
However, relative time expressions such as i tors-
dags ?last Tuesday? are more complex. Our gram-
mar handles the most frequent ones, mainly those
387
that need the publishing date for their interpreta-
tion.
5.3 Detection of Signals
We detect signals using a lexicon and na?ve string
matching. We annotate each signal with a sense
where the possible values are: negation, before, af-
ter, later, when, and continuing. TimeML only de-
fines one attribute for the SIGNAL tag, an identifier,
and encodes the sense as an attribute of the LINKs
that refer to it. We found it more appropriate to
store the sense directly in the SIGNAL element, and
so we extended it with a second attribute.
We use the sense information in decision trees
as a feature to determine the order of events. Our
strategy based on string matching results in a lim-
ited overdetection. However, it does not break the
rest of the process.
5.4 Detection of Events
We detect the TimeML events using a part-of-
speech tagger and phrase-structure rules. We con-
sider that all verbs and verb groups are events. We
also included some nouns or compounds, which
are directly relevant to Carsim?s application do-
main, such as bilolycka ?car accident? or krock
?collision?. We detect these nouns through a set
of six morphemes.
TimeML annotates events with three features:
aspect, tense, and ?class?, where the class corre-
sponds to the type of the event. The TimeML spec-
ifications define seven classes. We kept only the
two most frequent ones: states and occurrences.
We determine the features using procedures at-
tached to each grammatical construct we extract.
The grammatical features aspect and tense are
straightforward and a direct output of the phrase-
structure rules. To infer the TimeML class, we use
heuristics such as these ones: predicative clauses
(copulas) are generally states and verbs in preterit
are generally occurrences.
The domain, reports of car accidents, makes
this approach viable. The texts describe sequences
of real events. They are generally simple, to the
point, and void of speculations and hypothetical
scenarios. This makes the task of feature identifi-
cation simpler than it is in more general cases.
In addition to the TimeML features, we extract
the grammatical properties of events. Our hypoth-
esis is that specific sequences of grammatical con-
structs are related to the temporal order of the de-
scribed events. The grammatical properties con-
sist of the part of speech, noun (NOUN) or verb
(VB). Verbs can be finite (FIN) or infinitive (INF).
They can be reduced to a single word or part of a
group (GR). They can be a copula (COP), a modal
(MOD), or a lexical verb. We combine these prop-
erties into eight categories that we use in the fea-
ture vectors of the decision trees (see ...EventStruc-
ture in Sect. 6.2).
6 Event Ordering
TimeML defines three different types of links:
subordinate (SLINK), temporal (TLINK), and aspec-
tual (ALINK). Aspectual links connect two event in-
stances, one being aspectual and the other the ar-
gument. As its significance was minor in the visu-
alization of car accidents, we set aside this type of
link.
Subordinate links generally connect signals to
events, for instance to mark polarity by linking a
not to its main verb. We identify these links simul-
taneously with the event detection. We augmented
the phrase-structure rules to handle subordination
cases at the same time they annotate an event. We
restricted the cases to modality and polarity and
we set aside the other ones.
6.1 Generating Temporal Links
To order the events in time and create the tempo-
ral links, we use a set of decision trees. We apply
each tree to sequences of events where it decides
the order between two of the events in each se-
quence. If e1, ..., en are the events in the sequence
they appear in the text, the trees correspond to the
following functions:
fdt1(ei, ei+1) ? trel(ei, ei+1)
fdt2(ei, ei+1, ei+2) ? trel(ei, ei+1)
fdt3(ei, ei+1, ei+2) ? trel(ei+1, ei+2)
fdt4(ei, ei+1, ei+2) ? trel(ei, ei+2)
fdt5(ei, ei+1, ei+2, ei+3) ? trel(ei, ei+3)
The possible output values of the trees are: si-
multaneous, after, before, is_included, includes,
and none. These values correspond to the relations
described by Setzer and Gaizauskas (2001).
The first decision tree should capture more gen-
eral relations between two adjacent events with-
out the need of a context. Decision trees dt2 and
dt3 extend the context by one event to the left re-
spectively one event to the right. They should cap-
ture more specific phenomena. However, they are
not always applicable as we never apply a decision
388
tree when there is a time expression between any
of the events involved. In effect, time expressions
?reanchor? the narrative temporally, and we no-
ticed that the decision trees performed very poorly
across time expressions.
We complemented the decision trees with a
small set of domain-independent heuristic rules
that encode common-sense knowledge. We as-
sume that events in the present tense occur after
events in the past tense and that all mentions of
events such as olycka ?accident? refer to the same
event. In addition, the Carsim event interpreter
recognizes some semantically motivated identity
relations.
6.2 Feature Vectors
The decision trees use a set of features correspond-
ing to certain attributes of the considered events,
temporal signals between them, and some other
parameters such as the number of tokens separat-
ing the pair of events to be linked. We list below
the features of fdt1 together with their values. The
first event in the pair is denoted by a mainEvent pre-
fix and the second one by relatedEvent:
? mainEventTense: none, past, present, future,
NOT_DETERMINED.
? mainEventAspect: progressive, perfective, per-
fective_progressive, none, NOT_DETERMINED.
? mainEventStructure: NOUN, VB_GR_COP_INF,
VB_GR_COP_FIN, VB_GR_MOD_INF,
VB_GR_MOD_FIN, VB_GR, VB_INF, VB_FIN,
UNKNOWN.
? relatedEventTense: (as mainEventTense)
? relatedEventAspect: (as mainEventAspect)
? relatedEventStructure: (as mainEventStructure)
? temporalSignalInbetween: none, before, after,
later, when, continuing, several.
? tokenDistance: 1, 2 to 3, 4 to 6, 7 to 10, greater
than 10.
? sentenceDistance: 0, 1, 2, 3, 4, greater than 4.
? punctuationSignDistance: 0, 1, 2, 3, 4, 5, greater
than 5.
The four other decision trees consider more
events but use similar features. The values for the
...Distance features are of course greater.
6.3 Temporal Loops
The process described above results in an overgen-
eration of temporal links. As some of them may be
conflicting, a post-processing module reorganizes
them and discards the temporal loops.
The initial step of the loop resolution assigns
each link with a score. This score is created by the
decision trees and is derived from the C4.5 metrics
(Quinlan, 1993). It reflects the accuracy of the leaf
as well as the overall accuracy of the decision tree
in question. The score for links generated from
heuristics is rule dependent.
The loop resolution algorithm begins with an
empty set of orderings. It adds the partial order-
ings to the set if their inclusion doesn?t introduce
a temporal conflict. It first adds the links with the
highest scores, and thus, in each temporal loop, the
ordering with the lowest score is discarded.
7 Experimental Setup and Evaluation
As far as we know, there is no available time-
annotated corpus in Swedish, which makes the
evaluation more difficult. As development and
test sets, we collected approximately 300 reports
of road accidents from various Swedish newspa-
pers. Each report is annotated with its publishing
date. Analyzing the reports is complex because
of their variability in style and length. Their size
ranges from a couple of sentences to more than a
page. The amount of details is overwhelming in
some reports, while in others most of the informa-
tion is implicit. The complexity of the accidents
described ranges from simple accidents with only
one vehicle to multiple collisions with several par-
ticipating vehicles and complex movements.
We manually annotated a subset of our corpus
consisting of 25 texts, 476 events and 1,162 tem-
poral links. We built the trees automatically from
this set using the C4.5 program (Quinlan, 1993).
Our training set is relatively small and the num-
ber of features we use relatively large for the set
size. This can produce a training overfit. However,
C4.5, to some extent, makes provision for this and
prunes the decision trees.
We evaluated three aspects of the temporal in-
formation extraction modules: the detection and
interpretation of time expressions, the detection
and interpretation of events, and the quality of the
final ordering. We report here the detection of
events and the final ordering.
389
Feature Ncorrect Nerroneous Correct
Tense 179 1 99.4%
Aspect 161 19 89.4%
Class 150 30 83.3%
Table 1: Feature detection for 180 events.
7.1 Event Detection
We evaluated the performance of the event detec-
tion on a test corpus of 40 previously unseen texts.
It should be noted that we used a simplified defi-
nition of what an event is, and that the manual an-
notation and evaluation were both done using the
same definition (i.e. all verbs, verb groups, and a
small number of nouns are events). The system
detected 584 events correctly, overdetected 3, and
missed 26. This gives a recall of 95.7%, a preci-
sion of 99.4%, and an F -measure of 97.5%.
The feature detection is more interesting and
Table 1 shows an evaluation of it. We carried out
this evaluation on the first 20 texts of the test cor-
pus.
7.2 Evaluation of Final Ordering
We evaluated the final ordering with the method
proposed by Setzer and Gaizauskas (2001). Their
scheme is comprehensive and enables to compare
the performance of different systems.
Description of the Evaluation Method. Set-
zer and Gaizauskas carried out an inter-annotator
agreement test for temporal relation markup.
When evaluating the final ordering of a text, they
defined the set E of all the events in the text and
the set T of all the time expressions. They com-
puted the set (E ? T )? (E ? T ) and they defined
the sets S`, I`, and B` as the transitive closures
for the relations simultaneous, includes, and be-
fore, respectively.
If S`k and S`r represent the set S` for the an-
swer key (?Gold Standard?) and system response,
respectively, the measures of precision and recall
for the simultaneous relation are:
R = |S
`
k ? S`r |
|S`k |
P = |S
`
k ? S`r |
|S`r |
For an overall measure of recall and precision,
Setzer and Gaizauskas proposed the following for-
mulas:
R = |S
`
k ? S`r | + |B`k ?B`r | + |I`k ? I`r |
|S`k | + |B`k | + |I`k |
P = |S
`
k ? S`r | + |B`k ?B`r | + |I`k ? I`r |
|S`r | + |B`r | + |I`r |
They used the classical definition of the F -
measure: the harmonic means of precision and re-
call. Note that the precision and recall are com-
puted per text, not for all relations in the test set
simultaneously.
Results. We evaluated the output of the Car-
sim system on 10 previously unseen texts against
our Gold Standard. As a baseline, we used a sim-
ple algorithm that assumes that all events occur in
the order they are introduced in the narrative. For
comparison, we also did an inter-annotator evalu-
ation on the same texts, where we compared the
Gold Standard, annotated by one of us, with the
annotation produced by another member in our
group.
As our system doesn?t support comparisons of
time expressions, we evaluated the relations con-
tained in the set E ? E. We only counted the
reflexive simultaneous relation once per tuples
(ex, ey) and (ey, ex) and we didn?t count relations
(ex, ex).
Table 2 shows our results averaged over the
10 texts. As a reference, we also included Set-
zer and Gaizauskas? averaged results for inter-
annotator agreement on temporal relations in six
texts. Their results are not directly comparable
however as they did the evaluation over the set
(E ? T ) ? (E ? T ) for English texts of another
type.
Comments. The computation of ratios on the
transitive closure makes Setzer and Gaizauskas?
evaluation method extremely sensitive. Missing a
single link often results in a loss of scores of gener-
ated transitive links and thus has a massive impact
on the final evaluation figures.
As an example, one of our texts contains six
events whose order is e4 < e5 < e6 < e1 < e2 <
e3. The event module automatically detects the
chains e4 < e5 < e6 and e1 < e2 < e3 correctly,
but misses the link e6 < e1. This gives a recall of
6/15 = 0.40. When considering evaluations per-
formed using the method above, it is meaningful
to have this in mind.
8 Carsim Integration
The visualization module considers a subset of the
detected events that it interprets graphically. We
390
Evaluation Average nwords Average nevents Pmean Rmean Fmean
Gold vs. Baseline 98.5 14.3 49.42 29.23 35.91
Gold vs. Automatic " " 54.85 37.72 43.97
Gold vs. Other Annotator " " 85.55 58.02 68.01
Setzer and Gaizauskas 312.2 26.7 67.72 40.07 49.13
Table 2: Evaluation results for final ordering averaged per text (with P , R, and F in %).
call this subset the Carsim events. Once the event
processing has been done, Carsim extracts these
specific events from the full set using a small do-
main ontology and inserts them into the template.
We use the event relations resulting from temporal
information extraction module to order them. For
all pairs of events in the template, Carsim queries
the temporal graph to determine their relation.
Figure 1 shows a part of the template represent-
ing the accident described in Section 1. It lists
the participants, with the unmentioned vehicle in-
ferred to be a car. It also shows the events and
their temporal order. Then, the visualization mod-
ule synthesizes a 3D scene and animates it. Fig-
ure 2 shows four screenshots picturing the events.
Figure 1: Representation of the accident in the ex-
ample text.
9 Conclusion and Perspectives
We have developed a method for detecting time
expressions, events, and for ordering these events
temporally. We have integrated it in a text-to-
scene converter enabling the animation of generic
actions.
The module to detect time expression and inter-
pret events performs significantly better than the
baseline technique used in previous versions of
Carsim. In addition, it should to be easy to sep-
arate it from the Carsim framework and reuse it in
other domains.
The central task, the ordering of all events,
leaves lots of room for improvement. The accu-
racy of the decision trees should improve with a
larger training set. It would result in a better over-
all performance. Switching from decision trees to
other training methods such as Support Vector Ma-
chines or using semantically motivated features, as
suggested by Mani (2003), could also be sources
of improvements.
More fundamentally, the decision tree method
we have presented is not able to take into account
long-distance links. Investigation into new strate-
gies to extract such links directly without the com-
putation of a transitive closure would improve re-
call and, given the evaluation procedure, increase
the performance.
References
James F. Allen. 1984. Towards a general theory of
action and time. Artificial Intelligence, 23(2):123?
154.
Brandon Bennett and Antony P. Galton. 2004. A uni-
fying semantics for time and events. Artificial Intel-
ligence, 153(1-2):13?48.
Branimir Boguraev and Rie Kubota Ando. 2005.
TimeML-compliant text analysis for temporal rea-
soning. In IJCAI-05, Proceedings of the Nineteenth
International Joint Conference on Artificial Intelli-
gence, pages 997?1003, Edinburgh, Scotland.
Donald Davidson. 1967. The logical form of action
sentences. In N. Rescher, editor, The Logic of Deci-
sion and Action. University of Pittsburgh Press.
David R. Dowty. 1986. The effects of aspectual class
on the temporal structure of discourse: Semantics or
pragmatics? Linguistics and Philosophy, 9:37?61.
391
Figure 2: Animation of the scene and event visualization.
Sylvain Dupuy, Arjan Egges, Vincent Legendre, and
Pierre Nugues. 2001. Generating a 3D simulation
of a car accident from a written description in nat-
ural language: The Carsim system. In ACL 2001,
Workshop on Temporal and Spatial Information Pro-
cessing, pages 1?8, Toulouse, France.
Richard Fikes and Nils J. Nilsson. 1971. Strips: A
new approach to the application of theorem proving
to problem solving. Artificial Intelligence, 2:189?
208.
Janet Hitzeman, Marc Noels Moens, and Clare Grover.
1995. Algorithms for analyzing the temporal struc-
ture of discourse. In Proceedings of the Annual
Meeting of the European Chapter of the Associa-
tion of Computational Linguistics, pages 253?260,
Dublin, Ireland.
Bob Ingria and James Pustejovsky. 2002. Specification
for TimeML 1.0.
Richard Johansson, Anders Berglund, Magnus
Danielsson, and Pierre Nugues. 2005. Automatic
text-to-scene conversion in the traffic accident
domain. In IJCAI-05, Proceedings of the Nineteenth
International Joint Conference on Artificial Intelli-
gence, pages 1073?1078, Edinburgh, Scotland.
Alex Lascarides and Nicholas Asher. 1993. Tem-
poral interpretation, discourse relations, and com-
mon sense entailment. Linguistics & Philosophy,
16(5):437?493.
Wenjie Li, Kam-Fai Wong, Guihong Cao, and Chunfa
Yuan. 2004. Applying machine learning to Chinese
temporal relation resolution. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics (ACL?04), pages 582?588, Barcelona.
Inderjeet Mani, Barry Schiffman, and Jianping Zhang.
2003. Inferring temporal ordering of events in
news. In Human Language Technology Conference
(HLT?03), Edmonton, Canada.
Inderjeet Mani. 2003. Recent developments in tempo-
ral information extraction. In Nicolas Nicolov and
Ruslan Mitkov, editors, Proceedings of RANLP?03.
John Benjamins.
John Ross Quinlan. 1993. C4.5: Programs for Ma-
chine Learning. Morgan Kauffman.
Hans Reichenbach. 1947. Elements of Symbolic Logic.
Academic Press, New York.
Andrea Setzer and Robert Gaizauskas. 2001. A pi-
lot study on annotating temporal relations in text. In
ACL 2001, Workshop on Temporal and Spatial Infor-
mation Processing, pages 73?80, Toulouse, France.
Andrea Setzer and Robert Gaizauskas. 2002. On the
importance of annotating temporal event-event rela-
tions in text. In LREC 2002, Workshop on Annota-
tion Standards for Temporal Information in Natural
Language.
Zeno Vendler. 1967. Linguistics in Philosophy. Cor-
nell University Press, Ithaca, New York.
392
Automatic Annotation for All Semantic Layers in FrameNet
Richard Johansson and Pierre Nugues
Department of Computer Science, Lund University
Box 118
SE-221 00 Lund, Sweden
{richard, pierre}@cs.lth.se
Abstract
We describe a system for automatic an-
notation of English text in the FrameNet
standard. In addition to the conventional
annotation of frame elements and their se-
mantic roles, we annotate additional se-
mantic information such as support verbs
and prepositions, aspectual markers, cop-
ular verbs, null arguments, and slot fillers.
As far as we are aware, this is the first sys-
tem that finds this information automati-
cally.
1 Introduction
Shallow semantic parsing has been an active area
of research during the last few years. Seman-
tic parsers, which are typically based on the
FrameNet (Baker et al, 1998) or PropBank for-
malisms, have proven useful in a number of NLP
projects, such as information extraction and ques-
tion answering. The main reason for their popular-
ity is that they can produce a flat layer of semantic
structure with a fair degree of robustness.
Building English semantic parsers for the
FrameNet standard has been studied widely
(Gildea and Jurafsky, 2002; Litkowski, 2004).
These systems typically address the task of identi-
fying and classifying Frame Elements (FEs), that
is semantic arguments of predicates, for a given
target word (predicate).
Although the FE layer is arguably the most cen-
tral, the FrameNet annotation standard defines a
number of additional semantic layers, which con-
tain information about support expressions (verbs
and prepositions), copulas, null arguments, slot-
fillers, and aspectual particles. This information
can for example be used in a semantic parser to
refine the meaning of a predicate, to link predi-
cates in a sentence together, or possibly to improve
detection and classification of FEs. The task of
automatic reconstruction of the additional seman-
tic layers has not been addressed by any previous
system. In this work, we describe a system that au-
tomatically identifies the entities in those layers.
2 Introduction to FrameNet
FrameNet (Baker et al, 1998; Johnson et al,
2003) is a comprehensive lexical database that
lists descriptions of words in the frame-semantic
paradigm (Fillmore, 1976). The core concept is
the frame, which is conceptual structure that rep-
resents a type of situation, object, or event, cou-
pled with a semantic valence description that de-
scribes what kinds of semantic arguments (frame
elements) are allowed or required for that partic-
ular frame. The frames are arranged in an ontol-
ogy using relations such as inheritance (such as the
relation between COMMUNICATION and COM-
MUNICATION_NOISE) and causative-of (such as
KILLING and DEATH).
For each frame, FrameNet lists a set of lemmas
or lexical units (mostly nouns, verbs, and adjec-
tives, but also a few prepositions and adverbs).
When such a word occurs in a sentence, it is called
a target word that evokes the frame. FrameNet
comes with a large set of manually annotated ex-
ample sentences, which is typically used by sta-
tistical systems for training and testing. Figure 1
shows an example of such a sentence. Here,
the target word eat evokes the INGESTION frame.
Three FEs are present: INGESTOR, INGESTIBLES,
and PLACE.
135
Often [an informal group]INGESTOR will eat
[lunch]INGESTIBLES [near a machine or other
work station]PLACE, even though a canteen is
available.
Figure 1: A sentence from the FrameNet example
corpus, with FEs bracketed and the target word in
italics.
3 Semantic Entities in FrameNet
The semantic annotation in FrameNet consists of
a set of layers. One of the layers defines the tar-
get, and the other layers provide additional infor-
mation with respect to the target. The following
layers are used:
? The FE layer, which defines the spans and se-
mantic roles of the arguments of the predi-
cate.
? A part-of-speech-specific layer, which con-
tains aspectual information for verbs; and
copulas, support expressions, and slot filling
information for nouns and adjectives.
? The ?Other? layer, containing special cases
such as null arguments.
The semantic entities that we consider in this
article are defined in the second and third of these
layers.
3.1 Support Expressions
Some noun targets, typically denoting events, are
often constructed using support verbs. In this case,
the noun carries most of the semantics (that is, it
evokes the frame), while the verb allows the slots
of the frame to be filled. Thus, the dependents
of a support verb are annotated as FEs, just like
for a verb target. Support verbs are annotated us-
ing the SUPP label on the Noun or Adjective layer.
In the following sentence, there is a support verb
(underwent) for the noun target (operation).
[Frances Patterson]PATIENT underwent an op-
eration at RMH today and is expected to be hos-
pitalized for a week or more.
The support verbs do not change the core se-
mantics of the noun target (that is, they bear no re-
lation to the frame). However, they may determine
the relation between the FEs and the target (?point-
of-view supports?, such as ?undergo an operation?
or ?perform an operation?) or provide aspectual
information (such as ?start an operation?).
The following sentence shows an example
where a governing verb is not a support verb of the
noun target. An automatic system must be able to
distinguish support verbs from other verbs.
A senior nurse observed the operation.
Although a large majority of the support expres-
sions are verbs, there are additionally some cases
of support prepositions, such as the following ex-
ample:
Secret agents of this ilk are at work all the time.
3.2 Copulas
Copular verbs, typically be, may be seen as a spe-
cial kind of support verb. They are marked us-
ing the COP label on the Noun or Adjective layer.
There are several uses of copulas:
? Class membership: John is a sailor.
? Qualities: Your literary masterpiece was delicious.
? Location: This was inside a desk drawer.
? Identity: Smithers is the vice-president of the arm-
chair division.
In FrameNet annotation, these uses of the cop-
ular verb are not distinguished.
3.3 Null Arguments
There are constructions that require special argu-
ments to be syntactically valid, but where these ar-
guments have no relation to the semantics of the
sentence. In the example below, it is an example
of this phenomenon.
I hate it when you do that.
Other common cases include existential con-
stuctions (?there are?) and subject requirement of
zero-place predicates (?it rains?). These null argu-
ments are tagged as NULL on the Other layer.
3.4 Aspectual Particles
Verb particles that indicate aspectual information
are marked using the ASPECT label. These parti-
cles must be distinguished from particles that are
parts of multiword units, such as carry out.
They just moan on and on about Fergie this and
Fergie that and I ?ve simply had enough.
136
3.5 Slot Fillers: GOV and X
FrameNet annotation contains some information
about the relation of predicates in the same sen-
tence when one predicate is a slot filler (that is,
an argument) of the other. This is most common
for noun target words, typically referring to natu-
ral kinds or artifacts.
In the following example, the target word
fingertips evokes the OBSERVABLE_BODYPARTS
frame, involving two FEs: POSSESSOR (?his?)
and BODY_PART (?fingertips?). This noun phrase
is also a slot filler (that is, an argument) of another
predicate in the sentence: cling on. In FrameNet,
such predicates are annotated using the GOV la-
bel. The constituent that contains the slot filler in
question is called (for lack of a better name) X.
Shares will boom and John Major will
[cling on]GOV [by [his]POSSESSOR
[fingertips]BODY_PART ]X.
If GOV and X are present, all FEs must be
contained in the span of the X node, such as
BODY_PART and POSSESSOR above. This may
be of use for automatic FE identifiers.
4 Identifying Semantic Entities
To find the semantic entities in the text, we used
the method that has previously been used for
FE detection: classification of nodes in a parse
tree. We divide the identification process into two
stages:
? The first stage finds SUPP, COP, and GOV.
? The second stage finds NULL, ASP, and X.
The reason for this division is that we expect
that the knowledge of the presence of SUPP, COP,
and GOV, which are almost always verbs, is use-
ful when detecting the other entities. The second
stage makes use of the information found in the
first stage. Above all, it is necessary to have infor-
mation about GOV to be able to detect X.
To train the classifiers, we selected the 150 most
common frames and divided the annotated exam-
ple sentences for those frames into a training set
of 100,000 sentences and a test set of 8,000 sen-
tences.
The classifiers used the Support Vector learning
method using the LIBSVM package (Chang and
Lin, 2001). The features used by the classifiers are
listed in Table 1. Apart from the features used by
Features for first and second stage
Target lemma
Target POS
Voice
Available semantic role labels
Position (before or after target)
Head word and POS
Phrase type
Parse tree path from target to node
Features for second stage only
Has SUPP
Has COP
Has GOV
Parse tree path from SUPP to node
Parse tree path from COP to node
Parse tree path from GOV to node
Table 1: Features used by the classifiers.
Stage 2, most of them are well-known from pre-
vious literature on FE identification and labeling
(Gildea and Jurafsky, 2002; Litkowski, 2004). For
all path features, we used both the traditional con-
stituent parse tree path (as by Gildea and Jurafsky
(2002)) and a dependency tree path (as by Ahn et
al. (2004)). We produced the parse trees using the
parser of Collins (1999).
5 Evaluation
We applied the system to a test set consisting of
approximately 8,000 sentences.
Because of inconsistent annotation, we did not
evaluate the performance of detection of the EX-
IST tag used in existential constructions. Prelim-
inary experiments indicated that the performance
was very poor.
The results, with confidence intervals at the
95% level, are shown in Table 2. They demon-
strate that the classical approach for FE identifica-
tion, that is classification of nodes in the parse tree,
is as well a viable method for detection of other
kinds of semantic information. The detection of
X shows the poorest performance. This is to be
expected, since it is very dependent on a GOV to
have been detected in the first stage.
The results for detection of aspectual particles
is not very reliable (the confidence interval was
?0.17 for precision and ?0.19 for recall), since
test corpus contained just 25 of these particles.
137
P R F?=1
SUPP 0.85 ? 0.046 0.64 ? 0.054 0.73
COP 0.90 ? 0.027 0.87 ? 0.030 0.88
NULL 0.76 ? 0.082 0.80 ? 0.080 0.78
ASP 0.83 ? 0.17 0.6 ? 0.19 0.70
GOV 0.79 ? 0.029 0.64 ? 0.030 0.71
X 0.59 ? 0.035 0.49 ? 0.032 0.54
Table 2: Results with 95% confidence intervals on
the test set.
6 Conclusion and Future Work
We have described a system that reconstructs all
semantic layers in FrameNet: in addition to the
traditional task of building the FE layer, it marks
up support expressions, aspectual particles, cop-
ulas, null arguments, and slot filling information
(GOV/X). As far as we know, no previous system
has addressed these tasks.
In the future, we would like to study how the
information provided by the additional layers in-
fluence the performance of the traditional task for
a semantic parser. FE identification, especially
for noun and adjective target words, may be made
easier by knowledge of the additional layers. As
mentioned above, if a support verb is present, its
dependents are arguments of the predicate. The
same holds for copular verbs. GOV/X nodes also
restrict where FEs may occur. In addition, support
verbs (such as ?perform? or ?undergo? an opera-
tion) may be beneficial when determining the re-
lationship between the FE and the predicate, that
is when assigning semantic roles.
References
David Ahn, Sisay Fissaha, Valentin Jijkoun, and
Maarten de Rijke. 2004. The university of Amster-
dam at Senseval-3: Semantic roles and logic forms.
In Proceedings of SENSEVAL-3.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
ings of COLING-ACL?98, pages 86?90, Montr?al,
Canada.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM:
a library for support vector machines.
Michael J. Collins. 1999. Head-driven statistical mod-
els for natural language parsing. Ph.D. thesis, Uni-
versity of Pennsylvania, Philadelphia.
Charles J. Fillmore. 1976. Frame semantics and
the nature of language. Annals of the New York
Academy of Sciences: Conference on the Origin and
Development of Language, 280:20?32.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Christopher Johnson, Miriam Petruck, Collin Baker,
Michael Ellsworth, Josef Ruppenhofer, and Charles
Fillmore. 2003. FrameNet: Theory and Practice.
Ken Litkowski. 2004. Senseval-3 task: Automatic
labeling of semantic roles. In Rada Mihalcea and
Phil Edmonds, editors, Senseval-3: Third Interna-
tionalWorkshop on the Evaluation of Systems for the
Semantic Analysis of Text, pages 9?12, Barcelona,
Spain, July. Association for Computational Linguis-
tics.
138
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 436?443,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A FrameNet-based Semantic Role Labeler for Swedish
Richard Johansson and Pierre Nugues
Department of Computer Science, LTH
Lund University, Sweden
{richard, pierre}@cs.lth.se
Abstract
We present a FrameNet-based semantic
role labeling system for Swedish text. As
training data for the system, we used an
annotated corpus that we produced by
transferring FrameNet annotation from the
English side to the Swedish side in a par-
allel corpus. In addition, we describe two
frame element bracketing algorithms that
are suitable when no robust constituent
parsers are available.
We evaluated the system on a part of the
FrameNet example corpus that we trans-
lated manually, and obtained an accuracy
score of 0.75 on the classification of pre-
segmented frame elements, and precision
and recall scores of 0.67 and 0.47 for the
complete task.
1 Introduction
Semantic role labeling (SRL), the process of auto-
matically identifying arguments of a predicate in
a sentence and assigning them semantic roles, has
received much attention during the recent years.
SRL systems have been used in a number of
projects in Information Extraction and Question
Answering, and are believed to be applicable in
other domains as well.
Building SRL systems for English has been
studied widely (Gildea and Jurafsky, 2002;
Litkowski, 2004), inter alia. However, all these
works rely on corpora that have been produced at
the cost of a large effort by human annotators. For
instance, the current FrameNet corpus (Baker et
al., 1998) consists of 130,000 manually annotated
sentences. For smaller languages such as Swedish,
such corpora are not available.
In this work, we describe a FrameNet-based se-
mantic role labeler for Swedish text. Since there
was no existing training corpus available ? no
FrameNet-annotated Swedish corpus of substan-
tial size exists ? we used an English-Swedish
parallel corpus whose English part was annotated
with semantic roles using the FrameNet annota-
tion scheme. We then applied a cross-language
transfer to derive an annotated Swedish part. To
evaluate the performance of the Swedish SRL
system, we applied it to a small portion of the
FrameNet example corpus that we translated man-
ually.
1.1 FrameNet: an Introduction
FrameNet (Baker et al, 1998) is a lexical database
that describes English words using Frame Seman-
tics (Fillmore, 1976). In this framework, predi-
cates (or in FrameNet terminology, target words)
and their arguments are linked by means of seman-
tic frames. A frame can intuitively be thought of
as a template that defines a set of slots, frame ele-
ments (FEs), that represent parts of the conceptual
structure and typically correspond to prototypical
participants or properties.
Figure 1 shows an example sentence annotated
with FrameNet information. In this example, the
target word statements belongs to (?evokes?) the
frame STATEMENT. Two constituents that fill slots
of the frame (SPEAKER and TOPIC) are annotated
as well.
As usual in these cases, [both parties]SPEAKER
agreed to make no further statements [on the
matter]TOPIC.
Figure 1: A sentence from the FrameNet example
corpus.
436
The initial versions of FrameNet were focused
on describing situations and events, i.e. typically
verbs and their nominalizations. Currently, how-
ever, FrameNet defines frames for a wider range of
semantic relations that can be thought of as predi-
cate/argument structures, including descriptions of
events, states, properties, and objects.
FrameNet consists of the following main parts:
? An ontology consisting of a set of frames,
frame elements for each frame, and rela-
tions (such as inheritance and causative-of)
between frames.
? A list of lexical units, that is word forms
paired with their corresponding frames. The
frame is used to distinguish between differ-
ent senses of the word, although the treatment
of polysemy in FrameNet is relatively coarse-
grained.
? A collection of example sentences that pro-
vide lexical evidence for the frames and the
corresponding lexical units. Although this
corpus is not intended to be representative, it
is typically used as a training corpus when
contructing automatic FrameNet labelers.
1.2 Related Work
Since training data is often a scarce resource for
most languages other than English, a wide range
of methods have been proposed to reduce the need
for manual annotation. Many of these have relied
on existing resources for English and a transfer
method based on word alignment in a parallel cor-
pus to automatically create an annotated corpus in
a new language. Although these data are typically
quite noisy, they have been used to train automatic
systems.
For the particular case of transfer of FrameNet
annotation, there have been a few projects that
have studied transfer methods and evaluated the
quality of the automatically produced corpus. Jo-
hansson and Nugues (2005) applied the word-
based methods of Yarowsky et al (2001) and ob-
tained promising results. Another recent effort
(Pad? and Lapata, 2005) demonstrates that deeper
linguistic information, such as parse trees in the
source and target language, is very beneficial for
the process of FrameNet annotation transfer.
A rather different method to construct bilingual
semantic role annotation is the approach taken by
BiFrameNet (Fung and Chen, 2004). In that work,
annotated structures in a new language (in that
case Chinese) are produced by mining for similar
structures rather than projecting them via parallel
corpora.
2 Automatic Annotation of a Swedish
Training Corpus
2.1 Training an English Semantic Role
Labeler
We selected the 150 most frequent frames in
FrameNet and applied the Collins parser (Collins,
1999) to the example sentences for these frames.
We built a conventional FrameNet parser for En-
glish using 100,000 of these sentences as a train-
ing set and 8,000 as a development set. The classi-
fiers were based on Support Vector Machines that
we trained using LIBSVM (Chang and Lin, 2001)
with the Gaussian kernel. When testing the sys-
tem, we did not assume that the frame was known
a priori. We used the available semantic roles for
all senses of the target word as features for the
classifier.
On a test set from FrameNet, we estimated that
the system had a precision of 0.71 and a recall of
0.65 using a strict scoring method. The result is
slightly lower than the best systems at Senseval-
3 (Litkowski, 2004), possibly because we used a
larger set of frames, and we did not assume that
the frame was known a priori.
2.2 Transferring the Annotation
We produced a Swedish-language corpus anno-
tated with FrameNet information by applying
the SRL system to the English side of Europarl
(Koehn, 2005), which is a parallel corpus that is
derived from the proceedings of the European Par-
liament. We projected the bracketing of the target
words and the frame elements onto the Swedish
side of the corpus by using the Giza++ word
aligner (Och and Ney, 2003). Each word on the
English side was mapped by the aligner onto a
(possibly empty) set of words on the Swedish side.
We used the maximal span method to infer the
bracketing on the Swedish side, which means that
the span of a projected entity was set to the range
from the leftmost projected token to the rightmost.
Figure 2 shows an example of this process.
To make the brackets conform to the FrameNet
annotation practices, we applied a small set of
heuristics. The FrameNet conventions specify that
linking words such as prepositions and subordinat-
437
SPEAKER express MESSAGE[We]             wanted to               [our perplexity as regards these points]             [by abstaining in committee]MEANS
MEANS SPEAKER[Genom att avst? fr?n att r?sta i utskottet]           har [vi]            velat                [denna v?r tveksamhet]uttrycka MESSAGE
Figure 2: Example of projection of FrameNet annotation.
ing conjunctions should be included in the brack-
eting. However, since constructions are not iso-
morphic in the sentence pair, a linking word on
the target side may be missed by the projection
method since it is not present on the source side.
For example, the sentence the doctor was answer-
ing an emergency phone call is translated into
Swedish as doktorn svarade p? ett larmsamtal,
which uses a construction with a preposition p?
?to/at/on? that has no counterpart in the English
sentence. The heuristics that we used are spe-
cific for Swedish, although they would probably
be very similar for any other language that uses
a similar set of prepositions and connectives, i.e.
most European languages.
We used the following heuristics:
? When there was only a linking word (preposi-
tion, subordinating conjunction, or infinitive
marker) between the FE and the target word,
it was merged with the FE.
? When a Swedish FE was preceded by a link-
ing word, and the English FE starts with such
a word, it was merged with the FE.
? We used a chunker and adjusted the FE
brackets to include only complete chunks.
? When a Swedish FE crossed the target word,
we used only the part of the FE that was on
the right side of the target.
In addition, some bad annotation was discarded
because we obviously could not use sentences
where no counterpart for the target word could be
found. Additionally, we used only the sentences
where the target word was mapped to a noun, verb,
or an adjective on the Swedish side.
Because of homonymy and polysemy problems,
applying a SRL system without knowing target
words and frames a priori necessarily introduces
noise into the automatically created training cor-
pus. There are two kinds of word sense ambigu-
ity that are problematic in this case: the ?internal?
ambiguity, or the fact that there may be more than
one frame for a given target word; and the ?exter-
nal? ambiguity, where frequently occurring word
senses are not listed in FrameNet. To sidestep the
problem of internal ambiguity, we used the avail-
able semantic roles for all senses of the target word
as features for the classifier (as described above).
Solving the problem of external ambiguity was
outside the scope of this work.
Some potential target words had to be ignored
since their sense ambiguity was too difficult to
overcome. This category includes auxiliaries such
as be and have, as well as verbs such as take and
make, which frequently appear as support verbs
for nominal predicates.
2.3 Motivation
Although the meaning of the two sentences in
a sentence pair in a parallel corpus should be
roughly the same, a fundamental question is
whether it is meaningful to project semantic
markup of text across languages. Equivalent
words in two different languages sometimes ex-
hibit subtle but significant semantic differences.
However, we believe that a transfer makes sense,
since the nature of FrameNet is rather coarse-
grained. Even though the words that evoke a frame
may not have exact counterparts, it is probable that
the frame itself has.
For the projection method to be meaningful, we
must make the following assumptions:
? The complete frame ontology in the English
FrameNet is meaningful in Swedish as well,
and each frame has the same set of semantic
roles and the same relations to other frames.
? When a target word evokes a certain frame in
English, it has a counterpart in Swedish that
evokes the same frame.
? Some of the FEs on the English side have
counterparts with the same semantic roles on
the Swedish side.
438
In addition, we made the (obviously simplistic)
assumption that the contiguous entities we project
are also contiguous on the target side.
These assumptions may all be put into ques-
tion. Above all, the second assumption will fail
in many cases because the translations are not lit-
eral, which means that the sentences in the pair
may express slightly different information. The
third assumption may be invalid if the information
expressed is realized by radically different con-
structions, which means that an argument may be-
long to another predicate or change its semantic
role on the Swedish side. Pad? and Lapata (2005)
avoid this problem by using heuristics based on a
target-language FrameNet to select sentences that
are close in meaning. Since we have no such re-
source to rely on, we are forced to accept that this
problem introduces a certain amount of noise into
the automatically annotated corpus.
3 Training a Swedish SRL System
Using the transferred FrameNet annotation, we
trained a SRL system for Swedish text. Like most
previous systems, it consists of two parts: a FE
bracketer and a classifier that assigns semantic
roles to FEs. Both parts are implemented as SVM
classifiers trained using LIBSVM. The semantic
role classifier is rather conventional and is not de-
scribed in this paper.
To construct the features used by the classifiers,
we used the following tools:
? An HMM-based POS tagger,
? A rule-based chunker,
? A rule-based time expression detector,
? Two clause identifiers, of which one is rule-
based and one is statistical,
? The MALTPARSER dependency parser
(Nivre et al, 2004), trained on a 100,000-
word Swedish treebank.
We constructed shallow parse trees using the
clause trees and the chunks. Dependency and shal-
low parse trees for a fragment of a sentence from
our test corpus are shown in Figures 3 and 4, re-
spectively. This sentence, which was translated
from an English sentence that read the doctor was
answering an emergency phone call, comes from
the English FrameNet example corpus.
doktorn svarade p? ett larmsamtal
SUB ADV
PR
DET
Figure 3: Example dependency parse tree.
[ doktorn ] svarade[ ] larmsamtal[[ ett ]NG_nomPPp?]VG_finNG_nom Clause[ ]
Figure 4: Example shallow parse tree.
3.1 Frame Element Bracketing Methods
We created two redundancy-based FE bracket-
ing algorithms based on binary classification of
chunks as starting or ending the FE. This is some-
what similar to the chunk-based system described
by Pradhan et al (2005a), which uses a segmenta-
tion strategy based on IOB2 bracketing. However,
our system still exploits the dependency parse tree
during classification.
We first tried the conventional approach to the
problem of FE bracketing: applying a parser to the
sentence, and classifying each node in the parse
tree as being an FE or not. We used a dependency
parser since there is no constituent-based parser
available for Swedish. This proved unsuccessful
because the spans of the dependency subtrees fre-
quently were incompatible with the spans defined
by the FrameNet annotations. This was especially
the case for non-verbal target words and when the
head of the argument was above the target word in
the dependency tree. To be usable, this approach
would require some sort of transformation, possi-
bly a conversion into a phrase-structure tree, to be
applied to the dependency trees to align the spans
with the FEs. Preliminary investigations were un-
successful, and we left this to future work.
We believe that the methods we developed are
more suitable in our case, since they base their
decisions on several parse trees (in our case, two
clause-chunk trees and one dependency tree). This
redundancy is valuable because the dependency
parsing model was trained on a treebank of just
100,000 words, which makes it less robust than
Collins? or Charniak?s parsers for English. In ad-
dition, the methods do not implicitly rely on the
common assumption that every FE has a counter-
part in a parse tree. Recent work in semantic role
labeling, see for example Pradhan et al (2005b),
has focused on combining the results of SRL sys-
tems based on different types of syntax. Still, all
439
systems exploiting recursive parse trees are based
on binary classification of nodes as being an argu-
ment or not.
The training sets used to train the final classi-
fiers consisted of one million training instances for
the start classifier, 500,000 for the end classifier,
and 272,000 for the role classifier. The features
used by the classifiers are described in Subsec-
tion 3.2, and the performance of the two FE brack-
eting algorithms compared in Subsection 4.2.
3.1.1 Greedy start-end
The first FE bracketing algorithm, the greedy
start-end method, proceeds through the sequence
of chunks in one pass from left to right. For each
chunk opening bracket, a binary classifier decides
if an FE starts there or not. Similarly, another bi-
nary classifier tests chunk end brackets for ends
of FEs. To ensure compliance to the FrameNet
annotation standard (bracket matching, and no FE
crossing the target word), the algorithm inserts ad-
ditional end brackets where appropriate. Pseu-
docode is given in Algorithm 1.
Algorithm 1 Greedy Bracketing
Input: A list L of chunks and a target word t
Binary classifiers starts and ends
Output: The sets S and E of start and end brackets
Split L into the sublists Lbefore , Ltarget , and Lafter , which correspond
to the parts of the list that is before, at, and after the target word, respectively.
Initialize chunk-open to FALSE
for Lsub in {Lbefore, Ltarget, Lafter} do
for c in Lsub do
if starts(c) then
if chunk-open then
Add an end bracket before c to E
end if
chunk-open? TRUE
Add a start bracket before c to S
end if
if chunk-open ? (ends(c) ? c is final in Lsub) then
chunk-open? FALSE
Add an end bracket after c to E
end if
end for
end for
Figure 5 shows an example of this algorithm,
applied to the example fragment. The small brack-
ets correspond to chunk boundaries, and the large
brackets to FE boundaries that the algorithm in-
serts. In the example, the algorithm inserts an end
bracket after the word doktorn ?the doctor?, since
no end bracket was found before the target word
svarade ?was answering?.
3.1.2 Globally optimized start-end
The second algorithm, the globally optimized
start-end method, maximizes a global probability
score over each sentence. For each chunk open-
ing and closing bracket, probability models assign
START
[ ] svarade [...  [doktorn]                    [p?] [ett larmsamtal]   ...]
Additional END inserted END
START
Figure 5: Illustration of the greedy start-end
method.
the probability of an FE starting (or ending, re-
spectively) at that chunk. The probabilities are
estimated using the built-in sigmoid fitting meth-
ods of LIBSVM. Making the somewhat unrealis-
tic assumption of independence of the brackets,
the global probability score to maximize is de-
fined as the product of all start and end proba-
bilities. We added a set of constraints to ensure
that the segmentation conforms to the FrameNet
annotation standard. The constrained optimiza-
tion problem is then solved using the JACOP fi-
nite domain constraint solver (Kuchcinski, 2003).
We believe that an n-best beam search method
would produce similar results. The pseudocode
for the method can be seen in Algorithm 2. The
definitions of the predicates no-nesting and
no-crossing, which should be obvious, are
omitted.
Algorithm 2 Globally Optimized Bracketing
Input: A list L of chunks and a target word t
Probability models P?starts and P?ends
Output: The sets Smax and Emax of start and end brackets
legal(S, E) ? |S| = |E|
? max(E) > max(S) ?min(S) < min(E)
? no-nesting(S, E) ? no-crossing(t, S, E)
score(S, E) ?
?
c?S P?starts(c) ?
?
c?L\S(1? P?starts(c))
?
?
c?E P?ends(c) ?
?
c?L\E(1 ? P?ends(c))
(Smax, Emax)? argmax{legal(S,E)}score(S, E)
Figure 6 shows an example of the globally op-
timized start-end method. In the example, the
global probability score is maximized by a brack-
eting that is illegal because the FE starting at dok-
torn is not closed before the target (0.8 ? 0.6 ? 0.6 ?
0.7 ? 0.8 ? 0.7 = 0.11). The solution of the con-
strained problem is a bracketing that contains an
end bracket before the target (0.8 ? 0.4 ? 0.6 ? 0.7 ?
0.8 ? 0.7 = 0.075)
3.2 Features Used by the Classifiers
Table 1 summarizes the feature sets used by
the greedy start-end (GSE), optimized start-end
(OSE), and semantic role classification (SRC).
440
[ ] svarade [...  [doktorn]                    [p?] [ett larmsamtal]   ...]
P^starts1? P
^
starts1? =0.4
P^startsP
^
starts P
^
starts
P^starts1?
Pends
^
Pends
^ Pends
^
Pends
^
Pends
^
Pends
^
1? 1? 1?
=0.4
=0.6
=0.3
=0.7
=0.7
=0.3
=0.8
=0.2
=0.6 =0.2
=0.8
Figure 6: Illustration of the globally optimized
start-end method.
GSE OSE SRC
Target lemma + + +
Target POS + + +
Voice + + +
Allowed role labels + + +
Position + + +
Head word (HW) + + +
Head POS + + +
Phrase/chunk type (PT) + + +
HW/POS/PT,?2 chunk window + + -
Dep-tree & shallow path ?target + + +
Starting paths ?target + + -
Ending paths ?target + + -
Path?start + - -
Table 1: Features used by the classifiers.
3.2.1 Conventional Features
Most of the features that we use have been used
by almost every system since the first well-known
description (Gildea and Jurafsky, 2002). The fol-
lowing of them are used by all classifiers:
? Target word (predicate) lemma and POS
? Voice (when the target word is a verb)
? Position (before or after the target)
? Head word and POS
? Phrase or chunk type
In addition, all classifiers use the set of allowed
semantic role labels as a set of boolean features.
This is needed to constrain the output to a la-
bel that is allowed by FrameNet for the current
frame. In addition, this feature has proven use-
ful for the FE bracketing classifiers to distinguish
between event-type and object-type frames. For
event-type frames, dependencies are often long-
distance, while for object-type frames, they are
typically restricted to chunks very near the target
word. The part of speech of the target word alone
is not enough to distinguish these two classes,
since many nouns belong to event-type frames.
For the phrase/chunk type feature, we use
slightly different values for the bracketing case
and the role assignment case: for bracketing, the
value of this feature is simply the type of the cur-
rent chunk; for classification, it is the type of the
largest chunk or clause that starts at the leftmost
token of the FE. For prepositional phrases, the
preposition is attached to the phrase type (for ex-
ample, the second FE in the example fragment
starts with the preposition p? ?at/on?, which causes
the value of the phrase type feature to be PP-p?).
3.2.2 Chunk Context Features
Similarly to the chunk-based PropBank ar-
gument bracketer described by Pradhan et al
(2005a), the start-end methods use the head word,
head POS, and chunk type of chunks in a window
of size 2 on both sides of the current chunk to clas-
sify it as being the start or end of an FE.
3.2.3 Parse Tree Path Features
Parse tree path features have been shown to be
very important for argument bracketing in several
studies. All classifiers used here use a set of such
features:
? Dependency tree path from the head to the
target word. In the example text, the first
chunk (consisting of the word doktorn), has
the value SUB-? for this feature. This means
that to go from the head of the chunk to the
target in the dependency graph (Figure 3),
you traverse a SUB (subject) link upwards.
Similarly, the last chunk (ett larmsamtal) has
the value PR-?-ADV-?.
? Shallow path from the chunk containing the
head to the target word. For the same chunks
as above, these values are both NG_nom-?-
Clause-?-VG_fin, which means that to tra-
verse the shallow parse tree (Figure 4) from
the chunk to the target, you start with a
NG_nom node, go upwards to a Clause
node, and finally down to the VG_fin node.
The start-end classifiers additionally use the full
set of paths (dependency and shallow paths) to the
target word from each node starting (or ending, re-
spectively) at the current chunk, and the greedy
end classifier also uses the path from the current
chunk to the start chunk.
441
4 Evaluation of the System
4.1 Evaluation Corpus
To evaluate the system, we manually translated
150 sentences from the FrameNet example corpus.
These sentences were selected randomly from the
English development set. Some sentences were re-
moved, typically because we found the annotation
dubious or the meaning of the sentence difficult to
comprehend precisely. The translation was mostly
straightforward. Because of the extensive use of
compounding in Swedish, some frame elements
were merged with target words.
4.2 Comparison of FE Bracketing Methods
We compared the performance of the two methods
for FE bracketing on the test set. Because of lim-
ited time, we used smaller training sets than for the
full evaluation below (100,000 training instances
for all classifiers). Table 2 shows the result of this
comparison.
Greedy Optimized
Precision 0.70 0.76
Recall 0.50 0.44
F?=1 0.58 0.55
Table 2: Comparison of FE bracketing methods.
As we can see from the Table 2, the globally op-
timized start-end method increased the precision
somewhat, but decreased the recall and made the
overall F-measure lower. We therefore used the
greedy start-end method for our final evaluation
that is described in the next section.
4.3 Final System Performance
We applied the Swedish semantic role labeler to
the translated sentences and evaluated the result.
We used the conventional experimental setting
where the frame and the target word were given
in advance. The results, with approximate 95%
confidence intervals included, are presented in Ta-
ble 3. The figures are precision and recall for the
full task, classification accuracy of pre-segmented
arguments, precision and recall for the bracket-
ing task, full task precision and recall using the
Senseval-3 scoring metrics, and finally the propor-
tion of full sentences whose FEs were correctly
bracketed and classified. The Senseval-3 method
uses a more lenient scoring scheme that counts a
FE as correctly identified if it overlaps with the
gold standard FE and has the correct label. Al-
though the strict measures are more interesting,
we include these figures for comparison with the
systems participating in the Senseval-3 Restricted
task (Litkowski, 2004).
We include baseline scores for the argument
bracketing and classification tasks, respectively.
The bracketing baseline method considers non-
punctuation subtrees dependent of the target word.
When the target word is a verb, the baseline puts
FE brackets around the words included in each of
these subtrees1. When the target is a noun, we also
bracket the target word token itself, and when it is
an adjective, we additionally bracket its parent to-
ken. As a baseline for the argument classification
task, every argument is assigned the most frequent
semantic role in the frame. As can be seen from
the table, all scores except the argument bracket-
ing recall are well above the baselines.
Precision (Strict scoring method) 0.67 ? 0.064
Recall 0.47 ? 0.057
Argument Classification Accuracy 0.75 ? 0.050
Baseline 0.41 ? 0.056
Argument Bracketing Precision 0.80 ? 0.055
Baseline Precision 0.50 ? 0.055
Argument Bracketing Recall 0.57 ? 0.057
Baseline Recall 0.55 ? 0.057
Precision (Senseval-3 scoring method) 0.77 ? 0.057
Overlap 0.75 ? 0.039
Recall 0.55 ? 0.057
Complete Sentence Accuracy 0.29 ? 0.073
Table 3: Results on the Swedish test set with ap-
proximate 95% confidence intervals.
Although the performance figures are better
than the baselines, they are still lower than for
most English systems (although higher than some
of the systems at Senseval-3). We believe that
the main reason for the performance is the qual-
ity of the data that were used to train the system,
since the results are consistent with the hypoth-
esis that the quality of the transferred data was
roughly equal to the performance of the English
system multiplied by the figures for the transfer
method (Johansson and Nugues, 2005). In that
experiment, the transfer method had a precision
of 0.84, a recall of 0.81, and an F-measure of
0.82. If we assume that the transfer performance
is similar for Swedish, we arrive at a precision of
0.71 ? 0.84 = 0.60, a recall of 0.65 ? 0.81 = 0.53,
1This is possible because MALTPARSER produces projec-
tive trees, i.e. the words in each subtree form a contiguous
substring of the sentence.
442
and an F-measure of 0.56. For the F-measure,
0.55 for the system and 0.56 for the product, the
figures match closely. For the precision, the sys-
tem performance (0.67) is significantly higher than
the product (0.60), which suggests that the SVM
learning method handles the noisy training set
rather well for this task. The recall (0.47) is lower
than the corresponding product (0.53), but the dif-
ference is not statistically significant at the 95%
level. These figures suggest that the main effort
towards improving the system should be spent on
improving the training data.
5 Conclusion
We have described the design and implementa-
tion of a Swedish FrameNet-based SRL system
that was trained using a corpus that was anno-
tated using cross-language transfer from English
to Swedish. With no manual effort except for
translating sentences for evaluation, we were able
to reach promising results. To our knowledge, the
system is the first SRL system for Swedish in liter-
ature. We believe that the methods described could
be applied to any language, as long as there ex-
ists a parallel corpus where one of the languages
is English. However, the relatively close relation-
ship between English and Swedish probably made
the task comparatively easy in our case.
As we can see, the figures (especially the FE
bracketing recall) leave room for improvement for
the system to be useful in a fully automatic set-
ting. Apart from the noisy training set, proba-
ble reasons for this include the lower robustness
of the Swedish parsers compared to those avail-
able for English. In addition, we have noticed
that the European Parliament corpus is somewhat
biased. For instance, a very large proportion of
the target words evoke the STATEMENT or DIS-
CUSSION frames, but there are very few instances
of the BEING_WET and MAKING_FACES frames.
While training, we tried to balance the selection
somewhat, but applying the projection methods
on other types of parallel corpora (such as novels
available in both languages) may produce a better
training corpus.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
ings of COLING-ACL?98, pages 86?90, Montr?al,
Canada.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM:
a library for support vector machines.
Michael J. Collins. 1999. Head-driven statistical mod-
els for natural language parsing. Ph.D. thesis, Uni-
versity of Pennsylvania, Philadelphia.
Charles J. Fillmore. 1976. Frame semantics and
the nature of language. Annals of the New York
Academy of Sciences: Conference on the Origin and
Development of Language, 280:20?32.
Pascale Fung and Benfeng Chen. 2004. BiFrameNet:
Bilingual frame semantics resource construction
by cross-lingual induction. In Proceedings of
COLING-2004.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Richard Johansson and Pierre Nugues. 2005. Using
parallel corpora for automatic transfer of FrameNet
annotation. In Proceedings of the 1st ROMANCE
FrameNet Workshop, Cluj-Napoca, Romania, 26-28
July.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. InMT Summit 2005.
Krzysztof Kuchcinski. 2003. Constraints-driven
scheduling and resource assignment. ACM Transac-
tions on Design Automation of Electronic Systems,
8(3):355?383.
Ken Litkowski. 2004. Senseval-3 task: Automatic la-
beling of semantic roles. In Senseval-3: Third Inter-
national Workshop on the Evaluation of Systems for
the Semantic Analysis of Text, pages 9?12.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Proceedings
of CoNLL-2004, pages 49?56.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Sebastian Pad? and Mirella Lapata. 2005. Cross-
lingual projection of role-semantic information. In
Proceedings of HLT/EMNLP 2005.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James Martin, and Dan Jurafsky.
2005a. Support vector learning for semantic argu-
ment classification. Machine Learning, 60(1):11?
39.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin, and Daniel Jurafsky. 2005b. Semantic role
labeling using different syntactic views. In Proceed-
ings of ACL-2005.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In Proceedings of HLT 2001.
443
Carsim: A System to Visualize Written Road Accident Reports as Animated
3D Scenes
Richard Johansson David Williams Anders Berglund Pierre Nugues
LUCAS, Department of Computer Science, Lund University
Box 118
SE-221 00 Lund, Sweden
{richard, pierre}@cs.lth.se, {d98dw, d98ab}@efd.lth.se
Abstract
This paper describes a system to create animated
3D scenes of car accidents from reports written in
Swedish. The system has been developed using
news reports of varying size and complexity. The
text-to-scene conversion process consists of two
stages. An information extraction module creates
a structured representation of the accident and a vi-
sual simulator generates and animates the scene.
We first describe the overall structure of the text-
to-scene conversion and the structure of the repre-
sentation. We then explain the information extrac-
tion and visualization modules. We show snapshots
of the car animation output and we conclude with
the results we obtained.
1 Text-to-Scene Conversion
As noted by Michel Denis, language and images are
two different representation modes whose cooper-
ation is needed in many forms of cognitive opera-
tions. The description of physical events, mathe-
matical theorems, or structures of any kind using
language is sometimes difficult to understand. Im-
ages and graphics can then help understand ideas or
situations and realize their complexity. They have
an indisputable capacity to represent and to commu-
nicate knowledge and are an effective means to rep-
resent and explain things, see (Kosslyn, 1983; Tufte,
1997; Denis, 1991).
Narratives of a car accidents, for instance, often
make use of space descriptions, movements, and
directions that are sometimes difficult to grasp for
most readers. We believe that forming consistent
mental images are necessary to understand them
properly. However, some people have difficulties in
imagining situations and may need visual aids pre-
designed by professional analysts.
In this paper, we will describe Carsim, a text-to-
scene converter that automates the generation of im-
ages from texts.
2 Related Work
The conversion of natural language texts into graph-
ics has been investigated in a few projects. NALIG
(Adorni et al, 1984; Manzo et al, 1986) is an early
example of them that was aimed at recreating static
2D scenes. One of its major goals was to study rela-
tionships between space and prepositions. NALIG
considered simple phrases in Italian of the type sub-
ject, preposition, object that in spite of their simplic-
ity can have ambiguous interpretations. From what
is described in the papers, NALIG has not been ex-
tended to process sentences and even less to texts.
WordsEye (Coyne and Sproat, 2001) is an im-
pressive system that recreates 3D animated scenes
from short descriptions. The number of 3D objects
WordsEye uses ? 12,000 ? gives an idea of its am-
bition. WordsEye integrates resources such as the
Collins? dependency parser and the WordNet lexical
database. The narratives cited as examples resemble
imaginary fairy tales and WordsEye does not seem
to address real world stories.
CogViSys is a last example that started with the
idea of generating texts from a sequence of video
images. The authors found that it could also be
useful to reverse the process and generate synthetic
video sequences from texts. The logic engine be-
hind the text-to-scene converter (Arens et al, 2002)
is based on the Discourse Representation Theory.
The system is limited to the visualization of single
vehicle maneuvers at an intersection as the one de-
scribed in this two-sentence narrative: A car came
from Kriegstrasse. It turned left at the intersection.
The authors give no further details on the text cor-
pus and no precise description of the results.
3 Carsim
Carsim (Egges et al, 2001; Dupuy et al, 2001) is
a program that analyzes texts describing car acci-
dents and visualizes them in a 3D environment. It
has been developed using real-world texts.
The Carsim architecture is divided into two parts
that communicate using a formal representation of
Input Text
Linguistic
Component
Formal
Description
Visualizer
Component
Output
Animation
Figure 1: The Carsim architecture.
the accident. Carsim?s first part is a linguistic mod-
ule that extracts information from the report and fills
the frame slots. The second part is a virtual scene
generator that takes the structured representation as
input, creates the visual entities, and animates them
(Figure 1).
4 A Corpus of Traffic Accident
Descriptions
As development and test sets, we have collected ap-
proximately 200 reports of road accidents from vari-
ous Swedish newspapers. The task of analyzing the
news reports is made more complex by their vari-
ability in style and length. The size of the texts
ranges from a couple of sentences to more than a
page. The amount of details is overwhelming in
some reports, while in others most of the informa-
tion is implicit. The complexity of the accidents de-
scribed ranges from simple accidents with only one
vehicle to multiple collisions with several partici-
pating vehicles and complex movements.
Although our work has concentrated on the press
clippings, we also have access to accident reports
from the STRADA database (Swedish TRaffic Ac-
cident Data Acquisition) of Va?gverket, the Swedish
traffic authority. STRADA registers nearly all the
accidents that occur in Sweden (Karlberg, 2003).
(All the accidents where there are casualties.) Af-
ter an accident, the victims describe the location
and conditions of it in a standardized form col-
lected in hospitals. The corresponding reports are
transcribed in a computer-readable format in the
STRADA database. This source contains two kinds
of reports: the narratives written by the victims of
the accident and their transcriptions by traffic ex-
perts. The original texts contain spelling mistakes,
abbreviations, and grammatical errors. The tran-
scriptions often simplify, interpret the original texts,
and contain jargon.
The next text is an excerpt from our development
corpus. This report is an example of a press wire
describing an accident.
En do?dsolycka intra?ffade inatt so?der
om Vissefja?rda pa? riksva?g 28. Det var en
bil med tva? personer i som kom av va?gen i
en va?nsterkurva och ko?rde i ho?g hastighet
in i en gran. Passageraren, som var fo?dd
-84, dog. Fo?raren som var 21 a?r gam-
mal va?rdas pa? sjukhus med sva?ra skador.
Polisen missta?nker att bilen de fa?rdades i,
en ny Saab, var stulen i Emmaboda och
det ska under dagen underso?kas.
Sveriges Radio, November 9, 2002
A fatal accident took place tonight south
of Vissefja?rda on Road 28. A car carry-
ing two persons departed from the road
in a left-hand curve and crashed at a high
speed into a spruce. The passenger, who
was born in 1984, died. The driver, who
was 21 years old, is severely injured and
is taken care of in a hospital. The police
suspects that the car they were traveling
in, a new Saab, was stolen in Emmaboda
and will investigate it today.
The text above, our translation.
5 Knowledge Representation
The Carsim language processing module reduces
the text content to a formal representation that out-
lines what happened and enables a conversion to a
symbolic scene. It uses information extraction tech-
niques to map a text onto a structure that consists of
three main elements:
? A scene object, which describes the static pa-
rameters of the environment, such as weather,
light, and road configuration.
? A list of road objects, for example cars, trucks,
and trees, and their associated sequences of
movements.
? A list of collisions between road objects.
The structure of the formalism, which sets the
limit of what information can be expressed, was de-
signed with the help of traffic safety experts at the
Department of Traffic and Road at Lund University.
It contains the information necessary to reproduce
and animate the accident entities in our visualiza-
tion model. We used an iterative process to design
it. We started from a first incomplete model (Dupuy
et al, 2001) and we manually constructed the rep-
resentation of about 50 texts until we had reached a
sufficient degree of expressivity.
The representation we use is a typical example of
frames a` la Minsky, where the objects in the rep-
resentation consist of a number of attribute/values
slots which are to be filled by the information ex-
traction module. Each object in the representation
Figure 2: Representation of the accident in the ex-
ample above.
belongs to a concept in a domain ontology we have
developed. The concepts are ordered in an inheri-
tance hierarchy.
Figure 2 shows how Carsim?s graphical user in-
terface presents the representation of the accident
in the example above. The scene element contains
the location of the accident and the configuration of
roads, in this case a left-hand bend. The list of road
objects contains one car and one tree. The event
chain for the car describes the movements: the car
leaves the road. Finally, the collision list describes
one collision between the car and the tree.
6 The Information Extraction Module
The information extraction subsystem fills the frame
slots. Its processing flow consists in analyzing the
text linguistically using the word groups obtained
from the linguistic modules and a sequence of se-
mantic modules. The information extraction sub-
system uses the literal content of certain phrases it
finds in the text or infers the environment and the
actions.
We use a pipeline of modules in the first stages
of the natural language processing chain. The
tasks consists of tokenizing, part-of-speech tagging,
splitting into sentences, detecting the noun groups,
clause boundaries, and domain-specific multiwords.
We use the Granska part-of-speech tagger (Carl-
berger and Kann, 1999) and Ejerhed?s algorithm
(Ejerhed, 1996) to detect clause boundaries.
6.1 Named Entity Recognition
Carsim uses a domain-specific named entity recog-
nition module, which detects names of persons,
places, roads, and car makes (Persson and Daniels-
son, 2004).
The recognition is based on a small database of
2,500 entries containing person names, city and re-
gion names, and car names. It applies a cascade
of regular expressions that takes into account the
morphology of Swedish proper noun formation and
the road nomenclature. The recall/precision perfor-
mance of the detector is 0.89/0.97.
6.2 Finding the Participants
The system uses the detected noun groups to iden-
tify the physical objects, which are involved in the
accident. It extracts the headword of each group and
associates it to an entity in the ontology. We used
parts of the Swedish WordNet as a resource to de-
velop this dictionary (A?ke Viberg et al, 2002).
We track the entities along the text with a sim-
ple coreference resolution algorithm. It assumes
that each definite expression corefers with the last
sortally consistent (according to the ontology) en-
tity which was mentioned. Indefinite expressions
are assumed to be references to previously unmen-
tioned entities. This is similar to the algorithm men-
tioned in (Appelt and Israel, 1999). Although this
approach is relatively simple, we get reasonable re-
sults with it and could use it as a baseline when in-
vestigating other approaches.
Figure 3 shows an excerpt from a text with the
annotation of the participants as well as their coref-
erences.
Olyckan intra?ffade na?r [bilen]1 som de fem
fa?rdades i ko?rde om [en annan personbil]2 . Na?r
[den]1 sva?ngde tillbaka in framfo?r [den omko?rda
bilen]2 fick [den]1 sladd och for med sidan rakt
mot fronten pa? [den mo?tande lastbilen]3 .
The accident took place when [the car]1 where the
five people were traveling overtook [another car]2.
When [it]1 pulled in front of [the overtaken car]2,
[it]1 skidded and hit with its side the front of [the
facing truck]3.
Figure 3: A sentence where references to road ob-
jects have been marked.
6.3 Resolution of Metonymy
Use of metonymy, such as alternation between the
driver and his vehicle, is frequent in the Swedish
press clippings. An improper resolution of it intro-
duces errors in the templates and in the visualiza-
tion. It can create independently moving graphic
entities i.e. the vehicle and its driver, that should be
represented as one single object, a moving vehicle,
or stand together.
We detect the metonymic relations between
drivers and their vehicles. We use either cue phrases
like lastbilschauffo?ren (?the truck driver?) or the lo-
cation or instrument semantic roles in phrases like
Mannen som fa?rdades i lastbilen (?The man who
was traveling in the truck?). We then apply con-
straints on the detected events and directions to ex-
clude wrong candidates. For example, given the
phrase Mannen krockade med en traktor (?The man
collided with a tractor?), we know that the man can-
not be the driver of the tractor.
We do not yet handle the metonymic relations be-
tween parts of vehicles and the vehicles themselves.
They are less frequent in the texts we have exam-
ined.
6.4 Marking Up the Events
Events in car accident reports correspond to vehicle
motions and collisions. We detect them to be able
to visualize and animate the scene actions. To carry
out the detection, we created a dictionary of words
? nouns and verbs ? depicting vehicle activity and
maneuvers. We use these words to anchor the event
identification as well as the semantic roles of the
dependents to determine the event arguments.
6.4.1 Detecting the Semantic Roles
Figure 4 shows a sentence that we translated from
our corpus of news texts, where the groups have
been marked up and labeled with semantic roles.
[En personbil]Actor ko?rde [vid femtiden]T ime
[pa? torsdagseftermiddagen]T ime [in i ett rad-
hus]V ictim [i ett a?ldreboende]Loc [pa? Alva?gen]Loc
[i Enebyberg]Loc [norr om Stockholm]Loc .
[About five]T ime [on Thursday afternoon]T ime , [a
car]Actor crashed [into a row house]V ictim [in an
old people?s home]Loc [at Alva?gen street]Loc [in
Enebyberg]Loc [north of Stockholm]Loc.
Figure 4: A sentence tagged with semantic roles.
Gildea and Jurafsky (2002) describe an algorithm
to label automatically semantic roles in a general
context. They use the semantic frames and associ-
ated roles defined in FrameNet (Baker et al, 1998)
and train their classifier on the FrameNet corpus.
They report a performance of 82 percent.
Carsim uses a classification algorithm similar to
the one described in this paper. However, as there is
no lexical resource such as FrameNet for Swedish
and no widely available parser, we adapted it. Our
classifier uses a more local strategy as well as a dif-
ferent set of attributes.
The analysis starts from the words in our dictio-
nary for which we designed a specific set of frames
and associated roles. The classifier limits the scope
of each event to the clause where it appears. It iden-
tifies the verb and nouns dependents: noun groups,
prepositional groups, and adverbs that it classifies
according to semantic roles.
The attributes of the classifier are:
? Target word: the keyword denoting the event.
? Head word: the head word of the group to be
classified.
? Syntactic class of head word: noun group,
prepositional group, or adverb.
? Voice of the target word: active or passive.
? Domain-specific semantic type: Dynamic ob-
ject, static object, human, place, time, cause,
or speed.
The classifier chooses the role, which maximizes
the estimated probability of a role given the values
of the target, head, and semantic type attributes:
P? (r|t, head, sem) = C(r, t, head, sem)C(t, head, sem) .
If a particular combination of target, head, and
semantic type is not found in the training set, the
classifier uses a back-off strategy, taking the other
attributes into account.
We annotated manually a set of 819 examples on
which we trained and tested our classifier. We used
a random subset of 100 texts as a test set and the
rest as a learning set. On the test set, the classi-
fier achieved an accuracy of 90 percent. A classi-
fier based on decision trees built using the ID3 algo-
rithm with gain ratio measure yielded roughly the
same performance.
The value of the semantic type attribute is set us-
ing domain knowledge. Removing this attribute de-
graded the performance of the classifier to 80 per-
cent.
6.4.2 Interpreting the Events
When the events have been detected in the text, they
can be represented and interpreted in the formal de-
scription of the accidents.
We observed that event coreferences are very fre-
quent in longer texts: A same action like a colli-
sion is repeated in several places in the text. As
for metonymy, duplicated events in the template en-
tails a wrong visualization. We solve it through the
unification of as many events as possible, taking
metonymy relations into account, and we remove
the duplicates.
6.5 Time Processing and Event Ordering
In some texts, the order in which events are men-
tioned does not correspond to their chronological
order. To address this issue and order the events cor-
rectly, we developed a module based on the generic
TimeML framework (Pustejovsky et al, 2002). We
use a machine learning approach to annotate the
whole set of events contained in a text and from this
set, we extract events used specifically by the Car-
sim template ? the Carsim events.
TimeML has tags for time expressions (today),
?signals? indicating the polarity (not), the modal-
ity (could), temporal prepositions and connectives
such as for, during, before, after, events (crashed,
accident), and tags that indicate relations between
entities. Amongst the relations, the TLINKs are
the most interesting for our purposes. They ex-
press temporal relations between time expressions
and events as well as temporal relations between
pairs of events.
We developed a comprehensive phrase-structure
grammar to detect the time expressions, signals, and
TimeML events and to assign values to the enti-
ties? attributes. The string den tolfte maj (?May
12th?) is detected as a time expression with the
attribute value=?YYYY-05-12?. We extended the
TimeML attributes to store the events? syntactic fea-
tures. They include the part-of-speech annotation
and verb group structure, i.e. auxiliary + participle,
etc.
We first apply the PS rules to detect the time ex-
pressions, signals, and events. Let e1, e2, e3, ...,
en be the events in the order they are mentioned
in a text. We then generate TLINKs to relate these
events together using a set of decision trees.
We apply three decision trees on se-
quences of two to four consecutive events
(ei, ei+1, [, ei+2[, ei+3]]), with the constraint
that there is no time expression between them,
as they might change the temporal ordering sub-
stantially. The output of each tree is the temporal
relation holding between the first and last event
of the considered sequence, i.e. respectively:
adjacent pairs (ei, ei+1), pairs separated by one
event (ei, ei+2), and by two events (ei, ei+3). The
possible output values are simultaneous, after,
before, is included, includes, and none. As a result,
each event is linked by TLINKs to the three other
events immediately after and before it.
We built automatically the decision trees using
the ID3 algorithm (Quinlan, 1986). We trained them
on a set of hand-annotated examples, which consists
of 476 events and 1,162 TLINKs.
As a set of features, the decision trees use certain
attributes of the events considered, temporal signals
between them, and some other parameters such as
the number of tokens separating the pair of events
to be linked. The complete list of features with x
ranging from 0 to 1, 0 to 2, and 0 to 3 for each tree
respectively, and their possible values is:
? Eventi+xTense: none, past, present, future,
NOT DETERMINED.
? Eventi+xAspect: progressive, per-
fective, perfective progressive, none,
NOT DETERMINED.
? Eventi+xStructure: NOUN,
VB GR COP INF, VB GR COP FIN,
VB GR MOD INF, VB GR MOD FIN,
VB GR, VB INF, VB FIN.
? temporalSignalInbetween: none, before, after,
later, when, still, several.
? tokenDistance: 1, 2 to 3, 4 to 6, 7 to 10, greater
than 10.
? sentenceDistance: 0, 1, 2, 3, 4, greater than 4.
? punctuationSignDistance: 0, 1, 2, 3, 4, 5,
greater than 5.
The process results in an overgeneration of links.
The reason for doing this is to have a large set of
TLINKs to ensure a fine-grained ordering of the
events. As the generated TLINKs can be conflict-
ing, we assign each of them a score, which is de-
rived from the C4.5 metrics (Quinlan, 1993).
We complement the decision trees with heuris-
tics and hints from the event interpreter that events
are identical. Heuristics represent common-sense
knowledge and are encoded as nine production
rules. An example of them is that an event in the
present tense is after an event in the past tense.
Event identity and heuristics enable to connect
events across the time expressions. The TLINKs
generated by the rules also have a score that is rule
dependent.
When all TLINKs are generated, we resolve tem-
poral loops by removing the TLINK with the lowest
score within the loop. Finally, we extract the Carsim
events from the whole set of TimeML events and we
order them using the relevant TLINKs.
6.6 Detecting the Roads
The configuration of roads is inferred from the in-
formation in the detected events. When one of the
involved vehicles makes a turn, this indicates that
the configuration is probably a crossroads.
Additional information is provided using key-
word spotting in the text. Examples of relevant key-
words are korsning (?crossing?), ?rondell? (?round-
about?) and kurva (?bend?), which are very likely
indicators of the road configuration if seen in the
text.
These methods are very simple, but the cases
where they fail are quite rare. During the evalua-
tion described below, we found no text where the
road configuration was misclassified.
7 Evaluation of the Information
Extraction Module
To evaluate the performance of the information ex-
traction component, we applied it to 50 previously
unseen texts, which were collected from newspaper
sources on the web. The size of the texts ranged
from 31 to 459 words. We calculated precision and
recall measures for detection of road objects and for
detection of events. A road object was counted as
correctly detected if there was a corresponding ob-
ject in the text, and the type of the object was cor-
rect. The same criteria apply to the detection of
events, but here we also add the criterion that the
actor (and victim, where this applies) must be cor-
rect. The performance figures are shown in Tables 1
and 2.
Total number of objects in the texts 105
Number of detected objects 110
Number of correctly detected objects 94
Precision 0.85
Recall 0.90
F-measure (? = 1) 0.87
Table 1: Statistics for the detection of road objects
in the test set.
Total number of events in the texts 92
Number of detected events 91
Number of correctly detected events 71
Precision 0.78
Recall 0.77
F-measure (? = 1) 0.78
Table 2: Statistics for the detection of events in the
test set.
The system was able to extract or infer all rele-
vant information correctly in 23 of the 50 texts. In
order to find out the causes of the errors, we investi-
gated what simplifications of the texts needed to be
Figure 5: Planning the trajectories.
made to make the system produce a correct analysis.
The result of this investigation is shown in Table 3.
Object coreference 6
Role labeling 5
Metonymy 5
Clause segmentation 3
Representational expressivity 3
Unknown objects 2
Event detection 2
Unknown event 1
Tagger error 1
PP attachment 1
Table 3: Causes of errors.
8 Scene Synthesis and Visualization
The visualizer reads its input from the formal de-
scription. It synthesizes a symbolic 3D scene and
animates the vehicles. We designed the graphic el-
ements in the scene with the help of traffic safety
experts.
The scene generation algorithm positions the
static objects and plans the vehicle motions. It uses
rule-based modules to check the consistency of the
description and to estimate the 3D start and end co-
ordinates of the vehicles.
The visualizer uses a planner to generate the vehi-
cle trajectories. A first module determines the start
and end positions of the vehicles from the initial di-
rections, the configuration of the other objects in the
scene, and the chain of events as if they were no ac-
cident. Then, a second module alters these trajecto-
ries to insert the collisions according to the accident
slots in the accident representation (Figure 5).
This two-step procedure can be justified by the
descriptions found in most reports. The car drivers
generally start the description of their accident as if
it were a normal movement, which is subsequently
been modified by the abnormal conditions of the ac-
cident.
Finally, the temporal module of the planner as-
signs time intervals to all the segments of the trajec-
tories.
Figure 6 shows two screenshots that the Carsim
visualizer produces for the text above. It should be
noted that the graphic representation is intended to
be iconic in order not to convey any meaning which
is not present in the text.
9 Conclusion and Perspectives
We have presented an architecture and a strategy
based on information extraction and a symbolic vi-
sualization that enable to convert real texts into 3D
scenes. We have obtained promising results that val-
idate our approach. They show that the Carsim ar-
chitecture is applicable to Swedish and other lan-
guages. As far as we know, Carsim is the only
text-to-scene conversion system working on non-
invented narratives.
We are currently improving Carsim and we hope
in future work to obtain better results in the reso-
lution of coreferences. We are implementing and
adapting algorithms such as the one described in
(Soon et al, 2001) to handle this. We also intend
to improve the visualizer to handle more complex
scenes and animations.
The current aim of the Carsim project is to visu-
alize the content of a text as accurately as possible,
with no external knowledge. In the future, we would
like to integrate additional knowledge sources in or-
der to make the visualization more realistic and un-
derstandable. Geographical and meteorological in-
formation systems are good examples of this, which
could be helpful to improve the realism. Another
topic, which has been prominent in our discussions
with traffic safety experts, is how to reconcile dif-
ferent narratives that describe a same accident.
In our work on the information extraction mod-
ule, we have concentrated on the extraction of data
which are relevant for the visual reconstruction of
the scene. We believe that the information extrac-
tion component could be interesting in itself to ex-
tract other relevant data, for example casualty statis-
tics or traffic conditions.
Acknowledgements
We are very grateful to Karin Brundell-Freij, A?se
Svensson, and Andra?s Va?rhelyi, traffic safety ex-
perts at LTH, for helping us in the design the Carsim
template and advising us with the 3D graphic repre-
sentation.
This work is partly supported by grant num-
ber 2002-02380 from the Spra?kteknologi program
of Vinnova, the Swedish Agency of Innovation
Systems.
References
Giovanni Adorni, Mauro Di Manzo, and Fausto
Giunchiglia. 1984. Natural language driven im-
age generation. In Proceedings of COLING 84,
pages 495?500, Stanford, California.
Douglas E. Appelt and David Israel. 1999. In-
troduction to information extraction technology.
Tutorial Prepared for IJCAI-99. Artificial Intelli-
gence Center, SRI International.
Michael Arens, Artur Ottlik, and Hans-Hellmut
Nagel. 2002. Natural language texts for a cogni-
tive vision system. In Frank van Harmelen, edi-
tor, ECAI2002, Proceedings of the 15th European
Conference on Artificial Intelligence, Lyon, July
21-26.
Collin F. Baker, Charles J. Fillmore, and John B.
Lowe. 1998. The Berkeley FrameNet Project. In
Proceedings of COLING-ACL?98, pages 86?90,
Montre?al, Canada.
Johan Carlberger and Viggo Kann. 1999. Imple-
menting an efficient part-of-speech tagger. Soft-
ware Practice and Experience, 29:815?832.
Bob Coyne and Richard Sproat. 2001. Wordseye:
An automatic text-to-scene conversion system.
In Proceedings of the Siggraph Conference, Los
Angeles.
Michel Denis. 1991. Imagery and thinking. In Ce-
sare Cornoldi and Mark A. McDaniel, editors,
Imagery and Cognition, pages 103?132. Springer
Verlag.
Sylvain Dupuy, Arjan Egges, Vincent Legendre,
and Pierre Nugues. 2001. Generating a 3D simu-
lation of a car accident from a written descrip-
tion in natural language: The Carsim system.
In Proceedings of The Workshop on Temporal
and Spatial Information Processing, pages 1?8,
Toulouse, July 7. Association for Computational
Linguistics.
Arjan Egges, Anton Nijholt, and Pierre Nugues.
2001. Generating a 3D simulation of a car ac-
cident from a formal description. In Venetia Gi-
agourta and Michael G. Strintzis, editors, Pro-
ceedings of The International Conference on
Augmented, Virtual Environments and Three-
Dimensional Imaging (ICAV3D), pages 220?223,
Mykonos, Greece, May 30-June 01.
Eva Ejerhed. 1996. Finite state segmentation of
discourse into clauses. In Proceedings of the 12th
European Conference on Artificial Intelligence
(ECAI-96) Workshop on Extended Finite State
Models of Language, Budapest, Hungary.
Daniel Gildea and Daniel Jurafsky. 2002. Auto-
Figure 6: Screenshots from the animation of the text above.
matic labeling of semantic roles. Computational
Linguistics, 28(3):245?288.
Nils-Olof Karlberg. 2003. Field results from
STRADA ? a traffic accident data system telling
the truth. In ITS World Congress, Madrid, Spain,
November 16-20.
Stephen Michael Kosslyn. 1983. Ghosts in the
Mind?s Machine. Norton, New York.
Mauro Di Manzo, Giovanni Adorni, and Fausto
Giunchiglia. 1986. Reasoning about scene de-
scriptions. IEEE Proceedings ? Special Issue on
Natural Language, 74(7):1013?1025.
Lisa Persson and Magnus Danielsson. 2004. Name
extraction in car accident reports for Swedish.
Technical report, LTH, Department of Computer
science, Lund, January.
James Pustejovsky, Roser Saur??, Andrea Setzer, Rob
Gaizauskas, and Bob Ingria. 2002. TimeML An-
notation Guidelines. Technical report.
John Ross Quinlan. 1986. Induction of decision
trees. Machine Learning, 1(1):81?106.
John Ross Quinlan. 1993. C4.5: Programs for Ma-
chine Learning. Morgan Kauffman.
A?ke Viberg, Kerstin Lindmark, Ann Lindvall, and
Ingmarie Mellenius. 2002. The Swedish Word-
Net project. In Proceedings of Euralex 2002,
pages 407?412, Copenhagen.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Edward R. Tufte. 1997. Visual Explanations: Im-
ages and Quantities, Evidence and Narrative.
Graphic Press.
Proceedings of the 2nd Workshop on Building Educational Applications Using NLP,
pages 53?60, Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Direkt Profil: A System for Evaluating Texts of Second Language Learners
of French Based on Developmental Sequences
Jonas Granfeldt1 Pierre Nugues2 Emil Persson1 Lisa Persson2
Fabian Kostadinov3 Malin ?gren1 Suzanne Schlyter1
1Dept. of Romance Languages 2Dept. of Computer Science 3Dept. of Computer Science
Lund University Lund University University of Zurich
Box 201, 221 00 Lund, Sweden Box 118, 221 00 Lund, Sweden CH-8057 Zurich, Switzerland
{Jonas.Granfeldt, Malin.Agren, Suzanne.Schlyter}@rom.lu.se
emil.person@telia.com nossrespasil@hotmail.com
Pierre.Nugues@cs.lth.se fabian.kostadinov@access.unizh.ch
Abstract
Direkt Profil is an automatic analyzer of
texts written in French as a second lan-
guage. Its objective is to produce an eval-
uation of the developmental stage of stu-
dents under the form of a grammatical
learner profile. Direkt Profil carries out
a sentence analysis based on developmen-
tal sequences, i.e. local morphosyntactic
phenomena linked to a development in the
acquisition of French.
The paper presents the corpus that we use
to develop the system and briefly, the de-
velopmental sequences. Then, it describes
the annotation that we have defined, the
parser, and the user interface. We con-
clude by the results obtained so far: on the
test corpus the systems obtains a recall of
83% and a precision of 83%.
1 Introduction
With few exceptions, systems for evaluating lan-
guage proficiency and for Computer-Assisted Lan-
guage Learning (CALL) do not use Natural Lan-
guage Processing (NLP) techniques. Typically, ex-
isting commercial and non-commercial programs
apply some sort of pattern-matching techniques to
analyze texts. These techniques not only reduce the
quality and the nature of the feedback but also limit
the range of possible CALL applications.
In this paper, we present a system that imple-
ments an automatic analysis of texts freely written
by learners. Research on Second Language Acqui-
sition (SLA) has shown that writing your own text
in a communicative and meaningful situation with
a feedback and/or an evaluation of its quality and
its form constitutes an excellent exercise to develop
second language skills.
The aim of the program, called Direkt Profil, is
to evaluate the linguistic level of the learners? texts
in the shape of a learner profile. To analyze sen-
tences, the program relies on previous research on
second language development in French that item-
ized a number of specific constructions correspond-
ing to developmental sequences.
2 The CEFLE Lund Corpus
For the development and the evaluation of the sys-
tem, we used the CEFLE corpus (Corpus ?crit de
Fran?ais Langue ?trang?re de Lund ?Lund Written
Corpus of French as a Foreign Language?). This
corpus currently contains approximately 100,000
words (?gren, 2005). The texts are narratives of
varied length and levels. We elicited them by ask-
ing 85 Swedish high-school students and 22 young
French to write stories evoked by a sequence of im-
ages. Figure 1 shows pictures corresponding to one
of them: Le voyage en Italie ?The journey to Italy?.
The goal of the system being to analyze French as
a foreign language, we used the texts of the French
native speakers as control group.
The following narrative is an example from a be-
53
ginner learner:
Elles sont deux femmes. Elles sont a
italie au une vacanse. Mais L?Auto est
tr?s petite. Elles va a Italie. Au l?hothel
elles demande une chambre. Un homme a
le cl?. Le chambre est grande avec deux
lies. Il fait chaud. C?est noir. Cette
deux femmes est a une restaurang. Dans
une bar cet deux hommes. Ils amour les
femmes. Ils parlons dans la bar. Ils ont
tres bien. Le homme et la femme partic-
ipat a un sightseeing dans la Rome. Ils
achetons une robe. La robe est verte. La
femme et l?homme reste au un banqe. Ils
c?est amour. La femme et l?homme est
au une ristorante. es hommes va avec les
femmes. L?auto est petite.
This text contains a certain number of typical
constructions for French as a foreign language:
parataxis, very simple word order, absence of ob-
ject pronouns, basic verb forms, agreement errors,
spelling mistakes. Research on the acquisition of
French as a foreign language has shown that these
constructions (and others) appear in a certain sys-
tematic fashion according to the proficiency level of
the learners. With Direkt Profil, we aim at detecting
automatically these structures and gathering them
so that they represent a grammatical learner profile.
This learner profile can ultimately be used to assess
learners? written production in French.
3 Direkt Profil and Previous Work
Direkt Profil is an analyzer of texts written in French
as a foreign language. It is based on the linguistic
constructions that are specific to developmental se-
quences. We created an annotation scheme to mark
up these constructions and we used it to describe
them systematically and detect them automatically.
The analyzer parses the text of a learner, annotates
the constructions, and counts the number of occur-
rences of each phenomenon. The result is a text pro-
file based on these criteria and, possibly, an indica-
tion of the level of the text. A graphical user inter-
face (GUI) shows the results to the user and visual-
izes by different colors the detected structures. It is
important to stress that Direkt Profil is not a gram-
mar checker.
The majority of the tools in the field can be de-
scribed as writing assistants. They identify and
sometimes correct spelling mistakes and grammat-
ical errors. The line of programs leading to PLNLP
(Jensen et al, 1993) and NLPWin (Heidorn, 2000)
is one of the most notable achievements. The gram-
matical checker of PLNLP carries out a complete
parse. It uses binary phrase-structure rules and takes
into account some dependency relations. PLNLP is
targeted primarily, but not exclusively, to users writ-
ing in their mother tongue. It was created for En-
glish and then applied to other languages, including
French.
Other systems such as FreeText (Granger et al,
2001) and Granska (Bigert et al, 2005) are rele-
vant to the CALL domain. FreeText is specifically
designed to teach language and adopts a interactive
approach. It uses phrase-structure rules for French.
In case of parsing failure, it uses relaxed constraints
to diagnose an error (agreement errors, for exam-
ple). Granska, unlike FreeText, carries out a par-
tial parsing. The authors justify this type of analysis
by a robustness, which they consider superior and
which makes it possible to accept more easily incor-
rect sentences.
4 An Analysis Based on Developmental
Sequences
The current systems differ with regard to the type
of analysis they carry out: complete or partial. The
complete analysis of sentences and the correction of
errors are difficult to apply to texts of learners with
(very) low linguistic level since the number of un-
known words and incorrect sentences are often ex-
tremely high.
We used a test corpus of 6,842 words to evalu-
ate their counts. In the texts produced by learners
at the lowest stage of development, Stage 1, nearly
100% of the sentences contained a grammatical er-
ror (98.9% were incorrect1) and 24.7% of the words
were unknown.2 At this stage of development, any
complete analysis of the sentences seems very diffi-
cult to us. On the other hand, in the control group the
1An ?incorrect sentence? was defined as a sentence contain-
ing at least one spelling, syntactic, morphological, or semantic
error.
2An ?unknown word? is a token that does not appear in the
lexicon employed by the system (ABU CNAM, see below)
54
Figure 1: Le voyage en Italie ?The journey to Italy?.
corresponding figures are 32.7% for incorrect sen-
tences and 10.6% for unknown words. More impor-
tantly, this analysis shows that using a quantification
of ?unknown words? and ?incorrect sentences? only
is insufficient to define the linguistic level of learn-
ers? texts. Learners at Stage 3 have in fact fewer in-
correct sentences than learners from Stage 4 (70.5%
vs. 80.2%). Moreover, the percentage of unknown
words in the control group (the natives) is slightly
higher than that of learners from the Stage 4 (10.6%
vs. 10.4%). Thus, the simple count of errors is
also insufficient to distinguish more advanced learn-
ers from natives. To identify properly and to define
learners of various linguistic levels, we need more
detailed analyses and more fine-grained measures.
This is exactly the purpose of the developmental se-
quences and learner profiles implemented in Direkt
Profil.
5 Developmental Sequences in French
Direkt Profil carries out an analysis of local phenom-
ena related to a development in the acquisition of
French. These phenomena are described under the
form of developmental sequences. The sequences
are the result of empirical observations stemming
from large learner corpora of spoken language (Bart-
ning and Schlyter, 2004). They show that certain
grammatical constructions are acquired and can be
produced in spontaneous spoken language in a fixed
order. Clahsen and al. (1983) as well as Piene-
mann and Johnston, (1987) determined developmen-
tal sequences for German and spoken English. For
spoken French, Schlyter (2003) and Bartning and
Schlyter (2004) proposed 6 stages of development
and developmental sequences covering more than 20
local phenomena. These morphosyntactic phenom-
ena are described under the form of local structures
inside the verbal or nominal domain. Table 1 shows
a subset of these phenomena. It is a matter of current
debate in field of SLA to what extent these devel-
opmental sequences are independent of the mother
tongue.
The horizontal axis indicates the temporal devel-
opment for a particular phenomenon: The develop-
mental sequence. The vertical axis indicates the set
of grammatical phenomena gathered in such way
that they make up a ?profile? or a stage of acqui-
sition. To illustrate better how this works, we will
compare the C (finite verb forms in finite contexts)
and G (object pronouns) phenomena.
55
At Stage 1, the finite and infinitive forms coexist
in finite contexts. As the main verb of the sentence,
we find in the learners? production je parle (tran-
scription of /je parl/ analyzed as a ?finite form?) as
well as /je parle/ i.e. *je parler or *je parl?. The cur-
rent estimation is that in Stage 1, there are between
50 and 75% of finite forms in finite contexts. At
Stage 4, the percentage of finite forms has increased
to 90?98%. For this morphological phenomenon,
the developmental sequence describes a successive
?morphologization?.
The G phenomenon concerns the developmental
sequence of object pronouns. The first object pro-
nouns are placed in a postverbal position according
to the scheme Subject-Verb-Object (SVO), e.g. *je
vois le/la/lui (instead of je le/la vois). At Stage 3,
learners can produce phrases according to the SvOV
scheme (Pronoun-Auxiliary-Object-Verb): Je veux
le voir (correct) but also *j?ai le vu (incorrect). At
Stage 5, we observe je l?ai vu. For this syntactic phe-
nomenon, the developmental sequence describes a
change in the linear organization of the constituents.
6 Annotation
The concept of group, either noun group or verb
group, correct or not, represents the essential gram-
matical support of our annotation. The majority of
syntactic annotation standards for French takes such
groups into account in one way or another. Gendner
et al (2004) is an example that reconciles a great
number of annotations. These standards are how-
ever insufficient to mark up all the constructions in
Table 1.
We defined a text annotation specific to Direkt
Profil based on the inventory of the linguistic phe-
nomena described by Bartning and Schlyter (2004)
(Table 1). We represented these phenomena by de-
cision trees whose final nodes correspond to a cate-
gory of analysis.
The annotation uses the XML format and anno-
tates the texts using 4 layers. Only the 3rd layer is
really grammatical:
? The first layer corresponds to the segmentation
of the text in words.
? The second layer annotates prefabricated ex-
pressions or sentences (e.g. je m?appelle).
These structures correspond to linguistic ex-
pressions learned ?by heart? in a holistic fash-
ion. It has been shown that they have a great
importance in the first years of learning French.
? The third layer corresponds to a chunk anno-
tation of the text, restricted to the phenomena
to identify. This layer marks up simulta-
neously each word with its part-of-speech
and the verb and noun groups to which they
belong. The verb group incorporates subject
clitic pronouns. The XML element span
marks the groups and features an attribute
to indicate their class in the table. The tag
element annotates the words with attributes to
indicate the lemma, the part-of-speech, and
the grammatical features. The verb group in
the sentence Ils parlons dans la bar extracted
from the learner text above is annotated as:
<span class="p1_t1_c5131"><tag
pos="pro:nom:pl:p3:mas">Ils</tag>
<tag pos="ver:impre:pl:p1">
parlons </tag></span> dans la
bar. The class denoted p1_t1_c5131
corresponds to a ?finite lexical verb, no
agreement?.
? The fourth layer counts structures typical of an
acquisition stage. It uses the counter XML
element, <counter id="counter.2"
counter_name="passe_compose"
rule_id="participe_4b"
value="1"/>.
7 Implementation
The running version of Direkt Profil is restricted to
the analysis of the verb groups and clitic pronouns.
For each category in Table 1, the program identifies
the corresponding constructions in a text and counts
them.
The analyzer uses manually written rules and a
lexicon of inflected terms. The variety of the con-
structions contained in the corpus is large and in or-
der not to multiply the number of rules, we chose
a constraint reinforcement approach. Conceptually,
the analyzer seeks classes of phrase structures in
which all the features are removed. It gradually
identifies the structures while varying the feature
56
Ph. Stages 1 2 3 4 5 6
A. % of sentences
containing a verb
(in a conversa-
tion)
20?40% 30?40% 50% 60% 70% 75%
B. % of lexical
verbs showing
+/-finite opposi-
tion (types)
No opp.;
% in finite
forms
1?3sg
10?20%
of types in
opposition
About 50%
in opposi-
tion
Most in op-
position
All in op-
position
+
C. % of finite
forms of lexical
verbs in oblig-
atory contexts
(occurrences)
Finite
forms
50%?75%
Finite
forms
70?80%
Finite
forms:
80?90%
Finite
forms:
90?98%
Finite
forms:
100%
+
D. 1st, 2nd, 3rd
pers. sing.
(copula/aux)
est, a, va
No opposi-
tion:
J?ai/ c?est
Opposition
j?ai ? il a
je suis ? il
est
Isolated er-
rors *je va,
*je a
+ + +
E. % of 1st pers.
plural S-V agree-
ment nous V-ons
(occurrences)
? 70?80% 80?95% Errors in
complex
construc-
tions
+ +
F. 3rd pers. plural
S-V agreement
with viennent,
veulent, pren-
nent
? ?
ils *prend
Isolated
cases of
agreement
50% of
cases with
agreement
Some
problems
remain
+
G. Object pronouns
(placement)
? SVO S(v)oV SovV
appears
Productive + (y, en)
H. % of gender
agreement
Article-Noun
(occurrences)
55?75% 60?80% 65?85% 70?90% 75?95% 90?
100%
Table 1: Developmental sequences adapted from Schlyter (2003); Bartning and Schlyter (2004).
Legend: ? = no occurrences; + = acquired at a native-like level; aux = auxiliary; pers. = person; S-V =
Subject-Verb
57
values. The recognition of the group boundaries is
done by a set of closed-class words and heuristics
inside the rules. It thus follows an old but robust
strategy used in particular by Vergne (1999), inter
alia, for French.
Direkt Profil applies a cascade of three sets of
rules to produce the four annotation layers. The
first unit segments the text in words. An interme-
diate unit identifies the prefabricated expressions.
The third unit annotates simultaneously the parts-of-
speech and the groups. Finally, the engine creates a
group of results and connects them to a profile. It
should be noted that the engine neither annotates all
the words, nor all segments. It considers only those
which are relevant for the determination of the stage.
The engine applies the rules from left to right then
from right to left to solve certain problems of agree-
ment.
The rules represent partial structures and are di-
vided into a condition part and an action part. The
condition part contains the search parameters. It can
be a lemma, a regular expression, or a class of inflec-
tion. The engine goes through the text and applies
the rules using a decision tree. It tests the condition
part to identify the sequences of contiguous words.
Each rule produces a positive (?match?) or negative
(?no match?) result. The rules are applied accord-
ing to the result of the condition part and annotate
the text, count the number of occurrences of the phe-
nomenon, and connect to another rule. By traversing
the nodes of the tree, the engine memorizes the rules
it has passed as well as the results of the condition
parts of these rules. When arriving at a final node,
the engine applies the action parts of all the rules.
The engine finds the words in a dictionary of
inflected terms. It does not correct the spelling
mistakes except for the accents and certain stems.
Learners frequently build erroneous past participles
inferring a wrong generalization of stems. An exam-
ple is the word *prendu (taken) formed on the stem
prend|re and of the suffix -u.
We used a lexicon available from the As-
sociation des Bibliophiles Universels? web site
(http://abu.cnam.fr/) that we corrected and trans-
posed into XML. We also enriched it with verb
stems.
8 Interface
Direkt Profil merges the annotation levels in a result
object. This object represents the original text, the
annotation, the trace of the rule application, and the
counters. The result object, which can be saved, is
then transformed by the program to be presented to
the user. The display uses the XHTML 1.1 spec-
ifications which can be read by any Web browser.
Direkt Profil has a client-server architecture where
the server carries out the annotation of a text and the
client collects the text with an input form and inter-
acts with the user.
Figure 2 shows a screenshot of Direkt Profil?s
GUI displaying the analysis of the learner text
above. The interface indicates to the user by dif-
ferent colors all the structures that the analyzer de-
tected.
9 Results and Evaluation
We evaluated Direkt Profil with a subset of the CE-
FLE corpus. We chose 20 texts randomly distributed
on 4 learner stages. We also used 5 texts coming
from the control group. In this version, we did not
test the correction of the misspelled words: accent
and stems. Table 2 shows some statistics on the size
of the texts and Table 3 shows the results in the form
of recall and precision.
The results show that Direkt Profil detects well
the desired phenomena. It reveals also interesting
differences according to the levels of the texts. The
results show that Direkt Profil analyzes better the
learner texts than the texts from the native French
adolescents (control group). Without knowing ex-
actly why, we note that it suggests that the adopted
strategy, which aims at analyzing texts in French as
a foreign language, seems promising.
10 Conclusion and Future Work
We presented a system carrying out a machine anal-
ysis of texts based on developmental sequences. The
goal is to produce a learner profile. We built a parser
and developed a set of rules to annotate the texts. Di-
rekt Profil is integrated in a client-server architecture
and has an interface allowing the interaction with the
user.
The results show that it is possible to describe the
vast majority of the local structures defined by the
58
Figure 2: The graphical user interface.
Stage 1 Stage 2 Stage 3 Stage 4 Control Total
Number of analyzed texts 5 5 5 5 5 25
Word count 740 1233 1571 1672 1626 6842
Sentence count 85 155 166 126 107 639
Average text length (in words) 148 247 314 334 325 274
Average length of sentences (in words) 8.7 7.9 9.5 13.3 15.2 10.9
Table 2: Test corpus.
Stage 1 Stage 2 Stage 3 Stage 4 Control Total
Reference structures 23 97 101 119 85 425
Detected structures 27 98 100 112 92 429
Correctly detected structures 15 81 89 96 73 354
Non detected structures 5 16 12 20 11 ()64
Overdetected structures 10 17 11 17 19 ()74
Recall 65% 84% 88% 81% 86% 83%
Precision 56% 83% 89% 86% 79% 83%
F-measure 0.6 0.83 0.89 0.83 0.82 0.83
Table 3: Results.
59
developmental sequences under the form of rules.
Direkt Profil can then detect them and automatically
analyze them. We can thus check the validity of the
acquisition criteria.
In the future, we intend to test Direkt Profil in
teaching contexts to analyze and specify, in an au-
tomatic way, the grammatical level of a learner. The
program could be used by teachers to assess student
texts as well as by the students themselves as a self-
assessment and as a part of their learning process.
A preliminary version of Direkt Pro-
fil is available on line from this address
http://www.rom.lu.se:8080/profil
References
Malin ?gren. 2005. Le marquage morphologique
du nombre dans la phrase nominale. une ?tude sur
l?acquisition du fran?ais L2 ?crit. Technical report, In-
stitut d??tudes romanes de Lund. Lund University.
Inge Bartning and Suzanne Schlyter. 2004. Stades et
itin?raires acquisitionnels des apprenants su?dophones
en fran?ais l2. Journal of French Language Studies,
14(3):281?299.
Johnny Bigert, Viggo Kann, Ola Knutsson, and Jonas
Sj?bergh. 2005. Grammar checking for Swedish sec-
ond language learners. In CALL for the Nordic Lan-
guages, Copenhagen Studies in Language, pages 33?
47. Copenhagen Business School, Samfundslitteratur.
Harald Clahsen, J?rgen M. Meisel, and Manfred Piene-
mann. 1983. Deutsch als Fremdsprache. Der
Spracherwerb ausl?ndischer Arbeiter. Narr, T?bin-
gen.
V?ronique Gendner, Anne Vilnat, Laura Mon-
ceaux, Patrick Paroubek, and Isabelle Robba.
2004. Les annotations syntaxiques de r?f?rence
peas. Technical report, LIMSI, Orsay.
http://www.limsi.fr/Recherche/CORVAL/easy/
PEAS_reference_annotations_v1.6.html.
Sylviane Granger, Anne Vandeventer, and Marie-Jos?e
Hamel. 2001. Analyse de corpus d?apprenants pour
l?ELAO bas? sur le TAL. Traitement automatique des
langues, 42(2):609?621.
George E. Heidorn. 2000. Intelligent writing assistance.
In Robert Dale, Hermann Moisl, and Harold Somers,
editors, Handbook of Natural Language Processing.
Marcel Dekker.
Karen Jensen, George E. Heidorn, and Stephen D.
Richardson. 1993. Natural Language Processing:
The PLNLP Approach. Kluwer Academic Publishers.
Manfred Pienemann and Malcolm Johnston. 1987. Fac-
tors influencing the development of second language
proficiency. In David Nunan, editor, Applying sec-
ond language acquisition research, pages 45?141. Na-
tional Curriculum Resource Centre, Adelaide.
Suzanne Schlyter. 2003. Stades de d?veloppe-
ment en fran?ais L2. Technical report, Insti-
tut d??tudes romanes de Lund, Lund University.
http://www.rom.lu.se/durs/
STADES_DE_DEVELOPPEMENT_EN
_FRANCAIS_L2.PDF.
Jacques Vergne. 1999. ?tude et mod?lisation de la
syntaxe des langues ? l?aide de l?ordinateur. Analyse
syntaxique automatique non combinatoire. Synth?se et
R?sultats. Habilitation ? diriger des recherches, Uni-
versit? de Caen, 29 septembre.
60
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 177?180, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Sparse Bayesian Classification of Predicate Arguments
Richard Johansson and Pierre Nugues
LUCAS, Department of Computer Science, Lund University
Box 118
SE-221 00 Lund, Sweden
{richard, pierre}@cs.lth.se
Abstract
We present an application of Sparse
Bayesian Learning to the task of semantic
role labeling, and we demonstrate that this
method produces smaller classifiers than
the popular Support Vector approach.
We describe the classification strategy and
the features used by the classifier. In par-
ticular, the contribution of six parse tree
path features is investigated.
1 Introduction
Generalized linear classifiers, in particular Support
Vector Machines (SVMs), have recently been suc-
cessfully applied to the task of semantic role iden-
tification and classification (Pradhan et al, 2005),
inter alia.
Although the SVM approach has a number of
properties that make it attractive (above all, excel-
lent software packages exist), it also has drawbacks.
First, the resulting classifier is slow since it makes
heavy use of kernel function evaluations. This is
especially the case in the presence of noise (since
each misclassified example has to be stored as a
bound support vector). The number of support vec-
tors typically grows with the number of training ex-
amples. Although there exist optimization methods
that speed up the computations, the main drawback
of the SVM approach is still the classification speed.
Another point is that it is necessary to tune the
parameters (typically C and ?). This makes it nec-
essary to train repeatedly using cross-validation to
find the best combination of parameter values.
Also, the output of the decision function of the
SVM is not probabilistic. There are methods to map
the decision function onto a probability output using
the sigmoid function, but they are considered some-
what ad-hoc (see (Tipping, 2001) for a discussion).
In this paper, we apply a recent learning
paradigm, namely Sparse Bayesian learning, or
more specifically the Relevance Vector learning
method, to the problem of role classification. Its
principal advantages compared to the SVM ap-
proach are:
? It typically utilizes fewer examples compared
to the SVM, which makes the classifier faster.
? It uses no C parameter, which reduces the need
for cross-validation.
? The decision function is adapted for probabilis-
tic output.
? Arbitrary basis functions can be used.
Its significant drawback is that the training pro-
cedure relies heavily on dense linear algebra, and is
thus difficult to scale up to large training sets and
may be prone to numerical difficulties.
For a description of the task and the data, see (Car-
reras and M?rquez, 2005).
2 Sparse Bayesian Learning and the
Relevance Vector Machine
The Sparse Bayesian method is described in detail in
(Tipping, 2001). Like other generalized linear learn-
ing methods, the resulting binary classifier has the
form
signf(x) = sign
m
?
i=1
?
i
f
i
(x) + b
177
where the f
i
are basis functions. Training the
model then consists of finding a suitable ? =
(b, ?
1
, . . . , ?
m
) given a data set (X,Y ).
Analogous with the SVM approach, we can let
f
i
(x) = k(x, x
i
), where x
i
is an example from the
training set and k a function. We have then arrived
at the Relevance Vector Machine (RVM). There are
however no restrictions on the function k (such as
Mercer?s condition for SVM). We use the Gaussian
kernel k(x, y) = exp(???x ? y?2) throughout this
work.
We first model the probability of a positive ex-
ample as a sigmoid applied to f(x). This can be
used to write the likelihood function P (Y |X,?).
Instead of a conventional ML approach (maximiz-
ing the likelihood with respect to ?, which would
give an overfit model), we now adopt a Bayesian
approach and encode the model preferences using
priors on ?. For each ?
i
, we introduce a parame-
ter s
i
and assume that ?
i
? N(0, s?1
i
) (i.e. Gaus-
sian). This is in effect an ?Occam penalty? that en-
codes our preference for sparse models. We should
finally specify the distributions of the s
i
. However,
we make the simplifying assumption that their dis-
tribution is flat (noninformative).
We now find the maximum of the marginal likeli-
hood, or ?evidence?, with respect to s, that is
p(Y |X, s) =
?
P (Y |X,?)p(?|s)d?.
This integral is not tractable, hence we approximate
the integrand using a Gaussian centered at the mode
of the integrand (Laplace?s approximation). The
marginal likelihood can then be differentiated with
respect to s, and maximized using iterative methods
such as gradient descent.
The algorithm thus proceeds iteratively as fol-
lows: First maximize the penalized likelihood func-
tion P (Y |X,?)p(?|s) with respect to ? (for ex-
ample via the Newton-Raphson method), then up-
date the parameters s
i
. This goes on until a con-
vergence criterion is met, for example that the s
i
changes are small enough. During iteration, the s
i
parameters for redundant examples tend to infinity.
They (and the corresponding columns of the kernel
matrix) are then removed from the model. This is
necessary because of numerical stability and also re-
duces the training time considerably.
We implemented the RVM training method using
the ATLAS (Whaley et al, 2000) implementation
of the BLAS and LAPACK standard linear algebra
APIs. To make the algorithm scale up, we used a
working-set strategy that used the results of partial
solutions to train the final classifier. Our implemen-
tation is based on the original description of the al-
gorithm (Tipping, 2001) rather than the greedy opti-
mized version (Tipping and Faul, 2003), since pre-
liminary experiments suggested a decrease in clas-
sification accuracy. Our current implementation can
handle training sets up to about 30000 examples.
We used the conventional one-versus-one method
for multiclass classification. Although the Sparse
Bayesian paradigm is theoretically not limited to bi-
nary classifiers, this is of little use in practice, since
the size of the Hessian matrix (used while maximiz-
ing the likelihood and updating s) grows with the
number of classes.
3 System Description
Like previous systems for semantic role identifica-
tion and classification, we used an approach based
on classification of nodes in the constituent tree.
To simplify training, we used the soft-prune ap-
proach as described in (Pradhan et al, 2005), which
means that before classification, the nodes were fil-
tered through a binary classifier that classifies them
as having a semantic role or not (NON-NULL or
NULL). The NULL nodes missed by the filter were
included in the training set for the final classifier.
Since our current implementation of the RVM
training algorithm does not scale up to large training
sets, training on the whole PropBank was infeasible.
We instead trained the multiclass classifier on sec-
tions 15 ? 18, and used an SVM for the soft-pruning
classifier, which was then trained on the remaining
sections. The excellent LIBSVM (Chang and Lin,
2001) package was used to train the SVM.
The features used by the classifiers can be
grouped into predicate and node features. Of the
node features, we here pay most attention to the
parse tree path features.
3.1 Predicate Features
We used the following predicate features, all of
which first appeared in (Gildea and Jurafsky, 2002).
178
? Predicate lemma.
? Subcategorization frame.
? Voice.
3.2 Node Features
? Head word and head POS. Like most previous
work, we used the head rules of Collins to ex-
tract this feature.
? Position. A binary feature that describes if the
node is before or after the predicate token.
? Phrase type (PT), that is the label of the con-
stituent.
? Named entity. Type of the first contained NE.
? Governing category. As in (Gildea and Juraf-
sky, 2002), this was used to distinguish subjects
from objects. For an NP, this is either S or VP.
? Path features. (See next subsection.)
For prepositional phrases, we attached the prepo-
sition to the PT and replaced head word and head
POS with those of the first contained NP.
3.3 Parse Tree Path Features
Previous studies have shown that the parse tree path
feature, used by almost all systems since (Gildea and
Jurafsky, 2002), is salient for argument identifica-
tion. However, it is extremely sparse (which makes
the system learn slowly) and is dependent on the
quality of the parse tree. We therefore investigated
the contribution of the following features in order
to come up with a combination of path features that
leads to a robust system that generalizes well.
? Constituent tree path. As in (Gildea and Ju-
rafsky, 2002), this feature represents the path
(consisting of step directions and PTs of the
nodes traversed) from the node to the predicate,
for example NP?VP?VB for a typical object.
Removing the direction (as in (Pradhan et al,
2005)) improved neither precision nor recall.
? Partial path. To reduce sparsity, we introduced
a partial path feature (as in (Pradhan et al,
2005)), which consists of the path from the
node to the lowest common ancestor.
? Dependency tree path. We believe that la-
beled dependency paths provide more informa-
tion about grammatical functions (and, implic-
itly, semantic relationships) than the raw con-
stituent structure. Since the grammatical func-
tions are not directly available from the parse
trees, we investigated two approximations of
dependency arc labels: first, the POSs of the
head tokens; secondly, the PTs of the head node
and its immediate parent (such labels were used
in (Ahn et al, 2004)).
? Shallow path. Since the UPC shallow parsers
were expected to be more robust than the full
parsers, we used a shallow path feature. We
first built a parse tree using clause and chunk
bracketing, and the shallow path feature was
then constructed like the constituent tree path.
? Subpaths. All subpaths of the constituent path.
We used the parse trees from Charniak?s parser to
derive all paths except for the shallow path.
4 Results
4.1 Comparison with SVM
The binary classifiers that comprise the one-versus-
one multiclass classifier were 89% ? 98% smaller
when using RVM compared to SVM. However, the
performance dropped by about 2 percent. The rea-
son for the drop is possibly that the classifier uses a
number of features with extremely sparse distribu-
tions (two word features and three path features).
4.2 Path Feature Contributions
To estimate the contribution of each path feature, we
measured the difference in performance between a
system that used all six features and one where one
of the features had been removed. Table 2 shows
the results for each of the six features. For the final
system, we used the dependency tree path with PT
pairs, the shallow path, and the partial path.
4.3 Final System Results
The results of the complete system on the test sets
are shown in Table 1. The smaller training set (as
mentioned above, we used only sections 15 ? 18
179
Precision Recall F
?=1
Development 73.40% 70.85% 72.10
Test WSJ 75.46% 73.18% 74.30
Test Brown 65.17% 60.59% 62.79
Test WSJ+Brown 74.13% 71.50% 72.79
Test WSJ Precision Recall F
?=1
Overall 75.46% 73.18% 74.30
A0 84.56% 85.18% 84.87
A1 73.40% 73.35% 73.37
A2 61.99% 57.30% 59.55
A3 71.43% 46.24% 56.14
A4 72.53% 64.71% 68.39
A5 100.00% 40.00% 57.14
AM-ADV 58.13% 51.58% 54.66
AM-CAU 70.59% 49.32% 58.06
AM-DIR 59.62% 36.47% 45.26
AM-DIS 81.79% 71.56% 76.33
AM-EXT 72.22% 40.62% 52.00
AM-LOC 54.05% 55.10% 54.57
AM-MNR 54.33% 52.91% 53.61
AM-MOD 98.52% 96.73% 97.62
AM-NEG 96.96% 96.96% 96.96
AM-PNC 36.75% 37.39% 37.07
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 76.00% 70.19% 72.98
R-A0 83.33% 84.82% 84.07
R-A1 68.75% 70.51% 69.62
R-A2 57.14% 25.00% 34.78
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 100.00% 25.00% 40.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 92.31% 57.14% 70.59
R-AM-MNR 40.00% 33.33% 36.36
R-AM-TMP 75.00% 69.23% 72.00
V 98.82% 98.82% 98.82
Table 1: Overall results (top) and detailed results on
the WSJ test (bottom).
for the role classifier) causes the result to be signifi-
cantly lower than state of the art (F-measure of 79.4,
reported in (Pradhan et al, 2005)).
5 Conclusion and Future Work
We have provided an application of Relevance Vec-
tor Machines to a large-scale NLP task. The re-
sulting classifiers are drastically smaller that those
produced by the SV training methods. On the other
hand, the classification accuracy is lower, probably
because of the use of lexicalized features.
The results on the Brown test set shows that the
genre has a significant impact on the performance.
An evaluation of the contribution of six parse tree
P R F
?=1
Const. tree -0.2% -0.6% -0.4
Partial -0.4% +0.4% 0
Dep. w/ POSs -0.1% -0.4% -0.3
Dep. w/ PT pairs +0.4% +0.4% +0.4
Shallow -0.1% +0.4% +0.1
Const. subpaths -10.9% +2.5% -4.5
Table 2: Contribution of path features
path features suggests that dependency tree paths are
more useful for semantic role labeling than the tra-
ditional constituent tree path.
In the future, we will investigate if it is possible
to incorporate the ? parameter into the probability
model, thus eliminating the need for cross-validation
completely. In addition, the training algorithm will
need to be redesigned to scale up to larger training
sets. The learning paradigm is still young and op-
timized methods (such as for SVM) have yet to ap-
pear. One possible direction is the greedy method
described in (Tipping and Faul, 2003).
References
David Ahn, Sisay Fissaha, Valentin Jijkoun, and Maarten
de Rijke. 2004. The university of Amsterdam at
Senseval-3: Semantic roles and logic forms. In Pro-
ceedings of SENSEVAL-3.
Xavier Carreras and Llu?s M?rquez. 2005. Introduction
to the CoNLL-2005 Shared Task: Semantic Role La-
beling. In Proceedings of CoNLL-2005.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a
library for support vector machines.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James Martin, and Dan Jurafsky. 2005.
Support vector learning for semantic argument classi-
fication. Machine Learning. To appear.
Michael E. Tipping and Anita Faul. 2003. Fast marginal
likelihood maximisation for sparse bayesian models.
In 9th International Workshop on AI and Statistics.
Michael E. Tipping. 2001. Sparse bayesian learning
and the relevance vector machine. Journal of Machine
Learning Research, 1:211 ? 244.
R. Clint Whaley, Antoine Petitet, and Jack J. Dongarra.
2000. Automated empirical optimizations of software
and the ATLAS project.
180
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 206?210, New York City, June 2006. c?2006 Association for Computational Linguistics
Investigating Multilingual Dependency Parsing
Richard Johansson
Department of Computer Science
LTH, Lund University
221 00 Lund, Sweden
Richard.Johansson@cs.lth.se
Pierre Nugues
Department of Computer Science
LTH, Lund University
221 00 Lund, Sweden
Pierre.Nugues@cs.lth.se
Abstract
In this paper, we describe a system for
the CoNLL-X shared task of multilin-
gual dependency parsing. It uses a base-
line Nivre?s parser (Nivre, 2003) that first
identifies the parse actions and then la-
bels the dependency arcs. These two steps
are implemented as SVM classifiers using
LIBSVM. Features take into account the
static context as well as relations dynami-
cally built during parsing.
We experimented two main additions to
our implementation of Nivre?s parser: N -
best search and bidirectional parsing. We
trained the parser in both left-right and
right-left directions and we combined the
results. To construct a single-head, rooted,
and cycle-free tree, we applied the Chu-
Liu/Edmonds optimization algorithm. We
ran the same algorithm with the same pa-
rameters on all the languages.
1 Nivre?s Parser
Nivre (2003) proposed a dependency parser that cre-
ates a projective and acyclic graph. The parser is an
extension to the shift?reduce algorithm. As with the
regular shift?reduce, it uses a stack S and a list of
input words W . However, instead of finding con-
stituents, it builds a set of arcs G representing the
graph of dependencies.
Nivre?s parser uses two operations in addition to
shift and reduce: left-arc and right-arc. Given a se-
quence of words, possibly annotated with their part
of speech, parsing simply consists in applying a se-
quence of operations: left-arc (la), right-arc (ra),
reduce (re), and shift (sh) to the input sequence.
2 Parsing an Annotated Corpus
The algorithm to parse an annotated corpus is
straightforward from Nivre?s parser and enables us
to obtain, for any projective sentence, a sequence of
actions taken in the set {la,ra,re,sh} that parses
it. At a given step of the parsing process, let TOP
be the top of the stack and FIRST , the first token of
the input list, and arc, the relation holding between
a head and a dependent.
1. if arc(TOP,FIRST ) ? G, then ra;
2. else if arc(FIRST, TOP ) ? G, then la;
3. else if ?k ? Stack, arc(FIRST, k) ? G or
arc(k, FIRST ) ? G, then re;
4. else sh.
Using the first sentence of the Swedish corpus
as input (Table 1), this algorithm produces the se-
quence of 24 actions: sh, sh, la, ra, re, la, sh,
sh, sh, la, la, ra, ra, sh, la, re, ra, ra, ra,
re, re, re, re, and ra (Table 2).
3 Adapting Nivre?s Algorithm to
Machine?Learning
3.1 Overview
We used support vector machines to predict the
parse action sequence and a two step procedure to
206
Table 1: Dependency graph of the sentence ?kten-
skapet och familjen ?r en gammal institution, som
funnits sedan 1800-talet ?Marriage and family are
an old institution that has been around from the 19th
century?.
ID Form POS Head Rel.
1 ?ktenskapet NN 4 SS
2 och ++ 3 ++
3 familjen NN 1 CC
4 ?r AV 0 ROOT
5 en EN 7 DT
6 gammal AJ 7 AT
7 institution NN 4 SP
8 , IK 7 IK
9 som PO 10 SS
10 funnits VV 7 ET
11 sedan PR 10 TA
12 1800-talet NN 11 PA
13 . IP 4 IP
produce the graph. We first ran the classifier to se-
lect unlabeled actions, la, ra, sh, re. We then ran
a second classifier to assign a function to ra and la
parse actions.
We used the LIBSVM implementation of the
SVM learning algorithm (Chang and Lin, 2001). We
used the Gaussian kernel throughout. Optimal val-
ues for the parameters (C and ?) were found using a
grid search. The first predicted action is not always
possible, given the parser?s constraints. We trained
the model using probability estimates to select the
next possible action.
3.2 Feature Set
We used the following set of features for the classi-
fiers:
? Word and POS of TOP and FIRST
? Word and POS of the second node on the stack
? Word and POS of the second node in the input
list
? POS of the third and fourth nodes in the input
list
? The dependency type of TOP to its head, if any
? The word, POS, and dependency type of the
leftmost child of TOP to TOP, if any
? The word, POS, and dependency type of the
rightmost child of TOP to TOP, if any
? The word, POS, and dependency type of the
leftmost child of FIRST to FIRST, if any
For the POS, we used the Coarse POS, the Fine
POS, and all the features (encoded as boolean flags).
We did not use the lemma.
Table 2: Actions to parse the sentence ?ktenskapet
och familjen ?r en gammal institution, som funnits
sedan 1800-talet.
Ac. Top word First word Rel.
sh nil ?ktenskapet
sh ?ktenskapet och
la och familjen ++
ra ?ktenskapet familjen CC
re familjen ?r
la ?ktenskapet ?r SS
sh nil ?r
sh ?r en
sh en gammal
la gammal institution AT
la en institution DT
ra ?r institution SP
ra institution , IK
sh , som
la som funnits SS
re , funnits
ra institution funnits ET
ra funnits sedan TA
ra sedan 1800-talet PA
re 1800-talet .
re sedan .
re funnits .
re institution .
ra ?r . IP
4 Extensions to Nivre?s Algorithm
4.1 N -best Search
We extended Nivre?s original algorithm with a beam
search strategy. For each action, la, ra, sh and re,
207
we computed a probability score using LIBSVM.
These scores can then be used to carry out an N -
best search through the set of possible sequences of
actions.
We measured the improvement over a best-first
strategy incrementing values of N . We observed the
largest difference between N = 1 and N = 2, then
leveling off and we used the latter value.
4.2 Bidirectionality and Voting
Tesni?re (1966) classified languages as centrifuge
(head to the left) and centripetal (head to the right)
in a table (page 33 of his book) that nearly exactly
fits corpus evidence from the CONLL data. Nivre?s
parser is inherently left-right. This may not fit all
the languages. Some dependencies may be easier
to capture when proceeding from the reverse direc-
tion. Jin et al (2005) is an example of it for Chinese,
where the authors describe an adaptation of Nivre?s
parser to bidirectionality.
We trained the model and ran the algorithm in
both directions (left to right and right to left). We
used a voting strategy based on probability scores.
Each link was assigned a probability score (simply
by using the probability of the la or ra actions for
each link). We then summed the probability scores
of the links from all four trees. To construct a single-
head, rooted, and cycle-free tree, we finally applied
the Chu-Liu/Edmonds optimization algorithm (Chu
and Liu, 1965; Edmonds, 1967).
5 Analysis
5.1 Experimental Settings
We trained the models on ?projectivized? graphs fol-
lowing Nivre and Nilsson (2005) method. We used
the complete annotated data for nine langagues. Due
to time limitations, we could not complete the train-
ing for three languages, Chinese, Czech, and Ger-
man.
5.2 Overview of the Results
We parsed the 12 languages using exactly the same
algorithms and parameters. We obtained an average
score of 74.93 for the labeled arcs and of 80.39 for
the unlabeled ones (resp. 74.98 and 80.80 for the
languages where we could train the model using the
complete annotated data sets). Table 3 shows the
results per language. As a possible explanation of
the differences between languages, the three lowest
figures correspond to the three smallest corpora. It
is reasonable to assume that if corpora would have
been of equal sizes, results would have been more
similar. Czech is an exception to this rule that ap-
plies to all the participants. We have no explanation
for this. This language, or its annotation, seems to
be more complex than the others.
The percentage of nonprojective arcs also seems
to play a role. Due to time limitations, we trained
the Dutch and German models with approximately
the same quantity of data. While both languages
are closely related, the Dutch corpus shows twice
as much nonprojective arcs. The score for Dutch is
significantly lower than for German.
Our results across the languages are consistent
with the other participants? mean scores, where we
are above the average by a margin of 2 to 3% ex-
cept for Japanese and even more for Chinese where
we obtain results that are nearly 7% less than the av-
erage for labeled relations. Results are similar for
unlabeled data. We retrained the data with the com-
plete Chinese corpus and you obtained 74.41 for the
labeled arcs, still far from the average. We have no
explanation for this dip with Chinese.
5.3 Analysis of Swedish and Portuguese
Results
5.3.1 Swedish
We obtained a score of 78.13% for the labeled at-
tachments in Swedish. The error breakdown shows
significant differences between the parts of speech.
While we reach 89% of correct head and dependents
for the adjectives, we obtain 55% for the preposi-
tions. The same applies to dependency types, 84%
precision for subjects, and 46% for the OA type of
prepositional attachment.
There is no significant score differences for the
left and right dependencies, which could attributed
to the bidirectional parsing (Table 4). Distance plays
a dramatic role in the error score (Table 5). Preposi-
tions are the main source of errors (Table 6).
5.3.2 Portuguese
We obtained a score 84.57% for the labeled at-
tachments in Portuguese. As for Swedish, error
distribution shows significant variations across the
208
Table 3: Summary of results. We retrained the Chi-
nese* model after the deadline.
Languages Unlabeled Labeled
Completed training
Arabic 75.53 64.29
Chinese* 79.13 74.41
Danish 86.59 81.54
Dutch 76.01 72.67
Japanese 87.11 85.63
Portuguese 88.4 84.57
Slovene 74.36 66.43
Spanish 81.43 78.16
Swedish 84.17 78.13
Turkish 73.59 63.39
x 80.80 74.98
? 5.99 8.63
Noncompleted training
Chinese 77.04 72.49
Czech 77.4 71.46
German 83.09 80.43
x all languages 80.39 74.93
? all languages 5.36 7.65
parts of speech, with a score of 94% for adjectives
and only 67% for prepositions.
As for Swedish, there is no significant score dif-
ferences for the left and right dependencies (Ta-
ble 7). Distance also degrades results but the slope is
not as steep as with Swedish (Table 8). Prepositions
are also the main source of errors (Table 9).
5.4 Acknowledgments
This work was made possible because of the anno-
tated corpora that were kindly provided to us: Ara-
bic (Hajic? et al, 2004), Bulgarian (Simov et al,
2005; Simov and Osenova, 2003), Chinese (Chen
et al, 2003), Czech (B?hmov? et al, 2003), Danish
(Kromann, 2003), Dutch (van der Beek et al, 2002),
German (Brants et al, 2002), Japanese (Kawata and
Bartels, 2000), Portuguese (Afonso et al, 2002),
Slovene (D?eroski et al, 2006), Spanish (Civit Tor-
ruella and Mart? Anton?n, 2002), Swedish (Nilsson
et al, 2005), and Turkish (Oflazer et al, 2003; Ata-
lay et al, 2003).
Table 4: Precision and recall of binned HEAD direc-
tion. Swedish.
Dir. Gold Cor. Syst. R P
to_root 389 330 400 84.83 82.50
left 2745 2608 2759 95.01 94.53
right 1887 1739 1862 92.16 93.39
Table 5: Precision and recall of binned HEAD dis-
tance. Swedish.
Dist. Gold Cor. Syst. R P
to_root 389 330 400 84.83 82.50
1 2512 2262 2363 90.05 95.73
2 1107 989 1122 89.34 88.15
3-6 803 652 867 81.20 75.20
7-... 210 141 269 67.14 52.42
Table 6: Focus words where most of the errors occur.
Swedish.
Word POS Any Head Dep Both
till PR 48 20 45 17
i PR 42 25 34 17
p? PR 39 22 32 15
med PR 28 11 25 8
f?r PR 27 22 25 20
Table 7: Precision and recall of binned HEAD direc-
tion. Portuguese.
Dir. Gold Cor. Syst. R P
to_root 288 269 298 93.40 90.27
left 3006 2959 3020 98.44 97.98
right 1715 1649 1691 96.15 97.52
Table 8: Precision and recall of binned HEAD dis-
tance. Portuguese.
Dist. Gold Cor. Syst. R P
to_root 288 269 298 93.40 90.27
1 2658 2545 2612 95.75 97.43
2 1117 1013 1080 90.69 93.80
3-6 623 492 647 78.97 76.04
7-... 323 260 372 80.50 69.89
209
Table 9: Focus words where most of the errors occur.
Portuguese.
Word POS Any Head Dep Both
em prp 66 38 47 19
de prp 51 37 35 21
a prp 46 30 39 23
e conj 28 28 0 0
para prp 21 13 18 10
References
A. Abeill?, editor. 2003. Treebanks: Building and Us-
ing Parsed Corpora, volume 20 of Text, Speech and
Language Technology. Kluwer Academic Publishers,
Dordrecht.
S. Afonso, E. Bick, R. Haber, and D. Santos. 2002. ?Flo-
resta sint?(c)tica?: a treebank for Portuguese. In Proc.
of the Third Intern. Conf. on Language Resources and
Evaluation (LREC), pages 1698?1703.
N. B. Atalay, K. Oflazer, and B. Say. 2003. The annota-
tion process in the Turkish treebank. In Proc. of the 4th
Intern. Workshop on Linguistically Interpreteted Cor-
pora (LINC).
A. B?hmov?, J. Hajic?, E. Hajic?ov?, and B. Hladk?. 2003.
The PDT: a 3-level annotation scenario. In Abeill?
(Abeill?, 2003), chapter 7.
S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith.
2002. The TIGER treebank. In Proc. of the
First Workshop on Treebanks and Linguistic Theories
(TLT).
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a
library for support vector machines.
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica treebank: Design criteria,
representational issues and implementation. In Abeill?
(Abeill?, 2003), chapter 13, pages 231?248.
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
M. Civit Torruella and Ma A. Mart? Anton?n. 2002. De-
sign principles for a Spanish treebank. In Proc. of the
First Workshop on Treebanks and Linguistic Theories
(TLT).
S. D?eroski, T. Erjavec, N. Ledinek, P. Pajas, Z. ?abokrt-
sky, and A. ?ele. 2006. Towards a Slovene depen-
dency treebank. In Proc. of the Fifth Intern. Conf. on
Language Resources and Evaluation (LREC).
J. Edmonds. 1967. Optimum branchings. Journal of Re-
search of the National Bureau of Standards, 71B:233?
240.
J. Hajic?, O. Smr?, P. Zem?nek, J. ?naidauf, and E. Be?ka.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools,
pages 110?117.
Meixun Jin, Mi-Young Kim, and Jong-Hyeok Lee.
2005. Two-phase shift-reduce deterministic depen-
dency parser of Chinese. In Proceedings of the Second
International Joint Conference on Natural Language
Processing.
Y. Kawata and J. Bartels. 2000. Stylebook for the
Japanese treebank in VERBMOBIL. Verbmobil-
Report 240, Seminar f?r Sprachwissenschaft, Univer-
sit?t T?bingen.
M. T. Kromann. 2003. The Danish dependency treebank
and the underlying linguistic theory. In Proc. of the
Second Workshop on Treebanks and Linguistic Theo-
ries (TLT).
J. Nilsson, J. Hall, and J. Nivre. 2005. MAMBA meets
TIGER: Reconstructing a Swedish treebank from an-
tiquity. In Proc. of the NODALIDA Special Session on
Treebanks.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05), pages 99?106, Ann Arbor, June.
Joakim Nivre. 2003. An efficient algorithm for projec-
tive dependency parsing. In Proceedings of the 8th In-
ternational Workshop on Parsing Technologies (IWPT
03), pages 149?160, Nancy, 23-25 April.
K. Oflazer, B. Say, D. Zeynep Hakkani-T?r, and G. T?r.
2003. Building a Turkish treebank. In Abeill?
(Abeill?, 2003), chapter 15.
K. Simov and P. Osenova. 2003. Practical annotation
scheme for an HPSG treebank of Bulgarian. In Proc.
of the 4th Intern. Workshop on Linguistically Inter-
preteted Corpora (LINC), pages 17?24.
K. Simov, P. Osenova, A. Simov, and M. Kouylekov.
2005. Design and implementation of the Bulgarian
HPSG-based treebank. In Journal of Research on Lan-
guage and Computation ? Special Issue, pages 495?
522. Kluwer Academic Publishers.
Lucien Tesni?re. 1966. ?l?ments de syntaxe structurale.
Klincksieck, Paris, 2e edition.
L. van der Beek, G. Bouma, R. Malouf, and G. van No-
ord. 2002. The Alpino dependency treebank. In Com-
putational Linguistics in the Netherlands (CLIN).
210
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 227?230,
Prague, June 2007. c?2007 Association for Computational Linguistics
LTH: Semantic Structure Extraction using Nonprojective Dependency Trees
Richard Johansson and Pierre Nugues
Department of Computer Science, Lund University, Sweden
{richard, pierre}@cs.lth.se
Abstract
We describe our contribution to the SemEval
task on Frame-Semantic Structure Extrac-
tion. Unlike most previous systems de-
scribed in literature, ours is based on depen-
dency syntax. We also describe a fully auto-
matic method to add words to the FrameNet
lexical database, which gives an improve-
ment in the recall of frame detection.
1 Introduction
The existence of links between grammatical rela-
tions and various forms of semantic interpretation
has long been observed; grammatical relations play
a crucial role in theories of linking, i.e. the realiza-
tion of the semantic arguments of predicates as syn-
tactic units (Manning, 1994; Mel?c?uk, 1988). Gram-
matical relations may be covered by many defini-
tions but it is probably easier to use them as an exten-
sion of dependency grammars, where relations take
the form of arc labels. In addition, some linguistic
phenomena such as wh-movement and discontinu-
ous structures are conveniently described using de-
pendency syntax by allowing nonprojective depen-
dency arcs. It has also been claimed that dependency
syntax is easier to understand and to teach to people
without a linguistic background.
Despite these advantages, dependency syntax has
relatively rarely been used in semantic structure ex-
traction, with a few exceptions. Ahn et al (2004)
used a post-processing step to convert constituent
trees into labeled dependency trees that were then
used as input to a semantic role labeler. Pradhan et
al. (2005) used a rule-based dependency parser, but
the results were significantly worse than when using
a constituent parser.
This paper describes a system for frame-semantic
structure extraction that is based on a dependency
parser. The next section presents the dependency
grammar that we rely on. We then give the de-
tails on the frame detection and disambiguation, the
frame element (FE) identification and classification,
and dictionary extension, after which the results and
conclusions are given.
2 Dependency Parsing with the Penn
Treebank
The last few years have seen an increasing interest
in dependency parsing (Buchholz and Marsi, 2006)
with significant improvements of the state of the art,
and dependency treebanks are now available for a
wide range of languages. The parsing algorithms
are comparatively easy to implement and efficient:
some of the algorithms parse sentences in linear time
(Yamada and Matsumoto, 2003; Nivre et al, 2006).
In the semantic structure extraction system, we
used the Stanford part-of-speech tagger (Toutanova
et al, 2003) to tag the training and test sentences and
MaltParser, a statistical dependency parser (Nivre et
al., 2006), to parse them.
We trained the parser on the Penn Treebank (Mar-
cus et al, 1993). The dependency trees used to
train the parser were created from the constituent
trees using a conversion program (Johansson and
Nugues, 2007)1. The converter handles most of
the secondary edges in the Treebank and encodes
those edges as (generally) nonprojective dependency
arcs. Such information is available in the Penn Tree-
bank in the form of empty categories and secondary
edges, it is however not available in the output of
traditional constituent parsers, although there have
been some attempts to apply a post-processing step
to predict it, see Ahn et al (2004), inter alia.
Figures 1 and 2 show a constituent tree from the
Treebank and its corresponding dependency tree.
Note that the secondary edge from the wh-trace to
Why is converted into a nonprojective PRP link.
3 Semantic Structure Extraction
This section describes how the dependency trees are
used to create the semantic structure. The system
1Available at http://nlp.cs.lth.se/pennconverter
227
NPNP
ADVPWHADVP
PRPSBJ
VP
SQ
SBARQ
*T*
Why would intelligent beings kidnap seven Soviet mailmen *T* ?
Figure 1: A constituent tree from the Penn Treebank.
Why would intelligent beings kidnap seven Soviet mailmen ?
NMOD
NMOD
NMOD
OBJ
VC
PROOT?SBARQ
SBJ
PRP
Figure 2: Converted dependency tree.
is divided into two main components: frame detec-
tion and disambiguation, and frame element detec-
tion and classification.
3.1 Frame Detection and Disambiguation
3.1.1 Filtering Rules
Since many potential target words appear in
senses that should not be tagged with a frame, we
use a filtering component as a first step in the frame
detection. We also removed some words (espe-
cially prepositions) that caused significant perfor-
mance degradation because of lack of training data.
With the increasing availability of tagged running
text, we expect that we will be able to replace the
filtering rules with a classifier in the future.
? have was retained only if it had an object,
? be only if it was preceded by there,
? will was removed in its modal sense,
? of course and in particular were removed,
? the prepositions above, against, at, below, be-
side, by, in, on, over, and under were removed
unless their head was marked as locative,
? after and before were removed unless their
head was marked as temporal,
? into, to, and through were removed unless their
head was marked as direction,
? as, for, so, and with were always removed,
? since the only sense of of was PARTITIVE,
we removed it unless it was preceded by only,
member, one, most, many, some, few, part, ma-
jority, minority, proportion, half, third, quar-
ter, all, or none, or if it was followed by all,
group, them, or us.
We also removed all targets that had been tagged
as support verbs for some other target.
3.1.2 Sense Disambiguation
For the target words left after the filtering, we
used a classifier to assign a frame, following
Erk (2005). We trained a disambiguating SVM clas-
sifier on all ambiguous words listed in FrameNet. Its
accuracy was 84% on the ambiguous words, com-
pared to a first-sense baseline score of 74%.
The classifier used the following features: target
lemma, target word, subcategorization frame (for
verb targets only), the set of dependencies of the
target, the set of words of the child nodes, and the
parent word of the target.
The subcategorization frame feature was formed
by concatenating the dependency labels of the chil-
dren, excluding subject, parentheticals, punctuation
and coordinations. For instance, for kidnap in Fig-
ure 2, the feature is PRP+OBJ.
3.1.3 Extending the Lexical Database
Coverage is one of the main weaknesses of the
current FrameNet lexical database ? it lists only
10,197 lexical units, compared to 207,016 word?
sense pairs in WordNet 3.0 (Fellbaum, 1998). We
tried to remedy this problem by training classifiers
to find words that are related to the words in a frame.
We designed a feature representation for each
lemma in WordNet, which uses a sequence of iden-
tifiers for each synset in its hypernym tree. All
senses of the lemma were used, and the features
were weighted with respect to the relative frequency
of the sense. Using this feature representation, we
trained an SVM classifier for each frame that tells
whether a lemma belongs to that frame or not.
The FrameNet dictionary could thus be extended
by 18,372 lexical units. If we assume a Zipf distri-
bution and that the lexical units already in FrameNet
are the most common ones, this would increase the
228
coverage by up to 9%. In the test set, the new lexical
units account for 53 out of the 808 target words our
system detected (6.5%). We roughly estimated the
precision to 70% by manually inspecting 100 ran-
domly selected words in the extended dictionary.
This strategy is most successful when the frame
is equivalent to one or a few synsets (and their
subtrees). For instance, for the frame MEDI-
CAL_CONDITION, we can add the complete sub-
tree of the synset pathological state, resulting in
641 new lemmas referring to all sorts of diseases.
On the other hand, the strategy also works well for
motion verbs (which often exhibit complex patterns
of polysemy): 137 lemmas could be added to the
SELF_MOTION frame. Examples of frames with fre-
quent errors are LEADERSHIP, which includes many
insects (probably because the most frequent sense
of queen in SemCor is the queen bee), and FOOD,
which included many chemical substances as well
as inedible plants and animals.
3.2 Frame Element Extraction
Following convention, we divided the FE extraction
into two subtasks: argument identification and argu-
ment classification. We did not try to assign multiple
labels to arguments. Figure 3 shows an overview. In
addition to detecing the FEs, the argument identifi-
cation classifier detects the dependency nodes that
should be tagged on the layers other than the frame
element layer: SUPP, COP, NULL, EXIST, and ASP.
The ANT and REL labels could be inserted using
simple rules. Similarly to Xue and Palmer (2004),
Argument
identification
FE
Supp
Cop
Asp
Exist
Null
Argument
None
Self_mover
Path
etc
classification
Figure 3: FE extraction steps.
we could filter away many nodes before the argu-
ment identification step by assuming that the argu-
ments for a given predicate correspond to a subset of
the dependents of the target or of its transitive heads.
Both classifiers were implemented using SVMs
and use the following features: target lemma, voice
(for verb targets only), subcategorization frame (for
verb targets only), the set of dependencies of the tar-
get, part of speech of the target node, path through
the dependency tree from the target to the node, po-
sition (before, after, or on), word and part of speech
for the head, word and part of speech for leftmost
and rightmost descendent.
In the path feature, we removed steps through
verb chains and coordination. For instance, in the
sentece I have seen and heard it, the path from heard
to I is only SBJ? and to it OBJ?.
3.3 Named Entity Recognition
In addition to the frame-semantic information, the
SemEval task also scores named entities. We used
YamCha (Kudo and Matsumoto, 2003) to detect
named entities, and we trained it on the SemEval
full-text training sets. Apart from the word and part
of speech, we used suffixes up to length 5 as fea-
tures. We think that results could be improved fur-
ther by using an external NE tagger.
4 Results
The system was evaluated on three texts. Table 1
shows the results for frame detection averaged over
the test texts. In the Setting colums, the first shows
whether Exact or Partial frame matching was used
by the evaluation script, and the second whether La-
bels or Dependencies were used. Table 2 compares
the results of the system using the extended dictio-
nary with one using the orignal FrameNet dictio-
nary, using the Partial matching and Labels scoring.
The extended dictionary introduces some noise and
thus lowers the precision slightly, but the effects on
the recall are positive. Table 3 shows the aver-
Table 1: Results for frame detection.
Setting Recall Precision F1
E L 0.528 0.688 0.597
P L 0.581 0.758 0.657
E D 0.549 0.715 0.621
P D 0.601 0.784 0.681
Table 2: Comparison of dictionaries.
Dictionary Recall Precision F1
Original 0.550 0.767 0.634
Extended 0.581 0.758 0.657
229
aged precision, recall, and F1 measures for differ-
ent evaluation parameters. The third column shows
whether named entities were used (Y) or not (N).
Interestingly, the scores are higher for the seman-
tic dependency graphs than for flat labels, while the
two other teams generally had higher scores for flat
labels. We believe that the reason for this is that we
used a dependency parser, and that the rules that we
used to convert dependency nodes into spans may
have produced some errors. It is possible that the fig-
ures would have been slightly higher if our program
produced semantic dependency graphs directly.
Table 3: Results for frame and FE detection.
Setting Recall Precision F1
E L Y 0.372 0.532 0.438
P L Y 0.398 0.570 0.468
E D Y 0.389 0.557 0.458
P D Y 0.414 0.594 0.488
E L N 0.364 0.530 0.432
P L N 0.391 0.570 0.464
E D N 0.384 0.561 0.456
P D N 0.411 0.600 0.488
5 Conclusion and Future Work
We have presented a system for frame-semantic
structure extraction that achieves promising results.
While most previous systems have been based on
constituents, our system relies on a dependency
parser. We also described an automatic method to
add new units to the FrameNet lexical database.
To improve labeling quality, we would like to ap-
ply constraints to the semantic output so that se-
mantic type and coreness rules are obeyed. In ad-
dition, while the system described here is based on
pipelined classification, recent research on seman-
tic role labeling has shown that significant perfor-
mance improvements can be gained by exploiting
interdependencies between arguments (Toutanova et
al., 2005). With an increasing amount of running
text annotated with frame semantics, we believe that
this insight can be extended to model interdependen-
cies between frames as well.
Our motivation for using dependency grammar is
that we hope that it will eventually make semantic
structure extraction easier to implement and more
theoretically well-founded. How to best design the
dependency syntax is also still an open question.
Ideally, all arguments would be direct dependents of
the predicate node and we could get rid of the sparse
and brittle Path feature in the classifier.
References
David Ahn, Sisay Fissaha, Valentin Jijkoun, and Maarten
de Rijke. 2004. The university of Amsterdam at
Senseval-3: Semantic roles and logic forms. In Pro-
ceedings of SENSEVAL-3.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the CoNLL-X.
Katrin Erk. 2005. Frame assignment as word sense dis-
ambiguation. In Proceedings of IWCS 6.
Christiane Fellbaum, editor. 1998. WordNet: An elec-
tronic lexical database. MIT Press.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proceedings of NODALIDA 2007. To appear.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods for
kernel-based text analysis. In ACL-2003.
Chistopher Manning. 1994. Ergativity: Argument struc-
ture and grammatical relations.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Igor A. Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University Press of New York, Albany.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
Parser: A data-driven parser generator for dependency
parsing. In Proceedings of LREC.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin, and Daniel Jurafsky. 2005. Semantic role la-
beling using different syntactic views. In ACL-2005.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of HLT-NAACL.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic role
labeling. In Proceedings of ACL 2005.
Nianwen Xue and Martha Palmer. 2004. Calibrating fea-
tures for semantic role labeling. In Proc. of EMNLP.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of IWPT-03.
230
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 43?48,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Multilingual Semantic Role Labeling
Anders Bjo?rkelund Love Hafdell Pierre Nugues
Department of Computer Science, Lund University
S-221 00 Lund, Sweden
fte04abj@student.lth.se
love hafdell@hotmail.com
Pierre.Nugues@cs.lth.se
Abstract
This paper describes our contribution to the
semantic role labeling task (SRL-only) of the
CoNLL-2009 shared task in the closed chal-
lenge (Hajic? et al, 2009). Our system con-
sists of a pipeline of independent, local clas-
sifiers that identify the predicate sense, the ar-
guments of the predicates, and the argument
labels. Using these local models, we carried
out a beam search to generate a pool of candi-
dates. We then reranked the candidates using
a joint learning approach that combines the lo-
cal models and proposition features.
To address the multilingual nature of the data,
we implemented a feature selection procedure
that systematically explored the feature space,
yielding significant gains over a standard set
of features. Our system achieved the second
best semantic score overall with an average la-
beled semantic F1 of 80.31. It obtained the
best F1 score on the Chinese and German data
and the second best one on English.
1 Introduction
In this paper, we describe a three-stage analysis ap-
proach that uses the output of a dependency parser
and identifies the arguments of the predicates in a
sentence. The first stage consists of a pipeline of
independent classifiers. We carried out the pred-
icate disambiguation with a set of greedy classi-
fiers, where we applied one classifier per predicate
lemma. We then used a beam search to identify
the arguments of each predicate and to label them,
yielding a pool of candidate propositions. The sec-
ond stage consists of a reranker that we applied to
the candidates using the local models and proposi-
tion features. We combined the score of the greedy
classifiers and the reranker in a third stage to select
the best candidate proposition. Figure 1 shows the
system architecture.
We evaluated our semantic parser on a set of seven
languages provided by the organizers of the CoNLL-
2009 shared task: Catalan and Spanish (Taule? et
al., 2008), Chinese (Palmer and Xue, 2009), Czech
(Hajic? et al, 2006), English (Surdeanu et al, 2008),
German (Burchardt et al, 2006), and Japanese
(Kawahara et al, 2002). Our system achieved an
average labeled semantic F1 of 80.31, which cor-
responded to the second best semantic score over-
all. After the official evaluation was completed, we
discovered a fault in the training procedure of the
reranker for Spanish. The revised average labeled
semantic F1 after correction was 80.80.
2 SRL Pipeline
The pipeline of classifiers consists of a predicate
disambiguation (PD) module, an argument identi-
fication module (AI), and an argument classifica-
tion (AC) module. Aside from the lack of a pred-
icate identification module, which was not needed,
as predicates were given, this architecture is identi-
cal to the one adopted by recent systems (Surdeanu
et al, 2008), as well as the general approach within
the field (Gildea and Jurafsky, 2002; Toutanova et
al., 2005).
We build all the classifiers using the L2-
regularized linear logistic regression from the LIB-
LINEAR package (Fan et al, 2008). The package
implementation makes models very fast to train and
43
N candidates
N candidates
Reranker
Local features + proposition features
Global model
Linear combination of models
Local classifier pipeline
Sense disambiguation
greedy search
Argument identification
beam search
Argument labeling
beam search
Reranked 
candidates
Figure 1: System architecture.
use for classification. Since models are logistic, they
produce an output in the form of probabilities that
we use later in the reranker (see Sect. 3).
2.1 Predicate Disambiguation
We carried out a disambiguation for all the lem-
mas that had multiple senses in the corpora and we
trained one classifier per lemma. We did not use the
predicate lexicons and we considered lemmas with a
unique observed sense as unambiguous.
English required a special processing as the sense
nomenclature overlapped between certain nominal
and verbal predicates. For instance, the nominal
predicate plan.01 and the verbal predicate plan.01
do not correspond to the same semantic frame.
Hence, we trained two classifiers for each lemma
plan that could be both a nominal and verbal predi-
cate.
Table 1: Feature sets for predicate disambiguation.
ca ch cz en ge sp
PredWord ? ? ?
PredPOS ? ?
PredDeprel ? ? ?
PredFeats ? ? ?
PredParentWord ? ? ? ? ?
PredParentPOS ? ? ?
PredParentFeats ? ?
DepSubCat ? ? ? ? ?
ChildDepSet ? ? ? ? ? ?
ChildWordSet ? ? ? ? ? ?
ChildPOSSet ? ? ? ? ?
2.2 Argument Identification and Classification
We implemented the argument identification and
classification as two separate stages, because it en-
abled us to apply and optimize different feature sets
in each step. Arguments were identified by means
of a binary classifier. No pruning was done, each
word in the sentence was considered as a potential
argument to all predicates of the same sentence.
Arguments were then labeled using a multiclass
classifier; each class corresponding to a certain la-
bel. We did not apply any special processing with
multiple dependencies in Czech and Japanese. In-
stead, we concatenated the composite labels (i.e.
double edge) to form unique labels (i.e. single edge)
having their own class.
2.3 Identification and Classification Features
For the English corpus, we used two sets of features
for the nominal and the verbal predicates both in the
AI and AC steps. This allowed us to create different
classifiers for different kinds of predicates. We ex-
tended this approach with a default classifier catch-
ing predicates that were wrongly tagged by the POS
tagger. For both steps, we used the union of the two
feature sets for this catch-all class.
We wanted to employ this procedure with the two
other languages, Czech and Japanese, where predi-
cates had more than one POS type. As feature selec-
tion (See Sect. 2.4) took longer than expected, par-
ticularly in Czech due to the size of the corpus and
the annotation, we had to abandon this idea and we
trained a single classifier for all POS tags in the AI
and AC steps.
For each data set, we extracted sets of features
similar to the ones described by Johansson and
Nugues (2008). We used a total of 32 features that
we denote with the prefixes: Pred-, PredParent-,
Arg-, Left-, Right-, LeftSibling-, and RightSibling-
for, respectively, the predicate, the parent of the
predicate, the argument, the leftmost and rightmost
dependents of the argument, and the left and right
44
Table 2: Feature sets for argument identification and classification.
Argument identification Argument classification
ca ch cz en ge ja sp ca ch cz en ge ja sp
PredWord ? N ?
PredPOS N ? ? V ?
PredLemma N ? ? ? ? N,V ? ?
PredDeprel
Sense ? ? V ? ? ? ? N,V ? ? ?
PredFeats ? ? ?
PredParentWord V ? V ?
PredParentPOS V V ?
PredParentFeats ?
DepSubCat ? ?
ChildDepSet ? ? ? ? V ? ? ?
ChildWordSet N ? ?
ChildPOSSet ? ? N ?
ArgWord ? ? N,V ? ? ? ? ? ? N,V ? ? ?
ArgPOS ? ? N,V ? ? ? ? ? ? N,V ?
ArgFeats ? ? ? ? ?
ArgDeprel ? ? ? V ? ? ? ? ? V ? ?
DeprelPath ? ? ? N,V ? ? ? ? ? V ?
POSPath ? ? ? N,V ? ? ? ? V ? ?
Position ? N,V ? ? ? ? N,V ? ?
LeftWord ? ? ? ? N ? ?
LeftPOS ? ? V
LeftFeats ? ? ?
RightWord ? N ? ? N,V ?
RightPOS N ? ? N,V ?
RightFeats ? ?
LeftSiblingWord ? ? ? ? N ?
LeftSiblingPOS ? ? ? ? N,V ?
LeftSiblingFeats ? ? ?
RightSiblingWord ? ? V ? ? ? ? ? ?
RightSiblingPOS ? ?
RightSiblingFeats ?
sibling of the argument. The suffix of these names
corresponds to the column name of the CoNLL for-
mat, except Word which corresponds to the Form
column. Additional features are:
? Sense: the value of the Pred column, e.g.
plan.01.
? Position: the position of the argument with re-
spect to the predicate, i.e. before, on, or after.
? DepSubCat: the subcategorization frame of the
predicate, e.g. OBJ+OPRD+SUB.
? DeprelPath: the path from predicate to argu-
ment concatenating dependency labels with the
direction of the edge, e.g. OBJ?OPRD?SUB?.
? POSPath: same as DeprelPath, but depen-
dency labels are exchanged for POS tags, e.g.
NN?NNS?NNP?.
? ChildDepSet: the set of dependency labels of
the children of the predicate, e.g. {OBJ, SUB}.
? ChildPOSSet: the set of POS tags of the chil-
dren of the predicate, e.g. {NN, NNS}.
? ChildWordSet: the set of words (Form) of the
children of the predicate, e.g. {fish, me}.
45
2.4 Feature Selection
We selected the feature sets using a greedy forward
procedure. We first built a set of single features and,
to improve the separability of our linear classifiers,
we paired features to build bigrams. We searched
the space of feature bigrams using the same proce-
dure. See Johansson (2008, page 83), for a com-
plete description. We intended to carry out a cross-
validation search. Due to the lack of time, we re-
sorted to using 80% of the training set for training
and 20% for evaluating the features. Table 2 con-
tains the complete list of single features we used.
We omitted the feature bigrams.
Feature selection turned out to be a massive task.
It took us three to four weeks searching the feature
spaces, yet in most cases we were forced to interrupt
the selection process after a few bigram features in
order to have our system ready in time. This means
that our feature sets can probably be further opti-
mized.
When the training data was initially released,
we used the exact feature set from Johansson and
Nugues (2008) to compute baseline results on the
development set for all the languages. After feature
selection, we observed an increase in labeled seman-
tic F1 close to 10% in most languages.
2.5 Applying Beam Search
The AI module proceeds left to right considering
each word as an argument of the current predicate.
The current partial propositions are scored by com-
puting the product of the probabilities of all the
words considered so far. After each word, the cur-
rent pool of partial candidates is reduced to the beam
size, k, and at the end of the sentence, the top k scor-
ing propositions are passed on to the AC module.
Given k unlabeled propositions, the AC module
applies a beam search on each of these propositions
independently. This is done in a similar manner,
proceeding from left to right among the identified
arguments, keeping the l best labelings in its beam,
and returning the top l propositions, when all iden-
tified arguments have been processed. This yields
n = k ? l complete propositions, unless one of the
unlabeled propositions has zero arguments, in which
case we have n = (k ? 1) ? l + 1.
The probability of a labeled proposition according
to the local pipeline is given by PLocal = PAI ?
PAC , where PAI and PAC is the output probability
from the AI and AC modules, respectively. In the
case of empty propositions, PAC was set to 1.
3 Global Reranker
We implemented a global reranker following
Toutanova et al (2005). To generate training ex-
amples for the reranker, we trained m AI and AC
classifiers by partitioning the training set in m parts
and using m ? 1 of these parts for each AI and AC
classifier, respectively.
We applied these AI and AC classifiers on the part
of the corpus they were not trained on and we then
generated the top n propositions for each predicate.
We ran the CoNLL evaluation script on the proposi-
tions and we marked the top scoring one(s) as pos-
itive. We marked the others negative. If the correct
proposition was not in the pool of candidates, we
added it as an extra positive example. We used these
positive and negative examples as training data for
the global reranker.
3.1 Reranker Features
We used all the features from the local pipeline for
all the languages. We built a vector where the AI
features were prefixed with AI- and the AC features
prefixed with lab?, where lab was any of the argu-
ment labels.
We added one proposition feature to the concate-
nation of local features, namely the sequence of core
argument labels, e.g. A0+plan.01+A1. In Catalan
and Spanish, we considered all the labels prefixed by
arg0, arg1, arg2, or arg3 as core labels. In Chinese
and English, we considered only the labels A0, A1,
A2, A3, and A4. In Czech, German, and Japanese,
we considered all the labels as core labels.
Hence, the total size of the reranker vector space
is |AI| + |L| ? |AC| + |G|, where |AI| and |AC|
denotes the size of the AI and AC vector spaces, re-
spectively, |L| corresponds to the number of labels,
and |G| is the size of additional global features.
We ran experiments with the grammatical
voice that we included in the string represent-
ing the sequence of core argument labels, e.g.
A1+plan.01/Passive+A0. The voice was derived by
hand-crafted rules in Catalan, English, German, and
46
Spanish, and given in the Feat column in Czech.
However, we did not notice any significant gain in
performance. The hand-crafted rules use lexical
forms and dependencies, which we believe classi-
fiers are able to derive themselves using the local
model features. This also applies to Czech, as Pred-
Feats was a feature used in the local pipeline, both
in the AI and AC steps.
3.2 Weighting the Models
In Sect. 2.5, we described how the pipeline was used
to generate the top n propositions, each with its own
local probability PLocal. Similar to softmax, we nor-
malized these local probabilities by dividing each of
them by their total sum. We denote this normalized
probability by P ?Local. The reranker gives a proba-
bility on the complete proposition, PReranker. We
weighted these probabilities and chose the proposi-
tion maximizing PF inal = (P ?Local)? ? PReranker.
This is equivalent to a linear combination of the log
probabilities.
3.3 Parameters Used
For the submission to the CoNLL 2009 Shared Task,
we set the beam widths to k = l = 4, yielding can-
didate pools of size n = 13 or n = 16 (See Sec-
tion 2.5). We used m = 5 for training the reranker
and ? = 1 for combining the local model with the
reranker.
4 Results
Our system achieved the second best semantic score,
all tasks, with an average labeled semantic F1 of
80.31. It obtained the best F1 score on the Chinese
and German data and the second best on English.
Our system also reached the third rank in the out-of-
domain data, all tasks, with a labeled semantic F1 of
74.38. Post-evaluation, we discovered a bug in the
Spanish reranker model causing the poor results in
this language. After correcting this, we could reach
a labeled semantic F1 of 79.91 in Spanish. Table 3
shows our official results in the shared task as well
as the post-evaluation update.
We also compared the performance of a greedy
strategy with that of a global model. Table 4 shows
these figures with post-evaluation figures in Spanish.
Table 5 shows the training time, parsing time, and
the parsing speed in predicates per second. These
figures correspond to complete execution time of
parsing, including loading models into memory, i.e.
a constant overhead, that explains the low parsing
speed in German. We implemented our system to
be flexible for easy debugging and testing various
ideas. Optimizing the implementation would reduce
execution times significantly.
Table 3: Summary of submitted results: closed challenge,
semantic F1. * denotes the post-evaluation results ob-
tained for Spanish after a bug fix.
Unlabeled Labeled
Catalan 93.60 80.01
Chinese 84.76 78.60
Czech 92.63 85.41
English 91.17 85.63
German 92.13 79.71
Japanese 83.45 76.30
Spanish 92.69 76.52
Spanish* 93.76 79.91
Average 90.06 80.31
Average* 90.21 80.80
Table 4: Improvement of reranker. * denotes the post-
evaluation results obtained for Spanish after a bug fix.
Greedy Reranker Gain
Catalan 79.54 80.01 0.47
Chinese 77.84 78.60 0.76
Czech 84.99 85.41 0.42
English 84.44 85.63 1.19
German 79.01 79.71 0.70
Japanese 75.61 76.30 0.69
Spanish 79.28 76.52 -2.76
Spanish* 79.28 79.91 0.63
Average 80.10 80.31 0.21
Average* 80.10 80.80 0.70
5 Conclusion
We have built and described a streamlined and ef-
fective semantic role labeler that did not use any
lexicons or complex linguistic features. We used a
generic feature selection procedure that keeps lan-
guage adaptation minimal and delivers a relatively
even performance across the data sets. The system is
47
Table 5: Summary of training and parsing times on an Apple Mac Pro, 3.2 GHz.
Training Parsing (Greedy) Speed (Greedy) Parsing (Reranker) Speed (Reranker)
(min) (min:sec) (pred/sec) (min:sec) (pred/sec)
Catalan 46 1:10 71 1:21 62
Chinese 139 2:35 79 3:45 55
Czech 299 18:47 40 33:49 22
English 421 6:25 27 8:51 20
German 15 0:21 26 0:22 25
Japanese 48 0:37 84 1:02 50
Spanish 51 1:15 69 1:47 48
robust and can handle incorrect syntactic parse trees
with a good level of immunity. While input parse
trees in Chinese and German had a labeled syntac-
tic accuracy of 78.46 (Hajic? et al, 2009), we could
reach a labeled semantic F1 of 78.60 and 79.71 in
these languages. We also implemented an efficient
global reranker in all languages yielding a 0.7 av-
erage increase in labeled semantic F1. The reranker
step, however, comes at the expense of parsing times
increased by factors ranging from 1.04 to 1.82.
References
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k Z?abokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic?semantic analysis
with PropBank and NomBank. In Proceedings of the
Shared Task Session of CoNLL-2008.
Richard Johansson. 2008. Dependency-based Semantic
Analysis of Natural-language Text. Ph.D. thesis, Lund
University, December 5.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013, Las Palmas, Canary
Islands.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and JoakimNivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008).
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic role
labeling. In Proceedings of ACL-2005.
48
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 37?48,
Paris, October 2009. c?2009 Association for Computational Linguistics
Predictive Text Entry using Syntax and Semantics
Sebastian Ganslandt
sebastian@ganslandt.nu
Jakob J?rwall
Department of Computer Science
Lund University
S-221 00 Lund, Sweden
d02jjr@student.lth.se
Pierre Nugues
pierre.nugues@cs.lth.se
Abstract
Most cellular telephones use numeric key-
pads, where texting is supported by dic-
tionaries and frequency models. Given a
key sequence, the entry system recognizes
the matching words and proposes a rank-
ordered list of candidates. The ranking
quality is instrumental to an effective en-
try.
This paper describes a new method to en-
hance entry that combines syntax and lan-
guage models. We first investigate com-
ponents to improve the ranking step: lan-
guage models and semantic relatedness.
We then introduce a novel syntactic model
to capture the word context, optimize
ranking, and then reduce the number of
keystrokes per character (KSPC) needed
to write a text. We finally combine this
model with the other components and we
discuss the results.
We show that our syntax-based model
reaches an error reduction in KSPC of
12.4% on a Swedish corpus over a base-
line using word frequencies. We also show
that bigrams are superior to all the other
models. However, bigrams have a mem-
ory footprint that is unfit for most devices.
Nonetheless, bigrams can be further im-
proved by the addition of syntactic mod-
els with an error reduction that reaches
29.4%.
1 Introduction
The 12-key input is the most common keypad lay-
out on cellular telephones. It divides the alpha-
bet into eight lists of characters and each list is
mapped onto one key as shown in Figure 1. Since
three or four characters are assigned to a key, a
single key press is ambiguous.
Figure 1: Standard 12-button keypad layout (ISO
9995-8).
1.1 Multi-tap
Multi-tap is an elementary method to disam-
biguate input for a 12-button keypad. Each charac-
ter on a key is assigned an index that corresponds
to its visual position, e.g. ?A?, 1, ?B?, 2, and ?C?,
3 and each consecutive stroke ? tap ? on the same
key increments the index. When the user wants
to type a letter, s/he presses the corresponding key
until the desired index is reached. The user then
presses another key or waits a predefined time to
verify that the correct letter is selected. The key
sequence 8-4-4-3-3, for example, leads to the word
the.
Multi-tap is easy to implement and no dictio-
nary is needed. At the same time, it is slow and
tedious for the user, notably when two consecutive
characters are placed on the same key.
1.2 Single Tap with Predictive Text
Single tap with predictive text requires only one
key press to enter a character. Given a keystroke
sequence, the system proposes words using a dic-
tionary or language modeling techniques.
Dictionary-based techniques search the words
matching the key sequence in a list that is stored
by the system (Haestrup, 2001). While some
37
keystroke sequences produce a unique word, oth-
ers are ambiguous and the system returns a list
with all the candidates. The key sequence 8-4-3,
for example, corresponds to at least three possi-
ble words: the, tie, and vie. The list of candidates
is then sorted according to certain criteria, such
as the word or character frequencies. If the word
does not exist in the dictionary, the user has to fall
back to multi-tap to enter it. The T91 commercial
product is an example of a dictionary-based sys-
tem (Grover et al, 1998).
LetterWise (MacKenzie et al, 2001) is a tech-
nique that uses letter trigrams and their frequen-
cies to predict the next character. For example,
pressing the key 3 after the letter bigram ?th? will
select ?e?, because the trigram ?the? is far more fre-
quent than ?thd? or ?thf? in English. When the sys-
tem proposes a wrong letter, the user can access
the next most likely one by pressing a next-key.
LetterWise does not need a dictionary and has a
KSPC of 1.1500 (MacKenzie, 2002).
1.3 Modeling the Context
Language modeling can extend the context from
letter sequences to word n-grams. In this case, the
system is not restricted to the disambiguation or
the prediction of the typed characters. It can com-
plete words and even predict phrases. HMS (Has-
selgren et al, 2003) is an example of this that uses
word bigrams in Swedish. It reports a KSPC
ranging from 0.8807 to 1.0108, depending on the
type of text. eZiText2 is a commercial example of
a word and phrase completion system. However,
having a large lexicon of bigrams still exceeds the
memory capacity of many mobile devices.
Some systems use a combination of syntac-
tic and semantic information to model the con-
text. Gong et al (2008) is a recent example that
uses word frequencies, a part-of-speech language
model, and a semantic relatedness metric. The
part-of-speech language model acts as a lexical
n-gram language model, but occupies much less
memory since the vocabulary is restricted to the
part-of-speech tagset. The semantic relatedness,
modified from Li and Hirst (2005), is defined as
the conditional probability of two stems appearing
in the same context (the same sentence):
1www.t9.com
2www.zicorp.com/ezitext.htm
SemR(w1|w2) = C(stem(w1), stem(w2))C(w2) .
The three components are combined linearly
and their coefficients are adjusted using a devel-
opment set. Setting 1 as the limit of the KSPC
figure, Gong et al (2008) reported an error reduc-
tion over the word frequency baseline of 4.6% for
the semantic model, 12.6% for the part-of-speech
language model, and 15.8% for the combination
of both.
1.4 Syntax in Predictive Text
Beyond part-of-speech language modeling, there
are few examples of systems using syntax in pre-
dictive text entry. Matiasek et al (2002) describes
a predictive text environment aimed at disabled
persons, which originally relied on language mod-
els. Gustavii and Pettersson (2003) added a syn-
tactic component to it based on grammar rules.
The rules corresponded to common grammatical
errors and were used to rerank the list of candidate
words. The evaluation results were disappointing
and the syntactic component was not added be-
cause of the large overhead it introduced (Mati-
asek, 2006).
In the same vein, Sundarkantham and Shalinie
(2007) used grammar rules to discard infeasible
grammatical constructions. The authors evaluated
their system by giving it an incomplete sentence
and seeing how often the system correctly guessed
the next word (Shannon, 1951). They achieved
better results than previously reported, although
their system has not been used in the context of
predictive text entry for mobile devices.
2 Predictive Text Entry Using Syntax
We propose a new technique that makes use of
a syntactic component to model the word context
and improve the KSPC figure. It builds on Gong
et al (2008)?s system and combines a dependency
grammar model with word frequencies, a part-of-
speech language model, and the semantic related-
ness defined in Sect. 1.3. As far as we are aware,
no predictive text entry system has yet used a data-
driven syntactic model of the context.
We used Swedish as our target language all
over our experiments, but the results we obtained
should be replicable in any other language.
38
2.1 Reranking Candidate Words
The system consists of two components. The first
one disambiguates the typed characters using a
dictionary and produces a list of candidate words.
The second component reranks the candidate list.
Although the techniques we describe could be ap-
plied to word completion, we set aside this aspect
in this paper.
More formally, we frame text input as a se-
quence of keystrokes, ksi = ksi1 . . . ksin, to en-
ter a desired word, wi. The words matching
the key sequence in the system dictionary form
an ordered set of alternatives, match(ksi) =
{cw0, . . . , cwm}, where it takes k extra keystrokes
to reach candidate cwk. Using our example
in Sect. 1.2, a lexical ordering would yield
match(8 ? 4 ? 3) = {the, tie, vie}, where two
extra keystrokes are needed to reach vie.
We assign each candidate word w member of
match(ksi) a score
Score(w|Context) =?
s?S
?s ? s(w|Context),
to rerank (sort) the prediction list, where s is a
scoring function from a set S, ?s, the weight of
s, and Score(w|Context), the total score of w in
the current context.
In this framework, optimizing predictive text
entry is the task of finding the scoring functions,
s, and the weights, ?s, so that they minimize k on
average.
As scoring functions, we considered lexical lan-
guage models in the form of unigrams and bi-
grams, sLM1 and sLM2, a part-of-speech model
using sequences of part-of-speech tags of a length
of up to five tags, sPOS , and a semantic affin-
ity, sSemA, derived from the semantic relatedness.
In addition, we introduce a syntactic component
in the form of a data-driven dependency syntax,
sDepSyn so that the complete scoring set consists
of
S = {sLM1, sLM2, sSemA, sPOS , sDepSyn}.
2.2 Language and Part-of-Speech Models
The language model score is the probability of a
candidate word w, knowing the sequence entered
so far, w1, . . . , wi:
P (w|w1, w2, . . . , wi).
We approximate it using unigrams, sLM1(w) =
P (w), or bigrams, sLM2(w) = P (w|wi) that we
derive from a corpus using the maximum like-
lihood estimate. To cope with sparse data, we
used a deleted interpolation so that sLM2(w) =
?1P (w|wi)+?2P (w), where we adjusted the val-
ues of ?1 and ?2 on a development corpus.
In practice, it is impossible to maintain a large
list of bigrams on cellular telephones as it would
exceed the available memory of most devices. In
our experiments, the sLM2 score serves as an indi-
cator of an upper-limit performance, while sLM1
serves as a baseline, as it is used in commercial
dictionary-based products.
Part-of-speech models offer an interesting alter-
native to lexical models as the number of parts
of speech does not exceed 100 tags in most lan-
guages. The possible number of bigrams is then at
most 10,000 and much less in practice. We defined
the part-of-speech model score, sPOS as
P (t|t1, t2, . . . , ti),
where ti is the part of speech of wi and t, the part
of speech of the candidate word w. We used a
5-gram approximation of this probability with a
simple back-off model:
sPOS =
?
?????
?????
P (t|ti?3, . . . , ti) if C(ti?3, ..., ti) 6= 0
P (t|ti?2, . . . , ti) if C(ti?2, ..., ti) 6= 0
...
P (t), otherwise
We used the Granska tagger (Carlberger and
Kann, 1999) to carry out the part-of-speech anno-
tation of the word sequence.
3 Semantic Affinity
Because of their arbitrary length, language mod-
els miss possible relations between words that are
semantically connected in a sentence but within
a distance greater than one, two, or three words
apart, the practical length of most n-grams mod-
els. Li and Hirst (2005) introduced the semantic
relatedness between two words to measure such
relations within a sentence. They defined it as
SemR(wi, wj) = C(wi, wj)C(wi)C(wj) ,
where C(wi, wj) is the number of times the words
wi and wj co-occur in a sentence in the corpus,
39
and C(wi) is the count of word wi in the corpus.
The relation is symmetrical, i.e.
C(wi, wj) = C(wj , wi).
The estimated semantic affinity of a word w is
defined as:
SemA(w|H) = ?
wj?H
SemR(w,wj),
where H is the context of the word w. In our case,
H consists of words to the left of the current word.
Gong et al (2008) used a similar model in a pre-
dictive text application with a slight modification
to the SemR function:
SemR(wi, wj) = C(stem(wi), stem(wj))C(stem(wj)) ,
where the stem(w) function removes suffixes
from words. We refined this model further and we
replaced the stemming function with a real lemma-
tization.
4 Dependency Parsing
Dependency syntax (Tesni?re, 1966) has attracted
a considerable interest in the recent years, spurred
by the availability of data-driven parsers as well
as annotated data in multiple languages includ-
ing Arabic, Chinese, Czech, English, German,
Japanese, Portuguese, or Spanish (Buchholz and
Marsi, 2006; Nivre et al, 2007). We used this
syntactic formalism because of its availability in
many languages.
4.1 Parser Implementation
There are two main classes of data-driven de-
pendency parsers: graph-based (McDonald and
Pereira, 2006) and transition-based (Nivre, 2003).
We selected Nivre?s parser because of its imple-
mentation simplicity, small memory footprint, and
linear time complexity. Parsing is always achieved
in at most 2n? 1 actions, where n is the length of
the sentence. Both types of parser can be com-
bined, see Zhang and Clark (2008) for a discus-
sion.
Nivre?s parser is an extension to the shift?
reduce algorithm that creates a projective and
acyclic graph. It uses a stack, a list of input words,
and builds a set of arcs representing the graph of
dependencies. The parser uses two operations in
addition to shift and reduce, left-arc and right-arc:
? Shift pushes the next input word onto the
stack.
? Reduce pops the top of the stack with the
condition that the corresponding word has a
head.
? LeftArc adds an arc from the next input
word to the top of the stack and pops it.
? RightArc adds an arc from the top of the
stack to the next input word and pushes the
input word on the stack.
Table 1 shows the start and final parser states as
well as the four transitions and their conditions
and Algorithm 1 describes the parsing algorithm.
4.2 Features
At each step of the parsing procedure, the parser
turns to a guide to decide on which transition
to apply among the set {LeftArc, RightArc,
Shift, Reduce}. We implemented this guide
as a four-class classifier that uses features it ex-
tracts from the parser state. The features consist
of words and their parts of speech in the stack, in
the queue, and in the partial graph resulting from
what has been parsed so far. The classifier is based
on a linear logistic regression function that evalu-
ates the transition probabilities from the features
and predicts the next one.
In the learning phase, we extracted a data set
of feature vectors using the gold-standard parsing
procedure (Algorithm 2) that we applied to Tal-
banken corpus of Swedish text (Einarsson, 1976;
Nilsson et al, 2005). Each vector being labeled
with one of the four possible transitions. We
trained the classifiers using the LIBLINEAR im-
plementation (Fan et al, 2008) of logistic regres-
sion.
However, classes are not always separable us-
ing linear classifiers. We combined single features
as pairs or triples. This emulates to some extent
quadratic kernels used in support vector machines,
while preserving the speed of the linear models.
Table 2 shows the complete feature set to predict
the transitions. A feature is defined by
? A source: S for stack and Q for the queue;
? An offset: 0 for the top of the stack and first
in the queue; 1 and 2 for levels down in the
stack or to the right in the queue;
40
Name Action Condition
Initialization ?nil,W, ??
Termination ?S, nil, A?
LeftArc ?n|S, n?|Q,A? ? ?S, n?|Q,A ? {?n?, n?}? ??n??, ?n, n??? ? A
RightArc ?n|S, n?|Q,A? ? ?n?|n|S,Q,A ? ?n, n??? ??n??, ?n?, n??? ? A
Reduce ?n|S,Q,A? ? ?S,Q,A? ?n?, ?n, n?? ? A
Shift ?S, n|Q,A? ? ?n|S,Q,A?
Table 1: Parser transitions. W is the original input sentence, A is the dependency graph, S is the stack,
andQ is the queue. The triplet ?S,Q,A? represents the parser state. n, n?, and n?? are lexical tokens. The
pair ?n?, n? represents an arc from the head n? to the dependent n.
? Possible applications of the function head,H ,
leftmost child, LC, or righmost child, RC;
? The value: word, w, or POS tag, t, at the
specified position.
Queue Q0w
Q1w
Q0t
Q1t
Q0tQ0w
Q0tQ1t
Q1wQ1t
Q0tQ1tQ2t
Q0wQ1tQ2t
Stack S0t
S0w
S0tS0w
S0tS1t
Stack/Queue S0wQ0w
Q0tS0t
Q1tS0t
Q0tS1t
Q1tS1t
S0tQ0tQ1t
S0tQ0wQ0t
Partial Graph S0HtS0tQ0t
Q0LCtS0tQ0t
Q0LCtS0tQ0w
S0RCtS0tQ0t
S0RCtS0tQ0w
Table 2: Feature model for predicting parser ac-
tions with combined features.
4.3 Calculating Graph Probabilities
Nivre (2006) showed that every terminating tran-
sition sequence Am1 = (a1, ..., am) applied to
a sentence Wn1 = (w1, ..., wn) defines exactly
one parse tree G. We approximated the prob-
ability P (G|Wn1 ) of a dependency graph G as
P (Am1 |Wn1 ) and we estimated the probability of
G as the product of the transition probabilities, so
that
PParse(G|Wn1 ) = P (Am1 |Wn1 )
= ?mk=1 P (ak|Ak?11 ,W ?(k?1)1 ),
where ak is member of the set {LeftArc,
RightArc, Shift, Reduce} and ?(k) corre-
sponds to the index of the current word at tran-
sition k.
We finally approximated the term
Ak?11 ,W
?(k?1)
1 to the feature set and com-
puted probability estimates using the logistic
regression output.
4.4 Beam Search
We extended Nivre?s parser with a beam search to
mitigate error propagation that occurs with a de-
terministic parser (Johansson and Nugues, 2006).
We maintained N parser states in parallel and we
applied all the possible transitions to each state.
We scored each transition action and we ranked
the states with the product of the action?s proba-
bilities leading to this state. Algorithm 3 outlines
beam search with a diameter of N .
An alternative to training parser transitions us-
ing local features is to use an online learning al-
gorithm (Johansson and Nugues, 2007; Zhang and
Clark, 2008). The classifiers are then computed
over the graph that has already been built instead
of considering the probability of a single transi-
tion.
41
4.5 Evaluation
We evaluated our dependency parser separately
from the rest of the application and Table 3 shows
the results. We optimized our parameter selection
for the unlabeled attachment score (UAS). This
explains the relatively high difference with the la-
beled attachment score (LAS): about ?8.6.
Table 3 also shows the highest scores ob-
tained on the same Talbanken corpus of Swedish
text (Einarsson, 1976; Nilsson et al, 2005) in
the CoNLL-X evaluation (Buchholz and Marsi,
2006): 89.58 for unlabeled attachments (Corston-
Oliver and Aue, 2006) and 84.58 for labeled at-
tachments (Nivre et al, 2006). CoNLL-X systems
were optimized for the LAS category.
The figures we reached were about 1.10% be-
low those reported in CONLL-X for the UAS cat-
egory. However our results are not directly compa-
rable as the parsers or the classifiers in CONLL-X
have either a higher complexity or are more time-
consuming. We chose linear classifiers over kernel
machines as it was essential to our application to
run on mobile devices with limited resources in
both CPU power and memory size.
This paper CONLL-X
Beam width LAS UAS LAS UAS
1 79.45 88.05 84.58 89.54
2 79.76 88.41
4 79.75 88.40
8 79.77 88.41
16 79.78 88.42
32 79.77 88.41
64 79.79 88.44
Table 3: Parse results on the Swedish Talbanken
corpus obtained for this paper as well as the best
reported results in CONLL-X on the same corpus
(Buchholz and Marsi, 2006).
5 Dependencies to Predict the Next Word
We built a syntactic score to measure the grammat-
ical relevance of a candidate word w in the current
context, that is the word sequence so farw1, ..., wi.
We defined it as the weighted sum of three terms:
the score of the partial graph resulting from the
analysis of the words to the left of the candidate
word and the scores of the link from w to its head,
h(w), using their lexical forms and their parts of
speech:
sDepSyn(w) = ?1PParse(G(w)|w1, ..., wi, w)+
?2PLink(w, h(w))+
?3PLink(POS(w), POS(h(w))),
where G(w) is the partial graph representing the
word sequence w1, ..., wi, w. The PLink terms are
intended to give an extra-weight to the probabil-
ity of an association between the predicted word
and a possible head to the left of it. They hint at
the strength of the ties between w and the words
before it.
We used the transition probabilities described in
Sect. 4.3 to compute the score of the partial graph,
yielding
PParse(G(w)|w1, ..., wi, w) =
j?
k=1
P (ak),
where a1, ..., aj is the sequence of transition ac-
tions producing G(w) and P (ak), the probability
output of transition k given by the logistic regres-
sion engine.
The last two terms PLink(w, h(w)) and
PLink(POS(w), POS(h(w))) are computed
from counts in the training corpus using maxi-
mum likelihood estimates:
PLink(w, h(w)) =
C(Link(w, h(w)) + 1?
wl?PW C(Link(wl, h(wl)))
+ |PW |
and
PLink(POS(w), POS(h(w))) =
C(Link(POS(w), POS(h(w)))) + 1?
wl?PW C(Link(POS(wl), h(POS(wl))))+|PW |,
where PW = match(ksi), is the set of predicted
words for the current key sequence.
If the current word w has not been assigned a
head yet, we default h(w) to the root of the graph
and POS(h(w)) to the ROOT value.
6 Experiments and Results
6.1 Experimental Setup
Figure 2 shows an overview of the three stages
to produce and evaluate our models: training,
42
tuning, and testing. Ideally, we would have
trained the classifiers on a corpus matching a
text entry application. However, as there is no
large available SMS corpus in Swedish, we used
the Stockholm-Ume? corpus (SUC) (Ejerhed and
K?llgren, 1997). SUC is balanced and the largest
available POS-tagged corpus in Swedish with
more than 1 million words.
We parsed the corpus and we divided it ran-
domly into a training set (80%), a development set
(10%), and a test set (10%). The training set was
used to gather statistics on word n-grams, POS
n-grams, collocations, lemma frequencies, depen-
dent/head relations. We discarded hapaxes: rela-
tions and sequences occurring only once. We used
lemmas instead of stems in the semantic related-
ness score, SemR, because stemming is less ap-
propriate in Swedish than in English.
We used the development set to find optimal
weights for the scoring functions, resulting in the
lowest KSPC. We ran an exhaustive search using
all possible linear combinations with increments
of 0.1, except for two functions, where this was
too coarse. We used 0.01 then.
We applied the resulting linear combinations of
scoring functions to the test set. We first compared
the frequency-based disambiguation acting as a
baseline to linear combinations involving or not
involving syntax, but always excluding bigrams.
Table 4 shows the most significant combinations.
We then compared a set of other combinations
with the bigram model. They are shown in Ta-
ble 6.
6.2 Metrics
We redefined the KSPC metric of MacKenzie
(2002), since the number of characters needed to
input a word is now dependent on the word?s left
context in the sentence. Let S = (w1, . . . , wn) ?
L be a sentence in the test corpus. The KSPC for
the test corpus then becomes
KSPC =
?
S?L
?
w?SKS(w|LContext(w, S))?
S?L
?
w?S Chars(w)
where KS(w|LContext) is the number of key
strokes needed to enter a word in a given context,
LContext(w, S) is the left context of w in S, and
Chars(w) is the number of characters in w.
Another performance measure is the disam-
biguation accuracy (DA), which is the percentage
of words that are correctly disambiguated after all
the keys have been pressed
DA =
?
S?L
?
w?S
PredHit(w|LContext(w, S))
#w ,
where PredHit(w|Context) = 1 if w is the
top prediction and 0 otherwise, and #w, the to-
tal number of words in L. A good DA means that
the user can more often simply accept the default
proposed word instead of navigating the prediction
list for the desired word.
As scoring tokens, we chose to keep the ones
that actually have the ability to differentiate the
models, i.e. we did not count the KSPC and DA
for words that were not in the dictionary. Neither
did we count white spaces, nor the punctuation
marks.
All our measures are without word or phrase
completion. This means that the lower-limit fig-
ure for KSPC is 1.
6.3 Results
As all the KSPC figures are close to 1, we com-
puted the error reduction rate (ERR), i.e. the re-
duction in the number of extra keystrokes needed
beyond one. We carried out all the optimizations
considering KSPC, but we can observe that KSPC
ERR and DA ERR strongly correlate.
Table 5 shows the results with scoring func-
tions using the word frequencies. The columns
include KSPC and DA together with KSPC ERR
and DA ERR compared with the baseline. Table 7
shows the respective results when using a bigram-
based disambiguation instead of just frequency.
The ERR is still compared to the word frequency
baseline but attention should also be drawn on the
relative increases: how much the new models can
improve bigram-based disambiguation.
7 Discussion
We can observe from the results that a model based
on dependency grammars improves the prediction
considerably. The DepSyn model is actually the
most effective one when applied together with the
frequency counts. Furthermore, the improvements
from the POS, SemA, and DepSyn model are
almost disjunct, as the combined model improve-
ment matches the sum of their respective individ-
ual contributions.
The 4.2% ERR observed when adding the
SemA model is consistent with the result from
43
Figure 2: System architecture, where the set of scoring functions is S = {sLM , sSemA, sPOS , sDepSyn}
and the linear combination is =?
s?S
?s ? s(w).
Gong et al (2008), where a 4.6% ERR was found.
On the other hand, the POS model only con-
tributed 4.7% ERR in our case, whereas Gong et
al. (2008) observed 12.6%. One possible expla-
nation for this is that they clustered related POS
tags into 19 groups reducing the sparseness prob-
lem. By performing this grouping, we can effec-
tively ignore morphological and lexical features
that have no relevance, when deciding which word
should come next. Other possible explanations in-
clude that our backoff model is not well suited for
this problem or that the POS sequences are not an
applicable model for Swedish.
The bigram language model has the largest im-
pact on the performance. The ERR for bigrams
alone is higher than all the other models com-
bined. Still, the other models have the ability to
contribute on top of the bigram model. For exam-
ple, the POS model increases the ERR by about
5% both when using bigram- and frequency-based
disambiguation, suggesting that this information is
not captured by the bigrams. On the other hand,
DepSyn increases the ERR by a more modest 3%
when using bigrams instead of 7% with word fre-
quencies. This is likely due to the fact that about
half of the dependency links only stretch to the
next preceding or succeeding word in the corpus.
The most effective combination of models are
the bigrams together with the POS sequence and
the dependency structure, both embedding syntac-
tic information. With this combination, we were
able to reduce the number of erroneous disam-
biguations as well as extra keystrokes by almost
one third.
8 Further Work
SMS texting, which is the target of our system,
is more verbal than the genres gathered in the
Stockholm-Ume? corpus. The language models
of a final application would then change consid-
erably from the ones we extracted from the SUC.
A further work would be to collect a SMS corpus
and replicate the experiments: retrain the models
and obtain the corresponding performance figures.
Moreover, we carried out our implementation
and simulations on desktop computers. The POS
model has an estimated size of 700KB (Gong et
al., 2008). The PParse term of theDepSynmodel
can be made as small as the feature model. We ex-
pect the optimized size of this model to be under
100KB in an embedded environment. The size of
the lexical variant of PLink is comparable to the bi-
gram model. This could however be remedied by
using the probability of the action that constructed
this last link. The computational power required
by LIBLINEAR is certainly within the reach of
modern hand-held devices. However, a prototype
simulation with real hardware conditions would
44
be needed to prove an implementability on mobile
devices.
Finally, a user might perceive subtle differences
in the presentation of the words compared with
that of popular commercial products. Gutowitz
(2003) noted the reluctance to single-tap input
methods because of their ?unpredictable? behav-
ior. Introducing syntax-based disambiguation
could increase this perception. A next step would
be to carry out usability studies and assess this el-
ement.
References
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency parsing.
In Proceedings of the Tenth Conference on Com-
putational Natural Language Learning (CoNLL-X),
pages 149?164, New York City.
Johan Carlberger and Viggo Kann. 1999. Implement-
ing an efficient part-of-speech tagger. Software ?
Practice and Experience, 29(2):815?832.
Simon Corston-Oliver and Anthony Aue. 2006. De-
pendency parsing with reference to slovene, spanish
and swedish. In Proceedings of the Tenth Confer-
ence on Computational Natural Language Learning
(CoNLL-X), pages 196?200, New York City, June.
Jan Einarsson. 1976. Talbankens skriftspr?kskonkor-
dans. Technical report, Lund University, Institutio-
nen f?r nordiska spr?k, Lund.
Eva Ejerhed and Gunnel K?llgren. 1997. Stockholm
Ume? Corpus version 1.0, SUC 1.0.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Jun Gong, Peter Tarasewich, and I. Scott MacKenzie.
2008. Improved word list ordering for text entry on
ambiguous keypads. In NordiCHI ?08: Proceedings
of the 5th Nordic conference on Human-computer
interaction, pages 152?161, Lund, Sweden.
Dale L. Grover, Martin T. King, and Clifford A. Kush-
ler. 1998. Reduced keyboard disambiguating com-
puter. U.S. Patent no. 5,818,437.
Ebba Gustavii and Eva Pettersson. 2003. A Swedish
grammar for word prediction. Technical report, De-
partment of Linguistics, Uppsala University.
Howard Gutowitz. 2003. Barriers to adoption of
dictionary-based text-entry methods; a field study.
In Proceedings of the Workshop on Language Mod-
eling for Text Entry Systems (EACL 2003), pages 33?
41, Budapest.
Jan Haestrup. 2001. Communication terminal hav-
ing a predictive editor application. U.S. Patent no.
6,223,059.
Jon Hasselgren, Erik Montnemery, Pierre Nugues, and
Markus Svensson. 2003. HMS: A predictive text
entry method using bigrams. In Proceedings of
the Workshop on Language Modeling for Text Entry
Methods (EACL 2003), pages 43?49, Budapest.
Richard Johansson and Pierre Nugues. 2006. In-
vestigating multilingual dependency parsing. In
Proceedings of the Tenth Conference on Compu-
tational Natural Language Learning (CONLL-X),
pages 206?210, New York.
Richard Johansson and Pierre Nugues. 2007. Incre-
mental dependency parsing using online learning.
In Proceedings of the CoNLL Shared Task Session
of EMNLP-CoNLL, pages 1134?1138, Prague, June
28-30.
Jianhua Li and Graeme Hirst. 2005. Semantic knowl-
edge in word completion. In Assets ?05: Proceed-
ings of the 7th international ACM SIGACCESS con-
ference on Computers and accessibility, pages 121?
128, Baltimore.
I. Scott MacKenzie, Hedy Kober, Derek Smith, Terry
Jones, and Eugene Skepner. 2001. LetterWise:
Prefix-based disambiguation for mobile text input.
In 14th Annual ACM Symposium on User Interface
Software and Technology, Orlando, Florida.
I. Scott MacKenzie. 2002. KSPC (keystrokes per char-
acter) as a characteristic of text entry techniques. In
Proceedings of the Fourth International Symposium
on Human Computer Interaction with Mobile De-
vices, pages 195?210, Heidelberg, Germany.
Johannes Matiasek, Marco Baroni, and Harald Trost.
2002. FASTY ? A multi-lingual approach to text
prediction. In ICCHP ?02: Proceedings of the
8th International Conference on Computers Helping
People with Special Needs, pages 243?250, London.
Johannes Matiasek. 2006. The language component
of the FASTY predictive typing system. In Karin
Harbusch, Kari-Jouko Raiha, and Kumiko Tanaka-
Ishii, editors, Efficient Text Entry, number 05382 in
Dagstuhl Seminar Proceedings, Dagstuhl, Germany.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of the 11th Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL), pages 81?88, Trento.
Jens Nilsson, Johan Hall, and Joakim Nivre. 2005.
MAMBA meets TIGER: Reconstructing a Swedish
treebank from antiquity. In Proceedings of the
NODALIDA Special Session on Treebanks, Joensuu,
Finland.
45
Joakim Nivre, Johan Hall, Jens Nilsson, G?lsen
Eryigit, and Svetoslav Marinov. 2006. Labeled
pseudo-projective dependency parsing with support
vector machines. In Proceedings of the Tenth Con-
ference on Computational Natural Language Learn-
ing (CoNLL-X), pages 221?225, June.
Joakim Nivre, Johan Hall, Sandra K?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
915?932, Prague.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT), pages 149?160, Nancy.
Joakim Nivre. 2006. Inductive Dependency Parsing.
Springer, Dordrecht, The Netherlands.
Claude Elwood Shannon. 1951. Prediction and en-
tropy of printed English. The Bell System Technical
Journal, pages 50?64, January.
K. Sundarkantham and S. Mercy Shalinie. 2007. Word
predictor using natural language grammar induction
technique. Journal of Theoretical and Applied In-
formation Technology, 3:1?8.
Lucien Tesni?re. 1966. ?l?ments de syntaxe struc-
turale. Klincksieck, Paris, 2e edition.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: Investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 562?571, Hawaii, October 25?27.
46
Algorithm 1 Nivre?s algorithm.
1: Queue?W
2: Stack ? nil
3: while ?Queue.isEmpty() do
4: features? ExtractFeatures()
5: action? guide.Predict(features)
6: if action = RightArc ? canRightArc() then
7: RightArc()
8: else if action = LeftArc ? canLeftArc() then
9: LeftArc
10: else if action = Reduce ? canReduce() then
11: Reduce()
12: else
13: Shift()
14: end if
15: end while
16: return(A)
Algorithm 2 Reference parsing.
1: Queue?W
2: Stack ? nil
3: while ?Queue.isEmpty() do
4: x? ExtractFeatures()
5: if ?Stack.peek(), Queue.get(0)? ? A ? canRightArc() then
6: t? RightArc
7: else if ?Queue.get(0), Stack.peek()? ? A ? canLeftArc() then
8: t? LeftArc
9: else if ?w ? Stack : ?w,Queue.get(0)? ? A? ?Queue.get(0), w? ? A) ? canReduce() then
10: t? Reduce
11: else
12: t? Shift
13: end if
14: store training example ?x, t?
15: end while
Algorithm 3 Beam parse.
1: Agenda.add(InititalParserState)
2: while ?done do
3: for parserState ? Agenda do
4: Output.add(parserState.doLeftArc())
5: Output.add(parserState.doRightArc())
6: Output.add(parserState.doReduce())
7: Output.add(parserState.doShift())
8: end for
9: Sort(Output)
10: Clear(Agenda)
11: Take N best parse trees from Output and put in Agenda.
12: end while
13: Return best item in Agenda.
47
Configuration Scoring model DepSyn weights
F1 baseline 1? LM1 (Word frequencies) ?
F2 0.9? LM1 + 0.1? POS ?
F3 0.7? LM1 + 0.3? SemA ?
F4 0.6? LM1 + 0.4?DepSyn (0.3, 0.7, 0.0)
F5 0.6? LM1 + 0.1? POS + 0.3?DepSyn (0.0 1.0 0.0)
F6 0.5? LM1 + 0.2? SemA+ 0.3?DepSyn (0.2 0.7 0.1)
F7 0.4? LM1 + 0.1? POS + 0.3?DepSyn+ 0.2? SemA (0.2, 0.8, 0.0)
Table 4: The different combinations of scoring models using frequency-based disambiguation as a base-
line. The DepSyn weight triples corresponds to (?1, ?2, ?3) in Sect. 5.
Configuration KSPC DA KSPC ERR DA ERR
F1 1.015559 94.15% 0.00% 0.00%
F2 1.014829 94.31% 4.69% 2.72%
F3 1.014902 94.36% 4.22% 3.62%
F4 1.014462 94.56% 7.05% 7.04%
F5 1.013625 94.75% 12.43% 10.28%
F6 1.014159 94.62% 9.00% 8.10%
F7 1.013438 94.86% 13.63% 12.16%
Table 5: Results for the disambiguation based on word frequencies together with the semantic and syn-
tactic models.
Configuration Scoring model Bigram weights DepSyn weights
B1 1? LM2 (Bigram frequencies) (0.9, 0.1) ?
B2 0.9? LM2 + 0.1? POS (0.8, 0.2) ?
B3 0.95? LM2 + 0.05? SemA (0.8, 0.2) ?
B4 0.9? LM2 + 0.1?DepSyn (0.8, 0.2) (0.2, 0.8, 0.0)
B5 0.8? LM2 + 0.1? POS + 0.1? SemA (0.8, 0.2) ?
B6 0.81? LM2 + 0.08? POS + 0.11?DepSyn (0.8, 0.2) (0.2, 0.8, 0.0)
Table 6: The different combinations of scoring models using bigram-based disambiguation as baseline.
In addition to the DepSyn weights, this table also shows the language model interpolation weights, ?1
and ?2 described in Sect. 2.2.
Label KSPC DA KSPC ERR DA ERR
B1 1.012159254 95.48% 21.85% 22.81%
B2 1.011434213 95.75% 26.51% 27.41%
B3 1.011860573 95.50% 23.77% 23.20%
B4 1.011698693 95.62% 24.81% 25.19%
B5 1.011146932 95.80% 28.36% 28.23%
B6 1.010980592 95.91% 29.43% 30.09%
Table 7: Results for the disambiguation based on bigrams plus the semantic and syntactical models. The
error reduction rate is relative to the word frequency baseline.
48
Generating a 3D Simulation of a Car Accident from a Written
Description in Natural Language: the CarSim System
Sylvain DUPUY, Arjan EGGES, Vincent LEGENDRE, and Pierre NUGUES
GREYC laboratory - ISMRA
6, bd du Mare?chal Juin
F-14050 Caen, France
Email: {dupuy,vlegendr}@ensicaen.ismra.fr
pnugues@greyc.ismra.fr
egges@cs.utwente.nl
Abstract
This paper describes a prototype system
to visualize and animate 3D scenes from
car accident reports, written in French.
The problem of generating such a 3D
simulation can be divided into two sub-
tasks: the linguistic analysis and the vir-
tual scene generation. As a means of
communication between these two mod-
ules, we first designed a template for-
malism to represent a written accident
report. The CarSim system first pro-
cesses written reports, gathers relevant
information, and converts it into a for-
mal description. Then, it creates the cor-
responding 3D scene and animates the
vehicles.
1 Introduction
This paper describes a prototype system to visu-
alize and animate a 3D scene from a written de-
scription. It considers the narrow class of texts
describing car accident reports. Such a system
could be applied within insurance companies to
generate an animated scene from reports written
by drivers. The research is related to the TACIT
project (Pied et al, 1996) at the GREYC labora-
tory of the University of Caen and ISMRA.
There are few projects that consider automatic
scene generation from a written text, although
many projects exist that incorporate natural lan-
guage interaction in virtual worlds, like Ulysse
(Bersot et al, 1998; Gode?reaux et al, 1999) or An-
imNL (Badler et al, 1993). Visualizing a written
car accident report requires a different approach.
It is closer to projects focusing on text-to-scene
conversion, like WordsEye (Coyne and Sproat,
2001). However, unlike the latter, our objective
is to build an animation rather than a static pic-
ture and behavior of dynamic objects must then
be taken into account. There also exist systems
that carry out the reverse processing, from video
data to text description, as ANTLIMA (Blocher
and Schirra, 1995).
We present here an overview of the CarSim
system that includes a formalism to describe and
represent car accidents, a linguistic module that
summarizes car accident reports according to this
formalism, and a visualizing module that converts
formal descriptions to 3D animations. In our case,
the linguistic module has to deal with texts where
syntax and semantics involve time and space de-
scription and simultaneous actions of two or more
actors (i.e. the cars).
The remainder of our paper is organized as fol-
lows. Section 2 presents the formalism for describ-
ing an accident. Section 3 describes the template
filling methods that lead to the conversion of a
text into its formal representation. Section 4 cov-
ers planning techniques and accident modelling al-
gorithms that we use. Finally, Section 5 presents
and discusses the evaluation of the system on the
test corpus (MAIF corpus).
FD
Linguistic
analysis
Virtual scene
generator
Figure 1: The two subsystems and the FD (For-
mal Description) as a means of communication.
2 Formal Representation in
CarSim
?Ve?hicule B venant de ma gauche, je me
trouve dans le carrefour, a` faible vitesse en-
viron 40 km/h, quand le ve?hicule B, per-
cute mon ve?hicule, et me refuse la priorite?
a` droite. Le premier choc atteint mon aile
arrie`re gauche, sous le choc, et a` cause de
la chausse?e glissante, mon ve?hicule de?rape,
et percute la protection me?tallique d?un ar-
bre, d?ou` un second choc frontal.? Text A4,
MAIF corpus.
?I was driving on a crossroads with a
slow speed, approximately 40 km/h. Vehicle
B arrived from my left, ignored the priority
from the right and collided with my vehicle.
On the first impact, my rear fender on the
left side was hit and because of the slippery
road, I lost control of my vehicle and hit the
metallic protection of a tree, hence a second
frontal collision.? Text A4, MAIF corpus,
our translation.
The text above is an accident report from the
MAIF1 corpus, which contains 87 texts in French.
It is a good example of the possible contents of an
accident description: a rather complex interaction
between a set of different objects (two cars and a
tree). This section describes the formal represen-
tation used in the CarSim system. The example
of Text A4 will be explained with more details in
Section 2.5.
2.1 The General Accident Model
In CarSim, the general accident model con-
sists of three lists of objects: motionless objects
(STATIC), moving objects (DYNAMIC), and fi-
nally collisions (ACCIDENT).
STATIC and DYNAMIC lists describe the gen-
eral environment in which the accident takes
place. Knowing them, the accident itself is the
only remaining item to determine. Using manual
simulation, we realized that most accidents in the
corpus could be framed using an ordered list of
collisions2. Each collision is represented by a re-
lation between two objects either in DYNAMIC
and/or STATIC lists
2.2 Static Objects
In general, a static object can be defined with two
parameters: one defining the nature of the object
and another one that defines its location. In Car-
Sim, a static object can be either a road type or
an object that can participate in a collision (e.g.
a tree). In the formal description, a reference to
the latter kind of object can occur in a collision
specification. This is why these static objects are
defined with an identity parameter (ID).
Concerning ROAD objects, their nature is spec-
ified in the KIND parameter. The possible KIND
values in the present prototype are: crossroads,
straightroad, turn left, and turn right.
TREEs, LIGHTs (traffic lights), STOPSIGNs,
and CROSSINGs (pedestrian crossings) are the
1Mutuelle Assurance Automobile des Instituteurs
de France. MAIF is a French insurance company.
2Two collisions will never happen at the same time.
other possible static objects. Their location is
given by the COORD parameter. Since trees and
traffic lights can participate in collisions, they also
have an ID, that allows further references. Finally,
traffic lights contain a COLOR parameter to in-
dicate the color of the light (red, orange, green or
inactive).
2.3 Dynamic Objects
Dynamic objects cannot be defined by giving only
their nature and position. Rather than the posi-
tion, the movement of the object must be defined.
In the CarSim formal representation, each
dynamic object is represented by a VEHICLE,
with a KIND parameter indicating its nature,
(car or truck) and a unique identifier ID. The
movement of a dynamic object is defined by
two parameters. The INITIAL DIRECTION
defines the direction to which the object is
headed before it starts driving (north, south,
east, or west). The second parameter is an
ordered list of atomic movements that are de-
scribed by EVENTs. This list is called the event
chain and corresponds to the CHAIN parame-
ter. KIND specifies the nature of each event.
At present, CarSim recognizes the following
events: driving forward, stop, turn left, turn right,
change lane left, change lane right, overtake.
Figure 2 shows the motion of a dynamic object
with KIND = car, INITIAL DIRECTION = East
and CHAIN =<driving forward, turn left, driv-
ing forward>.
event 1
event 2
event 3
Figure 2: A crossroads with a vehicle driving for-
ward, turning left and driving forward with an
initial direction to the East.
2.4 Collisions
As we said before, the accident is described by
an ordered list of collisions. The order of the
collisions in the list corresponds to the order in
which they take place in the accident simulation.
A collision is defined by giving the two objects
that participate in the collision and some addi-
tional attributes. At present, these attributes are
the collision coordinates and the parts of the vehi-
cles that are involved in the collision (participat-
ing parts).
There is a slight distinction between the vehi-
cle that collides (in other words: the actor) and
the vehicle that is hit (the victim). For planning
reasons (and also for linguistic grounds) it is use-
ful to maintain this distinction in the formalism.
To summarize, a collision occurs between an actor
and a victim. The victim can be either a static or
a dynamic object, the actor clearly has to be a
dynamic object. The notions of actor and victim
are not related with the responsibility of one par-
ticular vehicle within the accident. This kind of
relationships must be deduced from a complex re-
sponsibilities analysis, that could be based on the
traffic rules.
Next to the location (coordinates) of the colli-
sion, something has to be said about the configu-
ration of the objects while colliding. The partici-
pating parts are sometimes given in the text, see
for example Text A4 at the beginning of this sec-
tion. The CarSim system uses a simplified model
of these vehicle parts. They are divided in four
categories: front, rear, leftside, and rightside, plus
one unknown category.
2.5 An Example
In order to give an example of a formal accident
description and also to introduce the linguistic
part, we will give now more details about the man-
ually written FD of Text A4.
In a written text, information can be given ei-
ther explicitly or implicitly. Besides, the contents
of implicit information differs in each text. In Text
A4, what information can we directly gather from
the text?
Text A4 describes an accident with two
collisions, involving two vehicles and a tree. It
takes place at a crossroads. The first collision in-
volves two vehicles. One of them is referred to
as the ?vehicle B?, the other is the narrator?s ve-
hicle (?my vehicle?). From now on, vehicles will
be called vehicleB and vehicleA respectively. The
second collision involves vehicleA and the tree. In
the FD, the tree is identified in a unique way as
tree1. From this information, we already know
how many objects will be needed to describe the
accident: two static objects (a crossroads and a
tree tree1 ), two dynamic objects (vehicleB and
vehicleA) and finally two collisions.
The text does not mention any special behavior
of the two vehicles. They are both driving when
the accident occurs. Hence, the event chain is the
same for both vehicles, a single driving forward
event.
The roles played by the vehicles in each colli-
sion are also given. As human beings, we deduce
them from the grammatical functions of the noun
groups or pronouns referring to the vehicles in the
sentences where collisions are described. In the
first collision, the actor is vehicleB and the victim
vehicleA (respectively, subject and object of the
verb ?percuter?, ?to collide with? in the transla-
tion). In the second one, the actor is vehicleA and
the victim tree1.
The parts of the vehicles that participate in
a collision are sometimes explicitly given in the
report, as for example for vehicleA in Text A4.
In the first collision, the impact occurs at the
rear left-hand side of the vehicle (?On the first
impact, my rear fender on the left side was hit?)
and in the second one, vehicleA hits the tree
with the front of the car (?hence a second frontal
collision?).
Actually, we don?t know whether the vehicles
in the text are cars, trucks or something else. As
no precise information is explicitly given in the
text, we simply assume that these vehicles are
cars3. The type of vehicles is not the only im-
plicit piece of information in the text. The initial
directions of the vehicles are only known relatively
to each other. We know that vehicleB is coming
from the left-hand side of vehicleA (?Vehicle B
arrived from my left?) and if we arbitrary decide
that vehicleA starts heading to the North, then
vehicleB has to start heading to the East. The
same fragment of the text gives us the participat-
ing part of vehicleB. Since the participating part
of vehicleA in the first collision is leftside, we can
conclude that vehicleB?s part is front. The tree
has no particular participating part. Thus, it will
be defined as unknown but we can assume that
the impact occurs with the trunk because all the
scene takes place in a two-dimensional plane.
Below is the formal description of Text A4 that
can be given to the simulation module of CarSim:
// Static objects
STATIC [
ROAD [
KIND = crossroads;
]
TREE [
ID = tree1; COORD = ( 5.0, -5.0 );
3car will be the default value of the KIND param-
eter of dynamic objects.
]
]
// Dynamic objects
DYNAMIC [
VEHICLE [
ID = vehicleB; KIND = car;
INITDIRECTION = east;
CHAIN [
EVENT [
KIND = driving_forward;
]
]
]
VEHICLE [
ID = vehicleA; KIND = car;
INITDIRECTION = north;
CHAIN [
EVENT [
KIND = driving_forward;
]
]
]
]
// Collision objects
ACCIDENT [
COLLISION [
ACTOR = vehicleB, front;
VICTIM = vehicleA, leftside;
COORD = ( 1.0, 1.0);
]
COLLISION [
ACTOR = vehicleA, front;
VICTIM = tree1, unknown;
]
]
The only information we did not discuss yet are
the coordinates of static objects and impacts. Co-
ordinates are numbers. They are never explicitly
given in the text and obviously, even if some num-
bers appeared in the text, the semantic of these
numbers would be implicit too. CarSim assumes
that coordinates (0,0) are the center of the scene.
In Text A4, the origin is the center of the cross-
roads. The first collision occurs in the crossroads,
hence the coordinates will be close to the origin.
The coordinates of the tree are chosen so that they
match the idea of the scene as a reader could imag-
ine it. They also depend on the size of the graphi-
cal objects that are used in the 3D scene (e.g. the
size of the roads).
3 The Information Extraction Task
The first stage of the CarSim processing chain is
an information extraction (IE) task that consists
in filling a template corresponding to the formal
accident description (FD) described in Section 2.
Such systems have been already implemented, as
FASTUS (Hobbs et al, 1996), and proved their
robustness. Our information retrieval subsystem
is restricted to car accident reports and is goal-
driven. The main idea is to start from a default
description, a pre-formatted FD, that the IE task
alters or refines using inference rules. Hence, the
default output will be a well-formatted FD, de-
scribing a collision between two cars, even if the
given text is a poem.
3.1 Parsing
The first step of the information extraction pro-
cess is a lexical analysis and a partial parsing. The
parser generates tokenized sentences, where noun
groups, verb groups, and prepositional groups are
extracted. The parser uses DCG rules (Pereira
and Shieber, 1987) and a dictionary containing
all the words that occur in the corpus.
3.2 Extracting Static Objects
The formalism describes two types of static ob-
jects: the type of road (the road configuration)
and some other static objects (stop signs, traf-
fic lights, pedestrian crossings and trees). The
method used to extract these objects consists in
looking up for keywords in the tokenized text.
The extraction of static objects is done at the
beginning of the information extraction task. We
realized that the road configuration is the most
relevant piece of information in the description of
an accident, since it conditions all the following
steps (see Section 3.4 for further explanations).
The formalism considers four different config-
urations: straightroad, crossroads, turn left, and
turn right. In the present system, we restricted it
to three types of road:
? crossroads, indicated by cue words such
as ?carrefour?, ?intersection?, ?croisement?
(crossroads, intersection, junction).
? turn left, with cues such as ?virage?,
?courbe?, ?tournant? (bend, curb, turn).
We assume that turn left and turn right are
equivalent.
? straightroad, that corresponds to the situa-
tion when none of the previous words have
been found.
3.3 Extracting Collisions
A collision consists of a verb, an actor, a vic-
tim and of the participating parts of the two
vehicles. We select verbs describing a colli-
sion such as ?heurter? (?to hit?), ?taper? (?to
bang?), ?percuter? (?to crash into?), ?toucher?
(?to touch?),. . .
For each extracted verb, the system checks
whether the verb group is in passive or active
form, then identify the related grammatical rela-
tions: subject-verb and verb-object or verb-agent.
Extraction techniques of such dependencies have
already been implemented, as in (A??t-Mokhtar
and Chanod, 1997). Our system uses three pred-
icates in order to find the subject (find subject)
and either the object (find object) or the agent
(find agent) of the verb. If the verb is in an active
form, it makes the assumption that the subject
and the object of the verb will be respectively the
actor and the victim of the collision. In the case
of a passive form, the subject will be the victim
and the agent, the actor.
Below is the sketch of the algorithm of these
three predicates:
? find subject looks for the last noun group be-
fore the verb that describes a valid actor, that
is a vehicle or a personal pronoun like ?je?
(?I?), ?il? (?he?), or ?nous? (?we?) .
? find object starts looking for the first noun
group after the verb that describes a valid
victim, that is both vehicles and static ob-
jects. If no valid victim is found, it searches
for a reflexive or personal pronoun inside the
verb group. In case of failure, the first noun
group after the verb is chosen.
? find agent looks for a valid actor in a prepo-
sitional group introduced by ?par? (?by?).
3.4 Generating Collisions and Dynamic
Objects
For each collision, the system tries to extract the
participating parts of the vehicles in the noun
groups that refer to the actor and the victim. To
do this, it looks for cues like ?avant?, ?arrie`re?,
?droite?, or ?gauche? (?front?, ?rear?, ?right?, or
?left?).
Then, the system creates two dynamic objects
(see Section 3.5) and a collision between them.
The generated properties of the collision depend
on the road configuration:
? Straight road: the first vehicle heads to the
East, the other one starts from the opposite
end of the road, heading to the West. The
collision is a head-on impact.
? Turn: The first vehicle starts heading to the
East, then turns to the Left. The second one
starts heading to the South, then turns to the
Right. The collision is frontal and happens at
the center of the turn.
? Crossroads: We choose to represent here the
most frequent traffic offence (in France). The
first vehicle drives straight to the East, the
second one drives to the North. The front of
the actor?s vehicle collides with the left-hand
side of the victim.
As we do not extract the initial directions of
the vehicles, these three cases are the only possi-
ble ones. When the system cannot find the actor
or the victim of a collision, default objects are cre-
ated matching the road configuration.
3.5 Deleting Useless Objects
When creating collision objects, two new vehicles
are instantiated for each collision, even if the vic-
tim is a static object. Moreover, one vehicle can
obviously participate in several collisions. All the
unnecessary vehicles should then be thrown away.
A vehicle that represents a static object can be
removed easily, since the real static object still
exists. All we have to do is to modify the reference
given in the victim parameter of the collision in
the template, then delete the redundant vehicle.
Deleting the duplicates is more difficult and in-
volves a coreference resolution. An identification
mechanism of the narrator has been added to the
system. All the personal pronouns in the first per-
son or some expressions like ?the vehicle A? will
be designated with the id enunciator. In the other
cases, coreference occurs only when the two ids
are strictly the same (in the sense of string com-
parison). Then, the system keeps only the first
created object between the duplicates and delete
the others.
3.6 Extracting Event Chains
The vehicles generally do not drive straight
forward. They carry out two or more
successive actions. In the formal descrip-
tion, these possible actions correspond to the
events of dynamic objects and are in limited
number: driving forward, turn left, turn right,
change lane right, change lane left, overtake, and
stop.
In written reports, these actions are mostly in-
dicated by verbs. The system has to identify them
and to link the corresponding event(s) to the ap-
propriate vehicle. When the subject is identified
as the narrator, the link is obvious. In the other
cases, if there are only two vehicles, the narra-
tor and another one, a new event is added to the
event chain of the second vehicle. Otherwise, the
system checks whether the subject of the verb is
strictly identical (string comparison) to one vehi-
cle?s id. In this case, a new event is also created
and added to the event chain. Some verbs imply
multiple events, e.g. ?rede?marrer? (?to get driv-
ing again?) that indicates that the driver stopped
beforehand. Consequently, a stop event then a
driving forward event are added.
With this simple extraction mechanism, the or-
der of the events in the event chain does not neces-
sarily respect the chronology but rather the order
of the text. We assume that the story is linear,
which is the case in most accident reports.
3.7 Writing the Formal Description
The final step of the linguistic part consists in for-
matting a template corresponding to the accident
description. Because the inferred facts have ex-
actly the same attributes as the formalism?s el-
ements, a very simple transcription algorithm is
used to convert the facts in a text file that can be
processed afterwards by the simulator.
4 Planning
Planning complex events like collisions requires
a well-defined and flexible planning architecture.
General planning algorithms which apply methods
incorporating artificial intelligence, are discussed
in (Nilsson, 1998). The CarSim planner is much
more straightforward, because the planning pro-
cess is not as complex as a lot of traditional AI
planning problems, see also (Norvig and Russell,
1995). The total planning process is performed by
using five different subplanners, which all perform
a small part of the total planning task.
4.1 The Preplanner
The preplanner is a planner that ensures the con-
sistency of the formal description. If some values
are not given (e.g. coordinates of a static object or
initial directions of dynamic objects) or some val-
ues imply a contradiction (a vehicle turning left on
a straight road), this planner tries to find (default)
values and to solve the conflicts. This planner is
a simple knowledge base, as discussed in (Norvig
and Russell, 1995).
4.2 The Position Planner
The position planner estimates the start and end
positions of the vehicles in the simulation. By de-
fault, a vehicle is placed 20 meters away from the
center of the (cross)road. If two or more vehicles
are moving in the same direction, they can?t all be
placed at this distance because they would over-
lap. Therefore, if there is more than one vehicle
facing a particular direction, the second vehicle is
placed at a distance of 26 meters from the center
and if there is a third vehicle, it is placed at 32 me-
ters from the center4. Regarding the end points of
the vehicles, the vehicle that is placed closest to
the center, will have its end point placed farther
away from the center. The vehicle initially having
a start point far away from the center will have an
end point close to the center, so that every vehicle
traverses approximately the same distance.
4.3 The Trajectory Planner
Based on the (very global) description of the
movement of every vehicle in the formal model,
this planner constructs a trajectory, represented
by a set of points in the Euclidian space. Every
event in the event chain is converted to a list of
trajectory points. A turn is approximated by a
number of points lying on a circle arc. Overtak-
ing is modelled by using a goniometrical function.
4.4 The Accident Planner
The accident planner uses the trajectory that is
created by the trajectory planner. Since event
chains only include atomic movements and not
collisions, this trajectory is planned as if there was
no collision at all. The task of the accident plan-
ner is to change this trajectory in such a way that
it incorporates the collision. Some part of it has
to be thrown away and an alternative part (which
ultimately leads to the point of collision) has to be
added to the trajectory. For every vehicle, actor
or victim, the trajectory is thus changed in two
steps:
1. Remove a part of the trajectory.
2. Add a part to the trajectory so that the fi-
nal result will be a trajectory that leads the
vehicle to the point of collision.
The part of the trajectory that has to be re-
moved depends on the coordinates where the colli-
sion occurs. We designed an algorithm that draws
a circle around the collision point and removes the
trajectory part that lies within the circle region.
Also, the segment that comes after the removed
trajectory part is thrown away, because a trajec-
tory does not allow gaps. The radius of the circle
is thus a parameter that defines the precision of
the algorithm. If a large radius is chosen, a large
part of the trajectory will be removed. An appli-
cation of the algorithm using a small radius only
removes the trajectory part closest to the collision
point.
4In the CarSim system, the maximum number of
vehicles that can have the same initial direction is
three.
4.5 The Temporal Planner
The temporal planner of the CarSim system is
not a planner in the sense of the planners de-
scribed in (Nilsson, 1998) The temporal planner
of the CarSim system plans the temporal values
of the trajectory in two steps. Generally, a trajec-
tory consists of a number of ?normal? trajectory
points, followed by a number of trajectory points
that represent a collision. First the segment that
is not part of any collision is planned. After that,
the system plans the remaining segment. In the
CarSim system, every trajectory point has a time
value. This is a value between 0 and 1, with 0 rep-
resenting the beginning of the simulation and 1
being the end of it. The temporal planner tries to
find time values for the trajectory points so that
the collisions happen in a natural way.
5 Results and Discussion
The CarSim system has been implemented and
evaluated over the MAIF corpus. The assessment
method does not consist, as usually done with IE
systems, in calculating a precision and a recall.
Our objective is to design a system that carries
out the whole processing chain, that is from a
written report up to a 3D animation. Therefore,
we preferred to compare the simulation with the
understanding and mental representation of the
scene that could have a human reader. This im-
plies that some aspects of the formal description
are not taken into account when evaluating the
system, e.g. we assume that the value of the INI-
TIAL DIRECTION parameter is less important
than the positions of the vehicles relatively to each
other. Hence, we considered that the result is ac-
ceptable as far as the latter is correct.
According to such criteria, we considered that
the simulation provided by the system corre-
sponds, in 17% of the texts, with what could have
imagined a human being. Figure 3 & 4 show the
two collisions described in Text A4.
Failure cases have many different grounds.
They may be related either to the IE task, to
the simulator, or to a lack of cooperation between
the two subsystems. Evaluating separately each
subsystem leads to a better understanding of the
actual limits of the system.
Feeding the simulator with manually written
formal descriptions provides a good way to eval-
uate it for itself. According to such tests, the
CarSim system generates an acceptable simula-
tion of almost 60% of the reports. This implies
that the results of the overall system will be lower.
CarSim?s simulator does not succeed in simulat-
ing manually written formal descriptions because
Figure 3: The first collision in Text A4.
Figure 4: The second collision in Text A4.
of three main causes: expressivity of the formal-
ism that does not cover all possible accidents (e.g.
synchronization between event chains of different
objects), the restricted number of scenarios con-
sidered by the CarSim visualizer and the limited
database of 3D graphical objects. Depending on
the text, the failure is the result of either only
one of these restrictions or a combination. Future
work on the project will focus on these issues.
The efficiency of the IE task varies with the
nature of extracted information. First, the results
clearly depend on the accuracy with which the
system can correctly extract impacts, that is
find the verb representing the collision and also
resolve the actor, the victim and possibly their
participating parts5. This task is successfully
accomplished in 69% of the texts6. In addition,
the system correctly extracts EVENTS in 35% of
the texts. This means that in 35% of the texts,
all the events are properly extracted with a good
ordering.
5when the parts are explicitly described
6In the rest, it generates default impacts or impacts
are erroneous.
Concerning time and space information, the
system provides only simple mechanisms to ob-
tain them. Our system is at an early stage and
our objective when designing it was to see whether
such an approach was feasible. It represents a
sort of improved baseline with which we can com-
pare further results. At this time, the temporal
information known by the system is restricted to
the events associated with dynamic objects. Our
method assumes that they are given in the text in
the same order they occur in reality. This is a sim-
plification that proves wrong in some reports. Fur-
ther improvements could take into account tenses
of verbs, temporal adverbs and prepositions, so
that the system could determine the real chrono-
logical relationships between events.
A similar comment can be given with regards to
spatial information. In CarSim, the spatial con-
figuration (the background of the scene) is given
mainly by the type of roads. The extraction of
participating parts also provides additional in-
formation that influence the relative positions of
the vehicles when colliding. During preplanning
stage, the system checks the consistency of the
FD and tries to resolve conflicts between the dif-
ferent information. At present, initial directions
of the vehicles depend only on the background of
the scene, that is the road configuration. The co-
ordinates are also chosen arbitrary from the be-
ginning. See for example the tree referred as tree1
in Text A4: no information about its location is
given in the text. The only facts relative to it that
we can deduce from the original report are its ex-
istence and its involvement in a collision. More-
over, the problem of choosing a referential from
which to calculate coordinates is quite unsolvable
for texts that do not mention it explicitly. The IE
task could involve deeper semantic analysis that
provides means of constructing a more global spa-
tial representation of the scene.
6 Conclusion
This paper has presented a prototype system that
is able to process correctly 17% of our corpus of
car accident reports up to a 3D simulation of the
scene. The chosen approach divides the task be-
tween information extraction to fill templates and
planning to animate the scene. It leads to en-
couraging results, considering that the informa-
tion retrieval could be improved by integrating
more elaborate methods to deal with space and
time in written texts.
References
S. A??t-Mokhtar and J-P. Chanod. 1997. Subject
and object dependency extraction using finite-
state transducers. In Proceedings of ACL work-
shop on Automatic Information Extraction and
Building of Lexical Semantic Resources for NLP
Applications.
N. Badler, W. Becket, B. Di Eugenio, C. Geib,
L. Levison, M. Moore, B. Webber, M. White,
and X. Zhao. 1993. Intentions and expectations
in animating instructions: the AnimNL project.
In Intentions in Animation and Action. Insti-
tute for Research in Cognitive Science, Univer-
sity of Pennsylvania, March.
O. Bersot, P.O. El-Guedj, C. Gode?reaux, and
P. Nugues. 1998. A conversational agent to
help navigation and collaboration in virtual
worlds. Virtual Reality, 3(1):71?82.
A. Blocher and J.R.J. Schirra. 1995. Optional
deep case filling and focus control with mental
images: ANTLIMA-KOREF. In Proceedings of
IJCAI-95, pages 417?423.
R.E. Coyne and R. Sproat. 2001. Wordseye:
An automatic text-to-scene conversion system.
In Proceedings of International Conference on
Computer Graphics and Interactive Technolo-
gies (SIGGRAPH 2001). AT&T Research Lab.
C. Gode?reaux, P.O. El-Guedj, F. Revolta, and
P. Nugues. 1999. Ulysse: An interactive, spo-
ken dialogue interface to navigate in virtual
worlds, lexical, syntactic, and semantic issues.
In John Vince and Ray Earnshaw, editors, Vir-
tual Worlds on the Internet, chapter 4, pages
53?70. IEEE Computer Society Press.
J.R. Hobbs, D. Appelt, J. Bear, D. Israel,
M. Kameyama, M. Stickel, and M. Tyson. 1996.
FASTUS: A cascaded finite-state transducer for
extracting information from natural-language
text. In Roche and Schabes, editors, Finite
State Devices for Natural Language Processing.
MIT Press.
N.J. Nilsson. 1998. Artificial Intelligence, a New
Synthesis. Morgan Kaufmann Publishers, Inc.
P. Norvig and S.J. Russell. 1995. Artificial intel-
ligence: a modern approach. Prentice Hall.
F.C.N. Pereira and S.M. Shieber. 1987. Prolog
and Natural Language Analysis. Stanford Uni-
versity. CSLI Lecture Notes No.10.
F. Pied, C. Poirier, P. Enjalbert, and B. Vic-
torri. 1996. From language to model. In
Workshop Corpus-Oriented Semantic Analysis
in European Conference on Artificial Intelli-
gence (ECAI), August.
191
192
193
194
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 824?832,
Beijing, August 2010
Automatic Discovery of Feature Sets for Dependency Parsing
Peter Nilsson Pierre Nugues
Department of Computer Science
Lund University
peter.nilsson.lund@telia.com Pierre.Nugues@cs.lth.se
Abstract
This paper describes a search procedure
to discover optimal feature sets for depen-
dency parsers. The search applies to the
shift?reduce algorithm and the feature sets
are extracted from the parser configura-
tion. The initial feature is limited to the
first word in the input queue. Then, the
procedure uses a set of rules founded on
the assumption that topological neighbors
of significant features in the dependency
graph may also have a significant contri-
bution. The search can be fully automated
and the level of greediness adjusted with
the number of features examined at each
iteration of the discovery procedure.
Using our automated feature discovery
on two corpora, the Swedish corpus in
CoNLL-X and the English corpus in
CoNLL 2008, and a single parser system,
we could reach results comparable or bet-
ter than the best scores reported in these
evaluations. The CoNLL 2008 test set
contains, in addition to a Wall Street Jour-
nal (WSJ) section, an out-of-domain sam-
ple from the Brown corpus. With sets of
15 features, we obtained a labeled attach-
ment score of 84.21 for Swedish, 88.11 on
the WSJ test set, and 81.33 on the Brown
test set.
1 Introduction
The selection of relevant feature sets is crucial
to the performance of dependency parsers and
this process is still in large part manual. More-
over, feature sets are specific to the languages be-
ing analyzed and a set optimal for, say, English
can yield poor results in Chinese. With depen-
dency parsers being applied today to dozens of
languages, this makes the parametrization of a
parser both a tedious and time-consuming opera-
tion. Incidentally, the advent of machine-learning
methods seems to have shifted the tuning steps in
parsing from polishing up grammar rules to the
optimization of feature sets. And as with the writ-
ing of a grammar, the selection of features is a
challenging task that often requires a good deal of
effort and inspiration.
Most automatic procedures to build feature sets
resort to greedy algorithms. Forward selection
constructs a set by adding incrementally features
from a predetermined superset while backward
elimination removes them from the superset (At-
tardi et al, 2007). Both methods are sometimes
combined (Nivre et al, 2006b). The selection pro-
cedures evaluate the relevance of a candidate fea-
ture in a set by its impact on the overall parsing
score: Does this candidate improve or decrease
the performance of the set?
Greedy search, although it simplifies the design
of feature sets, shows a major drawback as it starts
from a closed superset of what are believed to be
the relevant features. There is a broad consen-
sus on a common feature set including the words
close to the top of the stack or the beginning of the
queue, for the shift?reduce algorithm, but no clear
idea on the limits of this set.
In this paper, we describe an automatic discov-
ery procedure that is not bounded by any prior
knowledge of a set of potentially relevant features.
It applies to the shift?reduce algorithm and the ini-
824
tial feature consists solely of the first word of the
queue. The search explores nodes along axes of
the parser?s data structures and the partially built
graph using proximity rules to uncover sequences
of relevant, efficient features. Using this proce-
dure on the Swedish corpus in CoNLL-X and the
English corpus in CoNLL 2008, we built feature
sets that enabled us to reach a labeled attachment
score of 84.21 for Swedish, 88.11 on the Wall
Street Journal section of CoNLL 2008, and 81.33
on the Brown part of it with a set cardinality of 15.
2 Transition-based Parsing
Transition-based methods (Covington, 2001;
Nivre, 2003; Yamada and Matsumoto, 2003;
Zhang and Clark, 2009) have become a popular
approach in multilingual dependency parsing be-
cause of their speed and performance. Transition-
based methods share common properties and
build a dependency graph from a sequence of ac-
tions, where each action is determined using a fea-
ture function. In a data-driven context, the func-
tion is typically implemented as a classifier and
the features are extracted from the partially built
graph and the parser?s data structures, most often
a queue and a stack.
2.1 Parser Implementation
In this study, we built a parser using Nivre?s al-
gorithm (Nivre, 2003). The parser complexity is
linear and parsing completes in at most 2n+1 op-
erations, where n is the length of the sentence. Ta-
ble 1 shows the transitions and actions to construct
a dependency graph.
Given a sentence to parse, we used a classifier-
based guide to predict the transition sequence to
apply. At each step, the guide extracts features
from the parser configuration and uses them as in-
put to a classifier to predict the next transition. Be-
fore training the classification models, we projec-
tivized the corpus sentences (Kunze, 1967; Nivre
and Nilsson, 2005). We did not attempt to recover
nonprojective sentences after parsing.
2.2 Training and Parsing Procedure
We extracted the features using a gold-standard
parsing of the training set. We organized the clas-
sification, and hence the feature extraction, as a
Action Parser configuration
Init. ?nil,W, /0?
End ?S,nil,G?
LeftArc ?n|S,n?|Q,G? ?
?S,n?|Q,G?{?n?,n?}?
RightArc ?n|S,n?|Q,G? ?
?n?|n|S,Q,G?{?n,n??}?
Reduce ?n|S,Q,G? ? ?S,Q,G?
Shift ?S,n|Q,G? ? ?n|S,Q,G?
Table 1: Parser transitions (Nivre, 2003). W is
the input, G, the graph, S, the stack, and Q, the
queue. The triple ?S,Q,G? represents the parser
configuration and n, n ?, and n?? are lexical tokens.
?n?,n? represents an arc from n? to n.
two-step process. The first step determines the ac-
tion among LeftArc, RightArc, Reduce, and Shift;
the second one, the grammatical function, if the
action is either a left arc or a right arc.
Once the features are extracted, we train the
corresponding models that we apply to the test
corpus to predict the actions and the arc labels.
3 Feature Discovery
We designed an automatic procedure to discover
and select features that is guided by the structure
of the graph being constructed. The search al-
gorithm is based on the assumption that if a fea-
ture makes a significant contribution to the parsing
performance, then one or more of its topological
neighbors in the dependency graph may also be
significant. The initial state, from which we de-
rive the initial feature, consists of the first word in
the queue. There is no other prior knowledge on
the features.
3.1 Node Attributes
In the discovery procedure, we considered the
nodes of four data structures: the queue, the stack,
the sentence, and the graph being constructed.
We extracted three attributes (or fields) from each
node: two static ones, the lexical value of the
node and its part of speech, and a dynamic one
evaluated at parse time: the dependency label of
the arc linking the node to its head, if it exists.
We denoted the attributes of node w, respectively,
825
LEX(w), POS(w), and DEP(w). These attributes
are used as input by most dependency parsers,
whatever the language being parsed.
3.2 Search Axes
The feature search covers three different axes: the
parser?s data structures ? the queue and the stack
?, the graph being constructed, and the sentence.
Given a feature set at step n of the discovery pro-
cedure, we defined a successor function that gen-
erates the set of topological neighbors of all the
members in the feature set alng these three axes.
For a particular feature:
The data structure axis consists of the nodes in
the stack and the queue. The immediate
neighbors of a node in the stack are the ad-
jacent nodes above and below. In the queue,
these are the adjacent nodes before and af-
ter it. The top node on the stack and the
next node in the queue have a special con-
nection, since they are the ones used by the
parser when creating an arc. Therefore, we
considered them as immediate neighbors to
each other. For a node that is neither in the
stack, nor in the queue, there is no connec-
tion along this axis.
The graph axes traverse the partially con-
structed graph horizontally and vertically.
The horizontal axis corresponds to the
sibling nodes connected by a common head
(Figure 1). The immediate neighbors of a
node are its nearest siblings to the left and
to the right. The vertical axis corresponds
to the head and child nodes. The immediate
neighbors are the head node as well as the
leftmost and rightmost child nodes. There is
no connection for nodes not yet part of the
graph.
The sentence axis traverses the nodes in the or-
der they occur in the original sentence. The
immediate neighbors of a node are the previ-
ous and next words in the sentence.
4 Representing Features and Their
Neighbors
We represented features with a parameter format
partly inspired by MaltParser (Nivre et al, 2006a).
Head
Left sibling
CN
Right sibling
Leftmost child Rightmost child
Vertical axis
Horizontal axis
Figure 1: The vertical and horizontal axes, respec-
tively in light and dark gray, relative to CN.
Each parameter consists of two parts. The first
one represents a node in a data structure (STACK
or QUEUE) and an attribute:
The nodes are identified using a zero-based in-
dex. Thus STACK1 designates the second
node on the stack.
The attribute of a node is one of part of speech
(POS), lexical value (LEX), or dependency
label (DEP), as for instance LEX(QUEUE0)
that corresponds to the lexical value of the
first token in the queue.
The second part of the parameter is an optional
navigation path that allows to find other destina-
tion nodes in the graph. It consists of a sequence
of instructions to move from the start node to the
destination node. The list of possible instructions
are:
? h: head of the current node;
? lc/rc: leftmost/rightmost child of the node;
? pw/ f w: previous/following word of the
node in the original sentence.
An example of a feature obtained using the nav-
igation part is POS(STACK1 lc pw), which is in-
terpreted as: start from STACK1. Then, using the
instructions lc and pw, move to the left child of the
start node and to the previous word of this child in
the sentence. The requested feature is the part of
speech of the destination node.
826
5 Initial State and Successor Function
The feature discovery is an iterative procedure that
grows the feature set with one new feature at each
iteration. We called generation such an iteration,
where generation 1 consists of a single node. We
denoted FeatSeti = { f1, f2, ..., fi} the feature set
obtained at generation i.
Although the features of a classifier can be
viewed as a set, we also considered them as a tu-
ple, where Feati = ? f1, f2, ..., fi? is the i-tuple at
generation i and fk, the individual feature discov-
ered at generation k with 1 6 k 6 i. This enables
us to keep the order in which the individual fea-
tures are obtained during the search.
5.1 Initial State
We start the feature search with the empty set, /0,
that, by convention, has one neighbor: the first
node in the queue QUEUE0. We chose this node
because this is the only one which is certain to
exist all along the parsing process. Intuitively,
this is also obvious that QUEUE0 plays a signifi-
cant role when deciding a parsing action. We de-
fined the successor function of the empty set as:
SUCC( /0) = {POS(QUEUE0),LEX(QUEUE0)}.
5.2 Successors of a Node
The successors of a node consist of itself and all
its topological neighbors along the three axes with
their three possible attributes: part of speech, lex-
ical value, and dependency label. For a particular
feature in FeatSet, the generation of its successors
is carried out through the following steps:
1. Interpret the feature with its possible naviga-
tion path and identify the destination node n.
2. Find all existing immediate neighboring
nodes of n along the three search axes.
3. Assign the set of attributes ? POS, LEX , and
DEP ? to n and its neighboring nodes.
If at any step the requested node does not exist,
the feature evaluates to NOTHING.
5.3 Rules to Generate Neighbors
The generation of all the neighbors of the features
in FeatSet may create duplicates as a same node
can sometimes be reached from multiple paths.
For instance, if we move to the leftmost child of a
node and then to the head of this child, we return
to the original node.
To compute the successor function, we built a
set of rules shown in Table 2. It corresponds to
a subset of the rules described in the axis search
(Sect. 3.2) so that it omits the neighbors of a node
that would unavoidably create redundancies. The
third column in Table 2 shows the rules to gener-
ate the neighbors of POS(QUEUE0). They corre-
spond to the rows:
PL. This stands for the POS and LEX attributes
of the node. We only add LEX(QUEUE0)
as we already have POS(QUEUE0).
PLD lc and PLD rc. POS, LEX , and DEP of the
node?s leftmost and rightmost children.
PLD pw. POS, LEX , and DEP of the previous
word in the original string. The following
word is the same as the next node in the
queue, which is added in the next step. For
that reason, following word is not added.
PL QUEUE1. POS and LEX of QUEUE1.
PLD STACK0. POS, LEX , and DEP of STACK0.
This rule connects the queue to the top node
of the stack.
Table 3 summarizes the results of the rule appli-
cation and shows the complete list of successors
of POS(QUEUE0). In this way, the search for a
node?s neighbors along the axes is reduced to one
direction, either left or right, or up or down, that
will depend on the topological relation that intro-
duced the node in the feature set.
6 Feature Selection Algorithm
At each generation, we compute the Cartesian
product of the current feature tuple Feati and the
set defined by its neighbors. We define the set of
candidate tuples CandFeati+1 at generation i+ 1
as:
CandFeati+1 = {Feati}?SUCC(Feati),
where we have Card(CandFeati+1) =
Card(SUCC(Feati)).
The members of CandFeati+1 are ranked ac-
cording to their parsing score on the development
827
Data structures Navigation paths
STACK0 STACKn,n > 0 QUEUE0 QUEUEn,n > 0 h lc, rc ls rs pw fw
PLD PLD PL PL h h h
PLD h PLD h lc lc lc lc lc
PLD lc PLD lc PLD lc rc rc rc rc rc
PLD rc PLD rc PLD rc ls ls ls ls ls
PLD ls PLD ls rs rs rs rs rs
PLD rs PLD rs pw pw pw pw pw
PLD pw PLD pw PLD pw fw fw fw fw fw
PLD fw PLD fw
PLD STACK1 PLD STACKn+1 PL QUEUE1 PL QUEUEn+1
PL QUEUE0 PLD STACK0
Table 2: Rules to compute the successors of a node. For each node category given in row 2, the
procedure adds the features in the column headed by the category. PLD stands for the POS, LEX ,
and DEP attributes. In the right-hand side of the table, the category corresponds to the last instruction
of the navigation path, if it exists, for instance pw in the feature POS(STACK1 lc pw). We read the
six successors of this node in the fifth column headed by pw: STACK1 lc pw h, STACK1 lc pw lc,
STACK1 lc pw rc, STACK1 lc pw ls, STACK1 lc pw rs, and STACK1 lc pw pw. We then apply all the
attributes to these destination nodes to generate the features.
Initial feature POS QUEUE 0
Successors LEX QUEUE 0
PLD QUEUE 0 lc
PLD QUEUE 0 rc
PLD QUEUE 0 pw
PL QUEUE 1
PLD STACK 0
Table 3: Features generated by the successor func-
tion SUCC({POS(QUEUE0)}). PLD stands for
the three attributes POS, LEX , and DEP of the
node; PL for POS and LEX .
set and when applying a greedy best-first search,
Feati+1 is assigned with the tuple yielding the
highest score:
Feati+1 ? eval best(CandFeati+1).
The procedure is repeated with the immediate
neighbors of Feati+1 until the improvement of the
score is below a certain threshold.
We extended this greedy version of the discov-
ery with a beam search that retains the N-best
successors from the candidate set. In our exper-
iments, we used beam widths of 4 and 8.
7 Experimental Setup
In a first experiment, we used the Swedish cor-
pus of the CoNLL-X shared task (Buchholz and
Marsi, 2006). In a second experiment, we applied
the feature discovery procedure to the English cor-
pus from CoNLL 2008 (Surdeanu et al, 2008), a
dependency corpus converted from the Penn Tree-
bank and the Brown corpus. In both experiments,
we used the LIBSVM package (Chang and Lin,
2001) with a quadratic kernel, ? = 0.2, C = 0.4,
and ? = 0.1. These parameters are identical to
Nivre et al (2006b) to enable a comparison of the
scores.
We evaluated the feature candidates on a de-
velopment set using the labeled and unlabeled at-
tachment scores (LAS and UAS) that we com-
puted with the eval.pl script from CoNLL-X.
As there was no development set for the Swedish
corpus, we created one by picking out every 10th
sentence from the training set. The training was
then carried out on the remaining part of the set.
8 Feature Discovery on a Swedish
Corpus
In a first run, the search was optimized for the
UAS. In a second one, we optimized the LAS. We
also report the results we obtained subsequently
on the CoNLL-X test set as an indicator of how
828
well the training generalized.
8.1 The First and Second Generations
Table 4 shows the feature performance at the first
generation sorted by UAS. The first row shows the
two initial feature candidates, ?POS(QUEUE0)?
and ?LEX(QUEUE0)?. The third row shows
the score produced by the initial features alone.
The next rows show the unlabeled and labeled
attachment scores with feature pairs combining
one of the initial features and the one listed in
the row. The combination of POS(QUEUE0)
and POS(STACK0) yielded the best UAS: 74.02.
The second feature improves the performance of
POS(QUEUE0) by more than 30 points from
43.49.
For each generation, we applied a beam
search. We kept the eight best pairs as start-
ing states for the second generation and we
added their neighboring nodes. Table 5 shows
the eight best results out of 38 for the pair
?POS(QUEUE0),POS(STACK0)?.
Parent state: ?POS(QUEUE0),POS(STACK0)?
Dev set Test set
UAS LAS UAS LAS Successors
79.50 65.34 79.07 65.86 P QUEUE 1
78.73 66.98 76.04 64.51 L STACK 0 fw
77.42 63.08 74.63 61.86 L QUEUE 1
77.06 64.54 75.28 62.90 L QUEUE 0 pw
76.83 66.01 73.61 63.77 L QUEUE 0
76.63 63.62 74.75 63.17 P STACK 0 fw
76.44 64.24 74.09 62.02 L STACK 0
76.39 63.12 73.99 61.16 L QUEUE 0 lc
Table 5: Ranking the successors of
?POS(QUEUE0),POS(STACK0)? on the
Swedish corpus. Out of the 38 successors,
we show the eight that yielded the best results. P
stands for POS, L for LEX , and D for DEP.
8.2 Optimizing the Unlabeled Attachement
Score
We iterated the process over a total of 16 gener-
ations. Table 6, left-hand side, shows the list of
the best scores for each generation. The scores on
the development set increased steadily until gen-
eration 13, then reached a plateau, and declined
around generation 15. The test set closely fol-
lowed the development set with values about 1%
lower. On this set, we reached a peak performance
at generation 12, after which the results decreased.
Table 6, right-hand side, shows the features pro-
ducing the final score in their order of inclusion
in the feature set. As we applied a beam search,
a feature listed at generation i does not necessary
correspond to the highest score for this generation,
but belongs to the feature tuple producing the best
result at generation 16.
8.3 Optimizing the Labeled Attachement
Score
We also applied the feature discovery with a
search optimized for the labeled attachment score.
This time, we reduced the beam width used in the
search from 8 to 4 as we noticed that the candi-
dates between ranks 5 and 8 never contributed to
the best scoring feature set for any generation.
We observed a score curve similar to that of the
UAS-optimized search. The train set followed the
development set with increasing values for each
generation but 1-2% lower. The optimal value was
obtained at generation 15 with 84.21% for the test
set. Then, the score for the test set decreased.
9 Feature Discovery on a Corpus of
English
The training and development sets of the CoNLL
2008 corpus contain text from the Wall Street
Journal exclusively. The test set contains text
from the Brown corpus as well as from the Wall
Street Journal. Table 7 shows the results after 16
generations. We used a beam width of 4 and the
tests were optimized for the unlabeled attachment
score. As for Swedish, we reached the best scores
around generation 14-15. The results on the in-
domain test set peaked at 90.89 and exceeded the
results on the development set. As expected, the
results for the out-of-domain corpus were lower,
87.50, however the drop was limited to 3.4.
10 Discussion and Conclusion
The results we attained with feature set sizes as
small as 15 are competitive or better than figures
829
Parent state ?POS(QUEUE0)? ?LEX(QUEUE0)?
UAS LAS Successors UAS LAS Successors
43.49 26.45 None 42.76 23.56 None
74.02 59.67 POS STACK 0 65.86 52.18 POS STACK 0
67.77 54.50 LEX STACK 0 58.59 45.51 LEX STACK 0
58.37 41.83 POS QUEUE 0 pw 51.98 37.70 POS QUEUE 0 pw
55.28 38.49 LEX QUEUE 0 pw 50.44 29.71 POS QUEUE 1
51.53 30.43 POS QUEUE 1 50.38 35.24 LEX QUEUE 0 pw
51.05 32.66 LEX QUEUE 0 lc 49.37 32.27 POS QUEUE 0
49.71 31.54 POS QUEUE 0 lc 48.91 27.77 LEX QUEUE 1
49.49 29.18 LEX QUEUE 1 48.66 29.91 LEX QUEUE 0 lc
49.37 32.27 LEX QUEUE 0 47.25 28.92 LEX QUEUE 0 rc
48.68 29.34 DEP STACK 0 47.09 28.65 POS QUEUE 0 lc
48.47 30.84 LEX QUEUE 0 rc 46.68 27.08 DEP QUEUE 0 lc
46.77 26.86 DEP QUEUE 0 lc 45.69 27.83 POS QUEUE 0 rc
46.40 29.95 POS QUEUE 0 rc 44.77 26.17 DEP STACK 0
42.27 25.21 DEP QUEUE 0 pw 44.43 26.47 DEP QUEUE 0 rc
41.04 26.56 DEP QUEUE 0 rc 41.87 23.04 DEP QUEUE 0 pw
Table 4: Results of the beam search on the Swedish corpus at the first generation with the two initial
feature candidates, ?POS(QUEUE0)? and ?LEX(QUEUE0)?, respectively on the left- and right-hand
side of the table. The third row shows the score produced by the initial features alone and the next rows,
the figures for the candidate pairs combining the initial feature and the successor listed in the row. The
eight best combinations shown in bold are selected for the next generation.
Generation Dev set Test set Features
UAS LAS UAS LAS
1 43.49 26.45 45.93 30.19 POS QUEUE 0
2 74.02 59.67 71.60 58.37 POS STACK 0
3 79.50 65.34 79.07 65.86 POS QUEUE 1
4 83.58 71.76 82.75 70.98 LEX STACK 0 fw
5 85.96 76.03 84.82 74.75 LEX STACK 0
6 87.23 77.32 86.34 76.52 LEX QUEUE 0 lc
7 88.42 80.00 87.67 78.99 POS STACK 1
8 89.43 81.56 88.09 80.26 LEX QUEUE 1
9 89.84 83.20 88.69 82.33 LEX QUEUE 0
10 90.23 83.89 89.17 83.31 DEP STACK 0 lc
11 90.49 84.31 89.58 83.85 POS STACK 0 fw
12 90.73 84.47 89.66 83.83 LEX STACK 0 fw ls
13 90.81 84.60 89.52 83.75 LEX STACK 0 fw ls lc
14 90.81 84.70 89.32 83.73 POS STACK 1 h
15 90.85 84.67 89.13 83.21 LEX STACK 1 rs
16 90.84 84.68 88.65 82.75 POS STACK 0 fw ls rc
Table 6: Best results for each generation on the Swedish corpus, optimized for UAS. Figures in bold
designate the best scores. The right-hand side of the table shows the feature sequence producing the
best result at generation 16.
830
Generation Dev set Test set WSJ Test set Brown Features
UAS LAS UAS LAS UAS LAS
1 45.25 33.77 45.82 34.49 52.12 40.70 POS QUEUE 0
2 64.42 55.64 64.71 56.44 71.29 62.41 LEX STACK 0
3 78.62 68.77 78.99 70.30 78.67 65.17 POS QUEUE 1
4 81.83 76.67 82.46 77.82 80.57 72.95 LEX STACK 0 fw
5 84.43 79.78 84.89 80.88 84.03 76.99 POS STACK 0
6 85.95 81.60 86.61 82.93 84.55 77.80 DEP QUEUE 0 lc
7 86.95 82.73 87.73 84.09 85.26 78.48 LEX STACK 1
8 88.03 83.62 88.52 84.74 85.66 78.73 LEX QUEUE 1
9 88.61 84.97 89.15 86.20 86.29 79.86 LEX QUEUE 0
10 89.09 85.43 89.47 86.60 86.43 80.02 POS QUEUE 2
11 89.54 85.87 90.25 87.40 87.00 80.75 POS STACK 0 pw
12 89.95 86.21 90.63 87.77 86.87 80.46 POS QUEUE 3
13 90.26 86.56 90.64 87.80 87.35 80.86 POS STACK 1 pw
14 90.54 86.81 90.71 87.88 87.50 81.30 POS QUEUE 0 pw
15 90.61 86.94 90.89 88.11 87.47 81.33 LEX STACK 0 lc
16 90.65 87.00 90.88 88.09 87.42 81.28 POS STACK 0 pw ls
Table 7: Best results for each generation. English corpus. Selection optimized for UAS.
reported by state-of-the-art transition-based sys-
tems. We reached a UAS of 89.66 on the CoNLL-
X Swedish corpus. On the same corpus, the top
scores reported in the shared task were slightly
lower: 89.54 and 89.50. Our best LAS was 84.21,
and the two best scores in CoNLL-X were 84.58
and 82.55. Our results for the English corpus from
CoNLL 2008 were optimized for an unlabeled at-
tachment score and we obtained 90.89 for the in-
domain test set and 87.50 for the out-of-domain
one. Our best LAS were 88.11 and 81.33. Official
results in CoNLL 2008 only reported the labeled
attachment scores, respectively 90.13 and 82.811.
We believe these results remarkable. We used a
single-parser system as opposed to ensemble sys-
tems and the results on the Brown corpus show
an excellent resilience and robustness on out-of-
domain data. The automatic discovery produced
results matching or exceeding comparable sys-
tems, although no prior knowledge of the lan-
guage being analyzed was used and no feature set
was provided to the parser.
Although, a systematic search requires no in-
tuitive guessing, it still consumes a considerable
1Results are not exactly comparable as we used the
CoNLL-X evaluation script that gives slightly higher figures.
machine time. Due to the learning algorithm we
use, SVM, training a model takes between 1 and
130 hours depending on the size of the corpus.
The number of models to train at each generation
corresponds to the number of feature candidates
times the beam width. The first generation con-
tains about 15 feature candidates per feature set
and since features are only added, the number of
candidates can grow to 100 at generation 10.
We believe there is a margin for improvement
both in the parsing scores and in the time needed
to determine the feature sets. Our scores in Swed-
ish were obtained with models trained on 90% of
the training set. They could probably be slightly
improved if they had been trained on a com-
plete set. In our experiments, we used three at-
tributes: the part of speech, lexical value, and de-
pendency label of the node. These attributes could
be extended to lemmas and grammatical features.
SVMs yield a high performance, but they are slow
to train. Logistic regression with, for instance,
the LIBLINEAR package (Fan et al, 2008) would
certainly reduce the exploration time.
831
Acknowledgments
The research leading to these results has received
funding from the European community?s seventh
framework program FP7/2007-2013, challenge 2,
cognitive systems, interaction, robotics, under
grant agreement No 230902?ROSETTA.
References
Attardi, Giuseppe, Felice Dell?Orletta, Maria Simi,
Atanas Chanev, and Massimiliano Ciaramita. 2007.
Multilingual dependency parsing and domain adap-
tation using DeSR. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
1112?1118, Prague, Czech Republic, June. Associ-
ation for Computational Linguistics.
Buchholz, Sabine and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency parsing.
In Proceedings of the Tenth Conference on Com-
putational Natural Language Learning (CoNLL-X),
pages 149?164, New York City, June. Association
for Computational Linguistics.
Chang, Chih-Chung and Chih-Jen Lin. 2001. LIB-
SVM: a library for support vector machines.
Software available at http://www.csie.ntu.
edu.tw/?cjlin/libsvm.
Covington, Michael A. 2001. A fundamental algo-
rithm for dependency parsing. In Proceedings of the
39th Annual ACM Southeast Conference, Athens,
Georgia.
Fan, Rong-En, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Kunze, Ju?rgen. 1967. Die Behandlung nicht-
projektiver Strukturen bei der syntaktischen Anal-
yse und Synthese des englischen und des deutschen.
In MASPEREVOD-67: Internationales Symposium
der Mitgliedsla?nder des RGW, pages 2?15, Bu-
dapest, 10.?13. Oktober.
Nivre, Joakim and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In Proceedings
of the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL?05), pages 99?
106, Ann Arbor, June.
Nivre, Joakim, Johan Hall, and Jens Nilsson. 2006a.
Maltparser: A data-driven parser-generator for de-
pendency parsing. In Proceedings of the fifth in-
ternational conference on Language Resources and
Evaluation (LREC2006), pages 2216?2219, Genoa,
May 24-26.
Nivre, Joakim, Johan Hall, Jens Nilsson, Gu?lsen
Eryigit, and Svetoslav Marinov. 2006b. Labeled
pseudo-projective dependency parsing with support
vector machines. In Proceedings of the Tenth Con-
ference on Computational Natural Language Learn-
ing (CoNLL), pages 221?225, New York, June, 8-9.
Nivre, Joakim. 2003. An efficient algorithm for
projective dependency parsing. In Proceedings of
the 8th International Workshop on Parsing Tech-
nologies (IWPT 03), pages 149?160, Nancy, 23-25
April.
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The
CoNLL 2008 shared task on joint parsing of syn-
tactic and semantic dependencies. In CoNLL 2008:
Proceedings of the 12th Conference on Computa-
tional Natural Language Learning, pages 159?177,
Manchester, August.
Yamada, Hiroyasu and Yuji Matsumoto. 2003. Sta-
tistical dependency analysis with support vector
machines. In Proceedings of the 8th Interna-
tional Workshop on Parsing Technologies (IWPT
03), pages 195?206, Nancy, 23-25 April.
Zhang, Yue and Stephen Clark. 2009. Transition-
based parsing of the Chinese treebank using a global
discriminative model. In Proceedings of the 11th
International Conference on Parsing Technologies
(IWPT 09), pages 162?171, Paris, October.
832
Coling 2010: Demonstration Volume, pages 33?36,
Beijing, August 2010
A High-Performance Syntactic and Semantic Dependency Parser
Anders Bjo?rkelund? Bernd Bohnet? Love Hafdell? Pierre Nugues?
?Department of Computer science ?Institute for Natural Language Processing
Lund University University of Stuttgart
anders.bjorkelund@cs.lth.se bohnet@ims.uni-stuttgart.de
love.hafdell@cs.lth.se
pierre.nugues@cs.lth.se
Abstract
This demonstration presents a high-
performance syntactic and semantic de-
pendency parser. The system consists of a
pipeline of modules that carry out the to-
kenization, lemmatization, part-of-speech
tagging, dependency parsing, and seman-
tic role labeling of a sentence. The sys-
tem?s two main components draw on im-
proved versions of a state-of-the-art de-
pendency parser (Bohnet, 2009) and se-
mantic role labeler (Bjo?rkelund et al,
2009) developed independently by the au-
thors.
The system takes a sentence as input and
produces a syntactic and semantic anno-
tation using the CoNLL 2009 format. The
processing time needed for a sentence typ-
ically ranges from 10 to 1000 millisec-
onds. The predicate?argument structures
in the final output are visualized in the
form of segments, which are more intu-
itive for a user.
1 Motivation and Overview
Semantic analyzers consist of processing
pipelines to tokenize, lemmatize, tag, and parse
sentences, where all the steps are crucial to their
overall performance. In practice, however, while
code of dependency parsers and semantic role
labelers is available, few systems can be run as
standalone applications and even fewer with a
processing time per sentence that would allow a
?Authors are listed in alphabetical order.
user interaction, i.e. a system response ranging
from 100 to 1000 milliseconds.
This demonstration is a practical semantic
parser that takes an English sentence as input
and produces syntactic and semantic dependency
graphs using the CoNLL 2009 format. It builds
on lemmatization and POS tagging preprocessing
steps, as well as on two systems, one dealing with
syntax and the other with semantic dependencies
that reported respectively state-of-the-art results
in the CoNLL 2009 shared task (Bohnet, 2009;
Bjo?rkelund et al, 2009). The complete system ar-
chitecture is shown in Fig. 1.
The dependency parser is based on Carreras?s
algorithm (Carreras, 2007) and second order span-
ning trees. The parser is trained with the margin
infused relaxed algorithm (MIRA) (McDonald et
al., 2005) and combined with a hash kernel (Shi et
al., 2009). In combination with the system?s lem-
matizer and POS tagger, this parser achieves an
average labeled attachment score (LAS) of 89.88
when trained and tested on the English corpus
of the CoNLL 2009 shared task (Surdeanu et al,
2008).
The semantic role labeler (SRL) consists of a
pipeline of independent, local classifiers that iden-
tify the predicates, their senses, the arguments of
the predicates, and the argument labels. The SRL
module achieves an average labeled semantic F1
of 80.90 when trained and tested on the English
corpus of CoNLL 2009 and combined with the
system?s preprocessing steps and parser.
2 The Demonstration
The demonstration runs as a web application and
is available from a server located at http://
33
	

	


	
	
	


	
	

CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 183?187
Manchester, August 2008
Dependency-based Syntactic?Semantic Analysis with PropBank and
NomBank
Richard Johansson and Pierre Nugues
Lund University, Sweden
{richard, pierre}@cs.lth.se
Abstract
This paper presents our contribution in the
closed track of the 2008 CoNLL Shared
Task (Surdeanu et al, 2008). To tackle the
problem of joint syntactic?semantic anal-
ysis, the system relies on a syntactic and
a semantic subcomponent. The syntactic
model is a bottom-up projective parser us-
ing pseudo-projective transformations, and
the semantic model uses global inference
mechanisms on top of a pipeline of clas-
sifiers. The complete syntactic?semantic
output is selected from a candidate pool
generated by the subsystems.
The system achieved the top score in the
closed challenge: a labeled syntactic accu-
racy of 89.32%, a labeled semantic F1 of
81.65, and a labeled macro F1 of 85.49.
1 Introduction: Syntactic?Semantic
Analysis
Intuitively, semantic interpretation should help
syntactic disambiguation, and joint syntactic?
semantic analysis has a long tradition in linguis-
tic theory. This motivates a statistical modeling of
the problem of finding a syntactic tree y?
syn
and a
semantic graph y?
sem
for a sentence x as maximiz-
ing a function F that scores the joint syntactic?
semantic structure:
?y?
syn
, y?
sem
? = arg max
y
syn
,y
sem
F (x, y
syn
, y
sem
)
The dependencies in the feature representation
used to compute F determine the tractability of the
search procedure needed to perform the maximiza-
tion. To be able to use complex syntactic features
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
such as paths when predicting semantic structures,
exact search is clearly intractable. This is true even
with simpler feature representations ? the problem
is a special case of multi-headed dependency anal-
ysis, which is NP-hard even if the number of heads
is bounded (Chickering et al, 1994).
This means that we must resort to a simplifica-
tion such as an incremental method or a reranking
approach. We chose the latter option and thus cre-
ated syntactic and semantic submodels. The joint
syntactic?semantic prediction is selected from a
small list of candidates generated by the respective
subsystems.
2 Syntactic Submodel
We model the process of syntactic parsing of
a sentence x as finding the parse tree y?
syn
=
argmax
y
F (x, y) that maximizes a scoring func-
tion F . The learning problem consists of fitting
this function so that the cost of the predictions is
as low as possible according to a cost function ?.
In this work, we consider linear scoring functions
of the following form:
F (x, y) = w ??(x, y)
where ?(x, y) is a numeric feature representation
of the pair (x, y) andw a vector of feature weights.
We defined the syntactic cost ? as the sum of link
costs, where the link cost was 0 for a correct de-
pendency link with a correct label, 0.5 for a correct
link with an incorrect label, and 1 for an incorrect
link.
A widely used framework for fitting the weight
vector is the max-margin model (Taskar et al,
2003), which is a generalization of the well-
known support vector machines to general cost-
based prediction problems. Since the large num-
ber of training examples and features in our case
make an exact solution of the max-margin opti-
mization problem impractical, we used the on-
line passive?aggressive algorithm (Crammer et al,
183
2006), which approximates the optimization pro-
cess in two ways:
? The weight vector w is updated incremen-
tally, one example at a time.
? For each example, only the most violated con-
straint is considered.
The algorithm is a margin-based variant of the per-
ceptron (preliminary experiments show that it out-
performs the ordinary perceptron on this task). Al-
gorithm 1 shows pseudocode for the algorithm.
Algorithm 1 The Online PA Algorithm
input Training set T = {(x
t
, y
t
)}
T
t=1
Number of iterations N
Regularization parameter C
Initialize w to zeros
repeat N times
for (x
t
, y
t
) in T
let y?
t
= argmax
y
F (x
t
, y) + ?(y
t
, y)
let ?
t
= min
?
C,
F (x
t
,y?
t
)?F (x
t
,y
t
)+?(y
t
,y?
t
)
??(x,y
t
)??(x,y?
t
)?
2
?
w ? w + ?
t
(?(x, y
t
)??(x, y?
t
))
returnwaverage
We used a C value of 0.01, and the number of
iterations was 6.
2.1 Features and Search
The feature function ? is a second-order edge-
factored representation (McDonald and Pereira,
2006; Carreras, 2007). The second-order repre-
sentation allows us to express features not only of
head?dependent links, but also of siblings and chil-
dren of the dependent. This feature set forces us
to adopt the expensive search procedure by Car-
reras (2007), which extends Eisner?s span-based
dynamic programming algorithm (1996) to allow
second-order feature dependencies. Since the cost
function ? is based on the cost of single links, this
procedure can also be used to find the maximizer
of F (x
i
, y
ij
)+?(y
i
, y
ij
), which is needed at train-
ing time. The search was constrained to disallow
multiple root links.
2.2 Handling Nonprojective Links
Although only 0.4% of the links in the training set
are nonprojective, 7.6% of the sentences contain at
least one nonprojective link. Many of these links
represent long-range dependencies ? such as wh-
movement ? that are valuable for semantic pro-
cessing. Nonprojectivity cannot be handled by
span-based dynamic programming algorithms. For
parsers that consider features of single links only,
the Chu-Liu/Edmonds algorithm can be used in-
stead. However, this algorithm cannot be gen-
eralized to the second-order setting ? McDonald
and Pereira (2006) proved that this problem is NP-
hard, and described an approximate greedy search
algorithm.
To simplify implementation, we instead opted
for the pseudo-projective approach (Nivre and
Nilsson, 2005), in which nonprojective links are
lifted upwards in the tree to achieve projectivity,
and special trace labels are used to enable recovery
of the nonprojective links at parse time. The use
of trace labels in the pseudo-projective transfor-
mation leads to a proliferation of edge label types:
from 69 to 234 in the training set, many of which
occur only once. Since the running time of our
parser depends on the number of labels, we used
only the 20 most frequent trace labels.
3 Semantic Submodel
Our semantic model consists of three parts:
? A SRL classifier pipeline that generates a list
of candidate predicate?argument structures.
? A constraint system that filters the candidate
list to enforce linguistic restrictions on the
global configuration of arguments.
? A global classifier that rescores the predicate?
argument structures in the filtered candidate
list.
Rather than training the models on gold-
standard syntactic input, we created an automati-
cally parsed training set by 5-fold cross-validation.
Training on automatic syntax makes the semantic
classifiers more resilient to parsing errors, in par-
ticular adjunct labeling errors.
3.1 SRL Pipeline
The SRL pipeline consists of classifiers for predi-
cate identification, predicate disambiguation, sup-
port identification (for noun predicates), argument
identification, and argument classification. We
trained one set of classifiers for verb predicates
and another for noun predicates. For the pred-
icate disambiguation classifiers, we trained one
subclassifier for each lemma. All classifiers in the
pipeline were L2-regularized linear logistic regres-
sion classifiers, implemented using the efficient
LIBLINEAR package (Lin et al, 2008). For multi-
class problems, we used the one-vs-all binarization
184
method, which makes it easy to prevent outputs not
allowed by the PropBank or NomBank frame.
Since our classifiers were logistic, their output
values could be meaningfully interpreted as prob-
abilities. This allowed us to combine the scores
from subclassifiers into a score for the complete
predicate?argument structure. To generate the can-
didate lists used by the global SRL models, we ap-
plied beam search based on these scores using a
beam width of 4.
The features used by the classifiers are listed in
Tables 1 and 2. In the tables, the features used
by the classifiers for noun and verb predicates are
indicated by N and V, respectively. We selected the
feature sets by greedy forward subset selection.
Feature PredId PredDis
PREDWORD N,V N,V
PREDLEMMA N,V N,V
PREDPARENTWORD/POS N,V N,V
CHILDDEPSET N,V N,V
CHILDWORDSET N,V N,V
CHILDWORDDEPSET N,V N,V
CHILDPOSSET N,V N,V
CHILDPOSDEPSET N,V N,V
DEPSUBCAT N,V N,V
PREDRELTOPARENT N,V N,V
Table 1: Classifier features in predicate identifica-
tion and disambiguation.
Feature Supp ArgId ArgCl
PREDPARENTWORD/POS N N,V
CHILDDEPSET N N,V N,V
PREDLEMMASENSE N N,V N,V
VOICE V V
POSITION N N,V N,V
ARGWORD/POS N N,V N,V
LEFTWORD/POS N N,V
RIGHTWORD/POS N N,V N,V
LEFTSIBLINGWORD/POS N,V
RIGHTSIBLINGWORD/POS N N
PREDPOS N N,V V
RELPATH N N,V N,V
POSPATH N
RELPATHTOSUPPORT N N
VERBCHAINHASSUBJ V V
CONTROLLERHASOBJ V N
PREDRELTOPARENT N N,V N,V
FUNCTION N,V
Table 2: Classifier features in argument identifica-
tion and classification and support detection.
Features Used in Predicate Identification and
Disambiguation
PREDWORD, PREDLEMMA. The lexical form
and lemma of the predicate.
PREDPARENTWORD and PREDPARENTPOS.
Form and part-of-speech tag of the parent
node of the predicate.
CHILDDEPSET, CHILDWORDSET, CHILD-
WORDDEPSET, CHILDPOSSET, CHILD-
POSDEPSET. These features represent the
set of dependents of the predicate using
combinations of dependency labels, words,
and parts of speech.
DEPSUBCAT. Subcategorization frame: the con-
catenation of the dependency labels of the
predicate dependents.
PREDRELTOPARENT. Dependency relation be-
tween the predicate and its parent.
Features Used in Argument Identification and
Classification
PREDLEMMASENSE. The lemma and sense
number of the predicate, e.g. give.01.
VOICE. For verbs, this feature is Active or Pas-
sive. For nouns, it is not defined.
POSITION. Position of the argument with respect
to the predicate: Before, After, or On.
ARGWORD and ARGPOS. Lexical form and
part-of-speech tag of the argument node.
LEFTWORD, LEFTPOS, RIGHTWORD, RIGHT-
POS. Form/part-of-speech tag of the left-
most/rightmost dependent of the argument.
LEFTSIBLINGWORD, LEFTSIBLINGPOS,
RIGHTSIBLINGWORD, RIGHTSIBLING-
POS. Form/part-of-speech tag of the
left/right sibling of the argument.
PREDPOS. Part-of-speech tag of the predicate.
RELPATH. A representation of the complex
grammatical relation between the predicate
and the argument. It consists of the sequence
of dependency relation labels and link direc-
tions in the path between predicate and argu-
ment, e.g. IM?OPRD?OBJ?.
POSPATH. An alternative view of the grammat-
ical relation, which consists of the POS tags
passed when moving from predicate to argu-
ment, e.g. VB?TO?VBP?PRP.
RELPATHTOSUPPORT. The RELPATH from the
argument to a support chain.
VERBCHAINHASSUBJ. Binary feature that is set
to true if the predicate verb chain has a sub-
ject. The purpose of this feature is to resolve
verb coordination ambiguity as in Figure 1.
CONTROLLERHASOBJ. Binary feature that is
true if the link between the predicate verb
chain and its parent is OPRD, and the parent
has an object. This feature is meant to resolve
control ambiguity as in Figure 2.
185
FUNCTION. The grammatical function of the ar-
gument node. For direct dependents of the
predicate, this is identical to the RELPATH.
I
SBJ
eat drinkyouand
COORD SBJ
CONJROOT
SBJ COORD
ROOT
drinkandeatI
CONJ
Figure 1: Coordination ambiguity: The subject I is
in an ambiguous position with respect to drink.
I to
IMSBJ
want sleephim
OBJ
OPRD
ROOT
IM
sleepI
SBJ
want
ROOT
to
OPRD
Figure 2: Subject/object control ambiguity: I is in
an ambiguous position with respect to sleep.
3.2 Linguistically Motivated Global
Constraints
The following three global constraints were used
to filter the candidates generated by the pipeline.
CORE ARGUMENT CONSISTENCY. Core argu-
ment labels must not appear more than once.
DISCONTINUITY CONSISTENCY. If there is a la-
bel C-X, it must be preceded by a label X.
REFERENCE CONSISTENCY. If there is a label
R-X and the label is inside a relative clause, it
must be preceded by a label X.
3.3 Global SRL Model
Toutanova et al (2005) have showed that a
global model that scores the complete predicate?
argument structure can lead to substantial perfor-
mance gains. We therefore created a global SRL
classifier using the following global features in ad-
dition to the features from the pipeline:
CORE ARGUMENT LABEL SEQUENCE. The
complete sequence of core argument labels.
The sequence also includes the predicate and
voice, for instance A0+break.01/Active+A1.
MISSING CORE ARGUMENT LABELS. The set
of core argument labels declared in the Prop-
Bank/NomBank frame that are not present in
the predicate?argument structure.
Similarly to the syntactic submodel, we trained
the global SRL model using the online passive?
aggressive algorithm. The cost function ? was
defined as the number of incorrect links in the
predicate?argument structure. The number of it-
erations was 20 and the regularization parameter
C was 0.01. Interestingly, we noted that the global
SRL model outperformed the pipeline even when
no global features were added. This shows that the
global learning model can correct label bias prob-
lems introduced by the pipeline architecture.
4 Syntactic?Semantic Integration
Our baseline joint feature representation contained
only three features: the log probability of the syn-
tactic tree and the log probability of the semantic
structure according to the pipeline and the global
model, respectively. This model was trained on the
complete training set using cross-validation. The
probabilities were obtained using the multinomial
logistic function (?softmax?).
We carried out an initial experiment with a more
complex joint feature representation, but failed to
improve over the baseline. Time prevented us from
exploring this direction conclusively.
5 Results
The submitted results on the development and test
corpora are presented in the upper part of Table 3.
After the submission deadline, we corrected a bug
in the predicate identification method. This re-
sulted in improved results shown in the lower part.
Corpus Syn acc Sem F1 Macro F1
Development 88.47 80.80 84.66
Test WSJ 90.13 81.75 85.95
Test Brown 82.81 69.06 75.95
Test WSJ + Brown 89.32 80.37 84.86
Development 88.47 81.86 85.17
Test WSJ 90.13 83.75 86.61
Test Brown 82.84 69.85 76.34
Test WSJ + Brown 89.32 81.65 85.49
Table 3: Results.
5.1 Syntactic Results
Table 4 shows the effect of adding second-order
features to the parser in terms of accuracy as well
as training and parsing time on a Mac Pro, 3.2
GHz. The training times were measured on the
complete training set and the parsing time and ac-
curacies on the development set. Similarly to Car-
reras (2007), we see that these features have a very
large impact on parsing accuracy, but also that the
parser pays dearly in terms of efficiency as the
search complexity increases fromO(n3) toO(n4).
186
Since the low efficiency of the second-order parser
restricts its use to batch applications, we see an in-
teresting research direction to find suitable com-
promises between the two approaches, for instance
by sacrificing the exact search procedure.
System Training Parse Labeled Unlabeled
1st order 65 min 28 sec 85.78 89.51
2nd order 60 hours 34 min 88.33 91.43
Table 4: Impact of second-order features.
Table 5 shows the dependency types most af-
fected by the addition of second-order features to
the parser when ordered by the increase in F1. As
can be seen, they are all verb adjunct categories,
which demonstrates the effect of grandchild fea-
tures on PP attachment and labeling.
Label ?R ?P ?F
1
TMP 14.7 12.9 13.9
DTV 0 19.9 10.5
LOC 7.8 12.3 9.9
PRP 12.4 6.7 9.6
DIR 5.9 7.2 6.5
Table 5: Labels affected by second-order features.
5.2 Semantic Results
To assess the effect of the components in the se-
mantic submodel, we tested their performance on
the top-scoring parses from the syntactic model.
Table 6 shows the results. The baseline system
consists of the SRL pipeline only (P). Adding lin-
guistic constraints (C) results in a more precision-
oriented system with slightly lower recall, but sig-
nificantly higher F1. Even higher performance is
obtained when adding the global SRL model (G).
System P R F1
P 80.74 77.98 79.33
P+C 82.42 77.66 79.97
P+C+G 83.64 78.14 80.40
Table 6: SRL results on the top-scoring parse trees.
5.3 Syntactic?Semantic Integration
The final experiment concerned the integration of
syntactic and semantic analysis. In this setting,
the system chooses the output that maximizes the
joint syntactic?semantic score, based on the top N
syntactic trees. Table 7 shows the results on the
development set. We see that syntactic?semantic
integration improves both syntactic accuracy and
semantic F1. This holds for the constraint-based
SRL system as well as for the full system.
Sem model N Syn acc Sem F1 Macro F1
P+C 1 88.33 79.97 84.17
P+C 16 88.42 80.42 84.44
P+C+G 1 88.33 80.40 84.39
P+C+G 16 88.47 80.80 84.66
Table 7: Syntactic?semantic integration.
6 Conclusion
We have described a system1 for syntactic and se-
mantic dependency analysis based on PropBank
and NomBank, and detailed the implementation
of its subsystems. Crucial to our success was the
high performance of the syntactic parser, which
achieved a high accuracy. In addition, we recon-
firmed the benefits of global inference in semantic
analysis: both constraint-based and learning-based
methods resulted in improvements over a baseline.
Finally, we showed that integration of syntactic
and semantic analysis is beneficial for both sub-
tasks. We hope that this shared task will spur fur-
ther research that leads to new feature representa-
tions and search procedures to handle the problem
of joint syntactic and semantic analysis.
References
Carreras, Xavier. 2007. Experiments with a higher-order pro-
jective dependency parser. In Proceedings of CoNLL.
Chickering, David M., Dan Geiger, and David Heckerman.
1994. Learning Bayesian networks: The combination of
knowledge and statistical data. Technical Report MSR-
TR-94-09, Microsoft Research.
Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai Shalev-
Schwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. JMLR, 2006(7):551?585.
Eisner, Jason M. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proc. of ICCL.
Lin, Chih-Jen, Ruby C. Weng, and S. Sathiya Keerthi. 2008.
Trust region Newton method for large-scale logistic regres-
sion. JMLR, 2008(9):627?650.
McDonald, Ryan and Fernando Pereira. 2006. Online learn-
ing of approximate dependency parsing algorithms. In
Proceedings of EACL-2006.
Nivre, Joakim and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proceedings of ACL-2005.
Surdeanu, Mihai, Richard Johansson, Adam Meyers, Llu?s
M?rquez, and Joakim Nivre. 2008. The CoNLL?2008
shared task on joint parsing of syntactic and semantic de-
pendencies. In Proceedings of CoNLL?2008.
Taskar, Ben, Carlos Guestrin, and Daphne Koller. 2003.
Max-margin Markov networks. In Proceedings of NIPS.
Toutanova, Kristina, Aria Haghighi, and Christopher D. Man-
ning. 2005. Joint learning improves semantic role label-
ing. In Proceedings of ACL-2005.
1Our system is freely available for download at
http://nlp.cs.lth.se/lth_srl.
187
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 45?50,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Exploring Lexicalized Features for Coreference Resolution
Anders Bjo?rkelund
Lund University / LTH
Lund / Sweden
Anders.Bjorkelund@cs.lth.se
Pierre Nugues
Lund University / LTH
Lund / Sweden
Pierre.Nugues@cs.lth.se
Abstract
In this paper, we describe a coreference solver
based on the extensive use of lexical fea-
tures and features extracted from dependency
graphs of the sentences. The solver uses Soon
et al (2001)?s classical resolution algorithm
based on a pairwise classification of the men-
tions.
We applied this solver to the closed track of
the CoNLL 2011 shared task (Pradhan et al,
2011). We carried out a systematic optimiza-
tion of the feature set using cross-validation
that led us to retain 24 features. Using this set,
we reached a MUC score of 58.61 on the test
set of the shared task. We analyzed the impact
of the features on the development set and we
show the importance of lexicalization as well
as of properties related to dependency links in
coreference resolution.
1 Introduction
In this paper, we present our contribution to the
closed track of the 2011 CoNLL shared task (Prad-
han et al, 2011). We started from a baseline system
that uses Soon et al (2001)?s architecture and fea-
tures. Mentions are identified by selecting all noun
phrases and possessive pronouns. Then, the reso-
lution algorithm relies on a pairwise classifier that
determines whether two mentions corefer or not.
Lexicalization has proved effective in numerous
tasks of natural language processing such as part-
of-speech tagging or parsing. However, lexicalized
models require a good deal of annotated data to
avoid overfit. The data set used in the CoNLL 2011
shared task has a considerable size compared to cor-
pora traditionally used in coreference resolution ?
the training set comprises 2,374 documents. See
Pradhan et al (2007) for a previous work using an
earlier version of this dataset. Leveraging this size,
we investigated the potential of lexicalized features.
Besides lexical features, we created features that
use part-of-speech tags and semantic roles. We also
constructed features using dependency tree paths
and labels by converting the constituent trees pro-
vided in the shared task into dependency graphs.
The final feature set was selected through an au-
tomated feature selection procedure using cross-
validation.
2 System Architecture
During both training and decoding, we employed
the same mention detection and preprocessing steps.
We considered all the noun phrases (NP) and posses-
sive pronouns (PRP$) as mentions. In order to ex-
tract head words from the NP constituents, we con-
verted the constituent trees provided in the data sets
to dependency graphs using the Penn treebank con-
verter of Johansson and Nugues (2007). Using the
dependency tree, we extracted the head word of all
the NPs by taking the word that dominates the sub-
tree constructed from the NP.
The dependency tree is also used later to ex-
tract features of mentions based on dependency tree
paths, which is further described in Sec. 3.
In the preprocessing step, we assigned a number
and a gender to each mention. For the pronominal
mentions, we used a manually compiled lists of pro-
nouns, where we marked the number and gender.
45
For nonpronominal mentions, we used the number
and gender data (Bergsma and Lin, 2006) provided
by the task organizers and queried it for the head
word of the mention. In cases of ambiguity (e.g. the
pronoun you), or missing entries in the data for non-
pronominals, we assigned an unknown value.
2.1 Generation of training examples
To create a set of training examples, we used pairs
of mentions following the method outlined by Soon
et al (2001). For each anaphoric mention mj and
its closest preceding antecedent mi, we built a pos-
itive example: P = {(mi,mj)}. We constructed
the negative examples with noncoreferring pairs of
mentions, where the first term is a mention occur-
ring between mi and mj and the second one is mj :
N = {(mk,mj)|i < k < j)}.
The training examples collected from the CoNLL
2011 training set consist of about 5.5% of positive
examples and 94.5% of negative ones.
2.2 Learning method
We evaluated two types of classifiers: decision trees
and logistic regression. We used the decision trees
and the C4.5 algorithm from the Weka distribution
(Hall et al, 2009) for our baseline system. We then
opted for linear logistic regression as it scaled better
with the number of features and feature values.
Logistic regression is faster to train and allowed
us to carry out an automated feature selection, which
is further described in Sec. 3.4. In addition, the lo-
gistic classifiers enabled us to interpret their results
in terms of probabilities, which we used for the de-
coding step. We trained the logistic regression clas-
sifiers using the LIBLINEAR package (Fan et al,
2008).
2.3 Decoding
The decoding algorithm devised by Soon et al
(2001) selects the closest preceding mention deemed
to be coreferent by the classifier. This clustering
algorithm is commonly referred to as closest-first
clustering. Ng and Cardie (2002) suggested a dif-
ferent clustering procedure, commonly referred to
as best-first clustering. This algorithm selects the
most likely antecedent classified as coreferent with
the anaphoric mention. During early experiments,
we found that while the best-first method increases
the performance on nonpronominal anaphoric ex-
pressions, it has the opposite effect on pronominal
anaphoric expressions. Consequently, we settled on
using the closest-first clustering method for pronom-
inal mentions, and the best-first clustering method
otherwise. For the best-first clustering, we used the
probability output from our logistic classifiers and a
threshold of 0.5.
After clustering mentions in a document, we dis-
card all remaining singleton mentions, as they were
excluded from the annotation in the CoNLL 2011
shared task.
2.4 Postprocessing
The initial detection of mentions is a direct mapping
from two categories of constituents: NP and PRP$.
In the postprocessing step, we reclaim some of the
mentions that we missed in the initial step.
The automatically generated constituent trees pro-
vided in the data set contain errors and this causes
the loss of many mentions. Another source of loss
is the bracketing of complex NPs, where the in-
ternal structure uses the tag NML. In a few cases,
these nested nodes participate in coreference chains.
However, when we tried to include this tag in the
mention detection, we got worse results overall.
This is possibly due to an even more skewed dis-
tribution of positive and negative training examples.
In the postprocessing step, we therefore search
each document for sequences of one or more proper
noun tokens, i.e. tokens with the part-of-speech
tags NNP or NNPS. If their common ancestor, i.e.
the parse tree node that encloses all the tokens, is
not already in a mention, we try to match this se-
quence to any existing chain using the binary fea-
tures: STRINGMATCH and ALIAS (cf. Sec. 3). If
either of them evaluates to true, we add this span of
proper nouns to the matched chain.
3 Features
For our baseline system, we started with the feature
set described in Soon et al (2001). Due to space
limitations, we omit the description of these features
and refer the reader to their paper.
We also defined a large number of feature tem-
plates based on the syntactic dependency tree, as
well as features based on semantic roles. In the fol-
46
lowing sections, we describe these features as well
as the naming conventions we use. The final feature
set we used is given in Sec. 4.
3.1 Mention-based features
On the mention level, we considered the head word
(HD) of the mention, and following the edges in the
dependency tree, we considered the left-most and
right-most children of the head word (HDLMC and
HDRMC), the left and right siblings of the head word
(HDLS and HDRS), as well as the governor1 of the
head word (HDGOV).
For each of the above mentioned tokens, we ex-
tracted the surface form (FORM), the part-of-speech
tag (POS), and the grammatical function of the token
(FUN), i.e. the label of the dependency edge of the
token to its parent. For head words that do not have
any leftmost or rightmost children, or left or right
siblings, we used a null-value placeholder.
In each training pair, we extracted these values
from both mentions in the pair, i.e. both the anaphor
and the tentative antecedent. Table 3 shows the fea-
tures we used in our system. We used a naming
nomenclature consisting of the role in the anaphora,
where I stands for antecedent and J for anaphor; the
token we selected from the dependency graph, e.g.
HD or HDLMC; and the value extracted from the
token, e.g. POS or FUN. For instance, the part-of-
speech tag of the governor of the head word of the
anaphor is denoted: J-HDGOVPOS.
The baseline features taken from Soon et al
(2001) include features such as I-PRONOUN and J-
DEMONSTRATIVE that are computed using a word
list and by looking at the first word in the mention,
respectively. Our assumption is that these traits can
be captured by our new features by considering the
part-of-speech tag of the head word and the surface
form of the left-most child of the head word, respec-
tively.
3.2 Path-based features
Between pairs of potentially coreferring mentions,
we also considered the path from the head word of
the anaphor to the head word of the antecedent in
the syntactic dependency tree. If the mentions are
not in the same sentence, this is the path from the
1We use the term governor in order not to confuse it with
head word of an NP.
anaphor to the root of its sentence, followed by the
path from the root to the antecedent in its sentence.
We differentiate between the features depending on
whether they are in the same sentence or in different
sentences. The names of these features are prefixed
with SS and DS, respectively.
Following the path in the dependency tree, we
concatenated either the surface form, the part-of-
speech tag, or the grammatical function label with
the direction of the edge to the next token, i.e. up or
down. This way, we built six feature templates. For
instance, DSPATHFORM is the concatenation of the
surface forms of the tokens along the path between
mentions in different sentences.
Bergsma and Lin (2006) built a statistical model
from paths that include the lemma of the intermedi-
ate tokens, but replace the end nodes with noun, pro-
noun, or pronoun-self for nouns, pronouns, and re-
flexive pronouns, respectively. They used this model
to define a measure of coreference likelihood to re-
solve pronouns within the same sentence. Rather
than building an explicit model, we simply included
these paths as features in our set. We refer to this
feature template as BERGSMALINPATH in Table 3.
3.3 Semantic role features
We tried to exploit the semantic roles that were in-
cluded in the CoNLL 2011 data set. Ponzetto and
Strube (2006) suggested using the concatenation of
the predicate and the role label for a mention that
has a semantic role in a predicate. They introduced
two new features, I SEMROLE and J SEMROLE, that
correspond to the semantic roles filled by each of the
mentions in a pair. We included these features in our
pool of feature templates, but we could not see any
contribution from them during the feature selection.
We also introduced a number of feature templates
that only applied to pairs of mentions that occur in
the same semantic role proposition. These templates
included the concatenation of the two labels of the
arguments and the predicate sense label, and vari-
ations of these that also included the head words
of either the antecedent or anaphor, or both. The
only feature that was selected during our feature se-
lection procedure corresponds to the concatenation
of the argument labels, the predicate sense, and the
head word of the anaphor: SEMROLEPROPJHD in
Table 3. In the sentence A lone protestor parked
47
herself outside the UN, the predicate park has the
arguments A lone protestor, labeled ARG0, and her-
self, labeled ARG1. The corresponding value of this
feature would be ARG0-park.01-ARG1-herself.
3.4 Feature selection
Starting from Soon et al (2001)?s feature set, we
performed a greedy forward selection. The fea-
ture selection used a 5-fold cross-validation over the
training set, where we evaluated the features using
the arithmetic mean of MUC, BCUB, and CEAFE.
After reaching a maximal score using forward se-
lection, we reversed the process using a backward
elimination, leaving out each feature and removing
the one that had the worst impact on performance.
This backwards procedure was carried out until the
score no longer increased. We repeated this forward-
backward procedure until there was no increase in
performance. Table 3 shows the final feature set.
Feature bigrams are often used to increase the
separability of linear classifiers. Ideally, we would
have generated a complete bigram set from our fea-
tures. However, as this set is quadratic in nature
and due to time constraints, we included only a sub-
set of it in the selection procedure. Some of them,
most notably the bigram of mention head words (I-
HDFORM+J-HDFORM) were selected in the proce-
dure and appear in Table 3.
4 Evaluation
Table 1 shows some baseline figures using the binary
features STRINGMATCH and ALIAS as sole corefer-
ence properties, as well as our baseline system using
Soon et al (2001)?s features.
MD MUC BCUB
STRINGMATCH 59.91 44.43 63.65
ALIAS 19.25 16.77 48.07
Soon baseline/LR 60.79 47.50 63.97
Soon baseline/C4.5 58.96 47.02 65.36
Table 1: Baseline figures using string match and alias
properties, and our Soon baseline using decision trees
with the C4.5 induction program and logistic regression
(LR). MD stands for mention detection.
4.1 Contribution of postprocessing
The postprocessing step described in Sec. 2.4 proved
effective, contributing from 0.21 to up to 1 point to
the final score across the metrics. Table 2 shows the
detailed impacts on the development set.
MD MUC BCUB CEAFE
No postproc. 66.56 54.61 65.93 40.46
With postproc. 67.21 55.62 66.29 40.67
Increase 0.65 1.01 0.36 0.21
Table 2: Impact of the postprocessing step on the devel-
opment set.
4.2 Contribution of features
The lack of time prevented us from running a com-
plete selection from scratch and describing the con-
tribution of each feature on a clean slate. Nonethe-
less, we computed the scores when one feature is
removed from the final feature set. Table 3 shows
the performance degradation observed on the devel-
opment set, which gives an indication of the impor-
tance of each feature. In these runs, no postprocess-
ing was not used.
Toward the end of the table, some features show
a negative contribution to the score on the devel-
opment set. This is explained by the fact that our
feature selection was carried out in a cross-validated
manner over the training set.
4.3 Results on the test set
Table 4 shows the results we obtained on the test set.
The figures are consistent with the performance on
the development set across the three official metrics,
with an increase of the MUC score and a decrease
of both BCUB and CEAFE. The official score in the
shared task is computed as the mean of these three
metrics.
The shared task organizers also provided a test set
with given mention boundaries. The given bound-
aries included nonanaphoric and singleton mentions
as well. Using this test set, we replaced our mention
extraction step and used the given mention bound-
aries instead. Table 4 shows the results with this
setup. As mention boundaries were given, we turned
off our postprocessing module for this run.
48
Metric\Corpus Development set Test set Test set with gold mentions
R P F1 R P F1 R P F1
Mention detection 65.68 68.82 67.21 69.87 68.08 68.96 74.18 70.74 72.42
MUC 55.26 55.98 55.62 60.20 57.10 58.61 64.33 60.05 62.12
BCUB 65.07 67.56 66.29 66.74 64.23 65.46 68.26 65.17 66.68
CEAFM 52.51 52.51 52.51 51.45 51.45 51.45 53.84 53.84 53.84
CEAFE 41.02 40.33 40.67 38.09 41.06 39.52 39.86 44.23 41.93
BLANC 69.6 70.41 70 71.99 70.31 71.11 72.53 71.04 71.75
Official CoNLL score 53.78 54.62 54.19 55.01 54.13 54.53 57.38 56.48 56.91
Table 4: Scores on development set, on the test set, and on the test set with given mention boundaries: recall (R),
precision (P), and harmonic mean (F1). The official CoNLL score is computed as the mean of MUC, BCUB, and
CEAFE.
MD MUC BCUB
All features 66.56 54.61 65.93
I-HDFORM+J-HDFORM -1.35 -2.66 -1.82
STRINGMATCH? -1.12 -1.32 -1.55
DISTANCE? -0.16 -0.62 -0.59
J-HDGOVPOS -0.51 -0.49 -0.13
I-HDRMCFUN -0.27 -0.39 -0.2
ALIAS? -0.47 -0.36 -0.06
I-HDFORM -0.42 -0.18 0.04
I-GENDER+J-GENDER -0.3 -0.15 0.05
NUMBERAGREEMENT? 0.01 -0.14 -0.41
I-HDPOS -0.32 -0.14 0.05
J-PRONOUN? -0.25 -0.08 -0.09
I-HDLMCFORM+
J-HDLMCFORM -0.41 -0.04 0.08
I-HDLSFORM -0.01 0.01 0
SSBERGSMALINPATH -0.04 0.02 -0.13
I-HDGOVFUN -0.09 0.09 0.01
J-HDFUN -0.01 0.13 -0.04
I-HDLMCPOS -0.08 0.13 -0.09
DSPATHFORM -0.03 0.16 -0.02
J-HDGOVFUN -0.04 0.16 -0.05
J-DEMONSTRATIVE? -0.03 0.18 0.03
GENDERAGREEMENT? 0 0.18 -0.01
SEMROLEPROPJHD 0.01 0.2 0.01
I-PRONOUN? 0.01 0.22 0.04
I-HDFUN 0.05 0.22 -0.06
Table 3: The final feature set and, for each feature, the
degradation in performance when leaving out this feature
from the set. All evaluations were carried out on the de-
velopment set. The features marked with a dagger ? orig-
inate from the Soon et al (2001) baseline feature set.
5 Conclusions
The main conclusions and contributions of our work
to the CoNLL 2011 shared task concern the detec-
tion of mention boundaries, feature lexicalization,
and dependency features.
The mention boundaries are relatively difficult to
identify. Although far from perfect, we applied a di-
rect mapping from constituents to extract the men-
tions used in the resolution procedure. We then re-
claimed some mentions involving proper nouns in a
postprocessing step. Using the gold-standard men-
tion boundaries in the test set, we saw an increase in
all metrics with up to 3.51 for the MUC score.
The lexicalization of the feature set brings a sig-
nificant improvement to the scores. By order of per-
formance loss in Table 3, the first feature of our
model is a lexical one. This property does not seem
to have been systematically explored before, possi-
bly because of a tradition of using corpora of modest
sizes in coreference resolution.
Grammatical dependencies seem to play an im-
portant role in the anaphoric expressions. Results in
Table 3 also show this, although in a less pronounced
manner than lexicalization. Features extracted from
dependencies are implicit in many systems, but are
not explicitly mentioned as such. We hope our work
helped clarified this point through a more systematic
exploration of this class of features.
Acknowledgements
This research was supported by Vetenskapsra?det, the
Swedish research council, under grant 621-2010-
4800.
References
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proceedings of the
49
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the ACL, pages
33?40, July.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
SIGKDD Explorations, 11(1):10?18, July.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muischnek,
and Mare Koit, editors, NODALIDA 2007 Conference
Proceedings, pages 105?112, Tartu, May 25-26.
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the 40th Annual Meeting of the Association
for Computational Linguistics, pages 104?111.
Simone Paolo Ponzetto and Michael Strube. 2006. Se-
mantic role labeling for coreference resolution. In
Proceedings of the 11th Conference of EACL: Posters
and Demonstrations, pages 143?146, April.
Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Unre-
stricted coreference: Identifying entities and events in
OntoNotes. In Proceedings of the IEEE International
Conference on Semantic Computing (ICSC), Irvine,
CA, September 17-19.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 shared task: Modeling unre-
stricted coreference in OntoNotes. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning (CoNLL 2011), Portland, Oregon,
June.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521?544.
50
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 64?70,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
Using Syntactic Dependencies to Solve Coreferences
Marcus Stamborg Dennis Medved Peter Exner Pierre Nugues
Lund University
Lund, Sweden
cid03mst@student.lu.se, dt07dm0@student.lth.se
Peter.Exner@cs.lth.se, Pierre.Nugues@cs.lth.se
Abstract
This paper describes the structure of the LTH
coreference solver used in the closed track of
the CoNLL 2012 shared task (Pradhan et al,
2012). The solver core is a mention classifier
that uses Soon et al (2001)?s algorithm and
features extracted from the dependency graphs
of the sentences.
This system builds on Bjo?rkelund and Nugues
(2011)?s solver that we extended so that it can
be applied to the three languages of the task:
English, Chinese, and Arabic. We designed
a new mention detection module that removes
pleonastic pronouns, prunes constituents, and
recovers mentions when they do not match ex-
actly a noun phrase. We carefully redesigned
the features so that they reflect more com-
plex linguistic phenomena as well as discourse
properties. Finally, we introduced a minimal
cluster model grounded in the first mention of
an entity.
We optimized the feature sets for the three lan-
guages: We carried out an extensive evalua-
tion of pairs of features and we complemented
the single features with associations that im-
proved the CoNLL score. We obtained the re-
spective scores of 59.57, 56.62, and 48.25 on
English, Chinese, and Arabic on the develop-
ment set, 59.36, 56.85, and 49.43 on the test
set, and the combined official score of 55.21.
1 Introduction
In this paper, we present the LTH coreference solver
used in the closed track of the CoNLL 2012 shared
task (Pradhan et al, 2012). We started from an
earlier version of the system by Bjo?rkelund and
Nugues (2011), to which we added substantial im-
provements. As base learning and decoding algo-
rithm, our solver extracts noun phrases and posses-
sive pronouns and uses Soon et al (2001)?s pairwise
classifier to decide if a pair corefers or not. Similarly
to the earlier LTH system, we constructed a primary
feature set from properties extracted from the depen-
dency graphs of the sentences.
2 System Architecture
The training and decoding modules consist of a
mention detector, a pair generator, and a feature ex-
tractor. The training module extracts a set of positive
and negative pairs of mentions and uses logistic re-
gression and the LIBLINEAR package (Fan et al,
2008) to generate a binary classifier. The solver ex-
tracts pairs of mentions and uses the classifier and its
probability output, Pcoref (Antecedent,Anaphor), to
determine if a pair corefers or not. The solver has
also a post processing step to recover some mentions
that do not match a noun phrase constituent.
3 Converting Constituents to Dependency
Trees
Although the input to coreference solvers are pairs
or sets of constituents, many systems use concepts
from dependency grammars to decide if a pair is
coreferent. The most frequent one is the con-
stituent?s head that solvers need then to extract us-
ing ad-hoc rules; see the CoNLL 2011 shared task
(Pradhan et al, 2011), for instance. This can be te-
dious as we may have to write new rules for each
new feature to incorporate in the classifier. That is
64
why, instead of writing sets of rules applicable to
specific types of dependencies, we converted all the
constituents in the three corpora to generic depen-
dency graphs before starting the training and solving
steps. We used the LTH converter (Johansson and
Nugues, 2007) for English, the Penn2Malt converter
(Nivre, 2006) with the Chinese rules for Chinese1,
and the CATiB converter (Habash and Roth, 2009)
for Arabic.
The CATiB converter (Habash and Roth, 2009)
uses the Penn Arabic part-of-speech tagset, while
the automatically tagged version of the CoNLL Ara-
bic corpus uses a simplified tagset inspired by the
English version of the Penn treebank. We translated
these simplified POS tags to run the CATiB con-
verter. We created a lookup table to map the simpli-
fied POS tags in the automatically annotated corpus
to the Penn Arabic POS tags in the gold annotation.
We took the most frequent association in the lookup
table to carry out the translation. We then used the
result to convert the constituents into dependencies.
We translated the POS tags in the development set
using a dictionary extracted from the gold training
file and we translated the tags in the training file by a
5-fold cross-validation. We used this dictionary dur-
ing both training and classifying since our features
had a better performance with the Arabic tagset.
4 Mention Extraction
4.1 Base Extraction
As first step of the mention selection stage, we ex-
tracted all the noun phrases (NP), pronouns (PRP),
and possessive pronouns (PRP$) for English and
Arabic, with the addition of PN pronouns for Chi-
nese. This stage is aimed at reaching a high recall of
the mentions involved in the coreference chains and
results in an overinclusive set of candidates. Table 1
shows the precision and recall figures for the respec-
tive languages when extracting mentions from the
training set. The precision is significantly lower for
Arabic than for English and Chinese.
4.2 Removal of the Pleonastic it
In the English corpus, the pronoun it in the first step
of the mention extraction stage creates a high num-
ber of false positive mentions. We built a classifier
1http://stp.lingfil.uu.se/ nivre/research/Penn2Malt.html
Language Recall Precision
English 92.17 32.82
English with named entities 94.47 31.61
Chinese 87.32 32.29
Arabic 87.22 17.64
Table 1: Precision and recall for the mention detection
stage on the training set.
Feature name
HeadLex
HeadRightSiblingPOS
HeadPOS
Table 2: Features used by the pleonastic it classifier.
to discard as many of these pleonastic it as possible
from the mention list.
Table 2 shows the features we used to train the
classifier and Table 3 shows the impact on the final
system. We optimized the feature set using greedy
forward and backward selections. We explored var-
ious ways of using the classifier: before, after, and
during coreference resolving. We obtained the best
results when we applied the pleonastic classifier dur-
ing coreference solving and we multiplied the prob-
ability outputs from the two classifiers. We used the
inequality:
Pcoref (Antecedent, it)? (1? Ppleo(it)) > 0.4,
where we found the optimal threshold of 0.4 using a
5-fold cross-validation.
4.3 Named Entities
The simple rule to approximate entities to noun
phrases and pronouns leaves out between ?8% and
?13 % of the entities in the corpora (Table 1). As the
named entities sometimes do not match constituents,
we tried to add them to increase the recall. We
carried out extensive experiments for the three lan-
English CoNLL score
Without removal 59.15
With removal 59.57
Table 3: Score on the English development set with and
without removal of the pleonastic it pronouns.
65
English Total score
Without named entities 58.85
With named entities 59.57
Table 4: Impact on the overall score on the English devel-
opment set by addition of named entities extracted from
the corpus.
Language Without pruning With pruning
English 56.42 59.57
Chinese 50.94 56.62
Arabic 48.25 47.10
Table 5: Results on running the system on the develop-
ment set with and without pruning for all the languages.
guages. While the named entities increased the score
for the English corpus, we found that it lowered
the results for Chinese and Arabic. We added all
single and multiword named entities of the English
corpus except the CARDINAL, ORDINAL, PER-
CENT, and QUANTITY tags. Table 1 shows the re-
call and precision for English and Table 4 shows the
named entity impact on the overall CoNLL score on
the development set.
4.4 Pruning
When constituents shared the same head in the list
of mentions, we pruned the smaller ones. This in-
creased the scores for English and Chinese, but low-
ered that of Arabic (Table 5). The results for the
latter language are somewhat paradoxical; they are
possibly due to errors in the dependency conversion.
5 Decoding
Depending on the languages, we applied different
decoding strategies: For Chinese and Arabic, we
used a closest-first clustering method as described
by Soon et al (2001) for pronominal anaphors and
a best-first clustering otherwise as in Ng and Cardie
English Total score
Without extensions 57.22
With extensions 59.57
Table 6: Total impact of the extensions to the mention
extraction stage on the English development set.
(2002). For English, we applied a closest-first clus-
tering for pronominal anaphors. For nonpronomi-
nal anaphors, we used an averaged best-first cluster-
ing: We considered all the chains before the current
anaphor and we computed the geometric mean of the
pair probabilities using all the mentions in a chain.
We linked the anaphor to the maximal scoring chain
or we created a new chain if the score was less than
0.5. We discarded all the remaining singletons.
As in Bjo?rkelund and Nugues (2011), we recov-
ered some mentions using a post processing stage,
where we clustered named entities to chains having
strict matching heads.
6 Features
We started with the feature set described in
Bjo?rkelund and Nugues (2011) for our baseline sys-
tem for English and with the feature set in Soon et al
(2001) for Chinese and Arabic. Due to space limita-
tions, we omit the description of these features and
refer to the respective papers.
6.1 Naming Convention
We denoted HD, the head word of a mention in a de-
pendency tree, HDLMC and HDRMC, the left-most
child and the right-most child of the head, HDLS and
HDRS, the left and right siblings of the head word,
and HDGOV, the governor of the head word.
From these tokens, we can extract the surface
form, FORM, the part-of-speech tag, POS, and the
grammatical function of the token, FUN, i.e. the la-
bel of the dependency edge of the token to its parent.
We used a naming nomenclature consisting of the
role in the anaphora, where J- stands for the anaphor,
I-, for the antecedent, F-, for the mention in the chain
preceding the antecedent (previous antecedent), and
A- for the first mention of the entity in the chain;
the token we selected from the dependency graph,
e.g. HD or HDLMC; and the value extracted from
the token e.g. POS or FUN. For instance, the part-
of-speech tag of the governor of the head word of
the anaphor is denoted J-HDGOVPOS.
6.2 Combination of Features
In addition to the single features, we combined them
to create bigram, trigram, and four-gram features.
Table 7 shows the features we used, either single or
in combination, e.g. I-HDFORM+J-HDFORM.
66
We emulated a simple cluster model by uti-
lizing the first mention in the chain and/or the
previous antecedent, e.g. A-EDITDISTANCE+F-
EDITDISTANCE+EDITDISTANCE, where the edit
distance of the anaphor is calculated for the first
mention in the chain, previous antecedent, and an-
tecedent.
6.3 Notable New Features
Edit Distance Features. We created edit distance-
based features between pairs of potentially
coreferring mentions: EDITDISTANCE is the
character-based edit distance between two
strings; EDITDISTANCEWORD is a word-level
edit distance, where the symbols are the com-
plete words; and PROPERNAMESIMILARITY
is a character-based edit distance between
proper nouns only.
Discourse Features. We created features to reflect
the speaker agreement, i.e. when the pair of
mentions corresponds to the same speaker, of-
ten in combination with the fact that both men-
tions are pronouns. For example, references to
the first person pronoun I from a same speaker
refer probably to a same entity; in this case, the
speaker himself.
Document Type Feature. We created the I-HD
FORM+J-HDFORM+DOCUMENTTYPE fea-
ture to capture the genre of different document
types, as texts from e.g. the New Testament are
likely to differ from internet blogs.
6.4 Feature Selection
We carried out a greedy forward selection of the fea-
tures starting from Bjo?rkelund and Nugues (2011)?s
feature set for English, and Soon et al (2001)?s for
Chinese and Arabic. The feature selection used a 5-
fold cross-validation over the training set, where we
evaluated the features using the arithmetic mean of
MUC, BCUB, and CEAFE.
After reaching a maximal score using forward se-
lection, we reversed the process using a backward
elimination, leaving out each feature and removing
the one that had the worst impact on performance.
This backwards procedure was carried out until the
score no longer increased. We repeated this forward-
backward procedure until there was no increase in
performance.
7 Evaluation
Table 7 shows the final feature set for each language
combined with the impact each feature has on the
score on the development set when being left out. A
dash (?) means that the feature is not part of the
feature set used in the respective language. As we
can see, some features increase the score. This is
due to the fact that the feature selection was carried
out in a cross-validated manner over the training set.
Table 8 shows the results on the development and
test sets as well as on the test set with gold mentions.
For each language, the figures are overall consistent
between the development and test sets across all the
metrics. The scores improve very significantly with
the gold mentions: up to more than 10 points for
Chinese.
8 Conclusions
The LTH coreference solver used in the CoNLL
2012 shared task uses Soon et al (2001)?s algorithm
and a set of lexical and nonlexical features. To a
large extent, we extracted these features from the de-
pendency graphs of the sentences. The results we
obtained seem to hint that this approach is robust
across the three languages of the task.
Our system builds on an earlier system that we
evaluated in the CoNLL 2011 shared task (Pradhan
et al, 2011), where we optimized significantly the
solver code, most notably the mention detection step
and the feature design. Although not exactly compa-
rable, we could improve the CoNLL score by 4.83
from 54.53 to 59.36 on the English corpus. The
mention extraction stage plays a significant role in
the overall performance. By improving the qual-
ity of the mentions extracted, we obtained a perfor-
mance increase of 2.35 (Table 6).
Using more complex feature structures also
proved instrumental. Scores of additional feature
variants could be tested in the future and possibly
increase the system?s performance. Due to limited
computing resources and time, we had to confine the
search to a handful of features that we deemed most
promising.
67
All features En (+/-) Zh (+/-) Ar (+/-)
STRINGMATCH -0.003 -0.58 -1.79
A-STRINGMATCH+STRINGMATCH -0.11 ? ?
DISTANCE -0.19 -0.57 -0.24
DISTANCE+J-PRONOUN 0.03 ? ?
I-PRONOUN 0.02 ? ?
J-PRONOUN 0.02 ? ?
J-DEMOSTRATIVE -0.02 0.01 ?
BOTHPROPERNAME ? 0.03 ?
NUMBERAGREEMENT -0.23 ? ?
GENDERAGREEMENT 0.003 ? ?
NUMBERBIGRAM ? 0.06 ?
GENDERBIGRAM -0.03 0.01 ?
I-HDFORM -0.16 ? -0.67
I-HDFUN 0.05 ? ?
I-HDPOS -0.02 ? -0.52
I-HDRMCFUN 0.003 ? ?
I-HDLMCFORM ? ? -0.05
I-HDLMCPOS 0.01 ? ?
I-HDLSFORM -0.08 ? -0.18
I-HDGOVFUN 0.06 ? ?
I-HDGOVPOS ? -0.003 -0.19
J-HDFUN 0.003 ? ?
J-HDGOVFUN 0.03 ? ?
J-HDGOVPOS -0.05 ? ?
J-HDRSPOS ? ? -0.2
A-HDCHILDSETPOS ? 0.06 ?
I-HDFORM+J-HDFORM 0.08 ? -0.57
A-HDFORM+J-HDFORM ? ? -0.46
I-HDGOVFORM+J-HDFORM ? -0.14 0.04
I-LMCFORM+J-LMCFORM -0.07 -0.15 ?
A-HDFORM+I-HDFORM+J-HDFORM 0.11 ? ?
F-HDFORM+I-HDFORM+J-HDFORM ? -0.1 ?
I-HDPOS+J-HDPOS+I-HDFUN+J-HDFUN ? -0.09 ?
I-HDPOS+J-HDPOS+I-HDFORM+J-HDFORM ? ? -0.05
I-HDFORM+J-HDFORM+SPEAKAGREE ? -0.55 ?
I-HDFORM+J-HDFORM+BOTHPRN+SPEAKAGREE -0.11 ? ?
I-HDGOVFORM+J-HDFORM+BOTHPRN+SPEAKAGREE -0.23 ? ?
A-HDFORM+J-HDFORM+SPEAKAGREE 0.04 ? ?
I-HDFORM+J-HDFORM+DOCUMENTTYPE -0.4 -0.18 ?
SSPATHBERGSMALIN -0.07 ? ?
SSPATHFORM ? ? -0.19
SSPATHFUN -0.08 ? -0.14
SSPATHPOS -0.1 -0.11 -0.53
DSPATHBERGSMALIN ? ? 0
DSPATHFORM 0.07 ? ?
DSPATHFORM+DOCUMENTTYPE 0.03 ? ?
DSPATHPOS 0.07 -0.06 0.05
EDITDISTANCE -0.05 -0.16 0
EDITDISTANCEWORD ? ? -0.25
A-EDITDISTANCE+EDITDISTANCE ? ? -0.02
A-EDITDISTANCE+F-EDITDISTANCE ? -0.01 -0.01
A-EDITDISTANCE+F-EDITDISTANCE+EDITDISTANCE ? ? -0.09
EDITDISTANCEWORD+BOTHPROPERNAME 0.02 ? ?
PROPERNAMESIMILARITY -0.03 ? ?
SEMROLEPROPJHD 0.01 ? ?
Table 7: The feature sets for English, Chinese and Arabic, and for each feature, the degradation in performance when
leaving out this feature from the set; the more negative, the better the feature contribution. We carried out all the
evaluations on the development set. The table shows the difference with the official CoNLL score.
68
Metric/Corpus Development set Test set Test set (Gold mentions)
English R P F1 R P F1 R P F1
Mention detection 74.21 72.81 73.5 75.51 72.39 73.92 78.17 100 87.74
MUC 65.27 64.25 64.76 66.26 63.98 65.10 71.22 88.12 78.77
BCUB 69.1 70.94 70.01 69.09 69.54 69.31 64.75 83.16 72.8
CEAFM 57.56 57.56 57.56 56.76 56.76 56.76 66.74 66.74 66.74
CEAFE 43.44 44.47 43.95 42.53 44.89 43.68 71.94 43.74 54.41
BLANC 75.36 77.41 76.34 74.03 77.28 75.52 78.68 81.47 79.99
CoNLL score 59.57 59.36 68.66
Chinese R P F1 R P F1 R P F1
Mention detection 60.55 68.73 64.38 57.65 71.93 64.01 68.97 100 81.63
MUC 54.63 60.96 57.62 52.56 64.13 57.77 63.52 88.23 73.86
BCUB 66.91 74.4 70.46 64.43 77.55 70.38 63.54 88.12 73.84
CEAFM 55.09 55.09 55.09 55.57 55.57 55.57 65.60 65.60 65.60
CEAFE 44.65 39.25 41.78 47.90 38.04 42.41 72.56 42.01 53.21
BLANC 73.23 72.95 73.09 72.74 77.84 75.00 76.96 83.70 79.89
CoNLL score 56.62 56.85 66.97
Arabic R P F1 R P F1 R P F1
Mention detection 55.54 61.7 58.46 56.1 63.28 59.47 56.13 100 71.9
MUC 39.18 43.76 41.34 39.11 43.49 41.18 41.99 69.78 52.43
BCUB 59.16 67.94 63.25 61.57 67.95 64.61 50.45 81.30 62.26
CEAFM 47.8 47.8 47.8 50.16 50.16 50.16 54.00 54.00 54.00
CEAFE 42.57 38.01 40.16 44.86 40.36 42.49 66.16 34.52 45.37
BLANC 62.44 67.18 64.36 66.80 66.94 66.87 67.37 73.46 69.87
CoNLL score 48.25 49.43 53.35
Table 8: Scores on the development set, test set, and test set with gold mentions for English, Chinese, and Arabic:
recall R, precision P, and harmonic mean F1. The official CoNLL score is computed as the arithmetic mean of MUC,
BCUB, and CEAFE.
69
Acknowledgments
This research was supported by Vetenskapsra?det, the
Swedish research council, under grant 621-2010-
4800, and the European Union?s seventh framework
program (FP7/2007-2013) under grant agreement
no. 230902.
References
Anders Bjo?rkelund and Pierre Nugues. 2011. Explor-
ing lexicalized features for coreference resolution. In
Proceedings of the Fifteenth Conference on Compu-
tational Natural Language Learning: Shared Task,
pages 45?50, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Nizar Habash and Ryan Roth. 2009. CATiB: The
Columbia Arabic treebank. In Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers, pages
221?224, Suntec, Singapore, August. Association for
Computational Linguistics.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muischnek,
and Mare Koit, editors, NODALIDA 2007 Conference
Proceedings, pages 105?112, Tartu, May 25-26.
Vincent Ng and Claire Cardie. 2002. Improving machine
laerning approaches to coreference resolution. In Pro-
ceedings of the 40th Annual Meeting of the Association
for Computational Linguistics, pages 104?111.
Joakim Nivre. 2006. Inductive Dependency Parsing.
Springer, Dordrecht, The Netherlands.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 shared task: Modeling unre-
stricted coreference in OntoNotes. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning: Shared Task, pages 1?27, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unrestricted
coreference in OntoNotes. In Proceedings of the
Sixteenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2012), Jeju, Korea.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521?544.
70
