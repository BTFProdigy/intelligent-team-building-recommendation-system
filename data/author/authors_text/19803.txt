Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1521?1532, Dublin, Ireland, August 23-29 2014.
3arif: A Corpus of Modern Standard and Egyptian Arabic Tweets 
Annotated for Epistemic Modality Using Interactive Crowdsourcing 
 
Rania Al-Sabbagh?, Roxana Girju?, Jana Diesner? 
?Department of Linguistics and Beckman Institute  
?School of Library and Information Science  
University of Illinois at Urbana-Champaign, USA 
{alsabba1, girju, jdiesner} @illinois.edu 
Abstract 
We present 3arif1, a large-scale corpus of Modern Standard and Egyptian Arabic tweets annotated for 
epistemic modality2. To create 3arif, we design an interactive crowdsourcing annotation procedure that 
splits up the annotation process into a series of simplified questions, dispenses with the requirement for 
expert linguistic knowledge and captures nested modality triggers and their attributes semi-
automatically.  
1 Introduction 
Epistemic modality, according to Palmer (2001), defines the speaker's subjective knowledge, beliefs 
and judgments about the world's states of affairs. Epistemic modality is used as a linguistic feature for 
multiple NLP tasks and applications, including sentiment analysis (Abdul-Mageed and Diab 2011), 
opinion mining (Benamara et al. 2012) and scientific discourse evaluation (Waard and Maat  2012), 
among others.  
To-date, there are no large-scale modality-annotated Arabic corpora compared to English (Baker et 
al. 2010, 2012; Rubinstein et al. 2013), Chinese (Cui and Chi 2013), Portuguese (Hendrickx et al. 
2012) and Japanese (Matsuyoshi et al. 2010). The creation of modality-annotated corpora is non-trivial 
because there is no consensus definition of modality and its attributes in theoretical linguistics to be 
rendered into annotation tasks and guidelines. Furthermore, most current modality annotation schemes 
rely on sophisticated theoretically-grounded guidelines that require annotators from linguistics back-
ground; hence, annotation is usually restricted to small-scale in-lab settings.  
In this paper, we present 3arif, a large-scale Arabic corpus annotated for epistemic modality. 3arif 
comprises 9822 unique tweets in Modern Standard Arabic (MSA) and Egyptian Arabic (EA), annotat-
ed for 9966 tokens that map to 214 unique types of epistemic modality. Each epistemic modality is 
annotated for sense, polarity, intensification, tense, holder(s) and scope(s). The reason that 3arif fea-
tures the tweets' genre with an emphasis on MSA and EA tweets is that it comes as part of a larger 
project to incorporate linguistic features, such as modality, with network-based features to automati-
cally identify the key players of Twitter's political discourse in counties of political unrest such as 
Egypt. We harvested 3arif from a variety of Twitter users including newspapers, TV stations, political 
campaigns, among others, as well as individuals. As a result 3arif is diglossic for MSA, the formal 
Arabic variety, and EA, the native Arabic dialect of Egypt.  
For the annotation of 3arif, we design a simplified procedure that depicts the following ideas: first, it 
defines each annotation task as a series of open and closed questions that do not require sophisticated 
linguistics background and, meanwhile, provide annotators with self-explanatory annotation guide-
lines; second, it is interactive so that questions are displayed/hidden based on annotators' prior an-
swers; and finally, it semi-automatically identifies and merges nested epistemic modality based on an-
notators' answers to a number of easy-to-administer questions.  
                                               
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer 
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
 
1 Pronounced as ?a:rif in Arabic IPA and as EArif in Buckwalter's transliteration scheme. It means I/he know(s). 
2 3arif is available at http://www.rania-alsabbagh.com/3arif.html 
1521
We evaluate our annotation results using Krippendorff's reliability (Krippendorff 2011) and agree-
ment. Results show high inter-annotator reliability and agreement rates and indicate that our annota-
tion scheme and procedure are efficient. The contribution of this research, therefore, is twofold: first, 
we create a novel resource for Arabic NLP which is expected to enhance research on modality auto-
matic identification and extraction; second, we present an efficient and easy-to-administer annotation 
procedure with interactive crowdsourcing potentials for the complex task of modality annotation. 
The rest of this paper is organized as follows: Section 2 outlines our annotation scheme including 
annotation tasks, guidelines and the interactive structure; Section 3 gives examples for the representa-
tion of the final annotation outputs; Section 4 describes corpus harvesting and sampling; Section 5 dis-
cusses the results and presents a disagreement analysis; Section 6 compares and contrasts our work to 
related work; and Section 7 highlights the points not covered in this current version of 3arif. 
2 Annotation Scheme 
Our annotation scheme consists of six tasks to label sense, polarity, intensification, tense, holders and 
scopes for each epistemic modality. Prior to the beginning of the interactive annotation procedure, we 
highlighted all candidate epistemic modalities in each tweet using a string-match algorithm and the 
lexicons from Al-Sabbagh et al. (2013, 2014).  The algorithm finds all potential epistemic modality 
triggers (i.e. words and phrases that may convey epistemic modality) within each tweet in our corpus 
and marks them as annotation units. A total of 9966 candidate epistemic modality triggers are high-
lighted in 9822 tweets. 
2.1 Task 1: Sense  
Sense annotation is to decide for each highlighted candidate trigger in context whether it actually con-
veys epistemic modality. The same lexical verb ???? A$Er is used as an epistemic modality trigger an-
ticipating a future possibility in example 1; but as a non-modal lexical verb in example 2.  
1. 3]?????? ????? 30?? ????? ??? ??[?? ????  
A$Er An[nA snksr rqm Al30 mlywn mtZAhr] 
I feel that [we will get 30+ million protesters]. 
2. #?????? ?????? ???? ?? ???? ??? ?????? ????: ????  
#hykl: A$Er bAlfxr wAlqlq >yDA fy *krY Hrb >ktwbr 
#Heikl: I feel proud but also worried when I remember October's war. 
We define sense annotation as a synonymy judgment task, following Al-Sabbagh et al. (2013). Epis-
temic modality is represented by an exemplar set manually selected so that: (1) each exemplar is an 
unambiguous epistemic trigger, (2) exemplars are in both MSA and EA, (3) exemplars comprise both 
simple words and multiword expressions, (4) exemplars are both affirmative and negative, and (5) ex-
emplars are of different lexical intensities. Furthermore, we create multiple versions of the same set so 
that we cover the inflections for gender, number, person, tense, mood, and aspect in Arabic. We then 
use the set that morphologically matches the candidate trigger to be annotated. Presented with a pre-
highlighted candidate trigger in context and the exemplar set, annotators are to decide whether the giv-
en candidate trigger is synonymous to the exemplar set, and is hence an epistemic modality trigger, or 
not.  
If an annotator decides that a given candidate trigger does not convey epistemic modality, no further 
questions about polarity, intensification, tense, holders or scopes are displayed. To guarantee that an-
notators do not select the non-synonymous option as an easy escape, they are not allowed to move 
forward without submitting at least one synonym of their own to the candidate trigger.  
Designing the interactive procedure as such results in disagreement propagation. If one annotator 
decides that a given candidate trigger is not epistemic, but another annotator decides that it is, the for-
mer will not have to answer any further questions about polarity, intensification, tense, holders or 
scopes; whereas the latter will have to provide answers for each of those annotation tasks. 
                                               
3 Throughout the examples, epistemic modality triggers are represented in boldface and scopes are in-between square brack-
ets.  
1522
2.2 Task 2: Polarity  
Task 2 uses as input the candidates labeled as valid epistemic modality triggers in Task 1 and labels 
each as either affirmative or negative. An affirmative trigger indicates that the speaker holds the given 
state of affairs (i.e. propositions) as TRUE; whereas a negative trigger indicates that the given proposi-
tions are held as FALSE by the speaker.  
To decide on whether the polarity is affirmative or negative, annotators are instructed to look for the 
absence/presence of such negation markers as: 
? Negation particles such as ?? m$ (not), ?? lA (not) and ??? gyr (not), among others. 
? Negation affixes like the circumfix m...$ in ???? mZn$ (I do not think). 
? Negative polarity items like ???? Emry (never) and ?? ??? lm yEd (no longer). 
? Negative auxiliaries where negation is placed on the past tense auxiliary as in ????? ???? mknt$ 
wAvq (I was not sure).  
? Inherently-negative triggers that encode negation in their lexical meanings such as ?????? 
mstHyl (impossible). 
Annotators are instructed that using multiple negation markers results in an affirmative sense. Thus, 
??? ?? ???????? lys mn AlmstHyl (it is not impossible) means that the proposition is actually possible 
according to the speaker. Put differently, it means that the speaker holds the proposition as TRUE. An-
notators are required to give the reason for negation if they decide that a given trigger is negative. 
2.3 Task 3: Intensification  
Epistemic modality triggers can have different lexical intensities (i.e. intensities encoded in the lexical 
meaning of the word/phrase regardless of the context). For instance, even without a context, Arabic 
speakers know that ?????  mt>kd (I am/he is sure) expresses higher possibility than ???????  mthy>ly (I 
imagine). When used in context, the trigger's lexical intensity can be maintained as is. Yet, it can also 
be amplified or mitigated by various linguistic means such as: 
? Modification: adverbs like ????? tmAmA (absolutely) and ?????? bAlfEl (indeed), among others, 
amplify lexical intensity; whereas mitigation can be caused by such adverbs as ?????? tqrybA (al-
most) and ????? gAlbA (most probably), among others.  
? Categorical negation typically amplifies lexical intensity as in  ???? ??????  m$ mmkn >bdA (it is 
not possible at all).  
? Emphatic expressions such as ?? qd (indeed) and ????? wAllh (I swear), among others, lead to lex-
ical intensity amplification.  
? Coordination of two or more triggers usually results in intensity amplification as in ???? ?????? 
EArf wmt>kd (I know and I am sure). 
The annotators' task for intensification is to decide for each candidate labeled as a valid epistemic 
modality trigger in Task 1 whether its lexical intensity is amplified (AMP), mitigated (MTG), or main-
tained (AS IS). During interactive annotation, annotators are asked to provide the reason for their selec-
tion; that is, whether the lexical intensity is affected by an adverb, categorical negation, an emphatic 
expression, coordination, or any other reason. 
2.4 Task 4: Tense  
In this version of 3arif, we work on the present and past tenses only. Thus, Task 4 is to decide for each 
valid epistemic trigger from Task 1 whether it is present (PRS) or past (PST). Tense can be marked ei-
ther morphologically by inflections and affixes or contextually by auxiliary verbs such as ??? kAn 
(was), among others. Annotators are also required to give their reasons for selecting either PRS or PST. 
2.5 Task 5: Holder  
Holder annotation is to identify the holder of the epistemic modality which is the ?RATIONAL entity 
that expresses its knowledge, beliefs or judgments about the world's states of affairs.  
1523
Holders can be ?RATIONAL entities as in example 3. The entity that is making the assumption that 
the former Palestinian president - Yasser Arafat - may have died of natural causes is the report issued 
by the French government.  
3. ???? ?????? ?????? ???? ?????# ????[: ????? ?????[  
tqryr frnsy: [wfAp #ErfAt rbmA tEwd lAsbAb TbyEyp] 
A French report: [natural causes might be behind the death of #Arafat].  
The holder is not necessarily the same as the trigger's grammatical subject. In example 4, the gram-
matical subject of ???? ybdw (seems) is ???? ???????????  AlAElAn Aldstwry (the constitutional declara-
tion). However, the entity that is making the judgment about this declaration is the French govern-
ment, which is then the real holder of ybdw. 
4. ??? ???? ??????? ?????? ???????? ???????? ?????? ????? ?? ???[: ?????[  
frnsA: [AlAElAn Aldstwry Aljdyd lmrsy lA ybdw Anh yslk AlAtjAh AlSHyH] 
France: [Morsi's new constitutional declaration does not seem to be a correct move].  
Twitter users do not only post their own knowledge, beliefs and judgments about the world's states 
of affairs, but also they (1) directly and indirectly quote others and (2) make assumptions about others' 
knowledge, beliefs and judgments. This means that we can have nested holders, according to Wiebe et 
al. (2005) and Saur? and Pustejovsky (2009), where we know about others' knowledge, beliefs and 
judgments only though the writer or the Twitter user in our case.  
In example 5, the Twitter user quotes Elbaradei stating that he may run for presidency if the people 
want him to. That is, the holder of the epistemic modality is actually Elbaradei not the Twitter user.  
5. ??? ??? ?????  ]????? ?? ???????? ???????[ ??: ????????  
AlbrAdEy: qd [>tr$H fy AntxAbAt Alr}Asp] <*A Tlb Al$Eb  
Elbaradei: I may [run for presidency] if the people want me to.  
The holder of the epistemic modality in example 6 is not the Twitter user, either. However, the Twit-
ter user is not quoting anyone here, but is rather making an assumption about what the Egyptian Na-
tional Party holds as TRUE. 
6.  ? ???? ????[?? ?????????? ??????[  #Jan25 
AlHzb AlwTny mqtnE An[h mmkn yrjE] #Jan25 
The National Party is convinced that [it may get back to authority]. #Jan25 
We can have two or more nested holders. In example 5, we have two: the first is ElBaradei and the 
second is the Twitter user who is quoting ElBaradei. Similarly, in example 6, we have two nested 
holders: the first is the Egyptian National Party and the second is the Twitter user who makes the as-
sumptions about the party's beliefs.  
In example 7, however, we have three nested holders. The first is ??????? AlAxwAn (the Muslim 
Brotherhood) that holds as TRUE the proposition that the Military Council is conspiring against them. 
That belief of the Muslim Brotherhood is communicated to us through the politician ??? ?????? Abw 
AlftwH (Abulfotoh) who is then the second holder. Yet, Abulfotoh has not posted his assumption 
about the Muslim Brotherhood's belief on his personal account. Instead, he has been quoted by another 
Twitter user, who is the third holder.  
7. ???? ?????? ?? ???????[??  ????????????? : ??? ??????[  
Abw AlftwH: AlAxwAn tSwrwA An [hnAk m&Amrp mn AlEskry] 
Abulfotoh: The Muslim Brotherhood members thought that [there was a conspiracy by the Military 
Council]. 
During the interactive procedure, annotators are first asked whether the holder is the same as the 
Twitter user. If not, more questions are displayed to determine: (1) who the real holder is; (2) whether 
the tweet is a(n) (in)direct quote (e.g. there are direct quotation markers or such words as ??? qAl (he 
said) and ??? SrH (he declared), among others), or the tweet conveys the Twitter user's assumptions 
about others. 
1524
When the holder is not the same as the Twitter user, annotators are asked to mark the boundaries of 
the linguistic unit that corresponds to the holder in the tweet's text, following the maximal length prin-
ciple from Szarvas et al. (2008), so that they mark the largest possible, meaningful linguistic unit. 
Hence, in example 8 the holder is the Islamist opponents in #KSA not only the Islamist opponents. 
8. ???# ?? ?????? ???? ???? ??[?? ?????? ????????# ?? ????????? ??????????[  
Al<slAmywn AlmEArDwn fy #AlsEwdyp mwqnwn >n[hA tsEY lqtl Alvwrp fy #mSr] 
Islamist opponents in #KSA know for sure that [it tries to put an end to #Egypt's revolution]. 
2.6 Task 6: Scope  
Scopes are the states of affairs modified by the epistemic modality triggers. Modality scopes in Arabic 
are most likely realized as clauses, deverbal nouns or to-infinitives, according to Al-Sabbagh et al. 
(2013). We use the same maximal length guideline from Task 5 so that the scope segment marked by 
the annotators is the largest possible segment typically delimited by: (1) punctuation markers and (2) 
subordinate conjunctions such as ???  lAn (because) and ?? lw (if), among others. 
In the case of nested triggers as in example 9, where a trigger and its scope are both embedded in 
another trigger's scope, the interactive procedure prompts the annotators to label each trigger and its 
scope separately at first. Afterwards, we automatically merge them as we further explain in Section 3. 
9. ????[ ???? ?[?? ????? ?????? ?????[[  #Jan25 
AlHzb AlwTny mqtnE >n[h mmkn [yrjE]] #Jan25 
The National Party is convinced that [it may [get back to power]] #Jan25 
Annotators are instructed that a single trigger may have one or more scopes. In example 10, the trig-
ger ????????? bythy>lhm (they imagine) scopes over two complement clauses, which annotators are re-
quired to identify. Furthermore, annotators are given the guideline that two or more triggers - typically 
conjoined by a coordinating conjunction - can share the same scope as in example 11. In the cases like 
example 11, each trigger and its attributes are first annotated separately and then once our system finds 
out that they share the same polarity, intensification, tense, holder, and scope, they are merged togeth-
er as we show in Section 3.  
10.  ?????? ??? ?? ?????? ??? ???????? [???] ?? ??????? ??? ???[??  ????????????????[  
>wlAdnA bythy>lhm An [dm AxwAthm rAH hdr] wAn[hm Endhm v>r mE AlslTp bkl >$kAlhA]  
Our children imagine that [their friends were killed for no reason] and that [they now have to take re-
venge from the authorities].  
11. ???????? ???? ?????? ?? [???? 12 % ?? ???????] ?????? ??? ?? ????? ???? 
AlbrAdEy EArf wmtAkd An [nsbp 12% bs htntxbh] wEl$An kdp m$ hyr$H nfsh 
Elbaradei knows and is sure that [only 12% will vote for him]. So, he will not run for presidency. 
Annotators are instructed that scopes are not necessarily adjacent to their triggers. In example 12, 
the scope starts three words to the right of its trigger ?????? bAqtnE (get convinced) given that the adver-
bial phrase ???? ????? Aktr wAktr (more and more) falls in between it and its scope.  
12.  ?? ??? ???????? ??????? ???? ????[???? ????? ?? ???????? ??? ?????[  
kl ywm byEdy bAqtnE Aktr wAktr An[nA knA mHtAjyn dktAtwr wTny EAdl] 
Every day, I get more and more convinced that [we needed a patriotic and fair dictator]. 
Annotators are also instructed that scopes can (1) precede, (2) follow or (3) surround their triggers. 
Many of the aforementioned examples have the scopes following their triggers. Yet, in example 13 the 
scope surrounds its trigger and in example 14 it precedes its trigger.  
13. ]???? ??? ???? ???? ???? ???? ????[  
 [wEwd mrsy lyst fymA ybdw dyn Elyh] 
[Morsi's promises are not seemingly doable]. 
14. ]????? ??? ]???? ????? ???? ????? ?????? ????? ?????? ????? ??? ?????? ????  
[Hmlp t$wyh vwrp ynAyr w<EAdp EqArb AlsAEp tmAmA <lY AlwrA' bd>t] fymA ybdw  
1525
[A campaign to distort the image of January's revolution and to restore everything back to its original 
state has started], seemingly.  
3 Final Output Representation  
All elicited answers during annotation are automatically organized into the representations illustrated 
in the examples below. The representation of example 15 reads as follows: the USER (i.e. the Twitter 
user) used to moderately hold as TRUE the proposition that the revolutionist candidates were unable to 
compete for presidency. We know that this is a past belief that the USER used to have because annota-
tors have labeled the trigger ????? tSwrt (I thought) as past (PST). There are no nested holders given 
that the USER is the same as the holder. The intensity value of MODerate comes from the fact that 
????? tSwrt (I thought) is of a moderate lexical intensity being weaker than such epistemic triggers as 
????? mtAkd (I am sure) and ???? EArf (I know) but stronger than such epistemic triggers as ??? AZn (I 
guess) and ??????? mthyAly (I imagine). Meanwhile, the lexical intensity of tSwrt is neither amplified 
nor mitigated; hence annotators have given it an AS IS intensification label in Task 3. Consequently, in 
the final annotation output the original lexical intensity value has been used to represent how far the 
holder used to consider his/her belief as TRUE. 
15.  ??????? ??? ?? ?????????????? ?????? [? ? ??????? ???????[  
fy AlbdAyp tSwrt An [mr$Hy Alvwrp ADEf mn AlmnAfsp llr}Asp] 
At first, I thought that [the revolutionist candidates are too weak to compete for presidency]. 
rep. USER, MOD PST TRUE, (mr$Hy Alvwrp ADEf mn AlmnAfsp llr}Asp) 
Example 16 shows how two epistemic modality triggers in the same tweet are given two separate 
representations because they share the same holder but neither the same intensity nor the same scopes. 
The first representation illustrates the epistemic trigger ??? ArY (I think) and reads as follows: the US-
ER currently holds as TRUE the proposition that the media is misleading the people; s/he is MODerately 
confident about that. The second representation is for the epistemic trigger ???? wADH (obviously). It 
indicates that the same USER strongly holds as TRUE the proposition that the media is trying to stop the 
change that the people are longing for. Both triggers are labeled as present (PRS) tense. Furthermore, 
both triggers are labeled as maintaining their lexical intensity AS IS. The trigger ??? ArY (I think) is 
then labeled in the final representation as being of MODerate intensity because it is weaker than ????? 
mtAkd (I am sure), for instance, but stronger than ??????? mthyAly (I imagine); whereas the trigger ???? 
wADH (obviously) is labeled as indicating a strong (STRG) belief being synonymous to ????? mtAkd (I 
am sure) and ???? AErf (I know) among other triggers that express speakers' high confidence about 
their knowledge, beliefs and judgments.  
16. ?? ??????? ??????? ???? ???? ??[?? ???? ]???? ???? ???? ?????? ????????[??  ???[  
ArY An [AlAElAm yqdm $bAb yxdrwn Al$Eb] wADH An[hm yqAwmwn Altgyyr Al*y nTmH lh] 
I think [the media presents young speakers who mislead the people]. Obviously, [they are resisting 
the change we are longing for]. 
rep1. USER, MOD PRS TRUE, (AlAElAm yqdm $bAb yxdrwn Al$Eb) 
rep2. USER, STRG PRS TRUE, (hm yqAwmwn Altgyyr Al*y nTmH lh) 
Example 17 illustrates how two coordinating epistemic triggers sharing the same polarity, tense, in-
tensification, holder and scope are represented. They are simply merged in one representation. The 
same example shows how assumptions made by Twitter users about others' knowledge, beliefs and 
judgments are represented. The representation reads as follows: the USER MODerately holds as TRUE 
the proposition that Elbaradei strongly (STRG) holds as TRUE that only 12% of the Egyptians will vote 
for him for presidency. The values of TRUE, MODerate and present (PRS) assigned to the USER's as-
sumption about Elbaradei are default values used to mark Twitter users' assumptions about others' 
knowledge, beliefs and judgments.  
17. ???????? ???? ?????? ?? [???? 12 % ?? ???????] ?????? ??? ?? ????? ???? 
AlbrAdEy EArf wmtAkd An [nsbp 12% bs htntxbh] wEl$An kdp m$ hyr$H nfsh 
Elbaradei knows and is sure that [only 12% will vote for him]. So, he will not run for presidency. 
rep. USER, MOD PRS TRUE, (AlbrAdEy, STRG PRS TRUE, (nsbp 12% bs htntxbh))  
1526
Example 18 represents an epistemic trigger with multiple scopes. The example also represents 
Twitter users making assumptions about others' knowledge, beliefs and judgments. As we mentioned 
in example 17, the values of TRUE, MODerate and present (PRS) assigned to the USER's assumption are 
assigned by default. The trigger ????????? bythy>lhm (they imagine) is labeled as a present (PRS) tense 
affirmative trigger. Its original lexical intensity - which is weak (WK) - is labeled as being maintained 
AS IS. The trigger ????????? bythy>lhm (they imagine) is of a weak lexical intensity because it is weaker 
than ????? mtAkd (I am sure) and even ??? AZn (I think). 
18. ??????? ??? ???????? ????? ??? ?? [???] ?? ??????? ??? ???[??  ??????????????? ?[  
>wlAdnA bythy>lhm An [dm AxwAthm rAH hdr] wAn[hm Endhm v>r mE AlslTp bkl >$kAlhA]  
Our children imagine that [their friends were killed for no reason] and that [they now have to take re-
venge from the authorities]. 
rep. USER, MOD PRS TRUE, (>wlAdnA, WK PRS TRUE ,(dm AxwAthm rAH hdr; hm Endhm v>r mE AlslTp 
bkl >$kAlhA)) 
Example 19 illustrates embedded triggers. Its representation reads as: the USER MODerately holds as 
TRUE that the Egyptian National Party strongly (STRG) holds as TRUE that it (i.e. the Egyptian National 
Party) may get back to ruling. It is important to notice that both the matrix trigger ????? mqtnE (is con-
vinced) and the embedded trigger (i.e. ???? mmkn (may)) share the same holder which is the Egyptian 
National Party.  
19.  ????[ ????? [?? ?????????? ??????[[  #Jan25 
AlHzb AlwTny mqtnE An[h mmkn [yrjE]] #Jan25 
The National Party is convinced that [it may [get back to power]]. 
rep. USER, MOD PRS TRUE, (AlHzb AlwTny, STRG PRS TRUE,(MOD PRS TRUE, (yrjE))) 
Example 20 shows how reported knowledge, beliefs and judgments are represented. The USER in 
this example has no other role but to report Darrag's strong belief that the army will interfere to stop 
the chaos.  
20. ???????#???? #??? #] ?????? ?? ???? ?????? ????????? #: [????  
drAj: [#Aljy$ HtmA sytdxl fy HAlp AlfwDY] #mSr #mrsy #AlAxwAn 
Darrag: [the #army will definitely interfere in the case of chaos] #Egypt #Morsi #Ikhwan 
rep. USER, report, (drAj, STRG PRS TRUE (#Aljy$ sytdxl fy HAlp AlfwDY)) 
4 Corpus Harvesting  
In order to restrict our corpus to political discourse and ensure that we compile a representative corpus 
of epistemic modality, we harvested our corpus so that each tweet (1) has at least one trendy political 
English or Arabic hashtag such as #Egypt and #???? mrsy (Morsi)4, and (2) has at least one epistemic 
modality trigger from the Arabic Modality Lexicons of Al-Sabbagh et al. (2013, 2014). Table 1 gives 
statistics for the sampled corpus that comprises 9822 unique tweets, with 9966 candidate epistemic 
modality triggers that map to 214 unique types. 
 Tokens Types 
Epistemic candidates 9966 214 
All words 175964 47696 
Table 1: Statistics for the sampled corpus  
5 Annotation Results 
5.1 Evaluation Methodology and Metrics  
Our annotation tasks are of two types: (1) Tasks 1-4 are label-based where there is a pre-defined set of 
labels from which annotators choose; and (2) Tasks 5-6 are segmentation-based where the output of 
the annotation is a text segment. For the segmentation-based tasks, we use an all-or-nothing method to 
                                               
4 A total of 304 unique English and Arabic hashtags are found in the sampled corpus. 
1527
measure reliability and agreement: for segments to be considered as agreement, they must share both 
the beginning and end boundaries. We use Krippendorff's alpha ? (Krippendorff 2011) as our inter-
annotator reliability measure, following the most recent work on modality annotation for other lan-
guages including English (Rubinstein et al. 2013) and Chinese (Cui and Chi 2013). For more details 
on Krippendorff's alpha and a comparison of inter-annotator agreement measures, we refer the reader 
to Artstein and Poesio (2008).   
5.2 Results 
We use the surveygizmo services to implement our interactive annotation procedure given that their 
survey structure is one that allows for using conditional branching and skip logic5. We distributed the 
survey on Twitter and we had three annotators participating. According to the short qualifying quiz 
given at the beginning of the survey, all three participants are native Egyptian Arabic (EA) speakers 
who have at least two-year experience with using Twitter. They are also university graduates who, 
therefore, master Modern Standard Arabic. None of the participants has a linguistics background. 
Table 2 shows alpha and agreement rates for each annotation task. We measure the rates in four dif-
ferent scenarios so that we can (1) estimate the effect of the inclusion of the NON-EPISTEMIC category 
agreement, (2) estimate the effect of disagreement propagation from Task 1, and (3) evaluate the 
guidelines and procedures for each annotation task separately. The four scenarios are:  
? w/NONE w/DP: candidates agreed upon as non-epistemic and disagreement propagating from 
Task 1 are both included. 
? w/NONE w/o DP: candidates agreed upon as non-epistemic are included, but disagreement prop-
agating from Task 1 is excluded.  
? w/o NONE w/DP: candidates agreed upon as non-epistemic are excluded, but disagreement prop-
agating from Task 1 is included.  
? w/o NONE w/o DP: candidates agreed upon as non-epistemic and disagreement propagating from 
Task 1 are both excluded. This scenario focuses on each annotation task separately without any 
distractions.  
  Alpha  Agreement 
  w/NONE w/o NONE w/NONE w/o NONE 
Annotation Task w/ DP w/o DP w/ DP w/o DP w/ DP w/o DP w/ DP w/o DP 
1 Sense -- 0.899 -- -- -- 0.949 -- -- 
2 Polarity 0.904 0.974 0.798 0.949 0.939 0.983 0.895 0.976 
3 Intensification 0.880 0.942 0.658 0.768 0.926 0.966 0.844 0.939 
4 Tense 0.911 0.995 0.772 0.983 0.947 0.997 0.909 0.994 
5 Holder 0.878 0.930 0.672 0.727 0.933 0.956 0.884 0.969 
6 Scope 0.825 0.916 0.620 0.618 0.899 0.955 0.819 0.911 
Table 2: Inter-annotator alpha reliability and agreement rates 
In the case of Task 1 (i.e. sense annotation), only the second scenario is applicable: we cannot ex-
clude the candidates agreed upon as non-epistemic because the target is to know how reliable the an-
notation is with regards to distinguishing between epistemic and non-epistemic candidates. It is the 
first annotation task, thus there is no prior disagreement propagation. From Table 2, we derive the fol-
lowing observations:  
? Disagreement in Task 1 propagates ~ 0.05 to 0.1 disagreement for the other annotation tasks. 
? Adding the agreed upon non-epistemic candidates yields up to ~ 0.2 gain for both alpha reliabil-
ity and agreement rates. 
? For an end-to-end automatic system that first identifies triggers and then their attributes, the 
benchmark rates are those from the w/NONE w/DP scenario. 
                                               
5 http://www.surveygizmo.com/ 
1528
5.3 Discussion and Disagreement Analysis 
Among the factors that lead to high inter-annotator alpha reliability and agreement rates are that: (1) 
the vast majority of negation is explicitly marked by negation particles that are easy to detect by hu-
man annotators; (2) the vast majority of triggers are used without any amplification or mitigation 
markers; and (3) punctuation markers are surprisingly informative for marking scope boundaries and 
direct quotations and, hence, holders. 
Sense-related disagreement is attributed to: (1) nominal triggers with main grammatical functions, 
(2) stative triggers, (3) opinionated-evidential triggers and (4) highly-polysemous triggers. 
The majority of epistemic triggers are adjunct constituents that add an extra-layer of meaning and 
can be removed without disturbing the syntactic structure of their propositions. Yet, in example 21, 
?????? AHtmAl (a possibility) is the grammatical subject of the proposition it modifies. Most of the ex-
emplars from Section 2.1 are adjuncts and, thus, none can be both a lexical and a grammatical substi-
tute for ?????? AHtmAl (a possibility) in such a context.  
21. ???? ?????? ]???? ???? ????? ??? ?????? ????? ????? ?????[??  ??????  
AHtmAl An [r}ys mntxb yHl Almjls AvnA' SyAgp dstwr jdyd] AHtmAl whmy 
The possibility that [an elected president dissolves the parliament during the constitution's write-up] is 
an unrealistic possibility.  
Stative triggers such as ???? yErf (he knows) and ???? ydrk (he realizes) invoke disagreement as to 
whether they indicate the acquisition of new information; that is, they literally mean perceive, or they 
mark confirmed beliefs as in be sure that. For example 22, the annotators have two interpretations: (1) 
a non-modal interpretation that whoever says so does not perceive that the Supreme Guide cannot 
make resolutions without the Brotherhood, and (2) a modal interpretation that whoever says so does 
not believe that the Supreme Guide cannot make resolutions without the Brotherhood. 
22.  ?????? ?? ?????? ??? ???? ??? ?????? ??? ???????[??  ???????? ???? ??? ?????? ??[  
Al*y yqwl h*A AlklAm lA yErf An [Almr$d lA ystTyE Ax* qrAr dwn AlrjwE AlY AljmAEp]. 
Whoever says so does not perceive/believe that [the Supreme Guide cannot make resolutions without 
the Brotherhood].  
Opinionated-evidential triggers like ???? yzEm (he claims) do not only mark reported speech, but al-
so they communicate the reporter's own opinion about the truth value of the reported proposition. They 
entail that from the reporter's perspective the proposition is FALSE. Hence, annotators disagree as to 
whether yzEm and similar triggers should be labeled as epistemic or not. We have eventually excluded 
such triggers as epistemic and have included them as evidential triggers for another corpus that is left 
for a future publication.  
Highly-polysemous triggers like ???? ymkn (can/possible) lead to disagreement because in many cas-
es even the context is ambiguous. In example 23, both interpretations of it is not possible that (epis-
temic) and it is not doable that (abilitive) seem to be acceptable.  
23.  ???? ?????"?" ????? ?????: "?? ???????? ???????????? ???" ???? ?? ?????"??? ???? ??? ?[ ??????"[  
lA ymkn [fhm ktAb mHmd mrsy "vA}r mn Al$rq" AlA btAml AlktAbyn AlmjAwryn: "srqAt Sgyrp" w 
"jnwn AlHkm"] 
It is not possible/doable [to understand Morsi's book - A Revolutionist from the East - without reading 
the other two books of Small Robberies and Ruling Mania]. 
Intensity-related disagreement is attributed to (1) intensity on the holder that propagates to the trig-
ger and (2) negation with moderate-intensity triggers. In example 24, the USER uses categorical nega-
tion on the holder  ? ????? ??????? ????  lA ywjd Ay AnsAn EAql (there is no one sane person). For some 
annotators, the power of categorical negation spreads to the trigger, moving its intensity up the scale. 
As for negation with moderate-intensity triggers, some annotators think that ?? ???? lA ymkn (not possi-
ble) is synonymous to impossible. Hence, they consider the negation as an amplification marker.  
24.  ??????? ????? ????????[???  ??????? ???? ?? ????? ????[  
lA ywjd >y AnsAn EAql yEtqd b>n [AlArhAb yEAlj bAlsyAsp]  
There is no one sane person who thinks that [terrorism can be defeated through politics]. 
1529
Polarity-related disagreement is mainly caused by negation due to (1) negated holders and (2) con-
textual negation. Negated holders as in example 24 perplex the annotators as to whether the negation 
scopes over the holder only or both the holder and the trigger. Thus, for some annotators, ????? yEtqd 
(he thinks) is affirmative; and for others it is negative. By contextual negation we mean using words 
such as ??????? Alm$klp (the problem) to describe triggers as in example 25. The USER says that the 
problem is to think that it is a small-scale conflict. To describe this as a problem means that the USER 
thinks of the proposition as FALSE; that is, according to the USER it is actually a large-scale conflict.  
25.  ?????? ????? ?? ??????? ?????? ???? ?????? ????[??  ???????????? ????[  
Alm$klp <nnA ntSwr <n [AlSrAE mHSwr fY AldA}rp AlDyqp AllY bntHrk fyhA] 
The problem is to think that [the conflict is only happening at this small-scale we are working on]. 
Holder-related disagreement is attributed mainly to generic nouns and impersonal pronouns such as 
????? Al$Eb (the people) and ?????? AlwAHd (one). Some annotators interpret them as implicitly refer-
ring to the USER. Therefore, they select the USER as the only holder with zero nesting in example 26. 
Other annotators interpret them as referring to people in general but not necessarily with the USER in-
cluded; and thus, they select two-level nested holders.  
26.  ?? ?????? ???? ????? ??????? ??????????????? ??????????? ?? ???? ??[??  ?????????[  
Al$Eb yErf An [AlmmArsp AldymwqrATyp hy Alty st>ty bAEDA' mjls Al$Eb wAlr}ys AlqAdm] 
People know that [democracy will result in real parliamentary and presidential elections]. 
Scope-related disagreement is attributed to (1) ambiguous subordinate conjunctions, (2) triggers 
modifiers, (3) absent punctuation markers, and (4) embedding within the scope boundaries. For in-
stance, in example 27, the adverbial clause starting with ??? bEd (after) confuses the annotators as to 
whether it is part of the scope or it describes the verb epistemic trigger ????? AtwqE (I expect).   
27. ???? ????? ?? ???????? ?????? ??? ???? ????? ????? ????? ????? ??????? ??????? ???[??? ??  ?????[  
AtwqE jdA An [AEtSAm AltHryr ytfD bnfs Tryqp fD AlAEtSAm AlAxyr bEd Zhwr A$kAl grybp fljAn 
AlAmn] 
I very much expect that [the sit-in in Tahrir will be broken up in the same way as the last sit-in after 
seeing some strange faces at the security checkpoints].  
Tense yields almost perfect inter-annotator alpha reliability and agreement rates. The one main disa-
greement factor, however, is such contexts as ?????? ???? Abtdyt ASdq (I started to believe). While the 
majority of annotators agree that such contexts mark present tense knowledge, beliefs and judgments, 
some annotators consider them as past tense.  
5.4 Majority Statistics for 3arif 
Based on majority annotations, Table 3 gives statistics for 3arif in terms of sense, polarity, intensifica-
tion and tense. Furthermore, approximately 62% of the triggers have zero-nested holders (i.e. the Twit-
ter user is the same as the holder). As for scope syntactic structures, they are distributed as 86% claus-
es, 9% deverbal nouns and the rest are to-infinitives.   
 Sense Polarity Intensification Tense 
 Epistemic Non-epistemic True False Amplified Mitigated As is Present Past 
Tokens 5591 4375 3425 2166 1083 330 4178 4399 1192 
Types 209 175 176 134 133 50 150 175 104 
Table 3: Majority statistics for 3arif 
6 Related Work 
Epistemic modality has been the focus of many annotation projects for multiple languages. Diab et al. 
(2009) annotate three belief categories for English: (1) committed belief is when writers indicate that 
they hold propositions as TRUE, (2) non-committed belief is when writers hold propositions as FALSE, 
and (3) not applicable is when propositions are not denoting beliefs at all. Interest is given to writers' 
beliefs only. Thus, a default value for the modality holder is the writer, and nested holders are not an-
1530
notated. Their corpus contains 10k words of running text from different domains and genres, including 
newswire, blog data, email and letter correspondence and transcribed dialogue data. Inter-annotator 
agreement rate is 0.95 including the NONE category where no belief markers exist. 
Baker et al. (2010, 2012) simultaneously annotate modality and modality-based negation to build 
modality taggers to enhance Urdu-English machine translation systems. Their annotation scheme dis-
tinguishes eight modality types: requirements, permissions, success, effort, intention, ability, desires 
and beliefs. Originally, their annotation scheme labels three attributes for each modality type: triggers, 
holders and targets (i.e. scopes). Yet, holders have not been eventually labeled. A unique feature of 
their annotation scheme is using a simplified operational procedure to label modality semantic mean-
ings. The procedure relies on a list of thirteen choices of the form of H (modal) [P true/false] where H 
is a holder and P is a proposition or an event. The annotators' task is then to select the best form to rep-
resent the modality meaning of a given trigger. Reported kappa ? inter-annotator agreement rates are 
0.82 for triggers and 0.76 for targets.  
Rubinstein et al. (2013) propose a linguistically-motivated scheme for modality annotation in the 
MPQA English corpus. They attain macro alpha inter-annotator reliability rates of 0.89 and 0.65 for 
sense and scope, respectively. Cui and Chi (2013) apply the same scheme from Rubinstein et al. 
(2013) to the Chinese Penn Treebank and get alpha inter-annotator reliability rates of 0.81and 0.39 for 
sense and scope annotation, respectively.  
Al-Sabbagh et al. (2013) annotate epistemic modality in MSA and EA tweets. We attain kappa inter-
annotator agreement rates of 0.90 and 0.93 for sense and scope annotation, respectively, for only 548 
epistemic tokens.  
Our annotation results, therefore, are comparable to the results in the literature. Furthermore, our an-
notation scheme is orthogonal to most of the aforementioned schemes. However, the key differences 
between our work and related work are:  
? We annotate nested modality, unlike Diab et al. (2009) and Baker et al. (2010, 2012).  
? We use a wider range of negation and intensification markers compared to prior work, especial-
ly Al-Sabbagh et al. (2013)  
? We use interactive crowdsourcing with simplified guidelines, unlike in-lab annotations includ-
ing Rubinstein et al. (2013) and Cui and Chi (2013), among others.  
7 Uncovered Points in 3arif 
The current version of 3arif does not cover modality entailment that example 28 illustrates. The USER 
criticizes whoever holds as TRUE the proposition that Egypt can blackmail UAE using the Iranian 
threat. This criticism entails that the USER holds the same proposition as FALSE. 
28.  ?????#?????  ??????????? ???? ?? ????? #[??  ??????? ??[  
yxTY' mn yZn An [#mSr ymkn An tsAwm #Al<mArAt bwrqp #<yrAn] 
Whoever thinks that [Egypt can blackmail #UAE using #Iran] is wrong. 
We do not also cover the future tense, the interrogative, the imperative or the hypothetical moods. 
This is because they have different interpretations when it comes to intensification and polarity that we 
do not cover in this version of 3arif but we will in future work.  
8 Conclusion 
We presented 3arif, a large-scale corpus annotated for epistemic modality in MSA and EA tweets. We 
used a simplified approach that defines each annotation task as a series of questions, implemented in-
teractively. Our scheme covers a wide range of the most common annotation units mentioned in the 
literature, including modality sense, polarity, intensification, tense, holders and scopes. We deal with 
nested holders that are crucial in a highly interactive genre such as tweets where users frequently quote 
others and make assumptions about them. We also automatically merge triggers with shared holders 
and scopes based on elicited annotators' answers. The annotation procedure yields reliable results and 
creates a novel resource for Arabic NLP. For future versions of the corpus, we plan to cover the points 
1531
from Section 7. 3arif will also be used to train and test an automatic machine learning system to iden-
tify epistemic modality and its attributes in MSA and EA tweets. 
References 
Muhammad Abdul-Mageed and Mona Diab. 2011. Subjectivity and Sentiment Annotation of Modern Standard 
Arabic Newswire. In Proceedings of the 5th Linguistic Annotation Workshop (LAW V), pages 110-118, Port-
land, Oregon, June 23-24, 2011. 
Rania Al-Sabbagh, Jana Diesner and Roxana Girju. 2013. Using the Semantic-Syntactic Interface for Reliable 
Arabic Modality Annotation. In Proceedings of IJCNLP'13, pages 410-418, Nagoya, Japan, October 14-18, 
2013.  
Rania Al-Sabbagh, Roxana Girju and Jana Diesner. 2014. Unsupervised Construction of a Lexicon and a Pattern 
Repository of Arabic Modal Multiword Expressions. In Proceedings of the 10th Workshop of Multiword Ex-
pressions at EACL 2014, pages 114-123, Gothenburg, Sweden, April 26-27, 2014. 
Ron Artstein and Massimo Poesio. 2008. Inter-Coder Agreement for Computational Linguistics. Computational 
Linguistics, volume 34, issue 4, pages 555-596. 
Kathrin Baker, Michael Bloodgood, Mona Diab, Bonnie Dorr, Nathaniel W. Filardo, Lori Levin and Christine 
Piatko. 2010. A Modality Lexicon and its Use in Automatic Tagging. In Proceedings of LREC'10, pages 1402-
1407, Valetta, Malta, May 19-21, 2010. 
Kathryn Baker, Michael Bloodgood, Bonnie J. Dorr, Chris Callison-Burch, Nathaniel W. Filardo, Christine 
Piatko, Lori Levin and Scott Miller. 2012. Modality and Negation in SIMT. Computational Linguistics. vol-
ume 38, issue 2, pages 411-438. 
Farah Benamara, Baptiste Chardon, Yannick Mathieu, Vladimir Popescu and Nicholas Asher. 2012. How do 
Negation and Modality Impact on Opinions. In Proceedings of the ACL-2012 Workshop on ExProM-2012, 
pages 10-18, Jeju, Republic of Korea, July 13, 2012. 
Yanyan Cui and Ting Chi. 2013. Annotating Modal Expressions in the Chinese Treebank. In Proceedings of the 
IWC 2013 Workshop on Annotation of Modal Meaning in Natural Language (WAMM), pages 24-32, Potsdam, 
Germany, March 19, 2013. 
Mona Diab, Lori Levin, Teruko Mitamura, Owen Rambow, Vinodkumar Prabhakaran, and Weiwei Guo. 2009. 
Committed Belief Annotation and Tagging. In Proceedings of the 3rd Linguistic Annotation Workshop, ACL-
IJCNLP'09, pages 68-73, Suntec, Singapore, August 6-7, 2009. 
Iris Hendrickx, Am?lia Mendes and Silvia Mencarelti. 2012. Modality in Text: A Proposal for Corpus Annota-
tion. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC'12), 
pages 1805-1812, Istanbul, Turkey, May 21-27, 2012. 
Klaus Krippendorff. 2011. Computing Krippendorff's Alpha Reliability. Annenberg School of Communication, 
Departmental Papers: University of Pennsylvania. 
Suguru Matsuyoshi, Megumi Eguchi, Chitose Sao, Koji Murakami, Kentaro Inui and Yuji Matsumoto. 2010. 
Annotating Event Mentions in Text with Modality Focus and Source Information.  In Proceedings of 
LREC'10,  pages 1456-1463, Valletta, Malta, May 19-21, 2010. 
Frank R. Palmer. 2001. Mood and Modality. 2nd Edition. Cambridge University Press, Cambridge, UK. 
Aynat Rubinstein, Hillary Harner, Elizabeth Krawczyk, Daniel Simoson, Graham Katz and Paul Portner. 2013. 
Toward Fine-Grained Annotation of Modality in Text. In Proceedings of the IWC 2013 Workshop on Annota-
tion of Modal Meaning in Natural Language (WAMM), pages 38-46, Potsdam, Germany, March 19, 2013.  
Roser Saur? and James Pustejovsky. 2009. FactBank: A Corpus Annotated with Event Factuality. Language Re-
sources and Evaluation, volume 43, pages 227-268. 
Gy?rgy Szarvas, Veronika Vincze, Rich?rd Farkas and J?nos Csirik. 2008. The BioScope Corpus: Annotation 
for Negation, Uncertainty and their Scope in Biomedical Texts. In Proceedings of BioNLP 2008: Current 
Trends in Biomedical Natural Language Processing, pages 38-45, Columbus, Ohio, USA, June 2008. 
Anita de Waard and Henk Pander Maat. 2012. Epistemic Modality and Knowledge Attribution in Scientific Dis-
course: a Taxonomy of Types and Overview of Features. In Proceedings of the 50th ACL, pages 47-55, Jeju, 
Republic of Korea, July 12, 2012.  
Janyce Wiebe, Theresa Wilson and Claire Cardie. 2005. Annotating Expressions of Opinions and Emotions in 
Language. Language Resources and Evaluation, volume 39, issue 2-3, pages 163-210. 
1532
Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 114?123,
Gothenburg, Sweden, 26-27 April 2014. c?2014 Association for Computational Linguistics
Unsupervised Construction of a Lexicon and a Repository of Variation
Patterns for Arabic Modal Multiword Expressions
Rania Al-Sabbagh?, Roxana Girju?, Jana Diesner?
?Department of Linguistics and Beckman Institute
?School of Library and Information Science
University of Illinois at Urbana-Champaign, USA
{alsabba1, girju, jdiesner}@illinois.edu
Abstract
We present an unsupervised approach to build
a lexicon of Arabic Modal Multiword
Expressions (AM-MWEs) and a repository of
their variation patterns. These novel resources
are likely to boost the automatic identification
and extraction  of AM-MWEs1.
1 Introduction
Arabic Modal Multiword Expressions (AM-
MWEs) are complex constructions that convey
modality senses. We define seven modality
senses, based on Palmer's (2001) cross-lingual
typology, which are (un)certainty, evidentiality,
obligation, permission, commitment, ability and
volition.
AM-MWEs range from completely fixed,
idiomatic and sometimes semantically-opaque
expressions, to morphologically, syntactically
and/or lexical productive constructions. As a
result, the identification and extraction of AM-
MWEs have to rely on both a lexicon and a
repository of their variation patterns. To-date and
to the best of our knowledge, neither resource is
available. Furthermore, AM-MWEs are quite
understudied despite the extensive research on
general-purpose Arabic MWEs.
To build both the lexicon and the repository,
we design a four-stage unsupervised method.
Stage 1, we use Log-Likelihood Ratio and a
root-based procedure to extract candidate AM-
MWEs from large Arabic corpora. Stage 2, we
use token level features with k-means clustering
to construct two clusters. Stage 3, from the
clustering output we extract patterns that
describe the morphological, syntactic and
semantic variations of AM-MWEs, and store
1 Both resources are available at
http://www.rania-alsabbagh.com/am-mwe.html
them in the pattern repository. Stage 4, we use
the most frequent variation patterns to bootstrap
low-frequency and new AM-MWEs. The final
lexicon and repository are manually inspected.
Both resources are made publicly available.
The contributions of this paper are: (1) we
address the lack of lexica and annotated
resources for Arabic linguistic modality; and
hence, we support NLP applications and domains
that use modality  to identify (un)certainty (Diab
et al. 2009), detect power relations (Prabhakaran
and Rambow 2013), retrieve politeness markers
(Danescu-Niculescu-Mizil et al. 2013), extract
and reconstruct storylines (Pareti et al. 2013) and
classify request-based emails (Lampert et al.
2010); (2) we provide both a lexicon and a
repository of variation patterns to help increase
recall while keeping precision high for the
automatic identification and extraction of
productive AM-MWEs; and (3) we explore the
morphological, syntactic and lexical properties of
the understudied AM-MWEs.
For the rest of this paper, Section 2 defines
AM-MWEs. Section 3 outlines related work.
Sections 4 describes our unsupervised method.
Section 5 describes manual verification and the
final resulting resources.
2 What are AM-MWEs?
AM-MWEs are complex constructions that
convey (un)certainty, evidentiality, obligation,
permission, commitment, ability and volition.
Based on their productivity, we define five types
of AM-MWEs:
Type 1 includes idiomatic expressions like  ????
????? HtmA wlAbd (must), ??? ???? lEl wEsY
(maybe) and ???? ???? fymA ybdw (seemingly).
Type 2 covers morphologically productive
expressions such as ???? ?? yrgb fy (he wants to)
and ???? ?? wAvq mn (sure about). They inflect
114
AM-MWEs Unigram Synonym(s) English GlossArabic Transliteration Arabic Transliteration
???? ????? ??? Eqdt AlEzm ElY  ????-???? Ezmt - nwyt I intended (to)
?? ?????? ?? fy AmkAny An ?????? ymknny I can/I have the ability to
??? ?????? ??? ldy AEtqAd bAn ????? AEtqd I think
???? ?????? ??? hnAk AHtmAl bAn ????????? yuHotamal possibly/there is a possibility that
Table 1: Example AM-MWEs and their unigram synonyms
for gender, number, person, and possibly for
tense, mood and aspect. Neither the head word
nor the preposition is replaceable by a synonym.
In the literature of MWEs, Type 2 is referred to
as phrasal verbs. In the literature of modality, it
is referred to as quasi-modals (i.e. modals that
subcategorize for prepositions).
Type 3 comprises lexically productive
expressions whose meanings rely on the head
noun, adjective or verb. If the head word is
replaced by another of the same grammatical
category but a different meaning, the meaning of
the entire expression changes. Hence, if we
replace the head adjective ??????? AlDrwry
(necessary) in ?? ?? ??????? mn AlDrwry An (it
is necessary to) with ?????? Almmkn (possible),
the meaning changes from obligation to
uncertainty.
Type 4 comprises syntactically productive
expressions. It is similar to Type 3 except that
the head words are modifiable and their
arguments, especially indirect objects, can be
included within the boundaries of the MWE.
Thus, the same expression from Type 3 can be
modified as in ?? ??? ?? ??????? mn AlDrwry jdA
An (it is very necessary to). Furthermore, we can
have an inserted indirect object as in ?? ???????
?? ???????? mn AlDrwry llmSryyn An (it is
necessary for Egyptians to).
Type 5 includes morphologically, lexically
and syntactically productive expressions like  ???
???? ?? ldy yqyn An (I have faith that).
Morphologically, the object pronoun in ??? ldy (I
have) inflects for person, gender and number.
Syntactically, the head noun can be modified by
adjectives as in ?? ??? ???? ???? ldy yqyn rAsx An
(I have a strong faith that). Lexically, the
meaning of the expression relies on the head
noun ???? yqyn (faith) which is replaceable for
other modality-based nouns such as ?? ??? ??? ldy
nyp An (I have an intention to).
Despite the semantic transparency and the
morpho-syntactic and lexical productivity of the
expressions in Types 3-5, we have three reasons
to consider them as AM-MWEs:
First, although the head words in those
expressions are transparent and productive, the
other components, including prepositions,
relative adverbials and verbs, are fixed and
conventionalized. In ?? ??????? ?? mn AlDrwry
An (literally: from the necessary to; gloss: it is
necessary to), the preposition ?? mn (from)
cannot be replaced by any other preposition. In
?????? ??? ???? hnAk AHtmAl bAn (there is a
possibility that), the relative adverbial ???? hnAk
(there is) cannot be replaced by another relative
adverbial such as ??? hnA (there is). In  ????? ??????
?? ?? yHdwny AlAml fy An (hope derives me to),
the head is the noun ????? AlAml (the hope).
Therefore, the lexical verb ?????? yHdwny (drives
me) cannot be replaced by other synonymous
verbs such as ?????? yqwdqny (leads me) or ??????
ydfEny (pushes/drives me).
Second, each of those expressions has a strictly
fixed word order. Even for expressions that allow
the insertion of modifiers and verb/noun
arguments, the inserted elements hold fixed
places within the boundaries of the expression.
Complex constructions that adhere to strict
constraints on word order but undergo lexical
variation are classified by Sag et al. (2002) as
semi-fixed MWEs.
Finally, each expression of those types is
lexically perceived as a one linguistic unit that
can be replaced in many contexts by a unigram
synonym as illustrated in Table 1. According to
Stubbs (2007) and Escart?n et al. (2013), the
perception of complex constructions as single
linguistic units is characteristic of MWEs.
3 Related Work
There is a plethora of research on general-
purpose Arabic MWEs. Yet, no prior work has
focused on AM-MWEs. Hawwari et al. (2012)
describe the manual construction of a repository
for Arabic MWEs that classifies them based on
their morpho-syntactic structures.
115
Corpus Token # Types # Description
Ajdir 113774517 2217557 a monolingual newswire corpus of Modern Standard Arabic
LDC ISI 28880558 532443 an LDC parallel Arabic-English corpus (Munteanu & Marcu 2007)
YADAC 6328248 457361 a dialectal Arabic corpus of Weblogs and tweets (Al-Sabbagh & Girju 2012)
Tashkeel 6149726 358950 a vowelized corpus of Classical and Modern Standard Arabic books
Total 41472307 3566311
Table 2: Statistics for the extraction corpora
Attia et al. (2010) describe the construction of
a lexicon of Arabic MWEs based on (1)
correspondence asymmetries between Arabic
Wikipedia titles and titles in 21 different
languages, (2) English MWEs extracted from
Princeton WordNet 3.0 and automatically
translated into Arabic, and (3) lexical association
measures.
Bounhas and Slimani (2009) use syntactic
patterns and Log-Likelihood Ratio to extract
environmental Arabic MWEs. They achieve
precision rates of 0.93, 0.66 and 0.67 for
bigrams, trigrams and quadrigrams, respectively.
Al-Sabbagh et al. (2013) manually build a
lexicon of Arabic modals with a small portion of
MWEs and quasi-modals. In this paper, quasi-
modals are bigram AM-MWEs. Hence, their
lexicon has 1,053 AM-MWEs.
Nissim and Zaninello (2013) build a lexicon
and a repository of variation patterns for MWEs
in the morphologically-rich Romance languages.
Similar to our research, their motivation to
represent the productivity of Romance MWEs
through variation patterns is to boost their
automatic identification and extraction. Another
similarity is that we define variation patterns as
part-of-speech sequences. The  difference
between their research and ours is that our
variation patterns have a wider scope because we
cover both the morpho-syntactic and lexical
variations of AM-MWEs, whereas their variation
patterns deal with morphological variation only.
4 The Unsupervised Method
4.1 Extracting AM-MWEs
4.1.1 Extraction Resources
Table 22 shows the token and type counts as well
as the descriptions of the corpora used for
extraction. For corpus preprocessing, (1) html
mark-up and diacritics are removed. (2) Meta-
2Ajdir: http://aracorpus.e3rab.com/
Tashkeel: http://sourceforge.net/projects/tashkeela/
linguistic information such as document and
segment IDs, section headers, dates and sources,
as well as English data are removed. (3)
Punctuation marks are separated from words. (4)
Words in Roman letters are removed. (5)
Orthographical normalization is done so that all
alef-letter variations are normalized to A, the
elongation letter (_) and word lengthening are
removed. (6) Finally, the corpus is tokenized and
Part-of-Speech (POS) tagged by MADAMIRA
(Pasha et a. 2014); the latest version of state-of-
the-art Arabic tokenizers and POS taggers.
4.1.2 Extraction Set-up and Results
We restrict the size of AM-MWEs in this paper
to quadrigrams. Counted grams include function
and content words but not affixes. Working on
longer AM-MWEs is left for future research.
The extraction of candidate AM-MWEs is
conducted in three steps:
Step 1: we use root-based information to
identify the words that can be possible
derivations of modality roots. For modality roots,
we use the Arabic Modality Lexicon from Al-
Sabbagh et al. (2013).
In order to identify possible derivations of
modality roots, we use RegExps. For instance,
we use the RegExp (\w*)m(\w*)k(\w*)n(\w*) to
identify words such as ?????? Almmkn (the
possible), ????? Atmkn (I manage) and ???????
bAmkAny (I can) which convey modality.
This RegExp-based procedure can result in
noise. For instance, the aforementioned RegExp
also returns the word ????????? AlAmrykAn
(Americans) which happens to have the same
three letters of the root in the same order
although it is not one of its derivations. Yet, the
procedure still filters out many irrelevant words
that have nothing to do with the modality roots.
Step 2: for the resulting words from Step 1, we
extract bigrams, trigrams and quadrigrams given
the frequency thresholds of 20, 15 and 10,
respectively.
116
In previous literature on MWEs with corpora
of 6-8M words, thresholds were set to 5, 8 and
10 for MWEs of different sizes. Given the large
size of our corpus, we decide to use higher
thresholds.
Step 3: for the extracted ngrams we use the
Log-Likelihood Ratio (LLR) to measure the
significance of association between the ngram
words. LLR measures the deviation between the
observed data and what would be expected if the
words within the ngram were independent. Its
results are easily interpretable: the higher the
score, the less evidence there is in favor of
concluding that the words are independent.
LLR is computed as  in Eq. 1 where Oij and Eij
are the observed and expected frequencies,
respectively3. LLR is not, however, the only
measure used in the literature of MWEs.
Experimenting with more association measures
is left for future work.
Eq. 1: LLR = 2 ? O log
Table 3 shows the unique type counts of the
extracted ngrams. The extracted ngrams include
both modal and non-modal MWEs. For instance,
both ?? ?????? ??? ?? mn Almmkn lnA An (it is
possible for us to) and ?? ???? ??? ???? fy Aqrb
wqt mmkn (as soon as possible) are extracted as
valid quadrigrams. Both have the word ????
mmkn (possible) derived from the root m-k-n.
Both are frequent enough to meet the frequency
threshold. The words within each quadrigram are
found to be significantly associated according to
LLR. Nevertheless, mn Almmkn lnA An is an
AM-MWE according to our definition in Section
2, but fy Aqrb wqt mmkn is not. This is because
the former conveys the modality sense of
possibility; whereas the latter does not.
Therefore, we need the second clustering stage in
our unsupervised method to distinguish modal
from non-modal MWEs.
Ngram size Unique Types
Bigrams 86645
Trigrams 43397
Quadrigrams 25634
Total 96031
Table 3: Statistics for the extracted MWEs
3 We use Banerjee and Pedersen's (2003) Perl
implementation of ngram association measures.
4.2 Clustering AM-MWEs
Clustering is the second stage of our
unsupervised method to build the lexicon of the
AM-MWEs and the repository of their variation
patterns. This stage takes as input the extracted
ngrams from the first extraction stage; and aims
to distinguish between the ngrams that convey
modality senses and the ngrams that do not.
4.2.1 Clustering Set-up
The clustering feature set includes token level
morphological, syntactic, lexical and positional
features. It also has a mixture of nominal and
continuous-valued features as we explain in the
subsequent sections.
4.2.1.1 Morphological  Features
Roots used to guide the extraction of candidate
AM-MWEs in Section 4.1.2 are used as
clustering morphological features. The reason is
that some roots have more modal derivations
than others. For instance, the derivations of the
root ?
 -
?
 -
? D-r-r include ????? Drwry
(necessary), ???????? bAlDrwrp (necessarily),
and ???? yDTr (he has to); all of which convey
the modality sense of obligation. Consequently,
to inform the clustering algorithm that a given
ngram was extracted based on the root D-r-r
indicates that it is more likely to be an AM-
MWE.
4.2.1.2 Syntactic Features
In theoretical linguistics, linguists claim that
Arabic modality triggers (i.e. words and phrases
that convey modality senses) subcategorize for
clauses, verb phrases, to-infinitives and deverbal
nouns. For details, we refer the reader to Mitchell
and Al-Hassan (1994), Brustad (2000), Badawi
et al. (2004) and Moshref (2012).
These subcategorization frames can be
partially captured at the token level. For
example, clauses can be marked by
complementizers, subject and demonstrative
pronouns and verbs. To-infinitives in Arabic are
typically marked by ?? An (to). Even deverbal
nouns can be detected with some POS tagsets
such as Buckwalter's (2002) that labels them as
NOUN.VN.
Based on this, we use the POS information
around the extracted ngrams as contextual
syntactic features for clustering. We limit the
117
window size of the contextual syntactic features
to ?1 words.
Furthermore, as we mentioned in Section 2, we
define AM-MWEs as expressions with fixed
word order. That is, the sequence of the POS tags
that represent the internal structure of the
extracted ngrams can be used as syntactic
features to distinguish modal from non-modal
MWEs.
4.2.1.3 Lexical Features
As we mentioned in Section 2, except for the
head words of the AM-MWEs, other components
are usually fixed and conventionalized.
Therefore, the actual lexical words of the
extracted ngrams can be distinguishing features
for AM-MWEs.
4.2.1.4 Positional Features
AM-MWEs, especially trigrams and quadrigrams
that scope over entire clauses, are expected to
come in sentence-initial positions. Thus we use
@beg (i.e. at beginning) to mark whether the
extracted ngrams occur at sentence-initial
positions.
4.2.1.5 Continuous Features
Except for nominal morphological and lexical
features, other features are continuous. They are
not extracted per ngram instance, but are defined
as weighted features across all the instances of a
target ngram.
Thus, @beg for ngrami is the probability of
ngrami to occur in a sentence-initial position. It
is computed as the frequency of ngrami
occurring at a sentence-initial position
normalized by the total number n of ngrami in
the corpus.
Similarly, POS features are continuous. For
instance, the probability that ngrami is followed
by a deverbal noun is the frequency of its POS+1
tagged as a deverbal noun normalized by the
total number n of ngrami in the corpus.
4.2.2 Clustering Resources
As we mentioned earlier, the extracted ngrams
from the extraction stage are the input for this
clustering stage. The root features are the same
roots used for extraction. The POS features are
extracted based on the output of MADAMIRA
(Pasha et al. 2014) that is used to preprocess the
corpus - Section 4.1.1. The positional features
are determined based on the availability of
punctuation markers for sentence boundaries.
We implement k-means clustering with k set to
two and the distance metric set to the Euclidean
distance4. The intuition for using k-means
clustering is that we want to identify AM-MWEs
against all other types of MWEs based on their
morpho-syntactic, lexical and positional features.
Thus the results of k-means clustering with k set
to two will be easily interpretable. Other
clustering algorithms might be considered for
future work.
4.2.3 Clustering Evaluation and Results
4.2.3.1 Evaluation Methodology
We use precision, recall and F1-score as
evaluation metrics, with three gold sets: BiSet,
TriSet and QuadSet, for bigrams, trigrams and
quadrigrams, respectively. Each gold set has
1000 positive data points (i.e. AM-MWEs).
The gold sets are first compiled from multiple
resources, including Mitchell and Al-Hassan
(1994), Brustad (2000), Badawi et al. (2004) and
Moshref (2012). Second, each compiled gold set
is further evaluated by two expert annotators.
They are instructed to decide whether a given
ngram is an AM-MWE or not according to the
following definitions of AM-MWEs:
? They convey modality senses - Section 1
? They have unigram synonyms
? They have fixed word orders
? Their function words are fixed
Inter-annotator kappa ? scores for the BiSet,
TriSet and QuadSet are 0.93, 0.95 and 0.96,
respectively. Most disagreement is attributed to
the annotators' failure to find unigram synonyms.
The positive BiSet includes (1) phrasal verbs
such as ????? ?? ytmkn mn (he manages to),  ????
?? yEjz En (he fails to) and ???? ? yHlm be (he
longs for), (2) prepositional phrases such as ??
?????? mn Almmkn (it is possible that) and ??
??????? fy AlHqyqp (actually), (3) nominal phrases
such as ???? ?? Amly hw (my hope is to) and (4)
AM-MWEs subcategorizing for
complementizers such as ???? ??? ySrH bAn (he
declares that) and ???? ?? yErf An (he knows
that).
4 We use the k-means clustering implementation from
Orange toolkit http://orange.biolab.si/
118
The positive TriSet includes verb phrases like
???? ?? ?? yf$l fy An (he fails to) and prepositional
phrases like ?? ???????? ?? mn AlmstHyl An (it is
impossible to) and ???? ????? ??? Endy AymAn bAn
(I have faith that).
The positive QuadSet includes verb phrases
such as  ?? ???????? ????? yHdwny AlAml fy An
(hope drives me to) and prepositional phrases
such as ?? ??? ??????? ?? mn gyr Almqbwl An (it is
unacceptable to).
With these gold sets, we first decide on the
best cluster per ngram size. We use an all-or-
nothing approach; that is, for the two clusters
created for bigrams, we select the cluster with
the highest exact matches with the BiSet to be
the best bigram cluster. We do the same thing for
the trigram and quadrigram clusters. With
information about the best cluster per ngram
size, our actual evaluation starts.
To evaluate clustered bigram AM-MWEs, we
consider the output of best bigram, trigram and
quadrigram clusters to allow for evaluating
bigrams with gaps. We also tolerate
morphological differences in terms of different
conjugations for person, gender, number, tense,
mood and aspect.
For example, true positives for the bigram
AM-MWE ????? ?? ytmkn mn (he manages to)
include its exact match and the morphological
alternations of ????? ?? Atmkn mn (I manage to)
and ????? ?? ntmkn mn (we manage to), among
others. In other words, if the output of the bigram
clustering has Atmkn mn or ntmkn mn but the
BiSet has only ytmkn mn, we consider this as a
true positive.
The bigram ytmkn mn can have a (pro)noun
subject after the verb ytmkn: ytmkn ((pro)noun
gap) mn. Thus, we consider the output of the
trigram best cluster. If we find instances such as
????? ?????? ?? ytmkn Alt}ys mn (the president
manages to) or ????? ??? ?? ntmkn nHn mn (we
manages to), we consider them as true positives
for the bigram ytmkn mn as long as the trigram
has the two defining words of the bigram,
namely the verb ytmkn in any of its conjugations
and the preposition mn.
The same bigram - ytmkn mn - can have two
gaps after the head verb ytmkn as in  ????? ??????
?????? ?? ytmkn Alr}ys AlmSry mn (the
Egyptian president manages to). For that reason,
we consider the best quadrigram cluster. If we
find ytmkn ((pro)noun gap) ((pro)noun gap) mn,
we consider this as a true positive for  the bigram
ytmkn mn as long as the two boundaries of the
bigrams are represented. We could not go any
further with more than two gaps because we did
not cluster beyond quadrigrams.
False positives for the bigram ytmkn mn would
be the bigrams ????? ?????? ytmkn Alr}ys (the
president manages) and ?????? ?? Alr}ys mn (the
president to) in the bigram cluster where one of
the bigram's components - either the verb or the
preposition - is missing.
False negatives of bigrams would be those
bigrams that could not be found in any of the
best clusters whether with or without gaps.
Similar to evaluating bigrams, we consider the
output of the trigram and quadrigram best
clusters to evaluate trigram AM-MWEs. We also
tolerate morphological productivity.
For instance, the trigram ????? ????? ??? EndnA
AymAn bAn (we have faith that) conjugated for
the first person plural is a true positive for the
gold set trigram ???? ????? ??? Endy AymAn bAn
(I have faith that), that is conjugated for the first
person singular.
The same trigram Endy AymAn bAn can have
two types of gaps. The first can be a noun-based
indirect object after the preposition End. Thus,
we can have ??? ????? ????? ??? End AlnAs AymAn
bAn (people have faith that). The second can be
an adjective after the head noun AymAn. Thus we
can have ???? ????? ???? ??? Endy AymAn mTlq
bAn (I have a strong faith that).
Consequently, in the output of the quadrigram
best cluster, if we find matches to Endy AymAn
(adjective gap) bAn in any conjugations of Endy,
or if we find any matches for End (noun gap)
AymAn bAn, we consider them as true positives
for the trigram Endy AymAn bAn .
If the pronoun in End is replaced by a noun
and the adjective gap is filled, we will have a
pentagram like ??? ????? ????? ???? ??? End AlnAs
AymAn mTlq bAn (people have a strong faith
that). Since we do not extract pentagrams, we
consider chunks such as ??? ????? ????? End AlnAs
AymAn (people have faith) and ????? ???? ???
AymAn mTlq bAn (strong faith that) as false
positive trigrams. This is because the former
misses the complementizer ??? bAn (in that), and
the latter misses the first preposition ??? End
(literally: in; gloss: have).
119
Since we do not cluster pentagrams, we could
not tolerate gaps in the output of the
quadrigrams. We, however, tolerate
morphological variation. As a result,  ?????? ?????
?? ?? yHdwnA AlAml fy An (hope drives us to) is
considered as a true positive for ?????? ????? ?? ??
yHdwny AlAml fy An (hope derives me to).
It is important to note that we do not consider
the next best cluster of the larger AM-MWEs
unless we do not find any true positives in the
AM-MWE's original cluster. For example, we do
not search for bigrams' true positives in the
trigram and quadrigram clusters, unless there are
not any exact matches of the gold-set bigrams in
the bigrams' best cluster itself. The same thing
applies when evaluating trigram AM-MWEs.
4.2.3.2 Clustering Results and Error Analysis
Table 4 shows the evaluation results for bigrams,
trigrams and quadrigrams. We attribute the good
results to our evaluation methodology in the first
place because it allows counting true positives
across clusters of different ngram sizes to
account for gaps and tolerates morphological
variations. Our methodology captures the
morphological productivity of AM-MWEs which
is expected given that Arabic is morphologically-
rich. It also accounts for the syntactic
productivity in terms of insertion.
Precision Recall F1
Bigrams 0.663 0.776 0.715
Trigrams 0.811 0.756 0.783
Quadrigrams 0.857 0.717 0.780
Table 4: Clustering evaluation results
Long dependencies are a source of errors at the
recall level. Clustering could not capture such
instances as ??????? ?????? ???? ????? ??? SrH
Alr}ys AlmSry Hsny mbArk b (the Egyptian
president Hosni Mubarak declared to) because
they go beyond our quadrigram limit.
Another type of recall errors results from AM-
MWEs that do not meet the extraction frequency
threshold despite the large size of our corpus.
Our positive gold sets are sampled from
theoretical linguistics studies in which the
included illustrative examples are not necessarily
frequent. For example, we could not find
instances for the volitive ???? ??? ytwq Aly (he
longs for).
Precision errors result from the fact that our
RegExp-based procedure to guide the first
extraction stage is noisy. For instance, the
RegExp (\w*)t(\w*)w(\w*)q(\w*) that was
supposed to extract the volitive ???? ytwq (he
longs) did not return any instances for the
intended modal but rather instances for ?????
ytwqf (he stops) which interestingly
subcategorizes for a preposition and a
complementizer as in ????? ?? ?? ytwqf En An
(literally: stops from to). This subcategorization
frame is the same for modals such as ???? ?? ??
yEjz En An (literally: unable from to).
Consequently, ????? ?? ?? ytwqf En An (he stops
from to) has been clustered as a trigram AM-
MWE although it does not convey any modality
senses. This highlights another reason for
precision errors. The subcategorization frames
and hence the syntactic features used for
clustering are not always distinctive for AM-
MWEs.
The @beg feature was the least informative
among all features. In the case of bigrams, they
are mostly lexical verbs that do not occur in
sentence initial positions. Meanwhile,
punctuation inconsistencies do not enable us to
reliably mark @beg for many ngrams.
4.3 Identifying Variation Patterns
Our target is to build a lexicon and a repository
of the variation patterns for AM-MWEs to boost
their automatic identification and extraction,
given their morpho-syntactic and lexical
productivity.
In order to identify variation patterns, we use
as input the best clusters from the previous
clustering stage and follow these steps:
? We keep all function words as is with their
lexical and POS representations
? We collapse all morphological tags for
gender, number, person, tense, mood, aspect
and case
? We add a HEAD tag to the head words (i.e.
words whose roots were used for extraction)
? We add a GAP tag for adverbs, pronouns and
other gap fillers to explicitly mark gap
locations
An example pattern for the root  ?
 -
 ?
 -
? T-m-H
(wish) is  ((HEAD/*IV*) + (AlY/PREP) +
(An/SUB_CONJ)) which reads as follows: a
120
trigram AM-MWE whose head is a verb in any
conjugation followed by the preposition AlY (to)
and the subordinate conjunction An (that; to).
Another pattern that results from the
aforementioned steps for the same root of T-m-H
is ((HEAD/*IV*) + (ADV/GAP) + (AlY/PREP) +
(An/SUB_CONJ)). It means that an adverb can be
inserted in-between the HEAD and the preposition
AlY (to).
4.4 Bootstrapping AM-MWEs
We use the patterns identified in the previous
stage in two ways: first, to extract low-frequency
AM-MWEs whose HEADs have the same roots as
the pattern's HEAD; and second, to extract AM-
MWEs that have the same lexical, POS patterns
but are not necessarily derived from the modality
roots we used in extraction.
For example, from the previous section we
used ((HEAD/*IV*) + (AlY/PREP) +
(An/SUB_CONJ)) to extract the third person
feminine plural conjugation of the root T-m-H in
the trigram ??? ??? ???? yTmHn AlY An (they
wish for) that occurred only once in the corpus.
We used the same pattern to extract ???? ??? ??
ySbw AlY An (he longs for) that has the same
pattern but whose HEAD'S root S-b-b was not in
our list of modality roots.
Among the new extracted AM-MWEs are the
expressions ?? ?????? ?? mn AlmwADH An (it is
clear that) and ?? ??????? ?? mn AlTbyEy An (it is
normal that) that share the same pattern with  ??
?????? ?? mn Almmkn An (it is possible that). We
decide to consider those expressions as AM-
MWEs although they are not epistemic in the
conventional sense. That is, they do not evaluate
the truth value of their clause-based propositions,
but rather presuppose the proposition as true, and
express the speakers' sentiment towards it.
This bootstrapping stage results in 358 AM-
MWEs. They are inspected during manual
verification.
5 Manual Verification and Final Results
We manually verify the best clusters, the
bootstrapped AM-MWEs and the constructed
patterns before including them in the final
lexicon and repository to guarantee accuracy.
Besides, we manually add modality senses to the
lexicon entries. We also manually complete the
morphological paradigms of the morphologically
productive AM-MWEs. That is, if we only have
the bigram ???? ?? yrgb fy (he longs for)
conjugated for the third singular masculine
person, we manually add the rest of the
conjugations.
The final lexicon is represented in XML and is
organized by modality senses and then roots
within each sense. The lexicon comprises 10,664
entries. The XML fields describe: the Arabic
string, the size of the AM-MWE, the corpus
frequency and the pattern ID. The pattern ID is
the link between the lexicon and the repository
because it maps each lexicon entry to its lexical,
POS pattern in the repository.
Roots Senses Sizes
A-m-l 710 Epistemic 4233 Bigrams 4806
A-k-d 693 Evidential 811 Trigrams 3244
r-g-b 396 Obligative 748 Quadrigrams 2614
$-E-r 378 Permissive 755
H-s-s 370 Commissive 111
q-n-E 312 Abilitive 676
E-q-d 293 Volitive 3330
Total: 10,664
Table 5: Statistics for the AM-MWE lexicon for the
top 7 roots and the distributions of modality senses
and AM-MWE sizes
If a lexicon entry is manually added, the tag
MANUAL is used for the corpus frequency field.
Table 5 gives more statistics about the lexicon in
terms of modality senses, AM-MWE sizes and
the top 7 frequent modality roots.
The XML repository is given in the three  POS
tagsets supported by MADAMIRA. The XML
fields describe: the pattern's ID, the POS of the
head and the pattern itself with the HEADs and
GAPs marked. Appendices A and B give
snapshots of the lexicon and the repository in
Buckwalter's POS tagset.
6 Conclusion and Outlook
We described the unsupervised construction of a
lexicon and a repository of variation patterns for
AM-MWEs to boost their automatic
identification and extraction. In addition to the
creation of novel resources, our research gives
insights about the morphological, syntactic and
lexical properties of such expressions. We also
propose an evaluation methodology that accounts
for the productive insertion patterns of AM-
MWEs and their morphological variations.
For future work, we will work on larger AM-
MWEs to cover insertion patterns that we could
121
not cover in this paper. We will experiment with
different association measures such as point-wise
mutual information. We will also try different
clustering algorithms.
Acknowledgement
This work was supported by grant NPRP 09-410-
1-069 of the Qatar National Research Fund. We
would also like to thank the four anonymous
reviewers for their constructive comments.
References
Rania Al-Sabbagh and Roxana Girju. 2012. YADAC:
Yet another Dialectal Arabic Corpus. Proc. of
LREC'12, Istanbul, Turkey, May 23-25 2012
Rania Al-Sabbagh, Jana Diesner and Roxana Girju.
2013. Using the Semantic-Syntactic Interface for
Reliable Arabic Modality Annotation. Proc. of
IJCNLP'13, Nagoya, Japan, October 14-18 2013
Mohammed Attia, Antonio Toral, Lamia Tounsi,
Pavel Pecina and Josef van Genbith. 2010.
Automatic Extraction of Arabic Multiword
Expressions. Proc. of the Workshop on MWE 2010,
Beijing, August 2010
Elsaid Badawi, M.G. Carter and Adrian Gully. 2004.
Modern Written Arabic: A Comprehensive
Grammar. UK: MPG Books Ltd
Satanjeev Banerjee and Ted Pedersen. 2003. The
Design, Implementation, and Use of the Ngram
Statistic Package. Proc. of CiCling'03,  Mexico
City, USA
Ibrahim Bounhas and Yahya Slimani. 2009. A Hybrid
Approach for Arabic Multi-Word Term Extraction.
Proceedings of NLP-KE 2009, Dalian, China,
September 24-27 2009
Kristen E. Brustad. 2000. The Syntax of Spoken
Arabic: A Comparative Study of Moroccan,
Egyptian, Syrian and Kuwaiti Dialects.
Georgetown Uni. Press, Washington DC, USA
Tim Buckwalter. 2002. Arabic Morphological
Analyzer. Technical Report, Linguistic Data
Consortium, Philadelphia
Cristian Danescu-Niculescu-Mizil, Moritz Sudhof,
Dan Jurafsky, Jure Leskovec and Christopher Potts.
2013. A Computational Approach to Politeness
with Application to Social Factors. Proc. of the 51st
ACL, , Sofia, Bulgaria, August 4-9  2013
Mona Diab, Lori Levin, Teruko Mitamura, Owen
Rambow, Vinodkumar Prabhakaran, and Weiwei
Guo. 2009. Committed Belief Annotation and
Tagging. Proc. of the 3rd LAW Workshop, ACL-
IJCNLP'09, pp. 68-73, Singapore
Carla Parra Escart?n, Gyri Sm?rdal Losnegaard, Gunn
Inger Lyse Samdal and Pedro Pati?o Garc?a. 2013.
Representing Multiword Expressions in Lexical and
Terminological Resources: An Analysis for Natural
Language Processing Purposes. Proc. of eLex 2013,
pages 338-357, Tallinn, Estonia, October 17-19
2013
Abdelati Hawwari, Kfir Bar and Mona Diab. 2012.
Building an Arabic Multiword Expressions
Repository. Proc. of the 50th ACL, pages 24-29,
Jeju, Republic of Korea, July 12 2012
Andrew Lampert, Robert Dale and Cecile Paris. 2010,
Detecting Emails Containing Requests for Action.
Proc. of the 2010 ACL, pages 984-992, Los
Angeles, California, June 2010
F. Mitchell and S. A. Al-Hassan. 1994. Modality,
Mood and Aspect in Spoken Arabic with Special
Reference to Egypt and the Levant. London and
NY: Kegan Paul International
Ola Moshref. 2012. Corpus Study of Tense, Aspect,
and Modality in Diglossic Speech in Cairene
Arabic. PhD Thesis. University of Illinois at
Urbana-Champaign
Dragos Stefan Munteanu and Daniel Marcu. 2007. ISI
Arabic-English Automatically Extracted Parallel
Text, Linguistic Data Consortium, Philadelphia
Malvin Nissim and Andrea Zaninello. 2013. A
Repository of Variation Patterns for Multiword
Expressions. Proc. of the 9th Workshop of MWE,
pp. 101-105, Atlanta, Georgia, June 13-14 2013
Frank R. Palmer. 2001. Mood and Modality. 2nd
Edition. Cambridge University Press, Cambridge,
UK
Silvia Pareti, Tim O'Keefe, Ioannis Konstas, James R.
Curran and Irena Koprinska. 2013. Automatically
Detecting and Attributing Indirect Quotations.
Proc. of the 2013 EMNLP, pages. 989-1000,
Washington, USA, October 18-21 2013
Arfath Pasha, Mohamed Al-Badrashiny, Ahmed El
Kholy, Ramy Eskander, Mona Diab, Nizar Habash,
Manoj Pooleery, Owen Rambow and Ryan Roth.
2014. MADAMIRA: A Fast, Comprehensive Tool
for Morphological Analysis and Disambiguation of
Arabic. Proc. of the 9th International Conference
on Language Resources and Evaluation, Reykjavik,
Iceland, May 26-31 2014
Vinodkumar Prabhakaran and Owen Rambow. 2013.
Written Dialog and Social Power: Manifestations of
Different Types of Power in Dialog Behavior.
Proceedings of the 6th IJCNLP, pp. 216-224,
Nagoya, Japan, October 14-18  2013
Ivan Sag, Timothy Baldwin, Francis Bond, Ann
Copestake and Dan Flickinger. 2002. Multiword
Expressions: A Pain in the Neck for NLP.
Proceedings of CiCling 2002, pages 1-15, Mexico
City, Mexico
Michael Stubbs. 2007. An Example of Frequent
English Phraseology: Distributions, Structures and
122
Functions. Language and Computers: Corpus
Linguistics 25 Years on, pages 89-105, (17)
Appendix A: A snapshot of the XML lexicon
<lexicon name="AM-MWE Lexicon v1.0">
<modality sense="abilitive">
<head root="q-d-r">
<am-mwe string="???? ???"  len="2" freq="283" patternID="23"> </am-mwe>
<am-mwe string="???? ?????? ???" len="3" freq="7" patternID="45"> </am-mwe>
...
</head>
</modality>
<modality sense="epistemic">
<head root="g-l-b">
<am-mwe string="?? ??????" len="2" freq="122" patternID="15"> </am-mwe>
...
</head>
<head root="H-w-l">
<am-mwe string="?????? ??" len="2" freq="70" patternID="10"> </am-mwe>
...
</head>
<head root="n-Z-r">
<am-mwe string="?? ??????? ???? ?? " len="4" freq="38" patternID="50"> </am-mwe>
...
</head>
</modality>
</lexicon>
Appendix B: A snapshot of the XML repository
<repository name="AM-MWE Variation Patterns v1.0">
<tagset name="Buckwalter" pos-tagger="MADAMIRA v1.0">
...
<pattern ID="10" head-pos="*+IV+*" pos="(HEAD)+ (An/SUB_CONJ)"></pattern>
...
<pattern ID="15" head-pos="DET+NOUN+*" pos="(fy/PREP)+(HEAD)"></pattern>
...
<pattern ID="23" head-pos="ADJ+*" pos="(HEAD)+(ElY/PREP)"> </pattern>
...
<pattern ID="45" head-pos="DET+NOUN+*" pos="(lyd/NOUN)+(PRON*/GAP)*+(HEAD)+(ElY/PREP)">
</pattern>
...
<pattern ID="50" head-pos="DET+NOUN+*" pos="(mn/PREP)+(HEAD)+(ADV/GAP)*+(An/SUB_CONJ)">
</pattern>
....
</tagset>
</repository>
123
LAW VIII - The 8th Linguistic Annotation Workshop, pages 139?148,
Dublin, Ireland, August 23-24 2014.
Interactive Annotation for Event Modality in Modern Standard 
and Egyptian Arabic Tweets 
 
Rania Al-Sabbagh?, Roxana Girju?, Jana Diesner? 
?Department of Linguistics and Beckman Institute  
?School of Library and Information Science  
University of Illinois at Urbana-Champaign, USA 
{alsabba1, girju, jdiesner} @illinois.edu 
 
Abstract 
We present an interactive procedure to annotate a large-scale corpus of Modern Standard and 
Egyptian Arabic tweets for event modality that comprises obligation, permission, commitment, 
ability, and volition. The procedure splits up the annotation process into a series of simplified 
questions, dispenses with the requirement of expert linguistic knowledge, and captures nested 
modality triggers and their attributes semi-automatically.  
1  Introduction 
Event modality, according to Palmer (2001), describes events that are not actualized but are 
merely potential. It comprises obligation, permission, commitment, ability, and volition. Both 
obligation and permission emanate from an external authority such as the law; whereas 
commitments are the obligations placed by speakers on themselves as in promises. Ability is the 
(in)capacity to do something. Volition is broadly defined as intensions, desires, wishes, and 
preferences. Event modality is used for several NLP tasks, including sales and marketing 
analysis (Ramanand et al. 2010, Carlos and Yalamanchi 2012), sentiment analysis (Chardon et 
al. 2013), the automatic detection of request emails (Lampert et al. 2010), and the classification 
of animacy and writers' emotions (Liao and Liao 2009, Bowman and Chopra 2012). 
To-date, there are no large-scale Arabic corpora annotated for event modality compared to 
English (Baker et al. 2010, 2012; Rubinstein et al. 2013), Japanese (Matsuyoshi et al. 2010), 
Portuguese (Hendrickx et al. 2012), and Chinese (Cui and Chi 2013). One obstacle for the 
creation of modality-annotated corpora is the lack of consensus definitions of modality and its 
attributes to be rendered into annotation tasks and guidelines. Furthermore, most modality 
annotation schemes use sophisticated theoretical guidelines that need annotators with linguistic 
background; hence, annotation typically takes place in in-lab settings at small scales.  
In this paper, we present an interactive annotation procedure to annotate event modality and 
its attributes of sense, polarity, intensification, tense, holders, and scopes in Modern Standard 
and Egyptian Arabic tweets. The procedure depicts the following ideas: first, it defines each 
annotation task as a series of questions displayed1/hidden based on prior answers; second, it 
avoids lengthy theoretically-sophisticated definitions and uses the questions instead as 
simplified self-explanatory annotation prompts; and third, based on the elicited answers it 
automatically determines nested triggers and their attributes. The fact that our procedure does 
not require special linguistic background and consists of easy-to-administer questions makes it 
eligible for large-scale crowdsourcing annotation.  
Our corpus comprises 9949 unique tweets, annotated for 12134 tokens that map to 315 unique 
types of event modality triggers and their attributes of sense, polarity, intensification, tense, 
holders, and scopes. The reason to work on the genre of tweets is that our corpus is part of a 
larger project to incorporate linguistic features, such as modality, with network-based features 
to automatically identify the key players of political discourse on Twitter for countries with 
fast-changing politics such as Egypt. The fact that our corpus is harvested from the Arabic 
Egyptian Twitter entails that the corpus is diglossic for Modern Standard Arabic (MSA), the 
                                                            
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and 
proceedings footer are added by the organizers. License details: http:// creativecommons.org/licenses/by/4.0/ 
139
formal Arabic variety, and Egyptian Arabic (EA), the native Arabic dialect of Egypt. We 
evaluate the annotation results with Krippendorff's alpha (Krippendorff 2011). Results show 
high inter-annotator reliability rates, indicating that our annotation scheme and procedure are 
effective. The contribution of this paper, therefore, is twofold: first, we create a novel annotated 
resource for Arabic NLP that is larger than existing corpora even for languages other than 
Arabic; and second, we present an efficient and easy-to-administer annotation procedure with 
interactive crowdsourcing potentials.  
The rest of this paper is organized as follows: Section 2 outlines the annotation scheme, 
guidelines and the interactive procedure; Section 3 gives examples for the final output 
representations; Section 4 describes corpus harvesting and sampling; Section 5 provides the 
annotation results and disagreement analysis; and Section 6 compares and contrasts our work 
with related work. 
2  Annotation Scheme: Tasks and Guidelines 
Our annotation scheme comprises six tasks to label sense, polarity, intensification, tense, 
holders, and scopes for each event modality. Prior to the beginning of the interactive procedure, 
we highlight all event modalities in each tweet using a string-match algorithm and the lexicons 
from Al-Sabbagh et al. (2013, 2014a). The algorithm finds all potential event modality triggers 
(i.e. words/phrases that convey event modality) within each tweet in our corpus and marks them 
as annotation units. A total of 12134 candidate triggers are highlighted in 9949 tweets.  
2.1 Task 1: Sense 
Sense annotation is to decide for each candidate trigger in context whether it actually conveys 
event modality given the tweet's context. The same present participle ???? HAbb in example 1 is 
a volition trigger meaning I want/desire; whereas in example 2 it is a non-modal present 
participle meaning like/prefer/respect. 
1. ???? ???? ????[ ???? ?? ??? ????[ 1 
TbEA >nA m$ HAbb [Emrw mwsY yksb] 
Definitely, I do not want [Amr Moussa to win]. 
2. ??? ????  ????????? ???????? ?? : ???? ????  #egypt #qalyoum 
Emrw >dyb: rsmyA AlktAtny m$ HAbb >bw HAmd #egypt #qalyoum 
Amr Adeeb: Alkatatny does not officially like Abu Hamed #egypt #qalyoum 
We define sense annotation as a synonymy judgment task, following Al-Sabbagh et al. (2013, 
2014b). Each event modality sense is represented by an exemplar set manually selected so that: 
(1) each exemplar is an unambiguous event modality trigger; (2) exemplars are in both MSA 
and EA; (3) exemplars comprise both simple words and multiword expressions; (4) exemplars 
are both affirmative and negative; and (5) exemplars are of different intensities. Presented with 
a pre-highlighted candidate trigger in context and the exemplar sets, annotations are to decide 
whether the candidate trigger is synonymous with any of the exemplar sets. If not, the trigger is 
then assumed as non-modal.   
If an annotator decides that a given candidate trigger is a non-modal, no further questions 
about polarity, intensification, tense, holders, or scopes are displayed. In order to guarantee that 
annotators do not select the non-modal option as an easy escape, they are not allowed to move 
forward without giving at least one synonym of their own to the candidate trigger.   
2.2 Task 2: Polarity 
Task 2 uses as input the candidates labeled as valid event modality triggers in Task 1 and label 
each as either affirmative (AFF) or negative (NEG). To decide, annotators are instructed to 
consider the absence/presence of: 
? Negation particles such as ?? m$ (not), ?? lA (not), and ??? gyr (not), among others. 
? Negation affixes, especially in EA, like the circumfix m...$ in ????? mqdr$ (I cannot).  
                                                            
1 Throughout the examples, modality triggers are marked in boldface, and scopes are in-between brackets. 
140
? Negative polarity items like ???? Emry (never) and ?? ??? lm yEd (no longer). 
? Negative auxiliaries where negation is placed on the past tense auxiliary as in ????? ???? 
mknt$ EAyz (I did not want).  
? Inherently-negative triggers that encode negation in their lexical meanings such as ???? 
EAjz (incapable) and ???? ymnE (prohibit). 
? Embedding under negated epistemic modality triggers as in ?? ????? ??? ??? lA >Etqd >nh 
yjb (I do not think it is necessary) which entails that the speaker is not actually setting 
an obligation.  
Annotators are instructed that using multiple negation markers results in an affirmative sense. 
Thus, ?? ???? lm yEjz (he was not unable to) means that he was actually able to. Annotators are 
required to give the reason for negation if they decide that a given trigger is negative. 
2.3 Task 3: Intensification 
Event modality triggers have different lexical intensities (i.e. intensities encoded in the lexical 
meaning of the word/phrase regardless of the context). In obligation triggers, for instance, even 
without a context, Arabic speakers know that ????? Drwry (necessary) expresses a higher 
necessity than ??????? AlmfrwD (should). When used in context, the trigger's lexical intensity 
can be maintained as is, or amplified/mitigated by such linguistic means as: 
? Modification: adverbs like ????? tmAmA (absolutely) amplify lexical intensity; whereas 
mitigation is invoked by such adverbs as ????? gAlbA (most probably).  
? Categorical negation typically amplifies lexical intensity as in ?? ??????? ???? m$ 
AlmfrwD >bdA (it should never be).  
? Emphatic expressions such as ?? qd (indeed), ????? wAllh (I swear), and ?? ?? ???? mn kl 
qlby (wholeheartedly), among others, lead to lexical intensity amplification.  
? Coordination of two or more triggers typically results in intensity amplification as in 
???? ?????? lAzm wDrwry (must and necessary). 
? Embedding under epistemic modality triggers can affect the lexical intensities of event 
modality triggers. In ????? ?? ??????? ?? >Etqd mn AlDrwry >n (I think it is necessary 
to) the strong obligation associated with ??????? AlDrwry (necessary) is mitigated by 
the moderate-intensity epistemic ????? >Etqd (I think), being embedded under it.  
The annotators' task for intensification annotation is to decide for each candidate labeled as a 
valid event modality trigger in Task 1 whether its lexical intensity is amplified (AMP), mitigated 
(MTG) or maintained (AS IS). During interactive annotation, annotators are asked to provide the 
reason for their selection; that is, whether the lexical intensity is affected by modification, 
coordination, negation, embedding or any other reason whether listed above or not. 
2.4 Task 4: Tense 
In this version of our event modality corpus, we work on the present and past tenses only. Thus, 
Task 4 is to decide for each valid event modality trigger from Task 1 whether it is present (PRS) 
or past (PST). Annotators are required to give their reasons for selecting either PRS or PST. 
2.5 Task 5: Holders  
Holder annotation identifies the source of the obligation, permission, commitment, ability, or 
volition. In example 3, the source that sets the obligation that Egyptians have to learn the 
meaning of democracy is the Twitter user. 
3. ???????? ??????? ???? ??? ?????????? ?????[ ????[  
lAzm [AlmSryyn ytElmwA yEny <yh dymwqrATyp Al>wl] 
[Egyptians have to learn what democracy is first] 
The holder is not always the Twitter user, however. In example 4, the Twitter user quotes 
Kamal Alganzoury - a former Egyptian Prime Minster - stating that he does not want to 
141
continue as the Prime Minister. Therefore, the holder of the negated volition trigger ??? ??? ???? 
lys ldy rgbp (not have a will) is Alganzoury not the Twitter user. This is an example of the 
nested holder notion first proposed by Wiebe et al. (2005) and Saur? and Pustejovsky (2009).  
4. ?????????[ ?? ??? ???? ???: ??????? ???? ????????[  #SCAF #Tahrir #Egypt 
Aldktwr kmAl Aljnzwry: lys ldy rgbp fy [AlAstmrAr] #SCAF #Tahrir #Egypt 
Dr. Kamal Alganzoury: I do not wish to [continue] #SCAF #Tahrir #Egypt 
Another example of nested holders is example 5. We know that the regime is incapable of 
maintaining security and protecting the people only because the Twitter user says so. Put 
differently, the best way to understand this tweet is that according to what the Twitter user holds 
as a true proposition, the regime is unable to maintain security and protect the people.  
5.  ????? ????? ?? ????? ?????????[ ??? ?????????? ???[  
AlnZAm gyr qAdr ElY [twfyr Al>mn >w HmAyp AlmwATnyn] 
The regime is not able to [maintain security and protect the people] 
We can have two or more nested holders. In example 4, the two holders are Alganzoury who 
expresses his unwillingness to continue as a Prime Minster and the Twitter user who is quoting 
Alganzoury. In example 5, the two holders are the regime that is incapable of marinating 
security and protecting its people and the Twitter user who holds this proposition as true. In 
example 6, we have three nested holders: the Iranians who are unwilling to confront the outside 
world, Obama who holds that as a true proposition about Iranians, and the Twitter user who is 
quoting Obama stating his proposition.  
6. ??? ??????????????? ?? ???[ ???? ?? ?? ??? ????? ????????: ??????[  
AwbAmA: Al$Eb AlAyrAny lm yEd yrgb fy [AlmwAjhp mE AlEAlm AlxArjy] 
Obama: the Iranians no longer want to [confront the other countries].  
During the interactive procedure, annotators are first asked whether the holder is the same as 
the Twitter user. If not, more questions are displayed to determine (1) who the real holder is; (2) 
whether the tweet is a(n) (in)direct quote; or it conveys the Twitter user's assumptions. 
When the holder is not the Twitter user, annotators are asked to mark the boundaries of the 
linguistic unit that corresponds to the holder in the tweet's text. Annotators are instructed to use 
the maximal length principle from Szarvas et al. (2008) so that they mark the largest possible 
meaningful linguistic unit. Thus, in example 4 the holder is ??????? ???? ???????? Aldktwr kmAl 
Aljnzwry (Dr. Kamal Alganzoury) not only Kamal Alganzoury.  
2.6 Task 6: Scopes  
Scopes are the events modified by the trigger, syntactically realized as clauses, verb phrases, 
deverbal nouns or to-infinitives, according to Al-Sabbagh et al. (2013). We use the same 
maximal length principle from Task 5 so that the marked scope segment corresponds to the 
largest meaningful linguistic unit that describes the event. Typically, scope segments are 
delimited by: (1) punctuation markers and (2) subordinate conjunctions. 
Annotators are instructed that: (1) a single trigger may have one or more scopes; (2) two or 
more triggers - especially conjoined by coordinating particles - can share the same scope; and 
(3) scopes are not necessarily adjacent to their triggers. Examples 7, 8 and 9 illustrate each of 
these guidelines, repecetively.  
7.  ?????? ????? ???????[?] ?????[ ???????? ?????? ????[  
lw AstbEd $fyq ystTyE [AlTEn] w[AlEwdp lsbAq Alr}Asp] 
If Shafiq is excluded, he can [appeal] and [run again for presidency].  
8.  [?????? ???????? ???? ??? ??? ???? ????? ?????? ???? [???? ??? ?? ??????? 
mlAyyn AlmSryyn Ally brh mSr lAzm wHtmA wDrwry wyjb [ybqY lhm Hq AltSwyt] 
It is necessary, it is a must, it is a need that [Egyptians abroad are given the right to vote]. 
9. [???? ????? ??? ??? ?? ???? [???? #??? ???? ????? ??? ??????? 
nfsy wAllh bjd qbl mA Amwt [A$wf #mSr AHsn wAHlY bld fAldnyA] 
I really wish before I die to [see #Egypt becoming one of the best counties in the world]. 
3 Final Output Representation   
All elicited answers during annotation are organized into the representations illustrated in the 
following examples. The representation of example 10 reads as: the Twitter USER strongly did 
142
not want Shafiq to win the presidential elections. The trigger ?????? Atmnyt (wished) is tagged as 
synonymous with the volition exemplar set; therefore, it denotes a DESIRE. It is then labeled as a 
past tense (PST), negative (NEG) trigger. Furthermore, its lexical intensity is labeled as amplified 
(AMP) because of the categorical negation ???? ?? Emry mA (never ever). Originally, ?????? 
Atmnyt (wished) is of moderate lexical intensity, being less intense than ?????? A$thyt (longed 
for) but more intense than ???? >rdt (wanted). Given the categorical negation, the lexical 
intensity of ?????? Atmnyt (wished) goes up the scale from moderate to strong (STRG). 
10. ????#????? ???? ???? ??????? ?? ???? . ]???? ????[??  ????????? ??  
Emry mAtmnyt An [$fyq yksb]. AlHmd Allh rbnA mHrmny$ mn HAjp #mrsy 
I have never ever wished [Shafiq to win]. Thank God! #Morsi. 
rep. USER, STRG PST NEG DESIRE ($fyq yksb)  
Example 11 reads as: the Twitter USER reports Hegazy stating that he has the ability to 
become the Muslim's caliphate. The trigger ???? >SlH (can) is labeled as synonymous with the 
ability exemplar set. It is also labeled as a present (PRS), affirmative (AFF) trigger whose lexical 
intensity is maintained (AS IS) in the context. Therefore, its lexical intensity is maintained to its 
original level which is moderate (MOD). 
11.  ?????? ???? ?????? ]??? ?????? ?????????[??  ??????? : ?????  #Ikhwan 
HjAzy: >nA >SlH >n [>kwn xlyfp llmslmyn] wsnkwn sAdp AlEAlm #Ikhwan 
Hegazy: I can [be the Muslims' caliphate] and we will become the world's masters. #Ikhwan 
rep. USER, report, (HjAzy, MOD PRS AFF ABLE, (>kwn xlyfp llmslmyn)) 
Example 12 shows a Twitter user who holds as true that the only thing Egypt needed was a 
wise politician to avoid the bloodshed. The trigger ????? tHtAj (needs) is labeled as an obligation 
trigger synonymous with ????? ttTlb (requires). It is also labeled as past tense (PST) given the 
preceding past tense auxiliary ??? tkn (was). The assigned strong (STRG) lexical intensity label is 
attributed to the fact that the original moderate intensity of ????? tHtAj (needs) is amplified by 
the categorical negation structure  ???... ??  lm ... <lA (nothing but). 
12. # ????????? ???? ???? ?? ??????? ???? ????? [???  ???????? ?? ???[  
#mSr lm tkn tHtAj AlA [rjl EAql yxrj mn AlAzmAt bdwn ArAqp AldmA'] 
#Egypt needed nothing but [a rational politician who solves crises without bloodshed] 
rep. USER, true, (mSr , STRG PST AFF REQUIRE (rjl EAql yxrj mn AlAzmAt bdwn ArAqp AldmA')) 
Example 13 illustrates the representation of three-level nested holders. It reads as: the USER 
reports Obama's assumption as the latter holds as true that the Iranians do not want to confront 
other countries.  
13. ???????? ?? ?????? ???????[ ???? ???? ??? ???????? ????? : ??????[  
AwbAmA: Al$Eb AlAyrAny lm yEd yrgb fy [AlmwAjhp mE AlEAlm AlxArjy] 
Obama: the Iranians no longer want to [confront other countries].  
rep. USER, report, (AwbAmA, true, (Al$Eb AlAyrAny, MOD PRS NEG DESIRE, (AlmwAjhp mE 
AlEAlm AlxArjy))) 
Example 14 shows how two conjoined triggers (i.e. ???? lAzm (must) and ????? Drwry 
(necessary)) that share the same holder and scope are merged into one representation, and the 
conjunction leads to amplifying the intensity of the obligation set by them both. 
14.  ???? ???? ???? ??? ???????? ?????? ???? ??????[ ??????????[  
lAzm wDrwry [klnA nkwn qdAm mqr AlmHAkmp wmEAnA Swrp Alr}ys]  
We must and it is necessary that [we go to the court with President's pictures]. 
rep. USER, STRG PRS AFF REQUIRE, (klnA nkwn qdAm mqr AlmHAkmp wmEAnA Swrp Alr}ys)  
4  Corpus Harvesting  
Tweets are harvested from the Arabic Egyptian Twitter provided that (1) each tweet has at least 
one trendy political English or Arabic hashtag; and (2) each tweet has at least one candidate 
event modality trigger from the Arabic modality lexicons (Al-Sabbagh et al. 2013, 2014a). We 
harvest tweets from a variety of users such as newspapers, TV stations, political and 
humanitarian campaigns, politicians, celebrities, and ordinary people. Thus, our corpus 
comprises both MSA, the formal Arabic variety, and EA, the native Arabic dialect of Egypt.  
The harvested corpus comprises 9949 unique tweets, with 12134 tokens of event modality 
triggers that map to 315 unique types. 
143
5 Annotation Results  
5.1 Evaluation Methodology and Metrics  
Our annotation tasks are of two types: (1) Tasks 1-4 are label-based where there is a pre-defined 
set of labels from which annotators choose; and (2) Tasks 5-6 are segmentation-based where the 
output of the annotation is a text segment. For the segmentation-based tasks, we use an all-or-
nothing method to measure inter-annotator reliability: for segments to be considered as 
agreement, they must share both the beginning and end boundaries. We use Krippendorff's 
alpha ? (Krippendorff 2011) as our inter-annotator reliability measure, following the most 
recent work on modality annotation for other languages including English (Rubinstein et al. 
2013) and Chinese (Cui and Chi 2013). For more details on Krippendorff's alpha and a, we refer 
the reader to Artstein and Poesio (2008).   
5.2 Results 
We use the surveygizmo survey services2 to implement our interactive annotation procedure 
given that their survey structure is one that uses conditional branching and skip logic. We 
distribute the survey on Twitter and we have three annotators participating. According to the 
short qualifying quiz given at the beginning of the survey, all three participants are native 
Egyptian Arabic (EA) speakers who have at least two-year experience with Twitter. They are 
also university graduates who, therefore, master MSA. None of the participants has a linguistics 
background. Table 1 shows alpha rates for each annotation task. 
 Sense Polarity Intensification Tense Holder Scope 
Obligation 0.890 0.893 0.892 0.978 0.829 0.744 
Permission 0.864 0.905 0.821 0.983 0.800 0.739 
Commitment 0.760 0.794 0.783 0.947 0.702 0.654 
Ability 0.895 0.914 0.905 0.950 0.828 0.763 
Volition 0.921 0.921 0.867 0.982 0.858 0.779 
Averages 0.866 0.885 0.854 0.968 0.803 0.736 
Table 1: Krippendorff's alpha rates for inter-annotator reliability 
5.3 Discussion and Disagreement Analysis   
Among the factors that lead to high inter-annotator reliability are that: (1) the vast majority of 
negation is explicitly marked by negation particles that are easy to detect by human annotators; 
(2) the vast majority of triggers are used without any amplification or mitigation markers; and 
(3) punctuation markers are surprisingly informative for marking scope boundaries and direct 
quotations; and hence, holders. 
Sense-related disagreement is attributed to: (1) nominal triggers, (2) highly-polysemous 
triggers, and (3) different interpretations invoked by the ?RATIONAL (i.e. non-human) holders. 
Typically, event modality triggers are adjunct constituents that add an extra-layer of meaning 
and can be removed without disturbing the syntactic structure. Yet, in example 15, ???? wAjb (a 
must) and ???? >wjb (a more important must) have main grammatical functions as the 
predicates of the phrases they modify. Most of the exemplars from Section 2.1 are adjuncts; 
and, thus, none can substitute ???? wAjb (a must) or ???? >wjb (a more important must) in such 
a context.   
15. ]???? ]?????? ??? ?????[???  ???? ]?????? ?? ?????? ??????  
[AltHfZ mn AxtTAf Alvwrp] wAjb lkn [AltwHd xlf m$rwE] >wjb 
[Being cautious about manipulating the revolution] is a must but [getting united for one project] 
is a more important must.   
Highly-polysemous triggers invoke disagreement because in many cases even the context is 
ambiguous. In example 16, ???? >qsm (I swear) has two eligible interpretations: an epistemic 
trigger interpretation I assure (you) that and a commitment trigger interpretation I promise (you) 
                                                            
2 http://www.surveygizmo.com/ 
144
that. Even the context is not enough to disambiguate the two interpretations and annotators go 
by the most common sense for the trigger according to their own opinions.  
16. ????: ???? ????  T??? ????? ????? ??? ????? 90? ???? ??? ]???#?? ???? [??  
Emrw >dyb: >qsm bAllh [ln tsqT #mSr], AHnA $Eb 90 mlywn wm$ <$Arp htsqT bld 
Amr Adeeb: I promise/assure (you) by God that [#Egypt will not collapse]. We are 90 million 
Egyptians and we will not be defeated by a sign.   
Non-human or ?RATIONAL holders invoke disagreement, especially for obligation versus 
volition triggers.  The most common sense of such triggers as ????? EAyzp (want) is volition. 
Yet, when the holder is ?RATIONAL like ?????????? AlAntxAbAt (the elections) in example 17, 
annotators disagree as to whether ????? EAyzp means want (i.e. a volition trigger) or need (i.e. an 
obligation trigger). 
17.  ????? ??????? ???? ???????? ]??????[ ???????????????  
AlAntxAbAt EAyzp [mr$Hyn] wHmlAt Al>HzAb tyjy brAHthA 
Elections want/need [candidates] and later we can establish the political parties.  
Intensity-related disagreement is attributed mostly to progressive verb aspect. Some 
annotators consider progressive verb aspect as indicated by the EA prefix b as a marker for 
lexical intensity amplification. Thus they tag the volition trigger ????? btmnY (I wish) in example 
18 as amplified, especially it is modified by ?? ??? kl ywm (everyday).   
18.  ????#???? ??? [ ??????? ???[  
kl ywm btmnY [sqwT Hkm #mrsy]  
Every day, I wish for [#Morsi's regime to fall].  
Polarity-related disagreement is mainly caused by (1) negated holders and (2) contextual 
negation. In ???? ?? ???? mfy$ Hd yqdr (no one can), annotators disagree as to whether ???? yqdr 
(can) should be labeled as affirmative or negative. By contextual negation we mean examples 
like ?? ????? ?? ????? ?? mn AlSEb >n ntmnY >n (it is hard to wish to), which entails negation 
due to the adjective ????? AlSEb (hard).  
Holder-related disagreement is attributed mainly to generic nouns and impersonal pronouns 
like ????? Al$Eb (the people) and ?????? AlwAHd (one), respectively. They are interpreted by 
some annotators as referring implicitly to the Twitter USER. Therefore, the annotators select the 
USER as the only holder with zero nesting. Other annotators interpret them as referring to people 
in general not necessarily the Twitter USER and thus they consider these as instances of nested 
holders.  
Scope-related disagreement is attributed to (1) ambiguous subordinate conjunctions, (2) 
triggers' modifiers, and (3) absent punctuation markers.  
Tense yields almost perfect inter-annotator reliability rates. Annotation disagreement does not 
show any particular pattern. Therefore, we attribute minor disagreement to random errors, 
resulting from fatigue.  
5.4 Majority Statistics  
Based on majority annotations, Table 2 gives the statistics for our corpus in terms of sense, 
polarity, intensification, and tense. As for holder annotations, approximately 60.5% of the 
triggers have zero-nested holders (i.e. the tweet's writer is the same as the holder).  
 Sense Polarity Intensification Tense 
 MD NMD AFF NEG AMP MTG ASIS PRS PST 
Ability 1729 920 1047 682 348 308 1073 1175 554 
Commitment 1048 495 599 449 221 220 607 639 409 
Obligation 1786 848 1059 727 369 399 1018 1018 768 
Permission 1699 980 1054 645 286 428 985 1053 646 
Volition 1622 1007 974 648 341 292 989 1038 584 
Totals 7884 4250 4733 3151 1565 1647 4672 4923 2961 
Table 2: Token statistics for each annotation task per event modality sense where MD is modal, NMD is 
non-modal, AFF is affirmative, NEG is negative, AMP is amplified, MTG is mitigated, ASIS is as is, PRS is 
present, and PST is past 
 
145
6 Related Work 
Event modality is the focus of many annotation projects. Matsuyoshi et al. (2010) annotate a 
corpus of English and Japanese blog posts for a number of modality senses including volition, 
wishes, and permission. They annotate sense, tense, polarity, holders as well as other attributes 
that we have not covered in our scheme such as grammatical mood. They report macro kappa 
inter-annotator agreement rates of 0.69, 0.70, 0.66 and 0.72 for holders, tense, sense, and 
polarity, respectively.  
Baker et al. (2010, 2012) simultaneously annotate modality and modality-based negation for 
Urdu-English machine translation systems. Among the modality senses they work on are 
requirement, permission, success, intention, ability, and desires. They report macro kappa inter-
annotator agreement rates of 0.82 for sense annotation and 0.76 for scopes. They, however, do 
not annotate holders and do not consider nested modalities.  
Hendrickx et al. (2012) annotate eleven modality senses in Portuguese, including necessity, 
capacity, permission, obligation, and volition, among others. They report a macro kappa inter-
annotator rate of 0.85 for sense annotation. 
Rubinstein et al. (2013) propose a linguistically-motivated annotation scheme for modalities 
in the MPQA English corpus. They annotate sense, polarity, holders, and scopes, among other 
annotation units. They work on obligation, ability, and volition among other modality senses. 
They attain macro alpha inter-annotator reliability rates of 0.89 and 0.65 for sense and scope, 
respectively.  
Cui and Chi (2013) apply the same scheme of Rubinstein et al. (2013) to the Chinese Penn 
Treebank and get alpha inter-annotator reliability rates of 0.81 and 0.39 for sense and scope 
annotation, respectively.  
Finally, Al-Sabbagh et al. (2013) annotate event modality in MSA and EA tweets. We attain 
kappa inter-annotator agreement rates of 0.90 and 0.93 for sense and scope annotation, 
respectively, for only 772 tokens of event modality triggers.  
Our annotation results, therefore, are comparable to the results in the literature. Furthermore, 
our annotation scheme and its tasks are orthogonal to most of the aforementioned schemes. 
However, the key differences between our work and related work are:  
? We use a standardized taxonomy of event modality - Palmer's (2001) - that has been 
proved valid for a variety of languages, including Arabic, according to Mitchell and 
Al-Hassan (1994), Brustad (2000), and Moshref (2012). 
? We annotate nested holders unlike some of the aforementioned studies  (e.g. Baker et 
al. 2010, 2012) and use a wider range of negation and intensification markers.  
? We use crowdsourcing with simplified guidelines implemented interactively to 
annotate a larger-scale corpus of 12134 tokens for event modality and its attributes.  
7 Conclusion and Outlook 
We presented a large-scale corpus annotated for event modality in MSA and EA tweets. We use 
a simplified annotation procedure that defines each annotation task as a series of questions, 
implemented interactively. Our scheme covers a wide range of the most common annotation 
units mentioned in the literature, including modality sense, polarity, intensification, tense, 
holders, and scopes. We deal with nested holders - which are crucial in a highly interactive 
genre such as tweets where users frequently quote others and make assumptions about them. 
We also automatically merge triggers with shared holders and scopes based on elicited 
annotators' answers. The annotation procedure yields reliable results and creates a novel 
resources for Arabic NLP. The current version of our corpus does not, however, cover a number 
of issues including: the future tense, grammatical moods other than the declarative, and 
modality entailment. By modality entailment, we mean, for example, when a tweet's user 
criticizes the obligation of another quoted person, this entails that the user does not consider 
such an event as required. For a future version of the corpus, we plan to cover such points. 
Furthermore, we will use the corpus to train and test a machine learning system for the 
automatic processing of Arabic event modality.  
146
References 
Rania Al-Sabbagh, Jana Diesner and Roxana Girju. 2013. Using the Semantic-Syntactic Interface for 
Reliable Arabic Modality Annotation. In Proceedings of IJCNLP'13, pages 410-418, October 14-18, 
2013, Nagoya, Japan.  
Rania Al-Sabbagh, Roxana Girju and Jana Diesner. 2014a. Unsupervised Construction of a Lexicon and a 
Pattern Repository of Arabic Modal Multiword Expressions. In Proceedings of the 10th Workshop of 
Multiword Expressions at EACL'14, April 26-27, 2014, Gothenburg, Sweden. 
Rania Al-Sabbagh, Roxana Girju and Jana Diesner. 2014b. 3arif: A Corpus of Modern Standard and 
Egyptian Arabic Tweets Annotated for Epistemic Modality Using Interactive Crowdsourcing. In 
Proceedings of the 25th International Conference on Computational Linguistics, August 23-29, 2014, 
Dublin, Ireland. 
Ron Artstein and Massimo Poesio. 2008. Inter-Coder Agreement for Computational Linguistics. 
Computational Linguistics, volume 34, issue 4, pages 555-596. 
Kathrin Baker, Michael Bloodgood, Mona Diab, Bonnie Dorr, Nathaniel W. Filardo, Lori Levin and 
Christine Piatko. 2010. A Modality Lexicon and its Use in Automatic Tagging. In Proceedings of the 
7th International Conference on Language Resources and Evaluation (LREC'10), pages 1402-1405, 
May 19-21, 2010, Valetta, Malta.   
Kathryn Baker, Michael Bloodgood, Bonnie J. Dorr, Chris Callison-Burch, Nathaniel W. Filardo, 
Christine Piatko, Lori Levin and Scott Miller. 2012. Modality and Negation in SIMT. Computational 
Linguistics. volume 38, issue 2, pages 411-438. 
Samuel R. Bowman and Harshit Chopra. 2012. Automatic Animacy Classification. In Proceedings of the 
NAACL HTL 2012 Student Research Workshop, pages 7-10, June 3-8, 2012, Montreal, Canada.  
Kristen E. Brustad. 2000. The Syntax of Spoken Arabic: A Comparative Study of Moroccan, Egyptian, 
Syrian and Kuwaiti Dialects. Georgetown University Press, Washington DC, USA. 
Cohan Sujay Carlos and Madulika Yalamanchi. 2012. Intention Analysis for Sales, Marketing and 
Customer Service. In Processing of COLING 2012: Demonstration Papers, pages 33-40, December 
2012, Mumbai, India. 
Baptiste Chardon, Farah Benamara, Yannick Mathieu, Vladimir Popescu and Nicholas Asher. 2013. 
Sentiment Composition Using a Parabolic Model. In Proceedings of the 10th International Conference 
on Computational Semantics (IWCS 2013), pages 47-58, March 20-22, 2013, Potsdam, Germany.  
Yanyan Cui and Ting Chi. 2013. Annotating Modal Expressions in the Chinese Treebank. In Proceedings 
of the IWC 2013Workshop on Annotation of Modal Meaning in Natural Language (WAMM), pages 24-
32, March 2013, Potsdam, Germany. 
Iris Hendrickx, Am?lia Mendes and Silvia Mencarelti. 2012. Modality in Text: A Proposal for Corpus 
Annotation. In Proceedings of the 8th International Conference on Language Resources and Evaluation 
(LREC'12), pages 1805-1812, May 21-27, 2012, Istanbul, Turkey. 
Klaus Krippendorff. 2011. Computing Krippendorff's Alpha-Reliability. Annenberg School of 
Communication, Departmental Papers: University of Pennsylvania. 
Andrew Lampert, Robert Dale and Cecile Paris. 2010. Detecting Emails Containing Requests for Action. 
In Proceedings of Human Language Technologies: the 2010 Annual Conference of the North American 
Chapter of the ACL, pages 984-992, June 2010, Los Angles, California. 
Ying-Shu Liao and Ting-Gen Liao. 2009. Modal Verbs for the Advice Move in Advice Columns. In 
Proceedings of the 23rd Pacific Asia Conference on language, Information and Computation, pages 
307-316, December 3-5, 2009, Hong Kong, China. 
Suguru Matsuyoshi, Megumi Eguchi, Chitose Sao, Koji Murakami, Kentaro Inui and Yuji Matsumoto. 
2010. Annotating Event Mentions in Text with Modality, Focus and Source Information. In 
Proceedings of LREC'10, pages 1456-1463, May 19-21, 2010, Valletta, Malta.  
T. F. Mitchell and S. A. Al-Hassan. 1994. Modality, Mood and Aspect in Spoken Arabic with Special 
Reference to Egypt and the Levant. London and NY: Kegan Paul International. 
147
Ola Moshref. 2012. Corpus Study of Tense, Aspect, and Modality in Diglossic Speech in Cairene Arabic. 
PhD Thesis. University of Illinois at Urbana-Champaign. 
Frank R. Palmer. 2001. Mood and Modality. 2nd Edition. Cambridge University Press, Cambridge, UK. 
J. Ramanand, Krishna Bhavsar and Niranjan Pedanekar. 2010. Wishful Thinking: Finding Suggestions 
and "Buy" Wishes for Product Reviews. In Proceedings of the NAACL HLT 2010 Workshop on 
Computational Approaches to the Analysis and Generation of Emotion in Text, pages 54-61, June 
2010, Los Angeles, California. 
Aynat Rubinstein, Hillary Harner, Elizabeth Krawczyk, Daniel Simoson, Graham Katz and Paul Portner. 
2013. Toward Fine-Grained Annotation of Modality in Text. In Proceedings of the IWC 2013Workshop 
on Annotation of Modal Meaning in Natural Language (WAMM), pages 38-46, March 2013, Potsdam, 
Germany. 
Roser Saur? and James Pustejovsky. 2009. FactBank: A Corpus Annotated with Event Factuality. 
Language Resources and Evaluation, 43:227-268 
Gy?rgy Szarvas, Veronika Vincze, Rich?rd Farkas and J?nos Csirik. 2008. The BioScope Corpus: 
Annotation for Negation, Uncertainty and their Scope in Biomedical Texts. In Proceedings of BioNLP 
2008: Current Trends in Biomedical Natural Language Processing, pages 38-45, June 2008, 
Columbus, Ohio, USA. 
Janyce Wiebe, Theresa Wilson and Claire Cardie. 2005. Annotating Expressions of Opinions and 
Emotions in Language. Language Resources and Evaluation, volume 39, issue 203, pages 1663-210. 
148
