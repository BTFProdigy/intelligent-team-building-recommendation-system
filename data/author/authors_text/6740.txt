Non-Native Users in the Let?s Go!! Spoken Dialogue System:
Dealing with Linguistic Mismatch
Antoine Raux and Maxine Eskenazi
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA 15232, USA
{antoine+,max+}@cs.cmu.edu
Abstract
This paper describes the CMU Let?s Go!! bus
information system, an experimental system
designed to study the use of spoken dialogue
interfaces by non-native speakers. The differ-
ences in performance of the speech recogni-
tion and language understanding modules of
the system when confronted with native and
non-native spontaneous speech are analyzed.
Focus is placed on the linguistic mismatch be-
tween the user input and the system?s expecta-
tions, and on its implications in terms of lan-
guage modeling and parsing performance. The
effect of including non-native data when build-
ing the speech recognition and language under-
standing modules is discussed. In order to close
the gap between non-native and native input, a
method is proposed to automatically generate
confirmation prompts that are both close to the
user?s input and covered by the system?s lan-
guage model and grammar, in order to help the
user acquire idiomatic expressions appropriate
to the task.
1 Introduction
1.1 Spoken Dialogue Systems and Non-Native
Speakers
Spoken dialogue systems rely on models of human lan-
guage to understand users? spoken input. Such models
cover the acoustic and linguistic space of the common
language used by the system and the user. In current
systems, these models are learned from large corpora of
recorded and transcribed conversations matching the do-
main of the system. In most of the cases, these cor-
pora are gathered from native speakers of the language
because they are the main target of the system and be-
cause developers and researchers are often native speak-
ers themselves. However, when the common language
is not the users? native language, their utterances might
fall out of this ?standard? native model, seriously degrad-
ing the recognition accuracy and overall system perfor-
mance. As telephone-based information access systems
become more common and available to the general pub-
lic, this inability to deal with non-native speakers (or with
any ?non-standard? subgroup such as the elderly) is a
serious limitation since, at least for some applications,
(e.g. tourist information, legal/social advice) non-native
speakers represent a significant portion of the everyday
user population.
1.2 Previous Work on Non-Native Speech
Recognition
Over the past ten years, extensive work has been done
on non-native speech recognition. Early research aimed
at endowing Computer Assisted Language Learning soft-
ware with speech recognition capabilities (e.g. (Eske-
nazi and Hansma, 1998), (Witt and Young, 1997)). Usu-
ally such systems are targeted at one specific popula-
tion, that is, people who share the same native language
(L1). Thus, most research in non-native speech recog-
nition uses knowledge of the L1, as well as databases
of accented speech specially recorded from speakers of
the target population. Ideally, by training acoustic mod-
els on target non-native speech, one would capture its
specific characteristics just as training on native speech
does. However collecting amounts of non-native speech
that are large enough to fully train speaker-independent
models is a hard and often impractical task. Therefore, re-
searchers have resorted to using smaller amounts of non-
native speech to retrain or adapt models that were orig-
inally trained on large corpora of native speech. As for
native speech, such methods were mostly applied to read
speech, with some success (e.g. (Mayfield Tomokiyo and
Waibel, 2001)).
Unfortunately, we know from past research on na-
tive speech recognition that read speech models perform
poorly on conversational speech (Furui, 2001), which is
the style used when talking to spoken dialogue systems.
A few studies have built and used databases of non-native
conversational speech for evaluation (Byrne et al, 1998),
and training (Wang and Schultz, 2003).
In all those cases, the native language of the speaker is
known in advance. One exception is (Fischer et al, 2001)
who apply multilingual speech recognition methods to
non-native speech recognition. The authors train acoustic
models on a database comprising native speech from five
European languages (English, Spanish, French, German
and Italian) and use them to recognize non-native English
from speakers of 10 European countries. However, their
task is the recognition of read digit strings, quite different
from conversational speech.
Also, because of the difficulty researchers have to
record large amounts of spontaneous non-native speech,
no thorough study of the impact of the linguistic differ-
ences between native and non-native spontaneous speech
has been conducted to our knowledge. The two spon-
taneous non-native speech studies cited above, report
perplexity and out-of-vocabulary (OOV) word rate (for
(Wang and Schultz, 2003)) but do not provide any analy-
sis.
In this paper, while acknowledging the importance of
acoustic mismatch between native models and non-native
input, we focus on linguistic mismatch in the context of
a task-based spoken dialogue system. This includes dif-
ferences in word choices which influences the number of
OOV words, and syntax which affects the performance of
the speech recognizer?s language model and of the natu-
ral language understanding (NLU) grammar.
1.3 Non-Native Speakers as Language Learners
All the research on non-native speech recognition de-
scribed in the previous section sees non-native speakers
as a population whose acoustic characteristics need to
be modeled specifically but in a static way, just like one
would model the acoustics of male and female voices dif-
ferently. A different approach to the problem is to see
non-native speakers as engaged in the process of acquir-
ing the target language?s acoustic, phonetic and linguistic
properties. In this paradigm, adapting dialogue systems
to non-native speakers does not only mean being able to
recognize and understand their speech as it is, but also
to help them acquire the vocabulary, grammar, and pho-
netic knowledge necessary to fulfill the task the system
was designed for.
This idea follows decades of language teaching re-
search that, since the mid sixties, has emphasized the
value of learning language in realistic situations, in order
to perform specific tasks. Immersion is widely consid-
ered as the best way to learn to speak a language and mod-
ern approaches to foreign language teaching try to mimic
its characteristics. If the student cannot be present in the
country the language is spoken in, then the student should
be put into a series of situations imitating the linguistic
experience that he/she would have in the target country.
Thus, most current language teaching methods, following
the Communicative Approach (Littlewood, 1981) have
focused on creating exercises where the student is forced
to use language quickly in realistic situations and thus to
learn from the situation itself as well as from reactions to
the student?s actions.
From a different viewpoint, (Bortfeld and Brennan,
1997) showed in a psycholinguistic study that non-native
speakers engaged in conversation-based tasks with native
speakers do not only achieve the primary goal of the task
through collaborative effort but also acquire idiomatic ex-
pressions about the task from the interaction.
The research described in this paper, has the dual goal
of improving the accessibility of spoken dialogue systems
to non-native speakers and of studying the usability of a
computer for task-based language learning that simulates
immersion.
The next section gives an overview of the CMU Let?s
Go!! bus information system that we built and use in
our experiments. Section 3 describes and analyzes the re-
sults of experiments aimed at comparing the accuracy of
speech recognition and the quality of language modeling
on both native and non-native data. Section 4 describes
the use of automatically generated confirmation prompts
to help the user speak the language expected by the sys-
tem. Finally, section 5 draws conclusions and presents
future directions of research.
2 Overview of the System
2.1 The CMU Let?s Go!! Bus Information System
In order to study the use of spoken dialogue systems by
non-native speakers in a realistic setting, we built Let?s
Go!!, a spoken dialogue system that provides bus sched-
ule information for the Pittsburgh area(Raux et al, 2003).
As shown in Figure 1, the system is composed of five ba-
sic modules: the speech recognizer, the parser, the dia-
log manager, the language generator, and the speech syn-
thesizer. Speech recognition is performed by the Sphinx
II speech recognizer (Huang et al, 1992). The Phoenix
parser (Ward and Issar, 1994) is in charge of natural lan-
guage understanding. The dialogue manager is based
on the RavenClaw framework (Bohus and Rudnicky,
2003). Natural language generation is done by a simple
template-based generation module, and speech synthe-
sis by the Festival speech synthesis system (Black et al,
1998). The original system uses a high quality limited-
domain voice recorded especially for the project but for
some experiments, lower quality, more flexible voices
Figure 1: General architecture of the Let?s Go!! bus in-
formation system.
have been used. All modules communicate through the
Galaxy-II (Seneff et al, 1998) framework.
2.2 Definition of the Domain
The Port Authority of Allegheny County, which man-
ages the buses in Pittsburgh provided the full database of
bus routes and schedules. Overall, this database contains
more than 10,000 bus stops but we restricted our system
to 5 routes and 559 bus stops in areas where international
students are likely to travel since they are our main target
population at present.
In order to improve speech recognition accuracy, we
concatenated the words in the name of each bus stop
(e.g. ?Fifth and Grant?) and made them into a single en-
try in the recognizer?s lexicon. Because there are usu-
ally several variant names for each bus stop and since we
included other places such as landmarks and neighbor-
hoods, the total size of the lexicon is 9914 words.
2.3 Data Collection Experiments
To gather enough data to train and test acoustic and lan-
guage models, we had the system running, advertising
it to international students at our university, as well as
conducting several studies. In those studies, we gave sce-
narios to the participants in the form of a web page with
maps indicating the places of departure and destination,
as well as additional time and/or route preferences. There
was as little written English as possible in the descrip-
tion of the scenarios to prevent influencing the language
habits of the participants. Participants then called the sys-
tem over the phone to get the required information. One
experiment conducted in June 2003 netted 119 calls from
11 different non-native speakers (5 of them were from
India and 6 from Japan), as well as 25 calls from 4 na-
tive speakers of American English. Another experiment
in August 2003 allowed the collection of 47 calls from
6 non-native speakers of various linguistic backgrounds.
The rest of the non-native data comes from unsollicited
Native Non-Native
Word Error Rate 20.4 % 52.0 %
Table 1: Word Error Rate of the speech recognizer with a
native language model on native and non-native data.
individual callers labelled as non-native by a human an-
notator who transcribed their speech. The total size of the
spontaneous non-native corpus is 1757 utterances.
3 Recognition and Understanding of
Non-Native Speech
3.1 Recognition Accuracy
We used acoustic models trained on data consisting of
phone calls to the CMU Communicator system(Rudnicky
et al, 2000). The data was split into gender specific
sets and corresponding models were built. At recognition
time, the system runs the two sets of models in parallel
and for each utterance selects the result that has the high-
est recognition score, as computed by Sphinx. The lan-
guage model is a class-based trigram model built on 3074
utterances from past calls to the Let?s Go!! system, in
which place names, time expressions and bus route names
are each replaced by a generic class name to compensate
for the lack of training data.
In order to evaluate the performance of these models on
native and non-native speakers, we used 449 utterances
from non-native users (from the August experiment and
the unsollicited calls) and 452 from native users of the
system. The results of recognition on the two data sets
are given in Table 1. Even for native speakers, perfor-
mance was not very high with a word error rate of 20.4%.
Yet, this is acceptable given the small amount of training
data for the language model and the conversational na-
ture of the speech. However, performance degrades sig-
nificantly for non-native speakers, with a word error rate
of 52.0%. The two main potential reasons for this loss
are acoustic mismatch and linguistic mismatch. Acoustic
mismatch arises from the variations between the native
speech on which the acoustic models were trained and
non-native speech, which often include different accents
and pronunciations. On the other hand, linguistic mis-
match stems from variations or errors in syntax and word
choice, between the native corpus on which the language
model was trained and non-native speech.
3.2 Impact of Linguistic Mismatch on the
Performance of the Language Model
To analyze the effect of linguistic mismatch, we com-
pared the number of out-of-vocabulary words (OOV) and
the perplexity of the model on the transcription of the test
utterances. Table 2 shows the results. The percentage of
Native Non-Native Difference Significance
% OOV words 1.2 % 3.09 % 157.5 % p < 10?4
% utt. w/ OOV words 5.9 % 14.0 % 174.5 % p < 10?5
Perplexity 22.89 36.55 59.7 % ?
% words parsed 63.3 % 56.0 % 56.0 % p < 10?9
% utt. fully parsed 56.4 % 49.7 % 49.7 % p < 0.05
Table 2: The native language model and parsing grammar applied to native and non-native speech transcriptions. The
statistical significance of the difference between the native and non-native sets is computed using the chi-square test
for equality of distributions.
OOVs is 3.09% for non-native speakers, more than 2.5
times higher than it is for native speakers, which shows
the difference in word choices made by each population.
Such differences include words that are correctly used but
are not frequent in native speech. For example, when
referring to bus stops by street intersections, all native
speakers in our training set simply used ?A and B?, hence
the word ?intersection? was not in the language model.
On the other hand, many non-native speakers used the full
expression ?the intersection of A and B?. Note that the
differences inside the place name itself (e.g. ?A and B? vs
?A at B?) are abstracted away by the class-based model,
since all variants are replaced by the same class name
(words like ?intersection? and ?corner? were kept out of
the class to reduce the number of elements in the ?place?
class). In other cases non-native speakers used inappro-
priate words, such as ?bus timing? for ?bus schedule?,
which were not in the language model. Ultimately, OOVs
affect 14.0% of the utterances as opposed to 5.9% for na-
tive utterances, which is significant, since an utterance
containing an OOV is more likely to contain recognition
errors even on its in-vocabulary words, since the OOV
prevents the language model from accurately matching
the utterance. Differences between the native and non-
native set in both OOV rate and the ratio of utterances
containing OOVs were statistically significant.
We computed the perplexity of the model on the utter-
ances that did not contain any OOV. The perplexity of the
model on this subset of the non-native test set is 36.55,
59.7% higher than that on the native set. This reflects
differences in syntax and selected constructions. For ex-
ample, although native speakers almost always used the
same expression to request a bus departure time (?When
does the bus leave ...??), non-natives used a wider variety
of sentences (e.g. ?Which time I have to leave??, ?What
the next bus I have to take??). Both the difference be-
tween native and non-native and the larger variability of
non-native language account for the larger perplexity of
the model over the non-native set. This results seems to
disagree with what (Wang and Schultz, 2003) found in
their study, where the perplexity was larger on the native
set. Unfortunately, they do not describe the data used to
train the language model so it is hard to draw any conclu-
sions. But one main difference is that their experiment
focused only on German speakers of English, whereas
we collected data from a much more diverse population.
3.3 Impact of the Linguistic Mismatch on Language
Understanding
The Phoenix parser used in the natural language under-
standing module of the system is a robust, context-free
grammar-based parser. Grammar rules, including op-
tional words, are compiled into a grammar network that
is used to parse user input. When no complete parse
is found, which is often the case with spoken language,
Phoenix looks for partial parses and returns the parse for-
est that it is most confident in. Confidence is based on
internal measures such as the number of words covered
by the parses and the number of parse trees in the parse
forest (for an equal number of covered words, a smaller
number of parse trees is preferred).
The grammar rules were hand written by the devel-
opers of the system. Initially, since no data was avail-
able, choices were made based on their intuition and on
a small scale Wizard-of-Oz experiment. Then, after the
first version of the system was made available, the gram-
mar was extended according to actual calls to the system.
The grammar has thus undergone continuous change, as
is often the case in spoken dialogue systems.
The grammar used in this experiment (the ?native?
grammar) was designed based for native speech without
adaptation to non-native data. It provides full parses of
sentences like ?When is the next bus going to the air-
port??, but also, due to the robustness of the parser, partial
parses to ungrammatical sentences like ?What time bus
leave airport??. Once compiled, the grammar network
consisted of 1537 states and 3076 arcs. The two bot-
tom rows of Table 2 show the performance of the parser
on human-transcribed native and non-native utterances.
Both the number of words that could be parsed and the
number of sentences for which a full parse was obtained
are larger for native speakers (resp. 63.3% and 56.4%)
than non-native (56% and 49.7%), although the relative
differences are not as large as those observed for the lan-
Figure 2: Comparison of the relative gain obtained by
using a language model and grammar that includes some
non-native data over the original purely native model, on
transcribed native and non-native speech.
guage model. This can be attributed to the original dif-
ficulty of the task since even native speech contains a
lot of disfluencies that make it difficult to parse. As a
consequence, robust parsers such as Phoenix, which are
designed to be flexible enough to handle native disfluen-
cies, can deal with some of the specificities of non-native
speech. Yet, the chi-square test shows that the difference
between the native and non-native set is very significant
for the ratio of words parsed and mildly so for the ratio
of fully parsed sentences. The weak significance of the
latter can be partly explained by the small number of ut-
terances in the corpora.
3.4 Effect of Additional Non-Native Data on
Language Modeling and Parsing
In order to study the improvement of performance pro-
vided by mixing native and non-native data in the lan-
guage model, we built a second language model (the
?mixed? model), using the 3074 sentences of the native
model to which were added 1308 sentences collected
from non-native calls to the system not included in the
test set. Using this model, we were able to reduce the
OOV rate by 56.6% and perplexity by 23.6% for our non-
native test set. While the additional data also improved
the performance of the model on native utterances, the
improvement was relatively smaller than for non-native
speakers (12.1%). As can be seen by comparing Tables
2 and 3, this observation is also true of OOV rate (56.6%
improvement for non-native vs 50.0% for native) and the
proportion of sentences with OOVs (43.1% vs 55.7%).
Figure 2 shows the relative improvement due to the mixed
LM over the native LM on the native and non-native set.
We also evaluated the impact of additional non-native
data on natural language understanding. In this case,
since we wrote the grammar manually and incrementally
over time, it is not possible to directly ?add the non-
native data? to the grammar. Instead, we compared the
June 2003 version of the grammar, which is mostly based
on native speech, to its September 2003 version, which
contains modifications based on the non-native data col-
lected during the summer. This part is therefore an eval-
uation of the impact of the human grammar design done
by the authors based on additional non-native data. At
that point, the compiled grammar had grown to contain
1719 states and 3424 arcs which represents an increase
of respectively 11.8% and 11.3% over the ?native? gram-
mar. Modifications include the addition of new words
(e.g. ?reach? as a synonym of ?arrive?), new constructs
(e.g. ?What is the next bus??) and the relaxation of some
syntactic constraints to accept ungrammatical sentences
(e.g. ?I want to arrive the airport at five? instead of ?I
want to arrive at the airport at five?). Using this new
grammar, the proportion of words parsed and sentences
fully parsed improved by respectively 10.4% and 11.3%
for the native set and by 17.3% and 11.7% for the non-
native set. We believe that, as for the language model, the
reduction in the number of OOVs is the main explana-
tion behind the better improvement in word coverage ob-
served for the non-native set compared to the native set.
The reduction of the difference between the native and
non-native sets is also reflected in the weaker significance
levels for all ratios except that of fully parsed utterances,
in 3, larger p-values meaning that there is a larger proba-
bility that the differences between the ratios were due to
spurious differences between the corpora rather than to
their (non-)nativeness.
This confirms that even for populations with a wide
variety of linguistic backgrounds, adding non-native data
does reduce the linguistic mismatch between the model
and new, unseen, non-native speech. Another explana-
tion is that, on a narrow domain such as bus schedule
information, the linguistic variance of non-native speech
is much larger than that of native speech. Therefore,
less data is required to accurately model native speech
than non-native speech. It also appears from these results
that, in the context of task-based spoken dialogue sys-
tems, higher-level modules, such as the natural language
understanding module, are less sensitive to explicit mod-
eling of non-nativeness. This can be explained by the fact
that such modules were designed to be flexible in order to
compensate for speech recognition errors. This flexibility
benefits non-native speakers as well, regardless of addi-
tional recognition errors.
3.5 Effect of Additional Non-Native Data on Speech
Recognition
Unfortunately, the reduction of linguistic mismatch was
not observed on recognition results. While using the new
language model improved word error rate on both native
Native Non-Native Difference Significance
% OOV words 0.6 % 1.34 % 123.3 % p < 0.05
% utt. w/ OOV words 2.9 % 6.2 % 113.8 % p < 0.01
Perplexity 20.12 27.92 38.8 % ?
% words parsed 69.9 % 65.7 % 65.7 % p < 10?3
% utt. fully parsed 62.8 % 55.5 % 55.5 % p < 0.05
Table 3: The mixed language model and parsing grammar applied to native and non-native speech transcriptions.
Significance is computed using the chi-square test, except for perplexity where the relative difference is reported.
Figure 3: Word Error Rate on Native and Non-Native
Data using a Native and a Mixed Language Model
and non-native utterances (resp. to 17.8% and 47.8%,
see Figure 3 ), the impact was relatively larger for native
speech. This is an indication that acoustics play a promi-
nent role in the loss of accuracy of speech recognition on
non-native speech. Acoustic differences between native
and non-native speakers are likely to be larger than the
linguistic ones, since, particularly on such a limited and
common domain, it is easier for non-native speakers to
master syntax and word choice than to improve their ac-
cent and pronunciation habits. Differences among non-
native speakers of different origins are also very large
in the acoustic domain, making it hard to create a single
acoustic model matching all non-native speakers. Finally,
the fact that additional non-native data improves perfor-
mance on native speech is a sign that, generally speak-
ing, the lack of training data for the language model is a
limiting factor for recognition accuracy. Indeed, if there
was enough data to model native speech, additional non-
native data should increase the variance and therefore the
perplexity on native speech.
4 Adaptive Lexical Entrainment as a
Solution to Linguistic Mismatch
4.1 Gearing the User To the System?s Language
The previous section described the issue of recogniz-
ing and understanding non-native speech and solutions to
adapt traditional systems to non-native speakers. Another
approach is to help non-native users adapt to the system
by learning appropriate words and expressions. Lexical
entrainment is the phenomenon by which, in a conversa-
tion, speakers negotiate a common ground of expressions
to refer to objects or topics. Developers of spoken di-
alogue systems frequently take advantage of lexical en-
trainment to help users speak utterances that are within
the language model of the system. This is done by care-
fully designing the system prompts to contain only words
that are recognized by the recognition and understanding
modules (Gustafson et al, 1997). However, in the case
of non-native users, there is no guarantee that users actu-
ally know the words the system wants them to use. Also,
even if they do, some non-native speakers might prefer
to use other words, which they pronounce better or that
they better know how to use. For those reasons, we be-
lieve that to be optimal, the system must try to match the
user?s choice of words in its own prompts. This idea is
motivated by the observations of (Bortfeld and Brennan,
1997), who showed that this type of adaptation occurs
in human-human conversations between native and non-
native speakers.
The role of the system?s ?native? prompts is to take
the users through the shortest path from their current lin-
guistic state to the system?s expectations. In fact, this is
not only true for non-native speakers and lexical entrain-
ment is often described as a negotiation process between
the speakers (Clark and Wilkes-Gibbs, 1986). However,
while it is possible for limited-domain system design-
ers to establish a set of words and constructions that are
widely used among native speakers, the variable nature
of the expressions mastered by non-native speakers make
adaptation a desirable feature of the system.
4.2 Automatic Generation of Corrective Prompts
In this study, not all prompts were modified to match the
user?s choice of words. Instead, the focus was placed on
confirmation prompts that both ensure proper understand-
ing between the user and the system and lexically entrain
the user towards the system?s expected input. Two ques-
tions arise: how to generate the prompts and when to
trigger them. Our approach has been to design a list of
target prompts that fit the system?s language model and
grammar and find the closest target prompt to each user
input. The distance between a user utterance as recog-
nized by Sphinx and each of the target utterances is com-
puted by the same dynamic programming algorithm that
is traditionally used to compute word error rate in speech
recognition evaluation. It determines the number of word
insertions, deletions and substitutions that lead from the
target prompt to the user?s utterance. The target prompt
that is closest, i.e. that requires the fewest operations to
match the input, is selected. In addition, words that rep-
resent important concepts such as places, times or bus
route numbers, are given additional weight. This follows
the assumption that a target sentence is not appropriate
if it has a missing or an extra concept compared to the
utterance. We also used this heuristic to answer the sec-
ond question: when to trigger the confirmation prompts.
The system asks for a confirmation whenever a target sen-
tence is found that contains the same concepts as the user
input and differs from it by at least one word. In this case
a prompt like ?Did you mean ...? followed by the tar-
get sentence is generated. Finally, the dynamic program-
ming algorithm used to align the utterances also locates
the words that actually differ between the input and the
target. This information is sent to the speech synthesizer,
which puts particular emphasis on the words that differ.
To provide natural emphasis, the intonation of all sen-
tences is generated by the method described in (Raux and
Black, 2003) that concatenates portions of natural into-
national contours from recorded utterances into a contour
appropriate for each prompt. Since the domain-limited
voice recorded for the project does not allow us to either
generate non-recorded prompts or to modify the contour
of the utterances, we used a different, generic voice for
this version of the system.
4.3 Application and Example
The method described in the previous paragraph was im-
plemented in the system and tested in a small pilot study.
We manually wrote 35 different target prompts describing
departure and destination places, times and route num-
bers, based on our knowledge of the system?s language
model and grammar. An example of a confirmation di-
alogue obtained from one of these prompts is given in
Figure 4. In the first user utterance, the preposition ?to? is
missing, either because it was not pronounced by the user
or because it was not recognized by the speech recog-
nition module. As a consequence, the utterance cannot
be fully parsed by the language understanding module.
In parallel, the confirmation module computes the dis-
tance between the user?s input and each of the 35 target
prompts, and identifies the closest one as ?I want to go to
the airport?. At the same time it finds that the user?s utter-
ance is obtained from the target by deleting the word ?to?
and therefore stresses it in the confirmation prompt. Once
S: What can I do for you?
U: I want to go the airport.
S: Sorry, I didn?t get that.
Did you mean:
I want to go TO the airport?
U: Yes
S: To the airport.
Where are you leaving from?
U: ...
Figure 4: Example of an adaptive confirmation dialogue.
The capital ?TO? indicate that the word was emphasized
by the system.
the user answers ?yes? to the confirmation prompt, the
target prompt is sent to the parser as if it had been uttered
by the user and the state of the dialogue is updated ac-
cordingly. If the user answers ?no?, the prompt is simply
discarded. We found that this method works well when
speech recognition is only slightly degraded and/or when
the recognition errors mostly concern grammar and func-
tion words. In such cases, this approach is often able to
repair utterances that would not be parsed correctly other-
wise. However, when too many recognition errors occur,
or when they affect the values of the concepts (i.e. the
system recognizes one place name instead of another),
the users receive too many confirmation prompts to which
they must respond negatively. Combined with the diffi-
culty that non-native speakers have in understanding un-
expected synthesized utterances, this results in cognitive
overload on the user. Yet, this method provides an easy
way (since the designer only has to provide the list of tar-
get prompts) to generate adaptive confirmation prompts
that are likely to help lexical entrainment.
5 Conclusion and Future Directions
In this paper, we described the Let?s Go!! bus information
system, a dialogue system targetted at non-native speak-
ers of English. In order to investigate ways to improve the
communication between non-native users and the system,
we recorded calls from both native and non-native speak-
ers and analyzed their linguistic properties. We found
that besides the problem of acoustic mismatch that results
from the differences in accent and pronunciation habits,
linguistic mismatch is also significant and degrades the
performance of the language model and the natural lan-
guage understanding module. We are exploring two solu-
tions to reduce the linguistic gap between native and non-
native users. First we studied the impact of taking into
account non-native data to model the user?s language and
second we designed a mechanism to generate confirma-
tion prompts that both match the user?s input and a set of
predefined target utterances, so as to help the user acquire
idiomatic expressions related to the task.
Real-world systems like Let?s Go!! are in constant evo-
lution because the data that is collected from users call-
ing the system is used to refine the acoustic and linguis-
tic models of the system. In the near future, our priority
is to collect more data to improve the acoustic models
of the system and address the specific issues related to
a general non-native population, which does not share a
common native language. We will also work on integrat-
ing the confirmation prompt generation method proposed
in this work with state-of-the-art confidence annotation
methods.
6 Acknowledgments
The authors would like to thank Alan W Black, Dan Bo-
hus and Brian Langner for their help with this research.
This material is based upon work supported by the U.S.
National Science Foundation under Grant No. 0208835,
?LET?S GO: improved speech interfaces for the general
public?. Any opinions, findings, and conclusions or rec-
ommendations expressed in this material are those of the
authors and do not necessarily reflect the views of the Na-
tional Science Foundation.
References
A. Black, P. Taylor, and R. Caley. 1998. The Festival
speech synthesis system. http://festvox.org/festival.
D. Bohus and A. Rudnicky. 2003. Ravenclaw: Dia-
log management using hierarchical task decomposition
and an expectation agenda. In Proc. Eurospeech 2003,
pages 597?600, Geneva, Switzerland.
H. Bortfeld and S. Brennan. 1997. Use and acquisition
of idiomatic expressions in referring by native and non-
native speakers. Discourse Processes, 23:119?147.
W. Byrne, E. Knodt, S. Khudanpur, and J. Bernstein.
1998. Is automatic speech recognition ready for non-
native speech? A data collection effort and initial ex-
periments in modeling conversational hispanic english.
In Proc. ESCA Workshop on Speech Technology in
Language Learning, pages 37?40, Marholmen, Swe-
den.
H. Clark and D. Wilkes-Gibbs. 1986. Referring as a
collaborative process. Cognition, 22:1?39.
M. Eskenazi and S. Hansma. 1998. The Fluency pronun-
ciation trainer. In Proc. ESCA Workshop on Speech
Technology in Language Learning, pages 77?80.
V. Fischer, E. Janke, S. Kunzmann, and T. Ross. 2001.
Multilingual acoustic models for the recognition of
non-native speech. In Proc. ASRU ?01, Madonna di
Campiglio, Italy.
S. Furui. 2001. From read speech recognition to spon-
taneous speech understanding. In Proc. 6th Natural
Language Processing Pacific Rim Symposium, pages
19?25, Tokyo, Japan.
J Gustafson, A. Larsson, R. Carlson, and K. Hellman.
1997. How do system questions influence lexical
choices in user answers? In Proc. Eurospeech ?97,
pages 2275?2278, Rhodes, Greece.
X. Huang, F. Alleva, H.-W. Hon, K.-F. Hwang, M.-Y.
Lee, and R. Rosenfeld. 1992. The SPHINX-II speech
recognition system: an overview. Computer Speech
and Language, 7(2):137?148.
W. Littlewood. 1981. Communicative Language Teach-
ing. Cambridge University Press.
L. Mayfield Tomokiyo and A. Waibel. 2001. Adapta-
tion methods for non-native speech. In Proc. Multilin-
guality in Spoken Language Processing, Aalborg, Den-
mark.
A. Raux and A. Black. 2003. A unit selection ap-
proach to f0 modeling and its application to empha-
sis. In Proc. IEEE Automatic Speech Recognition and
Understanding Workshop 2003, pages 700?705, Saint
Thomas, US Virgin Islands.
A. Raux, B. Langner, A. Black, and M. Eskenazi. 2003.
Lets go: Improving spoken dialog systems for the el-
derly and non-natives. In Proc. Eurospeech 2003,
pages 753?756, Geneva, Switzerland.
A. Rudnicky, C. Bennett, A. Black, A. Chotimongkol,
K. Lenzo, A. Oh, and R. Singh. 2000. Task and do-
main specific modelling in the carnegie mellon com-
municator system. In Proc. ICSLP 2000, Beijing,
China.
S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, and
V. Zue. 1998. Galaxy-II: A reference architecture for
conversational system development. In Proc. ICSLP
?98, Sydney, Australia.
Z. Wang and T. Schultz. 2003. Non-native spontaneous
speech recognition through polyphone decision tree
specialization. In Proc. Eurospeech ?03, pages 1449?
1452, Geneva, Switzerland.
W. Ward and S. Issar. 1994. Recent improvements in the
CMU spoken language understanding system. In Proc.
ARPA Human Language Technology Workshop, pages
213?216, Plainsboro, NJ.
S. Witt and S. Young. 1997. Language learning based on
non-native speech recognition. In Proc. Eurospeech
?97, pages 633?636, Rhodes, Greece.
Proceedings of NAACL HLT 2007, Companion Volume, pages 9?12,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Conquest ? an Open-Source Dialog System for Conferences 
 
Dan Bohus, Sergio Grau Puerto, David Huggins-Daines, Venkatesh Keri,  
Gopala Krishna,  Rohit Kumar, Antoine Raux, Stefanie Tomko 
School of Computer Science 
Carnegie Mellon University 
{ dbohus, sgrau, dhuggins, vkeri, gopalakr, rohitk, antoine, stef }@ cs.cmu.edu 
 
 
  
Abstract 
We describe ConQuest, an open-source, 
reusable spoken dialog system that pro-
vides technical program information dur-
ing conferences. The system uses a 
transparent, modular and open infrastruc-
ture, and aims to enable applied research 
in spoken language interfaces. The con-
ference domain is a good platform for ap-
plied research since it permits periodical 
redeployments and evaluations with a real 
user-base. In this paper, we describe the 
system?s functionality, overall architec-
ture, and we discuss two initial deploy-
ments.  
1 Introduction  
Conducting applied spoken language interface re-
search is generally a costly endeavor. Developing, 
deploying and maintaining real-world spoken lan-
guage interfaces requires an existing infrastructure, 
a significant amount of engineering effort, and can 
greatly benefit from the availability of certain re-
sources such as transcribed in-domain data.  
In an effort to enable applied research and to 
lower this high cost of entry, we have developed 
ConQuest (Conference Questions) an open-source 
spoken dialog system that provides access to 
schedule and technical program information during 
conferences. We believe the conference domain 
has a number of good properties for applied re-
search: it includes a number of tasks of different 
complexities, it provides regular access to a real-
world user population; it permits periodical rede-
ployments and evaluations and therefore can pro-
vide a natural common evaluation task for the 
spoken language interfaces community.  
The ConQuest system is constructed on top of 
the open, transparent and modular Olympus dialog 
system framework (2007), and can be easily reused 
across different conferences. To date, the system 
has been deployed in two conferences: InterSpeech 
2006 and IJCAI 2007. Together with corpora col-
lected from these deployments, the system is freely 
available for download (Conquest, 2007).  
We begin by describing the ConQuest function-
ality in the next section. Then, in section 3 we pro-
vide an overview of the system architecture and 
discuss the development process. In section 4 we 
briefly discuss the two deployment efforts. Finally, 
in section 5 we discuss related work and draw a 
number of conclusions.   
2 Functionality 
As Figure 1 illustrates, ConQuest is a mixed-
initiative spoken dialog system that provides ac-
cess to schedule and technical program information 
during conferences.  
Users can browse the schedule and find details 
about various papers or sessions of interest by pro-
viding identifying information, such as topics, ses-
sion names, special events, paper titles, author 
names, specific dates and times, specific locations, 
or a combination thereof (e.g. turns 2, 4, 14). Addi-
tionally, the system also allows uses to listen to 
current announcements and to cast their votes for a 
Best Paper Award (e.g. turns 10-17).  
The interaction is mixed-initiative; at any point, 
the user may switch the current focus of the con-
versation (e.g. turn 8). ConQuest employs an adap-
tive information presentation strategy that allows 
9
users to easily navigate the schedule (see turns 3, 5 
and 15). The system uses a rich repertoire of error 
recovery strategies to handle potential errors, in-
cluding several fall-back strategies (e.g. turn 13).  
3 System Architecture  
The ConQuest system was built using RavenClaw/ 
Olympus (2007), an open-source framework that 
facilitates research and development in task ori-
ented conversational spoken language interfaces. 
Olympus consists of a collection of components 
for recognition, language understanding, dialog 
management, language generation, speech synthe-
sis, etc., and the corresponding communication 
infrastructure. To date, Olympus has been used to 
develop and deploy a number of other systems 
spanning different domains and interaction types 
(Bohus and Rudnicky, 2003).  
A key characteristic of the Olympus framework 
is a clear separation between the domain independ-
ent programs (or components) and domain specific 
resources. This decoupling promotes reusability 
and significantly lessens the system development 
effort. In ConQuest, the authoring effort was fo-
cused on developing resources such as the lexicon, 
language model, grammar, dialog task specifica-
tion, etc. Some interesting, unanticipated engineer-
ing challenges we faced during development were 
dealing with foreign names and accented charac-
ters and performing text normalization on various 
fields (e.g. Alex Smith and Alexander Smith are 
the same author), while at the same time ensuring 
consistency between these various resources. Be-
low, we briefly comment of each component and 
the corresponding resources. Figure 2 provides a 
top-level architectural view. 
Speech Recognition. ConQuest uses a recogni-
tion server coupled to a set of parallel recognition 
engines: two SPHINX-II decoders (Huang et al, 
1992) that use gender-specific acoustic models, 
and a DTMF (touch-tone decoder). Each recogni-
tion engine uses class-based (e.g. paper titles, au-
thor names, etc.), state-specific trigram-language 
models. We started with an initial language model 
built using data collected with an early text-only 
prototype. We then internally deployed a speech 
based system, collected more data, transcribed it, 
and used it to retrain the language models. The 
Figure 1. A sample interaction with ConQuest 
1 S: Welcome to ConQuest [?] I can give you information about papers, sessions, special events and announcements at Inter-
Speech 2006. You can also cast a vote for the best paper award. How can I help you? 
2 U: are there any papers on spoken dialog systems today 
3 S:  There are 23 papers on spoken dialog systems today. These papers are being presented in 3 sessions: Dialog and Discourse, 
Language Modeling for Spoken Dialog Systems and Spoken Dialog Systems I. To browse this list of papers, please tell me the 
author name, the paper title, the time or the venue you?re looking for. If you?re not interested in any of these papers say ?quit?. 
4 U: which ones are in the afternoon 
5 S:  There will be 11 papers on spoken dialog systems this afternoon. Would you like to hear information about all these papers? 
6 U: yes 
7 S:  The list of papers on spoken dialog systems this afternoon is: Paper 1 - ?Dynamic Extension of a Grammar-based Dialogue 
System: Constructing an All-Recipes Knowing Robot?, Paper 2 ?  [?] Would you like more information about any of these? 
8 U: tell me more about the third one 
9 S:  Paper 3 is titled ?System- versus user-initiative dialog strategy for driver information systems?, and was presented yesterday at 2 
pm in the poster session in Allegheny Ballroom Foyer. This paper was written by Chantal Ackermann, and Marion Libossek. 
Would you like more information about any of these? 
8 U: no when is the banquet 
9 S:  The Banquet will be held Wednesday from 7pm to 9:30 pm in the Heintz History Center. The Heinz History Center is located on 
Smallman street between 12th and 13th streets. The History Center is a museum that relates and interprets 250 years of history 
in the Western Pennsylvania area. Is there anything else I can do for you? 
10 U: yes I?d like to cast a vote 
11 S:  Please tell me the title of the paper you?d like to vote for.. 
12 U: I don?t really know the title but I think it had Diane as one of the main authors ? [non-understanding] 
13 S:  Sorry, I didn?t catch that. Please dial the first six letters from the title of the paper you would like to vote for.  
14 U: I don?t know it it?s a paper by Diane Litman 
15 S:  There are 3 papers by Diane Litman. Paper 1 ? ?Discourse Structure and Speech Recognition Problems?, Paper 2 ? [?], Paper 
3 ? [?] Would you like to vote for any of these? 
16 U: Discourse Structure and Speech Recognition Problems 
17 S:  Okay, I?ve recorded your vote for the paper entitled ?Discourse Structure and Speech Recognition Problems? What else can I do 
for you?  
18 U: [?] 
10
final language models used during the InterSpeech 
deployment were trained from on a corpus of 6350 
utterances. The system operated with a lexicon of 
4795 words, which included 659 lexicalized (con-
catenated) paper titles, and 1492 lexicalized author 
names, and 78 lexicalized session names. The pro-
nunciations were generated using CMU Dictionary 
and later manually corrected.  
Language understanding. The system uses the 
Phoenix (Ward and Issar, 1994) robust parser to 
extract concepts from the recognition results. A 
domain-specific shallow semantic grammar was 
developed and concatenated with a domain-
independent grammar for generic expressions like 
[Yes], [No], [Date], [Time], etc.  
Dialog management. ConQuest uses a Raven-
Claw-based dialog manager (Bohus and Rudnicky, 
2003). We developed a dialog task specification 
for the conference schedule domain, expressed as a 
hierarchical plan for the interaction, which the 
RavenClaw engine uses to drive the dialog. In the 
process, the RavenClaw engine automatically pro-
vides additional generic conversational skills such 
as error recovery strategies and support for various 
universal dialog mechanisms (e.g. repeat, start-
over, what-can-I-say, etc.)  
Backend/Database. A backend agent looks up 
schedule information from the database (stored as 
a flat text file). The backend agent also performs 
domain specific pre-lookup normalization (e.g. 
mapping author names to their canonical forms), 
and post-lookup processing of the returned records 
(e.g. clustering papers by sessions). The database 
file serves as starting point for constructing a 
number of other system resources (e.g. language 
model classes, lexicon, etc.)  
Figure 2. The Olympus dialog system reference architecture (a typical system) 
Temporal reference resolution agent. Apart 
from the database agent, the dialog manager also 
communicates with an agent that resolves temporal 
expressions (e.g. tomorrow at four p.m.) into ca-
nonical forms.  
Language generation. ConQuest uses Rosetta, 
a template-based language generation component. 
The authoring effort at this level consisted of writ-
ing various templates for the different system ques-
tions and information presentation prompts.  
Speech synthesis. ConQuest uses the Cepstral 
(2005) speech synthesis engine, configured with an 
open-domain unit selection voice. We manually 
checked and corrected pronunciations for author 
names, various technical terms and abbreviations.  
4 Development and Deployment 
The first development of ConQuest system was 
done for the Interspeech 2006 conference held in 
Pittsburgh, PA. The iterative development process 
involved regular interaction with potential users 
i.e. researchers who regularly attend conferences. 
Seven developers working half time participated in 
this development for about three months. An esti-
mated one man-year of effort was spent. This esti-
mate does not include the effort involved in 
transcribing the data collected after the conference. 
Two systems were deployed at the Interspeech 
2006 conference: a desktop system using a close-
talking microphone placed by the registration desk, 
and a telephone-based system. Throughout the 
conference we collected a corpus of 174 sessions. 
We have orthographically transcribed the user ut-
11
terances and are currently analyzing the data; we 
plan to soon release it to the community, together 
with detailed statistics, the full system logs as well 
as the full system source code (Conquest, 2007). 
Following Interspeech 2006, ConQuest was re-
deployed at IJCAI 2007 conference held in Hy-
derabad, India. The second deployment took an 
estimated two man-months: three developers work-
ing half-time for over a month. The significant 
parts of the second deployment involved incorpo-
rating scheduling data for the IJCAI 2007 and im-
plementing two new requirements i.e. support for 
workshops and Indian English speech recognition. 
The IJCAI development had fewer iterations than 
the first effort. The two desktop systems set up at 
the conference venue collected 129 sessions of 
data. This data is currently being transcribed and 
will soon be released to the community through the 
Conquest website (Conquest, 2007). 
Through these two deployments of ConQuest 
the system specifications have been refined and we 
expect the development time to asymptote to less 
than a month after a few more deployments. 
5 Discussion and Conclusion  
Our primary goal in developing ConQuest was to 
enable research by constructing and releasing an 
open-source, full-fledged dialog system, as well as 
an initial corpus collected with this system. The 
system is built on top of an open, transparent and 
modular infrastructure that facilitates research in 
spoken language interfaces (Olympus, 2007).  
There have been a number of other efforts to 
collect and publish dialog corpora, for instance 
within the DARPA Communicator project. A more 
recent project, that operates in a domain similar to 
ConQuest is DiSCoH, a Dialog System for Confer-
ence Help developed by researchers at AT&T, 
ICSI and Edinburgh University, and deployed dur-
ing the SLT-2006 workshop (Adreani et al, 2006). 
While their goals are similar, i.e. to enable re-
search, DiSCoH and ConQuest differ in a number 
of dimensions. Functionality-wise, DiSCoH offers 
general conference information about the venue, 
accommodation options and costs, paper submis-
sion, etc., while ConQuest provides access to the 
technical schedule and allows participants to vote 
for a best paper award. DiSCoH is built using 
AT&T technology and a call-routing approach; 
ConQuest relies on a plan-based dialog manage-
ment framework (RavenClaw) and an open-source 
infrastructure (Olympus). Finally, the DiSCoH ef-
fort aims to develop a richly annotated dialog cor-
pus to be used for research; ConQuest?s aim is to 
provide both the full system and an initial tran-
scribed and annotated corpus to the community. 
The conference domain is interesting in that it 
allows for frequent redeployment and in theory 
provides regular access to a certain user-base. It 
should therefore facilitate research and periodical 
evaluations. Unfortunately, the dialog corpora col-
lected so far using DiSCoH and ConQuest have 
been somewhat smaller than our initial expecta-
tions. We believe this is largely due to the fact that 
the systems provide information that is already 
accessible to users by other means (paper confer-
ence program, web-sites, etc.). Perhaps combining 
the functionalities of these two systems, and ex-
panding into directions where the system provides 
otherwise hard-to-access information (e.g. local 
restaurants, transportation, etc.) would lead to in-
creased traffic.  
References  
Adreani, G., Di Fabbrizio, G., Gilbert, M., Gillick, D., 
Hakkani-Tur, D., and Lemon, O., 2006 Let?s DiS-
CoH: Collecting an Annotated Open Corpus with 
Dialogue Acts and Reward Signals for Natural Lan-
guage Helpdesk, in Proceedings of IEEE SLT-2006 
Workshop, Aruba Beach, Aruba. 
Bohus, D., and Rudnicky, A., 2003. RavenClaw: Dialog 
Management Using Hierarchical Task Decomposi-
tion and an Expectation Agenda, in Proceedings of 
Eurospeech 2003, Geneva, Switzerland. 
Cepstral, LLC, 2005, SwiftTM: Small Footprint Text-to-
Speech Synthesizer, http://www.cepstral.com. 
Conquest, 2007, http://www.conquest-dialog.org. 
Huang, X., Alleva, F., Hon, H.-W., Hwang, M.-Y., Lee, 
K.-F. and Rosenfeld, R., 1992. The SPHINX-II 
Speech Recognition System: an overview, in Com-
puter Speech and Language, 7(2), pp 137-148, 1992. 
Olympus/RavenClaw web page, as of January 2007: 
http://www.ravenclaw-olympus.org/. 
Ward, W., and Issar, S., 1994. Recent improvements in 
the CMU spoken language understanding system, in 
Proceedings of the ARPA Human Language Tech-
nology Workshop, pages 213?216, Plainsboro, NJ. 
12
Tutorial Abstracts of ACL-08: HLT, page 2,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Building Practical Spoken Dialog Systems 
Antoine Raux1, Brian Langner2, Alan W Black3, Maxine Eskenazi4 Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA {antoine,blangner,awb,max}@cs.cmu.edu 
                                                           1 http://www.cs.cmu.edu/~antoine 2 http://www.cs.cmu.edu/~blangner 3 http://www.cs.cmu.edu/~awb 4 http://www.cs.cmu.edu/~max 
 
 
1 Abstract This tutorial will give a practical description of the free software Carnegie Mellon Olympus 2 Spoken Dialog Architecture. Building real working dialog systems that are robust enough for the general pub-lic to use is difficult. Most frequently, the func-tionality of the conversations is severely limited - down to simple question-answer pairs. While off-the-shelf toolkits help the development of such simple systems, they do not support more ad-vanced, natural dialogs nor do they offer the trans-parency and flexibility required by computational linguistic researchers.  However, Olympus 2 offers a complete dialog system with automatic speech recognition (Sphinx) and synthesis (SAPI, Festi-val) and has been used, along with previous ver-sions of Olympus, for teaching and research at Carnegie Mellon and elsewhere for some 5 years. Overall, a dozen dialog systems have been built using various versions of Olympus, handling tasks ranging from providing bus schedule information to guidance through maintenance procedures for complex machinery, to personal calendar manage-ment. In addition to simplifying the development of dialog systems, Olympus provides a transparent platform for teaching and conducting research on all aspects of dialog systems, including speech rec-ognition and synthesis, natural language under-standing and generation, and dialog and interaction management. The tutorial will give a brief introduction to spoken dialog systems before going into detail 
about how to create your own dialog system within Olympus 2, using the Let's Go bus information system as an example. Further, we will provide guidelines on how to use an actual deployed spo-ken dialog system such as Let's Go to validate re-search results in the real world. As a possible testbed for such research, we will describe Let's Go Lab, which provides access to both the Let's Go system and its genuine user population for research experiments. 2 Outline Part 1 1.1 Introduction 1.2 Overview of current spoken dialog  system architectures 1.3 Description of the Olympus2 dialog  architecture 1.4 How to build an Olympus2 spoken  dialog system Part 2 2.1 Advanced Topics a. Improving ASR b. Improving TTS c. Dealing with ASR Errors d. Logs and Tools 2.2 Using Olympus2 for research and  applications 2.3 Final summary 
2
Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 32?39,
NAACL-HLT, Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Olympus: an open-source framework 
for conversational spoken language interface research 
 
 
Dan Bohus, Antoine Raux, Thomas K. Harris, 
Maxine Eskenazi, Alexander I. Rudnicky 
School of Computer Science 
Carnegie Mellon University 
{dbohus, antoine, tkharris, max, air}@cs.cmu.edu 
 
 
 
 
Abstract 
We introduce Olympus, a freely available 
framework for research in conversational 
interfaces. Olympus? open, transparent, 
flexible, modular and scalable nature fa-
cilitates the development of large-scale, 
real-world systems, and enables research 
leading to technological and scientific ad-
vances in conversational spoken language 
interfaces. In this paper, we describe the 
overall architecture, several systems 
spanning different domains, and a number 
of current research efforts supported by 
Olympus.  
1 Introduction  
Spoken language interfaces developed in industrial 
and academic settings differ in terms of goals, the 
types of tasks and research questions addressed, 
and the kinds of resources available.  
In order to be economically viable, most indus-
try groups need to develop real-world applications 
that serve large and varied customer populations. 
As a result, they gain insight into the research 
questions that are truly significant for current-
generation technologies. When needed, they are 
able to focus large resources (typically unavailable 
in academia) on addressing these questions. To 
protect their investments, companies do not gener-
ally disseminate new technologies and results. 
In contrast, academia pursues long-term scien-
tific research goals, which are not tied to immedi-
ate economic returns or customer populations. As a 
result, academic groups are free to explore a larger 
variety of research questions, even with a high risk 
of failure or a lack of immediate payoff. Academic 
groups also engage in a more open exchange of 
ideas and results. However, building spoken lan-
guage interfaces requires significant investments 
that are sometimes beyond the reach of academic 
researchers. As a consequence, research in acade-
mia is oftentimes conducted with toy systems and 
skewed user populations. In turn, this raises ques-
tions about the validity of the results and hinders 
the research impact.  
In an effort to address this problem and facilitate 
research on relevant, real-world questions, we have 
developed Olympus, a freely available framework 
for building and studying conversational spoken 
language interfaces. The Olympus architecture, 
described in Section 3, has its roots in the CMU 
Communicator project (Rudnicky et al, 1999). 
Based on that experience and subsequent projects, 
we have engineered Olympus into an open, trans-
parent, flexible, modular, and scalable architecture. 
To date, Olympus has been used to develop and 
deploy a number of spoken language interfaces 
spanning different domains and interaction types; 
these systems are presented in Section 4. They are 
currently supporting research on diverse aspects of 
spoken language interaction. Section 5 discusses 
three such efforts: error handling, multi-participant 
conversation, and turn-taking. 
We believe that Olympus and other similar tool-
kits, discussed in Section 6, are essential in order 
to bridge the gap between industry and academia. 
Such frameworks lower the cost of entry for re-
32
search on practical conversational interfaces. They 
also promote technology transfer through the reuse 
of components, and support direct comparisons 
between systems and technologies.  
2 Desired characteristics 
While developing Olympus, we identified a num-
ber of characteristics that in our opinion are neces-
sary to effectively support and foster research. The 
framework should be open, transparent, flexible, 
modular, and scalable.  
Open. Complete source code should be avail-
able for all the components so that researchers and 
engineers can inspect and modify it towards their 
ends. Ideally, source code should be free for both 
research and commercial purposes and grow 
through contributions from the user community. 
Transparent / Analytic. Open source code 
promotes transparency, but beyond that researchers 
must be able to analyze the system?s behavior. To 
this end, every component should provide detailed 
accounts of their internal state. Furthermore, tools 
for data visualization and analysis should be an 
integral part of the framework. 
Flexible. The framework should be able to ac-
commodate a wide range of applications and re-
search interests, and allow easy integration of new 
technologies. 
Modular / Reusable. Specific functions (e.g. 
speech recognition, parsing) should be encapsu-
lated in components with rich and well-defined 
interfaces, and an application-independent design. 
This will promote reusability, and will lessen the 
system development effort.  
Scalable. While frameworks that rely on sim-
ple, well established approaches (e.g. finite-state 
dialogs in VoiceXML) allow the development of 
large-scale systems, this is usually not the case for 
frameworks that provide the flexibility and trans-
parency needed for research. However, some re-
search questions are not apparent until one moves 
from toy systems into large-scale applications. The 
framework should strive to not compromise scal-
ability for the sake of flexibility or transparency. 
3 Architecture  
At the high level, a typical Olympus application 
consists of a series of components connected in a 
classical, pipeline architecture, as illustrated by the 
bold components in Figure 1. The audio signal for 
the user utterance is captured and passed through a 
speech recognition module that produces a recog-
nition hypothesis (e.g., two p.m.). The recognition 
hypothesis is then forwarded to a language under-
standing component that extracts the relevant con-
cepts (e.g., [time=2p.m.]), and then through a 
confidence annotation module that assigns a confi-
dence score. Next, a dialog manager integrates this 
semantic input into the current context, and pro-
duces the next action to be taken by the system in 
the form of the semantic output (e.g., {request 
end_time}). A language generation module pro-
duces the corresponding surface form, which is 
subsequently passed to a speech synthesis module 
and rendered as audio.  
Galaxy communication infrastructure. While 
the pipeline shown in bold in Figure 1 captures the 
logical flow of information in the system, in prac-
tice the system components communicate via a 
centralized message-passing infrastructure ? Gal-
axy (Seneff et al, 1998). Each component is im-
plemented as a separate process that connects to a 
traffic router ? the Galaxy hub. The messages are 
sent through the hub, which forwards them to the 
appropriate destination. The routing logic is de-
scribed via a configuration script. 
Speech recognition. Olympus uses the Sphinx 
decoding engine (Huang et al, 1992). A recogni-
tion server captures the audio stream, forwards it to 
a set of parallel recognition engines, and collects 
the corresponding recognition results. The set of 
best hypotheses (one from each engine) is then 
forwarded to the language understanding compo-
nent. The recognition engines can also generate n-
best lists, but that process significantly slows down 
the systems and has not been used live. Interfaces 
to connect Sphinx-II and Sphinx-III engines, as 
well as a DTMF (touch-tone) decoder to the recog-
nition server are currently available. The individual 
recognition engines can use either n-gram- or 
grammar-based language models. Dialog-state 
specific as well as class-based language models are 
supported, and tools for constructing language and 
acoustic models from data are readily available. 
Most of the Olympus systems described in the next 
section use two gender-specific Sphinx-II recog-
nizers in parallel. Other parallel decoder configura-
tions can also be created and used.  
Language understanding is performed by 
Phoenix, a robust semantic parser (Ward and Issar, 
33
1994). Phoenix uses a semantic grammar to parse 
the incoming set of recognition hypotheses. This 
grammar is assembled by concatenating a set of 
reusable grammar rules that capture domain-
independent constructs like [Yes], [No], [Help], 
[Repeat], and [Number], with a set of domain-
specific grammar rules authored by the system de-
veloper. For each recognition hypothesis the output 
of the parser consists of a sequence of slots con-
taining the concepts extracted from the utterance.  
Confidence annotation. From Phoenix, the set 
of parsed hypotheses is passed to Helios, the con-
fidence annotation component. Helios uses features 
from different knowledge sources in the system 
(e.g., recognition, understanding, dialog) to com-
pute a confidence score for each hypothesis. This 
score reflects the probability of correct understand-
ing, i.e. how much the system trusts that the cur-
rent semantic interpretation corresponds to the 
user?s intention. The hypothesis with the highest 
score is forwarded to the dialog manager.  
Dialog management. Olympus uses the Raven-
Claw dialog management framework (Bohus and 
Rudnicky, 2003). In a RavenClaw-based dialog 
manager, the domain-specific dialog task is repre-
sented as a tree whose internal nodes capture the 
hierarchical structure of the dialog, and whose 
leaves encapsulate atomic dialog actions (e.g., ask-
ing a question, providing an answer, accessing a 
database). A domain-independent dialog engine 
executes this dialog task, interprets the input in the 
current dialog context and decides which action to 
engage next. In the process, the dialog manager 
may exchange information with other domain-
specific agents (e.g., application back-end, data-
base access, temporal reference resolution agent). 
Language generation. The semantic output of 
the dialog manager is sent to the Rosetta template-
based language generation component, which pro-
duces the corresponding surface form. Like the 
Phoenix grammar, the language generation tem-
plates are assembled by concatenating a set of pre-
defined, domain-independent templates, with 
manually authored task-specific templates.  
Speech synthesis. The prompts are synthesized 
by the Kalliope speech synthesis module. Kalliope 
can be currently configured to use Festival (Black 
and Lenzo, 2000), which is an open-source speech 
synthesis system, or Cepstral Swift (Cepstral 
2005), a commercial engine. Finally, Kalliope also 
supports the SSML markup language.  
Other components. The various components 
briefly described above form the core of the Olym-
pus dialog system framework. Additional compo-
nents have been created throughout the 
development of various systems, and, given the 
modularity of the architecture, can be easily re-
used. These include a telephony component, a text 
Parsing 
PHOENIX 
Recognition 
Server 
Lang. Gen 
ROSETTA 
Synthesis 
KALLIOPE 
? 
SPHINX SPHINX 
SPHINX 
Confidence 
HELIOS 
HUB 
Text I/O 
TTYSERVER 
Application 
Back-end 
Dialog. Mgr. 
RAVENCLAW 
Date-Time 
resolution 
Process 
Monitor 
Until what time 
would you like  
the room? 
{request end_time} 
Figure 1. The Olympus dialog system reference architecture (a typical system) 
two p.m. [time=2pm] [time=2pm]/0.65 
34
input-and-output interface, and a temporal refer-
ence resolution agent that translates complex date-
time expressions (including relative references, 
holidays, etc.) into a canonical form. Recently, a 
Jabber interface was implemented to support inter-
actions via the popular GoogleTalk internet mes-
saging system. A Skype speech client component 
is also available.  
Data Analysis. Last but not least, a variety of 
tools for logging, data processing and data ana-
lytics are also available as part of the framework. 
These tools have been used for a wide variety of 
tasks ranging from system monitoring, to trends 
analysis, to training of internal models. 
A key characteristic shared by all the Olympus 
components is the clear separation between do-
main-independent programs and domain-specific 
resources. This decoupling promotes reuse and 
lessens the system development effort. To build a 
new system, one can focus simply on developing 
resources (e.g., language model, grammar, dialog 
task specification, generation templates) without 
having to do any programming. On the other hand, 
since all components are open-source, any part of 
the system can be modified, for example to test 
new algorithms or compare approaches. 
4 Systems 
To date, the Olympus framework has been used to 
successfully build and deploy several spoken dia-
log systems spanning different domains and inter-
action types (see Table 1). Given the limited space, 
we discuss only three of these systems in a bit 
more detail: Let?s Go!, LARRI, and TeamTalk. 
More information about the other systems can be 
found in (RavenClaw-Olympus, 2007). 
4.1 Let?s Go! 
The Let?s Go! Bus Information System (Raux et al
2005; 2006) is a telephone-based spoken dialog 
system that provides access to bus schedules. In-
teraction with the system starts with an open 
prompt, followed by a system-directed phase 
where the user is asked the missing information. 
Each of the three or four pieces of information 
provided (origin, destination, time of travel, and 
optional bus route) is explicitly confirmed. The 
system knows 12 bus routes, and about 1800 place 
names. 
Originally developed as an in-lab research sys-
tem, Let?s Go! has been open to the general public 
since March, 2005. Outside of business hours, calls 
to the bus company are transferred to Let?s Go!, 
providing a constant flow of genuine dialogs 
(about 40 calls per weeknight and 70 per weekend 
night). As of March, 2007, a corpus of about 
30,000 calls to the system has been collected and 
partially transcribed and annotated. In itself, this 
publicly available corpus constitutes a unique re-
source for the community. In addition, the system 
itself has been modified for research experiments 
(e.g., Raux et al, 2005, Bohus et al, 2006). Be-
tween-system studies have been conducted by run-
ning several versions of the system in parallel and 
picking one at random for every call. We have re-
cently opened this system to researchers from other 
groups who wish to conduct their own experi-
ments. 
4.2 LARRI 
LARRI (Bohus and Rudnicky, 2002a) is a multi-
modal system for support of maintenance and re-
pair activities for F/A-18 aircraft mechanics. The 
system implements an Interactive Electronic Tech-
nical Manual.  
LARRI integrates a graphical user interface for 
easy visualization of dense technical information 
(e.g., instructions, schematics, video-streams) with 
a spoken dialog system that facilitates information 
access and offers assistance throughout the execu-
tion of procedural tasks. The GUI is accessible via 
a translucent head-worn display connected to a 
wearable client computer. A rotary mouse (dial) 
provides direct access to the GUI elements.  
After an initial log-in phase, LARRI guides the 
user through the selected task, which consists of a 
sequence of steps containing instructions, option-
ally followed by verification questions. Basic steps 
can include animations or short video sequences 
that can be accessed by the user through the GUI 
or through spoken commands. The user can also 
take the initiative and access the documentation, 
either via the GUI or by simple commands such as 
?go to step 15? or ?show me the figure?. 
The Olympus architecture was easily adapted 
for this mobile and multi-modal setting. The wear-
able computer hosts audio input and output clients, 
as well as the graphical user interface. The Galaxy 
hub architecture allows us to easily connect these 
35
components to the rest of the system, which runs 
on a separate server computer. The rotary-mouse 
events from the GUI are rendered as semantic in-
puts and are sent to Helios which in turn forwards 
the corresponding messages to the dialog manager.  
4.3 TeamTalk 
TeamTalk (Harris et al, 2005) is a multi-modal 
interface that facilitates communication between a 
human operator and a team of heterogeneous ro-
bots, and is designed for a multi-robot-assisted 
treasure-hunt domain. The human operator uses 
spoken language in concert with pen-gestures on a 
shared live map to elicit support from teams of ro-
bots. This support comes in the forms of mapping 
unexplored areas, searching explored areas for ob-
jects of interest, and leading the human to said ob-
jects. TeamTalk has been built as a fully functional 
interface to real robots, including the Pioneer 
P2DX and the Segway RMP. In addition, it can 
interface with virtual robots within the high-
fidelity USARSim (Balakirsky et al, 2006) simula-
tion environment. TeamTalk constitutes an excel-
lent platform for multi-agent dialog research. 
To build TeamTalk, we had to address two chal-
lenges to current architecture. The multi-
participant nature of the interaction required multi-
ple dialog managers; the live map with pen-
gestured references required a multi-modal integra-
tion. Again, the flexibility and transparency of the 
Olympus framework allowed for relatively simple 
solutions to both of these challenges. To accom-
modate multi-participant dialog, each robot in the 
domain is associated with its own RavenClaw-
based dialog manager, but all robots share the 
other Olympus components: speech recognition, 
language understanding, language generation and 
speech synthesis. To accommodate the live map 
GUI, a Galaxy server was built in Java that could 
send the user?s inputs to Helios and receive outputs 
from RavenClaw. 
5 Research 
The Olympus framework, along with the systems 
developed using it, provides a robust basis for re-
search in spoken language interfaces. In this sec-
tion, we briefly outline three current research 
efforts supported by this architecture. Information 
about other supported research can be found in 
(RavenClaw-Olympus, 2007). 
5.1 Error handling  
A persistent and important problem in today?s spo-
ken language interfaces is their lack of robustness 
when faced with understanding errors. This prob-
lem stems from current limitations in speech rec-
ognition, and appears across most domains and 
interaction types. In the last three years, we con-
ducted research aimed at improving robustness in 
spoken language interfaces by: (1) endowing them 
with the ability to accurately detect errors, (2) de-
System name Domain / Description Genre 
RoomLine 
(Bohus and Rudnicky 2005) 
telephone-based system that provides support for conference 
room reservation and scheduling within the School of Com-
puter Science at CMU. 
information access (mixed 
initiative) 
Let?s Go! Public 
(Raux et al2005) 
telephone-based system that provides access to bus schedule 
information in the greater Pittsburgh area. 
information access 
(system initiative) 
LARRI 
(Bohus and Rudnicky 2002) 
multi-modal system that provides assistance to F/A-18 aircraft 
personnel during maintenance tasks. 
multi-modal task guidance 
and procedure browsing 
Intelligent Procedure  
Assistant 
(Aist et al2002) 
early prototype for a multi-modal system aimed at providing 
guidance and support to the astronauts on the International 
Space Station during the execution of procedural tasks and 
checklists. 
multi-modal task guidance 
and procedure browsing 
TeamTalk 
(Harris et al2005) 
multi-participant spoken language command-and-control inter-
face for a team of robots in the treasure-hunt domain. 
multi-participant command-
and-control 
VERA telephone-based taskable agent that can be instructed to de-liver messages to a third party and make wake-up calls. 
voice mail / message deliv-
ery 
Madeleine text-based dialog system for medical diagnosis. diagnosis 
ConQuest 
(Bohus et al2007) 
telephone-based spoken dialog system that provides confer-
ence schedule information. 
information access 
(mixed-initiative) 
RavenCalendar 
(Stenchikova et al2007). 
multimodal dialog system for managing personal calendar 
information, such as meetings, classes, appointments and 
reminders (uses Google Calendar as a back-end)  
information access and 
scheduling 
 Table 1. Olympus-based spoken dialog systems (shaded cells indicate deployed systems) 
36
veloping a rich repertoire of error recovery strate-
gies and (3) developing scalable, data-driven ap-
proaches for building error recovery policies1. Two 
of the dialog systems from Table 1 (Let?s Go! and 
RoomLine) have provided a realistic experimental 
platform for investigating these issues and evaluat-
ing the proposed solutions.   
With respect to error detection, we have devel-
oped tools for learning confidence annotation 
models by integrating information from multiple 
knowledge sources in the system (Bohus and Rud-
nicky, 2002b). Additionally, Bohus and Rudnicky 
(2006) proposed a data-driven approach for con-
structing more accurate beliefs in spoken language 
interfaces by integrating information across multi-
ple turns in the conversation. Experiments with the 
RoomLine system showed that the proposed belief 
updating models led to significant improvements 
(equivalent with a 13.5% absolute reduction in 
WER) in both the effectiveness and the efficiency 
of the interaction.  
With respect to error recovery strategies, we 
have developed and evaluated a large set of strate-
gies for handling misunderstandings and non-
understandings (Bohus and Rudnicky, 2005). The 
strategies are implemented in a task-decoupled 
manner in the RavenClaw dialog management 
framework. 
Finally, in (Bohus et al, 2006) we have pro-
posed a novel online-learning based approach for 
building error recovery policies over a large set 
of non-understanding recovery strategies. An em-
pirical evaluation conducted in the context of the 
Let?s Go! system showed that the proposed ap-
proach led to a 12.5% increase in the non-
understanding recovery rate; this improvement was 
attained in a relatively short (10-day) time period.  
The models, tools and strategies developed 
throughout this research can and have been easily 
reused in other Olympus-based systems. 
5.2 Multi-participant conversation  
Conversational interfaces are generally built for 
one-on-one conversation. This has been a workable 
assumption for telephone-based systems, and a 
useful one for many single-purpose applications. 
However this assumption will soon become 
strained as a growing collection of always-
                                                          
1
 A policy specifies how the system should choose between 
different recovery strategies at runtime.  
available agents (e.g., personal trainers, pedestrian 
guides, or calendar systems) and embodied agents 
(e.g., appliances and robots) feature spoken lan-
guage interfaces. When there are multiple active 
agents that wish to engage in spoken dialog, new 
issues arise. On the input side, the agents need to 
be able to identify the addressee of any given user 
utterance. On the output side, the agents need to 
address the problem of channel contention, i.e., 
multiple participants speaking over each other. 
Two architectural solutions can be envisioned: 
(1) the agents share a single interface that under-
stands multi-agent requirements, or (2) each agent 
uses its own interface and handles multi-participant 
behavior. Agents that provide different services 
have different dialog requirements, and we believe 
this makes a centralized interface problematic. Fur-
thermore, the second solution better fits human 
communication behavior and therefore is likely to 
be more natural and habitable.  
TeamTalk is a conversational system that fol-
lows the second approach: each robot has its own 
dialog manager. Processed user inputs are sent to 
all dialog managers in the system; each dialog 
manager decides based on a simple algorithm 
(Harris et al, 2004) whether or not the current in-
put is addressed to it. If so, an action is taken. Oth-
erwise the input is ignored; it will be processed and 
responded to by another robot. On the output side, 
to address the channel contention problem, each 
RavenClaw output message is augmented with in-
formation about the identity of the robot that gen-
erated it. The shared synthesis component queues 
the messages and plays them back sequentially 
with the corresponding voice. 
We are currently looking into two additional 
challenges related to multi-participant dialog. We 
are interested in how to address groups and sub-
groups in addition to individuals of a group, and 
we are also interested in how to cope with multiple 
humans in addition to multiple agents. Some ex-
periments investigating solutions to both of these 
issues have been conducted. Analysis of the results 
and refinements of these methods are ongoing. 
5.3 Timing and turn-taking  
While a lot of research has focused on higher lev-
els of conversation such as natural language under-
standing and dialog planning, low-level inter-
actional phenomena such as turn-taking have not 
37
received as much attention. As a result, current 
systems either constrain the interaction to a rigid 
one-speaker-at-a-time style or expose themselves 
to interactional problems such as inappropriate 
delays, spurious interruptions, or turn over-taking 
(Raux et al, 2006). To a large extent, these issues 
stem from the fact that in common dialog architec-
tures, including Olympus, the dialog manager 
works asynchronously from the real world (i.e., 
utterances and actions that are planned are as-
sumed to be executed instantaneously). This means 
that user barge-ins and backchannels are often in-
terpreted in an incorrect context, which leads to 
confusion, unexpected user behavior and potential 
dialog breakdowns. Additionally, dialog systems? 
low-level interactional behavior is generally the 
result of ad-hoc rules encoded in different compo-
nents that are not precisely coordinated. 
In order to investigate and resolve these is-
sues, we are currently developing version 2 of the 
Olympus framework. In addition to all the compo-
nents described in this paper, Olympus 2 features 
an Interaction Manager which handles the precise 
timing of events perceived from the real world 
(e.g., user utterances) and of system actions (e.g., 
prompts). By providing an interface between the 
actual conversation and the asynchronous dialog 
manager, Olympus 2 allows a more reactive behav-
ior without sacrificing the powerful dialog man-
agement features offered by RavenClaw. Olympus 
2 is designed so that current Olympus-based sys-
tems can be upgraded with minimal effort.  
This novel architecture, initially deployed in 
the Let?s Go system, will enable research on turn-
taking and other low-level conversational phenom-
ena. Investigations within the context of other ex-
isting systems, such as LARRI and TeamTalk, will 
uncover novel challenges and research directions.  
6 Discussion and conclusion 
The primary goal of the Olympus framework is to 
enable research that leads to technological and sci-
entific advances in spoken language interfaces.  
Olympus is however by no means a singular ef-
fort. Several other toolkits for research and devel-
opment are available to the community. They 
differ on a number of dimensions, such as objec-
tives, scientific underpinnings, as well as techno-
logical and implementation aspects. Several 
toolkits, both commercial, e.g., TellMe, BeVocal, 
and academic, e.g., Ariadne (2007), SpeechBuilder 
(Glass et al, 2004), and the CSLU toolkit (Cole, 
1999), are used for rapid development. Some, e.g., 
CSLU and SpeechBuilder, have also been used for 
educational purposes. And yet others, such as 
Olympus, GALATEEA (Kawamoto et al, 2002) 
and DIPPER (Bos et al, 2003) are primarily used 
for research. Different toolkits rely on different 
theories and dialog representations: finite-state, 
slot-filling, plan-based, information state-update. 
Each toolkit balances tradeoffs between complex-
ity, ease-of-use, control, robustness, flexibility, etc. 
We believe the strengths of the Olympus 
framework lie not only in its current components, 
but also in its open, transparent, and flexible na-
ture. As we have seen in the previous sections, 
these characteristics have allowed us to develop 
and deploy practical, real-world systems operating 
in a broad spectrum of domains. Through these 
systems, Olympus provides an excellent basis for 
research on a wide variety of spoken dialog issues. 
The modular construction promotes the transfer 
and reuse of research contributions across systems.  
While desirable, an in-depth understanding of 
the differences between all these toolkits remains 
an open question. We believe that an open ex-
change of experiences and resources across toolkits 
will create a better understanding of the current 
state-of-the-art, generate new ideas, and lead to 
better systems for everyone. Towards this end, we 
are making the Olympus framework, as well as a 
number of systems and dialog corpora, freely 
available to the community. 
Acknowledgements 
We would like to thank all those who have brought 
contributions to the components underlying the 
Olympus dialog system framework. Neither Olym-
pus nor the dialog systems discussed in this paper 
would have been possible without their help. We 
particularly wish to thank Alan W Black for his 
continued support and advice. Work on Olympus 
components and systems was supported in part by 
DARPA, under contract NBCH-D-03-0010, Boe-
ing, under contract CMU-BA-GTA-1, and the US 
National Science Foundation under grant number 
0208835. Any opinions, findings, and conclusions 
or recommendations expressed in this material are 
those of the authors and do not necessarily reflect 
the views of the funding agencies.  
38
References  
Aist, G., Dowding, J., Hockey, B.A., Rayner, M., 
Hieronymus, J., Bohus, D., Boven, B., Blaylock, N., 
Campana, E., Early, S., Gorrell, G., and Phan, S., 
2003. Talking through procedures: An intelligent 
Space Station procedure assistant, in Proc. of EACL-
2003, Budapest, Hungary 
Ariadne, 2007, The Ariadne web-site, as of January 
2007, http://www.opendialog.org/. 
Balakirsky, S., Scrapper, C., Carpin, S., and Lewis, M. 
2006. UsarSim: providing a framework for multi-
robot performance evaluation, in Proc. of PerMIS. 
Black, A. and Lenzo, K., 2000. Building Voices in the 
Festival Speech System, http://festvox.org/bsv/, 2000. 
Bohus, D., Grau Puerto, S., Huggins-Daines, D., Keri, 
V., Krishna, G., Kumar, K., Raux, A., Tomko, S., 
2007. Conquest ? an Open-Source Dialog System for 
Conferences, in Proc. of HLT 2007, Rochester, USA. 
Bohus, D., Langner, B., Raux, A., Black, A., Eskenazi, 
M., Rudnicky, A.  2006.  Online Supervised Learning 
of Non-understanding Recovery Policies, in Proc. of 
SLT-2006, Aruba.  
Bohus, D., and Rudnicky, A.  2006.  A K-hypotheses + 
Other Belief Updating Model, in Proc. of the AAAI 
Workshop on Statistical and Empirical Methods in 
Spoken Dialogue Systems, 2006. 
Bohus, D., and Rudnicky, A.,  2005.  Sorry I didn?t 
Catch That: An Investigation of Non-understanding 
Errors and Recovery Strategies, in Proc. of SIGdial-
2005, Lisbon, Portugal. 
Bohus, D., and Rudnicky, A., 2003. RavenClaw: Dialog 
Management Using Hierarchical Task Decomposi-
tion and an Expectation Agenda, in Proc. of Eu-
rospeech 2003, Geneva, Switzerland. 
Bohus, D., and Rudnicky, A., 2002a. LARRI: A Lan-
guage-based Maintenance and Repair Assistant, in 
Proc. of IDS-2002, Kloster Irsee, Germany. 
Bohus, D., and Rudnicky, A., 2002b. Integrating Multi-
ple Knowledge Sources in the CMU Communicator 
Dialog System, Technical Report CMU-CS-02-190. 
Bos, J., Klein, E., Lemon, O., and Oka, T., 2003. 
DIPPER: Description and Formalisation of an In-
formation-State Update Dialogue System Architec-
ture, in Proc. of SIGdial-2003, Sapporo, Japan 
Cepstral, LLC, 2005. SwiftTM: Small Footprint Text-to-
Speech Synthesizer, http://www.cepstral.com. 
Cole, R., 1999. Tools for Research and Education in 
Speech Science, in Proc. of the International Confer-
ence of Phonetic Sciences, San Francisco, USA. 
Glass, J., Weinstein, E., Cyphers, S., Polifroni, J., 2004. 
A Framework for Developing Conversational Inter-
faces, in Proc. of CADUI, Funchal, Portugal. 
Harris, T. K., Banerjee, S., Rudnicky, A., Sison, J. 
Bodine, K., and Black, A. 2004. A Research Platform 
for Multi-Agent Dialogue Dynamics, in Proc. of The 
IEEE International Workshop on Robotics and Hu-
man Interactive Communications, Kurashiki, Japan. 
Harris, T. K., Banerjee, S., Rudnicky, A. 2005. Hetero-
genous Multi-Robot Dialogues for Search Tasks, in 
AAAI Spring Symposium: Dialogical Robots, Palo 
Alto, California. 
Huang, X., Alleva, F., Hon, H.-W., Hwang, M.-Y., Lee, 
K.-F. and Rosenfeld, R., 1992. The SPHINX-II 
Speech Recognition System: an overview, in Com-
puter Speech and Language, 7(2), pp 137-148, 1992. 
Kawamoto, S.,  Shimodaira, H., Nitta, T., Nishimoto, 
T., Nakamura, S., Itou, K., Morishima, S., Yotsukura, 
T., Kai, A., Lee, A., Yamashita, Y., Kobayashi, T., 
Tokuda, K., Hirose, K., Minematsu, N., Yamada, A., 
Den, Y., Utsuro, T., and Sagayama, S., 2002. Open-
source software for developing anthropomorphic 
spoken dialog agent, in Proc. of PRICAI-02, Interna-
tional Workshop on Lifelike Animated Agents. 
Raux, A., Langner, B., Bohus, D., Black, A., and Eske-
nazi, M.  2005, Let's Go Public! Taking a Spoken 
Dialog System to the Real World, in Proc. of Inter-
speech 2005, Lisbon, Portugal. 
Raux, A., Bohus, D., Langner, B., Black, A., and Eske-
nazi, M. 2006 Doing Research on a Deployed Spoken 
Dialogue System: One Year of Let's Go! Experience, 
in Proc. of Interspeech 2006, Pittsburgh, USA. 
RavenClaw-Olympus web page, as of January 2007: 
http://www.ravenclaw-olympus.org/. 
Rudnicky, A., Thayer, E., Constantinides, P., Tchou, C., 
Shern, R., Lenzo, K., Xu W., and Oh, A., 1999. Cre-
ating natural dialogs in the Carnegie Mellon Com-
municator system, in Proc. of Eurospeech 1999. 
Seneff, S., Hurley, E., Lau, R., Pao, C., Schmid, P., and 
Zue V. 1998 Galaxy-II: A reference architecture for 
conversational system development, in Proc. of 
ICSLP98, Sydney, Australia. 
Stenchikova, S., Mucha, B., Hoffman, S., Stent, A., 
2007. RavenCalendar: A Multimodal Dialog System 
for Managing A Personal Calendar, in Proc. of HLT 
2007, Rochester, USA.  
Ward, W., and Issar, S., 1994. Recent improvements in 
the CMU spoken language understanding system, in 
Proc. of the ARPA Human Language Technology 
Workshop, pages 213?216, Plainsboro, NJ. 
39
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 1?10,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Optimizing Endpointing Thresholds using Dialogue
Features in a Spoken Dialogue System
Antoine Raux and Maxine Eskenazi
{antoine,max}@cs.cmu.edu
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
Abstract
This paper describes a novel algorithm to dy-
namically set endpointing thresholds based on
a rich set of dialogue features to detect the end
of user utterances in a dialogue system. By
analyzing the relationship between silences in
user?s speech to a spoken dialogue system and
a wide range of automatically extracted fea-
tures from discourse, semantics, prosody, tim-
ing and speaker characteristics, we found that
all features correlate with pause duration and
with whether a silence indicates the end of the
turn, with semantics and timing being the most
informative. Based on these features, the pro-
posed method reduces latency by up to 24%
over a fixed threshold baseline. Offline evalu-
ation results were confirmed by implementing
the proposed algorithm in the Let?s Go system.
1 Introduction
1.1 Responsiveness in Dialogue
Although the quality of speech technologies has im-
proved drastically and spoken interaction with ma-
chines is becoming a part of the everyday life of
many people, dialogues with artificial agents still
fall far short of their human counterpart in terms of
both comfort and efficiency. Besides lingering prob-
lems in speech recognition and understanding, Ward
et al(Ward et al, 2005) identified turn-taking is-
sues, specifically responsiveness, as important short-
comings. Dialogues with artificial agents are typi-
cally rigid, following a strict one-speaker-at-a-time
structure with significant latencies between turns.
In a previous paper, we concurred with these find-
ings when analyzing issues with the Let?s Go system
(Raux et al, 2006). In contrast, empirical studies
of conversation have shown that human-human dia-
logues commonly feature swift exchanges with lit-
tle or no gap between turns, or even non-disruptive
overlap (Jaffe and Feldstein, 1970; Sacks et al,
1974). According to Conversation Analysis and
psycholinguistic studies, responsiveness in human
conversations is possible because participants in the
conversation exchange cues indicating when a turn
might end, and are able to anticipate points at which
they can take over the floor smoothly. Much re-
search has been devoted to finding these cues, lead-
ing to the identification of many aspects of language
and dialogue that relate to turn-taking behavior, in-
cluding syntax (Sacks et al, 1974; Ford and Thomp-
son, 1996; Furo, 2001), prosody (Duncan, 1972;
Orestro?m, 1983; Chafe, 1992; Ford and Thompson,
1996; Koiso et al, 1998; Furo, 2001), and seman-
tics (Orestro?m, 1983; Furo, 2001). However, re-
garding this last aspect, Orestrom notes about his
corpus that ?there is no simple way to formaliz-
ing a semantic analysis of this conversational mate-
rial?. This difficulty in formalizing higher levels of
conversation might explain the relatively low inter-
est that conversational analysts have had in seman-
tics and discourse. Yet, as conversational analysts
focused on micro-levels of dialogue such as turn-
taking, computational linguists uncovered and for-
malized macro-level dialogue structure and devised
well-defined representations of semantics for at least
some forms of dialogues (Allen and Perrault, 1980;
Grosz and Sidner, 1986; Clark, 1996), which have in
turn been implemented in spoken dialogue systems
(Rich and Sidner, 1998; Allen et al, 2005).
1
1.2 Current Approaches to Turn-Taking in
Spoken Dialogue Systems
Unfortunately, while socio- and psycho-linguists re-
vealed the complexity of conversational turn-taking
behavior, designers of practical spoken dialogue sys-
tems have stuck to a simplistic approach to end-of-
turn detection (hereafter endpointing). Typically, si-
lences in user speech are detected using a low-level
Voice Activity Detector (VAD) and a turn is consid-
ered finished once a silence lasts longer than a fixed
threshold. This approach has the advantage of being
simple, only relying on easily computable low-level
features. However, it leads to suboptimal behavior
in many instances. First, False Alarms (FA) hap-
pen when a pause lasts longer than the threshold and
gets wrongly classified as a gap1. Second, latency
occurs at every gap, because the system must wait
for the duration of the threshold before classifying a
silence as gap. When setting the threshold, system
designers must consider the trade-off between these
two issues: setting a low threshold reduces latency
but increases FA rate, while setting a high threshold
reduces FA rate but increases latency.
To help overcome the shortcomings of the single-
threshold approach, several researchers have pro-
posed to exploit various features. Sato et al(Sato
et al, 2002) used decision trees to classify pauses
longer than 750 ms as gap or pause. By using fea-
tures from semantics, syntax, dialogue state, and
prosody, they were able to improve the classification
accuracy from a baseline of 76.2% to 83.9%. While
this important study shows encouraging results on
the value of using various sources of information in
a dialogue system, the proposed approach (classify-
ing long silences) is not completely realistic (what
happens when a gap is misclassified as a pause?) and
does not attempt to optimize latency. An extension
to this approach was proposed in (Takeuchi et al,
2004), in which a turn-taking decision is made every
100 ms during pauses. However, in this latter work
the features are limited to timing, prosody, and syn-
tax (part-of-speech). Also the reported classification
results, with F-measures around 50% or below do
not seem to be sufficient for practical use.
1We use the terminology from (Sacks et al, 1974) where a
pause is a silence within a turn while a gap is a silence between
turns. We use the term silence to encompass both types.
Similarly, Ferrer and her colleagues (Ferrer et al,
2003) proposed the use of multiple decision trees,
each triggered at a specific time in the pause, to de-
cide to either endpoint or defer the decision to the
next tree, unless the user resumes speaking. Using
features like vowel duration or pitch for the region
immediately preceding the silence, combined with a
language model that predicts gaps based on the pre-
ceding words, Ferrer et alare able shorten latency
while keeping the FA rate constant. On a corpus
of recorded spoken dialogue-like utterances (ATIS),
they report reductions of up to 81% for some FA
rates. While very promising, this approach has sev-
eral disadvantages. First it relies on a small set of
possible decision points for each pause, preventing
fine optimization between them. Second, the trees
are trained on increasingly smaller datasets requir-
ing smoothing of the tree scores to compensate for
poor training of the later trees (which are trained
on increasingly small subsets of pauses from the
training set). Finally, and perhaps most importantly,
these authors have investigated prosodic and lexical
features, but not other aspects of dialogue, such as
discourse structure, timing, and semantics.
In this paper, we propose a new approach to end-
pointing that directly optimizes thresholds using au-
tomatically extracted dialogue features ranging from
discourse to timing and prosody. Section 2 out-
lines the proposed algorithm. Section 3 describes
the analysis of the relationship between silences and
a wide range of features available to a standard spo-
ken dialogue system (hereafter dialogue features).
Evaluation results, both offline and in the deployed
Let?s Go system are given in Section 4.
2 Dynamic Endpointing Threshold
Decision Trees
2.1 Overview
One issue with current approaches to endpointing
is that they rely on binary gap/pause classifiers and
the relationship between optimizing for classifica-
tion accuracy vs optmizing to minimize latency is
unclear. Also, the performance we obtained when
applying classification-based approaches to the Let?s
Go data was disappointing. The accuracy of the clas-
sifiers was not sufficient for practical purposes, even
with the improvements proposed by (Ferrer et al,
2
2003). We hypothesize that the discrepancy between
these results and the good performances reported by
others is due to the noisiness of the Let?s Go data
(see Section 3.1.1). To overcome these issues, we
propose a method that directly optimizes endpoint-
ing thresholds using a two-stage process. First, si-
lences are clustered based on dialogue features so
as to create groups of silences with similar proper-
ties. Second, a single threshold is set for each clus-
ter, so as to minimize the overall latency at a given
false alarm rate. The result of the training process
is thus a decision tree on dialogue features that con-
tains thresholds at its leaves. At runtime, every time
a silence is detected, the dialogue system runs the
decision tree and sets its endpointing threshold ac-
cordingly. The following sections describe the two
training stages.
2.2 Feature-based Silence Clustering
The goal of the first stage of training is to clus-
ter silences with a similar FA rate/latency trade-
off. The intuition is that we would like to generate
low-threshold clusters, which contain mostly gaps
and short pauses, and clusters where long pauses
would be concentrated with no or very few gaps,
allowing to set high thresholds that reduce cut-in
rate without hurting overall latency. We used a
standard top-down clustering algorithm that exhaus-
tively searches binary splits of the data based on fea-
ture values. The split that yields the minimal overall
cost is kept, where the cost Cn of cluster Kn is de-
fined by the following function:
Cn = Gn ?
?
1
|K|
?
p?K
Duration(p)2 (1)
where Gn the number of gaps in Kn and
Duration(p) the duration of a pause p, set to zero
for gaps. While other cost functions are possible, the
intuition behind this formula is that it captures both
the cluster?s gap ratio (first factor) and its pause du-
ration distribution (second factor: root mean square
of pause duration). The splitting process is repeated
recursively until the reduction in cost between the
original cost and the sum of the costs of the two split
clusters falls below a certain threshold. By minimiz-
ing C(K), the clustering algorithm will find ques-
tions that yield clusters with either a small Gn, i.e.
mostly pauses, or a small root mean square pause
duration. Ultimately, at the leaves of the tree are sets
of silences that will share the same threshold.
2.3 Cluster Threshold Optimization
Given the clusters generated by the first phase, the
goal of the second phase is to find a threshold for
each cluster so that the overall latency is minimized
at a given FA rate. Under the assumption that pause
durations follow an exponential distribution, which
is supported by previous work and our own data (see
Section 3.2), we show in Figure 3 in appendix that
there is a unique set of thresholds that minimizes la-
tency and that the threshold for any cluster n is given
by:
?n =
?n ? log(?n ?
E??n
P
?n
)
Gn
(2)
where ?n and ?n can be estimated from the data.
3 Silences and Dialogue Features
3.1 Overview of the Data
3.1.1 The Let?s Go Corpus
Let?s Go is a telephone-based spoken dialogue
system that provides bus schedule information for
the Pittsburgh metropolitan area. It is built on the
Olympus architecture (Bohus et al, 2007), using the
RavenClaw dialogue management framework, and
the Apollo interaction manager (Raux et al, 2007)
as core components. Outside of business hours
callers to the bus company?s customer service are
offered the option to use Let?s Go. All calls are
recorded and extensively logged for further analy-
sis. The corpus used for this study was collected
between December 26, 2007 and January 25, 2008,
with a total of 1326 dialogues, and 18013 user turns.
Of the calls that had at least 4 user turns, 73% were
complete, meaning that the system provided some
schedule information to the user.
While working on real user data has its advan-
tages (large amounts of data, increased validity of
the results), it also has its challenges. In the case of
Let?s Go, users call from phones of varying quality
(cell phones and landlines), often with background
noises such as cars, infant cries, loud television sets,
etc. The wide variability of the acoustic conditions
makes any sound processing more prone to error
3
than on carefully recorded corpora. For example, as
reported in (Raux et al, 2005), the original speech
recognizer had been found to yield a 17% word error
rate on a corpus of dialogues collected by recruit-
ing subjects to call the system from an office. On
the live Let?s Go data, that same recognizer had a
68% WER. After acoustic and language model re-
training/adaptation, that number was brought down
to about 30% but it is still a testimony to the diffi-
culty of obtaining robust features, particularly from
acoustics.
3.1.2 Correcting Runtime Endpointing Errors
Let?s Go uses a GMM-based VAD trained on pre-
viously transcribed dialogues. Endpointing deci-
sions are based on a fixed 700 ms threshold on the
duration of the detected silences. One issue when
analyzing pause distributions from the corpus is that
observed user behavior was affected by system?s be-
havior at runtime. Most notably, because of the fixed
threshold, no recorded pause lasts more than 700 ms.
To compensate for that, we used a simple heuristic
to rule some online endpointing decisions as erro-
neous. If a user turn is followed within 1200 ms by
another user turn, we consider these two turns to be
in fact a single turn, unless the first turn was a user
barge-in. This heuristic was established by hand-
labeling 200 dialogues from a previous corpus with
endpointing errors (i.e. each turn was annotated as
correctly or incorrectly endpointed). On this dataset,
the heuristic has a precision of 70.6% and a recall of
75.5% for endpointing errors. Unless specified, all
subsequent results are based on this modified cor-
pus.
3.2 Turn-Internal Pause Duration Distribution
Overall there were 9563 pauses in the corpus, which
amounts to 0.53 pauses per turn. The latency / FA
rate trade-off for the corpus is plotted in Figure 1.
This curve follows an exponential function (the R2
on the linear regression of latency on Log(FA) is
0.99). This stems from the fact that pause duration
approximately follows an exponential distribution,
which has been observed by others in the past (Jaffe
and Feldstein, 1970; Lennes and Anttila, 2002).
One consequence of the exponential-like distribu-
tion is that short pauses strongly dominate the distri-
bution. We decided to exclude silences shorter than
Figure 1: Overall False Alarm / Latency trade-off in the
Let?s Go corpus. The dashed line represents a fitted curve
of the form FA = e?+??Latency .
200 ms from most of the following analysis for two
reasons: 1) they are more prone to voice activity
detection errors or short non-pause silences within
speech (e.g. unvoiced stop closure), and 2) in order
to apply the results found here to online endpointing
by the system, some amount of time is required to
detect the silence and compute necessary features,
making endpointing decisions on such very short si-
lences impractical. Once short silences have been
excluded, there are 3083 pauses in the corpus, 0.17
per turn.
3.3 Relationship Between Dialogue Features
and Silence Distributions
3.3.1 Statistical Analysis
In order to get some insight into the interaction
of the various aspects of dialogue and silence char-
acteristics, we investigated a number of features au-
tomatically extracted from the dialogue recordings
and system logs. Each feature is used to split the
set of silences into two subsets. For nominal fea-
tures, all possible splits of one value vs all the others
are tested, while for continuous and ordinal features,
we tried a number of thresholds and report the one
that yielded the strongest results. In order to avoid
extreme cases that split the data into one very large
and one very small set, we excluded all splits where
either of the two sets had fewer than 1000 silences.
All the investigated splits are reported in Appendix,
in Table 1 and 2. We compare the two subsets gen-
erated by each possible split in terms of two metrics:
? Gap Ratio (GR), defined as the proportion of
4
gaps among all silences of a given set. We re-
port the absolute difference in GR between the
two sets, and use chi-square in a 2x2 design
(pause vs gap and one subset vs the other) to
test for statistical significance at the 0.01 level,
using Bonferroni correction to compensate for
multiple testings.
? Mean pause duration. The strength of the in-
teraction is shown by the difference in mean
pause duration, and we use Mann Whitney?s
Rank Sum test for statistical significance, again
at the 0.01 level, using Bonferroni correction.
We group features into five categories: discourse,
semantics, prosody, turn-taking, and speaker charac-
teristics, described in the following sections.
3.3.2 Discourse Structure
Discourse structure is captured by the system?s di-
alogue act immediately preceding the current user
turn. In the Let?s Go dialogues, 97.9% of sys-
tem dialogue acts directly preceding user turns are
questions2. Of these, 13% are open questions (e.g.
?What can I do for you??), 39% are closed ques-
tions (e.g. ?Where are you leaving from??) and 46%
are confirmation requests (e.g. ?Leaving from the
airport. Is this correct??)3. There are many more
pauses in user responses to open questions than to
the other types (cf Table 1). One explanation is that
user answers to open questions tend to be longer
(2046 ms on average, to be contrasted with 1268 ms
for turns following closed questions and 819 ms for
responses to confirmation questions). Conversely,
confirmation questions lead to responses with sig-
nificantly fewer pauses. 78% of such turns con-
tained only one word, single YES and NO answers
accounting for 81% of these one-word responses,
which obviously do not lend themselves to pauses.
Discourse context also has an effect on pause dura-
tions, albeit a weak one, with open questions leading
to turns with shorter pauses. One possible explana-
tion for this is that pauses after closed and confirma-
tion questions tend to reflect more hesitations and/or
2The remaining 2.1% belong to other cases such as the user
barging in right after the system utters a statement.
3The high number of confirmations comes from the fact that
Let?s Go is designed to ask the user to explicitly confirm every
concept.
confusion on the user?s side, whereas responses to
open questions also have pauses in the normal flow
of speech.
3.3.3 Semantics
Semantic features are based on partial speech
recognition results and on their interpretation in the
current dialogue context. We use the most recent
recognition hypothesis available at the time when
the silence starts, parse it using the system?s standard
parser and grammar, and match the parse against the
?expectation agenda? that RavenClaw (Bohus and
Rudnicky, 2003) maintains. The expectation level
of a partial utterance indicates how well it fits in the
current dialogue context. A level of 0 means that
the utterance can be interpreted as a direct answer
to the last system prompt (e.g. a ?PLACE? con-
cept as an answer to ?Where are you leaving from??,
a ?YES? or a ?NO? after a confirmation question).
Higher levels correspond to utterances that fit in a
broader dialogue context (e.g. a place name after
the system asks ?Leaving from the airport. Is this
correct??, or ?HELP? in any context). Finally, non-
understandings, which do not match any expecta-
tion, are given a matching level of +?.
Expectation level is strongly related to both fi-
nality and pause duration. Pauses following par-
tial utterances of expectation level 0 are signifi-
cantly more likely to be gaps than those matching
any higher level. Also, very unexpected partial ut-
terances (and non-understandings) contain shorter
pauses than more expected ones. Another indica-
tive feature for finality is the presence of a posi-
tive marker (i.e. a word like ?YES? or ?SURE?) in
the partial utterance. Utterances that contain such a
marker are more likely to be finished than others. In
contrast, the effect of negative markers is not signif-
icant. This can be explained by the fact that nega-
tive responses to confirmation often lead to longer
corrective utterances more prone to pauses. Indeed,
91% of complete utterances that contain a positive
marker are single-word, against 67% for negative
markers.
3.3.4 Prosody
We extracted three types of prosodic features:
acoustic energy of the last vowel, pitch of the last
voiced region, and duration of the last vowel. Vowel
5
location and duration were estimated by performing
phoneme alignment with the speech recognizer. Du-
ration was normalized to account for both vowel and
speaker identity. Energy was computed as the log-
transformed signal intensity on 10ms frames. Pitch
was extracted using the Snack toolkit (Sjolander,
2004), also at 10ms intervals. For both energy and
pitch, the slope of the contour was computed by lin-
ear regression, and the mean value was normalized
by Z-transformation using statistics of the dialogue-
so-far. As a consequence, all threshold values for
means are expressed in terms of standard deviations
from the current speaker?s mean value.
Vowel energy, both slope and mean, yielded the
highest correlation with silence finality, although it
did not rank as high as features from other cate-
gories. As expected, vowels immediately preced-
ing gaps tend to have lower and falling intensity,
whereas rising intensity makes it more likely that the
turn is not finished. On the other hand, extremely
high pitch is a strong cue to longer pauses, but only
happen in 5.6% of the pauses.
3.3.5 Timing
Timing features, available from the Interaction
Manager, provide the strongest cue to finality. The
longer the on-going turn has been, the less likely it is
that the current silence is a gap. This is true both in
terms of time elapsed since the beginning of the ut-
terance and number of pauses observed so far. This
latter feature also correlates well with mean pause
duration, earlier pauses of a turn tending to be longer
than later ones.
3.3.6 Speaker Characteristics
These features correspond to the observed pausal
behavior so far in the dialogue. The idea is that dif-
ferent speakers follow different patterns in the way
they speak (and pause), and that the system should
be able to learn these patterns to anticipate future
behavior. Specifically, we look at the mean num-
ber of pauses per utterance observed so far, and the
mean pause duration observed so far for the current
dialogue. Both features correlate reasonably well
with silence finality: a higher mean duration indi-
cates that upcoming silences are also less likely to
be final, so does a higher mean number of pauses
per turn.
3.4 Discussion
What emerges from the analysis above is that fea-
tures from all aspects of dialogue provide informa-
tion on silence characteristics. While most previous
research has focused on prosody as a cue to detect
the end of utterances, timing, discourse, semantic
and previously observed silences appear to corre-
late more strongly with silence finality in our corpus.
This can be partly explained by the fact that prosodic
features are harder to reliably estimate on noisy data
and that prosodic features are in fact correlated to
higher levels of dialogue such as discourse and se-
mantics. However, we believe our results make a
strong case in favor of a broader approach to turn-
taking for conversational agents, making the most
of all the features that are readily available to such
systems. Indeed, particularly in constrained systems
like Let?s Go, higher level features like discourse
and semantics might be more robust to poor acoustic
conditions than prosodic features. Still, our findings
on mean pause durations suggest that prosodic fea-
tures might be best put to use when trying to pre-
dict pause duration, or whether a pause will occur
or not. The key to more natural and responsive di-
alogue systems lies in their ability to combine all
these features in order to make prompt and robust
turn-taking decisions.
4 Evaluation of Threshold Decision Trees
4.1 Offline Evaluation Set-Up
We evaluated the approach introduced in Section 2
on the Let?s Go corpus. The set of features was ex-
tended to contain a total of 4 discourse features, 6
semantic features, 5 timing/turn-taking features, 43
prosodic features, and 6 speaker characteristic fea-
tures. All evaluations were performed by 10-fold
cross-validation on the corpus. Based on the pro-
posed algorithm, we built a decision tree and com-
puted optimal cluster thresholds for different overall
FA rates. We report average latency as a function
of the proportion of turns for which any pause was
erroneously endpointed, which is closer to real per-
formance than silence FA rate since, once a turn has
been endpointed, all subsequent silences are irrele-
vant.
6
Figure 2: Performance of the proposed approach using
different feature sets.
4.2 Performance of Different Feature Sets
First we evaluated each feature set individually. The
results are shown in Figure 2. We concentrate on the
2-6% range of turn cut-in rate where any reasonable
operational value is likely to lie (the 700 ms thresh-
old of the baseline Let?s Go system yields about 4%
cut-in rate). All feature sets improve over the base-
line. Statistical significance of the result was tested
by performing a paired sign test on latencies for the
whole dataset, comparing, for each FA rate the pro-
portion of gaps for which the proposed approach
gives a shorter threshold than the single-threshold
baseline. Latencies produced by the decision tree
for all feature sets were all found to be significantly
shorter (p < 0.0001) than the corresponding base-
line threshold.
The best performing feature set is semantics, fol-
lowed by timing, prosody, speaker, and discourse.
The maximum relative latency reductions for each
feature set range from 12% to 22%. When using all
features, the performance improves by a small but
significant amount compared to any single set, up to
a maximum latency reduction of 24%. This confirms
that the algorithm is able to combine features effec-
tively, and that the features themselves are not com-
pletely redundant. However, while removing seman-
tic or timing features from the complete set degrades
the performance, this is not the case for discourse,
speaker, nor prosodic features. This result, similar
to what (Sato et al, 2002) reported in their own ex-
periment, indicates that prosodic features might be
redundant with semantic and timing features.
4.3 Live Evaluation
We confirmed the offline evaluation?s findings by
implementing the proposed approach in Let?s Go?s
Interaction Manager. Since prosodic features were
not found to be helpful and since their online ex-
traction is costly and error-prone, we did not include
them. At the beginning of each dialogue, the sys-
tem was randomly set as a baseline version, using a
700 ms fixed threshold, or as an experimental ver-
sion using the tree learned from the offline corpus.
Results show that median latency (which includes
both the endpointing threshold and the time to pro-
duce the system?s response) is significantly shorter
in the experimental version (561 ms) than in the
baseline (957 ms). Overall, the proposed approach
reduced latency by 50% or more in about 48% of the
turns. However, global results like these might not
reflect the actual improvement in user experience.
Indeed, we know from human-human dialogues that
relatively long latencies are normal in some circum-
stances while very short or no latency is expected
in others. The proposed algorithm reproduces some
of these aspects. For example, after open questions,
where more uncertainty and variability is expected,
the experimental version is in fact slightly slower
(1047 ms vs 993 ms). On the other hand, it is faster
after closed question (800 ms vs 965 ms) and par-
ticularly after confirmation requests (324 ms vs 965
ms), which are more predictable parts of the dia-
logue where high responsiveness is both achievable
and natural. This latter result indicates that our ap-
proach has the potential to improve explicit confir-
mations, which are often thought to be tedious and
irritating to the user.
5 Conclusion
In this paper, we described an algorithm to dynami-
cally set endpointing threshold for each silence. We
analyzed the relationship between silence distribu-
tion and a wide range of automatically extracted fea-
tures from discourse, semantics, prosody, timing and
speaker characteristics. When all features are used,
the proposed method reduced latency by up to 24%
for reasonable false alarm rates. Prosodic features
did not help threshold optimization once other fea-
ture were included. The practicality of the approach
and the offline evaluation results were confirmed by
7
implementing the proposed algorithm in the Let?s
Go system.
Acknowledgments
This work is supported by the US National Science
Foundation under grant number 0208835. Any opin-
ions, findings, and conclusions or recommendations
expressed in this material are those of the authors
and do not necessarily reflect the views of the NSF.
We would like to thank Alan Black for his many
comments and advice.
References
J. F. Allen and C. R. Perrault. 1980. Analyzing intention
in utterances. Artificial Intelligence, 15:143?178.
J. F. Allen, G. Ferguson, A. Stent, S. Stoness, M. Swift,
L. Galescu, N. Chambers, E. Campana, and G. S. Aist.
2005. Two diverse systems built using generic compo-
nents for spoken dialogue (recent progress on trips). In
Interactive Demonstration Track, Association of Com-
putational Linguistics Annual Meeting, Ann Arbor,
MI.
D. Bohus and A. Rudnicky. 2003. RavenClaw: Dia-
log management using hierarchical task decomposi-
tion and an expectation agenda. In Eurospeech03,
Geneva, Switzerland.
D. Bohus, A. Raux, T. Harris, M. Eskenazi, and A. Rud-
nicky. 2007. Olympus: an open-source framework
for conversational spoken language interface research.
In HLT-NAACL 2007 workshop on Bridging the Gap:
Academic and Industrial Research in Dialog Technol-
ogy, Rochester, NY, USA.
W. L. Chafe, 1992. Talking Data: Transcription
and Coding Methods for Language Research, chapter
Prosodic and Functional Units of Language, pages 33?
43. Lawrence Erlbaum.
H.H. Clark. 1996. Using language. Cambridge Univer-
sity Press.
S. Duncan. 1972. Some signals and rules for taking
speaking turns in conversations. Journal of Person-
ality and Social Psychology, 23(2):283?292.
L. Ferrer, E. Shriberg, and A. Stolcke. 2003. A prosody-
based approach to end-of-utterance detection that does
not require speech recognition. In ICASSP, Hong
Kong.
C. E. Ford and S. A. Thompson, 1996. Interaction and
Grammar, chapter Interactional Units in Conversation:
Syntactic, Intonational, and Pragmatic Resources for
the Management of Turns, pages 134?184. Cambridge
University Press.
H. Furo. 2001. Turn-Taking in English and Japanese.
Projectability in Grammar, Intonation, and Semantics.
Routeledge.
B. J. Grosz and C. Sidner. 1986. Attention, intentions,
and the structure of discourse. Computational Lin-
guistics, 12(3):175?204.
J. Jaffe and S. Feldstein. 1970. Rhythms of Dialogue.
Academic Press.
H. Koiso, Y. Horiuchi, S. Tutiya, A. Ichikawa, and
Y. Den. 1998. An analysis of turn-taking and
backchannels based on prosodic and syntactic features
in japanese map task dialogs. Language and Speech,
41(3-4):295?321.
Mietta Lennes and Hanna Anttila. 2002. Prosodic fea-
tures associated with the distribution of turns in finnish
informal dialogues. In Petri Korhonen, editor, The
Phonetics Symposium 2002, volume Report 67, pages
149?158. Laboratory of Acoustics and Audio Signal
Processing, Helsinki University of Technology.
B. Orestro?m. 1983. Turn-Taking in English Conversa-
tion. CWK Gleerup, Lund.
A. Raux, B. Langner, D. Bohus, A. W. Black, and M. Es-
kenazi. 2005. Let?s Go Public! taking a spoken dialog
system to the real world. In Proc. Interspeech 2005,
Lisbon, Portugal.
A. Raux, D. Bohus, B. Langner, A. W. Black, and M. Es-
kenazi. 2006. Doing research on a deployed spoken
dialogue system: One year of Let?s Go! experience.
In Proc. Interspeech 2006, Pittsburgh, PA, USA.
A. Raux, , and M. Eskenazi. 2007. A multi-layer ar-
chitecture for semi-synchronous event-driven dialogue
management. In Proc. ASRU 2007, Kyoto, Japan.
C. Rich and C.L. Sidner. 1998. Collagen: A collabora-
tion manager for software interface agents. An Inter-
national Journal: User Modeling and User-Adapted
Interaction, 8(3-4):315?350.
H. Sacks, E. A. Schegloff, and G. Jefferson. 1974.
A simplest systematics for the organization of turn-
taking for conversation. Language, 50(4):696?735.
R. Sato, R. Higashinaka, M. Tamoto, M. Nakano, and
K. Aikawa. 2002. Learning decision trees to deter-
mine turn-taking by spoken dialogue systems. In IC-
SLP 2002, Denver, CO.
Kare Sjolander. 2004. The snack sound toolkit.
http://www.speech.kth.se/snack/.
M. Takeuchi, N. Kitaoka, and S. Nakagawa. 2004.
Timing detection for realtime dialog systems using
prosodic and linguistic information. In Proc. Speech
Prosody 04, Nara, Japan.
N. Ward, A. Rivera, K. Ward, and D. Novick. 2005. Root
causes of lost time and user stress in a simple dialog
system. In Interspeech 2005, Lisbon, Portugal.
8
Category Feature test Number of Gap Ratio DifferenceSilences
Timing Pause start time ? 3000 ms 1836 / 19260 65% / 87% -23%
Timing Pause number ? 2 3379 / 17717 69% / 88% -19%
Discourse Previous question is open 3376 / 17720 70% / 88% -18%
Semantics Utterance expectation level ? 1 10025 / 11071 78% / 92% -14%
Individual Mean pause duration ? 500 ms 1336 / 19760 72% / 86% -14%
Semantics Utterance contains a positive marker 4690 / 16406 96% / 82% 13%
Prosody Mean energy of last vowel ? 5 1528 / 19568 74% / 86% -12%
Prosody Slope of energy on last vowel ? 0 6922 / 14174 78% / 89% -10%
Individual Mean number of pauses per utterance ? 3 1929 / 19267 76% / 86% -10%
Semantic Utterance is a non-understanding 6023/15073 79% / 88% -9%
Discourse Previous question is a confirmation 8893 / 12203 90% / 82% 8%
Prosody Duration of last vowel ? 1 1319 / 19777 78% / 86% -8%
Prosody Mean pitch on last voiced region ? 5 1136 / 19960 92% / 85% 7%
Prosody Slope of pitch on last voiced region ? 0 6617 / 14479 82% / 87% -4%
Semantics Utterance contains a negative marker 2667 / 18429 87% / 85% 2%*
Discourse Previous question is closed 8451 / 12645 86% / 85% 1%*
Table 1: Effect of Dialogue Features on Pause Finality. In columns 3 and 4, the first number is for silences for which
the condition in column 2 is true, while the second number is for those silences where the condition is false. * indicates
that the results are not statistically significant at the 0.01 level.
Category Feature test Number of Mean pause DifferencePauses Duration (ms) (ms)
Prosody Mean pitch on last voiced region ? 4 172 / 2911 608 / 482 126
Semantics Utterance Expectation Level ? 4 2202 / 881 475 / 526 -51
Prosody Slope of energy on last vowel ? 1 382 / 2701 446 / 495 -39
Timing Pause number ? 2 1031 / 2052 459 / 504 -45
Discourse Previous question is open 1015 / 2068 460 / 504 -43
Individual Mean pause duration ? 500 ms 370 / 2713 455 / 494 -39*
Prosody Mean energy of last vowel ? 4.5 404 / 2679 456 / 494 -38*
Semantics Utterance contains a positive marker 211 / 2872 522 / 487 35*
Discourse Previous question is closed 1178 / 1905 510 / 477 33*
Timing Pause start time ? 3000 ms 650 / 2433 465 / 496 -31*
Semantic Utterance is a non-understanding 1247 / 1836 472 / 502 -30*
Prosody Duration of last vowel ? 0.4 1194 / 1889 507 / 478 29*
Individual Mean number of pauses per utterance ? 2 461 / 2622 474 / 492 -19*
Semantics Utterance contains a negative marker 344 / 2739 504 / 488 16*
Prosody Slope of pitch on last voiced segment ? 0 1158 / 1925 482 / 494 -12*
Discourse Previous question is a confirmation 867 / 2216 496 / 487 9*
Table 2: Effect of Dialogue Features on Pause Duration. In columns 3 and 4, the first number is for silences for which
the condition in column 2 is true, while the second number is for those silences where the condition is false. * indicates
that the results are not statistically significant at the 0.01 level.
9
Let (Kn) be a set of n silence clusters, the goal is to set the thresholds (?n) that minimize overall mean
latency, while yielding a fixed, given number of false alarms E. let us define Gn the number of gaps among
the silences of Kn. For each cluster, let us define En(?n) the number of false alarms yielded by threshold
?n in cluster n, and the total latency Ln by:
Ln(?n) = Gn ? ?n (3)
Assuming pause durations follow an exponential distribution, as shown in Section 3, the following relation
holds between Ln and En:
e
Ln(?n)
?n = ?n ? En(?n) (4)
where ?K and ?K are cluster-specific coefficients estimated by linear regression in the log domain. If we
take the log of both sides, we obtain:
Ln(?n) = ?n ? log(?n ? En(?n)) (5)
Theorem 1. If (?n) is a set of thresholds that minimizes
?
n Ln such that
?
n En(?n) = E, then
?As.t.?n, dLndEn (?n) = A
Informal proof. The proof can be done by contradiction. Let us assume (?n) is a set of thresholds that
minimizes
?
n Ln, and ?(p, q)s.t.
dLp
dEp
(?p) >
dLq
dEq
(?q). Then, there exists small neighborhoods of ?p and ?q
where Lp(Ep) and Lq(Eq) can be approximated by their tangents. Since their slopes differ, it is possible to
find a small  such that the decrease in FA yielded by ?p +  is exactly compensated by the increase yielded
by ?q ? , but the reduction in latency in Kq is bigger than the increase in Kp, which contradicts the fact
that (?n) minimizes L.
From Theorem 1, we get ?As.t.?n dLndEn = A. Thus, by deriving Equation 5,
?n
En
= A which gives En = ?nA .
Given that
?
En = E,
P
?n
A = E. Hence, A =
P
?n
E . From 5, we can infer the values of Ln(?n) and,
using 3, the optimal threshold ?n for each cluster:
?n =
?n ? log(?n ?
E??n
P
?n
)
Gn
(6)
where the values of ?n and ?n can be estimated by linear regression from the data based on 5.
Figure 3: Derivation of the formula for optimal thresholds
10
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 629?637,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Finite-State Turn-Taking Model for Spoken Dialog Systems
Antoine Raux?
Honda Research Institute
800 California Street
Mountain View, CA 94041, USA
araux@hra.com
Maxine Eskenazi
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
max@cs.cmu.edu
Abstract
This paper introduces the Finite-State Turn-
Taking Machine (FSTTM), a new model to
control the turn-taking behavior of conversa-
tional agents. Based on a non-deterministic
finite-state machine, the FSTTM uses a cost
matrix and decision theoretic principles to se-
lect a turn-taking action at any time. We show
how the model can be applied to the problem
of end-of-turn detection. Evaluation results on
a deployed spoken dialog system show that the
FSTTM provides significantly higher respon-
siveness than previous approaches.
1 Introduction
Turn-taking, the process by which participants in a
conversation alternate speech and silence, is an es-
sential component of spoken interaction. In order to
lead productive conversations, people need not only
know what to say but also when to say it. Decades
of research on Conversation Analysis and psycholin-
guistics (Duncan, 1972; Sacks et al, 1974; Ore-
stro?m, 1983; Schegloff, 2000; Wesseling and van
Son, 2005) have shown that human turn-taking be-
havior relies on a wide range of rules and signals
at many different levels of language, from prosody
to syntax, semantics, and discourse structure. In
contrast, turn-taking in spoken dialog systems is of-
ten reduced to ad hoc rules only based on very low
level features. This simplistic approach leads to in-
efficient, unnatural, and possibly confusing behavior
(Porzel and Baudis, 2004; Ward et al, 2005).
? This research was conducted when the first author was a
student at the Language Technologies Institute.
Recently, more complex models of turn-taking
have been proposed (Cassell et al, 2001; Thorisson,
2002; Kronild, 2006). Yet, these models still rely
extensively on hand-coded expert knowledge and
do not lend themselves to data-driven optimization.
Furthermore, to our knowledge, no such model has
been deployed in a widely used system outside of the
laboratory. In this paper, we propose a flexible, prac-
tical model of turn-taking behavior that builds upon
previous work on finite-state models of the conver-
sational floor. Because of its simplicity and gener-
ality, this model can be applied to many turn-taking
phenomena. At the same time, being grounded in
decision theory, it lends itself well to data-driven op-
timization. We illustrate our approach by applying
the model to a specific turn-taking task: end-of-turn
detection.
2 Conversational Floor as a Finite-State
Machine
2.1 6-state finite state models of turn-taking
In the 1960?s and early 1970?s, several researchers
proposed models to explain the rhythmic turn-taking
patterns in human conversation. In particular, Jaffe
and Feldstein (1970) studied the mean duration of
pauses, switching pauses (when a different speaker
takes the floor), simultaneous speech, and (single-
speaker) vocalizations in recorded dyadic conversa-
tions. Based on their observation that these dura-
tions follow exponential distributions, they proposed
first-order Markov models to capture the alterna-
tion of speech and silence in dialog. Their initial
model had four states: only participant A is speak-
629
Figure 1: Our six-state model of turn-taking, inspired by
Jaffe and Feldstein (1970) and Brady (1969). See section
3.1 for a description of the states.
ing; only participant B is speaking; both participants
are speaking; and neither participant is speaking.
However, such a model fails to distinguish switch-
ing pauses from A to B from switching pauses from
B to A. Based on this observation, they extend their
model to a six-state model which they found to bet-
ter fit their data than the four-state model. Around
the same time, Brady (1969) developed a very sim-
ilar six-state model. He trained the parameters on a
recorded conversation and compared the generated
conversations to the original real one along several
dimensions (pause and speech segment durations,
overlaps, etc), finding that his model generally pro-
duced a good fit of the data.
2.2 Finite-State Models for Control
While Jaffe, Feldstein and Brady were primarily
concerned with the analysis of human-human con-
versations, more recently, several researchers have
proposed finite-state machines to control conversa-
tional agents. For instance, Cassell et al (2001)
model the conversational state of an embodied real
estate agent as a 5-state machine. Two states indi-
cate whether a user is present or not, whereas the
other three indicate who holds the floor between the
user and the agent, or whether the floor is open.
Floor conflicts are not captured by this machine and
are presumably resolved through simple rules (e.g.
when the user speaks, the agent yields the floor).
Kronild (2006) proposes a much more complex
model, based on Harel statecharts, which are an ex-
tension of finite-state machines for modeling and vi-
sualizing abstract control (Harel, 1987).
Thorisson?s Ymir architecture (Thorisson, 2002)
is an attempt to model the cognitive processes in-
volved in conversation. It features dialog states, cap-
turing, for example, who has the floor, and rules that
govern the transition from one state to another based
on ?boolean conditions of perceptual features?.
All these models are deterministic. At any point
in time, the agent knows who owns the floor and uses
fixed rules to take appropriate actions. These ap-
proaches assume 1) that the system can obtain per-
fectly reliable information on the state of the world,
and 2) that the state itself is unambiguous.
3 The Finite-State Turn-Taking Machine
3.1 Extending the 6-state model for control
Our model, the Finite-State Turn-Taking Machine
(FSTTM), uses the same six states as Jaffe and
Feldstein: USER and SY STEM represent states
where one and only one of the participants claims
the floor, FREES and FREEU states where no
participant claims the floor (following, resp., a
SY STEM and USER state), and BOTHS and
BOTHU states where both participants claim the
floor (following, resp. a SY STEM and USER
state). However, we apply this model to the control
of a conversational agent, with a goal similar to that
of Cassel, Thorisson, and Kronild. One important
distinction is that we define the states in terms of the
participants? intentions and obligations (in the sense
of Traum and Allen (1994)) rather than the surface
level observation of speech vs silence. For example,
the state is USER when the user has the obligation
to speak (to respond to a system question) or the in-
tention to speak, while at the same time, the system
does not hold the floor. This does not necessarily
mean that the user is speaking, for example at pauses
during a user utterance.
As can be seen in Figure 1, not all transitions are
valid. First, there is no direct transition between any
of the intermediate states (the two FREE states and
two BOTH states). The assumption is that to go
from any of these state to another, the model will
first go to either SY STEM or USER. This is an
630
approximation as there might be cases where, for
example, both the system and user start speaking
at the exact same time, going from a FREE state
to a BOTH state. However these cases are rare
enough that they can be approximated using a tran-
sition through either SY STEM or USER. Sec-
ond, because intermediate states are conditioned on
who had the floor previously, not all valid transitions
are bidirectional. For example, there is no transi-
tion from SY STEM to BOTHU . We associate
pairs of user/system actions to each transition. The
four possible actions are Grab the floor, Release the
floor, Wait while not claiming the floor, and Keep
the floor. For example, transition from SY STEM
to FREES corresponds to the user waiting silently
and the system releasing the floor at the end of a
prompt, noted (R,W ) (we always note the system
action first and user action second).
This representation allows us to formalize a wide
variety of turn-taking phenomena in a unified frame-
work. Specifically, there are 4 types of 2-step transi-
tions from a single-floor-holder state (SY STEM or
USER) to another (or the same) single-floor-holder
state, which represent typical turn-taking phenom-
ena:
Turn transitions with gap are the most common
way the floor goes from one participant to the
other. For example, at the end of a user utter-
ance, once the user finishes speaking, the floor
becomes free, after which the system starts re-
sponding, thus grabbing the floor. The resulting
state sequence is:
SY STEM (R,W )? FREES
(W,G)? USER
Conversely, the transition with gap following a
system prompt corresponds to:
USER (R,W )? FREES
(W,G? USER
Turn transitions with overlap happen when a par-
ticipant grabs the floor while it still belongs to
the other. For example, when a user barges in
on a system prompt, both participants hold the
floor. Then, the system recognizes the barge-
in attempt and relinquishes the floor, which be-
comes user?s.
SY STEM (K,G)? BOTHS
(R,K? USER
And conversely, when the system interrupts the
user mid-utterance (which in dialog systems is
more often the result of an intentional cut-in,
rather than intentional interruption), the state
sequence is:
USER (G,K)? BOTHU
(K,R)? SY STEM
Failed interruptions happen when a participant
barges in on the other and then withdraws be-
fore the original floor holder releases the floor.
For example, when the system interrupts the
user (often by mistake) but detects it and in-
terrupts itself:
USER (G,K)? BOTHU
(R,K? USER
The converse is usually the result of the system
failing to react fast enough to a user barge-in:
SY STEM (K,G)? BOTHS
(K,R)? SY STEM
Note that backchannels seem to fit in this cat-
egory too. However, since backchannels, by
definition, do not represent an attempt to grab
the floor, they are not captured by the model
as it is (for example, the floor should remain
SY STEM when a user backchannels a sys-
tem utterance).
Time outs start like transitions with gap but the in-
tended next speaker (e.g. the user after a system
prompt) does not take the floor and the original
floor holder grabs it back. For instance, after a
system prompt, if the floor remains free for a
certain amount of time, the system attempts to
re-establish the communication with the user,
as follows:
SY STEM (R,W )? FREES
(G,W? SY STEM
The opposite also happens when the system is
to slow to respond to the user:
USER (W,R)? FREEU
(W,G? USER
While all the transitions above were described
as deterministic, the actual state of the model is
not fully observable. Specifically, while the system
631
knows whether its claiming the floor or not, it can
only believe with some degree of uncertainty that
the user does so. The system?s knowledge of its own
claim to the floor splits the state space into two dis-
joint subsets. When the system claims the floor, the
state can be SY STEM , BOTHS , or BOTHU ).
When the system does not claim the floor, the state
can be USER, FREEU , or FREES). In either
case, the system needs to recognize the user?s in-
tention (i.e. whether the user claims to the floor or
not) to maintain a probability distribution over the
three states. Since the distinction between the two
BOTH states (resp. the two FREE states) is based
on past history that can be known with a high level
of certainty, the uncertainty in state distribution is
fully characterized by the probability that the user is
claiming the floor, which will have to be estimated
from observations, as we will see below.
3.2 Cost of Turn-Taking Actions
The problem we are facing is that of choosing the
best system action given the system?s belief about
the current state of the model. That is achieved by
applying the probabilistic decision theory principle
of selecting the action with lowest expected cost.
The actions available to the system are the four de-
scribed above (G,R,K,W ), although not all actions
are available in all states. In fact, as can be seen in
Table 1, there are always only two actions available
in each state, depending on whether the system is
claiming the floor or not.
Each action in each state has a particular cost.
While there are many possible ways of defining
these costs, we propose a simple cost structure that
derives from the principles laid out in Sacks et al
(1974):
Participants in a conversation attempt to
minimize gaps and overlaps.
From this general principle, we derive three rules to
drive the design of a cost matrix:
1. The cost of an action that resolves either a gap
or an overlap is zero
2. The cost of an action that creates unwanted gap
or overlap is equal to a constant parameter (po-
tentially different for each action/state pair)
3. The cost of an action that maintains a gap or
overlap is either a constant or an increasing
function of the total time spent in that state
The resulting cost matrix is shown in Table 1, where
? CS is the cost of interrupting a system prompt
before its end when the user is not claiming the
floor (false interruption)
? CO(? ) is the cost of remaining in an overlap
that is already ? ms long
? CU is the cost of grabbing the floor when the
user is holding it (cut-in)
? CG(? ) is the cost of remaining in a gap that is
already ? ms long
This cost structure makes a number of simplifying
assumptions and there are many other possible cost
matrices. For example, the cost of interrupting the
user might vary depending on what has already been
said in the utterance, so does the cost of interrupt-
ing a system prompt. A more principled approach
to setting the costs would be to estimate from per-
ceptual experiments or user studies what the impact
of remaining in gap or overlap is compared to that
of a cut-in or false interruption. However, as a first
approximation, the proposed cost structure offers a
simple way to take into account some of the con-
straints of interaction.
3.3 Decision Theoretic Action Selection
Given the state space and the cost matrix given
above, the optimal decision at any point in time is
the one that yields the lowest expected cost, where
the expected cost of action A is:
C(A) = ?
S??
P (s = S|O) ? C(A,S)
where ? is the set of states, O are the observable
features of the world, and C(A,S) is the cost of ac-
tion A in state S, from the cost matrix in Table 1.
In addition to the cost matrix? four constants, which
we will consider as parameters of the model, it is
thus necessary to estimate P (s = S|O), which as
seen above amounts to estimate the probability that
the user is claiming the floor. Key to applying the
FSTTM to a practical turn-taking problem is thus
the construction of accurate estimates of the proba-
bilities P (s = S|O).
632
PPPPPPPPState
Action K R W G
SY STEM 0 CS - -
BOTHS CO(?) 0 - -
BOTHU CO(?) 0 - -
USER - - 0 CU
FREEU - - CG(?) 0
FREES - - CG(?) 0
Table 1: Cost of system actions in each state (K: keep the floor, R: release the floor, W : wait without the floor, G:
grab the floor, ? : time spent in current state, -: action unavailable).
4 Endpointing with the FSTTM
4.1 Problem formalization
In our FSTTM formalism, endpointing is the prob-
lem of selecting between the Wait and the Grab ac-
tions during a user utterance. We make the simplify-
ing assumption that, once a user utterance has been
detected, the only states with non-zero probability
are USER and FREEU . While this does not cap-
ture cases where the system erroneously detects user
speech (because there is, for example, background
noise), it represents a working first approximation
of the problem.
The main issue is to estimate the probability
P (s = FREEU |Ot) (hereafter abbreviated as
P (F |Ot), P (s = USER|Ot) being abbreviated as
P (U |Ot)) where Ot represents all observable fea-
tures at time t. Given that probability, the expected
cost of grabbing the floor is:
C(G|Ot) = P (U |Ot) ? CU + P (F |Ot) ? 0
= (1? P (F |Ot)) ? CU
Similarly, the expected cost of waiting is:
C(W |Ot) = P (F |Ot) ? CG(?)
The system endpoints whenever the expected cost
of grabbing the floor becomes higher than that of
waiting.
We consider two separate cases for computing
both P (F |Ot) and CG(?): when a pause has been
detected by the voice activity detector (VAD), and
when no pause has been detected (yet). In the fol-
lowing sections, we provide details on the approxi-
mations and estimation methods for these two cases.
4.2 At pauses
If a pause has been detected by the VAD, we set
the cost of waiting in the FREEU state to be pro-
portional to the duration of the pause so far. If the
user has released the floor, the duration of the current
pause corresponds to the time spent in the FREEU
state, i.e. ? in the cost matrix of Table 1. In this case,
we set CG(?) = CpG ? ? as a simple application of
rule 3 from section 3.2.
We decompose the observations at time t,Ot, into
observations available at the start of the pause (O),
and observations made during the pause. With only
audio information available, the only information
available during the pause is its duration so far, i.e.
? . Specifically, we know that d ? ? , where d is the
total duration of the pause (with d = ? at the end
of a turn1). Consequently, P (F |Ot) can be rewritten
using Bayes rule as
P (F |Ot) = P (d ? ? |O,F ) ? P (F |O)P (d ? ? |O)
= P (F |O)P (d ? ? |O)
where P (F |O) is the probability that the user re-
leased the floor without any knowledge of the dura-
tion of the pause, and P (d ? ? |O) is the probability
that the pause will last at least ? ms. We further de-
compose P (d ? ? |O) into
P (d ? ? |O) = P (d ? ?, U |O) + P (d ? ?, F |O)
1Note that this is an approximation since the user could start
speaking again after releasing the floor to reestablish the chan-
nel (e.g. by saying ?Hello??). However, in the vast majority of
cases, the time after which the user resumes speaking is signifi-
cantly longer than the time the system takes to endpoint.
633
= P (d ? ? |O,U) ? P (U |O) +
P (d ? ? |O,F ) ? P (F |O)
= P (d ? ? |O,U) ? (1? P (F |O))
+P (F |O)
Consequently, P (F |Ot) is a function of P (F |O)
and P (d ? ? |O,U). We estimate P (F |O) by step-
wise logistic regression on a training set of pauses
labeled for finality (whether the pause is turn-final or
turn-internal), using a wide range of features avail-
able from various components of the dialog system.
Based on the well established observation that pause
durations follow an exponential distribution (Jaffe
and Feldstein, 1970; Lennes and Anttila, 2002; Raux
et al, 2008), P (d ? ? |O,U) is a function of mean
pause duration, computed on the training set.
4.3 In speech
In some cases, it is not necessary to wait for the VAD
to detect a pause to know with high confidence that
the user has released the floor. For example, after a
simple yes/no question, if the user says ?YES?, they
are very likely to have released the floor, regardless
of how long they remain silent afterwards. In order
to exploit this fact and improve the responsiveness
of the system in these highly predictable cases, we
use a separate model to compute the expected costs
of waiting and grabbing the floor before any pause is
detected by the VAD (specifically, whenever the du-
ration of the current pause is between 0 and 200 ms).
In this case, we set the cost of waiting to a constant
CsG. We train a logistic regression model to estimate
P (F |Ot) each time a new partial hypothesis is pro-
duced by the ASR during a user utterance. We use
the same set of features as above.
5 Evaluation
5.1 Corpus and Features
We evaluated the effectiveness of the FSTTM on
an actual deployed spoken dialog system. The sys-
tem provides bus schedule information for a mid-
size North American city. It is actually used by the
general public and therefore constantly operates and
collects data. In order to train the various proba-
bility estimation models and evaluate the approach
in batch, we first collected a corpus of 586 dialogs
between May 4, and May 14, 2008 (the ?2008 cor-
pus?).
All of the features we used can be automatically
extracted at runtime, and most of them were readily
available in the system. They include dialog state in-
formation, turn-taking features, such as whether the
current user utterance is a barge-in, and semantic
information derived from the dialog state and par-
tial recognition hypotheses provided by the speech
recognizer. Dialog state is abstracted to three high-
level states, which correspond to the type of system
prompt directly preceding the user utterance: Open
question (?What can I do for you??); Closed ques-
tion (e.g. ?Where do you want to go??); and Confir-
mation (e.g. ?Going to the airport. Is this correct??).
To capture lexical cues correlated with the end of
turns, we created a new feature called the boundary
LM score. To compute it, we used previously col-
lected data to train dialog-state-dependent statistical
language models to estimate the probability that the
hypothesis is complete. Boundary LM score is de-
fined as the ratio of the log likelihood of the hypoth-
esis being complete by that of the hypothesis being
incomplete.
5.2 Estimating P (F |Ot)
We trained two logistic regression models using
stepwise regression and 10-fold cross-validation for
evaluation. The first model, whose performance
is given in Table 2, estimates P (F |O) at pauses.
The model is unable to improve classification accu-
racy over the majority baseline for each state, how-
ever, the statistically significant improvement in av-
erage log likelihood indicates that the probability
estimates are improved by using the features. The
most informative feature in all three states was the
boundary LM score introduced in section 5.1. Other
selected features included the average number of
words per user utterance so far and whether the cur-
rent utterance is a barge-in (for the Open and Closed
question states), as well as whether the partial hy-
pothesis contained a confirmation marker such as
?YES? or ?SURE? (for the Confirmation state).
The second model performs the same regression,
this time on all partial hypotheses received during
speech segments. As seen in the ?S? columns in Ta-
ble 2, classification error was significantly reduced
and the gain in average log likelihood were larger
634
Open question Closed question Confirmation
P S P S P S
Majority Baseline 38% 20% 25% 32% 12% 36%
Classification Error 35% 17% 26% 22% 12% 17%
Baseline log likelihood -0.66 -0.50 -0.56 -0.63 -0.36 -0.65
Log likelihood -0.61 -0.40 -0.50 -0.49 -0.30 -0.40
Table 2: Performance of state-specific logistic regression for estimating P (F |O) at pauses (P) and in speech (S).
(a) In-pause evaluation on the 2007 corpus. (a) Anytime evaluation on the 2008 corpus.
Figure 2: Batch evaluation of FSTTM endpointing.
than at pauses, particularly for the ?Closed ques-
tion? and ?Confirmation? states. Again, boundary
LM score was the most informative feature. The
duration of the pause at the end of the partial hy-
pothesis (between 0 and 200 ms) also proved well
correlated with finality.
5.3 Batch Evaluation of the FSTTM
We performed two batch evaluations of the FSTTM.
The first one aims at comparing in-pause-FSTTM
with a fixed-threshold baseline as well as previous
data-driven endpointing methods proposed in Ferrer
et al (2003) (reimplemented by us) and Raux et al
(2008). This evaluation was done on the corpus used
in Raux et al (2008) (the ?2007 corpus?). As seen
in Figure 2 (a), the FSTTM outperforms all other ap-
proaches (albeit only slightly compared to Ferrer et
al.), improving over the fixed threshold baseline by
up to 29.5%.
Second, we compared the anytime-FSTTM with
in-pause-FSTTM and a fixed-threshold baseline (for
reference) on the more recent 2008 corpus (since the
2007 corpus did not contain all necessary features
for anytime-FSTTM). We set CpG = 1 and set CsG
to either 0, leading to an endpointer that never end-
points during speech (in-pause-FSTTM), or 1000
(anytime-FSTTM). In both cases, we vary CU to
compute the latency / cut-in rate trade-off curve.
The results are shown in Figure 2 (b). Anytime-
FSTTM endpointing is consistently better than in-
pause-FSTTM. For example, at a cut-in rate of 5%,
anytime-FSTTM yields latencies that are on average
17% shorter than in-pause-FSTTM, and 40% shorter
than the baseline. Additionally, we found that, in
anytime-FSTTM, 30 to 40% of the turns are end-
pointed before the pause is detected by the VAD.
5.4 Live Evaluation
To confirm the results of the batch evaluation, we
implemented our FSTTM model in the deployed
system a let it run for ten days using either FSTTM
or a fixed threshold for endpointing, resulting in
a corpus of 171 FSTTM and 148 control dialogs.
For FSTTM, we set CpG = 1, CsG = 500, and
CU = 5000. In the batch evaluation, these values
correspond to a cut-in rate of 6.3% and an average
latency of 320 ms. For the control condition, we
set the fixed endpointing threshold to 555 ms, which
also corresponded to about 6.3% cut-ins.
Figure 3 shows the average latency and cut-in rate
635
(a) Latency (b) Cut-in rates
Figure 3: Live evaluation results. All confidence intervals for latency (not shown on the figure) fall within +/? 4ms.
for both conditions. The FSTTM improves over the
baseline on all metrics, reducing average latency by
193 ms (p < 0.05), cut-in rate by 1.5% (although
this result is not statistically significant).
6 Discussion
Both batch and live evaluation results confirm the
effectiveness of the FSTTM approach in improv-
ing system responsiveness. This approach signif-
icantly reduced endpointing latency over previous
approaches. Boundary LM score got the highest
weight in the regression, indicating that in a domain
such as telephone-based information access, lexical
cues are very informative for endpointing. The fact
that boundary LMs can be computed without any hu-
man transcription effort (since they are trained on
ASR output) makes them all the more appealing.
Essentially, the FSTTM provides a simple, unified
model of turn-taking that lends itself to data-driven
optimization. While we discussed specific cost
structures and probability estimation techniques, the
framework?s flexibility opens it to other choices at
many levels. By formalizing the overall turn-taking
process in a probabilistic, decision-theoretic frame-
work, the FSTTM extends and generalizes previous
classification-based approaches to endpointing such
as those proposed by Sato et al (2002), Ferrer et
al. (2003), Takeuchi et al (2004), and our previous
work (Raux et al, 2008).
Possible extensions of the approach include data-
driven cost matrices to relax some of the assump-
tions introduced in section 3.2, as well as more com-
plex state structures to handle, for example, multi-
party conversations.
Finally, we plan to investigate more principled ap-
proaches, such as Partially Observable Markov De-
cision Processes or Dynamic Bayesian Networks, to
model the different sources of uncertainty (detection
errors and inherent ambiguity) and track the state
distribution over time. Raux (2009) provides more
details on all aspects of the approach and its possi-
ble extensions.
7 Conclusion
In this paper, motivated by existing finite-state mod-
els of turn-taking in dyadic conversations, we pro-
pose the Finite-State Turn-Taking Machine, an ap-
proach to turn-taking that relies on three core ele-
ments: a non-deterministic finite-state machine that
captures the conversational floor; a cost matrix that
models the impact of different system actions in dif-
ferent states; and a decision-theoretic action selec-
tion mechanism. We describe the application of the
FSTTM to the key turn-taking phenomenon of end-
of-turn detection. Evaluation both offline and by
applying the FSTTM to a deployed spoken dialog
system system showed that it performs significantly
better than a fixed-threshold baseline.
Acknowledgments
This work is supported by the US National Science
Foundation under grant number 0208835. Any opin-
ions, findings, and conclusions or recommendations
expressed in this material are those of the authors
and do not necessarily reflect the views of the NSF.
We would like to thank Alan Black for his many
comments and advice.
636
References
P. T. Brady. 1969. A model for generating on-off speech
patterns in two-way conversation. The Bell System
Technical Journal, 48:2445?2472.
J. Cassell, T. Bickmore, L. Campbell, H. Vilhjalmsson,
and H. Yan. 2001. More than just a pretty face: con-
versational protocols and the affordances of embodi-
ment. Knowledge-Based Systems, 14:55?64.
S. Duncan. 1972. Some signals and rules for taking
speaking turns in conversations. Journal of Person-
ality and Social Psychology, 23(2):283?292.
L. Ferrer, E. Shriberg, and A. Stolcke. 2003. A prosody-
based approach to end-of-utterance detection that does
not require speech recognition. In ICASSP, Hong
Kong.
D. Harel. 1987. Statecharts: A visual formalism for
complex systems. Science of Computer Programming,
8:231?274.
J. Jaffe and S. Feldstein. 1970. Rhythms of Dialogue.
Academic Press.
F. Kronild. 2006. Turn taking for artificial conversational
agents. In Cooperative Information Agents X, Edin-
burgh, UK.
Mietta Lennes and Hanna Anttila. 2002. Prosodic fea-
tures associated with the distribution of turns in finnish
informal dialogues. In Petri Korhonen, editor, The
Phonetics Symposium 2002, volume Report 67, pages
149?158. Laboratory of Acoustics and Audio Signal
Processing, Helsinki University of Technology.
B. Orestro?m. 1983. Turn-Taking in English Conversa-
tion. CWK Gleerup, Lund.
R. Porzel and M. Baudis. 2004. The tao of chi:
Towards effective human-computer interaction. In
HLT/NAACL 2004, Boston, MA.
A. Raux, , and M. Eskenazi. 2008. Optimizing endpoint-
ing thresholds using dialogue features in a spoken dia-
logue system. In Proc. SIGdial 2008, Columbus, OH,
USA.
A. Raux. 2009. Flexible Turn-Taking for Spoken Dialog
Systems. Ph.D. thesis, Carnegie Mellon University.
H. Sacks, E. A. Schegloff, and G. Jefferson. 1974.
A simplest systematics for the organization of turn-
taking for conversation. Language, 50(4):696?735.
R. Sato, R. Higashinaka, M. Tamoto, M. Nakano, and
K. Aikawa. 2002. Learning decision trees to deter-
mine turn-taking by spoken dialogue systems. In IC-
SLP 2002, Denver, CO.
E.A. Schegloff. 2000. Overlapping talk and the orga-
nization of turn-taking for conversation. Language in
Society, 29:1?63.
M. Takeuchi, N. Kitaoka, and S. Nakagawa. 2004.
Timing detection for realtime dialog systems using
prosodic and linguistic information. In Proc. Speech
Prosody 04, Nara, Japan.
K. R. Thorisson, 2002. Multimodality in Language and
Speech Systems, chapter Natural Turn-Taking Needs
No Manual: Computational Theory and Model, From
Perception to Action, pages 173?207. Kluwer Aca-
demic Publishers.
D. R. Traum and J. F. Allen. 1994. Discourse obligations
in dialogue. In Proc. ACL-94, pages 1?8.
N. Ward, A. Rivera, K. Ward, and D. Novick. 2005. Root
causes of lost time and user stress in a simple dialog
system. In Interspeech 2005, Lisbon, Portugal.
W. Wesseling and R.J.J.H. van Son. 2005. Timing of
experimentally elicited minimal responses as quanti-
tative evidence for the use of intonation in projecting
TRPs. In Interspeech 2005, pages 3389?3392, Lisbon,
Portugal.
637
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 37?46,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Probabilistic Ontology Trees for Belief Tracking in Dialog Systems
Neville Mehta
Oregon State University
mehtane@eecs.oregonstate.edu
Rakesh Gupta
Honda Research Institute
rgupta@hra.com
Antoine Raux
Honda Research Institute
araux@hra.com
Deepak Ramachandran
Honda Research Institute
dramachandran@hra.com
Stefan Krawczyk
Stanford University
stefank@cs.stanford.edu
Abstract
We introduce a novel approach for robust
belief tracking of user intention within
a spoken dialog system. The space of
user intentions is modeled by a proba-
bilistic extension of the underlying do-
main ontology called a probabilistic on-
tology tree (POT). POTs embody a prin-
cipled approach to leverage the dependen-
cies among domain concepts and incorpo-
rate corroborating or conflicting dialog ob-
servations in the form of interpreted user
utterances across dialog turns. We tailor
standard inference algorithms to the POT
framework to efficiently compute the user
intentions in terms of m-best most proba-
ble explanations. We empirically validate
the efficacy of our POT and compare it to
a hierarchical frame-based approach in ex-
periments with users of a tourism informa-
tion system.
1 Introduction
A central function of a spoken dialog system
(SDS) is to estimate the user?s intention based on
the utterances. The information gathered across
multiple turns needs to be combined and under-
stood in context after automatic speech recogni-
tion (ASR). Traditionally, this has been addressed
by dialog models and data structures such as forms
(Goddeau et al, 1996) and hierarchical task de-
composition (Rich and Sidner, 1998). To formal-
ize knowledge representation within the SDS and
enable the development of reusable software and
resources, researchers have investigated the or-
ganization of domain concepts using IS-A/HAS-A
ontologies (van Zanten, 1998; Noh et al, 2003).
Because the SDS only has access to noisy ob-
servations of what the user really uttered due to
speech recognition and understanding errors, be-
lief tracking in speech understanding has received
particular attention from proponents of probabilis-
tic approaches to dialog management (Bohus and
Rudnicky, 2006; Williams, 2006). The mecha-
nism for belief tracking often employs a Bayesian
network (BN) that represents the joint probabil-
ity space of concepts while leveraging conditional
independences among them (Paek and Horvitz,
2000). Designing a domain-specific BN requires
significant effort and expert knowledge that is not
always readily available. Additionally, real-world
systems typically yield large networks on which
inference is intractable without major assumptions
and approximations. A common workaround to
mitigate the intensive computation of the joint dis-
tribution over user intentions is to assume full con-
ditional independence between concepts which vi-
olates the ground truth in most domains (Bohus
and Rudnicky, 2006; Williams, 2006).
We propose a novel approach to belief track-
ing for an SDS that solves both the design and
tractability issues while making more realistic
conditional independence assumptions. We repre-
sent the space of user intentions via a probabilistic
ontology tree (POT) which is a tree-structured BN
whose structure is directly derived from the hier-
archical concept structure of the domain specified
via an IS-A/HAS-A ontology. The specialization
(IS-A) and composition (HAS-A) relationships be-
tween the domain concepts are intuitive and pro-
vide a systematic way of representing ontological
knowledge for a wide range of domains.
The remainder of the paper is structured as fol-
lows. We begin by describing the construction of
the POT given a domain ontology. We show how
a POT employs null semantics to represent con-
sistent user intentions based on the specialization
and composition constraints of the domain. We
then show how standard inference algorithms can
be tailored to exploit the characteristics of the POT
to efficiently infer the m-best list of probable ex-
planations of user intentions given the observa-
37
tions. The POT and the associated inference al-
gorithm empower a dialog manager (DM) to ac-
count for uncertainty while avoiding the design
complexity, intractability issues, and other restric-
tive assumptions that characterize state-of-the-art
systems. The section on empirical evaluation de-
scribes experiments in a tourist information do-
main that compare the performance of the POT
system to a frame-based baseline system. The pa-
per concludes with a discussion of related work.
2 Problem Formulation
Let {X1, X2, . . . , XN} be a set of N concepts.
Every conceptXi takes its value from its finite dis-
crete domain D(Xi) which includes a special null
element for the cases where Xi is irrelevant. The
user intention space is defined as U = D(X1) ?
D(X2) ? ? ? ? ? D(XN ). At each dialog turn t,
the system makes a noisy observation ot about
the true user intention u ? U . ot consists of
a set of slots. A slot is a tuple ?v, d, c? where
v ? {X1, . . . , XN}, d ? D(v) is a value of v,
and c ? R is the confidence score assigned to that
concept-value combination by the speech under-
standing (SU) system.
The goal of belief tracking is to maintain
Pr(X1, . . . , XN |o1, . . . , ot), a distribution over
the N -dimensional space U conditioned on all the
observations made up to turn t. At each turn, the
belief is updated based on the new observations to
estimate the true, unobserved, user intention.
3 Probabilistic Ontology Trees
We model the space of the user intentions via a
POT. A POT is a tree-structured BN that extends
a domain ontology by specifying probability dis-
tributions over its possible instantiations based on
specializations and compositions.
3.1 Domain Ontology
To ensure that the corresponding POTs are tree-
structured, we consider a restricted class of do-
main ontologies over concepts.
Definition 1. A domain ontology is a labeled di-
rected acyclic graph. The set of vertices (corre-
sponding to the domain concepts) is partitioned
into {V0}, VS , and VC , where V0 is the only root
node, VS is the set of specialization nodes (re-
lated via IS-A to their parents), and VC is the set
of composition nodes (related via HAS-A to their
parents). The set of edges satisfy the constraints
A
B
D
C
E
H
F
I
G
J
K
I
J
K
Figure 1: The ontology for a sample domain where
B IS-A A, C IS-A A, D IS-A A, E IS-A B, F IS-A B,
C HAS-A G (essential), D HAS-A G (nonessential),
H IS-A D, E HAS-A I (essential), J IS-A G, and
K IS-A G. Specialization nodes are drawn single-
lined, composition nodes are drawn double-lined,
and the root node is drawn triple-lined. Special-
ization subtrees are marked by dashed ovals.
that a specialization node has exactly one parent
and a composition node may only have more than
one parent if they are all specialization nodes with
a common parent.
Specialization nodes represent refinements of
their parent concepts. Specializations of a con-
cept are disjoint, that is, for any particular instance
of the parent exactly one specialization is applica-
ble and the rest are inapplicable. For example, if
Dog IS-A Animal and Cat IS-A Animal, then Cat
is inapplicable when Dog is applicable, and vice
versa. Composition nodes represent attributes of
their parents and may be essential or nonessential,
e.g., Dog HAS-A Color (essential), Dog HAS-A
Tail (nonessential). These definitions correspond
with the standard semantics in the knowledge rep-
resentation community (Noh et al, 2003). An ex-
ample ontology is shown in Figure 1.
Definition 2. A specialization subtree (spec-tree)
in the ontology is a subtree consisting of a node
with its specialization children (if any).
3.2 POT Construction
We now describe how a POT may be constructed
from a domain ontology. The purpose of the POT
is to maintain a distribution of possible instanti-
ations of the ontology such that the ontological
structure is respected.
38
Given an ontology G, the corresponding POT is
a tree-structured BN defined as follows:
Variables. Let T be a spec-tree in G with root
R. Unless R is a (non-root) specialization node
with no specialization children, T is represented
in the POT by a variable X with the domain
D(X) =
?
??
??
{exists, null}, if ChildrenT (R) = ?
ChildrenT (R), if R = V0
ChildrenT (R) ? {null}, otherwise.
Edges. Let POT variables X and Y correspond
to distinct spec-trees TX and TY in G. There is a
directed edge from X to Y if and only if either
? A leaf of TX is the root of TY .
? There is an edge from a leaf in TX to the non-
specialization root of TY .
? There is an edge from the non-specialization
root of TX to that of TY .
Conditional Probability Tables (CPTs). If X
(corresponding to spec-tree TX ) is the parent of Y
(corresponding to spec-tree TY ) in the POT, then
Y ?s CPT is conditioned as follows:
? If TY is rooted at one of the leaves of TX ,
then
Pr(Y = null|X = Y) = 0
Pr(Y = null|X 6= Y) = 1
where Y is the domain value of X corre-
sponding to child Y .
? If R is the root of TX , and TY has a compo-
sition root node that is attached only to nodes
in S ? ChildrenTX (R), then
Pr(Y = null|X = V) = 1
for any domain value V of X corresponding
to a node V ? ChildrenTX (R)? S.
? If the root of TY is an essential composition
node attached to a leaf V of TX , then
Pr(Y = null|X = V) = 0
where V is the domain value of X corre-
sponding to the leaf V .
We label a POT variable with that of the root of
the corresponding spec-tree for convenience. The
domain of a POT variable representing a spec-tree
comprises the specialization children (node names
in sanserif font) and the special value null; the null
A
B
C
D
0.4
0.3
50
.25
B
D
E
F
nu
ll
B
0.6
0.4
0
C
0
0
1
D
0
0
1
H
nu
ll
B
0
1
C
0
1
D
1
0
ex
ist
sn
ull
E
1
0
F
0
1
nu
ll
0
1
J
K
nu
ll
B
0
0
1
C
0.8
0.2
0
D
0.7
0.1
0.2
I
G
Figure 2: The POT for the example domain. If a
node represents a spec-tree in the ontology, then it
is labeled by the root of the spec-tree; otherwise,
it is labeled with the name of the corresponding
ontology node. D(A) = {B, C, D}, D(B) = {E, F,
null}, D(D) = {H, null}, and Pr(A), Pr(B|A) and
Pr(D|A) represent some distributions over the re-
spective specializations. D(I) = {exists, null} and
D(G) = {J, K, null}. Note that a composition node
(G) can be shared between multiple specializa-
tions (C and D) in the ontology while the resulting
POT remains tree-structured.
value allows us to render any node (except the
root) inapplicable. Spec-trees comprising single
nodes have the domain value exists to switch be-
tween being applicable and inapplicable. The CPT
entries determine the joint probabilities over pos-
sible valid instantiations of the ontology and could
be based on expert knowledge or learned from
data. The conditions we impose on them (null se-
mantics) ensure that inconsistent instantiations of
the ontology have probability 0 in the POT. While
the ontology might have undirected cycles involv-
ing the children of spec-trees, the corresponding
POT is a tree because spec-trees in the ontology
collapse into single POT nodes. The POT for the
example domain is shown in Figure 2.
3.3 Tourist Information POT
For the empirical analysis, we designed a POT for
a tourist information system that informs the user
about places to shop, eat, get service, and displays
relevant information such as the distance to an in-
tended location. The user can also provide con-
versational commands such as stop, reset, undo,
etc. The full ontology for the tourist information
domain is shown in Figure 3 and the POT is in
Figure 4. In the POT, Action is the root node, with
D(Action) = {Venue, Command}, and D(Venue)
39
Ac
tio
n
Ve
nue
Co
mm
and
Sta
rt
Ca
nce
l
Re
sta
ura
nt
Sto
re
Ser
vic
e
Ar
ea
Dis
pla
y
Mi
les
Am
bie
nce
Cu
isin
e
Ho
urs
Ser
vic
eT
ype
Str
eet
Ra
tin
g
Sto
reT
ype
Pri
ceR
ang
e
Figure 3: The ontology for the tourist information domain. All the composition nodes have specializa-
tions of their own (such as Japanese and Greek for Cuisine), but have not been shown for the sake of
compactness.
= {Restaurant, Store, Service, null}. All the com-
position (or attribute) nodes such as Hours and
Rating are made children of Venue by construc-
tion. Since a Command is inapplicable when the
Action is a Venue, we have Pr(Command = null
| Action = Venue) = 1. The composition nodes
(Cuisine, Street, etc.) have specializations of their
own ({Japanese, Greek, . . . }, {Castro, Elm, . . . },
etc.), but are not shown for the sake of clarity.
Since Cuisine is an essential attribute of Restau-
rant, Pr(Cuisine = null | Venue = Restaurant) = 0;
moreover, Pr(Cuisine = null | Venue = Service) =
1 because Cuisine is not relevant for Service.
4 Inferring User Intention
We have seen how the POT provides the proba-
bilistic machinery to represent domain knowledge.
We now discuss how the POT structure can be
leveraged to infer user intention based on the slots
provided by the SU.
4.1 Soft Evidence
Every slot retrieved from the SU needs to be incor-
porated as observed evidence in the POT. We can
set the associated node within the POT directly to
its domain value as hard evidence when we know
these values with certainty. Instead, we employ
probabilistic observations to soften the evidence
entered into the POT. We assume that the confi-
dence score c ? [0, 100] of a slot corresponds to
the degree of certainty in the observation. For an
observed slot variableX , we create an observation
node X? on the fly with the same domain as X and
make it a child of X . If x is the observed value for
slot X , then the CPT of X? is constructed from the
slot?s confidence score as follows:
Pr(X?|X = x) =
{
c(|D(X)|?1)/100+1
|D(X)| , X? = x
1?c/100
|D(X)| , X? 6= x
The probability values are generated by lin-
early interpolating between the uniform probabil-
ity value and 1 based on the confidence score. For
the remaining values,
Pr(X?|X 6= x) =
{
1? ?(|D(X)| ? 1), X? = X
?, X? 6= X
where ? > 0.1 Since the confidence score gives an
indication of the probability for the observed value
of a slot but says nothing about the remaining val-
ues, the diagonal elements for the remaining val-
ues are near 1. We cannot make them exactly 1
because the observation node needs to coexist with
possibly conflicting observations in the POT.
If the user confirms the current POT hypothesis,
then observations corresponding to the current hy-
pothesis (with CPTs proportional to the score of
the confirmation) are added to the POT to enforce
the belief. If the user denies the current hypothe-
sis, then all observations corresponding to the cur-
rent hypothesis are removed from the POT.
1In our experiments, we use ? = 10?10.
40
Ac
tio
n
Ve
nue
Co
mm
and
Am
bie
nce
Cu
isin
e
Dis
pla
y
Ho
urs
Ser
vic
eT
ype
Mi
les
Ra
tin
g
Sto
reT
ype
Ar
ea
Str
eet
Pri
ceR
ang
e
Cu
isin
e
Str
eet
Cas
tro
Elm
nul
l
Cas
tro
0.8
0.1
0.1
Jap
ane
se
Gre
ek
nul
l
Jap
ane
se
0.6
0.2
0.2
Cu
isin
e
Str
eet
Elm
?
1?2
?
?
nul
l
?
?
1?2
?
p Gre
ek
?
1?2
?
?
nul
l
?
?
1?2
?
Figure 4: The POT for the tourist information domain. Assuming that D(Cuisine) = {Japanese, Greek,
null} and D(Street) = {Castro, Elm, null}, the shaded observation nodes represent the soft evidence for
input slots ?Cuisine, Japanese, 40? and ?Street, Castro, 70?.
The POT for the tourist information domain af-
ter getting two slots as input is shown in Figure 4.
The attached nodes are set to the observed slot val-
ues and the evidence propagates through the POT
as explained in the next section.
4.2 POT Inference
A probable explanation (PE) or hypothesis is an
assignment of values to the variables in the POT,
and the most probable explanation (MPE) within
the POT is the explanation that maximizes the
joint probability conditioned on the observed vari-
ables. The top m estimates of the user?s intentions
correspond to them-best MPEs. The design of the
POT ensures that the m-best MPEs are all con-
sistent across specializations, that is, exactly one
specialization is applicable per node in any PE; all
inconsistent explanations have a probability of 0.
The m-best MPEs could be found naively us-
ing the Join-Tree algorithm to compute the joint
distribution over all variables and then use that to
find the top m explanations. The space required to
store the joint distribution alone is O(nN ), where
N is the number of nodes and n the number of
values per node. Because the run time complex-
ity is at least as much as this, it is impractical for
any reasonably sized tree. However, we can get
a significant speedup for a fixed m by using the
properties of the POT.
Algorithm 1 uses a message-passing protocol,
similar to many in the graphical models litera-
ture (Koller and Friedman, 2009), to simulate a
Algorithm 1 COMPUTE-PE
Input: POT T with rootX0, number of MPEsm, evidence E
Output: m MPEs for T
1: for X ? T in reverse topological order do
2: Collect messages ?Yi from all children Yi of X
3: ?X = COMPUTE-MPE-MESSAGE(X,m, {?Yi})
4: end for
5: return top m elements of Pr(X0|E)?X0(?) without E
Algorithm 2 COMPUTE-MPE-MESSAGE
Input: POT node X , number of MPEs m, messages from
children ?Yi
Output: Message ?X(?)
1: if X is a leaf node then
2: ?X(x)? 1,?x ? D(X)
3: return ?X
4: end if
5: for x ? D(X) do
6: for ~z = ((y1, ~z1), . . . , (yk, ~zk)) ? {D(?Y1)? . . .?
D(?Yk ) : Pr (Yi = null|X = x,E) < 1} do
7: ??X(x, ~z)?
?
i
[
Pr(Yi = yi|X = x,E)?Yi (yi, ~zi)
]
8: end for
9: ?X(x)? top m elements of ??X(x).
10: end for
11: return ?X
dynamic programming procedure across the lev-
els of the tree (see Figure 5). In Algorithm 2, an
MPE message is computed at each node X using
messages from the children, and sent to the par-
ent. The message from X is the function (or ta-
ble) ?X(x,~z) that represents the probabilities of
the top m explanations, ~z, of the subtree rooted at
X for a particular value of X = x. At the root
node X0 we try all values of x0 to find the top m
MPEs for the entire tree. Note that in step 7, we
41
A
? D
? B B
D
? D
? B
?
? G
I
G
? I
(a)
A =
 B
? D
? B B
D
? D
? B
?
? G
I
G
? I
(b)
A =
 C
? D
? B B
D
? D
? B
?
? G
I
G
? I
(c)
A =
 D
? D
? B B
D
?
? G
? D
? B I
G
? I
(d)
Figure 5: COMPUTE-MPE applied to the exam-
ple POT. (a) Inference starts with the messages be-
ing passed up from the leaves to the root A. Every
message ?X is an m ? n table that contains the
probabilities for the m-best MPEs of the subtree
rooted at X for all the n domain values of X . (b)
At the root, A is set to its first element B, and its
marginal Pr(A = B) is combined with the mes-
sage ?B . The semantics of the POT ensures that
the other messages can be safely ignored because
those subtrees are known to be null with probabil-
ity 1. (c) A is set to C and only the essential at-
tribute G is non-null. (d) A is set to its final el-
ement D, and consequently both the node D and
the nonessential attribute G are non-null and their
messages are mutually independent.
need the marginal P (Y |X,E) which can be ef-
ficiently computed by a parallel message-passing
method. Evidence nodes can only appear as leaves
because of our soft evidence representation, and
are encompassed by the base case. The algorithm
leverages the fact that the joint of any entire sub-
tree rooted at a node that is null with probability 1
can be safely assumed to be null with probability
1. The validity of Algorithm 1 is proven in Ap-
pendix A.
4.3 Complexity Analysis
At a POT node with at most n values and branch-
ing factor k, we do nmaximizations over the prod-
uct space of k nm-sized lists. Thus, the time
complexity of Algorithm 1 on a POT with N
nodes is O(N(nm)k) and the space complexity is
O(Nnmk). (Insertion sort maintains a sorted list
truncated at m elements to keep track of the top
m elements at any time.) However, the algorithm
is significantly faster on specialization nodes be-
cause only one child is applicable and needs to be
considered in the maximization (step 7). In the ex-
treme case of a specialization-only POT, the time
and space complexities both drop to O(Nmn).
A similar algorithm for incrementally finding
m-best MPEs in a general BN is given in Srinivas
and Nayak (1996). However, our approach has the
ability to leverage the null semantics in POTs re-
sulting in significant speedup as described above.
This is crucial because the run-time complexity of
enumerating MPEs is known to be PPP -Complete
for a general BN (Kwisthout, 2008).
5 Empirical Evaluation
To test the effectiveness of our POT approach, we
compare it to a frame-based baseline system for
inferring user intentions.
The baseline system uses a hierarchical frame-
based approach. Each frame maps to a par-
ticular user intention, and the frames are filled
concurrently from the dialog observations. The
slots from a turn overwrite matching slots re-
ceived in previous turns. The baseline system uses
the same ontology as the POT to insure that it
only produces consistent hypotheses, e.g., it never
produces ?Venue=Service, Cuisine=Japanese? be-
cause Service does not have a Cuisine attribute.
When several hypotheses compete, the system se-
lects the one with the maximum allocated slots.
We implemented the POT engine based on the
Probabilistic Network Library (Intel, 2005). It
takes a POT specification as input, receives the
ASR slots, and returns its m-best MPEs.
Using a tourism information spoken dialog sys-
tem, we collected a corpus of 375 dialogs from
15 users with a total of 720 turns (details in
Appendix B). Evaluation is performed by run-
ning these collected dialogs in batch and pro-
viding the ASR slots of each turn to both the
baseline and POT belief-tracking systems.2 Af-
ter each turn, both systems return their best hy-
pothesis of the overall user intention in the form
of a set of concept-value pairs. These hypothe-
2Speech recognition and understanding was performed
using the Nuance Speech Recognition System v8.5 running
manual and statistical grammars with robust interpretation.
42
System Precision Recall F1
Top hypothesis 0.84 0.81 0.83
Top 2 hypotheses 0.87 0.84 0.85
POT Top 3 hypotheses 0.89 0.85 0.87
Top 4 hypotheses 0.91 0.86 0.89
Top 5 hypotheses 0.92 0.86 0.89
Baseline 0.84 0.79 0.81
Table 1: Precision/recall results comparing the
baseline system against the POT-based system on
the 25-scenario experiment. Results are averaged
over all 15 users.
?1 ?0.8 ?0.6 ?0.4 ?0.2 00.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Log?likelihood of top POT hypothesis 
F1
Figure 6: F1 score as a function of the log-
likelihood of the top hypothesis for the user?s goal.
ses are compared to the true user intention ex-
pressed so far in the dialog (e.g., if the user wants
a cheap restaurant but has not mentioned it yet,
PriceRange=Cheap is not considered part of the
ground truth). This offline approach allows us to
compare both versions on the same input.
Table 1 shows the precision/recall results for the
experiment based on comparing the set of true user
intention concepts to the inferred hypotheses of
the POT and baseline systems. The average word
error rate for all users is 29.6%. The POT sys-
tem shows a 2% improvement in recall and F1
over the baseline. Additionally, leveraging the m-
best hypotheses beyond just the top one could help
enhance performance or guide useful clarification
questions as shown by the improved performance
when using the top 2?5 hypotheses; we assume
an oracle for selecting the hypothesis with highest
F1 among the top m hypotheses. All of the CPTs
in the POT (besides the structural constraints) are
uniformly distributed. Thus, the performance of
the POT could be further improved by training the
CPTs on real data.
To assess the quality of likelihood returned by
the POT as a belief confidence measure, we binned
dialog turns according to the log-likelihood of the
top hypothesis and then computed the F1 score of
each bin. Figure 6 shows that belief log-likelihood
is indeed a good predictor of the F1 score. This
information could be very useful to a dialog man-
ager to trigger confirmation or clarification ques-
tions for example.
6 Discussion
The definition and construction of POTs provide a
principled and systematic way to construct proba-
bilistic models for an SDS. While any BN can be
used to model the space of user intentions, design-
ing an effective network is not an easy task for sys-
tem designers not well versed in graphical mod-
els. In previous belief tracking work, researchers
describe their networks with little indication on
how they arrived at the specific structure (Paek and
Horvitz, 2000; Thomson and Young, 2009). Prior
work on ontologies for SDSs (van Zanten, 1998;
Noh et al, 2003) as well as the prominence of
concept hierarchies in other areas such as object-
oriented programming and knowledge engineer-
ing make them a natural and intuitive way of repre-
senting SDS domains. The development of POTs
builds on past research on constructing BNs based
on ontological knowledge (Helsper and van der
Gaag, 2002; Pfeffer et al, 1999).
While most approaches to belief tracking in the
dialog systems community make a strict indepen-
dence assumption between concepts (Bohus and
Rudnicky, 2006; Williams, 2006), POTs model
the dependencies between concepts connected by
specialization and composition relationships while
remaining significantly more tractable than gen-
eral BNs and being very straightforward to de-
sign. The null semantics allow a POT to capture
disjoint values and the applicability of attributes
which are common aspects of concept ontologies.
Obviously, a POT cannot capture all types of con-
cept relationships since each concept can have
only one parent. However, this restriction allows
us to perform efficient exact computation of the
m-best MPEs which is a significant advantage.
Statistical Relational Learning approaches such as
Markov Logic Networks (Richardson and Domin-
gos, 2006) have been developed for more general
relational models than strict ontologies, but they
lack the parsimony and efficiency of POTs.
43
Thomson and Young (2009) describe an ap-
proach to dialog management based on a partially
observable Markov decision process (POMDP)
whose policy depends only on individual con-
cepts? marginal distributions rather than on the
overall user intention. Because their system per-
forms belief tracking with a dynamic Bayesian
network (DBN) rather than a static BN, the ex-
act marginal computation is intractable and the au-
thors use loopy belief propagation to compute the
marginals. Even then, they indicate that the depen-
dencies of the subgoals must be limited to enable
tractability. In practice, all concepts are made in-
dependent except for the binary validity nodes that
deterministically govern the dependence between
nodes (similar to the null semantics of a POT).
Williams (2007) also represents the user goal as
a DBN for a POMDP-based DM. They perform
belief updating using particle filtering and approx-
imate the joint probability over the user intention
with the product of the concept marginals. This
could lead to inaccurate estimation for condition-
ally dependent concepts.
Among authors who have used m-best lists of
dialog states for dialog management, Higashinaka
et al (2003) have shown empirically that main-
taining multiple state hypotheses facilitates shorter
dialogs. Their system scores each dialog state
using a linear combination of linguistic and dis-
course features, and this score is used by a hand-
crafted dialog policy. While illustrating the advan-
tages of m-best lists, this scoring approach lacks
theoretical justification and ability to include prior
knowledge that POTs inherit from BNs.
7 Conclusion
We have presented the POT framework for belief
tracking in an SDS. We have shown how a POT
can be constructed from the domain ontology and
provided an exact algorithm to infer the user?s in-
tention in real-time. POTs strike a balance be-
tween representing rich concept dependencies and
facilitating efficient tracking of them-best user in-
tentions based on exact joint probabilities rather
than approximations such as concept marginals.
References
D. Bohus and A. Rudnicky. 2006. A K Hypotheses
+ Other Belief Updating Model. In AAAI Workshop
on Statistical and Empirical Approaches to Spoken
Dialogue Systems.
D. Goddeau, H. Meng, J. Polifroni, S. Seneff, and
S. Busayapongchai. 1996. A Form-Based Dialogue
Manager for Spoken Language Applications. In IC-
SLP.
E. Helsper and L. van der Gaag. 2002. Building
Bayesian Networks through Ontologies. In Euro-
pean Conference on Artificial Intelligence.
R. Higashinaka, M. Nakano, and K. Aikawa. 2003.
Corpus based Discourse Understanding on Spoken
Dialog Systems. In Annual Meeting on Association
for Computational Linguistics.
Intel. 2005. Probabilistic Network Library. http://
sourceforge.net/projects/openpnl/.
D. Koller and N. Friedman. 2009. Probabilistic
Graphical Models: Principles and Techniques. MIT
Press.
J. Kwisthout. 2008. Complexity Results for Enumerat-
ing MPE and Partial MAP. In European Workshop
on Probabilistic Graphical Models.
H. Noh, C. Lee, and G. Lee. 2003. Ontology-based
Inference for Information-seeking in Natural Lan-
guage Dialog Systems. In IEEE International Con-
ference on Industrial Informatics.
T. Paek and E. Horvitz. 2000. Conversation as Ac-
tion under Uncertainty. In Uncertainty in Artificial
Intelligence.
A. Pfeffer, D. Koller, B. Milch, and K. T. Takusagawa.
1999. Spook: A system for probabilistic object-
oriented knowledge representation. In Uncertainty
in Artifical Intelligence.
C. Rich and C. Sidner. 1998. COLLAGEN: a Col-
laboration Manager for Software Interface Agents.
An International Journal: User Modeling and User
Adapted Interaction, 8.
M. Richardson and P. Domingos. 2006. Markov Logic
Networks. Machine Learning, 62:107?136.
S. Srinivas and P. Nayak. 1996. Efficient Enumeration
of Instantiations in Bayesian Networks. In UAI.
B. Thomson and S. Young. 2009. Bayesian Update
of Dialogue State: A POMDP Framework for Spo-
ken Dialogue Systems. Computer Speech and Lan-
guage.
G. van Zanten. 1998. Adaptive Mixed-Initiative Dia-
logue Management. In IEEE Workshop on Interac-
tive Voice Technology for Telecommunications Ap-
plications.
J. Williams. 2006. Partially Observable Markov Deci-
sion Processes for Dialog Management.
J. Williams. 2007. Using Particle Filters to Track
Dialogue State. In IEEE Workshop on Automatic
Speech Recognition & Understanding.
44
A Analysis of the Inference Algorithm
Theorem 1. Algorithm 1 returns the top m MPEs
of the POT along with their joint probabilities.
Proof. We first prove this for the special case of
m = 1 to simplify notation. For the base case
of a node with no children, Algorithm 2 sim-
ply returns a message with all probabilities at
1 for all values of that node. Now, consider a
node X with children Y1, . . . , Yk. Let Desc(Y )
be the descendants of node Y . Since Algo-
rithm 2 given node X returns exactly one expla-
nation, z for each x ? D(X), we will define
?X(x) = ?X(x, z). Now, to show that ?X(x) =
maxDesc(X) Pr(Desc(X)|X = x,E), that is, Al-
gorithm 2 returns the top explanation of the entire
subtree rooted at X for every value in D(X), we
use structural induction on the tree.
max
Desc(X)
Pr(Desc(X)|X = x,E)
= max
Y1:k,Desc(Y1:k)
Pr(Y1:k,Desc(Y1:k)|X = x,E)
= max
Y1:k,Desc(Y1:k)
?
i
Pr(Yi|X = x,E) Pr(Desc(Yi)|Yi, E)
=
?
i
max
Yi,Desc(Yi)
[
Pr(Yi|X = x,E) Pr(Desc(Yi)|Yi, E)
]
=
?
i
max
Yi
[
Pr(Yi|X = x,E) max
Desc(Yi)
Pr(Desc(Yi)|Yi, E)
]
=
?
i
max
Yi
[
Pr(Yi|X = x,E)?Yi(yi)
]
{Inductive step}
= ?X(x).
The proof for m > 1, where every maximization
returns a list of the top m elements, is similar.
B Dialogs in the Tourist Information
Domain
Each user conducted 25 dialogs according to pre-
scribed scenarios for the tourist information do-
main. The order of scenarios was randomized for
each user. Sample scenarios:
1. Find a good and cheap Mexican restaurant in
Mountain View.
2. There is a medical emergency and you need
to get to the hospital. Find a route.
3. You need to find your favorite coffee fran-
chise. You have 10 minutes to get coffee.
4. Find a place to buy some fruits and vegeta-
bles.
5. Find a Chinese restaurant in Santa Clara with
good ambiance, and display travel distance.
6. Find an ATM on Castro Street in Mountain
View.
Figure 7 shows a typical interaction with the
system for the first scenario along with a possi-
ble hypothesis inferred by the system at every turn
of the dialog. Figure 8 shows an example where
the POT system is able to discard an incorrect ob-
servation about a restaurant based on the accumu-
lated belief about bookstores over multiple turns.
Figure 9 shows how the POT is able to leverage the
ontological structure to pick out higher-level con-
cepts with lower confidence scores over spurious
low-level concepts with higher confidence scores.
45
User Find a Mexican restaurant in Mountain View.
Hypothesis [venue restaurant] [area mountain view] [cuisine italian]
{Note: Mexican is misrecognized as Italian.}
User No, Mexican.
Hypothesis [venue restaurant] [area mountain view] [cuisine mexican]
User Show me ones with at least four star rating.
Hypothesis [venue restaurant] [area mountain view] [cuisine mexican] [rating four star]
User I want a cheap place.
Hypothesis [venue restaurant] [area mountain view] [cuisine mexican] [rating four star] [price cheap]
User Is there anything on Castro?
Hypothesis [venue restaurant] [area mountain view] [street castro] [cuisine mexican] [rating four star] [price cheap]
Figure 7: A sample dialog in the tourism information domain showing the inferred hypothesis of the
user?s intention at every turn. The information response from the system?s back-end is based on its
current hypothesis.
User utterance Where is the bookstore?
ASR where is the bookstore
True hypothesis [action venue] [venue store] [sell book]
Baseline hypothesis [action venue] [venue store] [sell book]
POT hypothesis [action venue] [venue store] [sell book]
User utterance Store on Market Street.
ASR store on market street
True hypothesis [action venue] [venue store] [sell book] [street market]
Baseline hypothesis [action venue] [venue store] [sell book] [street market]
POT hypothesis [action venue] [venue store] [sell book] [street market]
User utterance In downtown.
ASR dennys
True hypothesis [action venue] [venue store] [sell book] [street market] [area downtown]
Baseline hypothesis [action venue] [venue restaurant] [brand dennys]
POT hypothesis [action venue] [venue store] [sell book] [street market]
Figure 8: A dialog showing the ASR input for the user?s utterance, and the corresponding true, baseline,
and POT hypotheses. The POT is able to correctly discard the inconsistent observation in the third turn
with the observations in previous turns.
User utterance Where should I go to buy Lego for my kid?
SU slots ?Venue Store 38? ?ServiceType GolfCourse 60?
True hypothesis [action venue] [venue store] [storetype toy]
Baseline hypothesis [action venue] [venue service] [servicetype golf course]
POT hypothesis [action venue] [venue store]
Figure 9: A single dialog turn showing the SU slots for the user?s utterance, and the corresponding
baseline, POT, and true hypotheses. Any system that looks at the individual confidence scores will base
its hypothesis on the ?ServiceType GolfCourse 60? slot. Instead, the POT hypothesis is influenced by
?Venue Store 38? because its score in combination with the concept?s location in the POT makes it more
likely than the other slot.
46
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 165?174,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
The Dynamics of Action Corrections in Situated Interaction
Antoine Raux
Honda Research Institute USA
Mountain View, CA, USA.
araux@honda-ri.com
Mikio Nakano
Honda Research Institute Japan
Wako, Japan
nakano@jp.honda-ri.com
Abstract
In spoken communications, correction ut-
terances, which are utterances correct-
ing other participants utterances and be-
haviors, play crucial roles, and detecting
them is one of the key issues. Previ-
ously, much work has been done on au-
tomatic detection of correction utterances
in human-human and human-computer di-
alogs, but they mostly dealt with the cor-
rection of erroneous utterances. How-
ever, in many real situations, especially in
communications between humans and mo-
bile robots, the misunderstandings man-
ifest themselves not only through utter-
ances but also through physical actions
performed by the participants. In this pa-
per, we focus on action corrections and
propose a classification of such utterances
into Omission, Commission, and Degree
corrections. We present the results of our
analysis of correction utterances in dialogs
between two humans who were engaging
in a kind of on-line computer game, where
one participant plays the role of the re-
mote manager of a convenience store, and
the other plays the role of a robot store
clerk. We analyze the linguistic content,
prosody as well as the timing of correction
utterances and found that all features were
significantly correlated with action correc-
tions.
1 Introduction
Recent progress in robot technology made it a re-
ality to have robots work in offices and homes, and
spoken dialog is considered to be one of the most
desired interface for such robots. Our goal is to
build a spoken dialog interface for robots that can
move around in an office or a house and execute
tasks according to humans? requests.
Building such spoken dialog interface for robots
raises new problems different from those of tra-
ditional spoken/multimodal dialog systems. The
intentions behind human utterances may vary de-
pending on the situation where the robot is and the
situation changes continuously not only because
the robot moves but also because humans and ob-
jects move, and human requests change. In this
sense human-robot interaction is situated.
Of the many aspects of situated interaction, we
focus on the timing structure of interaction. Al-
though traditional spoken dialog systems deal with
some timing issues such as turn-taking and han-
dling barge-ins, timing structure in human-robot
interaction is far more complex because the robot
can execute physical actions and those actions can
occur in parallel with utterances.
In this work we are concerned specifically with
corrections in situated interaction. In joint physi-
cal tasks, human corrective behavior, which allows
to repair discrepancies in participants? mutual un-
derstanding, is tightly tied to actions.
While past work on non-situated spoken dialog
systems has shown the necessity and feasibility of
detecting and handling corrections (Kitaoka et al,
2003; Litman et al, 2006; Gieselman and Osten-
dorf, 2007; Cevik et al, 2008), most of these mod-
els assume that corrections target past utterances
and rely on a strict turn-based structure which is
frequently violated in situated interaction. When
dialog is interleaved with physical actions, the spe-
cific timing of an utterance relative to other utter-
ances and actions is more relevant than the turn
sequence.
In this paper, we propose a classification of er-
rors and corrections in physical tasks and analyze
the properties of different types of corrections in
the context of human-human task-oriented inter-
actions in a virtual environment. The next section
gives some characteristics of corrections in situ-
ated interaction. Section 3 describes our experi-
165
Alice : Put it right above (1)
the lamp stand
Bob : Here? (2)
Alice : A little bit more (3)
to the right.
(Bob Moves the frame left) (4)
Alice : No the right! (5)
(Bob Moves the frame right) (6)
Alice : More... (7)
Alright, that?s ... (8)
(A bee flies next to Bob) (9)
Alice : Watch out! That bee is
going to sting you! (10)
Figure 1: Example dialog from a situated task.
mental set up and data collection effort. Section 4
presents the results of our analysis of corrections
in terms of timing, prosodic, and lexical features.
These results are discussed in Section 5.
2 Corrections in Situated Tasks
2.1 Situated Tasks
We define a situated task as one for which two or
more agents interact in order to perform physical
actions in the (real or virtual) world. Physical ac-
tions involve moving from one place to another,
as well as manipulating objects. In many cases,
interaction happens simultaneously with physical
actions and can be affected by them, or by other
external events happening in the world. For exam-
ple, Figure 1 shows an extract of a (constructed)
dialog where one person (Alice) assists another
(Bob) while he hangs a picture frame on a wall.
This interaction presents some similarities and
differences with unimodal, non-situated dialogs.
In addition to standard back-and-forth turn-taking
as in turns 1-3, this example features utterances
by Alice which are not motivated by Bob?s ut-
terances, but rather by (her perception of) his ac-
tions (e.g. utterance 5 is a reaction to action 4),
as well as external events such as 9, which trig-
gered response 10 from Alice. Therefore the con-
tent of Alice?s utterances is dependent not only on
Bob?s, but also on events happening in the world.
Similarly, the timing of Alice?s utterances is not
only conditioned on Bob?s speech, prosody, etc,
but also on asynchronous world events.
Robots and other agents that interact with peo-
ple in real world situations need to be able to ac-
count for the impact of physical actions and world
events on dialog. In the next section and the rest
of this paper, we focus on correction utterances
and how situational context affects how and when
speakers produce them.
2.2 Corrections
Generally speaking, a correction is an utterance
which aims at signaling or remedying a misunder-
standing between the participants of a conversa-
tion. In other word, corrections help (re)establish
common ground (Clark, 1996).
2.2.1 Previous Work
There are many dimensions along which correc-
tions can be analyzed and many researchers have
addressed this issue. Conversational Analysis
(CA) has, from its early days, 1 concerned it-
self with corrections (usually called repairs in CA
work) (Schegloff et al, 1977).
More recently, spoken dialog systems re-
searchers have investigated ways to automatically
recognize corrections. For instance, Litman et al
(2006) exploited features to automatically detect
correction utterances. In addition, several attempts
have been made to exploit the similarity in speech
sounds and speech recognition results of a correc-
tion and the previous user utterance to detect cor-
rections (Kitaoka et al, 2003; Cevik et al, 2008).
Going beyond a binary correction/non-
correction classification scheme, Levow (1998)
distinguished corrections of misrecognition errors
from corrections of rejection errors and found
them to have different prosodic features. Rog-
driguez and Schlangen (2004) and Rieser and
Moore (2005) classify corrections according to
their form (e.g. Repetition, Paraphrase, Addition
of information...) and function. The latter aspect
is mostly characterized in terms of the source of
the problem that is being corrected using models
of communication such as that of Clark (1996).
In all of this very rich literature, corrections are
assumed to target utterances from another partic-
ipant (or even oneself, in the case of self-repair)
that conflict with the hearer?s expectations. While
some work on embodied conversational agents
(Cassell et al, 2001; Traum and Rickel, 2002)
does consider physical actions as possible cues
to errors and corrections, the actions are typically
communicative in nature (e.g. nods, glances, ges-
tures). Comparatively, there is extremely little
work on corrections that target task actions.
166
A couple of exceptions are Traum et al (1999),
who discuss the type of context representation
needed to handle action corrections, and Fu-
nakoshi and Tokunaga (2006), who present a
model for identifying repair targets in human-
robot command-and-control dialogs. While im-
portant, these papers focus on theoretical planning
aspects of corrections whereas this paper focuses
on an empirical analysis of human conversational
behavior.
2.2.2 Action Corrections in Situated
Interaction
As seen above, the vast majority of prior work on
corrections concerned corrections of previous (er-
roneous) utterances (i.e. utterance corrections). In
contrast, in this paper, we focus exclusively on
corrections that target previous physical actions
(i.e. action corrections).
While some classification schemes of utterance
corrections are applicable to task corrections (e.g.
those based on the form of the correction itself),
we focus on differences that are specific to action
corrections.
Namely, we distinguish three types of action er-
rors and their related action corrections:
Commission errors occur when Bob performs an
action that conflicts with Alice?s expectation.
Action 4 of Figure 1 is a commission error,
which is corrected in turn 5.
Omission errors occur when Bob fails to react to
one of Alice?s utterances. A typical way for
Alice to correct an omission error is to repeat
the utterance to which Bob did not react.
Degree errors occur when Bob reacts with an ap-
propriate action to Alice?s utterance but fails
to completely fulfill Alice?s goal. This is il-
lustrated by Alice?s use of ?More? in turn 7
in response to Bob?s insufficient action 6.
Figure 3 illustrates the three error categories
based on extracts from the corpus.
In some ways, the dichotomy Commission er-
rors/Omission errors parallels that of Misrecogni-
tions/Rejections by Levow (1998). This type of
classification is also commonly used to analyze
human errors in human factors research (Wick-
ens et al, 1998). In addition to these two cat-
egories, we added the Degree category based on
our observation of the data we collected. This as-
pect is somewhat specific to certain kinds of phys-
ical actions (those that can be performed to differ-
ent degrees, as opposed to binary actions such as
?opening the door?). However, it seems general
enough to be applied to many collaborative tasks
relevant to robots such as guidance, tele-operation,
and joint construction.
For an automated agent, being able to classify
a user utterance into one of these four categories
(including non-action-correction utterances) could
be very useful to make fast, appropriate decisions
such as canceling the current action, or asking a
clarification question to the user. This is impor-
tant because in human-robot interaction, respon-
siveness to a correction can be critical in avoiding
physical accidents. For instance, if the robot de-
tects that the user issued a commission error cor-
rection, it can stop performing its current action
even before understanding the details of the cor-
rection.
In the rest of the paper, we analyze some lexi-
cal, prosodic and temporal characteristics of action
corrections in the context of human-human con-
versations in a virtual world.
3 The Konbini Domain and System
3.1 Simulated Environments for
Human-Robot Interaction Research
One obstacle to the empirical study of situated in-
teraction is that it requires a fully functional so-
phisticated robot to collect data and conduct ex-
periments. Most such complex robots are still
fragile and thus it is typically challenging to run
user studies with naive subjects without severely
limiting the tasks or the scope of the interaction.
Another issue which comes with real world inter-
action is that it is difficult for the experimenter to
control or monitor the events that affect the inter-
action. Most of the time, an expensive manual
annotation of events and actions is required (see
Okita et al (2009) for an example of such an ex-
perimental setup).
To avoid these issues, robot simulators have
been used. Koulouri and Lauria (2009) devel-
oped a simulator to collect dialogs between hu-
man and simulated robot using a Wizard-of-Oz
method. The human can see a map of a town and
teaches the robot a route and the operator oper-
ates the robot but he/she can see only a small area
around the robot in the map. However, the dialog
167
is keyboard-based, and the situation does not dy-
namically change in this setting, making this ap-
proach unsuitable to the study of timing aspects.
Byron and Fosler-Lussier (2006) describe a cor-
pus of spoken dialogs collected in a setting very
similar to the one we are using but, again, the en-
vironment appears to be static, thus limiting the
importance of the timing of actions and utterances.
In this section, we describe a realistic, PC-based
virtual world that we used to collect human-human
situated dialogs.
3.2 Experimental Setup
In our experiment, two human participants collab-
orate in order to perform certain tasks pertaining
to the management of a small convenience store in
a virtual world. The two participants sit in differ-
ent rooms, both facing a computer that presents
a view of the virtual store. One of the partici-
pants, the Operator (O) controls a (simulated) hu-
manoid robot whose role is to answer all customer
requests. The other participant plays the role of a
remote Manager (M) who sees the whole store but
can only interact with O through speech.
Figure 2 shows the Operator and Manager
views. M can see the whole store at any time, in-
cluding how many customers there are and where
they are. In addition, M knows when a particu-
lar customer has a request because the customer?s
character starts blinking (initially green, then yel-
low, then red, as time passes). M?s role is then to
guide O towards the customers needing attention.
On the other hand, O sees the world through the
?eyes? of the robot, whose vision is limited both
in terms of field of view (90 degrees) and depth
(degradation of vision with depth is produced by
adding a virtual ?fog? to the view). When ap-
proaching a customer who has a pending request,
O?s view display the customer?s request in the
form of a caption.1 O can act upon the virtual
world by clicking on certain object such as items
on the counter (to check them out), machines in
the store (to repair them when needed), and vari-
ous objects littering the floor (to clean them up).
Each action takes a certain amount of time to per-
form (between 3 and 45 seconds), indicated by a
progress bar that decreases as O keeps the pointer
on the target object and the left mouse button
down. Once the counter goes to zero the action is
1No actual speech interaction happens between the Oper-
ator and the simulated customers.
completed and the participants receive 50 (for par-
tially fulfilling a customer request) or 100 points
(for completely fulfilling a request).
When the session begins, customers start enter-
ing the store at random intervals, with a maximum
of 4 customers in the store at any time. Each cus-
tomer follows one of 14 predefined scenarios, each
involving between 1 and 5 requests. Scenarios
represent the customer?s moves in terms of fixed
way points. As a simplification, we did not imple-
ment any reactive path planning. Rather, the ex-
perimenter, sitting in a different room than either
subject has the ability to temporarily take control
of any customer to make them avoid obstacles.
3.3 System Implementation: the Siros
architecture
The experimental system described above was im-
plemented using Siros (SItuated RObot Simula-
tor) a new client/server architecture developed at
Honda Research Institute USA to conduct human-
robot interaction research in virtual worlds. Siros
is similar to the architectures used by certain on-
line video games. The server?s role is to manage
the virtual world and broadcast world updates to
all clients so that they can be rendered to the hu-
man participants. The server receives commands
from the Operator client (robot moves), runs the
simulated customers according to the scenarios,
and maintains the timer and the score. Anytime
the trajectory of an entity (robot, customer, object)
changes, the server broadcasts the related infor-
mation, including entity location, orientation, and
speed, to all clients.
Clients are in charge of rendering a given view
of the virtual world. Rendering itself is performed
by the open source Ogre 3D engine (Open Source
3D Graphics Engine, 2010). In addition, clients
handle all required user interaction such as robot
control and mouse-based object selection. All net-
work messages and user actions are logged into
a text file for further processing. Finally, clients
have the ability to log incoming audio to a wave
file, allowing synchronization between the audio
signal, the user actions, and virtual world events.
Spoken communication itself is handled by an ex-
ternal VoIP client.2
2We used the open source Mumble/Murmur (Mum-
ble/Murmur Project, 2010) system.
168
(a) Manager View (b) Robot View
Figure 2: Screenshots of the Konbini data collection system.
3.4 The Konbini Corpus
3.4.1 Data Collection
Using the system described above, we collected
data from 18 participants. There were 15 male and
3 female participants. All were adults living in the
United States, fluent in English. All were regular
users of computers but their experience with on-
line games was diverse (from none at all to regu-
lar player). All were co-workers (associates or in-
terns) at Honda Research Institute USA, and thus
they knew each other fairly well.
Participants were randomly paired into teams.
After being given the chance to read a brief in-
troduction to the experiment?s design and goals,
the participants did two two-minute practice ses-
sions to familiarize themselves with the task and
control. To avoid providing too much informa-
tion about the layout of the store from the start,
the practice sessions used a different virtual world
than the experimental sessions. The participants
switched roles between the two practice sessions
to get a sense of what both roles entailed (Man-
ager and Operator). After these sessions, the
team did one 10-minute experimental session, then
switched roles once again and did another 10-
minute session. Because the layout of the store
was kept the same between the two experimental
sessions, the first session represents a condition in
which the Operator learns the store layout as they
are performing the tasks, whereas the second ses-
sion corresponds to a case where the Operator al-
ready has knowledge of the layout. Overall, 18 10-
minute sessions were collected, including audio
recordings as well as timestamped logs of world
updates and operator actions.
3.4.2 Annotation
All recordings were orthographically transcribed
and checked. The first author then segmented the
transcripts into dialog acts (DAs). A DA label was
attached to each act, though this information is not
used in the present paper.
Subsequently, the first author annotated each
semantic unit with the action correction labels
described in section 2: Non-correction, Omis-
sion Correction, Commission Correction, Degree
Correction. This annotation was done using the
Anvil video annotation tool,3 which presented au-
dio recordings, transcripts, a timeline of operator
actions, as well as videos of the computer screens
of the participants. Only Manager DAs were anno-
tated for corrections. The second author also anno-
tated a subset of the data in the same way to evalu-
ate inter-annotator agreement. Cohen?s kappa be-
tween the two annotators was 0.67 for the 4-class
task, and 0.76 for the binary task of any-action-
correction vs non-action-correction, which is rea-
sonable, though not very high, indicating that cor-
rection annotation on this type of dialogs is a non-
trivial task, even for human annotators.
4 Analysis of Action Corrections
4.1 Overview
The total number of DAs in the corpus is 6170.
Of those, 826 are corrections and 5303 are non-
corrections. Overall, corrections account thus for
13.4% of the dialog acts. The split among the dif-
ferent correction classes is roughly equal as shown
in Table 5 given in appendix. We found however
significant differences across participants, in terms
3http://www.anvil-software.de
169
of total number of DAs (form 162 to 516), propor-
tion of corrections among those DAs (from 6.8%
to 30.6%), as well as distribution among the three
types of action corrections.
In this section, we present the results of our sta-
tistical analysis of the correlation between a num-
ber of features and correction type. To evaluate
statistical significance, we performed a one-way
ANOVA using each feature as dependent variable
and the correction type as independent variable.
All features described here were found to signifi-
cantly correlate with correction type.
4.2 Features Affecting Corrections
4.2.1 Timing
For each manager DA, we computed the time since
the beginning/end of the previous manager and
operator DAs, as well as of operator?s actions
(walk/turn). To account for reaction time, and
based on our observations we ignored events hap-
pening less than 1 second before a DA.
Table 1 shows the mean durations between these
events and a Manager DA, depending on the act?s
correction class. All corrections happen closer to
Manager dialog acts than non-corrections, which
reflects the fact that corrections typically occur in
phases when the Manager gives instructions, as
well as the fact that the Manager often repeats
corrections. Commission and Degree corrections
are produced closer to Operator actions than either
non-corrections or Omission corrections. This re-
flects the fact that both Commission and Degree
corrections are a reaction to an event that occurred
(the Operator moved or stopped moving unex-
pectedly), whereas Omission corrections address
a lack of action from the Operator, and act there-
fore as a ?time-out? mechanism.
To better understand the relationship between
moves and the timing of corrections, we computed
the probability of a given DA to be an Omission,
Commission and Degree correction as a function
Time since last... NC O C D
Mgr. DA 3.4 s 2.4 s 2.8 s 2.6 s
Ope. DA 5.8 s 6.7 s 6.5 s 7.5 s
Ope. move start 3.8 s 3.1 s 2.3 s 2.5 s
Ope. move end 3.9 s 3.3 s 2.7 s 2.3 s
Table 1: Mean duration between dialog acts and
Operator movements and the beginning of differ-
ent corrections.
Feature Non-Corr Om Com Deg
Perc. voiced 0.48 0.46 0.55 0.53
Min F0 -0.61 -0.41 -0.40 -0.56
Max F0 0.81 0.68 1.02 0.46
Mean F0 -0.03 0.12 0.28 -0.05
Min Power -1.35 -1.24 -1.18 -1.55
Max Power 0.85 0.89 1.14 0.62
Mean Power -0.03 0.09 0.24 -0.2
Table 2: Mean Z-score of prosodic features for dif-
ferent correction classes.
of the time elapsed since the Operator last started
to move. Figure 4 shows the results.
The probability of a DA being an Omission cor-
rection is relatively stable over time. This is con-
sistent with the fact that Omission corrections are
related to lack of action rather than to a specific
action to which the Manager reacts. On the other
hand, the probability of a Commission, and to
lesser extent, Degree correction sharply decreases
with time after an action.
4.2.2 Prosody
We extracted F0 and intensity from all manager
audio files using the Praat software (Boersma and
Weenink, 2010). We then normalized pitch and in-
tensity for each speaker using a Z-transform in or-
der to account for individual differences in mean
and standard deviation. For each DA, we com-
puted the minimum, maximum, and mean pitch
and intensity, using values from voiced frames.
Table 2 shows the mean Z-score of the prosodic
features for the different correction classes. Com-
mission corrections feature higher pitch and inten-
sity than all other classes. This is due to the fact
that such corrections typically involve a higher
emotional level, when the Manager is surprised
or even frustrated by the behavior of the Operator.
In contrast, Degree corrections, which represent a
smaller mismatch between the Operator?s action
and the Manager?s expectations are more subdued,
with mean power and intensity values lower than
even those of non-corrections.
4.2.3 Lexical Features
In order to identify potential lexical characteristics
of correction utterances, we created binary vari-
ables indicating that a specific word from the vo-
cabulary (804 distinct words in total) appears in
a given DA based on the manual transcripts. We
computed the mutual information of those binary
170
variables with DA?s correction label.
Figure 3 shows the 10 words with highest
mutual information. Not surprisingly, negative
words (?NO?, ?DON?T?), continuation words
(?MORE?, ?KEEP?) are correlated with respec-
tively commission and degree corrections. On the
other hand, positive words (?OKAY?, ?YEAH?)
are strong indicators that a DA is not a correction.
Another lexical feature we computed was a flag
indicating that a certain Manager DA is an ex-
act repetition of the immediately preceding Man-
ager DA. The intuition behind this feature is that
corrections often involve repetitions (e.g. ?Turn
left [Operator turns right] Turn left!?). Overall,
10.6% of the DAs are repetitions. This num-
ber is only 6.4% on non-corrections but jumps to
45.6%, 22.5%, and 43.4% on, respectively, Omis-
sion, Commission, and Degree corrections. This
confirms that, as for utterance corrections, detect-
ing exact repetitions could prove useful for correc-
tion classification.
4.2.4 ASR Features
Since our goal is to build artificial agents, we
investigated features related to automatic speech
recognition. We used the Nuance Speech Recog-
nition System v8.5. Using cross-validation, we
trained a statistical language model for each cor-
rection category on the transcripts of the training
portion of the data. We then ran the recognizer se-
quentially with all 4 language models, which gen-
erated a confidence score for each category.
Table 4 shows the mean confidence scores ob-
tained on DAs of each class using a language
model trained on specific classes. While the
matching LM gives the highest score for any given
class, some classes have consistently higher scores
than others. In particular, Commission corrections
receive low confidence scores, which might hurt
the effectiveness of these features. Indeed, lexi-
cal content alone might not be not enough to dis-
tinguish non-corrections and various categories of
corrections since the same expression (e.g. ?Turn
left?) can express a simple instruction, or any kind
of correction, depending on context.
5 Discussion
The results provide support for the correction
classification scheme we proposed. Not only
do corrections differ in many respects from non-
correction utterances, but there are also signifi-
cant differences between Omission, Commission,
PPPPPPPPLM
Corr.
NC O C D
Non-Correction 32.3 28.5 25.0 29.5
Omission 24.0 30.0 23.3 27.2
Commission 26.6 29.8 25.7 27.9
Degree 24.2 28.7 23.9 32.6
Table 4: Mean ASR confidence score using class-
specific LMs.
and Degree corrections. Timing features seem
most useful to distinguish Commission and De-
gree corrections from Omission corrections and
non-corrections. Emphasized prosody (high pitch
and energy) is a particularly strong indicator of
Commission, as well as Omission corrections.
Lexical cues could be useful to all categories, pro-
vided the speech recognizer is accurate enough to
recognize them, which is particularly challenging
on this data given the very conversational nature
of the speech. Finally, ASR scores are also po-
tentially useful features, particularly for Omission
and Degree corrections.
One advantage of timing over all other features
discussed here is that timing information is avail-
able before the correction is actually uttered. This
means that such information could be used to al-
low fast reaction, or to prime the speech recog-
nizer based on the instantaneous probability of the
different classes of correction.
6 Conclusion
In this paper, we analyzed correction utterances in
the context of situated spoken interaction within
a virtual world. We proposed a classification of
action correction utterances into Omission, Com-
mission, and Degree corrections. Our analysis
of human-human data collected using a PC-based
simulated environment shows that the three types
of corrections have unique characteristics in terms
of prosody, lexical features, as well as timing with
regards to physical actions. These results can
serve as the basis for further investigations into au-
tomatic detection and understanding of correction
utterances in situated interaction.
References
Paul Boersma and David Weenink. 2010.
Praat: doing phonetics by computer,
http://www.fon.hum.uva.nl/praat.
171
Word (W) P (Non? Corr|W ) P (Om|W ) P (Com|W ) P (Deg|W )
MORE 0.41 0.01 0.02 0.56
NO 0.55 0.04 0.33 0.07
RIGHT 0.67 0.15 0.04 0.14
TURN 0.69 0.17 0.06 0.08
LEFT 0.65 0.18 0.07 0.10
OKAY 0.99 0.00 0.00 0.00
YEAH 0.99 0.00 0.00 0.00
DON?T 0.59 0.01 0.33 0.07
WAY 0.49 0.05 0.42 0.04
KEEP 0.79 0.03 0.03 0.15
Table 3: Keywords with highest mutual information with correction category.
Donna K. Byron and Eric Fosler-Lussier. 2006.
The OSU Quake 2004 corpus of two-party situated
problem-solving dialogs. In Proc. 15th Language
Resource and Evaluation Conference (LREC?06).
Justine Cassell, Timothy Bickmore, Hannes Hgni
Vilhja?msson, and Hao Yan. 2001. More Than Just a
Pretty Face: Conversational Protocols and the Affor-
dances of Embodiment. Knowledge-Based Systems,
14:55?64.
Mert Cevik, Fuliang Weng, and Chin hui Lee. 2008.
Detection of repetitions in spontaneous speech di-
alogue sessions. In Proc. Interspeech 2008, pages
471?474.
Herbert Clark. 1996. Using Language. Cambridge
University Press.
Kotaro Funakoshi and Takenobu Tokunaga. 2006.
Identifying repair targets in action control dialogue.
In Proc. EACL 2006, pages 177?184.
Petra Gieselman and Mari Ostendorf. 2007. Problem-
Sensitive Response Generation in Human-Robot Di-
alogs. In Proc. SIGDIAL 2002.
Norihide Kitaoka, Naoko Kakutani, and Seiichi Naka-
gawa. 2003. Detection and Recognition of Correc-
tion Utterance in Spontaneously Spoken Dialog. In
Proc. Eurospeech 2003, pages 625?628.
Theodora Koulouri and Stanislao Lauria. 2009. Ex-
ploring miscommunication and collaborative be-
haviour in human-robot interaction. In Proc. SIG-
DIAL 2009, pages 111?119.
Gina-Anne Levow. 1998. Characterizing and recog-
nizing spoken corrections in human-computer dia-
logue. In Proc. COLING-ACL ?98, pages 736?742.
Diane Litman, Julia Hirschberg, and Marc Swerts.
2006. Characterizing and predicting corrections in
spoken dialogue systems. Computational Linguis-
tics, 32(3):417?438.
The Mumble/Murmur Project. 2010.
http://mumble.sourceforge.net.
Sandra Y Okita, Victor Ng-Thow-Hing, and Ravi K
Sarvadevabhatla. 2009. Learning Together:
ASIMO Developing an Interactive Learning Partner-
ship with Children. In Proc. RO-MAN 2009.
OGRE Open Source 3D Graphics Engine. 2010.
http://www.ogre3d.org.
Verena Rieser and Johanna Moore. 2005. Implications
for generating clarification requests in task-oriented
dialogues. In Proc. 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL-05),
pages 239?246.
Kepa Josepa Rogdriguez and David Schlangen. 2004.
Form, intonation and function of clarification re-
quests in german task-oriented spoken dialogues. In
Proc. 8th Workshop on the Semantics and Pragmat-
ics of Dialogue (CATALOG?04).
Emanuel A. Schegloff, Gail Jefferson, and Harvey
Sacks. 1977. The preference for self-correction
in the organization of repair in conversation. Lan-
guage, 53(2):361?382.
David Traum and Jeff Rickel. 2002. Embodied
agents for multiparty dialogue in immersive virtual
worlds. In Proc. International Joint Conference on
Autonomous Agents and Multi-agent Systems (AA-
MAS 2002), pages 766?773.
David R. Traum, Carl F. Andersen, Waiyian Chong,
Darsana P. Josyula, Yoshi Okamoto, Khemdut Pu-
rang, Michael O?Donovan-Anderson, and Donald
Perlis. 1999. Representations of Dialogue State
for Domain and Task Independent Meta-Dialogue.
Electron. Trans. Artif. Intell., 3(D):125?152.
Christopher D. Wickens, Sallie E. Gordon, and Yili
Liu. 1998. An Introduction to Human Factors En-
gineering. Addison-Wesley Educational Publishers
Inc.
172
operator 
action
operator 
utterance
manager 
utterance
walk
turn
to your 
right
forward
left
omission 
correction
3?13??
to your 
right
turn to your 
right
3?14?? 3?15?? 3?16??
stop and .. 
ah too late
(a) Omission correction
operator 
action
operator 
utterance
manager 
utterance
walk
turn
forward
right
2?49?? 2?50?? 2?51??
commission
correction
2?52??
keep on going straight and then uh
no no
the other way
(b) Commission correction
operator 
action
operator 
utterance
manager 
utterance
walk
turn
forward
right
0?11?? 0?12?? 0?13?? 0?14??
left
degree 
correction
0?15?? 0?16??
more 
right
more 
right
more 
rightturn slightly right
(c) Degree correction
Figure 3: Example omission, commission, and degree errors and corrections. The corresponding videos
can be found at http://sites.google.com/site/antoineraux/konbini.
173
2 4 6 80
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
Time since last move (s)
P(Om
)
(a) Omissions
2 4 6 80
0.01
0.02
0.03
0.04
0.05
0.06
Time since last move (s)
P(Co
m)
(b) Commissions
2 4 6 80
0.01
0.02
0.03
0.04
0.05
0.06
Time since last move (s)
P(De
g)
(c) Degree
Figure 4: Evolution of the probability of occurrence of corrections over time after an Operator move.
Participant Total Non-Corr Omission Commission Degree
Total 6170 5303 (86.6%) 298 (4.8%) 277 (4.5%) 251 (4.1%)
1 338 299 (88.5%) 19 (5.6%) 15 (4.4%) 5 (1.5%)
2 249 232 (93.1%) 10 (4.0%) 2 (0.8%) 0 (0.0%)
3 440 383 (87.0%) 25 (5.7%) 11 (2.5%) 9 (2.0%)
4 265 247 (93.2%) 4 (1.5%) 8 (3.0%) 0 (0.0%)
5 313 270 (86.3%) 15 (4.8%) 5 (1.6%) 17 (5.4%)
6 238 198 (83.2%) 22 (9.2%) 13 (5.5%) 5 (2.1%)
7 426 361 (84.7%) 30 (7.0%) 10 (2.3%) 23 (5.4%)
8 244 218 (89.3%) 3 (1.2%) 13 (5.3%) 9 (3.7%)
9 162 137 (84.6%) 4 (2.5%) 13 (8.0%) 8 (4.9%)
10 229 202 (88.2%) 6 (2.6%) 3 (1.3%) 12 (5.2%)
11 380 326 (85.8%) 16 (4.2%) 19 (5.0%) 19 (5.0%)
12 427 385 (90.2%) 16 (3.7%) 11 (2.6%) 15 (3.5%)
13 327 281 (85.9%) 5 (1.5%) 14 (4.3%) 27 (8.3%)
14 516 358 (69.4%) 38 (7.4%) 79 (15.3%) 39 (7.6%)
15 362 332 (91.7%) 13 (3.6%) 6 (1.7%) 11 (3.0%)
16 392 321 (81.9%) 34 (8.7%) 27 (6.9%) 10 (2.6%)
17 362 338 (85.4%) 19 (4.8%) 22 (5.6%) 17 (4.3%)
18 466 415 (89.1%) 19 (4.1%) 6 (1.3%) 25 (5.4%)
Table 5: Frequency of the different types of corrections per participant.
174
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 169?178,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Landmark-based Location Belief Tracking in a Spoken Dialog System
Yi Ma
The Ohio State University
Columbus, OH 43210
may@cse.ohio-state.edu
Antoine Raux, Deepak Ramachandran, Rakesh Gupta
Honda Research Institute, USA
425 National Ave, Mountain View, CA 94043
{araux,dramachandran,
rgupta}@hra.com
Abstract
Many modern spoken dialog systems use
probabilistic graphical models to update their
belief over the concepts under discussion, in-
creasing robustness in the face of noisy input.
However, such models are ill-suited to prob-
abilistic reasoning about spatial relationships
between entities. In particular, a car naviga-
tion system that infers users? intended desti-
nation using nearby landmarks as descriptions
must be able to use distance measures as a fac-
tor in inference. In this paper, we describe
a belief tracking system for a location iden-
tification task that combines a semantic belief
tracker for categorical concepts based on the
DPOT framework (Raux and Ma, 2011) with
a kernel density estimator that incorporates
landmark evidence from multiple turns and
landmark hypotheses, into a posterior proba-
bility over candidate locations. We evaluate
our approach on a corpus of destination set-
ting dialogs and show that it significantly out-
performs a deterministic baseline.
1 Introduction
Mobile devices such as smart phones and in-car in-
fotainment systems have generated demand for a
new generation of location-based services such as
local business search, turn-by-turn navigation, and
social event recommendation. Accessing such ser-
vices in a timely manner through speech is a crucial
requirement, particularly on the go when the user is
unable to resort to other modalities e.g. where safety
regulations prohibit drivers from using buttons or a
touchscreeen while driving.
In such systems, a Point of Interest (POI)
or a destination such as a restaurant, store or a
public place is often specified. For example, a
car navigation system needs the user to input the
destination before giving directions. Similarly, a
photo tagging application must allow its users to
designate the location where a picture was taken.
While postal addresses can be used to unambigously
identify locations, they are often either unknown
or hard for users to remember. A more natural
(though potentially ambiguous) means of speci-
fying locations is to use landmarks such as ?the
Italian restaurant near Red Rock
cafe on Castro Street? or ?the bakery
near that mall with a Subway and
a 7 Eleven?. A location-based dialog system
that understands referring expressions using land-
marks could lead to more succinct dialogs, higher
recognition accuracy and a greater appearance of
intelligence to the user.
We present a system that performs belief track-
ing over multiple turns of user speech input to infer
the most probable target location. The user inter-
acts with the system through speech in order to spec-
ify a target location, and may include references to
one or more landmarks. Such a system must han-
dle two sources of uncertainty. First, ASR is notori-
ously error-prone and modern ASR engines provide
ranked lists of possible interpretations of speech in-
put rather than single hypotheses. Second, the suit-
ability of a particular landmark or its likelihood of
usage by the speaker depends on a number of factors
such as distance, size and prominence of the land-
mark, familiarity of the user and his expectation of
169
common ground for understanding. These factors,
or at least the resulting variability, must be taken into
account when making inferences about target loca-
tions from landmark-based expressions.
The first source of ambiguity (speech understand-
ing) has been the target of research on belief tracking
(Mehta et al, 2010; Raux and Ma, 2011; Thomson
and Young, 2010). In previous work, the concepts
of interest are entities that are ontologically related
(i.e. with is-a or has-a relations), thus discrete prob-
abilistic graphical models such as DBNs have gen-
erally sufficed as representations. But these mod-
els are ill-suited for dense continuous spatial rela-
tions like the distance between any two locations on
a map. In this paper, we introduce a kernel-based
belief tracker as a probabilistic model for inferring
target locations from (uncertain) landmarks. The
kernel-based representation allows a natural way to
weigh the suitability of a landmark and the speech
understanding confidence. The output of this tracker
is combined with that of a Dynamic Probabilistic
Ontology Tree (DPOT) (Raux and Ma, 2011), which
performs ontological reasoning over other features
of the target location, to give a posterior distribu-
tion over the intended location. We evaluate our ap-
proach on a new corpus of location setting dialogs
specially collected for this work and find it to signif-
icantly outperform a deterministic baseline.
2 Related Work
In the context of a location-based dialog system,
Seltzer et al (2007) describes a speech understand-
ing system designed to recognize street intersec-
tions and map them to a database of valid intersec-
tions using information retrieval techniques. Ro-
bustness is achieved by exploiting both words and
phonetic information at retrieval time, allowing a
soft-matching of the ASR result to the canonical in-
tersection name. Their approach is specifically tar-
geted at intersections, to the exclusion of other types
of landmarks. While intersections are frequently
used as landmarks in North America (where their
study was conducted), this is not always the case
in other cultures, such as Japan (Suzuki and Wak-
abayashi, 2005), where points of interests such as
train stations are more commonly used. Also, their
approach, which is framed as speech understanding,
does not exploit information from previous dialog
turns to infer user intention.
Landmarks have been integrated in route direc-
tions (Pierre-emmanuel Michon, 2001; Tversky and
Lee, 1999) with significant use at origin, destination
and decision points. Further, landmarks have been
found to work better than street signs in wayfind-
ing (Tom and Denis, 2003). The multimodal system
described in (Gruenstein and Seneff, 2007) supports
the use of landmarks from a limited set that the user
specifies by pointing at the map and typing landmark
names. While this allows the landmarks (and their
designations) to be of any kind, the burden of defin-
ing them is on the user.
Spatial language, including landmarks, has also
been the focus of research within the context of
human-robot interaction. (Huang et al, 2010;
MacMahon et al, 2006) describe systems that trans-
late natural language directions into motion paths or
physical actions. These works focus on understand-
ing the structure of (potentially complex) spatial lan-
guage and mapping it into a representation of the
environment. Issues such as imperfect spoken lan-
guage understanding have not been investigated in
this context. Similarly, this vein of spatial language
research has traditionally been conducted on small
artificial worlds with a few dozen objects and places
at most, whereas real-world location-based services
deal with thousands or millions of entities.
3 Hybrid Semantic / Location Belief
Tracking
Our belief tracking system consists of two trackers
running in parallel: a DPOT belief tracker (Raux and
Ma, 2011) and a novel kernel-based location tracker.
The final inference of user intentions is produced by
combining information from the two trackers. The
general idea is to rerank the user goals given spatial
information provided by the location tracker.
3.1 Semantic Belief Tracker
We perform belief tracking over non-landmark con-
cepts such as business name and street using a Dy-
namic Probabilistic Ontology Tree (DPOT) (Raux
and Ma, 2011). A DPOT is a Bayesian Network
composed of a tree-shaped subnetwork representing
the (static) user goal (Goal Network), connected to
170
Figure 1: Top view heat map of spatial distribution with landmarks Subway and 7 Eleven over potential target
places in Mountain View, CA
a series of subnetworks representing the evidence
gathered from each successive dialog turn (Evidence
Networks). Details of the model and an efficient in-
ference method for posterior probability computa-
tions can be found in (Raux and Ma, 2011).
In the context of this paper, the purpose of the
semantic tracker is to update a list of the most
likely target locations using attributes of that
location provided by the user (see Figure 2). In
a local business database, such attributes include
Business Name, Street, Category (e.g.
Japanese restaurant or convenience store), etc.
The structure and parameters of the Goal Network
encode probabilistic ontological relations between
the attributes (e.g. a Mcdonalds would be described
as a fast-food restaurant with high probability)
that can be exploited during inference. These can
be derived from expert knowledge, learned from
data, or as is the case in our experimental system,
populated from a database of local businesses (see
section 4). After each user utterance, the DPOT
outputs a ranked list of user goal hypotheses (an ex-
ample goal hypothesis is [Category=italian
restaurant,Street=castro street]).
Each hypothesis is converted into a query to the
backend database, and the posterior probability of
the hypothesis is split equally among all matching
entries. This results in a ranked list of database
entries corresponding to the system?s belief over
potential target locations, with potentially many
entries having the same probability.
3.2 Kernel-based Location Tracker
Landmark concepts extracted by the Natural Lan-
guage Understanding module (NLU) are passed to
the location tracker, which maintains a distribution
over coordinates of potential target locations. Each
such landmark concept is treated as evidence of spa-
tial proximity of the target to the landmark and the
distribution is accordingly updated. Any location in
the database can serve as a landmark observation,
including major POIs such as train stations or pub-
lic facilities. If the name of a generic chain store
with multiple locations such as Subway is used for
the landmark, then an observation corresponding to
each individual location is added to the tracker.
For each observed landmark `, the location
tracker constructs a 2-dimensional Gaussian kernel
with mean equal to the longitude and latitude of the
landmark (?` = (long`, lat`)) and a fixed covari-
171
Figure 2: Overview of the hybrid semantic / location belief tracking approach; the database entry in shade is the
underlying true target place to which the provided landmark is close
ance matrix ?` for each landmark:
 `(t) =
1
2?|?|1/2 exp( 
1
2(t  ?`)
T? 1` (t  ?`))
This kernel density determines the conditional prob-
ability that the target is at coordinates t =
(longt, latt) given the fixed landmark `. The covari-
ance matrix ?` and hence the shape of the kernel
can be adjusted for different landmarks depending
on considerations such as the familiarity, size and
prominence of the landmark (a large historic monu-
ment is likely to be used as a landmark for locations
much further away than a small corner grocery store)
etc.
The probability density of the location t being the
target is then given by a weighted mixture model:
Pr(t|L) =
X
`2L
w` `(t) (1)
where L is the set of candidate landmarks returned
by the NLU (see Section 4.1) up to the current turn
and w` is set to the confidence score of ` from the
NLU. Thus candidate landmarks that have higher
confidence in the NLU will contribute more strongly
to the total likelihood. Since Pr(t|L) is a den-
sity function, it is unnormalized. In Figure 1, we
show the kernel tracker distribution for a dialog state
where Subway and 7 Eleven are provided as
landmarks.
The kernel density estimator is a simple approach
to probabilistic spatial reasoning. It is easy to imple-
ment and requires only a moderate amount of tuning.
It naturally models evidence from multiple speech
hypotheses and multiple provided landmarks, and
it benefits from accumulated evidence across dia-
log turns. It can also potentially be used to model
more general kinds of spatial expressions by using
appropriate kernel functions. For example, ?Along
Castro street? can be modeled by a Gaussian
with an asymmetric covariance matrix such that the
shape of the resulting distribution is elongated and
concentrated on the street. While ?Two blocks
away from ...? could be modeled by adding
an extra ?negative? density kernel that extends from
172
Figure 3: Overview of the Destination Setting System
the center of the landmark to a distance two blocks
away.
3.3 Combining the Two Trackers
At each turn, the updated results from the Seman-
tic and Location tracker are combined to give a
single ranked list of likely target locations. In
Figure 2, this process is illustrated for a dia-
log turn where two possible concepts are identi-
fied   a category attribute [Category:italian
restaurant] and a landmark [Landmark:red
rock coffee company]. These are passed to
the DPOT tracker and the location tracker respec-
tively. The output of the DPOT is used to retrieve
and score matching database entries. The score for
each entry is reweighted by the kernel density esti-
mator measured at the coordinates of the location 1:
Pr(eij) = (
pi
Ni
)? ? Pr(eij |L) (2)
where Ni is the number of matching database en-
tries retrieved from ith goal hypothesis (having joint
probability pi) and eij is the jth such entry (j 2
[1..Ni]). The exponent ? for the posterior term is
introduced to account for scale difference between
the semantic score and the kernel density.
The set of candidate entries can then be reranked
according to Eq 2 and returned as the output of the
combined belief tracker.
Figure 4: Structure of the Goal Network for the experi-
mental system.
4 Evaluation
4.1 Experimental System
The architecture of our experimental system is
shown in Figure 3. The web client, shown in Figure
5, runs in the participant?s web browser and displays
the target location of the current scenario using the
Google Map API. The user?s goal is to convey this
target location to the system through speech only.
The system backend consists of a database of
2902 businesses located in Mountain View, Cali-
fornia with their name, street, street number, busi-
ness category, latitude and longitude provided. The
grammar rules for the NLU and the probability ta-
bles in the DPOT are populated from this database.
The web client captures the user speech and sends
it to our server with a push-to-talk interface based
on the WAMI toolkit (Gruenstein et al, 2008). The
server uses a commercial cloud-based ASR service
with generic acoustic and language models, which
were not adapted to our task. The n-best list of hy-
potheses from the ASR is sent to our robust natural
1The scores are renormalized to between 0 and1.
173
language understanding module for parsing.
Our NLU uses a hybrid approach combining
a weighted finite-state transducer (WFST) with
string matching based rescoring of the output. The
WFST incorporates out-of-grammar word loops
that allow skipping input words at certain points
in the parse2. This parser robustly maps free form
utterances (e.g. ?Okay let?s go to that
Italian place near, uh..., Red
Rock Cafe, on Castro?) to semantic frames
(e.g. [Category=italian restaurant,
Street=castro street, Landmark=red
rock coffee company]).
The NLU confidence score is computed based on
the number of words skipped while parsing, and
how close the important concept words match the
canonical phrases found in the database. For in-
stance, ?Red Rock Cafe? matches the canoni-
cal name ?Red Rock Coffee Company? with
high confidence because rare words (Red, Rock)
are identical, and differing but common words
(Cafe, Coffee, Company) have a low weight
in the score. The string matching score is based
on the term-frequency/inverse document frequency
(TF-IDF) metric commonly used in information re-
trieval. In our case, the weight of different terms
(IDF) is estimated based on their frequency of occur-
rence in different database entries (i.e. how uniquely
they describe a matching entry). We use the sec-
ondstring open-source library (Cohen et al, 2003)
for string matching. For any ASR hypothesis, the
NLU is likely to generate several parses which are
all merged in a global list of candidate parses.
For each candidate parse, the system generates
a set of dialog acts (one per concept in the parse)
which are input to the belief tracker with their confi-
dence score. Following the approach described in
section 3, dialog acts corresponding to the Land-
mark concept are sent to the kernel-based location
belief tracker, while all other concepts are sent to a
Dynamic Probabilistic Ontology Trees (DPOT) se-
mantic belief tracker, whose structure is shown in
Figure 4. We use a two-level tree. The value of
the root node (Id) is never directly observed and
represents the database entry targeted by the user.
2This module is implemented using the OpenFST library
(Allauzen et al, 2007)
The leaf nodes correspond to the relevant attributes
Name, Category, and Street. For any database
entry e, attribute a and value of that attribute va, the
conditional probability P (a = va|Id = e) is set to 1
if the value of a is va for entry e in the database, and
to 0 otherwise. For attributes such as Category,
which allow several possible values for each entry,
the probability is split equally among valid values.
After each user utterance, the network is augmented
with a new Evidence Network capturing the possi-
ble interpretations and their likelihood, as computed
by the NLU. The posterior probability distribution
over user goals is computed and rescored using the
kernel-based location tracker.
Finally, the Response Generator takes the highest
scoring target location from the belief tracker and
sends it back to the web client which displays it on
the map and also indicates what are the values of
the Name, Category, and Street concepts for
the top belief (see Figure 5). If the top belief lo-
cation does not match the goal of the scenario, the
user can speak again to refine or correct the system
belief. After the user has spoken 5 utterances, they
also get the choice of moving on to the next scenario
(in which case the dialog is considered a failure).
4.2 Data collection
To evaluate our approach, we ran a data collection
experiment using the Amazon Mechanical Turk on-
line marketplace. We defined 20 scenarios grouped
into 4 Human Intelligence Tasks (HITs). Figure 5
shows a screen shot of the web interface to the sys-
tem. In each scenario, the worker is given a target
location to describe by referring to nearby landmark
information. The target locations were chosen so as
to cover a variety of business categories and nearby
landmarks. The compensation for completing each
set of 5 scenarios is 1 US dollar. Before their first
scenario, workers are shown a video explaining the
goal of the task and how to use the interface, in
which they are specifically encouraged to use land-
marks in their descriptions.
At the beginning of each scenario, the target
location is displayed on the map with a call-
out containing a short description using either a
generic category (e.g. Italian restaurant,
Convenience store) or the name of a chain
store (e.g. Subway, Mcdonalds). The worker
174
Figure 5: Screen capture of the data collection web interface where the target location is an Italian restaurant (in
green, underlying target place is [Ristorante Don Giovanni]) and after the first turn user input ?Italian
restaurant? with a system belief [Frankie, Johnnie & Luigi, Too] in blue returned without any land-
mark information provided so far
then interacts with the system described in section
4.1 until either the system?s top belief matches the
target location, or they decide to skip the scenario.
4.3 Data Statistics
Overall, 99 workers participated in the data col-
lection, providing 948 dialogs (2,869 utterances, 3
turns per scenario on average), which two of the
authors manually transcribed and annotated for di-
alog acts. 76% of the dialogs (46% of utterances)
contained a reference to a landmark. Other strate-
gies commonly used by workers to uniquely identify
a location include using a category or chain name
and a street, as well as explicitly mentioning the tar-
get business name (although workers were explicitly
discouraged form doing so). Figure 7 in appendix
provides one example dialog from the corpus.
Overall, the workers provided 203 unique land-
marks, of which 143 (70%) are in the database.
Workers were able to set the target destination
within 5 turns in 60.1% of the dialogs, which we
hereafter refer to as task successes. However, based
on the manual transcripts, 19.0% of the dialogs
could not have succeeded with the current system
because the workers used landmark or attributes that
do not appear in the database. Since the focus of this
study is robustness rather than coverage, we base our
evaluation on the remaining 768 dialogs, which we
split between a development set of 74 dialogs and
a test set of 694 dialogs. On this test set, the live
system has a task success rate of 70.6%. By inspect-
ing the log files, we noticed that runtime issues such
as timeouts prevented the system from getting any
belief from the belief tracker in 6.3% of the dialogs.
The mean Word Error Rate (WER) per worker on
the test set is 27.5%. There was significant variabil-
ity across workers, with a standard deviation 20.7%.
Besides the usual factors such as acoustic noise and
non-native accents, many of the errors came from
the misrecognition of business names, due to the fact
that ASR uses an open-ended language model that is
tuned neither to Mountain View, nor to businesses,
nor to the kind of utterances that our set up tends
to yield, which is a realistic situation for large scale
practical applications.
Concept precision of the top scoring NLU hypoth-
esis is 73.0% and recall is 57.7%. However, when
considering the full list of NLU hypotheses and us-
ing an oracle to select the best one for each turn,
precision increases to 89.3% and recall to 66.2%,
underscoring the potential of using multiple input
hypotheses in the belief tracker.
175
42% 
50% 
69% 
83% 
0% 
10% 
20% 
30% 
40% 
50% 
60% 
70% 
80% 
90% 
W/o landmarks 
baseline 
W/o landmarks BT W/ landmarks 
baseline 
W/ landmarks BT 
Tas
k S
ucc
ess
 Ra
te 
Figure 6: Batch evaluation of the proposed (BT) and baseline approaches with and without landmark information.
4.4 Batch Results
To further analyze the performance of our approach,
we conducted a series of batch experiments on the
data collected with the runtime system. We first
tuned the parameters of the belief tracker ? and ?l
(see section 3) on the development set (? = 3 and
?l corresponds to a circular Gaussian with standard
deviation 500 meters).
We compare the tuned proposed belief tracking
system (labeled BT) with three other versions. First,
we define a deterministic baseline system which, at
each turn, updates its belief by overwriting each con-
cept?s value with the value found in the top NLU
hypothesis. Based on this (single) user goal hy-
pothesis, we query the database to retrieve match-
ing entries. If the current goal hypothesis con-
tains a Landmark concept, the baseline system se-
lects the matching entry that is closest to any loca-
tion matching the landmark name, by computing the
pairwise distance between candidate target locations
and landmarks.
We also compute the performance of both the
baseline and our proposed approach without us-
ing landmark information at all. In these versions,
the belief over the attributes (Name, Street, and
Category) is updated according to either the top
NLU hypothesis (baseline) or the DPOT model (BT)
and the first matching database entry is returned, ig-
noring any landmark information.
Figure 6 shows the task success of each of the four
versions on the test set. First, it is clear that land-
mark information is critical to complete the tasks in
this corpus since both systems ignoring landmarks
perform significantly worse than their counterparts.
Second, the belief tracking approach significantly
outperforms the deterministic baseline (83.0% vs
69.3%, p < 0.001 using sign test for matched pairs).
To further analyze the performance of the sys-
tem in different input conditions, we split the di-
alogs based on their measured concept accuracy (ex-
pressed in terms of concept F-measure). All dialogs
with an F-measure higher than the median (70.0%)
are labeled as high-accuracy, while the other half of
the data is labeled as low-accuracy. While both the
proposed approach and the baseline perform simi-
larly well for high-accuracy dialogs (task success of
resp. 96.0% and 92.8%, difference is not statisti-
cally significant), the difference is much larger for
low-accuracy dialogs (70.0% vs 45.8%, p < 0.001)
confirming the robustness of the landmark-based be-
lief tracking approach when confronted with poor
input conditions.
5 Conclusion
In this paper, we have explored the possibilities of
incorporating spatial information into belief tracking
in spoken dialog systems. We proposed a landmark-
based location tracker which can be combined with
a semantic belief tracker to output inferred joint user
goal. Based on the results obtained from our batch
experiments, we conclude that integrating spatial in-
formation into a location-based dialog system could
improve the overall accuracy of belief tracking sig-
nificantly.
176
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. Openfst: A
general and efficient weighted finite-state transducer
library. In Proceedings of the Ninth International
Conference on Implementation and Application of Au-
tomata (CIAA) Lecture Notes in Computer Science,
volume 4783, pages 11?23. Springer.
W.W. Cohen, P. Ravikumar, and S.E. Fienberg. 2003.
A comparison of string distance metrics for name-
matching tasks. In Proceedings of the IJCAI-2003
Workshop on Information Integration on the Web
(IIWeb-03), pages 73?78.
Alexander Gruenstein and Stephanie Seneff. 2007. Re-
leasing a multimodal dialogue system into the wild:
User support mechanisms. In In Proceedings of the
8th SIGdial Workshop on Discourse and Dialogue,
pages 111?119, September.
A. Gruenstein, I. McGraw, and I. Badr. 2008. The wami
toolkit for developing, deploying, and evaluating web-
accessible multimodal interfaces. In Proceedings of
the 10th international conference on Multimodal in-
terfaces, pages 141?148. ACM.
Albert Huang, Stefanie Tellex, Abe Bachrach, Thomas
Kollar, Deb Roy, and Nick Roy. 2010. Natural lan-
guage command of an autonomous micro-air vehicle.
In International Conference on Intelligent Robots and
Systems (IROS).
M. MacMahon, B. Stankiewicz, and B. Kuipers. 2006.
Walk the talk: Connecting language, knowledge, and
action in route instructions. In Proceedings of the
National Conference on Artificial Intelligence, vol-
ume 21, page 1475. Menlo Park, CA; Cambridge, MA;
London; AAAI Press; MIT Press; 1999.
N. Mehta, R. Gupta, A. Raux, D. Ramachandran, and
S. Krawczyk. 2010. Probabilistic ontology trees for
belief tracking in dialog systems. In Proceedings of
the 11th Annual Meeting of the Special Interest Group
on Discourse and Dialogue, pages 37?46. Association
for Computational Linguistics.
Michel Denis Pierre-emmanuel Michon. 2001. When
and why are visual landmarks used in giving direc-
tions? In D. R. Montello, editor, Spatial Information
Theory, Volume 2205 of Lecture Notes in Computer
Science, pages 292?305. Springer, Berlin.
A. Raux and Y. Ma. 2011. Efficient probabilistic track-
ing of user goal and dialog history for spoken dialog
systems. In Proceedings of Interspeech 2011.
Michael L. Seltzer, Yun-Cheng Ju, Ivan Tashev, and Alex
Acero. 2007. Robust location understanding in spo-
ken dialog systems using intersections. In Proceed-
ings of Interspeech 2007, pages 2813?2816.
K. Suzuki and Y. Wakabayashi. 2005. Cultural dif-
ferences of spatial descriptions in tourist guidebooks.
Spatial Cognition IV. Reasoning, Action, and Interac-
tion, 3343:147?164.
B. Thomson and S. Young. 2010. Bayesian update of di-
alogue state: A pomdp framework for spoken dialogue
systems. Computer Speech & Language, 24(4):562?
588.
Ariane Tom and Michel Denis. 2003. Referring to land-
mark or street information in route directions: What
difference does it make? Spatial Information The-
ory. Foundations of Geoghraphic Information Science,
Lecture Notes in Computer Science, 2825/2003:362?
374.
Barbara Tversky and Paul U. Lee. 1999. Pictorial and
verbal tools for conveying routes. In Proceedings of
the International Conference on Spatial Information
Theory: Cognitive and Computational Foundations
of Geographic Information Science(COSIT). Springer-
Verlag London.
177
User: &Italian&restaurant&near&
ASR: %italian%restaurant%near%
NLU: %Category=Italian%Restaurant%
Category %Italian'Restaurant'
Target !Dominos!Pizza!
Category %Italian'Restaurant'
Target !Dominos!Pizza!
User: &Italian&restaurant&near&Kappo&Nami&Nami&
ASR: %italian%restaurant%near%camp%to%numa%numa%
NLU: %Category=Italian%Restaurant,%Street=Camp%Avenue%
%Category=Italian%Restaurant,%Landmark=Jefunira%Camp%
Category %Italian'Restaurant%%
Street %Camp'Avenue'
Target &No!match!
Category %Italian'Restaurant'
Landmark %Jefunira'Camp'
Target ! !Maldonado?s%
User: &Italian&restaurant&near&Tempta5ons&
ASR: %italian%restaurant%near%temptaAons%
NLU: %Category=Italian%Restaurant,%Landmark=TemptaAons%
Category %Italian'Restaurant'
Street %Camp'Avenue'
Landmark %Tempta5ons'
Target &No!match!
Category %Italian'Restaurant'
Landmark %Jefunira'Camp,'Tempta5ons'
Target &Don!Giovanni!
Baseline' DPOT+Kernels'
Example&Dialog
Figure 7: Comparison between baseline and proposed method on an example dialog whose underlying true target is
an Italian restaurant called Don Giovanni.
178
NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 41?44,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
HRItk: The Human-Robot Interaction ToolKit 
Rapid Development of Speech-Centric Interactive Systems in ROS 
 
 
 
 
 
Abstract 
Developing interactive robots is an extremely 
challenging task which requires a broad range 
of expertise across diverse disciplines, includ-
ing, robotic planning, spoken language under-
standing, belief tracking and action 
management. While there has been a boom in 
recent years in the development of reusable 
components for robotic systems within com-
mon architectures, such as the Robot Operat-
ing System (ROS), little emphasis has been 
placed on developing components for Human-
Robot-Interaction. In this paper we introduce 
HRItk (the Human-Robot-Interaction toolkit), 
a framework, consisting of messaging proto-
cols, core-components, and development tools 
for rapidly building speech-centric interactive 
systems within the ROS environment. The 
proposed toolkit was specifically designed for 
extensibility, ease of use, and rapid develop-
ment, allowing developers to quickly incorpo-
rate speech interaction into existing projects. 
1 Introduction 
Robots that operate along and with humans in settings 
such as a home or office are on the verge of becoming a 
natural part of our daily environment (Bohren et al, 
2011, Rosenthal and Veloso 2010, Kanda et al, 2009, 
Srinivasa et al, 2009). To work cooperatively in these 
environments, however, they need the ability to interact 
with people, both known and unknown to them. Natural 
interaction through speech and gestures is a prime can-
didate for such interaction, however, the combination of 
communicative and physical actions, as well as the un-
certainty inherent in audio and visual sensing make such 
systems extremely challenging to create. 
Developing speech and gesture-based interactive 
robots requires a broad range of expertise, including, 
robotic planning, computer vision, acoustic processing, 
speech recognition, natural language understanding, 
belief tracking, as well as dialog management and ac-
tion selection, among others. This complexity makes it 
difficult for all but very large research groups to devel-
op complete systems. While there has been a boom in 
recent years in the development and sharing of reusable 
components, such as path planning, SLAM and object 
recognition, within common architectures, such as the 
Robot Operating System (ROS) (Quigley, 2009), little 
emphasis has been placed on the development of com-
ponents for Human-Robot Interaction although despite 
the growing need for research in this area.  
Prior work in Human-Robot Interaction has gener-
ally resulted in solutions for specific robotic platforms 
(Clodic et al, 2008) or standalone frameworks (Fong et 
al., 2006) that cannot be easily combined with standard 
architectures used by robotics researchers. Earlier work 
(Kanda et al, 2009, Fong et al, 2006) has demonstrated 
the possibilities of multimodal and multiparty interac-
tion on robotic platforms, however, the tasks and inte-
ractions explored until now have been extremely 
limited, due to the complexity of infrastructure required 
to support such interactions and the expertise required to 
effectively implement and optimize individual compo-
nents. To make significant progress, we believe that a 
common, easy to use, and easily extensible infrastruc-
ture, similar to that supported by ROS, is required for 
multi-modal human-robot interaction. Such a frame-
work will allow researchers to rapidly develop initial 
speech and gesture-based interactive systems, enabling 
them to rapidly deploy systems, observe and collect 
interactions in the field and iteratively improve system 
components based on observed deficiencies. By using a 
common architecture and messaging framework, com-
ponents and component models can easily be upgraded 
and extended by a community of researchers, while not 
affecting other components. 
Towards this goal we have developed HRItk1 
(Human-Robot-Interaction toolkit), an infrastructure 
and set of components for developing speech-centric 
interactive systems within the ROS environment. The 
proposed toolkit provides the core components required 
for speech interaction, including, speech recognition, 
natural language understanding and belief tracking. Ad-
ditionally it provides basic components for gesture rec-
ognition and gaze tracking. 
                                                          
1 HRItk is available for download at: 
http://speech.sv.cmu.edu/HRItk 
Ian Lane1, Vinay Prasad1, Gaurav Sinha1, Arlette Umuhoza1,  
Shangyu Luo1, Akshay Chandrashekaran1 and Antoine Raux2 
1 Carnegie Mellon University, NASA Ames Research Park, Moffett Field, California, USA 
2 Honda Research Institute, Mountain View, California, USA 
lane@cs.cmu.edu, ?araux@honda-?ri.com ?
41
 Figure 1: Overview of core understanding and tracking components within HRItk 
 
2 Framework Overview 
An overview of the core components in the toolkit are 
highlighted in Figure 1. We introduce two classes of 
components required for speech and multimodal interac-
tion into the ROS framework, understanding nodes and 
tracking services. Understanding nodes are perceptual 
components that recognize and understand interaction 
events. Using input from sensors, intermediate 
processing nodes or other understanding components, 
these nodes generate hypotheses about current user in-
put. Tracking services monitor the long term and conti-
nuous aspects of interaction, including user dialog goals 
DQG WKH XVHU?V IRFXV RI DWWHQWLRQ. These services are 
leveraged by components including Dialog Manage-
ment and Action Selection to perform interaction. Addi-
tionally, these services provide context to understanding 
nodes enabling them to apply context-specific 
processing during the understanding phase. 
2.1 Data Processing Nodes 
The understanding components implemented in this 
work heavily leverage existing components developed 
in ROS (Quigley et al, 2009). TKHVHLQFOXGHWKH?open-
ni_kinect?  QRGH ZKLFK processes depth-images from 
the Microsoft Kinect sensor, the ?openni_tracker?
which performs skeletal tracking, and ?uvccam? QRGH
which processes color images from external USB cam-
eras. In the near future we also plan to support far-field 
speech recognition using the HARK_ROS toolkit (Na-
kadai et al, 2010). 
2.2 Understanding Nodes 
Understanding nodes recognize and understand events 
observed during interaction. As input they use either 
data obtained directly from sensors, preprocessed data 
from intermediate processing nodes or output from oth-
er understanding components. They either perform 
processing on explicit interaction events, such as speech 
or gesture input, or process continuous input such as 
joint position or gaze direction. The current understand-
ing nodes implemented within HRItk are listed in Table 
1 along with the ROS topics on which they publish.  
Understanding nodes publish two forms of messag-
HV ?state? PHVVDJHV^READY, START and STOP}, in-
dicating the state of the node and whether an interaction 
event has been detected, DQG ?hypothesis?  PHVVDges 
which enumerate the most likely observed events along 
with a likelihood measure for each. The specific struc-
WXUH RI WKH ?hypothesis?  PHVVDJH is dependent on the 
event being observed. 
2.3 State Tracking Services 
In addition to understanding specific events such as 
utterances or gestures, an interactive system needs to 
track longer term and/or continuous aspects of interac-
tion. Such aspects include user goals, which can span 
VHYHUDO XWWHUDQFHV LQ D GLDORJ DQG WKH XVHU?V IRFXV RI
attention (using, e.g., gaze and posture information). 
These can be defined as characterizing the state of the 
world (i.e. the user, the interaction, or the environment) 
at a given time, with possible reference to history.
42
Table 1: ROS nodes, Topics, Services and Messages implemented within HRItk 
ROS Node Topic / Service (* ) Description of Messages  
Speech Detection 
and Recognition 
speech/state 
speech/hypothesis 
speech/hypothesis/best 
speech/hypothesis/final 
speech/context 
State identifying interaction event, each with a unique eventID 
Partial and final hypotheses generated during speech recognition. 
Outputs include 1-best, N-best hypotheses and confusion net-
works. All output contains confidence or component model scores 
Context indicating dialog-state, domain, task of current interaction 
Natural Language 
Understanding 
dialogact/hypothesis 
dialogact/context 
Hypotheses of Concept/Value-pairs generated during NLU 
Context indicating dialog-state, domain, task of current interaction 
Gesture Recognition 
hand/hypothesis 
hand/context 
Hypothesis set of Gesture-Actions with confidence measure 
Context indicating domain or task of current interaction 
Gaze Tracking 
gaze/hypothesis 
hand/context 
Estimate of gaze direction 
Context listing visually salient objects within users field of view 
Dialog State  
Tracking 
dialogstate/state 
belief * 
dialogstate/context 
Receives an UPDATED message when the belief changes 
Belief over the concept set specified in the service request 
Context indicating system actions potentially affecting belief 
 
In addition, states can be significantly larger objects 
than individual event understanding results, which could 
unnecessarily consume significant bandwidth if con-
stantly broadcast. Therefore, state tracking modules use 
ROS services rather than topics to communicate their 
output to other modules. Any module can send a mes-
sage to the tracking service containing a specific query 
and will receive in response the matching state or belief 
over states. 
In order to allow components to react to changes in 
the state, each state-tracking module publishes an 
UPDATED message to its state topic whenever a new 
state is computed. 
2.4 Component Implementations 
Speech Detection and Recognition is performed using 
a ROS node developed around the Julius Speech Rec-
ognition Engine (Lee and Kawahara, 2009). We se-
lected this engine for its compatibility with HARK 
(Nakadai et al 2010), and its support of common model 
formats. A wrapper for Julius was implemented in C++ 
to support the ROS messaging architecture listed in Ta-
ble 1. Partial hypotheses are output during decoding, 
and final hypotheses are provided in 1-best, N-best and 
Confusion Network formats. Context is supported via 
language model switching. 
In order to develop a Speech Recognition compo-
nent for a new task at minimum two component models 
are required, a pronunciation dictionary, and a language 
model (or recognition grammar). Within HRItk we pro-
vide the tools required to generate these models from a 
set of labeled example utterances. We describe the rapid 
model building procedure in Section 4. 
 
Natural Language Understanding is implemented 
using Conditional Random Fields (Lafferty et al 2001) 
similar to the approach described in (Cohn, 2007). For 
example, given WKH LQSXW XWWHUDQFH ?Take this tray to 
the kitchen? listed in Table 3, three concept/value pairs 
are extracted: Action{Carry}, ?Object{tray}, ?
Room{kitchen}. ?Similar to the speech recognition 
component, the NLU component can be rapidly re-
trained using a set of tagged example sentences. 
 
Gesture Recognition of simple hand positions is im-
plemented using a Kinect depth sensor and previous 
work by Fujimura and Xu (2007) for palm/finger seg-
mentation. Currently, the module publishes a hypothesis 
for the number of fingers raised by the user, though 
more complex gestures can be implemented based on 
this model. 
 
Gaze Tracking is implemented using ASEF filters 
(Bolme et al, 2009) and geometric projection. Separate 
ASEF filters were training to locate the pupils of the left 
and right eye as well as their inner and outer corners. 
Filters were trained on hand-labeled images we col-
lected in-house.  
 
Dialog State Tracking is in charge of monitoring as-
pects of dialog that span multiple turns such as user 
goal. Our implementation is based on the Hound dialog 
belief tracking library developed at Honda Research 
Institute USA. Currently, our belief tracking model is 
Dynamic Probabilistic Ontology Trees (Raux and Ma 
2011), which capture the hidden user goal in the form of 
a tree-shaped Bayesian Network. Each node in the Goal 
Network represents a concept that can appear in lan-
guage and gesture understanding results. The structure 
of the network indicates (assumed) conditional indepen-
dence between concepts. With each new input, the net-
work is extended with evidence nodes according to the 
final understanding hypotheses and the system belief is 
estimated as the posterior probability of user goal nodes 
given the evidence so far. 
A request to the dialog state tracking service takes 
the form of a set of concept names, to which the service 
responds with an m-best list of concept value assign-
ments along with the joint posterior probability. 
  
43
 
 
 
3 Rapid System Build Environment 
The models required for the core interaction compo-
nents in the system can be build from a single set of 
labeled examples ?Examples.txt?DORQJZLWKDconcept 
VWUXFWXUH ILOH ?Structure.txt? used by the Dialog State 
Tracker as shown in Figure 2. Running the automatic 
build procedure on these two files will generate 3 new 
models,  
The data LQ WKH?([DPSOHVW[W? ILOH LVused to train 
the language model and pronunciation dictionary used 
by the Speech Detection and Understanding Node and 
the statistical CRF-parser applied in the Natural Lan-
guage Understanding component. Given a set of labeled 
examples, the three models listed above are trained au-
tomatically without any intervention required from the 
user. Once a system has been deployed, speech input is 
logged, and can be transcribed and labeled with seman-
tic concepts to improve the effectiveness of these com-
ponent models. 
As explained in section 3.5, our dialog state tracker 
organizes concepts in a tree structure. For a given do-
main, we specify that structure in a simple text file 
where each line contains a concept followed by the 
name of the parent concept or the keyword ROOT for 
the root of the tree. Based on this file and on the SLU 
data file, the resource building process generates the 
files required by the Hound belief tracker at runtime. 
7KLV ?RII-the-VKHOI? VWUXFWXUH assumes at each node a 
uniform conditional distribution of children values giv-
en the parent value. These distributions are stored in a 
human-readable text file and can thus be manually up-
dated to more informative values. 
Using the above tools, we have developed a sample 
using the proposed framework for robot navigation task. 
The entire system can be build from a single set of la-
beled examples as shown in Figure 3 used to train the 
language model and a component to perform actions on 
the SLU output. 
 
4 Conclusions  
In this paper we introduce HRItk (the Human-Robot-
Interaction toolkit), a framework, consisting of messag-
ing protocols, components, and development tools for 
rapidly building speech-centric interactive systems 
within the ROS environment. The proposed toolkit pro-
vides all the core components required for speech inte-
raction, including, speech recognition, natural language 
understanding and belief tracking and initial implemen-
tations for gesture recognition and gaze tracking. The 
toolkit is specifically designed for extensibility, ease of 
use, and rapid development, allowing developers to 
quickly incorporate speech interaction into existing 
ROS projects. 
References  
Bohren J., Rusu R., Jones E., Marder-Eppstein E., Pantofaru 
C., Wise M., Mosenlechner L., Meeussen W., and Holzer 
S. 2011. Towards autonomous robotic butlers: Lessons 
learned with the PR2, Proc. ICRA 2011 
Bolme, S., Draper, B., and  Beveridge, J. 2009. Average of 
Synthetic Exact Filters, Proc. CVPR 2009. 
Clodic, A., Cao, H., Alili, S., Montreuil, V., Alami, R. and-
Chatila, R. 2008. Shary: A Supervision System Adapted to-
Human-Robot Interaction. In Proc. ISER 2008. 
Cohn, T. 2007. Scaling conditional random fields for natural 
language processing. University of Melbourne. 
Fong T., Kunz C., Hiatt L. and Bugajska M. 2006. The Hu-
man-Robot Interaction Operating System. Proc. HRI 2006.  
Fujimura, K. and Xu, L. 2007. Sign recognition using con-
strained optimization. Proc. ACCV 2007. 
Kanda, T., Shiomi M., Miyashita Z., Ishiguro H., and Hagita 
N. 2009. An affective guide robot in a shopping mall. In 
Proc. HRI 2009 
Lafferty J., McCallum A., and Pereira F.. Conditional random 
fields: Probabilistic models for segmenting and labeling 
sequence data. In Intl. Conf. on Machine Learning, 2001. 
Lee, A. and Kawahara, T. 2009. Recent Development of Open-
Source Speech Recognition Engine Julius. Proc. Asia-
Pacific Signal and Information Processing Association An-
nual Summit and Conference (APSIPA ASC), 2009. 
Nakadai, K., Takahashi, T., Okuno, H.G., Nakajima, H., Ha-
segawa, Y., and Tsujino, H. 2010. Design and Implementa-
tion of Robot Audition System "HARK" . 
Quigley, M., Gerkey, B., Conley, K., Faust, J., Foote, T. 
Leibs, J., Berger, E., Wheeler, R. and Ng, A. 2009. ROS: 
an open-source robot operating system. Proc. Open-source 
Software Workshop, ICRA 2009. 
Raux, A. and Ma, Y. 2011. Efficient Probabilistic Tracking of 
User Goal and Dialog History for Spoken Dialog Systems. 
Proc. Interspeech 2011. 
Rosenthal S., Veloso M. 2010. Using Symbiotic Relationships 
with Humans to Help Robots Overcome Limitations. In 
Workshop for Collaborative Human/AI Control for Interac-
tive Experiences 2010. 
Srinivasa S., Ferguson D., Helfrich C., Berenson D., Collet A., 
Diankov R., Gallagher G., Hollinger G., Kuffner J., Vande-
Weghe M. 2009. Herb: A Home Exploring Robotic Butler. 
Autonomous Robots, 2009 
Examples.txt 
<Tagged example sentence> <Action> 
 
@Room{kitchen} ? None ?
on ?the ?@Floor{fifth} ?floor ? None ?
take ?this ?@Object{package} ? ?
to ?@Room{room ?123} ? ? Carry ?
Structure.txt 
<Node> <Parent> 
Room ? ? ROOT ?
Floor ? Room ?
Object ? Room 
 
Figure 2: Training examples for robot navigation task 
44
Proceedings of the SIGDIAL 2013 Conference, pages 404?413,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
The Dialog State Tracking Challenge
Jason Williams1, Antoine Raux2?, Deepak Ramachandran3?, and Alan Black4
1Microsoft Research, Redmond, WA, USA 2Lenovo Corporation, Santa Clara, CA, USA
3Nuance Communications, Mountain View, CA, USA 4Carnegie Mellon University, Pittsburgh, PA, USA
jason.williams@microsoft.com araux@lenovo.com deepak.ramachandran@nuance.com awb@cmu.edu
Abstract
In a spoken dialog system, dialog state
tracking deduces information about the
user?s goal as the dialog progresses, syn-
thesizing evidence such as dialog acts over
multiple turns with external data sources.
Recent approaches have been shown to
overcome ASR and SLU errors in some
applications. However, there are currently
no common testbeds or evaluation mea-
sures for this task, hampering progress.
The dialog state tracking challenge seeks
to address this by providing a heteroge-
neous corpus of 15K human-computer di-
alogs in a standard format, along with a
suite of 11 evaluation metrics. The chal-
lenge received a total of 27 entries from 9
research groups. The results show that the
suite of performance metrics cluster into 4
natural groups. Moreover, the dialog sys-
tems that benefit most from dialog state
tracking are those with less discriminative
speech recognition confidence scores. Fi-
nally, generalization is a key problem: in
2 of the 4 test sets, fewer than half of the
entries out-performed simple baselines.
1 Overview and motivation
Spoken dialog systems interact with users via nat-
ural language to help them achieve a goal. As the
interaction progresses, the dialog manager main-
tains a representation of the state of the dialog
in a process called dialog state tracking (DST).
For example, in a bus schedule information sys-
tem, the dialog state might indicate the user?s de-
sired bus route, origin, and destination. Dialog
state tracking is difficult because automatic speech
?Most of the work for the challenge was performed when
the second and third authors were with Honda Research In-
stitute, Mountain View, CA, USA
recognition (ASR) and spoken language under-
standing (SLU) errors are common, and can cause
the system to misunderstand the user?s needs. At
the same time, state tracking is crucial because
the system relies on the estimated dialog state to
choose actions ? for example, which bus schedule
information to present to the user.
Most commercial systems use hand-crafted
heuristics for state tracking, selecting the SLU re-
sult with the highest confidence score, and dis-
carding alternatives. In contrast, statistical ap-
proaches compute scores for many hypotheses for
the dialog state (Figure 1). By exploiting correla-
tions between turns and information from external
data sources ? such as maps, bus timetables, or
models of past dialogs ? statistical approaches can
overcome some SLU errors.
Numerous techniques for dialog state tracking
have been proposed, including heuristic scores
(Higashinaka et al, 2003), Bayesian networks
(Paek and Horvitz, 2000; Williams and Young,
2007), kernel density estimators (Ma et al, 2012),
and discriminative models (Bohus and Rudnicky,
2006). Techniques have been fielded which scale
to realistically sized dialog problems and operate
in real time (Young et al, 2010; Thomson and
Young, 2010; Williams, 2010; Mehta et al, 2010).
In end-to-end dialog systems, dialog state tracking
has been shown to improve overall system perfor-
mance (Young et al, 2010; Thomson and Young,
2010).
Despite this progress, direct comparisons be-
tween methods have not been possible because
past studies use different domains and system
components, for speech recognition, spoken lan-
guage understanding, dialog control, etc. More-
over, there is little agreement on how to evaluate
dialog state tracking. Together these issues limit
progress in this research area.
The Dialog State Tracking Challenge (DSTC)
provides a first common testbed and evaluation
404
Figure 1: Overview of dialog state tracking. In this example, the dialog state contains the user?s desired
bus route. At each turn t, the system produces a spoken output. The user?s spoken response is processed
to extract a set of spoken language understanding (SLU) results, each with a local confidence score. A
set of Nt dialog state hypotheses is formed by considering all SLU results observed so far, including the
current turn and all previous turns. Here, N1 = 3 and N2 = 5. The dialog state tracker uses features of
the dialog context to produce a distribution over all Nt hypotheses and the meta-hypothesis that none of
them are correct.
suite for dialog state tracking. The DSTC orga-
nizers made available a public, heterogeneous cor-
pus of over 15K transcribed and labeled human-
computer dialogs. Nine teams entered the chal-
lenge, anonymously submitting a total of 27 dialog
state trackers.
This paper serves two roles. First, sections 2
and 3 provide an overview of the challenge, data,
and evaluation metrics, all of which will remain
publicly available to the community (DST, 2013).
Second, this paper summarizes the results of the
challenge, with an emphasis on gaining new in-
sights into the dialog state tracking problem, in
Section 4. Section 5 briefly concludes.
2 Challenge overview
2.1 Problem statement
First, we define the dialog state tracking problem.
A dialog state tracker takes as input all of the ob-
servable elements up to time t in a dialog, includ-
ing all of the results from the automatic speech
recognition (ASR) and spoken language under-
standing (SLU) components, and external knowl-
edge sources such as bus timetable databases and
models of past dialogs. It also takes as input a
set of Nt possible dialog state hypotheses, where
a hypothesis is an assignment of values to slots in
the system. The tracker outputs a probability dis-
tribution over the set of Nt hypotheses, and the
meta-hypothesis REST which indicates that none
of them are correct. The goal is to assign probabil-
ity 1.0 to the correct state, and 0.0 to other states.
Note that the set of dialog states is given. Also
note that Nt varies with t ? typically as the dia-
log progresses and more concepts are discussed,
the number of candidate hypotheses increases. An
example is given in Figure 1.
In this challenge, dialog states are generated in
the usual way, by enumerating all slots values that
have appeared in the SLU N-best lists or system
output up until the current turn. While this ap-
proach precludes a tracker assigning a score to an
405
SLU value that has not been observed, the cardi-
nality of the slots is generally large, so the likeli-
hood of a tracker correctly guessing a slot value
which hasn?t been observed anywhere in the input
or output is vanishingly small.
2.2 Challenge design
The dialog state tracking challenge studies this
problem as a corpus-based task ? i.e., dialog state
trackers are trained and tested on a static corpus
of dialogs, recorded from systems using a variety
of state tracking models and dialog managers. The
challenge task is to re-run state tracking on these
dialogs ? i.e., to take as input the runtime system
logs including the SLU results and system output,
and to output scores for dialog states formed from
the runtime SLU results. This corpus-based de-
sign was chosen because it allows different track-
ers to be evaluated on the same data, and because a
corpus-based task has a much lower barrier to en-
try for research groups than building an end-to-end
dialog system.
In practice of course, a state tracker will be used
in an end-to-end dialog system, and will drive ac-
tion selection, thereby affecting the distribution of
the dialog data the tracker experiences. In other
words, it is known in advance that the distribu-
tion in the training data and live data will be mis-
matched, although the nature and extent of the
mis-match are not known. Hence, unlike much
of supervised learning research, drawing train and
test data from the same distribution in offline ex-
periments may overstate performance. So in the
DSTC, train/test mis-match was explicitly created
by choosing test data to be from different dialog
systems.
2.3 Source data and challenge corpora
The DSTC uses data from the public deployment
of several systems in the Spoken Dialog Challenge
(SDC) (Black et al, 2010), provided by the Dialog
Research Center at Carnegie Mellon University. In
the SDC, telephone calls from real passengers of
the Port Authority of Allegheny County, who runs
city buses in Pittsburgh, were forwarded to dialog
systems built by different research groups. The
goal was to provide bus riders with bus timetable
information. For example, a caller might want
to find out the time of the next bus leaving from
Downtown to the airport.
The SDC received dialog systems from three
different research groups, here called Groups A,
B, and C. Each group used its own ASR, SLU,
and dialog manager. The dialog strategies across
groups varied considerably: for example, Groups
A and C used a mixed-initiative design, where the
system could recognize any concept at any turn,
but Group B used a directed design, where the
system asked for concepts sequentially and could
only recognize the concept being queried. Groups
trialled different system variants over a period of
almost 3 years. These variants differed in acoustic
and language models, confidence scoring model,
state tracking method and parameters, number of
supported bus routes, user population, and pres-
ence of minor bugs. Example dialogs from each
group are shown in the Appendix.
The dialog data was partitioned into 5 train-
ing corpora and 4 testing corpora (Table 1).
The partioning was intended to explore different
types of mis-match between the training and test
data. Specifically, the dialog system in TRAIN1A,
TRAIN1B, TRAIN1C, TRAIN2, and TEST1 are all
very similar, so TEST1 tests the case where there
is a large amount of similar data. TEST2 uses the
same ASR and SLU but a different dialog con-
troller, so tests the case where there is a large
amount of somewhat similar data. TEST3 is very
similar to TRAIN3 and tests the case where there
is a small amount of similar data. TEST4 uses a
completely different dialog system to any of the
training data.
2.4 Data preparation
The dialog system log data from all three groups
was converted to a common format, which
described SLU results and system output using
a uniform set of dialog acts. For example, the
system speech East Pittsburgh Bus Schedules.
Say a bus route, like 28X, or say I?m not sure.
was represented as hello(), request(route), exam-
ple(route=28x), example(route=dontknow). The
user ASR hypothesis the next 61c from oakland to
mckeesport transportation center was represented
as inform(time.rel=next), inform(route=61c),
inform(from.neighborhood=oakland), in-
form(to.desc=?mckeesport transportation
center?). In this domain there were a total
of 9 slots: the bus route, date, time, and three
components each for the origin and destination,
corresponding to streets, neighborhoods, and
points-of-interest like universities. For complete
details see (Williams et al, 2012).
406
TRAIN TEST
1A 1B 1C 2 3 1 2 3 4
Group A A A A B A A B C
Year(s) 2009 2009 2009 2010 2010 2011 2012 2011-2 2010
Dialogs 1013 1117 9502 643 688 715 750 1020 438
Turns/Dialog 14.7 13.3 14.5 14.5 12.6 14.1 14.5 13.0 10.9
Sys acts/turn 4.0 3.8 3.8 4.0 8.4 2.8 3.2 8.2 4.6
Av N-best len 21.7 22.3 21.9 22.4 2.9 21.2 20.5 5.0 3.2
Acts/N-best hyp 2.2 2.2 2.2 2.3 1.0 2.1 2.0 1.0 1.6
Slots/turn 44.0 46.5 45.6 49.0 2.1 41.4 36.9 4.3 3.5
Transcribed? yes yes yes yes yes yes yes yes yes
Labelled? yes no no yes yes yes yes yes yes
1-best WER 42.9% 41.1% 42.1% 58.2% 40.5% 57.9% 62.1% 48.1% 55.6%
1-best SLU Prec. 0.356 - - 0.303 0.560 0.252 0.275 0.470 0.334
1-best SLU Recall 0.522 - - 0.388 0.650 0.362 0.393 0.515 0.376
N-best SLU Recall 0.577 - - 0.485 0.738 0.456 0.492 0.634 0.413
Table 1: Summary of the datasets. One turn includes a system output and a user response. Slots are
named entity types such as bus route, origin neighborhood, date, time, etc. N-best SLU Recall indicates
the fraction of concepts which appear anywhere on the SLU N-best list.
Group B and C systems produced N-best lists
of ASR and SLU output, which were included in
the log files. Group A systems produced only 1-
best lists, so for Group A systems, recognition was
re-run with the Pocketsphinx speech recognizer
(Huggins-Daines et al, 2006) with N-best output
enabled, and the results were included in the log
files.
Some information in the raw system logs was
specific to a group. For example, Group B?s logs
included information about word confusion net-
works, but other groups did not. All of this infor-
mation was included in a ?system specific? sec-
tion of the log files. Group A logs contained about
40 system-specific name/value pairs per turn, and
Group B about 600 system-specific name/value
pairs per turn. Group C logs contained no system
specific data.
3 Labeling and evaluation design
The output of a dialog state tracker is a proba-
bility distribution over a set of given dialog state
hypotheses, plus the REST meta-hypothesis. To
evaluate this output, a label is needed for each di-
alog state hypothesis indicating its correctness.
In this task-oriented domain, we note that the
user enters the call with a specific goal in mind.
Further, when goal changes do occur, they are
usually explicitly marked: since all of the sys-
tems first collect slot values, and then provide bus
timetables, if the user wishes to change their goal,
they need to start over from the beginning. These
?start over? transitions are obvious in the logs.
This structure allows the correctness of each di-
alog state to be equated to the correctness of the
SLU items it contains. As a result, in the DSTC
we labeled the correctness of SLU hypotheses in
each turn, and then assumed these labels remain
valid until either the call ends, or until a ?start
over? event. Thus to produce the labels, the la-
beling task followed was to assign a correctness
value to every SLU hypothesis on the N-best list,
given a transcript of the words actually spoken in
the dialog up to the current turn.
To accomplish this, first all user speech was
transcribed. The TRAIN1 datasets had been tran-
scribed using crowd-sourcing in a prior project
(Parent and Eskenazi, 2010); the remainder were
transcribed by professionals. Then each SLU hy-
pothesis was labled as correct or incorrect. When a
transcription exactly and unambiguously matched
a recognized slot value, such as the bus route
?sixty one c?, labels were assigned automati-
cally. The remainder were assigned using crowd-
sourcing, where three workers were shown the true
words spoken and the recognized concept, and
asked to indicate if the recognized concept was
correct ? even if it did not match the recognized
words exactly. Workers were also shown dialog
407
history, which helps decipher the user?s meaning
when their speech was ambiguous. If the 3 work-
ers were not unanimous in their labels (about 4%
of all turns), the item was labeled manually by the
organizers. The REST meta-hypothesis was not
explicitly labeled; rather, it was deemed to be cor-
rect if none of the prior SLU results were labeled
as correct.
In this challenge, state tracking performance
was measured on each of the 9 slots separately,
and also on a joint dialog state consisting of all the
slots. So at each turn in the dialog, a tracker output
10 scored lists: one for each slot, plus a 10th list
where each dialog state contains values from all
slots. Scores were constrained to be in the range
[0, 1] and to sum to 1.
To evaluate tracker output, at each turn, each hy-
pothesis (including REST) on each of the 10 lists
was labeled as correct or incorrect by looking up
its corresponding SLU label(s). The scores and la-
bels over all of the dialogs were then compiled to
compute 11 metrics. Accuracy measures the per-
cent of turns where the top-ranked hypothesis is
correct. This indicates the correctness of the item
with the maximum score. L2 measures the L2 dis-
tance between the vector of scores, and a vector of
zeros with 1 in the position of the correct hypoth-
esis. This indicates the quality of all scores, when
the scores as viewed as probabilities.
AvgP measures the mean score of the first cor-
rect hypothesis. This indicates the quality of the
score assigned to the correct hypothesis, ignoring
the distribution of scores to incorrect hypotheses.
MRR measures the mean reciprocal rank of the
first correct hypothesis. This indicates the quality
of the ordering the scores produces (without nec-
essarily treating the scores as probabilities).
The remaining measures relate to receiver-
operating characteristic (ROC) curves, which
measure the discrimination of the score for the
highest-ranked state hypothesis. Two versions
of ROC are computed ? V1 and V2. V1 com-
putes correct-accepts (CA), false-accepts (FA),
and false-rejects (FR) as fractions of all utter-
ances, so for example
CA.V 1(s) = #CA(s)N (1)
where #CA(s) indicates the number of correctly
accepted states when only those states with score
? s are accepted, and N is the total number
of states in the sample. The V1 metrics are a
20%
30%
40%
50%
60%
70%
80%
90%
100%
sche
dule
2 ac
cura
cy fo
r all
 slot
s
Trackers Oracle Baseline0 Baseline1
train293% test175% test289% test348%train382% test438%
Figure 2: Schedule2 accuracy averaged over slots
for every tracker on every dataset. Percentages un-
der the datasets indicate the percent of the track-
ers which exceeded the performance of both base-
lines.
useful indication of overall performance because
they combine discrimination and overall accuracy
? i.e., the maximum CA.V 1(s) value is equal to
accuracy computed above.
V2 considers fractions of correctly classified ut-
terances, so for example
CA.V 2(s) = #CA(s)#CA(0) . (2)
The V2 metrics are useful because they measure
the discrimination of the scoring independently of
accuracy ? i.e., the maximum value of CA.V 2(s)
is always 1, regardless of accuracy.
From these ROC statistics, several met-
rics are computed. ROC.V1.EER computes
FA.V 1(s) where FA.V 1(s) = FR.V 1(s).
The metrics ROC.V1.CA05, ROC.V1.CA10,
and ROC.V1.CA20 compute CA.V 1(s) when
FA.V 1(s) = 0.05, 0.10, and 0.20 respec-
tively. ROC.V2.CA05, ROC.V2.CA10, and
ROC.V2.CA20 do the same using the V2 ver-
sions.
Apart from what to measure, there is currently
no standard that specifies when to measure ? i.e.,
which turns to include when computing each met-
ric. So for this challenge, a set of 3 schedules were
used. schedule1 includes every turn. schedule2
include turns where the target slot is either present
on the SLU N-best list, or where the target slot
is included in a system confirmation action ? i.e.,
where there is some observable new information
408
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0% 20% 40% 60% 80% 100%
True 
posit
ive ra
te
 
False positive rate  
test4test3test2test1
Figure 3: Receiver operating characteristc (ROC)
curve for SLU confidence scores of the 1-best hy-
pothesis in the test datasets. The SLU confidence
score in TEST3 is most discriminative; TEST1 and
TEST2 are the least discriminative.
about the target slot. schedule3 includes only the
last turn of a dialog.
In sum, for each tracker, one measurement is re-
ported for each test set (4), schedule (3), and met-
ric (11) for each of the 9 slots, the ?joint? slot, and
a weighted average of the individual slots (11), for
a total of 4 ? 3 ? 11 ? 11 = 1452 measurements per
tracker. In addition, each tracker reported average
latency per turn ? this ranged from 10ms to 1s.
3.1 Baseline trackers
For comparisons, two simple baselines were im-
plemented. The first (Baseline0) is a majority
class baseline that always guesses REST with
score 1. The second (Baseline1) follows simple
rules which are commonly used in spoken dialog
systems. It maintains a single hypothesis for each
slot. Its value is the SLU 1-best with the highest
confidence score observed so far, with score equal
to that SLU item?s confidence score.
4 Results and discussion
Logistically, the training data and labels, bus
timetable database, scoring scripts, and baseline
system were publicly released in late December
2012. The test data (without labels) was released
on 22 March 2013, and teams were given a week to
run their trackers and send results back to the orga-
nizers for evaluation. After the evaluation, the test
labels were published. Each team could enter up
to 5 trackers. For the evaluation, teams were asked
to process the test dialogs online ? i.e., to make a
1
3
5
7
9
11
13
15
17
19
accuracy l2 roc.v1_eer roc.v2_ca05
Average
 rank
 in tes
t datase
ts
Metric - schedule2 - weighted average over all slots
T3.E2T5.E1T5.E2T5.E5T6.E2T6.E3T6.E4T6.E5T9.E1
Figure 4: Average rank of top-performing trackers
for the four metrics identified in Figure 6. Rank-
ing was done using the given metric, schedule2,
and the weighted average of all slots. Tn.Em in-
dicates team n, entry m.
single pass over the data, as if the tracker were be-
ing run in deployment. Participation was open to
researchers at any institution, including the orga-
nizers and advisory board. To encourage partici-
pation, the organizers agreed not to identify par-
ticipants in publications, and there was no require-
ment to disclose how trackers were implemented.
9 teams entered the DSTC, submitting a total of
27 trackers. The raw output and all 1452 measure-
ments for each tracker (and the 2 baselines) are
available from the DSTC homepage (DST, 2013).
4.1 Analysis of trackers and datasets
We begin by looking at one illustrative metric,
schedule2 accuracy averaged over slots, which
measures the accuracy of the top dialog hypothe-
sis for every slot when it either appears on the SLU
N-best list or is confirmed by the system.1 Results
in Figure 2 show two key trends. First, relative
to the baselines, performance on the test data is
markedly lower than the training data. Comparing
TRAIN2 to TEST1/TEST2 and TRAIN3 to TEST3,
the relative gain over the baselines is much lower
on test data. Moreover, only 38% of trackers per-
formed better than a simple majority-class base-
line on TEST4, for which there was no matched
training data. These findings suggests that gen-
eralization is an important open issues for dialog
state trackers.
Second, Figure 2 indicates that the gains made
1Results using the joint dialog state are broadly similar,
and are omitted for space.
409
20%
30%
40%
50%
60%
70%
80%
90%
0% 5% 10% 15% 20% 25% 30% 35% 40%
sche
dule2
 accu
racy 
for a
ll slo
ts
% of turns where top dialog hypothesis was not top SLU result
Trackers Baseline0 Baseline1
(a) TEST1
20%
30%
40%
50%
60%
70%
80%
90%
0% 5% 10% 15% 20% 25% 30% 35% 40%
sche
dule2
 accu
racy 
for a
ll slo
ts
% of turns where top dialog hypothesis was not top SLU result
(b) TEST2
20%
30%
40%
50%
60%
70%
80%
90%
0% 5% 10% 15% 20% 25% 30% 35% 40%
sche
dule2
 accu
racy 
for a
ll slo
ts
% of turns where top dialog hypothesis was not top SLU result
(c) TEST3
20%
30%
40%
50%
60%
70%
80%
90%
0% 5% 10% 15% 20% 25% 30% 35% 40%
sche
dule2
 accu
racy 
for a
ll slo
ts
% of turns where top dialog hypothesis was not top SLU result
(d) TEST4
Figure 5: Percent of highest-scored dialog state hypotheses which did not appear in the top-ranked SLU
position vs. schedule2 accuracy over all slots. Trackers ? including those with the highest accuracy ?
for TEST1 and TEST2 rarely assigned the highest score to an SLU hypothesis other than the top. All
trackers for TEST3 and TEST4 assigned the highest score to an SLU hypothesis other than the top in a
non-trivial percent of turns.
by the trackers over the baselines are larger
for Group A systems (TEST1 and TEST2) than
for Group B (TEST3) and C (TEST4) systems.
Whereas the baselines consider only the top SLU
hypothesis, statistical trackers can make use of
the entire N-best list, increasing recall ? compare
the 1-best and N-best SLU recall rates in Table 1.
However, Group A trackers almost never assigned
the highest score to an item below the top position
in the SLU N-best list. Rather, the larger gains for
Group A systems seem due to the relatively poor
discrimination of Group A?s SLU confidence score
(Figure 3): whereas the trackers use a multitude
of features to assign scores, the baselines rely en-
tirely on the SLU confidence for their scores, so
undiscriminative SLU confidence measures ham-
per baseline performance.
4.2 Analysis of metrics
This challenge makes it possible to study the em-
pirical differences among the evaluation metrics.
Intuitively, if the purpose of a metric is to order
a set of trackers from best to worst, then 2 met-
rics are similar if they yield a similar ordering over
trackers. Specifically, for every metricm, we have
a value x(m, d, s, t) where d is the dataset, and
s is the evaluation schedule, and t is the tracker.
We define r(m, d, s, t) as the rank of tracker t
when ordered using metric m, dataset d and eval-
uation schedule s. Using these ranks, we compute
Kendall?s Tau for every d, s, and pair of metrics
m1 and m2 (Kendall, 1938). We then compute the
average Kendall?s Tau for m1 and m2 by averag-
ing over all d and s.2
Results are in Figure 6. Here we see 4 natu-
ral clusters emerge: a cluster for correctness with
Accuracy, MRR, and the ROC.V1.CA measures; a
cluster for probability quality with L2 and Aver-
age score; and two clusters for score discrimina-
tion ? one with ROC.V1.EER and the other with
the three ROC.V2 metrics. This finding suggest
2A similar analysis over schedules showed that the differ-
ences in ranking for different schedules were smaller than for
metrics.
410
accuracy
mrr
roc_v1.ca05
roc_v1.ca10
roc_v1.ca20
roc.v1_eer
avgp
l2
roc.v2_ca05
roc.v2_ca05
roc.v2_ca05
Figure 6: Average divergence between rank orderings produced by different metrics. The size of a circle
at (x, y) is given by 1?? , where ? is the average Kendall?s Tau computed on the rank orderings produced
by methods x and y. Larger circles indicate dissimilar rankings; smaller circles indicate similar rankings;
missing circles indicate identical rankings. The red boxes indicate groups of metrics that yield similar
rankings.
that measuring one metric from each cluster will
contain nearly the same information as all 9 met-
rics. For example, one might report only Accu-
racy, L2, ROC.V1.EER, and ROC.V2.CA5.
Using these 4 metrics, we rank-ordered each
tracker, using schedule2 and a weighted average
of all slots. We then computed the average rank
across the 4 test sets. Finally we selected the set
of trackers with the top three average ranks for
each metric. Results in Figure 4 emphasize that
different trackers are tuned for different perfor-
mance measures, and the optimal tracking algo-
rithm depends crucially on the target performance
measure.
5 Conclusion
The dialog state tracking challenge has provided
the first common testbed for this task. The data,
evaluation tools, and baselines will continue to be
freely available to the research community (DST,
2013). The details of the trackers themselves will
be published at SIGDIAL 2013.
The results of the challenge show that the
suite of performance metrics cluster into 4 natural
groups. We also find that larger gains over conven-
tional rule-based baselines are present in dialog
systems where the speech recognition confidence
score has poor discrimination. Finally, we observe
substantial limitations on generalization: in mis-
matched conditions, around half of the trackers en-
tered did not exceed the performance of two sim-
ple baselines.
In future work, it should be verified that im-
provements in dialog state tracking lead to im-
provements in end-to-end dialog performance
(e.g., task completion, user satisfaction, etc.). In
addition, it would be interesting to study dialogs
where goal changes are more common.
Acknowledgements
The organizers thank the advisory board for their
valuable input on the design of the challenge:
Daniel Boies, Paul Crook, Maxine Eskenazi, Mil-
ica Gasic, Dilek Hakkani-Tur, Helen Hastie, Kee-
Eung Kim, Ian Lane, Sungjin Lee, Teruhisa Misu,
Olivier Pietquin, Joelle Pineau, Blaise Thomson,
David Traum, and Luke Zettlemoyer. The orga-
nizers also thank Ian Lane for his support for tran-
scription, and Microsoft and Honda Research In-
stitute USA for funding the challenge. Finally,
we thank the participants for making the challenge
successful.
411
References
AW Black, S Burger, B Langner, G Parent, and M Es-
kenazi. 2010. Spoken dialog challenge 2010. In
Proc SLT, Berkeley.
D Bohus and AI Rudnicky. 2006. A ?K hypotheses +
other? belief updating model. In Proc AAAI Work-
shop on Statistical and Empirical Approaches for
Spoken Dialogue Systems, Boston.
2013. Dialog State Tracking Challenge Home-
page. http://research.microsoft.com/
events/dstc/.
H Higashinaka, M Nakano, and K Aikawa. 2003.
Corpus-based discourse understanding in spoken di-
alogue systems. In Proc ACL, Sapporo.
D Huggins-Daines, M Kumar, A Chan, A W Black,
M Ravishankar, and A I Rudnicky. 2006. Pock-
etSphinx: A Free, Real-Time Continuous Speech
Recognition System for Hand-Held Devices. In
Proc ICASSP, Toulouse.
M Kendall. 1938. A new measure of rank correlation.
Biometrika, 30(1-2):81?89.
Y Ma, A Raux, D Ramachandran, and R Gupta. 2012.
Landmark-based location belief tracking in a spoken
dialog system. In Proc SigDial, Seoul.
N Mehta, R Gupta, A Raux, D Ramachandran, and
S Krawczyk. 2010. Probabilistic ontology trees for
belief tracking in dialog systems. In Proc SigDial,
Tokyo.
T Paek and E Horvitz. 2000. Conversation as ac-
tion under uncertainty. In Proc UAI, Stanford, pages
455?464.
G Parent and M Eskenazi. 2010. Toward Bet-
ter Crowdsourced Transcription: Transcription of a
Year of the Let?s Go Bus Information System Data.
In Proc SLT, Berkeley.
B Thomson and SJ Young. 2010. Bayesian update
of dialogue state: A POMDP framework for spoken
dialogue systems. Computer Speech and Language,
24(4):562?588.
JD Williams and SJ Young. 2007. Partially observable
Markov decision processes for spoken dialog sys-
tems. Computer Speech and Language, 21(2):393?
422.
JD Williams, A Raux, D Ramachandran, and
AW Black. 2012. Dialog state tracking challenge
handbook. Technical report, Microsoft Research.
JD Williams. 2010. Incremental partition recombina-
tion for efficient tracking of multiple dialogue states.
In Proc. of ICASSP.
SJ Young, M Gas?ic?, S Keizer, F Mairesse, J Schatz-
mann, B Thomson, and K Yu. 2010. The hid-
den information state model: a practical framework
for POMDP-based spoken dialogue management.
Computer Speech and Language, 24(2):150?174.
Appendix 1: Sample dialog excerpts
Below are transcripts of calls in the DSTC corpus.
The user?s side shows utterances as transcribed by
human annotators.
Group A
SYS: Welcome to the CMU Let?s Go bus informa-
tion system. To get help at any time, just say Help
or press zero. What can I do for you?
USR: 61c evening hours
SYS: For example, you can say, when is the next
28X from DOWNTOWN to THE AIRPORT? or
I?d like to go from MCKEESPORT to HOME-
STEAD tomorrow at 10 a.m.
USR: i?d like to go [noise]
SYS: Where are you leaving from?
USR: from Kennywood tomorrow at 6:30 PM
SYS: Leaving from MCKEE. Is this correct?
USR: yes
SYS: Okay What is your destination?
USR: downtown
SYS: Going to DOWNTOWN. Is this correct?
USR: yes
SYS: Right When do you want to travel?
USR: 6:30 PM
SYS: Leaving at 6 30 p.m.. Is this correct?
USR: [noise] yes
SYS: Alright Hold on. I?ll look that up. There is
a 71D leaving FIFTH AVENUE AT DARRAGH
MCKEE at 6 40 p.m.. It will arrive at FIFTH AV-
ENUE AT MARKET at 6 58 p.m.. To get more
information about buses related to this trip, you
can say, when is the next bus, or, when is the pre-
vious bus. To ask about a different trip, you can
say, start a new query. If you are finished, you can
say goodbye.
USR: start a new query
SYS: Okay, let?s start from the beginning.
USR: ...
Group B
SYS: East Pittsburgh Bus Schedules. Say a bus
route, like 28X, or say I?m not sure.
USR: 61A
SYS: Okay, 61A. To change, say go back. Where
are you leaving from?
USR: Downtown
SYS: Okay, downtown. You can always say go
back. And where are you going to?
USR: East Pittsburgh East Pittsburgh
SYS: Pittsburgh. Is that right?
412
USR: No
SYS: Oh, sorry, where are you going to?
USR: East Pittsburgh
SYS: Okay, East Pittsburgh. Do you want times
for the next few buses? Say yes or no.
USR: Yes
SYS: Ok, the next few buses. To change, say go
back. At 7:45 PM today, there is a 61 A, from
Fifth Avenue And Market Street, Downtown, ar-
riving Dynamo Alley And Beech Avenue, In East
Pittsburgh, at 8:42 PM. Say repeat, next, previous,
or start over.
Group C
SYS: Welcome to the Lets Go! bus information
system for Pittsburgh. Where would you like to
leave from?
USR: [noise]
SYS: Where do you wanna leave from?
USR: [noise] um I?m leaving from Forbes avenue
and Murray avenue
SYS: I?m sorry but I do not know any stops in the
moon neighborhood. The routes I currently cover
are the following: 28X, 54C, 56U, 61A, 61B, 61C,
61D and 64.
USR: um I need the scheduling information for the
61D
SYS: Where would you like to leave from?
USR: Forbes and Murray
SYS: Where do you wanna go?
USR: Greenfield
SYS: When are you going to take that bus?
USR: 61D
SYS: When are you going to take that bus?
USR: 61D
SYS: When would you like to travel? Say next bus
for the the next bus .
USR: next bus
SYS: The next bus from forbes avenue at murray
to greenfield is the 61D, leaving at 8 43 p.m. . It
arrives at murray avenue at loretta at 8 52 p.m. .
You may now say. next bus, previous bus, restart,
or goodbye.
USR: [noise] good-bye
SYS: Thank you goodbye
Appendix 2: Example log fragment
Following is a log from a call in TRAIN3. Someelements have been omitted for space. Logs areformatted in JavaScript Object Notation (JSON).
{
"session-id": "dt-201007221854-4808D",
"turns": [ {
"input": {
"asr-hyps": [
{ "asr-hyp": "sixty one b",
"score": 0.834314,
},
...
],
"slu-hyps": [
{ "slu-hyp": [
{"act": "inform",
"slots": [["route", "61b"]]},
],
"score": 0.834314,
},
...
],
"audio-file": "002.raw",
"start-time": 7.504
},
"output": {
"start-time": 0.078,
"transcript": "East Pittsburgh Bus
Schedules. Say a bus route, like
28X, or say I?m not sure.",
"dialog-acts": [
{ "act": "hello",
"slots": [] },
{ "act": "request",
"slots": [["route",null]] },
{ "act": "example",
"slots": [["route","28x"]] },
{ "act": "example",
"slots": [["route","dontknow"]] }
],
}
"system-specific": {
"wcn": {
"best_path_score": 0.9965,
"best_path": {
"word": ["sixty","one","b"],
"prob": [ 1.0,1.0,0.9965 ],
}
"network": [ ... ]
},
"nbest": [
{
"normSpeechLhood": -152.654,
"lastSpeechFrame": 266,
"numFrames": 354,
"udelta": -3.0280,
"speechLikelihood": -15876.0,
},
...
],
...
},
},
...
]
}
413
Proceedings of the SIGDIAL 2014 Conference, pages 22?31,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
Situated Language Understanding at 25 Miles per Hour
Teruhisa Misu, Antoine Raux
?
, Rakesh Gupta
Honda Research Institute USA
425 National Avenue
Mountain View, CA 94040
tmisu@hra.com
Ian Lane
Carnegie Mellon University
NASA Ames Research Park
Moffett Field, CA 93085
Abstract
In this paper, we address issues in situ-
ated language understanding in a rapidly
changing environment ? a moving car.
Specifically, we propose methods for un-
derstanding user queries about specific tar-
get buildings in their surroundings. Unlike
previous studies on physically situated in-
teractions such as interaction with mobile
robots, the task is very sensitive to tim-
ing because the spatial relation between
the car and the target is changing while
the user is speaking. We collected situated
utterances from drivers using our research
system, Townsurfer, which is embedded
in a real vehicle. Based on this data, we
analyze the timing of user queries, spa-
tial relationships between the car and tar-
gets, head pose of the user, and linguis-
tic cues. Optimized on the data, our al-
gorithms improved the target identification
rate by 24.1% absolute.
1 Introduction
Recent advances in sensing technologies have en-
abled researchers to explore applications that re-
quire a clear awareness of the systems? dynamic
context and physical surroundings. Such appli-
cations include multi-participant conversation sys-
tems (Bohus and Horvitz, 2009) and human-robot
interaction (Tellex et al., 2011; Sugiura et al.,
2011). The general problem of understanding and
interacting with human users in such environments
is referred to as situated interaction.
We address yet another environment, where sit-
uated interactions takes place ? a moving car. In
the previous work, we collected over 60 hours of
in-car human-human interactions, where drivers
interact with an expert co-pilot sitting next to them
in the vehicle (Cohen et al., 2014). One of the
?
Currently with Lenovo.
insights from the analysis on this corpus is that
drivers frequently use referring expressions about
their surroundings. (e.g. What is that big building
on the right?) Based on this insight, we have de-
veloped Townsurfer (Lane et al., 2012; Misu et
al., 2013), a situated in-car intelligent assistant.
Using geo-location information, the system can
answer user queries/questions that contain object
references about points-of-interest (POIs) in their
surroundings. We use driver (user) face orienta-
tion to understand their queries and provide the re-
quested information about the POI they are look-
ing at. We have previously demonstrated and eval-
uated the system in a simulated environment (Lane
et al., 2012). In this paper, we evaluate its utility
in real driving situations.
Compared to conventional situated dialog tasks,
query understanding in our task is expected to be
more time sensitive, due to the rapidly changing
environment while driving. Typically, a car will
move 10 meters in one second while driving at 25
mi/h. So timing can be a crucial factor. In addi-
tion, it is not well understood what kind of linguis-
tic cues are naturally provided by drivers, and their
contributions to situated language understanding
in such an environment. To the best of our knowl-
edge, this is the first study that tackles the issue of
situated language understanding in rapidly moving
vehicles.
In this paper, we first present an overview of the
Townsurfer in-car spoken dialog system (Section
2). Based on our data collection using the sys-
tem, we analyze user behavior while using the sys-
tem focusing on language understanding (Section
3). Specifically, we answer the following research
questions about the task and the system through
data collection and analysis:
1. Is timing an important factor of situated lan-
guage understanding?
2. Does head pose play an important role in lan-
guage understanding? Or is spatial distance
information enough?
22
Speech recognition
Natural language understanding
Gaze (Head-pose) estimation
3) POI Posterior calculation            by Belief tracking
1) (Candidate)     POI look-up
Microphone
Depth sensor
Sensors Sensor signal understanding POI identification (situated understanding)
Speech
Gaze
2) POI score (prior) calculation
Understanding result (POI with maximum posterior)
Geo-location estimation Semantic         geo-spatial database
GPS
IMU
Geo-location
Figure 1: System overview of Townsurfer
Table 1: Example dialog with Townsurfer
U1: What is that place. (POI in gaze)
S1: This is Specialty Cafe, a mid-scale coffee
shop that serves sandwiches.
U2: What is its (POI in dialog history) rating.
S2: The rating of Specialty Cafe is above av-
erage.
U3: How about that one on the left.
(POI located on the left)
S3: This is Roger?s Deli, a low-priced restau-
rant that serves American food.
3. What is the role of linguistic cues in this task?
What kinds of linguistic cues do drivers nat-
urally provide?
Based on the hypothesis obtained from the analy-
sis for these questions, we propose methods to im-
prove situated language understanding (Section 4),
and analyze their contributions based on the col-
lected data (Sections 5 and 6). We then clarify our
research contributions through discussion (Section
7) and comparison with related studies (Section 8).
2 Architecture and Hardware of
Townsurfer
The system uses three main input modalities,
speech, geo-location, and head pose. Speech is
the main input modality of the system. It is used to
trigger interactions with the system. User speech
is recognized, then requested concepts/values are
extracted. Geo-location and head pose informa-
tion are used to understand the target POI of the
user query. An overview of the system with a pro-
cess flow is illustrated in Figure 1 and an exam-
ple dialog with the system is shown in Table 1. A
video of an example dialog is also attached.
In this paper, we address issues in identify-
ing user intended POI, which is a form of ref-
erence resolution using multi-modal information
sources
1
. The POI identification process consists
of the following three steps (cf. Figure 1). This
is similar to but different from our previous work
on landmark-based destination setting (Ma et al.,
2012).
1) The system lists candidate POIs based on geo-
location at the timing of a driver query. Rela-
tive positions of POIs to the car are also cal-
culated based on geo-location and the head-
ing of the car.
2). Based on spatial linguistic cues in the user
utterance (e.g. to my right, on the left), a
2D scoring function is selected to identify ar-
eas where the target POI is likely to be. This
function takes into account the position of the
POI relative to the car, as well as driver head
pose. Scores for all candidate POIs are cal-
culated.
3) Posterior probabilities of each POI are cal-
culated using the score of step 2 as prior,
and non-spatial linguistic information (e.g.
POI categories, building properties) as obser-
vations. This posterior calculation is com-
puted using our Bayesian belief tracker called
DPOT (Raux and Ma, 2011).
The details are explained in Section 4.
System hardware consists of a 3D depth sen-
sor (Primesense Carmine 1.09), a USB GPS (BU-
353S4), an IMU sensor (3DM-GX3-25) and a
close talk microphone (plantronics Voyage Leg-
1
We do not deal with issues in language understanding
related to dialog history and query type. (e.g. General infor-
mation request such as U1 vs request about specific property
of POI such as U2 in Table 1)
23
end UC). These consumer grade sensors are in-
stalled in our Honda Pilot experiment car. We
use Point Cloud Library (PCL) for the face direc-
tion estimation. Geo-location is estimated based
on Extended Kalman filter-based algorithm using
GPS and gyro information as input at 1.5 Hz. The
system is implemented based on the Robot Oper-
ating System ROS (Quigley et al., 2009). Each
component is implemented as a node of ROS, and
communications between the nodes are performed
using the standard message passing mechanisms
in ROS.
3 Data Collection and Analysis
3.1 Collection Setting
We collected data using a test route. The route
passes through downtown Mountain View
2
and
residential area around Honda Research Institute.
We manually constructed our database containing
250 POIs (businesses such as restaurants, compa-
nies) in this area. Each database entry (POI) has
name, geo-location, category and property infor-
mation explained in Section 3.4. POI geo-location
is represented as a latitude-longitude pair (e.g.
37.4010,-122.0539). Size and shape of buildings
are not taken into account. It takes about 30 min-
utes to drive the route. The major difference be-
tween residential area and downtown is the POI
density. While each POI in downtown has on aver-
age 7.2 other POIs within 50 meters, in residential
area POIs have only 1.9 neighbors. Speed limits
also differ between the two (35 mi/h vs 25 mi/h).
We collected data from 14 subjects. They were
asked to drive the test route and make queries
about surrounding businesses. We showed a demo
video
3
of the system to the users before starting the
data collection. We also told them that the objec-
tive is a data collection for a situated spoken dia-
log system, rather than the evaluation of the whole
system. We asked subjects to include the full de-
scription of the target POI within a single utterance
to avoid queries whose understanding requires di-
alog history information
4
. Although the system
answered based on the baseline strategy explained
in Section 4.1, we asked subjects to ignore the sys-
tem responses.
As a result, we collected 399 queries with a
valid target POI. Queries about businesses that do
2
We assumed that a POI is in downtown when it is located
within the rectangle by geo-location coordinates (37.3902, -
122.0827) and (37.3954, -122.0760).
3
not the attached one.
4
Understanding including dialog history information is
our future work.
POI
x
y ?
face direction
target 
direction
Heading 
direction
Figure 2: Parameters used to calculate POI score
(prior)
?  :   rightX:   left+:   no cue
Distance (m)y
RightLeft Distance (m)
x
Figure 3: Target POI positions
not exist on our database (typically a vacant store)
were excluded. The data contains 171 queries in
downtown and 228 in residential area. The queries
were transcribed and the user-intended POIs were
manually annotated by confirming the intended
target POI with the subjects after the data collec-
tion based on a video taken during the drive.
3.2 Analysis of Spatial Relation of POI and
Head Pose
We first analyze the spatial relation between posi-
tion cues (right/left) and the position of the user-
intended target POIs Out of the collected 399
queries, 237 (59.4%) of them contain either right
or left position cue (e.g. What is that on the left?).
The relation between the position cues (cf. Figure
2) and POI positions at start-of-speech timing
5
is
plotted in Figure 3. The X-axis is a lateral distance
(a distance in the direction orthogonal to the head-
ing; a positive value means the right direction) and
the Y-axis is an axial distance (a distance in the
heading direction; a negative value means the POI
is in back of the car. ). The most obvious finding
from the scatter plot is that right and left are pow-
5
Specifically, the latest GPS and face direction informa-
tion at that timing is used.
24
Table 2: Comparison of average and standard deviation of distance (in meter) of POI form the car
ASR result timing Start-of-speech timing
Position cue Site Ave dist. Std dist. Ave dist. Std dist.
Right/left Downtown 17.5 31.0 31.9 28.3
Residential 22.0 36.3 45.2 36.5
No right/left Downtown 17.4 27.8 31.1 26.5
cue Residential 38.3 45.9 52.3 43.4
Distance (m)y
?angular difference (degree)
Figure 4: Relation between POI positions and
head pose
erful cues for the system to identify target POIs.
We can also see that the POI position distribution
has a large standard deviation. This is partly be-
cause the route has multiple sites from downtown
and residential area. Interestingly, while the aver-
age distance to the target POI in downtown is 37.0
meters, that of residential area is 57.4 meters.
We also analyze the relation between face di-
rection and POI positions. Figure 4 plots the re-
lation between the axial distance and the angular
difference ? (between the user face direction and
the target POI direction) (cf. Figure 2). The scat-
ter plot suggests that the angular differences for
distant target POIs is often small. For close target
POIs the angular differences are larger and have a
large variance
6
.
3.3 Analysis of Timing
Referring expressions such as ?the building on the
right? must be resolved with respect to the context
in which the user intended. However, in a moving
car, such a context (i.e. the position of the car and
the situation in the surroundings) can be very dif-
ferent between the time when the user starts speak-
ing the sentence and the time they finish speaking
it. Therefore, situated understanding must be very
time sensitive.
To confirm and investigate this issue, we ana-
lyze the difference in the POI positions between
the time the ASR result is output vs the time the
user actually started speaking. The hypothesis is
6
We will discuss the reason for this in Section 6.2.
Table 3: User-provided linguistic cues
Category of linguistic cue Percentage
used (%)
Relative position to the car (right/left) 59.4
Business category (e.g. restaurant, cafe) 31.8
Color of the POI (e.g. green, yellow) 12.8
Cuisine (e.g. Chinese, Japanese, Mexican) 8.3
Equipments (e.g. awning, outside seating) 7.2
Relative position to the road (e.g. corner) 6.5
that the latter yields a more accurate context in
which to interpret the user sentence. In contrast,
our baseline system uses the more straightforward
approach of resolving expressions using the con-
text at the time of resolution, i.e. whenever the
ASR/NLU has finished processing an utterance
(hereafter ?ASR results timing?).
Specifically, we compare the average axial dis-
tance to the target POIs and its standard deviation
between these two timings. Table 2 lists these fig-
ures broken down by position cue types and sites.
The average axial distance from the car to the tar-
get POIs is often small at the ASR result timing,
but the standard deviation is generally small at the
start-of-speech timing. This indicates that the tar-
get POI positions at the start-of-speech timing is
more consistent across users and sentence lengths
than that at the ASR result timing. This result indi-
cates the presence of a better POI likelihood func-
tion using the context (i.e. car position and orien-
tation) at the start-of-speech timing than using the
ASR result timing.
3.4 Analysis of Linguistic Cues
We then analyze the linguistic cues provided by
the users. Here, we focus on objective and sta-
ble cues. We exclude subjective cues (e.g. big,
beautiful, colorful) and cues that might change in
a short period of time (e.g. with a woman dressed
in green in front). We have categorized the linguis-
tic cues used to describe the target POIs. Table 3
lists the cue types and the percentage of user utter-
ances containing each cue type.
The cues that the users most often provided con-
cern POI position related to the car (right and left).
Nearly 60% of queries included this type of cue
and every subject provided it at least once. The
second most frequent cue is category of business,
especially in downtown. Users also provided col-
25
ors of POIs. Other cues include cuisine, equip-
ments, relative position to the road (e.g. on the
corner).
Another interesting finding from the analysis is
that the users provided more linguistic cues with
increasing candidate POIs in their field of view.
Actually, the users provided 1.51 categories in av-
erage per query in downtown, while they provided
1.03 categories in residential area. (cf. POI den-
sity in Section 3.2: 7.2 vs 1.9) This indicates that
users provide cues considering environment com-
plexity.
4 Methods for Situated Language
Understanding
4.1 Baseline Strategy
We use our previous version (Misu et al., 2013)
as the baseline system for situated language un-
derstanding. The baseline strategy consists of the
following three paragraphs, which correspond to
the process 1)-3) in Section 2 and Figure 1.
The system makes a POI look-up based on the
geo-location information at the time ASR result
is obtained. The search range of candidate POIs
is within the range (relative geo-location of POIs
against the car location) of -50 to 200 meters in
the travelling direction and 100 meters to the left
and 100 meters to the right in the lateral direction.
The ASR result timing is also used to measure the
distances to the candidate POIs.
POI priors are calculated based on the distance
from the car (= axial distance) based on ?the closer
to the car the likely? principle. We use a likelihood
function inversely proportional to the distance. We
use position cues simply to remove POIs from a
list of candidates. For example ?right? position
cue is used to remove candidate POIs that are lo-
cated on< 0 position in the lateral distance. When
no right/left cue is provided, POIs outside of 45
degrees from the face direction are removed from
the list of candidates.
No linguistic cues except right/left are used to
calculate POI posterior probabilities. So, the sys-
tem selects the POI with the highest prior (POI
score) as the language understanding result.
4.2 Strategies Toward Better Situated
Language Understanding
To achieve better situated language understanding
(POI identification) based on the findings of the
analysis in Section 3, we modify steps 1)-3) as fol-
lows:
1. Using start-of-speech timing for the POI
prior calculation
Distance (m)y
?  :   rightX:   left
RightLeft Distance (m)x
Figure 5: Example GMM fitting
2. Gaussian mixture model (GMM)-based POI
probability (prior) calculation
3. Linguistic cues for the posterior calculation.
We use the start-of-speech timing instead of the
time ASR result is output. Because the standard
deviations of the POI distances are small (cf. Sec-
tion 3.2), we expect that a better POI probability
score estimation with the POI positions at this tim-
ing in the subsequent processes than the positions
at the ASR result timing. The POI look-up range
is the same as the baseline.
We apply Gaussian mixture model (GMM) with
diagonal covariance matrices over the input pa-
rameter space. The POI probability (prior) is cal-
culated based on these Gaussians. We use two in-
put parameters of the lateral and axial distances for
queries with right/left cue, and three parameters of
the lateral and axial distances and the difference
in degree between the target and head pose direc-
tions for queries without right/left cue. (The effect
of the parameters is discussed later in Section 6.2.)
We empirically set the number of Gaussian com-
ponents to 2. An example GMM fitting to the POI
positions for queries with right and left cues is il-
lustrated in Figures 5. The center of ellipse is the
mean of the Gaussian.
We use the five linguistic cue categories of Sec-
tion 3.4 for the posterior calculation by the belief
tracker. In the following experiments, we use ei-
ther 1 or 0 as a likelihood of natural language un-
derstanding (NLU) observation. The likelihood
for the category value is 1 if a user query (NLU
result) contains the target value, otherwise 0. This
corresponds to a strategy of simply removing can-
didate POIs that do not have the category values
specified by the user. Here, we assume a clean POI
database with all their properties annotated manu-
ally.
26
Table 4: Comparison of POI identification rate
Method Success
rate (%)
right/left linguistic cues,
the-closer-the-likely likelihood, 43.1
ASR result timing) (Baseline)
1) Start-of-speech timing 42.9
2) GMM-based likelihood 47.9
3) Linguistic cues 54.6
1) + 2) 50.6
1) + 3) 54.4
2) + 3) 62.2
1) + 2) + 3) 67.2
5 Experiments
We use manual transcriptions and natural language
understanding results of the user queries to focus
our evaluations on the issues listed in Section 1.
We evaluate the situated language understanding
(POI identification) performance based on cross
validation. We use the data from 13 users to train
GMM parameters and to define a set of possible
linguistic values, and the data from the remaining
user for evaluation. We train the model parameters
of the GMM using the EM algorithm. Knowledge
about the sites (downtown or residential area) is
not used in the training
7
.
We do not set a threshold for the presentation.
We judge the system successfully understands a
user query when the posterior of the target (user-
intended) POI is the highest. The chance rate,
given by the average of the inverse number of can-
didate POIs in the POI look-up is 10.0%.
6 Analysis of the Results
We first analyze the effect of our three methods
described in Section 4.2. The results are listed in
Table 4.
Simply using the POI positions at the start-of-
speech timing instead of those of the ASR result
timing did not lead to an improvement. This re-
sult is reasonable because the distances to target
POIs are often smaller at the ASR result timing
as we showed in Table 2. However, we achieved
a better improvement (7.5% over the baseline) by
combining it with the GMM-based likelihood cal-
culation. The results supports our Section 3.3 hy-
pothesis that the POI position is less dependent
on users/scenes at the start-of-speech timing. The
linguistic cues were the most powerful informa-
7
The performance was better when the knowledge was not
used.
Confusion
Linguistic cue
Localization error
User error
Figure 6: Breakdown of error causes
tion for this task. The improvement over the base-
line was 11.5%. By using these three methods to-
gether, we obtained more than additive improve-
ment of 24.1% in the POI identification rate over
the baseline
8
. The success rates per site were
60.8% in downtown and 71.9% in residential area.
6.1 Error Analysis
To analyze the causes of the remaining errors, we
have categorized the errors into the following four
categories:
1. Ambiguous references: There were multi-
ple POIs that matched the user query. (e.g.
another yellow building sat next to the target)
2. Linguistic cue: The driver used undefined
linguistic cues such subjective expressions or
dynamic references objects (e.g. optometrist,
across the street, colorful)
3. Localization error: Errors in estimating
geo-location or heading of the car.
4. User error: There were errors in the user
descriptions (e.g. user misunderstood the
neighbor POI?s outside seating as the tar-
get?s)
The distribution of error causes is illustrated in
Figure 6. More than half of the errors are due
to reference ambiguity. These errors are expected
to be resolved through clarification dialogs. (e.g.
asking user ?Did you mean the one in front or
back??) Linguistic errors might be partly resolved
by using a better database with detailed category
information. For dynamic references and subjec-
tive cues, use of image processing techniques will
help. Localization errors can be solved by using
high-quality GPS and IMU sensors. User errors
were rare and only made in downtown.
6.2 Breakdown of Effect of the Spatial
Distance and Head Pose
We then evaluate the features used for the POI
prior calculation to investigate the effect of the in-
put parameters of the lateral and axial distances
8
For reference, the performances of ?1) + 2) + 3)? were
62.9%, 67.2%, 66.1%, 67.2%, and 66.2% when the number
of Gaussian components were 1, 2, 3, 4, and 5.
27
Table 5: Relation between the parameters used for
the POI identification and success rates (%)
query type
parameters used right/left no cue
lateral (x) distance 58.6 51.2
axial (y) distance 59.5 53.7
face direction 43.3 44.4
lateral + axial (x+ y) 73.8 54.3
lateral (x) + face direction 57.8 48.1
axial (y) + face direction 59.1 54.9
lateral + axial + face 68.4 57.4
and the difference in degree between the target
and user face direction angles. Table 5 lists the
relationship between the parameters used for the
GMM-based likelihood calculation and the POI
identification performances
9
.
The results indicate that the axial distance is
the most important parameter. We got a slight
improvement by using the face direction informa-
tion for the queries without right/left cue, but the
improvement was not significant. On the other
hand, use of face direction information for the
right/left queries clearly degraded the POI iden-
tification performance. We think this is because
the users finished looking at the POI and returned
the face to the front when they started speaking,
thus they explicitly provided right/left information
to the system. However, we believe that using a
long-term trajectory of the user face direction will
contribute to an improve in the POI identification
performance.
6.3 Breakdown of the Effect of Linguistic
Cues
We then evaluate the effect of the linguistic cues
per category. Table 6 lists the relationship between
the categories used for the posterior calculation
and the success rates. There is a strong correlation
between the frequency of the cues used (cf. Table
3) and their contributions to the improvement in
success rate. For example, business category in-
formation contributed the most, boosting the per-
formance by 8.5%.
Another point we note is that the contribution of
business category and cuisine categories is large.
Because other categories (e.g. color) are not read-
ily available in a public POI database (e.g. Google
Places API, Yelp API), we can obtain reasonable
performance without using a special database or
9
Note that, we first determine the function to calculate
POI scores (priors) based on the position cues, then calculate
scores with the selected function.
Table 6: Effect of linguistic cues
linguistic cue Success
category used rate (%)
No linguistic cues (*) 50.6
(*) + Business category (e.g. cafe) 59.1
(*) + Color of the POI (e.g. green) 57.6
(*) + Cuisine (e.g. Chinese) 54.1
(*) + Equipments (e.g. awning) 53.9
(*) + Relative position (e.g. corner) 51.4
image processing.
We also found that linguistic cues were espe-
cially effective in downtown. Actually, while the
improvement
10
was 20.0% in downtown that for
residential area was 14.4%. This mainly would be
because the users provided more linguistic cues in
downtown considering the difficulty of the task.
6.4 Using Speech Recognition Results
We evaluate the degradation by using automatic
speech recognition (ASR) results. We use Google
ASR
11
and Julius (Kawahara et al., 2004) speech
recognition system with a language model trained
from 38K example sentences generated from a
grammar. An acoustic model trained from the
WSJ speech corpus is used. Note that they are
not necessarily the best system for this domain.
Google ASR uses a general language model for
dictation and Julius uses a mismatched acoustic
model in terms of the noise condition.
The query success rate was 56.3% for Julius and
60.3% for Google ASR. We got ASR accuracies
of 77.9% and 80.4% respectively. We believe the
performance will improve when N-best hypothe-
ses with confidence scores are used in the posterior
calculating using the belief tracker.
7 Discussion
The main limitation of this work comes from the
small amount of data that we were able to collect.
It is not clear how the results obtained here would
generalize to other sites, POI density, velocities
and sensor performances. Also, results might de-
pend on experimental conditions, such as weather,
hour, season. Hyper-parameters such as the opti-
mal number of Gaussian components might have
to be adapted to different situations. We there-
fore acknowledge that the scenes we experimented
are only a limited cases of daily driving activities.
10
1) + 2) vs 1) + 2) + 3).
11
Although it is not realistic to use cloud-based speech
recognition system considering the current latency, we use
this as a reference system.
28
However, the methods we propose are general and
our findings should be verifiable without loss of
generality by collecting more data and using more
input parameters (e.g. velocity) for the POI prior
calculation.
In addition, much future work remains to realize
a natural interaction with the system, such as tak-
ing into account dialog history and selecting opti-
mal system responses. On the other hand, we be-
lieve this is one of the best platform to investigate
situated interactions. The major topics that we are
going to tackle are:
1. Dialog strategy: Dialog strategy and system
prompt generation for situated environments
are important research topics, especially to
clarify the target when there is ambiguity as
mentioned in Section 6.1. The topic will in-
clude an adaptation of system utterances (en-
trainment) to the user (Hu et al., 2014).
2. Eye tracker: Although we believe head pose
is good enough to estimate user intentions be-
cause we are trained to move the head in driv-
ing schools to look around to confirm safety,
we would like to confirm the difference in
this task between face direction and eye-gaze.
3. POI identification using face direction trajec-
tory: Our analysis showed that the use of face
direction sometimes degrades the POI identi-
fication performance. However, we believe
that using a trajectory of face direction will
change the result.
4. Database: We assumed a clean and perfect
database but we are going to evaluate the per-
formance when noisy database is used. (e.g.
A database based on image recognition re-
sults or user dialog log.)
5. Feedback: Koller et al. (2012) demonstrated
referential resolution is enhanced by giving
gaze information feedback to the user. We
would like to analyze the effect of feedback
with an automotive augmented reality envi-
ronment using our 3D head-up display (Ng-
Thow-Hing et al., 2013).
8 Related Work
The related studies include a landmark-based nav-
igation that handles landmarks as information for
a dialog. Similar system concepts have been
provided for pedestrian navigation situations (Ja-
narthanam et al., 2013; Hu et al., 2014), they do
not handle a rapidly changing environment.
Several works have used timing to enhance
natural interaction with systems. Rose and
Horvitz (2003) and Raux and Eskenazi (2009)
used timing information to detect user barge-ins.
Studies on incremental speech understanding and
generation (Skantze and Hjalmarsson, 2010; Deth-
lefs et al., 2012) have proved that real-time feed-
back actions have potential benefits for users.
Komatani et al. (2012) used user speech timing
against user?s previous and system?s utterances
to understand the intentions of user utterances.
While the above studies have handled timing fo-
cusing on (para-)linguistic aspect, our work han-
dles timing issues in relation to the user?s physical
surroundings.
Recent advancements in gaze and face direction
estimation have led to better user behavior under-
standing. There are a number of studies that have
analyzed relationship between gaze and user in-
tention, such as user focus (Yonetani et al., 2010),
preference (Kayama et al., 2010), and reference
expression understanding (Koller et al., 2012), be-
tween gaze and turn-taking (Jokinen et al., 2010;
Kawahara, 2012). Nakano et al. (2013) used face
direction for addressee identification. The previ-
ous studies most related to ours are reference res-
olution methods by Chai and Prasov (2010), Iida
et al. (2011) and Kennington et al. (2013). They
confirmed that the system?s reference resolution
performance is enhanced by taking the user?s eye
fixation into account. However, their results are
not directly applied to an interaction in a rapidly
changing environment while driving, where eye
fixations are unusual activities.
Marge and Rudnicky (2010) analyzed the effect
of space and distance for spatial language under-
standing for a human-robot communication. Our
task differs with this because we handle a rapidly
changing environment. We believe we can im-
prove our understanding performance based on
their findings.
9 Conclusion
We addressed situated language understanding in
a moving car. We focused on issues in understand-
ing user language of timing, spatial distance, and
linguistic cues. Based on the analysis of the col-
lected user utterances, we proposed methods of us-
ing start-of-speech timing for the POI prior calcu-
lation, GMM-based POI probability (prior) calcu-
lation, and linguistic cues for the posterior calcula-
tion to improve the accuracy of situated language
understanding. The effectiveness of the proposed
methods was confirmed by achieving a significant
improvement in a POI identification task.
29
10 Acknowledgments
The authors would like to thank Yi Ma at Ohio
State University for his contributions to the devel-
opment of HRItk.
References
D. Bohus and E. Horvitz. 2009. Models for Multi-
party Engagement in Open-World Dialog. In Proc.
SIGDIAL, pages 225?234.
J. Chai and Z. Prasov. 2010. Fusing eye gaze with
speech recognition hypotheses to resolve exophoric
reference in situated dialogue. In Proc. EMNLP.
D. Cohen, A. Chandrashekaran, I. Lane, and A. Raux.
2014. The hri-cmu corpus of situated in-car interac-
tions. In Proc. IWSDS, pages 201?212.
N. Dethlefs, H. Hastie, V. Rieser, and O. Lemon. 2012.
Optimising incremental dialogue decisions using in-
formation density for interactive systems. In Proc.
EMNLP, pages 82?93.
Z. Hu, G. Halberg, C. Jimenez, and M. Walker. 2014.
Entrainment in pedestrian direction giving: How
many kinds of entrainment? In Proc. IWSDS, pages
90?101.
R. Iida, M. Yasuhara, and T. Tokunaga. 2011. Multi-
modal reference resolution in situated dialogue by
integrating linguistic and extra-linguistic clues. In
Proc. IJCNLP, pages 84?92.
S. Janarthanam, O. Lemon, X. Liu, P. Bartie, W. Mack-
aness, and T. Dalmas. 2013. A multithreaded con-
versational interface for pedestrian navigation and
question answering. In Proc. SIGDIAL, pages 151?
153.
K. Jokinen, M. Nishida, and S. Yamamoto. 2010. On
eye-gaze and turn-taking. In Proc. EGIHMI.
T. Kawahara, A. Lee, K. Takeda, K. Itou, and
K. Shikano. 2004. Recent Progress of Open-Source
LVCSR Engine Julius and Japanese Model Reposi-
tory. In Proc. ICSLP, volume IV.
T. Kawahara. 2012. Multi-modal sensing and analysis
of poster conversations toward smart posterboard. In
Proc. SIGDIAL.
K. Kayama, A. Kobayashi, E. Mizukami, T. Misu,
H. Kashioka, H. Kawai, and S. Nakamura. 2010.
Spoken Dialog System on Plasma Display Panel Es-
timating User?s Interest by Image Processing. In
Proc. 1st International Workshop on Human-Centric
Interfaces for Ambient Intelligence (HCIAmi).
C. Kennington, S. Kousidis, and D. Schlangen. 2013.
Interpreting situated dialogue utterances: an update
model that uses speech, gaze, and gesture informa-
tion. In Proc. SIGDIAL.
A. Koller, K. Garoufi, M. Staudte, and M. Crocker.
2012. Enhancing referential success by tracking
hearer gaze. In Proc. SIGDIAL, pages 30?39.
K. Komatani, A. Hirano, and M. Nakano. 2012. De-
tecting system-directed utterances using dialogue-
level features. In Proc. Interspeech.
I. Lane, Y. Ma, and A. Raux. 2012. AIDAS - Immer-
sive Interaction within Vehicles. In Proc. SLT.
Y. Ma, A. Raux, D. Ramachandran, and R. Gupta.
2012. Landmark-based location belief tracking in
a spoken dialog system. In Proc. SIGDIAL, pages
169?178.
M. Marge and A. Rudnicky. 2010. Comparing Spo-
ken Language Route Instructions for Robots across
Environment Representations. In Proc. SIGDIAL,
pages 157?164.
T. Misu, A. Raux, I. Lane, J. Devassy, and R. Gupta.
2013. Situated multi-modal dialog system in vehi-
cles. In Proc. Gaze in Multimodal Interaction, pages
25?28.
Y. Nakano, N. Baba, H. Huang, and Y. Hayashi.
2013. Implementation and evaluation of a multi-
modal addressee identification mechanism for mul-
tiparty conversation systems. In Proc. ICMI, pages
35?42.
V. Ng-Thow-Hing, K. Bark, L. Beckwith, C. Tran,
R. Bhandari, and S. Sridhar. 2013. User-centered
perspectives for automotive augmented reality. In
Proc. ISMAR.
M. Quigley, K. Conley, B. Gerkey, J. Faust, T. Foote,
J. Leibs, R. Wheeler, and A. Ng. 2009. ROS:
an open-source Robot Operating System. In Proc.
ICRA Workshop on Open Source Software.
A. Raux and M. Eskenazi. 2009. A Finite-state Turn-
taking Model for Spoken Dialog Systems. In Proc.
HLT/NAACL, pages 629?637.
A. Raux and Y. Ma. 2011. Efficient probabilistic track-
ing of user goal and dialog history for spoken dialog
systems. In Proc. Interspeech, pages 801?804.
R. Rose and H. Kim. 2003. A hybrid barge-in proce-
dure for more reliable turn-taking in human-machine
dialog systems. In Proc. Automatic Speech Recog-
nition and Understanding Workshop (ASRU), pages
198?203.
G. Skantze and A. Hjalmarsson. 2010. Towards incre-
mental speech generation in dialogue systems. In
Proc. SIGDIAL, pages 1?8.
K. Sugiura, N. Iwahashi, H. Kawai, and S. Nakamura.
2011. Situated spoken dialogue with robots using
active learning. Advance Robotics, 25(17):2207?
2232.
30
Table 7: Example user utterances
- What is that blue restaurant on the right?
- How about this building to my right with outside seating?
- What is that Chinese restaurant on the left?
- Orange building to my right.
- What kind of the restaurant is that on the corner?
- The building on my right at the corner of the street.
- What about the building on my right with woman with a jacket in front
- Do you know how good is this restaurant to the left?
- Townsurfer, there is an interesting bakery what is that?
- Is this restaurant on the right any good?
S. Tellex, T. Kollar, S. Dickerson, M. Walter, A. Baner-
jee, S. Teller, and N. Roy. 2011. Understanding nat-
ural language commands for robotic navigation and
mobile manipulation. In Proc. AAAI.
R. Yonetani, H. Kawashima, T. Hirayama, and T. Mat-
suyama. 2010. Gaze probing: Event-based estima-
tion of objects being focused on. In Proc. ICPR,
pages 101?104.
11 Appendix
Test route:
https://www.google.com/maps/
preview/dir/Honda+Research+
Institute,+425+National+Ave+
%23100,+Mountain+View,+CA+
94043/37.4009909,-122.0518957/
37.4052337,-122.0565795/37.
3973374,-122.0595982/37.4004787,
-122.0730021/Wells+Fargo/37.
4001639,-122.0729708/37.3959193,
-122.0539449/37.4009821,-122.
0540093/@37.3999836,-122.
0792529,14z/data=!4m21!4m20!
1m5!1m1!1s0x808fb713c225003d:
0xcf989a0bb230e5c0!2m2!
1d-122.054006!2d37.401016!
1m0!1m0!1m0!1m0!1m5!1m1!1s0x0:
0x86ca9ba8a2f15150!2m2!1d-122.
082546!2d37.388722!1m0!1m0!1m0!
3e0
31
