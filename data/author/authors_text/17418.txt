Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 30?35,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Unsupervised frame based semantic role induction: application to French
and English
Alejandra Lorenzo
Lorraine University/LORIA Nancy
alejandra.lorenzo@loria.fr
Christophe Cerisara
CNRS/LORIA Nancy
christophe.cerisara@loria.fr
Abstract
This paper introduces a novel unsupervised
approach to semantic role induction that uses
a generative Bayesian model. To the best of
our knowledge, it is the first model that jointly
clusters syntactic verbs arguments into seman-
tic roles, and also creates verbs classes ac-
cording to the syntactic frames accepted by
the verbs. The model is evaluated on French
and English, outperforming, in both cases, a
strong baseline. On English, it achieves re-
sults comparable to state-of-the-art unsuper-
vised approaches to semantic role induction.
1 Introduction and background
Semantic Role Labeling (SRL) is a major task in
Natural Language Processing which provides a shal-
low semantic parsing of a text. Its primary goal is
to identify and label the semantic relations that hold
between predicates (typically verbs), and their asso-
ciated arguments (Ma`rquez et al, 2008).
The extensive research carried out in this area re-
sulted in a variety of annotated resources, which,
in time, opened up new possibilities for supervised
SRL systems. Although such systems show very
good performance, they require large amounts of
annotated data in order to be successful. This an-
notated data is not always available, very expen-
sive to create and often domain specific (Pradhan
et al, 2008). There is in particular no such data
available for French. To bypass this shortcoming,
?annotation-by-projection? approaches have been
proposed (Pado and Lapata, 2006) which in essence,
(i) project the semantic annotations available in one
language (usually English), to text in another lan-
guage (in this case French); and (ii) use the resulting
annotations to train a semantic role labeller. Thus
Pado and Pitel (2007) show that the projection-based
annotation framework permits bootstrapping a se-
mantic role labeller for FrameNet which reaches an
F-measure of 63%; and van der Plas et al (2011)
show that training a joint syntactic-semantic parser
based on the projection approach permits reaching
an F-measure for the labeled attachment score on
PropBank annotation of 65%.
Although they minimize the manual effort in-
volved, these approaches still require both an an-
notated source corpus and an aligned target corpus.
Moreover, they assume a specific role labeling (e.g.,
PropBank, FrameNet or VerbNet roles) and are not
generally portable from one framework to another.
These drawbacks with supervised approaches mo-
tivated the need for unsupervised methods capable
of exploiting large amounts of unannotated data. In
this context several approaches have been proposed.
Swier and Stevenson (2004) were the first to intro-
duce unsupervised SRL in an approach that used
the VerbNet lexicon to guide unsupervised learning.
Grenager and Manning (2006) proposed a directed
graphical model for role induction that exploits lin-
guistic priors for syntactic and semantic inference.
Following this work, Lang and Lapata (2010) for-
mulated role induction as the problem of detecting
alternations and mapping non-standard linkings to
cannonical ones, and later as a graph partitioning
problem in (Lang and Lapata, 2011b). They also
proposed an algorithm that uses successive splits and
merges of semantic roles clusters in order to improve
30
their quality in (Lang and Lapata, 2011a). Finally,
Titov and Klementiev (2012), introduce two new
Bayesian models that treat unsupervised role induc-
tion as the clustering of syntactic argument signa-
tures, with clusters corresponding to semantic roles,
and achieve the best state-of-the-art results.
In this paper, we propose a novel unsupervised
approach to semantic role labeling that differs from
previous work in that it integrates the notion of verb
classes into the model (by analogy with VerbNet,
we call these verb classes, frames). We show that
this approach gives good results both on the En-
glish PropBank and on a French corpus annotated
with VerbNet style semantic roles. For the English
PropBank, although the model is more suitable for
a framework that uses a shared set of role labels
such as VerbNet, we obtain results comparable to
the state-of-the-art. For French, the model is shown
to outperform a strong baseline by a wide margin.
2 Probabilistic Model
As mentioned in the introduction, semantic role la-
beling comprises two sub-tasks: argument identifi-
cation and role induction. Following common prac-
tice (Lang and Lapata, 2011a; Titov and Klemen-
tiev, 2012), we assume oracle argument identifica-
tion and focus on argument labeling. The approach
we propose is an unsupervised generative Bayesian
model that clusters arguments into classes each of
which can be associated with a semantic role. The
model starts by generating a frame assignment to
each verb instance where a frame is a clustering of
verbs and associated roles. Then, for each observed
verb argument, a semantic role is drawn conditioned
on the frame. Finally, the word and dependency la-
bel of this argument are generated. The model ad-
mits a simple Gibbs algorithm where the number of
latent variables is proportional to the number of roles
and frames to be clustered.
There are two key benefits of this model architec-
ture. First, it directly encodes linguistic intuitions
about semantic frames: the model structure reflects
the subcategorisation property of the frame variable,
which also groups verbs that share the same set of
semantic roles, something very close to the VerbNet
notion of frames. Second, by ignoring the ?verb-
specific? nature of PropBank labels, we reduce the
Figure 1: Plate diagram of the proposed directed
Bayesian model.
need for a large amount of data and we better share
evidence across roles.
In addition, because it is unsupervised, the model
is independent both of the language and of the spe-
cific semantic framework (since no inventory of se-
mantic role is a priori chosen).
2.1 Model description
The goal of the task is to assign argument instances
to clusters, such that each argument cluster repre-
sents a specific semantic role, and each role corre-
sponds to one cluster. The model is represented in
the form of a plate diagram in Figure 1. The ob-
served random variables are the verb V (lemma), its
voice V o (active or passive), the words W (lemma)
that are arguments of this verb, and the syntactic de-
pendency labelsD that link the argument to its head.
There are two latent variables: the frame F that rep-
resents the class of the verb, and the role R assigned
to each of its arguments. The parameters ? of all
multinomial distributions are Dirichlet distributed,
with fixed symmetric concentration hyper-parameter
?. The frame plays a fundamental role in this set-
ting, since it intends to capture classes of verbs that
share similar distributions of role arguments.
The model?s generative story is described next,
followed by a description of the inference algorithm
used to apply the model to an unannotated corpus.
2.2 Generative story
For each verb instance, the proposed model first gen-
erates a frame cluster, a voice (active or passive), and
31
then a verb lemma from the distribution of verbs in
this frame. The number of arguments is assumed
fixed. For each argument, a role is sampled condi-
tioned on the frame. Then, a word is sampled from
the distribution of words associated to this role, and
finally a dependency label is generated, conditioned
both on the role and the voice. All multinomial pa-
rameters are collapsed, and thus not sampled. All
Dirichlet hyper-parameters are assumed constant.
To identify words, we use either word lemmas or
part-of-speech tags. In order to avoid data sparse-
ness issues, we consider the word lemma only in
cases where there are more than 9 instances of the
word lemma in the corpus. Otherwise, if the number
of word lemma instances is less than 10, we use the
part-of-speech tags.
2.3 Learning and Inference
A collapsed Gibbs sampler is used to perform poste-
rior inference on the model. Initially, all frames Fi
are sampled randomly from a uniform distribution,
while the roles Ri,j are assigned either randomly or
following the deterministic syntactic function base-
line, which simply clusters predicate arguments ac-
cording to their syntactic function. This function is
described in detail in Section 3.
The Gibbs sampling algorithm samples each la-
tent variable (Fi and Ri,j) in turn according to its
posterior distribution conditioned on all other in-
stances of this variable (noted F?i and R?(i,j) re-
spectively) and all other variables. These posteriors
are detailed next.
In the following, Ri,j represents the random vari-
able for the jth role of the ith verb in the corpus: its
value is Ri,j = ri,j at a given iteration of the sam-
pling algorithm. nrf,r is the count of occurrences of
(Fi = f,Ri,j = r) in the whole corpus, excluding
the ith instance when the superscript ?i is used. A
star ? matches any possible value. The joint proba-
bility over the whole corpus with collapsed multino-
mial parameters is:
p(F,R, V,W,D, V o|?)
=
?Nf
i=1 ?(nf i + ?
F )
?(
?Nf
i=1 nf i + ?
F )
?(
?Nf
i=1 ?
F )
?Nf
i=1 ?(?
F )
?
Nf?
i=1
?Nv
j=1 ?(nvi,j + ?
V )
?(
?Nv
j=1 nvi,j + ?
V )
?(
?Nv
j=1 ?
V )
?Nv
j=1 ?(?
V )
?
Nf?
i=1
?Nr
j=1 ?(nri,j + ?
R)
?(
?Nr
j=1 nri,j + ?
R)
?(
?Nr
j=1 ?
R)
?Nr
j=1 ?(?
R)
?
Nvo?
i=1
Nr?
j=1
?Nd
k=1 ?(ndi,j,k + ?
D)
?(
?Nd
k=1 ndi,j,k + ?
D)
?(
?Nd
k=1 ?
D)
?Nd
k=1 ?(?
D)
?
Nr?
i=1
?Nw
j=1 ?(nwi,j + ?
W )
?(
?Nw
j=1 nwi,j + ?
W )
?(
?Nw
j=1 ?
W )
?Nw
j=1 ?(?
W )
?
?Nvo
i=1 ?(nvoi + ?
V o)
?(
?Nvo
i=1 nvoi + ?
V o)
?(
?Nvo
i=1 ?
V o)
?Nvo
i=1 ?(?
V o)
The posterior from which the frame is sampled is
derived from the joint distribution as follows:
p(Fi = y|F?i, R, V,W, V o) (1)
?
p(F,R, V,W,D, V o)
p(F?i, R?i, V?i,W?i, D?i, V o?i)
=
(nf?iy + ?
F )
(
?Nf
z=1 nf
?i
z + ?F )
?
(nv?iy,vi + ?
V )
(
?Nv
j=1 nv
?i
y,j + ?
V )
?
?
r?ri,?
?nr+ir ?1
x=0 (nr
?i
y,r + ?R + x)
?Mi
x=0(
?Nr
r=1 nr
?i
y,r + ?R + x)
where nr+ir is the count of occurrences of role r in
the arguments of verb instance i (Mi =
?
r nr
+i
r ).
The update equation for sampling the role be-
comes:
p(Ri,j = y|R?(i,j), F, V,W,D, V o) (2)
?
p(F,R, V,W,D, V o)
p(F?i, V?i, R?(i,j),W?(i,j), D?(i,j), V o?(i,j))
=
(nr?(i,j)fi,y + ?
R)
(
?Nr
k=1 nr
?(i,j)
fi,k
+ ?R)
?
(nd?(i,j)voi,y,di,j + ?
D)
(
?Nd
k=1 nd
?(i,j)
voi,y,k
+ ?D)
?
(nw?(i,j)y,wi,j + ?
W )
(
?Nw
k=1 nw
?(i,j)
y,k + ?
W )
After T iterations, the process is stopped and the
expected value of the sampled frames and roles af-
ter the burn-in period (20 iterations) is computed.
With deterministic (syntactic) initialization, T is set
to 200, while it is set to 2000 with random initializa-
tion because of slower convergence.
3 Evaluations and results
We evaluate our model both on English to situate
our approach with respect to the state of the art; and
on French to demonstrate its portability to other lan-
guages.
3.1 Common experimental setup
The model?s parameters have been tuned with a
few rounds of trial-and-error on the English devel-
opment corpus: For the hyper-parameters, we set
32
?F = 0.5, ?R = 1.e?3, ?V = 1.e?7, ?V o = 1.e?3,
?D = 1.e?8 and ?W = 0.5. For the evaluation on
French, we only changed the ?F and ?W parame-
ters. In order to reflect the rather uniform distribu-
tion of verb instances across verb classes we set ?F
to 1. Moreover, we set ?W to 0.001 because of the
smaller number of words and roles in the French cor-
pus. The number of roles and frames were chosen
based on the properties of each corpus. We set num-
ber of roles to 40 and 10, and the number of frames
to 300 and 60 for English and French respectively.
As done in (Lang and Lapata, 2011a) and (Titov and
Klementiev, 2012), we use purity and collocation
measures to assess the quality of our role induction
process. For each verb, the purity of roles? clusters
is computed as follows:
PU =
1
N
?
i
max
j
|Gj ? Ci|
where Ci is the set of arguments in the ith clus-
ter found, Gj is the set of arguments in the jth gold
class, and N is the number of argument instances.
In a similar way, the collocation of roles? clusters is
computed as follows:
CO =
1
N
?
j
max
i
|Gj ? Ci|
Then, each score is averaged over all verbs. In the
same way as (Lang and Lapata, 2011a), we use the
micro-average obtained by weighting the scores for
individual verbs proportionally to the number of ar-
gument instances for that verb. Finally the F1 mea-
sure is the harmonic mean of the aggregated values
of purity and collocation:
F1 =
2 ? CO ? PU
CO + PU
3.2 Evaluations on French
To evaluate our model on French, we used a manu-
ally annotated corpora consisting on sentences from
the Paris 7 Treebank (Abeille? et al, 2000), con-
taining verbs extracted from the gold standard V-
GOLD (Sun et al, 2010)1. For each verb, at most 25
sentences from the Paris 7 Treebank were randomly
1V-GOLD consists of 16 fine grained Levin classes with 12
verbs each (translated to French) whose predominant sense in
English belong to that class.
Role VerbNet roles
Agent Agent, Actor, Actor1, Actor2
Experiencer Experiencer
Theme Stimulus, Theme, Theme1, Theme2
Topic Proposition, Topic
PredAtt Predicate, Attribute
Patient Patient, Patient1, Patient2
Start Material, Source
End Product, Destination, Recipient
Location Location
Instrument Instrument
Cause Cause
Beneficiary Beneficiary
Extent Asset, Extent, Time, Value
Table 1: VerbNet role groups (French).
selected and annotated with VerbNet-style thematic
roles. In some cases, the annotated roles were ob-
tained by merging some of the VerbNet roles (e.g.,
Actor, Actor1 and Actor2 are merged); or by group-
ing together classes sharing the same thematic grids.
The resulting roles assignment groups 116 verbs into
12 VerbNet classes, each associated with a unique
thematic grid. Table 1 shows the set of roles used
and their relation to VerbNet roles. This constitutes
our gold evaluation corpus.
The baseline model is the ?syntactic function?
used for instance in (Lang and Lapata, 2011a),
which simply clusters predicate arguments accord-
ing to the dependency relation to their head. This
is a standard baseline for unsupervised SRL, which,
although simple, has been shown difficult to outper-
form. As done in previous work, it is implemented
by allocating a different cluster to each of the 10
most frequent syntactic relations, and one extra clus-
ter for all the other relations. Evaluation results are
shown in Table 2. The proposed model significantly
outperforms the deterministic baseline, which vali-
dates the unsupervised learning process.
PU CO F1
Synt.Func. (baseline) 78.9 73.4 76.1
Proposed model - rand. init 74.6 82.9 78.5
Table 2: Comparison of the Syntactic Function baseline
with the proposed system initialized randomly, evaluated
with gold parses and argument identification (French).
3.3 Evaluations on English
We made our best to follow the setup used in previ-
ous work (Lang and Lapata, 2011a; Titov and Kle-
33
mentiev, 2012), in order to compare with the current
state of the art.
The data used is the standard CoNLL 2008 shared
task (Surdeanu et al, 2008) version of Penn Tree-
bank WSJ and PropBank. Our model is evaluated
on gold generated parses, using the gold PropBank
annotations. In PropBank, predicates are associated
with a set of roles, where roles A2-A5 or AA are
verb specific, while adjuncts roles (AM) are con-
sistent across verbs. Besides, roles A0 and A1 at-
tempt to capture Proto-Agent and Proto-Patient roles
(Dowty, 1991), and thus are more valid across verbs
and verb instances than A2-A5 roles.
Table 3 reports the evaluation results of the pro-
posed model along with those of the baseline system
and of some of the latest state-of-the-art results.
PU CO F1
Synt.Func.(LL) 81.6 77.5 79.5
Split Merge 88.7 73.0 80.1
Graph Part. 88.6 70.7 78.6
TK-Bay.1 88.7 78.1 83.0
TK-Bay.2 89.2 74.0 80.9
Synt.Func. 79.6 84.6 82.0
Proposed model - rand. init 82.2 83.4 82.8
Proposed model - synt. init 83.4 84.1 83.7
Table 3: Comparison of the proposed system (last 2 rows)
with other unsupervised semantic role inducers evaluated
on gold parses and argument identification.
We can first note that, despite our efforts to
reproduce the same baseline, there is still a dif-
ference between our baseline (Synt.Func.) and
the baseline reported in (Lang and Lapata, 2011a)
(Synt.Func.(LL)) 2.
The other results respectively correspond to the
Split Merge approach presented in (Lang and Lap-
ata, 2011a) (Split Merge), the Graph Partitioning al-
gorithm (Graph Part.) presented in (Lang and Lap-
ata, 2011b), and two Bayesian approaches presented
in (Titov and Klementiev, 2012), which achieve the
best current unsupervised SRL results. The first such
model (TK-Bay.1) clusters argument fillers and di-
rectly maps some syntactic labels to semantic roles
for some adjunct like modifiers that are explicitly
represented in the syntax, while the second model
(TK-Bay.2) does not include these two features.
2We identified afterwards a few minor differences in both
experimental setups that partly explain this, e.g., evaluation on
the test vs. train sets, finer-grained gold classes in our case...
Two versions of the proposed model are reported
in the last rows of Table 3: one with random (uni-
form) initialization of all variables, and the other
with deterministic initialization of all Ri from the
syntactic function. Indeed, although many unsuper-
vised system are very sensitive to initialization, we
observe that in the proposed model, unsupervised in-
ference reaches reasonably good performances even
with a knowledge-free initialization. Furthermore,
when initialized with the strong deterministic base-
line, the model still learns new evidences and im-
proves over the baseline to give comparable results
to the best unsupervised state-of-the-art systems.
4 Conclusions and future work
We have presented a method for unsupervised SRL
that is based on an intuitive generative Bayesian
model that not only clusters arguments into seman-
tic roles, but also explicitly integrates the concept
of frames in SRL. Previous approaches to seman-
tic role induction proposed some clustering of roles
without explicitly focusing on the verb classes gen-
erated. Although there has been work on verb clus-
tering, this is, to the best of our knowledge, the first
approach that jointly considers both tasks.
In this work in progress, we focused on the role
induction task and we only evaluated this part, leav-
ing the evaluation of verb classes as future work. We
successfully evaluated the proposed model on two
languages, French and English, showing, in both
cases, consistent performances improvement over
the deterministic baseline. Furthermore, its accu-
racy reaches a level comparable to that of the best
state-of-the-art unsupervised systems.
The model could be improved in many ways, and
in particular by including some penalization term for
sampling the same role for several arguments of a
verb instance (at least for core roles). Moreover, we
believe that our model better fits within a framework
that allows roles sharing between verbs (or frames),
such as VerbNet, and we would like to carry out a
deeper evaluation on this concept.
Acknowledgments
The authors wish to thank Claire Gardent for her
valuable suggestions and Ingrid Falk for providing
the data for the evaluation on French.
34
References
A. Abeille?, L. Cle?ment, and A. Kinyon. 2000. Building
a treebank for French. In Proceedings of the LREC
2000.
David Dowty. 1991. Thematic proto-roles and argument
selection. Language, 67:547?619.
Trond Grenager and Christopher D. Manning. 2006.
Unsupervised discovery of a statistical verb lexicon.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?06, pages 1?8, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Joel Lang and Mirella Lapata. 2010. Unsupervised in-
duction of semantic roles. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, HLT ?10, pages 939?947, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Joel Lang and Mirella Lapata. 2011a. Unsupervised se-
mantic role induction via split-merge clustering. In
ACL, pages 1117?1126. Association for Computer
Linguistics.
Joel Lang and Mirella Lapata. 2011b. Unsupervised
semantic role induction with graph partitioning. In
EMNLP, pages 1320?1331. Association for Computer
Linguistics.
Llu??s Ma`rquez, Xavier Carreras, Kenneth C. Litkowski,
and Suzanne Stevenson. 2008. Semantic role label-
ing: an introduction to the special issue. Comput. Lin-
guist., 34(2):145?159, June.
Sebastian Pado and Mirella Lapata. 2006. Optimal con-
stituent alignment with edge covers for semantic pro-
jection. In Proceedings of ACL-COLING 2006, pages
1161?1168, Sydney, Australia.
Sebastian Pado and Guillaume Pitel. 2007. Annotation
pre?cise du franais en se?mantique de ro?les par projec-
tion cross-linguistique. In Proceedings of TALN-07,
Toulouse, France.
Sameer S. Pradhan, Wayne Ward, and James H. Martin.
2008. Towards robust semantic role labeling. Comput.
Linguist., 34(2):289?310, June.
L. Sun, A. Korhonen, T. Poibeau, and C. Messiant. 2010.
Investigating the cross-linguistic potential of VerbNet-
style classification. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics,
COLING ?10, pages 1056?1064, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The
conll-2008 shared task on joint parsing of syntactic
and semantic dependencies. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning, CoNLL ?08, pages 159?177, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Robert S. Swier and Suzanne Stevenson. 2004. Unsu-
pervised Semantic Role Labelling. In EMNLP, pages
95?102. Association for Computational Linguistics.
Ivan Titov and Alexandre Klementiev. 2012. A bayesian
approach to unsupervised semantic role induction. In
Proceedings of the Conference of the European Chap-
ter of the Association for Computational Linguistics,
Avignon, France, April.
Lonneke van der Plas, Paola Merlo, and James Hender-
son. 2011. Scaling up cross-lingual semantic annota-
tion transfer. In Proceedings of ACL/HLT, pages 299?
304.
35
Proceedings of the SIGDIAL 2013 Conference, pages 12?20,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Unsupervised structured semantic inference for spoken dialog reservation
tasks
Alejandra Lorenzo
Universite? de Lorraine
LORIA, UMR 7503
Nancy, France
alejandra.lorenzo@loria.fr
Lina M. Rojas-Barahona
LORIA, UMR 7503
Nancy, France
lina.rojas@loria.fr
Christophe Cerisara
LORIA, UMR 7503
Nancy, France
cerisara@loria.fr
Abstract
This work proposes a generative model
to infer latent semantic structures on top
of manual speech transcriptions in a spo-
ken dialog reservation task. The proposed
model is akin to a standard semantic role
labeling system, except that it is unsuper-
vised, it does not rely on any syntactic in-
formation and it exploits concepts derived
from a domain-specific ontology. The
semantic structure is obtained with un-
supervised Bayesian inference, using the
Metropolis-Hastings sampling algorithm.
It is evaluated both in terms of attachment
accuracy and purity-collocation for clus-
tering, and compared with strong baselines
on the French MEDIA spoken-dialog cor-
pus.
1 Introduction
Many concrete applications that involve human-
machine spoken dialogues exploit some hand-
crafted ontology that defines and relates the con-
cepts that are useful for the application. The main
challenge for the dialog manager used in the appli-
cation is then to interpret the user?s spoken input
in order to correctly answer the user?s expectations
and conduct a dialogue that shall be satisfactory
for the user. This whole process may be decom-
posed into the following stages:
? Automatic speech recognition, to transform
the acoustic signal into a sequence of words
(or sequences of word hypotheses);
? Spoken language understanding, to segment
and map these sequences of words into con-
cepts of the ontology;
? Semantic analysis, to relate these concepts
together and interpret the semantic of the user
input at the level of the utterance, or of the
speaker turn;
? Dialogue act recognition
? Dialogue planning
? Text generation
? ...
Note that the process sketched here often further
involves several other important steps that are used
internally within one or several of these broad
stages, for instance named entity recognition, co-
reference resolution, syntactic parsing, marcov de-
cision process, reinforcement learning, etc.
This work focuses mainly on the second and
third stages, since we assume that segmentation
is given and we want to discover the underly-
ing concepts and relations in the data. The third
stage is very important because it exhibits the la-
tent semantic structure hidden in the user utter-
ance: what is the object affected by a given pred-
icate ? What are the modifiers that may alter the
meaning of a predicate ? Without such a structure,
the system can hardly push understanding beyond
lexical semantics and reach fine-grained seman-
tic representations, which are thus often limited
to well-formed inputs and cannot handle sponta-
neous speech as considered here. But still, despite
its importance, most spoken dialog systems do not
make use of such structure.
We propose an approach here to address this
issue by directly inferring the semantic structure
from the flat sequence of concepts using the un-
supervised Bayesian learning framework. Hence,
the proposed model does not rely on any prede-
fined corpus annotated with semantic structure,
which makes it much more robust to spoken inputs
and adaptable to new domains than traditional su-
pervised approaches.
12
2 Related work
In recent years, an increasing number of works
have addressed robustness and adaptability issues
in most of standard Natural Language Processing
tasks with unsupervised or semi-supervised ma-
chine learning approaches. Unsupervised learn-
ing attempts to induce the annotations from large
amounts of unlabeled data. Several approaches
have recently been proposed in this context for the
semantic role labeling task. (Swier and Stevenson,
2004) were the first to introduce an unsupervised
semantic parser, followed by (Grenager and Man-
ning, 2006), (Lang and Lapata, 2010), (Lang and
Lapata, 2011b) and (Lang and Lapata, 2011a). Fi-
nally, (Titov and Klementiev, 2012), introduced
two new Bayesian models that achieve the best
current state-of-the-art results. However, all these
works use some kind of supervision (namely a
verb lexicon or a supervised syntactic system,
which is the case in most of the approaches).
(Abend et al, 2009) proposed an unsupervised
algorithm for argument identification that uses
a fully unsupervised syntactic parser and where
the only supervised annotation is part-of-speech
(POS) tagging.
Semi-supervised learning attempts to improve
the performance of unsupervised algorithms by
using both labeled and unlabeled data for train-
ing, where typically the amount of labeled data is
smaller. A variety of algorithms have been pro-
posed for semi-supervised learning1. In the con-
text of semantic role labeling, (He and Gildea,
2006) and (Lee et al, 2007) hence tested self-
training and co-training, while (Fu?rstenau and La-
pata, 2009) used a graph-alignment method to
semantic role labeling (SRL). Finally, in (De-
schacht and Moens, 2009) the authors present a
semi-supervised Latent Words Language Model,
which outperforms a state-of-the-art supervised
baseline. Although semi-supervised learning ap-
proaches minimize the manual effort involved,
they still require some amount of annotation. This
annotation is not always available, sometimes ex-
pensive to create and often domain specific. More-
over, these systems assume a specific role labeling
(e.g. PropBank, FrameNet or VerbNet) and are not
generally portable from one framework to another.
A number of works related to semantic infer-
ence have already been realized on the French
1We refer the reader to (Zhu, 2005) or (Pise and Kulkarni,
2008) for an overview on semi-supervised learning methods.
MEDIA corpus. Hence, dynamic Bayesian net-
works were proposed for semantic composition
in (Meurs et al, 2009), however their model re-
lies on manual semantic annotation (i.e. concept-
value pairs) and supervised training through the
definition of 70 rules. In (Huet and Lefe`vre, 2011;
Camelin et al, 2011) unsupervised models were
proposed that use stochastic alignment and Latent
Dirichlet Allocation respectively, but these mod-
els infer a flat concept-value semantic representa-
tion. Compared to these works, we rather propose
a purely unsupervised approach for structured se-
mantic Metropolis-Hastings inference with a gen-
erative model specifically designed for this task.
3 Proposed model
3.1 Principle
We consider a human-machine dialog, with the ob-
jective of automatically building a semantic struc-
ture on top of the user?s spoken utterances that
shall help the dialog system to interpret the user
inputs. This work focuses on inferring the seman-
tic structure, and it assumes that a segmentation of
users? utterances into concepts is given. More pre-
cisely, we exploit as input a manual segmentation
of each utterance into word segments, where each
segment represents a single concept that belongs
to MEDIA ontology (Denis et al, 2006) (see Fig-
ure 1).
Attributes
Price General
Park
Relative
Near
Restaurant
Location Person Time
Hotel Room
Object
Thing
Figure 1: Excerpt of MEDIA ontology
This ontology identifies the concepts that can
have arguments, and we thus use this informa-
tion to further distinguish between head segments
that can have arguments (noted Wh2 in Figure 3)
and argument segments that cannot govern another
concept (noted Wa). From these two classes of
2Wh actually represents one word in a segment composed
of Nh words, but by extension, we implicitly refer here to the
full segment.
13
segments and the words? inflected forms that com-
pose each segment we infer:
? A semantic structure composed of triplets
(Wa,Wh, A) where A is the type of argu-
ment, or, in other words, the type of semantic
relation between both segments;
? A semantic class Ct for the head segment
An example of the target structure we want to ob-
tain is shown in Figure 2.
Inference of these structure and classes is real-
ized with an unsupervised Bayesian model, i.e.,
without training the model on any corpus anno-
tated with such relations. Instead, the model is
trained on an unlabeled dialog corpus composed
of raw manual speech transcriptions, which have
also been manually segmented into utterances and
words? segments as described above. Training is
actually realized on this corpus using an approxi-
mate Bayesian inference algorithm that computes
the posterior distribution of the model?s param-
eters given the dataset. We have used for this
purpose the Metropolis-Hastings Markov Chain
Monte Carlo algorithm.
3.2 Bayesian model
Figure 3 shows the plate diagram of the proposed
model. The plate Nh (respectively Nw) that sur-
rounds a shaded node represents a single words?
segment of length Nh (respectively Nw). The
outer plate Nu indicates that the graphical model
shall be repeated for each of the Nu utterances in
the corpus.
Variable Description
Ct latent semantic type assigned to predicate t
Wh observed words in each head segment.
P (Wh|Ct) encodes lexical preferences for the
semantic inference
Ai latent semantic type assigned to the ith argu-
ment of predicate t
Rpi latent relative position assigned to the ith argu-
ment of predicate t
Wa observed words in each argument segment.
P (Wa|Ai) encodes lexical preferences for the
semantic inference
Table 1: Variables of the model
Each head word segment has a latent semantic
type Ct, and governs Na arguments. Each argu-
ment is represented by an argument words? seg-
ment, which has a latent semantic typeA. Each ar-
gument is further characterized by its relative po-
sition Rp with respect to its head segment. Rp
C1 ? ? ? Ct?1 Ct Ct+1 ? ? ? CNc
Wh
Nh
A
Wa
Nw
Rp
Na
Nu
Figure 3: Plate diagram of the proposed model.
Nu represents the number of utterances; Nh, the
number of words in a head segment; Nw, the num-
ber of words in an argument segment; and Na the
number of arguments assigned to predicate t.
can have 4 values, depending on whether the argu-
ment is linked to the closest (1) or another (2) ver-
bal3 head, or the closest (3) or another (4) nominal
head. Rp is derived from the argument-to-head
assignment, which is latent. So, Rp is also latent.
The sequence of Nc head segments in utterance u
is captured by the HMM shown on top of the plate
diagram, which models the temporal dependency
between successive ?semantic actions? of the user.
The variables of the model are explained in Ta-
ble 1.
The most important property of this model is
that the number of arguments Na is not known be-
forehand. In fact, every argument segment can be
governed by any of the Nc head segments in the
utterance, and it is the role of the inference pro-
cess to actually decide with which head it should
be linked. This is why the model performs struc-
tured inference.
Concretely, at any time during training, every
argument is governed by a single head. Then, in-
ference explores a new possible head attachment
for an argument Wa, which impacts the model as
follows:
? The number of arguments Na of the previous
head is decreased by one;
? The number of argumentsNa of the new head
is increased by one;
3Morphosyntactic classes are obtained with the Treetag-
ger
14
Je voudrais le prix en fait je euh une chambre pas che`re
I ?d like the price well in fact I uh a room not expensive
Reserve Room
Agent
Price Price
Booked object
Figure 2: Example of inferred semantic structure for a sentence in the MEDIA corpus. Traditional
dependency notations are used: the head segment points to the argument segment, where segments are
shown with boxes (arrows link segments, not words !). The semantic class assigned to each head segment
is shown in bold below the translated text.
? The relative position Rp of the argument is
recomputed based on its new head position;
? The argument typeA is also re sampled given
the new head type Ct.
This reassignment process, which is at the heart of
our inference algorithm, is illustrated in Figure 4.
3.3 Metropolis inference
Bayesian inference aims at computing the poste-
rior distribution of the model?s parameters, given
the observed data. We assume that all distributions
in our model are multinomial with uniform priors.
The parameters are thus:
P (Wh|Ct) ?M(?HCt)
Distribution of the
words for a given
head semantic class
P (Ct|Ct?1) ?M(?CCt?1)
Transition prob-
abilities between
semantic classes
P (Wa|A) ?M(?WA )
Distribution of the
words for a given
argument type
P (Rp|A) ?M(?RA)
Distrib. of the rel-
ative position of a
given argument to
its head given the
argument type
P (A|Ct) ?M(?ACt)
Distrib. of the ar-
gument types given
a head semantic
class
3.3.1 Inference algorithm
To perform inference, we have chosen a Markov
Chain Monte Carlo algorithm. As our model is
finite, parametric and identifiable, Doob?s theo-
rem guarantees the consistency of its posterior,
and thus the convergence of MCMC algorithms
towards the true posterior. Because changing the
head of one argument affects several variables si-
multaneously in the model, it is problematic to
use the basic Gibbs sampling algorithm. A block-
Gibbs sampling would have been possible, but this
would have increased the computational complex-
ity and we also wanted to keep as much flexibility
as possible in the jumps that could be realized in
the search space, in order to prevent slow-mixing
and avoid (nearly) non-ergodic Markov chains,
which are likely to occur in such structured infer-
ence problems.
We have thus chosen a Metropolis-Hastings
sampling algorithm, which allows us to design an
efficient proposal distribution that is adapted to our
task. The algorithm proceeds by first initializing
the variables with a random assignment of argu-
ments to one of the heads in the utterance, and a
uniform sampling of the class variables. Then, it
iterates through the following steps:
1. Sample uniformly one utterance u
2. Sample one jump following the proposal dis-
tribution detailed in Section 3.3.2.
3. Because the proposal is uniform, compute the
acceptance ratio between the model?s joint
probability at the proposed (noted with a ?)
and current states:
r = P (C
?,W ?h,W ?a, Rp?, A?)
P (C,Wh,Wa, Rp,A)
4. Accept the new sample with probability
min(1, r); while the sample is not accepted,
iterate from step 2.
15
Je voudrais le prix en fait je euh une chambre pas che`re
I ?d like the price well in fact I uh a room not expensive
Agent
Price Price
Booked object
Agent
Price
Booked object
Price
Figure 4: Illustration of the reassignment process following the expample presented in Figure 2. This
example illustrates the third Metropolis proposed move, which changes the head of argument ?le prix?:
arcs above the text represent the initial state, while arcs below the text represent the new proposed state.
5. When the sample is accepted, update the
multinomials accordingly and iterate from
step 1 until convergence.
This process is actually repeated for 2,000,000
iterations, and the sample that gives the largest
joint probability is chosen.
3.3.2 Metropolis proposal distribution
The proposal distribution is used to explore the
search space in an efficient way for the target
application. Each state in the search space is
uniquely defined by a value assignment to every
variable in the model, for every utterance in the
corpus. It corresponds to one possible sample of
all variables, or in other words, to the choice of
one possible semantic structure and class assign-
ment to all utterances in the corpus.
Given a current state in this search space, the
proposal distribution ?proposes? to jump to a
new state, which will then be evaluated by the
Metropolis algorithm. Our proposal samples a
new state in the following successive steps:
1. Sample uniformly one of the three possible
moves:
Move1: Change the semantic class of a head;
Move2: Change the argument type of an argu-
ment segment;
Move3: Change the assignment of an argument
to a new head;
2. If Move1 is chosen, sample uniformly one
head segment and one target semantic class;
3. If Move2 is chosen, sample uniformly one
argument segment and one target argument
type;
4. If Move3 is chosen, sample uniformly one
argument segment Wa and ?detach? it from
its current head. Then, sample uniformly one
target head segment W ?h, and reattach Wa to
its new head W ?h. Because the distribution of
argument types differ from one head class to
another, it would be interesting at this stage
to resample the argument type of Wa from
the new head class distribution. But in this
work, we resample the argument type from
the uniform distribution.
This proposal distribution Q(x ? x?) is re-
versible, i.e., Q(x? x?) > 0? Q(x? ? x) > 0.
We can show that it is further symmetric, i.e.,
Q(x ? x?) = Q(x? ? x), because the same
move is sampled to jump from x to x? than to jump
from x? to x, and because the proposal distribution
within each move is uniform.
4 Experimental validation
4.1 Experimental setup
The French MEDIA corpus collects about 70
hours of spontaneous speech (1258 dialogues,
46k utterances, 494.048 words and 4068 dis-
tinct words) for the task of hotel reservation
and tourist information (Bonneau-Maynard et al,
2005). Calls from 250 speakers to a simulated
reservation system (i.e. the Wizard-of-Oz) were
recorded and transcribed. Dialogues are full of
disfluencies, hesitations, false starts, truncations or
fillers words (e.g., euh or ben).
16
Gold Standard Annotation
Semantic Relation Frequency
Agent 320
Booked object 298
Location 285
Time 209
Coordination 134
Beneficiary 117
Price 108
Reference Location 66
Table 2: Most frequent semantic relations in the
gold annotation.
This corpus has been semantically annotated
as part of the French ANR project PORT-
MEDIA (Rojas-Barahona et al, 2011). We are
using a set of 330 utterances manually annotated
with gold semantic relations (i.e. High-Level Se-
mantics). This gold corpus gathers 653 head seg-
ments and 1555 argument segments, from which
around 20% are both arguments and heads, such
as une chambre in Figure 4. Table 2 shows the
semantic relations frequencies in the gold annota-
tion. 12 head segment types and 19 different argu-
ment segment types are defined in the gold anno-
tations. In the evaluation, we assume the number
of both classes is given. A possible extension of
the approach to automatically infer the number of
classes would be to use a non-parametric model,
but this is left for future work.
4.2 Evaluation metrics
The proposed method infers three types of seman-
tic information:
? The semantic relation between an argument
and its head;
? The argument type A
? The semantic class of the head Ct.
The three outcomes are evaluated as follows.
? The output structure is a forest of trees that
is similar to a partial syntactic dependency
structure. We thus use a classical unsuper-
vised dependency parsing metric, the Un-
labeled Attachment Score (UAS), which is
simply the accuracy of argument attachment:
an argument is correctly attached if and only
if its inferred head matches the gold head.
? Both argument and head classes correspond
to the outcome of a clustering process into
semantic classes, akin to the semantic classes
obtained in unsupervised semantic role la-
beling tasks. We then evaluate them with a
classical metric used to evaluate these classes
in unsupervised SRL (as done for instance
in (Lang and Lapata, 2011a) and (Titov and
Klementiev, 2012)): purity and collocation.
Purity measures the degree to which each clus-
ter contains instances that share the same gold
class, while collocation measures the degree to
which instances with the same gold class are as-
signed to a single cluster.
More formally, the purity of argument seg-
ments? (head segment?) clusters for the whole cor-
pus is computed as follows:
PU = 1N
?
i
max
j
|Gj ? Ci|
whereCi is the set of argument (head) segments
in the ith cluster found, Gj is the set of argument
(head) segments in the jth gold class, and N is
the number of gold argument (head) segment in-
stances. In a similar way, the collocation of argu-
ment segments? (head segment?) clusters is com-
puted as follows:
CO = 1N
?
j
max
i
|Gj ? Ci|
Finally the F1 measure is the harmonic mean of
the purity and collocation:
F1 = 2 ? CO ? PUCO + PU
4.3 Experimental results
We compare the proposed approach against two
baselines:
? An argument-head ?attachment? baseline,
which attaches each argument to the closest
head segment.
? A strong clustering baseline, which respec-
tively clusters the head and argument seg-
ments using a very effective topic model:
the Latent Dirichlet Allocation (LDA) ap-
proach (Blei et al, 2003).
17
Table 3 shows the UAS obtained for the pro-
posed model on the MEDIA corpus, while Table 4
shows the obtained Purity, Collocation and F1-
measure. In both cases, we compare the perfor-
mances of the proposed model with the respective
baseline. Our system outperforms both baselines
by a large margin.
System UAS
Closest attachment 68%
(?2%)
Proposed - UAS 74%
(?2%)
Table 3: Experimental results for UAS on the ME-
DIA database. The statistical confidence interval
at 95% with Gaussian approximation is reported.
System Purity Col. F-mes
LDA - Heads 51.7% 25.5% 34.2%
LDA - Args 31.7% 22.2% 26.1%
Proposed - Heads 78.7% 50.8% 61.8%
Proposed - Args 61.8% 53.3% 59.3%
Table 4: Experimental results on the MEDIA
database for purity, collocation and F1-measure.
4.3.1 Qualitative Evaluation
We further carried out a qualitative evaluation,
where we inspected the inferred clusters and com-
pared them with the baseline. Figures 7 and 8
show, for every head class Ct in each stacked col-
umn, the distribution of instances from all gold
clusters. Each column can also be viewed as a
graphical representation of the intersection of one
inferred class with all gold clusters. Figure 7 illus-
trates this for our model, and Figure 8 for LDA.
The same comparison for the argument types is
shown, respectively, in Figure 5 and Figure 6.
For head segment clusters, we can observe that
most inferred clusters contain many instances of
the Reservation type (in dark blue), both in the
LDA baseline and in the proposed system. The
main reason for that is that the corpus is very un-
balanced in favor of the Reservation class, while
we do not assume any prior knowledge about the
data and thus use a uniform prior. Still, every other
gold type that occurs with a reasonnably high
enough frequency, apart from two special types
that are discussed next, is well captured by one of
Figure 5: Distribution of the gold types (one per
color) into the clusters inferred by our system
(shown on the X-axis) for argument segments.
our inferred class: this is the case for ?Room? that
mainly intersects with our class 1, ?Place? with
our class 2 and ?Hotel? with our class 9.
Some examples of instances for each case are:
? Reservation: ?voudrais re?server?, ?aimerais
partir?, ?voudrais une *re?servation une
re?servation?, ?prends?, ?recherche? ,
?*de?sire de?sire?, ?il me faudrait?, ?opte?,
?aimerais s? il vous pla??t si c? est possible
avoir prendre?.
? Room: ?deux chambres pour un coup(le)
avec trois enfants avec bon standing?, ?trois
singles?, ?deux chambres de bon standing
a` peu pre`s niveau trois e?toiles?, ?trois dou-
bles?.
? Place: ?Paris?, ?a` Saintes?, ?a`
Charleville?, ?dans le dix huitie`me ar-
rondissement de Paris?.
? Hotel: ?un ho?tel deux e?toiles?, ?dans un
ho?tel beau standing?, ?un ho?tel formule un?,
?l? ho?t(el) le l? ho?tel?, ?un autre ho?tel dans
les me?mes conditions?, ?le Beaugency?, ?l?
autre?, ?au Novotel?, ?le premier?.
Two ?special? head segment types that are nei-
ther nicely captured by our system nor LDA are
Coordination and Inform, which are instead as-
signed to the clusters corresponding to the gold
segments that they coordinate or inform about.
For argument segments we also observed that
the inferred clusters are semantically related to the
gold types. We found, for instance, four clusters
18
Figure 6: Distribution of the gold types (one per
color) into the clusters inferred by the LDA base-
line (shown on the X-axis) for argument segments.
Figure 7: Distribution of the gold types (one per
color) into the clusters inferred by our system
(shown on the X-axis) for head segments.
(2, 5, 12 and 15) containing mainly ?Time? ar-
guments (?du premier au trois Novembre?, ?dix
nuit?, ?le festival du film?, ?au seize Novembre?,
etc.), two (3 and 14) dedicated to ?Location? argu-
ments (?a` Menton?, ?au festival lyrique de belle
euh Belle Ile En mer?, ?bastille?, ?sur le ville de
Paris?, ?parking prive??), one (10) for ?Price? ar-
guments (?pas plus de cent euros par personne?,
?un tarif infe?rieur a` quatre vingts euros?, ?pas
trop che`re?, ?a` cent vingt euros?, ?moins de cent*
cent euros?) etc.
Finally, as noted for the head segments, we can
observe that the most frequent gold types largely
intersect with several inferred clusters, for the
same reason: data is very unbalanced and we do
not assume any prior knowledge about the data
Figure 8: Distribution of the gold types (one per
color) into the clusters inferred by the LDA base-
line (shown on the X-axis) for head segments.
and thus use an uniform prior. Nevertheless, sev-
eral other important classes such as Event, Price
and Agent are well captured by our system.
5 Conclusions
This work proposes an unsupervised generative
model to infer latent semantic structures on top
of user spontaneous utterances. It relies on the
Metropolis-Hastings sampling algorithm to jointly
infer both the structure and semantic classes. It
is evaluated in the context of the French MEDIA
corpus for the hotel reservation task. Although the
system proposed in this work is evaluated on a spe-
cific spoken dialog reservation task, it actually re-
lies on a generic unsupervised structured inference
model and can thus be applied to many other struc-
tured inference tasks, as long as observed word
segments are given.
An interesting future direction of research
would be to modify this model so that it jointly
infers both the latent syntactic and semantic struc-
tures, which are known to be closely related but
still carry complementary information. We of
course also plan to evaluate the proposed model
with automatic speech transcriptions and concepts
decoding. Another advantage of the proposed
model is the possibility to build better Metropolis-
Hastings proposals, which may greatly improve
the convergence rate of the algorithm. In partic-
ular, we would like to investigate the use of some
non-uniform proposal distributions when reattach-
ing an argument to a new head, which shall im-
prove mixing.
19
References
Omri Abend, Roi Reichart, and Ari Rappoport. 2009.
Unsupervised argument identification for semantic
role labeling. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, ACL ?09,
pages 28?36, Stroudsburg, PA, USA. Association
for Computational Linguistics.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
dirichlet alocation. the Journal of machine Learn-
ing research, 3:993?1022.
Helene Bonneau-Maynard, Sophie Rosset, Christelle
Ayache, Anne Kuhn, and Djamel Mostefa. 2005.
Semantic annotation of the french MEDIA dialog
corpus. In INTERSPEECH-2005, 3457-3460.
N. Camelin, B. Detienne, S. Huet, D. Quadri, and
F. Lefe`vre. 2011. Unsupervised concept annota-
tion using latent dirichlet alocation and segmental
methods. In EMNLP 1st Workshop on Unsupervised
Learning in NLP, Edinburgh (UK).
Alexandre Denis, Matthieu Quignard, and Guillaume
Pitel. 2006. A Deep-Parsing Approach to Natural
Language Understanding in Dialogue System: Re-
sults of a Corpus-Based Evaluation. In Proceedings
of the 5th international Conference on Language Re-
sources and Evaluation (LREC 2006) Proceedings
of Language Resources and Evaluation Conference,
pages 339?344, Genoa Italie.
Koen Deschacht and Marie-Francine Moens. 2009.
Semi-supervised semantic role labeling using the la-
tent words language model. In Proc. EMNLP, pages
21?29.
Hagen Fu?rstenau and Mirella Lapata. 2009. Graph
alignment for semi-supervised semantic role label-
ing. In Proc. EMNLP, pages 11?20.
Trond Grenager and Christopher D. Manning. 2006.
Unsupervised discovery of a statistical verb lexicon.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?06, pages 1?8, Stroudsburg, PA, USA. Association
for Computational Linguistics.
S. He and H. Gildea. 2006. Self-training and Cotrain-
ing for Semantic Role Labeling: Primary Report.
Technical report, TR 891, University of Colorado at
Boulder.
Ste?phane Huet and Fabrice Lefe`vre. 2011. Unsuper-
vised alignment for segmental-based language un-
derstanding. In Proceedings of the First Workshop
on Unsupervised Learning in NLP, EMNLP ?11,
pages 97?104, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Joel Lang and Mirella Lapata. 2010. Unsuper-
vised induction of semantic roles. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, HLT ?10, pages 939?
947, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Joel Lang and Mirella Lapata. 2011a. Unsupervised
semantic role induction via split-merge clustering.
In Proc. ACL, pages 1117?1126.
Joel Lang and Mirella Lapata. 2011b. Unsupervised
semantic role induction with graph partitioning. In
EMNLP, pages 1320?1331. Association for Com-
puter Linguistics.
Joo-Young Lee, Young-In Song, and Hae-Chang Rim.
2007. Investigation of weakly supervised learning
for semantic role labeling. In ALPIT, pages 165?
170. IEEE Computer Society.
Marie-Jean Meurs, Fabrice Lefe`vre, and Renato
de Mori. 2009. Spoken language interpretation: On
the use of dynamic bayesian networks for semantic
composition. In Proc. ICASSP, pages 4773?4776.
Nitin Namdeo Pise and Parag Kulkarni. 2008. A sur-
vey of semi-supervised learning methods. In Pro-
ceedings of the 2008 International Conference on
Computational Intelligence and Security - Volume
02, CIS ?08, pages 30?34, Washington, DC, USA.
IEEE Computer Society.
Lina Maria Rojas-Barahona, Thierry Bazillon,
Matthieu Quignard, and Fabrice Lefevre. 2011.
Using MMIL for the high level semantic annotation
of the french MEDIA dialogue corpus. In Pro-
ceedings of the Ninth International Conference on
Computational Semantics (IWCS 2011).
Robert S. Swier and Suzanne Stevenson. 2004. Un-
supervised Semantic Role Labelling. In EMNLP,
pages 95?102. Association for Computational Lin-
guistics.
Ivan Titov and Alexandre Klementiev. 2012. A
bayesian approach to unsupervised semantic role in-
duction. In Proceedings of the Conference of the
European Chapter of the Association for Computa-
tional Linguistics, Avignon, France, April.
Xiaojin Zhu. 2005. Semi-Supervised Learning Liter-
ature Survey. Technical report, Computer Sciences,
University of Wisconsin-Madison.
20
