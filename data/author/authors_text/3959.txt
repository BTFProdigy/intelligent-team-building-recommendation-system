Turn-taking in Graphical Communication: an exploratory study 
Atsue Takeoka1,2, Atsushi Shimojima1,2, Yasuhiro Katagiri2 
1 School of Knowledge Science 
Japan Advanced Institute of Science and Technology 
1-1, Asahidai, Tatsunokuchi-machi, Nomi-gun, Ishikawa, 
923-1292, Japan 
{takeoka, ashimoji}@jaist.ac.jp 
2 ATR Media Information Science Labs 
 
2-2 Hikaridai Seika-cho, Soraku-gun, Kyoto, 
619-0288 Japan 
{katagiri}@atr.co.jp 
 
Abstract 
This paper reports an investigation of the 
turn-taking functions of drawings in 
graphical communication.  Based on the 
examination of dialogue data collected that 
involve collaborative drawing interactions, 
as well as spoken dialogue interactions, in a 
joint problem solving task, we found that 
?drawing turns,? in which the drawer 
presents a piece of information to her 
partner through drawing, had almost the 
same turn-keeping effects as ?speech turns.? 
1 Introduction 
2 
                                                     
Dialogue is characterized by taking turns, that is, 
who should talk next and when should they talk.  
Studies on turn-taking in dialogues have mostly 
focused on phenomena and processes of dialogue 
participants taking turns in spoken conversations.  
But speech is not the only means of interaction.  
We draw maps to give directions.  We draw plans 
to discuss floor plans.  We make diagrams to solve 
problems.  It is crucial to take account of these 
non-speech modes to obtain a comprehensible 
model of taking turns as the basic mechanisms of 
human interactions.  It is crucial to elucidate the 
nature of turn-taking in multi-modal setting, as 
communication tools become increasingly 
multi-modal to include such modes as free 
drawings, photographs, web page references and 
other visual information presentations. 
Several researchers have looked at roles of 
non-verbal cues, e.g., eye gaze, gestures and other 
bodily movements, in coordinating turn-taking in 
spoken utterances.  Non-verbal cues were regarded 
as ancillary to speech by providing additional 
signals to support orderly turn-takings in speech 
domain.  In contrast, when we expand the scope of 
dialogues to include multi-modal interactions, we 
need to consider the possibilities of non-speech 
turns, such as drawing turns, and cross-modal turn 
changes, such as those between drawings and 
speech. 
Based on graphical communication data we 
collected, we conducted a preliminary analysis on 
the existence and the functions of drawing turns.  
We found that drawing turns have almost the same 
turn-keeping power as speech turns. 
Mechanism of dialogue interactions: 
turn-taking and grounding 
Sacks, Schegloff and Jefferson (1974) noted 
surprisingly orderly nature of taking turns in our 
ordinary conversations, and proposed a set of rules 
to characterize the underlying mechanisms for the 
turn-taking structures in dialogues. 
We conduct dialogues with turn-takings 
governed by the following rules. 
At each transition-relevance place1 of each turn: 
-If during this turn the current speaker has 
selected A as the next speaker, then A must 
speak next. 
-If the current speaker does not select the next 
speaker, any other speaker may take the next 
turn. 
-If no one else takes the next turn, the current 
speaker may take the next turn. 
In this notion of taking turns, the current 
speaker presents a piece of information to the 
partner in a turn. When a turn consists of clauses, it 
presents a series of information. Even when a turn 
consists of a single word, it also presents a piece of 
information. To take a turn, the partner needs to 
1 Transition-relevance place is where the structure of the 
language allows speaker shifts to occur. 
issue an utterance that is more than a particular 
signal of acceptance of the information presented 
by the current speaker. 
From a different perspective, Clark (1996) 
developed the notion of grounding to capture 
processes with which conversants establish a set of 
information as a shared common ground for 
on-going (and possibly subsequent) dialogue. In a 
grounding process, a conversant presents a piece of 
information verbally or non-verbally, and the 
partner issues a particular public signal to show the 
receipt, understanding, or acceptance of that 
information. 
On the basis of Clark?s work, Traum (1994) 
substantiated the process of grounding in a 
finite-state transition model. The model specifies 
what contributions individual utterance in a 
dialogue make to a grounding process and how 
individual contributions combine themselves to 
make a sequence that completes the grounding. In 
Traum?s terms, such contributions are grounding 
acts, the units of utterance that can perform 
grounding acts are utterance units or 
UUs?according to their contribution to a 
grounding process, the sequences of utterance units 
that complete the grounding are discourse units or 
DUs. Table 1 shows the seven categories of 
grounding acts and their definitions. 
 
Table 1. Seven categories of grounding acts 
Initiate 
(init) 
An initial utterance component of a 
discourse unit. 
Continue 
(cont) 
A continuation of a previous act 
performed by the same speaker. 
Acknowledgement 
(ack) 
An acknowledgement claiming or 
demonstrating understanding of a 
previous utterance. It may be either a 
repetition or paraphrase of all or part of 
the utterance, an explicit signal of 
comprehension such as ?ok? or ?uh 
huh?, or an implicit signaling of 
understanding. 
Repair 
(repair) 
Changes the content of the current DU.
ReqRepair 
(reqRepair) 
A request for a repair by the other 
party. 
ReqAck 
(reqAck) 
Attempt to get the other agent to 
acknowledge the previous utterance. 
Cancel 
(cancel) 
Closes off the current DU as 
ungrounded. 
 
Now, we can define a turn as a constituent of 
one or more DUs that initiate a piece of 
information. The following figure schematically 
shows speech turns consisting of DUs that are 
made of annotated UUs. 
 
A:init
A-DU
A:cont B:ack A:init 
A-DU 
B:ack B:init 
B-DU 
B Speech TurnA Speech Turn
A:ack
 
 
 
 
Figure 1. Speech turns defined by the notion of 
DUs and UUs 
 
In graphical communication, especially in 
settings where one speaker gives information to a 
partner, communicator not only speak but also 
draw to present a piece of information. Thus, using 
the definition of speech turns mentioned above, 
natural questions arise as to ?drawing turns?, 
namely, turns that have analogical characteristics 
attributed to speech turns. 
This paper investigates this question in 
dialogues in which one is instructed to inform 
one?s floor plans to a partner. Do drawing turns 
have similar turn-taking rules to speech turns? 
How do drawing turns keep their turns? 
To address these questions, we collected a 
spoken dialogue data with drawings. However, 
since participants used only one pen at a time, no 
such phenomena as taking turns with a pen were 
observed2. Under this setting, our purpose in this 
study is to account for the effect of keeping turns 
when participants are taking drawing turns. 
3 Method 
3.1 Data 
3.2 Transcription 
                                                     
Our data consist of 2 dialogues. Both dyads were 
familiar with each other: in one case they were 
close friends and in the other they were mother and 
son. The objective of each dialogue is to inform 
each other of their floor plans. We prepared a pen 
and papers for the first dyad and two pens and 
papers for the second. However, the second dyad 
seldom drew at the same time. A video camera was 
used to videotape dyads and drawing surfaces from 
above. They participated on a voluntary basis. 
We transcribed 16 minutes and 30 seconds of 
2 The partner could take a turn by using their finger to 
draw transparently. However, this rarely occurred in our 
dialogues. 
dialogues in total. According to the objective of 
each dialogue, a person in each dyad first 
represented their plans with a drawing, and then 
they communicated by pointing or adding 
drawings for further explanation. Then the other 
person in each dialogue did the same. Therefore, 
we had four drawers in two dialogues. Hereafter, 
we refer to a drawer as X and the other party as Y. 
We took one representative portion from each 
drawer. The length of the transcriptions varied 
from one minute and 35 seconds to 5 minutes and 
42 seconds. All audible words and word fragments 
were transcribed, including overlapping speech, 
nonlexical fillers (such as ?uh?), and other 
vocalizations (such as laughter and harrumph). 
Silences of more than 100 milliseconds were also 
written down and the time recorded. 
After audible transcriptions, we added 
segmented drawings in transcriptions, at which 
point of the utterance the drawing segment starts 
and ends. We defined the starting point as the time 
the drawer begins to draw with a pen and the 
ending point as the time the drawer changes grip, 
does not move the pen on the shortest track, or 
takes their hand off of the drawing surface. 
3.3 Coding 
4 Results 
4.1 
On the basis of Traum?s finite-state transition 
model of grounding, we divided all speech in our 
dialogue data into utterance units (UUs), namely, 
?continuous speech by the same speaker, 
punctuated by prosodic boundaries (including 
pauses of significant length and boundary tones)? 
(Traum, 1994). One of the authors then classified 
each utterance unit into one of the seven categories 
of grounding acts. 
Summary of statistics 
In this section, we show a summary of the statistics 
of speech and drawing data. First, we counted the 
number of drawing segments and divided them into 
two groups. One group of drawings were started 
during X?s utterance, followed by X?s utterances or 
X?s utterances followed the drawings. Another 
group of drawings were started during Y?s 
utterance or between Y?s utterances. We call the 
former X-drawings and the latter Y-drawings. 
There were 149 X-drawings and 8 Y-drawings. We 
found four drawing segments by Y with Y?s finger 
but we excluded these from this summary. 
Second, we counted the number of utterances. 
Table 2 shows a summary of the statistics of the 
dialogue data. 
 
Table 2. Number of Utterance Units and their 
breakdown of the seven categories of grounding 
acts and number of Discourse Units 
 Total X Y 
UU 1028 587 441
Grounding Acts 
init 290 234 56
cont 269 218 51
repair 8 6 2
reqRepair 6 0 6
reqAck 5 5 0
ack 344 70 274
ack init 108 54 52
DU 395 287 108
*We also excluded utterances relevant to Y?s finger drawings. 
In this setting of dialogues, only X informed Y 
by drawing. However, X initiated 2.7 times more 
DUs than Y did. The objective of the dialogues was 
for one party to inform their floor plan to the other 
party. These summary data showed that 
information flowed from X to Y not only by 
drawings but also by utterances. 
4.2 Silences 
We first looked at the occurrences of silence in 
speech.  There were 436 periods of silence of more 
than 100 milliseconds in the portion analyzed.  
Silence was classified into 2 categories: during 
drawings and not during drawings. There were 164 
of the former and 272 of the latter. Figure 2 showed 
the frequency distribution of silent periods for each 
group. 
0.0%
5.0%
10.0%
15.0%
20.0%
25.0%
les
s t
ha
n 1
00
ms
10
1m
s-2
00
ms
20
1m
s-3
00
ms
30
1m
s-4
00
ms
40
1m
s-5
00
ms
50
1m
s-6
00
ms
60
1m
s-7
00
ms
70
1m
s-8
00
ms
80
1m
s-9
00
ms
90
1m
s-1
00
0m
s
10
01
ms
-11
00
ms
11
01
ms
-12
00
ms
12
01
ms
-13
00
ms
13
01
ms
-14
00
ms
14
01
ms
-15
00
ms
15
01
ms
-16
00
ms
16
01
ms
-17
00
ms
17
01
ms
-18
00
ms
18
01
ms
-19
00
ms
19
01
ms
-20
00
ms
mo
re
 th
an
 20
01
ms
Not Drawing
Drawing
Figure 2. Frequency distribution of silent periods 
of more than 100ms 
The average time of silences during drawing 
was 667ms and that of during not drawing was 
519ms. 
The data indicates that silences during drawing 
tend to be longer than those during not drawing.  
This suggests that drawings function as 
independent turns.  An occurrence of a drawing 
turn can keep the next speech turns from starting, 
despite no speech is uttered at the time. 
4.3 Overlaps 
If drawing turns work similarly to speech turns in 
terms of their turn-keeping functions, interruptions 
by overlapping speech should occur about the same 
amount toward drawing turns and speech turns. 
Case 1 in Figure 3 schematically shows a regular 
speech turn change structure, whereas Case 2 and 
Case 3, respectively, show a speech turn and a 
drawing turn interrupted by following speech 
turns. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3. Speech turns and drawing turns 
interrupted by speech turns 
Table 3 shows the relative amount of speech 
turns and drawing turns which were followed by 
overlapping next speech turns.  The result appears 
to indicate that drawing turns are more amenable to 
interruption than speech turns, though the 
difference is not statistically significant (Z0= 1.26, 
p=0.21 two-tailed). 
 
Table 3. Number of speech turn during not drawing 
(see Case 2 in figure 3) 
Total of X speech turns 153
Overlapping X speech turns 17
Ratio 11.1%
 
Table 4. Number of drawing turn during drawing 
(see Case 3 in figure 3) 
Total of X drawing turns 149
Overlapping X drawing turns 25
Ratio 16.8%
 
Close examination of Case 3, in which drawing 
turns are interrupted by the following speech turns, 
revealed that some of the overlapping speech turns 
were actually responses, either acknowledgments 
or request for repairs, directed toward drawing 
contents.  An example of an acknowledgment in 
the drawing context is found in the excerpt below. 
Case1: Ordinary speech turn-taking 
X Speech Turn Y Speech Turn
Case3: Overlappint X?s drawing turn with Y?s speech
turn 
Case2: Overlapping X?s speech turn with Y?s speech 
turn 
Y Speech Turn
X:ackY:init 
Y-DU 
X Speech Turn 
X:cont X:init 
X-DU 
Y:ack X:cont 
X-DU 
X:init 
X:ack
Y-DU
Y:initY:ack 
X-DU 
X:init Y:ack X:cont 
X-DU 
X:init 
Y Speech Turn
X:ackY:init 
Y-DU 
X Drawing Turn 
X Speech Turn 
Y:ack 
X-DU 
X:init Y:ack X:cont 
X-DU 
X:init 
 
act UU Utterance 
init26 41.1 X: kou kocchino houni [mizumawariga ari]
 (like here there is a sink) 
ack26 init27 42.1 Y:                                  [uraomote na] 
 (it is located back to back) 
 
Here, X was drawing a line between two houses 
she already drew before, and directing Y?s 
attention toward the location of kitchen sinks in 
them.  Y could recognize the location of the kitchen 
sinks in the two houses from the drawing, and Y?s 
subsequent short utterance ?uraomote na? verbally 
confirmed the relative locations of them.  Such 
utterances were directed toward drawings and 
worked to give acknowledgments toward their 
contents.  These types of utterances might appear 
as instances of initiation acts if we only look at 
speech domain, but, actually, they do not constitute 
independent initiation acts if we admit grounding 
acts that work across different modalities. 
Table 5 shows the result of recounting speech 
overlaps by excluding cross modal responses.  The 
result shows that drawing turns and speech turns 
are exactly the same (Z0=0.26, p=0.32 two-tailed) 
 
in terms of their vulnerability toward interruptions. 
Table 5. Number of drawing turn during drawing 
(see Case 3 in figure 3) 
Total of X drawing turns 149
Overlapping X drawing turns 19
Ratio 12.8%
5 Discussions 
6 Conclusion 
Generally, when it is one?s turn to speak, the other 
party is less likely to cut into the turn. They try to 
present their information clearly to avoid 
overlapping. In other words, the present speaker?s 
speech turn is  some form of resistance to the other 
party?s interruptions. On the other hand, during 
one?s drawing turn, even though part of the 
drawing period is filled with silence and during 
that silence the other party could easily utter ?init? 
utterances, the data on periods silence showed that 
longer silent periods were allowed during drawing. 
We suppose that drawing turns has some form of 
resistance to keep the other party from interrupting 
their to speak. 
In this study, the percentage of overlapped X?s 
speech turns was 11.1%. That is to say, X?s speech 
turns had 89.9% of resistance to keep turns. In the 
same way, X?s drawing turns was interrupted by 
Y?s ?init? or ?ack init? utterances at 12.8%, which 
showed almost the same extent of resistance, 
87.2%. Thus, our comparison suggests that the 
turn-keeping resistance of X?s drawing turns is 
almost equivalent to that of X?s speech turns. 
The equivalence of the extent of resistance of 
speech and drawing turns also suggests that silence 
during drawing tended to be longer not because the 
other party could not speak while drawing but 
because he/she did not want to speak while 
drawing. In other words, drawing could make an 
excuse for not speaking while preserving the 
smoothness of turn-taking interactions. 
Finally, within the data we analyzed, all the 
cases in which a speech turn overlaps a preceding 
drawing turns were accompanied by co-temporal 
speech by the drawer.  This is schematically shown 
in Case 3 of Figure 3.  In more than half of these 
cases, the following speech makes an overlaps to 
the preceding speech as well as to the drawing 
(Case 4 in Figure 4).  In contrast, although 
theoretically possible, we did not find any cases in 
which bare drawing turns, e.g., without 
accompanying speech by the drawer, were 
interrupted by the following speech turns (Case 5 
in Figure 4).   
 
Case4: Overlap of both X?s speech and X?s
drawing turn with Y?s speech turn 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 4. Drawing turns interrupted by a speech 
turn 
We conducted a preliminary study on the drawing 
turns in graphical communication.  Based on 
collected dialogue data that involve graphical as 
well as spoken interactions, we conducted an 
analysis of both speech and drawing turns.  We 
found that drawing turns have almost the same 
turn-keeping effect as speech turns.  Further 
investigations are necessary on both semantic and 
grounding contributions of drawings to develop 
fuller account of graphical interactions. 
 
References 
Clark, H. H. 1996. Using language. Cambridge: 
Cambridge University Press. 
Sack, H., Schegloff, E. A. & Jefferson, G. 1974. 
A simplest systematics for the organization of  
turn-taking in conversation. Language, 50, 
696-735. 
Traum, D. R. 1994. A Computational Theory of 
Grounding in Natural Language Conversation. 
Technical Report 545. University of Rochester. 
 
X:init
X-DU
X:cont Y:ack
X-DU 
X:init 
X Speech Turn
X:cont 
X Drawing Turn
Y-DU 
Y:init X:ack 
Y Speech Turn
Case5: Overlapping independent X?s drawing turn
with Y?s speech turn 
X Drawing Turn
Y-DU 
Y:init X:ack 
Y Speech Turn
Proceedings of the Workshop on Embodied Language Processing, pages 9?16,
Prague, Czech Republic, June 28, 2007. c?2007 Association for Computational Linguistics
Aiduti in Japanese Multi-party Design Conversations
Yasuhiro Katagiri
Future University - Hakodate
116-2 Kameda-Nakano Hakodate Hokkaido, Japan
katagiri@fun.ac.jp
Abstract
Japanese backchannel utterances, aizuti, in
a multi-party design conversation were ex-
amined, and aizuti functions were analyzed
in comparison with its functions in two-
party dialogues. In addition to the two
major functions, signaling acknowledgment
and turn-management, it was argued that
aizuti in multi-party conversations are in-
volved in joint construction of design plans
through management of the floor structure,
and display of participants? readiness to en-
gage in collaborative elaboration of jointly
constructed proposals.
1 Introduction
Backchannel utterances are one of the representa-
tive phenomena characterizing conversational inter-
actions. We can find backchannel utterances in ev-
ery natural conversation in every culture. Differ-
ent languages have different repertoire of expres-
sions that work as backchannels. In terms of what
functions they serve in conversations, it has widely
been acknowledged that backchannels, by convey-
ing the hearer feedback to the speaker, serve to con-
tribute to informational coordination between con-
versational participants, through conversational flow
management in terms of both common grounding
and smooth turn-taking. It has also been acknowl-
edged, perhaps less explicitly, that backchannels
serve to contribute to affective coordination by pro-
moting rapport between conversational participants.
It is still unclear how these two contrasting views on
backchannels can be integrated. How are the infor-
mational and the affective coordination functions of
backchannels inter-related? What factors determine
relative salience of these two functions in certain us-
age of backchannels? Are there any categories of
conversational interactions that promote one or the
other functions? Are there cultural differences in
backchannel usages?
We focus, in this paper, on the use of Japanese
backchannels, aiduti, in multi-party conversations.
Based on the analysis of how aiduti utterances are
employed in experimentally captured multi-party
design conversation data, we argue that aiduti ut-
terances in Japanese have, on top of the informa-
tional coordination functions of common-grounding
and turn-management, the function of expressing the
readiness, a positive attitude, on the part of a partic-
ipant to engage in the joint construction of an ongo-
ing proposal currently under discussion, which then
leads to affective coordination.
2 Backchannels in Dialogues
Backchannel utterances were conceived initially
in two-party dialogues with one speaker and one
hearer. Schegloff (1982) picked up hearer?s short
utterances such as ?uh, huh? produced in response
to the speaker?s main utterances, and character-
ized them as backchannels, whose functions are to
convey backward messages from the hearer to the
speaker indicating that the hearer is attending to, lis-
tening to, understanding, and expecting to continue
the production of the speaker?s main message.
Heritage (2006) provides a broader conception of
backchannels and lists the following four functions
9
Figure 1: Meeting archiver equipment MARC used
in data collection
for backchannel utterances.
? Provide Acknowledgments to prior locutions
by the speaker
? Projection of further talk in turn taking
? Recipient epistemic states triggered by the
speaker?s message
? Recipient affiliative attitude, how the recipient
is aligned with speaker?s message
Maynard (1986) compared Japanese and Amer-
ican dialogues, and observed that Japanese dia-
logues have almost twice as much aizuti as Ameri-
can backchannel utterances. This observation sug-
gests that significance of backchannels and their
functions in conversational interactions may depend
on social groups, types of activities and other social
or task related parameters.
3 Multi-party Conversation
3.1 Varieties of multi-party conversations
Wewill focus on aiduti utterances in Japanese multi-
party design conversations. In order to locate the
type of activity we?ve been working on within the
broad range of interaction activities collectively cat-
egorized as multi-party conversations, we first try to
list up potential parameters that might influence the
Figure 2: Setting for multi-party design conversation
capture
structure and organization of conversational interac-
tions.
Number of participants
We call a conversation between more than two
people a multi-party conversation. A conversation
between three people and a conversation between 10
people are not the same in their conversational orga-
nization. It has been observed (Fay et al, 2000) that
conversations with a small number of participants
tend to be homogeneous that contain a number of
equal status pairwise interactions, whereas conver-
sations with a large number of participants tend to
be more hierarchical with a central control person
working as a chairperson.
Types of activities
Conversational interactions are often embedded in
larger activities, and the type of embedding activities
makes a difference in the organization of conversa-
tions.
(a) Purpose
One-way information transfer in lectures and
joint problem solving in a group of people have
both fixed but different types of goals. When
people chat for socialization, having a conver-
sation itself becomes its own purpose. These
different types of goals could produce different
organizational structures in conversations.
(b) Rigidity of purpose
Even within joint problem solving activities,
10
Sp Utterance Sp Utterance
D: ?? B: ?? (un)
D: ?????????????????? C: ???????????? (D ?)???
D: ????????:?????????? C: (laugh)
(I often think that even when you keep using the
same mobile carrier, if you could have one more
mail address,)
C: ?????????????????????
E: ?????? (un-un-un) (when I try to correspond by mail with elderly, eh,
with my parents)
D: ?????? E: ???? (un-un)
(it would be nice) B: ?? (un)
D: ????????????? E: ?? (un)
(with PC, any number of addresses) C: ????:???????:??
F: ???? (un-un) (parents cannot type)
E: ??:????????? (aa-aa, hai-hai-hai-hai) B: ?? (un)
D: ?????????? E: ?? (un)
(you can have) C: ????????
F: (D ??) (but, they can talk on the phone)
B: ?? (un): B: ?? (un)
C: ?? (un) E: ???? (un-un)
D: ??????? C: ????:?????????????????
E: ?? (un) (but, when you?d rather want to use mails, if possi-
ble)
D: ?????????? E: ?????? (un-un-un)
(I would definitely want one) B: ?? (un)
B: ?? (un): C: ????
E: (D ???) F: ?? (un)
E: ??????????????? C: ???:
(You mean, multiple mail addresses, right?) C: ?????:
D: ?? (hai) (how about, with speech recognition)
C: ?? (un): E: ?? (aa)?????:
C: ????????? (that?s good)
(for elderly people) B: ?? (un):
E: ???? (un-un) C: ?????
C: ??: (convert speech into text)
Figure 3: Aiduti in design conversation
we can conceive of different degree of rigid-
ity of problem goals conversational participants
are working on. In one extreme lies a pursuit
of a fixed goal such as mathematical problem
solving, in which a problem with a clearly de-
termined answer is given to the group. In other
extreme lies a problem solving under a loosely
stated goal such as floor planning of an apart-
ment for the group, in which the only require-
ment is to reach an agreement, and factors to
consider must be made explicit in the course of
conversation. The design conversation we?ve
looked at belong to the latter category.
(c) Reality
Every experimental data collection has to face
this problem. Whether or not and how much
the outcome of the conversation has real import
in participants? life makes a big difference in
conversational organization.
(d) Use of objects
Use of physical objects, particularly informa-
tional artifact such as whiteboard and projec-
tors, changes the use pattern of multi-modal
signals: gaze, gestures and body postures, and
needs to be taken into account in experimental
data collection.
Characteristics of participants
(a) Participant properties
Differences in capabilities such as in knowl-
edge and in expertise, and dispositional proper-
ties, such as preferences, beliefs, and personal-
ities of participants greatly contribute to shape
the interaction.
(b) Participant roles
11
Start -End Sp Utterances
243.1950-243.7450 F: are-wo
- (that)
244.2075-246.1200 F: tatoeba keitai-wo nakusita toki-no
sono
- (when you lose your mobile phone)
246.6300-247.9800 F: timei-tekina doai-wa
- (how fatal it will be)
248.2725-248.8850 F: nn daibu
- (big)
248.7550-249.7350 B: un:
249.3000-250.0200 E: un:
249.5150-249.8225 D: un:
250.1250-254.8225 F: un:are ikko otosityattara
ironna houmen-no raihu-rain-
ga soredakede tataretyautte iunoga
atte
- (if you lose one, your life line will
be cut out in a lot of ways)
Figure 4: Aiduti overlap
Conversation setting often dictates certain role
assignment to each participant, which in turn
determines the shape of interactions that takes
place between people under those participant
roles. Instructor and follower in instruction giv-
ing tasks, and clerk and customer in commer-
cial transactions are typical examples. The role
of chairperson also is significant in determining
the structure of conversations.
(c) Participant relationships
Age and social status often provide a fixed base
for dominance relationship among conversa-
tional participants. Affiliative familiarity be-
tween participants are less fixed but still stable
relationships. Sharing of opinions is temporary
and can change quite quickly during the course
of a conversation.
3.2 Multi-party design conversation
We have been collecting data on multi-party design
conversation in Japanese. Multi-party design con-
versation is a type of joint problem solving conversa-
tion, in which participants engage in a discussion to
come up with an agreement on the final design plan.
The design goal, however, is only partially specified,
and participants need to jointly decide on evaluative
criteria for the design goal during the course of the
discussion.
Start -End Sp Utterance
781.5050-781.6100 C: ?
781.7750-782.7050 C: ????????????
- (one more thing)
782.6300-782.8900 E: ??
782.9550-784.4925 C: ?????????????
- (related to communication)
784.6050-784.7750 E: ??
784.6500-786.6450 C: ???????????????
???:
- (just a thought)
786.1925-786.6950 E: ????
787.3475-788.4875 C: ??:??????:
788.6125-789.9975 C: ????????????:
790.5375-790.6175 C: (W ????)
790.7975-792.2575 C: ????????????:
792.6125-795.7250 C: ???????????????
???????
- (when you exchange mails, particu-
larly on mobile phones, people ex-
pect quicker responses)
. . .
830.4175-831.7750 C: ?????????????
831.7125-832.0400 B: ?:
832.1475-833.1475 E: ?:
832.3675-833.3750 C: ?????:????
832.3875-834.4525 B: ???????????
- (drive mode)
834.4450-835.5700 E: ?:?
834.6325-834.9900 C: ??
834.8000-836.3450 B: ??????????
- (on the mobile phone)
836.0075-837.5475 C: ???????????????
- (Ah, I know drive mode)
837.4525-842.8250 B: (W ???????) ?????
???????????????
???????????
- (same as, when you make a call, it
says it?s on drive now)
- . . .
844.8375-846.2475 B: ???????????
- (you get those responses)
845.1450-846.6375 E: ?????????????
- (it seems rather simple to realize)
846.2325-846.3950 C: ??
846.5650-846.7850 C: ??
846.5750-848.2600 D: ???????????????
????????
- (you know Yahoo messenger?)
847.5825-847.8575 B: ?
847.9050-848.5125 E: ??:
848.1400-848.6200 B: ??:
848.4450-849.5650 D: ???????:??
849.9125-851.3250 D: ??????????????
- (you can see the situation of your
correspondent)
851.3600-851.8900 E: ??:
Figure 5: Floor structure
12
Speaker Utterance Aiduti
A 158 3
B 426 179
C 420 125
D 346 138
E 612 343
F 206 69
Total 2,168 857
Table 1: Number of utterances and aiduti produced
in multi-party design conversation
Japanese form sound translation
?? hai (yes)
?? un (yeah)
?? aa (ah)
?? ee (correct)
?? sou (I agree)
Table 2: Linguistic forms of aiduti
The condition of our data collection was as fol-
lows:
Number of participants: six for each session
Arrangement: face-to-face conversation
Task: Proposal for a new mobile phone business
Role: No pre-determined role was imposed
In order to minimize the intimidating effect of
a huge recording setup, we used a compact meet-
ing archiver equipment, MARC, currently under de-
velopment in AIST Japan (Asano and Ogata, 2006)
shown in Fig. 1. MARC is equipped with an ar-
ray of 6 cameras together with an array of 8 mi-
crophones, and it captures panoramic video with
up to 15 frames/sec. and speaker-separated speech
streams with 16kHz sampling rate. A meeting cap-
ture scene is shown in Fig. 2.
The data we examine in this paper consists of one
30 minutes conversation conducted by 5 males and
1 female. Even though we did not assign any roles, a
chairperson and a clerk were spontaneously elected
by the participants at the beginning of the session.
4 Aizuti in multi-party design conversation
4.1 Aiduti types and amounts
We first looked at how frequent people produce
aiduti in the conversation. Table 1 shows the number
of utterances and aiduti utterances for each of the six
speakers, both in terms of the number of inter-pausal
units (IPUs). Table 2 indicates expressions identified
as aiduti utterances. Positive responses to questions
and requests are not included in aiduti, even if they
share the surface forms of Table 2. Reduplicated
forms of each of the aiduti expressions in Table 2
are also frequently observed, and they were counted
as one aiduti occurences.
We can see that a sizable portion of utterances,
about 30% to 40%, were actually aiduti utterances
in our data. An example excerpt demonstrating the
abundance of aiduti is shown in Fig. 3, where aiduti
utterances are marked by bold characters.
4.2 Conversation flow management
Overlapping aiduti
One reason why multi-party conversation con-
tains a lot of aiduti is that there are more hearers,
potential backchannel producers. Fig. 4 shows an
example in which three hearers B,E, and D produced
aiduti almost simultaneously to the speaker F?s ut-
terance. The fact that these three aiduti were over-
lapping shows that they are independently directed
to the speaker F?s preceding utterance. This type of
aiduti response is expected to increase in numbers as
the number of conversation participants increases.
Aiduti for turn-holding
In the same example in Fig. 4, the speaker F pro-
duced aiduti ?un? after all aiduti utterances by hear-
ers B, E, and D, and immediately before he con-
tinued his turn. This type of speaker aiduti can be
taken to serve the turn-holding function. It gives an
acknowledgment to all the acknowledgments from
hearers collectively and signals that the speaker is
going on producing his own message.
Aiduti for floor transition
A relatively clear structure was observed in the
conversation we analyzed. The conversation con-
sisted of a sequence of idea proposals produced by
13
Aiduti?Floor Num
Aiduti speaker becomes the next floor main speaker 53
non-Aiduti speaker becomes next floor main speaker 17
Total 70
Table 3: Aiduti and floor transition
different speakers. We identified a stretch of conver-
sation as a floor in which one main speaker makes
a proposal on his or her ideas. As long as the spe-
cific proposal is being discussed as the conversation
topic, other participants may contribute clarification
or elaboration utterances within the same floor. An
example of a sequence of floors is shown in Fig. 5.
C first talks about the difference in people?s expected
response between mobile mails and PC mails in the
first floor. B then brings about in the second floor
a suggestion on some functionality similar to drive
mode which indicates to the original sender that the
recipient is not available at the moment. D in the
third floor follows on by mentioning Yahoo messen-
ger. We extracted 71 floors total from the 30 minute
conversation data.
Table 3 indicates the relationship between the pro-
duction of aiduti in one floor and the claiming of
the main speaker-hood in the next floor. The table
shows that many of the main speaker of a floor had
produced aiduti as a non-main speaker in the preced-
ing floor. This suggests that aiduti utterances from
non-main speakers indicate their readiness to make
a positive contribution to the joint task, by taking the
next floor and by contributing a proposal for the task
when they find a suitable opportunity.
4.3 Collaborative elaboration of proposals
When we take a closer look into floors, we find pos-
itive collaborative behaviors from non-main speaker
participants. Typical behaviors of non-main speaker
participants of a floor include giving aiduti, pro-
viding (positive) evaluations to the idea proposed,
and inserting clarification questions. On top of
these behaviors, it was often observed in a floor that
non-main speaker participants try to make positive
contributions to the idea currently on the table, by
adding new elements of ideas or providing concrete
ideas to part of the proposal that heretofore remained
vague at the time. We call these behaviors on the
Start -End Sp Utterance
505.2500-506.4500 D: ??????????
- (as an image)
505.4375-505.7225 E: ??
506.8000-508.1450 D: (W ???)???????????
?????
- (this says three years from now)
508.6675-509.3500 D: ???
509.4875-509.9375 E: ??
509.5300-510.1400 D: ????
510.3875-510.9250 E: ????
510.4875-511.1200 B: ?:
510.5100-510.6650 D: ?
510.9125-511.5975 D: ????????
511.8375-512.2125 E: ??
512.0075-512.1975 B: ??
512.1125-512.8725 D: ???????
512.2650-512.8800 C: ?:
512.6075-513.2850 B: ?:
513.0050-514.0925 D: ????:?????????
- (it would be really convenient to com-
bine PC with Skype)
513.3875-514.2525 E: ??
513.6325-514.1650 C: ?????
- (good)
514.3150-515.2350 C: ????:?
- (with Skype)
514.3725-515.5525 E: ??
514.5375-515.3200 B: ?????:
- (good)
515.8950-516.0875 C: ??
516.2775-516.4825 C: ??
516.8400-517.8825 C: ???????
- (you can call free)
517.8100-518.8125 B: ?? (W ????)
- (free call)
518.2200-520.8650 D: ????????????????
??????
- (frequency assignment problem will
somehow be solved)
518.2500-519.0475 C: ?
519.0500-519.9925 B: ?
519.5150-520.6675 C: ?
520.4400-521.6825 E: ??????????
520.6800-521.3675 B: ?
521.5100-522.3050 B: ??
521.8250-522.7875 E: ??
521.9025-522.9375 D: ????????
- (if its available worldwide)
522.7975-523.0550 C: ??
Figure 6: Collaborative elaboration: Success
14
Start -End Sp Utterance
543.1750-544.3800 C: ????:????
544.6725-548.2050 C: ? (W ????????)(W ??
??????)??????? (D
?)????????????:?
????????????:
- (this is closer to Google service)
546.8350-547.5825 E: ??
548.5725-551.8350 C: ?????????????:??
?????????????:
- (if we place data in the network
rather than on the terminal)
551.4175-552.3400 E: ??
551.7175-552.1175 B: ??
552.4425-556.9450 C: ?????????:????:?
???????????????
??????????:
- (even when we lose your terminal,
if you setup so that other people can
not use, not access)
553.7150-554.2625 E: ????
554.3650-554.6675 D: ??
556.2425-557.2450 E: ????
556.7250-557.1150 B: ??
557.5875-558.4275 C: ????????
- (nobody can get data from there)
. . .
567.2850-567.7400 B: ?:
567.6250-567.9725 E: ??
568.0075-568.8775 B: ???????
569.0300-569.7200 B: ??????
- (there might be backup)
569.7725-570.4025 E: ??
570.1050-570.7775 C: ??? (W ?????????)
- (well, backup)
570.2625-570.6350 D: ??
571.1225-572.4825 B: ???????????????
- (maybe backup is not such a good
idea)
571.3425-573.5925 C: ??????????:?????
???????:
- (backup leaves data on the terminal)
572.6925-573.2025 B: ???
- (right)
Figure 7: Collaborative elaboration: failure
Condition Num
Floor with aiduti 67
Floor with no aiduti 4
Floor with Collab-Elab. 29
Floor with no Collab-Elab. 42
Aiduti speaker initiated Collab-Elab. 25
non-Aiduti speaker initiated Collab-Elab. 4
Table 4: Aiduti in Collaborative elaboration
part of non-main speaker participants ?collaborative
elaboration.? Collaborative elaboration can be a suc-
cess or a failure. Figures 6 and 7 show two con-
trasting examples. In the example in Fig. 6, non-
main speaker participants C and B successfully con-
tribute to the idea proposal by the main speaker D on
combining PC and Skype functionalities, by explic-
itly pointing out the concrete merit, e.g., free phone
call, as a support of the proposal. In the example in
Fig. 7, on the other hand, a non-main speaker par-
ticipant B first tried to make a contribution, the idea
of local data backup, to the proposal produced by
the main speaker C, storage of data in the network,
but gave up after a non-positive response from C and
retracted his additional proposal.
Table 4 shows the relationship between collab-
orative evaluation and aiduti utterances in a floor.
Aiduti utterances were observed in almost every
floor. Collaborative elaboration is also rather fre-
quent. It takes place in about 40% of all floors. Fi-
nally, the table shows that participants who perform
collaborative evaluation in a floor are likely to pro-
duce aiduti utterances in the same floor. This sug-
gests, again, that aiduti utterances from non-main
speaker participants of a floor indicate their readi-
ness to make a positive contribution to the joint task,
by improving on the proposal currently being dis-
cussed.
5 Discussions
Frequency of aiduti utterances
We observed that multi-party conversations con-
tain a high rate (30?40%) of aiduti utterances. a
great number of aiduti utterances were produced by
the chairperson among all the participants. Saft
(2006), based on the analysis of Japanese TV dis-
cussion programs, pointed out that chairperson pro-
15
duces a large portion of aiduti among all the discus-
sion participants. These findings appear to confirm
the idea that aiduti utterances have functions to man-
age the flow of conversations, and chairpersons ex-
ploit these functions in discussion sessions. But, ex-
act conversation flow management function of aiduti
may not be unique. According to Saft (2006), the
chairperson in the particular TV discussion program
uses aiduti to claim their addressee-hood in order to
prevent the discussion from free-floating and out of
control. In our design conversation data, it appears
that the chairperson frequently inserts aiduti in or-
der to encourage other participants to engage in the
discussion and to make the session more lively.
Floor structure
It may not always be a good strategy for ev-
erybody to produce aiduti as acknowledgment in a
multi-party conversation, since with a lot of hearers
it can be a nuisance for the speaker to get too many
aiduti in every possible grounding point. It follows
that the fact that a certain participant produces aiduti
at a certain timing in a multi-party conversation can
have significance other than the grounding of the
message just produced. It is interesting to note that
even though at the level of turn-taking, an aiduti ut-
terance works as a continuer, a turn-yielding signal,
at the level of floor, aiduti utterances seem to indi-
cate positive involvement attitude of the participant
toward the joint problem solving activity.
Collaborative elaboration
We observed a number of instances of joint con-
struction of proposals through collaborative elabo-
ration in our design conversation data. It was also
observed that in most of the cases of collaborative
elaboration, aiduti utterances were accompanied by
participants engaging in collaborative elaboration.
These facts seem to imply that aiduti utterances both
signal and produce among conversation participants
an affiliative awareness toward joint construction of
the proposal for the problem at hand, through the
exchange of readiness signal, among all the group
members, toward making positive contributions to
the ongoing joint problem solving activity. We be-
lieve that these contribution readiness and affilia-
tive awareness are the basis of affective functions of
aiduti in Japanese conversations.
6 Conclusions
An analysis of aiduti utterances, Japanese backchan-
nels, in a Japanese multi-party design conversation
was conducted. It was argued, based on the analysis,
that, in addition to the two major functions, signal-
ing acknowledgment and turn-management, aiduti
utterances in multi-party conversations are involved
in joint construction of design plans through man-
agement of the floor structure, and display of par-
ticipants? readiness to engage in collaborative elab-
oration of jointly constructed proposals. It was also
suggested that these additional functions eventually
lead to affective functions of aiduti.
Acknowledgment
The work reported in this paper was partially sup-
ported by Japan Society for the Promotion of
Science Grants-in-aid for Scientific Research (B)
18300052.
References
Futoshi Asano and Jun Ogata. 2006. Detection and sepa-
ration of speech events in meeting recordings. In Proc.
Interspeech, pages 2586?2589.
Nicholas Fay, Simon Garrod, and Jean Carletta. 2000.
Group discussion as interactive dialogue or as serial
monologue: The influence of group size. Psychologi-
cal Science, 11(6):487?492.
John Heritage. 2006. An overview of English backchan-
nels. International workshop on cross-cultural and
culture-specific aspects of conversational backchan-
nels and feedback, December.
Senko K. Maynard. 1986. On back-channel behavior in
Japanese and English casual conversation. Linguistics,
24:1079?1108.
Scott L. Saft. 2006. The moderator in control: Use
of names, the particle ne, and response tokens on a
Japanese discussion TV program. Research on Lan-
guage and Social Interaction, 39(2):155?193.
Emanuel A. Schegloff. 1982. Discourse as interactional
achievement: some uses of?uh huh?and other things
that come between sentences. In Deborah Tannen, ed-
itor, Analyzing Discourse, Text, and Talk, pages 71?93.
Georgetown University Press.
16
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 100?103,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Implicit Proposal Filtering
in Multi-Party Consensus-Building Conversations
Yasuhiro Katagiri
Future University ? Hakodate
katagiri@fun.ac.jp
Yosuke Matsusaka
National Institute of Advanced
Industrial Science and Technology
yosuke.matsusaka@aist.go.jp
Yasuharu Den
Chiba University
den@cogsci.l.chiba-u.ac.jp
Mika Enomoto
Tokyo University of Technology
menomoto@media.teu.ac.jp
Masato Ishizaki
The University of Tokyo
ishizaki@iii.u-tokyo.ac.jp
Katsuya Takanashi
Kyoto University
takanasi@ar.media.kyoto-u.ac.jp
Abstract
An attempt was made to statistically estimate
proposals which survived the discussion to
be incorporated in the final agreement in an
instance of a Japanese design conversation.
Low level speech and vision features of hearer
behaviors corresponding to aiduti, noddings
and gaze were found to be a positive pre-
dictor of survival. The result suggests that
non-linguistic hearer responses work as im-
plicit proposal filters in consensus building,
and could provide promising candidate fea-
tures for the purpose of recognition and sum-
marization of meeting events.
1 Introduction
Non-verbal signals, such as gaze, head nods, fa-
cial expressions and bodily gestures, play signif-
icant roles in the conversation organization func-
tions. Several projects have been collecting multi-
modal conversation data (Carletta et al, 2006) for
multi-party dialogues in order to develop techniques
for meeting event recognitions from non-verbal as
well as verbal signals. We investigate, in this paper,
hearer response functions in multi-party consensus-
building conversations. We focus particularly on the
evaluative aspect of verbal and non-verbal hearer re-
sponses. During the course of a consensus-building
discussion meeting, a series of proposals are put
on the table, examined, evaluated and accepted or
rejected. The examinations of proposals can take
the form of explicit verbal exchanges, but they can
also be implicit through accumulations of hearer
responses. Hearers would express, mostly uncon-
sciously for non-verbal signals, their interest and
positive appraisals toward a proposal when it is
introduced and is being discussed, and that these
hearer responses would collectively contribute to the
determination of final consensus making. The ques-
tion we address is whether and in what degree it is
possible and effective to filter proposals and estimate
agreement by using verbal and non-verbal hearer re-
sponses in consensus-building discussion meetings.
2 Multi-Party Design Conversation Data
2.1 Data collection
We chose multi-party design conversations for the
domain of our investigation. Different from a fixed
problem solving task with a ?correct? solution, par-
ticipants are given partially specified design goals
and engage in a discussion to come up with an agree-
ment on the final design plan. The condition of our
data collection was as follows:
Number of participants: six for each session
Arrangement: face-to-face conversation
Task: Proposal for a new mobile phone business
Role: No pre-determined role was imposed
A compact meeting archiver equipment, AIST-
MARC (Asano and Ogata, 2006), which can cap-
ture panoramic video and speaker-separated speech
streams, was used to record conversations (Fig. 1).
The data we examined consist of one 30 minutes
conversation conducted by 5 males and 1 female.
Even though we did not assign any roles, a chairper-
son and a clerk were spontaneously elected by the
participants at the beginning of the session.
100
Figure 1: AIST-MARC and a recording scene
2.2 Data Annotation
2.2.1 Clause units
In order to provide a clause level segmentation
of a multi-channel speech stream, we extended the
notion of ?clause units (CUs)?, originally developed
for analyzing spoken monologues in the Corpus of
Spontaneous Japanese (Takanashi et al, 2003), to
include reactive tokens (Clancy et al, 1996) and
other responses in spoken conversations. Two of the
authors who worked on the Corpus of Spontaneous
Japanese independently worked on the data and re-
solved the differences, which created 1403 CUs con-
sisting of 469 complete utterances, 857 reactive to-
kens, and 77 incomplete or fragmental utterances.
2.2.2 Proposal units
We developed a simple classification scheme of
discourse segments for multi-party consensus build-
ing conversations based on the idea of ?interaction
process analysis? (Bales, 1950).
Proposal: Presentation of new ideas and their eval-
uation. Substructure are often realized through
elaboration and clarification.
Summary: Sum up multiple proposals possibly
with their assessment
Orientation: Lay out a topic to be discussed and
signal a transition of conversation phases, initi-
ated mostly by the facilitator of the discussion
Miscellaneous: Other categories including opening
and closing segments
The connectivity between clause units, the content
of the discussion, interactional roles, relationship
with adjacent segments and discourse markers were
considered in the identification of proposal units.
Two of the authors, one worked on the Corpus of
Spontaneous Japanese and the other worked for the
Figure 2: Image processing algorithm
project of standardization of discourse tagging, in-
dependently worked on the data and resolved the
differences, which resulted in 19 proposals, 8 sum-
maries, 19 orientations and 2 miscellaneouses.
2.3 Core clause units and survived proposal
units
Core clause units (CUs) were selected, out of all the
clause units, based on whether the CUs have sub-
stantial content as a proposal. A CU was judged
as a core CU, when the annotator would find it ap-
propriate to express, upon hearing the CU, either an
approval or a disapproval to its content if she were
in the position of a participant of the conversation.
Three of the authors worked on the text data exclud-
ing the reactive tokens, and the final selection was
settled by majority decision. 35 core CUs were se-
lected from 235 CUs in the total of 19 proposal PUs.
Cohen?s kappa agreement rate was 0.894.
Survived proposal units (PUs) were similarly se-
lected, out of all the proposal units, based on
whether the PUs were incorporated in the final
agreement among all the participants. 9 survived
PUs were selected from 19 proposal PUs.
3 Feature Extraction of Hearer?s Behavior
For each clause unit (CU), verbal and non-verbal
features concerning hearer?s behavior were ex-
tracted from the audio and the video data.
3.1 Non-Verbal Features
We focused on nodding and gaze, which were ap-
proximated by vertical and horizontal head move-
ments of participants.
An image processing algorithm (Figure 2) was ap-
plied to estimate head directions and motions (Mat-
susaka, 2005). Figure 3 shows a sample scene and
the results of applying head direction estimation al-
gorithm.
101
Figure 3: Sample scene with image processing results.
The circles represent detected face areas, and the lines in
the circles represent head directions.
For each CU, the vertical and horizontal compo-
nents of head movements of 5 hearers were calcu-
lated for two regions, the region inside the CU and
the 1-sec region immediately after the CU. For each
of the two regions, the mean and the peak values and
the relative location, in the region, of the peak were
computed. These 12 non-verbal features were used
for the statistical modeling.
3.2 Verbal Features
Verbal features were extracted from the audio data.
For each CU, power values of 5 hearers were ex-
tracted for two regions, ?within? and ?after? CU, and
for each of the two regions, the mean and the peak
values and the relative location, in the region, of
the peak were computed. In addition to these ver-
bal features, we also used aiduti features of reactive
tokens (RTs). The percentage of the total duration
of RTs, the total number of RTs, and the number of
participants who produced an RT were computed in
?within? and ?after? regions for each of the CUs. A
total of 12 CU verbal features were used for the sta-
tistical modeling.
4 Experiments
4.1 Overview of the Algorithm
Statistical modeling was employed to see if it is pos-
sible to identify the proposal units (PUs) that are sur-
vived in the participants? final consensus. To this
end, we, first, find the dominant clause unit (CU) in
each PU, and, then, based on the verbal and non-
verbal features of these CUs, we classify PUs into
?survived? and ?non-survived.?
Table 1: The optimal model for finding core-CUs
Estimate
(Intercept) ?1.72
within/speech power/mean ?11.54
after/vertical motion/peak loc. ?4.25
after/speech power/mean 3.91
after/aiduti/percent 3.02
Table 2: Confusion matrix of core-CU prediction experi-
ment (precision = 0.50, recall = 0.086)
Predicted
Observed Non-core Core
Non-core 431 3
Core 32 3
4.2 Finding Dominant CUs
A logistic regression model was used to model the
coreness of CUs. A total of 24 verbal and non-verbal
features were used as explanatory variables. Since
the number of non-core CUs was much larger than
that of core CUs, down-sampling of negative in-
stances was performed. To obtain a reliable estima-
tion, a sort of Monte Carlo simulation was adopted.
A model selection by using AIC was applied for
the 35 core CUs and another 35 non-core CUs that
were re-sampled from among the set of 434 com-
plete and non-core CUs. This process was repeated
100 times, and the features frequently selected in
this simulation were used to construct the optimal
model. Table 1 shows the estimated coefficient for
the optimal model, and Table 2 shows the accu-
racy based on a leave-1-out cross validation. The
dominant CU in each PU was identified as the CU
600 700 800 900 1000 1100 1200
0.0
0.2
0.4
0.6
0.8
1.0
time[sec]
core
?CU 
likelih
ood
O P S S O O P S S S P P O S P OO0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0(0) (0) (1) (0) (0)(0)(0) (0) (0) (0) (0) (0) (1) (0) (0) (0) (0)(0)
Figure 4: The predicted coreness of CUs. Dominant CUs
were defined to be CUs with the highest coreness in each
of the PUs. Black and white dots are CUs labeled as core
and non-core.
102
Table 3: The optimal model for finding survived-PUs
Estimate
within/vertical motion/peak val. 3.96
within/speech power/mean ?27.76
after/speech power/peak val. 1.49
Table 4: Result of the survived-PU prediction (precision
= 0.83, recall = 0.44)
Predicted
Observed Non-survived Survived
Non-survived 37 1
Survived 4 5
with the highest predicted value in that PU. Figure 4
shows the predicted values for coreness.
4.3 Finding Survived PUs
The verbal and non-verbal features of the dominant
CUs of each of the PUs were used for the modeling
of the survived-PU prediction. Discriminant analy-
sis was utilized and a model selection was applied
for the 47 PUs. Table 3 shows the estimated coeffi-
cient for the optimal model, and Table 4 shows the
accuracy based on a leave-1-out cross validation.
5 Discussions
The results of our estimation experiments indicate
that the final agreement outcome of the discus-
sion can be approximately estimated at the proposal
level. Though it may not be easy to identify actual
utterances contributing to the agreement (core-CUs),
the dominant CUs in PUs were found to be effective
in the identification of survived-PUs. The prediction
accuracy of survived-PUs was about 89%, with the
chance level of 69%, whereas that of core-CUs was
about 92%, with the chance level of 86%.
In terms of hearer response features, intensity
of verbal responses (within/speech power/mean, af-
ter/speech power/mean), and immediate nodding re-
sponses (after/vertical motion/peak loc.) were the
most common contributing features in core-CU es-
timation. In contrast, occurrence of a strong aiduti
immediately after, rather than within, the core-
CU (after/speech power/peak val.), and a strong
nodding within the core-CU (within/vertical mo-
tion/peak val.) appear to be signaling support from
hearers to the proposal. It should be noted that iden-
tification of target hearer behaviors must be vali-
dated against manual annotations before these gen-
eralizations are established. Nevertheless, the re-
sults are mostly coherent with our intuitions on the
workings of hearer responses in conversations.
6 Conclusions
We have shown that approximate identification of
the proposal units incorporated into the final agree-
ment can be obtained through the use of statistical
pattern recognition techniques on low level speech
and vision features of hearer behaviors. The result
provides a support for the idea that hearer responses
convey information on hearers? affective and evalu-
ative attitudes toward conversation topics, which ef-
fectively functions as implicit filters for the propos-
als in the consensus building process.
Acknowledgments
The work reported in this paper was supported by Japan
Society for the Promotion of Science Grants-in-aid for
Scientific Research (B) 18300052.
References
F. Asano and J. Ogata. 2006. Detection and separation
of speech events in meeting recordings. In Proc. Inter-
speech, pages 2586?2589.
R. F. Bales. 1950. A set of categories for the analysis
of small group interaction. American Sociological Re-
view, 15:257?263.
J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guille-
mot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M.
Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska, I.
McCowan, W. Post, D. Reidsma, and P.Wellner. 2006.
The AMI meeting corpus: A pre-announcement. In
Machine Learning for Multimodal Interaction, pages
28?39.
P. M. Clancy, S. A. Thompson, R. Suzuki, and H. Tao.
1996. The conversational use of reactive tokens in En-
glish, Japanese and Mandarin. Journal of Pragmatics,
26:355?387.
Y. Matsusaka. 2005. Recognition of 3 party conversation
using prosody and gaze. In Proc. Interspeech, pages
1205?1208.
K. Takanashi, T. Maruyama, K. Uchimoto, and H.
Isahara. 2003. Identification of ?sentence? in.
spontaneous Japanese: detection and modification of
clause boundaries. In Proc. ISCA & IEEE Workshop
on Spontaneous Speech Processing and Recognition,
pages 183?186.
103
