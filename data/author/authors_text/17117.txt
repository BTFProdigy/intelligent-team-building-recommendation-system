Proceedings of NAACL-HLT 2013, pages 95?105,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
An Analysis of Frequency- and Memory-Based Processing Costs
Marten van Schijndel
The Ohio State University
vanschm@ling.osu.edu
William Schuler
The Ohio State University
schuler@ling.osu.edu
Abstract
The frequency of words and syntactic con-
structions has been observed to have a sub-
stantial effect on language processing. This
begs the question of what causes certain con-
structions to be more or less frequent. A the-
ory of grounding (Phillips, 2010) would sug-
gest that cognitive limitations might cause lan-
guages to develop frequent constructions in
such a way as to avoid processing costs. This
paper studies how current theories of working
memory fit into theories of language process-
ing and what influence memory limitations
may have over reading times. Measures of
such limitations are evaluated on eye-tracking
data and the results are compared with predic-
tions made by different theories of processing.
1 Introduction
Frequency effects in language have been isolated
and observed in many studies (Trueswell, 1996;
Jurafsky, 1996; Hale, 2001; Demberg and Keller,
2008). These effects are important because they il-
luminate the ontogeny of language (how individual
speakers have acquired language), but they do not
answer questions about the phylogeny of language
(how the language came to its current form).
Phillips (2010) has hypothesized that grammar
rule probabilities may be grounded in memory lim-
itations. Increased delays in processing center-
embedded sentences as the number of embeddings
increases, for example, are often explained in terms
of a complexity cost associated with maintaining in-
complete dependencies in working memory (Gib-
son, 2000; Lewis and Vasishth, 2005). Other stud-
ies have shown a link between processing delays
and the low frequency of center-embedded construc-
tions like object relatives (Hale, 2001), but they
have not explored the source of this low frequency.
A grounding hypothesis would claim that the low
probability of generating such a structure may arise
from an associated memory load. In this account,
while these complexity costs may involve language-
specific concepts such as referent or argument link-
ing, the underlying explanation would be one of
memory limitations (Gibson, 2000) or neural acti-
vation (Lewis and Vasishth, 2005).
This paper seeks to explore the different predic-
tions made by these theories on a broad-coverage
corpus of eye-tracking data (Kennedy et al, 2003).
In addition, the current experiment seeks to isolate
memory effects from frequency effects in the same
task. The results show that memory load measures
are a significant factor even when frequency mea-
sures are residualized out.
The remainder of this paper is organized as fol-
lows: Sections 2 and 3 describe several frequency
and memory measures. Section 4 describes a proba-
bilistic hierarchic sequence model that allows all of
these measures to be directly computed. Section 5
describes how these measures were used to predict
reading time durations on the Dundee eye-tracking
corpus. Sections 6 and 7 present results and discuss.
2 Frequency Measures
2.1 Surprisal
One of the strongest predictors of processing com-
plexity is surprisal (Hale, 2001). It has been shown
in numerous studies to have a strong correlation
with reading time durations in eye-tracking and self-
paced reading studies when calculated with a variety
95
of models (Levy, 2008; Roark et al, 2009; Wu et al,
2010).
Surprisal predicts the integration difficulty that a
word xt at time step t presents given the preceding
context and is calculated as follows:
surprisal(xt) = ? log2
(
?
s?S(x1...xt) P (s)
?
s?S(x1...xt?1) P (s)
)
(1)
where S(x1 . . . xt) is the set of syntactic trees whose
leaves have x1 . . . xt as a prefix.1
In essence, surprisal measures how unexpected
constructions are in a given context. What it does
not provide is an explanation for why certain con-
structions would be less common and thus more sur-
prising.
2.2 Entropy Reduction
Processing difficulty can also be measured in terms
of entropy (Shannon, 1948). A larger entropy over a
random variable corresponds to greater uncertainty
over the observed value it will take. The entropy of
a syntactic derivation over the sequence x1 . . . xt is
calculated as:2
H(x1...t) =
?
s?S(x1...xt)
?P (s) ? log2 P (s) (2)
Reduction in entropy has been found to predict
processing complexity (Hale, 2003; Hale, 2006;
Roark et al, 2009; Wu et al, 2010; Hale, 2011):
?H(x1...t) = max(0, H(x1...t?1)?H(x1...t)) (3)
This measures the change in uncertainty about the
discourse as each new word is processed.
3 Memory Measures
3.1 Dependency Locality
In Dependency Locality Theory (DLT) (Gibson,
2000), complexity arises from intervening referents
introduced between a predicate and its argument.
Under the original formulation of DLT, there is a
1The parser in this study uses a beam. However, given high
parser accuracy, Roark (2001) showed that calculating com-
plexity metrics over a beam should obtain similar results to the
full complexity calculation.
2The incremental formulation used here was first proposed
in Wu et al (2010).
storage cost for each new referent introduced and an
integration cost for each referent intervening in a de-
pendency projection. This is a simplification made
for ease of computation, and subsequent work has
found DLT to be more accurate cross-linguistically
if the intervening elements are structurally defined
rather than defined in terms of referents (Kwon et
al., 2010). That is, simply having a particular ref-
erent intervene in a dependency projection may not
have as great an effect on processing complexity as
the syntactic construction the referent appears in.
Therefore, this work reinterprets the costs of depen-
dency locality to be related to the events of begin-
ning a center embedding (storage) and completing
a center embedding (integration). Note that anti-
locality effects (where longer dependencies are eas-
ier to process) have also been observed in some lan-
guages, and DLT is unable to account for these phe-
nomena (Vasishth and Lewis, 2006).
3.2 ACT-R
Processing complexity has also been attributed to
confusability (Lewis and Vasishth, 2005) as defined
in domain-general cognitive models like ACT-R
(Anderson et al, 2004).
ACT-R is based on theories of neural activation.
Each new word is encoded and stored in working
memory until it is retrieved at a later point for mod-
ification before being re-encoded into the parse. A
newly observed sign (word) associatively activates
any appropriate arguments from working memory,
so multiple similarly appropriate arguments would
slow processing as the parser must choose between
the highly activated hypotheses. Any intervening
signs (words or phrases) that modify a previously
encoded sign re-activate it and raise its resting acti-
vation potential. This can ease later retrieval of that
sign in what is termed an anti-locality effect, con-
tra predictions of DLT. In this way, returning out of
an embedded clause can actually speed processing
by having primed the retrieved sign before it was
needed. ACT-R attributes locality phenomena to fre-
quency effects (e.g. unusual constructions) overrid-
ing such priming and to activation decay if embed-
ded signs do not prime the target sign through mod-
ification (as in parentheticals). Finally, ACT-R pre-
dicts something like DLT?s storage cost due to the
need to differentiate each newly encoded sign from
96
SVP
NP
??NP
ND
the
V
bought
NP
N
studio
D
the
?
?
?
?
?
?
?
S/NP
}
NP/N
Figure 1: Two disjoint connected components of a phrase
structure tree for the sentence The studio bought the pub-
lisher?s rights, shown immediately prior to the word pub-
lisher.
those previously encoded (similarity-based encod-
ing interference) (Lewis et al, 2006).
3.3 Hierarchic Sequential Prediction
Current models of working memory in structured
tasks are defined in terms of hierarchies of sequen-
tial processes, in which superordinate sequences can
be interrupted by subordinate sequences and resume
when the subordinate sequences have concluded
(Botvinick, 2007). These models rely on temporal
cueing as well as content-based cueing to explain
how an interrupted sequence may be recalled for
continuation.
Temporal cueing is based on a context of temporal
features for the current state (Howard and Kahana,
2002). The temporal context in which the subor-
dinate sequence concludes must be similar enough
to the temporal context in which it was initiated to
recall where in the superordinate sequence the sub-
ordinate sequence occurred. For example, the act
of making breakfast may be interrupted by a phone
call. Once the call is complete, the temporal context
is sufficiently similar to when the call began that one
is able to continue preparing breakfast. The associ-
ation between the current temporal context and the
temporal context prior to the interruption is strong
enough to cue the next action.
Temporal cueing is complemented by sequential
(content-based) cueing (Botvinick, 2007) in which
the content of an individual element is associated
with, and thus cues, the following element. For ex-
ample, recalling the 20th note of a song is difficult,
but when playing the song, each note cues the fol-
lowing note, leading one to play the 20th note with-
out difficulty.
Hierarchic sequential prediction may be directly
applicable to processing syntactic center embed-
dings (van Schijndel et al, in press). An ongoing
parse may be viewed graph-theoretically as one or
more connected components of incomplete phrase
structure trees (see Figure 1). Beginning a new sub-
ordinate sequence (a center embedding) introduces
a new connected component, disjoint from that of
the superordinate sequence. As the subordinate se-
quence proceeds, the new component gains asso-
ciated discourse referents, each sequentially cued
from the last, until finally it merges with the super-
ordinate connected component at the end of the em-
bedded clause, forming a single connected compo-
nent representing the parse up to that point. Since
it is not connected to the subordinate connected
component prior to merging, the superordinate con-
nected component must be recalled through tempo-
ral cueing.
McElree (2001; 2006) has found that retrieval
of any non-focused (or in this case, unconnected)
element from memory leads to slower processing.
Therefore, integrating two disjoint connected com-
ponents should be expected to incur a processing
cost due to the need to recall the current state of the
superordinate sequence to continue the parse. Such
a cost would corroborate a DLT-like theory where
integration slows processing.
3.4 Dynamic Recruitment of Additional
Processing Resources
Language processing is typically centered in the left
hemisphere of the brain (for right-handed individ-
uals). Just and Varma (2007) provide fMRI re-
sults suggesting readers dynamically recruit addi-
tional processing resources such as the right-side ho-
mologues of the language processing areas of the
brain when processing center-embedded construc-
tions. Once an embedded construction terminates,
the reader may still have temporary access to these
extra processing resources, which may briefly speed
processing.
This hypothesis would, therefore, predict an en-
coding cost when a center embedding is initiated.
The resulting inhibition would trigger recruitment of
additional processing resources, which would then
97
allow the rest of the embedded structure to be pro-
cessed at the usual speed. Upon completing an em-
bedding, the difficulty arising frommemory retrieval
(McElree, 2001) would be ameliorated by these ex-
tra processing resources, and the reduced process-
ing complexity arising from reduced memory load
would yield a temporary facilitation in processing.
No longer requiring the additional resources to cope
with the increased embedding, the processor would
release them, returning the processor to its usual
speed. Unlike anti-locality, where processing is
facilitated in longer passages due to accumulating
probabilistic evidence, a model of dynamic recruit-
ment of additional processing resources would pre-
dict universal facilitation after a center embedding
of any length, modulo frequency effects.
3.5 Embedding Difference
Wu et al (2010) propose an explicit measure of
the difficulty associated with processing center-
embedded constructions, which is similar to the pre-
dictions of dynamic recruitment and is defined in
terms of changes in memory load. They calcu-
late a probabilistically-weighted average embedding
depth as follows:
?emb(x1 . . . xt) =
?
s?S(x1...xt)
d(s) ? P (s) (4)
where d(s) returns the embedding depth of the
derivation s at xt in a variant of a left-corner pars-
ing process.3 Embedding difference may then be de-
rived as:
EmbDiff (x1 . . . xt) =?emb(x1 . . . xt)? (5)
?emb(x1 . . . xt?1)
This is hypothesized to correlate positively with
processing load: increasing the embedding depth in-
creases processing load and decreasing it reduces
processing load. Note that embedding difference
makes the opposite prediction from DLT in that in-
tegrating an embedded clause is predicted to speed
processing. In fact, the predictions of embedding
3As pointed out by Wu et al (2010), in practice this can be
computed over a beam of potential parses in which case it must
be normalized by the total probability of the beam.
difference are such that it may be viewed as an im-
plementation of the predictions of a hierarchic se-
quential processing model with dynamic recruitment
of additional resources.
4 Model
This paper uses a hierarchic sequence model imple-
mentation of a left-corner parser variant (van Schijn-
del et al, in press), which represents connected com-
ponents of phrase structure trees in hierarchies of
hidden random variables. This requires, at each time
step t:
? a hierarchically-organized set of N connected
component states qnt , each consisting of an ac-
tive sign of category aqnt , and an awaited sign
of category bqnt , separated by a slash ?/?; and
? an observed word xt.
Each connected component state in this model then
represents a contiguous portion of a phrase structure
tree (see Figure 1 on preceding page).
The operations of this parser can be defined as a
deductive system (Shieber et al, 1995) with an input
sequence consisting of a top-level connected com-
ponent state ?/?, corresponding to an existing dis-
course context, followed by a sequence of observed
words x1, x2, . . . 4 If an observation xt can attach as
the awaited sign of the most recent (most subordi-
nate) connected component a/b, it is hypothesized
to do so, turning this incomplete sign into a com-
plete sign a (F?, below); or if the observation can
serve as a lower descendant of this awaited sign, it
is hypothesized to form the first complete sign a? in
a newly initiated connected component (F+):
a/b xt
a b ? xt (F?)
a/b xt
a/b a? b
+? a? ... ; a? ? xt (F+)
Then, if either of these complete signs (a or a?
above, matched to a?? below) can attach as an initial
4A deductive system consists of inferences or productions
of the form:
P
QR, meaning premise P entails conclusion Q ac-
cording to rule R.
98
?/? the
?/?, D F+
?/?, NP/N L? studio
?/?, NP F?
?/?, S/VP L? bought
?/?, S/VP, V F+
?/?, S/NP L+ the
?/?, S/NP, D F+
?/?, S/NP, NP/N L? publisher
?/?, S/NP, NP F?
?/?, S/NP, D/G L? ?s
?/?, S/NP, D F?
?/?, S/N L+ rights
?/?, S F?
?/? L+
Figure 2: Example parse (in the form of a deductive proof) of the sentence The studio bought the publisher?s rights,
using F+, F?, L+, and L? productions. Each pair of deductions combines a context of one or more connected compo-
nent states with a sign (word) observed in that context. By applying the F and L rules to the observed sign and context,
the parser is able to generate a consequent context. Initially, the context corresponds to a connected pre-sentential
dialogue state ?/?. When the is observed, the parser applies F+ to begin a new connected component state D. By
applying L?, the parser determines that this new connected component is unfinished and generates an appropriate
incomplete connected component state NP/N, encoding the superordinate state ?/? for later retrieval. Further on, the
parser observes ?s and uses F? to avoid generating a new connected component, which completes the sign D. The
parser follows this up with L+ to recall the superordinate connected component state S/NP and integrate it into the
most deeply embedded connected component, which results in a less deeply embedded structure.
child of the awaited sign of the immediately superor-
dinate connected component state a/b, it is hypoth-
esized to do so and terminate the subordinate con-
nected component state, with xt as the last observa-
tion of the terminated connected component (L+); or
if the observation can serve as a lower descendant of
this awaited sign, it is hypothesized to remain dis-
joint and form its own connected component (L?):
a/b a??
a/b?? b ? a
?? b?? (L+)
a/b a??
a/b a?/b?? b
+? a? ... ; a? ? a?? b?? (L?)
These operations can be made probabilistic. The
probability ? of a transition at time step t is defined
in terms of (i) a probability ? of initiating a new con-
nected component state with xt as its first observa-
tion, multiplied by (ii) the probability ? of terminat-
ing a connected component state with xt as its last
observation, multiplied by (iii) the probabilities ?
and ? of generating categories for active and awaited
signs aqnt and bqnt in the resulting most subordinate
connected component state qnt . This kind of model
can be defined directly on PCFG probabilities and
trained to produce state-of-the-art accuracy by using
the latent variable annotation of Petrov et al (2006)
(van Schijndel et al, in press).5
An example parse is shown in Figure 2. Since
two binary structural decisions (F and L) must be
made in order to generate each word, there are four
possible structures that may be generated (see Ta-
ble 1). The F+L? transition initiates a new level
of embedding at word xt and so requires the super-
ordinate state to be encoded for later retrieval (e.g.
on observing the in Figure 2). The F?L+ transi-
tion completes the deepest level of embedding and
therefore requires the recall of the current superor-
dinate connected component state with which the
5The model has been shown to achieve an F-score of 87.8,
within .2 points of the Petrov and Klein (2007) parser, which
obtains an F-score of 88.0 on the same task. Because the se-
quence model is defined over binary-branching phrase structure,
both parsers were evaluated on binary-branching phrase struc-
ture trees to provide a fair comparison.
99
F?L? Cue Active Sign
F+L? Initiate/Encode
F?L+ Terminate/Integrate
F+L+ Cue Awaited Sign
Table 1: The hierarchical structure decisions and the op-
erations they represent. F+L? initiates a new connected
component, F?L+ integrates two disjoint connected com-
ponents into a single connected component, and F?L?
and F+L+ sequentially cue, respectively, a new active
sign (along with an associated awaited sign) and a new
awaited sign from the most recent connected component.
subordinate connected component state will be in-
tegrated. For example, in Figure 2, upon observ-
ing ?s, the parser must use temporal cueing to re-
call that it is in the middle of processing an NP (to
complete an S), which sequentially cues a prediction
of N. F?L? transitions complete the awaited sign of
the most subordinate state and so sequentially cue
a following connected component state at the same
tier of the hierarchy. For example, in Figure 2, after
observing studio, the parser uses the completed NP
to sequentially cue the prediction that it has finished
the left child of an S. F+L+ transitions locally ex-
pand the awaited sign of the most subordinate state
and so should also not require any recall or encod-
ing. For example, in Figure 2, observing bought
while awaiting a VP sequentially cues a prediction
of NP.
F+L?, then, loosely corresponds to a storage ac-
tion under DLT as more hierarchic levels must now
be maintained at each future step of the parse. As
stated before, it differs from DLT in that it is sensi-
tive to the depth of embedding rather than a partic-
ular subset of syntactic categories. Wu et al (2010)
found that increasing the embedding depth led to
longer reading times in a self-paced reading experi-
ment. In ACT-R terms, F+L? corresponds to an en-
coding action, potentially causing processing diffi-
culty resulting from the similarity of the current sign
to previously encoded signs.
F?L+, by contrast, is similar to DLT?s integra-
tion action since a subordinate connected compo-
nent is integrated into the rest of the parse structure.
This represents a temporal cueing event in which
the awaited category of the superordinate connected
Theory F+L? F?L+
DLT positive positive
ACT-R positive positive
Hier. Sequential Prediction positive
Dynamic Recruitment positive negative
Embedding Difference positive negative
Table 2: Each theory?s prediction of the direction of
the correlation between each hierachical structure predic-
tor and reading times. Hierarchic sequential prediction
is agnostic about the processing speed of F+L? opera-
tions, and none of the theories make any predictions as to
the sign associated with the within-embedding measures
F?L? and F+L+.
component is recalled. In contrast to DLT, embed-
ding difference and dynamic recruitment would pre-
dict a shorter reading time in the F?L+ case be-
cause of the reduction in memory load. In an ACT-R
framework, reading time durations can increase at
the retrieval site because the retrieval causes compe-
tition among similarly encoded signs in the context
set. While it is possible for reading times to decrease
when completing a center embedding in ACT-R (Va-
sishth and Lewis, 2006), this would be expressed
as a frequency effect due to certain argument types
commonly foreshadowing their predicates (Jaeger et
al., 2008). Since frequency effects are factored sep-
arately from memory effects in this study, ACT-R
would predict longer residual (memory-based) read-
ing times when completing an embedding.
Predicted correlations to reading times for the F
and L transitions are summarized in Table 2.
5 Eye-tracking
Eye-tracking and reading time data are often used to
test complexity measures (Gibson, 2000; Demberg
and Keller, 2008; Roark et al, 2009) under the as-
sumption that readers slow down when reading more
complex passages. Readers saccade over portions of
text and regress back to preceding text in complex
patterns, but studies have correlated certain mea-
sures with certain processing constraints (see Clifton
et al 2007 for a review). For example, the initial
length of time fixated on a single word is correlated
with word identification time; whereas regression
durations after a word is fixated (but prior to a fix-
ation in a new region) are hypothesized to correlate
100
with integration difficulty.
Since this work focuses on incremental process-
ing, all processing that occurs up to a given point in
the sentence is of interest. Therefore, in this study,
predictions will be compared to go-past durations.
Go-past durations are calculated by summing all fix-
ations in a region of text, including regressions, un-
til a new region is fixated, which accounts for addi-
tional processing that may take place after initial lex-
ical access, but before the next region is processed.
For example, if one region ends at word 5 in a sen-
tence, and the next fixation lands on word 8, then the
go-past region consists of words 6-8 and the go-past
duration sums all fixations until a fixation occurs af-
ter word 8.
6 Evaluation
The measures presented in this paper were evaluated
on the Dundee eye-tracking corpus (Kennedy et al,
2003). The corpus consists of 2388 sentences of nat-
urally occurring news text written in standard British
English. The corpus also includes eye-tracking data
from 10 native English speakers, which provides
a test corpus of 260,124 subject-duration pairs of
reading time data. Of this, any fixated words ap-
pearing fewer than 5 times in the training data were
considered unknown and were filtered out to obtain
accurate predictions. Fixations on the first or last
words of a line were also filtered out to avoid any
?wrap-up? effects resulting from preparing to sac-
cade to the beginning of the next line or resulting
from orienting to a new line. Additionally, following
Demberg and Keller (2008), any fixations that skip
more than 4 words were attributed to track loss by
the eyetracker or lack of attention of the reader and
so were excluded from the analysis. This left the fi-
nal evaluation corpus with 151,331 subject-duration
pairs.
The evaluation consisted of fitting a linear mixed-
effects model (Baayen et al, 2008) to reading time
durations using the lmer function of the lme4 R
package (Bates et al, 2011; R Development Core
Team, 2010). This allowed by-subject and by-item
variation to be included in the initial regression as
random intercepts in addition to several baseline pre-
dictors.6 Before fitting, the durations extracted from
6Each fixed effect was centered to reduce collinearity.
the corpus were log-transformed, producing more
normally distributed data to obey the assumptions of
linear mixed effects models.7
Included among the fixed effects were the posi-
tion in the sentence that initiated the go-past region
(SENTPOS) and the number of characters in the ini-
tiating word (NRCHAR). The difficulty of integrat-
ing a word may be seen in whether the immediately
following word was fixated (NEXTISFIX), and sim-
ilarly if the immediately previous word was fixated
(PREVISFIX) the current word probably need not be
fixated for as long. Finally, unigram (LOGPROB)
and bigram probabilities are included. The bigram
probabilities are those of the current word given the
previous word (LOGFWPROB) and the current word
given the following word (LOGBWPROB). Fossum
and Levy (2012) showed that for n-gram probabili-
ties to be effective predictors on the Dundee corpus,
they must be calculated from a wide variety of texts,
so following them, this study used the Brown corpus
(Francis and Kucera, 1979), the WSJ Sections 02-21
(Marcus et al, 1993), the written text portion of the
British National Corpus (BNC Consortium, 2007),
and the Dundee corpus (Kennedy et al, 2003). This
amounted to an n-gram training corpus of roughly
87 million words. These statistics were smoothed
using the SRILM (Stolcke, 2002) implementation of
modified Kneser-Ney smoothing (Chen and Good-
man, 1998). Finally, total surprisal (SURP) was in-
cluded to account for frequency effects in the base-
line.
The preceding measures are commonly used in
baseline models to fit reading time data (Demberg
and Keller, 2008; Frank and Bod, 2011; Fossum and
Levy, 2012) and were calculated from the final word
of each go-past region. The following measures
create a more sophisticated baseline by accumulat-
ing over the entire go-past region to capture what
must be integrated into the discourse to continue the
parse. One factor (CWDELTA) simply counts the
number of words in each go-past region. Cumula-
7In particular, these models assume the noise in the data is
normally distributed. Initial exploratory trials showed that the
residuals of fitting any sensible baseline also become more nor-
mally distributed if the response variable is log-transformed. Fi-
nally, the directions of the effects remain the same whether or
not the reading times are log-transformed, though significance
cannot be ascertained without the transform.
101
tive total surprisal (CUMUSURP) and cumulative en-
tropy reduction (ENTRED) give the surprisal (Hale,
2001) and entropy reduction (Hale, 2003) summed
over the go-past region. To avoid convergence is-
sues, each of the cumulative measures is residual-
ized from the next simpler model in the following
order: CWDELTA from the standard baseline, CU-
MUSURP from the baseline with CWDELTA, and EN-
TRED from the baseline with all other effects.
Residualization was accomplished by using the
simpler mixed-effects model to fit the measure of in-
terest. The residuals from that model fit were then
used in place of the factor of interest. All joint inter-
actions were included in the baseline model as well.
Finally, to account for spillover effects (Just et al,
1982) where processing from a previous region con-
tributes to the following duration, the above baseline
predictors from the previous go-past region were in-
cluded as factors for the current region.
Having SURP as a predictor with CUMUSURP may
seem redundant, but initial analyses showed SURP
was a significant predictor over CUMUSURP when
CWDELTA was a separate factor in the baseline (cur-
rent: p = 2.2 ? 10?16 spillover: p = 2 ? 10?15)
and vice versa (current: p = 2.2 ? 10?16 spillover:
p = 6 ? 10?5). One reason for this could be that
go-past durations conflate complexity experienced
when initially fixating on a region with the difficulty
experienced during regressions. By including both
versions of surprisal, the model is able to account
for frequency effects occurring in both conditions.
This study is only interested in how well the pro-
posed memory-based measures fit the data over the
baseline, so to avoid fitting to the test data or weak-
ening the baseline by overfitting to training data, the
full baseline was used in the final evaluation.
Each measure proposed in this paper was summed
over go-past regions to make it cumulative and
was residualized from all non-spillover factors be-
fore being included on top of the full baseline as a
main effect. Likewise, the spillover version of each
proposed measure was residualized from the other
spillover factors before being included as a main ef-
fect. Only a single proposed measure (or its spillover
corrollary) was included in each model. The results
shown in Table 3 reflect the probability of the full
model fit being obtained by the model lacking each
factor of interest. This was found via posterior sam-
Factor Operation t-score p-value
F?L? Cue Active 0.60 0.55
F+L? Initiate 7.10 2.22?10?14
F?L+ Integrate -5.44 5.23?10?8
F+L+ Cue Awaited -1.55 0.12
Table 3: Significance of each of the structure generation
outcomes at predicting log-transformed durations when
added to the baseline as a main effect after being residu-
alized from it. The sign of the t-score indicates the direc-
tion of the correlation between the residualized factor and
go-past durations. Note that these factors are all based
on the current go-past region; the spillover corollaries of
these were not significant predictors of reading times.
pling of each factor using the Markov chain Monte
Carlo implementation of the languageR R package
(Baayen, 2008).
The results indicate that the F+L? and F?L+ mea-
sures were both significant predictors of duration as
expected. Further, F?L? and F+L+, which both sim-
ply reflect sequential cueing, were not significant
predictors of go-past duration, also as expected.
7 Discussion and Conclusion
The fact that F+L? was strongly predictive over the
baseline is encouraging as it suggests that memory
limitations could provide at least a partial explana-
tion of why certain constructions are less frequent in
corpora and thus yield a high surprisal. Moreover,
it indicates that the model corroborates the shared
prediction of most of the memory-based models that
initiating a new connected component slows pro-
cessing.
The fact that F?L+ is predictive but has a neg-
ative coefficient could be evidence of anti-locality,
or it could be an indication of some sort of pro-
cessing momentum due to dynamic recruitment of
additional processing resources (Just and Varma,
2007). Since anti-locality is an expectation-based
frequency effect, and since this study controlled for
frequency effects with n-grams, surprisal, and en-
tropy reduction, an anti-locality explanation would
rely on either (i) more precise variants of the met-
rics used in this study or (ii) other frequency metrics
altogether. Future work could investigate the possi-
bility of anti-locality by looking at the distance be-
tween an encoding operation and its corresponding
102
integration action to see if the integration facilita-
tion observed in this study is driven by longer em-
beddings or if there is simply a general facilitation
effect when completing embeddings.
The finding of a negative integration cost was pre-
viously observed by Wu et al (2010) as well as
Demberg and Keller (2008), although Demberg and
Keller calculated it using the original referent-based
definitions of Gibson (1998; 2000) and varied which
parts of speech counted for calculating integration
cost. Ultimately, Demberg and Keller (2008) con-
cluded that the negative coefficient was evidence
that integration cost was not a good broad-coverage
predictor of reading times; however, this study has
replicated the effect and showed it to be a very strong
predictor of reading times, albeit one that is corre-
lated with facilitation rather than inhibition.
It is interesting that many studies have found
negative integration cost using naturalistic stimuli
while others have consistently found positive inte-
gration cost when using constructed stimuli with
multiple center embeddings presented without con-
text (Gibson, 2000; Chen et al, 2005; Kwon et al,
2010). It may be the case that any dynamic re-
cruitment is overwhelmed by the memory demands
of multiply center-embedded stimuli. Alternatively,
it may be that the difficulty of processing multiply
center-embedded sentences containing ambiguities
produces anxiety in subjects, which slows process-
ing at implicit prosodic boundaries (Fodor, 2002;
Mitchell et al, 2008). In any case, the source of this
discrepancy presents an attractive target for future
research.
In general, sequential prediction does not seem
to present people with any special ease or difficulty
as evidenced by the lack of significance of F?L?
and F+L+ predictions when frequency effects are
factored out. This supports a theory of sequential,
content-based cueing (Botvinick, 2007) that predicts
that certain states would directly cue other states and
thus avoid recall difficulty. An example of this may
be seen in the case of a transitive verb triggering
the prediction of a direct object. This kind of cue-
ing would show up as a frequency effect predicted
by surprisal rather than as a memory-based cost,
due to frequent occurrences becoming ingrained as
a learned skill. Future work could use these sequen-
tial cueing operations to investigate further claims
of the dynamic recruitment hypothesis. One of the
implications of the hypothesis is that recruitment of
resources alleviates the initial encoding cost, which
allows the parser to continue on as before the em-
bedding. DLT, on the other hand, predicts that there
is a storage cost for maintaining unresolved depen-
dencies during a parse (Gibson, 2000). By weight-
ing each of the sequential cueing operations with the
embedding depth at which it occurs, an experiment
may be able to test these two predictions.
This study has shown that measures based on
working memory operations have strong predictivity
over other previously proposed measures including
those associated with frequency effects. This sug-
gests that memory limitations may provide a partial
explanation of what gives rise to frequency effects.
Lastly, this paper provides evidence that there is a
robust facilitation effect in English that arises from
completing center embeddings.
The hierarchic sequence model, all evaluation
scripts, and regression results for all baseline pre-
dictors used in this paper are freely available at
http://sourceforge.net/projects/modelblocks/.
Acknowledgements
Thanks to Peter Culicover, Micha Elsner, and three
anonymous reviewers for helpful suggestions. This
work was funded by an OSU Department of Lin-
guistics Targeted Investment for Excellence (TIE)
grant for collaborative interdisciplinary projects
conducted during the academic year 2012-13.
References
John R. Anderson, Dan Bothell, Michael D. Byrne,
S. Douglass, Christian Lebiere, and Y. Qin. 2004. An
integrated theory of the mind. Psychological Review,
111(4):1036?1060.
R. Harald Baayen, D. J. Davidson, and Douglas M. Bates.
2008. Mixed-effects modeling with crossed random
effects for subjects and items. Journal of Memory and
Language, 59:390?412.
R. Harald Baayen. 2008. Analyzing Linguistic Data:
A Practical Introduction to Statistics using R. Cam-
bridge University Press, New York, NY.
Douglas Bates, Martin Maechler, and Ben Bolker, 2011.
lme4: Linear mixed-effects models using S4 classes.
BNC Consortium. 2007. The british national corpus.
103
Matthew Botvinick. 2007. Multilevel structure in behav-
ior and in the brain: a computational model of Fuster?s
hierarchy. Philosophical Transactions of the Royal So-
ciety, Series B: Biological Sciences, 362:1615?1626.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical report, Harvard University.
Evan Chen, Edward Gibson, and Florian Wolf. 2005.
Online syntactic storage costs in sentence comprehen-
sion. Journal of Memory and Language, 52(1):144?
169.
Charles Clifton, Adrian Staub, and Keith Rayner. 2007.
Eye movements in reading words and sentences. In
Eye movements: A window on mind and brain, pages
341?372. Elsevier.
Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109(2):193?210.
Janet Fodor. 2002. Prosodic disambiguation in silent
reading. In M. Hirotani, editor, In Proceedings of
NELS 32.
Victoria Fossum and Roger Levy. 2012. Sequential
vs. hierarchical syntactic models of human incremen-
tal sentence processing. In Proceedings of CMCL-
NAACL 2012. Association for Computational Linguis-
tics.
W. Nelson Francis and Henry Kucera. 1979. The brown
corpus: A standard corpus of present-day edited amer-
ican english.
Stefan Frank and Rens Bod. 2011. Insensitivity of
the human sentence-processing system to hierarchical
structure. Psychological Science.
Edward Gibson. 1998. Linguistic complexity: Locality
of syntactic dependencies. Cognition, 68(1):1?76.
Edward Gibson. 2000. The dependency locality theory:
A distance-based theory of linguistic complexity. In
Image, language, brain: Papers from the first mind ar-
ticulation project symposium, pages 95?126.
John Hale. 2001. A probabilistic earley parser as a
psycholinguistic model. In Proceedings of the second
meeting of the North American chapter of the Associ-
ation for Computational Linguistics, pages 159?166,
Pittsburgh, PA.
John Hale. 2003. Grammar, Uncertainty and Sen-
tence Processing. Ph.D. thesis, Cognitive Science,
The Johns Hopkins University.
John Hale. 2006. Uncertainty about the rest of the sen-
tence. Cognitive Science, 30(4):609?642.
John Hale. 2011. What a rational parser would do. Cog-
nitive Science, 35(3):399?443.
Marc W. Howard and Michael J. Kahana. 2002. A dis-
tributed representation of temporal context. Journal of
Mathematical Psychology, 45:269?299.
F. T. Jaeger, E. Fedorenko, P. Hofmeister, and E. Gib-
son. 2008. Expectation-based syntactic processing:
Antilocality outside of head-final languages. In The
21st CUNY Sentence Processing Conference.
Daniel Jurafsky. 1996. A probabilistic model of lexical
and syntactic access and disambiguation. Cognitive
Science: A Multidisciplinary Journal, 20(2):137?194.
Marcel Adam Just and Sashank Varma. 2007. The or-
ganization of thinking: What functional brain imaging
reveals about the neuroarchitecture of complex cogni-
tion. Cognitive, Affective, & Behavioral Neuroscience,
7:153?191.
Marcel Adam Just, Patricia A. Carpenter, and Jacque-
line D. Woolley. 1982. Paradigms and processes in
reading comprehension. Journal of Experimental Psy-
chology: General, 111:228?238.
Alan Kennedy, James Pynte, and Robin Hill. 2003. The
Dundee corpus. In Proceedings of the 12th European
conference on eye movement.
Nayoung Kwon, Yoonhyoung Lee, Peter C. Gordon,
Robert Kluender, and Maria Polinsky. 2010. Cog-
nitive and linguistic factors affecting subject/object
asymmetry: An eye-tracking study of pre-nominal rel-
ative clauses in korean. Language, 86(3):561.
Roger Levy. 2008. Expectation-based syntactic compre-
hension. Cognition, 106(3):1126?1177.
Richard L. Lewis and Shravan Vasishth. 2005.
An activation-based model of sentence processing
as skilled memory retrieval. Cognitive Science,
29(3):375?419.
Richard L. Lewis, Shravan Vasishth, and Jane A. Van
Dyke. 2006. Computational principles of working
memory in sentence comprehension. Trends in Cog-
nitive Science, 10(10):447?454.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Brian McElree. 2001. Working memory and focal atten-
tion. Journal of Experimental Psychology, Learning
Memory and Cognition, 27(3):817?835.
Brian McElree. 2006. Accessing recent events. The Psy-
chology of Learning and Motivation, 46:155?200.
D. Mitchell, X. Shen, M. Green, and T. Hodgson. 2008.
Accounting for regressive eye-movements in models
of sentence processing: A reappraisal of the selective
reanalysis hypothesis. Journal of Memory and Lan-
guage, 59:266?293.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
NAACL HLT 2007, pages 404?411, Rochester, New
York, April. Association for Computational Linguis-
tics.
104
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 44th
Annual Meeting of the Association for Computational
Linguistics (COLING/ACL?06).
Colin Phillips. 2010. Some arguments and non-
arguments for reductionist accounts of syntactic phe-
nomena. Language and Cognitive Processes, 28:156?
187.
R Development Core Team, 2010. R: A Language and
Environment for Statistical Computing. R Foundation
for Statistical Computing, Vienna, Austria. ISBN 3-
900051-07-0.
Brian Roark, Asaf Bachrach, Carlos Cardenas, and
Christophe Pallier. 2009. Deriving lexical and syn-
tactic expectation-based measures for psycholinguistic
modeling via incremental top-down parsing. Proceed-
ings of the 2009 Conference on Empirical Methods in
Natural Langauge Processing, pages 324?333.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
Claude Shannon. 1948. A mathematical theory of com-
munication. Bell System Technical Journal, 27:379?
423, 623?656.
Stuart M. Shieber, Yves Schabes, and Fernando C.N.
Pereira. 1995. Principles and implementation of de-
ductive parsing. Journal of Logic Programming, 24:3?
36.
Andreas Stolcke. 2002. Srilm ? an extensible language
modeling toolkit. In Seventh International Conference
on Spoken Language Processing.
John Trueswell. 1996. The role of lexical frequency
in syntactic ambiguity resolution. Journal of Memory
and Language, 35:566?585.
Marten van Schijndel, Andy Exley, and William Schuler.
in press. A model of language processing as hierarchic
sequential prediction. Topics in Cognitive Science.
Shravan Vasishth and Richard L. Lewis. 2006.
Argument-head distance and processing complexity:
Explaining both locality and antilocality effects. Lan-
guage, 82(4):767?794.
Stephen Wu, Asaf Bachrach, Carlos Cardenas, and
William Schuler. 2010. Complexity metrics in an in-
cremental right-corner parser. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics (ACL?10), pages 1189?1198.
105
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1084?1093,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Bootstrapping into Filler-Gap: An Acquisition Story
Marten van Schijndel Micha Elsner
The Ohio State University
{vanschm,melsner}@ling.ohio-state.edu
Abstract
Analyses of filler-gap dependencies usu-
ally involve complex syntactic rules or
heuristics; however recent results suggest
that filler-gap comprehension begins ear-
lier than seemingly simpler constructions
such as ditransitives or passives. Therefore,
this work models filler-gap acquisition as a
byproduct of learning word orderings (e.g.
SVO vs OSV), which must be done at a
very young age anyway in order to extract
meaning from language. Specifically, this
model, trained on part-of-speech tags, rep-
resents the preferred locations of semantic
roles relative to a verb as Gaussian mix-
tures over real numbers.
This approach learns role assignment in
filler-gap constructions in a manner con-
sistent with current developmental findings
and is extremely robust to initialization
variance. Additionally, this model is shown
to be able to account for a characteristic er-
ror made by learners during this period (A
and B gorped interpreted as A gorped B).
1 Introduction
The phenomenon of filler-gap, where the argument
of a predicate appears outside its canonical posi-
tion in the phrase structure (e.g. [the apple]
i
that
the boy ate t
i
or [what]
i
did the boy eat t
i
), has long
been an object of study for syntacticians (Ross,
1967) due to its apparent processing complexity.
Such complexity is due, in part, to the arbitrary
length of the dependency between a filler and its
gap (e.g. [the apple]
i
that Mary said the boy ate t
i
).
Recent studies indicate that comprehension of
filler-gap constructions begins around 15 months
(Seidl et al, 2003; Gagliardi et al, 2014). This
finding raises the question of how such a complex
phenomenon could be acquired so early since chil-
dren at that age do not yet have a very advanced
grasp of language (e.g. ditransitives do not seem
to be generalized until at least 31 months; Gold-
berg et al 2004, Bello 2012). This work shows
that filler-gap comprehension in English may be
Age
Wh-S
Wh-O
1-1
13mo
No
No
15mo
Yes
(Yes)
20mo
Yes
Yes
Yes
25mo
Yes
Yes
No
Figure 1: The developmental timeline of subject
(Wh-S) and object (Wh-O) wh-clause extraction
comprehension suggested by experimental results
(Seidl et al, 2003; Gagliardi et al, 2014). Paren-
theses indicate weak comprehension. The final row
shows the timeline of 1-1 role bias errors (Naigles,
1990; Gertner and Fisher, 2012). Missing nodes de-
note a lack of studies.
acquired through learning word orderings rather
than relying on hierarchical syntactic knowledge.
This work describes a cognitive model of the de-
velopmental timecourse of filler-gap comprehension
with the goal of setting a lower bound on the mod-
eling assumptions necessary for an ideal learner
to display filler-gap comprehension. In particular,
the model described in this paper takes chunked
child-directed speech as input and learns orderings
over semantic roles. These orderings then permit
the model to successfully resolve filler-gap depen-
dencies.
1
Further, the model presented here is also
shown to initially reflect an idiosyncratic role as-
signment error observed in development (e.g. A
and B kradded interpreted as A kradded B ; Gert-
ner and Fisher, 2012), though after training, the
model is able to avoid the error. As such, this work
may be said to model a learner from 15 months to
between 25 and 30 months.
1
This model does not explicitly learn gap positions,
but rather assigns thematic roles to arguments based
on where those arguments are expected to manifest.
This approach to filler-gap comprehension is supported
by findings that show people do not actually link fillers
to gap positions but instead link the filler to a verb
with missing arguments (Pickering and Barry, 1991)
1084
2 Background
The developmental timeline during which children
acquire the ability to process filler-gap construc-
tions is not well-understood. Language comprehen-
sion precedes production, and the developmental
literature on the acquisition of filler-gap construc-
tions is sparsely populated due to difficulties in de-
signing experiments to test filler-gap comprehen-
sion in preverbal infants. Older studies typically
looked at verbal children and the mistakes they
make to gain insight into the acquisition process
(de Villiers and Roeper, 1995).
Recent studies, however, indicate that filler-
gap comprehension likely begins earlier than pro-
duction (Seidl et al, 2003; Gagliardi and Lidz,
2010; Gagliardi et al, 2014). Therefore, studies
of verbal children are probably actually testing
the acquisition of production mechanisms (plan-
ning, motor skills, greater facility with lexical ac-
cess, etc) rather than the acquisition of filler-
gap. Note that these may be related since filler-
gap could introduce greater processing load which
could overwhelm the child?s fragile production ca-
pacity (Phillips, 2010).
Seidl et al (2003) showed that children are able
to process wh-extractions from subject position
(e.g. [who]
i
t
i
ate pie) as young as 15 months
while similar extractions from object position (e.g.
[what]
i
did the boy eat t
i
) remain unparseable until
around 20 months of age.
2
This line of investiga-
tion has been reopened and expanded by Gagliardi
et al (2014) whose results suggest that the ex-
perimental methodology employed by Seidl et al
(2003) was flawed in that it presumed infants have
ideal performance mechanisms. By providing more
trials of each condition and controlling for the prag-
matic felicity of test statements, Gagliardi et al
(2014) provide evidence that 15-month old infants
can process wh-extractions from both subject and
object positions. Object extractions are more diffi-
cult to comprehend than subject extractions, how-
ever, perhaps due to additional processing load in
object extractions (Gibson, 1998; Phillips, 2010).
Similarly, Gagliardi and Lidz (2010) show that rel-
ativized extractions with a wh-relativizer (e.g. find
[the boy]
i
who t
i
ate the apple) are easier to com-
prehend than relativized extractions with that as
the relativizer (e.g. find [the boy]
i
that t
i
ate the
apple).
Yuan et al (2012) demonstrate that 19-month
olds use their knowledge of nouns to learn both
verbs and their associated argument structure. In
2
Since the wh-phrase is in the same (or a very simi-
lar) position as the original subject when the wh-phrase
takes subject position, it is not clear that these con-
structions are true extractions (Culicover, 2013), how-
ever, this paper will continue to refer to them as such
for ease of exposition.
their study, infants were shown video of a person
talking on a phone using a nonce verb with ei-
ther one or two nouns (e.g. Mary kradded Susan).
Under the assumption that infants look longer at
things that correspond to their understanding of
a prompt, the infants were then shown two im-
ages that potentially depicted the described action
? one picture where two actors acted independently
(reflecting an intransitive proposition) and one pic-
ture where one actor acted on the other (reflecting
a transitive proposition).
3
Even though the infants
had no extralinguistic knowledge about the verb,
they consistently treated the verb as transitive if
two nouns were present and intransitive if only one
noun was present.
Similarly, Gertner and Fisher (2012) show that
intransitive phrases with conjoined subjects (e.g.
John and Mary gorped) are given a transitive in-
terpretation (i.e. John gorped Mary) at 21 months
(henceforth termed ?1-1 role bias?), though this ef-
fect is no longer present at 25 months (Naigles,
1990). This finding suggests both that learners
will ignore canonical structure in favor of using
all possible arguments and that children have a
bias to assign a unique semantic role to each argu-
ment. It is important to note, however, that cross-
linguistically children do not seem to generalize be-
yond two arguments until after at least 31 months
of age (Goldberg et al, 2004; Bello, 2012), so a
predicate occurring with three nouns would still
likely be interpreted as merely transitive rather
than ditransitive.
Computational modeling provides a way to test
the computational level of processing (Marr, 1982).
That is, given the input (child-directed speech,
adult-directed speech, and environmental experi-
ences), it is possible to probe the computational
processes that result in the observed output. How-
ever, previous computational models of grammar
induction (Klein and Manning, 2004), including in-
fant grammar induction (Kwiatkowski et al, 2012),
have not addressed filler-gap comprehension.
4
The closest work to that presented here is the
work on BabySRL (Connor et al, 2008; Connor et
al., 2009; Connor et al, 2010). BabySRL is a com-
putational model of semantic role acquistion using
a similar set of assumptions to the current work.
BabySRL learns weights over ordering constraints
(e.g. preverbal, second noun, etc.) to acquire se-
mantic role labelling while still exhibiting 1-1 role
bias. However, no analysis has evaluated the abil-
3
There were two actors in each image to avoid bias-
ing the infants to look at the image with more actors.
4
As one reviewer notes, Joshi et al (1990) and sub-
sequent work show that filler-gap phenomena can be
formally captured by mildly context-sensitive grammar
formalisms; these have the virtue of scaling up to adult
grammar, but due to their complexity, do not seem to
have been described as models of early acquisition.
1085
Susan said John gave girl book
-3 -2 -1 0 1 2
Table 1: An example of a chunked sentence (Su-
san said John gave the girl a red book) with the
sentence positions labelled. Nominal heads of noun
chunks are in bold.
ity of BabySRL to acquire filler-gap constructions.
Further comparison to BabySRL may be found in
Section 6.
3 Assumptions
The present work restricts itself to acquiring filler-
gap comprehension in English. The model pre-
sented here learns a single, non-recursive ordering
for the semantic roles in each sentence relative to
the verb since several studies have suggested that
early child grammars may consist of simple lin-
ear grammars that are dictated by semantic roles
(Diessel and Tomasello, 2001; Jackendoff and Wit-
tenberg, in press). This work assumes learners can
already identify nouns and verbs, which is sup-
ported by Shi et al (1999) who show that chil-
dren at an extremely young age can distinguish be-
tween content and function words and by Waxman
and Booth (2001) who show that children can dis-
tinguish between different types of content words.
Further, since Waxman and Booth (2001) demon-
strate that, by 14 months, children are able to dis-
tinguish nouns from modifiers, this work assumes
learners can already chunk nouns and access the
nominal head. To handle recursion, this work as-
sumes that children treat the final verb in each
sentence as the main verb (implicitly assuming sen-
tence segmentation), which ideally assigns roles to
each of the nouns in the sentence.
Due to the findings of Yuan et al (2012),
this work adopts a ?syntactic bootstrapping? the-
ory of acquisition (Gleitman, 1990), where struc-
tural properties (e.g. number of nouns) inform the
learner about semantic properties of a predicate
(e.g. how many semantic roles it confers). Since
infants infer the number of semantic roles, this
work further assumes they already have expecta-
tions about where these roles tend to be realized
in sentences, if they appear. These positions may
correspond to different semantic roles for different
predicates (e.g. the subject of run and of melt);
however, the role for predicates with a single argu-
ment is usually assigned to the noun that precedes
the verb while a second argument is usually as-
signed after the verb. The semantic properties of
these roles may be learned lexically for each pred-
icate, but that is beyond the scope of this work.
Therefore, this work uses syntactic and semantic
roles interchangeably (e.g. subject and agent).
? ? pi
G
SC
-1 0.5 .999
G
SN
-1 3 .001
G
OC
1 0.5 .999
G
ON
1 3 .001
? .00001
Table 2: Initial values for the mean (?), standard
deviation (?), and prior (pi) of each Gaussian as
well as the skip penalty (?) used in this paper.
Finally, following the finding by Gertner and
Fisher (2012) that children interpret intransitives
with conjoined subjects as transitives, this work as-
sumes that semantic roles have a one-to-one corre-
spondence with nouns in a sentence (similarly used
as a soft constraint in the semantic role labelling
work of Titov and Klementiev, 2012).
4 Model
The model represents the preferred locations of
semantic roles relative to the verb as distribu-
tions over real numbers. This idea is adapted from
Boersma (1997) who uses it to learn constraint
rankings in optimality theory.
In this work, the final (main) verb is placed at
position 0; words (and chunks) before the verb are
given progressively more negative positions, and
words after the verb are given progressively more
positive positions (see Table 1). Learner expecta-
tions of where an argument will appear relative
to the verb are modelled as two-component Gaus-
sian mixtures: one mixture of Gaussians (G
S?
) cor-
responds to the subject argument, another (G
O?
)
corresponds to the object argument. There is no
mixture for a third argument since children do not
generalize beyond two arguments until later in de-
velopment (Goldberg et al, 2004; Bello, 2012).
One component of each mixture learns to repre-
sent the canonical position for the argument (G
?C
)
while the other (G
?N
) represents some alternate,
non-canonical position such as the filler position
in filler-gap constructions. To reflect the fact that
learners have had 15 months of exposure to their
language before acquiring filler-gap, the mixture is
initialized so that there is a stronger probability
associated with the canonical Gaussian than with
the non-canonical Gaussian of each mixture.
5
Fi-
nally, the one-to-one role bias is explicitly encoded
such that the model cannot use a label that has
already been used elsewhere in the sentence.
5
Akhtar (1999) finds that learners may not have
strong expectations of canonical argument positions
until four years of age, but the results of the current
study are extremely robust to changes in initialization,
as discussed in Section 7 of this paper, so this assump-
tion is mostly adopted for ease of exposition.
1086
P
r
o
b
a
b
i
l
i
t
y
Position relative to verb
P
r
o
b
a
b
i
l
i
t
y
Position relative to verb
Figure 2: Visual representations of (Left) the initial model?s expectations of where arguments will appear,
given the initial parameters in Table 2 and (Right) the converged model?s expectations of where arguments
will appear.
Thus, the initial model conditions (see Figure 2)
are most likely to realize an SVO ordering, al-
though it is possible to obtain SOV (by sampling
a negative number from the blue curve) or even
OSV (by also sampling the red curve very close
to 0). The model is most likely to hypothesize a
preverbal object when it has already assigned the
subject role to something and, in addition, there is
no postverbal noun competing for the object label.
In other words, the model infers that an object ex-
traction may have occurred if there is a ?missing?
postverbal argument.
Finally, the probability of a given sequence is the
product of the label probabilities for the compo-
nent argument positions (e.g. G
SC
generating an
argument at position -2, etc). Since many sentences
have more than two nouns, the model is allowed to
skip nouns by multiplying a penalty term (?) into
the product for each skipped noun; the cost is set
at 0.00001 for this study, though see Section 7 for a
discussion of the constraints on this parameter. See
Table 2 for initialization parameters and Figure 2
for a visual representation of the initial expecta-
tions of the model.
This work uses a model with 2-component mix-
tures for both subjects and objects (termed the
symmetric model). This formulation achieves the
best fit to the training data according to the
Bayesian Information Criterion (BIC).
6
However,
follow-up experiments find that the non-canonical
subject Gaussian only improves the likelihood of
the data by erroneously modeling postverbal nouns
in imperative statements. The lack of a canonical
subject in English imperatives allows the model to
improve the likelihood of the data by using the
non-canonical subject Gaussian to capture ficti-
6
The BIC rewards improved log-likelihood but pe-
nalizes increased model complexity.
tious postverbal arguments. When imperatives are
filtered out of the training corpus, the symmetric
model obtains a worse BIC fit than a model that
lacks the non-canonical subject Gaussian. There-
fore, if one makes the assumption that impera-
tives are prosodically-marked for learners (e.g. the
learner is the implicit subject), the best model is
one that lacks a non-canonical subject.
7
The re-
mainder of this paper assumes a symmetric model
to demonstrate what happens if such an assump-
tion is not made; for the evaluations described in
this paper, the results are similar in either case.
This model differs from other non-recursive
computational models of grammar induction (e.g.
Goldwater and Griffiths, 2007) since it is not based
on Hidden Markov Models. Instead, it determines
the best ordering for the sentence as a whole. This
approach bears some similarity to a Generalized
Mallows model (Chen et al, 2009), but the current
formulation was chosen due to being independently
posited as cognitively plausible (Boersma, 1997).
Figure 2 (Right) shows the converged, final state
of the model. The model expects the first argu-
ment (usually agent) to be assigned preverbally
and expects the second (say, patient) to be assigned
postverbally; however, there is now a larger chance
that the second argument will appear preverbally.
5 Evaluation
The model in this work is trained using transcribed
child-directed speech (CDS) from the BabySRL
portions (Connor et al, 2008) of CHILDES
(MacWhinney, 2000). Chunking is performed us-
7
This finding suggests that a Dirichlet Process or
other means of dynamically determining the number
of components in each mixture would converge to a
model that lacks non-canonical subjects if imperative
filtering were employed.
1087
Eve (n = 4820) Adam (n = 4461)
P R F P R F
Initial .54 .64 .59 .53 .60 .56
Trained .52 .69 .59
?
.51 .65 .57
?
Initial
c
.56 .66 .60 .55 .62 .58
Trained
c
.54 .71 .61
?
.53 .67 .59
?
Table 3: Overall accuracy on the Eve and Adam
sections of the BabySRL corpus. Bottom rows re-
flect accuracy when non-agent roles are collapsed
into a single role. Note that improvements are nu-
merically slight since filler-gap is relatively rare
(Schuler, 2011).
?
p << .01
ing a basic noun-chunker from NLTK (Bird et al,
2009). Based on an initial analysis of chunker per-
formance, yes is hand-corrected to not be a noun.
Poor chunker perfomance is likely due to a mis-
match in chunker training and testing domains
(Wall Street Journal text vs transcribed speech),
but chunking noise may be a good estimation of
learner uncertainty, so the remaining text is left
uncorrected. All noun phrase chunks are then re-
placed with their final noun (presumed the head)
to approximate the ability of children to distin-
guish nouns from modifiers (Waxman and Booth,
2001). Finally, for each sentence, the model assigns
sentence positions to each word with the final verb
at zero.
Viterbi Expectation-Maximization is performed
over each sentence in the corpus to infer the pa-
rameters of the model. During the Expectation
step, the model uses the current Gaussian param-
eters to label the nouns in each sentence with ar-
gument roles. Since the model is not lexicalized,
these roles correspond to the semantic roles most
commonly associated with subject and object. The
model then chooses the best label sequence for each
sentence.
These newly labelled sentences are used during
the Maximization step to determine the Gaussian
parameters that maximize the likelihood of that
labelling. The mean of each Gaussian is updated
to the mean position of the words it labels. Sim-
ilarly, the standard deviation of each Gaussian is
updated with the standard deviation of the posi-
tions it labels. A learning rate of 0.3 is used to
prevent large parameter jumps. The prior proba-
bility of each Gaussian is updated as the ratio of
that Gaussian?s labellings to the total number of
labellings from that mixture in the corpus:
pi
??
=
| G
??
|
| G
??
|
(1)
where ? ? {S,O} and ? ? {C,N}.
Best results seem to be obtained when the skip-
penalty is loosened by an order of magnitude dur-
Subject Extraction filter: S x V . . .
Object Extraction filter: O . . . V . . .
Eve (n = 1345) Adam (n = 1287)
P R F P R F
Initial
c
.53 .57 .55 .53 .52 .52
Trained
c
.55 .67 .61
?
.54 .63 .58
?
Table 4: (Above) Filters to extract filler-gap con-
structions: A) the subject and verb are not ad-
jacent, B) the object precedes the verb. (Below)
Filler-gap accuracy on the Eve and Adam sections
of the BabySRL corpus when non-agent roles are
collapsed into a single role.
?
p << .01
ing testing. Essentially, this forces the model to
tightly adhere to the perceived argument struc-
ture during training to learn more rigid parame-
ters, but the model is allowed more leeway to skip
arguments it has less confidence in during testing.
Convergence (see Figure 2) tends to occur after
four iterations but can take up to ten iterations
depending on the initial parameters.
Since the model is unsupervised, it is trained on
a given corpus (e.g. Eve) before being tested on
the role annotations of that same corpus. The Eve
corpus was used for development purposes,
8
and
the Adam data was used only for testing.
For testing, this study uses the semantic role
annotations in the BabySRL corpus. These anno-
tations were obtained by automatically semantic
role labelling portions of CHILDES with the sys-
tem of Punyakanok et al (2008) before roughly
hand-correcting them (Connor et al, 2008). The
BabySRL corpus is annotated with 5 different
roles, but the model described in this paper only
uses 2 roles. Therefore, overall accuracy results (see
Table 3) are presented both for the raw BabySRL
corpus and for a collapsed BabySRL corpus where
all non-agent roles are collapsed into a single role
(denoted by a subscript
c
in all tables).
Since children do not generalize above two ar-
guments during the modelled age range (Goldberg
et al, 2004; Bello, 2012), the collapsed numbers
more closely reflect the performance of a learner
at this age than the raw numbers. The increase in
accuracy obtained from collapsing non-agent ar-
guments indicates that children may initially gen-
eralize incorrectly to some verbs and would need
to learn lexically-specific role assignments (e.g.
double-object constructions of give). Since the cur-
rent work is interested in general filler-gap com-
prehension at this age, including over unknown
verbs, the remaining analyses in this paper con-
8
This is included for transparency, though the ini-
tial parameters have very little bearing on the final re-
sults as stated in Section 7, so the danger of overfitting
to development data is very slight.
1088
P R F P R F
Eve Subj (n = 691) Obj (n = 654)
Initial
c
.66 .83 .74 .35 .31 .33
Trained
c
.64 .84 .72
?
.45 .52 .48
?
Adam Subj (n = 886) Obj (n = 1050)
Initial
c
.69 .81 .74 .33 .27 .30
Trained
c
.66 .81 .73 .44 .48 .46
?
P R F P R F
Eve Wh- (n = 689) That (n = 125)
Initial
c
.63 .45 .53 .43 .48 .45
Trained
c
.73 .75 .74
?
.44 .57 .50
?
Adam Wh- (n = 748) That (n = 189)
Initial
c
.50 .37 .42 .50 .50 .50
Trained
c
.61 .65 .63
?
.47 .56 .51
?
Table 5: (Left) Subject-extraction accuracy and object-extraction accuracy and (Right) Wh-relative ac-
curacy and that-relative accuracy; calculated over the Eve and Adam sections of the BabySRL corpus
with non-agent roles collapsed into a single role.
?
p = .02
?
p << .01
sider performance when non-agent arguments are
collapsed.
9
Next, a filler-gap version of the BabySRL cor-
pus is created using a coarse filtering process: the
new corpus is comprised of all sentences where an
associated object precedes the final verb and all
sentences where the relevant subject is not imme-
diately followed by the final verb (see Table 4). For
these filler-gap evaluations, the model is trained on
the full version of the corpus in question (e.g. Eve)
before being tested on the filler-gap subset of that
corpus. The overall results of the filler-gap evalua-
tion (see Table 4) indicate that the model improves
significantly at parsing filler-gap constructions af-
ter training.
The performance of the model on role-
assignment in filler-gap constructions may be
analyzed further in terms of how the model
performs on subject-extractions compared with
object-extractions and in terms of how the model
performs on that-relatives compared with wh-
relatives (see Table 5).
The model actually performs worse at subject-
extractions after training than before training.
This is unsurprising because, prior to training,
subjects have little-to-no competition for prever-
bal role assignments; after training, there is a pre-
verbal extracted object category, which the model
can erroneously use. This slight, though signifi-
cant in Eve, deficit is counter-balanced by a very
substantial and significant improvement in object-
extraction labelling accuracy.
Similarly, training confers a large and significant
improvement for role assignment in wh-relative
constructions, but it yields less of an improve-
ment for that-relative constructions. This differ-
ence mimics a finding observed in the developmen-
tal literature where children seem slower to ac-
quire comprehension of that-relatives than of wh-
relatives (Gagliardi and Lidz, 2010).
9
Though performance is slightly worse when argu-
ments are not collapsed, all the same patterns emerge.
6 Comparison to BabySRL
The acquisition of semantic role labelling (SRL) by
the BabySRL model (Connor et al, 2008; Connor
et al, 2009; Connor et al, 2010) bears many sim-
ilarities to the current work and is, to our knowl-
edge, the only comparable line of inquiry to the
current one. The primary function of BabySRL is
to model the acquisition of semantic role labelling
while making an idiosyncratic error which infants
also make (Gertner and Fisher, 2012), the 1-1 role
bias error (John and Mary gorped interpreted as
John gorped Mary). Similar to the model presented
in this paper, BabySRL is based on simple ordering
features such as argument position relative to the
verb and argument position relative to the other
arguments.
This section will demonstrate that the model in
this paper initially reflects 1-1 role bias comparably
to BabySRL, though it progresses beyond this bias
after training.
10
Further, the model in this paper is
able to reflect the concurrent acquisition of filler-
gap whereas BabySRL does not seem well-suited
to such a task. Finally, BabySRL performs unde-
sirably in intransitive settings whereas the model
in this paper does not.
Connor et al (2008) demonstrate that a super-
vised perceptron classifier, based on positional fea-
tures and trained on the silver role label annota-
tions of the BabySRL corpus, manifests 1-1 role
bias errors. Follow-up studies show that supervi-
sion may be lessened (Connor et al, 2009) or re-
moved (Connor et al, 2010) and BabySRL will still
reflect a substantial 1-1 role bias.
Connor et al (2008) and Connor et al (2009)
run direct analyses of how frequently their mod-
els make 1-1 role bias errors. A comparable eval-
uation may be run on the current model by
generating 1000 sentences with a structure of
NNV and reporting how many times the model
chooses a subject-first labelling (see Table 6).
11
10
All evaluations in this section are preceded by
training on the chunked Eve corpus.
11
While Table 6 analyzes erroneous labellings of
NNV structure, the ?Obj? column of Table 5 (Left)
1089
Error rate
Initial .36
Trained .11
Initial (given 2 args) .66
Trained (given 2 args) .13
2008 arg-arg position .65
2008 arg-verb position 0
2009 arg-arg position .82
2009 arg-verb position .63
Table 6: 1-1 role bias error in this model compared
to the models of Connor et al (2008) and Connor
et al (2009). That is, how frequently each model
labelled an NNV sentence SOV. Since the Connor
et al models are perceptron-based, they require
both arguments be labelled. The model presented
in this paper does not share this restriction, so the
raw error rate for this model is presented in the
first two lines; the error rate once this additional
restriction is imposed is given in the second two
lines.
The results of Connor et al (2008) and Connor
et al (2009) depend on whether BabySRL uses
argument-argument relative position as a feature
or argument-verb relative position as a feature
(there is no combined model). Further, the model
presented here from Connor et al (2009) has a
unique argument constraint, similar to the model
in this paper, in order to make comparison as di-
rect as possible.
The 1-1 role bias error rate (before training) of
the model presented in this paper is comparable
to that of Connor et al (2008) and Connor et al
(2009), which shows that the current model pro-
vides comparable developmental modeling benefits
to the BabySRL models. Further, similar to real
children (see Figure 1) the model presented in this
paper develops beyond this error by the end of its
training,
12
whereas the BabySRL models still make
this error after training.
Connor et al (2010) look at how frequently
their model correctly labels the agent in transitive
and intransitive sentences with unknown verbs (to
demonstrate that it exhibits an agent-first bias).
This evaluation can be replicated for the current
study by generating 1,000 sentences with the tran-
sitive form of NVN and a further 1,000 sentences
with the intransitive form of NV (see Table 7).
Since Connor et al (2010) investigate the effects
shows model accuracy on NNV structures.
12
It is important to note that the unique argument
constraint prevents the current model from actually
getting the correct, conjoined-subject parse, but it no
longer exhibits agent-first bias, an important step for
acquiring passives, which occurs between 3 and 4 years
(Thatcher et al, 2008).
NVN NV
Sents in Eve 1173 1513
Sents in Adam 1029 1353
Initial .67 1
Trained .65 .96
Weak (10) lexical .71 .59
Strong (365) lexical .74 .41
Gold Args .77 .58
Table 7: Agent-prediction recall accuracy in tran-
sitive (NVN) and intransitive (NV) settings of the
model presented in this paper (middle) and the
combined model of Connor et al (2010) (bottom),
which has features for argument-argument relative
position as well as argument-predicate relative po-
sition and so is closest to the model presented in
this paper.
of different initial lexicons, this evaluation com-
pares against the resulting BabySRL from each ini-
tializer: they initially seed their part-of-speech tag-
ger with either the 10 or 365 most frequent nouns
in the corpus or they dispense with the tagger and
use gold part-of-speech tags.
As with subject extraction, the model in this
paper gets less accurate after training because of
the newly minted extracted object category that
can be mistakenly used in these canonical settings.
While the model of Connor et al (2010) outper-
forms the model presented here when in a tran-
sitive setting, their model does much worse in an
intransitive setting. The difference in transitive set-
tings stems from increased lexicalization, as is ap-
parent from their results alone; the model pre-
sented here initially performs close to their weakly
lexicalized model, though training impedes agent-
prediction accuracy due to an increased probability
of non-canonical objects.
For the intransitive case, however, whereas the
model presented in this paper is generally able to
successfully label the lone noun as the subject, the
model of Connor et al (2010) chooses to label lone
nouns as objects about 40% of the time. This likely
stems from their model?s reliance on argument-
argument relative position as a feature; when there
is no additional argument to use for reference, the
model?s accuracy decreases. This is borne out by
their model (not shown in Table 7) that omits
the argument-argument relative position feature
and solely relies on verb-argument position, which
achieves up to 70% accuracy in intransitive set-
tings. Even in that case, however, BabySRL still
chooses to label lone nouns as objects 30% of the
time. The fact that intransitive sentences are more
common than transitive sentences in both the Eve
and Adam sections of the BabySRL corpus sug-
gests that learners should be more likely to assign
1090
correct roles in an intransitive setting, which is not
reflected in the BabySRL results.
The overall reason for the different results be-
tween the current work and BabySRL is that
BabySRL relies on positional features that mea-
sure the relative position of two individual ele-
ments (e.g. where a given noun is relative to the
verb). Since the model in this paper operates over
global orderings, it implicitly takes into account
the positions of other nouns as it models argument
position relative to the verb; object and subject
are in competition as labels for preverbal nouns,
so a preverbal object is usually only assigned once
a subject has already been detected.
Further, while BabySRL consistently reflects 1-
1 role bias (corresponding to a pre 25-month old
learner), it also learns to productively label five
roles, which developmental studies have shown
does not take place until at least 31 months (Gold-
berg et al, 2004; Bello, 2012). Finally, it does not
seem likely that BabySRL could be easily extended
to capture filler-gap acquisition. The argument-
verb position features impede acquisition of filler-
gap by classifying preverbal arguments as agents,
and the argument-argument position features in-
hibit accurate labelling in intransitive settings and
result in an agent-first bias which would tend to
label extracted objects as agents. In fact, these ob-
servations suggest that any linear classifier which
relies on positioning features will have difficulties
modeling filler-gap acquisition.
In sum, the unlexicalized model presented in this
paper is able to achieve greater labelling accuracy
than the lexicalized BabySRL models in intran-
sitive settings, though this model does perform
slightly worse in the less common transitive set-
ting. Further, the unsupervised model in this pa-
per initially reflects developmental 1-1 role bias as
well as the supervised BabySRL models, and it
is able to progress beyond this bias. Finally, un-
like BabySRL, the model presented here provides a
cognitive model of the acquisition of filler-gap com-
prehension, which BabySRL does not seem well-
suited to model.
7 Discussion
This paper has presented a simple cognitive model
of filler-gap acquisition, which is able to capture
several findings from developmental psychology.
Training significantly improves role labelling in
the case of object-extractions, which improves the
overall accuracy of the model. This boost is ac-
companied by a slight decrease in labelling ac-
curacy in subject-extraction settings. The asym-
metric ease of subject versus object comprehen-
sion is well-documented in both children and
adults (Gibson, 1998), and while training improves
the model?s ability to process object-extractions,
there is still a gap between object-extraction and
subject-extraction comprehension even after train-
ing.
Further, the model exhibits better comprehen-
sion of wh-relatives than that-relatives similar to
children (Gagliardi and Lidz, 2010). This could
also be an area where a lexicalized model could
do better. As Gagliardi and Lidz (2010) point
out, whereas wh-relatives such as who or which
always signify a filler-gap construction, that can
occur for many different reasons (demonstrative,
determiner, complementizer, etc) and so is a much
weaker filler-gap cue. A lexical model could poten-
tially pick up on clues which could indicate when
that is a relativizer or simply improve on its com-
prehension of wh-relatives even more.
It is interesting to note that the cuurent model
does not make use of that as a cue at all and
yet is still slower at acquiring that-relatives than
wh-relatives. This fact suggests that the findings
of Gagliardi and Lidz (2010) may be partially ex-
plained by a frequency effect: perhaps the input to
children is simply biased such that wh-relatives are
much more common than that-relatives (as shown
in Table 5).
This model also initially reflects the 1-1 role bias
observed in children (Gertner and Fisher, 2012) as
well as previous models (Connor et al, 2008; Con-
nor et al, 2009; Connor et al, 2010) without sac-
rificing accuracy in canonical intransitive settings.
Finally, this model is extremely robust to differ-
ent initializations. The canonical Gaussian expec-
tations can begin far from the verb (?3) or close
to the verb (?0.1), and the standard deviations
of the distributions and the skip-penalty can vary
widely; the model always converges to give compa-
rable results to those presented here. The only con-
straint on the initial parameters is that the proba-
bility of the extracted object occurring preverbally
must exceed the skip-penalty (i.e. extraction must
be possible). In short, this paper describes a sim-
ple, robust cognitive model of the development of
a learner between 15 months until somewhere be-
tween 25- and 30-months old (since 1-1 role bias is
no longer present but no more than two arguments
are being generalized).
In future, it would be interesting to incorporate
lexicalization into the model presented in this pa-
per, as this feature seems likely to bridge the gap
between this model and BabySRL in transitive set-
tings. Lexicalization should also help further dis-
tinguish modifiers from arguments and improve the
overall accuracy of the model.
It would also be interesting to investigate how
well this model generalizes to languages besides
English. Since the model is able to use the verb
position as a semi-permeable boundary between
canonical subjects and objects, it may not work as
1091
well in verb-final languages, and thus makes the
prediction that filler-gap comprehension may be
acquired later in development in such languages
due to a greater reliance on hierarchical syntax.
Ordering is one of the definining characteris-
tics of a language that must be acquired by learn-
ers (e.g. SVO vs SOV), and this work shows that
filler-gap comprehension can be acquired as a by-
product of learning orderings rather than having to
resort to higher-order syntax. Note that this model
cannot capture the constraints on filler-gap usage
which require a hierarchical grammar (e.g. subja-
cency), but such knowledge is really only needed
for successful production of filler-gap construc-
tions, which occurs much later (around 5 years;
de Villiers and Roeper, 1995). Further, the kind of
ordering system proposed in this paper may form
an initial basis for learning such grammars (Jack-
endoff and Wittenberg, in press).
8 Acknowledgements
Thanks to Peter Culicover, William Schuler, Laura
Wagner, and the attendees of the OSU 2013 Fall
Linguistics Colloquium Fest for feedback on this
work. This work was partially funded by an OSU
Dept. of Linguistics Targeted Investment for Ex-
cellence (TIE) grant for collaborative interdisci-
plinary projects conducted during the academic
year 2012-13.
References
Nameera Akhtar. 1999. Acquiring basic word or-
der: evidence for data-driven learning of syn-
tactic structure. Journal of Child Language,
26:339?356.
Sophia Bello. 2012. Identifying indirect objects
in French: An elicitation task. In Proceedings
of the 2012 annual conference of the Canadian
Linguistic Association.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python:
Analyzing Text with the Natural Language
Toolkit. O?Reilly, Beijing.
Paul Boersma. 1997. How we learn variation, op-
tionality, and probability. Proceedings of the In-
stitute of Phonetic Sciences of the University of
Amsterdam, 21:43?58.
Harr Chen, S.R.K. Branavan, Regina Barzilay, and
David R. Karger. 2009. Content modeling using
latent permutations. Journal of Artificial Intel-
ligence Research, 36:129?163.
Michael Connor, Yael Gertner, Cynthia Fisher, and
Dan Roth. 2008. Baby srl: Modeling early lan-
guage acquisition. In Proceedings of the Twelfth
Conference on Computational Natural Language
Learning.
Michael Connor, Yael Gertner, Cynthia Fisher, and
Dan Roth. 2009. Minimally supervised model of
early language acquisition. In Proceedings of the
Thirteenth Conference on Computational Natu-
ral Language Learning.
Michael Connor, Yael Gertner, Cynthia Fisher, and
Dan Roth. 2010. Starting from scratch in se-
mantic role labelling. In Proceedings of ACL
2010.
Peter Culicover. 2013. Explaining syntax: repre-
sentations, structures, and computation. Oxford
University Press.
Jill de Villiers and Thomas Roeper. 1995. Bar-
riers, binding, and acquisition of the dp-np dis-
tinction. Language Acquisition, 4(1):73?104.
Holger Diessel and Michael Tomasello. 2001. The
acquisition of finite complement clauses in en-
glish: A corpus-based analysis. Cognitive Lin-
guistics, 12:1?45.
Annie Gagliardi and Jeffrey Lidz. 2010. Mor-
phosyntactic cues impact filler-gap dependency
resolution in 20- and 30-month-olds. In Poster
session of BUCLD35.
Annie Gagliardi, Tara M. Mease, and Jeffrey
Lidz. 2014. Discontinuous development
in the acquisition of filler-gap dependen-
cies: Evidence from 15- and 20-month-
olds. Harvard unpublished manuscript:
http://www.people.fas.harvard.edu/?gagliardi.
Yael Gertner and Cynthia Fisher. 2012. Predicted
errors in children?s early sentence comprehen-
sion. Cognition, 124:85?94.
Edward Gibson. 1998. Linguistic complexity:
Locality of syntactic dependencies. Cognition,
68(1):1?76.
Lila R. Gleitman. 1990. The structural sources of
verb meanings. Language Acquisition, 1:3?55.
Adele E. Goldberg, Devin Casenhiser, and Nitya
Sethuraman. 2004. Learning argument struc-
ture generalizations. Cognitive Linguistics,
14(3):289?316.
Sharon Goldwater and Tom Griffiths. 2007. A
fully Bayesian approach to unsupervised part-
of-speech tagging. In Proceedings of the 45th
Annual Meeting of the Association for Compu-
tational Linguistics.
Ray Jackendoff and Eva Wittenberg. in press.
What you can say without syntax: A hierarchy
of grammatical complexity. In Fritz Newmeyer
and Lauren Preston, editors, Measuring Linguis-
tic Complexity. Oxford University Press.
Aravind K. Joshi, K. Vijay Shanker, and David
Weir. 1990. The convergence of mildly context-
sensitive grammar formalisms. Technical Report
MS-CIS-90-01, Department of Computer and In-
formation Science, University of Pennsylvania.
1092
Dan Klein and Christopher D. Manning. 2004.
Corpus-based induction of syntactic structure:
Models of dependency and constituency. In Pro-
ceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics.
Tom Kwiatkowski, Sharon Goldwater, Luke S.
Zettlemoyer, and Mark Steedman. 2012. A
probabilistic model of syntactic and semantic
acquisition from child-directed utterances and
their meanings. In Proceedings of EACL 2012.
Brian MacWhinney. 2000. The CHILDES project:
Tools for analyzing talk. Lawrence Elrbaum As-
sociates, Mahwah, NJ, third edition.
David Marr. 1982. Vision. A Computational In-
vestigation into the Human Representation and
Processing of Visual Information. W.H. Free-
man and Company.
Letitia R. Naigles. 1990. Children use syntax to
learn verb meanings. The Journal Child Lan-
guage, 17:357?374.
Colin Phillips. 2010. Some arguments and non-
arguments for reductionist accounts of syntactic
phenomena. Language and Cognitive Processes,
28:156?187.
Martin Pickering and Guy Barry. 1991. Sentence
processing without empty categories. Language
and Cognitive Processes, 6(3):229?259.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih.
2008. The importance of syntactic parsing and
inference in semantic role labeling. Computa-
tional Linguistics, 34(2):257?287.
John R. Ross. 1967. Constraints on Variables in
Syntax. Ph.D. thesis, Massachusetts Institute of
Technology.
William Schuler. 2011. Effects of filler-gap de-
pendencies on working memory requirements for
parsing. In Proceedings of COGSCI, pages 501?
506, Austin, TX. Cognitive Science Society.
Amanda Seidl, George Hollich, and Peter W.
Jusczyk. 2003. Early understanding of subject
and object wh-questions. Infancy, 4(3):423?436.
Rushen Shi, Janet F. Werker, and James L. Mor-
gan. 1999. Newborn infants? sensitivity to per-
ceptual cues to lexical and grammatical words.
Cognition, 72(2):B11?B21.
Katherine Thatcher, Holly Branigan, Janet
McLean, and Antonella Sorace. 2008. Chil-
dren?s early acquisition of the passive: Evidence
from syntactic priming. In Proceedings of the
Child Language Seminar 2007, pages 195?205,
University of Reading.
Ivan Titov and Alexandre Klementiev. 2012.
Crosslingual induction of semantic roles. In Pro-
ceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-
2011).
Sandra R. Waxman and Amy E. Booth. 2001. See-
ing pink elephants: Fourteen-month-olds? inter-
pretations of novel nouns and adjectives. Cogni-
tive Psychology, 43:217?242.
Sylvia Yuan, Cynthia Fisher, and Jesse Snedeker.
2012. Counting the nouns: Simple structural
cues to verb meaning. Child Development,
83(4):1382?1399.
1093
In: R. Levy & D. Reitter (Eds.), Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2012), pages 51?60,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Connectionist-Inspired Incremental PCFG Parsing
Marten van Schijndel
The Ohio State University
vanschm@ling.ohio-state.edu
Andy Exley
University of Minnesota
exley@cs.umn.edu
William Schuler
The Ohio State University
schuler@ling.ohio-state.edu
Abstract
Probabilistic context-free grammars (PCFGs)
are a popular cognitive model of syntax (Ju-
rafsky, 1996). These can be formulated to
be sensitive to human working memory con-
straints by application of a right-corner trans-
form (Schuler, 2009). One side-effect of the
transform is that it guarantees at most a sin-
gle expansion (push) and at most a single re-
duction (pop) during a syntactic parse. The
primary finding of this paper is that this prop-
erty of right-corner parsing can be exploited to
obtain a dramatic reduction in the number of
random variables in a probabilistic sequence
model parser. This yields a simpler structure
that more closely resembles existing simple
recurrent network models of sentence compre-
hension.
1 Introduction
There may be a benefit to using insights from human
cognitive modelling in parsing. Evidence for in-
cremental processing can be seen in garden pathing
(Bever, 1970), close shadowing (Marslen-Wilson,
1975), and eyetracking studies (Tanenhaus et al,
1995; Allopenna et al, 1998; Altmann and Kamide,
1999), which show humans begin attempting to pro-
cess a sentence immediately upon receiving lin-
guistic input. In the cognitive science community,
this incremental interaction has often been mod-
elled using recurrent neural networks (Elman, 1991;
Mayberry and Miikkulainen, 2003), which utilize
a hidden context with a severely bounded repre-
sentational capacity (a fixed number of continuous
units or dimensions), similar to models of activation-
based memory in the prefrontal cortex (Botvinick,
2007), with the interesting possibility that the dis-
tributed behavior of neural columns (Horton and
Adams, 2005) may directly implement continuous
dimensions of recurrent hidden units. This paper
presents a refinement of a factored probabilistic se-
quence model of comprehension (Schuler, 2009) in
the direction of a recurrent neural network model
and presents some observed efficiencies due to this
refinement.
This paper will adopt an incremental probabilis-
tic context-free grammar (PCFG) parser (Schuler,
2009) that uses a right-corner variant of the left-
corner parsing strategy (Aho and Ullman, 1972)
coupled with strict memory bounds, as a model of
human-like parsing. Syntax can readily be approxi-
mated using simple PCFGs (Hale, 2001; Levy, 2008;
Demberg and Keller, 2008), which can be easily
tuned (Petrov and Klein, 2007). This paper will
show that this representation can be streamlined to
exploit the fact that a right-corner parse guarantees
at most one expansion and at most one reduction can
take place after each word is seen (see Section 2.2).
The primary finding of this paper is that this prop-
erty of right-corner parsing can be exploited to ob-
tain a dramatic reduction in the number of random
variables in a probabilistic sequence model parser
(Schuler, 2009) yielding a simpler structure that
more closely resembles connectionist models such
as TRACE (McClelland and Elman, 1986), Short-
list (Norris, 1994; Norris and McQueen, 2008), or
recurrent models (Elman, 1991; Mayberry and Mi-
ikkulainen, 2003) which posit functional units only
for cognitively-motivated entities.
The rest of this paper is structured as follows:
Section 2 gives the formal background of the right-
corner parser transform and probabilistic sequence
51
model parsing. The simplification of this model is
described in Section 3. A discussion of the interplay
between cognitive theory and computational mod-
elling in the resulting model may be found in Sec-
tion 4. Finally, Section 5 demonstrates that such
factoring also yields large benefits in the speed of
probabilistic sequence model parsing.
2 Background
2.1 Notation
Throughout this paper, PCFG rules are defined over
syntactic categories subscripted with abstract tree
addresses (c??). These addresses describe a node?s
location as a path from a given ancestor node. A 0
on this path represents a leftward branch and a 1 a
rightward branch. Positions within a tree are repre-
sented by subscripted ? and ? so that c?0 is the left
child of c? and c?1 is the right child of c?. The set of
syntactic categories in the grammar is denoted by C.
Finally, J?K denotes an indicator probability which
is 1 if ? and 0 otherwise.
2.2 Right-Corner Parsing
Parsers such as that of Schuler (2009) model hierar-
chically deferred processes in working memory us-
ing a coarse analogy to a pushdown store indexed
by an embedding depth d (to a maximum depth D).
To make efficient use of this store, a CFG G must
be transformed using a right-corner transform into
another CFG G? with no right recursion. Given an
optionally arc-eager attachment strategy, this allows
the parser to clear completed parse constituents from
the set of incomplete constituents in working mem-
ory much earlier than with a conventional syntactic
structure. The right-corner transform operates de-
terministically over a CFG following three mapping
rules:
c? ? c?0 c?1 ? G
c?/c?1 ? c?0 ? G?
(1)
c?? ? c??0 c??1 ? G, c? ? C
c?/c??1 ? c?/c?? c??0 ? G?
(2)
c?? ? x?? ? G, c? ? C
c? ? c?/c?? c?? ? G?
(3)
A bottom-up incremental parsing strategy com-
bined with the way the right-corner transform pulls
each subtree into a left-expanding hierarchy ensures
at most a single expansion (push) will occur at
any given observation. That is, each new observa-
tion will be the leftmost leaf of a right-expanding
subtree. Additionally, by reducing multiply right-
branching subtrees to single rightward branches, the
transform also ensures that at most a single reduc-
tion (pop) will take place at any given observation.
Schuler et al (2010) show near complete cover-
age of the Wall Street Journal portion of the Penn
Treebank (Marcus et al, 1993) can be achieved with
a right-corner incremental parsing strategy using no
more than four incomplete contituents (deferred pro-
cesses), in line with recent estimates of human work-
ing memory capacity (Cowan, 2001).
Section 3 will show that, in addition to being de-
sirable for bounded working memory restrictions,
the single expansion/reduction guarantee reduces
the search space between words to only two decision
points ? whether to expand and whether to reduce.
This allows rapid processing of each candidate parse
within a sequence modelling framework.
2.3 Model Formulation
This transform is then extended to PCFGs and inte-
grated into a sequence model parser. Training on
an annotated corpus yields the probability of any
given syntactic state executing an expansion (creat-
ing a syntactic subtree) or a reduction (completing
a syntactic subtree) to transition from every suffi-
ciently probable (in this sense active) hypothesis in
the working memory store.
The probability of the most likely sequence of
store states q?1..D1..T can then be defined as the prod-
uct of nonterminal ?Q, preterminal ?P,d, and termi-
nal ?X factors:
q?1..D1..T
def= argmax
q1..D1..T
T
?
t=1
P?Q(q1..Dt | q1..Dt?1 pt?1)
? P?P,d? (pt | b
d?
t ) ? P?X (xt | pt) (4)
where all incomplete constituents qdt are factored
into active adt and awaited bdt components:
qdt
def= adt /bdt (5)
and d? determines the deepest non-empty incomplete
constituent of q1..Dt :
52
d? def= max{d | qdt 6= ???} (6)
The preterminal model ?P,d denotes the expecta-
tion of a subtree containing a given preterminal, ex-
pressed in terms of side- and depth-specific gram-
mar rules P?Gs,d(c? ? c?0 c?1) and expected counts
of left progeny categories E?G?,d(c?
?? c?? ...) (see
Appendix A):
P?P,d(c?? | c?)
def= E?G?,d(c?
?? c?? ...)
?
?
x??
P?GL,d(c?? ? x??) (7)
and the terminal model ?X is simply:
P?X (x? | c?)
def=
P?G(c? ? x?)
?
x? P?G(c? ? x?)
(8)
The Schuler (2009) nonterminal model ?Q is
computed from a depth-specific store element
model ?Q,d and a large final state model ?F,d:
P?Q(q1..Dt | q1..Dt?1 pt?1)
def=
?
f1..Dt
D
?
d=1
P?F,d(fdt | fd+1t qdt?1 qd?1t?1 )
? P?Q,d(qdt | fd+1t fdt qdt?1 qd?1t ) (9)
After each time step t and depth d, ?Q generates
a set of final states to generate a new incomplete
constituent qdt . These final states fdt are factored
into categories cfdt and boolean variables (0 or 1)
encoding whether a reduction has taken place at
depth d and time step t. The depth-specific final state
model ?F,d gives the probability of generating a final
state fdt from the preceding qdt and qd?1t which is the
probability of executing a reduction or consolidation
of those incomplete constituents:
P?F,d(fdt | fd+1t qdt?1 qd?1t?1 )
def=
{
if fd+1t = ??? : Jfdt = 0K
if fd+1t 6= ??? : P?F,d,R(fdt | qdt?1 qd?1t?1 )
(10)
With these depth-specific fdt in hand, the model can
calculate the probabilities of each possible qdt for
each d and t based largely on the probability of tran-
sitions (?Q,d,T ) and expansions (?Q,d,E) from the in-
complete constituents at the previous time step:
P?Q,d(qdt | fd+1t fdt qdt?1 qd?1t?1 )
def=
?
?
?
if fd+1t = ???, fdt = ??? : Jqdt = qdt?1K
if fd+1t 6= ???, fdt = ??? : P?Q,d,T (qdt | fd+1t fdt qdt?1 qd?1t )
if fd+1t 6= ???, fdt 6= ??? : P?Q,d,E (qdt | qd?1t )
(11)
This model is shown graphically in Figure 1.
The probability distributions over reductions
(?F,d,R), transitions (?Q,d,T ) and expansions
(?Q,d,E) are then defined, also in terms of side- and
depth-specific grammar rules P?Gs,d(c? ? c?0 c?1)
and expected counts of left progeny cate-
gories E?G?,d(c?
?? c?? ...) (see Appendix A):
P?Q,d,T (qdt | fd+1t fdt qdt?1qd?1t )
def=
{
if fdt 6= ???: P?Q,d,A(qdt | qd?1t fdt )
if fdt = ???: P?Q,d,B (qdt | qdt?1fd+1t )
(12)
P?F,d,R(fdt | fd+1t qdt?1qd?1t?1 )
def=
{
if cfd+1t 6=xt : Jf
d
t = ???K
if cfd+1t =xt : P?F,d,R(f
d
t | qdt?1qd?1t?1 )
(13)
P?Q,d,E (c??/c??? | /c?)
def=
E?G?,d(c?
?? c?? ...) ? Jx?? = c??? = c??K (14)
The subcomponent models are obtained by ap-
plying the transform rules to all possible trees pro-
portionately to their probabilities and marginalizing
over all constituents that are not used in the models:
? for active transitions (from Transform Rule 1):
P?Q,d,A(c??/c??1 | /c? c??0)
def=
E?G?,d(c?
?? c?? ...) ? P?GL,d(c?? ? c??0 c??1)
E?G?,d(c?
+? c??0 ...)
(15)
53
p1
x1
q11
q21
q31
q41
p2
x2
q12
q22
q32
q42
p3
x3
q13
q23
q33
q43
p4
x4
q14
q24
q34
q44
p5
x5
q15
q25
q35
q45
p6
x6
q16
q26
q36
q46
p7
x7
q17
q27
q37
q47
f12
f22
f32
f42
f13
f23
f33
f43
f14
f24
f34
f44
f15
f25
f35
f45
f16
f26
f36
f46
f17
f27
f37
f47
=
DT
=
the
=
0,D
T
=
NP
/N
N
=
NN
=
fun
d
=
0,N
P
=
S/V
P
=
VB
=
bo
ug
ht
=
0,V
B = S
/V
P
=
VP
/N
P
=
DT
=
tw
o
=
1,D
T
=
S/V
P
=
VP
/N
N
=
JJ
=
re
gio
na
l
=
1,J
J
=
S/V
P
=
VP
/N
N
=
NN
=
ban
ks
=
1,V
P = S
/RB
=
RB
=
tod
ay
Figure 1: Schuler (2009) Sequence Model
? for awaited transitions (Transform Rule 2):
P?Q,d,B (c?/c??1 | c??/c?? c??0)
def=
Jc? = c??K ?
P?GR,d(c?? ? c??0 c??1)
E?G?,d(c??
0? c??0 ...)
(16)
? for reductions (from Transform Rule 3):
P?F,d,R(c??,1 | /c? c???/ )
def=
Jc?? = c???K ?
E?G?,d(c?
0? c?? ...)
E?G?,d(c?
?? c?? ...)
(17)
P?F,d,R(c??,0 | /c? c???/ )
def=
Jc?? = c???K ?
E?G?,d(c?
+? c?? ...)
E?G?,d(c?
?? c?? ...)
(18)
3 Simplified Model
As seen in the previous section, the right-corner
parser of Schuler (2009) makes the center embed-
ding depth explicit and each memory store element
is modelled as a combination of an active and an
awaited component. Each input can therefore either
increase (during an expansion) or decrease (during
a reduction) the store of incomplete constituents or
it can alter the active or awaited component of the
deepest incomplete constituent (the affectable ele-
ment). Alterations of the awaited component of the
affectable element can be thought of as the expan-
sion and immediate reduction of a syntactic con-
stituent. The grammar models transitions in the ac-
tive component implicitly, so these are conceptual-
ized as consisting of neither an expansion nor a re-
duction.
Removing some of the variables in this model re-
sults in one that looks much more like a neural net-
work (McClelland and Elman, 1986; Elman, 1991;
Norris, 1994; Norris and McQueen, 2008) in that
all remaining variables have cognitive correllates ?
in particular, they correspond to incomplete con-
stituents in working memory ? while still maintain-
ing the ability to explicitly represent phrase struc-
ture. This section will demonstrate how it is possi-
ble to exploit this to obtain a large reduction in the
number of modelled random variables.
In the Schuler (2009) sequence model, eight ran-
dom variables are used to model the hidden states
at each time step (see Figure 1). Half of these vari-
ables are joint consisting of two further (active and
awaited) constituent variables, while the other half
are merely over intermediate final states. Although
the entire store is carried from time step to time
step, only one memory element is affectable at any
one time, and this element may be reduced zero or
54
one times (using an intermediate final state), and ex-
panded zero or one times (using an incomplete con-
stituent state), yielding four possible combinations.
This means the model only actually needs one of its
intermediate final states.
The transition model ?Q can therefore be simpli-
fied with terms ?F,d for the probability of expand-
ing the incomplete constituent at d, and terms ?A,d
and ?B,d for reducing the resulting constituent
(defining the active and awaited components of
a new incomplete constituent), along with terms
for copying incomplete constituents above this af-
fectable element, and for emptying the elements be-
low it:
P?Q(q1..Dt | q1..Dt?1 pt?1)
def=P?F,d? (?+? | bd
?
t?1 pt?1) ? P?A,d? (??? | b
d??1
t?1 ad
?
t?1)
? Jad??1t =ad
??1
t?1 K ? P?B,d??1(b
d??1
t | bd
??1
t?1 ad
?
t?1)
? Jq1..d??2t =q1..d
??2
t?1 K ? Jqd
?..D
t = ???K
+P?F,d? (?+? | bd
?
t?1 pt?1) ? P?A,d? (ad
?
t | bd
??1
t?1 ad
?
t?1)
? P?B,d? (b
d?
t | ad
?
t ad
?+1
t?1 )
? Jq1..d??1t =q1..d
??1
t?1 K ? Jqd
?+1..D
t = ???K
+P?F,d? (??? | bd
?
t?1 pt?1) ? P?A,d? (??? | bd
?
t?1 pt?1)
? Jad?t =ad
?
t?1K ? P?B,d? (b
d?
t | bd
?
t?1 pt?1)
? Jq1..d??1t =q1..d
??1
t?1 K ? Jqd
?+1..D
t = ???K
+P?F,d? (??? | bd
?
t?1 pt?1) ? P?A,d? (a
d?+1
t | bd
?
t?1 pt?1)
? P?B,d? (b
d?+1
t | ad
?+1
t pt?1)
? Jq1..d?t =q1..d
?
t?1 K ? Jqd
?+2..D
t = ???K (19)
The first element of the sum in Equation 19 com-
putes the probability of a reduction with no expan-
sion (decreasing d?). The second corresponds to the
probability of a store undergoing neither an expan-
sion nor a reduction (a transition to a new active con-
stituent at the same embedding depth). In the third
is the probability of an expansion and a reduction
(a transition among awaited constituents at the same
embedding depth). Finally, the last term yields the
probability of an expansion without a reduction (in-
creasing d?).
From Equation 19 it may be seen that the unaf-
fected store elements of each time step are main-
tained sans change as guaranteed by the single-
reduction feature of the right-corner parser. This re-
sults in a large representational economy by mak-
ing the majority of store state decisions determinis-
tic. This representational economy will later trans-
late into computational efficiencies (see section 5).
In this sense, cognitive modelling contributes to a
practical speed increase.
Since the bulk of the state remains the same,
the recognizer can access the affectable variable
and operate solely over the transition possibili-
ties from that variable to calculate the distribu-
tion over store states for the next time step to ex-
plore. Reflecting this change, the hidden states
now model a single final-state variable (f) for
results of the expansion decision, and the af-
fectable variable resulting from the reduction de-
cision (both its active (a) and awaited (b) cate-
gories), as well as the preterminal state (p) defined
in the previous section. These models are again ex-
pressed in terms of side- and depth-specific grammar
rules P?Gs,d(c? ? c?0 c?1) and expected counts of
left progeny categories E?G?,d(c?
?? c?? ...) (see
Appendix A).
Expansion probabilities are modelled as a binary
decision depending on whether or not the awaited
component of the affectable variable c? is likely to
expand immediately into an anticipated pretermi-
nal c?? (resulting in a non-empty final state: ?+?) or
if intervening embeddings are necessary given the
affectable active component (yielding no final state:
???):
P?F,d(f | c? c??)
def=
?
?
?
?
?
?
?
?
?
if f= ?+? :
E?G?,d (c?
0
?c?? ...)
E?G?,d (c?
?
?c?? ...)
if f= ??? :
E?G?,d (c?
+
?c?? ...)
E?G?,d (c?
?
?c?? ...)
(20)
The active component category c?? is defined as de-
pending on the category of the awaited component
above it c? and its left-hand child c??0:
P?A,d(c?? | c? c??0)
def=
E?G?,d (c?
1
?c??0 ...)
E?G?,d (c?
+
?c??0 ...)
? Jc??= ???K
+
E?G?,d (c?
+
?c?? ...)?P?GL,d (c???c??0 ...)
E?G?,d (c?
+
?c??0 ...)
(21)
The awaited component category c?1 is defined as
55
depending on the category of its parent c? and the
preceding sibling c?0:
P?B,d(c?1 | c? c?0)
def=
P?GR,d (c??c?0 c?1)
E?G?,d (c?
1
?c?0 ...)
(22)
Both of these make sense given the manner in which
the right-corner parser shifts dependencies to the left
down the tree in order to obtain incremental infor-
mation about upcoming constituents.
3.1 Graphical Representation
In order to be represented graphically, the working
memory store ?Q is factored into a single expansion
term ?F and a product of depth-specific reduction
terms ?Q,d:
P?Q(q1..Dt | q1..Dt?1 pt?1)
def=
?
ft
P?F (ft | q1..Dt?1 )
?
D
?
d=1
P?Q,d(qdt | q1..Dt?1 pt?1 ft qd+1t ) (23)
and the depth-specific reduction model ?Q,d is fac-
tored into individual decisions over each random
variable:
P?Q,d(qdt | q1..Dt?1 pt?1 ft qd+1t )
def=
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
if qd+1t = ???, ft 6= ???, d=d??1 :
Jadt =adt?1K ? P?B,d(bdt | bdt?1 ad+1t?1 )
if qd+1t = ???, ft 6= ???, d=d? :
P?A,d(adt | bd?1t?1 adt?1) ? P?B,d(bdt | adt adt?1)
if qd+1t = ???, ft= ???, d=d? :
Jadt =adt?1K ? P?B,d(bdt | bdt?1 pt?1)
if qd+1t = ???, ft= ???, d=d?+1 :
P?A,d(adt | bd?1t?1 pt?1) ? P?B,d(bdt | adt pt?1)
if qd+1t 6= ??? : Jqdt =qdt?1K
otherwise : Jqdt = ???K
(24)
This dependency structure is represented graphically
in Figure 2.
The first conditional in Equation 24 checks
whether the input causes a reduction but no expan-
sion (completing a subtree parse). In this case, d? is
reduced from the previous t, and the relevant qdt?1 is
copied to qdt except the awaited constituent is altered
to reflect the completion of its preceding awaited
subtree. In the second case, the parser makes an
active transition as it completes a left subtree and
begins exploring the right subtree. The third case
is similar to the first except it transitions between
two like depths (awaited transition), and depends on
the preterminal just seen to contrive a new subtree
to explore. In the fourth case, d? is incremented as
another incomplete constituent opens up in working
memory. The final two cases simply update the un-
affected store states to reflect their previous states at
time t? 1.
4 Discussion
This factoring of redundant hidden states out of the
Schuler (2009) probabilistic sequence model shows
that cognitive modelling can more closely approx-
imate a simple recurrent network model of lan-
guage processing (Elman, 1991). Probabilistic se-
quence model parsers have previously been mod-
elled with random variables over incomplete con-
stituents (Schuler, 2009). In the current implementa-
tion, each variable can be thought of as a bank of ar-
tificial neurons. These artificial neurons inhibit one
another through the process of normalization. Con-
versely, they activate artificial neurons at subsequent
time steps by contributing probability mass through
the transformed grammar. This point was made by
Norris and McQueen (2008) with respect to lexical
access; this model extends it to parsing.
Recurrent networks can parse simple sentences
but run into problems when running over more com-
plex datasets. This limitation comes from the unsu-
pervised methods typically used to train them, which
have difficulty scaling to sufficiently large training
sets for more complex constructions. The approach
described in this paper uses a hidden context simi-
lar to that of a recurrent network to inform the pro-
gression of the parse, except that the context is in
terms of random variables with distributions over a
set of explicit syntactic categories. By framing the
variable domains in a linguistically-motivated fash-
ion, the problem of acquisition can be divested from
the problem of processing. This paper then uses the
semi-supervised grammar training of Petrov et al
(2006) in order to develop a simple, accurate model
for broad-coverage parsing independent of scale.
56
p1
x1
q11
q21
q31
q41
p2
x2
q12
q22
q32
q42
p3
x3
q13
q23
q33
q43
p4
x4
q14
q24
q34
q44
p5
x5
q15
q25
q35
q45
p6
x6
q16
q26
q36
q46
p7
x7
q17
q27
q37
q47
f2 f3 f4 f5 f6 f7 f8=D
T
=the
=NP/
NN
=NN
=fund
=+
=S/V
P
=VB
=bou
ght
=S/V
P
=VP/
NP
=DT
=two
=S/V
P
=VP/
NN
=JJ
=re
giona
l
=S/V
P
=VP/
NN
=NN
=ban
ks
=+
=S/R
B
=RB
=toda
y
=+
Figure 2: Parse using Simplified Model
Like Schuler (2009), the incremental parser dis-
cussed here operates in O(n) time where n is the
length of the input. Further, by its incremental na-
ture, this parser is able to run continuously on a
stream of input, which allows any other processes
dependent on the input (such as discourse integra-
tion) to run in parallel regardless of the length of the
input.
5 Computational Benefit
Due to the decreased number of decisions required
by this simplified model, it is substantially faster
than previous similar models. To test this speed in-
crease, the simplified model was compared with that
of Schuler (2009). Both parsers used a grammar that
had undergone 5 iterations of the Petrov et al (2006)
split-merge-smooth algorithm as found to be opti-
mal by Petrov and Klein (2007), and both used a
beam-width of 500 elements. Sections 02-21 of the
Wall Street Journal Treebank were used in training
the grammar induction for both parsers according to
Petrov et al (2006), and Section 23 was used for
evaluation. No tuning was done as part of the trans-
form to a sequence model. Speed results can be seen
in Table 1. While the speed is not state-of-the-art in
the field of parsing at large, it does break new ground
for factored sequence model parsers.
To test the accuracy of this parser, it was com-
pared using varying beam-widths to the Petrov and
Klein (2007) and Roark (2001) parsers. With the
exception of the Roark (2001) parser, all parsers
used 5 iterations of the Petrov et al (2006) split-
System Sec/Sent
Schuler 2009 74
Current Model 12
Table 1: Speed comparison with an unfactored proba-
bilistic sequence model using a beam-width of 500 ele-
ments
System P R F
Roark 2001 86.6 86.5 86.5
Current Model (500) 86.6 87.3 87.0
Current Model (2000) 87.8 87.8 87.8
Current Model (5000) 87.8 87.8 87.8
Petrov Klein (Binary) 88.1 87.8 88.0
Petrov Klein (+Unary) 88.3 88.6 88.5
Table 2: Accuracy comparison with state-of-the-art mod-
els. Numbers in parentheses are number of parallel acti-
vated hypotheses
merge-smooth algorithm, and the training and test-
ing datasets remained the same. These results may
be seen in Table 2. Note that the Petrov and Klein
(2007) parser allows unary branching within the
phrase structure, which is not well-defined under the
right-corner transform. To obtain a fair comparison,
it was also run with strict binarization. The cur-
rent approach achieves comparable accuracy to the
Petrov and Klein (2007) parser assuming a strictly
binary-branching phrase structure.
57
6 Conclusion
The primary goal of this paper was to demonstrate
that a cognitively-motivated factoring of an exist-
ing probabilistic sequence model parser (Schuler,
2009) is not only more attractive from a modelling
perspective but also more efficient. Such factor-
ing yields a much slimmer model where every vari-
able has cognitive correlates to working memory el-
ements. This also renders several transition prob-
abilities deterministic and the ensuing representa-
tional economy leads to a 5-fold increase in pars-
ing speed. The results shown here suggest cognitive
modelling can lead to computational benefits.
References
Alfred V. Aho and Jeffery D. Ullman. 1972. The The-
ory of Parsing, Translation and Compiling; Volume. I:
Parsing. Prentice-Hall, Englewood Cliffs, New Jersey.
P. D. Allopenna, J. S. Magnuson, and M. K. Tanenhaus.
1998. Tracking the time course of spoken word recog-
nition using eye movements: evidence for continuous
mapping models. Journal of Memory and Language,
38:419?439.
G. T. M. Altmann and Y. Kamide. 1999. Incremental
interpretation at verbs: restricting the domain of sub-
sequent reference. Cognition, 73:247?264.
Richard Bellman. 1957. Dynamic Programming.
Princeton University Press, Princeton, NJ.
Thomas G. Bever. 1970. The cognitive basis for lin-
guistic structure. In J. ?R. Hayes, editor, Cognition and
the Development of Language, pages 279?362. Wiley,
New York.
Matthew Botvinick. 2007. Multilevel structure in behav-
ior and in the brain: a computational model of fusters
hierarchy. Philosophical Transactions of the Royal So-
ciety, Series B: Biological Sciences, 362:1615?1626.
Nelson Cowan. 2001. The magical number 4 in short-
term memory: A reconsideration of mental storage ca-
pacity. Behavioral and Brain Sciences, 24:87?185.
Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109(2):193?210.
Jeffrey L. Elman. 1991. Distributed representations,
simple recurrent networks, and grammatical structure.
Machine Learning, 7:195?225.
John Hale. 2001. A probabilistic earley parser as a psy-
cholinguistic model. In Proceedings of the Second
Meeting of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 159?166,
Pittsburgh, PA.
Jonathan C Horton and Daniel L Adams. 2005. The cor-
tical column: a structure without a function. Philo-
sophical Transactions of the Royal Society of London
- Series B: Biological Sciences, 360(1456):837?862.
Daniel Jurafsky. 1996. A probabilistic model of lexical
and syntactic access and disambiguation. Cognitive
Science: A Multidisciplinary Journal, 20(2):137?194.
Roger Levy. 2008. Expectation-based syntactic compre-
hension. Cognition, 106(3):1126?1177.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
William D. Marslen-Wilson. 1975. Sentence per-
ception as an interactive parallel process. Science,
189(4198):226?228.
Marshall R. Mayberry, III and Risto Miikkulainen. 2003.
Incremental nonmonotonic parsing through semantic
self-organization. In Proceedings of the 25th Annual
Conference of the Cognitive Science Society, pages
798?803, Boston, MA.
James L. McClelland and Jeffrey L. Elman. 1986. The
trace model of speech perception. Cognitive Psychol-
ogy, 18:1?86.
Dennis Norris and James M. McQueen. 2008. Shortlist
b: A bayesian model of continuous speech recognition.
Psychological Review, 115(2):357?395.
Dennis Norris. 1994. Shortlist: A connectionist model
of continuous speech recognition. Cognition, 52:189?
234.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
NAACL HLT 2007, pages 404?411, Rochester, New
York, April. Association for Computational Linguis-
tics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 44th
Annual Meeting of the Association for Computational
Linguistics (COLING/ACL?06).
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
William Schuler, Samir AbdelRahman, Tim Miller, and
Lane Schwartz. 2010. Broad-coverage incremental
parsing using human-like memory constraints. Com-
putational Linguistics, 36(1):1?30.
William Schuler. 2009. Parsing with a bounded stack
using a model-based right-corner transform. In Pro-
ceedings of NAACL/HLT 2009, NAACL ?09, pages
344?352, Boulder, Colorado. Association for Compu-
tational Linguistics.
58
Michael K. Tanenhaus, Michael J. Spivey-Knowlton,
Kathy M. Eberhard, and Julie E. Sedivy. 1995. Inte-
gration of visual and linguistic information in spoken
language comprehension. Science, 268:1632?1634.
A Grammar Formulation
Given D memory elements indexed by d (see Sec-
tion 2.2) and a PCFG ?G, the probability ?(k)Ts,d of a
tree rooted at a left or right sibling s ? {L,R} of
category c? ? C requiring d ? 1..D memory ele-
ments is defined recursively over paths of increasing
length k:
P?(0)Ts,d
(1 | c?) def= 0 (25)
P?(k)TL,d
(1 | c?) def=
?
x?
P?G(c? ? x?)
+
?
c?0,c?1
P?G(c? ? c?0 c?1)
? P?(k?1)TL,d (1 | c?0) ? P?(k?1)TR,d(1 | c?1)
(26)
P?(k)TR,d
(1 | c?) def=
?
x?
P?G(c? ? x?)
+
?
c?0,c?1
P?G(c? ? c?0 c?1)
? P?(k?1)TL,d+1(1 | c?0) ? P?(k?1)TR,d(1 | c?1)
(27)
Note that the center embedding depth d increases
only for left children of right children. This is be-
cause in a binary branching structure, center embed-
dings manifest as zigzags. Since the model is also
sensitive to the depth d of each decomposition, the
side- and depth-specific probabilities of ?GL,d and
?GR,d are defined as follows:
P?GL,d(c? ? x?)
def=
P?G(c? ? x?)
P?(?)TL,d
(1 | c?)
(28)
P?GR,d(c? ? x?)
def=
P?G(c? ? x?)
P?(?)TR,d
(1 | c?)
(29)
P?GL,d(c? ? c?0 c?1)
def= P?G(c? ? c?0 c?1)
? P?(?)TL,d(1 | c?0) ? P?(?)TR,d(1 | c?1)
? P?(?)TL,d(1 | c?)
?1 (30)
P?GR,d(c? ? c?0 c?1)
def= P?G(c? ? c?0 c?1)
? P?(?)TL,d+1(1 | c?0) ? P?(?)TR,d(1 | c?1)
? P?(?)TR,d(1 | c?)
?1 (31)
The model will also need an expected count
E?G?,d(c?
?? c?? ...) of the given child constituent
c?? dominating a prefix of constituent c?. Expected
versions of these counts may later be used to derive
probabilities of memory store state transitions (see
Sections 2.3, 3).
E?G?,d(c?
0? c? ...) def=
?
x?
P?GR,d(c? ? x?)
(32)
E?G?,d(c?
1? c?0 ...) def=
?
c?1
P?GR,d(c? ? c?0 c?1)
(33)
E?G?,d(c?
k? c??0 ...) def=
?
c??
E?G?,d(c?
k?1? c?? ...)
?
?
c??1
P?GL,d(c?? ? c??0 c??1)
(34)
E?G?,d(c?
?? c?? ...) def=
?
?
k=0
E?G?,d(c?
k? c?? ...)
(35)
E?G?,d(c?
+? c?? ...) def=
?
?
k=1
E?G?,d(c?
k? c?? ...)
(36)
Equation 32 gives the probability of a constituent
appearing as an observation, and Equation 33 gives
the probability of a constituent appearing as a left
59
child. Equation 34 extends the previous two equa-
tions to account for a constituent appearing at an ar-
bitrarily deep embedded path of length k. Taking
the sum of all k path lengths (as in Equation 35)
allows the model to account for constituents any-
where in the left progeny of the dominated subtree.
Similarly, Equation 36 gives the expectation that the
constituent is non-immediately dominated by c?. In
practice the infinite sum is estimated to some con-
stant K using value iteration (Bellman, 1957).
60
Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 37?46,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
An Analysis of Memory-based Processing Costs using Incremental
Deep Syntactic Dependency Parsing?
Marten van Schijndel
The Ohio State University
vanschm@ling.osu.edu
Luan Nguyen
University of Minnesota
lnguyen@cs.umn.edu
William Schuler
The Ohio State University
schuler@ling.osu.edu
Abstract
Reading experiments using naturalistic
stimuli have shown unanticipated facili-
tations for completing center embeddings
when frequency effects are factored out.
To eliminate possible confounds due to
surface structure, this paper introduces a
processing model based on deep syntac-
tic dependencies. Results on eye-tracking
data indicate that completing deep syntac-
tic embeddings yields significantly more
facilitation than completing surface em-
beddings.
1 Introduction
Self-paced reading and eye-tracking experiments
have often been used to support theories about
inhibitory effects of working memory operations
in sentence processing (Just and Carpenter, 1992;
Gibson, 2000; Lewis and Vasishth, 2005), but it
is possible that many of these effects can be ex-
plained by frequency (Jurafsky, 1996; Hale, 2001;
Karlsson, 2007). Experiments on large naturalis-
tic text corpora (Demberg and Keller, 2008; Wu et
al., 2010; van Schijndel and Schuler, 2013) have
shown significant memory effects at the ends of
center embeddings when frequency measures have
been included as separate factors, but these mem-
ory effects have been facilitatory rather than in-
hibitory.
Some of the memory-based measures that pro-
duce these facilitatory effects (Wu et al, 2010; van
Schijndel and Schuler, 2013) are defined in terms
of initiation and integration of connected compo-
nents of syntactic structure,1 with the presumption
?*Thanks to Micha Elsner and three anonymous review-
ers for their feedback. This work was funded by an Ohio State
University Department of Linguistics Targeted Investment
for Excellence (TIE) grant for collaborative interdisciplinary
projects conducted during the academic year 2012?13.
1Graph theoretically, the set of connected components
that referents that belong to the same connected
component may cue one another using content-
based features, while those that do not must rely
on noisier temporal features that just encode how
recently a referent was accessed. These measures,
based on left-corner parsing processes (Johnson-
Laird, 1983; Abney and Johnson, 1991), abstract
counts of unsatisfied dependencies from noun or
verb referents (Gibson, 2000) to cover all syntactic
dependencies, motivated by observations of Dem-
berg and Keller (2008) and Kwon et al (2010) of
the inadequacies of Gibson?s narrower measure.
But these experiments use naturalistic stimuli
without constrained manipulations and therefore
might be susceptible to confounds. It is possible
that the purely phrase-structure-based connected
components used previously may ignore some in-
tegration costs associated with filler-gap construc-
tions, making them an unsuitable generalization of
Gibson-style dependencies. It is also possible that
the facilitatory effect for integration operations in
naturally-occurring stimuli may be driven by syn-
tactic center embeddings that arise from modifiers
(e.g. The CEO sold [[the shares] of the com-
pany]), which do not require any dependencies
to be deferred, but which might be systematically
under-predicted by frequency measures, produc-
ing a confound with memory measures when fre-
quency measures are residualized out.
In order to eliminate possible confounds due to
exclusion of unbounded dependencies in filler-gap
constructions, this paper evaluates a processing
model that calculates connected components on
deep syntactic dependency structures rather than
surface phrase structure trees. This model ac-
counts unattached fillers and gaps as belonging
to separate connected components, and therefore
performs additional initiation and integration op-
of a graph ?V, E? is the set of maximal subsets of
it {?V1, E1?, ?V2, E2?, ...} such that any pair of vertices in
each Vi can be connected by edges in the corresponding Ei.
37
a) Noun Phrase
Relative Clause
Sentence w. Gap
Verb Phrase w. Gap
Sentence w. Gap
millionsstole
say
officials
who
Noun Phrase
personthe
b)
i1
i2 i3
i4
i5
i6
i7
1
2
1
2
1
1
say
officials
stole
who
person
millionsthe
0
0
0
0
0
00
Figure 1: Graphical representation of (a) a single
connected component of surface syntactic phrase
structure corresponding to (b) two connected com-
ponents of deep syntactic dependency structure for
the noun phrase the person who officials say stole
millions, prior to the word say. Connections es-
tablished prior to the word say are shown in black;
subsequent connections are shown in gray.
erations in filler-gap constructions as hypothesized
by Gibson (2000) and others. Then, in order to
control for possible confounds due to modifier-
induced center embedding, this refined model is
applied to two partitions of an eye-tracking cor-
pus (Kennedy et al, 2003): one consisting of sen-
tences containing only non-modifier center em-
beddings, in which dependencies are deferred, and
the other consisting of sentences containing no
center embeddings or containing center embed-
dings arising from attachment of final modifiers,
in which no dependencies are deferred. Processing
this partitioned corpus with deep syntactic con-
nected components reveals a significant increase
in facilitation in the non-modifier partition, which
lends credibility to the observation of negative
integration cost in processing naturally-occurring
sentences.
2 Connected Components
The experiments described in this paper evalu-
ate whether inhibition and facilitation in reading
correlate with operations in a hierarchic sequen-
tial prediction model that initiate and integrate
connected components of hypothesized syntactic
structure during incremental parsing. The model
used in these experiments refines previous con-
nected component models by allowing fillers and
gaps to occur in separate connected components
of a deep syntactic dependency graph (Mel?c?uk,
1988; Kintsch, 1988), even when they belong to
the same connected component when defined on
surface structure.
For example, the surface syntactic phrase struc-
ture and deep syntactic dependency structure for
the noun phrase the person who officials say stole
millions are shown in Figure 1.2 Notice that af-
ter the word officials, there is only one connected
component of surface syntactic phrase structure
(from the root noun phrase to the verb phrase with
gap), but two disjoint connected components of
deep syntactic dependency structure (one ending
at i3, and another at i5). Only the deep syntactic
dependency structure corresponds to familiar (Just
and Carpenter, 1992; Gibson, 1998) notions of
how memory is used to store deferred dependen-
cies in filler-gap constructions. The next section
will describe a generalized categorial grammar,
which (i) can be viewed as context-free, to seed a
latent-variable probabilistic context-free grammar
to accurately derive parses of filler-gap construc-
tions, and (ii) can be viewed as a deep syntactic
dependency grammar, defining dependencies for
connected components in terms of function appli-
cations.
3 Generalized Categorial Grammar
In order to evaluate memory effects for hypothe-
sizing unbounded dependencies between referents
of fillers and referents of clauses containing gaps,
a memory-based processor must define connected
components in terms of deep syntactic dependen-
cies (including unbounded dependencies) rather
than in terms of surface syntactic phrase structure
trees. To do this, at least some phrase structure
edges must be removed from the set of connec-
tions that define a connected component.
Because these unbounded dependencies are not
represented locally in the original Treebank for-
mat, probabilities for operations on these modified
2Following Mel?c?uk (1988) and Kintsch (1988),
the graphical dependency structure adopted here uses
positionally-defined labels (?0? for the predicate label, ?1?
for the first argument ahead of a predicate, ?2? for the last
argument behind, etc.) but includes unbounded dependen-
cies between referents of fillers and referents of clauses
containing gaps. It is assumed that semantically-labeled
structures would be isomorphic to the structures defined
here, but would generalize across alternations such as active
and passive constructions, for example.
38
connected components are trained on a corpus an-
notated with generalized categorial grammar de-
pendencies for ?gap? arguments at all categories
that subsume a gap (Nguyen et al, 2012). This
representation is similar to the HPSG-like repre-
sentation used by Hale (2001) and Lewis and Va-
sishth (2005), but has a naturally-defined depen-
dency structure on which to calculate connected
components. This generalized categorial grammar
is then used to identify the first sign that introduces
a gap, at which point a deep syntactic connected
component containing the filler can be encoded
(stored), and a separate deep syntactic connected
component for a clause containing a gap can be
initiated.
A generalized categorial grammar (Bach, 1981)
consists of a set U of primitive category types;
a set O of type-constructing operators allowing a
recursive definition of a set of categories C =def
U ? (C ? O ? C); a set X of vocabulary items;
a mapping M from vocabulary items in X to se-
mantic functions with category types in C; and
a set R of inference rules for deriving functions
with category types inC from other functions with
category types in C. Nguyen et al (2012) use
primitive category types for clause types (e.g. V
for finite verb-headed clause, N for noun phrase
or nominal clause, D for determiners and pos-
sessive clauses, etc.), and use the generalized set
of type-constructing operators to characterize not
only function application dependencies between
arguments immediately ahead of and behind a
functor (-a and -b, corresponding to ?\? and ?/? in
Ajdukiewicz-Bar-Hillel categorial grammars), but
also long-distance dependencies between fillers
and categories subsuming gaps (-g), dependencies
between relative pronouns and antecedent modif-
icands of relative clauses (-r), and dependencies
between interrogative pronouns and their argu-
ments (-i), which remain unsatisfied in derivations
but function to distinguish categories for content
and polar questions. A lexicon can then be de-
fined in M to introduce lexical dependencies and
obligatory pronominal dependencies using num-
bered functions for predicates and deep syntactic
arguments, for example:
the ? (?i (0 i)=the) : D
person ? (?i (0 i)=person) : N-aD
who ? (?k i (0 i)=who ? (1 i)=k) : N-rN
officials ? (?i (0 i)=officials) : N
the
D
person
N-aD
N Aa
who
N-rN
officials
N
say
V-aN-bV
stole
V-aN-bN
millions
N
V-aN Ae
V-gN Ga
V-aN-gN
Ag
V-gN Ac
V-rN Fc
N R
Figure 2: Example categorization of the noun
phrase the person who officials say stole millions.
say ? (?i (0 i)=say) : V-aN-bV
stole ? (?i (0 i)=stole) : V-aN-bN
millions ? (?i (0 i)=millions) : N
Inference rules in R are then defined to com-
pose arguments and modifiers and propagate gaps.
Arguments g of type d ahead of functors h of
type c-ad are composed by passing non-local de-
pendencies ? ? {-g, -i, -r} ? C from premises to
conclusion in all combinations:
g:d h: c-ad ? ( fc-ad g h): c (Aa)
g:d? h: c-ad ? ?k ( fc-ad (g k) h): c? (Ab)
g:d h: c-ad? ? ?k ( fc-ad g (h k)): c? (Ac)
g:d? h: c-ad? ? ?k ( fc-ad (g k) (h k)): c? (Ad)
Similar rules compose arguments behind functors:
g: c-bd h:d ? ( fc-bd g h): c (Ae)
g: c-bd? h:d ? ?k ( fc-bd (g k) h): c? (Af)
g: c-bd h:d? ? ?k ( fc-bd g (h k)): c? (Ag)
g: c-bd? h:d? ? ?k ( fc-bd (g k) (h k)): c? (Ah)
These rules use composition functions fc-ad
and fc-bd for initial and final arguments, which de-
fine dependency edges numbered v from referents
of predicate functors i to referents of arguments j,
where v is the number of unsatisfied arguments
?1...?v ? {-a, -b} ?C in a category label:
fu?1..v?1-ac
def= ?g h i ? j (v i)= j ? (g j) ? (h i) (1a)
fu?1..v?1-bc
def= ?g h i ? j (v i)= j ? (g i) ? (h j) (1b)
R also contains inference rules to compose mod-
ifier functors g of type u-ad ahead of modifi-
cands h of type d:
g: u-ad h:c ? ( fIM g h):c (Ma)
g: u-ad? h:c ? ?k ( fIM (g k) h):c? (Mb)
g: u-ad h:c? ? ?k ( fIM g (h k)):c? (Mc)
39
?i1 j1.. i? j? ... ? (g?:c/d { j?} i?) xt
?i1 j1.. i? ... ? ((g? f ):c i?)
xt ? f :d (?Fa)
?i1 j1.. i? j? ... ? (g?:c/d { j?} i?) xt
?i1 j1.. i? j?i?+1 ... ? (g?:c/d { j?} i?) ? ( f :e i?+1)
xt ? f :e (+Fa)
?i1 j1.. i??1 j??1i? ... ? (g?:d i?)
?i1 j1.. i? j? ... ? (( f g?):c/e { j?} i?)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
g:d h:e ? ( f g h):c or
g:d h:e ? ?k( f (g k) h):c or
g:d h:e ? ?k( f g (h k)):c or
g:d h:e ? ?k( f (g k) (h k)):c
(?La)
?i1 j1.. i??1 j??1i? ... ? (g??1:a/c { j??1} i??1) ? (g?:d i?)
?i1 j1.. i??1 j??1 ... ? (g??1 ? ( f g?):a/e { j??1} i??1)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
g:d h:e ? ( f g h):c or
g:d h:e ? ?k( f (g k) h):c or
g:d h:e ? ?k( f g (h k)):c or
g:d h:e ? ?k( f (g k) (h k)):c
(+La)
Figure 3: Basic processing productions of a right-corner parser.
g: u-ad? h:c? ? ?k ( fIM (g k) (h k)):c? (Md)
or for modifier functors behind a modificand:
g:c h: u-ad ? ( fFM g h):c (Me)
g:c? h: u-ad ? ?k ( fFM (g k) h):c? (Mf)
g:c h: u-ad? ? ?k ( fFM g (h k)):c? (Mg)
g:c? h: u-ad? ? ?k ( fFM (g k) (h k)):c? (Mh)
These rules use composition functions fIM and fFM
for initial and final modifiers, which define depen-
dency edges numbered ?1? from referents of mod-
ifier functors i to referents of modificands j:
fIM
def= ?g h j ?i (1 i)= j ? (g i) ? (h j) (2a)
fFM
def= ?g h j ?i (1 i)= j ? (g j) ? (h i) (2b)
R also contains inference rules for hypothesiz-
ing gaps -gd for arguments and modifiers:3
g: c-ad ? ?k ( fc-ad {k} g): c-gd (Ga)
g: c-bd ? ?k ( fc-ad {k} g): c-gd (Gb)
g:c ? ?k ( fIM {k} g):c-gd (Gc)
and for attaching fillers e, d-re, d-ie as gaps -gd:
g:e h: c-gd ? ?i ? j (g i) ? (h i j):e (Fa)
g:d-re h: c-gd ? ?k j ?i (g k i) ? (h i j): c-re (Fb)
g:d-ie h: c-gd ? ?k j ?i (g k i) ? (h i j): c-ie (Fc)
3Since these unary inferences perform no explicit compo-
sition, they are defined to use only initial versions composi-
tion functions fc-ad and fIM.
and for attaching modificands as antecedents of
relative pronouns:
g:e h:c-rd ? ?i ? j (g i) ? (h i j):e (R)
An example derivation of the noun phrase the per-
son who officials say stole millions using these
rules is shown in Figure 2. The semantic expres-
sion produced by this derivation consists of a con-
junction of terms defining the edges in the graph
shown in Figure 1b.
This GCG formulation captures many of the in-
sights of the HPSG-like context-free filler-gap no-
tation used by Hale (2001) or Lewis and Vasishth
(2005): inference rules with adjacent premises can
be cast as context-free grammars and weighted us-
ing probabilities, which allow experiments to cal-
culate frequency measures for syntactic construc-
tions. Applying a latent variable PCFG trainer
(Petrov et al, 2006) to this formulation was shown
to yield state-of-the-art accuracy for recovery of
unbounded dependencies (Nguyen et al, 2012).
Moreover, the functor-argument dependencies in
a GCG define deep syntactic dependency graphs
for all derivations, which can be used in incremen-
tal parsing to calculate connected components for
memory-based measures.
4 Incremental Processing
In order to obtain measures of memory opera-
tions used in incremental processing, these GCG
inference rules are combined into a set of parser
40
?i1 j1.. in jn.. i? j? ... ? (gn:y/z? { jn} in) ? ... ? (g?:c/d { j?} i?) xt
?i1 j1.. in jn.. i? ... ? (gn:y/z? { jn} in) ? ... ? ((g?( f ?{ jn} f )):c i?)
xt ? ?k( f ?{k} f ):d
(?Fb)
?i1 j1.. in jn.. i? j? ... ? (gn:y/z? { jn} in) ? ... ? (g?:c/d { j?} i?) xt
?i1 j1.. in jn.. i? j?i?+1 ... ? (gn:y/z? { jn} in) ? ... ? (g?:c/d { j?} i?) ? (( f ?{ jn} f ):e i?+1)
xt ? ?k( f ?{k} f ):e
(+Fb)
?i1 j1.. in jn.. i??1 j??1i? ... ? (gn:y/z? { jn} in) ? ... ? (g?:d i?)
?i1 j1.. in jn.. i? j? ... ? (gn:y/z? { jn} in) ? ... ? (( f g?) ? ( f ?{ jn}):c?/e { j?} i?)
g:d h:e ? ?k( f g ( f ?{k} h)):c? (?Lb)
?i1 j1.. in jn.. i??1 j??1i? ... ? (gn:y/z? { jn} in) ? ... ? (g??1:a/c? { j??1} i??1) ? (g?:d i?)
?i1 j1.. in jn.. i??1 j??1 ... ? (gn:y/z? { jn} in) ? ... ? (g??1 ? ( f g?) ? ( f ?{ jn}):a/e { j??1} i??1)
g:d h:e ? ?k( f g ( f ?{k} h)):c? (+Lb)
Figure 4: Additional processing productions for attaching a referent of a filler jn as the referent of a gap.
productions, similar to those of the ?right corner?
parser of van Schijndel and Schuler (2013), ex-
cept that instead of recognizing shallow hierarchi-
cal sequences of connected components of surface
structure, the parser recognizes shallow hierarchi-
cal sequences of connected components of deep
syntactic dependencies. This parser exploits the
observation (van Schijndel et al, in press) that left-
corner parsers and their variants do not need to ini-
tiate or integrate more than one connected compo-
nent at each word. These two operations are then
augmented with rules to introduce fillers and at-
tach fillers as gaps.
This parser is defined on incomplete connected
component states which consist of an active sign
(with a semantic referent and syntactic form or
category) lacking an awaited sign (also with a ref-
erent and category) yet to come. Semantic func-
tions of active and awaited signs are simplified to
denote only sets of referents, with gap arguments
(?k) stripped off and handled by separate con-
nected components. Incomplete connected com-
ponents, therefore, always denote semantic func-
tions from sets of referents to sets of referents.
This paper will notate semantic functions of
connected components using variables g and h, in-
complete connected component categories as c/d
(consisting of an active sign of category c and an
awaited sign of category d), and associations be-
tween them as g:c/d. The semantic representa-
tion used here is simply a deep syntactic depen-
dency structure, so a connected component func-
tion is satisfied if it holds for some output ref-
erent i given input referent j. This can be no-
tated ?i j (g:c/d { j} i), where the set { j} is equiva-
lent to (? j? j?= j). Connected component functions
that have a common referent j can then be com-
posed into larger connected components:4
?i jk (g { j} i) ? (h {k} j) ? ?i j (g?h {k} i) (3)
Hierarchies of ? connected compo-
nents can be represented as conjunctions:
?i1 j1... i? j? (g1:c1/d1 { j1} i1) ? ... ? (g?:c?/d? { j?} i?).
This allows constraints such as unbounded depen-
dencies between referents of fillers and referents
of clauses containing gaps to be specified across
connected components by simply plugging vari-
ables for filler referents into argument positions
for gaps.
A nondeterministic incremental parser can now
be defined as a deductive system, given an input
sequence consisting of an initial connected com-
ponent state of category T/T, corresponding to an
existing discourse context, followed by a sequence
of observations x1, x2, . . . , processed in time order.
As each xt is encountered, it is connected to an ex-
isting connected component or it introduces a new
disjoint component using the productions shown
in Figures 3, 4, and 5.
4These are connected components of dependency struc-
ture resulting from one or more composition functions being
composed, with each function?s output as the previous func-
tion?s second argument. This uses a standard definition of
function composition: (( f ? g) x) = ( f (g x)).
41
?i1 j1.. i??1 j??1i? ... ? (g?:d i?)
?i1 j1.. i? j? ... ? (( f g?) ? (?h k i (h k)):a/e? { j?} i?)
g:d h:e? ? ( f g h):c (?Lc)
?i1 j1.. i??1 j??1i? ... ? (g??1:a/c { j??1} i??1) ? (g?:d i?)
?i1 j1.. i??1 j??1 ... ? (g??1 ? ( f g?) ? (?h k i (h k)):a/e? { j??1} i??1)
g:d h:e? ? ( f g h):c (+Lc)
?i1 j1.. i? j? ... ? (g??1:c/d? { j??1} i??1) ? (g?:d?/e { j?} i?)
?i1 j1.. i??1 j??1 ... ? (g??1 ? (?h i? j(h j)) ? g?:c/e { j??1} i??1)
(+N)
Figure 5: Additional processing productions for hypothesizing filler-gap attachment.
Operations on dependencies that can be derived
from surface structure (see Figure 3) are taken
directly from van Schijndel and Schuler (2013).
First, if an observation xt can immediately fill
the awaited sign of the last connected component
g?:c/d, it is hypothesized to do so, turning this
incomplete connected component into a complete
connected component (g? f ):c (Production ?Fa); or
if the observation can serve as an initial sub-sign
of this awaited sign, it is hypothesized to form a
new complete sign f :e in a new component with xt
as its first observation (Production +Fa). Then,
if either of these resulting complete signs g?:d
can immediately attach as an initial child of the
awaited sign of the most recent connected com-
ponent g??1:a/c, it is hypothesized to merge and
extend this connected component, with xt as the
last observation of the completed connected com-
ponent (Production +La); or if it can serve as an
initial sub-sign of this awaited sign, it is hypoth-
esized to remain disjoint and form its own con-
nected component (Production ?La). The side
conditions of La productions are defined to unpack
gap propagation (instances of ?k that distinguish
rules Aa?h and Ma?h) from the inference rules
in Section 3, because this functionality will be re-
placed with direct substitution of referent variables
into subordinate semantic functions, below.
The Nguyen et al (2012) GCG was defined
to pass up unbounded dependencies, but in in-
cremental deep syntactic dependency processing,
unbounded dependencies are accounted as sepa-
rate connected components. When hypothesizing
an unbounded dependency, the processing model
simply cues the active sign of a previous connected
component containing a filler without completing
the current connected component. The four +F,
?F, +L, and ?L operations are therefore combined
with applications of unary rules Ga?c for hypoth-
esizing referents as fillers for gaps (providing f ?
in the equations in Figure 4). Productions ?Fb
and +Fb fill gaps in initial children, and Produc-
tions ?Lb and +Lb fill gaps in final children. Note
that the Fb and Lb productions apply to the same
types of antecedents as Fa and La productions re-
spectively, so members of these two sets of pro-
ductions cannot be applied together.
Applications of rules Fa?c and R for introduc-
ing fillers are applied to store fillers as existentially
quantified variable values in Lc productions (see
Figure 5). These Lc productions apply to the same
type of antecedent as La and Lb productions, so
these also cannot be applied together.
Finally, connected components separated by
gaps which are no longer hypothesized (?) are
reattached by a +N production. This +N pro-
duction may then be paired with a ?N production
which yields its antecedent unchanged as a conse-
quent. These N productions apply to antecedents
and consequents of the same type, so they may be
applied together with one F and one L production,
but since the +N production removes in its conse-
quent a ? argument required in its antecedent, it
may not apply more than once in succession (and
applying the ?N production more than once in suc-
cession has no effect).
An incremental derivation of the noun phrase
the person who officials say stole millions, using
these productions, is shown in Figure 6.
5 Evaluation
The F, L, and N productions defined in the pre-
vious section can be made probabilistic by first
computing a probabilistic context-free grammar
(PCFG) from a tree-annotated corpus, then trans-
forming that PCFG model into a model of prob-
abilities over incremental parsing operations us-
ing a grammar transform (Schuler, 2009). This
allows the intermediate PCFG to be optimized us-
ing an existing PCFG-based latent variable trainer
42
?i0 (.. :T/T {i0} i0) the
?i0 i2 (.. :T/T {i0} i0) ? (.. :N/N-aD {i2} i2)
+Fa,?La,?N
person
?i0 i2 (.. :T/T {i0} i0) ? (.. :N/V-rN {i2} i2)
?Fa,?La,?N
who
?i0 i2 i3 (.. :T/T {i0} i0) ? (.. :N/V-gN {i3} i2)
+Fa,+Lc,?N
officials
?i0 i2 i3 i5 (.. :T/T {i0} i0) ? (.. :N/V-gN {i3} i2) ? (.. :V-gN/V-aN-gN {i5} i5)
+Fa,?La,?N
say
?i0 i2 i6 (.. :T/T {i0} i0) ? (.. :N/V-aN {i6} i2)
+Fb,+La,+N
stole
?i0 i2 i7 (.. :T/T {i0} i0) ? (.. :N/N {i7} i2)
+Fa,+La,?N
millions
?i0 (.. :T/T {i0} i0)
?Fa,+La,?N
Figure 6: Derivation of the person who officials say stole millions, showing connected components with
unique referent variables (calculated according to the equations in Section 4). Semantic functions are
abbreviated to ?..? for readability. This derivation yields the following lexical relations: (0 i1)=the,
(0 i2)=person, (0 i3)=who, (0 i4)=officials, (0 i5)=say, (0 i6)=stole, (0 i7)=millions, and the following
argument relations: (1 i2)=i1, (1 i3)=i2, (1 i5)=i4, (2 i5)=i6, (1 i6)=i3, (2 i6)=i7.
(Petrov et al, 2006). When applied to the output
of this trainer, this transform has been shown to
produce comparable accuracy to that of the origi-
nal Petrov et al (2006) CKY parser (van Schijn-
del et al, 2012). The transform used in these ex-
periments diverges from that of Schuler (2009), in
that the probability associated with introducing a
gap in a filler-gap construction is reallocated from
a ?F?L operation to a +F?L operation (to encode
the previously most subordinate connected com-
ponent with the filler as its awaited sign and be-
gin a new disjoint connected component), and the
probability associated with resolving such a gap is
reallocated from an implicit ?N operation to a +N
operation (to integrate the connected component
containing the gap with that containing the filler).
In order to verify that the modifications to the
transform correctly reallocate probability mass for
gap operations, the goodness of fit to reading
times of a model using this modified transform
is compared against the publicly-available base-
line model from van Schijndel and Schuler (2013),
which uses the original Schuler (2009) transform.5
To ensure a valid comparison, both parsers are
trained on a GCG-reannotated version of the Wall
Street Journal portion of the Penn Treebank (Mar-
cus et al, 1993) before being fit to reading times
using linear mixed-effects models (Baayen et al,
2008).6 This evaluation focuses on the process-
ing that can be done up to a given point in a sen-
tence. In human subjects, this processing includes
both immediate lexical access and regressions that
5The models used here also use random slopes to reduce
their variance, which makes them less anticonservative.
6The models are built using lmer from the lme4R package
(Bates et al, 2011; R Development Core Team, 2010).
aid in the integration of new information, so the
reading times of interest in this evaluation are log-
transformed go-past durations.7
The first and last word of each line in the
Dundee corpus, words not observed at least 5
times in the WSJ training corpus, and fixations af-
ter long saccades (>4 words) are omitted from the
evaluation to filter out wrap-up effects, parser in-
accuracies, and inattention and track loss of the
eyetracker. The following predictors are centered
and used in each baseline model: sentence posi-
tion, word length, whether or not the previous or
next word were fixated upon, and unigram and bi-
gram probabilities.8 Then each of the following
predictors is residualized off each baseline before
being centered and added to it to help residualize
the next factor: length of the go-past region, cumu-
lative total surprisal, total surprisal (Hale, 2001),
and cumulative entropy reduction (Hale, 2003).9
All 2-way interactions between these effects are
7Go-past durations are calculated by summing all fixa-
tions in a region of text, including regressions, until a new
region is fixated, which accounts for additional processing
that may take place after initial lexical access, but before the
next region is processed. For example, if one region ends at
word 5 in a sentence, and the next fixation lands on word 8,
then the go-past region consists of words 6-8 while go-past
duration sums all fixations until a fixation occurs after word
8. Log-transforming eye movements and fixations may make
their distributions more normal (Stephen and Mirman, 2010)
and does not substantially affect the results of this paper.
8For the n-gram model, this study uses the Brown corpus
(Francis and Kucera, 1979), the WSJ Sections 02-21 (Mar-
cus et al, 1993), the written portion of the British National
Corpus (BNC Consortium, 2007), and the Dundee corpus
(Kennedy et al, 2003) smoothed with modified Kneser-Ney
(Chen and Goodman, 1998) in SRILM (Stolcke, 2002).
9Non-cumulative metrics are calculated from the final
word of the go-past region; cumulative metrics are summed
over the go-past region.
43
included as predictors along with the predictors
from the previous go-past region (to account for
spillover effects). Finally, each model has sub-
ject and item random intercepts added in addition
to by-subject random slopes (cumulative total sur-
prisal, whether the previous word was fixated, and
length of the go-past region) and is fit to centered
log-transformed go-past durations.10
The Akaike Information Criterion (AIC)
indicates that the gap-reallocating model
(AIC = 128,605) provides a better fit to reading
times than the original model (AIC = 128,619).11
As described in Section 1, previous findings of
negative integration cost may be due to a confound
whereby center-embedded constructions caused
by modifiers, which do not require deep syntac-
tic dependencies to be deferred, may be driving
the effect. Under this hypothesis, embeddings
that do not arise from final adjunction of mod-
ifiers (henceforth canonical embeddings) should
yield a positive integration cost as found by Gib-
son (2000).
To investigate this potential confound, the
Dundee corpus is partitioned into two parts. First,
the model described in this paper is used to anno-
tate the Dundee corpus. From this annotated cor-
pus, all sentences are collected that contain canon-
ical embeddings and lack modifier-induced em-
beddings.12 This produces two corpora: one con-
sisting entirely of canonical center-embeddings
such as those used in self-paced reading exper-
iments with findings of positive integration cost
(e.g. Gibson 2000), the other consisting of the
remainder of the Dundee corpus, which contains
sentences with canonical embeddings but also in-
cludes modifier-caused embeddings.
The coefficient estimates for integration oper-
ations (?F+L and +N) on each of these corpora
are then calculated using the baseline described
above. To ensure embeddings are driving any ob-
served effect rather than sentence wrap-up effects,
the first and last words of each sentence are ex-
cluded from both data sets. Integration cost is
measured by the amount of probability mass the
parser allocates to ?F+L and +N operations, accu-
10Each fixed effect that has an absolute t-value greater than
10 when included in a random-intercepts only model is added
as a random slope by-subject.
11The relative likelihood of the original model to the gap-
sensitive model is 0.0009 (n = 151,331), which suggests the
improvement is significant.
12Modifier-induced embeddings are found by looking for
embeddings that arise from inference rules Ma-h in Section 3.
Model coeff std err t-score
Canonical -0.040 0.010 -4.05
Other -0.017 0.004 -4.20
Table 1: Fixed effect estimates for integration cost
when used to fit reading times over two partitions
of the Dundee corpus: one containing only canon-
ical center embeddings and the other composed of
the rest of the sentences in the corpus.
mulated over each go-past region, and this cost is
added as a fixed effect and as a random slope by
subject to the mixed model described earlier.13
The fixed effect estimate for cumulative inte-
gration cost from fitting each corpus is shown
in Table 1. Application of Welch?s t-test shows
that the difference between the estimated distri-
butions of these two parameters is highly signif-
icant (p < 0.0001).14 The strong negative corre-
lation of integration cost to reading times in the
purely canonical corpus suggests canonical (non-
modifier) integrations contribute to the finding of
negative integration cost.
6 Conclusion
This paper has introduced an incremental parser
capable of using GCG dependencies to distinguish
between surface syntactic embeddings and deep
syntactic embeddings. This parser was shown to
obtain a better fit to reading times than a surface-
syntactic parser and was used to parse the Dundee
eye-tracking corpus in two partitions: one consist-
ing of canonical embeddings that require deferred
dependencies and the other consisting of sentences
containing no center embeddings or center em-
beddings arising from the attachment of clause-
final modifiers, in which no dependencies are de-
ferred. Using linear mixed effects models, com-
pletion (integration) of canonical center embed-
dings was found to be significantly more nega-
tively correlated with reading times than comple-
tion of non-canonical embeddings. These results
suggest that the negative integration cost observed
in eye-tracking studies is at least partially due to
deep syntactic dependencies and not due to con-
founds related to surface forms.
13Integration cost is residualized off the baseline before be-
ing centered and added as a fixed effect.
14Integration cost is significant as a fixed effect (p = 0.001)
in both partitions: canonical (n = 16,174 durations) and
non-canonical (n = 131,297 durations).
44
References
Steven P. Abney and Mark Johnson. 1991. Memory
requirements and local ambiguities of parsing strate-
gies. J. Psycholinguistic Research, 20(3):233?250.
R. Harald Baayen, D. J. Davidson, and Douglas M.
Bates. 2008. Mixed-effects modeling with crossed
random effects for subjects and items. Journal of
Memory and Language, 59:390?412.
Emmon Bach. 1981. Discontinuous constituents in
generalized categorial grammars. Proceedings of
the Annual Meeting of the Northeast Linguistic So-
ciety (NELS), 11:1?12.
Douglas Bates, Martin Maechler, and Ben Bolker,
2011. lme4: Linear mixed-effects models using S4
classes.
BNC Consortium. 2007. The british national corpus.
Stanley F. Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical report, Harvard University.
Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109(2):193?210.
W. Nelson Francis and Henry Kucera. 1979. The
brown corpus: A standard corpus of present-day
edited american english.
Edward Gibson. 1998. Linguistic complexity: Local-
ity of syntactic dependencies. Cognition, 68(1):1?
76.
Edward Gibson. 2000. The dependency locality the-
ory: A distance-based theory of linguistic complex-
ity. In Image, language, brain: Papers from the first
mind articulation project symposium, pages 95?126,
Cambridge, MA. MIT Press.
John Hale. 2001. A probabilistic earley parser as a
psycholinguistic model. In Proceedings of the sec-
ond meeting of the North American chapter of the
Association for Computational Linguistics, pages
159?166, Pittsburgh, PA.
John Hale. 2003. Grammar, Uncertainty and Sentence
Processing. Ph.D. thesis, Cognitive Science, The
Johns Hopkins University.
Philip N. Johnson-Laird. 1983. Mental models: to-
wards a cognitive science of language, inference,
and consciousness. Harvard University Press, Cam-
bridge, MA, USA.
Daniel Jurafsky. 1996. A probabilistic model of lexical
and syntactic access and disambiguation. Cognitive
Science: A Multidisciplinary Journal, 20(2):137?
194.
Marcel Adam Just and Patricia A. Carpenter. 1992. A
capacity theory of comprehension: Individual differ-
ences in working memory. Psychological Review,
99:122?149.
Fred Karlsson. 2007. Constraints on multiple center-
embedding of clauses. Journal of Linguistics,
43:365?392.
Alan Kennedy, James Pynte, and Robin Hill. 2003.
The Dundee corpus. In Proceedings of the 12th Eu-
ropean conference on eye movement.
Walter Kintsch. 1988. The role of knowledge in dis-
course comprehension: A construction-integration
model. Psychological review, 95(2):163?182.
Nayoung Kwon, Yoonhyoung Lee, Peter C. Gordon,
Robert Kluender, and Maria Polinsky. 2010. Cog-
nitive and linguistic factors affecting subject/object
asymmetry: An eye-tracking study of pre-nominal
relative clauses in korean. Language, 86(3):561.
Richard L. Lewis and Shravan Vasishth. 2005.
An activation-based model of sentence processing
as skilled memory retrieval. Cognitive Science,
29(3):375?419.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Igor Mel?c?uk. 1988. Dependency syntax: theory and
practice. State University of NY Press, Albany.
Luan Nguyen, Marten van Schijndel, and William
Schuler. 2012. Accurate unbounded dependency
recovery using generalized categorial grammars. In
Proceedings of the 24th International Conference on
Computational Linguistics (COLING ?12), Mumbai,
India.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the
44th Annual Meeting of the Association for Compu-
tational Linguistics (COLING/ACL?06).
R Development Core Team, 2010. R: A Language and
Environment for Statistical Computing. R Foun-
dation for Statistical Computing, Vienna, Austria.
ISBN 3-900051-07-0.
William Schuler. 2009. Parsing with a bounded stack
using a model-based right-corner transform. In Pro-
ceedings of NAACL/HLT 2009, NAACL ?09, pages
344?352, Boulder, Colorado. Association for Com-
putational Linguistics.
Damian G. Stephen and Daniel Mirman. 2010. Inter-
actions dominate the dynamics of visual cognition.
Cognition, 115(1):154?165.
Andreas Stolcke. 2002. Srilm ? an extensible language
modeling toolkit. In Seventh International Confer-
ence on Spoken Language Processing.
Marten van Schijndel and William Schuler. 2013. An
analysis of frequency- and recency-based processing
costs. In Proceedings of NAACL-HLT 2013. Associ-
ation for Computational Linguistics.
45
Marten van Schijndel, Andy Exley, and William
Schuler. 2012. Connectionist-inspired incremental
PCFG parsing. In Proceedings of CMCL 2012. As-
sociation for Computational Linguistics.
Marten van Schijndel, Andy Exley, and William
Schuler. in press. A model of language processing
as hierarchic sequential prediction. Topics in Cogni-
tive Science.
Stephen Wu, Asaf Bachrach, Carlos Cardenas, and
William Schuler. 2010. Complexity metrics in an
incremental right-corner parser. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics (ACL?10), pages 1189?1198.
46
