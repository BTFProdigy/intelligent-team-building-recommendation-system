Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 761?768
Manchester, August 2008
Event Frame Extraction Based on a Gene Regulation Corpus 
Yutaka Sasaki 1    Paul Thompson 1    Philip Cotter 1    John McNaught 1, 2 
Sophia Ananiadou1, 2 
 
1 School of Computer Science, University of Manchester 
2  National Centre for Text Mining 
MIB, 131 Princess Street, Manchester, M1 7DN, United Kingdom 
Yutaka.Sasaki@manchester.ac.uk 
 
 Abstract 
This paper describes the supervised ac-
quisition of semantic event frames  based 
on a corpus of biomedical abstracts, in 
which the biological process of E. coli 
gene regulation has been linguistically 
annotated by a group of biologists in the 
EC research project "BOOTStrep". Gene 
regulation is one of the rapidly advancing 
areas for which information extraction 
could boost research. Event frames are an 
essential linguistic resource for extraction 
of information from biological literature.  
This paper presents a specification for 
linguistic-level annotation of gene regu-
lation events, followed by novel methods 
of automatic event frame extraction from 
text.  The event frame extraction per-
formance has been evaluated with 10-
fold cross validation.  The experimental 
results show that a precision of nearly 
50% and a recall of around 20% are 
achieved.  Since the goal of this paper is 
event frame extraction, rather than event 
instance extraction, the issue of low re-
call could be solved by applying the 
methods to a larger-scale corpus. 
1 Introduction 
This paper describes the automatic extraction of 
linguistic event frames based on a corpus of 
MEDLINE abstracts that has been annotated 
with gene regulation events by a group of do-
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
 
main experts. Annotation is centred on both 
verbs and nominalised verbs that describe rele-
vant events. For each event, semantic arguments 
that occur within the same sentence are marked 
and labelled with semantic roles and named en-
tity (NE) types. 
The focus of the paper is the extraction of 
event frames on the basis of the annotated corpus 
using machine learning techniques. Event frames 
are linguistic specifications concerning the be-
haviour of verbs and nominalised verbs, in terms 
of the number and types of semantic arguments 
with which they typically co-occur in texts. Our 
eventual goal is to exploit such information to 
improve information extraction. Event frame ex-
traction is different to event instance extraction 
(or template filling). Our event frames are des-
tined for incorporation in the BOOTStrep 
BioLexicon to support identification of relevant 
event instances and  discovery of event instance 
participants by NLP systems. 
2 Background 
There are several well-established, large-scale 
repositories of semantic frames for general lan-
guage, e.g., VerbNet (Kipper-Schuler, 2005), 
PropBank (Palmer et al, 2005) and FrameNet 
(Rupenhoffer et al 2006). These all aim to char-
acterise verb behaviour in terms of the semantic 
arguments with which verbs occur but differ in 
how they represent semantic arguments and 
groupings of verbs.  
In VerbNet, the semantic roles of arguments 
come from frame-independent roles, e.g. Agent, 
Patient, Location and Instrument.  
In contrast, PropBank and FrameNet use a 
mixture of role types: some are common amongst 
a number of frames; others are specific to par-
ticular frames.  
Whilst FrameNet and VerbNet differ in their 
treatment of semantic roles, they both specify  
761
semantic frames that correspond to groups of 
verbs with similar behaviour. However, frames 
in PropBank correspond to individual verbs. 
   Biology-specific extensions have been at-
tempted both for PropBank (Wattarujeekrit et al, 
2004) and FrameNet (Dolbey et al, 2006). How-
ever, to our knowledge, there has been no such 
attempt at extending VerbNet into the biological 
domain. 
In common with VerbNet, our work is focus-
sed on producing event frames that use a set of 
frame-independent semantic roles. However, we 
adopt a smaller set of roles tailored to the domain. 
This use of frame-independent roles allows lin-
guistic generalisations to be captured more easily 
(Cohen and Hunter, 2006). Also, the use of such 
roles is more suitable for direct exploitation by 
NLP systems (Zaphirain et al, 2008).  
Unlike VerbNet, we aim to produce a set of 
frames that are verb-specific (rather than frames 
that apply to groups of verbs). Verb-specific 
frames are able to provide more detailed argu-
ment specifications?particularly important in 
the biomedical field, where phrases that identify 
information such as location, manner, timing and 
condition are essential for correct interpretation 
of events (Tsai et al 2007).  
3 Annotated corpus 
To aid semantic event frame extraction, we need 
a corpus annotated with event-level information.  
Several already exist for biology.  Some target 
extraction of PropBank-style frames (e.g. Chou 
et al (2006), Kulick et al (2004)). The corpus 
produced by Kim et al (2008) uses frame-
independent roles. However, only a few semantic 
argument types are annotated.  
The target of our event frame extraction is a 
set of semantic frames which specify all potential 
arguments of gene regulation events. For this 
purpose, we had to produce our own annotated 
corpus, using a larger set of event-independent 
semantic roles than Kim et al (2008). Our roles 
had to cover sufficiently wide scope to allow an-
notation and characterization of all instantiated 
arguments of relevant events within texts. To our 
knowledge, this makes our scheme unique within 
the biomedical field. 
In contrast to many other comparable re-
sources, annotated events are centred on both 
verbs and nominalised verbs, such as transcrip-
tion and control. Nominalised verbs play an im-
portant and possibly dominant role in biological 
texts (Cohen and Hunter, 2006). Our own corpus 
confirms this, in that the nominalised verb ex-
pression is the most commonly annotated word 
on which gene regulation events are centred. By 
annotating events centred on nominalised verbs 
in a similar way to verbs, it becomes possible to 
extract separate event frames for nominalised 
verbs. This enables their potentially idiosyncratic 
behaviour to be accounted for.  
Role Name Description Example (bold = semantic argument, italics = focussed verb)  
AGENT Drives/instigates event The narL gene product activates the nitrate reductase operon 
THEME a) Affected by/results from event 
b) Focus of events describing states 
recA protein was induced by UV radiation 
The FNR protein resembles CRP 
MANNER Method/way in which event is car-
ried out 
cpxA gene increases the levels of csgA transcription by dephosphoryla-
tion of CpxR 
INSTRUMENT Used to carry out event EnvZ functions through OmpR to control NP porin gene expression in 
Escherichia coli K-12. 
LOCATION Where complete event takes place Phosphorylation of OmpR modulates expression of the ompF and ompC 
genes in Escherichia coli 
SOURCE Start point of event A transducing lambda phage was isolated from a strain harboring a 
glpD??lacZ fusion  
DESTINATION End point of event Transcription of gntT is activated by binding of the cyclic AMP (cAMP)-
cAMP receptor protein (CRP) complex to a CRP binding site 
TEMPORAL Situates event in time w.r.t another 
event 
The Alp protease activity is detected in cells after introduction of plas-
mids carrying the alpA gene 
CONDITION Environmental conditions/changes 
in conditions 
Strains carrying a mutation in the crp structural gene fail to repress ODC 
and ADC activities in response to increased cAMP 
RATE Change of level or rate marR mutations elevated inaA expression by  10-  to 20-fold over that of 
the wild-type. 
DESCRIPTIVE-
AGENT 
Provides descriptive information 
about the AGENT of the event 
It is likely that HyfR acts as a formate-dependent regulator of the hyf 
operon 
DESCRIPTIVE-
THEME 
Provides descriptive information 
about the AGENT of the event 
The FNR protein resembles CRP. 
PURPOSE Purpose/reason for the event occur-
ring 
The fusion strains were used to study the regulation of the cysB gene by 
assaying the fused lacZ gene product 
Table 1. Semantic Roles 
762
Our annotated corpus consists of 677 MED-
LINE abstracts on E. Coli. Within them, a total 
of 4770 gene regulation events have been anno-
tated. 
3.1 Semantic Roles 
Based on the observations of Tsai et al(2007) 
regarding the most important types of informa-
tion specified for biomedical events, together 
with detailed examination of a large number of 
relevant events within our corpus, in discussion 
with biologists, we defined a set of 13 frame-
independent semantic roles that are suitable for 
the domain.   
 Certain roles within the set are domain-
independent, and are based on those used in 
VerbNet, e.g. AGENT, THEME, and LOCA-
TION. To these, we have added a number of do-
main-dependent roles, e.g. CONDITION and 
MANNER. The size of the role set attempts to 
balance the need for a sufficiently wide-ranging 
set of roles with the need for one that is as small 
and general as possible, to reduce the burden on 
annotators, whilst also helping to ensure consis-
tency across extracted verb frames. The full set 
of semantic roles used is shown in Table 1.  
3.2  Named Entity Categorisation 
 Although our semantic roles are rather general, 
the annotation scheme allows more detailed in-
formation about semantic arguments to be en-
coded in the corpus through the assignment of 
named entity (NE) tags. Unlike other corpus pro-
jects, we do not annotate all entities within each 
abstract, but just those entities that occur as se-
mantic arguments of annotated gene regulation 
events. 
Our set of NE tags goes beyond the traditional 
view of NEs,  in that labelling is extended to in-
clude events represented by nominalised verbs 
(e.g. repression). A total of 61 NE classes have 
been defined as being relevant to the gene regu-
lation field, which are divided into four entity-
specific super-classes (DNA, PROTEIN, EX-
PERIMENTAL and ORGANISMS) and one 
event-specific super-class (PROCESSES). The 
NEs within each of these classes are hierarchi-
cally-structured. Table 2 provides definitions of 
each of these five super-classes. The NEs corre-
spond to classes in the Gene Regulation Ontol-
ogy (Splendiani et al 2007), which has been de-
veloped as part of the BOOTStrep project in 
which this work has been carried out. The Gene 
Regulation Ontology integrates parts of other 
established bio-ontologies, such as Gene Ontol-
ogy (Ashburner et al, 2000) and Sequence On-
tology (Eilbeck,2005). 
3.3 Annotation process 
Annotation was carried out over a period of three 
months by seven PhD students with experience 
in gene regulation and with native or near-native 
competence in English. 
 Prior to annotation, each abstract was auto-
matically processed. Firstly, linguistic pre-
processing (i.e. morphological analysis, POS 
tagging and syntactic chunking)1 was carried out.  
 Secondly, all occurrences from a list of 700 
biologically relevant verbs were automatically 
marked. Annotators then considered each marked 
verb within an abstract. If the verb denoted a 
gene regulation event, annotators then: 
a. Identified all semantic arguments of the 
verb within the sentence 
b. Assigned a semantic role to each identi-
fied argument 
                                                 
1 Each abstract to be annotated is first pre-processed with 
the GENIA tagger (Tsuruoka et al 2005). 
NE class Definition 
DNA 
Entities chiefly composed of nucleic 
acids and their structural or positional 
references. This includes the physical 
structure of all DNA-based entities 
and the functional roles associated 
with regions thereof. 
PROTEIN 
Entities chiefly composed of amino 
acids and their positional references. 
This includes the physical structure 
and functional roles associated with 
each type. 
EXPERIMENTAL 
Both physical and methodological 
entities, either used, consumed or 
required for a reaction to take place. 
ORGANISMS 
Entities representing individuals or 
collections of living things and their 
component parts. 
PROCESSES A set of event classes used to label biological processes described in text.  
Table 2. Description of NE super-classes  Table 3. Most commonly annotated verbs and 
nominalised verbs 
Word Count Type 
expression 409 NV 
encode 351 V 
transcription 125 NV 
bind 110 V 
require 100 V 
express 93 V 
regulate 91 V 
synthesis 90 NV 
contain 80 V 
induce 78 V 
763
c. If appropriate, assigned named entity 
categories to (parts of) the semantic ar-
gument span 
d. If the argument corresponded to a nomi-
nalised verb, repeated steps a?c to iden-
tify its own arguments. 
Syntactic chunks were made visible to annota-
tors. In conjunction with annotation guidelines, 
the chunks were used to help ensure consistency 
of annotated semantic arguments. For example, 
the guidelines state that semantic arguments 
should normally consist of complete (and pref-
erably single) syntactic chunks.  The annotation 
was performed using a customised version of 
WordFreak (Morton and LaCivita, 2003), a Java-
based linguistic annotation tool.  
3.4  Corpus statistics 
The corpus is divided into 2 parts, i.e. 
1) 597 abstracts, each annotated by a single 
annotator, containing a total of 3612 
events, 
2) 80 pairs of double-annotated documents, 
allowing checking of inter-annotator 
agreement and consistency, and contain-
ing 1158 distinct events.  
 
 In the corpus, 277 distinct verbs were annotated 
as denoting gene regulation events, of which 73 
were annotated 10 times or more. In addition, 
annotation has identified 135 relevant nominal-
ised verbs, of which 22 were annotated 10 times 
or more. The most commonly annotated verbs 
and nominalised verbs are shown in Table 3.  
3.5 Inter-annotator agreement 
Inter-annotator agreement statistics for the 80 
pairs of duplicate-annotated abstracts are shown 
in Table 4.  
The figures shown in Table 4 are direct 
agreement rates. Whilst the Kappa statistic is 
very familiar for calculating inter-annotator 
agreement, we follow Wilbur et al (2006) and 
Pyysalo (2007) in choosing not to use it, because 
it is not appropriate or possible to calculate it for 
all of the above statistics. For instance: 
 
1. For some tasks, like annotation of events and 
arguments spans, deciding how to calculate 
random agreement is not clear. 
2. The Kappa statistic assumes that annotation 
categories are discrete and mutually exclu-
sive. This is not the case for the NE catego-
ries, which are hierarchically structured.   
 
 Table 4 shows that, in terms of identifying 
events  (i.e. determining which verbs denote gene 
regulation events), agreement between annotators 
is reached about half the time. The main reason 
for this relatively low figure is that reaching a 
consensus on the specific types of events to be 
annotated under the heading of ?gene regulation? 
required a large amount of discussion. Thus, par-
ticularly towards the start of the annotation phase, 
annotators tended to either under- or over-
annotate the events. 
Greater amounts of consistency seem to be 
achievable for other sub-tasks of the annotation, 
with agreement rates for the identification and 
subsequent labelling of semantic arguments be-
ing achieved in around three quarters of cases.  
Comparable, but slightly lower rates of agree-
ment were achieved in the identification of NEs. 
In terms of assigning categories to them, the 
agreement rate for exact category matches is a 
little lower (62%). However, if we relax the 
matching conditions by exploiting the hierarchi-
cal structure of the NE categories (i.e. if we 
count as a match the cases where the category 
assigned by one annotator was the ancestor of the 
category assigned by the other annotator), then 
the agreement increases by around 11%.  
The large number of NE categories (61), 
makes the decision of the most appropriate cate-
gory rather complex; this was verified by the an-
notators themselves. Based on this, we will con-
sider the use of a more coarse-grained scheme 
when carrying out further annotation of this type. 
However, in the current corpus, the hierarchical 
structuring of the NE categories means that it 
would be possible to use a smaller set of catego-
ries by mapping the specific categories to more 
general ones.   
4 Corpus Format 
For the purposes of event frame extraction, the 
annotations in the corpus were converted to an 
XML-style inline format consisting of three dif-
ferent types of element: 
 
Table 4. Inter-annotator agreement rates  
AGREEMENT RATE VALUE 
Event identification 0.49 
Argument identification (partial span match) 0.73 
Semantic role assignment 0.78 
NE identification (partial span match) 0.68 
NE category assignment (exact) 0.62 
NE category assignment (including parent) 0.65 
NE category assignment (including ancestors) 0.73 
  
764
EVENT ? surrounds text spans (i.e. verb 
phrases and nominalised verbs) on which 
events are centred. 
SLOT ? surrounds spans corresponding to se-
mantic arguments (i.e. slots) of events.  The 
head verb/nominalised verb of the event is also 
treated as a SLOT, with role type Verb. The 
eventid attribute links each slot with its respec-
tive event, whilst the Role attribute indicates 
the semantic role assigned to the slot.  
NE ? surrounds text spans annotated as named 
entities. The cat attribute stores the NE cate-
gory assigned. 
 
Where there are several annotations over some 
text span, elements are embedded inside each 
other. If more than one annotation begins at a 
particular offset, then the ordering of the embed-
ding is fixed, so that SLOT elements are embed-
ded inside EVENT elements, and that NE ele-
ments are embedded inside SLOT elements. An 
example of the annotation for the sentence "TaqI 
restriction endonuclease has been subcloned 
downstream from an inducible phoA promoter" 
is shown below: 
 
<SLOT argid="4" eventid="5" Role="Theme">  
<NE cat="ENZYME">TaqI restriction endonucle-
ase</NE></SLOT> <EVENT id="5"> 
has been <SLOT argid="6" eventid="5" 
Role="Verb">subcloned </SLOT></EVENT>  
<SLOT argid="8" eventid="5" 
Role="Location">downstream from  
<NE cat="PROMOTER">an inducible phoA pro-
moter</NE></SLOT>. 
 
The EVENT created over the VP chunk has 
been subcloned has been annotated as having 2 
semantic arguments (SLOTs), i.e. a THEME,  
TaqI restriction endonuclease and a LOCATION, 
i.e. downstream from an inducible phoA pro-
moter. A 3rd SLOT element corresponds to the 
head verb in the VP chunk. Named entity tags 
have also been assigned to the THEME span and 
part of the LOCATION span.  
5 Event Patterns and Event Frames 
This section defines event patterns and event 
frames.  Event patterns are syntactic patterns of 
sequences of surface words, NEs, and semantic 
roles, whilst event frames are the record-like data 
structures consisting of event slots and event slot 
values. 
5.1 Event Patterns 
Event patterns are fragments of event annotations 
in which semantic arguments are generalized to 
their semantic role and NE categories, if present. 
An event pattern is extracted for each unique 
event id within an abstract. An event annotation 
span begins with the earliest SLOT span, and 
ends with the latest SLOT assigned to the event. 
An example event span is as follows: 
 
<SLOT eventid="9" Role="Agent">  
<NE cat="OPERON"> transfer operon</NE></SLOT> 
<EVENT id="9"><SLOT eventid="9" Role="Verb"> 
expression </SLOT></EVENT></SLOT> of  
<SLOT eventid="9" Role="Theme">  
<NE cat="DNA_FRAGMENT"> F-like plasmids 
</NE></SLOT> 
 
For each event, each event span is generalized 
into an event pattern as follows:  
? ?Verb? role slots of the event are converted 
into a tuple consisting of the role type, part-
of-speech and surface form, i.e., 
[Verb:POS:verb].  
? Other semantic role slots and their NE slots 
for the event are generalized to tuples con-
sisting of the role and NE super class, i.e., 
[role:NE_super_class]. 
? Other XML tags are removed. 
 
The above example event span is thus general-
ized to the following event pattern: 
 
[Agent:DNA] [Verb:NN:expression] of [Theme:DNA]. 
 
5.2 Event frames 
Event frames are directly extracted from event 
patterns, and take the following general form: 
 
event_frame_name( 
     slot_name => slot_value, 
     ? 
     slot_name => slot_value). 
where 
? event_frame_name is the base form of the 
event verb or nominalized verb; 
? slot_names are  the names of the semantic 
roles within the event pattern; 
? slot_values are NE categories, if present 
within the event pattern. 
 
For example, the event frame corresponding to 
the event pattern shown in the previous section is 
as follows: 
expression( Agent=>DNA, 
            Theme=>DNA ). 
 
765
6 Event Frame Extraction 
Our event frame extraction is a fusion of sequen-
tial labelling based on Conditional Random 
Fields (CRF), and event pattern matching. Event 
frames are extracted in three steps.  Firstly, a 
CRF-based Named Entity Recognizer (NER) 
assigns biological NEs to word sequences. Sec-
ondly, a CRF-based semantic role labeller deter-
mines the semantic roles of word sequences with 
NE labels.  Thirdly, word sequences are com-
pared with event patterns derived from the cor-
pus.  Only those event frames whose semantic 
roles, NEs, and verb POS satisfy event pattern 
conditions will be extracted. 
6.1 Biological NER  
Since it is costly and time-consuming to create a 
large-scale training corpus annotated by biolo-
gists, we need to concede to use coarse-grained 
biological NE categories. That is, the NER com-
ponent is trained on the five NE super classes, 
i.e., Protein, DNA, Experimental, Organisms, 
and Processes. 
The NER models are trained by CRFs 
(Lafferty et al, 2001) using the standard IOB2 
labelling method.  That is, the label ``B-NE'' is 
given to the first token of the target NE sequence, 
?I-NE? to each remaining token in the target se-
quence,  and ``O'' to other tokens. 
Features used are as follows: 
? word feature 
- orthographic features: 
 the first letter and the last four letters of the 
word form, in which capital letters in a word are 
normalized to ?A?, lower case letters are normal-
ized to ?a?, and digits are replaced by ?0?. For 
example, the word form ?IL-2? is normalised to 
?AA-0?. 
- postfix features:  the last two and four let-
ters 
? POS feature 
 
We applied first-order CRFs using the above fea-
tures for the tokens within a window size of  ?2 
of the current token. 
6.2 Semantic Role Labelling  
First of all, each NE token sequence identified by 
B and I labels is merged into a single token with 
the NE category name. Then, the semantic role 
labelling models are trained by CRFs in a similar 
way to NER.  That is, the label ``B-Role'' is given 
to the first token of the target Role sequence, ?I-
Role? to each remaining token in the target se-
quence, and ?O? to other tokens. 
Features used here are as follows: 
? word feature 
?  base form feature 
? POS feature 
? NE feature 
 
The window size was ?2 of the current token. 
6.3 Event pattern matching  
When a new sentence is given, sequential label-
ling models decide NE and semantic role labels 
of tokenized input sentences. Then, the token 
sequences are converted into the following token 
sequences with POS, semantic role, and NE in-
formation (called augmented token sequences): 
 
1. Each token sequence labelled by IOB seman-
tic role labels is merged into a token labelled 
with the role. 
2. Verbs and nominalized verbs are converted 
to [Verb:POS:surface_form]. 
3. Tokens with semantic role label and NE su-
per-class are converted into the form 
[Role:NE_super_class]. 
4. Other tokens with O label are converted to 
surface tokens. 
 
Then, event patterns are generalized: 
5. Event patterns are modified so that elements 
corresponding to verbs and nominalized 
verbs will match any words with the same 
POS, e.g., [Verb:POS:*]. 
 
Finally, each event pattern is applied to aug-
mented token sequences one by one:  
6. By matching the generalized event patterns 
with augmented token sequences, i.e. when 
verbs or nominalized verbs and the surround-
ing semantic roles and NEs satisfy the event 
pattern conditions, then successfully unified 
event patterns are extracted as new event pat-
terns. 
7. The newly obtained event patterns are con-
verted into event frames in the same way as 
described in Section 5.2.  
7 Experimental Results 
The aim of this section is to evaluate semantic 
frame extraction performance, given a set of an-
notated training data. 
The annotated corpus was randomly separated 
into 10 document groups and their event patterns 
766
and event frames were segmented into 10 groups 
according to the document separation. 
We conducted 10-fold cross validation based 
on the 10 document groups.  Named entity rec-
ognizers and semantic role labellers were trained 
using 9 groups of annotated documents.  Event 
frames were then extracted from the remaining 
group of documents.  Micro-average precision 
and recall for the set of event frames extracted 
from all the folds were evaluated. 
Table 5 shows the event frame extraction per-
formance.  #TP, #FN, and #FP indicate the num-
ber of true positives, false negatives, and false 
positives, respectively.   
Named entity recognition performance was 
also evaluated (Table 6).  Since the training data 
size is small, the performance is between ap-
proximately 20-60% F-measure. However, this 
will not cause a problem for the event frame ex-
traction task.  This is because, if a particular 
event frame occurs multiple times in a corpus, it 
is sufficient to extract only a single occurrence of 
the event description. So, whilst the NE and se-
mantic role labelling may not be successful for 
all occurrences of the event frame, there is a 
good chance that at least one occurrence of the 
event will be realized in the text in such a way as 
to allow the labelling to be carried out success-
fully, thus allowing the extraction of an appro-
priate event frame.  
8 Discussion 
Linguistic-level event annotation of biological 
events is an inherently difficult task.  This is 
supported by the fact that the inter-annotator 
agreement level for the identification of events 
was 0.49 (see Table 4).  Therefore, in terms of 
event extraction performance, a precision of 
49.0% on 10-fold cross validation is almost 
comparable to human experts. The low recall of 
18.6% may not be an issue, as the recall is likely 
to improve with the size of the target corpus.   
The precision may additionally be underesti-
mated in the evaluation due to inconsistencies in 
the annotation.  We found that the average preci-
sion of our event frame extraction over 10 folds 
is around 30%, despite the fact that the precision 
of all event frames extracted from 10 folds is 
almost 50% compared with the annotated event 
frames in the whole corpus.  This happens be-
cause some events not annotated in a particular 
fold are annotated in the rest of corpus.  From 
this insight, our conjecture is that the true preci-
sion against the whole corpus would be some-
what higher (potentially 70-80%) if we were us-
ing an annotated corpus 10 times larger for the 
evaluation. 
The automatic NER performance was also 
comparable to human annotators. 
There are several approaches to the generation 
of information extraction patterns (e.g. Soderland 
et al, 1995; Califf et al, 1997; Kim and Moldo-
van, 1995).  Our event patterns are similar to in-
formation extraction rules used in conventional 
IE systems.  However, the goal of this paper is 
not event instance extraction but event (or se-
mantic) frame extraction. We also combined 
CRF-based NER and semantic role labelling 
tuned for gene regulation with event extraction 
from sentences so that the clues of gene regula-
tion event frames could be assigned automati-
cally to un-annotated text. 
9 Conclusion  
This paper has presented linguistic annotation of 
gene regulation events in MEDLINE abstracts, 
and automatic event frame extraction based on 
the annotated corpus. Semantic event frames are 
linguistic resources effective in bridging between 
domain knowledge and text in IE tasks. 
Although biological event annotations carried 
out by domain experts is a challenging task, ex-
perimental results on event frame extraction 
demonstrate a precision of almost 50%, which is 
close to the inter-annotator agreement rate of 
human annotators. 
The extracted event frames will be included in 
the BOOTStrep BioLexicon, which will be made 
available for research purposes. 
Acknowledgement 
This research is supported by EC IST project 
FP6-028099 (BOOTStrep), whose Manchester 
team is hosted by the JISC/BBSRC/EPSRC 
sponsored National Centre for Text Mining. 
 
Table 5. 10-fold cross validation results 
 Score #TP #FN #FP 
Recall  0.186 165 730  
Precision 0.490 165  172 
 
Table 6.  NE identification performance 
NE Type Recall Precision F 
DNA 0.627  0.660  0.643  
Protein 0.525  0.633  0.574  
Experimental 0.224  0.512  0.312  
Processes 0.125  0.337  0.182  
Organisms 0.412  0.599  0.488  
 
767
References 
Califf, Mary E. and Raymond J. Mooney (1997).  
Relational Learning of Pattern-Match Rules for In-
formation Extraction, In Proceedings of the ACL-
97 Workshop in Natural Language Learning, pp 9?
15. 
Chou, Wen-Chi., Richard T.H. Tsai, Ying-Shan Su, 
Wei Ku, Ting-Yi Sung and Wen-Lian Hsu (2006). 
A Semi-Automatic Method for Annotating a Bio-
medical Proposition Bank. In Proceedings of the 
Workshop on Frontiers in Linguistically Annotated 
Corpora 2006, pp 5?12. 
Cohen, K. Bretonnel and Laurence Hunter (2006). A 
critical review of PASBio's argument structures for 
biomedical verbs. BMC Bioinformatics 7 (Suppl. 3), 
S5.  
Dolbey, Andrew, Michael Ellsworth and Jan 
Scheffczykx (2006). BioFrameNet: A Domain-
Specific FrameNet Extension with Links to Bio-
medical Ontologies. In O. Bodenreider (Ed.), In 
Proceedings of KR-MED, pp 87?94. 
Eilbeck, Karen, Suzanna .E Lewis., Christopher J. 
Mungall, Mark Yandell, Lincoln Stein, Richard 
Durbin and Michael Ashburner. (2005) The Se-
quence Ontology: A tool for the unification of ge-
nome annotations. Genome Biology 6:R44 
Kim, Jin-Dong,  Tomoko Ohta and Jun?ichi Tsujii 
(2008).  Corpus annotation for mining biomedical 
events from literature. BMC Bioinformatics 9:10.   
Kim, Jun-Tae and Dan I. Moldovan (1995).  Acquisi-
tion of Linguistic Patterns for Knowledge-Based 
Information Extraction. IEEE Transaction on 
Knowledge and Data Engineering (IEEE TKDE), 
7(5), pp.713?724.   
Kipper-Schuler, Karen (2005). VerbNet: A broad-
coverage, comprehensive verb lexicon. PhD Thesis. 
Computer and Information Science Dept., Univer-
sity of Pennsylvania. Philadelphia, PA. 
Kulick Seth, Ann Bies, Mark Liberman, Mark Mandel,  
Ryan McDonald, Martha Palmer, Andrew Schein, 
and Lyle Ungar  (2004) Integrated Annotation for 
Biomedical Information Extraction. In HLT-
NAACL 2004 Workshop: BioLink 2004, Linking 
Biological Literature, Ontologies and Databases, 
pp 61?68.  
Lafferty John, Andrew McCallum and Fernando 
Pereira (2001).  Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labelling Se-
quence Data. In Proceedings of the Eighteenth In-
ternational Conference on    Machine Learning 
(ICML-2001), pp 282?289.  
Morton, Thomas and Jeremy LaCivita (2003). Word-
Freak: an open tool for linguistic annotation. In 
Proceedings of the 2003 Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology, 
pp 17?18. 
Palmer Martha, Paul Kingsbury and Daniel Gildea 
(2005). The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics, 
31(1), pp 71?106. 
Pyysalo, Sampo, Filip Ginter, Juho Heimonen, Jari 
Bj?rne, Jorma Boberg, Jouni J?rvinen and  Tapio 
Salakoski (2007). BioInfer: a corpus for informa-
tion extraction in the biomedical domain?.  BMC 
Bioinformatics 8:50. 
Ruppenhofer, Josef, Michael Ellsworth, Miriam R.L. 
Petruck, Christopher R. Johnson, and Jan  
Scheffczyk (2006).   FrameNet II: Extended The-
ory and Practice. Available online at 
http://framenet.icsi.berkeley.edu/ 
Soderland, Steven, David Fisher, Jonathan Aseltine 
and  Wendy Lenert (1995). CRYSTAL: Inducing a 
Conceptual Dictionary, In Proceedings of The 13th 
International Joint Conference on Artificial Intelli-
gence (IJCAI-95). pp.1314?1319. 
The Gene Ontology Consortium. (2000). Gene Ontol-
ogy: tool for the unification of biology. Nature Ge-
netetics 25, pp 25?29. 
Tsai Richard T.H, Wen-Chi Chou, Ying-San Su, Yu-
Chun Lin, Chen-Lung Sung, Hong-Jie Dai, Irene 
T.H Yeh, Wei Ku, Ting-Yi Sung and Wen-Lian 
Hsu (2007). BIOSMILE: A semantic role labeling 
system for biomedical verbs using a maximum-
entropy model with automatically generated tem-
plate features, BMC Bioinformatics 8:325  
Tsuruoka, Yoshimasa, Yuka Tateishi, Jin-Dong Kim, 
Tomoko Ohta, John McNaught, Sophia Ananiadou, 
and Jun?ichi Tsujii (2005). Developing a Robust 
Part-of-Speech Tagger for Biomedical Text, In Ad-
vances in Informatics - 10th Panhellenic Confer-
ence on Informatics, pp 382?392. 
Wattarujeekrit, Tuangthong, Parantu K. Shah and 
Nigel Collier (2004). PASBio: predicate-argument 
structures for event extraction in molecular biology, 
BMC Bioinformatics 5:155. 
Wilbur, W.John, Andrey Rzhetsky, and Hagit Shatkay 
(2006). New Directions in Biomedical Text Anno-
tations: Definitions. Guidelines and Corpus Con-
struction. BMC Bioinformatics. 7:356 
Zapirain, Be?at, Eneko Agirre, Llu?s M?rquez (2008). 
A Preliminary Study on the Robustness and Generali-
zation of Role Sets for Semantic Role Labeling. In 
Alexander F. Gelbukh (Ed.), Computational Linguis-
tics and Intelligent Text Processing, 9th International 
Conference, CICLing 2008. 
 
768
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 63?70,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
How to Make the Most of NE Dictionaries in Statistical NER
Yutaka Sasaki2 Yoshimasa Tsuruoka2 John McNaught1,2 Sophia Ananiadou1,2
1 National Centre for Text Mining
2 School of Computer Science, University of Manchester
MIB, 131 Princess Street, Manchester, M1 7DN, UK
Abstract
When term ambiguity and variability are very
high, dictionary-based Named Entity Recogni-
tion (NER) is not an ideal solution even though
large-scale terminological resources are avail-
able. Many researches on statistical NER have
tried to cope with these problems. However,
it is not straightforward how to exploit exist-
ing and additional Named Entity (NE) dictio-
naries in statistical NER. Presumably, addi-
tion of NEs to an NE dictionary leads to bet-
ter performance. However, in reality, the re-
training of NER models is required to achieve
this. We have established a novel way to im-
prove the NER performance by addition of
NEs to an NE dictionary without retraining.
We chose protein name recognition as a case
study because it most suffers the problems re-
lated to heavy term variation and ambiguity.
In our approach, first, known NEs are identi-
fied in parallel with Part-of-Speech (POS) tag-
ging based on a general word dictionary and
an NE dictionary. Then, statistical NER is
trained on the tagger outputs with correct NE
labels attached. We evaluated performance of
our NER on the standard JNLPBA-2004 data
set. The F-score on the test set has been im-
proved from 73.14 to 73.78 after adding the
protein names appearing in the training data to
the POS tagger dictionary without any model
retraining. The performance further increased
to 78.72 after enriching the tagging dictionary
with test set protein names. Our approach
has demonstrated high performance in pro-
tein name recognition, which indicates how
to make the most of known NEs in statistical
NER.
1 Introduction
The accumulation of online biomedical informa-
tion has been growing at a rapid pace, mainly at-
tributed to a rapid growth of a wide range of repos-
itories of biomedical data and literature. The auto-
matic construction and update of scientific knowl-
edge bases is a major research topic in Bioinformat-
ics. One way of populating these knowledge bases
is through named entity recognition (NER). Unfortu-
nately, biomedical NER faces many problems, e.g.,
protein names are extremely difficult to recognize
due to ambiguity, complexity and variability. A fur-
ther problem in protein name recognition arises at
the tokenization stage. Some protein names include
punctuation or special symbols, which may cause to-
kenization to lose some word concatenation infor-
mation in the original sentence. For example, IL-2
and IL - 2 fall into the same token sequence IL
- 2 as usually dash (or hyphen) is designated as a
token delimiter.
Research into NER is centred around three ap-
proaches: dictionary-based, rule-based and machine
learning-based approaches. To overcome the usual
NER pitfalls, we have opted for a hybrid approach
combining dictionary-based and machine learning
approaches, which we call dictionary-based statisti-
cal NER approach. After identifying protein names
in text, we link these to semantic identifiers, such as
UniProt accession numbers. In this paper, we focus
on the evaluation of our dictionary-based statistical
NER.
2 Methods
Our dictionary-based statistical approach consists of
two components: dictionary-based POS/PROTEIN
tagging and statistical sequential labelling. First,
63
dictionary-based POS/PROTEIN tagging finds can-
didates for protein names using a dictionary. The
dictionary maps strings to parts of speech (POS),
where the POS tagset is augmented with a tag
NN-PROTEIN. Then, sequential labelling applies
to reduce false positives and false negatives in the
POS/PROTEIN tagging results. Expandability is
supported through allowing a user of the NER tool to
improve NER coverage by adding entries to the dic-
tionary. In our approach, retraining is not required
after dictionary enrichment.
Recently, Conditional Random Fields (CRFs)
have been successfully applied to sequence labelling
problems, such as POS tagging and NER, and have
outperformed other machine learning techniques.
The main idea of CRFs is to estimate a conditional
probability distribution over label sequences, rather
than over local directed label sequences as with Hid-
den Markov Models (Baum and Petrie, 1966) and
Maximum Entropy Markov Models (McCallum et
al., 2000). Parameters of CRFs can be efficiently
estimated through the log-likelihood parameter esti-
mation using the forward-backward algorithm, a dy-
namic programming method.
2.1 Training and test data
Experiments were conducted using the training and
test sets of the JNLPBA-2004 data set(Kim et al,
2004).
Training data The training data set used in
JNLPBA-2004 is a set of tokenized sentences with
manually annotated term class labels. The sentences
are taken from the Genia corpus (version 3.02) (Kim
et al, 2003), in which 2,000 abstracts were manu-
ally annotated by a biologist, drawing on a set of
POS tags and 36 biomedical term classes. In the
JNLPBA-2004 shared task, performance in extract-
ing five term classes, i.e., protein, DNA, RNA, cell
line, and cell type classes, were evaluated.
Test Data The test data set used in JNLPBA-2004
is a set of tokenized sentences extracted from 404
separately collected MEDLINE abstracts, where the
term class labels were manually assigned, following
the annotation specification of the Genia corpus.
2.2 Overview of dictionary-based statistical
NER
Figure 1 shows the block diagram of dictionary-
based statistical NER. Raw text is analyzed by
a POS/PROTEIN tagger based on a CRF tagging
Figure 1: Block diagram of dictionary-based statistical
NER
Figure 2: Block diagram of training procedure
model and dictionary, and then converted into to-
ken sequences. Strings in the text that match with
protein names in the dictionary will be tagged as
NN-PROTEIN depending on the context around the
protein names. Since it is not realistic to enumer-
ate all protein names in the dictionary, due to their
high variability of form, instead previously unseen
forms are predicted to be protein names by statisti-
cal sequential labelling. Finally, protein names are
identified from the POS/PROTEIN tagged token se-
quences via a CRF labelling model.
Figure 2 shows the block diagram of the train-
ing procedure for both POS/PROTEIN tagging and
sequential labelling. The tagging model is created
using the Genia corpus (version 3.02) and a dic-
tionary. Using the tagging model, MEDLINE ab-
stracts used for the JNLPBA-2004 training data set
are then POS/PROTEIN-tagged. The output token
sequences over these abstracts are then integrated
with the correct protein labels of the JNLPBA-2004
training data. This process results in the preparation
of token sequences with features and correct protein
labels. A CRF labelling model is finally generated
by applying a CRF tool to these decorated token se-
quences.
64
IL/NNP
-/- 2/CD
-/-
mediated/VVD
mediated/VVN
activation/NN
IL-2/NN-PROTEIN
IL-2/NN-PROTEIN
-/-
2/CD
mediated/VVN
mediated/VVD
mediate/VVP
mediate/VV
activation/NN
IL/NNP
IL-2-mediated activation ...
POS/PROTEIN tagging
Lexicon
Figure 3: Dictionary based approach
2.2.1 Dictionary-based POS/PROTEIN tagging
The dictionary-based approach is beneficial when
a sentence contains some protein names that con-
flict with general English words. Otherwise, if the
POS tags of sentences are decided without consider-
ing possible occurrences of protein names, POS se-
quences could be disrupted. For example, in ?met
proto-oncogene precursor?, met might be falsely
recognized as a verb by a non dictionary-based tag-
ger.
Given a sentence, the dictionary-based approach
extracts protein names as follows. Find all word se-
quences that match the lexical entries, and create a
token graph (i.e., trellis) according to the word order.
Estimate the score of every path using the weights of
node and edges estimated by training using Condi-
tional Random Fields. Select the best path.
Figure 3 shows an example of our dictionary-
based approach. Suppose that the input is ?IL-
2-mediated activation?. A trellis is created based
on the lexical entries in a dictionary. The se-
lection criteria for the best path are determined
by the CRF tagging model trained on the Genia
corpus. In this example, IL-2/NN-PROTEIN
-/- mediated/VVN activation/NN is se-
lected as the best path. Following Kudo et al (Kudo
et al, 2004), we adapted the core engine of the
CRF-based morphological analyzer, MeCab1, to our
POS/PROTEIN tagging task. MeCab?s dictionary
databases employ double arrays (Aoe, 1989) which
enable efficient lexical look-ups.
The features used were:
? POS
? PROTEIN
1http://sourceforge.net/project/showfiles.php?group id=177856/
? POS-PROTEIN
? bigram of adjacent POS
? bigram of adjacent PROTEIN
? bigram of adjacent POS-PROTEIN
During the construction of the trellis, white space
is considered as the delimiter unless otherwise stated
within dictionary entries. This means that unknown
tokens are character sequences without spaces.
2.2.2 Dictionary construction
A dictionary-based approach requires the dictio-
nary to cover not only a wide variety of biomedical
terms but also entries with:
? all possible capitalization
? all possible linguistic inflections
We constructed a freely available, wide-coverage
English word dictionary that satisfies these condi-
tions. We did consider the MedPost pos-tagger
package2 which contains a free dictionary that has
downcased English words; however, this dictionary
is not well curated as a dictionary and the number of
entries is limited to only 100,000, including inflec-
tions.
Therefore, we started by constructing an English
word dictionary. Eventually, we created a dictionary
with about 266,000 entries for English words (sys-
tematically covering inflections) and about 1.3 mil-
lion entries for protein names.
We created the general English part of the dictio-
nary from WordNet by semi-automatically adding
POS tags. The POS tag set is a minor modifica-
tion of the Penn Treebank POS tag set3, in that pro-
tein names are given a new POS tag, NN-PROTEIN.
Further details on construction of the dictionary now
follow.
Protein names were extracted from the BioThe-
saurus4. After selecting only those terms
clearly stated as protein names, 1,341,992 pro-
tein names in total were added to the dictionary.
2ftp://ftp.ncbi.nlm.nih.gov/pub/lsmith/MedPost/
3ftp://ftp.cis.upenn.edu/pub/treebank/
doc/tagguide.ps.gz
4http://pir.georgetown.edu/iprolink/
biothesaurus/
65
Nouns were extracted from WordNet?s noun list.
Words starting with lower case and upper case
letters were determined as NN and NNP, re-
spectively. Nouns in NNS and NNPS cate-
gories were collected from the results of POS
tagging articles from Plos Biology Journal5
with TreeTagger6.
Verbs were extracted from WordNet?s verb list. We
manually curated VBD, VBN, VBG and VBZ
verbs with irregular inflections based on Word-
Net. Next, VBN, VBD, VBG and VBZ forms
of regular verbs were automatically generated
from the WordNet verb list.
Adjectives were extracted from WordNet?s adjec-
tive list. We manually curated JJ, JJR and JJS
of irregular inflections of adjectives based on
the WordNet irregular adjective list. Base form
(JJ) and regular inflections (JJR, JJS) of adjec-
tives were also created based on the list of ad-
jectives.
Adverbs were extracted from WordNet?s adverb
list. Both the original and capitalised forms
were added as RB.
Pronouns were manually curated. PRP and PRP$
words were added to the dictionary.
Wh-words were manually curated. As a result,
WDT, WP, WP$ and WRB words were added
to the dictionary.
Words for other parts of speech were manually
curated.
2.2.3 Statistical prediction of protein names
Statistical sequential labelling was employed to
improve the coverage of protein name recognition
and to remove false positives resulting from the pre-
vious stage (dictionary-based tagging).
We used the JNLPBA-2004 training data, which
is a set of tokenized word sequences with
IOB2(Tjong Kim Sang and Veenstra, 1999) protein
labels. As shown in Figure 2, POSs of tokens re-
sulting from tagging and tokens of the JNLPBA-
2004 data set are integrated to yield training data for
sequential labelling. During integration, when the
single token of a protein name found after tagging
5http://biology.plosjournals.org/
6http://www.ims.uni-stuttgart.de/projekte/
corplex/TreeTagger/DecisionTreeTagger.html/
corresponds to a sequence of tokens from JNLPBA-
2004, its POS is given as NN-PROTEIN1, NN-
PROTEIN2,..., according to the corresponding token
order in the JNLPBA-2004 sequence.
Following the data format of the JNLPBA-2004
training set, our training and test data use the IOB2
labels, which are ?B-protein? for the first token of
the target sequence, ?I-protein? for each remaining
token in the target sequence, and ?O? for other to-
kens. For example, ?Activation of the IL 2 precursor
provides? is analyzed by the POS/PROTEIN tagger
as follows.
Activation NN
of IN
the DT
IL 2 precursor NN-PROTEIN
provides VVZ
The tagger output is given IOB2 labels as follows.
Activation NN O
of IN O
the DT O
IL NN-PROTEIN1 B-protein
2 NN-PROTEIN2 I-protein
precursor NN-PROTEIN3 I-protein
provides VVZ O
We used CRF models to predict the IOB2 la-
bels. The following features were used in our ex-
periments.
? word feature
? orthographic features
? the first letter and the last four letters of
the word form, in which capital letters in
a word are normalized to ?A?, lower case
letters are normalized to ?a?, and digits are
replaced by ?0?, e.g., the word form of IL-
2 is AA-0.
? postfixes, the last two and four letters
? POS feature
? PROTEIN feature
The window size was set to ?2 of the current to-
ken.
3 Results and discussion
66
Table 1: Experimental Rusults
Tagging R P F
Full 52.91 43.85 47.96
(a) POS/PROTEIN tagging Left 61.48 50.95 55.72
Right 61.38 50.87 55.63
Sequential Labelling R P F
Full 63.23 70.39 66.62
(b) Word feature Left 68.15 75.86 71.80
Right 69.88 77.79 73.63
Full 77.17 67.52 72.02
(c) (b) + orthographic feature Left 82.51 72.20 77.01
Right 84.29 73.75 78.67
Full 76.46 68.41 72.21
(d) (c) + POS feature Left 81.94 73.32 77.39
Right 83.54 74.75 78.90
Full 77.58 69.18 73.14
(e) (d) + PROTEIN feature Left 82.69 73.74 77.96
Right 84.37 75.24 79.54
Full 79.85 68.58 73.78
(f) (e) + after adding protein names in the Left 84.82 72.85 78.38
training set to the dictionary Right 86.60 74.37 80.02
3.1 Protein name recognition performance
Table 1 shows our protein name recognition results,
showing the differential effect of various combina-
tions of strategies. Results are expressed accord-
ing to recall (R), precision (P), and F-measure (F),
which here measure how accurately our various ex-
periments determined the left boundary (Left), the
right boundary (Right), and both boundaries (Full)
of protein names. The baseline for tagging (row
(a)) shows the protein name detection performance
of our dictionary-based tagging using our large pro-
tein name dictionary, where no training for protein
name prediction was involved. The F-score of this
baseline tagging method was 47.96.
The baseline for sequential labelling (row (b))
shows the prediction performance when using only
word features where no orthographic and POS fea-
tures were used. The F-score of the baseline la-
belling method was 66.62. When orthographic fea-
ture was added (row (c)), the F-score increased by
5.40 to 72.02. When the POS feature was added
(row (d)), the F-score increased by 0.19 to 72.21.
Using all features (row (e)), the F-score reached
73.14. Surprisingly, adding protein names appear-
ing in the training data to the dictionary further im-
proved the F-score by 0.64 to 73.78, which is the
second best score for protein name recognition us-
ing the JNLPBA-2004 data set.
Table 2: After Dictionary Enrichment
Method R P F
Tagging Full 79.02 61.87 69.40
(+test set Left 82.28 64.42 72.26
protein names) Right 80.96 63.38 71.10
Labelling full 86.13 72.49 78.72
(+test set Left 89.58 75.40 81.88
protein names) Right 90.23 75.95 82.47
Tagging and labelling speeds were measured us-
ing an unloaded Linux server with quad 1.8 GHz
Opteron cores and 16GB memory. The dictionary-
based POS/PROTEIN tagger is very fast even
though the total size of the dictionary is more than
one million. The processing speed for tagging and
sequential labelling of the 4,259 sentences of the test
set data took 0.3 sec and 7.3 sec, respectively, which
means that in total it took 7.6 sec. for recognizing
protein names in the plain text of 4,259 sentences.
3.2 Dictionary enrichment
The advantage of the dictionary-based statistical ap-
proach is that it is versatile, as the user can easily
improve its performance with no retraining. We as-
sume the following situation as the ideal case: sup-
pose that a user needs to analyze a large amount of
text with protein names. The user wants to know
67
the maximum performance achievable for identify-
ing protein names with our dictionary-based statis-
tical recognizer which can be achieved by adding
more protein names to the current dictionary. Note
that protein names should be identified in context.
That is, recall of the NER results with the ideal dic-
tionary is not 100%. Some protein names in the ideal
dictionary are dropped during statistical tagging or
labelling.
Table 2 shows the scores after each step of dic-
tionary enrichment. The first block (Tagging) shows
the tagging performance after adding protein names
appearing in the test set to the dictionary. The sec-
ond block (Labelling) shows the performance of the
sequence labelling of the output of the first step.
Note that tagging and the sequence labelling mod-
els are not retrained using the test set.
3.3 Discussion
It is not possible in reality to train the recognizer
on target data, i.e., the test set, but it would be pos-
sible for users to add discovered protein names to
the dictionary so that they could improve the overall
performance of the recognizer without retraining.
Rule-based and procedural approaches are taken
in (Fukuda et al, 1998; Franzen et al, 2002). Ma-
chine learning-based approaches are taken in (Col-
lier et al, 2000; Lee et al, 2003; Kazama et al,
2002; Tanabe and Wilbur, 2002; Yamamoto et al,
2003; Tsuruoka, 2006; Okanohara et al, 2006).
Machine learning algorithms used in these studies
are Naive Bayes, C4.5, Maximum Entropy Models,
Support Vector Machines, and Conditional Random
Fields. Most of these studies applied machine learn-
ing techniques to tokenized sentences.
Table 3 shows the scores reported by other sys-
tems. Tsai et al (Tsai et al, 2006) and Zhou and
Su (Zhou and Su, 2004) combined machine learning
techniques and hand-crafted rules. Tsai et al (Tsai
et al, 2006) applied CRFs to the JNLPBA-2004
data. After applying pattern-based post-processing,
they achieved the best F-score (75.12) among those
reported so far. Kim and Yoon(Kim and Yoon, 2007)
also applied heuristic post-processing. Zhou and Su
(Zhou and Su, 2004) achieved an F-score of 73.77.
Purely machine learning-based approaches have
been investigated by several researchers. The
GENIA Tagger (Tsuruoka, 2006) is trained on
the JNLPBA-2004 Corpus. Okanohara et al
(Okanohara et al, 2006) employed semi-Markov
CRFs whose performance was evaluated against the
JNLPBA-2004 data set. Yamamoto et al (Ya-
mamoto et al, 2003) used SVMs for character-
based protein name recognition and sequential la-
belling. Their protein name extraction performance
was 69%. This paper extends the machine learning
approach with a curated dictionary and CRFs and
achieved high F-score 73.78, which is the top score
among the heuristics-free NER systems. Table 4
shows typical recognition errors found in the recog-
nition results that achieved F-score 73.78. In some
cases, protein name boundaries of the JNLPBA-
2004 data set are not consistent. It is also one of
the reasons for the recognition errors that the data
set contains general protein names, such as domain,
family, and binding site names as well as anaphoric
expressions, which are usually not covered by pro-
tein name repositories. Therefore, our impression on
the performance is that an F-score of 73.78 is suffi-
ciently high.
Furthermore, thanks to the dictionary-based ap-
proach, it has been shown that the upper bound per-
formance using ideal dictionary enrichment, with-
out any retraining of the models, has an F-score of
78.72.
4 Conclusions
This paper has demonstrated how to utilize known
named entities to achieve better performance in sta-
tistical named entity recognition. We took a two-
step approach where sentences are first tokenized
and tagged based on a biomedical dictionary that
consists of general English words and about 1.3 mil-
lion protein names. Then, a statistical sequence
labelling step predicted protein names that are not
listed in the dictionary and, at the same time, re-
duced false negatives in the POS/PROTEIN tagging
results. The significant benefit of this approach is
that a user, not a system developer, can easily en-
hance the performance by augmenting the dictio-
nary. This paper demonstrated that the state-of-
the-art F-score 73.78 on the standard JNLPBA-2004
data set was achieved by our approach. Further-
more, thanks to the dictionary-based NER approach,
the upper bound performance using ideal dictionary
enrichment, without any retraining of the models,
yielded F-score 78.72.
5 Acknowledgments
This research is partly supported by EC IST project
FP6-028099 (BOOTStrep), whose Manchester team
is hosted by the JISC/BBSRC/EPSRC sponsored
National Centre for Text Mining.
68
Table 3: Conventional results for protein name recognition
Authors R P F
Tsai et al(Tsai et al, 2006) 71.31 79.36 75.12
Our system 79.85 68.58 73.78
Zhou and Su(Zhou and Su, 2004) 69.01 79.24 73.77
Kim and Yoon(Kim and Yoon, 2007) 75.82 71.02 73.34
Okanohara et al(Okanohara et al, 2006) 77.74 68.92 73.07
Tsuruoka(Tsuruoka, 2006) 81.41 65.82 72.79
Finkel et al(Finkel et al, 2004) 77.40 68.48 72.67
Settles(Settles, 2004) 76.1 68.2 72.0
Song et al(Song et al, 2004) 65.50 73.04 69.07
Ro?ssler(Ro?ssler, 2004) 72.9 62.0 67.0
Park et al(Park et al, 2004) 69.71 59.37 64.12
References
J. Aoe, An Efficient Digital Search Algorithm by Using
a Double-Array Structure, IEEE Transactions on Soft-
ware Engineering, 15(9):1066?1077, 1989.
L.E. Baum and T. Petrie, Statistical inference for proba-
bilistic functions of finite state Markov chains, The An-
nals of Mathematical Statistics, 37:1554?1563, 1966.
J. Chang, H. Schutze, R. Altman, GAPSCORE: Finding
Gene and Protein names one Word at a Time, Bioin-
formatics, Vol. 20, pp. 216-225, 2004.
N. Collier, C. Nobata, J. Tsujii, Extracting the Names
of Genes and Gene Products with a Hidden Markov
Model, Proc. of the 18th International Conference
on Computational Linguistics (COLING?2000), Saar-
brucken, 2000.
Jenny Finkel, Shipra Dingare, Huy Nguyen, Malvina Nis-
sim, Gail Sinclair and Christopher Manning, Exploit-
ing Context for Biomedical Entity Recognition: From
Syntax to the Web, Proc. of the Joint Workshop on Nat-
ural Language Processing in Biomedicine and its Ap-
plications (JNLPBA-2004), pp. 88?91, 2004.
K. Franzen, G. Eriksson, F. Olsson, L. Asker, P. Liden,
and J. Koster, Protein Names and How to Find Them,
Int. J. Med. Inf., Vol. 67, pp. 49?61, 2002.
K. Fukuda, T. Tsunoda, A. Tamura, and T. Takagi,
Toward information extraction: identifying protein
names from biological papers, PSB, pp. 705-716,
1998.
J. Kazama, T. Makino, Y. Ohta, J. Tsujii, Tuning Support
Vector Machines for Biomedical Named Entity Recog-
nition, Proc. of ACL-2002 Workshop on Natural Lan-
guage Processing in the Biomedical Domain, pp. 1?8,
2002.
J.-D. Kim, T. Ohta, Y. Tateisi, J. Tsujii: GENIA corpus
- semantically annotated corpus for bio-textmining,
Bioinformatics 2003, 19:i180-i182.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,
Yuka Tateisi, Introduction to the Bio-Entity Recogni-
tion Task at JNLPBA, Proc. of the Joint Workshop on
Natural Language Processing in Biomedicine and its
Applications (JNLPBA-2004), pp. 70?75, 2004.
S. Kim, J. Yoon: Experimental Study on a Two Phase
Method for Biomedical Named Entity Recognition,
IEICE Transactions on Informaion and Systems 2007,
E90-D(7):1103?1120.
Taku Kudo and Kaoru Yamamoto and Yuuji Matsumoto,
Applying Conditional Random Fields to Japanese
Morphological Analysis, Proc. of Empirical Methods
in Natural Language Processing (EMNLP), pp. 230?
237, 2004.
J. Lafferty, A. McCallum, and F. Pereira, Conditional
Random Fields: Probabilistic Models for Segment-
ing and Labeling Sequence Data, Proc. of ICML-2001,
pp.282?289, 2001
K. J. Lee, Y. S. Hwang and H. C. Rim (2003), Two-Phase
Biomedical NE Recognition based on SVMs, Proc. of
ACL 2003 Workshop on Natural Language Processing
in Biomedicine, Sapporo, 2003.
McCallum A, Freitag D, Pereira F.: Maximum entropy
Markov models for information extraction and seg-
mentation, Proceedings of the Seventeenth Interna-
tional Conference on Machine Learning, 2000:591-
598.
Daisuke, Okanohara, Yusuke Miyao, Yoshimasa Tsu-
ruoka and Jun?ichi Tsujii, Improving the Scalability of
Semi-Markov Conditional Random Fields for Named
Entity Recognition, Proc. of ACL 2006, Sydney, 2006.
Kyung-Mi Park, Seon-Ho Kim, Do-Gil Lee and
Hae-Chang Rim. Boosting Lexical Knowledge for
Biomedical Named Entity Recognition, Proc. of the
Joint Workshop on Natural Language Processing in
Biomedicine and its Applications (JNLPBA-2004), pp.
76-79, 2004.
Marc Ro?ssler, Adapting an NER-System for German to
the Biomedical Domain, Proc. of the Joint Workshop
on Natural Language Processing in Biomedicine and
its Applications (JNLPBA-2004), pp. 92?95, 2004.
Burr Settles, Biomedical Named Entity Recognition Us-
ing Conditional Random Fields and Novel Feature
69
Table 4: Error Analysis
False positives
Cause Correct extraction Identified term
1 dictionary - protein, binding sites
2 prefix word trans-acting factor common trans-acting factor
3 unknown word - ATTTGCAT
4 sequential labelling error - additional proteins
5 test set error - Estradiol receptors
False negatives
Cause Correct extraction Identified term
1 anaphoric (the) receptor, (the) binding sites -
2 coordination (and, or) transcription factors NF-kappa B and AP-1 transcription factors NF-kappa B
3 prefix word activation protein-1 protein-1
catfish STAT STAT
4 postfix word nuclear factor kappa B complex nuclear factor kappa B
5 plural protein tyrosine kinase(s) protein tyrosine kinase
6 family name, biding site, T3 binding sites -
and domain residues 639-656 -
7 sequential labelling error PCNA -
Chloramphenicol acetyltransferase -
8 test set error superfamily member -
Sets, Proc. of the Joint Workshop on Natural Lan-
guage Processing in Biomedicine and its Applications
(JNLPBA-2004), pp. 104?1007, 2004.
Yu Song, Eunju Kim, Gary Geunbae Lee and Byoung-
kee Yi, POSBIOTM-NER in the shared task of
BioNLP/NLPBA 2004, Proc. of the Joint Workshop on
Natural Language Processing in Biomedicine and its
Applications (JNLPBA-2004), pp. 100-103, 2004.
L. Tanabe and W. J. Wilbur, Tagging Gene and Protein
Names in Biomedical Text, Bioinformatics, 18(8), pp.
1124?1132, 2002.
E.F. Tjong Kim Sang and J. Veenstra, Representing Text
Chunks,EACL-99, pp. 173-179, 1999.
Richard Tzong-Han Tsai, W.-C. Chou, S.-H. Wu, T.-Y.
Sung, J. Hsiang, and W.-L. Hsu, Integrating Linguistic
Knowledge into a Conditional Random Field Frame-
work to Identify Biomedical Named Entities, Expert
Systems with Applications, 30 (1), 2006.
Yoshimasa Tsuruoka, GENIA Tagger 3.0,
http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA/tagger/, 2006.
K. Yamamoto, T. Kudo, A. Konagaya and Y. Matsumoto,
Protein Name Tagging for Biomedical Annotation in
Text, in Proc. of ACL-2003 Workshop on Natural Lan-
guage Processing in Biomedicine, Sapporo, 2003.
Guofeng Zhou and Jian Su, Exploring Deep Knowledge
Resources in Biomedical Name Recognition, Proceed-
ings of the Joint Workshop on Natural Language Pro-
cessing of Biomedicine and its Applications (JNLPBA-
2004), pp. 96-99, 2004.
70
Empirical Study of Utilizing Morph-Syntactic
Information in SMT
Young-Sook Hwang, Taro Watanabe, and Yutaka Sasaki
ATR SLT Research Labs, 2-2-2 Hikaridai Seika-cho,
Soraku-gun Kyoto, 619-0288, Japan
{youngsook.hwang, taro.watanabe, yutaka.sasaki}@atr.jp
Abstract. In this paper, we present an empirical study that utilizes
morph-syntactical information to improve translation quality. With three
kinds of language pairs matched according to morph-syntactical similar-
ity or difference, we investigate the effects of various morpho-syntactical
information, such as base form, part-of-speech, and the relative positional
information of a word in a statistical machine translation framework.
We learn not only translation models but also word-based/class-based
language models by manipulating morphological and relative positional
information. And we integrate the models into a log-linear model. Ex-
periments on multilingual translations showed that such morphological
information as part-of-speech and base form are effective for improving
performance in morphologically rich language pairs and that the relative
positional features in a word group are useful for reordering the local
word orders. Moreover, the use of a class-based n-gram language model
improves performance by alleviating the data sparseness problem in a
word-based language model.
1 Introduction
For decades, many research efforts have contributed to the advance of statisti-
cal machine translation. Such an approach to machine translation has proven
successful in various comparative evaluations. Recently, various works have im-
proved the quality of statistical machine translation systems by using phrase
translation [1,2,3,4] or using morpho-syntactic information [6,8]. But most sta-
tistical machine translation systems still consider surface forms and rarely use
linguistic knowledge about the structure of the languages involved[8]. In this
paper, we address the question of the effectiveness of morpho-syntactic features
such as parts-of-speech, base forms, and relative positions in a chunk or an ag-
glutinated word for improving the quality of statistical machine translations.
Basically, we take a statistical machine translation model based on an IBM
model that consists of a language model and a separate translation model [5]:
eI1 = argmaxeI1Pr(f
J
1 |eI1)Pr(eI1) (1)
The translation model links the source language sentence to the target language
sentence. The target language model describes the well-formedness of the target
language sentence.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 474?485, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Empirical Study of Utilizing Morph-Syntactic Information in SMT 475
One of the main problems in statistical machine translation is to learn the
less ambiguous correspondences between the words in the source and target
languages from the bilingual training data. When translating one source lan-
guage(which may be inflectional or non-inflectional) into the morphologically
rich language such like Japanese or Korean, the bilingual training data can be
exploited better by explicitly taking into account the interdependencies of re-
lated inflected or agglutinated forms. In this study, we represent a word with
its morphological features in both sides of the source and the target language
to learn less ambiguous correspondences between the source and the target lan-
guage words or phrases. In addition, we utilize the relative positional information
of a word in its word group to consider the word order in an agglutinated word
or a chunk.
Another problem is to produce a correct target sentence. To produce more
correct target sentence, we should consider the following problems: word re-
ordering in a language pair with different word order, production of correct
inflected and agglutinated words in an inflectional or agglutinative target lan-
guage. In this study, we tackle the problem with language models. For learning
language model that can treat morphological and word-order problem, we rep-
resent a word with its morphological and positional information. However, a
word-based language model with enriched word is likely to suffer from a severe
data sparseness problem. To alleviate the problem, we interpolate the word-based
language model with a class-based n-gram model.
In the next section, we briefly discuss related works. Then, we describe the
method that utilizes morpho-syntactic information under consideration for im-
proving the quality of translations. Then we report the experimental results with
some analysis and conclude our study.
2 Related Work
Few papers deal with the integration of linguistic information into the process of
statistical machine translation. [8] introduced hierarchical lexicon models includ-
ing base-form and POS information for translation from German into English.
Irrelevant information contained in the German entries for the generation of the
English translation were omitted. They trained the lexicon model using maxi-
mum entropy. [6] enriched English with knowledge to help select the correct full-
form from morphologically richer languages such as Spanish and Catalan. In other
words, they introduced a splicing operation that merged the pronouns/modals
and verbs for treating differences in verbal expressions. To treat the unknown en-
tries in the lexicon resulting from the splicing operation, they trained the lexicon
model using maximum entropy and used linguistic knowledge just in the source
language part and not in the target language. They don?t use any linguistic knowl-
edge in the target language and use full-form words during training.
In addition, [6] and [8] proposed re-ordering operations to make similar word
orders in the source and target language sentences. In other words, for the in-
terrogative phrases with different word order from the declarative sentences,
476 Y.-S. Hwang, T. Watanabe, and Y. Sasaki
they introduced techniques of question inversion and removed unnecessary aux-
iliary verbs. But, such inversion techniques require additional preprocessing with
heuristics.
Unlike them, we investigate methods for utilizing linguistic knowledge in
both of the source and the target language at the morpheme level. To generate
a correct full-form word in a target language, we consider not only both the
surface and base form of a morpheme but also the relative positional informa-
tion in a full-form word. We strongly utilize the combined features in language
modeling. By training alignments and language models with morphological and
positional features at the morpheme-level, the severe data sparseness problem
can be alleviated with the combined linguistic features. And the correspondence
ambiguities between the source and target words can be decreased.
3 Utilization of Morpho-Syntactic Information in SMT
Generally, the probabilistic lexicon resulting from training a translation model
contains all word forms occurring in the training corpus as separate entries,
not taking into account whether they are inflected forms. A language model
is also composed of the words in the training corpus. However, the use of a
full-form word itself may cause severe data sparseness problem, especially rel-
evant for more inflectional/agglutinative languages like Japanese and Korean.
One alternative is to utilize the results of morphological analysis such as base
form, part-of-speech and other information at the morpheme level. We address
the usefulness of morphological information to improve the quality of statistical
machine translation.
3.1 Available Morpho-Syntactic Information
A prerequisite for methods that improve the quality of statistical machine trans-
lation is the availability of various kinds of morphological and syntactic infor-
mation. In this section, we examine the morpho-syntactic information available
from the morphological analyzers of Korean, Japanese, English and Chinese and
describe a method of utilizing the information.
Japanese and Korean are highly inflectional and agglutinative languages, and
in English inflection has only a marginal role; whereas Chinese usually is regarded
as an isolating language since it has almost no inflectional morphology. As the
syntactic role of each word within Japanese and Korean sentences are often
marked, word order in a sentence plays a relatively small role in characterizing
the syntactic function of each word than in English or Chinese sentences. Thus,
Korean and Japanese sentences have a relatively free word order; whereas words
within Chinese and English sentences adhere to a rigid order. The treatment
of inflection, and not word order, plays the most important role in processing
Japanese and Korean, while word order has a central role in Chinese and English.
Figure 1 shows some examples of morphological information by Chinese,
Japanese, English and Korean morphological analyzers and Figure 2 the corre-
spondences among the words. Note that Korean and Japanese are very similar:
Empirical Study of Utilizing Morph-Syntactic Information in SMT 477
Fig. 1. Examples of linguistic information from Chinese, Japanese, English, and Korean
morphological analyzers
Fig. 2. Correspondences among the words in parallel sentences
highly inflected and agglutinated. One difference in Korean from Japanese is
that a Korean sentence consists of spacing units, eojeols,1 while there are no
space in a Japanese sentence. Especially, a spacing unit(i.e., eojeol) in Korean
often becomes a base phrase that contains such syntactic information as subject,
object, and the mood/tense of a verb in a given sentence. The treatment of such
a Korean spacing unit may contribute to the improvement of translation quality
because a morpheme can be represented with its relative positional information
within an eojeol. The relative positional information is obtained by calculating
the distance between the beginning syllable of a given eojeol and the beginning
of each morpheme within the eojeol. The relative positional information is rep-
resented with indexes of the beginning and the ending syllables (See Figure 1).
3.2 Word Representation
A word(i.e. morpheme) is represented by the combination of the information pro-
vided by a morphological analyzer including the surface form, base form, part-of-
speech or other information such as relative position within an eojeol. The word
1 An eojeol is composed of no less than one morpheme by agglutination principle.
478 Y.-S. Hwang, T. Watanabe, and Y. Sasaki
Table 1. Word Representation According to Morpho-Syntactic Characteristics (S: sur-
face form, B:base form, P:part-of-speech, L:RelativePosition)
Chinese English Japanese Korean
Morph-Syntactic no inflection Inflectional Inflectional, Inflectional
Characteristics Agglutinative Agglutinative
Spacing Unit
(Word-Order) Rigid Rigid Partial Free Partial Free
Word Representation S?P S?B?P S?B?P S?B?P?L
S?B, S?P S?B, S?P S?B?P, S?B?L, S?P?L
S?B, S?P, S?L
enriched by the combination of morph-syntactic information must alway include
the surface form of a given word for the direct generation of target sentence
without any post-processing. Other different morphological information is com-
bined according to representation models such as surface plus base form (SB),
surface plus part-of-speech (SP), surface plus relative position (SL), and so on.
Table 1 shows the word representation of each language with every possible
morphological information. Yet, we are not limited to only this word represen-
tation, but we have many possibilities of word representation by removing some
morphological information or inserting additional morpho-syntactic information
as mentioned previously. In order to develop the best translation systems, we
select the best word representation models of the source and the target language
through empirical experiments.
The inherent in the original word forms is augmented by a morphological
analyzer. Of course, this results in an enlarged vocabulary while it may provide
useful disambiguation clues. However, since we regard a morpheme as a word
in a corpus(henceforth, we call a morpheme a word), the enlarged vocabulary
does not make more severe data sparseness problem than using the inflected or
agglutinated word. By taking the approch of morpheme-level alignment, we may
obtain more accurate correspondences among words as illustrated in Figure 2.
Moreover, by learning the language model with rich morph-syntactic informa-
tion, we can generate more syntactically fluent and correct sentence.
3.3 Log-Linear Model for Statistical Machine Translation
In order to improve translation quality, we evaluate the translation candidates
by using the relevant features in a log-linear model framework[11]. The log-linear
model used in our statistical translation process, Pr(eI1|fJ1 ), is:
Pr(eI1|f I1 ) =
exp(
?
m ?mhm(e
I
1, f
J
1 , a
J
1 ))
?
e?I1 ,f
I
1 ,a
I
1
exp(
?
m ?mhm(e
?I
1 , f
J
1 , a
J
1 ))
(2)
where hm(eI1, f
J
1 , a
J
1 ) is the logarithm value of the m-th feature; ?m is the weight
of the m-th feature. Integrating different features in the equation results in dif-
ferent models.
Empirical Study of Utilizing Morph-Syntactic Information in SMT 479
The statistical machine translation process in IBM models is as follows; a
given source string fJ1 = f1 ? ? ? fJ is to be translated into eI1 = e1 ? ? ? eI . Accord-
ing to the Bayes? decision rule, we choose the optimal translation for given string
fJ1 that maximizes the product of target language model Pr(e
I
1) and translation
model Pr(fJ1 |eI1)
eI1 = argmaxeI1Pr(f
J
1 |eI1)Pr(eI1) (3)
In IBM model 4, translation model P (fJ1 |eI1) is further decomposed into four
submodels:
? Lexicon Model, t(f |e): probability of word f in the source language being
translated into word e in the target language.
? Fertility model, n(?|e): probability of target language word e generating ?
words.
? Distortion model d: probability of distortion, which is decomposed into the
distortion probabilities of head words and non-head words.
? NULL translation model p1: a fixed probability of inserting a NULL word
after determining each target word.
In addition to the five features (Pr(eI1), t(f |e), n(?|e), d, p1) from IBM model
4, we incorporate the following features into the log-linear translation model:
? Class-based n-gram model Pr(eI1) =
?
i Pr(ei|ci)Pr(ci|c
i?1
1 ): Grouping of
words into C classes is done according to the statistical similarity of their
surroundings. Target word ei is mapped into its class, ci, which is one of C
classes[13].
? Length model Pr(l|eI1, fJi ): l is the length (number of words) of a translated
target sentence.
? Example matching score: The translated target sentence is matched with
phrase translation examples. A score is derived based on the number of
matches [10]. To extract phrase translation examples, we compute the inter-
section of word alignment of both directions and derive the union. Then we
grab the phrase translation pairs that contain at least one intersected word
alignment and some unioned word alignments[1].
Under the framework of log-linear models, we investigate the effects of morpho-
syntactic information with word representation. The overall training and testing
process with morphological and positional information is depicted in Figure 3. In
the training step, we train the word- and class-based language models with var-
ious word representation methods[12]. Also, we make word alignments through
the learning of IBM models by using GIZA++ toolkit[3]: we learn the translation
model toward IBM model 4, initiating translation iterations from IBM model
1 with intermediate HMM model iterations. Then, we extract example phrases
and translation model features from the alignment results.
Then in the test step, we perform morphological anlysis of a given sentence for
word representation corresponding to training corpus representation. We decode
the best translation of a given test sentence by generating word graphs and
searching for the best hypothesis in a log-linear model[7].
480 Y.-S. Hwang, T. Watanabe, and Y. Sasaki
Fig. 3. Overview of training and test of statistical machine translation system with
linguistic information
4 Experiments
4.1 Experimental Environments
The corpus for the experiment was extracted from the Basic Travel Expression
Corpus (BTEC), a collection of conversational travel phrases for Chinese, En-
glish, Japanese and Korean[15]. The entire corpus was split into three parts:
152,169 sentences in parallel for training, 10,150 sentences for testing and the
remaining 10,148 sentences for parameter tuning, such as termination criteria
for training iteration and parameter tuning for decoders. For the reconstruction
of each corpus with morphological information, we used in-house morphological
Table 2. Statistics of Basic Travel Expression Corpus
Chinese English Japanese Korean
# of sentences 167,163
# of words(morph) 1,006,838 1,128,151 1,226,774 1,313,407
Vocabulary size(S) 17,472 11,737 19,485 17,600
Vocabulary size(B) 17,472 9172 15,939 15,410
Vocabulary size(SB) 17,472 13,385 20,197 18,259
Vocabulary size(SP) 18,505 13,467 20,118 20,249
Vocabulary size(SBP(L)) 18,505 14,408 20,444 20,369(26,668)
# of singletons(S) 7,137 4,046 8,107 7,045
# of singletons(B) 7,137 3,025 6,497 6,303
# of singletons(SB) 7,137 4,802 9,453 7,262
# of singletons(SP) 7,601 4,693 8,343 7,921
# of singletons(SBP(L)) 7,601 5,140 8,525 7,983(11,319)
Empirical Study of Utilizing Morph-Syntactic Information in SMT 481
Table 3. Perplexities of tri-gram language model trained on the training corpora
with S, SB, SP SBP, SBL, and SBPL morpho-syntactic representation: word-based
3-gram/class-based 5-gram
S SB SP SBP SBL SBPL
Chinese 31.57/24.09 N/S 35.83/26.28 N/A N/A N/A
English 22.35/18.82 22.19/18.54 22.24/18.12 22.08/18.03 N/A N/A
Japanese 17.89/ 13.44 17.92/13.29 17.82/13.13 17.83/13.06 N/A N/A
Korean 15.54/12.42 15.41/12.09 16.04/11.89 16.03/11.88 16.48/12.24 17.13/11.99
analyzers for four languages: Chinese morphological analyzer with 31 parts-of-
speech tags, English morphological analyzer with 34 tags, Japanese morphologi-
cal analyzer with 34 tags, and Korean morphological analyzer with 49 tags. The
accuracies of Chinese, English, Japanese and Korean morphological analyzers in-
cluding segmentation and POS tagging are 95.82% , 99.25%, 98.95%, and 98.5%
respectively. Table 2 summarizes the morph-syntactic statistics of the Chinese,
English, Japanese, and Korean.
For the four languages, word-based and class-based n-gram language models
were trained on the training set by using SRILM toolkit[12]. The perplexity of
each language model is shown in Table 3.
For the four languages, we chose three kinds of language pairs according to
the linguistic characteristics of morphology and word order, Chinese-Korean,
Japanese-Korean, and English-Korean. 42 translation models based on word
representation methods(S, SB, SP, SBP, SBL, SPL,SBPL) were trained by using
GIZA++[3].
4.2 Evaluation
Translation evaluations were carried out on 510 sentences selected randomly
from the test set. The metrics for the evaluations are as follows:
mWER(multi-reference Word Error Rate), which is based on the minimum
edit distance between the target sentence and the sentences in the reference
set [9].
BLEU, which is the ratio of the n-gram for the translation results found in the
reference translations with a penalty for too short sentences [14].
NIST which is a weighted n-gram precision in combination with a penalty for
too short sentences.
For this evaluation, we made 16 multiple references available. We computed all
of the above criteria with respect to these multiple references.
Table 4, 5 and 6 show the evaluation results on three kinds of language pairs.
The effects of morpho-syntactic information and class-based n-gram language
models on multi-lingual machine translation are shown: The combined morpho-
logical information was useful for improving the translation quality in the NIST,
BLEU and mWER evaluations. Moreover, the class-based n-gram language mod-
els were effective in the BLEU and the mWER scores.
482 Y.-S. Hwang, T. Watanabe, and Y. Sasaki
Table 4. Evaluation results of Japanese to Korean and Korean to Japaneses transla-
tions(with class-based n-gram/word-based n-gram language model)
J to K K to J
NIST BLEU WER NIST BLEU WER
S 8.46/8.64 0.694/0.682 26.33/26.73 8.21/8.39 0.666/0.649 25.00/25.81
SB 8.05/8.32 0.705/0.695 26.82/26.97 7.67/8.17 0.690/0.672 23.77/24.68
SP 9.15/9.25 0.755/0.747 21.71/22.22 9.02/9.13 0.720/0.703 21.94/23.50
SL 8.37/8.47 0.699/0.667 25.49/27.76 8.48/8.74 0.671/0.629 25.14/27.88
SBL 8.92/9.12 0.748/0.730 22.66/23.36 8.85/8.92 0.712/0.691 21.88/23.37
SBP 8.19/8.57 0.713/0.696 26.17/27.09 8.21/8.39 0.698/0.669 22.94/24.88
SBPL 8.41/8.85 0.772/0.757 22.30/21.74 7.77/7.83 0.626/0.619 25.19/25.57
Table 5. Evaluation results of English to Korean and Korean to English transla-
tions(with class-based n-gram/word-based n-gram language model)
E to K K to E
NIST BLEU WER NIST BLEU WER
S 5.12/5.79 0.353/0.301 51.12/58.52 5.76/6.05 0.300/0.255 52.54/61.23
SB 6.71/6.87 0.533/0.474 39.10/47.18 7.72/8.15 0.482/0.446 37.86/42.71
SP 6.88/7.19 0.552/0.502 37.63/42.34 8.01/8.46 0.512/0.460 35.13/40.91
SL 6.66/6.96 0.546/0.516 38.20/40.67 7.71/8.02 0.484/0.436 36.79/42.88
SPL 6.16/7.01 0.542/0.519 38.21/39.85 7.83/8.22 0.482/0.443 37.52/41.63
SBL 6.52/6.93 0.547/0.504 37.76/42.23 7.64/8.08 0.479/0.439 37.10/42.30
SBP 7.42/7.60 0.612/0.573 32.17/35.96 8.86/9.05 0.551/0.523 33.13/37.07
SBPL 6.29/6.59 0.580/0.561 36.73/38.36 8.08/8.36 0.528/0.515 36.46/38.21
Table 6. Evaluation results of Chinese to Korean and Korean to Chinese transla-
tions(with class-based n-gram/word-based n-gram language model)
C to K K to C
NIST BLEU WER NIST BLEU WER
S 7.62/7.82 0.640/0.606 30.01/32.79 7.85/7.69 0.380/0.365 53.65/58.46
SB 7.73/7.98 0.643/0.632 29.26/30.08 7.68/7.50 0.366/0.349 54.48/60.49
SP 7.71/7.98 0.651/0.643 28.26/28.60 8.00/7.77 0.383/0.362 54.15/58.30
SL 7.64/7.97 0.656/0.635 28.94/30.33 7.84/7.65 0.373/0.350 54.53/58.38
SPL 7.69/7.93 0.665/0.659 28.43/28.88 7.78/7.62 0.373/0.351 56.14/59.54
SBL 7.65/7.94 0.659/0.635 28.76/30.87 7.85/7.64 0.377/0.354 55.01/58.39
SBP 7.81/7.98 0.660/0.643 28.85/29.61 7.94/7.68 0.386/0.360 53.99/58.94
SBPL 7.64/7.90 0.652/0.634 29.54/30.46 7.82/7.66 0.376/0.358 55.64/58.79
In detail, Table 4 shows the effects of the morphological and relative posi-
tional information on Japanese-to-Korean and Korean-to-Japanese translation.
In almost of the evaluation metrics, the SP model in which a word is repre-
sented by a combination of its surface form and part-of-speech showed the best
performance. The SBL model utilizing the base form and relative positional
information only in Korean showed the second best performance. In Korean-
Empirical Study of Utilizing Morph-Syntactic Information in SMT 483
to-Japanese translation, the SBPL model showed the best score in BLEU and
mWER. In this language pair of highly inflectional and agglutinative languages,
the part-of-speech information combined with surface form was the most ef-
fective in improving the performance. The base form and relative positional
information were less effective than part-of-speech. It could be explained in sev-
eral points: Japanese and Korean are very similar languages in the word order
of SOVs and the ambiguities of translation correspondences in both directions
were converged into 1.0 by combining the distinctive morphological information
with the surface form. When refering to the vocabulay size of SP model in Table
2, it makes it more clear. The Japanese-to-Korean translation outperforms the
Korean-to-Japanese. It might be closely related to the language model: the per-
plexity of the Korean language model is lower than Japanese according to our
corpus statistics.
Table 5 shows the performance of the English-to-Korean and Korean-to-
English translation: a pair of highly inflectional and agglutinative language with
partially free word-order and an inflectional language with rigid word order. In
this language pair, the combined word representation models improved the trans-
lation performance into significantly higher BLEU and mWER scores in both
directions. The part-of-speech and the base form information were distinctive
features. When comparing the performance of SP, SB and SL models, part-of-
speech might be more effective than base form or relative positional information,
and the relative positional information in Korean might play a role not only in
controlling word order in the language models but also in discriminating word
correspondences during alignment.
When the target language was Korean, we had higher BLEU scores in all the
morpho-syntactic models but lower NIST scores. In other words, we took advan-
tage of generating more accurate full-form eojeol with positional information,
i.e. local word ordering.
Table 6 shows the performance of the Chinese-to-Korean and Korean-to-
Chinese translation: a pair of a highly inflectional and agglutinative language
with partially free word order and a non-inflectional language with rigid word
order. This language pair is a quite morpho-syntactically different. When a non-
inflectional language is a target language(i.e. Korean-to-Chinese translation), the
performance was the worst compared with other language pairs and directions in
BLEU and mWER. On the other hand, the performance of Chinese-to-Korean
was much better than Korean-to-Chinese, meaning that it is easier to generate
Korean sentence from Chinese the same as in Japanese-to-Korean and English-
to-Korean. In this language pair, we had gradual improvements according to
the use of combined morpho-syntactic information, but there was no significant
difference from the use of only the surface form. There was scant contribution of
Chinese morphological information such as part-of-speech. On the other hand,
we could get some advantageous Korean morpho-syntactic information in the
Chinese-to-Korean translation, i.e., the advantage of language and translation
models using morpho-syntactic information.
484 Y.-S. Hwang, T. Watanabe, and Y. Sasaki
5 Conclusion and Future Works
In this paper, we described an empirical study of utilizing morpho-syntactic
information in a statistical machine translation framework. We empirically in-
vestigated the effects of morphological information with several language pairs:
Japanese and Korean with the same word order and high inflection/
agglutination, English and Korean, a pair of a highly inflecting and agglutinat-
ing language with partial free word order and an inflecting language with rigid
word order, and Chinese-Korean, a pair of a highly inflecting and agglutinating
language with partially free word order and a non-inflectional language with
rigid word order. As the results of experiments, we found that combined mor-
phological information is useful for improving the translation quality in BLEU
and mWER evaluations. According to the language pair and the direction, we
had different combinations of morpho-syntactic information that are the best
for improving the translation quality: SP(surface form and part-of-speech) for
translating J-to-K or K-to-J, SBP(surface form, base form and part-of-speech)
for E-to-K or K-to-E, SPL(surface form, part-of-speech and relative position) for
C-to-K. The utilization of morpho-syntactic information in the target language
was the most effective. Language models based on morpho-syntactic informa-
tion were very effective for performance improvement. The class-based n-gram
models improved the performance with smoothing effects in the statistical lan-
guage model. However, when translating an inflectional language, Korean into
a non-inflectional language, Chinese with quite different word order, we found
very few advantages using morphological information. One of the main reasons
might be the relatively low performance of the Chinese morphological analyzer.
The other might come from the linguistic difference. For the latter case, we need
to adopt approaches to reflect the structural characteristics such like using a
chunker/parser, context-dependent translation modeling.
Acknowledgments
The research reported here was supported in part by a contract with the National
Institute of Information and Communications Technology entitled ?A study of
speech dialogue translation technology based on a large corpus?.
References
1. Koehn P., Och F.J., and Marcu D.: Statistical Phrase-Based Translation, Proc. of
the Human Language Technology Conference(HLT/NAACL) (2003)
2. Och F. J., Tillmann C., Ney H.: Improved alignment models for statistical machine
translation, Proc. of EMNLP/WVLC (1999).
3. Och F.J. and Ney H. Improved Statistical Alignment Models, Proc. of the 38th
Annual Meeting of the Association for Computational Linguistics (2000) pp. 440-
447.
4. Zens R. and Ney H.: Improvements in Phrase-Based Statistical Machine Transla-
tion, Proc. of the Human Language Technology Conference (HLT-NAACL) (2004)
pp. 257-264
Empirical Study of Utilizing Morph-Syntactic Information in SMT 485
5. Brown P. F., Della Pietra S. A., Della Pietra V. J., and Mercer R. L.: The math-
ematics of statistical machine translation: Parameter estimation, Computational
Linguistics, (1993) 19(2):263-311
6. Ueffing N., Ney H.: Using POS Information for Statistical Machine Translation
into Morphologically Rich Languages, In Proc. 10th Conference of the European
Chapter of the Association for Computational Linguistics (EACL), (2003) pp. 347-
354
7. Ueffing N., Och F.J., Ney H.: Generation of Word Graphs in Statistical Machine
Translation In Proc. Conference on Empirical Methods for Natural Language Pro-
cessing, (2002) pp. 156-163
8. Niesen S., Ney H.: Statistical Machine Translation with Scarce Resources using
Morpho-syntactic Information, Computational Linguistics, (2004) 30(2):181-204
9. Niesen S., Och F.J., Leusch G., Ney H: An Evaluation Tool for Machine Transla-
tion: Fast Evaluation for MT Research, Proc. of the 2nd International Conference
on Language Resources and Evaluation, (2000) pp. 39-45
10. Watanabe T. and Sumita E.: Example-based Decoding for Statistical Machine
Translation, Proc. of MT Summit IX (2003) pp. 410?417
11. Och F. J. Och and Ney H.: Discriminative Training and Maximum Entropy Models
for Statistical Machine Translation, Proc. of ACL (2002)
12. Stolcke, A.: SRILM - an extensible language modeling toolkit. In Proc. Intl. Conf.
Spoken Language Processing, (2002) Denver.
13. Brown P. F., Della Pietra V. J. and deSouza P. V. and Lai J. C. and Mercer
R.L.: Class-Based n-gram Models of Natural Language, Computational Linguistics
(1992) 18(4) pp. 467-479
14. Papineni K., Roukos S., Ward T., and Zhu W.-J.: Bleu: a method for automatic
evaluation of machine translation, IBM Research Report,(2001) RC22176.
15. Takezawa T., Sumita E., Sugaya F., Yamamoto H., and Yamamoto S.: Toward a
broad-coverage bilingual corpus for speech translation of travel conversations in
the real world, Proc. of LREC (2002), pp. 147-152.
Proceedings of the 43rd Annual Meeting of the ACL, pages 549?556,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Context-dependent SMT Model using Bilingual Verb-Noun Collocation
Young-Sook Hwang
ATR SLT Research Labs
2-2-2 Hikaridai Seika-cho
Soraku-gun Kyoto, 619-0288, JAPAN
youngsook.hwang@atr.jp
Yutaka Sasaki
ATR SLT Research Labs
2-2-2 Hikaridai Seika-cho
Soraku-gun Kyoto, 619-0288, JAPAN
yutaka.sasaki@atr.jp
Abstract
In this paper, we propose a new context-
dependent SMT model that is tightly cou-
pled with a language model. It is de-
signed to decrease the translation ambi-
guities and efficiently search for an opti-
mal hypothesis by reducing the hypothe-
sis search space. It works through recipro-
cal incorporation between source and tar-
get context: a source word is determined
by the context of previous and correspond-
ing target words and the next target word
is predicted by the pair consisting of the
previous target word and its correspond-
ing source word. In order to alleviate
the data sparseness in chunk-based trans-
lation, we take a stepwise back-off trans-
lation strategy. Moreover, in order to ob-
tain more semantically plausible transla-
tion results, we use bilingual verb-noun
collocations; these are automatically ex-
tracted by using chunk alignment and a
monolingual dependency parser. As a case
study, we experimented on the language
pair of Japanese and Korean. As a result,
we could not only reduce the search space
but also improve the performance.
1 Introduction
For decades, many research efforts have contributed
to the advance of statistical machine translation.
Recently, various works have improved the quality
of statistical machine translation systems by using
phrase translation (Koehn et al, 2003; Marcu et al,
2002; Och et al, 1999; Och and Ney, 2000; Zens
et al, 2004). Most of the phrase-based translation
models have adopted the noisy-channel based IBM
style models (Brown et al, 1993):



 












 (1)
In these model, we have two types of knowledge:
translation model, 




 and language model,



. The translation model links the source lan-
guage sentence to the target language sentence. The
language model describes the well-formedness of
the target language sentence and might play a role
in restricting hypothesis expansion during decoding.
To recover the word order difference between two
languages, it also allows modeling the reordering by
introducing a relative distortion probability distribu-
tion. However, in spite of using such a language
model and a distortion model, the translation outputs
may not be fluent or in fact may produce nonsense.
To make things worse, the huge hypothesis search
space is much too large for an exhaustive search. If
arbitrary reorderings are allowed, the search prob-
lem is NP-complete (Knight, 1999). According
to a previous analysis (Koehn et al, 2004) of how
many hypotheses are generated during an exhaustive
search using the IBM models, the upper bound for
the number of states is estimated by 	   




 ,
where  is the number of source words and 


 is
the size of the target vocabulary. Even though the
number of possible translations of the last two words
is much smaller than 




, we still need to make
further improvement. The main concern is the ex-
549
ponential explosion from the possible configurations
of source words covered by a hypothesis. In order
to reduce the number of possible configurations of
source words, decoding algorithms based on  as
well as the beam search algorithm have been pro-
posed (Koehn et al, 2004; Och et al, 2001). (Koehn
et al, 2004; Och et al, 2001) used heuristics for
pruning implausible hypotheses.
Our approach to this problem examines the pos-
sibility of utilizing context information in a given
language pair. Under a given target context, the cor-
responding source word of a given target word is al-
most deterministic. Conversely, if a translation pair
is given, then the related target or source context is
predictable. This implies that if we considered bilin-
gual context information in a given language pair
during decoding, we can reduce the computational
complexity of the hypothesis search; specifically, we
could reduce the possible configurations of source
words as well as the number of possible target trans-
lations.
In this study, we present a statistical machine
translation model as an alternative to the classical
IBM-style model. This model is tightly coupled
with target language model and utilizes bilingual
context information. It is designed to not only re-
duce the hypothesis search space by decreasing the
translation ambiguities but also improve translation
performance. It works through reciprocal incorpo-
ration between source and target context: source
words are determined by the context of previous
and corresponding target words, and the next target
words are predicted by the current translation pair.
Accordingly, we do not need to consider any dis-
tortion model or language model as is the case with
IBM-style models.
Under this framework, we propose a chunk-based
translation model for more grammatical, fluent and
accurate output. In order to alleviate the data sparse-
ness problem in chunk-based translation, we use a
stepwise back-off method in the order of a chunk,
sub-parts of the chunk, and word level. Moreover,
we utilize verb-noun collocations in dealing with
long-distance dependency which are automatically
extracted by using chunk alignment and a monolin-
gual dependency parser.
As a case study, we developed a Japanese-to-
Korean translation model and performed some ex-
periments on the BTEC corpus.
2 Overview of Translation Model
The goal of machine translation is to transfer the
meaning of a source language sentence, 




   

, into a target language sentence, 




   

. In most types of statistical machine trans-
lation, conditional probability 




 is used to
describe the correspondence between two sentences.
This model is used directly for translation by solving
the following maximization problem:



 









 (2)
 














(3)
 






 


 (4)
Since a source language sentence is given and the



 probability is applied to all possible corre-
sponding target sentences, we can ignore the denom-
inator in equation (3). As a result, the joint proba-
bility model can be used to describe the correspon-
dence between two sentences. We apply Markov
chain rules to the joint probability model and obtain
the following decomposed model:



 


 



	



 






	


(5)
where 

is the index of the source word that is
aligned to the word 

under the assumption of the
fixed one-to-one alignment. In this model, we have
two probabilities:
 source word prediction probability under a
given target language context, 
	



 


 target word prediction probability under the
preceding translation pair, 



 
	


The probability of target word prediction is used for
selecting the target word that follows the previous
target words. In order to make this more determin-
istic, we use bilingual context, i.e. the translation
pair of the preceding target word. For a given target
word, the corresponding source word is predicted by
source word prediction probability based on the cur-
rent and preceding target words.
550
Since a target and a source word are predicted
through reciprocal incorporation between source
and target context from the beginning of a target
sentence, the word order in the target sentence is
automatically determined and the number of pos-
sible configurations of source words is decreased.
Thus, we do not need to perform any computation
for word re-ordering. Moreover, since correspon-
dences are provided based on bilingual contextual
evidence, translation ambiguities can be decreased.
As a result, the proposed model is expected to re-
duce computational complexity during the decoding
as well as improve performance.
Furthermore, since a word-based translation ap-
proach is often incapable of handling complicated
expressions such as an idiomatic expressions or
complicated verb phrases, it often outputs nonsense
translations. To avoid nonsense translations and to
increase explanatory power, we incorporate struc-
tural aspects of the language into the chunk-based
translation model. In our model, one source chunk
is translated by exactly one target chunk, i.e., one-
to-one chunk alignment. Thus we obtain:




 













 (6)










 







	



 








	


(7)
where  is the number of chunks in a source and a
target sentence.
3 Chunk-based J/K Translation Model
with Back-Off
With the translation framework described above, we
built a chunk-based J/K translation model as a case
study. Since a chunk-based translation model causes
severe data sparseness, it is often impossible to ob-
tain any translation of a given source chunk. In order
to alleviate this problem, we apply back-off trans-
lation models while giving the consideration to lin-
guistic characteristics.
Japanese and Korean is a very close language pair.
Both are agglutinative and inflected languages in the
word formation of a bunsetsu and an eojeol. A bun-
setsu/eojeol consists of two sub parts: the head part
composed of content words and the tail part com-
posed of functional words agglutinated at the end of
the head part. The head part is related to the mean-
ing of a given segment, while the tail part indicates
a grammatical role of the head in a given sentence.
By putting this linguistic knowledge to practical
use, we build a head-tail based translation model
as a back-off version of the chunk-based translation
model. We place several constraints on this head-tail
based translation model as follows:
 The head of a given source chunk corresponds
to the head of a target chunk. The tail of the
source chunk corresponds to the tail of a target
chunk. If a chunk does not have a tail part, we
assign NUL to the tail of the chunk.
 The head of a given chunk follows the tail of the
preceding chunk and the tail follows the head of
the given chunk.
The constraints are designed to maintain the struc-
tural consistency of a chunk. Under these con-
straints, the head-tail based translation can be for-
mulated as the following equation:



	



 








	

  (8)




	




 











	






	




 











	


where 

denotes the head of the  chunk and 

means the tail of the chunk.
In the worst case, even the head-tail based model
may fail to obtain translations. In this case, we
back it off into a word-based translation model. In
the word-based translation model, the constraints
on the head-tail based translation model are not ap-
plied. The concept of the chunk-based J/K transla-
tion framework with back-off scheme can be sum-
marized as follows:
1. Input a dependency-parsed sentence at the
chunk level,
2. Apply the chunk-based translation model to the
given sentence,
3. If one of chunks does not have any correspond-
ing translation:
 divide the failed chunk into a head and a
tail part,
551
Figure 1: An example of (a) chunk alignment for chunk-based, head-tail based translation and (b) bilingual
verb-noun collocation by using the chunk alignment and a monolingual dependency parser
 back-off the translation into the head-tail
based translation model,
 if the head or tail does not have any corre-
sponding translation, apply a word-based
translation model to the chunk.
Here, the back-off model is applied only to the part
that failed to get translation candidates.
3.1 Learning Chunk-based Translation
We learn chunk alignments from a corpus that has
been word-aligned by a training toolkit for word-
based translation models: the Giza++ (Och and
Ney, 2000) toolkit for the IBM models (Brown
et al, 1993). For aligning chunk pairs, we con-
sider word(bunsetsu/eojeol) sequences to be chunks
if they are in an immediate dependency relationship
in a dependency tree. To identify chunks, we use
a word-aligned corpus, in which source language
sentences are annotated with dependency parse trees
by a dependency parser (Kudo et al, 2002) and tar-
get language sentences are annotated with POS tags
by a part-of-speech tagger (Rim, 2003). If a se-
quence of target words is aligned with the words in
a single source chunk, the target word sequence is
regarded as one chunk corresponding to the given
source chunk. By applying this method to the cor-
pus, we obtain a word- and chunk-aligned corpus
(see Figure 1).
From the aligned corpus, we directly estimate
the phrase translation probabilities,  ,
and the model parameters,  
	



 

,







	

. These estimation are made
based on relative frequencies.
3.2 Decoding
For efficient decoding, we implement a multi-stack
decoder and a beam search with  algorithm. At
each search level, the beam search moves through at
most -best translation candidates, and a multi-stack
is used for partial translations according to the trans-
lation cardinality. The output sentence is generated
from left to right in the form of partial translations.
Initially, we get  translation candidates for each
source chunk with the beam size . Every possible
translation is sorted according to its translation prob-
ability. We start the decoding with the initialized
beams and initial stack 

, the top of which has the
information of the initial hypothesis, 

 




. The decoding algorithm is described in Table 1.
In the decoding algorithm, estimating the back-
ward score is so complicated that the computational
complexity becomes too high because of the context
consideration. Thus, in order to simplify this prob-
lem, we assume the context-independence of only
the backward score estimation. The backward score
is estimated by the translation probability and lan-
guage model score of the uncovered segments. For
each uncovered segment, we select the best transla-
tion with the highest score by multiplying the trans-
lation probability of the segment by its language
model score. The translation probability and lan-
guage model score are computed without giving
consideration to context.
After estimating the forward and backward score
of each partial translation on stack 

, we try to
552
1. Push the initial hypothesis 

 



  on the initial
stack 

2. for i=1 to K
 Pop the previous state information of 







from stack 

 Get next target 

and corresponding source 


 for all pairs of 







? Check the head-tail consistency
? Mark the source segment as a covered one
? Estimate forward and backward score
? Push the state of pair 






 onto stack 

 Sort all translations on stack 

by the scores
 Prune the hypotheses
3. while (stack 

is not empty)
 Pop the state of the pair 







 Compose translation output, 

   


4. Output the best  translations
Table 1:  multi-stack decoding algorithm
prune the hypotheses. In pruning, we first sort the
partial translations on stack 

according to their
scores. If the gradient of scores steeply decreases
over the given threshold at the  translation, we
prune the translations of lower scores than the 
one. Moreover, if the number of filtered translations
is larger than 	 , we only take the top 	 transla-
tions. As a final translation, we output the single
best translation.
4 Resolving Long-distance Dependency
Since most of the current translation models take
only the local context into account, they cannot
account for long-distance dependency. This often
causes syntactically or semantically incorrect trans-
lation to be output. In this section, we describe
how this problem can be solved. For handling the
long-distance dependency problem, we utilize bilin-
gual verb-noun collocations that are automatically
acquired from the chunk-aligned bilingual corpora.
4.1 Automatic Extraction of Bilingual
Verb-Noun Collocation(BiVN)
To automatically extract the bilingual verb-noun
collocations, we utilize a monolingual dependency
parser and the chunk alignment result. The basic
concept is the same as that used in (Hwang et al,
2004): bilingual dependency parses are obtained by
sharing the dependency relations of a monolingual
dependency parser among the aligned chunks. Then
bilingual verb sub-categorization patterns are ac-
quired by navigating the bilingual dependency trees.
A verb sub-categorization is the collocation of a verb
and all of its argument/adjunct nouns, i.e. verb-noun
collocation(see Figure 1).
To acquire more reliable and general knowledge,
we apply the following filtering method with statis-
tical  test and unification operation:
 step 1. Filter out the reliable translation corre-
spondences from all of the alignment pairs by

 test at a probability level of 

 step 2. Filter out reliable bilingual verb-noun
collocations BiVN by a unification and  test
at a probability level of 

: Here, we assume
that two bilingual pairs, 

 

 and 

 


are unifiable into a frame 

 

 

 

 iff
both of them are reliable pairs filtered in step 1.
and they share the same verb pair 

 

.
4.2 Application of BiVN
The acquired BiVN is used to evaluate the bilingual
correspondence of a verb-noun pair dependent on
each other and to select the correct translation. It
can be applied to any verb-noun pair regardless of
the distance between them in a sentence. Moreover,
since the verb-noun relation in BiVN is bilingual
knowledge, the sense of each corresponding verb
and noun can be almost completely disambiguated
by each other.
In our translation system, we apply this BiVN
during decoding as follows:
1. Pivot verbs and their dependents in a given
dependency-parsed source sentence
2. When extending a hypothesis, if one of the piv-
oted verb and noun pairs is covered and its cor-
responding translation pair is in BiVN, we give
positive weight   	 to the hypothesis.
Learning Semantic-Level Information Extraction Rules by 
Type-Oriented ILP 
Yutaka Sasaki and Yoshihiro Matsuo  
NTT Communicat ion  Science Laboratories 
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237, Japan 
{sasaki, yosihiro} ~cslab.kecl .ntt .co. jp 
Abstract  
This paper describes an approach to using se- 
mantic rcprcsentations for learning information 
extraction (IE) rules by a type-oriented induc- 
tire logic programming (ILl)) system. NLP 
components of a lnachine translation system are 
used to automatically generate semantic repre- 
sentations of text corpus that can be given di- 
rectly to an ILP system. The latest experimen- 
tal results show high precision and recall of the 
learned rules. 
1 Int roduct ion 
Information extraction (IE) tasks in this paper 
involve the MUC-3 style IE. The input for the 
information extraction task is an empty tem- 
plate and a set of natural anguage texts that de- 
scribe a restricted target domain, such as corpo- 
rate mergers or terrorist atta.cks in South Amer- 
ica. Templates have a record-like data struc- 
ture with slots that have names, e.g., "company 
name" and "merger d~te", and v~lues. The out- 
put is a set of filled templates. IE tasks are 
highly domain-dependent, so rules and dictio- 
naries for filling values in the telnp\]ate slots de- 
pend on the domain. 
it is a heavy burden for IE system develop- 
ers that such systems depend on hand-made 
rules, which cannot be easily constructed and 
changed. For example, Umass/MUC-3 needed 
about 1,500 person-hours of highly skilled labor 
to build the IE rules and represent them as a 
dictionary (Lehnert, 1992). All the rules must 
be reconstructed i'rom scratch when the target 
domain is changed. 
To cope with this problem, some pioneers 
have studied methods for learning information 
extraction rules (Riloff,1996; Soderland ctal., 
1.995; Kim et el., 1995; Huffman, 1996; Califf 
and Mooney, 1997). Along these lines, our ap- 
preach is to a.pply an inductive logic program- 
ruing (ILP) (Muggleton, 1991)system to the 
learning of IE rules, where information is ex- 
tracted from semantic representations of news 
articles. The ILP system that we employed is 
a type-oriented ILP system I{\]\]B + (Sasaki and 
Haruno, 1997), which can efficiently and effec- 
tively h~mdle type (or sort) information in train- 
ing data. 
2 Our Approach to IE Tasks 
This section describes our approach to IE tasks. 
Figure 1. is an overview of our approach to learn- 
ing IE rules using an II, P system from seman- 
tic representations. First, training articles are 
analyzed and converted into semantic represen- 
tations, which are filled case fl'ames represented 
as atomic formulae. Training templates are pre- 
pared by hand as well. The ILP system learns 
\]!!; rules in the tbrm of logic l)rograms with type 
information. To extract key inlbrmation from a 
new ~rticle, semantic representation s au tomat- 
ically generated from the article is matched by 
the IE rules. Extracted intbrmation is filled into 
the template slots. 
3 NLP  Resources and Tools 
3.1 The Semantic Attribute System 
We used the semantic attribute system of "Ge l  
Taikei - -  A Japanese Lexicon" (lkehara el el., 
1997a; Kurohashi and Sakai, 1.999) compiled by 
the NTT Communication Science Laboratories 
for a Japanese-to-English machine translation 
system, ALT- J /E  (Ikehm:a et al, 1994). The se- 
mantic attribute system is a sort of hierarchical 
concept thesaurus represented as a tree struc- 
ture in which each node is called a semantic 
cateqory. An edge in the tree represents an is_a 
or has_a relation between two categories. The 
semantic attribute system is 11.2 levels deep and 
698 
semantic representation new article 
' ' '  \[\]\]\] s?~chy yze ~ Analyze I rolease(cl,pl) articles sentences announce(cl,dl) 
i'~kackgrou n d Anal 
.... nzwledge / \[E rules ~ F representatiOn sentences I semantic 
.ooitive ..... l 
I re,oa o x. , II 
answer 
templates filled Company: c2 ~7"A;p'-'"iyrule~='~" "~  
by hand Draotauotd..~2 to semantic I -  I ,opreseot t,on 
Figure l: l/lock diagram of IE using IM ) 
contains about 3,000 sema.ntic ategory nodes. 
More than 300,000 Japanese words a.re linked to 
the category nodes. 
3.2 Verb Case Frame Dict ionary 
The Japanese-to-li;nglish valency 1)a.ttern dic- 
t ionary of "(\]oi-Taikei" ( lkehara et al, 1997b; 
Kurohash.i and Saka.i, 1999) was also originally 
developed for ALT-,I/IB. The. wde:ncy dictionary 
conta.ins about 15,000 case frames with sema.n- 
tic restrictions on their arguments lbr 6,000 
a apanese verbs. Each ca.se frame consists of one 
predicate a.nd one or more case elements tha.t 
h ave a list; of sere an tic categories. 
3.3 Natural  Language Processing Tools 
We used the N I,P COml)onents of kl/ l ' - . I /F,  for 
text a, nalysis. These inclu<le the morphologica,l 
amdyzer, the syntactic analyzer, and the case 
aDalyzer for Japanese. The components a.re ro- 
bust a:nd generic tools, mainly ta:rgeted to news- 
paper articles. 
3.3.1 Generic Case Analyzer  
l,et us examine the case a.nalysis in more de- 
tail. The <'as(; analyzer eads a set of parse tree 
candidates produced by the J a.panese syntactic 
analyzer. The parse tree is :represented as a de- 
penden cy of ph rases (i. e., .\] al>anese bu'nsctmt). 
First, it divides the parse tree into unit sen- 
tences, where a unit sentence consists of one 
predicate and its noun and adverb dependent 
phrases. Second, it compares each unit sen- 
tence.with a verb case fl'alne dictionary, l!;ach 
frame consists a predicate condition and several 
cast elements conditions. The predicate con- 
dition specifies a verb that matches the frame 
a.:nd each case-role has a. case element condition 
whi ch sl>ecifie.s particles an d sere an tic categories 
of" noun phrases. The preference va.lue is de- 
lined as the summation of noun phrase \])refer- 
ences which are calculated from the distances 
between the categories of the input sentences 
m~d the categories written in the f i ' amcs .  The 
case a.na.lyzer then chooses the most preferable 
pa.rse tree and the most preferable combination 
of case frames. 
The valency dictionary also has case<roles 
(Table \] ) for :noun phrase conditions. The case- 
roles of adjuncts are determined by using the 
particles of adjuncts and the sema.ntic a.tegories 
of n ou n ph ra.ses. 
As a result, the OUtl)ut O\[' the case a.nalysis is 
a set; el" (;ase fl:ames for ca.oh unit se:ntence. The 
noun phra.ses in \['tames are la.beled by case-roh;s 
in Tal)le 1. 
l!'or siml)\]icity , we use case-role codes, such a.s 
N 1 and N2, a.s the labels (or slot ha.rues) to rep- 
resent case li:ames. The relation between sen- 
tences and case-roles is described in detail in 
( Ikehara el el., 1993). 
3.3.2 Logical Form Translator 
We developed a logical form translator li'E1 ~ 
that generates semantic representations ex- 
pressed a,s atomic Ibrmulae from the cast; fi:a.mes 
and parse trees. For later use, document II) 
and tense inlbrmation a.re also added to the case 
frames. 
For example, tile case fl:ame in 'l.'able 2 is ob- 
tained a:l'ter analyzing the following sentence of 
document l) 1: 
"Jalctcu(.lack) h,a suts,tkesu(suitca.se) we 
699 
Table 1: Case-Roles 
Name Code Description l~xampl.e 
Subject N1 the agent/experiencer of I throw a ball. 
an event/situation 
Objectl  N2 the object of an event 
Object2 N3 another object of an event 
Loc-Source N4 source location of a movement 
Loc-Goal N5 goal location of a movement 
Purpose N6 the purpose of an action 
Result N7 the result of an event 
Locative N8 the location of an event 
Comitative N9 co-experiencer 
Quotative N10 quoted expression 
Material N 11 material/ ingredient 
Cause N12 the reason for an event 
Instrument N13 a concrete instrument 
Means N14 an abstract instrument 
Time-Position TN1 the time of an event 
Time-Source TN2 the starting time of an event 
Time-Goal TN3 the end time of ~n event 
Amount QUANT quantity of something 
I throw a ball. 
I compare it with them. 
I start fl'om Japan. 
I go to Japan. 
I go shopping. 
It results in failure. 
it occurs at the station. 
I share a room with him. 
I say that .... 
I fill the glass with water. 
It collapsed fr'om the weight. 
I speak with a microphone. 
I speak in Japanese. 
I go to bed at i0:00. 
I work from Monday. 
It continues until Monday. 
I spend $10. 
 hok,,ba(the omce) kava(from)   o(the air 
port) ,),i(to)ha~obu(carry)" 
("Jack carries a suitcase from the office to the 
airport.") 
Table 2: Case Frame of the Sample Sentence 
predicate: hakobu (carry) 
article: 1) 1 
tense: present 
NI: Jakhu (Jack) 
N2: sutsukesu (suitcase) 
N4: sl, okuba (the office) 
N5: kuko (the airport) 
4 Induct ive Learning Tool 
Conventional ILP systems take a set of positive 
and negative xamples, and background knowl- 
edge. The output is a set of hypotheses in the 
form of logic programs that covers positives and 
do not cover negatives. We employed the type- 
oriented ILP system RHB +. 
4.1 Features of Type-orlented ILP 
System RHB + 
The type-oriented I\],P system has the tbllowing 
features that match the needs for learning l\]"~ 
rules. 
? A type-oriented ILP system can efficiently 
and effectively handle type (or seman- 
tic category) information in training data.. 
This feature is adwmtageous in controlling 
the generality and accuracy of learned IE 
rules. 
? It can directly use semantic representations 
of the text as background knowledge. 
, It can learn from only positive examples. 
? Predicates are allowed to have labels (or 
keywords) for readability and expressibil- 
ity. 
4.2 Summary of Type-oriented ILP 
System RHB + 
This section summarizes tile employed type- 
oriented ILP system RHB +. The input of 
RHB + is a set of positive examples and back- 
ground knowledge including type hierarchy (or 
700 
the semantic attribute system). The output is 
a set of I\[orn clauses (Lloyd, 11.987) having vari- 
;tl~les with tyl)e intbrmation. That is, the term 
is extended to the v-term. 
4.3 v-terms 
v-terms are the restricted form of 0-terms (Ai't- 
K~tci and Nasr, 1986; Ait-Kaci et al, 11994). In- 
l'ormttlly, v-terms are Prolog terms whose vari- 
ables a.re replaced with variable Var of type T, 
which is denoted as Var:T. Predicttte ~tnd tim(:- 
tion symbols ~tre allowed to h;we features (or 
labels). For examl)\]e, 
speak( agent~ X :human,objcct~ Y :language) 
is a clause based on r-terms which ha.s labels 
agent and object, and types human and 
language. 
4.,4 A lgor i thm 
The algorithm of lHllI + is basically ~t greedy 
covering algorithm. It constructs clauses one- 
by-one by calling inner_loop (Algorithm \]) 
which returns a hypothesis clause. A hypoth- 
esis clause is tel)resented in the form of head :-- 
body. Covered examples are removed from 1 ) in 
each cycle. 
The inner loop consists of two phases: the 
head construction phase and the body construc- 
tion I)hase. It constrncts heads in a bottom-up 
manner and constructs the body in a top-down 
lna.nner, following the result described in (Zelle 
el al., 1994). 
"\['he search heuristic PWI  is weighted infor- 
m~tivity eml)loying the l,a.place estimate. Let 
7' = {Head : -Body } U B K. 
rwz( r ,T )_  l I f ' l+ J 
- - I . f ' - - /?  1?g2 IQ-~\]'i\[ _12 2' 
where IPl denotes the number of positive ex- 
amples covered by T and Q(T) is the empirical 
content. The smaller the value of PWI, the can- 
didate clause is better. Q(T) is defined as the 
set of atoms (1) that are derivable from T ~md 
(2) whose predicate is the target I)redicate, i.e., 
the predicate name of the head. 
The dynamic type restriction, by positivc ex- 
amples uses positive examples currently covered 
in order to determine appropriate types to wtri- 
~bles for the current clause. 
A lgor i thm 1 inner_loop 
1. Given positives P, original positives 1~o, back- 
ground knowledge 1Hr. 
2. Decidc typcs of variables in a head by comput- 
ing the lyped least general generalizations (lgg) 
of N pairs of clcmcnts in P, and select he most 
general head as H cad. 
3. If the stopping condition is satisfied, return 
Head. 
It. Let Body bc empty. 
5, Create a set of all possible literals L using vari- 
ables in Head and Body. 
6. Let BEAM be top If litcrals l~, of L wilh 
respect to the positive weighted informalivily 
PWI.  
7. Do later steps, assuming that l~ is added to 
Body for each literal lk in BEAM.  
8. Dynamically restrict types in Body by callin, g 
the dynamic type restriction by positive exam- 
pies. 
9. If the slopping condition is satisfied, rct'aru 
(Head :- Body). 
lO. Goto 5. 
5 I l l us t ra t ion  o f  a Learn ing  Process  
Now, we examine tile two short notices of' new 
products release in Table 3. The following table 
shows a sample te:ml)late tbr articles reporting 
a new product relea.se. 
Tom pl ate 
1. article id: 
2. coml)any: 
3. product: 
4. release date: 
5.1 Preparat ion 
Suppose that the following semantic represen- 
tations is obtained from Article 1. 
(cl) announce( article => I, 
tense => past, 
tnl => "this week", 
nl => "ABC Corp.", 
nlO => (c2) ) .  
(c2) release( article => I, 
tense => future, 
tni => "Jan. 20", 
nl => "ABC Corp.", 
n2 => "a color printer" ). 
701 
Table 3: Sample Sentences 
Article id Sentence 
#1 "ABC Corp. this week zmnounced that it will release a color printer on Jan. 20." 
#2 "XYZ Corp. released a color scanner last month." 
The filled template for Article 1 is as follows. 
Template \] 
\]. article id: 1 
2. colnpany: ABC Corp. 
3. product: a color printer 
4. release date: Jan. 20 
Suppose that the following semantic represen- 
tation is obtained from Article 2. 
(c3) release( article => 2, 
tense => past, 
tnl => "last month", 
nl => "XYZ Corp.", 
n2 => "a color scanner" ). 
The filled template for Article 2 is as follows. 
Template 2 
1. article id: 2 
2. company: XYZ Corp. 
3. product: a color scanner 
4. release date: last month 
5.2 Head Const ruct ion  
Two positive examples are selected for the tem- 
plate slot "company". 
company(ar t i c le -number  => i 
name => "ABe Corp") .  
company(ar t i c le -number  => 2 
name => "XYZ Corp") .  
By computing a least general generalization 
(lgg)sasaki97, the following head is obtained: 
company( article-number => Art: number 
name => Co: organization). 
5.3 Body  Construction 
Generate possible literals 1 by combining predi- 
cate names and variables, then check the PWI  
1,1iterals,, here means atomic formulae or negated 
ones .  
values of clauses to which one of the literal 
added. In this case, suppose that adding the fol- 
lowing literal with predicate release is the best 
one. After the dynamic type restriction, the 
current clause satisfies the stopping condition. 
Finally, the rule for extracting "company name" 
is returned. Extraction rules for other slots 
"product" and "release date" can be learned in 
the sanle manner. Note that several literals may 
be needed in the body of the clause to satisfy 
the stopping condition. 
company(article-number => Art:number, 
name => Co: organization ) 
? - release( article => Art, 
tense => T: tense, 
tnl => D: time, 
nl => Co, 
n2 => P: product ). 
5.4 Ext rac t ion  
Now, we have tile following sen\]antic represen- 
tation extracted from the new article: 
Article 3: " JPN Corp. has released a new CI) 
player. ''2 
(c4) release( article => 3, 
tense => perfect_present, 
tnl => nil, 
n l  => "JPN Corp.", 
n2 => "a new CD player" ). 
Applying the learned IE rules and other rules, 
we can obtain the filled template for Article 3. 
Template 3 
1. article id: 3 
2. company: JPN Corp. 
3. product: C I )p layer  
4. release date: 
2\;Ve always assume nil for the case that is not in- 
cluded in the sentence. 
702 
Table d: Learning results of new product release 
(a) Without data correction 
company product release date 
Precision 89.6% 
Recall 82.1% 
Average time (set.) 15.8 
l)recision 911 .1% 
Recall 85.7% 
Average time (sec.) 22.9 
80.5% 
66.7% 
22.J 
90.6% 
66.7% 
ld.d 
announce date \[ price 
lOO.O% 58.4% 
82.4:% 60.8% 
2.2 I 1.0 
(b) With data. correction 
company product release date 
80.o% 
69.7% 
25.2 
92.3% 
82.8% 
33.55 
annotmce date \[ price 
100.0% 87.1% 
88.2% 82.4% 
5.1.5 11.9 
6 Experimental Results 
6.1. Setting of Experhnents 
We extracted articles related to the release of 
new products from a one-year newspaper cor- 
pus written in Japanese 3. One-hundred arti- 
cles were randomly selected fi'om 362 relevant 
articles. The template we used consisted of 
tive slots: company name, prod'uct name, re- 
lease date, a~tnomzcc date, and price. We also 
filled one template for each a.rticle. After an- 
a.lyzing sentences, case fi'ames were converted 
into atomic tbrmulae representing semantic rep- 
re,,~entationx a.  described in Section 2 and 3. All 
the semantic representations were given to the 
lea.rner as background \]?nowledge, ~md the tilled 
templates were given as positive examples. To 
speed-up the leCturing process, we selected pred- 
icate names that are relevant o the word s in the 
templates as the target predicates to be used by 
the ILl ~ system, and we also restricted the num- 
ber of literals in the body of hypotheses to one. 
Precision and recM1, the standard metrics \['or 
IF, tasks, are counted by using the remove-one- 
out cross validation on tile e, xamples for each 
item. We used a VArStation with tlie Pentium 
H Xeon (450 MHz):for this experiment. 
6.2 Results 
'l?M)le 4 shows the results of our experiment. In 
the experiment of learning from semantic repre- 
sentations, including errors in case-role selection 
and semantic ategory selection, precision was 
3We used ~rticles from the Mainichi Newspaimrs of 
1994 with permission. 
very high. 'l'he precision of the learned rules 
lot price was low beta.use the seman tic category 
name automatieaJly given to the price expres- 
sions in the dat~ were not quite a.ppropriate. 
For the tire items, 6?-82% recall was achieved. 
With the background knowledge having sere an- 
tic representations corrected by hand, precision 
was very high mid 70-88% recMl was achieved. 
The precision of price was markedly improved. 
It ix important that the extraction of live 
ditthrent pieces o1' information showed good re- 
sults. This indica.tex that the \]LI' system RIII~ + 
has a high potential in IE tasks. 
7 Related Work 
l)revious researches on generating lli; rules 
from texts with templates include AutoSlog- 
TS (Riloff,1996), (',I{YS'FAL (Soderland et al, 
1995), I'AIAKA (l(im et al, 1995), MlgP (Iluff- 
man, 11.996) and RAPII~;I~ (Califl' and Mooney, 
1997). In our approach, we use the type- 
oriented H,P system RItlJ +, which ix indepen- 
dent of natural language analysis. This point 
differentiates our ~pproach from the others. 
Learning semantic-level IE rules using an II,P 
system from semantic representations is also a 
new challenge in II'; studies. 
Sasald (Sasaki and Itaruno, 11997) applied 
RI{B + to the extraction of the number of deaths 
and injuries fi'om twenty five articles. That 
experiment was sufficient o assess the perfor- 
mance of the learner, but not to evaJuate its 
feasibility in IE tasks. 
703 
8 Conc lus ions  and Remarks  
This paper described a use of semantic repre- 
sentations for generating information extraction 
rules by applying a type-oriented ILP system. 
Experiments were conducted on the data gen- 
erated fi'om 100 news articles in the domain of 
new product release. The results showed very 
high precision, recall of 67-82% without data 
correction and 70-88% recall with correct se- 
mantic representations. The extraction of five 
different pieces of information showed good re- 
sults. This indicates that our learner RHB + has 
a high potential in IE tasks. 
References  
H. Ai't-Kaci and R. Nasr, LOGIN: A logic pro- 
gramming language with built-in inheritance, 
Journal oJ' Logic Programming, 3, pp.185- 
215, 1986. 
lt. Ai't-Kaci, B. Dumant, R. Meyer, A. Podel- 
ski, and P. Van Roy, The Wild Life Itandbook, 
1994. 
M. E. Califf and R. J. Mooney, Relational 
Learning of Pattern-Match Rules for Informa- 
tion Extraction, Proc. of ACL-97 Workshop 
in Natural Language Learning, 1997. 
S. B. Huffman, Learning Information Extrac- 
tion Patterns from Examples, Statistical and 
Symbolic Approaches to Learning for Natural 
Language Processing, pp.246 260, 1996. 
S. ikehara, M. Miyazaki, and A. Yokoo, Clas- 
si:fication of language knowledge for mean- 
ing analysis in machine translations, Trans- 
actions of Information Processing Society 
of Japan, Vol.34, pp.1692-1704, 1993. (in 
.Japanese) 
S. Ikehara, S. Shirai, K. Ogura, A. Yokoo, 
H. Nakaiwa and T. Kawaoka, ALT-J/E: A 
Japanese to English Machine Translation Sys- 
tem tbr Communication with Translation, 
Proc. of The 13th IFIP World Computer 
Congress, pp.80-85, 1994. 
S. Ikehara, M. Miyazaki, S. Shirai, A. Yokoo, 
H. Nakaiwa, K. Ogura, Y. Oyama and 
Y. Hayashi (eds.), The Semantic Attribute 
System, Goi-lktikci -- A Japanese Lexi- 
con, Vol.1, Iwanami Publishing, 1997. (in 
Japanese) 
S. Ikehara, M. Miyazaki, S. Shirai, A. Yokoo, 
H. Nakaiwa, K. Ogura, Y. Oyama and 
Y. Hayashi (eds.), The Valency Dictionary, 
Goi-Taikei -- A Japanese Lcxicon, Vol.5, 
Iwa.nami Publishing, 1997. (in Japanese) 
J.-T. Kim and D. I. Moldovan, Acquisition 
of Linguistic Patterns for Knowledge-Based 
Information Extraction, \[EEE Transaction 
on Knowledge and Data Engineering (IEEE 
TKDE), Vol.7, No.5, pp.713 724, 1995. 
S. Kurohashi and Y. Sakai, Semantic Analysis 
of Japanese Noun Phrases: A New Approach 
to Dictionary-Based Understanding Thc 37th 
Annual Meeting of the Association for Com- 
putational Linguistics (A CL-99), pp.481-488, 
1999. 
W. Lehnert, C. Cardie, D. Fisher, J. McCarthy, 
E. Riloff and S. Soderland, University of Mas- 
sachusetts: MUC-4 Test Results and Analy- 
sis, Proc. of The 1;burth Message Understand- 
ing Conference (MUC-4), pp.151-158, 1992. 
J. Lloyd, Foundations of Logic Prog'mmming, 
Springer, 1987. 
S. Muggleton, Inductive logic programming, 
New Generation Computing, Vol.8, No.4, 
pp.295-318, 1991. 
E. Riloff, Automatically Generating Extrac- 
tion Pattern from Untagged Text, Proc.of 
American Association for Artificial IntcIli- 
gcnce (AAAI-96), pp.1044-1049, 1996. 
Y. Sasaki and M. IIaruno, RHB+: A Type- 
Oriented 1LP System Learning from Positive 
Data, Proc. of The l/jth International Joint 
Conference on Artificial Intelligence (LJCA l- 
9"/), pp.894-899, 1997. 
S. Soderland, 1). Fisher, J. Aseltine, W. Lenert, 
CRYSTAL: Inducing a Conceptual Dictio- 
n~ry, Proc. of The 13th International Joint 
ConJ'crcnce on Artificial Intelligence (IJCAI- 
95), pp.1314 1319, 1995. 
J. M. Zelle and R. J. Mooney, J. B. Konvisser, 
Combining Top-down and Bottom-up Meth- 
ods in Inductive Logic Programming, Proc 
of The 11th Tntcrnational Conference on Ma- 
chine Learning (ML-94), pp.343-351, 1994. 
J 
704 
  
	 Hierarchical Directed Acyclic Graph Kernel:
Methods for Structured Natural Language Data
Jun Suzuki, Tsutomu Hirao, Yutaka Sasaki, and Eisaku Maeda
NTT Communication Science Laboratories, NTT Corp.
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan
  jun, hirao, sasaki, maeda  @cslab.kecl.ntt.co.jp
Abstract
This paper proposes the ?Hierarchical Di-
rected Acyclic Graph (HDAG) Kernel? for
structured natural language data. The
HDAG Kernel directly accepts several lev-
els of both chunks and their relations,
and then efficiently computes the weighed
sum of the number of common attribute
sequences of the HDAGs. We applied the
proposed method to question classifica-
tion and sentence alignment tasks to eval-
uate its performance as a similarity mea-
sure and a kernel function. The results
of the experiments demonstrate that the
HDAG Kernel is superior to other kernel
functions and baseline methods.
1 Introduction
As it has become easy to get structured corpora such
as annotated texts, many researchers have applied
statistical and machine learning techniques to NLP
tasks, thus the accuracies of basic NLP tools, such
as POS taggers, NP chunkers, named entities tag-
gers and dependency analyzers, have been improved
to the point that they can realize practical applica-
tions in NLP.
The motivation of this paper is to identify and
use richer information within texts that will improve
the performance of NLP applications; this is in con-
trast to using feature vectors constructed by a bag-
of-words (Salton et al, 1975).
We now are focusing on the methods that use nu-
merical feature vectors to represent the features of
natural language data. In this case, since the orig-
inal natural language data is symbolic, researchers
convert the symbolic data into numeric data. This
process, feature extraction, is ad-hoc in nature and
differs with each NLP task; there has been no neat
formulation for generating feature vectors from the
semantic and grammatical structures inside texts.
Kernel methods (Vapnik, 1995; Cristianini and
Shawe-Taylor, 2000) suitable for NLP have recently
been devised. Convolution Kernels (Haussler, 1999)
demonstrate how to build kernels over discrete struc-
tures such as strings, trees, and graphs. One of the
most remarkable properties of this kernel method-
ology is that it retains the original representation
of objects and algorithms manipulate the objects
simply by computing kernel functions from the in-
ner products between pairs of objects. This means
that we do not have to map texts to the feature
vectors by explicitly representing them, as long as
an efficient calculation for the inner products be-
tween a pair of texts is defined. The kernel method
is widely adopted in Machine Learning methods,
such as the Support Vector Machine (SVM) (Vap-
nik, 1995). In addition, kernel function 
	
has been described as a similarity function that
satisfies certain properties (Cristianini and Shawe-
Taylor, 2000). The similarity measure between texts
is one of the most important factors for some tasks in
the application areas of NLP such as Machine Trans-
lation, Text Categorization, Information Retrieval,
and Question Answering.
This paper proposes the Hierarchical Directed
Acyclic Graph (HDAG) Kernel. It can handle sev-
eral of the structures found within texts and can cal-
culate the similarity with regard to these structures
at practical cost and time. The HDAG Kernel can be
widely applied to learning, clustering and similarity
measures in NLP tasks.
The following sections define the HDAG Kernel
and introduce an algorithm that implements it. The
results of applying the HDAG Kernel to the tasks
of question classification and sentence alignment are
then discussed.
2 Convolution Kernels
Convolution Kernels were proposed as a concept of
kernels for a discrete structure. This framework de-
fines a kernel function between input objects by ap-
plying convolution ?sub-kernels? that are the kernels
for the decompositions (parts) of the objects.
Let  be a positive integer and 


be nonempty, separable metric spaces. This paper
focuses on the special case that 




are
countable sets. We start with ffSpoken Interactive ODQA System: SPIQA
Chiori Hori, Takaaki Hori, Hajime Tsukada,
Hideki Isozaki, Yutaka Sasaki and Eisaku Maeda
NTT Communication Science Laboratories
Nippon Telegraph and Telephone Corporation
2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan
Abstract
We have been investigating an interactive
approach for Open-domain QA (ODQA)
and have constructed a spoken interactive
ODQA system, SPIQA. The system de-
rives disambiguating queries (DQs) that
draw out additional information. To test
the efficiency of additional information re-
quested by the DQs, the system recon-
structs the user?s initial question by com-
bining the addition information with ques-
tion. The combination is then used for an-
swer extraction. Experimental results re-
vealed the potential of the generated DQs.
1 Introduction
Open-domain QA (ODQA), which extracts answers
from large text corpora, such as newspaper texts, has
been intensively investigated in the Text REtrieval
Conference (TREC). ODQA systems return an ac-
tual answer in response to a question written in a
natural language. However, the information in the
first question input by a user is not usually sufficient
to yield the desired answer. Interactions for col-
lecting additional information to accomplish QA are
needed. To construct more precise and user-friendly
ODQA systems, a speech interface is used for the
interaction between human beings and machines.
Our goal is to construct a spoken interactive
ODQA system that includes an automatic speech
recognition (ASR) system and an ODQA system.
To clarify the problems presented in building such
a system, the QA systems constructed so far have
been classified into a number of groups, depending
on their target domains, interfaces, and interactions
to draw out additional information from users to ac-
complish set tasks, as is shown in Table 1. In this
table, text and speech denote text input and speech
input, respectively. The term ?addition? represents
additional information queried by the QA systems.
This additional information is separate to that de-
rived from the user?s initial questions.
Table 1: Domain and data structure for QA systems
target domain specific open
data structure knowledge DB unstructured text
without addition CHAT-80 SAIQAtext
with addition MYCIN (SPIQA?)
without addition Harpy VAQA
speech
with addition JUPITER (SPIQA?)
? SPIQA is our system.
To construct spoken interactive ODQA systems,
the following problems must be overcome: 1. Sys-
tem queries for additional information to extract an-
swers and effective interaction strategies using such
queries cannot be prepared before the user inputs the
question. 2. Recognition errors degrade the perfor-
mance of QA systems. Some information indispens-
able for extracting answers is deleted or substituted
with other words.
Our spoken interactive ODQA system, SPIQA,
copes with the first problem by adopting disam-
biguating users? questions using system queries. In
addition, a speech summarization technique is ap-
plied to handle recognition errors.
2 Spoken Interactive QA system: SPIQA
Figure 1 shows the components of our system, and
the data that flows through it. This system com-
prises an ASR system (SOLON), a screening filter
that uses a summarization method, and ODQA en-
gine (SAIQA) for a Japanese newspaper text corpus,
a Deriving Disambiguating Queries (DDQ) module,
and a Text-to-Speech Synthesis (TTS) engine (Fi-
nalFluet).
ASR
TTS
Screening
filter
ODQA engine
(SAIQA)
DDQ
module
Answer
derived?
Answer
sentence generator
Question
reconstructor
No
Yes
Additional
info. New question
First
question
Question/
Additional info.
User Answer/
DDQ speech
Answer
sentence
DDQ
sentence
Recognition
result
Answer
Figure 1: Components and data flow in SPIQA.
ASR system
Our ASR system is based on the Weighted Finite-
State Transducers (WFST) approach that is becom-
ing a promising alternative formulation for the tra-
ditional decoding approach. The WFST approach
offers a unified framework representing various
knowledge sources in addition to producing an op-
timized search network of HMM states. We com-
bined cross-word triphones and trigrams into a sin-
gle WFST and applied a one-pass search algorithm
to it.
Screening filter
To alleviate degradation of the QA?s perfor-
mance by recognition errors, fillers, word fragments,
and other distractors in the transcribed question, a
screening filter that removes these redundant and
irrelevant information and extracts meaningful in-
formation is required. The speech summarization
approach (C. Hori et. al., 2003) is applied to the
screening process, wherein a set of words maximiz-
ing a summarization score that indicates the appro-
priateness of summarization is extracted automati-
cally from a transcribed question, and these words
are then concatenated together. The extraction pro-
cess is performed using a Dynamic Programming
(DP) technique.
ODQA engine
The ODQA engine, SAIQA, has four compo-
nents: question analysis, text retrieval, answer hy-
pothesis extraction, and answer selection.
DDQ module
When the ODQA engine cannot extract an appro-
priate answer to a user?s question, the question is
considered to be ?ambiguous.? To disambiguate the
initial questions, the DDQ module automatically de-
rives disambiguating queries (DQs) that require in-
formation indispensable for answer extraction. The
situations in which a question is considered ambigu-
ous are those when users? questions exclude indis-
pensable information or indispensable information
is lost through ASR errors. These instances of miss-
ing information should be compensated for by the
users.
To disambiguate a question, ambiguous phrases
within it should be identified. The ambiguity of
each phrase can be measured by using the struc-
tural ambiguity and generality score for the phrase.
The structural ambiguity is based on the dependency
structure of the sentence; phrase that is not modified
by other phrases is considered to be highly ambigu-
ous. Figure 2 has an example of a dependency struc-
ture, where the question is separated into phrases.
Each arrow represents the dependency between two
phrases. In this example, ?the World Cup? has no
Which  country won the  world  cupin Southeast Asia ?
Figure 2: Example of dependency structure.
modifiers and needs more information to be identi-
fied. ?Southeast Asia? also has no modifiers. How-
ever, since ?the World Cup?appears more frequently
than ?Southeast Asia? in the retrieved corpus, ?the
World Cup? is more difficult to identify. In other
words, words that frequently occur in a corpus rarely
help to extract answers in ODQA systems. There-
fore, it is adequate for the DDQ module to generate
questions relating to ?World Cup? in this example,
such as ?What kind of World Cup?? , ?What year
was the World Cup held??.
The structural ambiguity of the n-th phrase is de-
fined as
A
D
(P
n
) = log
{
1 ?
?
N
i=1:i=n
D(P
i
, P
n
)
}
,
where the complete question is separated into N
phrases, and D(P
i
, P
n
) is the probability that phrase
P
n
will be modified by phrase P
i
, which can be cal-
culated using Stochastic Dependency Context-Free
Grammar (SDCFG) (C. Hori et. al., 2003).
Using this SDCFG, only the number of non-
terminal symbols is determined and all combina-
tions of rules are applied recursively. The non-
terminal symbol has no specific function, such as
a noun phrase. All the probabilities of rules are
stochastically estimated based on data. Probabilities
for frequently used rules become greater, and those
for rarely used rules become smaller. Even though
transcription results given by a speech recognizer are
ill-formed, the dependency structure can be robustly
estimated by our SDCFG.
The generality score is defined as
A
G
(P
n
) =
?
w?P
n
:w=cont log P (w),
where P (w) is the unigram probability of w based
on the corpus to be retrieved. Thus, ?w = cont?
means that w is a content word such as a noun, verb
or adjective.
We generate the DQs using templates of interrog-
ative sentences. These templates contain an inter-
rogative and a phrase taken from the user?s question,
i.e., ?What kind of * ??, ?What year was * held??
and ?Where is * ??.
The DDQ module selects the best DQ based on its
linguistic appropriateness and the ambiguity of the
phrase. The linguistic appropriateness of DQs can
be measured by using a language model, N-gram.
Let S
mn
be a DQ generated by inserting the n-th
phrase into the m-th template. The DDQ module
selects the DQ that maximizes the DQ score:
H(S
mn
) = ?
L
L(S
mn
)+?
D
A
D
(P
n
)+?
G
A
G
(P
n
),
where L(?) is a linguistic score such as the loga-
rithm for trigram probability, and ?
L
, ?
D
, and ?
G
are weighting factors to balance the scores.
Hence, the module can generate a sentence that
is linguistically appropriate and asks the user to dis-
ambiguate the most ambiguous phrase in his or her
question.
3 Evaluation Experiments
Questions consisting of 69 sentences read aloud by
seven male speakers were transcribed by our ASR
system. The question transcriptions were processed
with a screening filter and input into the ODQA
engine. Each question consisted of about 19 mor-
phemes on average. The sentences were grammat-
ically correct, formally structured, and had enough
information for the ODQA engine to extract the cor-
rect answers. The mean word recognition accuracy
obtained by the ASR system was 76%.
3.1 Screening filter
Screening was performed by removing recognition
errors using a confidence measure as a threshold and
then summarizing it within an 80% to 100% com-
paction ratio. In this summarization technique, the
word significance and linguistic score for summa-
rization were calculated using text from Mainichi
newspapers published from 1994 to 2001, compris-
ing 13.6M sentences with 232M words. The SD-
CFG for the word concatenation score was calcu-
lated using the manually parsed corpus of Mainichi
newspapers published from 1996 to 1998, consist-
ing of approximately 4M sentences with 68M words.
The number of non-terminal symbols was 100. The
posterior probability of each transcribed word in a
word graph obtained by ASR was used as the confi-
dence score.
3.2 DDQ module
The word generality score A
G
was computed using
the same Mainichi newspaper text described above,
while the SDCFG for the dependency ambiguity
score A
D
for each phrase was the same as that used
in (C. Hori et. al., 2003). Eighty-two types of inter-
rogative sentences were created as disambiguating
queries for each noun and noun-phrase in each ques-
tion and evaluated by the DDQ module. The linguis-
tic score L indicating the appropriateness of inter-
rogative sentences was calculated using 1000 ques-
tions and newspaper text extracted for three years.
The structural ambiguity score A
D
was calculated
based on the SDCFG, which was used for the screen-
ing filter.
3.3 Evaluation method
The DQs generated by the DDQ module were eval-
uated in comparison with manual disambiguation
queries. Although the questions read by the seven
speakers had sufficient information to extract ex-
act answers, some recognition errors resulted in a
loss of information that was indispensable for ob-
taining the correct answers. The manual DQs were
made by five subjects based on a comparison of
the original written questions and the transcription
results given by the ASR system. The automatic
DQs were categorized into two classes: APPRO-
PRIATE when they had the same meaning as at
least one of the five manual DQs, and INAPPRO-
PRIATE when there was no match. The QA per-
formance in using recognized (REC) and screened
questions (SCRN) were evaluated by MRR (Mean
Reciprocal Rank) (http://trec.nist.gov/data/qa.html).
SCRN was compared with the transcribed question
that just had recognition errors removed (DEL). In
addition, the questions reconstructed manually by
merging these questions and additional information
requested the DQs generated by using SCRN, (DQ)
were also evaluated. The additional information was
extracted from the original users? question without
recognition errors. In this study, adding information
by using the DQs was performed only once.
3.4 Evaluation results
Table 2 shows the evaluation results in terms of
the appropriateness of the DQs and the QA-system
MRRs. The results indicate that roughly 50% of the
DQs generated by the DDQ module based on the
screened results were APPROPRIATE. The MRR
for manual transcription (TRS) with no recognition
errors was 0.43. In addition, we could improve the
MRR from 0.25 (REC) to 0.28 (DQ) by using the
DQs only once. Experimental results revealed the
potential of the generated DQs in compensating for
the degradation of the QA performance due to recog-
nition errors.
4 Conclusion
The proposed spoken interactive ODQA system,
SPIQA copes with missing information by adopt-
ing disambiguation of users? questions by system
queries. In addition, a speech summarization tech-
nique was applied for handling recognition errors.
Although adding information was performed using
DQs only once, experimental results revealed the
potential of the generated DQs to acquire indispens-
able information that was lacking for extracting an-
swers. In addition, the screening filter helped to gen-
erate the appropriate DQs. Future research will in-
Table 2: Evaluation results of disambiguating
queries generated by the DDQ module.
Word MRR w/o IN-SPK
acc. REC DEL SCRN DQ errors APP APP
A 70% 0.19 0.16 0.17 0.23 4 32 33
B 76% 0.31 0.24 0.29 0.31 8 36 25
C 79% 0.26 0.18 0.26 0.30 10 34 25
D 73% 0.27 0.21 0.24 0.30 4 35 30
E 78% 0.24 0.21 0.24 0.27 7 31 31
F 80% 0.28 0.25 0.30 0.33 8 34 27
G 74% 0.22 0.19 0.19 0.22 3 35 31
AVG 76% 0.25 0.21 0.24 0.28 9% 49% 42%
An integer without a % other than MRRs indicates number of
sentences. Word acc.:word accuracy, SPK:speaker, AVG: aver-
aged values, w/o errors: transcribed sentences without recog-
nition errors, APP: appropriate DQs and InAPP: inappropriate
DQs.
clude an evaluation of the appropriateness of DQs
derived repeatedly to obtain the final answers. In
addition, the interaction strategy automatically gen-
erated by the DDQ module should be evaluated in
terms of how much the DQs improve QA?s total per-
formance.
References
F. Pereira et. al., ?Definite Clause Grammars for Language
Analysis ?a Survey of the Formalism and a Comparison with
Augmented Transition Networks,? Artificial Intelligence, 13:
231-278, 1980.
E. H. Shortliffe, ?Computer-Based Medical Consultations:
MYCIN,? Elsevier/North Holland, New York NY, 1976.
B. Lowerre et. al., ?The Harpy speech understanding system,?
W. A. Lea (Ed.), Trends in Speech recognition, pp. 340, Pren-
tice Hall.
L. D. Erman et. al., ?The Hearsay-II Speech-Understanding
System: Integrating Knowledge to Resolve Uncertainty,?
ACM computing Survays, Vol. 12, No. 2, pp. 213 ? 253,
1980.
V. Zue, et al, ?JUPITER: A Telephone-Based Conversational
Interface for Weather Information,? IEEE Transactions on
Speech and Audio Processing, Vol. 8, No. 1, 2000.
S. Harabagiu et. al., ?Open-Domain Voice-Activated Ques-
tion Answering,? COLING2002, Vol.I, pp. 321?327, Taipei,
2002.
C. Hori et. al., ?A Statistical Approach for Automatic Speech
Summarization,? EURASIP Journal on Applied Signal Pro-
cessing (EURASIP), pp128?139, 2003.
Y. Sasaki et. al., ?NTT?s QA Systems for NTCIR QAC-1,?
Working Notes of the Third NTCIR Workshop Meeting,
pp.63?70, 2002.
Question Classification using HDAG Kernel
Jun Suzuki, Hirotoshi Taira, Yutaka Sasaki, and Eisaku Maeda
NTT Communication Science Laboratories, NTT Corp.
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan
  jun, taira, sasaki, maeda  @cslab.kecl.ntt.co.jp
Abstract
This paper proposes a machine learning
based question classification method us-
ing a kernel function, Hierarchical Di-
rected Acyclic Graph (HDAG) Kernel.
The HDAG Kernel directly accepts struc-
tured natural language data, such as sev-
eral levels of chunks and their relations,
and computes the value of the kernel func-
tion at a practical cost and time while re-
flecting all of these structures. We ex-
amine the proposed method in a ques-
tion classification experiment using 5011
Japanese questions that are labeled by
150 question types. The results demon-
strate that our proposed method improves
the performance of question classification
over that by conventional methods such as
bag-of-words and their combinations.
1 Introduction
Open-domain Question Answering (ODQA) in-
volves the extraction of correct answer(s) to a given
free-form factual question from a large collection
of texts. ODQA has been actively studied all over
the world since the start of the Question Answering
Track at TREC-8 in 1999.
The definition of ODQA tasks at the TREC QA-
Track has been revised and extended year after
year. At first, ODQA followed the Passage Retrieval
method as used at TREC-8. That is, the ODQA task
was to answer a question in the form of strings of
50 bytes or 250 bytes excerpted from a large set of
news wires. Recently, however, the ODQA task is
considered to be a task of extracting exact answers
to a question. For instance, if a QA system is given
the question ?When was Queen Victoria born??, it
should answer ?1832?.
Typically, QA systems have the following compo-
nents for achieving ODQA:
Question analysis analyzes a given question and
determines the question type and keywords.
Text retrieval finds the top  paragraphs or docu-
ments that match the result of the question anal-
ysis component.
Answer candidate extraction extracts answer can-
didates of the given question from the docu-
ments retrieved by the text retrieval component,
based on the results of the question types.
Answer selection selects the most plausible an-
swer(s) to the given question from among the
answer candidates extracted by the answer can-
didate extraction component.
One of the most important processes of those
listed above is identifying the target of intention in a
given question to determine the type of sought-after
answer. This process of determining the question
type for a given question is usually called question
classification. Without a question type, that is, the
result of question classification, it would be much
more difficult or even nearly infeasible to select cor-
rect answers from among the possible answer can-
didates, which would necessarily be all of the noun
phrases or named entities in the texts. Question clas-
sification provides the benefit of a powerful restric-
tion that reduces to a practical number of the answer
candidates that should be evaluated in the answer se-
lection process.
This work develops a machine learning approach
to question classification (Harabagiu et al, 2000;
Hermjakob, 2001; Li and Roth, 2002). We use the
Hierarchical Directed Acyclic Graph (HDAG) Ker-
nel (Suzuki et al, 2003), which is suited to handle
structured natural language data. It can handle struc-
tures within texts as the features of texts without
converting the structures to the explicit representa-
tion of numerical feature vectors. This framework is
useful for question classification because the works
of (Li and Roth, 2002; Suzuki et al, 2002a) showed
that richer information, such as structural and se-
mantical information inside a given question, im-
proves the question classification performance over
using the information of just simple key terms.
In Section 2, we present the question classifica-
tion problem. In Section 3, we explain our proposed
method for question classification. Finally, in Sec-
tion 4, we describe our experiment and results.
2 Question Classification
Question classification is defined as a task that maps
a given question to more than one of  question
types (classes).
In the general concept of QA systems, the result
of question classification is used in a downstream
process, answer selection, to select a correct answer
from among the large number of answer candidates
that are extracted from the source documents. The
result of the question classification, that is, the la-
bels of the question types, can reduce the number
of answer candidates. Therefore, we no longer have
to evaluate every noun phrase in the source docu-
ments to see whether it provides a correct answer to
a given question. Evaluating only answer candidates
that match the results of question classification is an
efficient method of obtaining correct answers. Thus,
question classification is an important process of a
QA system. Better performance in question classi-
fication will lead to better total performance of the
QA system.
2.1 Question Types: Classes of Questions
Numerous question taxonomies have been defined,
but unfortunately, no standard exists.
In the case of the TREC QA-Track, most systems
have their own question taxonomy, and these are re-
constructed year by year. For example, (Ittycheriah
et al, 2001) defined 31 original question types in
two levels of hierarchical structure. (Harabagiu et
al., 2000) also defined a large hierarchical question
taxonomy, and (Hovy et al, 2001) defined 141 ques-
tion types of a hierarchical question taxonomy.
Within all of these taxonomies, question types are
defined from the viewpoint of the target intention of
the given questions, and they have hierarchical struc-
tures, even though these question taxonomies are de-
fined by different researchers. This because the pur-
pose of question classification is to reduce the large
number of answer candidates by restricting the tar-
get intention via question types. Moreover, it is very
useful to handle question taxonomy constructed in a
hierarchical structure in the downstream processes.
Thus, question types should be the target intention
and constructed in a hierarchical structure.
2.2 Properties
Question classification is quite similar to Text Cate-
gorization, which is one of the major tasks in Nat-
ural Language Processing (NLP). These tasks re-
quire classification of the given text to certain de-
fined classes. In general, in the case of text catego-
rization, the given text is one document, such as a
newspaper article, and the classes are the topics of
the articles. In the case of question classification,
a given text is one short question sentence, and the
classes are the target answers corresponding to the
intention of the given question.
However, question classification requires much
more complicated features than text categorization,
as shown by (Li and Roth, 2002). They proved that
question classification needs richer information than
simple key terms (bag-of-words), which usually give
us high performance in text classification. More-
over, the previous work of (Suzuki et al, 2002a)
showed that the sequential patterns constructed by
different levels of attributes, such as words, part-of-
speech (POS) and semantical information, improve
the performance of question classification. The ex-
periments in these previous works indicated that
the structural and semantical features inside ques-
tions have the potential to improve the performance
of question classification. In other words, high-
performance question classification requires us to
extract the structural and semantical features from
the given question.
2.3 Learning and Classification Task
This paper focuses on the machine learning ap-
proach to question classification. The machine
learning approach has several advantages over man-
ual methods.
First, the construction of a manual classifier for
questions is a tedious task that requires the analy-
sis of a large number of questions. Moreover, map-
ping questions into question types requires the use
of lexical items and, therefore, an explicit represen-
tation of the mapping may be very large. On the
other hand, machine learning approaches only need
to define features. Finally, the classifier can be more
flexibly reconstructed than a manual one because it
can be trained on a new taxonomy in a very short
time.
As the machine learning algorithm, we chose the
Support Vector Machines (SVMs) (Cortes and Vap-
nik, 1995) because the work of (Joachims, 1998;
Taira and Haruno, 1999) reported state-of-the-art
performance in text categorization as long as ques-
tion classification is a similar process to text catego-
rization.
3 HDAG Kernel
Recently, the design of kernel functions has become
a hot topic in the research field of machine learning.
A specific kernel can drastically increase the perfor-
mance of specific tasks. Moreover, a specific kernel
can handle new feature spaces that are difficult to
manage directly with conventional methods.
The HDAG Kernel is a new kernel function that
is designed to easily handle structured natural lan-
guage data. According to the discussion in the pre-
vious section, richer information such as structural
and semantical information is required for high-
performance question classification.
We think that the HDAG Kernel is suitable for
improving the performance of question classifica-
tion: The HDAG Kernel can handle various linguis-
tic structures within texts, such as chunks and their
relations, as the features of the text without convert-
ing such structures to numerical feature vectors ex-
plicitly.
3.1 Feature Space
Figure 1 shows examples of the structures within
questions that are handled by the HDAG kernel.
As shown in Figure 1, the HDAG kernel accepts
several levels of chunks and their relations inside the
text. The nodes represent several levels of chunks in-
cluding words, and directed links represent their re-
lations. Suppose 
	  and 	  rep-
resent each node. Some nodes have a graph inside
themselves, which are called ?non-terminal nodes?.
Each node can have more than one attribute, such
as words, part-of-speech tags, semantic information
like WordNet (Fellbaum, 1998), and class names of
the named entity. Moreover, nodes are allowed to
not have any attribute, in other words, we do not
have to assign attributes to all nodes.
The ?attribute sequence? is a sequence of at-
tributes extracted from the node in sub-paths of
HDAGs. One type of attribute sequence becomes
one element in the feature vector. The framework of
the HDAG Kernel allows node skips during the ex-
traction of attribute sequences, and its cost is based
the decay factor ffProceedings of the 43rd Annual Meeting of the ACL, pages 215?222,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Question Answering as Question-Biased Term Extraction:
A New Approach toward Multilingual QA
Yutaka Sasaki
Department of Natural Language Processing
ATR Spoken Language Communication Research Laboratories
2-2-2 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0288 Japan
yutaka.sasaki@atr.jp
Abstract
This paper regards Question Answering
(QA) as Question-Biased Term Extraction
(QBTE). This new QBTE approach lib-
erates QA systems from the heavy bur-
den imposed by question types (or answer
types). In conventional approaches, a QA
system analyzes a given question and de-
termines the question type, and then it se-
lects answers from among answer candi-
dates that match the question type. Con-
sequently, the output of a QA system is
restricted by the design of the question
types. The QBTE directly extracts an-
swers as terms biased by the question. To
confirm the feasibility of our QBTE ap-
proach, we conducted experiments on the
CRL QA Data based on 10-fold cross val-
idation, using Maximum Entropy Models
(MEMs) as an ML technique. Experimen-
tal results showed that the trained system
achieved 0.36 in MRR and 0.47 in Top5
accuracy.
1 Introduction
The conventional Question Answering (QA) archi-
tecture is a cascade of the following building blocks:
Question Analyzer analyzes a question sentence
and identifies the question types (or answer
types).
Document Retriever retrieves documents related
to the question from a large-scale document set.
Answer Candidate Extractor extracts answer
candidates that match the question types from
the retrieved documents.
Answer Selector ranks the answer candidates ac-
cording to the syntactic and semantic confor-
mity of each answer with the question and its
context in the document.
Typically, question types consist of named en-
tities, e.g., PERSON, DATE, and ORGANIZATION,
numerical expressions, e.g., LENGTH, WEIGHT,
SPEED, and class names, e.g., FLOWER, BIRD, and
FOOD. The question type is also used for selecting
answer candidates. For example, if the question type
of a given question is PERSON, the answer candidate
extractor lists only person names that are tagged as
the named entity PERSON.
The conventional QA architecture has a drawback
in that the question-type system restricts the range of
questions that can be answered by the system. It is
thus problematic for QA system developers to care-
fully design and build an answer candidate extrac-
tor that works well in conjunction with the question-
type system. This problem is particularly difficult
when the task is to develop a multilingual QA sys-
tem to handle languages that are unfamiliar to the
developer. Developing high-quality tools that can
extract named entities, numerical expressions, and
class names for each foreign language is very costly
and time-consuming.
Recently, some pioneering studies have inves-
tigated approaches to automatically construct QA
components from scratch by applying machine
learning techniques to training data (Ittycheriah et
al., 2001a)(Ittycheriah et al, 2001b)(Ng et al, 2001)
(Pasca and Harabagiu)(Suzuki et al, 2002)(Suzuki
215
Table 1: Number of Questions in Question Types of CRL QA Data
# of Questions # of Question Types Example
1-9 74 AWARD, CRIME, OFFENSE
10-50 32 PERCENT, N PRODUCT, YEAR PERIOD
51-100 6 COUNTRY, COMPANY, GROUP
100-300 3 PERSON, DATE, MONEY
Total 115
et al, 2003) (Zukerman and Horvitz, 2001)(Sasaki
et al, 2004). These approaches still suffer from the
problem of preparing an adequate amount of training
data specifically designed for a particular QA sys-
tem because each QA system uses its own question-
type system. It is very typical in the course of sys-
tem development to redesign the question-type sys-
tem in order to improve system performance. This
inevitably leads to revision of a large-scale training
dataset, which requires a heavy workload.
For example, assume that you have to develop a
Chinese or Greek QA system and have 10,000 pairs
of question and answers. You have to manually clas-
sify the questions according to your own question-
type system. In addition, you have to annotate the
tags of the question types to large-scale Chinese or
Greek documents. If you wanted to redesign the
question type ORGANIZATION to three categories,
COMPANY, SCHOOL, and OTHER ORGANIZATION,
then the ORGANIZATION tags in the annotated doc-
ument set would need to be manually revisited and
revised.
To solve this problem, this paper regards Ques-
tion Answering as Question-Biased Term Extraction
(QBTE). This new QBTE approach liberates QA
systems from the heavy burden imposed by question
types.
Since it is a challenging as well as a very com-
plex and sensitive problem to directly extract an-
swers without using question types and only using
features of questions, correct answers, and contexts
in documents, we have to investigate the feasibility
of this approach: how well can answer candidates
be extracted, and how well are answer candidates
ranked?
In response, this paper employs the ma-
chine learning technique Maximum Entropy Models
(MEMs) to extract answers to a question from doc-
uments based on question features, document fea-
tures, and the combined features. Experimental re-
sults show the performance of a QA system that ap-
plies MEMs.
2 Preparation
2.1 Training Data
Document Set Japanese newspaper articles of The
Mainichi Newspaper published in 1995.
Question/Answer Set We used the CRL1 QA
Data (Sekine et al, 2002). This dataset com-
prises 2,000 Japanese questions with correct
answers as well as question types and IDs of
articles that contain the answers. Each ques-
tion is categorized as one of 115 hierarchically
classified question types.
The document set is used not only in the training
phase but also in the execution phrase.
Although the CRL QA Data contains question
types, the information of question types are not used
for the training. This is because more than the 60%
of question types have fewer than 10 questions as
examples (Table 1). This means it is very unlikely
that we can train a QA system that can handle this
60% due to data sparseness. 2 Only for the purpose
of analyzing experimental results in this paper do we
refer to the question types of the dataset.
2.2 Learning with Maximum Entropy Models
This section briefly introduces the machine learning
technique Maximum Entropy Models and describes
how to apply MEMs to QA tasks.
2.2.1 Maximum Entropy Models
Let X be a set of input symbols and Y be a set
of class labels. A sample (x, y) is a pair of input
x={x1,. . . , xm} (xi ? X ) and output y ? Y .
1Presently, National Institute of Information and Communi-
cations Technology (NICT), Japan
2A machine learning approach to hierarchical question anal-
ysis was reported in (Suzuki et al, 2003), but training and main-
taining an answer extractor for question types of fine granularity
is not an easy task.
216
The Maximum Entropy Principle (Berger et al,
1996) is to find a model p? = argmax
p?C
H(p), which
means a probability model p(y|x) that maximizes
entropy H(p).
Given data (x(1), y(1)),. . .,(x(n), y(n)), let
?
k
(x(k) ? {y(k)}) = {?x?1, y?1?, ..., ?x?i, y?i?, ...,
?x?m, y?m?}. This means that we enumerate all pairs
of an input symbol and label and represent them as
?x?i, y?i? using index i (1 ? i ? m).
In this paper, feature function fi is defined as fol-
lows.
fi(x, y) =
{
1 if x?i ? x and y = y?i
0 otherwise
We use all combinations of input symbols in x and
class labels for features (or the feature function) of
MEMs.
With Lagrangian ? = ?1, ..., ?m, the dual func-
tion of H is:
?(?) = ?
?
x
p?(x) log Z?(x) +
?
?ip?(fi),
where Z?(x) =
?
y
exp(
?
i
?ifi(x, y)) and p?(x)
and p?(fi) indicate the empirical distribution of x and
fi in the training data.
The dual optimization problem ?? =
argmax
?
?(?) can be efficiently solved as an
optimization problem without constraints. As a
result, probabilistic model p? = p?? is obtained as:
p??(y|x) =
1
Z?(x)
exp
(
?
i
?ifi(x, y)
)
.
2.2.2 Applying MEMs to QA
Question analysis is a classification problem that
classifies questions into different question types.
Answer candidate extraction is also a classifica-
tion problem that classifies words into answer types
(i.e., question types), such as PERSON, DATE, and
AWARD. Answer selection is an exactly classifica-
tion that classifies answer candidates as positive or
negative. Therefore, we can apply machine learning
techniques to generate classifiers that work as com-
ponents of a QA system.
In the QBTE approach, these three components,
i.e., question analysis, answer candidate extraction,
and answer selection, are integrated into one classi-
fier.
To successfully carry out this goal, we have to
extract features that reflect properties of correct an-
swers of a question in the context of articles.
3 QBTE Model 1
This section presents a framework, QBTE Model
1, to construct a QA system from question-answer
pairs based on the QBTE Approach. When a user
gives a question, the framework finds answers to the
question in the following two steps.
Document Retrieval retrieves the top N articles or
paragraphs from a large-scale corpus.
QBTE creates input data by combining the question
features and documents features, evaluates the
input data, and outputs the top M answers.3
Since this paper focuses on QBTE, this paper uses
a simple idf method in document retrieval.
Let wi be words and w1,w2,. . .wm be a docu-
ment. Question Answering in the QBTE Model 1
involves directly classifying words wi in the docu-
ment into answer words or non-answer words. That
is, given input x(i) for wi, its class label is selected
from among {I, O, B} as follows:
I: if the word is in the middle of the answer word
sequence;
O: if the word is not in the answer word sequence;
B: if the word is the start word of the answer word
sequence.
The class labeling system in our experiment is
IOB2 (Sang, 2000), which is a variation of
IOB (Ramshaw and Marcus, 1995).
Input x(i) of each word is defined as described be-
low.
3.1 Feature Extraction
This paper employs three groups of features as fea-
tures of input data:
? Question Feature Set (QF);
? Document Feature Set (DF);
? Combined Feature Set (CF), i.e., combinations
of question and document features.
3In this paper, M is set to 5.
217
3.1.1 Question Feature Set (QF)
A Question Feature Set (QF) is a set of features
extracted only from a question sentence. This fea-
ture set is defined as belonging to a question sen-
tence.
The following are elements of a Question Feature
Set:
qw: an enumeration of the word n-grams (1 ?
n ? N ), e.g., given question ?What is CNN??,
the features are {qw:What, qw:is, qw:CNN,
qw:What-is, qw:is-CNN } if N = 2,
qq: interrogative words (e.g., who, where, what,
how many),
qm1: POS1 of words in the question, e.g., given
?What is CNN??, { qm1:wh-adv, qm1:verb,
qm1:noun } are features,
qm2: POS2 of words in the question,
qm3: POS3 of words in the question,
qm4: POS4 of words in the question.
POS1-POS4 indicate part-of-speech (POS) of the
IPA POS tag set generated by the Japanese mor-
phological analyzer ChaSen. For example, ?Tokyo?
is analyzed as POS1 = noun, POS2 = propernoun,
POS3 = location, and POS4 = general. This paper
used up to 4-grams for qw.
3.1.2 Document Feature Set (DF)
Document Feature Set (DF) is a feature set ex-
tracted only from a document. Using only DF corre-
sponds to unbiased Term Extraction (TE).
For each word wi, the following features are ex-
tracted:
dw?k,. . .,dw+0,. . .,dw+k: k preceding and follow-
ing words of the word wi, e.g., { dw?1:wi?1,
dw+0:wi, dw+1:wi+1} if k = 1,
dm1?k,. . .,dm1+0,. . .,dm1+k: POS1 of k preced-
ing and following words of the word wi,
dm2?k,. . .,dm2+0,. . .,dm2+k: POS2 of k preced-
ing and following words of the word wi,
dm3?k,. . .,dm3+0,. . .,dm3+k: POS3 of k preced-
ing and following words of the word wi,
dm4?k,. . .,dm4+0,. . .,dm4+k: POS4 of k preced-
ing and following words of the word wi.
In this paper, k is set to 3 so that the window size is
7.
3.1.3 Combined Feature Set (CF)
Combined Feature Set (CF) contains features cre-
ated by combining question features and document
features. QBTE Model 1 employs CF. For each word
wi, the following features are created.
cw?k,. . .,cw+0,. . .,cw+k: matching results
(true/false) between each of dw?k,...,dw+k
features and any qw feature, e.g., cw?1:true if
dw?1:President and qw: President,
cm1?k,. . .,cm1+0,. . .,cm1+k: matching results
(true/false) between each of dm1?k,...,dm1+k
features and any POS1 in qm1 features,
cm2?k,. . .,cm2+0,. . .,cm2+k: matching results
(true/false) between each of dm2?k,...,dm2+k
features and any POS2 in qm2 features,
cm3?k,. . .,cm3+0,. . .,cm3+k: matching results
(true/false) between each of dm3?k,...,dm3+k
features and any POS3 in qm3 features,
cm4?k,. . .,cm4+0,. . .,cm4+k: matching results
(true/false) between each of dm4?k,...,dm4+k
features and any POS4 in qm4 features,
cq?k,. . .,cq+0,. . .,cq+k: combinations of each of
dw?k,...,dw+k features and qw features, e.g.,
cq?1:President&Who is a combination of dw?
1:President and qw:Who.
3.2 Training and Execution
The training phase estimates a probabilistic model
from training data (x(1),y(1)),...,(x(n),y(n)) gener-
ated from the CRL QA Data. The execution phase
evaluates the probability of y?(i) given inputx?(i) us-
ing the the probabilistic model.
Training Phase
1. Given question q, correct answer a, and docu-
ment d.
2. Annotate ?A? and ?/A? right before and after
answer a in d.
3. Morphologically analyze d.
4. For d = w1, ..., ?A?, wj , ..., wk, ?/A?, ..., wm,
extract features as x(1),...,x(m).
5. Class label y(i) = B if wi follows ?A?, y(i) = I
if wi is inside of ?A? and ?/A?, and y(i) = O
otherwise.
218
Table 2: Main Results with 10-fold Cross Validation
Correct Answer Rank MRR Top51 2 3 4 5
Exact match 453 139 68 35 19 0.28 0.36
Partial match 684 222 126 80 48 0.43 0.58
Ave. 0.355 0.47
Manual evaluation 578 188 86 55 34 0.36 0.47
6. Estimate p?? from (x(1),y(1)),...,(x(n),y(n)) us-
ing Maximum Entropy Models.
The execution phase extracts answers from re-
trieved documents as Term Extraction, biased by the
question.
Execution Phase
1. Given question q and paragraph d.
2. Morphologically analyze d.
3. For wi of d = w1, ..., wm, create input data
x?(i) by extracting features.
4. For each y?(j) ? Y , compute p? ? (y?(j)|x?(i)),
which is a probability of y?(j) given x?(i).
5. For each x?(i), y?(j) with the highest probability
is selected as the label of wi.
6. Extract word sequences that start with the word
labeled B and are followed by words labeled I
from the labeled word sequence of d.
7. Rank the top M answers according to the prob-
ability of the first word.
This approach is designed to extract only the most
highly probable answers. However, pin-pointing
only answers is not an easy task. To select the top
five answers, it is necessary to loosen the condition
for extracting answers. Therefore, in the execution
phase, we only give label O to a word if its probabil-
ity exceeds 99%, otherwise we give the second most
probable label.
As a further relaxation, word sequences that in-
clude B inside the sequences are extracted for an-
swers. This is because our preliminary experiments
indicated that it is very rare for two answer candi-
dates to be adjacent in Question-Biased Term Ex-
traction, unlike an ordinary Term Extraction task.
4 Experimental Results
We conducted 10-fold cross validation using the
CRL QA Data. The output is evaluated using the
Top5 score and MRR.
Top5 Score shows the rate at which at least one
correct answer is included in the top 5 answers.
MRR (Mean Reciprocal Rank) is the average re-
ciprocal rank (1/n) of the highest rank n of a
correct answer for each question.
Judgment of whether an answer is correct is done
by both automatic and manual evaluation. Auto-
matic evaluation consists of exact matching and par-
tial matching. Partial matching is useful for ab-
sorbing the variation in extraction range. A partial
match is judged correct if a system?s answer com-
pletely includes the correct answer or the correct an-
swer completely includes a system?s answer. Table 2
presents the experimental results. The results show
that a QA system can be built by using our QBTE ap-
proach. The manually evaluated performance scored
MRR=0.36 and Top5=0.47. However, manual eval-
uation is costly and time-consuming, so we use au-
tomatic evaluation results, i.e., exact matching re-
sults and partial matching results, as a pseudo lower-
bound and upper-bound of the performances. Inter-
estingly, the manual evaluation results of MRR and
Top5 are nearly equal to the average between exact
and partial evaluation.
To confirm that the QBTE ranks potential answers
to the higher rank, we changed the number of para-
graphs retrieved from a large corpus from N =
1, 3, 5 to 10. Table 3 shows the results. Whereas
the performances of Term Extraction (TE) and Term
Extraction with question features (TE+QF) signifi-
cantly degraded, the performance of the QBTE (CF)
did not severely degrade with the larger number of
retrieved paragraphs.
219
Table 3: Answer Extraction from Top N documents
Feature set Top N paragraphs Match Correct Answer Rank MRR Top51 2 3 4 5
1 Exact 102 109 80 71 62 0.11 0.21Partial 207 186 155 153 121 0.21 0.41
3 Exact 65 63 55 53 43 0.07 0.14TE (DF) Partial 120 131 112 108 94 0.13 0.28
5 Exact 51 38 38 36 36 0.05 0.10Partial 99 80 89 81 75 0.10 0.21
10 Exact 29 17 19 22 18 0.03 0.07Partial 59 38 35 49 46 0.07 0.14
1 Exact 120 105 94 63 80 0.12 0. 23Partial 207 198 175 126 140 0.21 0 .42
TE (DF) 3 Exact 65 68 52 58 57 0.07 0.15+ Partial 119 117 111 122 106 0.13 0.29
QF 5 Exact 44 57 41 35 31 0.05 0.10Partial 91 104 71 82 63 0.10 0.21
10 Exact 28 42 30 28 26 0.04 0.08Partial 57 68 57 56 45 0.07 0.14
1 Exact 453 139 68 35 19 0.28 0.36Partial 684 222 126 80 48 0.43 0.58
3 Exact 403 156 92 52 43 0.27 0.37QBTE (CF) Partial 539 296 145 105 92 0.42 0.62
5 Exact 381 153 92 59 50 0.26 0.37Partial 542 291 164 122 102 0.40 0.61
10 Exact 348 128 92 65 57 0.24 0.35Partial 481 257 173 124 102 0.36 0.57
5 Discussion
Our approach needs no question type system, and it
still achieved 0.36 in MRR and 0.47 in Top5. This
performance is comparable to the results of SAIQA-
II (Sasaki et al, 2004) (MRR=0.4, Top5=0.55)
whose question analysis, answer candidate extrac-
tion, and answer selection modules were indepen-
dently built from a QA dataset and an NE dataset,
which is limited to eight named entities, such as
PERSON and LOCATION. Since the QA dataset is
not publicly available, it is not possible to directly
compare the experimental results; however we be-
lieve that the performance of the QBTE Model 1 is
comparable to that of the conventional approaches,
even though it does not depend on question types,
named entities, or class names.
Most of the partial answers were judged correct
in manual evaluation. For example, for ?How many
times bigger ...??, ?two times? is a correct answer
but ?two? was judged correct. Suppose that ?John
Kerry? is a prepared correct answer in the CRL QA
Data. In this case, ?Senator John Kerry? would also
be correct. Such additions and omissions occur be-
cause our approach is not restricted to particular ex-
traction units, such as named entities or class names.
The performance of QBTE was affected little by
the larger number of retrieved paragraphs, whereas
the performances of TE and TE + QF significantly
degraded. This indicates that QBTE Model 1 is not
mere Term Extraction with document retrieval but
Term Extraction appropriately biased by questions.
Our experiments used no information about ques-
tion types given in the CRL QA Data because we are
seeking a universal method that can be used for any
QA dataset. Beyond this main goal, as a reference,
The Appendix shows our experimental results clas-
sified into question types without using them in the
training phase. The results of automatic evaluation
of complete matching are in Top5 (T5), and MRR
and partial matching are in Top5 (T5?) and MRR?.
It is interesting that minor question types were cor-
rectly answered, e.g., SEA and WEAPON, for which
there was only one training question.
We also conducted an additional experiment, as a
reference, on the training data that included question
types defined in the CRL QA Data; the question-
type of each question is added to the qw feature. The
performance of QBTE from the first-ranked para-
graph showed no difference from that of experi-
ments shown in Table 2.
220
6 Related Work
There are two previous studies on integrating
QA components into one using machine learn-
ing/statistical NLP techniques. Echihabi et al (Echi-
habi et al, 2003) used Noisy-Channel Models to
construct a QA system. In this approach, the range
of Term Extraction is not trained by a data set but se-
lected from answer candidates, e.g., named entities
and noun phrases, generated by a decoder. Lita et
al. (Lita and Carbonell, 2004) share our motivation
to build a QA system only from question-answer
pairs without depending on the question types. Their
method finds clusters of questions and defines how
to answer questions in each cluster. However, their
approach is to find snippets, i.e., short passages
including answers, not exact answers extracted by
Term Extraction.
7 Conclusion
This paper described a novel approach to extract-
ing answers to a question using probabilistic mod-
els constructed from only question-answer pairs.
This approach requires no question type system, no
named entity extractor, and no class name extractor.
To the best of our knowledge, no previous study has
regarded Question Answering as Question-Biased
Term Extraction. As a feasibility study, we built
a QA system using Maximum Entropy Models on
a 2000-question/answer dataset. The results were
evaluated by 10-fold cross validation, which showed
that the performance is 0.36 in MRR and 0.47 in
Top5. Since this approach relies on a morphological
analyzer, applying the QBTE Model 1 to QA tasks
of other languages is our future work.
Acknowledgment
This research was supported by a contract with the
National Institute of Information and Communica-
tions Technology (NICT) of Japan entitled, ?A study
of speech dialogue translation technology based on
a large corpus?.
References
Adam L. Berger, Stephen A. Della Pietra, and Vincent J.
Della Pietra: A Maximum Entropy Approach to Nat-
ural Language Processing, Computational Linguistics,
Vol. 22, No. 1, pp. 39?71 (1996).
Abdessamad Echihabi and Daniel Marcu: A Noisy-
Channel Approach to Question Answering, Proc. of
ACL-2003, pp. 16-23 (2003).
Abraham Ittycheriah, Martin Franz, Wei-Jing Zhu, and
Adwait Ratnaparkhi: Question Answering Using
Maximum-Entropy Components, Proc. of NAACL-
2001 (2001).
Abraham Ittycheriah, Martin Franz, Wei-Jing Zhu, and
Adwait Ratnaparkhi: IBM?s Statistical Question An-
swering System ? TREC-10, Proc. of TREC-10
(2001).
Lucian Vlad Lita and Jaime Carbonell: Instance-Based
Question Answering: A Data-Driven Approach: Proc.
of EMNLP-2004, pp. 396?403 (2004).
Hwee T. Ng, Jennifer L. P. Kwan, and Yiyuan Xia: Ques-
tion Answering Using a Large Text Database: A Ma-
chine Learning Approach: Proc. of EMNLP-2001, pp.
67?73 (2001).
Marisu A. Pasca and Sanda M. Harabagiu: High Perfor-
mance Question/Answering, Proc. of SIGIR-2001, pp.
366?374 (2001).
Lance A. Ramshaw and Mitchell P. Marcus: Text Chunk-
ing using Transformation-Based Learning, Proc. of
WVLC-95, pp. 82?94 (1995).
Erik F. Tjong Kim Sang: Noun Phrase Recognition by
System Combination, Proc. of NAACL-2000, pp. 55?
55 (2000).
Yutaka Sasaki, Hideki Isozaki, Jun Suzuki, Kouji
Kokuryou, Tsutomu Hirao, Hideto Kazawa, and
Eisaku Maeda, SAIQA-II: A Trainable Japanese QA
System with SVM, IPSJ Journal, Vol. 45, NO. 2, pp.
635-646, 2004. (in Japanese)
Satoshi Sekine, Kiyoshi Sudo, Yusuke Shinyama,
Chikashi Nobata, Kiyotaka Uchimoto, and Hitoshi Isa-
hara, NYU/CRL QA system, QAC question analysis
and CRL QA data, in Working Notes of NTCIR Work-
shop 3 (2002).
Jun Suzuki, Yutaka Sasaki, and Eisaku Maeda: SVM An-
swer Selection for Open-Domain Question Answer-
ing, Proc. of Coling-2002, pp. 974?980 (2002).
Jun Suzuki, Hirotoshi Taira, Yutaka Sasaki, and Eisaku
Maeda: Directed Acyclic Graph Kernel, Proc. of ACL
2003 Workshop on Multilingual Summarization and
Question Answering - Machine Learning and Beyond,
pp. 61?68, Sapporo (2003).
Ingrid Zukerman and Eric Horvitz: Using Machine
Learning Techniques to Interpret WH-Questions,
Proc. of ACL-2001, Toulouse, France, pp. 547?554
(2001).
221
Appendix: Analysis of Evaluation Results w.r.t.
Question Type ? Results of QBTE from the first-
ranked paragraph (NB: No information about these
question types was used in the training phrase.)
Question Type #Qs MRR T5 MRR? T5?
GOE 36 0.30 0.36 0.41 0.53
GPE 4 0.50 0.50 1.00 1.00
N EVENT 7 0.76 0.86 0.76 0.86
EVENT 19 0.17 0.21 0.41 0.53
GROUP 74 0.28 0.35 0.45 0.62
SPORTS TEAM 15 0.28 0.40 0.45 0.73
BROADCAST 1 0.00 0.00 0.00 0.00
POINT 2 0.00 0.00 0.00 0.00
DRUG 2 0.00 0.00 0.00 0.00
SPACESHIP 4 0.88 1.00 0.88 1.00
ACTION 18 0.22 0.22 0.30 0.44
MOVIE 6 0.50 0.50 0.56 0.67
MUSIC 8 0.19 0.25 0.56 0.62
WATER FORM 3 0.50 0.67 0.50 0.67
CONFERENCE 17 0.14 0.24 0.46 0.65
SEA 1 1.00 1.00 1.00 1.00
PICTURE 1 0.00 0.00 0.00 0.00
SCHOOL 21 0.10 0.10 0.33 0.43
ACADEMIC 5 0.20 0.20 0.37 0.60
PERCENT 47 0.35 0.43 0.43 0.55
COMPANY 77 0.45 0.55 0.57 0.70
PERIODX 1 1.00 1.00 1.00 1.00
RULE 35 0.30 0.43 0.49 0.69
MONUMENT 2 0.00 0.00 0.25 0.50
SPORTS 9 0.17 0.22 0.40 0.67
INSTITUTE 26 0.38 0.46 0.53 0.69
MONEY 110 0.33 0.40 0.48 0.63
AIRPORT 4 0.38 0.50 0.44 0.75
MILITARY 4 0.00 0.00 0.25 0.25
ART 4 0.25 0.50 0.25 0.50
MONTH PERIOD 6 0.06 0.17 0.06 0.17
LANGUAGE 3 1.00 1.00 1.00 1.00
COUNTX 10 0.33 0.40 0.38 0.60
AMUSEMENT 2 0.00 0.00 0.00 0.00
PARK 1 0.00 0.00 0.00 0.00
SHOW 3 0.78 1.00 1.11 1.33
PUBLIC INST 19 0.18 0.26 0.34 0.53
PORT 3 0.17 0.33 0.33 0.67
N COUNTRY 8 0.28 0.38 0.32 0.50
NATIONALITY 4 0.50 0.50 1.00 1.00
COUNTRY 84 0.45 0.60 0.51 0.67
OFFENSE 9 0.23 0.44 0.23 0.44
CITY 72 0.41 0.50 0.53 0.65
N FACILITY 4 0.25 0.25 0.38 0.50
FACILITY 11 0.20 0.36 0.25 0.55
TIMEX 3 0.00 0.00 0.00 0.00
TIME TOP 2 0.00 0.00 0.50 0.50
TIME PERIOD 8 0.12 0.12 0.48 0.75
TIME 13 0.22 0.31 0.29 0.38
ERA 3 0.00 0.00 0.33 0.33
PHENOMENA 5 0.50 0.60 0.60 0.80
DISASTER 4 0.50 0.75 0.50 0.75
OBJECT 5 0.47 0.60 0.47 0.60
CAR 1 1.00 1.00 1.00 1.00
RELIGION 5 0.30 0.40 0.30 0.40
WEEK PERIOD 4 0.05 0.25 0.55 0.75
WEIGHT 12 0.21 0.25 0.31 0.42
PRINTING 6 0.17 0.17 0.38 0.50
Question Type #Q MRR T5 MRR? T5?
RANK 7 0.18 0.29 0.54 0.71
BOOK 6 0.31 0.50 0.47 0.67
AWARD 9 0.17 0.33 0.34 0.56
N LOCATION 2 0.10 0.50 0.10 0.50
VEGETABLE 10 0.31 0.50 0.34 0.60
COLOR 5 0.20 0.20 0.20 0.20
NEWSPAPER 7 0.61 0.71 0.61 0.71
WORSHIP 8 0.47 0.62 0.62 0.88
SEISMIC 1 0.00 0.00 1.00 1.00
N PERSON 72 0.30 0.39 0.43 0.60
PERSON 282 0.18 0.21 0.46 0.55
NUMEX 19 0.32 0.32 0.35 0.47
MEASUREMENT 1 0.00 0.00 0.00 0.00
P ORGANIZATION 3 0.33 0.33 0.67 0.67
P PARTY 37 0.30 0.41 0.43 0.57
GOVERNMENT 37 0.50 0.54 0.53 0.57
N PRODUCT 41 0.25 0.37 0.37 0.56
PRODUCT 58 0.24 0.34 0.44 0.69
WAR 2 0.75 1.00 0.75 1.00
SHIP 7 0.26 0.43 0.40 0.57
N ORGANIZATION 20 0.14 0.25 0.28 0.55
ORGANIZATION 23 0.08 0.13 0.20 0.30
SPEED 1 0.00 0.00 1.00 1.00
VOLUME 5 0.00 0.00 0.18 0.60
GAMES 8 0.28 0.38 0.34 0.50
POSITION TITLE 39 0.20 0.28 0.30 0.44
REGION 22 0.17 0.23 0.46 0.64
GEOLOGICAL 3 0.42 0.67 0.42 0.67
LOCATION 2 0.00 0.00 0.50 0.50
EXTENT 22 0.04 0.09 0.13 0.18
CURRENCY 1 0.00 0.00 0.00 0.00
STATION 3 0.50 0.67 0.50 0.67
RAILROAD 1 0.00 0.00 0.25 1.00
PHONE 1 0.00 0.00 0.00 0.00
PROVINCE 36 0.30 0.33 0.45 0.50
N ANIMAL 3 0.11 0.33 0.22 0.67
ANIMAL 10 0.26 0.50 0.31 0.60
ROAD 1 0.00 0.00 0.50 1.00
DATE PERIOD 9 0.11 0.11 0.33 0.33
DATE 130 0.24 0.32 0.41 0.58
YEAR PERIOD 34 0.22 0.29 0.38 0.59
AGE 22 0.34 0.45 0.44 0.59
MULTIPLICATION 9 0.39 0.44 0.56 0.67
CRIME 4 0.75 0.75 0.75 0.75
AIRCRAFT 2 0.00 0.00 0.25 0.50
MUSEUM 3 0.33 0.33 0.33 0.33
DISEASE 18 0.29 0.50 0.43 0.72
FREQUENCY 13 0.18 0.31 0.19 0.38
WEAPON 1 1.00 1.00 1.00 1.00
MINERAL 18 0.16 0.22 0.25 0.39
METHOD 29 0.39 0.48 0.48 0.62
ETHNIC 3 0.42 0.67 0.75 1.00
NAME 5 0.20 0.20 0.40 0.40
SPACE 4 0.50 0.50 0.50 0.50
THEORY 1 0.00 0.00 0.00 0.00
LANDFORM 5 0.13 0.40 0.13 0.40
TRAIN 2 0.17 0.50 0.17 0.50
2000 0.28 0.36 0.43 0.58
222
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1858?1869,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Modeling Joint Entity and Relation Extraction with Table Representation
Makoto Miwa and Yutaka Sasaki
Toyota Technological Institute
2-12-1 Hisakata, Tempaku-ku, Nagoya, 468-8511, Japan
{makoto-miwa, yutaka.sasaki}@toyota-ti.ac.jp
Abstract
This paper proposes a history-based struc-
tured learning approach that jointly ex-
tracts entities and relations in a sentence.
We introduce a novel simple and flexible
table representation of entities and rela-
tions. We investigate several feature set-
tings, search orders, and learning meth-
ods with inexact search on the table. The
experimental results demonstrate that a
joint learning approach significantly out-
performs a pipeline approach by incorpo-
rating global features and by selecting ap-
propriate learning methods and search or-
ders.
1 Introduction
Extraction of entities and relations from texts has
been traditionally treated as a pipeline of two sep-
arate subtasks: entity recognition and relation ex-
traction. This separation makes the task easy to
deal with, but it ignores underlying dependencies
between and within subtasks. First, since entity
recognition is not affected by relation extraction,
errors in entity recognition are propagated to re-
lation extraction. Second, relation extraction is
often treated as a multi-class classification prob-
lem on pairs of entities, so dependencies between
pairs are ignored. Examples of these dependen-
cies are illustrated in Figure 1. For dependencies
between subtasks, a Live in relation requires PER
and LOC entities, and vice versa. For in-subtask
dependencies, the Live in relation between ?Mrs.
Tsutayama? and ?Japan? can be inferred from the
two other relations.
Figure 1 also shows that the task has a flexible
graph structure. This structure usually does not
cover all the words in a sentence differently from
other natural language processing (NLP) tasks
such as part-of-speech (POS) tagging and depen-
Mrs. Tsuruyama is from Kumamoto Prefecture in Japan .PER LOC LOC
Live_in Located_in
Live_in
Figure 1: An entity and relation example (Roth
and Yih, 2004). Person (PER) and location (LOC)
entities are connected by Live in and Located in
relations.
dency parsing, so local constraints are considered
to be more important in the task.
Joint learning approaches (Yang and Cardie,
2013; Singh et al., 2013) incorporate these de-
pendencies and local constraints in their models;
however most approaches are time-consuming and
employ complex structures consisting of multi-
ple models. Li and Ji (2014) recently proposed
a history-based structured learning approach that
is simpler and more computationally efficient than
other approaches. While this approach is promis-
ing, it still has a complexity in search and restricts
the search order partly due to its semi-Markov rep-
resentation, and thus the potential of the history-
based learning is not fully investigated.
In this paper, we introduce an entity and relation
table to address the difficulty in representing the
task. We propose a joint extraction of entities and
relations using a history-based structured learning
on the table. This table representation simplifies
the task into a table-filling problem, and makes
the task flexible enough to incorporate several en-
hancements that have not been addressed in the
previous history-based approach, such as search
orders in decoding, global features from relations
to entities, and several learning methods with in-
exact search.
2 Method
In this section, we first introduce an entity and re-
lation table that is utilized to represent the whole
1858
entity and relation structures in a sentence. We
then overview our model on the table. We finally
explain the decoding, learning, search order, and
features in our model.
2.1 Entity and relation table
The task we address in this work is the extraction
of entities and their relations from a sentence. En-
tities are typed and may span multiple words. Re-
lations are typed and directed.
We use words to represent entities and relations.
We assume entities do not overlap. We employ
a BILOU (Begin, Inside, Last, Outside, Unit) en-
coding scheme that has been shown to outperform
the traditional BIO scheme (Ratinov and Roth,
2009), and we will show that this scheme induces
several label dependencies between words and be-
tween words and relations in ?2.3.2. A label is
assigned to a word according to the relative posi-
tion to its corresponding entity and the type of the
entity. Relations are represented with their types
and directions. ? denotes a non-relation pair, and
? and? denote left-to-right and right-to-left re-
lations, respectively. Relations are defined on not
entities but words, since entities are not always
given when relations are extracted. Relations on
entities are mapped to relations on the last words
of the entities.
Based on this representation, we propose an en-
tity and relation table that jointly represents en-
tities and relations in a sentence. Figure 2 illus-
trates an entity and relation table corresponding to
an example in Figure 1. We use only the lower tri-
angular part because the table is symmetric, so the
number of cells is n(n + 1)/2 when there are n
words in a sentence. With this entity and relation
table representation, the joint extraction problem
can be mapped to a table-filling problem in that
labels are assigned to cells in the table.
2.2 Model
We tackle the table-filling problem by a history-
based structured learning approach that assigns la-
bels to cells one by one. This is mostly the same as
the traditional history-based model (Collins, 2002)
except for the table representation.
Let x be an input table, Y(x) be all possible
assignments to the table, and s(x,y) be a scoring
function that assesses the assignment of y ? Y(x)
to x. With these definitions, we define our model
to predict the most probable assignment as fol-
lows:
y
?
= argmax
y?Y(x)
s(x,y) (1)
This scoring function is a decomposable function,
and each decomposed function assesses the as-
signment of a label to a cell in the table.
s(x,y) =
|x|
?
i=1
s(x,y, 1, i) (2)
Here, i represents an index of a cell in the table,
which will be explained in ?2.3.1. The decom-
posed function s(x,y, 1, i) corresponds to the i-th
cell. The decomposed function is represented as a
linear model, i.e., an inner product of features and
their corresponding weights.
s(x,y, 1, i) = w?f(x,y, 1, i) (3)
The scoring function are further divided into two
functions as follows:
s(x,y, 1, i) = s
local
(x,y, i) + s
global
(x,y, 1, i)
(4)
Here, s
local
(x,y, i) is a local scoring func-
tion that assesses the assignment to the i-th
cell without considering other assignments, and
s
global
(x,y, 1, i) is a global scoring function that
assesses the assignment in the context of 1st to
(i ? 1)-th assignments. This global scoring func-
tion represents the dependencies between entities,
between relations, and between entities and rela-
tions. Similarly, features f are divided into local
features f
local
and global features f
global
, and they
are defined on its target cell and surrounding con-
texts. The features will be explained in ?2.5. The
weights w can also be divided, but they are tuned
jointly in learning as shown in ?2.4.
2.3 Decoding
The scoring function s(x,y, 1, i) in Equation (2)
uses all the preceding assignments and does not
rely on the Markov assumption, so we cannot em-
ploy dynamic programming.
We instead employ a beam search to find the
best assignment with the highest score (Collins
and Roark, 2004). The beam search assigns la-
bels to cells one by one with keeping the top K
best assignments when moving from a cell to the
next cell, and it returns the best assignment when
labels are assigned to all the cells. The pseudo
code for decoding with the beam search is shown
in Figure 3.
1859
Mrs. Tsutayama is from Kumamoto Prefecture in Japan .
Mrs. B-PER
Tsutayama ? L-PER
is ? ? O
from ? ? ? O
Kumamoto ? ? ? ? B-LOC
Prefecture ? Live in? ? ? ? L-LOC
in ? ? ? ? ? ? O
Japan ? Live in? ? ? ? Located in? ? U-LOC
. ? ? ? ? ? ? ? ? ?
Figure 2: The entity and relation table for the example in Figure 1.
INPUT: x: input table with no assignment,
K: beam size
OUTPUT: best assignment y
?
for x
1: b? [x]
2: for i = 1 to |x| do
3: T ? ?
4: for k = 1 to |b| do
5: for a ?A(i, b[k]) do
6: T ? T ? append(a, b[k])
7: end for
8: end for
9: b? top K tables from T using the scoring
function in Equation (2)
10: end for
11: return b[0]
Figure 3: Decoding with the beam search. A(i, t)
returns possible assignments for i-th cell of a table
t, and append(a, t) returns a table t updated with
an assignment a.
We explain how to map the table to a sequence
(line 2 in Figure 3), and how to calculate possible
assignments (line 6 in Figure 3) in the following
subsections.
2.3.1 Table-to-sequence mapping
Cells in an input table are originally indexed in
two dimensions. To apply our model in ?2.2 to the
cells, we need to map the two-dimensional table
to a one-dimensional sequence. This is equivalent
to defining a search order in the table, so we will
use the terms ?mapping? and ?search order? inter-
changeably.
Since it is infeasible to try all possible map-
pings, we define six promising static mappings
(search orders) as shown in Figure 4. Note that the
?left? and ?right? directions in the captions cor-
respond to not word orders, but tables. We de-
1 3 6
A B C
A 1
B 2 3
C 4 5 6
A B C
52 4
(a) Up to
down, left to
right
1 2 4
A B C
A 1
B 3 2
C 6 5 4
A B C
53 6
(b) Up to
down, right
to left
4 2 1
A B C
A 4
B 5 2
C 6 3 1
A B C
35 6
(c) Right to
left, up to
down
6 3 1
A B C
A 6
B 5 3
C 4 2 1
A B C
25 4
(d) Right to
left, down to
up
1 2 3
A B C
A 1
B 4 2
C 6 5 3
A B C
54 6
(e) Close-
first, left to
right
3 2 1
A B C
A 3
B 5 2
C 6 4 1
A B C
45 6
(f) Close-
first, right to
left
Figure 4: Static search orders.
fine two mappings (Figures 4(a) and 4(b)) with the
highest priority on the ?up to down? order, which
checks a sentence forwardly (from the beginning
of a sentence). Similarly, we also define two map-
pings (Figures 4(c) and 4(d)) with the highest pri-
ority on the ?right to left? order, which check a
sentence backwardly (from the end of a sentence).
From another point of view, entities are detected
before relations in Figures 4(b) and 4(c) whereas
the order in a sentence is prioritized in Figures 4(a)
1860
Condition Possible labels on w
i
Relation(s) on w
i?1
B-*, O, U-*
Relation(s) on w
i
L-*, U-*
Table 1: Label dependencies from relations to en-
tities. * indicates any type.
Label on w
i
Relations from/to w
i
B-*, I-*, O ?
L-*, U-* *
Label on w
i+1
Relations from/to w
i
I-*, L-* ?
B-*, U-*, O *
Table 2: Label dependencies from entities to rela-
tions.
and 4(d). We further define two close-first map-
pings (Figures 4(e) and 4(f)) since entities are
easier to find than relations and close relations are
easier to find than distant relations.
We also investigate dynamic mappings (search
orders) with an easy-first policy (Goldberg and El-
hadad, 2010). Dynamic mappings are different
from the static mappings above, since we reorder
the cells before each decoding
1
. We evaluate the
cells using the local scoring function, and assign
indices to the cells so that the cells with higher
scores have higher priorities. In addition to this
na??ve easy-first policy, we define two other dy-
namic mappings that restricts the reordering by
combining the easy-first policy with one of the fol-
lowing two policies: entity-first (all entities are de-
tected before relations) and close-first (closer cells
are detected before distant cells) policies.
2.3.2 Label dependencies
To avoid illegal assignments to a table, we have
to restrict the possible assignments to the cells ac-
cording to the preceding assignments. This restric-
tion can also reduce the computational costs.
We consider all the dependencies between cells
to allow the assignments of labels to the cells in
an arbitrary order. Our representation of entities
and relations in ?2.1 induces the dependencies be-
tween entities and between entities and relations.
Tables 1-3 summarize these dependencies on the i-
th wordw
i
in a sentence. We can further utilize de-
pendencies between entity types and relation types
if some entity types are involved in a limited num-
1
It is also possible to reorder the cells during decoding,
but it greatly increases the computational costs.
Label on w
i?2
Possible labels on w
i
B-TYPE B-*, I-TYPE, L-TYPE, O, U-*
I-TYPE B-*, I-TYPE, L-TYPE, O, U-*
L-TYPE B-*, I-*, L-*, O, U-*
O B-*, I-*, L-*, O, U-*
U-TYPE B-*, I-*, L-*, O, U-*
O/S B-*, I-*, L-*, O, U-*
Label on w
i?1
Possible labels on w
i
B-TYPE I-TYPE, L-TYPE
I-TYPE I-TYPE, L-TYPE
L-TYPE B-*, O, U-*
O B-*, O, U-*
U-TYPE B-*, O, U-*
O/S B-*, O, U-*
Label on w
i+1
Possible labels on w
i
B-TYPE L-*, O, U-*
I-TYPE B-TYPE, I-TYPE
L-TYPE B-TYPE, I-TYPE
O L-*, O, U-*
U-TYPE L-*, O, U-*
O/S L-*, O, U-*
Label on w
i+2
Possible labels on w
i
B-TYPE B-*, I-*, L-*, O, U-*
I-TYPE B-TYPE, I-TYPE, L-*, O, U-*
L-TYPE B-TYPE, I-TYPE, L-*, O, U-*
O B-*, I-*, L-*, O, U-*
U-TYPE B-*, I-*, L-*, O, U-*
O/S B-*, I-*, L-*, O, U-*
Table 3: Label dependencies between entities.
TYPE represents an entity type, and O/S means
the word is outside of a sentence.
ber of relation types or vice versa. We note that
the dependencies between entity types and rela-
tion types include not only words participating in
relations but also their surrounding words. For ex-
ample, the label on w
i?1
can restrict the types of
relations involving w
i
. We employ these type de-
pendencies in the evaluation, but we omit these de-
pendencies here since these dependencies are de-
pendent on the tasks.
2.4 Learning
The goal of learning is to minimize errors between
predicted assignments y
?
and gold assignments
y
gold
by tuning the weights w in the scoring func-
tion in Equation 3. We employ a margin-based
structured learning approach to tune the weights
w. The pseudo code is shown in Figure 5. This ap-
proach enhances the traditional structured percep-
1861
INPUT: training sets D = {(x
i
,y
i
)}
N
i=1
,
T: iterations
OUTPUT: weights w
1: w? 0
2: for t = 1 to T do
3: for x,y ? D do
4: y
?
? best assignment for x using decod-
ing in Figure 3 with s
?
in Equation (5)
5: if y
?
?= y
gold
then
6: m? argmax
i
{s
?
(x,y
gold
, 1, i)?
s
?
(x,y
?
, 1, i)}
7: w? update(w, f(x,y
gold
, 1,m),
f(x,y
?
, 1,m))
8: end if
9: end for
10: end for
11: return w
Figure 5: Margin-based structured learn-
ing approach with a max-violation update.
update(w, f(x,y
gold
, 1,m), f(x,y
?
, 1,m))
depends on employed learning methods.
tron (Collins, 2002) in the following ways. Firstly,
we incorporate a margin ? into the scoring func-
tion as follows so that wrong assignments with
small differences from gold assignments are pe-
nalized (lines 4 and 6 in Figure 5) (Freund and
Schapire, 1999).
s
?
(x,y) = s(x,y) + ?(y,y
gold
) (5)
Similarly to the scoring function s, the margin ?
is defined as a decomposable function using 0-1
loss as follows:
?(y,y
gold
) =
|x|
?
i=1
?(y
i
, y
gold
i
),
?(y
i
, y
gold
i
) =
{
0 if y
i
= y
gold
i
1 otherwise
(6)
Secondly, we update the weights w based on a
max-violation update rule following Huang et al.
(2012) (lines 6-7 in Figure 5). Finally, we em-
ploy not only perceptron (Collins, 2002) but also
AROW (Mejer and Crammer, 2010; Crammer et
al., 2013), AdaGrad (Duchi et al., 2011), and
DCD-SSVM (Chang and Yih, 2013) for learning
methods (line 7 in Figure 5.) We employ parame-
ter averaging except for DCD-SSVM. AROW and
AdaGrad store additional information for covari-
ance and feature counts respectively, and DCD-
SSVM keeps a working set and performs addi-
tional updates in each iteration. Due to space limi-
tations, we refer to the papers for the details of the
learning methods.
2.5 Features
Here, we explain the local features f
local
and the
global features f
global
introduced in ?2.2.
2.5.1 Local features
Our focus is not to exploit useful local features
for entities and relations, so we incorporate several
features from existing work to realize a reasonable
baseline. Table 4 summarizes the local features.
Local features for entities (or words) are similar
to the features used by Florian et al. (2003), but
some features are generalized and extended, and
gazetteer features are excluded. For relations (or
pairs of words), we employ and extend features in
Miwa et al. (2009).
2.5.2 Global features
We design global features to represent dependen-
cies among entities and relations. Table 5 summa-
rizes the global features
2
. These global features
are activated when all the information is available
during decoding.
We incorporate label dependency features like
traditional sequential labeling for entities. Al-
though our model can include other non-local fea-
tures between entities (Ratinov and Roth, 2009),
we do not include them expecting that global fea-
tures on entities and relations can cover them. We
design three types of global features for relations.
These features are activated when all the partic-
ipating relations are not ? (non-relations). Fea-
tures except for the ?Crossing? category are simi-
lar to global relation features in Li and Ji (2014).
We further incorporate global features for both en-
tities and relations. These features are activated
when the relation label is not ?. These features
can act as a bridge between entities and relations.
3 Evaluation
In this section, we first introduce the corpus and
evaluation metrics that we employed for evalua-
tion. We then show the performance on the train-
ing data set with explaining the parameters used
2
We tried other ?Entity+Relation? features to represent a
relation and both its participating entities, but they slightly
degraded the performance in our preliminary experiments.
1862
Target Category Features
Word Lexical Character n-grams (n=2,3,4)
(Entity) Attributes by parsers (base form, POS)
Word types (all-capitalized, initial-capitalized, all-digits, all-puncts, all-
digits-or-puncts)
Contextual Word n-grams (n=1,2,3) within a context window size of 2
Word pair Entity Entity lexical features of each word
(Relation) Contextual Word n-grams (n=1,2,3) within a context window size of 2
Shortest
path
Walk features (word-dependency-word or dependency-word-
dependency) on the shortest paths in parsers? outputs
n-grams (n=2,3) of words and dependencies on the paths
n-grams (n=1,2) of token modifier-modifiee pairs on the paths
The length of the paths
Table 4: Local features.
Target Category Details
Entity Bigram Bigrams of labels
Combinations of two labels and their corresponding POS tags
Combinations of two labels and their corresponding words
Trigram Trigrams of labels
Combinations of three labels and each of their corresponding POS tags
Combinations of three labels and each of their corresponding words
Entity Combinations of a label and its corresponding entity
Relation Entity-
sharing
Combinations of two relation labels that share a word (i.e., relations in
same columns or same rows in a table)
Combinations of two relation labels and the shared word
Relation shortest path features between non-shared words, augmented by
a combination of relation labels and the shared word
Cyclic Combinations of three relation labels that make a cycle
Crossing Combinations of two relation labels that cross each other
Entity + Entity- Relation label and the label of its participating entity
Relation relation Relation label and the label and word of its participating entity
Table 5: Global features.
for the test set evaluation, and show the perfor-
mance on the test data set.
3.1 Evaluation settings
We used an entity and relation recognition corpus
by Roth and Yih (2004)
3
. The corpus defines four
named entity types Location, Organization, Per-
son, andOther and five relation typesKill, Live In,
Located In, OrgBased In and Work For.
All the entities were words in the original cor-
pus because all the spaces in entities were replaced
with slashes. Previous systems (Roth and Yih,
2007; Kate and Mooney, 2010) used these word
3
conll04.corp at http://cogcomp.cs.illinois.
edu/page/resource_view/43
boundaries as they were, treated the boundaries as
given, and focused the entity classification prob-
lem alone. Differently from such systems, we re-
covered these spaces by replacing these slashes
with spaces to evaluate the entity boundary detec-
tion performance on this corpus. Due to this re-
placement and the inclusion of the boundary de-
tection problem, our task is more challenging than
the original task, and our results are not compara-
ble with those by the previous systems.
The corpus contains 1,441 sentences that con-
tain at least one relation. Instead of 5-fold cross
validation on the entire corpus by the previous sys-
tems, we split the data set into training (1,153 sen-
tences) and blind test (288 sentences) data sets and
1863
developed the system on the training data set. We
tuned the hyper-parameters using a 5-fold cross
validation on the training data set, and evaluated
the performance on the test set.
We prepared a pipeline approach as a baseline.
We first trained an entity recognition model using
the local and global features, and then trained a
relation extraction model using the local features
and global features without global ?Relation? fea-
tures in Table 5. We did not employ the global
?Relation? features in this baseline since it is com-
mon to treat relation extraction as a multi-class
classification problem.
We extracted features using the results from two
syntactic parsers Enju (Miyao and Tsujii, 2008)
and LRDEP (Sagae and Tsujii, 2007). We em-
ployed feature hashing (Weinberger et al., 2009)
and limited the feature space to 2
24
. The num-
bers of features greatly varied for categories and
targets. They also caused biased predictions that
prefer entities to relations in our preliminary ex-
periments. We thus chose to re-scale the features
as follows. We normalized local features for each
feature category and then for each target. We also
normalized global features for each feature cate-
gory, but we did not normalize them for each target
since normalization was impossible during decod-
ing. We instead scaled the global features, and the
scaling factor was tuned by using the same 5-fold
cross validation above.
We used the F1 score on relations with entities
as our primary evaluation measure and used it for
tuning parameters. In this measure, a relation with
two entities is considered correct when the offsets
and types of the entities and the type of the relation
are all correct. We also evaluated the F1 scores for
entities and relations individually on the test data
set by checking their corresponding cells. An en-
tity is correct when the offset and type are correct,
and a relation is correct when the type is correct
and the last words of two entities are correct.
3.2 Performance on Training Data Set
It is infeasible to investigate all the combinations
of the parameters, so we greedily searched for a
default parameter setting by using the evaluated
results on the training data set. The default pa-
rameter setting was the best setting except for the
beam size. We show learning curves on the train-
ing data set in Figure 6 when we varied each pa-
rameter from the default parameter setting. We
employed 5-fold cross validation. The default pa-
rameter setting used DCD-SSVM as the learning
method, entity-first, easy-first as the search order,
local and global features, and 8 as the beam size.
This section discusses how these parameters affect
the performance on the training data set and ex-
plains how the parameter setting was selected for
the test set.
Figure 6(a) compares the learning methods in-
troduced in ?2.4. DCD-SSVM and AdaGrad per-
formed slightly better than perceptron, which has
often been employed in history-based structured
learning. AROW did not show comparable per-
formance to the others. We ran 100 iterations to
find the number of iterations that saturates learn-
ing curves. The large number of iterations took
time and the performance of DCD-SSVM almost
converged after 30 iterations, so we employed 50
iterations for other evaluation on the training data
set. AdaGrad got its highest performance more
quickly than other learning methods and AROW
converged slower than other methods, so we em-
ployed 10 for AdaGrad, 90 for AROW, and 50 it-
erations for other settings on the test data set.
The performance was improved by widening
the beam as in Figure 6(b), but the improvement
was gradually diminished as the beam size in-
creased. Since the wider beam requires more train-
ing and test time, we chose 8 for the beam size.
Figure 6(c) shows the effects of joint learning
as well as features explained in ?2.5. We show the
performance of the pipeline approach (Pipeline)
introduced in ?3.1, and the performance with lo-
cal features alone (Local), local and global fea-
tures without global ?Relation? features in Table 5
(Local+global (?relation)) and all local and global
features (Local+global). We note that Pipeline
shows the learning curve of relation extraction in
the pipeline approach. Features in ?Local+global
(?relation)? are the same as the features in the
pipeline approach, and the result shows that the
joint learning approach performed slightly better
than the pipeline approach. The incorporation
of global ?Entity? and ?Entity+Relation? features
improved the performance as is common with the
existing pipeline approaches, and relation-related
features further improved the performance.
Static search orders in ?2.3.1 also affected the
performance as shown in Figure 6(d), although
search orders are not investigated in the joint en-
tity and relation extraction. Surprisingly, the gap
1864
(a) Learning methods (b) Beam sizes
(c) Features and pipeline / joint approaches (d) Static search orders
(e) Dynamic search orders
Figure 6: Learning curves of entity and relation extraction on the training data set using 5-fold cross
validation.
between the performances with the best order and
worst order was about 0.04 in an F1 score, which
is statistically significant, and the performance can
be worse than the pipeline approach in Figure 6(c).
This means improvement by joint learning can be
easily cancelled out if we do not carefully con-
sider search order. It is also surprising that the sec-
ond worst order (Figure 4(b)) is the most intuitive
?left-to-right? order, which is closest to the order
in Li and Ji (2014) among the six search orders.
Figure 6(e) shows the performance with dy-
namic search orders. Unfortunately, the easy-first
policy did not work well on this entity and relation
task, but, with the two enhancements, dynamic or-
ders performed as well as the best static order in
Figure 6(d). This shows that entities should be de-
1865
tected earlier than relations on this data set.
3.3 Performance on Test Data Set
Table 6 summarizes the performance on the test
data set. We employed the default parameter set-
ting explained in ?3.2, and compared parameters
by changing the parameters shown in the first col-
umn. We performed a statistical test using the ap-
proximate randomization method (Noreen, 1989)
on our primary measure (?Entity+Relation?). The
results are almost consistent with the results on the
training data set with a few exceptions.
Differently from the results on the training data
set, AdaGrad and AROW performed significantly
worse than perceptron and DCD-SSVM and they
performed slightly worse than the pipeline ap-
proach. This result shows that DCD-SSVM per-
forms well with inexact search and the selection of
learning methods can significantly affect the entity
and relation extraction performance.
The joint learning approach showed a signifi-
cant improvement over the pipeline approach with
relation-related global features, although the joint
learning approach alone did not show a signif-
icant improvement over the pipeline approach.
Unfortunately, no joint learning approach outper-
formed the pipeline approach in entity recognition.
This may be partly because hyper-parameters were
tuned to the primary measure. The results on the
pipeline approach also indicate that the better per-
formance on entity recognition does not necessar-
ily improve the relation extraction performance.
Search orders also affected the performance,
and the worst order (right to left, down to up) and
best order (close-first, left to right) were signifi-
cantly different. The performance of the worst or-
der was worse than that of the pipeline approach,
although the difference was not significant. These
results show that it is necessary to carefully select
the search order for the joint entity and relation
extraction task.
3.4 Comparison with Other Systems
To compare our model with the other sys-
tems (Roth and Yih, 2007; Kate and Mooney,
2010), we evaluated the performance of our model
when the entity boundaries were given. Differ-
ently from our setting in ?3.1, we used the gold
entity boundaries encoded in the BILOU scheme
and assigned entity labels to the boundaries. We
performed 5-fold cross validation on the data set
following Roth and Yih (2007) although the split
was different from theirs since their splits were not
available. We employed the default parameter set-
ting in ?3.2 for this comparison.
Table 7 shows the evaluation results. Although
we cannot directly compare the results, our model
performs better than the other models. Compared
to Table 6, Table 7 also shows that the inclusion
of entity boundary detection degrades the perfor-
mance about 0.09 in F-score.
4 Related Work
Search order in structured learning has been stud-
ied in several NLP tasks. Left-to-right and right-
to-left orderings have been often investigated in
sequential labeling tasks (Kudo and Matsumoto,
2001). Easy-first policy was firstly introduced
by Goldberg and Elhadad (2010) for dependency
parsing, and it was successfully employed in sev-
eral tasks, such as joint POS tagging and depen-
dency parsing (Ma et al., 2012) and co-reference
resolution (Stoyanov and Eisner, 2012). Search
order, however, has not been focused in relation
extraction tasks.
Named entity recognition (Florian et al., 2003;
Nadeau and Sekine, 2007) and relation extrac-
tion (Zelenko et al., 2003; Miwa et al., 2009)
have often been treated as separate tasks, but
there are some previous studies that treat enti-
ties and relations jointly in learning. Most stud-
ies built joint learning models upon individual
models for subtasks, such as Integer Linear Pro-
gramming (ILP) (Roth and Yih, 2007; Yang and
Cardie, 2013) and Card-Pyramid Parsing (Kate
and Mooney, 2010). Our approach does not re-
quire such individual models, and it also can de-
tect entity boundaries that these approaches except
for Yang and Cardie (2013) did not treat. Other
studies (Yu and Lam, 2010; Singh et al., 2013)
built global probabilistic graphical models. They
need to compute distributions over variables, but
our approach does not. Li and Ji (2014) proposed
an approach to jointly find entities and relations.
They incorporated a semi-Markov chain in repre-
senting entities and they defined two actions dur-
ing search, but our approach does not employ such
representation and actions, and thus it is more sim-
ple and flexible to investigate search orders.
5 Conclusions
In this paper, we proposed a history-based struc-
tured learning approach that jointly detects enti-
1866
Parameter Entity Relation Entity+Relation
Perceptron 0.809 / 0.809 / 0.809 0.760 / 0.547 / 0.636 0.731 / 0.527 / 0.612
?
AdaGrad 0.801 / 0.790 / 0.795 0.732 / 0.486 / 0.584 0.716 / 0.476 / 0.572
AROW 0.810 / 0.802 / 0.806 0.797 / 0.468 / 0.590 0.758 / 0.445 / 0.561
DCD-SSVM
?
0.812 / 0.802 / 0.807 0.783 / 0.524 / 0.628 0.760 / 0.509 / 0.610
?
Pipeline 0.823 / 0.814 / 0.818 0.672 / 0.542 / 0.600 0.647 / 0.522 / 0.577
Local 0.819 / 0.812 / 0.815 0.844 / 0.399 / 0.542 0.812 / 0.384 / 0.522
Local + global (?relation) 0.809 / 0.799 / 0.804 0.784 / 0.481 / 0.596 0.747 / 0.458 / 0.568
Local + global
?
0.812 / 0.802 / 0.807 0.783 / 0.524 / 0.628 0.760 / 0.509 / 0.610
?
(a) Up to down, left to right 0.824 / 0.801 / 0.813 0.821 / 0.433 / 0.567 0.787 / 0.415 / 0.543
(b) Up to down, right to left 0.828 / 0.808 / 0.818 0.850 / 0.461 / 0.597 0.822 / 0.445 / 0.578
(c) Right to left, up to down 0.823 / 0.799 / 0.811 0.826 / 0.448 / 0.581 0.789 / 0.427 / 0.554
(d) Right to left, down to up 0.811 / 0.784 / 0.797 0.774 / 0.445 / 0.565 0.739 / 0.425 / 0.540
(e) Close-first, left to right 0.821 / 0.806 / 0.813 0.807 / 0.522 / 0.634 0.780 / 0.504 / 0.612
?
(f) Close-first, right to left 0.817 / 0.801 / 0.809 0.832 / 0.491 / 0.618 0.797 / 0.471 / 0.592
Easy-first 0.811 / 0.790 / 0.801 0.862 / 0.415 / 0.560 0.831 / 0.399 / 0.540
Entity-first, easy-first
?
0.812 / 0.802 / 0.807 0.783 / 0.524 / 0.628 0.760 / 0.509 / 0.610
?
Close-first, easy-first 0.816 / 0.803 / 0.810 0.796 / 0.486 / 0.603 0.767 / 0.468 / 0.581
Table 6: Performance of entity and relation extraction on the test data set (precision / recall / F1 score).
The
?
denotes the default parameter setting in ?3.2 and
?
represents a significant improvement over the
underlined ?Pipeline? baseline (p<0.05). Labels (a)-(f) correspond to those in Figure 4.
Kate and Mooney (2010) Roth and Yih (2007) Entity-first, easy-first
Person 0.921 / 0.942 / 0.932 0.891 / 0.895 / 0.890 0.931 / 0.948 / 0.939
Location 0.908 / 0.942 / 0.924 0.897 / 0.887 / 0.891 0.922 / 0.939 / 0.930
Organization 0.905 / 0.887 / 0.895 0.895 / 0.720 / 0.792 0.903 / 0.896 / 0.899
All entities - - 0.924 / 0.924 / 0.924
Located In 0.675 / 0.567 / 0.583 0.539 / 0.557 / 0.513 0.821 / 0.549 / 0.654
Work For 0.735 / 0.683 / 0.707 0.720 / 0.423 / 0.531 0.886 / 0.642 / 0.743
OrgBased In 0.662 / 0.641 / 0.647 0.798 / 0.416 / 0.543 0.768 / 0.572 / 0.654
Live In 0.664 / 0.601 / 0.629 0.591 / 0.490 / 0.530 0.819 / 0.532 / 0.644
Kill 0.916 / 0.641 / 0.752 0.775 / 0.815 / 0.790 0.933 / 0.797 / 0.858
All relations - - 0.837 / 0.599 / 0.698
Table 7: Results of entity classification and relation extraction on the data set using the 5-fold cross
validation (precision / recall / F1 score).
ties and relations. We introduced a novel entity
and relation table that jointly represents entities
and relations, and showed how the entity and re-
lation extraction task can be mapped to a simple
table-filling problem. We also investigated search
orders and learning methods that have been fixed
in previous research. Experimental results showed
that the joint learning approach outperforms the
pipeline approach and the appropriate selection of
learning methods and search orders is crucial to
produce a high performance on this task.
As future work, we plan to apply this approach
to other relation extraction tasks and explore more
suitable search orders for relation extraction tasks.
We also plan to investigate the potential of this ta-
ble representation in other tasks such as semantic
parsing and co-reference resolution.
Acknowledgments
We thank Yoshimasa Tsuruoka and Yusuke Miyao
for valuable discussions, and the anonymous re-
viewers for their insightful comments. This work
was supported by the TTI Start-Up Research
Support Program and the JSPS Grant-in-Aid for
Young Scientists (B) [grant number 25730129].
1867
References
Ming-Wei Chang and Wen-Tau Yih. 2013. Dual coor-
dinate descent algorithms for efficient large margin
structured prediction. Transactions of the Associa-
tion for Computational Linguistics, 1:207?218.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL?04), Main Volume,
pages 111?118, Barcelona, Spain, July.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 1?8. Associ-
ation for Computational Linguistics, July.
Koby Crammer, Alex Kulesza, and Mark Dredze.
2013. Adaptive regularization of weight vectors.
Machine learning, 91(2):155?187.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121?2159.
Radu Florian, Abe Ittycheriah, Hongyan Jing, and
Tong Zhang. 2003. Named entity recognition
through classifier combination. In Walter Daele-
mans andMiles Osborne, editors, Proceedings of the
Seventh Conference on Natural Language Learning
at HLT-NAACL 2003, pages 168?171.
Yoav Freund and Robert E Schapire. 1999. Large
margin classification using the perceptron algorithm.
Machine learning, 37(3):277?296.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 742?750, Los Angeles, California,
June. Association for Computational Linguistics.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
142?151, Montr?eal, Canada, June. Association for
Computational Linguistics.
Rohit J. Kate and Raymond Mooney. 2010. Joint en-
tity and relation extraction using card-pyramid pars-
ing. In Proceedings of the Fourteenth Conference on
Computational Natural Language Learning, pages
203?212, Uppsala, Sweden, July. Association for
Computational Linguistics.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In Proceedings of the Sec-
ond Meeting of the North American Chapter of the
Association for Computational Linguistics on Lan-
guage Technologies, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Qi Li and Heng Ji. 2014. Incremental joint extrac-
tion of entity mentions and relations. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 402?412, Baltimore, Maryland, June.
Association for Computational Linguistics.
Ji Ma, Tong Xiao, Jingbo Zhu, and Feiliang Ren.
2012. Easy-first Chinese POS tagging and depen-
dency parsing. In Proceedings of COLING 2012,
pages 1731?1746, Mumbai, India, December. The
COLING 2012 Organizing Committee.
Avihai Mejer and Koby Crammer. 2010. Confidence
in structured-prediction using confidence-weighted
models. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, pages 971?981, Cambridge, MA, October. As-
sociation for Computational Linguistics.
Makoto Miwa, Rune S?tre, Yusuke Miyao, and
Jun?ichi Tsujii. 2009. A rich feature vector for
protein-protein interaction extraction from multiple
corpora. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 121?130, Singapore, August. Association
for Computational Linguistics.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35?80, March.
David Nadeau and Satoshi Sekine. 2007. A sur-
vey of named entity recognition and classification.
Lingvisticae Investigationes, 30(1):3?26.
Eric W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses : An Introduction. Wiley-
Interscience, April.
Lev Ratinov and Dan Roth. 2009. Design chal-
lenges and misconceptions in named entity recog-
nition. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), pages 147?155, Boulder, Colorado,
June. Association for Computational Linguistics.
Dan Roth and Wen-Tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Hwee Tou Ng and Ellen Riloff, ed-
itors, HLT-NAACL 2004 Workshop: Eighth Confer-
ence on Computational Natural Language Learning
(CoNLL-2004), pages 1?8, Boston, Massachusetts,
USA, May. Association for Computational Linguis-
tics.
Dan Roth and Wen-Tau Yih, 2007. Global Inference
for Entity and Relation Identification via a Linear
Programming Formulation. MIT Press.
1868
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency
parsing and domain adaptation with LR models and
parser ensembles. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
1044?1050, Prague, Czech Republic, June. Associ-
ation for Computational Linguistics.
Sameer Singh, Sebastian Riedel, Brian Martin, Jiaping
Zheng, and Andrew McCallum. 2013. Joint infer-
ence of entities, relations, and coreference. In Pro-
ceedings of the 2013 workshop on Automated knowl-
edge base construction, pages 1?6. ACM.
Veselin Stoyanov and Jason Eisner. 2012. Easy-first
coreference resolution. In Proceedings of COLING
2012, pages 2519?2534, Mumbai, India, December.
The COLING 2012 Organizing Committee.
Kilian Weinberger, Anirban Dasgupta, John Langford,
Alex Smola, and Josh Attenberg. 2009. Feature
hashing for large scale multitask learning. In Pro-
ceedings of the 26th Annual International Confer-
ence on Machine Learning, ICML ?09, pages 1113?
1120, New York, NY, USA. ACM.
Bishan Yang and Claire Cardie. 2013. Joint infer-
ence for fine-grained opinion extraction. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 1640?1649, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Xiaofeng Yu and Wai Lam. 2010. Jointly identifying
entities and extracting relations in encyclopedia text
via a graphical model approach. In Coling 2010:
Posters, pages 1399?1407, Beijing, China, August.
Coling 2010 Organizing Committee.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. The Journal of Machine Learning Re-
search, 3:1083?1106.
1869
Proceedings of the EACL 2009 Demonstrations Session, pages 61?64,
Athens, Greece, 3 April 2009. c?2009 Association for Computational Linguistics
  
Three BioNLP Tools Powered by a Biological Lexicon 
 
Abstract 
In this paper, we demonstrate three NLP 
applications of the BioLexicon, which is a 
lexical resource tailored to the biology 
domain. The applications consist of a 
dictionary-based POS tagger, a syntactic 
parser, and query processing for biomedical 
information retrieval.  Biological 
terminology is a major barrier to the 
accurate processing of literature within 
biology domain. In order to address this 
problem, we have constructed the 
BioLexicon using both manual and semi-
automatic methods. We demonstrate the 
utility of the biology-oriented lexicon 
within three separate NLP applications. 
1 Introduction 
Processing of biomedical text can frequently be 
problematic, due to the huge number of technical 
terms and idiosyncratic usages of those terms.  
Sometimes, general English words are used in 
different ways or with different meanings in 
biology literature. 
There are a number of linguistic resources 
that can be use to improve the quality of 
biological text processing.  WordNet (Fellbaum, 
1998) and the NLP Specialist Lexicon 1  are 
dictionaries commonly used within biomedical 
NLP. 
WordNet is a general English thesaurus which 
additionally covers biological terms. However, 
since WordNet is not targeted at the biology 
domain, many biological terms and derivational 
relations are missing.   
The Specialist Lexicon is a syntactic lexicon 
of biomedical and general English words, 
providing linguistic information about individual 
vocabulary items (Browne et al, 2003).  Whilst 
it contains a large number of biomedical terms, 
                                                 
1 http://SPECIALIST.nlm.hih.gov 
its focus is on medical terms. Therefore some 
biology-specific terms, e.g., molecular biology 
terms, are not the main target of the lexicon.  
In response to this, we have constructed the 
BioLexicon (Sasaki et al, 2008), a lexical 
resource tailored to the biology domain.  We will 
demonstrate three applications of the BioLexicon, 
in order to illustrate the utility of the lexicon 
within the biomedical NLP field.  
The three applications are: 
 
? BLTagger: a dictionary-based POS tagger 
based on the BioLexicon 
? Enju full parser enriched by the 
BioLexicon 
? Lexicon-based query processing for 
information retrieval 
2. Summary of the BioLexicon 
In this section, we provide a summary of the 
BioLexicon (Sasaki et al, 2008). It contains 
words belonging to four part-of-speech 
categories: verb, noun, adjective, and adverb.  
Quochi et al(2008) designed the database 
model of the BioLexicon which follows the 
Lexical Markup Framework (Francopoulo et al, 
2008).    
2.1 Entries in the Biology Lexicon 
The BioLexicon accommodates both general 
English words and terminologies. Biomedical 
terms were gathered from existing biomedical 
databases. Detailed information regarding the 
sources of biomedical terms can be found in  
(Rebholz-Schuhmann et al, 2008). The lexicon 
entries consist of the following: 
 
(1) Terminological verbs: 759 base forms (4,556 
inflections) of terminological verbs with 
automatically extracted verb 
subcategorization frames 
 
Yutaka Sasaki 1   Paul Thompson1   John McNaught 1, 2   Sophia Ananiadou1, 2 
 
1 School of Computer Science, University of Manchester 
2  National Centre for Text Mining 
MIB, 131 Princess Street, Manchester, M1 7DN, United Kingdom 
{Yutaka.Sasaki,Paul.Thompson,John.McNaught,Sophia.Ananiadou}@manchester.ac.uk 
61
(2)Terminological adjectives: 1,258 
terminological adjectives.   
(3) Terminological adverbs: 130 terminological 
adverbs. 
(4) Nominalized verbs: 1,771  nominalized verbs.   
(5) Biomedical terms: Currently, the BioLexicon 
contains biomedical terms in the categories of 
cell (842 entries, 1,400 variants), chemicals 
(19,637 entries, 106,302 variants), enzymes 
(4,016 entries, 11,674 variants), diseases 
(19,457 entries, 33,161 variants), genes and 
proteins (1,640,608 entries, 3,048,920 
variants), gene ontology concepts (25,219 
entries, 81,642 variants), molecular role 
concepts (8,850 entries, 60,408 variants), 
operons (2,672 entries, 3,145 variants), 
protein complexes (2,104 entries, 2,647 
variants), protein domains (16,940 entries, 
33,880 variants), Sequence ontology concepts 
(1,431 entries, 2,326 variants), species 
(482,992 entries, 669,481 variants), and 
transcription factors (160 entries, 795 
variants).   
In addition to the existing gene/protein names, 
70,105 variants of gene/protein names have been 
newly extracted from 15 million MEDLINE 
abstracts. (Sasaki et al, 2008) 
2.2. Comparison to existing lexicons 
This section focuses on the words and 
derivational relations of words that are covered 
by our BioLexicon but not by comparable 
existing resources. Figures 1 and 2 show the 
percentage of the terminological words and 
derivational relations (such as the word 
retroregulate and the derivational relation 
retroregulate ? retroregulation) in our lexicon 
that are also found in WorNet and the Specialist 
Lexicion. 
Since WordNet is not targeted at the biology 
domain, many biological terms and derivational 
relations are not included.   
Because the Specialist Lexicon is a 
biomedical lexicon and the target is broader than 
our lexicon, some biology-oriented words and 
relations are missing.  For example, the 
Specialist Lexicon includes the term retro-
regulator but not retro-regulate. This means that 
derivational relations of retro-regulate are not 
covered by the Specialist Lexicon.  
3. Application 1: BLTagger 
Dictionary-based POS tagging is advantageous 
when a sentence contains technical terms that 
conflict with general English words. If the POS 
tags are decided without considering possible 
occurrences of biomedical terms, then POS 
errors could arise.  
For example, in the protein name ?met proto-
oncogene precursor?, met might be incorrectly 
recognized as a verb by a non dictionary-based 
tagger.   
Input sentence: 
?IL-2-mediated activation of ??
IL/NP
IL-2/NN-BIOMED
-/-
2/CD
mediated/VVD
IL-2-mediated/UNKNOWN
IL/NP
2/CD
IL-2/NN-BIOMED
??????????
mediated/VVD
mediate/VVP
mediate/VV
of/IN
mediated/VVN
-/-
-/-
mediated/VVNdictionary-based tagging of/IN
Fig. 3 BLTagger example 
coverage
0
20
40
60
80
100
verb
noun
adjective
adverb
nom
inalization
adjetivial 
adverbal
Terminologies Derivational relations
 
Fig. 1  Comparison with WordNet 
coverage
0
20
40
60
80
100
verb
noun
adjective
adverb
nom
inalization
adjetivial 
adverbal
Terminologies Derivational relations
 
 
Fig. 2  Comparison with Specialist Lexicon 
62
In the dictionary, biomedical terms are given 
POS tag "NN-BIOMED". Given a sentence, the 
dictionary-based POS tagger works as follows.  
 
? Find all word sequences that match the 
lexical entries, and create a token graph (i.e., 
trellis) according to the word order.  
? Estimate the score of every path using the 
weights of the nodes and edges, through 
training using Conditional Random Fields.  
? Select the best path. 
 
Figure 3 shows an example of our dictionary-
based POS tagger BLTagger. 
Suppose that the input is ?IL-2-mediated 
activation of?. A trellis is created based on the 
lexical entries in the dictionary. The selection 
criteria for the best path are determined by the 
CRF tagging model trained on the Genia corpus 
(Kim et al, 2003). In this example,  
 
IL-2/NN-BIOMED -/- mediated/VVN 
activation/NN of/IN 
 
is selected as the best path.  
Following Kudo et al (2004), we adapted the 
core engine of the CRF-based morphological 
analyzer, MeCab2, to our POS tagging task.  
The features used were: 
 
? POS 
? BIOMED 
? POS-BIOMED 
? bigram of adjacent POS 
? bigram of adjacent BIOMED 
? bigram of adjacent POS-BIOMED 
 
During the construction of the trellis, white 
space is considered as the delimiter unless 
otherwise stated within dictionary entries. This 
means that unknown tokens are character 
sequences without spaces. 
As the BioLexicon associates biomedical 
semantic IDs with terms, the BLTagger attaches 
semantic IDs to the tokenizing/tagging results. 
4. Application 2: Enju full parser with the 
BioLexicon 
Enju (Miyao, et al, 2003) is an HPSG parser, 
which is tuned to the biomedical domain.  
Sentences are parsed based on the output of the 
                                                 
2 http://sourceforge.net/project/showfiles.php?group 
id=177856/ 
Stepp POS tagger, which is also tuned to the 
biomedical domain. 
To further tune Enju to the biology domain, 
(especially molecular biology), we have 
modified Enju to parse sentences based on the 
output of the BLTagger. 
As the BioLexicon contains many multi-word 
biological terms, the modified version of Enju 
parses token sequences in which some of the 
tokens are multi-word expressions.  This is 
effective when very long technical terms (e.g., 
more than 20 words) are present in a sentence. 
To use the dictionary-based tagging for 
parsing, unknown words should be avoided as 
much as possible. In order to address this issue, 
we added entries in WordNet and the Specialist 
Lexicion to the dictionary of BLTagger. 
The enhancement in the performance of Enju 
based on these changes is still under evaluation. 
However, we demonstrate a functional, modified 
version of Enju. 
5. Application 3: Query processing for IR 
It is sometimes the case that queries for 
biomedical IR systems contain long technical 
terms that should be handled as single multi-
word expressions.  
We have applied BLTagger to the TREC 2007 
Genomics Track data (Hersh et al, 2007).  The 
goal of the TREC Genomics Track 2007 was to 
generate a ranked list of passages for 36 queries 
that relate to biological events and processes.    
Firstly, we processed the documents with a 
conventional tokenizer and standard stop-word 
remover, and then created an index containing 
the words in the documents. Queries are 
processed with the BLTagger and multi-word 
expressions are used as phrase queries.  Passages 
are ranked with Okapi BM25 (Robertson et al, 
1995). 
Table 1 shows the preliminary Mean Average 
Precision (MAP) scores of applying the 
BLTagger to the TREC data set.   
By adding biology multi-word expressions 
identified by the BLTagger to query terms (row 
(a)), we were able to obtain a slightly better 
Passage2 score. As the BLTagger outputs 
semantic IDs which are defined in the 
BioLexicon, we tried to use these semantic IDs 
for query expansion (rows (b) and (d)).  However, 
the MAP scores degraded. 
63
6. Conclusions 
We have demonstrated three applications of the 
BioLexicon, which is a resource comprising 
linguistic information, targeted for use within 
bio-text mining applications.   
We have described the following three 
applications that will be useful for processing of 
biological literature. 
 
? BLTagger: dictionary-based POS tagger 
based on the BioLexicon 
? Enju full parser enriched by the 
BioLexicon 
? Lexicon-based query processing for 
information retrieval 
 
Our future work will include further intrinsic 
and extrinsic evaluations of the BioLexicon in 
NLP, including its  application to information 
extraction tasks in the biology domain. The 
BioLexicon is available for non-commercial 
purposes under the Creative Commons license. 
Acknowledgements 
This research has been supported by the EC IST 
project FP6-028099 (BOOTStrep), whose 
Manchester team is hosted by the 
JISC/BBSRC/EPSRC sponsored National Centre 
for Text Mining.   
References 
Browne, A.C., G. Divita, A.R. Aronson, and A.T. 
McCray. 2003. UMLS Language and Vocabulary 
Tools. In Proc. of AMIA Annual Symposium 2003, 
p.798. 
Dietrich Rebholz-Schuhmann, Piotr Pezik, Vivian Lee, 
Jung-Jae Kim, Riccardo del Gratta, Yutaka Sasaki, 
Jock McNaught, Simonetta Montemagni, Monica 
Monachini, Nicoletta Calzolari, Sophia Ananiadou, 
BioLexicon: Towards a Reference Terminological 
Resource in the Biomedical Domain, the 16th 
Annual International Conference on Intelligent 
Systems for Molecular Biology (ISMB-2008) 
(Poster), Toronto, Canada, 2008. 
(http://www.ebi.ac.uk/Rebholz-srv/BioLexicon/ 
BioLexicon_Poster_EBI_UoM_ILC.pdf) 
Fellbaum, C., editor. 1998. WordNet: An Electronic 
Lexical Database.  MIT Press, Cambridge, MA.. 
Francopoulo, G., M. George, N. Calzolari, M. 
Monachini, N. Bel, M. Pet, and C. Soria. 2006. 
Lexical Markup Framework (LMF). In Proc. of  
LREC 2006, Genova, Italy. 
Hersh, W., Aaron Cohen, Lynn Ruslen, and Phoebe 
Roberts, TREC 2007 Genomics Track Overview, 
TREC-2007, 2007. 
Kim, J-D., T. Ohta, Y. Tateisi, and J. Tsujii. 2003.  
GENIA Corpus - Semantically Annotated Corpus 
for Bio-Text Mining. Bioinformatics, 19:i180-i182. 
Kudo T., Yamamoto K., Matsumoto Y., Applying 
Conditional Random Fields to Japanese Mor- 
phological Analysis. In Proc. of Empirical 
Methods in Natural Language Processing 
(EMNLP-04), pp. 230?237, 2004. 
Lafferty, J., A. McCallum, and F. Pereira. 2001. 
Conditional Random Fields: Probabilistic Models 
for Segmenting and Labelling Sequence Data. In 
Proc. of the Eighteenth International Conference 
on Machine Learning (ICML-2001), pages 282-289.  
Miyao, Y. and J. Tsujii, 2003. Probabilistic modeling 
of argument structures including non-local 
dependencies. In Proc. of the Conference on 
Recent Advances in Natural Language Processing 
(RANLP 2003), pages 285-291. 
Quochi, V., Monachini, M., Del Gratta, R., Calzolari, 
N., A lexicon for biology and bioinformatics: the 
BOOTStrep experience. In Proc. of LREC 2008, 
Marrakech, 2008. 
Robertson, S.E., Walker S., Jones, S., Hancock-
Beaulieu M.M., and Gatford, M., 1995. Okapi at 
TREC-3. In Proc of Overview of the Third Text 
REtrieval Conference (TREC-3), pp. 109?126. 
Yutaka Sasaki, Simonetta Montemagni, Piotr Pezik, 
Dietrich Rebholz-Schuhmann, John McNaught, 
and Sophia Ananiadou, BioLexicon: A Lexical 
Resource for the Biology Domain, In Proc. of the 
Third International Symposium on Semantic 
Mining in Biomedicine (SMBM 2008), 2008. 
Table 1 Preliminary MAP scores for TREC Genomics Track 2007 data 
 
Query expansion method Passage2 MAP Aspect MAP Document MAP 
(a) BioLexicon terms 0.0702 0.1726 0.2158 
(b) BioLexicon terms 
 + semantic IDs 
0.0696 0.1673 0.2148 
(c) no query expansion  (baseline) 0.0683 0.1726 0.2183 
(d) semantic IDs 0.0677 0.1670 0.2177 
 
64
