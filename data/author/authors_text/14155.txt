Proceedings of the ACL 2007 Student Research Workshop, pages 61?66,
Prague, June 2007. c?2007 Association for Computational Linguistics
Identifying Linguistic Structure in a Quantitative
Analysis of Dialect Pronunciation
Jelena Prokic?
Alfa-Informatica
University of Groningen
The Netherlands
j.prokic@rug.nl
Abstract
The aim of this paper is to present a new
method for identifying linguistic structure in
the aggregate analysis of the language vari-
ation. The method consists of extracting the
most frequent sound correspondences from
the aligned transcriptions of words. Based
on the extracted correspondences every site
is compared to all other sites, and a corre-
spondence index is calculated for each site.
This method enables us to identify sound al-
ternations responsible for dialect divisions
and to measure the extent to which each al-
ternation is responsible for the divisions ob-
tained by the aggregate analysis.
1 Introduction
Computational dialectometry is a multidisciplinary
field that uses quantitative methods in order to mea-
sure linguistic differences between the dialects. The
distances between the dialects are measured at dif-
ferent levels (phonetic, lexical, syntactic) by aggre-
gating over entire data sets. The aggregate analyses
do not expose the underlying linguistic structure, i.e.
the specific linguistic elements that contributed to
the differences between the dialects. This is very of-
ten seen as one of the main drawbacks of the dialec-
tometry techniques and dialectometry itself. Two at-
tempts to overcome this drawback are presented in
Nerbonne (2005) and Nerbonne (2006). In both of
these papers the identification of linguistic structure
in the aggregate analysis is based on the analysis of
the pronunciation of the vowels found in the data set.
In work presented in this paper the identification
of linguistic structure in the aggregate analysis is
based on the automatic extraction of regular sound
correspondences which are further quantified in or-
der to characterize each site based on the frequency
of a certain sound extracted from the pool of the
site?s pronunciation. The results show that identifi-
cation of regular sound correspondences can be suc-
cessfully applied to the task of identifying linguistic
structure in the aggregate analysis of dialects based
on word pronunciations.
The rest of the paper is structured as follows. Sec-
tion 2 gives an overview of the work previously done
in the areas covered in this paper. In Section 3 more
information on the aggregate analysis of Bulgarian
dialects is given. Work done on the identification of
regular sound correspondences and their quantifica-
tion is presented in Section 4. Conclusion and sug-
gestions for future work are given in Section 5.
2 Previous Work
The work presented in this paper can be divided in
two parts: the aggregate analysis of Bulgarian di-
alects on one hand, and the identification of linguis-
tic structure in the aggregate analysis on the other. In
this section the work closely related to the one pre-
sented in this paper will be described in more detail.
2.1 Aggregate Analysis of Bulgarian
Dialectometry produces aggregate analyses of the
dialect variations and has been done for different
languages. For several languages aggregate analyses
have been successfully developed which distinguish
various dialect areas within the language area. The
61
most closely related to the work presented in this pa-
per is quantitative analysis of Bulgarian dialect pro-
nunciation reported in Osenova et al (2007).
In work done by Osenova et al (2007) aggregate
analysis of pronunciation differences for Bulgarian
was done on the data set that comprised 36 word
pronunciations from 490 sites. The data was digital-
ized from the four-volume set of Atlases of Bulgar-
ian Dialects (Stojkov and Bernstein, 1964; Stojkov,
1966; Stojkov et al, 1974; Stojkov et al, 1981).
Pronunciations of the same words were aligned and
compared using L04.1 Results were analyzed using
cluster analysis, composite clustering, and multidi-
mensional scaling. The analyses showed that results
obtained using aggregate analysis of word pronunci-
ations mostly conform with the traditional phonetic
classification of Bulgarian dialects as presented in
Stojkov (2002).
2.2 Extraction of Linguistic Structure
Although techniques in dialectometry have shown
to be successful in the analysis of the dialect vari-
ation, all of them aggregate over the entire available
data, failing to extract linguistic structure from the
aggregate analysis. Two attempts to overcome this
withdraw are presented in Nerbonne (2005) and Ner-
bonne (2006).
Nerbonne (2005) suggests aggregating over a lin-
guistically interesting subset of the data. Nerbonne
compares aggregate analysis restricted to vowel dif-
ferences to those using the complete data set. Re-
sults have shown that vowels are probably respon-
sible for a great deal of aggregate differences, since
there was high correlation between differences ob-
tained only by using vowels and by using complete
transcriptions (r = 0.936). Two ways of aggregate
analysis also resulted in comparable maps. How-
ever, no other subset has been analyzed in this pa-
per, making it impossible to conclude how success-
ful other subsets would be if similar analysis was
done.
The second paper (Nerbonne, 2006) applies fac-
tor analysis to the result of the dialectometric analy-
sis in order to extract linguistic structure. The study
focuses on the pronunciation of vowels found in the
1L04 is a freely available software used for di-
alectometry and cartography. It can be found at
http://www.let.rug.nl/kleiweg/L04/
data. Out of 1132 different vowels found in the data
204 vowel positions are investigated, where a vowel
position is, e.g., the first vowel in the word ?Wash-
ington? or the second vowel in the word ?thirty?.
Factor analysis has shown that 3 factors are most im-
portant, explaining 35% of the total amount of vari-
ance. The main drawback of applying this technique
in dialectometry is that it is not directly related to the
aggregate analysis, but is rather an independent step.
Just as in Nerbonne (2005), only vowels were exam-
ined.
2.3 Sound Correspondences
In his PhD thesis Kondrak (Kondrak, 2002) presents
techniques and algorithms for the reconstruction of
the proto-languages from cognates. In Chapter 6
the focus is on the automatic determination of sound
correspondences in bilingual word lists and the iden-
tification of cognates on the basis of extracted cor-
respondences. Kondrak (2002) adopted Melamed?s
parameter estimation models (Melamed, 2000) used
in statistical machine translation and successfully
applied them to determination of sound correspon-
dences, i.e. diachronic phonology. Kondrak in-
duced a model of sound correspondence in bilin-
gual word lists, where phoneme pairs with the high-
est scores represent the most likely correspondences.
The more regular sound correspondences the two
words share, the more likely it is that they are cog-
nates and not borrowings.
In this paper the identification of sound corre-
spondences will be used to extract linguistic ele-
ments (i.e. phones) responsible for the dialect di-
visions. The method presented in this study differs
greatly from Kondrak?s in that he uses regular sound
correspondences to directly compare two words and
determine if they are cognates. In this study ex-
tracted sound correspondences are further quantified
in order to characterize each site in the data set by
assigning it a unique index. This is the first time that
this method has been applied in dialectometry.
3 Aggregate Analysis
In the first phase of this project L04 toolkit was used
in order to make an aggregate analysis of Bulgarian
dialects. In this section more information on the data
set used in the project, as well as on the process of
the aggregate analysis will be given.
62
3.1 Data Set
The data used in this research, as well as the research
itself, are part of the project Buldialect?Measuring
linguistic unity and diversity in Europe.2 The data
set consisted of pronunciations of 117 words col-
lected from 84 sites equally distributed all over Bul-
garia. It comprises nouns, pronouns, adjectives,
verbs, adverbs and prepositions which can be found
in different word forms (singular and plural, 1st,
2nd, and 3rd person verb forms, etc.).
3.2 Measuring of Dialect Distances
Aggregate analysis of Bulgarian dialects done in this
project was based on the phonetic distances between
the various pronunciations of a set of words. No
morphological, lexical, or syntactic variation was
taken into account.
First, all word pronunciations were aligned based
on the following principles: a) a vowel can match
only with the vowel b) a consonant can match only
with the consonant c) [j] can match both vowels and
consonants.
An example of the alignment of two pronuncia-
tions is given in Figure 1.3
g l "A v A
g l @ v "?
???????????-
1 1
Figure 1: Alignment of word pronunciation pair
The alignments were carried out using the Leven-
sthein algorithm,4 which also results in the calcu-
lation of a distance between each pair of words.
The distance is the smallest number of insertions,
deletions, and substitutions needed to transform one
string to the other. In this work all three operations
were assigned the same value?1. All words are rep-
resented as series of phones which are not further
defined. The result of comparing two phones can be
1 or 0; they either match or they don?t. In Figure 1
2The project is sponsored by Volkswagen Stiftung.
More information can be found at http://www.sfs.uni-
tuebingen.de/dialectometry
3For technical reasons primary stress is indicated by a high
vertical line before the syllable?s vowel.
4Detailed explanation of Levensthein algorithm can be
found in Heeringa (2004).
the cheapest way to transform one pronunciation to
the other would be by making two substitutions: ["A]
should be replaced by [@], and [A] by ["?], meaning
that the distance between these two pronunciations
is 2. The distance between each pair of pronunci-
ations was further normalized by the length of the
longest alignment that gives the minimal cost.5 Af-
ter normalization, we get the final distance between
two strings, which is 0.4 (2/5) in the example shown
in Figure 1. If there are more plausible alignments
with the minimal cost, the longest is preferred. Word
pronunciations collected from all sites are aligned
and compared in this fashion, allowing us to cal-
culate the distance between each pair of sites. The
difference between two locations is the mean of all
differences between words collected from these two
sites.
Figure 2: Classification map
The results were analyzed using clustering (Fig-
ure 2) and multidimensional scaling (Figure 3).
Clustering is a common technique in a statistical
data analysis based on a partition of a set of ob-
jects into groups or clusters (Manning and Sch?tze,
1999). Multidimensional scaling is data analysis
technique that provides a spatial display of the data
revealing relationships between the instances in the
data set (Davison, 1992). On both the maps the
biggest division is between East and West. The bor-
der between these two areas goes around Pleven and
Teteven, and it is the border of ?yat? realization as
presented in the traditional dialectological atlases
(Stojkov, 2002). The most incoherent area is the
5An interesting discussion on the normalization by length
can be found in Heeringa et al (2006). In this paper the authors
report that contrary to results from previous work (Heeringa,
2004) non-normalized string distance measures are superior to
normalized ones.
63
area of Rodopi mountain, and the dialects present
in this area show the greatest similarity with the di-
alects found in the Southeastern part around Malko
Tyrnovo. On the map in Figure 3 it is also possible
to distinguish the area around Golica and Kozichino
on the East, which conforms to the maps found in
Stojkov (2002). Results of the aggregate analysis
conform both to the traditional maps presented in
Stojkov (2002), and to the work reported in Osen-
ova et al (2007).
Figure 3: MDS map
4 Regular Sound Correspondences
The same data used for the aggregate analysis was
reused to extract sound correspondences and to iden-
tify underlying linguistic structure in the aggregate
analysis. The method and the obtained results will
be presented in more detail.
4.1 Method
From the aligned pairs of word pronunciations all
non-matching segments were extracted and sorted
according to their frequency. In the entire data set
there were 683 different pairs of sound correspon-
dences that appeared 955199 times.
e i 36565 j - 21361
@ ? 26398 A @ 20515
o u 26108 e "e 19934
"6 "e 23689 r rj 19787
v - 22100 "? - 18867
Table 1: Most frequent sound correspondences
The most frequent correspondences were taken to
be the most important sound alternations responsi-
ble for dialect variation. The method was tested on
the 10 most frequent correspondences which were
responsible for the 25% of sound alternations in the
whole data set.
In order to determine which of the extracted sound
correspondences is responsible for which of the di-
visions present in the aggregate analysis, each site
was compared to all other sites with respect to the
10 most frequent sound correspondences. For each
pair of sites all sound correspondences were ex-
tracted, including both matching and non-matching
segments. For further analysis it was important to
distinguish which sound comes from which place.
For each pair of the sound correspondences from
Table 1 a correspondence index is calculated for
each site using the following formula:
1
n? 1
n?
i=1,j 6=i
si??s?j (1)
where n represents the number of sites, and si??s?j
the comparison of each two sites (i, j) with respect
to the sound correspondence s/s?. si??s?j is calcu-
lated applying the following formula:
|si, s?j |
|si, s?j |+ |si, sj |
(2)
In the above formula si and s?j stand for the pair of
sounds involved in one of the most frequent sound
correspondences from Table 1. |si, s?j | represents the
number of times s is seen in the word pronunciations
collected at site i, aligned with the s? in word pro-
nunciations collected at site j. |si, sj | is the number
of times s stayed unchanged. For each pair of sound
correspondences a correspondence index was calcu-
lated for the s, s? correspondence, as well as for the
s?, s correspondence. For example, for the pair of
correspondences [e] and [i], the relation of [e] cor-
responding to [i] is separated from the relation of [i]
corresponding to [e].6
For example, the indices for the sites Aldomirovci
and Borisovo with respect to the sound correspon-
dence [e]-[i] were calculated in the following way.
In the file with the sound correspondences extracted
from all aligned word pronunciations collected at
6It would also be possible to modify this formula and calcu-
late the ratio of s to s corresponding to any other sound. In this
case the result would be a very small number of sites with the
very high correspondence index.
64
these two sites, the algorithm searches for pairs rep-
resented in Table 2:
Aldomirovci e i e
Borisovo i e e
no. of correspondences 24 0 3
Table 2: How often [e] corresponds to [i] and [e]
For each of the sites the indices were calculated us-
ing the above formula. The index for site i (Al-
domirovci) was:
|e, i|
|e, i|+ |e, e| =
24
24 + 3 = 0.89 (3)
The index for site j (Borisovo) was calculated in the
similar fashion from the Table 2:
|e, i|
|e, i|+ |e, e| =
0
0 + 3 = 0.00 (4)
Each of these two sites was compared to all other
sites with respect to the [e]-[i] correspondence re-
sulting in 83 indices for each site. The general cor-
respondence index for each site represents the mean
of all 83 indices. For the site i (Aldomirovci) gen-
eral index was 0.40, and for the site j (Borisovo)
0.21. Sites with the higher values of the general cor-
respondence index represent the sites where sound
[e] tends to be present, with respect to the [e]-[i]
correspondence (see Figure 4). In the same fash-
ion general correspondence indices were calculated
for every site with respect to each pair of the most
frequent correspondences (Table 1).
4.2 Results
The methods described in the previous section were
applied to all phone pairs from the Table 1, resulting
in 17 different divisions of the sites.7
Data obtained by the analysis of sound correspon-
dences, i.e. indices of correspondences for sites was
used to draw maps in which every site is set off by
Voronoi tessellation from all other sites, and shaded
based on the value of the general correspondence in-
dex. Light polygons on the map represent areas with
7For three pairs where one sound doesn?t have a correspond-
ing one (when there was an insertion or deletion) it is not pos-
sible to calculate an index. Formulas for comparing two sites
from the previous section would always give value 1 for the in-
dex.
the higher values of the correspondence index, i.e.
areas where the first sound in the examined alterna-
tion tends to be present. This technique enables us
to visualize the geographical distribution of the ex-
amined sounds. For example, map in Figure 4 rep-
Figure 4: Distribution of [e] sound
resents geographical distribution of sound [e] with
respect to the [e]-[i] correspondence, while map in
Figure 5 reveals the presence of the sound [i] with
respect to the [i]-[e] correspondence.
Figure 5: Distribution of [i] sound
In order to compare the dialect divisions obtained
by the aggregate analysis, and those based on the
general correspondence index for a certain phone
pair, correlation coefficient was calculated for these
2 sets of distances. The results are shown in Ta-
ble 3. Dialect divisions based on the [r]-[rj] and [i]-
[e] alternations have the highest correlation with the
distances obtained by the aggregate analysis. The
square of the Pearson correlation coefficient pre-
sented in column 3 enables us to see that 39.0% and
30.7% of the variance in the aggregate analysis can
be explained by these two sound alternations.
65
Correspondence Correlation r2x100(%)
[e]-[i] 0.19 3.7
[i]-[e] 0.55 30.7
[@]-[?] 0.26 6.7
[?]-[@] 0.23 5.3
[o]-[u] 0.49 24.4
[u]-[o] 0.43 18.9
["A]-["e] 0.49 24.3
["e]-["A] 0.38 14.2
[v]- - 0.14 2.0
[j]- - 0.20 4.0
[A]-[@] 0.51 26.5
[@]-[A] 0.26 7.0
[e]-["e] 0.18 3.2
["e]-[e] 0.23 5.2
[r]-[rj] 0.62 39.0
[rj]-[r] 0.53 28.1
["?]- - 0.17 2.9
Table 3: Correlation coefficient
5 Conclusion and Future Work
The dialect division of Bulgaria based on the aggre-
gate analysis presented in this paper conforms both
to traditional maps (Stojkov, 2002) and to the work
reported in Osenova et al (2007), suggesting that
the novel data used in this project is representative.
The method of quantification of regular sound corre-
spondences described in the second part of the paper
was successful in the identification of the underlying
linguistic structure of the dialect divisions. It is an
important step towards more general investigation of
the role of the regular sound changes in the language
dialect variation. The main drawback of the method
is that it analyzes one sound alternation at the time,
while in the real data it is often the case that one
sound corresponds to several other sounds and that
sound correspondences involve series of segments.
In future work some kind of a feature represen-
tation of segments should be included in the anal-
ysis in order to deal with the drawbacks noted. It
would also be very important to analyze the context
in which examined sounds appear, since we can talk
about regular sound changes only with respect to the
certain phonological environments.
References
Mark L. Davison. 1992. Multidimensional scaling. Mel-
bourne, Fl. CA: Krieger Publishing Company.
Wilbert Heeringa, Peter Kleiweg, Charlotte Gooskens,
and John Nerbonne. 2006. Evaluation of String
Distance Algorithms for Dialectology. In John Ner-
bonne and Erhard Hinrichs, editors, Linguistic Dis-
tances. Workshop at the joint conference of Interna-
tional Committee on Computational Linguistics and
the Association for Computational Linguistics, Syd-
ney.
Wilbert Heeringa. 2004. Measuring Dialect Pronunci-
ation Differences using Levensthein Distance. PhD
Thesis, University of Groningen.
Grzegorz Kondrak. 2002. Algorithms for Language Re-
construction. PhD Thesis, University of Toronto.
Chris Manning and Hinrich Sch?tze. 1999. Founda-
tions of Statistical Natural Language Processing. MIT
Press. Cambridge, MA.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249.
John Nerbonne. 2005. Various Variation Aggregates in
the LAMSAS South. In Catherine Davis and Michael
Picone, editors, Language Variety in the South III. Uni-
versity of Alabama Press, Tuscaloosa.
John Nerbonne. 2006. Identifying Linguistic Structure
in Aggregate Comparison. Literary and Linguistic
Computing, 21(4).
Petya Osenova, Wilbert Heeringa, and John Nerbonne.
2007. A Quantitive Analysis of Bulgarian Dialect
Pronunciation. Accepted to appear in Zeitschrift f?r
slavische Philologie.
Stojko Stojkov and Samuil B. Bernstein. 1964. Atlas of
Bulgarian Dialects: Southeastern Bulgaria. Publish-
ing House of Bulgarian Academy of Science, volume
I, Sofia, Bulgaria.
Stojko Stojkov, Kiril Mirchev, Ivan Kochev, and Mak-
sim Mladenov. 1974. Atlas of Bulgarian Dialects:
Southwestern Bulgaria. Publishing House of Bulgar-
ian Academy of Science, volume III, Sofia, Bulgaria.
Stojko Stojkov, Ivan Kochev, and Maksim Mladenov.
1981. Atlas of Bulgarian Dialects: Northwestern Bul-
garia. Publishing House of Bulgarian Academy of
Science, volume IV, Sofia, Bulgaria.
Stojko Stojkov. 1966. Atlas of Bulgarian Dialects:
Northeastern Bulgaria. Publishing House of Bulgar-
ian Academy of Science, volume II, Sofia, Bulgaria.
Stojko Stojkov. 2002. Bulgarska dialektologiya. Sofia,
4th ed.
66
Proceedings of the EACL 2009 Workshop on Language Technology and Resources for Cultural Heritage,
Social Sciences, Humanities, and Education ?LaTeCH ? SHELT&R 2009, pages 18?25,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
Multiple sequence alignments in linguistics
Jelena Prokic?
University of Groningen
The Netherlands
j.prokic@rug.nl
Martijn Wieling
University of Groningen
The Netherlands
m.b.wieling@rug.nl
John Nerbonne
University of Groningen
The Netherlands
j.nerbonne@rug.nl
Abstract
In this study we apply and evaluate an
iterative pairwise alignment program for
producing multiple sequence alignments,
ALPHAMALIG (Alonso et al, 2004), us-
ing as material the phonetic transcriptions
of words used in Bulgarian dialectological
research. To evaluate the quality of the
multiple alignment, we propose two new
methods based on comparing each column
in the obtained alignments with the cor-
responding column in a set of gold stan-
dard alignments. Our results show that the
alignments produced by ALPHAMALIG
correspond well with the gold standard
alignments, making this algorithm suitable
for the automatic generation of multiple
string alignments. Multiple string align-
ment is particularly interesting for histor-
ical reconstruction based on sound corre-
spondences.
1 Introduction
Our cultural heritage is studied today not only in
museums, libraries, archives and their digital por-
tals, but also through the genetic and cultural lin-
eaments of living populations. Linguists, popula-
tion geneticists, archaeologists, and physical and
cultural anthropologists are all active in research-
ing cultural heritage on the basis of material that
may or may not be part of official cultural heritage
archives. The common task is that of understand-
ing the histories of the peoples of the world, espe-
cially their migrations and contacts. To research
and understand linguistic cultural heritage we re-
quire instruments which are sensitive to its signals,
and, in particular sensitive to signals of common
provenance. The present paper focuses on pronun-
ciation habits which have been recognized to bear
signals of common provenance for over two hun-
dred years (since the work of Sir William Jones).
We present work in a research line which seeks to
submit pronunciation data to phylogenetic analy-
sis (Gray and Atkinson, 2003) and which requires
an alignment of the (phonological) segments of
cognate words. We focus in this paper on evalu-
ating the quality of multi-aligned pronunciations.
In bioinformatics, sequence alignment is a way
of arranging DNA, RNA or protein sequences in
order to identify regions of similarity and deter-
mine evolutionary, functional or structural simi-
larity between the sequences. There are two main
types of string alignment: pairwise and multiple
string alignment. Pairwise string alignment meth-
ods compare two strings at a time and cannot di-
rectly be used to obtain multiple string alignment
methods (Gusfield, 1997, 343-344). In multiple
string alignment all strings are aligned and com-
pared at the same time, making it a good technique
for discovering patterns, especially those that are
weakly preserved and cannot be detected easily
from sets of pairwise alignments. Multiple string
comparison is considered to be the holy grail of
molecular biology (Gusfield, 1997, 332):
It is the most critical cutting-edge tool for ex-
tracting and representing biologically important,
yet faint or widely dispersed, commonalities
from a set of strings.
Multiple string comparison is not new in lin-
guistic research. In the late 19th century the
Neogrammarians proposed the hypothesis of the
regularity of sound change. According to THE
NEOGRAMMARIAN HYPOTHESIS sound change
occurs regularly and uniformly whenever the ap-
propriate phonetic environment is encountered
(Campbell, 2004). Ever since, the understand-
ing of sound change has played a major role in
the comparative method that is itself based on the
simultaneous comparison of different languages,
i.e. lists of cognate terms from the related lan-
guages. The correct analysis of sound changes
18
requires the simultaneous examination of corre-
sponding sounds in order to compare hypotheses
about their evolution. Alignment identifies which
sounds correspond. Historical linguists align the
sequences manually, while we seek to automate
this process.
In recent years there has been a strong fo-
cus in historical linguistics on the introduction
of quantitative methods in order to develop tools
for the comparison and classification of lan-
guages. For example, in his PhD thesis, Kondrak
(2002) presents algorithms for the reconstruction
of proto-languages from cognates. Warnow et al
(2006) applied methods taken from phylogenet-
ics on Indo-European phonetic data in order to
model language evolution. Heeringa and Joseph
(2007) applied the Levensthein algorithm to the
Dutch pronunciation data taken from Reeks Ned-
erlandse Dialectatlassen and tried to reconstruct a
?proto-language? of Dutch dialects using the pair-
wise alignments.
Studies in historical linguistics and dialectome-
try where string comparison is used as a basis for
calculating the distances between language vari-
eties will profit from tools to multi-align strings
automatically and to calculate the distances be-
tween them. Good multiple alignment is of ben-
efit to all those methods in diachronic linguistics
such as the comparative reconstruction method
or the so-called CHARACTER-BASED METHODS
taken from phylogenetics, which have also been
successfully applied in linguistics (Gray and Jor-
dan, 2000; Gray and Atkinson, 2003; Atkinson
et al, 2005; Warnow et al, 2006). The multi-
alignment systems can help historical linguistics
by reducing the human labor needed to detect the
regular sound correspondences and cognate pairs
of words. They also systematize the linguistic
knowledge in intuitive alignments, and provide a
basis for the application of the quantitative meth-
ods that lead to a better understanding of language
variation and language change.
In this study we apply an iterative pairwise
alignment program for linguistics, ALPHAMA-
LIG, on phonetic transcriptions of words used in
dialectological research. We automatically multi-
align all transcriptions and compare these gener-
ated alignments with manually aligned gold stan-
dard alignments. At the same time we propose
two methods for the evaluation of the multiple se-
quence alignments (MSA).
The structure of this paper is as follows. An
example of a multiple alignment and a discus-
sion of the advantages over pairwise alignment
is given in the next section, after which we dis-
cuss our data set in section 3. Section 4 explains
the iterative pairwise alignment algorithm and the
program ALPHAMALIG. Section 5 discusses the
gold standard and two baselines, while section 6
discusses the novel evaluation procedures. The re-
sults are given in section 7 and we end this paper
with a discussion in section 8.
2 Example of Multiple Sequence
Alignment
In this section we will give an example of the au-
tomatically multi-aligned strings from our data set
and point out some important features of the si-
multaneous comparison of more than two strings.
village1 j "A - - - -
village2 j "A z e - -
village3 - "A s - - -
village4 j "A s - - -
village5 j "A z e k a
village6 j "E - - - -
village7 - "6 s - - -
Figure 1: Example of multiple string alignment
In Figure 1 we have multi-aligned pronuncia-
tions of the word az ?I? automatically generated
by ALPHAMALIG. The advantages of this kind
of alignment over pairwise alignment are twofold:
? First, it is easier to detect and process corre-
sponding phones in words and their alterna-
tions (like ["A] and ["E] and ["6] in the second
column in Figure 1).
? Second, the distances/similarities between
strings can be different in pairwise compari-
son as opposed to multiple comparison. This
is so because multi-aligned strings, unlike
pairwise aligned strings, contain information
on the positions where phones were inserted
or deleted in both strings. For example,
in Figure 1 the pairwise alignment of the
pronunciations from village 1 and village 3
would be:
village1 j "A -
village3 - "A s
19
These two alignments have one matching el-
ement out of three in total, which means
that the similarity between them is 1/3 =
0.33. At the same time the similarity be-
tween these two strings calculated based on
the multi-aligned strings in Figure 1 would
be 4/6 = 0.66. The measurement based on
multi-alignment takes the common missing
material into account as well.
3 Data set
The data set used in this paper consists of pho-
netic transcriptions of 152 words collected from
197 sites evenly distributed all over Bulgaria. It
is part of the project Buldialect?Measuring lin-
guistic unity and diversity in Europe.1 Pronun-
ciations of almost all words were collected from
all the sites and for some words there are mul-
tiple pronunciations per site. Phonetic transcrip-
tions include various diacritics and suprasegmen-
tals, making the total number of unique characters
(types) in the data set 98.2
4 Iterative pairwise alignment
Multiple alignment algorithms iteratively merge
two multiple alignments of two subsets of strings
into a single multiple alignment that is union of
those subsets (Gusfield, 1997). The simplest ap-
proach is to align the two strings that have the
minimum distance over all pairs of strings and it-
eratively align strings having the smallest distance
to the already aligned strings in order to generate
a new multiple alignment. Other algorithms use
different initializations and different criteria in se-
lecting the new alignments to merge. Some begin
with the longest (low cost) alignment instead of
the least cost absolutely. A string with the smallest
edit distance to any of the already merged strings
is chosen to be added to the strings in the multiple
alignment. In choosing the pair with the minimal
distance, all algorithms are greedy, and risk miss-
ing optimal alignments.
ALPHAMALIG is an iterative pairwise align-
ment program for bilingual text alignment. It uses
the strategy of merging multiple alignments of
subsets of strings, instead of adding just one string
1The project is sponsored by Volkswagen Stiftung.
More information can be found at http://sfs.uni-
tuebingen.de/dialectometry.
2The data is publicly available and can be found at
http://www.bultreebank.org/BulDialects/index.html.
at the time to the already aligned strings.3 It was
originally developed to align corresponding words
in bilingual texts, i.e. with textual data, but it func-
tions with any data that can be represented as a
sequence of symbols of a finite alphabet. In addi-
tion to the input sequences, the program needs to
know the alphabet and the distances between each
token pair and each pair consisting of a token and
a gap.
In order to perform multiple sequence align-
ments of X-SAMPA word transcriptions we modi-
fied ALPHAMALIG slightly so it could work with
the tokens that consist of more than one symbol,
such as ["e], ["e:] and [t_S]. The distances be-
tween the tokens were specified in such a way that
vowels can be aligned only with vowels and con-
sonants only with consonants. The same tokens
are treated as identical and the distance between
them is set to 0. The distance between any token
in the data set to a gap symbol has the same cost
as replacing a vowel with a vowel or a consonant
with a consonant. Except for this very general lin-
guistic knowledge, no other data-specific informa-
tion was given to the program. In this research we
do not use any phonetic features in order to define
the segments more precisely and to calculate the
distances between them in a more sensitive way
than just making a binary ?match/does-not-match-
distinction?, since we want to keep the system lan-
guage independent and robust to the highest pos-
sible degree.
5 Gold standard and baseline
In order to evaluate the performance of AL-
PHAMALIG, we compared the alignments ob-
tained using this program to the manually aligned
strings, our gold standard, and to the alignments
obtained using two very simple techniques that
are described next: simple baseline and advanced
baseline.
5.1 Simple baseline
The simplest way of aligning two strings would be
to align the first element from one string with the
first element from the other string, the second el-
ement with the second and so on. If two strings
are not of equal length, the remaining unaligned
tokens are aligned with the gap symbol which rep-
3More information on ALPHAMALIG can be found
at http://alggen.lsi.upc.es/recerca/align/alphamalig/intro-
alphamalig.html.
20
resents an insertion or a deletion. This is the align-
ment implicit in Hamming distance, which ignores
insertions and deletions.
By applying this simple method, we obtained
multiple sequence alignments for all words in our
data set. An example of such a multiple sequence
alignment is shown in Figure 2. These align-
ments were used to check how difficult the mul-
tiple sequence alignment task is for our data and
how much improvement is obtained using more
advanced techniques to multi-align strings.
j "A - -
j "A z e
"A S - -
Figure 2: Simple baseline
5.2 Advanced baseline
Our second baseline is more advanced than the
first and was created using the following proce-
dure:
1. for each word the longest string among all
pronunciations is located
2. all strings are pairwise aligned against the
longest string using the Levensthein algo-
rithm (Heeringa, 2004). We refer to both se-
quences in a pairwise alignment as ALIGN-
MENT LINES. Note that alignment lines in-
clude hyphens indicating the places of inser-
tions and deletions.
3. the alignment lines?all of equal length?are
extracted
4. all extracted alignment lines are placed below
each other to form the multiple alignment
An example of combining pairwise alignments
against the longest string (in this case [j"aze]) is
shown in Figure 3.
5.3 Gold standard
Our gold standard was created by manually cor-
recting the advanced baseline alignments de-
scribed in the previous section. The gold stan-
dard results and both baseline results consist of
152 files with multi-aligned strings, one for each
word. The pronunciations are ordered alphabeti-
cally according to the village they come from. If
there are more pronunciations per site, they are all
present, one under the other.
j "A z e
j "A - -
j "A z e
- "A S -
j "A - -
j "A z e
- "A S -
Figure 3: Advanced baseline. The top two align-
ments each contain two alignment lines, and the
bottom one contains three.
6 Evaluation
Although multiple sequence alignments are
broadly used in molecular biology, there is still no
widely accepted objective function for evaluating
the goodness of the multiple aligned strings
(Gusfield, 1997). The quality of the existing
methods used to produce multiple sequence
alignments is judged by the ?biological meaning
of the alignments they produce?. Since strings
in linguistics cannot be judged by the biological
criteria used in string evaluation in biology, we
were forced to propose evaluation methods that
would be suitable for the strings in question. One
of the advantages we had was the existence of
the gold standard alignments, which made our
task easier and more straightforward?in order to
determine the quality of the multi-aligned strings,
we compare outputs of the different algorithms to
the gold standard. Since there is no off-the-shelf
method that can be used for comparison of multi-
aligned strings to a gold standard, we propose
two novel methods?one sensitive to the order of
columns in two alignments and another that takes
into account only the content of each column.
6.1 Column dependent method
The first method we developed compares the con-
tents of the columns and also takes the column se-
quence into account. The column dependent eval-
uation (CDE) procedure is as follows:
? Each gold standard column is compared to
the most similar column out of two neigh-
boring columns of a candidate multiple align-
ment. The two neighboring columns depend
on the previous matched column j and have
indices j +1 and j +2 (at the start j = 0). It
is possible that there are columns in the can-
didate multiple alignment which remain un-
matched, as well as columns at the end of the
gold standard which remain unmatched.
21
? The similarity of a candidate column with a
gold standard column is calculated by divid-
ing the number of correctly placed elements
in every candidate column by the total num-
ber of elements in the column. A score of
1 indicates perfect overlap, while a score of
0 indicates the columns have no elements in
common.
? The similarity score of the whole multiple
alignment (for a single word) is calculated by
summing the similarity score of each candi-
date column and dividing it by the total num-
ber of matched columns plus the total num-
ber of unmatched columns in both multiple
alignments.
? The final similarity score between the set of
gold standard alignments with the set of can-
didate multiple alignments is calculated by
averaging the multiple alignment similarity
scores for all strings.
As an example consider the multiple alignments
in Figure 4, with the gold standard alignment (GS)
on the left and the generated alignment (GA) on
the right.
w rj "E m e
v r "e m i
u rj "e m i
v rj "e m i
w - rj "E m e
v - r "e m i
- u rj "e m i
v - rj "e m i
Figure 4: GS and ALPHAMALIG multiple string
alignments, the gold standard alignment left, the
ALPHAMALIG output right.
The evaluation starts by comparing the first col-
umn of the GS with the first and second column
of the GA. The first column of the GA is the best
match, since the similarity score between the first
columns is 0.75 (3 out of 4 elements match). In
similar fashion, the second column of the GS is
compared with the second and the third column of
the GA and matched with the third column of GA
with a similarity score of 1 (all elements match).
The third GS column is matched with the fourth
GA column, the fourth GS column with the fifth
GA column and the fifth GS column with the sixth
GA column (all three having a similarity score of
1). As a consequence, the second column of the
GA remains unmatched. In total, five columns are
matched and one column remains unmatched. The
total score of the GA equals:
(0.75 + 1 + 1 + 1 + 1)
(5 + 1)
= 0.792
It is clear that this method punishes unmatched
columns by increasing the value of the denomina-
tor in the similarity score calculation. As a conse-
quence, swapped columns are punished severely,
which is illustrated in Figure 5.
"o rj @ j -
"o rj @ - u
"o rj @ f -
"o rj @ - j
"o rj @ u -
"o rj @ - f
Figure 5: Two alignments with swapped columns
In the alignments in Figure 5, the first three
columns of GS would be matched with the first
three columns of GA with a score of 1, the fourth
would be matched with the fifth, and two columns
would be left unmatched: the fifth GS column and
the fourth GA column yielding a total similarity
score of 4/6 = 0.66. Especially in this case this is
undesirable, as both sequences of these columns
represent equally reasonable multiple alignment
and should have a total similarity score of 1.
We therefore need a less strict evaluation method
which does not insist on the exact ordering. An
alternative method is introduced and discussed in
the following section.
6.2 Modified Rand Index
In developing an alternative evaluation we pro-
ceeded from the insight that the columns of a mul-
tiple alignment are a sort of PARTITION of the el-
ements of the alignment strings, i.e., they consti-
tute a set of disjoint multi-sets whose union is the
entire multi-set of segments in the multiple align-
ment. Each column effectively assigns its seg-
ments to a partition, which clearly cannot overlap
with the elements of another column (partition).
Since every segment must fall within some col-
umn, the assignment is also exhaustive.
Our second evaluation method is therefore
based on the modified Rand index (Hubert and
Arabie, 1985). The modified Rand index is used
in classification for comparing two different parti-
tions of a finite set of objects. It is based on the
Rand index (Rand, 1971), one of the most popular
measures for comparing the degree to which parti-
tions agree (in classification).
Given a set of n elements S = o1, ...on and two
partitions of S, U and V , the Rand index R is de-
fined as:
22
R =
a + b
a + b + c + d
where:
? a: the number of pairs of elements in S that
are in the same set (column) in U and in the
same set in V
? b: the number of pairs of elements in S that
are in different sets (columns) inU and in dif-
ferent sets in V
? c: the number of pairs of elements in S that
are in the same set in U and in different sets
in V
? d: the number of pairs of elements in S that
are in different sets in U and in the same set
in V
Consequently, a and b are the number of pairs of
elements on which two classifications agree, while
c and d are the number of pairs of elements on
which they disagree. In our case classifications
agree about concrete segment tokens only in the
cases where they appear in the same columns in
the alignments.
The value of Rand index ranges between 0 and
1, with 0 indicating that the two partitions (multi-
alignments) do not agree on any pair of points and
1 indicating that the data partitions are exactly the
same.4 A problem with the Rand index is that it
does not return a constant value (zero) if two par-
titions are picked at random. Hubert and Arabie
(1985) suggested a modification of the Rand in-
dex (MRI) that corrects this property. It can be
expressed in the general form as:
MRI =
Rand index? Expected index
Maximum index? Expected index
The expected index is the expected number of
pairs which would be placed in the same set in U
and in the same set in V by chance. The maximum
index represents the maximum number of objects
that can be put in the same set in U and in the
same set in V . The MRI value ranges between ?1
and 1, with perfect overlap being indicated by 1
and values ? 0 indicating no overlap. For a more
detailed explanation of the modified Rand index,
please refer to Hubert and Arabie (1985).
4In dialectometry, this index was used by Heeringa et al
(2002) to validate dialect clustering methods.
We would like to emphasize that it is clear that
the set of columns of a multi-alignment have more
structure than a partition sec, in particular because
the columns (subpartitions) are ordered, unlike the
subpartitions in a partition. But we shall compen-
sate for this difference by explicitly marking order.
"o [1] rj [2] @ [3] j [4] -
"o [5] rj [6] @ [7] - u [8]
"o [9] rj [10] @ [11] f [12] -
Figure 6: Annotated alignment
In our study, each segment token in each tran-
scription was treated as a different object (see Fig-
ure 6), and every column was taken to be a sub-
partition to which segment tokens are assigned.
Both alignments in Figure 5 have 12 phones that
are put into 5 groups. We ?tag? each token sequen-
tially in order to distinguish the different tokens of
a single segment from each other, but note that the
way we do this also introduces an order sensitivity
in the measure. The two partitions obtained are:
GS1 = {1,5,9}
GS2 = {2,6,10}
GS3 = {3,7,11}
GS4 = {4,12}
GS5 = {8}
GA1 = {1,5,9}
GA2 = {2,6,10}
GA3 = {3,7,11}
GA4 = {8}
GA5 = {4,12}
Using the modified Rand index the quality of
each column is checked, regardless of whether the
columns are in order. The MRI for the alignments
in Figure 5 will be 1, because both alignments
group segment tokens in the same way. Even
though columns four and five are swapped, in both
classifications phones [j] and [f] are grouped to-
gether, while sound [u] forms a separate group.
The MRI itself only takes into account the
quality of each column separately since it sim-
ply checks whether the same elements are together
in the candidate alignment as in the gold-standard
alignment. It is therefore insensitive to the order-
ing of columns. While it may have seemed coun-
terintuitive linguistically to proceed from an order-
insensitive measure, the comparison of ?tagged to-
kens? described above effectively reintroduces or-
der sensitivity.
In the next section we describe the results of ap-
plying both evaluation methods on the automati-
cally generated multiple alignments.
23
7 Results
After comparing all files of the baseline algo-
rithms and ALPHAMALIG against the gold stan-
dard files according to the column dependent eval-
uation method and the modified Rand index, the
average score is calculated by summing up all
scores and dividing them by the number of word
files (152).
The results are given in Table 1 and also in-
clude the number of words with perfect multi-
alignments (i.e. identical to the gold standard).
Using CDE, ALPHAMALIG scored 0.932 out of
1.0 with 103 perfectly aligned files. The result
for the simple baseline was 0.710 with 44 per-
fectly aligned files. As expected, the result for
the advanced baseline was in between these two
results?0.869 with 72 files that were completely
identical to the GS files. Using MRI to eval-
uate the alignments generated we obtained gen-
erally higher scores for all three algorithms, but
with the same ordering. ALPHAMALIG scored
0.982, with 104 perfectly aligned files. The ad-
vanced baseline had a lower score of 0.937 and
74 perfect alignments. The simple baseline per-
formed worse, scoring 0.848 and having 44 per-
fectly aligned files.
The scores of the CDE evaluation method are
lower than the MRI scores, which is due to the first
method?s problematic sensitivity to column order-
ing in the alignments. It is clear that in both evalu-
ation methods ALPHAMALIG outperforms both
baseline alignments by a wide margin.
It is important to notice that the scores for the
simple baseline are reasonably high, which can
be explained by the structure of our data set.
The variation of word pronunciations is relatively
small, making string alignment easier. However,
ALPHAMALIG obtained much higher scores us-
ing both evaluation methods.
Additional qualitative error analysis reveals that
the errors of ALPHAMALIG are mostly caused by
the vowel-vowel consonant-consonant alignment
restriction. In the data set there are 21 files that
contain metathesis. Since vowel-consonant align-
ments were not allowed in ALPHAMALIG, align-
ments produced by this algorithm were different
from the gold standard, as illustrated in Figure 7.
The vowel-consonant restriction is also respon-
sible for wrong alignments in some words where
metathesis is not present, but where the vowel-
consonant alignment is still preferred over align-
v l "7 k
v "7 l k
v l "7 - k
v - "7 l k
Figure 7: Two alignments with metathesis
ing vowels and/or consonants with a gap (see for
example Figure 4).
The other type of error present in the AL-
PHAMALIG alignments is caused by the fact
that all vowel-vowel and consonant-consonant dis-
tances receive the same weight. In Figure 8
the alignment of word bjahme ?were? produced
by ALPHAMALIG is wrong because instead of
aligning [mj] with [m] and [m] it is wrongly
aligned with [x] and [x], while [x] is aligned with
[S] instead of aligning it with [x] and [x].
b "E S u x - m e -
bj "A - - x - m i -
b "e x - mj - - 7 -
Figure 8: Alignment error produced by AL-
PHAMALIG
8 Discussion and future work
In this study we presented a first attempt to auto-
matically multi-align phonetic transcriptions. The
algorithmwe used to generate alignments has been
shown to be very reliable, produce alignments of
good quality, with less than 2% error at the seg-
ment level. In this study we used only very sim-
ple linguistic knowledge in order to align strings.
The only restriction we imposed was that a vowel
should only be aligned with a vowel and a con-
sonant only with a consonant. The system has
shown to be very robust and to produce good qual-
ity alignments with a very limited information on
the distances between the tokens. However, in the
future we would like to apply this algorithm using
more detailed segment distances, so that we can
work without vowel-consonant restrictions. Using
more detailed language specific feature system for
each phone, we believe we may be able to improve
the produced alignments further. This especially
holds for the type of errors illustrated in Figure 8
where it is clear that [mj] is phonetically closer to
[m] than to [x] sound.
As our data set was relatively simple (indicated
by the reasonable performance of our simple base-
line algorithm), we would very much like to eval-
uate ALPHAMALIG against a more complex data
24
CDE CDE perfect columns MRI MRI perfect columns
Simple baseline 0.710 44 0.848 44
Advanced baseline 0.869 72 0.937 74
ALPHAMALIG 0.932 103 0.982 104
Table 1: Results of evaluating outputs of the different algorithms against the GS
set and try to replicate the good results we ob-
tained here. On one hand, high performance of
both baseline algorithms show that our task was
relatively easy. On the other hand, achieving per-
fect alignments will be very difficult, if possible at
all.
Additionally, we proposed two methods to eval-
uate multiple aligned strings in linguistic research.
Although these systems could be improved, both
of them are giving a good estimation of the qual-
ity of the generated alignments. For the examined
data, we find MRI to be better evaluation tech-
nique since it overcomes the problem of swapped
columns.
In this research we tested and evaluated AL-
PHAMALIG on the dialect phonetic data. How-
ever, multiple sequence alignments can be also
applied on the sequences of sentences and para-
graphs. This makes multiple sequence alignment
algorithm a powerful tool for mining text data in
social sciences, humanities and education.
Acknowledgements
We are thankful to Xavier Messeguer of the Tech-
nical University of Catalonia who kindly sup-
plied us with the source code of ALPHAMALIG.
We also thank Therese Leinonen and Sebastian
K?rschner of the University of Groningen and Es-
teve Valls i Alecha of the University of Barcelona
for their useful feedback on our ideas.
References
Laura Alonso, Irene Castellon, Jordi Escribano, Xavier
Messeguer, and Lluis Padro. 2004. Multiple
Sequence Alignment for characterizing the linear
structure of revision. In Proceedings of the 4th In-
ternational Conference on Language Resources and
Evaluation.
Quentin Atkinson, Geoff Nicholls, David Welch, and
Russell Gray. 2005. From words to dates: water
into wine, mathemagic or phylogenetic inference.
Transcriptions of the Philological Society, 103:193?
219.
Lyle Campbell. 2004. Historical Linguistics: An In-
troduction. Edinburgh University Press, second edi-
tion.
Russel D. Gray and Quentin D. Atkinson. 2003.
Language-tree divergence times support the Ana-
tolian theory of Indo-European origin. Nature,
426:435?339.
Russel D. Gray and Fiona M. Jordan. 2000. Lan-
guage trees support the express-train sequence of
Austronesian expansion. Nature, 405:1052?1055.
Dan Gusfield. 1997. Algorithms on Strings, Trees and
Sequences: Computer Science and Computational
Biology. Cambridge University Press.
Wilbert Heeringa and Brian Joseph. 2007. The rela-
tive divergence of Dutch dialect pronunciations from
their common source: An exploratory study. In John
Nerbonne, T. Mark Ellison, and Grzegorz Kondrak,
editors, Proceedings of the Ninth Meeting of the ACL
Special Interest Group in Computational Morphol-
ogy and Phonology.
Wilbert Heeringa, John Nerbonne, and Peter Kleiweg.
2002. Validating dialect comparison methods. In
Wolfgang Gaul and Gunter Ritter, editors, Classifi-
cation, Automation, and New Media. Proceedings of
the 24th Annual Conference of the Gesellschaft f?r
Klassifikation e. V., University of Passau, March 15-
17, 2000, pages 445?452. Springer, Berlin, Heidel-
berg and New York.
Wilbert Heeringa. 2004. Measuring Dialect Pronunci-
ation Differences using Levenshtein Distance. Ph.D.
thesis, Rijksuniversiteit Groningen.
Lawrence Hubert and Phipps Arabie. 1985. Compar-
ing partitions. Journal of Classification, 2:193?218.
Grzegorz Kondrak. 2002. Algorithms for Language
Reconstruction. PhD Thesis, University of Toronto.
William M. Rand. 1971. Objective criteria for the
evaluation of clustering methods. Journal of Amer-
ican Statistical Association, 66(336):846?850, De-
cember.
Tandy Warnow, Steven N. Evans, Donald Ringe, and
Luay Nakhleh. 2006. A stochastic model of lan-
guage evolution that incorporates homoplasy and
borrowing. In Peter Forster and Colin Renfrew, ed-
itors, Phylogenetic Methods and the Prehistory of
Languages. MacDonald Institute for Archaeological
Research, Cambridge.
25
Proceedings of the EACL 2009 Workshop on Language Technology and Resources for Cultural Heritage,
Social Sciences, Humanities, and Education ?LaTeCH ? SHELT&R 2009, pages 26?34,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
Evaluating the pairwise string alignment of pronunciations
Martijn Wieling
University of Groningen
The Netherlands
m.b.wieling@rug.nl
Jelena Prokic?
University of Groningen
The Netherlands
j.prokic@rug.nl
John Nerbonne
University of Groningen
The Netherlands
j.nerbonne@rug.nl
Abstract
Pairwise string alignment (PSA) is an im-
portant general technique for obtaining a
measure of similarity between two strings,
used e.g., in dialectology, historical lin-
guistics, transliteration, and in evaluating
name distinctiveness. The current study
focuses on evaluating different PSA meth-
ods at the alignment level instead of via
the distances it induces. About 3.5 million
pairwise alignments of Bulgarian phonetic
dialect data are used to compare four al-
gorithms with a manually corrected gold
standard. The algorithms evaluated in-
clude three variants of the Levenshtein al-
gorithm as well as the Pair Hidden Markov
Model. Our results show that while all
algorithms perform very well and align
around 95% of all alignments correctly,
there are specific qualitative differences in
the (mis)alignments of the different algo-
rithms.
1 Introduction
Our cultural heritage is not only accessible
through museums, libraries, archives and their
digital portals, it is alive and well in the varied
cultural habits practiced today by the various peo-
ples of the world. To research and understand this
cultural heritage we require instruments which are
sensitive to its signals, and, in particular sensitive
to signals of common provenance. The present
paper focuses on speech habits which even today
bear signals of common provenance in the vari-
ous dialects of the world?s languages, and which
have also been recorded and preserved in major
archives of folk culture internationally. We present
work in a research line which seeks to develop
digital instruments capable of detecting common
provenance among pronunciation habits, focusing
in this paper on the issue of evaluating the quality
of these instruments.
Pairwise string alignment (PSA) methods, like
the popular Levenshtein algorithm (Levenshtein,
1965) which uses insertions (alignments of a seg-
ment against a gap), deletions (alignments of a gap
against a segment) and substitutions (alignments
of two segments) often form the basis of deter-
mining the distance between two strings. Since
there are many alignment algorithms and specific
settings for each algorithm influencing the dis-
tance between two strings (Nerbonne and Klei-
weg, 2007), evaluation is very important in deter-
mining the effectiveness of the distance methods.
Determining the distance (or similarity) be-
tween two phonetic strings is an important aspect
of dialectometry, and alignment quality is impor-
tant in applications in which string alignment is
a goal in itself, for example, determining if two
words are likely to be cognate (Kondrak, 2003),
detecting confusable drug names (Kondrak and
Dorr, 2003), or determining whether a string is
the transliteration of the same name from another
writing system (Pouliquen, 2008).
In this paper we evaluate string distance mea-
sures on the basis of data from dialectology. We
therefore explain a bit more of the intended use of
the pronunciation distance measure.
Dialect atlases normally contain a large num-
ber of pronunciations of the same word in various
places throughout a language area. All pairs of
pronunciations of corresponding words are com-
pared in order to obtain a measure of the aggre-
gate linguistic distance between dialectal varieties
(Heeringa, 2004). It is clear that the quality of the
measurement is of crucial importance.
Almost all evaluation methods in dialectometry
focus on the aggregate results and ignore the in-
dividual word-pair distances and individual align-
ments on which the distances are based. The fo-
cus on the aggregate distance of 100 or so word
26
pairs effectively hides many differences between
methods. For example, Heeringa et al (2006) find
no significant differences in the degrees to which
several pairwise string distance measures correlate
with perceptual distances when examined at an ag-
gregate level. Wieling et al (2007) and Wieling
and Nerbonne (2007) also report almost no differ-
ence between different PSA algorithms at the ag-
gregate level. It is important to be able to evaluate
the different techniques more sensitively, which is
why this paper examines alignment quality at the
segment level.
Kondrak (2003) applies a PSA algorithm to
align words in different languages in order to de-
tect cognates automatically. Exceptionally, he
does provide an evaluation of the string alignments
generated by different algorithms. But he restricts
his examination to a set of only 82 gold standard
pairwise alignments and he only distinguishes cor-
rect and incorrect alignments and does not look at
misaligned phones.
In the current study we introduce and evaluate
several alignment algorithms more extensively at
the alignment level. The algorithms we evaluate
include the Levenshtein algorithm (with syllabic-
ity constraint), which is one of the most popular
alignment methods and has successfully been used
in determining pronunciation differences in pho-
netic strings (Kessler, 1995; Heeringa, 2004). In
addition we look at two adaptations of the Lev-
enshtein algorithm. The first adaptation includes
the swap-operation (Wagner and Lowrance, 1975),
while the second adaptation includes phonetic seg-
ment distances, which are generated by applying
an iterative pointwise mutual information (PMI)
procedure (Church and Hanks, 1990). Finally we
include alignments generated with the Pair Hid-
den Markov Model (PHMM) as introduced to lan-
guage studies by Mackay and Kondrak (2005).
They reported that the Pair Hidden Markov Model
outperformed ALINE, the best performing algo-
rithm at the alignment level in the aforementioned
study of Kondrak (2003). The PHMM has also
successfully been used in dialectology by Wieling
et al (2007).
2 Dataset
The dataset used in this study consists of 152
words collected from 197 sites equally distributed
over Bulgaria. The transcribed word pronuncia-
tions include diacritics and suprasegmentals (e.g.,
intonation). The total number of different phonetic
types (or segments) is 98.1
The gold standard pairwise alignment was au-
tomatically generated from a manually corrected
gold standard set of N multiple alignments (see
Prokic? et al, 2009 ) in the following way:
? Every individual string (including gaps) in
the multiple alignment is aligned with ev-
ery other string of the same word. With 152
words and 197 sites and in some cases more
than one pronunciations per site for a cer-
tain word, the total number of pairwise align-
ments is about 3.5 million.
? If a resulting pairwise alignment contains a
gap in both strings at the same position (a
gap-gap alignment), these gaps are removed
from the pairwise alignment. We justify this,
reasoning that no alignment algorithm may
be expected to detect parallel deletions in a
single pair of words. There is no evidence for
this in the single pair.
To make this clear, consider the multiple align-
ment of three Bulgarian dialectal variants of the
word ?I? (as in ?I am?):
j "A s
"A z i
j "A
Using the procedure above, the three generated
pairwise alignments are:
j "A s j "A s "A z i
"A z i j "A j "A
3 Algorithms
Four algorithms are evaluated with respect to the
quality of their alignments, including three vari-
ants of the Levenshtein algorithm and the Pair
Hidden Markov Model.
3.1 The VC-sensitive Levenshtein algorithm
The Levenshtein algorithm is a very efficient dy-
namic programming algorithm, which was first in-
troduced by Kessler (1995) as a tool for computa-
tionally comparing dialects. The Levenshtein dis-
tance between two strings is determined by count-
ing the minimum number of edit operations (i.e.
insertions, deletions and substitutions) needed to
transform one string into the other.
1The dataset is available online at the website
http://www.bultreebank.org/BulDialects/
27
For example, the Levenshtein distance between
[j"As] and ["Azi], two Bulgarian dialectal variants
of the word ?I? (as in ?I am?), is 3:
j"As delete j 1
"As subst. s/z 1
"Az insert i 1
"Azi
3
The corresponding alignment is:
j "A s
"A z i
1 1 1
The Levenshtein distance has been used fre-
quently and successfully in measuring linguis-
tic distances in several languages, including Irish
(Kessler, 1995), Dutch (Heeringa, 2004) and Nor-
wegian (Heeringa, 2004). Additionally, the Lev-
enshtein distance has been shown to yield aggre-
gate results that are consistent (Cronbach?s ? =
0.99) and valid when compared to dialect speak-
ers judgements of similarity (r ? 0.7; Heeringa et
al., 2006).
Following Heeringa (2004), we have adapted
the Levenshtein algorithm slightly, so that it does
not allow alignments of vowels with consonants.
We refer to this adapted algorithm as the VC-
sensitive Levenshtein algorithm.
3.2 The Levenshtein algorithm with the swap
operation
Because metathesis (i.e. transposition of sounds)
occurs relatively frequently in the Bulgarian di-
alect data (in 21 of 152 words), we extend the
VC-sensitive Levenshtein algorithm as described
in section 3.1 to include the swap-operation (Wag-
ner and Lowrance, 1975), which allows two ad-
jacent characters to be interchanged. The swap-
operation is also known as a transposition, which
was introduced with respect to detecting spelling
errors by Damerau (1964). As a consequence the
Damerau distance refers to the minimum number
of insertions, deletions, substitutions and transpo-
sitions required to transform one string into the
other. In contrast to Wagner and Lowrance (1975)
and in line with Damerau (1964) we restrict the
swap operation to be only allowed for string X
and Y when xi = yi+1 and yi = xi+1 (with xi
being the token at position i in string X):
xi xi+1
yi yi+1
>< 1
Note that a swap-operation in the alignment is in-
dicated by the symbol ?><?. The first number fol-
lowing this symbol indicates the cost of the swap-
operation.
Consider the alignment of [vr"7] and [v"7r],2
two Bulgarian dialectal variants of the word ?peak?
(mountain). The alignment involves a swap and
results in a total Levenshtein distance of 1:
v r "7
v "7 r
>< 1
However, the alignment of the transcription [vr"7]
with another dialectal transcription [v"ar] does not
allow a swap and yields a total Levenshtein dis-
tance of 2:
v r "7
v "a r
1 1
Including just the option of swapping identical
segments in the implementation of the Leven-
shtein algorithm is relatively easy. We set the
cost of the swap operation to one3 plus twice the
cost of substituting xi with yi+1 plus twice the
cost of substituting yi with xi+1. In this way the
swap operation will be preferred when xi = yi+1
and yi = xi+1, but not when xi 6= yi+1 and/or
yi 6= xi+1. In the first case the cost of the swap
operation is 1, which is less than the cost of the
alternative of two substitutions. In the second case
the cost is either 3 (if xi 6= yi+1 or yi 6= xi+1) or
5 (if xi 6= yi+1 and yi 6= xi+1), which is higher
than the cost of using insertions, deletions and/or
substitutions.
Just as in the previous section, we do not allow
vowels to align with consonants (except in the case
of a swap).
3.3 The Levenshtein algorithm with
generated segment distances
The VC-sensitive Levenshtein algorithm as de-
scribed in section 3.1 only distinguishes between
vowels and consonants. However, more sensi-
tive segment distances are also possible. Heeringa
(2004) experimented with specifying phonetic
segment distances based on phonetic features and
2We use transcriptions in which stress is marked on
stressed vowels instead of before stressed syllables. We fol-
low in this the Bulgarian convention instead of the IPA con-
vention.
3Actually the cost is set to 0.999 to prefer an alignment
involving a swap over an alternative alignment involving only
regular edit operations.
28
also based on acoustic differences derived from
spectrograms, but he did not obtain improved re-
sults at the aggregate level.
Instead of using segment distances as these are
(incompletely) suggested by phonetic or phono-
logical theory, we tried to determine the sound
distances automatically based on the available
data. We used pointwise mutual information
(PMI; Church and Hanks, 1990) to obtain these
distances. It generates segment distances by as-
sessing the degree of statistical dependence be-
tween the segments x and y:
PMI(x, y) = log2
(
p(x, y)
p(x) p(y)
)
(1)
Where:
? p(x, y): the number of times x and y occur
at the same position in two aligned strings
X and Y , divided by the total number of
aligned segments (i.e. the relative occurrence
of the aligned segments x and y in the whole
dataset). Note that either x or y can be a gap
in the case of insertion or deletion.
? p(x) and p(y): the number of times x (or y)
occurs, divided by the total number of seg-
ment occurrences (i.e. the relative occurrence
of x or y in the whole dataset). Dividing by
this term normalizes the empirical frequency
with respect to the frequency expected if x
and y are statistically independent.
The greater the PMI value, the more segments tend
to cooccur in correspondences. Negative PMI val-
ues indicate that segments do not tend to cooccur
in correspondences, while positive PMI values in-
dicate that segments tend to cooccur in correspon-
dences. The segment distances can therefore be
generated by subtracting the PMI value from 0 and
adding the maximum PMI value (i.e. lowest dis-
tance is 0). In that way corresponding segments
obtain the lowest distance.
Based on the PMI value and its conversion to
segment distances, we developed an iterative pro-
cedure to automatically obtain the segment dis-
tances:
1. The string alignments are generated using the
VC-sensitive Levenshtein algorithm (see sec-
tion 3.1).4
4We also used the Levenshtein algorithm without the
vowel-consonant restriction to generate the PMI values, but
this had a negative effect on the performance.
2. The PMI value for every segment pair is cal-
culated according to (1) and subsequently
transformed to a segment distance by sub-
tracting it from zero and adding the maxi-
mum PMI value.
3. The Levenshtein algorithm using these seg-
ment distances is applied to generate a new
set of alignments.
4. Step 2 and 3 are repeated until the alignments
of two consecutive iterations do not differ
(i.e. convergence is reached).
The potential merit of using PMI-generated seg-
ment distances can be made clear by the following
example. Consider the strings [v"7n] and [v"7?k@],
Bulgarian dialectal variants of the word ?outside?.
The VC-sensitive Levenshtein algorithm yields
the following (correct) alignment:
v "7 n
v "7 ? k @
1 1 1
But also the alternative (incorrect) alignment:
v "7 n
v "7 ? k @
1 1 1
The VC-sensitive Levenshtein algorithm gener-
ates the erroneous alignment because it has no way
to identify that the consonant [n] is nearer to the
consonant [?] than to the consonant [k]. In con-
trast, the Levenshtein algorithm which uses the
PMI-generated segment distances only generates
the correct first alignment, because the [n] occurs
relatively more often aligned with [?] than with
[k] so that the distance between [n] and [?] will
be lower than the distance between [n] and [k].
The idea behind this procedure is similar to Ris-
tad?s suggestion to learn segment distances for edit
distance using an expectation maximization algo-
rithm (Ristad and Yianilos, 1998). Our approach
differs from their approach in that we only learn
segment distances based on the alignments gener-
ated by the VC-sensitive Levenshtein algorithm,
while Ristad and Yianilos (1998) learn segment
distances by considering all possible alignments of
two strings.
3.4 The Pair Hidden Markov Model
The Pair Hidden Markov Model (PHMM) also
generates alignments based on automatically gen-
erated segment distances and has been used suc-
29
Figure 1: Pair Hidden Markov Model. Image
courtesy of Mackay and Kondrak (2005).
cessfully in language studies (Mackay and Kon-
drak, 2005; Wieling et al, 2007).
A Hidden Markov Model (HMM) is a proba-
bilistic finite-state transducer that generates an ob-
servation sequence by starting in an initial state,
going from state to state based on transition prob-
abilities and emitting an output symbol in each
state based on the emission probabilities in that
state for that output symbol (Rabiner, 1989). The
PHMM was originally proposed by Durbin et al
(1998) for aligning biological sequences and was
first used in linguistics by Mackay and Kondrak
(2005) to identify cognates. The PHMM differs
from the regular HMM in that it outputs two ob-
servation streams (i.e. a series of alignments of
pairs of individual segments) instead of only a se-
ries of single symbols. The PHMM displayed in
Figure 1 has three emitting states: the substitution
(?match?) state (M) which emits two aligned sym-
bols, the insertion state (Y) which emits a symbol
and a gap, and the deletion state (X) which emits
a gap and a symbol.
The following example shows the state se-
quence for the pronunciations [j"As] and ["Azi] (En-
glish ?I?):
j "A s
"A z i
X M M Y
Before generating the alignments, all probabil-
ities of the PHMM have to be estimated. These
probabilities consist of the 5 transition probabili-
ties shown in Figure 1: , ?, ?, ?XY and ?M . In
addition there are 98 emission probabilities for the
insertion state and the deletion state (one for ev-
ery segment) and 9604 emission probabilities for
the substitution state. The probability of starting in
one of the three states is set equal to the probability
of going from the substitution state to that particu-
lar state. The Baum-Welch expectation maximiza-
tion algorithm (Baum et al, 1970) can be used to
iteratively reestimate these probabilities until a lo-
cal optimum is found.
To prevent order effects in training, every word
pair is considered twice (e.g., wa ? wb and wb ?
wa). The resulting insertion and deletion probabil-
ities are therefore the same (for each segment), and
the probability of substituting x for y is equal to
the probability of substituting y for x, effectively
yielding 4802 distinct substitution probabilities.
Wieling et al (2007) showed that using Dutch
dialect data for training, sensible segment dis-
tances were obtained; acoustic vowel distances
on the basis of spectrograms correlated signifi-
cantly (r = ?0.72) with the vowel substitution
probabilities of the PHMM. Additionally, proba-
bilities of substituting a symbol with itself were
much higher than the probabilities of substitut-
ing an arbitrary vowel with another non-identical
vowel (mutatis mutandis for consonants), which
were in turn much higher than the probabilities of
substituting a vowel for a consonant.
After training, the well known Viterbi algorithm
can be used to obtain the best alignments (Rabiner,
1989).
4 Evaluation
As described in section 2, we use the generated
pairwise alignments from a gold standard of multi-
ple alignments for evaluation. In addition, we look
at the performance of a baseline of pairwise align-
ments, which is constructed by aligning the strings
according to the Hamming distance (i.e. only al-
lowing substitutions and no insertions or deletions;
Hamming, 1950).
The evaluation procedure consists of comparing
the alignments of the previously discussed algo-
rithms including the baseline with the alignments
of the gold standard. For the comparison we use
the standard Levenshtein algorithm without any
restrictions. The evaluation proceeds as follows:
1. The pairwise alignments of the four algo-
rithms, the baseline and the gold standard are
generated and standardized (see section 4.1).
When multiple equal-scoring alignments are
30
generated by an algorithm, only one (i.e. the
final) alignment is selected.
2. In each alignment, we convert each pair of
aligned segments to a single token, so that ev-
ery alignment of two strings is converted to a
single string of segment pairs.
3. For every algorithm these transformed strings
are aligned with the transformed strings of
the gold standard using the standard Leven-
shtein algorithm.
4. The Levenshtein distances for all these
strings are summed up resulting in the total
distance between every alignment algorithm
and the gold standard. Only if individual
segments match completely the segment dis-
tance is 0, otherwise it is 1.
To illustrate this procedure, consider the following
gold standard alignment of [vl"7k] and [v"7lk], two
Bulgarian dialectal variants of the word ?wolf?:
v l "7 k
v "7 l k
Every aligned segment pair is converted to a single
token by adding the symbol ?/? between the seg-
ments and using the symbol ?-? to indicate a gap.
This yields the following transformed string:
v/v l/"7 "7/l k/k
Suppose another algorithm generates the follow-
ing alignment (not detecting the swap):
v l "7 k
v "7 l k
The transformed string for this alignment is:
v/v l/- "7/"7 -/l k/k
To evaluate this alignment, we align this string to
the transformed string of the gold standard and ob-
tain a Levenshtein distance of 3:
v/v l/"7 "7/l k/k
v/v l/- "7/"7 -/l k/k
1 1 1
By repeating this procedure for all alignments and
summing up all distances, we obtain total dis-
tances between the gold standard and every align-
ment algorithm. Algorithms which generate high-
quality alignments will have a low distance from
the gold standard, while the distance will be higher
for algorithms which generate low-quality align-
ments.
4.1 Standardization
The gold standard contains a number of align-
ments which have alternative equivalent align-
ments, most notably an alignment containing an
insertion followed by a deletion (which is equal
to the deletion followed by the insertion), or an
alignment containing a syllabic consonant such as
["?
"
], which in fact matches both a vowel and a
neighboring r-like consonant and can therefore be
aligned with either the vowel or the consonant. In
order to prevent punishing the algorithms which
do not match the exact gold standard in these
cases, the alignments of the gold standard and all
alignment algorithms are transformed to one stan-
dard form in all relevant cases.
For example, consider the correct alignment of
[v"iA] and [v"ij], two Bulgarian dialectal variations
of the English plural pronoun ?you?:
v "i A
v "i j
Of course, this alignment is as reasonable as:
v "i A
v "i j
To avoid punishing the first, we transform all in-
sertions followed by deletions to deletions fol-
lowed by insertions, effectively scoring the two
alignments the same.
For the syllabic consonants we transform all
alignments to a form in which the syllabic con-
sonant is followed by a gap and not vice versa.
For instance, aligning [v"?
"
x] with [v"Arx] (English:
?peak?) yields:
v "?
"
x
v "A r x
Which is transformed to the equivalent alignment:
v "?
"
x
v "A r x
5 Results
We will report both quantitative results using the
evaluation method discussed in the previous sec-
tion, as well as the qualitative results, where we
focus on characteristic errors of the different align-
ment algorithms.
5.1 Quantitative results
Because there are two algorithms which use gen-
erated segment distances (or probabilities) in their
alignments, we first check if these values are sen-
sible and comparable to each other.
31
5.1.1 Comparison of segment distances
With respect to the PMI results (convergence
was reached after 7 iterations, taking less than
5 CPU minutes), we indeed found sensible re-
sults: the average distance between identical sym-
bols was significantly lower than the distance be-
tween pairs of different vowels and consonants
(t < ?13, p < .001). Because we did not allow
vowel-consonants alignments in the Levenshtein
algorithm, no PMI values were generated for those
segment pairs.
Just as Wieling et al (2007), we found sen-
sible PHMM substitution probabilities (conver-
gence was reached after 1675 iterations, taking
about 7 CPU hours): the probability of matching
a symbol with itself was significantly higher than
the probability of substituting one vowel for an-
other (similarly for consonants), which in turn was
higher than the probability of substituting a vowel
with a consonant (all t?s > 9, p < .001).
To allow a fair comparison between the PHMM
probabilities and the PMI distances, we trans-
formed the PHMM probabilities to log-odds
scores (i.e. dividing the probability by the rela-
tive frequency of the segments and subsequently
taking the log). Because the residues after the
linear regression between the PHMM similarities
and PMI distances were not normally distributed,
we used Spearman?s rank correlation coefficient
to assess the relationship between the two vari-
ables. We found a highly significant Spearman?s
? = ?.965 (p < .001), which means that the re-
lationship between the PHMM similarities and the
PMI distances is very strong. When looking at the
insertions and deletions we also found a significant
relationship: Spearman?s ? = ?.736 (p < .001).
5.1.2 Evaluation against the gold standard
Using the procedure described in section 4, we cal-
culated the distances between the gold standard
and the alignment algorithms. Besides reporting
the total number of misaligned tokens, we also di-
vided this number by the total number of aligned
segments in the gold standard (about 16 million)
to get an idea of the error rate. Note that the error
rate is 0 in the perfect case, but might rise to nearly
2 in the worst case, which is an alignment consist-
ing of only insertions and deletions and therefore
up to twice as long as the alignments in the gold
standard. Finally, we also report the total number
of alignments (word pairs) which are not exactly
equal to the alignments of the gold standard.
The results are shown in Table 1. We can
clearly see that all algorithms beat the baseline
and align about 95% of all string pairs correctly.
While the Levenshtein PMI algorithm aligns most
strings perfectly, it misaligns slightly more indi-
vidual segments than the PHMM and the Leven-
shtein algorithm with the swap operation (i.e. it
makes more segment alignment errors per word
pair). The VC-sensitive Levenshtein algorithm
in general performs slightly worse than the other
three algorithms.
5.2 Qualitative results
Let us first note that it is almost impossible for
any algorithm to achieve a perfect overlap with the
gold standard, because the gold standard was gen-
erated from multiple alignments and therefore in-
corporates other constraints. For example, while a
certain pairwise alignment could appear correct in
aligning two consonants, the multiple alignment
could show contextual support (from pronuncia-
tions in other varieties) for separating the conso-
nants. Consequently, all algorithms discussed be-
low make errors of this kind.
In general, the specific errors of the VC-
sensitive Levenshtein algorithm can be separated
into three cases. First, as we illustrated in section
3.3, the VC-sensitive Levenshtein algorithm has
no way to distinguish between aligning a conso-
nant with one of two neighboring consonants and
sometimes chooses the wrong one (this also holds
for vowels). Second, it does not allow alignments
of vowels with consonants and therefore cannot
detect correct vowel-consonant alignments such as
correspondences of [u] with [v] initially. Third,
for the same reason the VC-sensitive Levenshtein
algorithm is also not able to detect metathesis of
vowels with consonants.
The misalignments of the Levenshtein algo-
rithm with the swap-operation can also be split in
three cases. It suffers from the same two prob-
lems as the VC-sensitive Levenshtein algorithm in
choosing to align a consonant incorrectly with one
of two neighboring consonants and not being able
to align a vowel with a consonant. Third, even
though it aligns some of the metathesis cases cor-
rectly, it also makes some errors by incorrectly ap-
plying the swap-operation. For example, consider
the alignment of [s"irjIni] and [s"irjnI], two Bul-
garian dialectal variations of the word ?cheese?, in
which the swap-operation is applied:
32
Algorithm Misaligned segments (error rate) Incorrect alignments (%)
Baseline (Hamming algorithm) 2510094 (0.1579) 726844 (20.92%)
VC-sens. Levenshtein algorithm 490703 (0.0309) 191674 (5.52%)
Levenshtein PMI algorithm 399216 (0.0251) 156440 (4.50%)
Levenshtein swap algorithm 392345 (0.0247) 161834 (4.66%)
Pair Hidden Markov Model 362423 (0.0228) 160896 (4.63%)
Table 1: Comparison to gold standard alignments. All differences are significant (p < 0.01).
s "i rj I n i
s "i rj n I
0 0 0 >< 1 1
However, the two I?s are not related and should not
be swapped, which is reflected in the gold standard
alignment:
s "i rj I n i
s "i rj n I
0 0 0 1 0 1
The incorrect alignments of the Levenshtein
algorithm with the PMI-generated segment dis-
tances are mainly caused by its inability to align
vowels with consonants and therefore, just as the
VC-sensitive Levenshtein algorithm, it fails to de-
tect metathesis. On the other hand, using seg-
ment distances often solves the problem of select-
ing which of two plausible neighbors a consonant
should be aligned with.
Because the PHMM employs segment substi-
tution probabilities, it also often solves the prob-
lem of aligning a consonant to one of two neigh-
bors. In addition, the PHMM often correctly
aligns metathesis involving equal as well as sim-
ilar symbols, even realizing an improvement over
the Levenshtein swap algorithm. Unfortunately,
many wrong alignments of the PHMM are also
caused by allowing vowel-consonant alignments.
Since the PHMM does not take context into ac-
count, it also aligns vowels and consonants which
often play a role in metathesis when no metathesis
is involved.
6 Discussion
This study provides an alternative evaluation of
string distance algorithms by focusing on their ef-
fectiveness in aligning segments. We proposed,
implemented, and tested the new procedure on a
substantial body of data. This provides a new per-
spective on the quality of distance and alignment
algorithms as they have been used in dialectology,
where aggregate comparisons had been at times
frustratingly inconclusive.
In addition, we introduced the PMI weight-
ing within the Levenshtein algorithm as a sim-
ple means of obtaining segment distances, and
showed that it improves on the popular Leven-
shtein algorithm with respect to alignment accu-
racy.
While the results indicated that the PHMM mis-
aligned the fewest segments, training the PHMM
is a lengthy process lasting several hours. Con-
sidering that the Levenshtein algorithm with the
swap operation and the Levenshtein algorithm
with the PMI-generated segment distances are
much quicker to (train and) apply, and that they
have only slightly lower performance with respect
to the segment alignments, we actually prefer us-
ing those methods. Another argument in favor of
using one of these Levenshtein algorithms is that
it is a priori clearer what type of alignment errors
to expect from them, while the PHMM algorithm
is less predictable and harder to comprehend.
While our results are an indication of the good
quality of the evaluated algorithms, we only evalu-
ated the algorithms on a single dataset for which a
gold standard was available. Ideally we would like
to verify these results on other datasets, for which
gold standards consisting of multiple or pairwise
alignments are available.
Acknowledgements
We are grateful to Peter Kleiweg for extending the
Levenshtein algorithm in the L04 package with the
swap-operation. We also thank Greg Kondrak for
providing the original source code of the Pair Hid-
den Markov Models. Finally, we thank Therese
Leinonen and Sebastian Ku?rschner of the Univer-
sity of Groningen and Esteve Valls i Alecha of the
University of Barcelona for their useful feedback
on our ideas.
33
References
Leonard E. Baum, Ted Petrie, George Soules, and Nor-
man Weiss. 1970. A maximization technique occur-
ring in the statistical analysis of probabilistic func-
tions of Markov Chains. The Annals of Mathemati-
cal Statistics, 41(1):164?171.
Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16(1):22?29.
Fred J. Damerau. 1964. A technique for computer de-
tection and correction of spelling errors. Communi-
cations of the ACM, 7:171?176.
Richard Durbin, Sean R. Eddy, Anders Krogh, and
Graeme Mitchison. 1998. Biological Sequence
Analysis: Probabilistic Models of Proteins and Nu-
cleic Acids. Cambridge University Press, United
Kingdom, July.
Richard Hamming. 1950. Error detecting and error
correcting codes. Bell System Technical Journal,
29:147?160.
Wilbert Heeringa, Peter Kleiweg, Charlotte Gooskens,
and John Nerbonne. 2006. Evaluation of string dis-
tance algorithms for dialectology. In John Nerbonne
and Erhard Hinrichs, editors, Linguistic Distances,
pages 51?62, Shroudsburg, PA. ACL.
Wilbert Heeringa. 2004. Measuring Dialect Pronunci-
ation Differences using Levenshtein Distance. Ph.D.
thesis, Rijksuniversiteit Groningen.
Brett Kessler. 1995. Computational dialectology in
Irish Gaelic. In Proceedings of the seventh con-
ference on European chapter of the Association for
Computational Linguistics, pages 60?66, San Fran-
cisco, CA, USA. Morgan Kaufmann Publishers Inc.
Grzegorz Kondrak and Bonnie Dorr. 2003. Identifica-
tion of Confusable Drug Names: A New Approach
and Evaluation Methodology. Artificial Intelligence
in Medicine, 36:273?291.
Grzegorz Kondrak. 2003. Phonetic Alignment and
Similarity. Computers and the Humanities, 37:273?
291.
Vladimir Levenshtein. 1965. Binary codes capable of
correcting deletions, insertions and reversals. Dok-
lady Akademii Nauk SSSR, 163:845?848.
Wesley Mackay and Grzegorz Kondrak. 2005. Com-
puting word similarity and identifying cognates with
Pair Hidden Markov Models. In Proceedings of
the 9th Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 40?47, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
John Nerbonne and Peter Kleiweg. 2007. Toward a di-
alectological yardstick. Journal of Quantitative Lin-
guistics, 14:148?167.
Bruno Pouliquen. 2008. Similarity of names across
scripts: Edit distance using learned costs of N-
Grams. In Bent Nordstro?m and Aarne Ranta, ed-
itors, Proceedings of the 6th international Con-
ference on Natural Language Processing (Go-
Tal?2008), volume 5221, pages 405?416.
Jelena Prokic?, Martijn Wieling, and John Nerbonne.
2009. Multiple sequence alignments in linguistics.
In Piroska Lendvai and Lars Borin, editors, Proceed-
ings of the EACL 2009 Workshop on Language Tech-
nology and Resources for Cultural Heritage, Social
Sciences, Humanities, and Education.
Lawrence R. Rabiner. 1989. A tutorial on Hidden
Markov Models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2):257?
286.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learn-
ing string-edit distance. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 20:522?
532.
Robert Wagner and Roy Lowrance. 1975. An exten-
sion of the string-to-string correction problem. Jour-
nal of the ACM, 22(2):177?183.
Martijn Wieling and John Nerbonne. 2007. Dialect
pronunciation comparison and spoken word recog-
nition. In Petya Osenova, editor, Proceedings of
the RANLPWorkshop on Computational Phonology,
pages 71?78.
Martijn Wieling, Therese Leinonen, and John Ner-
bonne. 2007. Inducing sound segment differences
using Pair Hidden Markov Models. In Mark Ellison
John Nerbonne and Greg Kondrak, editors, Comput-
ing and Historical Phonology: 9th Meeting of the
ACL Special Interest Group for Computational Mor-
phology and Phonology, pages 48?56.
34
Proceedings of the 11th Meeting of the ACL-SIGMORPHON, ACL 2010, pages 46?53,
Uppsala, Sweden, 15 July 2010.
c?2010 Association for Computational Linguistics
Exploring dialect phonetic variation using PARAFAC
Jelena Proki
?
c
University of Groningen
The Netherlands
j.prokic@rug.nl
Tim Van de Cruys
University of Groningen
The Netherlands
t.van.de.cruys@rug.nl
Abstract
In this paper we apply the multi-way de-
composition method PARAFAC in order to
detect the most prominent sound changes
in dialect variation. We investigate various
phonetic patterns, both in stressed and un-
stressed syllables. We proceed from regu-
lar sound correspondences which are auto-
matically extracted from the aligned tran-
scriptions and analyzed using PARAFAC.
This enables us to analyze simultaneously
the co-occurrence patterns of all sound
correspondences found in the data set and
determine the most important factors of
the variation. The first ten dimensions are
examined in more detail by recovering the
geographical distribution of the extracted
correspondences. We also compare dia-
lect divisions based on the extracted cor-
respondences to the divisions based on the
whole data set and to the traditional schol-
arship as well. The results show that PAR-
AFAC can be successfully used to detect
the linguistic basis of the automatically
obtained dialect divisions.
1 Introduction
Dialectometry is a multidisciplinary field that uses
quantitative methods in the analysis of dialect
data. From the very beginning, most of the re-
search in dialectometry has been focused on the
identification of dialect groups and development
of methods that would tell us how similar (or dif-
ferent) one variety is when compared to the neigh-
boring varieties. Dialect data is usually analyzed
on the aggregate level by summing up the differ-
ences between various language varieties into a
single number. The main drawback of aggregate
analyses is that it does not expose the underlying
linguistic structure, i.e. the specific linguistic ele-
ments that contributed to the differences between
the dialects. In recent years there have been sev-
eral attempts to automatically extract linguistic
basis from the aggregate analysis, i.e. to determine
which linguistic features are responsible for which
dialect divisions. Although interesting for dialect-
ology itself, this kind of research is very important
in the investigation of sound variation and change,
both on the synchronic and diachronic level.
The paper is structured as follows. In the next
section, we discuss a number of earlier approaches
to the problem of identifying underlying linguistic
structure in dialect divisions. In section 3, we give
a description of the dialect data used in this re-
search. Section 4 then describes the methodology
of our method, explaining our data representation
using tensors, our three-way factorization method,
and the design of our data set. In section 5, the res-
ults of our method are discussed, examining the
values that come out of our factorization method
in a number of ways. Section 6, then, draws con-
clusions and gives some pointers for future work.
2 Previous work
In order to detect the linguistic basis of dialect
variation Nerbonne (2006) applied factor analysis
to the results of the dialectometric analysis of
southern American dialects. The analysis is based
on 1132 different vowels found in the data. 204
vowel positions are investigated, where a vowel
position is, e.g., the first vowel in the word ?Wash-
ington? or the second vowel in the word ?thirty?.
Factor analysis has shown that 3 factors are most
important, explaining 35% of the total amount of
variation. However, this approach is based only on
vowel positions in specific words.
Proki?c (2007) extracted the 10 most frequent
non-identical sound correspondences from the
aligned word transcriptions. Based on the relative
frequency of each of these correspondences each
site in the data set was assigned a correspondence
index. Higher value of this index indicates sites
46
where the presence of a certain sound is domin-
ant with respect to some sound alternation. Al-
though successful in describing some important
sound alternations in the dialect variation, it ex-
amines only the 10 most frequent sound alterna-
tions without testing patterns of variation between
different sound correspondences.
Shackleton (2007) applies principal compon-
ent analysis (PCA) to a group of self constructed
articulation-based features. All segments found in
the data are translated into vectors of numerical
features and analyzed using PCA. Based on the
component scores for features, different groups
of varieties (in which a certain group of features
is present) are identified. We note that the main
drawback of this approach is the subjectivity of the
feature selection and segment quantification.
Wieling and Nerbonne (2009) used a bipart-
ite spectral graph partitioning method to simul-
taneously cluster dialect varieties and sound cor-
respondences. Although promising, this method
compares the pronunciation of every site only to
the reference site, rather than comparing it to all
other sites. Another drawback of this method is
that it does not use any information on the frequen-
cies of sound correspondences, but instead em-
ploys binary features to represent whether a cer-
tain correspondence is present at a certain site or
not.
In this paper we present an approach that tries
to overcome some of the problems described in
the previous approaches. It proceeds from auto-
matically aligned phonetic transcriptions, where
pronunciations of every site are compared to the
corresponding pronunciations for all other sites.
Extracted sound correspondences are analyzed us-
ing the multi-way decomposition method PARA-
FAC. The method allows us to make generaliza-
tions over multi-way co-occurrence data, and to
look simultaneously at the co-occurrence patterns
of all sound correspondences found in the data set.
3 Data description
The data set used in this paper consists of phon-
etic transcriptions of 152 words collected at 197
sites evenly distributed all over Bulgaria. It is part
of the project Buldialect ? Measuring Linguistic
unity and diversity in Europe. Phonetic transcrip-
tions include various diacritics and suprasegment-
als, making the total number of unique phones in
the data set 95: 43 vowels and 52 consonants.
1
The sign for primary stress is moved to a cor-
responding vowel, so that there is a distinction
between stressed and unstressed vowels. Vowels
are also marked for their length. Sonorants /r/ and
/l/ have a mark for syllabicity and for stress in case
they are syllabic. Here we list all phones present
in the data set:
"A, e, i, "e, @, "E, 7, "6, A, I, o, "o, u, "A:, U, "u:, "7, "@,
"a, "i, "I, "e:, E, "O, "2, "i:, "u, e:, 1, "1, "o:, "E:, "7:, u:, A:,
y, "a:, a, o:, 7:, "U, "y, "I:, j, g, n, n
j
, ?, r, w, x, r
j
, h,
C, f, s, v, c?, F, p,
>
?, m, k,
>
?C, p
j
, c, l, l
j
, t, t
j
, S, d, d
j
,
"r
"
, v
j
,
>
dz, Z, ?,
>
?, r
"
, c
j
, z, s
j
, b, g
j
, m
j
, l
"
, z
j
, "l
"
, k
j
, b
j
,
>
??,
>
dz, f
j
, ?
Each of the 152 words in the data set shows
phonetic variation, with some words displaying
more than one change. There are in total 39 dif-
ferent dialectal features that are represented in the
data set, with each of the features being present in
a similar number of words. For example, the re-
flexes of Old Bulgarian vowels that show dialect
variation are represented with the same or nearly
the same number of words. A more detailed de-
scription of all features can be found in Proki?c et
al. (2009). For all villages only one speaker was
recorded. In the data set, for some villages there
were multiple pronunciations of the same word. In
this reasearch we have randomly picked only one
per every village.
4 Methodology
4.1 Tensors
Co-occurrence data (such as the sound corres-
pondences used in this research) are usually rep-
resented in the form of a matrix. This form is per-
fectly suited to represent two-way co-occurrence
data, but for co-occurrence data beyond two
modes, we need a more general representation.
The generalization of a matrix is called a tensor.
A tensor is able to encode co-occurrence data of
any n modes. Figure 1 shows a graphical com-
parison of a matrix and a tensor with three modes
? although a tensor can easily be generalized to
more than three modes.
Tensor operations come with their own algeb-
raic machinery. We refer the interested reader to
Kolda and Bader (2009) for a thorough and in-
sightful introduction to the subject.
1
The data is publicly available and can be down-
loaded from http://www.bultreebank.org/
BulDialects/index.html
47




	
	





Figure 1: Matrix representation vs. tensor representation.
4.2 PARAFAC
In order to create a succinct and generalized
model, the co-occurrence data are often ana-
lyzed with dimensionality reduction techniques.
One of the best known dimensionality reduction
techniques is principal component analysis (PCA,
Pearson (1901)). PCA transforms the data into a
new coordinate system, yielding the best possible
fit in a least squares sense given a limited num-
ber of dimensions. Singular value decomposition
(SVD) is the generalization of the eigenvalue de-
composition used in PCA (Wall et al, 2003).
To be able to make generalizations among the
three-way co-occurrence data, we apply a statist-
ical dimensionality reduction technique called par-
allel factor analysis (PARAFAC, Harshman (1970);
Carroll and Chang (1970)), a technique that has
been sucessfully applied in areas such as psycho-
logy and bio-chemistry. PARAFAC is a multilinear
analogue of SVD. The key idea is to minimize the
sum of squares between the original tensor and the
factorized model of the tensor. For the three mode
case of a tensor T ? R
D
1
?D
2
?D
3
this gives the
objective function in 1, where k is the number of
dimensions in the factorized model and ? denotes
the outer product.
min
x
i
?R
D1
,y
i
?R
D2
,z
i
?R
D3
? T ?
k
?
i=1
x
i
? y
i
? z
i
?
2
F
(1)
The algorithm results in three matrices, indic-
ating the loadings of each mode on the factorized
dimensions. The model is represented graphically
in Figures 2 and 3. Figure 2 visualizes the fact
that the PARAFAC decomposition consists of the
summation over the outer products of n (in this
case three) vectors. Figure 3 represents the three
resulting matrices that come out of the factoriza-
tion, indicating the loadings of each mode on the
factorized dimensions. We will be using the latter
representation in our research.
Computationally, the PARAFAC model is fitted
by applying an alternating least-squares algorithm.
In each iteration, two of the modes are fixed and
the third one is fitted in a least squares sense. This
process is repeated until convergence.
2
4.3 Sound correspondences
In order to detect the most important sound vari-
ation within Bulgarian dialects, we proceed from
extracting all sound correspondences from the
automatically aligned word transcriptions. All
transcriptions were pairwise aligned using the
Levenshtein algorithm (Levenshtein, 1965) as im-
plemented in the program L04.
3
The Leven-
shtein algorithm is a dynamic programming al-
gorithm used to measure the differences between
two strings. The distance between two strings is
the smallest number of insertions, deletions, and
substitutions needed to transform one string to the
other. In this work all three operations were as-
signed the same value, namely 1. The algorithm is
also directly used to align two sequences. An ex-
ample showing two aligned pronunciations of the
word v?lna /v7lna/ ?wool? is given in Figure 4.
4
v "7 - n A
v "A l n @
Figure 4: Example of two pairwise aligned word
transcriptions.
From the aligned transcriptions for all words
and all villages in the data set we first extracted
2
The algorithm has been implemented in MATLAB, using
the Tensor Toolbox for sparse tensor calculations (Bader and
Kolda, 2009).
3
http://www.let.rug.nl/kleiweg/L04
4
For some pairs of transcriptions there are two or more
possible alignments, i.e. alignments that have the same cost.
In these cases we have randomly picked only one of them.
48
Figure 2: Graphical representation of PARAFAC as the sum of outer products.

	



	



	




	





Figure 3: Graphical representation of the PARAFAC as three loadings matrices.
all corresponding non-identical sounds. For ex-
ample, from the aligned transcriptions in Figure 4
we would extract the following sound pairs: ["7]-
["A], [-]-[l], [A]-[@]. The hyphen (?-?) stands for a
missing (i.e. inserted or deleted) sound, and in fur-
ther analyses it is treated the same as any sound
in the data set. For each pair of corresponding
sounds from the data set we counted how often it
appeared in the aligned transcriptions for each pair
of villages separately. In total we extracted 907
sound correspondences and stored the information
on each of them in a separate matrix. Every matrix
records the distances between each two villages
in the data set, measured as the number of times
a certain phonetic alternation is recorded while
comparing pronunciations from these sites.
Since we are interested in analyzing all sound
correspondences simultaneously, we merged the
information from all 907 two-mode matrices into
a three-mode tensor n?n?v, where n represents
the sites in the data set, and v represents the sound
alternations. By arranging our data in a cube in-
stead of a matrix, we are able to look into several
sets of variables simultaneously. We are especially
interested in the loadings for the third mode, that
contains the values for the sound correspondences.
5 Results
In order to detect the most prominent sound cor-
respondences we analyzed the three-mode tensor
described in the previous section using a PARAFAC
factorization with k = 10 dimensions. In Table 5
we present only the first five dimensions extracted
by the algorithm. The final model fits 44% of the
original data. The contribution of the first extrac-
ted dimension (dim1) to the final fit of the model
is the largest ? 23.81 per cent ? while the next four
dimensions contribute to the final fit with similar
percentages: dim2 with 10.63 per cent, dim3 with
9.50 per cent, dim4 with 9.26 per cent, and dim5
with 9.09 per cent. Dimensions six to ten contrib-
ute in the range from 8.66 per cent to 6.98 per cent.
For every dimension we extracted the twenty
sound correspondences with the highest scores. In
the first dimension we find 11 pairs involving vow-
els and 9 involving consonant variation. The three
sound correspondences with the highest scores
are the [A]-[@], [o]-[u], and [e]-[i] alternations.
This finding corresponds well with the traditional
scholarly views on Bulgarian phonetics (Wood
and Pettersson, 1988; Barnes, 2006) where we find
that in unstressed syllables mid vowels [e] and [o]
raise to neutralize with the high vowels [i] and [u].
The low vowel [a] raises to merge with [@].
For every sound alternation we also check their
geographical distribution. We do so by applying
the following procedure. From the aligned pairs
of transcriptions we extract corresponding pairs of
sounds for every alternation. We count how many
times each of the two sounds appears in the tran-
scriptions for every village. Thus, for every pair of
sound correspondences, we can create two maps
that show the distribution of each of the sounds
separately. On the map of Bulgaria these values
49
Table 1: First five dimensions for the sound cor-
respondences.
dim1 dim2 dim3 dim4 dim5
[A]-[@] [@]-[7] [u]-[o] [A]-[@] [e]-[i]
[u]-[o] [e]-[i] [A]-[7] [@]-[7] [i]-["e]
[e]-[i] ["e]-["E] [A]-[@] [U]-[o] [e]-[@]
[-]-[j] [-]-[j] [7]-[e] [e]-[@] [r]-[r
j
]
[e]-["e] [S]-[C] [e]-["e] [d]-[d
j
] [d]-[d
j
]
[S]-[C] [
>
?]-[
>
?C] ["e]-["E] [v]-[v
j
] ["e]-["A]
[
>
?]-[
>
?C] ["A]-["E] [-]-[j] [n]-[n
j
] [-]-[j]
["e]-["E] [r]-[r
j
] ["e]-["A] [-]-[j] ["o]-[u]
[n]-[n
j
] [l]-[l
j
] [e]-[i] ["e]-["E] [l]-[l
j
]
[A]-[7] [e]-[@] [n]-[n
j
] [l]-[l
j
] [v]-[v
j
]
[e]-[@] [d]-[d
j
] [r]-[r
j
] [t]-[t
j
] [u]-[o]
["A]-["E] [n]-[n
j
] [
>
?]-[
>
?C] ["e]-["A] [n]-[n
j
]
["e]-["A] [u]-[U] ["7]-["A] [e]-["e] [-]-[v]
[d]-[d
j
] ["7]-["O] [-]-[r] [S]-[C] ["7]-[@]
[7]-[e] [@]-["A] [S]-[C] [
>
?]-[
>
?C] [u]-[U]
[l]-[l
j
] [7]-[e] [l]-[l
j
] [r]-[r
j
] [
>
?]-[
>
?C]
[v]-[v
j
] ["o]-[u] [u]-[e] [p]-[p
j
] ["A]-["E]
[r]-[r
j
] [Z]-[?] [-]-["7] [Z]-[?] [A]-["7]
[Z]-[?] [i]-[@] [v]-[-] [@]-["A] [@]-["A]
["7]-["O] [v]-[v
j
] [A]-["7] [e]-[i] [b]-[b
j
]
are represented using a gradual color, which en-
ables us to see not only the geographic distribution
of a certain sound but also how regular it is in a
given sound alternation. The highest scoring sites
are coloured black and the lowest scoring sites are
coloured white.
In Figure 5 we see the geographical distribu-
tion of the first three extracted correspondences.
The first two alternations [A]-[@] and [o]-[u] have
almost the same geographical distribution and di-
vide the country into west and east. While in the
west there is a clear presence of vowels [A] and [o],
in the east those vowels would be pronounced as
[@] and [u]. The division into east and west corres-
ponds well with the so-called jat line, which is,
according to traditional dialectologists (Stojkov,
2002) the main dialect border in Bulgaria. On the
maps in Figure 5 we represent it with the black line
that roughly divides Bulgaria into east and west.
The third correspondence follows a slightly dif-
ferent pattern: mid vowel [e] is present not only
west of the jat line, but also in the southern part
of the country, in the region of Rodopi mountains.
In the central and northeastern areas this sound is
pronounced as high vowel [i]. For all three sound
correspondences we see a clear two-way division
of the country, with almost all sites being charac-
terized by one of the two pronunciations, which,
as we shall see later, is not always the case due
to multiple reflections of some sounds at certain
positions.
We also note that the distribution of the sound
correspondences that involve soft consonants and
their counterparts have the same east-west dis-
tribution (see Figure 6). In the first dimension
we find the following consonants and their pal-
atal counterparts [n], [d], [l], [v] and [r], but be-
cause of space limitations we show maps only
for three correspondences. The east-west division
also emerges with respect to the distribution of the
[A]-[7] and ["e]-["A] sounds.
Unlike the correspondences mentioned before,
the [S]-[C], [
>
?]-[
>
?C], and [Z]-[?] pairs are defining
the south part of the country as a separate zone.
As shown on the maps in Figure 7, the southern
part of the country (the region of Rodopi moun-
tains) is characterized by a soft pronunciation of
[S], [
>
?] and [Z]. In traditonal literature on Bul-
garain dialectology (Stojkov, 2002), we also find
that soft pronunciation of [S], [
>
?] and [Z] is one of
the most important phonetic features of the variet-
ies in the Rodopi zone. Based on the correspond-
ences extracted in the first dimension, this area
is also defined by the presence of the vowel ["E]
in stressed syllables (["e]-["E] and ["A]-["E] corres-
pondences).
In some extracted correspondences, only one of
the sounds has a geographically coherent distri-
bution, like in the case of the [7]-[e] pair where
[e] is found in the west and south, while the [7]
sound is only sporadically present in the central
region. This kind of asymmetrical distribution is
also found with respect to the pair [A]-[7].
Most of the sound correspondences in the first
dimension either divide the country along the jat
line or separate the Rodopi area from the rest of the
varieties. The only two exceptions are the [-]-[j]
and ["7]-["O] pairs. They both define the southwest
area as a separate zone, while the northwest shares
its pronunciation of the sound in question with the
eastern part of the country.
We use the first 20 correspondences from the
first dimension and perform k-means clustering in
order to check which dialect areas would emerge
based on this limited set of sound correspond-
50
Figure 5: [A]-[@] (left), [o]-[u] (middle), [e]-[i] (right) sound correspondences.
Figure 6: [d]-[d
j
] (left), [v]-[v
j
] (middle), [r]-[r
j
] (right) sound correspondences.
ences. The results of the 2-way, 3-way and 4-way
clustering are given in Figure 8.
In two-way clustering the algorithm detects an
east-west split approximately along the jat line,
slightly moved to the east. This fully corres-
ponds to the traditional dialectology but also to
the results obtained using Levenshtein algorithm
on the whole data set where only east, west and
south varieties could be asserted with great con-
fidence (Proki?c and Nerbonne, 2008). In Figure 9
we present the dialect divisions that we get if the
distances between the sites are calculated using
whole word transcriptions instead of only the 20
most prominent sound correspondences. We no-
tice a high correspondence between the two ana-
lyses at the two- and three-level division. On the
level of four and more groups, the two analyses
start detecting different groups. In the analysis
based on 20 sound correspondences, southern dia-
lects are divided into smaller and smaller groups,
while in the analysis based on the whole data set,
the area in the west ? near the Serbian border ?
emerges as the fourth group. This is no surprise, as
the first 20 extracted correspondences do not con-
tain any sounds typical only for this western area.
In order to compare two divisions of sites, we
calculated the adjusted Rand index (Hubert and
Arabie, 1985). The adjusted Rand index (ARI) is
used in classification for comparing two different
partitions of a finite set of objects. It is based on
the Rand index (Rand, 1971), one of the most pop-
ular measures for comparing the degree to which
partitions agree (in classification). Value 1 of the
ARI indicates that two classifications match per-
fectly, while value 0 means that two partitions do
not agree on any pair of points. For both two-
level and three-level divisions of the sites the ARI
for two classifications is 0.84. We also compared
51
Figure 7: [S]-[C] (left), [
>
?]-[
>
?C] (middle), [Z]-[?] (right) sound correspondences.
Figure 8: Dialect varieties detected by k-means clustering algorithm based on the first 20 sound corres-
pondences in the first dimension.
Figure 9: Dialect varieties detected by k-means clustering algorithm based on all word transcriptions.
both of the classifications to the classification of
the sites done by Stojkov (2002). For the classi-
fication based on the first dimension extracted by
PARAFAC, ARI is 0.73 for two-way and 0.64 for
the three-way division. ARI score for the clas-
sification based on whole word transcriptions is
0.69 for two-way and 0.62 for three-way. As in-
dicated by ARI the two classifications correspond
with a high degree to each other, but to the tra-
ditional classification as well. We note that two-
way classification based on the extracted sound
correspondences corresponds higher to the tradi-
tional classification than classification that takes
all sounds into account.
We conclude that the sound correspondences
detected by PARAFAC form the linguistic basis
of the two-way and three-way divisions of Bul-
garian dialect area. Using the PARAFAC method
we are able to detect that the most important sound
changes on which two-way division is based are
[o]-[u], [A]-[@] and palatal pronunciation of con-
sonants. In the three-way division of sites done
by k-means, the area in the south of the country
appears as the third most important dialect zone.
In the twenty investigated sound correspondences
we find that the soft pronunciation of [S],[
>
?] and
[Z] sounds is typical only for the varieties in this
area. Apart from divisions that divide the country
into west and east, including the southern variet-
ies, we also detect sound correspondences whose
distribution groups together western and southern
areas.
We also analyzed in more depth sound corres-
pondences extracted in other dimensions by the
PARAFAC algorithm. Most of the correspondences
found in the first dimension, also reappear in the
following nine dimensions. Closer inspection of
the language groups obtained using information
52
from these dimensions show that eastern, western
and southern varieties are the only three that are
identified. No other dialect areas were detected
based on the sound correspondences from these
nine dimensions.
6 Conclusion
In this paper we have applied PARAFAC in the task
of detecting the linguistic basis of dialect phonetic
variation. The distances between varieties were
expressed as a numerical vector that records in-
formation on all sound correspondences found in
the data set. Using PARAFAC we were able to ex-
tract the most important sound correspondences.
Based on the 20 most important sound correspond-
ences we performed clustering of all sites in the
data set and were able to detect three groups of
sites. As found in traditional literature on Bul-
garian dialects, these three dialects are the main
dialect groups in Bulgaria. Using the aggregate
approach on the same data set, the same three dia-
lects were the only groups in the data that could be
asserted with high confidence. We conclude that
this approach is successful in extracting underly-
ing linguistic structure in dialect variation, while
at the same time overcoming some of the problems
found in the earlier approaches to this problem.
In future work sounds in the data set could be
defined in a more sophisticated way, using some
kind of feature representation. Also, the role of
stress should be examined in more depth, since
there are different patterns of change in stressed
in unstressed syllables. We would also like to ex-
tend the method and examine more than just two
sound correspondences at a time.
References
Brett W. Bader and Tamara G. Kolda. 2009. Matlab
tensor toolbox version 2.3. http://csmr.ca.
sandia.gov/
?
tgkolda/TensorToolbox/,
July.
Jonathan Barnes. 2006. Strength and Weakness at
the Interface: Positional Neutralization in Phonetics
and Phonology. Walter de Gruyter GmbH, Berlin.
J. Douglas Carroll and Jih-Jie Chang. 1970. Analysis
of individual differences in multidimensional scal-
ing via an n-way generalization of ?eckart-young?
decomposition. Psychometrika, 35:283?319.
Richard A. Harshman. 1970. Foundations of the par-
afac procedure: models and conditions for an ?ex-
planatory? multi-mode factor analysis. In UCLA
Working Papers in Phonetics, volume 16, pages 1?
84, Los Angeles. University of California.
Lawrence Hubert and Phipps Arabie. 1985. Compar-
ing partitions. Journal of Classification, 2:193?218.
Tamara G. Kolda and Brett W. Bader. 2009. Tensor de-
compositions and applications. SIAM Review, 51(3),
September.
Vladimir Levenshtein. 1965. Binary codes capable of
correcting deletions, insertions and reversals. Dok-
lady Akademii Nauk SSSR, 163:845?848.
John Nerbonne. 2006. Identifying linguistic structure
in aggregate comparison. Literary and Linguistic
Computing, 21(4):463?476.
Karl Pearson. 1901. On lines and planes of closest
fit to systems of points in space. Philosophical
Magazine, 2(6):559?572.
Jelena Proki?c and John Nerbonne. 2008. Recogniz-
ing groups among dialects. International Journal of
Humanities and Arts Computing, Special Issue on
Language Variation ed. by John Nerbonne, Char-
lotte Gooskens, Sebastian K?urschner, and Ren?ee van
Bezooijen:153?172.
Jelena Proki?c, John Nerbonne, Vladimir Zhobov, Petya
Osenova, Krili Simov, Thomas Zastrow, and Erhard
Hinrichs. 2009. The Computational Analysis of
Bulgarian Dialect Pronunciation. Serdica Journal of
Computing, 3:269?298.
Jelena Proki?c. 2007. Identifying linguistic structure
in a quantitative analysis of dialect pronunciation.
In Proceedings of the ACL 2007 Student Research
Workshop, pages 61?66.
WilliamM. Rand. 1971. Objective criteria for the eval-
uation of clustering methods. Journal of American
Statistical Association, 66(336):846?850, Decem-
ber.
Robert G. Shackleton. 2007. Phonetic variation in the
traditional English dialects. Journal of English Lin-
guistics, 35(1):30?102.
Stojko Stojkov. 2002. Bulgarska dialektologiya.
Sofia, 4th ed.
Michael E. Wall, Andreas Rechtsteiner, and Luis M.
Rocha, 2003. Singular Value Decomposition and
Principal Component Analysis, chapter 5, pages 91?
109. Kluwer, Norwell, MA, Mar.
Martijn Wieling and John Nerbonne. 2009. Bipart-
ite spectral graph partitioning to co-cluster varieties
and sound correspondences in dialectology. In Text
Graphs 4, Workshop at the 47th Meeting of the Asso-
ciation for Computational Linguistics, pages 14?22.
Sidney A. J. Wood and Thore Pettersson. 1988. Vowel
reduction in Bulgarian: the phonetic data and model
experiments. Folia Linguistica, 22(3-4):239?262.
53
Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, pages 1?6,
Avignon, France, April 23 - 24 2012. c?2012 Association for Computational Linguistics
Visualization of Linguistic Patterns
and
Uncovering Language History from Multilingual Resources
Miriam Butt1 Jelena Prokic?2 Thomas Mayer2 Michael Cysouw3
1Department of Linguistics, University of Konstanz
2Research Unit Quantitative Language Comparison, LMU Munich
3Research Center Deutscher Sprachatlas, Philipp University of Marburg
1 Introduction
The LINGVIS and UNCLH (Visualization of Lin-
guistic Patterns & Uncovering Language His-
tory from Multilingual Resources) were originally
conceived of as two separate workshops. Due to
perceived similarities in content, the two work-
shops were combined and organized jointly.
The overal aim of the joint workshop was to
explore how methods developed in computational
linguistics, statistics and computer science can
help linguists in exploring various language phe-
nomena. The workshop focused particularly on
two topics: 1) visualization of linguistic patterns
(LINGVIS); 2) usage of multilingual resources in
computational historical linguistics (UNCLH).
2 LINGVIS
The overall goal of the first half of the work-
shop was to bring together researchers work-
ing within the emerging subfield of computa-
tional linguistics ? using methods established
within Computer Science in the fields of Infor-
mation Visualization (InfoVis) and Visual Ana-
lytics in conjunction with methodology and anal-
yses from theoretical and computational linguis-
tics. Despite the fact that statistical methods for
language analysis have proliferated in the last
two decades, computational linguistics has so far
only marginally availed itself of techniques from
InfoVis and Visual Analytics (e.g., Honkela et
al. (1995); Neumann et al (2007); Collins et
al. (2009); Collins (2010); Mayer et al (2010a);
Mayer et al (2010b); Rohrdantz et al (2011)).
The need to integrate methods from InfoVis and
Visual Analytics arises particularly with respect
to situations in which the amount of data to be
analyzed is huge and the interactions between rel-
evant features are complex. Both of these situ-
ations hold for much of current (computational)
linguistic analysis. The usual methods of sta-
tistical analysis do not allow for quick and easy
grasp and interpretation of the patterns discovered
through statistical processing and an integration
of innovative visualization techniques has become
imperative.
The overall aim of the first half of the workshop
was thus to draw attention to this need and to the
newly emerging type of work that is beginning to
respond to the need. The workshop succeeded in
bringing together researchers interesting in com-
bining techniques and methodology from theo-
retical and computational linguistics with InfoVis
and Visual Analytics.
Three of the papers in the workshop focused
on the investigation and visualization of lexical
semantics. Rohrdantz et al present a diachronic
study of fairly recently coined derivational suf-
fixes (-gate, -geddon, -athon) as used in newspa-
per corpora across several languages. Their anal-
ysis is able to pin-point systematic differences in
contextual use as well as some first clues as to
how and why certain new coinages spread bet-
ter than others. Heylen et al point out that me-
thods such as those used in Rohrdantz et al,
while producing interesting results, are essentially
black boxes for the researchers ? it is not clear
exactly what is being calculated. Their paper
presents some first steps towards making the black
box more transparent. In particular, they take
a close look at individual tokens and their se-
mantic use with respect to Dutch synsets. Cru-
cially, they anticipate an interactive visualization
that will allow linguistically informed lexicogra-
1
phers to work with the available data and patterns.
A slightly different take on synset relations is pre-
sented by Lohk et al, who use visualization me-
thods to help identify errors in WordNets across
different languages.
Understanding differences and relatedness be-
tween languages or types of a language is the sub-
ject of another three papers. Littauer et al use
data from the WALS (World Atlas of Language
Structures; Dryer and Haspelmath (2011)) to
model language relatedness via heat maps. They
overcome two difficulties: one is the sparseness
of the WALS data; another is that WALS does
not directly contain information about possible ef-
fects of language contact. Littauer et al attempt
to model the latter by taking geographical infor-
mation about languages into account (neighboring
languages and their structure). A different kind
of language relatedness is investigated by Yan-
nakoudakis et al, who look at learner corpora and
develop tools that allow an assessment of learner
competence with respect to various linguistic fea-
tures found in the corpora. The number of rel-
evant features is large and many of them are in-
terdependent or interact. Thus, the amount and
complexity of the data present a classic case of
complex data sets that are virtually impossible to
analyze well without the application of visualiza-
tion methods. Finally, Lyding et al take academic
texts and investigate the use of modality across
academic registers and across time in order to
identify whether the academic language used in
different subfields (or adjacent fields) of an aca-
demic field has an effect on the language use of
that field.
3 UNCLH
The second half of the workshop focused on
the usage of multilingual resources in computa-
tional historical linguistics. In the past 20 years,
the application of quantitative methods in his-
torical linguistics has received increasing atten-
tion among linguists (Dunn et al, 2005; Heg-
garty et al, 2010; McMahon and McMahon,
2006), computational linguists (Kondrak, 2001;
Hall and Klein, 2010) and evolutionary anthropol-
ogists (Gray and Atkinson, 2003). Due to the ap-
plication of these quantitative methods, the field
of historical linguistics is undergoing a renais-
sance. One of the main problems that researchers
face is the limited amount of suitable compara-
tive data, often falling back on relatively restricted
?Swadesh type? wordlists. One solution is to use
synchronic data, like dictionaries or texts, which
are available for many languages. For example,
in Kondrak (2001), vocabularies of four Algo-
nquian languages were used in the task of au-
tomatic cognate identification. Another solution
employed by Snyder et al (2010) is to apply a
non-parametric Bayesian framework to two non-
parallel texts in the task of text deciphering. Al-
though very promising, these approaches have so
far only received modest attention. Thus, many
questions and challenges in the automatization
of language resources in computational historical
linguistics remain open and ripe for investigation.
In dialectological studies, there is a long tra-
dition, starting with Se?guy (1971), in which lan-
guage varieties are grouped together on the ba-
sis of their similarity with respect to certain prop-
erties. Later work in this area has incorporated
methods of string alignment for a quantitative
comparison of individual words to obtain an aver-
age measure of the similarity of languages. This
line of research became known as dialectome-
try. Unlike traditional dialectology which is based
on the analysis of individual items, dialectometry
shifts focus on the aggregate level of differences.
Most of the work done so far in dialectometry
is based on the carefully selected wordlists and
problems with the limited amount of suitable data
(i.e. computer readable and comparable across di-
alects) are also present in this field.
This workshop brings together researchers in-
terested in computational approaches that uncover
sound correspondences and sound changes, auto-
matic identification of cognates across languages
and language comparison based both on wordlists
and parallel texts. First, Wettig et al investigate
the sound correspondences in cognate sets in a
sample of Uralic languages. Then, List?s contri-
bution to the volume introduces a novel method
for automatic cognate detection in multilingual
wordlists which combines various previous ap-
proaches for string comparison. The paper by
Mayer & Cysouw presents a first step to use par-
allel texts for a quantitative comparison of lan-
guages. The papers by Scherrer and Prokic? et
al. both are in the spirit of the dialectometric line
of research. Further, Ja?ger reports on quantify-
ing language similarity via phonetic alignment of
core vocabulary items. Finally, some of the pa-
2
pers presented in this workshop deal with further
topics in quantitative language comparison, like
the application of phylogenetic methods in cre-
ole research in the paper by Daval-Markussen &
Bakker, and the study of the evolution of the Aus-
tralian kinship terms reported on in the paper by
McConvell & Dousset.
In the next section, we give a brief introduc-
tion into the papers presented in this workshop,
ordered according to the program of the oral pre-
sentations at the workshop.
4 Papers
Christian Rohrdantz, Andreas Niekler, Annette
Hautli, Miriam Butt and Daniel A. Keim (?Lex-
ical Semantics and Distribution of Suffixes ?
A Visual Analysis) present a quantitative cross-
linguistic investigation of the lexical semantic
content expressed by three suffixes originating in
English: -gate, -geddon and -athon. Using data
from newspapers, they look at the distribution and
lexical semantic usage of these morphemes across
several languages and also across time, with a
time-depth of 20 years for English. Using tech-
niques from InfoVis and Visual Analytics is cru-
cial for the analysis as the occurrence of these suf-
fixes in the available corpora is comparatively rare
and it is only by dint of processing and visualiz-
ing huge amounts of data that a clear pattern can
begin to emerge.
Kris Heylen, Dirk Speelman and Dirk Geer-
aerts (?Looking at Word Meaning. An Interac-
tive Visualization of Semantic Vector Spaces for
Dutch synsets?) focus on the pervasive use of Se-
mantic Vector Spaces (SVS) in statistical NLP
as a standard technique for the automatic mod-
eling of lexical semantics. They take on the
fact that while the method appears to work fairly
well (though they criticize the standardly avail-
able evaluation measures via some created gold
standard), it is in fact quite unclear how it captures
word meaning. That is, the standard technology
can be seen as a black box. In order to find a way
of providing some transparency to the method,
they explore the way an SVS structures the indi-
vidual occurrences of words with respect to the
occurrences of 476 Dutch nouns. These were
grouped into 214 synsets in previous work. This
paper looks at a token-by-token similarity matrix
in conjunction with a visualization that uses the
Google Chart Tools and compares the results with
previous work, especially in light of different uses
in different versions of Dutch.
Ahti Lohk, Kadri Vare and Leo Vo?handu
(?First Steps in Checking and Comparing Prince-
ton WordNet and Estonian WordNet?) use visu-
alization methods to compare two existing Word-
Nets (English and Estonian) in order to identify
errors and semantic inconsistencies that are a re-
sult of the manual coding. Their method opens
up a potentially interesting way of automatically
checking for inconsistencies and errors not only
at a fairly basic and surface level, but by work-
ing with the lexical semantic classification of the
words in question.
Richard Littauer, Rory Turnbull and Alexis
Palmer (?Visualizing Typological Relationships:
Plotting WALS with Heat Maps?) present a novel
way of visualizing relationships between lan-
guages. The paper is based on data extracted from
the World Atlas of Language Structures (WALS),
which is the most complete set of typological and
digitized data available to date, but which presents
two challenges: 1) it actually has very low cover-
age both in terms of languages represented and
in terms of feature description for each language;
2) areal effects are not coded for. While the au-
thors find a way to overcome the first challenge,
the paper?s real contribution lies in proposing a
method for overcoming the second challenge. In
particular, the typological data is filtered by geo-
graphical proximity and then displayed by means
of heat maps, which reflect the strength of similar-
ity between languages for different linguistic fea-
tures. Thus, the data should allow one to be able
to ascertain areal typological effects via a single
integrated visualization.
Helen Yannakoudakis, Ted Briscoe and
Theodora Alexopoulou (?Automatic Second
Language Acquisition Research: Integrating
Information Visualisation and Machine Learn-
ing?) look at yet another domain of application.
They show how data-driven approaches to
learner corpora can support Second Language
Acquisition (SLA) research when integrated
with visualization tools. Learner corpora are
interesting because their analysis requires a good
understanding of a complex set of interacting
linguistic features across corpora with different
distributional patterns (since each corpus po-
tentially diverges from the standard form of the
language by a different set of features). The paper
3
presents a visual user interface which supports
the investigation of a set of linguistic features
discriminating between pass and fail exam
scripts. The system displays directed graphs to
model interactions between features and supports
exploratory search over a set of learner texts.
A very useful result for SLA is the proposal
of a new method for empirically quantifying
the linguistic abilities that characterize different
levels of language learning.
Verena Lyding, Ekaterina Lapshinova-
Koltunski, Stefania Degaetano-Ortlieb, Henrik
Dittmann and Chris Culy (?Visualizing Linguistic
Evolution in Academic Discourse?) describe
methods for visualizing diachronic language
changes in academic writing. In particular, they
look at the use of modality across different aca-
demic subfields and investigate whether adjacent
subfields affect the use of language in a given
academic subfield. Their findings potentially
provide crucial information for further NLP tasks
such as automatic text classification.
Grzegorz Kondrak?s invited contribution
(?Similarity Patterns in Words?) sketches a num-
ber of the author?s research projects on diachronic
linguistics. He first discusses computational tech-
niques for implementing several steps of the
comparative method. These techniques include
algorithms that deal with a wide range of prob-
lems: pairwise and multiple string alignment,
calculation of phonetic similarity between two
strings, automatic extraction of recurrent sound
correspondences, quantification of semantic
similarity between two words, identification of
sets of cognates and building of phylogenetic
trees. In the second part, Kondrak sketches
several NLP projects that directly benefitted
from his research on diachronic linguistics:
statistical machine translation, word align-
ment, identification of confusable drug names,
transliteration, grapheme-to-phoneme conver-
sion, letter-phoneme alignment and mapping of
annotations.
Thomas Mayer and Michael Cysouw (?Lan-
guage Comparison through Sparse Multilingual
Word Alignment?) present a novel approach on
how to calculate similarities among languages
with the help of massively parallel texts. In-
stead of comparing languages pairwise they sug-
gest a simultaneous analysis of languages with re-
spect to their co-occurrence statistics for individ-
ual words on the sentence level. These statistics
are then used to group words into clusters which
are considered to be partial (or ?sparse?) align-
ments. These alignments then serve as the basis
for the similarity count where languages are taken
to be more similar the more words they share in
the various alignments, regardless of the actual
form of the words. In order to cope with the
computationally demanding multilingual analysis
they introduce a sparse matrix representation of
the co-occurrence statistics.
Yves Scherrer (?Recovering Dialect Geogra-
phy from an Unaligned Comparable Corpus?) pro-
poses a simple metric of dialect distance, based
on the ratio between identical word pairs and cog-
nate word pairs occurring in two texts. Scherrer
proceeds from a multidialectal corpus and applies
techniques from machine translation in order to
extract identical words and cognate words. The
dialect distance is defined as as function of the
number of cognate word pairs and identical word
pairs. Different variations of this metric are tested
on a corpus containing comparable texts from dif-
ferent Swiss German dialects and evaluated on the
basis of spatial autocorrelation measures.
Jelena Prokic?, C?ag?r? Co?ltekin and John Ner-
bonne (?Detecting Shibboleths?) propose a gen-
eralization of the well-known precision and re-
call scores to deal with the case of detecting dis-
tinctive, characteristic variants in dialect groups,
in case the analysis is based on numerical differ-
ence scores. This method starts from the data that
has already been divided into groups using clus-
ter analyses, correspondence analysis or any other
technique that can identify groups of language va-
rieties based on linguistic or extra-linguistic fac-
tors (e.g. geography or social properties). The
method seeks items that differ minimally within a
group but differ a great deal with respect to ele-
ments outside it. They demonstrate the effective-
ness of their approach using Dutch and German
dialect data, identifying those words that show
low variation within a given dialect area, and high
variation outside a given area.
Gerhard Ja?ger (?Estimating and Visualizing
Language Similarities Using Weighted Align-
ment and Force-Directed Graph Layout?) reports
several studies to quantify language similarity
via phonetic alignment of core vocabulary items
(taken from the Automated Similarity Judgement
Program data base). Ja?ger compares several string
4
comparison measures based on Levenshtein dis-
tance and based on Needleman-Wunsch similar-
ity score. He also tests two normalization func-
tions, one based on the average score and the
other based on the informatic theoretic similar-
ity measure. The pairwise similarity between all
languages are analyzed and visualized using the
CLANS software, a force directed graph layout
that does not assume an underlying tree structure
of the data.
Aymeric Daval-Markussen and Peter Bakker
(?Explorations in Creole Research with Phyloge-
netic Tools?) employ phylogenetic tools to inves-
tigate and visualize the relationship of creole lan-
guages to other (non-)creole languages on the ba-
sis of structural features. Using the morphosyn-
tactic features described in the monograph on
Comparative Creole Syntax (Holm and Patrick,
2007), they create phylogenetic trees and net-
works for the languages in the sample, which
show the similarity between the various languages
with respect to the grammatical features inves-
tigated. Their results lend support to the uni-
versalist approach which assumes that creoles
show creole-specific characteristics, possibly due
to restructuring universals. They also apply their
methodology to the comparison of creole lan-
guages to other languages, on the basis of typo-
logical features from the World Atlas of Language
Structures. Their findings confirm the hypothe-
sis that creole languages form a synchronically
distinguishable subgroup among the world?s lan-
guages.
Patrick McConvell and Laurent Dousset
(?Tracking the Dynamics of Kinship and So-
cial Category Terms with AustKin II?) give an
overview of their ongoing work on kinship and
social category terms in Australian languages.
They describe the AustKin I database which
allows for the reconstruction of older kinship
systems as well as the visualization of patterns
and changes. In particular, their method recon-
structs so-called ?Kariera? kinship systems for the
proto-languages in Australia. This supports ear-
lier hypotheses about the primordial world social
organization from which Dravidian-Kariera sys-
tems are considered to have evolved. They also
report on more recent work within the AustKin II
project which is devoted to the co-evoluation of
marriage and social category systems.
Hannes Wettig, Kirill Reshetnikov and Roman
Yangarber (?Using Context and Phonetic Fea-
tures in Models of Etymological Sound Change?)
present a novel method for a context-sensitive
alignment of cognate words, which relies on the
information theoretic concept of Minimum De-
scription Length to decide on the most compact
representation of the data given the model. Start-
ing with an initial random alignment for each
word pair, their algorithm iteratively rebuilds de-
cision trees for each feature and realigns the cor-
pus while monotonically decreasing the cost func-
tion until convergence. They also introduce a
novel test for the quality of the models where one
word pair is omitted from the training phase. The
rules that have been learned are then used to guess
one word from the other in the pair. The Lev-
enshtein distance of the correct and the guessed
word is then computed to give an idea of how
good the model actually learned the regularities
in the sound correspondences.
Johann-Mattis List (?LexStat: Automatic De-
tection of Cognates in Multilingual Wordlists?)
presents a new method for automatic cognate
detection in multilingual wordlists. He com-
bines different approaches to sequence compari-
son in historical linguistics and evolutionary bi-
ology into a new framework which closely mod-
els central aspects of the comparative method.
The input sequences, i.e. words, are converted to
sound classes and their sonority profiles are deter-
mined. In step 2, a permutation method is used to
create language specific scoring schemes. In step
3, the pairwise distances between all word pairs,
based on the language-specific scoring schemes,
are computed. In step 4, the sequences are clus-
tered into cognate sets whose average distance is
beyond a certain threshold. The method is tested
on 9 multilingual wordlists.
5 Final remarks
The breadth and depth of the research collected
in this workshop more than testify to the scope
and possibilities for applying new methods that
combine quantitative methods with not only a so-
phisticated linguistic understanding of language
phenomena, but also with visualization methods
coming out of the Computer Science fields of In-
foVis and Visual Analytics. The papers in the
workshop addressed how the emerging new body
of work can provide advances and new insights
for questions pertaining to theoretical linguistics
5
(lexical semantics, derivational morphology, his-
torical linguistics, dialectology and typology) and
applied linguistic fields such as second language
acquisition and statistical NLP.
6 Acknowledgments
We are indebted to the members of the pro-
gram committee of the workshop for their ef-
fort in thoroughly reviewing the papers: Quentin
Atkinson, Christopher Collins, Chris Culy, Dan
Dediu, Michael Dunn, Sheila Embleton, Simon
Greenhill, Harald Hammarstro?m, Annette Hautli,
Wilbert Heeringa, Gerhard Heyer, Eric Hol-
man, Gerhard Ja?ger, Daniel Keim, Tibor Kiss,
Jonas Kuhn, Anke Lu?deling, Steven Moran, John
Nerbonne, Gerald Penn, Don Ringe, Christian
Rohrdantz, Tandy Warnow, S?ren Wichmann.
We also thank the organizers of the EACL 2012
conference for their help in setting up the joint
workshop.
References
Christopher Collins, Sheelagh Carpendale, and Ger-
ald Penn. 2009. Docuburst: Visualizing document
content using language structure. Computer Graph-
ics Forum (Proceedings of Eurographics/IEEE-
VGTC Symposium on Visualization (EuroVis ?09)),
28(3):1039?1046.
Christopher Collins. 2010. Interactive Visualizations
of Natural Language. Ph.D. thesis, University of
Toronto.
Matthew S. Dryer and Martin Haspelmath, editors.
2011. The World Atlas of Language Structures On-
line. Max Planck Digital Library, Munich, 2011
edition.
Michael Dunn, Angela Terrill, Ger Resnik, Robert A.
Foley, and Stephen C. Levinson. 2005. Structural
phylogenetics and the reconstruction of ancient lan-
guage history. Science, 309(5743):2072?2075.
Russell Gray and Quentin Atkinson. 2003. Language-
tree divergence times support the Anatolian theory
of Indo-European origins. Nature, 426:435?439.
David LW Hall and Dan Klein. 2010. Finding cognate
groups using phylogenies. In Proceedings of the
Association for Computational Linguistics.
Paul Heggarty, Warren Maguire, and April McMahon.
2010. Splits or waves? trees or webs? how diver-
gence measures and network analysis can unravel
language histories. In Philosophical Transactions
of the Royal Society (B), volume 365, pages 3829?
3843.
John Holm and Peter L. Patrick, editors. 2007. Com-
parative Creole Syntax. London: Battlebridge.
Timo Honkela, Ville Pulkki, and Teuvo Kohonen.
1995. Contextual relations of words in grimm tales,
analyzed by self-organizing map. In Proceedings of
International Conference on Artificial Neural Net-
works (ICANN-95), pages 3?7.
Grzegorz Kondrak. 2001. Identifying cognates by
phonetic and semantic similarity. In Proceedings
of the North American Chapter of the Association
of Computational Linguistics.
Thomas Mayer, Christian Rohrdantz, Miriam Butt,
Frans Plank, and Daniel A. Keim. 2010a. Visualiz-
ing vowel harmony. Linguistic Issues in Language
Technology (LiLT), 2(4).
Thomas Mayer, Christian Rohrdantz, Frans Plank,
Peter Bak, Miriam Butt, and Daniel A. Keim.
2010b. Consonant co-occurrence in stems across
languages: Automatic analysis and visualization of
a phonotactic constraint. In Proceedings of the
2010 Workshop on NLP and Linguistics: Finding
the Common Ground, ACL 2010, pages 70?78.
April McMahon and Robert McMahon. 2006. Lan-
guage Classification by Numbers. OUP.
Petra Neumann, Annie Tat, Torre Zuk, and Shee-
lagh Carpendale. 2007. Keystrokes: Personaliz-
ing typed text with visualization. In Proceedings
of Eurographics IEEE VGTC Symposium on Visu-
alization.
Christian Rohrdantz, Annette Hautli, Thomas Mayer,
Miriam Butt, Daniel A. Keim, and Frans Plank.
2011. Towards tracking semantic change by visual
analytics. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics (Short Papers), pages 305?310. Portland, Ore-
gon.
Jean Se?guy. 1971. La relation entre la distance spa-
tiale et la distance lexicale. Revue de Linguistique
Romane, 35(138):335?357.
Benjamin Snyder, Regina Barzilay, and Kevin Knight.
2010. A statistical model for lost language deci-
pherment. In Proceedings of the Association for
Computational Linguistics.
6
Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, pages 72?80,
Avignon, France, April 23 - 24 2012. c?2012 Association for Computational Linguistics
Detecting Shibboleths
Jelena Prokic?
Ludwig-Maximilians-Universita?t
j.prokic@lmu.de
C?ag?r? C?o?ltekin
University of Groningen
c.coltekin@rug.nl
John Nerbonne
University of Groningen
j.nerbonne@rug.nl
Abstract
A SHIBBOLETH is a pronunciation, or,
more generally, a variant of speech that
betrays where a speaker is from (Judges
12:6). We propose a generalization of the
well-known precision and recall scores to
deal with the case of detecting distinctive,
characteristic variants when the analysis is
based on numerical difference scores. We
also compare our proposal to Fisher?s linear
discriminant, and we demonstrate its effec-
tiveness on Dutch and German dialect data.
It is a general method that can be applied
both in synchronic and diachronic linguis-
tics that involve automatic classification of
linguistic entities.
1 Introduction and Background
The background of this contribution is the line of
work known as DIALECTOMETRY (Se?guy, 1973;
Goebl, 1982), which has made computational
work popular in dialectology. The basic idea of
dialectometry is simple: one acquires large sam-
ples of corresponding material (e.g., a list of lex-
ical choices, such as the word for carbonated
soft drink, which might be ?soda?, ?pop?, ?tonic?
etc.) from different sites within a language area,
and then, for each pair of samples, one counts
(or more generally measures) the difference at
each point of correspondence. The differences
are summed, and, given representative and suffi-
ciently large samples, the results characterizes the
degree to which one site differs from another.
Earlier work in dialectology mapped the dis-
tributions of individual items, recording lines of
division on maps, so-called ISOGLOSSES, and
then sought bundles of these as tell-tale indica-
tors of important divisions between DIALECT AR-
EAS. But as Chambers & Trudgill (1998) note,
the earlier methodology is fraught with prob-
lems, many of which stem from the freedom of
choice with respect to isoglosses, and their (nor-
mal) failure to ?bundle? neatly. Nerbonne (2009)
notes that dialectometry improves on the tradi-
tional techniques in many ways, most of which
stem from the fact that it shifts focus to AGGRE-
GATE LEVEL of differences. Dialectometry uses
large amounts of material; it reduces the sub-
jectivity inherent in choosing isoglosses; it fre-
quently analyzes material in ways unintended by
those who designed dialect data collection efforts,
including more sources of differences; and finally
it replaces search for categorical overlap by a sta-
tistical analysis of differences.
Dialectometry does not enjoy overwhelming
popularity in dialectology, however, and one of
the reasons is simply that dialectologists, but also
laymen, are interested not only in the aggregate
relations among sites, or even the determination
of dialect areas (or the structure of other geo-
graphic influence on language variation, such as
dialect continua), but are quite enamored of the
details involved. Dialectology scholars, but also
laymen, wish to now where ?coffee? is ordered (in
English) with a labialized /k/ sound ([kwOfi]) or
where in Germany one is likely to hear [p] and
where [
>
pf] in words such as Pfad ?path? or Pfund
?pound?.
Such characteristic features are known as SHIB-
BOLETHS, following a famous story in the old
testament where people were killed because of
where they were from, which was betrayed by
their inability to pronounce the initial [S] in the
word ?shibboleth? (Judges 12:6). We propose a
generalization of the well-known precision and
72
recall scores, appropriate when dealing with dis-
tances, and which are designed to detect distinc-
tive, characteristic variants when the analysis is
based on numerical difference scores. We also
compare our proposal to Fisher?s linear discrim-
inant, and we demonstrate its effectiveness on
Dutch and German dialect data. Finally we eval-
uate the success of the proposal by visually ex-
amining an MDS plot showing the distances one
obtains when the analysis is restricted to the fea-
tures determined to be characteristic.
The paper proceeds from a dialectometric per-
spective, but the technique proposed does not as-
sume an aggregate analysis, only that a group of
sites has been identified somehow or another. The
task is then to identify characteristic features of
(candidate) dialect areas.
1.1 Related Work
Wieling and Nerbonne (2011) introduced two
measures seeking to identify elements character-
istic of a given group, REPRESENTATIVENESS
and DISTINCTIVENESS. The intuition behind rep-
resentativeness is simply that a feature increases
in representativeness to the degree that it is found
at each site in the group. We simplify their defi-
nition slightly as they focus on sound correspon-
dences, i.e. categorical variables, while we shall
formulate ideas about features in general.
Representativeness(f, g) =
|gf |
|g|
where f is a feature (in their case sound corre-
spondence) in question, g is the set of sites in a
given cluster, and gf denotes the set of sites where
feature f is observed.
As Wieling (2012) notes, if one construes the
sites in the given group as ?relevant documents?
and features as ?queries?, then this definition is
equivalent to RECALL in information retrieval
(IR).
The intuition behind distinctiveness is similar
to that behind IR?s PRECISION, which measures
the fraction of positive query responses that iden-
tify relevant documents. In our case this would be
the fraction of those sites instantiating a feature
that are indeed in the group we seek to character-
ize. In the case of groups of sites in dialectologi-
cal analysis, however, we are dealing with groups
that may make up significant fractions of the en-
tire set of sites. Wieling and Nerbonne therefore
introduced a correction for ?chance instantiation?.
This is derived from the relative size of the group
in question:
RelSize(g) = |g||G|
RelOcc(f, g) = |g
f |
|Gf |
Distinct(f, g) = RelOcc(f,g)?RelSize(g)1?RelSize(g)
where, G is the set of sites in the larger area of
interest.
As a consequence, smaller clusters are given
larger scores than clusters that contain many ob-
jects. Distinctiveness may even fall below zero,
but these will be very uninteresting cases ? those
which occur relatively more frequently outside
the group under consideration than within it.
Critique
There are two major problems with the earlier
formulation which we seek to solve in this pa-
per. First, the formulation, if taken strictly, applies
only to individual values of categorical features,
not to the features themselves. Second, many
dialectological analyses are based on numerical
measures of feature differences, e.g., the edit dis-
tance between two pronunciation transcriptions or
the distance in formant space between two vowel
pronunciations (Leinonen, 2010).
We seek a more general manner of detecting
characteristic features below, i.e. one that applies
to features, and not just to their (categorical) val-
ues and, in particular, one that can work hand in
hand with numerical measures of feature differ-
ences.
2 Characteristic Features
Since dialectometry is built on measuring differ-
ences, we assume this in our formulation, and we
seek those features which differ little within the
group in question and a great deal outside that
group. We focus on the setting where we exam-
ine one candidate group at a time, seeking fea-
tures which characterize it best in distinction to
elements outside the group.
We assume therefore, as earlier, a group g that
we are examining consisting of |g| sites among a
larger area of interest G with |G| sites including
the sites s both within and outside g. We further
explicitly assume a measure of difference d be-
tween sites, always with respect to a given feature
73
f . Then we calculate a mean difference with re-
spect to f within the group in question:
d?gf =
2
|g|2 ? |g|
?
s,s??g
df (s, s
?)
and a mean difference with respect f involving
elements from outside the group:
?
d6gf =
1
|g|(|G| ? |g|)
?
s?g,s? 6?g
df (s, s
?)
We then propose to identify characteristic features
as those with relatively large differences between
?
d6gf and d?
g
f . However, we note that scale of these
calculations are sensitive to a number of factors,
including the size of the group and the number of
individual differences calculated (which may vary
due to missing values). To remedy the difficul-
ties of comparing different features, and possibly
very different distributions, we standardize both
?
d6gf and d?
g
f and calculate the difference between
the z-scores, where mean and standard deviation
of the difference values are estimated from all dis-
tance values calculated with respect to feature f .
As a result, we use the measure
?
d6gf ? d?f
sd(df )
?
d?gf ? d?f
sd(df )
where df represents all distance values with re-
spect to feature f (the formula is not simplified
for the sake of clarity). We emphasize that we
normalized the difference scores for each feature
separately. Had we normalized with respect to all
the differences, we would only have transformed
the original problem in a linear fashion.
Note that this formulation allows us to apply
the definitions to both categorical and to numer-
ical data, assuming only that the difference mea-
sure is numerical. See illustration in Figure 1.
For this work we used a difference function that
finds the aggregated minimum Levenshtein dis-
tance between two sites as calculated by Gabmap
(Nerbonne et al, 2011). However, we again em-
phasize that the benefit of this method in compari-
son to others proposed earlier is that it can be used
with any feature type as long as one can define
a numerical distance metric between the features.
Regardless of the type of data set, some distance
values between certain sites may not be possible
to calculate, typically due to missing values. This
S
Figure 1: Illustration of the calculation of a distance
function. Our proposal compares the mean distance
of all pairs of sites within a group, including all those
shown on the left (in blue) to the mean distance of the
pairs of sites where the first is within the group and the
second outside it.
may affect the scale and the reliability of the av-
erage distance calculations presented above. For
the experiments reported below, we calculated av-
erage scores only if the missing values did not ex-
ceed 20% of the total values used in the calcula-
tion.
Fisher?s Linear Discriminant
The formulation we propose looks a good deal
like the well-known Fisher?s linear discriminant
(FLD) (Schalkoff, 1992, 90ff), which maximizes
the differences in means between two data sets
with respect to (the sum of) their variances.
S =
?2between
?2within
But FLD is defined for vectors, while we wish
to generalize to cases where only differences are
guaranteed to be numerical measures. The mean
of categorical features, for example, is undefined.
We might imagine applying something like FLD
in the space of differences, but note that low vari-
ance does not necessarily correspond to a tightly
knit group in difference space. If we measure the
differences among all the pairs of sites in a can-
didate group, each of which realizes a given cate-
gorical feature differently, the mean difference of
pairs will be one (unit) and the variance zero. Dif-
ference spaces are simply constructed differently.
Silhouette method
We also note relation of our approach to the
SILHOUETTE method introduced by Rousseeuw
(1987) used to evaluate clustering validity. The
silhouette method is used to determine the optimal
number of clusters for a given dataset. It starts
from data that has already been clustered using
74
any of the (hierarchical or flat) clustering tech-
niques. For every object i in the data (these would
be sites in clustering to detect dialect groups) it
calculates the average dissimilarity to all other ob-
jects in the same cluster a(i), and the average dis-
similarity to all objects in all other clusters (for
every cluster separately). After the distances to
all other clusters are computed, the cluster with
the smallest average distance (b(i)) to the object
in question is selected as the most appropriate one
for that object. The silhouette s(i) is calculated as
s(i) =
b(i)? a(i)
max{a(i), b(i)}
Values close to 1 indicate that the object is ap-
propriately clustered, while negative values indi-
cate that the object should have been clustered in
its neighbouring cluster. By comparing silhouette
values obtained by clustering into different num-
bers of groups, this technique indicates an optimal
clustering.
We compare average distances within groups to
average distance to objects outside groups with re-
spect to individual features, making our proposal
different. A second point of difference is that we
aim not to score ?groupings?, but rather how char-
acteristic specific features are for a given group-
ing.
3 Experimental set up
The method we propose is tested on Dutch and
German dialect data. We use Levenshtein algo-
rithm in order to calculate the distances between
the sites and Ward?s clustering method to group
the sites. In this section we give a brief descrip-
tion of the data and the clustering procedure.
Dutch data set
Dutch dialect data comes form the Goeman-
Taeldeman-Van Reenen Project1 that comprises
1876 items collected from more than 600 loca-
tions in the Netherlands and Flanders. The data
was collected during the period 1979-1996, tran-
scribed into IPA and later digitalized. It consists
of inflected and uninflected words, word groups
and short sentences. More on this project can be
found in Goeman and Taeldeman (1996).
The data used in this paper is a subset of
the GTRP data set and consist of the pronunci-
ations of 562 words collected at 613 location in
1http://www.meertens.knaw.nl
the Netherlands and Flanders. It includes only
single word items that show phonetic variation.
Multi-word items and items that show morpho-
logical, rather than phonetic variation, were ex-
cluded from the analysis. Items where multiple
lexemes per site are possible were also excluded.2
German data set
German dialect data comes from the project
?Kleiner Deutscher Lautatlas ? Phonetik? at the
?Forschungszentrum Deutscher Sprachatlas? in
Marburg. In this project a number of sentences
from Georg Wenker?s huge collection of Ger-
man dialects (1870s-1880s)3 were recorded and
transcribed in the late 1970s and early 1990s
(Go?schel, 1992). The aim of the project was to
give an overview of the sound structure of mod-
ern German dialects.
In this paper we use a small subset of the data
that consists of the transcriptions of 40 words. We
have selected only words that are present at all or
almost all 186 locations evenly distributed over
Germany.
Distance matrices
The distances between each pair of sites within
each of the two data sets were calculated using
the Levenshtein algorithm (Levenshtein, 1966).
This method is frequently used in dialect com-
parison to measure the differences between two
sites (Nerbonne et al, 1996; Heeringa, 2004). It
aligns two strings and calculates the number of
mismatching segments in two strings. The total
distance between two sites is the average distance
between all compared strings collected at those
two sites. For the method proposed in this paper,
any other method whose output is a numerical dis-
tance metric between the features can be applied.
The final result is a site ? site distance matrix,
that can later be analyzed by means of clustering
or, alternatively, using a dimensionality reduction
technique such multidimensional scaling.
We analyze two distance matrices using Ward?s
clustering algorithm, also known as the minimal
variance algorithm. We use MDS plots (as im-
plemented in Gabmap (Nerbonne et al, 2011)) as
a visual basis to choose the optimal number for
clusters for the two data sets. The choice of the
2The data set used in this paper can be downloaded from
http://www.gabmap.nl/ app/examples/.
3See, too, the Digitaler Wenker Atlas (DiWA) project,
http://www.3.diwa.info/
75
appropriate clustering algorithm is a difficult task
as is the determination of the number of signif-
icant groups (Prokic? and Nerbonne, 2008), but
these questions are not the subjects of this pa-
per. At the risk of repeating ourselves, we empha-
size that our focus in this paper is not the choice
of clustering method or the determination of the
most significant (number of) groups. We do not
even assume that the groups were obtained via
clustering, only that candidate groups have some-
how been identified. We focus then on finding the
most characteristic features for a given group of
sites. In the next section we present the results
of applying our method to the Dutch and German
data sets.
Evaluation
We evaluate success in the task of selecting items
characteristic of an area by using MDS to ana-
lyze a distance matrix obtained from only that
item. We then project the first, most important
MDS dimension to a map asking whether the orig-
inal group of sites indeed is identified. Note that
in successful cases the area corresponding to the
group may be shaded either as darker than the rest
or as lighter. In either case the item (word) has
served to characterize the region and the sites in
it.
We also experimented with clustering to ana-
lyze the distances based on the pronunciations of
the candidate characteristic shibboleths, but single
word distances unsurprisingly yielded very unsta-
ble results. For that reason we use MDS.
4 Results
Dutch
We examine a clustering of the distance matrix
for Dutch varieties with six clusters, which we
present in Figure 2.
The clustering algorithm identified Frisian
(dark green), Low Saxon (Groningen and Over-
ijsel, light blue), Dutch Franconian varieties
(pink), Limburg (dark blue), Belgian Brabant
(red) and West Flanders (light green) dialect
groups. For each feature (word) in our data set
and for each group of sites (cluster) we calculated
the differences within the given site and also with
respect to each of the other five groups in order
to determine which words differ the least within
the given group and still differ a great deal with
respect to the sites outside the group. The top five
Figure 2: Six dialect groups in Dutch speaking area.
words for each group of sites are presented in Ta-
ble 1.
The results obtained show that the same word
could be prominent for more than one cluster;
for example, the word scheiden is scored highly
in two different dialect groups. In Figure 3 we
present maps of Dutch language area that are
based on the pronunciations of the best scoring
words for each of the six groups of sites. For
each word we calculated the Levenshtein distance
and analyzed the resulting distance matrices using
MDS. In maps in Figure 3 we present the first ex-
tracted dimension, which always explains most of
the variation in the data.4 We also supply the de-
gree to which the extracted dimensions correlate
with the distances in the input matrix.
Maps in Figure 3 reveal that the best scoring
word does indeed identify the cluster in question.
For example, the map in Figure 3(a) reveals that
based on the pronunciation of word vrijdag the
Frisian-speaking area is internally homogeneous
and distinct from the rest of the sites. No other
groups can be identified in the map. In Figure 3(b)
we present the analysis of a distance matrix based
on the pronunciation of the word wonen ?live? that
was found to be relevant for the Low Saxon area.
The map shows two areas, Low Saxon and West
Flanders, where it was also among top 10 best
scored words, as two distinct areas.5
4The only exception is Figure 3(b) where we present sec-
ond dimension.
5These two areas are both known for pronouncing the slot
?n in final unstressed syllables of the form /@n/ as a syllabic
nasal that has assimilated in place to the preceding conso-
nant.
76
(a) vrijdag (r = 0.78), selected as most character-
istic of the Frisian area.
(b) wonen (r = 0.54), characteristic both of Low
Saxon (in the northeast) but also of West Flanders
(southwest).
(c) durven (r = 0.54), characteristic of Franco-
nian Dutch.
(d) wegen (r = 0.59), characteristic of Limburg.
(e) gisteren (r = 0.60), selected as characteristic
of Belgian Brabant.
(f) heet (r = 0.58), selected as characteristic of
West Flanders, but in fact not awfully successful in
distinguishing exactly that area.
Figure 3: Dutch dialect area based on the pronunciation of words (a) vrijdag, (b) wonen, (c) durven, (d) wegen,
(f) heet and (e) gisteren selected as characteristic of respective areas.
77
Frisian Low Saxon Franconian Limburg West Flanders Belg.Brabant
2.891217 vrijdag 1.881354 wonen 1.131973 durven 2.317413 wegen 1.605255 heet 1.968656 gisteren
2.808631 zoet 1.875302 dopen 1.101160 maanden 2.048480 schoenen 1.587253 weten 1.803535 gewoon
2.659577 geven 1.784224 scheiden 1.096989 metselen 2.015069 schaven 1.573224 weer 1.794680 gal
2.618426 draden 1.747136 bijten 1.073387 houden 1.979678 schapen 1.567049 keuren 1.764176 kleden
2.606748 dun 1.721321 worden 1.054981 dorsen 1.956787 scheiden 1.548940 horen 1.753901 wippen
Table 1: Five most characteristic words for each Dutch dialect variety.
Figure 4: Two dialect groups in Germany.
German
We ran the same analysis for the German data set.
In Figure 4 we present the two largest groups in
the cluster analysis of the distances obtained using
40 words. We might have examined more groups,
but we wished to examine results based on larger
groups as well.
We focus on the top-level, two-way split that
divides Germany into north and south.6 These ar-
eas correspond with the traditional division into
Low German on one hand, and Middle and High
German on the other. Just as with the Dutch data,
for every word in the data set and for each group
of sites we calculate the distances with respect to
the word in order to see how well the words char-
acterize one of the two dialect groups. The results
are presented in Table 2. Because we are exam-
ining a two-way split, it is not surprising that the
same words sometimes characterize the areas (in-
versely).
In Figures 5(a) and 5(b) we present the MDS
maps based on the distances derived from com-
6In anticipation of worries about the analysis we hasten
to add that more finely discriminated groups may also be
distinguished. That is not our purpose here.
North South
1.057400 weisse 1.056600 gefahre
1.011804 gefahre 0.909610 gross
0.982128 bleib 0.825211 weisse
0.920354 Ochse 0.764463 Pfeffer
0.831812 gross 0.755694 baue
Table 2: Five most prominent words for two dialect
groups in Germany. Because we examine a two-way
split, some words characterize both areas.
paring the words weisse and gefahre, which were
two best ranked words.
The word weisse shows only small differences
within the north, which is illustrated by the light-
colored northern part of Germany in Figure 5(a).
The map in Figure 5(b) shows an even clearer split
highlighting the High German area based on the
best ranked word found by our method. This word
shows also low variation in the Low German area
(second best scored), which is also clearly visible
in Figure 5(b).
5 Conclusions
In this paper we have presented a method to de-
tect the most characteristic features of a candidate
group of linguistic varieties. The group might be
one obtained from cluster analysis, but it might
also be obtained from correspondence analysis
(Cichocki, 2006), or it might simply be another
group identified for theoretical or extra-linguistic
reasons (geography or social properties).
The method is applicable to any feature type as
long as one can define a numerical distance met-
ric between the elements. In particular the method
maybe applied to categorical data whose differ-
ences are individually zero or one, or to vowels
characterized by the Euclidean distance between
formant vectors (or pairs), and it may be applied
to edit distance measures applied to phonetic tran-
scriptions. The proposed method is therefore not
constrained in its application to only the categor-
ical features, as the proposal in Wieling & Ner-
bonne (2011) was.
Essentially the method seeks items that differ
minimally within a group but differ a great deal
78
(a) weisse (r = 0.63) (b) gefahre (r = 0.59)
Figure 5: First MDS dimensions based on the pronunciation of words (a) weisse and (b) gefahre.
with respect to elements outside it. We crucially
limited its application to elements that were in-
stantiated at least 20% of the sites, and we used
normalized z-scores in order to improve the com-
parability of the measurements.
We demonstrated the effectiveness of the pro-
posed method on real dialect data by trying to
identify the words that show low variation within
a given dialect area, and high variation outside a
given area. We evaluate the results of these ex-
periments by visually examining the distances in-
duced from single words. Although this indicated
that the technique is performing well, we concede
that alternative evaluations would be worth while,
e.g. simply mapping the density of low distances
between pairs in the distance matrix. This awaits
future work.
The proposed method can be used in dialectom-
etry to automatically identify characteristic fea-
tures in dialect variation, while at the same time it
offers traditional dialectologists insights into the
details involved. Its application may also not be
limited to dialectology (including dialectometry).
It is a general method that can be applied in other
branches of linguistics, such as historical linguis-
tics or typology, that deal with language classifi-
cation at various levels.
The method proposed in this paper might also
find use in the evaluation of clustering, specifi-
cally in helping researchers to determine the opti-
mal number of groups in a clustering solution. It
might then result in a modification of the silhou-
ette technique discussed earlier.
Application of computational methods in di-
alectology and historical linguistics is still not
generally accepted. This state of affairs is due less
to the questions that the groups of researchers are
trying to answer, and more to the methods they are
using to reach their goals. Bringing them together
is a challenging task. The method we propose can
analyse large amounts of data without losing sight
of the linguistic details.
References
J.K. Chambers and Peter Trudgill. 1998. Dialectol-
ogy. Cambridge University Press, Cambridge.
Wladyslaw Cichocki. 2006. Geographic variation in
Acadian French /r/: What can correspondence anal-
ysis contribute? Literary and Linguistic Comput-
ing, 21(4):529?542. Special Issue, J.Nerbonne &
W.Kretzschmar (eds.), Progress in Dialectometry:
Toward Explanation.
Hans Goebl. 1982. Dialektometrie: Prinzip-
ien und Methoden des Einsatzes der Numerischen
Taxonomie im Bereich der Dialektgeographie.
O?sterreichische Akademie der Wissenschaften,
Wien.
Antonie Goeman and Johan Taeldeman. 1996.
Fonologie en morfologie van de nederlandse di-
alecten. een nieuwe materiaalverzameling en twee
nieuwe atlasprojecten. Taal en Tongval, 48:38?59.
Joachim Go?schel. 1992. Das Forschungsinstitut fu?r
Deutsche Sprache ?Deutscher Sprachatlas?. Wis-
79
senschaftlicher Bericht, Das Forschungsinstitut fu?r
Deutsche Sprache, Marburg.
Wilbert Heeringa. 2004. Measuring Dialect Pro-
nunciation Differences using Levenshtein Distance.
Ph.D. thesis, Rijksuniversiteit Groningen.
Therese Leinonen. 2010. An Acoustic Analysis of
Vowel Pronunciation in Swedish Dialects. Ph.D.
thesis, University of Groningen.
Vladimir I. Levenshtein. 1966. Binary codes ca-
pable of correcting insertions, deletions and rever-
sals. Cybernetics and Control Theory, 10(8):707?
710. Russian orig. in Doklady Akademii Nauk SSR
163(4), 845?848, 1965.
John Nerbonne, Wilbert Heeringa, Erik van den Hout,
Peter van der Kooi, Simone Otten, and Willem
van de Vis. 1996. Phonetic distance between dutch
dialects. In Gert Durieux, Walter Daelemans, and
Steven Gillis, editors, CLIN VI: Proc. from the Sixth
CLIN Meeting, pages 185?202. Center for Dutch
Language and Speech, University of Antwerpen
(UIA), Antwerpen.
John Nerbonne, Rinke Coleand, Charlotte Gooskens,
Peter Kleiweg, and Therese Leinonen. 2011.
Gabmap: A web application for dialectology. Di-
alectologia, Special issue II:65?89.
John Nerbonne. 2009. Data-driven dialectology. Lan-
guage and Linguistics Compass, 3(1):175?198.
Jelena Prokic? and John Nerbonne. 2008. Recogniz-
ing groups among dialects. International Journal of
Humanities and Arts Computing, 2(1-2):153?172.
DOI: 10.13366/E1753854809000366.
Peter J. Rousseeuw. 1987. Silhouettes: a graphical aid
to the interpretation and validation of cluster analy-
sis. Journal of Computational and Applied Mathe-
matics, 20:53?65.
Robert Schalkoff. 1992. Pattern Recognition: Statisti-
cal, Structural and Neural Approaches. John Wiley,
New York.
Jean Se?guy. 1973. La dialectome?trie dans l?atlas lin-
guistique de gascogne. Revue de Linguistique Ro-
mane, 37(145):1?24.
Martijn Wieling and John Nerbonne. 2011. Bipar-
tite spectral graph partitioning for clustering di-
alect varieties and detecting their linguistic fea-
tures. Computer Speech and Language, 25:700?
715. DOI:10.1016/j.csl.2010.05.004. Published on-
line May 21, 2010.
Martijn Wieling. 2012. A Quantitative Approach to
Social and Geogrpahical Dialect Variation. Ph.D.
thesis, University of Groningen.
80
