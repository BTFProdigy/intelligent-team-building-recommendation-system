Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 651?659, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UTurku: Drug Named Entity Recognition and Drug-Drug Interaction
Extraction Using SVM Classification and Domain Knowledge
Jari Bjo?rne, Suwisa Kaewphan and Tapio Salakoski
Turku Centre for Computer Science (TUCS)
Department of Information Technology
University of Turku
Joukahaisenkatu 3-5B, 20520 Turku, Finland
firstname.lastname@utu.fi
Abstract
The DDIExtraction 2013 task in the SemEval
conference concerns the detection of drug
names and statements of drug-drug interac-
tions (DDI) from text. Extraction of DDIs
is important for providing up-to-date knowl-
edge on adverse interactions between co-
administered drugs. We apply the machine
learning based Turku Event Extraction Sys-
tem to both tasks. We evaluate three fea-
ture sets, syntactic features derived from deep
parsing, enhanced optionally with features de-
rived from DrugBank or from both DrugBank
and MetaMap. TEES achieves F-scores of
60% for the drug name recognition task and
59% for the DDI extraction task.
1 Introduction
Drug-drug interactions (DDI) refer to one drug af-
fecting the function of another when they are co-
administered. These interactions are often adverse,
frequently not well known and a source of poten-
tially life-threatening unintended consequences for
the patients. Databases such as DrugBank and Mi-
cromedex have been developed to store informa-
tion about known DDIs, but at present their cover-
age remains limited and there can be inconsistencies
in supplementary information (Knox et al, 2011;
Wong et al, 2008). Text mining has been proposed
as a solution for providing not only lists of DDIs
but also a connection to the scientific evidence and
supplementary information in the literature (Tari et
al., 2010). Several groups of researchers are devel-
oping text-mining techniques to extract DDIs from
literature and pharmaceutical documents (Tari et al,
2010; Segura-Bedmar et al, 2011a).
The DDIExtraction 2013 shared task concerns the
detection of drug mentions and statements of DDIs
from unannotated text (Segura-Bedmar et al, 2013).
The first version of the DDIExtraction shared task
was organized in 2011, with 10 teams participat-
ing from various universities (Segura-Bedmar et al,
2011b). The best result of 65.74% was achieved
by team WBI of Humboldt University of Berlin
(Thomas et al, 2011). University of Turku partic-
ipated also in this task, placing 4th with an F-score
of 62.99%, using the Turku Event Extraction System
(Bjo?rne et al, 2011).
The Turku Event Extraction System (TEES)1 is
an open source program for extracting events and re-
lations from biomedical texts. It was originally de-
veloped for extracting events in the BioNLP Shared
Task scheme, and it models event extraction as a
graph generation task, where keywords are nodes
and the event arguments connecting them are edges.
The system can be directly applied to pairwise re-
lation extraction, representing relations as edges and
the words they connect as nodes. The node detection
system is somewhat similar to named entity recog-
nition (NER) tools, and while quite flexible, can in
many tasks exhibit lower performance and higher
processing requirements than dedicated NER sys-
tems.
In the DDIExtraction 2013 task we apply the
Turku Event Extraction system to detecting both
drug name entities (task 9.1) as well as drug-drug
interactions (task 9.2). We evaluate three different
1http://jbjorne.github.com/TEES/
651
feature sets for both tasks. As a baseline system deep
syntactic parsing is used to generate large graph-
based feature sets. For additional features, we test
the impact of labeling examples with information
from external sources. We test both the DrugBank
Open Data Drug & Drug Target database (Knox et
al., 2011) as well as the MetaMap tool to enrich the
features derived from the corpus text.
MetaMap is a publicly available program devel-
oped at NLM for automatic mapping of texts to
UMLS Metathesaurus concepts (Aronson, 2001).
The UMLS Metathesaurus is an extensive reposi-
tory of biomedical vocabularies that is derived from
NLM databases and other external sources that con-
tain information about biomedical concepts, syn-
onyms and the relationship among them (Bodenrei-
der, 2004).
The version of TEES used in the 2011 DDIEx-
traction task had been publicly available as an open
source project since July 2012, but as small mod-
ifications were required for compatibility with the
2013 task, we published an updated 2.1 version that
task participants could use. To simplify utilization of
the numerous analyses TEES produces we also pro-
vided our drug-drug interaction predictions freely
available for all DDIExtraction 2013 task partici-
pants in the hope of encouraging further participa-
tion in this interesting shared task.
We demonstrate that TEES has good performance
for both drug name detection as well as drug-drug
interaction detection, achieving an F-score of 60%
in the drug name detection task 9.1 and an F-score of
59% in the drug-drug interaction detection task 9.2.
We show that external information from DrugBank
and MetaMap can considerably improve extraction
performance, but observe that the use of such in-
formation must always depend on the exact require-
ments of each text mining task.
2 Methods
We present a unified approach to drug name and
DDI extraction, utilizing largely the same machine
learning approaches in both tasks. We develop three
variants for tasks 9.1 & 9.2 each, testing the base-
line performance of TEES for these tasks, as well as
the impact of using external databases as additional
training data.
2.1 Turku Event Extraction System
The Turku Event Extraction System is described in
detail in Bjo?rne et al (2012). Here we give a gen-
eral overview about applying the system for the cur-
rent task. TEES processes text in a pipeline of com-
ponents, starting from preprocessing tasks such as
NER and parsing and proceeding to the multiple,
consecutive steps of event extraction. As tasks 9.1
and 9.2 are independent of each other the entity and
interaction detection components of TEES are used
independently, and for preprocessing, only the pars-
ing is done (See Figure 1).
2.2 Training data preparation
TEES is a machine learning system based on sup-
port vector machines (SVM) (Tsochantaridis et al,
2005). To train the system for a new task, two
datasets are required: a training set on which the
SVM model is trained, and a development set on
which the newly trained model is tested to deter-
mine parameter settings for optimal performance
(See Figure 2). The optimal model can then be
used to detect what it was trained for on unannotated
datasets, such as the hidden shared task test set.
The DDIExtraction 2013 corpus consists of two
parts: A training corpus used for system develop-
ment and a test corpus for evaluating the participat-
ing systems. The annotation of the test corpus is not
revealed to task participants. To develop the system,
we estimate performance on the training corpus us-
ing 10-fold cross validation. To provide the datasets
TEES requires, the training corpus is randomly di-
vided (on the document level) into ten parts. For
predicting drug names or DDIs for each part, seven
of the remaining nine parts are used as a training
set and two as a development set for parameter opti-
mization. When producing the final models for clas-
sifying the test corpus, five parts of the training cor-
pus are used for training and the other five for pa-
rameter optimization. In both cases, the parameter
optimization set is merged with the training set when
producing the final model for classifying the test set.
The DDIExtraction 2013 corpus is provided in an
XML format originally introduced as a unified for-
mat for several pairwise protein-protein interaction
(PPI) corpora (Pyysalo et al, 2008). TEES uses a
variant of this format as its internal data representa-
652
DrugAminoglutethimide the of Drugcoumarin and Drugvarfarin .diminishes effect
effect effect
A B
NN DT IN NN CC .conj_and><dobj prep_of>
NN
prep_of>
VBZ<nsubj <det NN
punct>
Drug(Aminoglutethimide) Drug(coumarin)effect
Drug(warfarin)
effect neg
Figure 1: TEES graph representation for drug name and interaction extraction, with example sentence DDI-
DrugBank.d372.s2 from the DDIExtraction 2013 training corpus. A) Both the annotation (above the sentence) and the
syntactic parse (below the sentence) are represented as graphs. Tokens form the nodes and dependencies the edges of
the syntactic parse graph. Drug names form the nodes and DDIs the edges of the annotation graph. Drug name entities
are linked to their syntactic head tokens, connecting the two graphs and allowing the parse to be used as a source of
features. For DDI edges, most features are derived from the shortest path of dependencies connecting the two drug
entities. B) For DDI extraction, one example is generated for each interaction type for each undirected pair of drug
entities. The gray neg class edge is a negative example.
A) Corpus
train classify
param.
train classify
parameters
train
devel
model
test
model
test
data
test
data
Training corpus Test corpus
C) Training the Final Model
0 1 2 3 4 5 6 7 8 9
train classify
parameters
train classify
parameters
train
devel
model
test
model
B) 10-fold cross-validation (for each set 0-9, shown for #9)
90 1 2 3 4 5 6 7 8
0 1 2 3 4 5 6 7 8 9
Figure 2: DDIExtraction 2013 corpus. A) To evaluate performance, and to provide analyses for the full training
corpus, the training corpus is divided for 10-fold cross validation. B) Each of the ten parts is classified using seven of
the remaining parts for training the model and the last two for optimizing parameters. After parameter optimization,
all nine parts are used to train the model (with the optimal parameters) for classifying the test set. C) To classify the
hidden DDIExtraction 2013 corpus half of the training corpus is used for training and the other half for determining
optimal parameters. The test corpus is finally classified with a model trained using the full training corpus.
653
tion. While close to the DDIExtraction 2013 format,
some differences exist, so we preprocess the corpora
for compatibility with TEES. Namely, ddi elements
are renamed as interaction elements, entity elements
in task 9.2 are tagged with the given attribute to mark
them as pre-annotated data for TEES and all charac-
ter offsets are converted to the TEES format by in-
creasing the end offset by one, resulting in spans de-
noted with the beginning character and end charac-
ter plus one, a common convention in programming
languages such as Java and Python.
Before use, all DDIExtraction 2013 corpora are
parsed with the TEES preprocessing pipeline, using
the BLLIP parser with David McClosky?s biomodel
to produce a Penn-tree style parse which is con-
verted with the Stanford parser tools to the collapsed
CC processed Stanford dependency scheme (Char-
niak and Johnson, 2005; McClosky, 2010; de Marn-
effe et al, 2006).
2.3 Drug name recognition with TEES
For drug name recognition the TEES entity detector
module is used. Baseline syntactic features (model
1) are generated from the parse, using both informa-
tion on the tokens and their linear context, as well
as dependency chains starting from the entity head
token. External data is added to the head token fea-
tures, from where it is combined into more complex
features. One example is generated for each token in
the sentence, and these are classified into negatives
or one of the positive classes.
As a new feature we generate all substrings start-
ing from the first and last characters of the drug
name, with the intention of detecting common pre-
fixes and suffixes among the drug names.
2.4 Drug-drug interaction detection with TEES
For DDI extraction we use the TEES edge detec-
tor module. DDIs are typed, undirected edges, so
one example is generated for each undirected pair of
drug name entities present in the sentence (See Fig-
ure 1). The baseline syntactic features (model 1) are
generated mostly from the shortest path of depen-
dencies connecting the pair of drug name entities?
head tokens. From this shortest path several feature
groups are generated, including N-grams of various
lengths, governor?dependent information for depen-
dencies etc. External data is added into the two drug
name entities, and combined into the path features.
We also use the TEES modification from DDIEx-
traction 2011 task where conj and dependencies are
ignored when calculating the shortest path, with the
aim of including more of the relevant interaction
words in the path.
2.5 Using DrugBank for Domain Knowledge
DrugBank2 is a public database of information on
drugs and drug targets. We use the downloadable
XML version of the database.
For drug name recognition, for each candidate to-
ken, we add as features its presence as a known
drug name in DrugBank and the synonym, brand,
group and category annotations this drug may have.
We also mark whether the candidate token exactly
equals an annotation of one of these types, indicating
cases where the token is e.g. a known brand name.
For DDI extraction, we mark as a feature whether
the drug name pair is listed in DrugBank as having
interactions or not. We also mark if one of the drug
names is not listed in DrugBank.
2.6 Using MetaMap for Domain Knowledge
The MetaMap program has been used extensively
for a wide array of BioNLP studies, such as auto-
matic indexing of biomedical literature and concept-
based text summarization (Reeve et al, 2007;
Quanzhi and Yi-Fang Brook, 2006). For drug-
related information extraction, two recent applica-
tions demonstrated that integrating the MetaMap
program to their existing systems produces high
overall performance in i.) identification and clas-
sification of the pharmaceutical substances and ii.)
extraction of drug indication information (Segura-
Bedmar et al, 2008; Fung et al, 2013).
MetaMap finds Metathesaurus concepts by per-
forming a shallow syntactic analysis of the input
text, producing a set of noun phrases. The noun
phrases are then used to generate sets of variants
which are consequently looked up from the Metathe-
saurus concepts. Matching concepts are evaluated
against the original text and the strength of the map-
pings are calculated. The candidates are finally
combined and the final scores are computed, where
the highest score of a complete mapping represents
2http://www.drugbank.ca/
654
MetaMap?s interpretation of the text.
The MetaMap program can be run both lo-
cally and remotely3. We ran the current version,
MetaMap2012, remotely via the batch mode facil-
ity by converting the sentences of the DDIExtrac-
tion corpora into the MetaMap input format. Many
of the applications that integrate MetaMap into their
systems use the default settings that are claimed to
be suitable for general purposes. However, we ap-
plied different options with the aim of increasing
the coverage of Metathesaurus concepts found by
MetaMap. The parameter set that influences the
performance of MetaMap included; using a relaxed
model, selecting the NLM2012AB Metathesaurus
version, including all derivational variants, enabling
unique acronym/abbreviation variants only, allow-
ing candidates from one or two character words, pre-
ferring multiple concepts and using word sense dis-
ambiguation.
The Relaxed Model is provided by MetaMap in
addition to the strict model which is offered as a
default setting in which all types of filterings are
applied. However, we chose the relaxed model in
which only manual and lexical filterings are used.
While the strict model is most appropriate for exper-
iments that require the highest accuracy, it covers
only 53% of the Metathesaurus strings. As we con-
sider high coverage of concepts an important factor,
we applied the relaxed model which consists of up
to 83% of Metathesaurus strings.
The versions of Metathesaurus, Base, USAbase
and NLM, provided with MetaMap are different
in their Metathesaurus coverage and the license
type required for using vocabulary sources. The
NLM2012AB version which is offered at no cost
for research purposes and covers all of the provided
Metathesaurus was used in our work.
Variants, such as inflectional and derivational
variants, are computed by MetaMap to account for
the textual variation in the text. With this setting,
many types of variants are generated recursively, and
only acronyms and abbreviations are restricted to the
unique ones. In addition, the candidates also include
words that can be prepositions, conjunctions or de-
terminers if they occur often enough in Metathe-
saurus.
3http://metamap.nlm.nih.gov/
Prefer multiple concepts causes MetaMap to
score the mappings with more concepts higher than
those with fewer concepts. This option is useful for
discovering higher-order relationships among con-
cepts found in the text and as such is assumed to be
helpful for discovering the DDIs.
Word sense disambiguation attempts to solve lex-
ical ambiguities by identifying the correct meaning
of a word based on its context. By using this option
in MetaMap, the program attempts to solve the am-
biguities among equally scoring concepts by choos-
ing the concept(s) based on semantic type.
We use the XML version of the MetaMap out-
put which is post-processed by TEES to extract rel-
evant features; candidate concepts, preferred con-
cepts, CUI (Concepts Unique Identifier), score, se-
mantic types and sources.
For drug name recognition, these are added as bi-
nary features for the candidate token, with the ex-
ception of the score, the value of which is normal-
ized into the [0, 1] range. For DDI extraction, the
binary features are added for the two drug names,
and combined into the shortest path features.
2.7 Public analyses
The TEES 2.0 system used in DDIExtraction 2011
Shared Task has been public since summer 2012.
While only small modifications are needed to make
the DDIExtraction 2013 corpus usable with the
TEES system, these can be complicated for new
users. Therefore, to make sure our public DDIEx-
traction 2011 system is usable not only in theory,
but easy enough to use in practice, we updated the
system into the 2.1 version capable of automatically
converting the DDIExtraction 2013 corpus and pro-
vided with precalculated models for DDI prediction.
To improve usability, we provided fully precal-
culated analysis files for the DDIExtraction 2013
corpus, produced using TEES 2.1. These analyses
contain the TEES drug-drug interaction predictions,
BLLIP Penn tree-bank style parses (using the Mc-
Closky biomodel), Stanford dependency parses (in
the collapsed CC processed format) and syntactic
head offsets for drug entities.
The analyses were calculated with the base-
line TEES 2.1 system, without using the external
datasets which were tested only later. The analy-
ses were provided for task 9.2, which is the direct
655
continuation of the 2011 task for which the public
TEES system was already available.
The analyses for the DDIExtraction 2013 corpus
were made available on February 25th 2013. De-
spite being published quite late in the training pe-
riod there was interest in this supporting data, and
before the task result submission deadline the analy-
ses were downloaded 14 times. The test set analyses
were provided for registered DDIExtraction 2013
participants during the test period.
3 Results and Discussion
Three feature sets were used to produce the results.
The baseline set (model 1) consisted of the TEES
entity and edge detectors which build a large feature
set from syntactic parses. Model 2 adds DrugBank
features to this baseline and model 3 further extends
model 2 with MetaMap information.
Three runs using these models were submitted for
both tasks 9.1 and 9.2. The results indicate the sys-
tem was capable of detecting both drug names and
drug-drug interactions with reasonable performance.
The best F-scores were 60% for task 9.1 drug name
detection and 59% for task 9.2 DDI extraction.
As task 9.1 is completely new, and task 9.2 was
extended from the 2011 DDI extraction task with
typed interactions and MEDLINE abstracts, the cur-
rent results are not directly comparable with the
2011 ones. The evaluation metric closest to the 2011
task is task 9.2 DDI detection regardless of type, us-
ing only the DrugBank subset of the corpus. With
this metric, our system achieved an F-score of 72%
in 2013 vs. 62.99% in 2011, which may indicate
higher baseline performance, potentially influenced
by a larger training dataset.
3.1 Drug name recognition
The decision to not attempt detection of more than
one token per drug entity proved to be not too detri-
mental to the final performance. In the training cor-
pus, there are 14,765 drug name entities of which
only 2,768 (18.7%) consist of more than one to-
ken, and of these only 38 are disjoint (not form-
ing a continuous span). For our best performing
drug name detection model (number 3) typed, par-
tial span matching was at 78% F-score vs. typed,
strict span matching at 65%. Therefore, detecting
only a single token per entity resulted in a maximum
loss of 13 percentage points (pp), but considering
that a scheme designed to detect multi-token entities
would be inherently more complex, potentially hav-
ing lower performance, and that not all of the spans
would be correctly detected, we feel this tradeoff in
performance is worth it for the considerably more
simple system design it allows.
Adding the external datasets to the classifier mod-
els proved to have a considerable impact on the task
performance (See Table 1). The baseline system
reached an F-score of 47% which was increased by
9 percentage points when including DrugBank infor-
mation and a further 4 percentage points when also
MetaMap information was included.
As seen from the type-specific F-scores (on the
training corpus), brand class entity detection was
improved by 30 pp when DrugBank information was
added, and increased slightly further with MetaMap
information (See Table 2). DrugBank lists brand
names for many drugs, and when this information
is added as a feature for each detected drug, deter-
mining the type of the drug is greatly improved.
The official primary metric in both tasks 9.1 and
9.2 is a macro-averaged F-score, which gives equal
weight to performance in each class, emphasizing
the importance of detecting also the difficult, small
classes. In particular, the class drug n (active sub-
stances not approved for use in humans for medical
purposes) was very difficult to detect for our system.
While performance remained low for all three mod-
els, including the MetaMap information gave a large
relative increase in drug n detection performance,
increasing it from 2% F-score to 8% (See Table 2).
With the macro-averaged overall performance, this
resulted in model three with the MetaMap informa-
tion having notably higher performance.
We hypothesized that the drug n category might
be hard to detect as it could contain entities simi-
lar to the drug category, which may differ only by
approval for use in humans, information that is not
likely present in the corpus. Analysis of classifi-
cation errors (See Table 3) confirms this hypothe-
sis, showing that drug n entities are by far the most
commonly misclassified ones. Addition of Drug-
Bank and MetaMap information considerably re-
duces drug n misclassifications into the drug cate-
gory.
656
M task P R F
1 9.1 0.48 (0.70) 0.46 (0.51) 0.47 (0.59)
2 9.1 0.6 (0.77) 0.52 (0.59) 0.56 (0.67)
3 9.1 0.69 (0.76) 0.54 (0.59) 0.6 (0.66)
1 9.2 0.73 (0.69) 0.47 (0.44) 0.57 (0.54)
2 9.2 0.76 (0.69) 0.48 (0.45) 0.59 (0.55)
3 9.2 0.73 (0.68) 0.48 (0.44) 0.58 (0.53)
Table 1: Official results for TEES in the DDIExtrac-
tion 2013 task and in parentheses corresponding 10-fold
cross-validation results on the training corpus. The three
models (M) used are 1) baseline syntactic features, 2)
baseline with DrugBank features and 3) baseline with
both DrugBank and MetaMap features.
Task rules allowed using the test corpus of task
9.2 (with annotated entities) as additional training
data for task 9.1. Due to time constraints we did not
use it for training, but it is likely that performance
could be further enhanced by using it.
3.2 Drug-drug interaction extraction
Performance of the three feature sets in the 9.2 DDI
extraction task are much closer than in the 9.1 drug
name recognition task. Still, additional informa-
tion from DrugBank and MetaMap slightly increase
performance, but DrugBank alone outperforms us-
ing both MetaMap and DrugBank. With the perfor-
mance difference range between the models being
only 2 pp, we think the results remain inconclusive.
That external data did not provide a further in-
crease might indicate that drug-drug interaction de-
tection is mostly a matter of interpreting the syn-
tactic parse, whereas drug-name recognition benefits
more from dictionary matching methods.
As with task 9.1, we analyse the classification er-
rors on the 10-fold classification performed on the
training dataset for which annotations are publicly
available (See Table 4). None of the DDI classes are
as hard to detect as the drug name class drug n, but
the int class has much lower performance than the
other classes, with most examples classified incor-
rectly as negatives.
4 Conclusions
We applied the Turku Event Extraction System 2.1
to detection of both drug names and drug-drug in-
teractions in the DDIExtraction 2013 task. The sys-
model drug brand group drug n
1 0.72 0.6 0.48 0.02
2 0.78 0.9 0.49 0.02
3 0.78 0.91 0.48 0.08
Table 2: Per-class micro-average scores for the drug
name recognition task 9.1.
tem showed good performance for both tasks, but we
must consider that name and interaction detection
were evaluated in isolation. In real world text min-
ing tasks, these steps will be consecutive and as such
result in lower overall performance. TEES achieves
good performance using deep syntactic parsing, but
this is a computationally expensive processing step.
When drug names are detected with TEES, all in-
put sentences need to be parsed, but if some other
method is used for drug name recognition, TEES can
parse just the sentences with drug names, as only
they can potentially contain DDIs, enabling much
faster DDI extraction.
We showed that adding external data from the
DrugBank database and from MetaMap prepro-
cessing can considerably increase extraction perfor-
mance. However, we assume this makes the sys-
tem more dependent on such data being available
for candidate drug names and DDIs in the text be-
ing processed, potentially making it harder to detect
completely new names and interactions. Therefore,
using external data is likely to introduce a tradeoff
of higher performance vs. wider detection. Use of
such data should be chosen according to the task, as
in some cases the goal is to retrieve documents with
known drugs and interactions, in others to maximize
detection of information not yet in the databases.
As with previous TEES versions, we will pro-
vide our source code freely available under an open
source license at the TEES project repository4. We
will also include a wrapper for using the MetaMap
tool via the TEES preprocessing pipeline, allowing
it to be easily integrated into event and relation ex-
traction tasks.
Acknowledgments
We thank CSC ? IT Center for Science Ltd, Espoo,
Finland for providing computational resources.
4http://jbjorne.github.com/TEES/
657
neg brand drug n group drug
neg 99.57
99.60
99.60
0.04
0.03
0.03
0.00
0.00
0.01
0.15
0.14
0.14
0.24
0.22
0.22
brand 21.43
8.91
8.63
67.92
89.70
89.98
0.07
0.07
0.07
0.63
0.21
0.28
9.95
1.11
1.04
drug n 49.70
63.27
65.27
2.79
0.00
0.00
12.18
15.37
15.37
0.40
1.00
1.20
34.93
20.36
18.16
group 13.80
14.13
14.04
0.12
0.00
0.06
0.03
0.03
0.06
85.15
84.97
85.00
0.90
0.87
0.84
drug 6.71
5.60
6.20
0.69
0.27
0.32
0.10
0.08
0.08
0.75
0.79
0.69
91.75
93.27
92.72
Table 3: Task 9.1 drug name classification errors for the training corpus. Each cell in the table lists from top to
bottom results for models one to three (baseline, baseline+DrugBank, baseline+DrugBank+MetaMap). The results
are percentage of SVM examples of each class (vertical) classified into each potential class (horizontal).
neg int advise effect mechanism
neg 97.27
97.32
97.40
0.02
0.03
0.03
0.52
0.49
0.47
1.09
1.06
1.04
1.09
1.09
1.05
int 61.70
61.70
70.74
22.87
23.40
19.15
0.53
0.00
0.00
9.57
8.51
7.45
5.32
6.38
2.66
advise 34.50
34.02
33.54
0.12
0.24
0.24
60.17
60.05
60.77
4.24
4.36
4.36
0.97
1.33
1.09
effect 38.59
38.41
39.18
0.41
0.41
0.41
3.85
3.73
3.68
54.06
54.30
53.59
3.08
3.14
3.14
mechanism 50.34
48.75
52.16
0.15
0.15
0.23
2.05
1.82
1.29
5.08
5.08
5.00
42.38
44.20
41.32
Table 4: Task 9.2 drug-drug interaction classification errors for the training corpus. Each cell in the table lists from top
to bottom results for models one to three (baseline, baseline+DrugBank, baseline+DrugBank+MetaMap). The results
are percentage of SVM examples of each class (vertical) classified into each potential class (horizontal).
658
References
Alan R Aronson. 2001. Effective mapping of biomed-
ical text to the UMLS Metathesaurus: the MetaMap
program. In Proceedings of the AMIA Symposium,
page 17. American Medical Informatics Association.
Jari Bjo?rne, Antti Airola, Tapio Pahikkala, and Tapio
Salakoski. 2011. Drug-drug interaction extraction
from biomedical texts with SVM and RLS classifiers.
In Proc. of the 1st Challenge task on Drug-Drug In-
teraction Extraction (DDIExtraction 2011) at SEPLN
2011, volume 761, pages 35?42, Sept 5.
Jari Bjo?rne, Filip Ginter, and Tapio Salakoski. 2012.
University of Turku in the BioNLP?11 Shared Task.
BMC Bioinformatics, 13(Suppl 11):S4.
Olivier Bodenreider. 2004. The unified medical lan-
guage system (UMLS): integrating biomedical termi-
nology. Nucleic acids research, 32(suppl 1):D267?
D270.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL?05),
pages 173?180. Association for Computational Lin-
guistics.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher Manning. 2006. Generating typed depen-
dency parses from phrase structure parses. In Proceed-
ings of LREC-06, pages 449?454.
Kin Wah Fung, Chiang S Jao, and Dina Demner-
Fushman. 2013. Extracting drug indication informa-
tion from structured product labels using natural lan-
guage processing. Journal of the American Medical
Informatics Association.
Craig Knox, Vivian Law, Timothy Jewison, Philip Liu,
Son Ly, Alex Frolkis, Allison Pon, Kelly Banco,
Christine Mak, Vanessa Neveu, Yannick Djoumbou,
Roman Eisner, Anchi Guo, and David S. Wishart.
2011. Drugbank 3.0: a comprehensive resource for
omics research on drugs. Nucleic Acids Research,
39(Database-Issue):1035?1041.
David McClosky. 2010. Any domain parsing: auto-
matic domain adaptation for natural language pars-
ing. Ph.D. thesis, Department of Computer Science,
Brown University.
Sampo Pyysalo, Antti Airola, Juho Heimonen, Jari
Bjo?rne, Filip Ginter, and Tapio Salakoski. 2008.
Comparative analysis of five protein-protein interac-
tion corpora. BMC bioinformatics, 9(Suppl 3):S6.
Li Quanzhi and Wu Yi-Fang Brook. 2006. Identifying
important concepts from medical documents. Journal
of Biomedical Informatics, 39(6):668 ? 679.
Lawrence H Reeve, Hyoil Han, and Ari D Brooks. 2007.
The use of domain-specific concepts in biomedical text
summarization. Information Processing & Manage-
ment, 43(6):1765?1776.
Isabel Segura-Bedmar, Paloma Mart??nez, and Mar??a
Segura-Bedmar. 2008. Drug name recognition and
classification in biomedical texts: a case study out-
lining approaches underpinning automated systems.
Drug discovery today, 13(17):816?823.
Isabel Segura-Bedmar, Paloma Mart??nez, and Ce?sar
de Pablo-Sa?nchez. 2011a. A linguistic rule-based ap-
proach to extract drug-drug interactions from pharma-
cological documents. BMC bioinformatics, 12(Suppl
2):S1.
Isabel Segura-Bedmar, Paloma Mart??nez, and Daniel
Sa?nchez-Cisneros. 2011b. The 1st DDIExtraction-
2011 challenge task: extraction of drug-drug interac-
tions from biomedical texts. In Proceedings of the 1st
Challenge Task on Drug-Drug Interaction Extraction
2011: 7 Sep 2011; Huelva, Spain, pages 1?9.
Isabel Segura-Bedmar, Paloma Mart??nez, and Maria
Herrero-Zazo. 2013. Semeval-2013 task 9: Extrac-
tion of drug-drug interactions from biomedical texts.
In Proceedings of the 7th International Workshop on
Semantic Evaluation (SemEval 2013).
Luis Tari, Saadat Anwar, Shanshan Liang, James Cai,
and Chitta Baral. 2010. Discovering drug?drug inter-
actions: a text-mining and reasoning approach based
on properties of drug metabolism. Bioinformatics,
26(18):i547?i553.
Philippe Thomas, Mariana Neves, Ille?s Solt, Domonkos
Tikk, and Ulf Leser. 2011. Relation extraction for
drug-drug interactions using ensemble learning. In
Proc. of the 1st Challenge task on Drug-Drug Interac-
tion Extraction (DDIExtraction 2011) at SEPLN 2011,
page 11?18, Huelva, Spain, Sept 5.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hof-
mann, and Yasemin Altun. 2005. Large margin
methods for structured and interdependent output vari-
ables. Journal of Machine Learning Research (JMLR),
6(Sep):1453?1484.
Chen-May Wong, Yu Ko, and Alexandre Chan. 2008.
Clinically significant drug-drug interactions between
oral anticancer agents and nonanticancer agents: pro-
filing and comparison of two drug compendia. The
Annals of pharmacotherapy, 42(12):1737?1748.
659
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 807?811,
Dublin, Ireland, August 23-24, 2014.
UTU: Disease Mention Recognition and Normalization with CRFs and
Vector Space Representations
Suwisa Kaewphan
1,2,3?
, Kai Hakaka
1?
, Filip Ginter
1
1
Dept. of Information Technology, University of Turku, Finland
2
Turku Centre for Computer Science (TUCS), Turku, Finland
3
The University of Turku Graduate School (UTUGS), University of Turku, Finland
sukaew@utu.fi, kahaka@utu.fi, ginter@cs.utu.fi
Abstract
In this paper we present our system par-
ticipating in the SemEval-2014 Task 7
in both subtasks A and B, aiming at
recognizing and normalizing disease and
symptom mentions from electronic medi-
cal records respectively. In subtask A, we
used an existing NER system, NERsuite,
with our own feature set tailored for this
task. For subtask B, we combined word
vector representations and supervised ma-
chine learning to map the recognized men-
tions to the corresponding UMLS con-
cepts. Our system was placed 2nd and 5th
out of 21 participants on subtasks A and B
respectively showing competitive perfor-
mance.
1 Introduction
The SemEval 2014 task 7 aims to advance the de-
velopment of tools for analyzing clinical text. The
task is organized by providing the researchers an-
notated clinical records to develop systems that
can detect the mentions of diseases and symptoms
in medical records. In particular, the SemEval task
7 comprises two subtasks, recognizing the men-
tions of diseases and symptoms (task A) and map-
ping the mentions to unique concept identifiers
that belong to the semantic group of disorders in
the Unified Medical Language System (UMLS).
Our team participated in both of these sub-
tasks. In subtask A, we used an existing named
entity recognition (NER) system, NERsuite, sup-
plemented with UMLS dictionary and normaliza-
tion similarity features. In subtask B, we com-
bined compositional word vector representations
?
These authors contributed equally.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
with supervised machine learning to map the rec-
ognized mentions from task A to the UMLS con-
cepts. Our best systems, evaluated on strict match-
ing criteria, achieved F-score of 76.6% for the sub-
task A and accuracy of 60.1% for the subtask B,
showing competitive performance in both tasks.
2 Task A: Named Entity Recognition
with NERSuite
The ML approach based on conditional random
fields (CRFs) has shown to have state-of-the-art
performance in recognizing the biological entities.
We thus performed task A by using NERsuite,
an existing NER toolkit with competitive perfor-
mance on biological entity recognition (Campos
et al., 2013).
NERsuite is a NER system that is built on
top of the CRFsuite (Okazaki, 2007). It con-
sists of three language processing modules: a to-
kenizer, a modified version of the GENIA tagger
and a named entity recognizer. NERsuite allows
user-implemented features in addition to dictio-
nary matching and features shown to benefit the
systems such as raw token, lemma, part-of-speech
(POS) and text chunk.
Prior to detecting the disease mentions by the
recognizer module of NERsuite, the clinical text
is split into sentences by using GENIA Sentence
Splitter, a supervised ML system that is known to
be well optimized for biomedical texts (S?tre et
al., 2007). The sentences are subsequently tok-
enized and POS tagged.
To represent the positive entities, the ?BIO?
model was used in our system. The first tokens
of positive mentions are labeled with ?B? and the
rest with ?I?. Negative examples, non-entities, are
thus labeled with ?O?. This model was used for
both contiguous and discontiguous entities.
The features include the normalization similar-
ity (see Section 3.3), types of medical records (dis-
charge, echo, radiology and ecg), and UMLS dic-
807
Trained Model Precision Recall F-score
train + positive samples 77.3% 72.4% 74.8%
train + development 76.7 % 76.5% 76.6%
Table 1: The results of our different NERsuite
models, announced by the organizers.
tionary matching in addition to NERsuite?s own
feature generation.
The UMLS dictionary is prepared by extract-
ing the UMLS database for the semantic types de-
manded by the task. In addition to those 11 seman-
tic types, ?Finding? was also included in our dic-
tionary since, according to its definition, the con-
cept is also deemed relevant for the task. Due to
the common use of acronyms, which are not ex-
tensively provided by UMLS, we also extended
the coverage of our prepared UMLS dictionary
by extracting medical acronyms from the UMLS
database using regular expression.
We assessed the effect of dictionary matching
by training the models with and without the com-
piled UMLS dictionary and evaluating against the
development set. The model trained with dictio-
nary features outperformed the one without. The
best model was obtained by training the NERsuite
with UMLS dictionary in case-number-symbol
normalization mode. In this mode, all letters,
numbers and symbols are converted to lower case,
zero (0) and underscore ( ) respectively.
The regularization parameter (C2) was selected
by using development set to evaluate the best
model. The default parameter (C2 = 1.0) gave
the best performing system and thus was used
throughout the work.
Finally, for the NER task, we submitted two
models. The first model was trained with the orig-
inal training data and duplicates of sentences with
at least one entity mention. The second model
was trained by using the combination of the first
model?s training data and development set.
2.1 Results and Discussions
Our NER system from both submissions benefited
from the increased number of training examples
while the more diverse training data set gave a bet-
ter performance. The official results are shown in
table 1.
The analysis of our best performing NER sys-
tem is not possible since the gold standard of the
test data is not publicly available. We thus simply
analyze our second NER system based on the eval-
uation on the development data. The F-score of the
system was 75.1% and 88.0% for the strict and re-
laxed evaluation criteria respectively. Among all
the mistakes made by the system, the discontigu-
ous entities were the most challenging ones for the
NERsuite. In development data, the discontiguous
entities contribute about 10% of all entities, how-
ever, only 2% were recognized correctly. On the
contrary, the system did well for the other types as
73% were correctly recognized under strict crite-
ria. This demonstrates that the ?BIO? model has
limitations in representing the discontiguous enti-
ties. Improving the model to better represent the
discontiguous entities can possibly boost the per-
formance of the NER system significantly.
3 Task B: Normalization with
Compositional Vector Representations
Our normalization approach is based on con-
tinuous distributed word vector representations,
namely the state-of-the-art method word2vec
(Mikolov et al., 2013a). Our word2vec model
was trained on a subset of abstracts and full ar-
ticles from the PubMed and PubMed Central re-
sources. This data was used as it was readily
available to us from the EVEX resource (Van Lan-
deghem et al., 2013). Before training, all non-
alphanumeric characters were removed and all to-
kens were lower-cased. Even though a set of unan-
notated clinical reports was provided in the task
to support unsupervised learning methods, our ex-
periments on the development set showed better
performance with the model trained with PubMed
articles. This might be due to the size of the cor-
pora, as the PubMed data included billions of to-
kens whereas the provided clinical reports totaled
in over 200 million tokens.
The dimensionality of the word vectors was set
to 300 and we used the continuous skip-gram ap-
proach. For other word2vec parameters default
values were used.
One interesting feature demonstrated by
Mikolov et al. (2013b; 2013c) is that the vectors
conserve some of the semantic characteristics in
element-wise addition and subtraction. In this task
we used the same approach of simply summing
the word-level vectors to create compositional
vectors for multi-word entities and concepts, i.e.
we looked up the vectors for every token appear-
ing in a concept name or entity and summed them
to form a vector to represent the whole phrase.
808
We then formed a lexicon including all preferred
terms and synonyms of all the concepts in the
subset of UMLS defined in the task guidelines.
This lexicon is a mapping from the compositional
vector representations of the concept names into
the corresponding UMLS identifiers. To select the
best concept for a recognized entity we calculated
cosine similarity between the vector representa-
tion of the given entity and all the concept vectors
in the lexicon and the concept with the highest
similarity was chosen.
Word2vec is generally able to relate different
forms of the same word to each other, but we no-
ticed a small improvement in accuracy when pos-
sessive suffixes were removed and all tokens were
lemmatized.
3.1 Detecting CUI-less Mentions
As some of the mentions in the training data do not
have corresponding concepts in the semantic cat-
egories listed in the task guidelines, they are an-
notated as ?CUI-less?. However, our normaliza-
tion approach will always find the nearest match-
ing concept, thus getting penalized for wrong pre-
dictions in the official evaluation. To overcome
this problem, we implemented three separate steps
for detecting the ?CUI-less? mentions. As the
simplest approach we set a fixed cosine similarity
threshold and if the maximal similarity falls below
it, the mention is normalized to ?CUI-less?. The
threshold value was selected using a grid search to
optimize the performance on the official develop-
ment set. Although this method resulted in decent
performance, it is not capable of coping with cases
where the mention has very high similarity or even
exact match with a concept name. For instance
our system normalized ?aspiration? mentions into
UMLS concept ?Pulmonary aspiration? which has
a synonym ?Aspiration?, thus resulting in an exact
match. To resolve this kind of cases, we used sim-
ilar approach as in the DNorm system (Leaman et
al., 2013b), where the ?CUI-less? mentions occur-
ring several times in the training data were added
to the concept lexicon with concept ID ?CUI-less?.
As the final step we trained a binary SVM classi-
fier to distinguish the ?CUI-less? mentions. The
classifier utilized bag-of-word features as well as
the compositional vectors. The performance im-
provement provided by each of these steps is pre-
sented in table 2. This evaluation shows that each
step increases the performance considerably, but
Method Strict accuracy
B 43.6
T 48.4
T + L 53.5
T + L + C 55.4
O 59.3
Table 2: Evaluation of the different approaches
to detect CUI-less entities on the official develop-
ment set compared to a baseline without CUI-less
detection and an oracle method with perfect de-
tection. This evaluation was done with the entities
recognized by our NER system instead of the gold
standard entities. B = baseline without CUI-less
detection, T = similarity threshold, L = Lexicon-
based method, C = classifier, O = Oracle.
the overall performance is still 3.9pp below per-
fect detection.
3.2 Acronym Resolution
Abbreviations, especially acronyms, form a con-
siderable portion of the entity mentions in clini-
cal reports. One of the problems in normalizing
the acronyms is disambiguation as one acronym
can be associated with multiple diseases. Previ-
ous normalization systems (Leaman et al., 2013b)
handle this by selecting the matching concept with
most occurrences in the training data. However,
this approach does not resolve the problem of
non-standard acronyms, i.e. acronyms that are not
known in the UMLS vocabulary or in other medi-
cal acronym dictionaries. Our goal was to resolve
both of these problems by looking at the other enti-
ties found in the same document instead of match-
ing the acronym against the concept lexicon. With
this approach for instance entity mention ?CP?
was on multiple occasions correctly normalized
into the concept ?Chest Pain?, even though UMLS
is not aware of this acronym for the given concept
and in fact associates it with several other con-
cepts such as ?Chronic Pancreatitis? and ?Cerebral
Palsy?. However, the overall gain in accuracy ob-
tained from this method was only minor.
3.3 Normalization Feedback to Named
Entity Recognition
While basic exact match dictionary features pro-
vide usually a large improvement in NER perfor-
mance, they are prone to bias the system to high
precision and low recall. As both noun and ad-
jective forms of medical concepts, e.g. ?atrium?
and ?atrial?, are commonly used in clinical texts,
809
the entities may not have exact dictionary matches.
Moreover the different forms of medical terms
may not share a common morphological root dis-
covered by simple stemming methods, thus com-
plicating approximate matching. In this task we
tried to boost the recall of our entity recognition by
feeding back the normalization similarity informa-
tion as features. These features included the max-
imum similarity between the token and the UMLS
concepts as a numerical value as well as a boolean
feature describing whether the similarity exceeded
a certain threshold.
In addition we experimented by calculating the
similarities for bigrams and trigrams in a slid-
ing window around the tokens, but these features
did not provide any further performance improve-
ments.
3.4 Other Directions Explored
The DNorm system utilizes TF-IDF vectors to rep-
resent the entities and concepts but instead of cal-
culating cosine similarity, the system trains a rank-
ing algorithm to measure the maximal similarity
(Leaman et al., 2013a). Their evaluation, carried
out on the NCBI disease corpus (Do?gan et al.,
2014), showed a notable improvement in perfor-
mance compared to cosine similarity. In our anal-
ysis we noticed that in 39% of the false predic-
tions made by our normalization system, the cor-
rect concept was in the top 10 most similar con-
cepts. This strongly suggested that a similar rank-
ing method might be beneficial with our system as
well. To test this we trained a linear SVM to rerank
the top 10 concepts with highest cosine similarity,
but we were not able to increase the overall per-
formance of the system. However, due to the strict
time constraints of the task, we cannot conclude
whether this approach is feasible or not.
As our compositional vectors are formed by
summing the word vectors, each word has an equal
weight in the sum. Due to this our system made
various errors where the entity was a single word
matching closely to several concepts with longer
names. For instance entity ?hypertensive? was
falsely normalized to concept ?Hypertensive car-
diopathy? whereas the correct concept was ?Hy-
pertensive disorder?. These mistakes could have
been prevented to some extent if the more impor-
tant words had had a larger weight in the sum, e.g.
word ?disorder? is of low significance when try-
ing to distinguish different disorders. However,
Team Strict accuracy Relaxed accuracy
UTH CCB 74.1 87.3
UWM 66.0 90.9
RelAgent 63.9 91.2
IxaMed 60.4 86.2
UTU 60.1 78.3
Table 3: Official evaluation results for the top 5
teams in the normalization task.
weighting the word vectors with their IDF values,
document in this case being an UMLS concept, did
not improve the performance.
3.5 Results
The official results for the normalization task are
shown in table 3. Our system achieved accuracy
of 60.1% when evaluated with the official strict
evaluation metric. This result suggests that com-
positional vector representations are a competitive
approach for entity normalization. However, the
best performing team surpassed our performance
by 14.0pp, showing that there is plenty of room for
other teams to improve. It is worth noting though
that their recall in the NER task tops ours by 8.2pp
thus drastically influencing the normalization re-
sults as well. To evaluate the normalization sys-
tems in isolation from the NER task, a separate
evaluation set with gold standard entities should
be provided.
4 Conclusions
Overall, our NER system can perform well with
the same default settings of NERsuite for gene
name recognition. The performance improves
when relevant features, such as UMLS dictionary
matching and word2vec similarity are added. We
speculated that representing the nature of the data
with more suitable model can improve the system
performance further. As a part of a combined sys-
tem, the improvement on NER system can result
in the increased performance of normalization sys-
tem.
Our normalization system showed competitive
results as well, indicating that word2vec-based
vector representations are a feasible way of solv-
ing the normalization task. As future work we
would like to explore different methods for cre-
ating the compositional vectors and reassess the
applicability of the reranking approach described
in section 3.4.
810
Acknowledgements
Computational resources were provided by CSC
? IT Center for Science Ltd, Espoo, Finland. This
work was supported by the Academy of Finland.
References
David Campos, S?ergio Matos, and Jos?e Lu??s Oliveira.
2013. Gimli: open source and high-performance
biomedical name recognition. BMC bioinformatics,
14(1):54.
Rezarta Islamaj Do?gan, Robert Leaman, and Zhiyong
Lu. 2014. NCBI disease corpus: a resource for dis-
ease name recognition and concept normalization.
Journal of Biomedical Informatics, 47:1?10, Feb.
Robert Leaman, Rezarta Islamaj Do?gan, and Zhiyong
Lu. 2013a. DNorm: disease name normaliza-
tion with pairwise learning to rank. Bioinformatics,
29(22):2909?2917.
Robert Leaman, Ritu Khare, and Zhiyong Lu.
2013b. NCBI at 2013 ShARe/CLEF eHealth Shared
Task: Disorder normalization in clinical notes with
DNorm. In Proceedings of the Conference and Labs
of the Evaluation Forum.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In ICLR Workshop.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013b. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In NIPS, pages 3111?3119.
Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In HLT-NAACL, pages 746?
751.
Naoaki Okazaki. 2007. CRFsuite: a fast im-
plementation of conditional random fields (CRFs).
http://www.chokkan.org/software/crfsuite/.
Rune S?tre, Kazuhiro Yoshida, Akane Yakushiji,
Yusuke Miyao, Y Matsubyashi, and Tomoko Ohta.
2007. AKANE system: protein-protein interaction
pairs in BioCreAtIvE2 challenge, PPI-IPS subtask.
In Proceedings of the BioCreative II, pages 209?
212.
Sofie Van Landeghem, Jari Bj?orne, Chih-Hsuan Wei,
Kai Hakala, Sampo Pyysalo, Sophia Ananiadou,
Hung-Yu Kao, Zhiyong Lu, Tapio Salakoski, Yves
Van de Peer, and Filip Ginter. 2013. Large-
scale event extraction from literature with multi-
level gene normalization. PLoS ONE, 8(4):e55814.
811
Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 63?71,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Evaluating large-scale text mining applications
beyond the traditional numeric performance measures
Sofie Van Landeghem1,2, Suwisa Kaewphan3,4, Filip Ginter3, Yves Van de Peer1,2
1. Dept. of Plant Systems Biology, VIB, Belgium
2. Dept. of Plant Biotechnology and Bioinformatics, Ghent University, Belgium
3. Dept. of Information Technology, University of Turku, Finland
4. Turku Centre for Computer Science (TUCS), Turku, Finland
solan@psb.ugent.be, sukaew@utu.fi
ginter@cs.utu.fi, yvpee@psb.ugent.be
Abstract
Text mining methods for the biomedical
domain have matured substantially and
are currently being applied on a large
scale to support a variety of applica-
tions in systems biology, pathway cura-
tion, data integration and gene summa-
rization. Community-wide challenges in
the BioNLP research field provide gold-
standard datasets and rigorous evaluation
criteria, allowing for a meaningful com-
parison between techniques as well as
measuring progress within the field. How-
ever, such evaluations are typically con-
ducted on relatively small training and
test datasets. On a larger scale, sys-
tematic erratic behaviour may occur that
severely influences hundreds of thousands
of predictions. In this work, we per-
form a critical assessment of a large-scale
text mining resource, identifying system-
atic errors and determining their underly-
ing causes through semi-automated analy-
ses and manual evaluations1.
1 Introduction
The development and adaptation of natural lan-
guage processing (NLP) techniques for the
biomedical domain are of crucial importance to
manage the abundance of available literature. The
inherent ambiguity of gene names and complex-
ity of biomolecular interactions present an intrigu-
ing challenge both for BioNLP researchers as well
as their targeted audience of biologists, geneticists
and bioinformaticians. Stimulating such research,
various community-wide challenges have been or-
ganised and received international participation.
1The supplementary data of this study is freely avail-
able from http://bioinformatics.psb.ugent.
be/supplementary_data/solan/bionlp13/
The BioCreative (BC) challenge (Hirschman et
al., 2005; Krallinger et al, 2008; Leitner et al,
2010; Arighi et al, 2011) touches upon a variety of
extraction targets. The identification of gene and
protein mentions (?named entity recognition?) is a
central task and a prerequisite for any follow-up
work in BioNLP. Linking these mentions to their
respective gene database identifiers, ?gene normal-
ization?, is a crucial step to allow for integration
of textual information with authoritative databases
and experimental results. Other BC tasks are en-
gaged in finding functional and physical relations
between gene products, including Gene Ontology
annotations and protein-protein interactions.
Focusing more specifically on the molecu-
lar interactions between genes and proteins, the
BioNLP Shared Task on Event Extraction (Kim et
al., 2009; Kim et al, 2011b; Nedellec and others,
2013) covers a number of detailed molecular event
types, including binding and transcription, regula-
tory control and post-translational modifications.
Additionally, separate tracks involve specific ap-
plications of event extraction, including infectious
diseases, bacterial biotopes and cancer genetics.
Performance of the participants in each of these
challenges is measured using numeric metrics
such as precision, recall, F-measure, slot error
rate, MAP and TAP scores. While such rig-
urous evaluations allow for a meaningful compar-
ison between different systems, it is often difficult
to translate these numeric values into a measure-
ment of practical utility when applied on a large
scale. Additionally, infrequent but consistent er-
rors are often not identified through small-scale
evaluations, though they may result in hundreds of
thousands of wrongly predicted interactions on a
larger scale. In this work, we perform an in-depth
study of an open-source state-of-the-art event ex-
traction system which was previously applied to
the whole of PubMed. Moving beyond the tra-
ditional numeric evaluations, we identify a num-
63
Figure 1: Example event and relation represen-
tations, depicted in solid and dotted lines respec-
tively. Picture by Kim et al (2011a).
ber of systematic errors in the large-scale data,
analyse their underlying causes, and design post-
processing rules to resolve these errors. We be-
lieve these findings to be highly relevant for any
practical large-scale implementation of BioNLP
techniques, as the presence of obvious mistakes in
a text mining resource might undermine the credi-
bility of text mining techniques in general.
2 Data and methods
In this section, we first describe the data and meth-
ods used in previous work for the construction
of the large-scale text mining resource that is the
topic of our error analyses (Section 3).
2.1 Event extraction
Event extraction has become a widely studied
topic within the field of BioNLP following the
first Shared Task (ST) in 2009. The ST?09 in-
troduced the event formalism as a more detailed
representation of the common binary relation an-
notation (Figure 1). Each event occurrence con-
sists of an event trigger; i.e. one or more con-
secutive tokens that are linked to a specific event
type. While the ST?09 included only 9 event types,
among which 3 regulatory event types, the ST?11
further broadened the coverage of event extraction
to post-translational modifications and epigenetics
(EPI).
To compose a fully correct event, an event trig-
ger needs to be connected to its correct arguments.
Within the ST, these arguments are selected from a
set of gold-standard gene and gene product anno-
tations (GGPs). The ST guidelines determine an
unambiguous formalism to which correct events
must adhere: most event types only take one theme
argument, while Binding events can be connected
to more than one theme. Regulation events further
have an optional cause slot (Figure 1). Connecting
the correct arguments to the correct trigger words
is denoted as ?edge detection?.
To perform event extraction, we rely on the
publicly available Turku Event Extraction System
(TEES) (Bjo?rne et al, 2012), which was origi-
nally developed for the ST?09. The TEES mod-
ules for trigger and edge detection are based upon
supervised learning principles, employing support
vector machines (SVMs) for multi-label classifi-
cation. TEES has been shown to obtain state-of-
the-art performance when measured on the gold-
standard datasets of the Shared Tasks of 2009,
2011 and 2013.
2.2 Large-scale processing
Previously, the whole of PubMed has been anal-
ysed using a large-scale event extraction pipeline
composed of the BANNER named entity rec-
ognizer, the McClosky-Charniak parser, and the
Turku Event Extraction System (Bjo?rne et al,
2010). BANNER identifies gene and protein sym-
bols in text through a machine learning approach
based on conditional random fields (Leaman and
Gonzalez, 2008). While the resulting large-scale
text mining resource EVEX was focused only on
abstracts and ST?09 event types (Van Landeghem
et al, 2011), it has matured substantially during
the past few years and now includes ST?11 EPI
event types, full-text processing and gene normal-
ization (Van Landeghem et al, 2013a). In this
work, we use the version of EVEX as publicly
available on 16 March 2013, containing 40 million
event occurrences among 122 thousand gene and
protein symbols in 22 million PubMed abstracts
and 460 thousand PubMed Central full-text arti-
cles. Each event occurrence is linked to a normal-
ized confidence value, automatically derived from
the original TEES SVM classification step and the
distance to the hyperplane of each prediction.
While this study focuses on the EVEX resource
as primary dataset, the findings are also highly rel-
evant for other large-scale text mining resources,
especially those based on supervised learning,
such as the BioContext (Gerner et al, 2012).
2.3 Cross-domain evaluation
Recently, a plant-specific, application-oriented as-
sessment of the EVEX text mining resource has
been conducted by manually evaluating 1,800
event occurrences (Van Landeghem et al, 2013b).
In that study, it was established that the general
performance rates as measured previously on the
ST, are transferrable also to other domains and or-
ganisms. Specifically, the 58.5% TEES precision
64
Event type Five most frequent trigger words
Binding binding interaction associated bind association
Gene expression expression expressed production expressing levels
Localization secretion release localization secreted localized
Protein catabolism degradation degraded cleavage proteolysis degrade
Transcription transcription expression levels transcribed detected
Acetylation acetylation acetylated deacetylation hyperacetylation activation
Glycosylation glycosylated glycosylation attached N-linked absence
Hydroxylation hydroxylation hydroxylated hydroxylate beta-hydroxylation hydroxylations
Methylation radiation methylation methylated diffractometer trimethylation
DNA methylation methylation hypermethylation methylated hypermethylated unmethylated
Phosphorylation phosphorylation phosphorylated dephosphorylation phosphorylates phosphorylate
Ubiquitination ubiquitination ubiquitinated ubiquitylation ubiquitous polyubiquitination
Regulation effect regulation effects regulated control
Positive regulation increased activation increase induced induction
Negative regulation reduced inhibition decreased inhibited inhibitor
Catalysis mediated dependent mediates removes induced
Table 1: The top-5 most frequently tagged trigger words per event type in EVEX. The first 5 rows
represent fundamental event types, the next 7 post-translational modifications (PTMs), and the last 4
rows are regulatory event types. In this analysis, the PTMs and their reverse types are pooled together.
Trigger words that refer to systematic errors are in italic and are discussed further in the text.
rate measured in the ST?09, with the literature data
concerning human blood cell transcription factors,
corresponded with a 58.6% precision rate for the
plant-specific evaluation dataset (?PLEV?). This
encouraging result supports the general applicabil-
ity of large-scale text mining methods trained on
relatively small corpora. The findings of this pre-
vious study and the resulting data are further inter-
preted and analysed in more detail in this study.
3 Results
While the text mining pipeline underlying the
EVEX resource has been shown to produce state-
of-the-art results which are transferrable across
domains and organisms, it is conceivable that the
mere scale of the resource allows the accumula-
tion of systematic errors. In this section, we per-
form several targeted semi-automated evaluations
to identify, explain and resolve such cases. It is
important to note that our main focus is on im-
proving the precision rate of the resource, rather
than the recall, aiming to increase the credibility
of large-scale text mining resources in general.
3.1 Most common triggers
The trigger detection algorithm of the TEES soft-
ware is based upon SVM classifiers (Section 2.1),
and has been shown to outperform dictionary-
based approaches (Kim et al, 2009; Kim et al,
2011c). To investigate its performance in a large-
scale application, we first analyse the most fre-
quent trigger words of each event type in EVEX
(Table 1). We notice the presence of different in-
flections of the same word as well as related verbs
and nouns, such as ?inhibition?, ?inhibited? and
?inhibitor?. The trigger recognition module suc-
cessfully uses character bigrams and trigrams in
its SVM classification algorithm to allow for the
identification of such related concepts, even when
some of these trigger words were not encountered
in the training phase (Bjo?rne et al, 2009).
However, occasionally this approach results in
confusion between words with small edit dis-
tances, such as the trigger word ?ubiquitous? for
Ubiquitination events. Similarly, the Acetylation
trigger ?activation? is found within the context of
a correct event structure in most cases, but should
actually be of the type Positive regulation. The
implementation of custom post-processing rules
to automatically detect and resolve these specific
cases would ultimately deal with more than 6,000
false-positive event predictions.
Further, the trigger ?radiation? seems to occur
frequently for a Methylation event, of which 82%
of the instances can be identified in the ?Exper-
imental? subsection of the article. The majority
of these articles relate to protein crystallography,
and that subsection describes the data from the ex-
perimental set-up. Within such sections, phrases
like ?Mo Kalpha radiation? are wrongly tagged as
Methylation events. Similarly, many false-positive
Methylation predictions refer to the trigger word
?diffractometer?. Removing these instances from
the resource would result in the deletion of more
65
Trigger word s Most frequent type t2 Count Frequency Infrequent type t1 Count Frequency
acetylation Acetylation 40,291 0.298383 Binding 1,332 0.000216
Phosphorylation 1,050 0.001045
Gene expression 969 0.000093
Localization 1,045 0.000579
secretion Localization 376,976 0.208888 Acetylation 243 0.001800
glycosylation Glycosylation 24,226 0.141052 Phosphorylation 389 0.000387
Gene expression 214 0.000020
phosphorylation Phosphorylation 589,681 0.586772 Binding 454 0.000074
DNA methylation 225 0.001297
ubiquitylation Ubiquitination 4961 0.055976 Binding 128 0.000021
hypermethylation Methylation 19,501 0.112434 Phosphorylation 365 0.000363
cleavage Protein catabolism 20,552 0.073728 Gene expression 2,451 0.000234
Binding 3,011 0.000489
decreased Negative regulation 374,859 0.062372 Positive regulation 1,721 0.000173
Binding 855 0.000139
Gene expression 2,928 0.000280
reduced Negative regulation 442,400 0.073610 Positive regulation 1,091 0.000110
reduction Negative regulation 164,736 0.027410 Positive regulation 389 0.000039
absence Negative regulation 65,180 0.010845 Positive regulation 226 0.000071
Table 2: Examples of trigger words that correspond to the type which has the highest relative frequency
(left), but are also found with much lower frequencies in other types (right). The instances corresponding
to the right-most column can thus be interpreted as wrong predictions. The full list is available as a
machine readible translation table in the supplementary data.
than 82,000 false-positive event predictions.
Finally, we notice that the trigger word ?ab-
sence? for Glycosylation usually refers to a Neg-
ative regulation. Similarly, some words appear as
most frequent for more than one event type, such
as ?levels? (Gene expression and Transcription).
This type of error in trigger type disambiguation
is analysed in more detail in the next section.
3.2 Event type disambiguation
While previous work has focused on the disam-
biguation of event types on a small, gold-standard
dataset (Martinez and Baldwin, 2011), the rich-
ness of a large-scale text mining resource provides
additional opportunities to detect plausible errors.
To exploit this large-scale information, we anal-
yse all EVEX trigger words and their correspond-
ing event types, summarizing their raw event oc-
currence counts as Occ(t, s) where t denotes the
trigger type and s the trigger string. As some
event types are more abundantly described in lit-
erature, we normalize these counts to frequen-
cies (Freq(t, s)) depending on the total number
of event occurrences per type (Tot(t)):
Freq(t, s) =
Occ(t, s)
Tot(t)
with
Tot(t) =
n?
i=1
Occ(t, si)
and n the number of different triggers for event
type t. We then compare all trigger words and their
relative frequencies across different event types.
First, we inspect those cases where a trigger
word appears with comparable frequencies for two
event types t1 and t2:
Freq(t1, s) ? Freq(t2, s) ? 10? Freq(t1, s)
(1)
A first broad category of these cases are trig-
ger words that refer to both regulatory and non-
regulatory events at the same time, such as ?over-
expression? (Gene expression and Positive regula-
tion), or ?ubiquitinates? (Ubiquitination and Catal-
ysis). The majority of these cases are perfectly
valid and are in fact modeled explicitly by the
TEES software (Bjo?rne et al, 2009).
Further, we find that two broad groups of non-
regulatory event types are semantically similar and
share common trigger words: Methylation and
DNA methylation (e.g. ?methylation?, ?unmethy-
lated?, ?hypomethylation?), as well as Gene ex-
pression and Transcription (?expression?, ?synthe-
sis?, ?levels?), with occasional overlap also with
Localization (?abundance?, ?found?). Similarly,
trigger words are often shared among the four
regulatory event types (?dependent?, ?role?, ?regu-
late?), as the exact type may depend on the broader
context within the sentence.
While the previous findings do not necessar-
66
Predicted event type
Curated event type Localization Transcription Expression
Localization 15 0 3
Transcription 0 12 1
Expression 0 2 12
No event 0 2 3
Total 15 16 19
Table 3: Targeted evaluation of 50 mixed events of type Localization, Transcription and Gene expression.
The curated event type is compared to the original (hidden) predicted type.
ily refer to wrong predictions, we also notice the
usage of punctuation marks as trigger words for
various event types. This option was specifically
provided in the TEES trigger detection algorithm
as the ST?09 training data contains Binding in-
stances with ?-? as trigger word. However, these
punctuation triggers are found to be largely false
positives in the PubMed-scale event dataset. Re-
moving them in an additional post-processing step
would result in the filtering of more than 130,000
event occurrences, of which the largest part is ex-
pected to be incorrect predictions. Similarly, we
can easily remove 25,000 events that are related to
trigger words that are numeric values.
In a second step, we analyse those cases where
k ? Freq(t1, s) ? Freq(t2, s). (2)
When this condition holds, it can be hypothesized
that trigger predictions of the word s as type t1
are false positives and should have instead been of
type t2. Automatically generating such lists from
the data, we have experimentally determined an
optimal value of k = 100 that represents a reason-
able trade-off between the amount of false posi-
tives that can be identified and the manual work
needed for this.
From the resulting list, we can easily identify a
number of such cases that are clearly incorrect (Ta-
ble 2, right column). Specifically, a large number
of Positive regulation events actually refer to Neg-
ative regulation, providing an explanation of the
lower precision rate of Positive regulation predic-
tions in the previous PLEV evaluation (Van Lan-
deghem et al, 2013b). This semi-automated de-
tection procedure can ultimately result in the cor-
rection of more than 242,000 events.
The remaining cases for which condition (2)
holds are more ambiguous and can not be au-
tomatically corrected. However, these cases are
more likely to be incorrect and their confidence
values could thus be automatically decreased de-
pending on the ratio between Freq(t1, s) and
Freq(t2, s). A general exception to this rule is
formed by the broad groups of semantically simi-
lar events, such as Transcription-Gene expression-
Localization, which we analyse in more detail in
the next section.
3.3 Gene expression, Transcription and
Localization
Transcription is a sub-process of Gene expression,
with both event types relating to protein produc-
tion. However, the distinction between the two in
text may not always be straightforward. Addition-
ally, the ST training data for Transcription events
is significantly smaller than for Gene expression
events, which may be the reason why not only the
TEES performance, but also those of other sys-
tems, is considerably lower for Transcription than
for Gene expression (Kim et al, 2011c). Further,
cell-type specific gene expression should be cap-
tured by additional site arguments connected to a
Localization event, which represents the presence
or a change in the location of a protein.
To gain a deeper insight into the interplay be-
tween these three different event types, we have
performed a manual curation of 50 event occur-
rences, sampled at random from the Gene expres-
sion, Transcription and Localization events avail-
able in EVEX. For each event, the trigger word
and the corresponding sentence was extracted, but
the predicted event type was hidden. An expert an-
notator subsequently decided on the correct event
type of the trigger. Within this evaluation we fol-
lowed the ST guidelines to only annotate Gene ex-
pression when there is no evidence for the more
detailed Transcription type.
Table 3 shows the results. All 15 predicted
Localization triggers are recorded to be correct.
From the 16 predicted Transcription events, two
involve incorrect event triggers, and two other
events refer to the more general Gene expression
type (75% overall precision). Likewise, only one
Gene expression event should be of the more spe-
67
Curated event type Error type Instances (%)
1 Single-argument Binding No error 5 10%
2 Single-argument Binding Edge detection error 0 0%
3 Multiple-argument Binding Edge detection error 4 8%
4 Single-argument Binding Entity recognition error 1 2%
5 Multiple-argument Binding Entity recognition error 19 38%
6 Other Trigger detection error 21 42%
Table 4: Targeted evaluation of 50 single-argument Binding event triggers. Row 1: Fully correct event.
Row 2: The correct argument was annotated but not linked. Row 3: At least one correct multiple-
argument Binding event could have been extracted using the annotated entities in the sentence. Row 4:
The correct argument was not annotated. Row 5: No event could be extracted due to missing argument
annotations. Row 6: The trigger did not refer to a Binding event.
Unannotated entity type Entity occurrence count Examples
GGP 10 SPF30, spinal muscular atrophy gene
Generic GGP 9 primary antibodies, peptides, RNA
Chemical compound 10 Ca(2+), iron, manganese(II)
Table 5: Manual inspection of the textual entity types for those Binding events where a relevant theme
argument was not annotated in the entity recognition step.
cific Transcription type, three instances should be
Localization, and three more are considered not to
be correct events at all (63% overall precision). In
general, we remark that the predicted event type
largely corresponds to the curated type (78% of
all predictions and 87% of all otherwise correct
events).
3.4 Binding
Moving beyond the event type specification as
determined by the ST guidelines, the previous
PLEV analysis (Section 2.3) has established a re-
markable difference between single-argument and
multiple-argument Binding. In contrast to the reg-
ular ST evaluations, this work considered single-
and multiple-argument Binding as two separate
event types, resulting in a precision rate of 93% for
multiple-argument Binding triggers and only 8%
precision for single-argument Binding triggers.
As the PLEV study only focused on textual
network data, single-argument Bindings were not
analysed further. In this work however, we fur-
ther investigate this performance discrepancy and
perform an in-depth manual evaluation to try and
detect the main causes of this systematic error.
Several hypotheses can be postulated to explain
the low precision rate of single-argument Binding
events. Firstly, a false negative instance of the
entity recognition module might result in the ab-
sence of annotation for a relevant second interac-
tion partner. Another plausible explanation is an
error by the edge detection module of the event
extraction mechanism, which would occasionally
decide to produce one or several single-argument
Binding events rather than one multiple-argument
Binding, even when all involved entities are cor-
rectly annotated. Finally, it is conceivable that
predicted single-argument triggers simply do not
refer to Binding events, i.e. they contain false pos-
itive predictions of the trigger detection module of
the event extraction system.
In some cases, one trigger leads to many dif-
ferent Binding events, such as the trigger ?bind?
in the sentence ?Sir3 and Sir4 bind preferentially
to deacetylated tails of histones H3 and H4?. In
these cases, error types may accumulate: some
events could be missed due to unannotated enti-
ties, while others may be due to errors in the edge
detection step. However, multiple events with the
same trigger word are often represented by very
similar feature vectors in the classification step,
and consequently have almost identical final con-
fidence values. For this reason, we summarize the
error as ?Edge detection error? as soon as one pair
of entities was correctly annotated but not linked,
and as ?Entity recognition error? otherwise.
Table 4 summarizes the results of a curation
effort of 50 event triggers linked to a single-
argument Binding event in EVEX. We notice
that in fact, 46% should have been multiple-
argument Binding events. The main underlying
reason for the prediction of an incorrect single-
argument Binding event, when it should have been
a multiple-argument one, is apparently caused by
68
Curated event type Error type Instances (%)
1 Phosphorylation No error 34 68%
2 Phosphorylation Edge detection error 4 8%
3 Invalid Phosphorylation Edge detection error 2 4%
4 Phosphorylation Edge directionality detection error 4 8%
5 Invalid Phosphorylation Edge directionality detection error 1 2%
6 Phosphorylation Entity recognition error 3 6%
7 Other Trigger detection error 2 4%
Table 6: Targeted evaluation of 50 Phosphorylation event triggers and their theme arguments. Row 1:
Fully correct event. Row 2: The correct argument was annotated but not linked. Row 3: An argument
was linked but should not have been. Row 4: A causal argument was wrongly annotated as the theme
argument. Row 5: A causal argument was wrongly annotated as the theme argument. Row 6: The correct
argument was not annotated. Row 7: The trigger did not refer to a Phosphorylation event.
an entity recognition error (19/23 or 83%), while
an edge detection error is much less frequent
(17%). When we examine these entity recogni-
tion errors in more detail, we find that 10 rele-
vant entities are true GGPs in the sense of the
Shared Task annotation. However, 9 entities refer
to generic GGPs, and 10 instances relate to chemi-
cal compounds (Table 5). As these type of entities
can not be unambiguously normalized to unique
gene identifiers, they fall out-of-scope of the orig-
inal ST challenge. However, we feel this practice
introduces an artificial bias on the classifier and
the evaluation. Additionally, this information can
prove to be of value within a large-scale text min-
ing resource geared towards practical applications
and explorative browsing of textual information.
Finally, we notice that a remarkable 42% of all
predicted events contain trigger detection errors.
Analysing this subclass in more detail, we found
that 5 cases are invalid event triggers, 6 cases re-
fer to other event types such as Localization and
Gene expression, and 10 more cases were consid-
ered to be out-of-scope of the ST challenge, such
as a factor-disease association.
3.5 Phosphorylation
Within the PLEV evaluation (Section 2.3), it be-
came apparent that Phosphorylation is easy to
recognise from the sentence (98%) but the full cor-
rect event has a much lower precision rate (65%).
As we have seen in the previous section, even
when a trigger word is correctly predicted, errors
may still be generated by the edge detection or en-
tity recognition step. For instance, we might hy-
pothesize that the main underlying reason for the
reduced final performance is an error by the en-
tity recognition step, forcing the edge detection
mechanism to link an incorrect theme due to lack
of other options. Other plausible explanations in-
volve genuine errors by the edge detection algo-
rithm when the correct argument is annotated, as
well as problems with the identification of causal-
ity. As the TEES version applied in this work was
developed for the Shared Task 2009 and 2011, it
does not predict causal arguments for a Phospho-
rylation event directly, but instead adds Regulation
events on top of the Phosphorylations. Occasion-
ally, we have noticed that the theme of a Phospho-
rylation event should in fact have been the cause
of the embedding Regulation association, resulting
in a wrongly directed causal relationship.
To investigate these possibilities, we have man-
ually inspected 50 Phosphorylation events picked
at random from the EVEX resource. Table 6 sum-
marizes the results of this effort. Only two events
are found not to be Phosphorylation events: one
is in fact a Gene expression mention, the other
involves an incorrect trigger. Additionally, three
more events can semantically be regarded as Phos-
phorylations, but do not follow the ST specifica-
tions (?Invalid Phosphorylation?), for instance be-
cause they only mention causal arguments (?an
inhibition of Ca2+/calmodulin-dependent protein
phosphorylation?). Among the 45 cases which
correctly refer to the Phosphorylation type, 34
events are fully correct (68% of the total). Four
cases are wrongly extracted by misinterpreting the
causal relationship (?Edge directionality detection
error?) and four more instances refer to genuine
mistakes of the edge detection algorithm. Only
three other cases can be attributed to a missing en-
tity annotation. In contrast to the previous find-
ings on single-argument Bindings, we thus es-
tablish that the incorrect Phosphorylation events
are mainly caused by errors in the edge detection
mechanism, which either picks the wrong theme
69
from the set of annotated GGPs, or misinterprets
the causality direction.
4 Discussion and conclusion
We have performed several semi-automated eval-
uations and targeted manual curation experiments,
identifying and explaining systematic errors in a
large-scale event dataset. As a first observation,
we notice that a few frequent trigger words are
almost always associated to incorrect event pre-
dictions, such as the trigger words ?ubiquitous?
and ?radiation?, or a punctuation symbol. These
cases were identified through a large-scale auto-
matic analysis in combination with a limited man-
ual evaluation effort. The results are distributed as
a blacklist of event triggers for the implementation
or filtering of future large-scale event predictions
efforts.
Further, a semi-automated procedure has iden-
tified a list of likely incorrect predictions, by
comparing the type-specific frequencies of trigger
words across all event types. Manual inspection of
the most frequent cases allowed us to determine a
number of trigger words for which the event type
can automatically be corrected. These results are
also made publicly available.
Additionally, after removal of the most obvi-
ous and frequent errors, a fully automated script
can automatically reduce the confidence scores of
those event occurrences where the trigger words
are found to be much more frequent for another
event type. We have established that this proce-
dure should disregard triggers identified within a
few specific semantically similar clusters: DNA
methylation/Methylation, Regulation/Positive reg-
ulation/Negative regulation/Catalysis and Gene
expression-Transcription/Localization. An addi-
tional targeted evaluation of these last three types
revealed that, despite their semantic overlap, the
largest fraction of these predictions refers to the
correct event type (78? 11.5%).
Finally, we note that trigger detection (47 ?
14.6%) and entity recognition errors (44?14.6%)
are the main causes of wrongly predicted Bind-
ing events. The latter causes the event extraction
mechanism to artificially produce single-argument
Bindings instead of multiple-argument Bindings.
We believe this issue can be resolved by broaden-
ing the scope of the entity recognition module to
generic GGPs and chemical compounds, and re-
applying the TEES algorithm to these entities as
if they were normal GGPs as defined in the ST
formalism. In contrast, edge detection errors are
much more frequently the cause of a wrongly pre-
dicted Phosphorylation event (statistically signifi-
cant difference with p < 0.05), caused by wrongly
identifying the thematic object or the causality of
the event. To resolve this issue, we propose fu-
ture annotation efforts to specifically annotate the
protein adding the phosphate group to the target
protein as a separate class than the regulation of
such a phosphorylation process by other cellular
machineries and components (Kim et al, 2013).
In conclusion, we have performed several
statistical analyses and targeted manual eval-
uations on a large-scale event dataset. As a
result, we were able to identify a set of rules
to automatically delete or correct a number
of false positive predictions (supplementary
material at http://bioinformatics.
psb.ugent.be/supplementary_data/
solan/bionlp13/). When applying these
rules to the winning submission of the recent
ST?13 (GE subchallenge), which was based
on the TEES classifier (Hakala et al, 2013),
3 false positive predictions could be identified
and removed. Even though this procedure only
marginally improves the classification results
(50.97% to 50.99% F-score), we believe the
cleaning procedure to be crucial specifically for
the credibility of any large-scale text mining
application. For example, applied on the EVEX
resource, it would ultimately result in the removal
of 242,000 instances and a corrected event type of
230,000 more cases (1.2% of all EVEX events in
total). These corrections will be implemented as
part of the next big EVEX release. Additionally,
the confidence score of more than 120,000 am-
biguous cases could be automatically decreased.
Alternatively, these cases could be the target of
a large-scale re-annotation, for instance using
the brat annotation tool (Stenetorp et al, 2012).
The resulting dataset could then serve as a new
training set to enable active learning on top of
existing event extraction approaches.
Acknowledgments
The authors thank Cindy Martens and the anony-
mous reviewers for a critical reading of the
manuscript and constructive feedback. SVL
thanks the Research Foundation Flanders (FWO)
for funding her research.
70
References
Cecilia Arighi, Zhiyong Lu, Martin Krallinger, Kevin
Cohen, J. Wilbur, Alfonso Valencia, Lynette
Hirschman, and Cathy Wu. 2011. Overview of
the BioCreative III workshop. BMC Bioinformatics,
12(Suppl 8):S1.
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Ex-
tracting complex biological events with rich graph-
based feature sets. In Proceedings of the BioNLP
2009 Workshop, pages 10?18.
Jari Bjo?rne, Filip Ginter, Sampo Pyysalo, Jun?ichi Tsu-
jii, and Tapio Salakoski. 2010. Scaling up biomed-
ical event extraction to the entire PubMed. In Pro-
ceedings of the BioNLP 2010 Workshop, pages 28?
36.
Jari Bjo?rne, Filip Ginter, and Tapio Salakoski. 2012.
Generalizing biomedical event extraction. BMC
Bioinformatics, 13(suppl. 8):S4.
Martin Gerner, Farzaneh Sarafraz, Casey M. Bergman,
and Goran Nenadic. 2012. BioContext: an in-
tegrated text mining system for large-scale extrac-
tion and contextualization of biomolecular events.
Bioinformatics, 28(16):2154?2161.
Kai Hakala, Sofie Van Landeghem, Tapio Salakoski,
Yves Van de Peer, and Filip Ginter. 2013. EVEX
in ST13: Application of a large-scale text mining
resource to event extraction and network construc-
tion. In Proceedings of the BioNLP Shared Task
2013 Workshop (in press).
Lynette Hirschman, Alexander Yeh, Christian
Blaschke, and Alfonso Valencia. 2005. Overview
of BioCreAtIvE: critical assessment of informa-
tion extraction for biology. BMC Bioinformatics,
6(Suppl 1):S1.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on event extraction. In
Proceedings of the BioNLP 2009 Workshop, pages
1?9.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Junichi Tsujii. 2011a. Ex-
tracting bio-molecular events from literature - the
BioNLP?09 Shared Task. Computational Intelli-
gence, 27(4):513?540.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011b.
Overview of BioNLP Shared Task 2011. In Pro-
ceedings of the BioNLP Shared Task 2011 Work-
shop, pages 1?6.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011c. Overview of Genia event
task in BioNLP Shared Task 2011. In Proceedings
of the BioNLP Shared Task 2011 Workshop, BioNLP
Shared Task ?11, pages 7?15.
Jin-Dong Kim, Yue Wang, Yamamoto Yasunori,
Sabine Bergler, Roser Morante, and Kevin Cohen.
2013. The Genia Event Extraction Shared Task,
2013 edition - overview. In Proceedings of the
BioNLP Shared Task 2013 Workshop (in press).
Martin Krallinger, Alexander Morgan, Larry Smith,
Florian Leitner, Lorraine Tanabe, John Wilbur,
Lynette Hirschman, and Alfonso Valencia. 2008.
Evaluation of text-mining systems for biology:
overview of the second BioCreative community
challenge. Genome Biology, 9(Suppl 2):S1.
Robert Leaman and Graciela Gonzalez. 2008. BAN-
NER: an executable survey of advances in biomedi-
cal named entity recognition. Pacific Symposium on
Biocomputing. Pacific Symposium on Biocomputing,
pages 652?663.
F. Leitner, S.A. Mardis, M. Krallinger, G. Cesareni,
L.A. Hirschman, and A. Valencia. 2010. An
overview of BioCreative II.5. Computational Bi-
ology and Bioinformatics, IEEE/ACM Transactions
on, 7(3):385?399.
David Martinez and Timothy Baldwin. 2011. Word
sense disambiguation for event trigger word detec-
tion in biomedicine. BMC Bioinformatics, 12(Suppl
2):S4.
Claire Nedellec et al 2013. Overview of BioNLP
Shared Task 2013. In Proceedings of the BioNLP
Shared Task 2013 Workshop (in press).
Pontus Stenetorp, Sampo Pyysalo, Goran Topic?,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. brat: a web-based tool for NLP-assisted
text annotation. In Proceedings of the Demonstra-
tions at the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 102?107.
Sofie Van Landeghem, Filip Ginter, Yves Van de Peer,
and Tapio Salakoski. 2011. EVEX: a PubMed-scale
resource for homology-based generalization of text
mining predictions. In Proceedings of the BioNLP
2011 Workshop, pages 28?37.
Sofie Van Landeghem, Jari Bjo?rne, Chih-Hsuan Wei,
Kai Hakala, Sampo Pyysalo, Sophia Ananiadou,
Hung-Yu Kao, Zhiyong Lu, Tapio Salakoski, Yves
Van de Peer, and Filip Ginter. 2013a. Large-
scale event extraction from literature with multi-
level gene normalization. PLoS ONE, 8(4):e55814.
Sofie Van Landeghem, Stefanie De Bodt, Zuzanna J.
Drebert, Dirk Inz, and Yves Van de Peer. 2013b.
The potential of text mining in data integration and
network biology for plant research: A case study on
arabidopsis. The Plant Cell, 25(3):794?807.
71
