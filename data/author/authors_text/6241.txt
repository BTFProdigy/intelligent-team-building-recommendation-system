Sentence Compression for Automated Subtitling: A Hybrid Approach
Vincent Vandeghinste and Yi Pan
Centre for Computational Linguistics
Katholieke Universiteit Leuven
Maria Theresiastraat 21
BE-3000 Leuven
Belgium
vincent.vandeghinste@ccl.kuleuven.ac.be, yi.pan@ccl.kuleuven.ac.be
Abstract
In this paper a sentence compression tool is de-
scribed. We describe how an input sentence gets
analysed by using a.o. a tagger, a shallow parser
and a subordinate clause detector, and how, based
on this analysis, several compressed versions of this
sentence are generated, each with an associated es-
timated probability. These probabilities were esti-
mated from a parallel transcript/subtitle corpus. To
avoid ungrammatical sentences, the tool also makes
use of a number of rules. The evaluation was done
on three different pronunciation speeds, averaging
sentence reduction rates of 40% to 17%. The num-
ber of reasonable reductions ranges between 32.9%
and 51%, depending on the average estimated pro-
nunciation speed.
1 Introduction
A sentence compression tool has been built with
the purpose of automating subtitle generation for
the deaf and hard-of-hearing. Verbatim transcrip-
tions cannot be presented as the subtitle presentation
time is between 690 and 780 characters per minute,
which is more or less 5.5 seconds for two lines (ITC,
1997), (Dewulf and Saerens, 2000), while the aver-
age speech rate contains a lot more than the equiva-
lent of 780 characters per minute.
The actual amount of compression needed de-
pends on the speed of the speaker and on the amount
of time available after the sentence. In documen-
taries, for instance, there are often large silent in-
tervals between two sentences, the speech is often
slower and the speaker is off-screen, so the avail-
able presentation time is longer. When the speaker
is off-screen, the synchrony of the subtitles with
the speech is of minor importance. When subti-
tling the news the speech rate is often very high
so the amount of reduction needed to allow the
synchronous presentation of subtitles and speech is
much greater. The sentence compression rate is a
parameter which can be set for each sentence.
Note that the sentence compression tool de-
scribed in this paper is not a subtitling tool. When
subtitling, only when a sentence needs to be re-
duced, and the amount of reduction is known, the
sentence is sent to the sentence compression tool.
So the sentence compression tool is a module of an
automated subtitling tool. The output of the sen-
tence compression tool needs to be processed ac-
cording to the subtitling guidelines like (Dewulf and
Saerens, 2000), in order to be in the correct lay-out
which makes it usable for actual subtitling. Manu-
ally post-editing the subtitles will still be required,
as for some sentences no automatic compression is
generated.
In real subtitling it often occurs that the sentences
are not compressed, but to keep the subtitles syn-
chronized with the speech, some sentences are en-
tirely removed.
In section 2 we describe the processing of a sen-
tence in the sentence compressor, from input to out-
put. In section 3 we describe how the system was
evaluated and the results of the evaluation. Section
4 contains the conclusions.
2 From Full Sentence to Compressed
Sentence
The sentence compression tool is inspired by (Jing,
2001). Although her goal is text summarization
and not subtitling, her sentence compression system
could serve this purpose.
She uses multiple sources of knowledge on which
her sentence reduction is based. She makes use of
a corpus of sentences, aligned with human-written
sentence reductions which is similar to the parallel
corpus we use (Vandeghinste and Tjong Kim Sang,
2004). She applies a syntactic parser to analyse the
syntactic structure of the input sentences. As there
was no syntactic parser available for Dutch (Daele-
mans and Strik, 2002), we created ShaRPA (Van-
deghinste, submitted), a shallow rule-based parser
which could give us a shallow parse tree of the
input sentence. Jing uses several other knowl-
edge sources, which are not used (not available for
Dutch) or not yet used in our system (like WordNet).
In figure 1 the processing flow of an input sen-
tence is sketched.
Input Sentence
Tagger
Abbreviator
Numbers to Digits
Chunker
Subordinate Clause Detector
Shallow Parse
Tree
Compressor
Word ReducerCompressed
Sentence
Removal,
Non?removal,
Reduction
Database
Grammar
Rules
Figure 1: Sentence Processing Flow Chart
First we describe how the sentence is analysed
(2.1), then we describe how the actual sentence
compression is done (2.2), and after that we de-
scribe how words can be reduced for extra compres-
sion (2.3). The final part describes the selection of
the ouput sentence (2.4).
2.1 Sentence Analysis
In order to apply an accurate sentence compression,
we need a syntactic analysis of the input sentence.
In a first step, the input sentence gets tagged for
parts-of-speech. Before that, it needs to be trans-
formed into a valid input format for the part-of-
speech tagger. The tagger we use is TnT (Brants,
2000) , a hidden Markov trigram tagger, which was
trained on the Spoken Dutch Corpus (CGN), Inter-
nal Release 6. The accuracy of TnT trained on CGN
is reported to be 96.2% (Oostdijk et al, 2002).
In a second step, the sentence is sent to the
Abbreviator. This tool connects to a database
of common abbreviations, which are often pro-
nounced in full words (E.g. European Union be-
comes EU) and replaces the full form with its ab-
breviation. The database can also contain the tag
of the abbreviated part (E.g. the tag for EU is
N(eigen,zijd,ev,basis,stan) [E: singular non-neuter
proper noun]).
In a third step, all numbers which are written in
words in the input are replaced by their form in dig-
its. This is done for all numbers which are smaller
than one million, both for cardinal and ordinal nu-
merals.
In a fourth step, the sentence is sent to ShaRPa,
which will result in a shallow parse-tree of the sen-
tence. The chunking accuracy for noun phrases
(NPs) has an F-value of 94.7%, while the chunk-
ing accuracy of prepositional phrases (PPs) has an
F-value of 95.1% (Vandeghinste, submitted).
A last step before the actual sentence compres-
sion consists of rule-based clause-detection: Rel-
ative phrases (RELP), subordinate clauses (SSUB)
and OTI-phrases (OTI is om ... te + infinitive1) are
detected. The accuracy of these detections was eval-
uated on 30 files from the CGN component of read-
aloud books, which contained 7880 words. The
evaluation results are presented in table 1.
Type of S Precision Recall F-value
OTI 71.43% 65.22% 68.18%
RELP 69.66% 68.89% 69.27%
SSUB 56.83% 60.77% 58.74%
Table 1: Clause Detection Accuracy
The errors are mainly due to a wrong analysis
of coordinating conjunctions, which is not only the
weak point in the clause-detection module, but also
in ShaRPa. A full parse is needed to accurately
solve this problem.
2.2 Sentence Compression
For each chunk or clause detected in the previous
steps, the probabilities of removal, non-removal and
reduction are estimated. This is described in more
detail in 2.2.1.
Besides the statistical component in the compres-
sion, there are also a number of rules in the com-
pression program, which are described in more de-
tail in 2.2.2.
The way the statistical component and the rule-
based component are combined is described in
2.2.3.
1There is no equivalent construction in English. OTI is a
VP-selecting complementizer.
2.2.1 Use of Statistics
Chunk and clause removal, non-removal and reduc-
tion probabilities are estimated from the frequencies
of removal, non-removal and reduction of certain
types of chunks and clauses in the parallel corpus.
The parallel corpus consists of transcripts of tele-
vision programs on the one hand and the subti-
tles of these television programs on the other hand.
A detailed description of how the parallel corpus
was collected, and how the sentences and chunks
were aligned is given in (Vandeghinste and Tjong
Kim Sang, 2004).
All sentences in the source corpus (transcripts)
and the target corpus (subtitles) are analysed in the
same way as described in section 2.1, and are chunk
aligned. The chunk alignment accuracy is about
95% (F-value).
We estimated the removal, non-removal and re-
duction probabilities for the chunks of the types NP,
PP, adjectival phrase (AP), SSUB, RELP, and OTI,
based on their chunk removal, non-removal and re-
duction frequencies.
For the tokens not belonging to either of these
types, the removal and non-removal probabilities
were estimated based on the part-of-speech tag for
those words. A reduced tagset was used, as the orig-
inal CGN-tagset (Van Eynde, 2004) was too fine-
grained and would lead to a multiplication of the
number of rules which are now used in ShaRPa. The
first step in SharPa consists of this reduction.
For the PPs, the SSUBs and the RELPs, as well
as for the adverbs, the chunk/tag information was
considered as not fine-grained enough, so the es-
timation of the removal, non-removal and reduc-
tion probabilities for these types are based on the
first word of those phrases/clauses and the reduc-
tion, removal and non-removal probabilities of such
phrases in the parallel corpus, as the first words of
these chunk-types are almost always the heads of
the chunk. This allows for instance to make the
distinction between several adverbs in one sentence,
so they do not all have the same removal and non-
removal probabilities. A disadvantage is that this
approach leads to sparse data concerning the less
frequent adverbs, for which a default value (average
over all adverbs) will be employed.
An example : A noun phrase.
de grootste Belgische bank
[E: the largest Belgian bank]
After tagging and chunking the sentence and af-
ter detecting subordinate clauses, for every non-
terminal node in the shallow parse tree we retrieve
the measure of removal (X), of non-removal (=) and
of reduction2 (   ). For the terminal nodes, only the
measures of removal and of non-removal are used.
NP
= 0.54
X 0.27
 0.05
DET
= 0.68
X 0.28
de
ADJ
= 0.56
X 0.35
groot-
ste
ADJ
= 0.56
X 0.35
Bel-
gische
N
= 0.65
X 0.26
bank
For every combination the probability estimate
is calculated. So if we generate all possible com-
pressions (including no compression), the phrase
de grootste Belgische bank will get the
probability estimate 
				
