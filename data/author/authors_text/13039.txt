Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177?180,
Prague, June 2007. c?2007 Association for Computational Linguistics 
Moses: Open Source Toolkit for Statistical Machine Translation 
Philipp Koehn 
Hieu Hoang  
Alexandra Birch 
Chris Callison-Burch 
University of Edin-
burgh1 
Marcello Federico 
Nicola Bertoldi 
ITC-irst2 
Brooke Cowan 
Wade Shen 
Christine Moran 
MIT3 
Richard Zens 
RWTH Aachen4 
Chris Dyer 
University of Maryland5 
 
Ond?ej Bojar 
Charles University6 
Alexandra Constantin 
Williams College7 
Evan Herbst 
Cornell8 
1 pkoehn@inf.ed.ac.uk, {h.hoang, A.C.Birch-Mayne}@sms.ed.ac.uk, callison-burch@ed.ac.uk. 
2{federico, bertoldi}@itc.it. 3 brooke@csail.mit.edu, swade@ll.mit.edu, weezer@mit.edu. 4 
zens@i6.informatik.rwth-aachen.de. 5 redpony@umd.edu. 6 bojar@ufal.ms.mff.cuni.cz. 7 
07aec_2@williams.edu. 8 evh4@cornell.edu 
 
Abstract 
We describe an open-source toolkit for sta-
tistical machine translation whose novel 
contributions are (a) support for linguisti-
cally motivated factors, (b) confusion net-
work decoding, and (c) efficient data for-
mats for translation models and language 
models. In addition to the SMT decoder, 
the toolkit also includes a wide variety of 
tools for training, tuning and applying the 
system to many translation tasks.  
1 Motivation 
Phrase-based statistical machine translation 
(Koehn et al 2003) has emerged as the dominant 
paradigm in machine translation research. How-
ever, until now, most work in this field has been 
carried out on proprietary and in-house research 
systems. This lack of openness has created a high 
barrier to entry for researchers as many of the 
components required have had to be duplicated. 
This has also hindered effective comparisons of the 
different elements of the systems. 
By providing a free and complete toolkit, we 
hope that this will stimulate the development of the 
field. For this system to be adopted by the commu-
nity, it must demonstrate performance that is com-
parable to the best available systems. Moses has 
shown that it achieves results comparable to the 
most competitive and widely used statistical ma-
chine translation systems in translation quality and 
run-time (Shen et al 2006). It features all the ca-
pabilities of the closed sourced Pharaoh decoder 
(Koehn 2004). 
Apart from providing an open-source toolkit 
for SMT, a further motivation for Moses is to ex-
tend phrase-based translation with factors and con-
fusion network decoding. 
The current phrase-based approach to statisti-
cal machine translation is limited to the mapping of 
small text chunks without any explicit use of lin-
guistic information, be it morphological, syntactic, 
or semantic. These additional sources of informa-
tion have been shown to be valuable when inte-
grated into pre-processing or post-processing steps. 
Moses also integrates confusion network de-
coding, which allows the translation of ambiguous 
input. This enables, for instance, the tighter inte-
gration of speech recognition and machine transla-
tion. Instead of passing along the one-best output 
of the recognizer, a network of different word 
choices may be examined by the machine transla-
tion system. 
Efficient data structures in Moses for the 
memory-intensive translation model and language 
model allow the exploitation of much larger data 
resources with limited hardware. 
177
 2 Toolkit 
The toolkit is a complete out-of-the-box trans-
lation system for academic research. It consists of 
all the components needed to preprocess data, train 
the language models and the translation models. It 
also contains tools for tuning these models using 
minimum error rate training (Och 2003) and evalu-
ating the resulting translations using the BLEU 
score (Papineni et al 2002).  
Moses uses standard external tools for some of 
the tasks to avoid duplication, such as GIZA++ 
(Och and Ney 2003) for word alignments and 
SRILM for language modeling.  Also, since these 
tasks are often CPU intensive, the toolkit has been 
designed to work with Sun Grid Engine parallel 
environment to increase throughput.  
In order to unify the experimental stages, a 
utility has been developed to run repeatable ex-
periments. This uses the tools contained in Moses 
and requires minimal changes to set up and cus-
tomize. 
The toolkit has been hosted and developed un-
der sourceforge.net since inception. Moses has an 
active research community and has reached over 
1000 downloads as of 1st March 2007.  
The main online presence is at  
http://www.statmt.org/moses/ 
where many sources of information about the 
project can be found. Moses was the subject of this 
year?s Johns Hopkins University Workshop on 
Machine Translation (Koehn et al 2006). 
The decoder is the core component of Moses. 
To minimize the learning curve for many research-
ers, the decoder was developed as a drop-in re-
placement for Pharaoh, the popular phrase-based 
decoder. 
In order for the toolkit to be adopted by the 
community, and to make it easy for others to con-
tribute to the project, we kept to the following 
principles when developing the decoder: 
? Accessibility 
? Easy to Maintain 
? Flexibility 
? Easy for distributed team development 
? Portability 
It was developed in C++ for efficiency and fol-
lowed modular, object-oriented design. 
3 Factored Translation Model 
Non-factored SMT typically deals only with 
the surface form of words and has one phrase table, 
as shown in Figure 1. 
i am buying you a green cat
using phrase dictionary:
i
 am buying
you
a
green
cat
je
ach?te
vous
un
vert
chat
a une
je vous ach?te un chat vert
Translate:
 
In factored translation models, the surface 
forms may be augmented with different factors, 
such as POS tags or lemma. This creates a factored 
representation of each word, Figure 2.  
1 1 1 / sing /
                                 
je vous achet un chat
PRO PRO VB ART NN
je vous acheter un chat
st st st present masc masc
? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?
1 1 / 1 sing sing
i buy you a cat
PRO VB PRO ART NN
i tobuy you a cat
st st present st
? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?  
 
Mapping of source phrases to target phrases 
may be decomposed into several steps. Decompo-
sition of the decoding process into various steps 
means that different factors can be modeled sepa-
rately. Modeling factors in isolation allows for 
flexibility in their application. It can also increase 
accuracy and reduce sparsity by minimizing the 
number dependencies for each step. 
For example, we can decompose translating 
from surface forms to surface forms and lemma, as 
shown in Figure 3. 
Figure 2. Factored translation 
Figure 1. Non-factored translation 
178
  
Figure 3. Example of graph of decoding steps 
By allowing the graph to be user definable, we 
can experiment to find the optimum configuration 
for a given language pair and available data.  
The factors on the source sentence are consid-
ered fixed, therefore, there is no decoding step 
which create source factors from other source fac-
tors. However, Moses can have ambiguous input in 
the form of confusion networks. This input type 
has been used successfully for speech to text 
translation (Shen et al 2006). 
Every factor on the target language can have its 
own language model. Since many factors, like 
lemmas and POS tags, are less sparse than surface 
forms, it is possible to create a higher order lan-
guage models for these factors. This may encour-
age more syntactically correct output. In Figure 3 
we apply two language models, indicated by the 
shaded arrows, one over the words and another 
over the lemmas. Moses is also able to integrate 
factored language models, such as those described 
in (Bilmes and Kirchhoff 2003) and (Axelrod 
2006). 
4 Confusion Network Decoding 
Machine translation input currently takes the 
form of simple sequences of words. However, 
there are increasing demands to integrate machine 
translation technology into larger information 
processing systems with upstream NLP/speech 
processing tools (such as named entity recognizers, 
speech recognizers, morphological analyzers, etc.). 
These upstream processes tend to generate multiple, 
erroneous hypotheses with varying confidence. 
Current MT systems are designed to process only 
one input hypothesis, making them vulnerable to 
errors in the input.  
In experiments with confusion networks, we 
have focused so far on the speech translation case, 
where the input is generated by a speech recog-
nizer. Namely, our goal is to improve performance 
of spoken language translation by better integrating 
speech recognition and machine translation models. 
Translation from speech input is considered more 
difficult than translation from text for several rea-
sons. Spoken language has many styles and genres, 
such as, formal read speech, unplanned speeches, 
interviews, spontaneous conversations; it produces 
less controlled language, presenting more relaxed 
syntax and spontaneous speech phenomena. Fi-
nally, translation of spoken language is prone to 
speech recognition errors, which can possibly cor-
rupt the syntax and the meaning of the input. 
There is also empirical evidence that better 
translations can be obtained from transcriptions of 
the speech recognizer which resulted in lower 
scores. This suggests that improvements can be 
achieved by applying machine translation on a 
large set of transcription hypotheses generated by 
the speech recognizers and by combining scores of 
acoustic models, language models, and translation 
models. 
Recently, approaches have been proposed for 
improving translation quality through the process-
ing of multiple input hypotheses. We have imple-
mented in Moses confusion network decoding as 
discussed in (Bertoldi and Federico 2005), and de-
veloped a simpler translation model and a more 
efficient implementation of the search algorithm. 
Remarkably, the confusion network decoder re-
sulted in an extension of the standard text decoder. 
5 Efficient Data Structures for Transla-
tion Model and Language Models 
With the availability of ever-increasing 
amounts of training data, it has become a challenge 
for machine translation systems to cope with the 
resulting strain on computational resources. Instead 
of simply buying larger machines with, say, 12 GB 
of main memory, the implementation of more effi-
cient data structures in Moses makes it possible to 
exploit larger data resources with limited hardware 
infrastructure. 
A phrase translation table easily takes up giga-
bytes of disk space, but for the translation of a sin-
gle sentence only a tiny fraction of this table is 
needed. Moses implements an efficient representa-
tion of the phrase translation table. Its key proper-
ties are a prefix tree structure for source words and 
on demand loading, i.e. only the fraction of the 
phrase table that is needed to translate a sentence is 
loaded into the working memory of the decoder. 
179
 For the Chinese-English NIST  task, the mem-
ory requirement of the phrase table is reduced from 
1.7 gigabytes to less than 20 mega bytes, with no 
loss in translation quality and speed (Zens and Ney 
2007). 
The other large data resource for statistical ma-
chine translation is the language model. Almost 
unlimited text resources can be collected from the 
Internet and used as training data for language 
modeling. This results in language models that are 
too large to easily fit into memory. 
The Moses system implements a data structure 
for language models that is more efficient than the 
canonical SRILM (Stolcke 2002) implementation 
used in most systems. The language model on disk 
is also converted into this binary format, resulting 
in a minimal loading time during start-up of the 
decoder.  
An even more compact representation of the 
language model is the result of the quantization of 
the word prediction and back-off probabilities of 
the language model. Instead of representing these 
probabilities with 4 byte or 8 byte floats, they are 
sorted into bins, resulting in (typically) 256 bins 
which can be referenced with a single 1 byte index. 
This quantized language model, albeit being less 
accurate, has only minimal impact on translation 
performance (Federico and Bertoldi 2006). 
6 Conclusion and Future Work 
This paper has presented a suite of open-source 
tools which we believe will be of value to the MT 
research community. 
We have also described a new SMT decoder 
which can incorporate some linguistic features in a 
consistent and flexible framework. This new direc-
tion in research opens up many possibilities and 
issues that require further research and experimen-
tation. Initial results show the potential benefit of 
factors for statistical machine translation, (Koehn 
et al 2006) and (Koehn and Hoang 2007). 
References 
Axelrod, Amittai. "Factored Language Model for Sta-
tistical Machine Translation." MRes Thesis. 
Edinburgh University, 2006. 
Bertoldi, Nicola, and Marcello Federico. "A New De-
coder for Spoken Language Translation Based 
on Confusion Networks." Automatic Speech 
Recognition and Understanding Workshop 
(ASRU), 2005. 
Bilmes, Jeff A, and Katrin Kirchhoff. "Factored Lan-
guage Models and Generalized Parallel Back-
off." HLT/NACCL, 2003. 
Koehn, Philipp. "Pharaoh: A Beam Search Decoder for 
Phrase-Based Statistical Machine Translation 
Models." AMTA, 2004. 
Koehn, Philipp, Marcello Federico, Wade Shen, Nicola 
Bertoldi, Ondrej Bojar, Chris Callison-Burch, 
Brooke Cowan, Chris Dyer, Hieu Hoang, 
Richard Zens, Alexandra Constantin, Christine 
Corbett Moran, and Evan Herbst. "Open 
Source Toolkit for Statistical Machine Transla-
tion". Report of the 2006 Summer Workshop at 
Johns Hopkins University, 2006. 
Koehn, Philipp, and Hieu Hoang. "Factored Translation 
Models." EMNLP, 2007. 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 
"Statistical Phrase-Based Translation." 
HLT/NAACL, 2003. 
Och, Franz Josef. "Minimum Error Rate Training for 
Statistical Machine Translation." ACL, 2003. 
Och, Franz Josef, and Hermann Ney. "A Systematic 
Comparison of Various Statistical Alignment 
Models." Computational Linguistics 29.1 
(2003): 19-51. 
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. "BLEU: A Method for Automatic 
Evaluation of Machine Translation." ACL, 
2002. 
Shen, Wade, Richard Zens, Nicola Bertoldi, and 
Marcello Federico. "The JHU Workshop 2006 
Iwslt System." International Workshop on Spo-
ken Language Translation, 2006. 
Stolcke, Andreas. "SRILM an Extensible Language 
Modeling Toolkit." Intl. Conf. on Spoken Lan-
guage Processing, 2002. 
Zens, Richard, and Hermann Ney. "Efficient Phrase-
Table Representation for Machine Translation 
with Applications to Online MT and Speech 
Recognition." HLT/NAACL, 2007. 
180
Proceedings of the Second Workshop on Statistical Machine Translation, pages 232?239,
Prague, June 2007. c?2007 Association for Computational Linguistics
English-to-Czech Factored Machine Translation
Ondr?ej Bojar
Institute of Formal and Applied Linguistics
?UFAL MFF UK, Malostranske? na?me?st?? 25
CZ-11800 Praha, Czech Republic
bojar@ufal.mff.cuni.cz
Abstract
This paper describes experiments with
English-to-Czech phrase-based machine
translation. Additional annotation of input
and output tokens (multiple factors) is used
to explicitly model morphology. We vary
the translation scenario (the setup of multi-
ple factors) and the amount of information
in the morphological tags. Experimental
results demonstrate significant improvement
of translation quality in terms of BLEU.
1 Introduction
Statistical phrase-based machine translation (SMT)
systems currently achieve top performing results.1
Known limitations of phrase-based SMT include
worse quality when translating to morphologically
rich languages as opposed to translating from them
(Koehn, 2005). One of the teams at the 2006 sum-
mer engineering workshop at Johns Hopkins Uni-
versity2 attempted to tackle these problems by in-
troducing separate FACTORS in SMT input and/or
output to allow explicit modelling of the underlying
language structure. The support for factored transla-
tion models was incorporated into the Moses open-
source SMT system3.
In this paper, we report on experiments with
English-to-Czech multi-factor translation. After a
brief overview of factored SMT and our data (Sec-
tions 2 and 3), we summarize some possible trans-
lating scenarios in Section 4. Section 5 studies the
1http://www.nist.gov/speech/tests/mt/
2http://www.clsp.jhu.edu/ws2006/
3http://www.statmt.org/moses/
level of detail useful for morphological representa-
tion and Section 6 compares the results to a setting
with more data available, albeit out of domain. The
second part (Section 7) is devoted to a brief analysis
of MT output errors.
1.1 Motivation for Improving Morphology
Czech is a Slavic language with very rich morphol-
ogy and relatively free word order. The Czech mor-
phological system (Hajic?, 2004) defines 4,000 tags
in theory and 2,000 were actually seen in a big
tagged corpus. (For comparison, the English Penn
Treebank tagset contains just about 50 tags.) In our
parallel corpus (see Section 3 below), the English
vocabulary size is 35k distinct token types but more
than twice as big in Czech, 83k distinct token types.
To further emphasize the importance of morphol-
ogy in MT to Czech, we compare the standard
BLEU (Papineni et al, 2002) of a baseline phrase-
based translation with BLEU which disregards word
forms (lemmatized MT output is compared to lem-
matized reference translation). The theoretical mar-
gin for improving MT quality is about 9 BLEU
points: the same MT output scores 12 points in stan-
dard BLEU and 21 points in lemmatized BLEU.
2 Overview of Factored SMT
In statistical MT, the goal is to translate a source
(foreign) language sentence fJ1 = f1 . . . fj . . . fJ
into a target language (Czech) sentence cI1 =
c1 . . . cj . . . cI . In phrase-based SMT, the assump-
tion is made that the target sentence can be con-
structed by segmenting source sentence into phrases,
translating each phrase and finally composing the
232
target sentence from phrase translations, sK1 de-
notes the segmentation of the input sentence into
K phrases. Among all possible target language
sentences, we choose the sentence with the highest
probability,
e?I?1 = argmax
I,cI1,K,sK1
{Pr(cI1|fJ1 , sK1 )} (1)
In a log-linear model, the conditional probability
of cI1 being the translation of fJ1 under the segmenta-
tion sK1 is modelled as a combination of independent
feature functions h1(?, ?, ?) . . . hM (?, ?, ?) describing
the relation of the source and target sentences:
Pr(cI1|fJ1 , sK1 ) =
exp(?Mm=1 ?mhm(cI1, fJ1 , sK1 ))
?
c?I?1
exp(?Mm=1 ?mhm(c?I
?
1 , fJ1 , sK1 ))
(2)
The denominator in 2 is used as a normalization
factor that depends on the source sentence fJ1 and
segmentation sK1 only and is omitted during maxi-
mization. The model scaling factors ?M1 are trained
either to the maximum entropy principle or opti-
mized with respect to the final translation quality
measure.
Most of our features are phrase-based and we re-
quire all such features to operate synchronously on
the segmentation sK1 and independently of neigh-
bouring segments. In other words, we restrict the
form of phrase-based features to:
hm(cI1, fJ1 , sK1 ) =
K
?
k=1
h?m(c?k, f?k) (3)
where f?k represents the source phrase and c? repre-
sents the target phrase k given the segmentation sK1 .
2.1 Decoding Steps
In factored SMT, source and target words f and c are
represented as tuples of F and C FACTORS, resp.,
each describing a different aspect of the word, e.g.
its word form, lemma, morphological tag, role in a
verbal frame. The process of translation consists of
DECODING steps of two types: MAPPING steps and
GENERATION steps. If more steps contribute to the
same output factor, they have to agree on the out-
come, i.e. partial hypotheses where two decoding
steps produce conflicting values in an output factor
are discarded.
A MAPPING step from a subset of source fac-
tors S ? {1 . . . F} to a subset of target factors
T ? {1 . . . C} is the standard phrase-based model
(see e.g. (Koehn, 2004a)) and introduces a feature
in the following form:
h?map:S?Tm (c?k, f?k) = log p(f?Sk |c?Tk ) (4)
The conditional probability of f?Sk , i.e. the phrase
f?k restricted to factors S, given c?Tk , i.e. the phrase
c?k restricted to factors T is estimated from relative
frequencies: p(f?Sk |c?Tk ) = N(f?S, c?T )/N(c?T ) where
N(f?S, c?T ) denotes the number of co-occurrences of
a phrase pair (f?S, c?T ) that are consistent with the
word alignment. The marginal count N(c?T ) is the
number of occurrences of the target phrase c?T in the
training corpus.
For each mapping step, the model is included in
the log-linear combination in source-to-target and
target-to-source directions: p(f?T |c?S) and p(c?S |f?T ).
In addition, statistical single word based lexica are
used in both directions. They are included to smooth
the relative frequencies used as estimates of the
phrase probabilities.
A GENERATION step maps a subset of target fac-
tors T1 to a disjoint subset of target factors T2,
T1,2 ? {1 . . . C}. In the current implementation
of Moses, generation steps are restricted to word-
to-word correspondences:
h?gen:T1?T2m (c?k, f?k) = log
length(c?k)
?
i=1
p(c?T1k,i|c?
T2
k,i) (5)
where c?Tk,i is the i-th words in the k-th target phrase
restricted to factors T . We estimate the conditional
probability p(c?T2k,i|c?
T1
k,i) by counting over words in the
target-side corpus. Again, the conditional probabil-
ity is included in the log-linear combination in both
directions.
In addition to features for decoding steps, we in-
clude arbitrary number of target language models
over subsets of target factors, T ? {1 . . . C}. Typi-
cally, we use the standard n-gram language model:
233
hTLMn(fJ1 , cI1) = log
I
?
i=1
p(cTi |cTi?1 . . . cTi?n+1) (6)
While generation steps are used to enforce ?verti-
cal? coherence between ?hidden properties? of out-
put words, language models are used to enforce se-
quential coherence of the output.
Operationally, Moses performs a stack-based
beam search very similar to Pharaoh (Koehn,
2004a). Thanks to the synchronous-phrases assump-
tion, all the decoding steps can be performed during
a preparatory phase. For each span in the input sen-
tence, all possible translation options are constructed
using the mapping and generation steps in a user-
specified order. Low-scoring options are pruned al-
ready during this phase. Once all translation options
are constructed, Moses picks source phrases (all out-
put factors already filled in) in arbitrary order, sub-
ject to a reordering limit, producing output in left-to-
right fashion and scoring it using the specified lan-
guage models exactly as Pharaoh does.
3 Data Used
The experiments reported in this paper were car-
ried out with the News Commentary (NC) corpus as
made available for the SMT workshop4 of the ACL
2007 conference.5
The Czech part of the corpus was tagged and lem-
matized using the tool by Hajic? and Hladka? (1998),
the English part was tagged MXPOST (Ratnaparkhi,
1996) and lemmatized using the Morpha tool (Min-
nen et al, 2001). After some final cleanup, the
corpus consists of 55,676 pairs of sentences (1.1M
Czech tokens and 1.2M English tokens). We use the
designated additional tuning and evaluation sections
consisting of 1023, resp. 964 sentences.
In all experiments, word alignment was obtained
using the grow-diag-final heuristic for symmetriz-
ing GIZA++ (Och and Ney, 2003) alignments. To
reduce data sparseness, the English text was lower-
cased and Czech was lemmatized for alignment es-
timation. Language models are based on the target
4http://www.statmt.org/wmt07/
5Our preliminary experiments with the Prague Czech-
English Dependency Treebank, PCEDT v.1.0 ( ?Cmejrek et al,
2004), 20k sentences, gave similar results, although with a
lower level of significance due to a smaller evaluation set.
side of the parallel corpus only, unless stated other-
wise.
3.1 Evaluation Measure and MERT
We evaluate our experiments using the (lowercase,
tokenized) BLEU metric and estimate the empiri-
cal confidence using the bootstrapping method de-
scribed in Koehn (2004b).6 We report the scores
obtained on the test section with model parameters
tuned using the tuning section for minimum error
rate training (MERT, (Och, 2003)).
4 Scenarios of Factored Translation
English?Czech
We experimented with the following factored trans-
lation scenarios.
The baseline scenario (labelled T for translation)
is single-factored: input (English) lowercase word
forms are directly translated to target (Czech) low-
ercase forms. A 3-gram language model (or more
models based on various corpora) checks the stream
of output word forms. The baseline scenario thus
corresponds to a plain phrase-based SMT system:
English Czech
lowercase lowercase +LM
lemma lemma
morphology morphology
In order to check the output not only for word-
level coherence but also for morphological coher-
ence, we add a single generation step: input word
forms are first translated to output word forms and
each output word form then generates its morpho-
logical tag.
Two types of language models can be used simul-
taneously: a (3-gram) LM over word forms and a
(7-gram) LM over morphological tags.
We used tags with various levels of detail, see sec-
tion 5. We call this the ?T+C? (translate and check)
scenario:
6Given a test set of sentences, we perform 1,000 random se-
lections with repetitions to estimate 1,000 BLEU scores on test
sets of the same size. The empirical 90%-confidence upper and
lower bounds are obtained after removing top and bottom 5% of
scores. For conciseness, we report the average of the distance
between to standard BLEU value and the empirical upper and
lower bound after the ??? symbol.
234
English Czech
lowercase lowercase +LM
lemma lemma
morphology morphology +LM
As a refinement of T+C, we also used T+T+C
scenario, where the morphological output stream is
constructed based on both output word forms and in-
put morphology. This setting should reinforce cor-
rect translation of morphological features such as
number of source noun phrases. To reduce the risk
of early pruning, the generation step operationally
precedes the morphology mapping step. Again,
two types of language models can be used in this
?T+T+C? scenario:
English Czech
lowercase lowercase +LM
lemma lemma
morphology morphology +LM
The most complex scenario we used is linguis-
tically appealing: output lemmas (base forms) and
morphological tags are generated from input in two
independent translation steps and combined in a sin-
gle generation step to produce output word forms.
The input English text was not lemmatized so we
used English word forms as the source for produc-
ing Czech lemmas.
The ?T+T+G? setting allows us to use three types
of language models. Trigram models are used for
word forms and lemmas and 7-gram language mod-
els are used over tags:
English Czech
lowercase lowercase +LM
lemma lemma +LM
morphology morphology +LM
4.1 Experimental Results: Improved over T
Table 1 summarizes estimated translation quality of
the various scenarios. In all cases, a 3-gram LM is
used for word forms or lemmas and a 7-gram LM
for morphological tags.
The good news is that multi-factored models al-
ways outperform the baseline T.
Unfortunately, the more complex multi-factored
scenarios do not bring any significant improvement
over T+C. Our belief is that this effect is caused by
search errors: with multi-factored models, more hy-
potheses get similar scores and future costs of partial
BLEU
T+T+G 13.9?0.7
T+T+C 13.9?0.6
T+C 13.6?0.6
Baseline: T 12.9?0.6
Table 1: BLEU scores of various translation scenar-
ios.
hypotheses might be estimated less reliably. With
the limited stack size (not more than 200 hypothe-
ses of the same number of covered input words), the
decoder may more often find sub-optimal solutions.
Moreover, the more steps are used, the more model
weights have to be tuned in the minimum error rate
training. Considerably more tuning data might be
necessary to tune the weights reliably.
5 Granularity of Czech Part-of-Speech
As stated above, the Czech morphological tag sys-
tem is very complex: in theory up to 4,000 different
tags are possible. In our T+T+C scenario, we exper-
iment with various simplifications of the system to
find the best balance between richness and robust-
ness of the statistics available in our corpus. (The
more information is retained in the tags, the more
severe data sparseness is.)
Full tags (1200 unique seen in the 56k corpus):
Full Czech positional tags are used. A tag
consists of 15 positions, each holding the value
of a morphological property (e.g. number, case
or gender).7
POS+case (184 unique seen): We simplify the tag
to include only part and subpart of speech (dis-
tinguishes also partially e.g. verb tenses). For
nouns, pronouns, adjectives and prepositions8 ,
also the case is included.
CNG01 (621 unique seen): CNG01 refines POS.
For nouns, pronouns and adjectives we include
not only the case but also number and gender.
7In principle, each of the 15 positions could be used as a
separate factor. The set of necessary generation steps to encode
relevant dependencies would have to be carefully determined.
8Some Czech prepositions select for a particular case, some
are ambiguous. Although the case is never shown on surface of
the preposition, the tagset includes this information and Czech
taggers are able to infer the case.
235
CNG02 (791 unique seen): Tag for punctuation is
refined: the lemma of the punctuation symbol
is taken into account; previous models disre-
garded e.g. the distributional differences be-
tween a comma and a question mark. Case,
number and gender added to nouns, pronouns,
adjectives, prepositions, but also to verbs and
numerals (where applicable).
CNG03 (1017 unique seen): Optimized tagset:
? Tags for nouns, adjectives, pronouns and
numerals describe the case, number and
gender; the Czech reflexive pronoun se or
si is highlighted by a special flag.
? Tag for verbs describes subpart of speech,
number, gender, tense and aspect; the tag
includes a special flag if the verb was the
auxiliary verb by?t (to be) in any of its
forms.
? Tag for prepositions includes the case and
also the lemma of the preposition.
? Lemma included for punctuation, parti-
cles and interjections.
? Tag for numbers describes the ?shape? of
the number (all digits are replaced by the
digit 5 but number-internal punctuation is
kept intact). The tag thus distinguishes be-
tween 4- or 5-digit numbers or the preci-
sion of floating point numbers.
? Part of speech and subpart of speech for
all other words.
5.1 Experimental Results: CNG03 Best
Table 2 summarizes the results of T+T+C scenario
with varying detail in morphological tag.
BLEU
Baseline: T (single-factor) 12.9?0.6
T+T+C, POS+case 13.2?0.6
T+T+C, CNG01 13.4?0.6
T+T+C, CNG02 13.5?0.7
T+T+C, full tags 13.9?0.6
T+T+C, CNG03 14.2?0.7
Table 2: BLEU scores of various granularities of
morphological tags in T+T+C scenario.
NC NC CzEng
mix
weighted = ?NC + ?mix
Scenario Phrases from LMs BLEU
T NC NC 12.9?0.6
T mix mix 11.8?0.6
T mix weighted 11.8?0.6
T+C CNG03 NC NC 13.7?0.7
T+C CNG03 mix mix 13.1?0.7
T+C CNG03 mix weighted 13.7?0.7
T+C full tags NC NC 13.6?0.6
T+C full tags mix mix 13.1?0.7
T+C full tags mix weighted 13.8?0.7
Figure 1: The effect of additional data in T and T+C
scenarios.
Our results confirm improvement over the single-
factored baseline. Detailed knowledge of the mor-
phological system also proves its utility: by choos-
ing the most relevant features of tags and lemmas
but avoiding sparseness, we can improve on BLEU
score by about 0.3 absolute over T+T+C with full
tags.
6 More Out-of-Domain Data in T and T+C
Scenarios
In order to check if the method scales up with
more parallel data available, we extend our train-
ing data using the CzEng parallel corpus (Bojar
and ?Zabokrtsky?, 2006). CzEng contains sentence-
aligned texts from the European Parliament (about
75%), e-books and stories (15%) and open source
documentation. By ?Baseline? corpus we denote
NC corpus only, by ?Large? we denote the combi-
nation of training sentences from NC and CzEng
(1070k sentences, 13.9M Czech and 15.5 English
tokens) where in-domain NC data amounts only to
5.2% sentences.
Figure 1 gives full details of our experiments with
the additional data. We varied the scenario (T or
T+C), the level of detail in the T+C scenario (full
tags vs. CNG03) and the size of the training corpus.
We extract phrases from either the in-domain corpus
only (NC) or the mixed corpus (mix). We use either
one LM per output factor, varying the corpus size
(NC or mix), or two LMs per output factors with
weights trained independently in the MERT proce-
236
dure (weighted). Independent weights allow us to
take domain difference into account, but we exploit
this in the target LM only, not the phrases.
The only significant difference is caused by the
scenario: T+C outperforms the baseline T, regard-
less of corpus size. Other results (insignificantly)
indicate the following observations:
? Ignoring the domain difference and using only
the mixed domain LM in general performs
worse than allowing MERT to optimize LM
weights for in-domain and generic data sepa-
rately.9
? CNG03 outperforms full tags only in small data
setting, with large data (treating the domain dif-
ference properly), full tags perform better.
7 Untreated Morphological Errors
The previous sections described improvements
gained on small data sets when checking morpho-
logical agreement using T+T+C scenario (BLEU
raised from 12.9% to 13.9% or up to 14.2% with
manually tuned tagset, CNG03). However, the best
result achieved is still far below the margin of lem-
matized BLEU (21%), as mentioned in Section 1.1.
When we searched for the unexploited morpho-
logical errors, visual inspection of MT output sug-
gested that local agreement (within 3-word span) is
relatively correct but Verb-Modifier relations are of-
ten malformed causing e.g. a bad case for the Mod-
ifier. To quantify this observation we performed a
micro-study of our best MT output using an intu-
itive metric. We checked whether Verb-Modifier re-
lations are properly preserved during the translation
of 15 sample sentences.
The source text of the sample sentences contained
77 Verb-Modifier pairs. Table 3 lists our observa-
tions on the two members in each Verb-Modifier
pair. We see that only 56% of verbs are translated
correctly and 79% of nouns are translated correctly.
The system tends to skip verbs quite often (27% of
cases).
9In our previous experiments with PCEDT as the domain-
specific data, the difference was more apparent because the cor-
pus domains were more distant. In the T scenario reported here,
the weighted LMs did not bring any improvement over ?mix?
and even performed worse than the baseline NC. We attribute
this effect to some randomness in the MERT procedure.
Translation of Verb Modifier
. . . preserves meaning 56% 79%
. . . is disrupted 14% 12%
. . . is missing 27% 1%
. . . is unknown (not translated) 0% 5%
Table 3: Analysis of 77 Verb-Modifier pairs in 15
sample sentences.
More importantly, our analysis has shown that
even in cases where both the Verb and the Modi-
fier are lexically correct, the relation between them
in Czech is either non-grammatical or meaning-
disrupted in 56% of these cases. Commented sam-
ples of such errors are given in Figure 2 below. The
first sample shows that a strong language model can
lead to the choice of a grammatical relation that nev-
ertheless does not convey the original meaning. The
second sample illustrates a situation where two cor-
rect options are available but the system chooses
an inappropriate relation, most probably because of
backing off to a generic pattern verb-nounaccusativeplural .
This pattern is quite common for expressing the ob-
ject role of many verbs (such as vydat, see Cor-
rect option 2 in Figure 2), but does not fit well
with the verb vybe?hnout. While the target-side
data may be rich enough to learn the generalization
vybe?hnout?s?instr, no such generalization is possi-
ble with language models over word forms or mor-
phological tags only. The target side data will be
hardly ever rich enough to learn this particular struc-
ture in all correct morphological and lexical variants:
vybe?hl?s?reklamou, vybe?hla?s?reklamami, vybe?hl?
s?prohla?s?en??m, vybe?hli?s?ozna?men??m, . . . . We
would need a mixed model that combines verb lem-
mas, prepositions and case information to properly
capture the relations.
Unfortunately, our preliminary experiments that
made use of automatic Czech dependency parse
trees to construct a factor explicitly highlighting the
Verb (lexicalized) its Modifiers (case and the lemma
of the preposition, if present) and boundary sym-
bols such as punctuation or conjunctions and using
a dummy token for all other words did not bring any
improvement over the baseline. A possible reason is
that we employed only a standard 7-gram language
model to this factor. A more appropriate treatment
237
is to disregard the dummy tokens in the language
model at all and use an n-gram language model that
looks at last n? 1 non-dummy items.
8 Related Research
Class-based LMs (Brown et al, 1992) or factored
LMs (Bilmes and Kirchhoff, 2003) are very similar
to our T+C scenario. Given the small differences
in all T+. . . scenarios? performance, class-based LM
might bring equivalent improvement. Yang and
Kirchhoff (2006) have recently documented minor
BLEU improvement using factored LMs in single-
factored SMT to English. The multi-factored ap-
proach to SMT of Moses is however more general.
Many researchers have tried to employ mor-
phology in improving word alignment techniques
(e.g. (Popovic? and Ney, 2004)) or machine trans-
lation quality (Nie?en and Ney (2001), Koehn and
Knight (2003), Zollmann et al (2006), among oth-
ers, for various languages; Goldwater and McClosky
(2005), Bojar et al (2006) and Talbot and Osborne
(2006) for Czech), however, they focus on translat-
ing from the highly inflectional language.
Durgar El-Kahlout and Oflazer (2006) report pre-
liminary experiments in English to Turkish single-
factored phrase-based translation, gaining signifi-
cant improvements by splitting root words and their
morphemes into a sequence of tokens. In might be
interesting to explore multi-factored scenarios for
different Turkish morphology representation sug-
gested the paper.
de Gispert et al (2005) generalize over verb forms
and generate phrase translations even for unseen tar-
get verb forms. The T+T+G scenario allows a sim-
ilar extension if the described generation step is re-
placed by a (probabilistic) morphological generator.
Nguyen and Shimazu (2006) translate from En-
glish to Vietnamese but the morphological richness
of Vietnamese is comparable to English. In fact the
Vietnamese vocabulary size is even smaller than En-
glish vocabulary size in one of their corpora. The
observed improvement due to explicit modelling of
morphology might not scale up beyond small-data
setting.
As an alternative option to our verb-modifier
experiments, structured language models (Chelba
and Jelinek, 1998) might be considered to improve
clause coherence, until full-featured syntax-based
MT models (Yamada and Knight (2002), Eisner
(2003), Chiang (2005) among many others) are
tested when translating to morphologically rich lan-
guages.
9 Conclusion
We experimented with multi-factored phrase-based
translation aimed at improving morphological co-
herence in MT output. We varied the setup of ad-
ditional factors (translation scenario) and the level
of detail in morphological tags. Our results on
English-to-Czech translation demonstrate signifi-
cant improvement in BLEU scores by explicit mod-
elling of morphology and using a separate morpho-
logical language model to ensure the coherence. To
our knowledge, this is one of the first experiments
showing the advantages of using multiple factors in
MT.
Verb-modifier errors have been studied and a fac-
tor capturing verb-modifier dependencies has been
proposed. Unfortunately, this factor has yet to bring
any improvement.
10 Acknowledgement
The work on this project was partially sup-
ported by the grants Collegium Informaticum
GA ?CR 201/05/H014, grants No. ME838 and
GA405/06/0589 (PIRE), FP6-IST-5-034291-STP
(Euromatrix), and NSF No. 0530118.
References
Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored language
models and generalized parallel backoff. In Proc. of NAACL
2003, pages 4?6.
Ondr?ej Bojar and Zdene?k ?Zabokrtsky?. 2006. CzEng: Czech-
English Parallel Corpus, Release version 0.5. Prague Bul-
letin of Mathematical Linguistics, 86:59?62.
Ondr?ej Bojar, Evgeny Matusov, and Hermann Ney. 2006.
Czech-English Phrase-Based Machine Translation. In Proc.
of FinTAL 2006, pages 214?224, Turku, Finland.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jen-
nifer C. Lai, and Robert L. Mercer. 1992. Class-based n-
gram models of natural language. Computational Linguis-
tics, 18(4):467?479.
Ciprian Chelba and Frederick Jelinek. 1998. Exploiting syntac-
tic structure for language modeling. In Proc. of ACL 1998,
pages 225?231, San Francisco, California.
238
Input: Keep on investing.
MT output: Pokrac?ovalo investova?n??. (grammar correct here!)
Gloss: Continued investing. (Meaning: The investing continued.)
Correct: Pokrac?ujte v investova?n??.
Input: brokerage firms rushed out ads . . .
MT Output: brokerske? firmy vybe?hl reklamy
Gloss: brokerage firmspl.fem ransg.masc adspl.voc,sg.genpl.nom,pl.acc
Correct option 1: brokerske? firmy vybe?hly s reklamamipl.instr
Correct option 2: brokerske? firmy vydaly reklamypl.acc
Figure 2: Two sample errors in translating Verb-Modifier relation from English to Czech.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proc. of ACL 2005, pages
263?270.
Martin ?Cmejrek, Jan Cur???n, Jir??? Havelka, Jan Hajic?, and
Vladislav Kubon?. 2004. Prague Czech-English Dependecy
Treebank: Syntactically Annotated Resources for Machine
Translation. In Proc. of LREC 2004, Lisbon, Portugal.
Adria` de Gispert, Jose? B. Marin?o, and Josep M. Crego. 2005.
Improving statistical machine translation by classifying and
generalizing inflected verb forms. In Proc. of Eurospeech
2005, pages 3185?3188, Lisbon, Portugal.
?Ilknur Durgar El-Kahlout and Kemal Oflazer. 2006. Initial Ex-
plorations in English to Turkish Statistical Machine Transla-
tion. In Proc. of the Workshop on Statistical Machine Trans-
lation, ACL 2006, pages 7?14, New York City.
Jason Eisner. 2003. Learning non-isomorphic tree mappings
for machine translation. In Proc. of ACL 2003, Companion
Volume, pages 205?208, Sapporo, Japan.
Sharon Goldwater and David McClosky. 2005. Improving
statistical MT through morphological analysis. In Proc. of
HLT/EMNLP 2005, pages 676?683.
Jan Hajic? and Barbora Hladka?. 1998. Tagging Inflective Lan-
guages: Prediction of Morphological Categories for a Rich,
Structured Tagset. In Proc. of COLING/ACL 1998, pages
483?490, Montreal, Canada.
Jan Hajic?. 2004. Disambiguation of Rich Inflection (Compu-
tational Morphology of Czech). Nakladatelstv?? Karolinum,
Prague.
Philipp Koehn and Kevin Knight. 2003. Empirical methods for
compound splitting. In Proc. of EACL 2003, pages 187?193.
Philipp Koehn. 2004a. Pharaoh: A beam search decoder for
phrase-based statistical machine translation models. In Proc.
of AMTA 2004, pages 115?124.
Philipp Koehn. 2004b. Statistical Significance Tests for Ma-
chine Translation Evaluation. In Proc. of EMNLP 2004,
Barcelona, Spain.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statisti-
cal Machine Translation. In Proc. of MT Summit X.
Guido Minnen, John Carroll, and Darren Pearce. 2001. Ap-
plied morphological processing of English. Natural Lan-
guage Engineering, 7(3):207?223.
T.P. Nguyen and A. Shimazu. 2006. Improving Phrase-Based
SMT with Morpho-Syntactic Analysis and Transformation.
In Proc. of AMTA 2006, pages 138?147.
Sonja Nie?en and Hermann Ney. 2001. Toward hierarchical
models for statistical machine translation of inflected lan-
guages. In Proc. of Workshop on Data-driven methods in
machine translation, ACL 2001, pages 1?8.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proc. of ACL 2003, Sapporo,
Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. BLEU: a Method for Automatic Evaluation of
Machine Translation. In Proc. of ACL 2002, pages 311?318.
M. Popovic? and H. Ney. 2004. Improving Word Alignment
Quality using Morpho-Syntactic Information. In Proc. of
COLING 2004, Geneva, Switzerland.
Adwait Ratnaparkhi. 1996. A Maximum Entropy Part-Of-
Speech Tagger. In Proc. of EMNLP 1996, Philadelphia,
USA.
David Talbot and Miles Osborne. 2006. Modelling lexical re-
dundancy for machine translation. In Proc. of COLING and
ACL 2006, pages 969?976, Sydney, Australia.
Kenji Yamada and Kevin Knight. 2002. A decoder for syntax-
based statistical MT. In Proc. of ACL 2002, pages 303?310.
Mei Yang and Katrin Kirchhoff. 2006. Phrase-based backoff
models for machine translation of highly inflected languages.
In Proc. of EACL 2006.
Andreas Zollmann, Ashish Venugopal, and Stephan Vogel.
2006. Bridging the inflection morphology gap for arabic sta-
tistical machine translation. In Proc. of HLT/NAACL.
239
Proceedings of the Third Workshop on Statistical Machine Translation, pages 143?146,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Phrase-Based and Deep Syntactic
English-to-Czech Statistical Machine Translation ?
Ondr?ej Bojar and Jan Hajic?
Institute of Formal and Applied Linguistics
?UFAL MFF UK, Malostranske? na?me?st?? 25
CZ-11800 Praha, Czech Republic
{bojar,hajic}@ufal.mff.cuni.cz
Abstract
This paper describes our two contributions to
WMT08 shared task: factored phrase-based
model using Moses and a probabilistic tree-
transfer model at a deep syntactic layer.
1 Introduction
Czech is a Slavic language with very rich morphol-
ogy and relatively free word order. The Czech
morphological system (Hajic?, 2004) defines 4,000
tags in theory and 2,000 were actually seen in a
big tagged corpus while the English Penn Treebank
tagset contains just about 50 tags. In our parallel
corpus (see below), the English vocabulary size is
148k distinct word forms but more than twice as big
in Czech, 343k distinct word forms.
When translating to Czech from an analytic lan-
guage such as English, target word forms have to
be chosen correctly to produce a grammatical sen-
tence and preserve the expressed relations between
elements in the sentence, e.g. verbs and their modi-
fiers.
This year, we have taken two radically different
approaches to English-to-Czech MT. Section 2 de-
scribes our setup of the phrase-based system Moses
(Koehn et al, 2007) and Section 3 focuses on a sys-
tem with probabilistic tree transfer employed at a
deep syntactic layer and the new challenges this ap-
proach brings.
?The work on this project was supported by the grants FP6-
IST-5-034291-STP (EuroMatrix), MSM0021620838, M?SMT
?CR LC536, and GA405/06/0589.
2 Factored Phrase-Based MT to Czech
Bojar (2007) describes various experiments with
factored translation to Czech aimed at improving
target-side morphology. We use essentially the same
setup with some cleanup and significantly larger
target-side training data:
Parallel data from CzEng 0.7 (Bojar et al, 2008),
with original sentence-level alignment and tokeniza-
tion. The parallel corpus was taken as a monolithic
text source disregarding differences between CzEng
data sources. We use only 1-1 aligned sentences.
Word alignment using GIZA++ toolkit (Och and
Ney, 2000), the default configuration as available in
training scripts for Moses. We based the word align-
ment on Czech and English lemmas (base forms
of words) as provided by the combination of tag-
gers and lemmatizers by Hajic? (2004) for Czech and
Brants (2000) followed by Minnen et al (2001) for
English. We symmetrized the two GIZA++ runs us-
ing grow-diag-final heuristic.
Truecasing. We attempted to preserve meaning-
bearing case distinctions. The Czech lemmatizer
produces case-sensitive lemmas and thus makes it
easy to cast the capitalization of the lemma back on
the word form.1 For English we approximate the
same effect by a two-step procedure.2
1We change the capitalization of the form to match the
lemma in cases where the lemma is lowercase, capitalized (uc-
first) or all-caps. For mixed-case lemmas, we keep the form
intact.
2We first collect a lexicon of the most typical ?shapes? for
each word form (ignoring title-like sentences with most words
capitalized and the first word in a sentence). Capitalized and
all-caps words in title-like sentences are then changed to their
143
Decoding steps. We use a simple two-step sce-
nario similar to class-based models (Brown and oth-
ers, 1992): (1) the source English word forms are
translated to Czech word forms and (2) full Czech
morphological tags are generated from the Czech
forms.
Language models. We use the following 6 inde-
pendently weighted language models for the target
(Czech) side:
? 3-grams of word forms based on all CzEng 0.7
data, 15M tokens,
? 3-grams of word forms in Project Syndicate
section of CzEng (in-domain for WMT07 and
WMT08 NC-test set), 1.8M tokens,
? 4-grams of word forms based on Czech Na-
tional Corpus (Kocek et al, 2000), version
SYN2006, 365M tokens,
? three models of 7-grams of morphological tags
from the same sources.
Lexicalized reordering using the mono-
tone/swap/discontinuous bidirectional model based
on both source and target word forms.
MERT. We use the minimum-error rate training
procedure by Och (2003) as implemented in the
Moses toolkit to set the weights of the various trans-
lation and language models, optimizing for BLEU.
Final detokenization is a simple rule-based pro-
cedure based on Czech typographical conventions.
Finally, we capitalize the beginnings of sentences.
See BLEU scores in Table 2 below.
3 MT with a Deep Syntactic Transfer
3.1 Theoretical Background
Czech has a well-established theory of linguistic
analysis called Functional Generative Description
(Sgall et al, 1986) supported by a big treebanking
enterprise (Hajic? and others, 2006) and on-going
adaptations for other languages including English
(Cinkova? and others, 2004). There are two layers
typical shape. In other sentences we change the case only if a
typically lowercase word is capitalized (e.g. at the beginning
of the sentence) or if a typically capitalized word is all-caps.
Unknown words in title-like sentences are lowercased and left
intact in other sentences.
Pred
Sb uvedla , z?e Pred
=
VP
NP said VP
Figure 1: Sample treelet pair, a-layer.
of syntactic analysis, both formally captured as la-
belled ordered dependency trees: the ANALYTICAL
(a-, surface syntax) representation bears a 1-1 corre-
spondence between tokens in the sentence and nodes
in the tree; the TECTOGRAMMATICAL (t-, deep syn-
tax) representation contains nodes only for autose-
mantic words and adds nodes for elements not ex-
pressed on the surface but required by the grammar
(e.g. dropped pronouns).
We use the following tools to automatically anno-
tate plaintext up to the t-layer: (1) TextSeg ( ?Ces?ka,
2006) for tokenization, (2) tagging and lemmatiza-
tion see above, (3) parsing to a-layer: Collins (1996)
followed by head-selection rules for English, Mc-
Donald and others (2005) for Czech, (4) parsing to t-
layer: ?Zabokrtsky? (2008) for English, Klimes? (2006)
for Czech.
3.2 Probabilistic Tree Transfer
The transfer step is based on Synchronous Tree Sub-
stitution Grammars (STSG), see Bojar and ?Cmejrek
(2007) for a detailed explanation. The essence is a
log-linear model to search for the most likely syn-
chronous derivation ?? of the source T1 and target T2
dependency trees:
?? = argmax
? s.t. source is T1
exp
(
M
?
m=1
?mhm(?)
)
(1)
The key feature function hm in STSG represents
the probability of attaching pairs of dependency
treelets ti1:2 such as in Figure 1 into aligned pairs of
frontiers ( ) in another treelet pair tj1:2 given fron-
tier state labels (e.g. Pred- VP in Figure 1):
hSTSG(?) = log
k
?
i=0
p(ti1:2 | frontier states) (2)
Other features include e.g. number of internal
nodes (drawn as in Figure 1) produced, number
of treelets produced, and more importantly the tra-
ditional n-gram language model if the target (a-)tree
144
is linearized right away or a binode model promot-
ing likely combinations of the governor g(e) and the
child c(e) of an edge e ? T2:
hbinode(?) = log
?
e?T2
p(c(e) | g(e)) (3)
The probabilistic dictionary of aligned treelet
pairs is extracted from node-aligned (GIZA++ on
linearized trees) parallel automatic treebank as in
Moses? training: all treelet pairs compatible with the
node alignment.
3.2.1 Factored Treelet Translation
Labels of nodes at the t-layer are not atomic but
consist of more than 20 attributes representing var-
ious linguistic features.3 We can consider the at-
tributes as individual factors (Koehn and Hoang,
2007). This allows us to condition the translation
choice on a subset of source factors only. In order to
generate a value for each target-side factor, we use
a sequence of mapping steps similar to Koehn and
Hoang (2007). For technical reasons, our current
implementation allows to generate factored target-
side only when translating a single node to a single
node, i.e. preserving the tree structure.
In our experiments we used 8 source (English) t-
node attributes and 14 target (Czech) attributes.
3.3 Recent Experimental Results
Table 1 shows BLEU scores for various configura-
tions of our decoder. The abbreviations indicate be-
tween which layers the tree transfer was employed
(e.g. ?eact? means English a-layer to Czech t-layer).
The ?p? layer is an approximation of phrase-based
MT: the surface ?syntactic? analysis is just a left-to-
right linear tree.4 For setups ending in t-layer, we
use a deterministic generation the of Czech sentence
by Pta?c?ek and ?Zabokrtsky? (2006).
For WMT08 shared task, Table 2, we used a vari-
ant of the ?etct factored? setup with the annotation
pipeline as incorporated in TectoMT ( ?Zabokrtsky?,
2008) environment and using TectoMT internal
3Treated as atomic, t-node labels have higher entropy
(11.54) than lowercase plaintext (10.74). The t-layer by itself
does not bring any reduction in vocabulary. The idea is that the
attributes should be more or less independent and should map
easier across languages.
4Unlike Moses, ?epcp? does not permit phrase reordering.
Tree-based Transfer LM Type BLEU
epcp n-gram 10.9?0.6
eaca n-gram 8.8?0.6
epcp none 8.7?0.6
eaca none 6.6?0.5
etca n-gram 6.3?0.6
etct factored, preserving structure binode 5.6?0.5
etct factored, preserving structure none 5.3?0.5
eact, target side atomic binode 3.0?0.3
etct, atomic, all attributes binode 2.6?0.3
etct, atomic, all attributes none 1.6?0.3
etct, atomic, just t-lemmas none 0.7?0.2
Phrase-based (Moses) as reported by Bojar (2007)
Vanilla n-gram 12.9?0.6
Factored to improve target morphology n-gram 14.2?0.7
Table 1: English-to-Czech BLEU scores for syntax-based
MT on WMT07 DevTest.
WMT07 WMT08
DevTest NC Test News Test
Moses 14.9?0.9 16.4?0.6 12.3?0.6
Moses, CzEng data only 13.9?0.9 15.2?0.6 10.0?0.5
etct, TectoMT annotation 4.7?0.5 4.9?0.3 3.3?0.3
Table 2: WMT08 shared task BLEU scores.
rules for t-layer parsing and generation instead of
Klimes? (2006) and (Pta?c?ek and ?Zabokrtsky?, 2006).
3.3.1 Discussion
Our syntax-based approach does not reach scores
of phrase-based MT due to the following reasons:
Cumulation of errors at every step of analysis.
Data loss due to incompatible parses and node
alignment. Unlike e.g. Quirk et al (2005) or Huang
et al (2006) who parse only one side and project the
structure, we parse both languages independently.
Natural divergence and random errors in either of
the parses and/or the alignment prevent us from ex-
tracting many treelet pairs.
Combinatorial explosion in target node at-
tributes. Currently, treelet options are fully built in
advance. Uncertainty in the many t-node attributes
leads to too many insignificant variations while e.g.
different lexical choices are pushed off the stack.
While vital for final sentence generation (see Ta-
ble 1), fine-grained t-node attributes should be pro-
duced only once all key structural, lexical and form
decisions have been made. The same sort of explo-
sion makes complicated factored setups not yet fea-
sible in Moses, either.
145
Lack of n-gram LM in the (deterministic) gen-
eration procedures from a t-tree. While we support
final LM-based rescoring, there is too little variance
in n-best lists due to the explosion mentioned above.
Too many model parameters given our stack
limit. We use identical MERT implementation to
optimize ?ms but in the large space of hypotheses,
MERT does not converge.
3.3.2 Related Research
Our approach should not be confused with the
TectoMT submission by Zdene?k ?Zabokrtsky? with a
deterministic transfer: heuristics fully exploiting the
similarity of English and Czech t-layers.
Ding and Palmer (2005) improve over word-based
MT baseline with a formalism very similar to STSG.
Though not explicitly stated, they seem not to en-
code frontiers in the treelets and allow for adjunction
(adding siblings), like Quirk et al (2005), which sig-
nificantly reduces data sparseness.
Riezler and III (2006) report an improvement in
MT grammaticality on a very restricted test set:
short sentences parsable by an LFG grammar with-
out back-off rules.
4 Conclusion
We have presented our best-performing factored
phrase-based English-to-Czech translation and a
highly experimental complex system with tree-
based transfer at a deep syntactic layer. We have
discussed some of the reasons why the phrase-based
MT currently performs much better.
References
Ondr?ej Bojar and Martin ?Cmejrek. 2007. Mathematical
Model of Tree Transformations. Project EuroMatrix -
Deliverable 3.2, ?UFAL, Charles University, Prague.
Ondr?ej Bojar, Zdene?k ?Zabokrtsky?, Pavel ?Ces?ka, Peter
Ben?a, and Miroslav Jan??c?ek. 2008. CzEng 0.7: Paral-
lel Corpus with Community-Supplied Translations. In
Proc. of LREC 2008. ELRA.
Ondr?ej Bojar. 2007. English-to-Czech Factored Machine
Translation. In Proc. of ACL Workshop on Statistical
Machine Translation, pages 232?239, Prague.
Thorsten Brants. 2000. TnT - A Statistical Part-of-
Speech Tagger . In Proc. of ANLP-NAACL.
Peter F. Brown et al 1992. Class-based n-gram mod-
els of natural language. Computational Linguistics,
18(4):467?479.
Pavel ?Ces?ka. 2006. Segmentace textu. Bachelor?s The-
sis, MFF, Charles University in Prague.
Silvie Cinkova? et al 2004. Annotation of English on the
tectogrammatical level. Technical Report TR-2006-
35, ?UFAL/CKL, Prague, Czech Republic.
Michael Collins. 1996. A New Statistical Parser Based
on Bigram Lexical Dependencies. In Proc. of ACL.
Yuan Ding and Martha Palmer. 2005. Machine Transla-
tion Using Probabilistic Synchronous Dependency In-
sertion Grammars. In Proc. of ACL.
Jan Hajic?. 2004. Disambiguation of Rich Inflection
(Computational Morphology of Czech). Nakladatel-
stv?? Karolinum, Prague.
Jan Hajic? et al 2006. Prague Dependency Treebank 2.0.
LDC2006T01, ISBN: 1-58563-370-4.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical Syntax-Directed Translation with Extended
Domain of Locality. In Proc. of AMTA, Boston, MA.
Va?clav Klimes?. 2006. Analytical and Tectogrammatical
Analysis of a Natural Language. Ph.D. thesis, ?UFAL,
MFF UK, Prague, Czech Republic.
Jan Kocek, Marie Kopr?ivova?, and Karel Kuc?era, edi-
tors. 2000. ?Cesky? na?rodn?? korpus - u?vod a pr???ruc?ka
uz?ivatele. FF UK - ?U ?CNK, Praha.
Philipp Koehn and Hieu Hoang. 2007. Factored Transla-
tion Models. In Proc. of EMNLP.
Philipp Koehn, Hieu Hoang, et al 2007. Moses: Open
Source Toolkit for Statistical Machine Translation. In
Proc. of ACL Demo and Poster Sessions.
Ryan McDonald et al 2005. Non-Projective Depen-
dency Parsing using Spanning Tree Algorithms. In
Proc. of HLT/EMNLP 2005.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Natu-
ral Language Engineering, 7(3):207?223.
Franz Josef Och and Hermann Ney. 2000. A Comparison
of Alignment Models for Statistical Machine Transla-
tion. In Proc. of COLING, pages 1086?1090.
Franz Josef Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proc. of ACL.
Jan Pta?c?ek and Zdene?k ?Zabokrtsky?. 2006. Synthesis
of Czech Sentences from Tectogrammatical Trees. In
Proc. of TSD, pages 221?228.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency Treelet Translation: Syntactically Informed
Phrasal SMT. In Proc. of ACL, pages 271?279.
Stefan Riezler and John T. Maxwell III. 2006. Grammat-
ical Machine Translation. In Proc. of HLT/NAACL.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The Meaning of the Sentence and Its Semantic and
Pragmatic Aspects. Academia, Prague.
Zdene?k ?Zabokrtsky?. 2008. Tecto MT. Technical report,
?UFAL/CKL, Prague, Czech Republic. In prep.
146
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 125?129,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
English-Czech MT in 2008 ?
Ondr?ej Bojar, David Marec?ek, Va?clav Nova?k, Martin Popel,
Jan Pta?c?ek, Jan Rous?, Zdene?k ?Zabokrtsky?
Charles University, Institute of Formal and Applied Linguistics
Malostranske? na?m. 25, Praha 1, CZ-118 00, Czech Republic
{bojar,marecek,novak,ptacek,zabokrtsky}@ufal.mff.cuni.cz
{popel,jan.rous}@matfyz.cz
Abstract
We describe two systems for English-to-
Czech machine translation that took part
in the WMT09 translation task. One of
the systems is a tuned phrase-based system
and the other one is based on a linguisti-
cally motivated analysis-transfer-synthesis
approach.
1 Introduction
We participated in WMT09 with two very dif-
ferent systems: (1) a phrase-based MT based
on Moses (Koehn et al, 2007) and tuned for
English?Czech translation, and (2) a complex
system in the TectoMT platform ( ?Zabokrtsky? et
al., 2008).
2 Data
2.1 Monolingual Data
Our Czech monolingual data consist of (1)
the Czech National Corpus (CNC, versions
SYN200[056], 72.6%, Kocek et al (2000)), (2)
a collection of web pages downloaded by Pavel
Pecina (Web, 17.1%), and (3) the Czech mono-
lingual data provided by WMT09 organizers
(10.3%). Table 1 lists sentence and token counts
(see Section 2.3 for the explanation of a- and t-
layer).
Sentences 52 M
with nonempty t-layer 51 M
a-nodes (i.e. tokens) 0.9 G
t-nodes 0.6 G
Table 1: Czech monolingual training data.
? The work on this project was supported by the grants
MSM0021620838, 1ET201120505, 1ET101120503, GAUK
52408/2008, M?SMT ?CR LC536 and FP6-IST-5-034291-STP
(EuroMatrix).
2.2 Parallel Data
As the source of parallel data we use an internal
release of Czech-English parallel corpus CzEng
(Bojar et al, 2008) extended with some additional
texts. One of the added sections was gathered
from two major websites containing Czech sub-
titles to movies and TV series1. The matching of
the Czech and English movies is rather straight-
forward thanks to the naming conventions. How-
ever, we were unable to reliably determine the se-
ries number and the episode number from the file
names. We employed a two-step procedure to au-
tomatically pair the TV series subtitle files. For
every TV series:
1. We clustered the files on both sides to remove
duplicates
2. We found the best matching using a provi-
sional translation dictionary. This proved to
be a successful technique on a small sample
of manually paired test data. The process was
facilitated by the fact that the correct pairs of
episodes usually share some named entities
which the human translator chose to keep in
the original English form.
Table 2 lists parallel corpus sizes and the distri-
bution of text domains.
English Czech
Sentences 6.91 M
with nonempty t-layer 6.89 M
a-nodes (i.e. tokens) 61 M 50 M
t-nodes 41 M 33 M
Distribution: [%] [%]
Subtitles 68.2 Novels 3.3
Software Docs 17.0 Commentaries/News 1.5
EU (Legal) Texts 9.5 Volunteer-supplied 0.4
Table 2: Czech-English data sizes and sources.
1www.opensubtitles.org and titulky.com
125
2.3 Data Preprocessing using TectoMT
platform: Analysis and Alignment
As we believe that various kinds of linguistically
relevant information might be helpful in MT, we
performed automatic analysis of the data. The
data were analyzed using the layered annotation
scheme of the Prague Dependency Treebank 2.0
(PDT 2.0, Hajic? and others (2006)), i.e. we used
three layers of sentence representation: morpho-
logical layer, surface-syntax layer (called analyti-
cal (a-) layer), and deep-syntax layer (called tec-
togrammatical (t-) layer).
The analysis was implemented using TectoMT,
( ?Zabokrtsky? et al, 2008). TectoMT is a highly
modular software framework aimed at creating
MT systems (focused, but by far not limited to
translation using tectogrammatical transfer) and
other NLP applications. Numerous existing NLP
tools such as taggers, parsers, and named entity
recognizers are already integrated in TectoMT, es-
pecially for (but again, not limited to) English and
Czech.
During the analysis of the large Czech mono-
lingual data, we used Jan Hajic??s Czech tagger
shipped with PDT 2.0, Maximum Spanning Tree
parser (McDonald et al, 2005) with optimized set
of features as described in Nova?k and ?Zabokrtsky?
(2007), and a tool for assigning functors (seman-
tic roles) from Klimes? (2006), and numerous other
components of our own (e.g. for conversion of an-
alytical trees into tectogrammatical ones).
In the parallel data, we analyzed the Czech side
using more or less the same scenario as used for
the monolingual data. English sentences were an-
alyzed using (among other tools) Morce tagger
Spoustova? et al (2007) and Maximum Spanning
Tree parser.2
The resulting deep syntactic (tectogrammatical)
Czech and English trees are then aligned using T-
aligner?a feature based greedy algorithm imple-
mented for this purpose (Marec?ek et al, 2008). T-
aligner finds corresponding nodes between the two
given trees and links them. For deciding whether
to link two nodes or not, T-aligner makes use of
a bilingual lexicon of tectogrammatical lemmas,
morphosyntactic similarities between the two can-
didate nodes, their positions in the trees and other
similarities between their parent/child nodes. It
2In some previous experiments (e.g. ?Zabokrtsky? et al
(2008)), we used phrase-structure parser Collins (1999) with
subsequent constituency-dependency conversion.
also uses word alignment generated from surface
shapes of sentences by GIZA++ tool, Och and Ney
(2003). We use acquired aligned tectogrammatical
trees for training some models for the transfer.
As analysis of such amounts of data is obvi-
ously computationally very demanding, we run it
in parallel using Sun Grid Engine3 cluster of 40
4-CPU computers. For this purpose, we imple-
mented a rather generic tool that submits any Tec-
toMT pipeline to the cluster.
3 Factored Phrase-Based MT
We essentially repeat our experiments from last
year (Bojar and Hajic?, 2008): GIZA++ align-
ments4 on a-layer lemmas (a-layer nodes corre-
spond 1-1 to surface tokens), symmetrized using
grow-diag-final (no -and) heuristic5 .
Probably due to the domain difference (the test
set is news), including Subtitles in the parallel data
and Web in the monolingual data did not bring any
improvement that would justify the additional per-
formance costs. For most of the phrase-based ex-
periments, we thus used only 2.2M parallel sen-
tences (27M Czech and 32M English tokens) and
43M Czech sentences (694 M tokens).
In Table 3 below, we report the scores for the
following setups selected from about 50 experi-
ments we ran in total:
Moses T is a simple phrase-based translation (T)
with no additional factors. The translation is
performed on truecased word forms (i.e. sen-
tence capitalization removed unless the first
word seems to be a name). The 4-gram lan-
guage model is based on the 43M sentences.
Moses T+C is a factored setup with form-to-form
translation (T) and target-side morphological
coherence check following Bojar and Hajic?
(2008). The setup uses two language mod-
els: 4-grams of word forms and 7-grams of
morphological tags.
Moses T+C+C&T+T+G 84k is a setup desirable
from the linguistic point of view. Two in-
dependent translation paths are used: (1)
form?form translation with two target-side
checks (lemma and tag generated from the
target-side form) as a fine-grained baseline
3http://gridengine.sunsource.net/
4Default settings, IBM models and iterations: 153343.
5Later, we found out that the grow-diag-final-and heuris-
tic provides insignificantly superior results.
126
with the option to resort to (2) an independent
translation of lemma?lemma and tag?tag
finished by a generation step that combines
target-side lemma and tag to produce the fi-
nal target-side form.
We use three language models in this setup
(3-grams of forms, 3-grams of lemmas, and
10-grams of tags).
Due to the increased complexity of the setup,
we were able to train this model on 84k par-
allel sentences only (the Commentaries sec-
tion) and we use the target-side of this small
training data for language models, too.
For all the setups we perform standard MERT
training on the provided development set.6
4 Translation Setup Based on
Tectogrammatical Transfer
In this translation experiment, we follow the tradi-
tional analysis-transfer-synthesis approach, using
the set of PDT 2.0 layers: we analyze the input
English sentence up to the tectogrammatical layer
(through the morphological and analytical ones),
then perform the tectogrammatical transfer, and
then synthesize the target Czech sentence from its
tectogrammatical representation. The whole pro-
cedure consists of about 80 steps, so the following
description is necessarily very high level.
4.1 Analysis
Each sentence is tokenized (roughly according to
the Penn Treebank conventions), tagged by the En-
glish version of the Morce tagger Spoustova? et al
(2007), and lemmatized by our lemmatizer. Then
the dependency parser (McDonald et al, 2005) is
applied. Then the analytical trees resulting from
the parser are converted to the tectogrammatical
ones (i.e. functional words are removed, only
morphologically indispensable categories are left
with the nodes using a sequence of heuristic proce-
dures). Unlike in PDT 2.0, the information about
the original syntactic form is stored with each t-
node (values such as v:inf for an infinitive verb
form, v:since+fin for the head of a subor-
dinate clause of a certain type, adj:attr for
an adjective in attribute position, n:for+X for a
given prepositional group are distinguished).
6We used the full development set of 2k sentences for
?Moses T? and a subset of 1k sentences for the other two
setups due to time constraints.
One of the steps in the analysis of English is
named entity recognition using Stanford Named
Entity Recognizer (Finkel et al, 2005). The nodes
in the English t-layer are grouped according to the
detected named entities and they are assigned the
type of entity (location, person, or organization).
This information is preserved in the transfer of the
deep English trees to the deep Czech trees to al-
low for the appropriate capitalization of the Czech
translation.
4.2 Transfer
The transfer phase consists of the following steps:
? Initiate the target-side (Czech) t-trees sim-
ply by ?cloning? the source-side (English) t-
trees. Subsequent steps usually iterate over
all t-nodes. In the following, we denote a
source-side t-node as S and the correspond-
ing target-side node as T.
? Translate formemes using
two probabilistic dictionaries
(p(T.formeme|S.formeme, S.parent.lemma)
and p(T.formeme|S.formeme)) and a few
manual rules. The formeme translation
probability estimates were extracted from a
part of the parallel data mentioned above.
? Translate lemmas using a probabilistic dictio-
nary (p(T.lemma|S.lemma)) and a few rules
that ensure compatibility with the previously
chosen formeme. Again, this probabilistic
dictionary was obtained using the aligned
tectogrammatical trees from the parallel cor-
pus.
? Fill the grammatemes (deep-syntactic equiv-
alent of morphological categories) gender
(for denotative nouns) and aspect (for verbs)
according to the chosen lemma. We also
fix grammateme values where the English-
Czech grammateme correspondence is non-
trivial (e.g. if an English gerund expression is
translated to Czech as a subordinating clause,
the tense grammateme has to be filled). How-
ever, the transfer of grammatemes is defi-
nitely much easier task than the transfer of
formemes and lemmas.
4.3 Synthesis
The transfer step yields an abstract deep
syntactico-semantical tree structure. Firstly,
127
we derive surface morphological categories
from their deep counterparts taking care of their
agreement where appropriate and we also remove
personal pronouns in subject positions (because
Czech is a pro-drop language).
To arrive at the surface tree structure, auxil-
iary nodes of several types are added, including
(1) reflexive particles, (2) prepositions, (3) subor-
dinating conjunctions, (4) modal verbs, (5) ver-
bal auxiliaries, and (6) punctuation nodes. Also,
grammar-based node ordering changes (imple-
mented by rules) are performed: e.g. if an English
possessive attribute is translated using Czech gen-
itive, it is shifted into post-modification position.
After finishing the inflection of nouns, verbs,
adjectives and adverbs (according to the values of
morphological categories derived from agreement
etc.), prepositions may need to be vocalized: the
vowel -e or -u is attached to the preposition if the
pronunciation of prepositional group would be dif-
ficult otherwise.
After the capitalization of the beginning of each
sentence (and each named entity instance), we ob-
tain the final translation by flattening the surface
tree.
4.4 Preliminary Error Analysis
According to our observations most errors happen
during the transfer of lemmas and formemes.
Usually, there are acceptable translations of
lemma and formeme in respective n-best lists
but we fail to choose the best one. The sce-
nario described in Section 4.2 uses quite a
primitive transfer algorithm where formemes
and lemmas are translated separately in two
steps. We hope that big improvements could
be achieved with more sophisticated algo-
rithms (optimizing the probability of the whole
tree) and smoothed probabilistic models (such
as p(T.lemma|S.lemma, T.parent.lemma) and
p(T.formeme|S.formeme, T.lemma, T.parent.lemma)).
Other common errors include:
? Analysis: parsing (especially coordinations
are problematic with McDonald?s parser).
? Transfer: the translation of idioms and col-
locations, including named entities. In these
cases, the classical transfer at the t-layer
is not appropriate and utilization of some
phrase-based MT would help.
? Synthesis: reflexive particles, word order.
5 Experimental Results and Discussion
Table 3 reports lowercase BLEU and NIST scores
and preliminary manual ranks of our submissions
in contrast with other systems participating in
English?Czech translation, as evaluated on the
official WMT09 unseen test set. Note that auto-
matic metrics are known to correlate quite poorly
with human judgements, see the best ranking but
?lower scoring? PC Translator this year and also
in Callison-Burch et al (2008).
System BLEU NIST Rank
Moses T 14.24 5.175 -3.02 (4)
Moses T+C 13.86 5.110 ?
Google 13.59 4.964 -2.82 (3)
U. of Edinburgh 13.55 5.039 -3.24 (5)
Moses T+C+C&T+T+G 84k 10.01 4.360 -
Eurotran XP 09.51 4.381 -2.81 (2)
PC Translator 09.42 4.335 -2.77 (1)
TectoMT 07.29 4.173 -3.35 (6)
Table 3: Automatic scores and preliminary human
rank for English?Czech translation. Systems in
italics are provided for comparison only. Best re-
sults in bold.
Unfortunately, this preliminary evaluation sug-
gests that simpler models perform better, partly
because it is easier to tune them properly both
from computational point of view (e.g. MERT
not stable and prone to overfitting with more fea-
tures7), as well as from software engineering point
of view (debugging of complex pipelines of tools
is demanding). Moreover, simpler models run
faster: ?Moses T? with 12 sents/minute is 4.6
times faster than ?Moses T+C?. (Note that we have
not tuned either of the models for speed.)
While ?Moses T? is probably nearly identical
setup as Google and Univ. of Edinburgh use,
the knowledge of correct language-dependent to-
kenization and the use of relatively high quality
large language model data seems to bring moder-
ate improvements.
6 Conclusion
We described our experiments with a complex lin-
guistically motivated translation system and vari-
ous (again linguistically-motivated) setups of fac-
tored phrase-based translation. An automatic eval-
uation seems to suggest that simpler is better, but
we are well aware that a reliable judgement comes
only from human annotators.
7For ?Moses T+C+C&T+T+G?, we observed BLEU
scores on the test set varying by up to five points absolute
for various weight settings yielding nearly identical dev set
scores.
128
References
Ondr?ej Bojar and Jan Hajic?. 2008. Phrase-Based and
Deep Syntactic English-to-Czech Statistical Ma-
chine Translation. In Proceedings of the Third
Workshop on Statistical Machine Translation, pages
143?146, Columbus, Ohio, June. Association for
Computational Linguistics.
Ondr?ej Bojar, Miroslav Jan??c?ek, Zdene?k ?Zabokrtsky?,
Pavel ?Ces?ka, and Peter Ben?a. 2008. CzEng 0.7:
Parallel Corpus with Community-Supplied Transla-
tions. In Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC?08), Mar-
rakech, Morocco, May. ELRA.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statisti-
cal Machine Translation, pages 70?106, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Michael Collins. 1999. Head-driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In ACL ?05: Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics, pages 363?370, Morristown, NJ, USA.
Association for Computational Linguistics.
Jan Hajic? et al 2006. Prague Dependency Treebank
2.0. CD-ROM, Linguistic Data Consortium, LDC
Catalog No.: LDC2006T0 1, Philadelphia.
Va?clav Klimes?. 2006. Analytical and Tectogrammat-
ical Analysis of a Natural Language. Ph.D. thesis,
Faculty of Mathematics and Physics, Charles Uni-
versity, Prague, Czech Rep.
Jan Kocek, Marie Kopr?ivova?, and Karel Kuc?era, edi-
tors. 2000. ?Cesky? na?rodn?? korpus - u?vod a pr???ruc?ka
uz?ivatele. FF UK - ?U ?CNK, Praha.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In ACL 2007, Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics Companion Volume Proceedings of the Demo
and Poster Sessions, pages 177?180, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
David Marec?ek, Zdene?k ?Zabokrtsky?, and Va?clav
Nova?k. 2008. Automatic Alignment of Czech and
English Deep Syntactic Dependency Trees. In Pro-
ceedings of European Machine Translation Confer-
ence (EAMT 08), pages 102?111, Hamburg, Ger-
many.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency
parsing using spanning tree algorithms. In HLT
?05: Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Natu-
ral Language Processing, pages 523?530, Vancou-
ver, British Columbia, Canada.
Va?clav Nova?k and Zdene?k ?Zabokrtsky?. 2007. Feature
engineering in maximum spanning tree dependency
parser. In Va?clav Matous?ek and Pavel Mautner, ed-
itors, Lecture Notes in Artificial Intelligence, Pro-
ceedings of the 10th I nternational Conference on
Text, Speech and Dialogue, Lecture Notes in Com-
puter Science, pages 92?98, Pilsen, Czech Repub-
lic. Springer Science+Business Media Deutschland
GmbH.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Drahom??ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel
Krbec, and Pavel Kve?ton?. 2007. The best of two
worlds: Cooperation of statistical and rule-based
taggers for czech. In Proceedings of the Work-
shop on Balto-Slavonic Natural Language Process-
ing, ACL 2007, pages 67?74, Praha.
Zdene?k ?Zabokrtsky?, Jan Pta?c?ek, and Petr Pajas. 2008.
TectoMT: Highly Modular Hybrid MT System
with Tectogrammatics Used as Transfer Layer. In
Proc. of the ACL Workshop on Statistical Machine
Translation, pages 167?170, Columbus, Ohio, USA.
129
Problems Of Reusing An Existing MT System?
Ondr?ej Bojar, Petr Homola, Vladislav Kubon?
Institute of Formal and Applied Linguistics
?UFAL MFF UK, Malostranske? na?me?st?? 25, Praha 1, CZ-11800
Czech Republic
{bojar,homola,vk}@ufal.mff.cuni.cz
Abstract
This paper describes an attempt to recy-
cle parts of the Czech-to-Russian ma-
chine translation system (MT) in the
new Czech-to-English MT system. The
paper describes the overall architecture
of the new system and the details of
the modules which have been added.
A special attention is paid to the prob-
lem of named entity recognition and
to the method of automatic acquisition
of lexico-syntactic information for the
bilingual dictionary of the system. The
paper concentrates on the problems en-
countered in the process of reusing ex-
isting modules and their solution.
1 Introduction
The last decade has witnessed several attempts to
increase the quality of MT systems by introduc-
ing new methods. The strong stress on stochastic
methods in the NLP in general and in the MT in
particular, the attempts to develop hybrid systems,
a wide acceptance of translation-memory based
systems among the translation professionals, the
aim at limited domain speech-to-speech transla-
tion systems, all these (and many other) trends
have demonstrated encouraging results in recent
years.
Developing and using new methods definitely
moves the whole MT field forward, but one
?The work described in this paper has been supported by
the grant of the Grant Agency of the Czech Republic GACR
No.405/03/0914 and partially also by the grant of the Grant
Agency of the Charles University GAUK No. 351/2005
should not forget about all the effort invested into
the old systems. Reusing at least some parts of
those systems may help to decrease the costs of
new systems, especially when one of the lan-
guages is not a ?big? language and therefore there
is not such a wide range of tools, grammars, dic-
tionaries available as for example for English,
German, Japanese or Spanish. In this paper we
would like to describe one such attempt to reuse
the existing system for a new language pair.
2 The original system
One of the systems which was silently abandoned
in early nineties was the system for the translation
from Czech to Russian called RUSLAN (Oliva,
1989). It was being developed in the second half
of eighties with the aim to translate texts from a
relatively closed thematic domain, the domain of
operating systems of mainframes.
The system used transfer-based architecture.
The implementation of the system was almost
completely done in Q-systems, a formalism cre-
ated by Alain Colmerauer (Colmerauer, 1969)
for the TAUM-METEO project. The Czech-to
Russian system also relied upon a set of dictio-
naries containing all data exploited by individ-
ual modules of the system. Each lexical item
in the main (bilingual) dictionary contained not
only lexico-syntactic data (valency frames etc.),
but also a set of semantic features.
The work on the system RUSLAN has been ter-
minated in 1990, in the final phase of system test-
ing and debugging. The reason was quite sim-
ple - after the political changes in 1989 there was
no more any commercial demand for Czech to
179
Russian MT system.
The demand for Czech-English translation has
grown dramatically during the years following the
abandonment of the system RUSLAN. On the
other hand, also the range of methods, tools and
resources for MT has grown substantially. Sev-
eral corpora were created for Czech, the most
prominent ones being the morphologically anno-
tated Czech National Corpus and syntactically an-
notated Prague Dependency Treebank. In 2002
we have started the work on the parallel bilin-
gual Prague Czech English Dependency Treebank
(PCEDT) (Cur???n et al, 2004), which contains
about a half of the texts from PennTreebank 3
translated into Czech by native speakers. A large
morphological dictionary of Czech has been de-
veloped (Hajic?, 2001), allowing for a good quality
morphological analysis of Czech, which has been
tested in numerous commercial applications and
scientific projects since then.
3 The background of the project
The main motivation for our Czech-English MT
experiment was to test several hypotheses. The
most prominent of these hypotheses concerns the
level, at which it is reasonable to perform the
transfer. Due to the differences between both lan-
guages it is not sufficient to perform the transfer
immediately after the morphological analysis or
shallow parsing, as it has been done in the MT
system eslko aiming at the translation between
closely related (and similar) languages [cf (Hajic?
et al, 2003)]. On the other hand, it is a ques-
tion whether the typological differences between
Czech and English justify the transfer being per-
formed at the tectogrammatical (deep syntactic)
level.
Last but not least, one of our aims was to de-
velop a rule-based MT system with minimal pos-
sible costs, either reusing the existing modules or
trying to use (semi)automatic methods whenever
possible, concentrating on areas where using the
human labor would be extremely expensive (for
example building a large coverage bilingual dic-
tionary, cf. the following paragraphs.)
4 Czech-English MT system
The main goal of our project is to develop an ex-
perimental MT system for the translation of texts
from the PCEDT from Czech to English. The sys-
tem investigates the possibility of reusing the ex-
isting resources (grammar, dictionary) in order to
decrease the development time. It also exploits
the parallel bilingual corpus of syntactically anno-
tated texts, although not as a direct learning ma-
terial, more like an additional source of linguis-
tic data especially for the dictionary development
and for the testing of the system.
The task is complicated by the fact that this
translation direction is according to our opinion
more complicated than the reverse one. There are
several reasons for this claim; the most prominent
one is the free word-order nature of the source
language. It generally means that it is very of-
ten necessary to make substantial changes of the
word order if we want to get a grammatical Eng-
lish sentence, while when translating from Eng-
lish to Czech the results are more or less gram-
matically correct and comprehensible even if we
don?t change the word order at all.
Another problem of the Czech-English transla-
tion is the insertion of articles. Czech doesn?t use
any articles and it is of course much easier to re-
move them from the text (when translating from
English) than to insert a proper article on a proper
place (when translating from Czech).
Let us now look at the individual modules of
the new system.
4.1 Morphological analysis
Due to the limited size of the original morpho-
syntactic dictionary of the system it was neces-
sary to replace the original module by a new one.
The new module of morphological analysis of
Czech (Hajic?, 2001) has been already exploited in
numerous applications. It covers almost the entire
Czech language, with very few exceptions (it is
estimated that it contains about 800 000 lemmas).
It is very reliable, due to a really large coverage
there are almost no unknown words in the whole
PCEDT. The only problem was the incorporation
of the new module into the system - the original
module of syntactic analysis of Czech from the
system RUSLAN was very closely bound to a dic-
tionary lookup and to the morphological module.
The new module also uses a different tagset.
180
4.2 Bilingual dictionary
The bilingual dictionary of the system RUSLAN
contained approximately 8000 lexical items with
a rich lexico-syntactic information. We have orig-
inally assumed that the information contained in
the dictionary might be transformed and reused in
the new system, but this assumption turned to be
false. Although the information contained in the
original bilingual dictionary is extremely valuable
for the module of syntactic analysis of Czech, we
have decided to sacrifice it. The mere 8000 lex-
ical items constitute too small part of the new
bilingual dictionary and we have decided to prefer
handling the dictionary in a uniform way.
At the moment there are no Czech-English dic-
tionaries exploitable in an MT system. The avail-
able machine-readable dictionaries built mainly
for a human user (such as WinGED1 or
Svoboda (2001)) suffer from important limita-
tions:
? Sometimes, several variants of translation
are combined in one entry2.
? No clear annotation of meta-language is
present, although the entries contain valu-
able morphological or syntactic information
to some extent. (E.g. valency frames are
encoded by means of rather inconsistent ab-
breviations in plain text: accession to = vs-
toupen?? do or adjudge sb. to be guilty = uz-
nat vinny?m koho.)
? Usually, no morphological information is
given along the entries, although the mor-
phological information can be vital for cor-
rectly recognizing an occurrence of the entry
in a text. For example, an expression kniha
u?c?etn?? can be translated as either an account-
ing book or a book of an accountant depend-
ing whether the Czech word u?c?etn?? is an ad-
jective or a noun.
? No syntactic information is available and no
consistent rules have been adopted by the
1http://www.rewin.cz/
2Throughout the text, we use the term ENTRY as a syn-
onym to translation pair, i.e. a pair of Czech and English
expressions.
lexicographers to annotate syntactic proper-
ties in plain text (such as putting the head of
the clause as the first word).
From the point of view of structural machine
translation, the lack of syntactic information in
the translation dictionary is crucial. In the course
of translation, the input sentence is syntactically
analyzed before searching for foreign language
equivalents. In order to check for presence of
multi-word expressions in the input, the dictio-
nary must encode the structural shape of such en-
tries, otherwise the system does not know how to
traverse the relevant part of the tree. Similarly,
some expressions require some constraints to be
met (such as an agreement in case or number) in
the input text. If these constraints are not fulfilled,
the proposed foreign language equivalent is not
applicable.
The importance of valency (subcategorization)
frames and their equivalents should be stressed,
too. In the described system, already the syntac-
tic analyzer requires verb and adjective valency
frames in order to allow for specific syntactic con-
structions. In general, knowledge of translation
equivalents of valencies is important to preserve
the meaning (pr?ij??t na ne?jaky? na?pad = come at an
idea, literal translation: come on an idea; chodit
na housle = attend violin lessons, lit. walk on vi-
olin) or to handle auxiliary words properly (c?ekat
na ne?hoko = wait for somebody, lit. wait on sb.;
r???ci ne?co = tell something but pr?ejet ne?co = run
over something).
4.2.1 Dictionary cleanup
In order to handle the problems mentioned
above, we performed an extensive cleanup of the
data from available machine-readable dictionar-
ies. The core steps of the cleanup are as follows:
Identifying meta-information.
We manually processed all the entries and
searched for frequent words that typically encode
some meta-information, such as sth., st., oneself.
We also checked all entries ending with a word
that is potentially a preposition. Based on the ex-
pression in the other language, we were able to
recognize the meaning and identify, whether the
suspicious word expresses a ?slot? in the expres-
sion or whether it is a fixed part of the expression.
(E.g. m??t o sobe? vysoke? m??ne?n?? = think something
181
of oneself, only the word oneself encodes a slot,
the word something is a fixed part of the expres-
sion.)
During this phase, entries encoding several
translation variants at once were disassembled
into separate translation pairs, too.
Part-of-speech disambiguation.
We processed the Czech part of each entry with
a morphological analyzer (Hajic?, 2001) and we
performed manual part-of-speech disambiguation
of expressions with ambiguity. It should be noted
that automatic tagging would not provide us with
satisfactory results due to the lack of sentential
context around the expressions.
Adding morphological constraints.
Morphological constraints on word entries de-
scribe which values of morphological features are
valid for each word of the entry or have to be
shared among some words of the entry. Once
identified, morphological constraints can be used
to check whether a word group in the input text
represents an entry or not. With respect to our fi-
nal task (translation from Czech to English), we
aim at Czech constraints only.
We decided to induce morphological con-
straints automatically, based on corpus examples
of the entries. For each entry, we look up sen-
tences that contain all the lemmas of the entry
in a close neighborhood (but irrespective to the
word order and possible presence of inserted extra
words). We weight the instances to promote those
with no intervening words and those with con-
nected dependency graph. The list of weighted
instances is scanned for both unary (such as ?case
is accusative?, ?number is singular?) and binary
(?the case of the first and second words match?)
pre-defined constraints selecting those that are
satisfied by at least 75% of total weight.
Most of the expressions with at least 10 corpus
instances obtain a valid set of constraints. Only
expressions containing very common words (so
that the words do appear quite often close together
without actually forming the expression) obtain
too weak constraints. For instance, no case and
gender agreement constraints are selected for the
expression bohaty? c?love?k (wealthy man).
Adding syntactic information.
Syntactic information (dependency relations
among words in the expression) is needed mainly
during the analysis of input sentences, therefore
we focused on adding the information to the
Czech part of entries first. For most of the en-
tries, it was possible to add the dependency struc-
ture manually, based on the part-of-speech pattern
of the entry. For instance all the entries contain-
ing an adjective followed by a noun get the same
structure: the noun governs the preceeding adjec-
tive. For the remaining entries (with very varied
POS patterns), we employ a corpus-based search
similar to the automatic procedure of identifying
morphological constraints.
4.3 Named entity recognition module
Named entities (NE) are atomic units such as
proper names, temporal expressions (e.g., dates)
and quantities (e.g., monetary expressions). They
occur quite often in various texts and carry impor-
tant information. Hence, proper analysis of NEs
and their translation has an enormous impact on
MT quality (Babych and Hartley, 2004). In our
system they are extremely important due to the
nature of input texts. The Wall Street Journal sec-
tion of PennTreebank shows much higher density
of named entities than ordinary texts. Their cor-
rect recognition therefore has a tremendous im-
pact on the performance of the whole system, es-
pecially if the evaluation of the translation quality
is based on golden standard translations.
NE translation involves both semantic transla-
tion and phonetic transliteration. Each type of NE
is handled in a different way. For instance, person
names do not undergo semantic translation (only
transliteration is required), while certain titles and
part of names do (e.g., prvn?? da?ma Laura Bushova?
? first lady Laura Bush). In case of organiza-
tions, application of regular transfer rules for NPs
seems to be sufficient (e.g., ?Ustav forma?ln?? a ap-
likovane? lingvistiky ? Institute of formal and ap-
plied linguistics), although an idiomatic transla-
tion may be probably preferable sometimes. With
respect to geographical places we apply bilingual
glossaries and a set of regular transfer rules as
well.
For NE-recognition, we have developed a
grammar based on regular expressions that
processes typed feature structures. The gram-
mar framework, similarly as the formally a bit
weaker platform SProUT (Bering et al, 2003),
182
uses finite-state techniques and unification, i.e., a
grammar consists of pattern/action rules, where
the left-hand side is a regular expression over
typed feature structures (TFS) with variables, rep-
resenting the recognition pattern, and the right-
hand side is a TFS specification of the output
structure.
The NE grammar is based on the experiment
described in (Piskorski et al, 2004). An example
of a simple rule is:
#subst[LEMMA: ministerstvo]$s1
+ #top[CASE: gen, PHRASE: $phr]$s2
== $s1#ministry[ATTR: $s2,
PHRASE: &(?ministerstvo ? + phr)]
(1)
The first TFS matches any morphological vari-
ant of the word ministerstvo (ministry), followed
by a genitive NP. The variables $s1, $s2 and $phr
create dynamic value assignments and allow to
transport these values to the slots in the output
structure of type ministry. The output structure
contains a new attribute called PHRASE with the
lemmatized value of the whole phrase.
If the input phrase is
informace ministerstva zahranic???
o cestova?n?? do ohroz?eny?ch oblast?? (2)
then the phrase ?ministerstva zahranic???? will be
recognized as a NE and handled as an atomic unit
in the whole MT process:
?
?
?
?
?
?
?
?
?
?
?
?
?
?
ministry
LEMMA ministerstvo
FORM ministerstva
PHRASE ministerstvo zahranic???
ATTR
?
?
?
?
?
subst
LEMMA zahranic???
PHRASE zahranic???
FORM zahranic???
CASE gen
NUMBER sg
GENDER n
?
?
?
?
?
CASE gen
NUMBER sg
GENDER n
?
?
?
?
?
?
?
?
?
?
?
?
?
?
(3)
Lemmatization of NEs is crucial in the context
of MT. However, it might pose a serious problem
in case of languages with rich inflection due to
structural ambiguities, e.g., internal bracketing of
complex noun phrases might be difficult to ana-
lyze. The core of the framework is based on gram-
mars that have been developed for the MT system
?Ces??lko (Hajic? et al, 2003).
4.4 Syntactic analysis of Czech
Although we have originally assumed that the
module of syntactic analysis of Czech will re-
quire only small modifications and its reuse in the
new system was one of the goals of our system,
it turned out that this module is one of the main
sources of problems.
In the course of testing and debugging of the
system we had to create a number of new gram-
mar rules covering the phenomena which were
not properly accounted for in the original system
due to the different nature of the original domain.
The texts from PCEDT show for example much
higher number of numerals and numeric expres-
sions, some of which require either special gram-
matical or transfer rules than operating systems
manuals from the system RUSLAN. The com-
plexity of input sentences with regard to the num-
ber of clauses and their mutual relationship is also
much higher. This, of course, decreases the num-
ber of sentences which are completely syntacti-
cally analyzed and thus degrades the translation
quality.
One of the biggest problems of the grammar
are the properties of Q-systems. It was quite
clear since the start of the project that it is im-
possible to extract only the knowledge encoded
into the grammar, the grammar rules written in
Q-systems are so complicated that rewriting them
into a different (even chart-parser based) formal-
ism would actually mean to write a completely
new grammar. Although we have at our disposal
a new, modernized and reimplemented version of
a Q-systems compiler and interpreter which over-
comes the technical problems of the original ver-
sion, the nature of the formalism is of course pre-
served.
4.5 Transfer
The main task of this module is to transform the
syntactic structure (syntactic tree) of the input
Czech sentence into the syntactic structure (tree)
of the corresponding English sentence. The trans-
fer module does not handle the translation of reg-
ularly translated lexical units, it is handled by the
bilingual dictionary in the earlier phases of the
system. The transfer concentrates on three main
tasks:
183
? The transformation of the Czech syntactic
tree into the English one reflecting the dif-
ferences in the word order between both lan-
guages.
? The identification and translation of those
constructions in Czech, which require spe-
cific (irregular) translation into English.
? The insertion of articles (which do not exist
in Czech) into the target language sentences.
The development of this module still continues,
the initial tests confirmed that a substantial im-
provement can be achieved in the future.
4.6 Syntactic synthesis of English
The syntactic synthesis of Russian in RUSLAN is
very closely bound to transfer, therefore we have
tried to use as big portion of the grammar as possi-
ble, but of course, substantial modifications of the
grammar were necessary. As well as the work on
the transfer module, also the work on this module
still continues.
4.7 Morphological synthesis of English
Due to the simplicity of English morphology this
module has a very limited role in our system. It
handles plurals, 3rd persons and irregular words.
5 Conclusion
The problems mentioned in this paper do not al-
low to formulate an answer to the crucial ques-
tion - does it really pay off to recycle the old sys-
tem or not? The integration of existing parts into
a new system is so complicated that we are still
not able to perform evaluation of results on texts
of a reasonable size. One way out of this situa-
tion would be the combination of the new mod-
ules mentioned in this paper with one of the ex-
isting stochastic parsers of Czech instead of the
rule-based grammar.
Another possible direction for the future re-
search might be the exploitation of two new mod-
ules. The first one will contain partial, but error-
free disambiguation of the results of morpholog-
ical analysis of Czech, which will substantially
decrease the morphological ambiguity of individ-
ual Czech word forms. This ambiguity (the aver-
age number of morphological tags per word form
exceeds four in Czech) also negatively influences
the performance of the syntactic analysis.
The second way how to decrease the ambigu-
ity is the exploitation of a special module resolv-
ing the lexical ambiguity in those cases when the
bilingual dictionary provides more than one lexi-
cal equivalent. This stochastic module would ex-
ploit the context and would suggest the best trans-
lation.
References
B. Babych and A. Hartley. 2004. Selecting transla-
tion strategies in MT using automatic named en-
tity recognition. In Proceedings of the Ninth EAMT
Workshop, Valetta, Malta.
C. Bering, W. Droz?dz?yn?ski, G. Erbach, C. Guasch,
P. Homola, S. Lehmann, H. Li, H.-U. Krieger,
J. Piskorski, U. Schaefer, A. Shimada, M. Siegel,
F. Xu, and D. Ziegler-Eisele. 2003. Corpora
and evaluation tools for multilingual named entity
grammar development.
Alain Colmerauer. 1969. Les Systemes Q ou un for-
malisme pour analyser et synthetiser des phrases sur
ordinateur.
Jan Cur???n, Martin ?Cmejrek, Jir??? Havelka, and
Vladislav Kubon?. 2004. Building a Parallel Bilin-
gual Syntactically Annotated Corpus. In Proceed-
ings of the 1st International Joint Conference on
NLP.
Jan Hajic?. 2001. Disambiguation of Rich Inflection
- Computational Morphology of Czech, volume I.
Prague Karolinum, Charles University Press. 334
pp.
J. Hajic?, P. Homola, and V. Kubon?. 2003. A sim-
ple multilingual machine translation system. In In:
Proceedings of the MT Summit IX, New Orleans.
Karel Oliva. 1989. A Parser for Czech Implemented
in Systems Q. Explizite Beschreibung der Sprache
und automatische Textbearbeitung.
J. Piskorski, P. Homola, M. Marciniak,
A. Mykowiecka, A. Przepio?rkowski, and
M. Wolin?ski. 2004. Information extraction
for Polish using the SProUT platform. In Pro-
ceedings of the International IIS:IIP WM?04
Conference, Zakopane, Poland.
Milan Svoboda. 2001. GNU/FDL English-Czech
Dictionary. http://slovnik.zcu.cz/.
184
Proceedings of the ACL 2010 Conference Short Papers, pages 86?91,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Tackling Sparse Data Issue in Machine Translation Evaluation ?
Ondr?ej Bojar, Kamil Kos, and David Marec?ek
Charles University in Prague, Institute of Formal and Applied Linguistics
{bojar,marecek}@ufal.mff.cuni.cz, kamilkos@email.cz
Abstract
We illustrate and explain problems of
n-grams-based machine translation (MT)
metrics (e.g. BLEU) when applied to
morphologically rich languages such as
Czech. A novel metric SemPOS based
on the deep-syntactic representation of the
sentence tackles the issue and retains the
performance for translation to English as
well.
1 Introduction
Automatic metrics of machine translation (MT)
quality are vital for research progress at a fast
pace. Many automatic metrics of MT quality have
been proposed and evaluated in terms of correla-
tion with human judgments while various tech-
niques of manual judging are being examined as
well, see e.g. MetricsMATR08 (Przybocki et al,
2008)1, WMT08 and WMT09 (Callison-Burch et
al., 2008; Callison-Burch et al, 2009)2.
The contribution of this paper is twofold. Sec-
tion 2 illustrates and explains severe problems of a
widely used BLEU metric (Papineni et al, 2002)
when applied to Czech as a representative of lan-
guages with rich morphology. We see this as an
instance of the sparse data problem well known
for MT itself: too much detail in the formal repre-
sentation leading to low coverage of e.g. a transla-
tion dictionary. In MT evaluation, too much detail
leads to the lack of comparable parts of the hy-
pothesis and the reference.
? This work has been supported by the grants EuroMa-
trixPlus (FP7-ICT-2007-3-231720 of the EU and 7E09003
of the Czech Republic), FP7-ICT-2009-4-247762 (Faust),
GA201/09/H057, GAUK 1163/2010, and MSM 0021620838.
We are grateful to the anonymous reviewers for further re-
search suggestions.
1http://nist.gov/speech/tests
/metricsmatr/2008/results/
2http://www.statmt.org/wmt08 and wmt09
0.06 0.08 0.10 0.12 0.14
0.4
0.6 b
cu-bojar
b
google
b
uedin
b
eurotranxp
b
pctrans
b
cu-tectomt
BLEU
Rank
Figure 1: BLEU and human ranks of systems par-
ticipating in the English-to-Czech WMT09 shared
task.
Section 3 introduces and evaluates some new
variations of SemPOS (Kos and Bojar, 2009), a
metric based on the deep syntactic representation
of the sentence performing very well for Czech as
the target language. Aside from including depen-
dency and n-gram relations in the scoring, we also
apply and evaluate SemPOS for English.
2 Problems of BLEU
BLEU (Papineni et al, 2002) is an established
language-independent MT metric. Its correlation
to human judgments was originally deemed high
(for English) but better correlating metrics (esp.
for other languages) were found later, usually em-
ploying language-specific tools, see e.g. Przy-
bocki et al (2008) or Callison-Burch et al (2009).
The unbeaten advantage of BLEU is its simplicity.
Figure 1 illustrates a very low correlation to hu-
man judgments when translating to Czech. We
plot the official BLEU score against the rank es-
tablished as the percentage of sentences where a
system ranked no worse than all its competitors
(Callison-Burch et al, 2009). The systems devel-
oped at Charles University (cu-) are described in
Bojar et al (2009), uedin is a vanilla configuration
of Moses (Koehn et al, 2007) and the remaining
ones are commercial MT systems.
In a manual analysis, we identified the reasons
for the low correlation: BLEU is overly sensitive
to sequences and forms in the hypothesis matching
86
Con- Error
firmed Flags 1-grams 2-grams 3-grams 4-grams
Yes Yes 6.34% 1.58% 0.55% 0.29%
Yes No 36.93% 13.68% 5.87% 2.69%
No Yes 22.33% 41.83% 54.64% 63.88%
No No 34.40% 42.91% 38.94% 33.14%
Total n-grams 35,531 33,891 32,251 30,611
Table 1: n-grams confirmed by the reference and
containing error flags.
the reference translation. This focus goes directly
against the properties of Czech: relatively free
word order allows many permutations of words
and rich morphology renders many valid word
forms not confirmed by the reference.3 These
problems are to some extent mitigated if several
reference translations are available, but this is of-
ten not the case.
Figure 2 illustrates the problem of ?sparse data?
in the reference. Due to the lexical and morpho-
logical variance of Czech, only a single word in
each hypothesis matches a word in the reference.
In the case of pctrans, the match is even a false
positive, ?do? (to) is a preposition that should be
used for the ?minus? phrase and not for the ?end
of the day? phrase. In terms of BLEU, both hy-
potheses are equally poor but 90% of their tokens
were not evaluated.
Table 1 estimates the overall magnitude of this
issue: For 1-grams to 4-grams in 1640 instances
(different MT outputs and different annotators) of
200 sentences with manually flagged errors4, we
count how often the n-gram is confirmed by the
reference and how often it contains an error flag.
The suspicious cases are n-grams confirmed by
the reference but still containing a flag (false posi-
tives) and n-grams not confirmed despite contain-
ing no error flag (false negatives).
Fortunately, there are relatively few false posi-
tives in n-gram based metrics: 6.3% of unigrams
and far fewer higher n-grams.
The issue of false negatives is more serious and
confirms the problem of sparse data if only one
reference is available. 30 to 40% of n-grams do
not contain any error and yet they are not con-
3Condon et al (2009) identify similar issues when eval-
uating translation to Arabic and employ rule-based normal-
ization of MT output to improve the correlation. It is beyond
the scope of this paper to describe the rather different nature
of morphological richness in Czech, Arabic and also other
languages, e.g. German or Finnish.
4The dataset with manually flagged errors is available at
http://ufal.mff.cuni.cz/euromatrixplus/
firmed by the reference. This amounts to 34% of
running unigrams, giving enough space to differ in
human judgments and still remain unscored.
Figure 3 documents the issue across languages:
the lower the BLEU score itself (i.e. fewer con-
firmed n-grams), the lower the correlation to hu-
man judgments regardless of the target language
(WMT09 shared task, 2025 sentences per lan-
guage).
Figure 4 illustrates the overestimation of scores
caused by too much attention to sequences of to-
kens. A phrase-based system like Moses (cu-
bojar) can sometimes produce a long sequence of
tokens exactly as required by the reference, lead-
ing to a high BLEU score. The framed words
in the illustration are not confirmed by the refer-
ence, but the actual error in these words is very
severe for comprehension: nouns were used twice
instead of finite verbs, and a misleading transla-
tion of a preposition was chosen. The output by
pctrans preserves the meaning much better despite
not scoring in either of the finite verbs and produc-
ing far shorter confirmed sequences.
3 Extensions of SemPOS
SemPOS (Kos and Bojar, 2009) is inspired by met-
rics based on overlapping of linguistic features in
the reference and in the translation (Gime?nez and
Ma?rquez, 2007). It operates on so-called ?tec-
togrammatical? (deep syntactic) representation of
the sentence (Sgall et al, 1986; Hajic? et al, 2006),
formally a dependency tree that includes only au-
tosemantic (content-bearing) words.5 SemPOS as
defined in Kos and Bojar (2009) disregards the
syntactic structure and uses the semantic part of
speech of the words (noun, verb, etc.). There are
19 fine-grained parts of speech. For each semantic
part of speech t, the overlapping O(t) is set to zero
if the part of speech does not occur in the reference
or the candidate set and otherwise it is computed
as given in Equation 1 below.
5We use TectoMT (Z?abokrtsky? and Bojar, 2008),
http://ufal.mff.cuni.cz/tectomt/, for the lin-
guistic pre-processing. While both our implementation of
SemPOS as well as TectoMT are in principle freely avail-
able, a stable public version has yet to be released. Our plans
include experiments with approximating the deep syntactic
analysis with a simple tagger, which would also decrease the
installation burden and computation costs, at the expense of
accuracy.
87
SRC Prague Stock Market falls to minus by the end of the trading day
REF praz?ska? burza se ke konci obchodova?n?? propadla do minusu
cu-bojar praha stock market klesne k minus na konci obchodn??ho dne
pctrans praha trh cenny?ch pap??ru? pada? minus do konce obchodn??ho dne
Figure 2: Sparse data in BLEU evaluation: Large chunks of hypotheses are not compared at all. Only a
single unigram in each hypothesis is confirmed in the reference.
-
0.2 0
 
0.2
 
0.4
 
0.6
 
0.8 1  0
.05
 
0.1
 
0.1
5
 
0.2
 
0.2
5
 
0.3
Correlation
BL
EU
 sc
ore
cs
-en
de-
en
es
-en
fr-e
n
hu-
en
en
-cs
en
-de
en
-es
en
-fr
Figure 3: BLEU correlates with its correlation to human judgments. BLEU scores around 0.1 predict
little about translation quality.
O(t) =
?
i?I
?
w?ri?ci
min(cnt(w, t, ri), cnt(w, t, ci))
?
i?I
?
w?ri?ci
max(cnt(w, t, ri), cnt(w, t, ci))
(1)
The semantic part of speech is denoted t; ci
and ri are the candidate and reference translations
of sentence i, and cnt(w, t, rc) is the number of
wordsw with type t in rc (the reference or the can-
didate). The matching is performed on the level of
lemmas, i.e. no morphological information is pre-
served in ws. See Figure 5 for an example; the
sentence is the same as in Figure 4.
The final SemPOS score is obtained by macro-
averaging over all parts of speech:
SemPOS =
1
|T |
?
t?T
O(t) (2)
where T is the set of all possible semantic parts
of speech types. (The degenerate case of blank
candidate and reference has SemPOS zero.)
3.1 Variations of SemPOS
This section describes our modifications of Sem-
POS. All methods are evaluated in Section 3.2.
Different Classification of Autosemantic
Words. SemPOS uses semantic parts of speech
to classify autosemantic words. The tectogram-
matical layer offers also a feature called Functor
describing the relation of a word to its governor
similarly as semantic roles do. There are 67
functor types in total.
Using Functor instead of SemPOS increases the
number of word classes that independently require
a high overlap. For a contrast we also completely
remove the classification and use only one global
class (Void).
Deep Syntactic Relations in SemPOS. In
SemPOS, an autosemantic word of a class is con-
firmed if its lemma matches the reference. We uti-
lize the dependency relations at the tectogrammat-
ical layer to validate valence by refining the over-
lap and requiring also the lemma of 1) the parent
(denoted ?par?), or 2) all the children regardless of
their order (denoted ?sons?) to match.
Combining BLEU and SemPOS. One of the
major drawbacks of SemPOS is that it completely
ignores word order. This is too coarse even for
languages with relatively free word order like
Czech. Another issue is that it operates on lemmas
and it completely disregards correct word forms.
Thus, a weighted linear combination of SemPOS
and BLEU (computed on the surface representa-
tion of the sentence) should compensate for this.
For the purposes of the combination, we compute
BLEU only on unigrams up to fourgrams (denoted
BLEU1, . . . , BLEU4) but including the brevity
penalty as usual. Here we try only a few weight
settings in the linear combination but given a held-
out dataset, one could optimize the weights for the
best performance.
88
SRC Congress yields: US government can pump 700 billion dollars into banks
REF kongres ustoupil : vla?da usa mu?z?e do bank napumpovat 700 miliard dolaru?
cu-bojar kongres vy?nosy : vla?da usa mu?z?e c?erpadlo 700 miliard dolaru? v banka?ch
pctrans kongres vyna?s??? : us vla?da mu?z?e c?erpat 700 miliardu dolaru? do bank
Figure 4: Too much focus on sequences in BLEU: pctrans? output is better but does not score well.
BLEU gave credit to cu-bojar for 1, 3, 5 and 8 fourgrams, trigrams, bigrams and unigrams, resp., but
only for 0, 0, 1 and 8 n-grams produced by pctrans. Confirmed sequences of tokens are underlined and
important errors (not considered by BLEU) are framed.
REF kongres/n ustoupit/v :/n vla?da/n usa/n banka/n napumpovat/v 700/n miliarda/n dolar/n
cu-bojar kongres/n vy?nos/n :/n vla?da/n usa/n moci/v c?erpadlo/n 700/n miliarda/n dolar/n banka/n
pctrans kongres/n vyna?s?et/v :/n us/n vla?da/n c?erpat/v 700/n miliarda/n dolar/n banka/n
Figure 5: SemPOS evaluates the overlap of lemmas of autosemantic words given their semantic part of
speech (n, v, . . . ). Underlined words are confirmed by the reference.
SemPOS for English. The tectogrammatical
layer is being adapted for English (Cinkova? et al,
2004; Hajic? et al, 2009) and we are able to use the
available tools to obtain all SemPOS features for
English sentences as well.
3.2 Evaluation of SemPOS and Friends
We measured the metric performance on data used
in MetricsMATR08, WMT09 and WMT08. For
the evaluation of metric correlation with human
judgments at the system level, we used the Pearson
correlation coefficient ? applied to ranks. In case
of a tie, the systems were assigned the average po-
sition. For example if three systems achieved the
same highest score (thus occupying the positions
1, 2 and 3 when sorted by score), each of them
would obtain the average rank of 2 = 1+2+33 .
When correlating ranks (instead of exact scores)
and with this handling of ties, the Pearson coeffi-
cient is equivalent to Spearman?s rank correlation
coefficient.
The MetricsMATR08 human judgments include
preferences for pairs of MT systems saying which
one of the two systems is better, while the WMT08
and WMT09 data contain system scores (for up to
5 systems) on the scale 1 to 5 for a given sentence.
We assigned a human ranking to the systems based
on the percent of time that their translations were
judged to be better than or equal to the translations
of any other system in the manual evaluation. We
converted automatic metric scores to ranks.
Metrics? performance for translation to English
and Czech was measured on the following test-
sets (the number of human judgments for a given
source language in brackets):
To English: MetricsMATR08 (cn+ar: 1652),
WMT08 News Articles (de: 199, fr: 251),
WMT08 Europarl (es: 190, fr: 183), WMT09
(cz: 320, de: 749, es: 484, fr: 786, hu: 287)
To Czech: WMT08 News Articles (en: 267),
WMT08 Commentary (en: 243), WMT09
(en: 1425)
The MetricsMATR08 testset contained 4 refer-
ence translations for each sentence whereas the re-
maining testsets only one reference.
Correlation coefficients for English are shown
in Table 2. The best metric is Voidpar closely fol-
lowed by Voidsons. The explanation is that Void
compared to SemPOS or Functor does not lose
points by an erroneous assignment of the POS or
the functor, and that Voidpar profits from check-
ing the dependency relations between autoseman-
tic words. The combination of BLEU and Sem-
POS6 outperforms both individual metrics, but in
case of SemPOS only by a minimal difference.
Additionally, we confirm that 4-grams alone have
little discriminative power both when used as a
metric of their own (BLEU4) as well as in a lin-
ear combination with SemPOS.
The best metric for Czech (see Table 3) is a lin-
ear combination of SemPOS and 4-gram BLEU
closely followed by other SemPOS and BLEUn
combinations. We assume this is because BLEU4
can capture correctly translated fixed phrases,
which is positively reflected in human judgments.
Including BLEU1 in the combination favors trans-
lations with word forms as expected by the refer-
6For each n ? {1, 2, 3, 4}, we show only the best weight
setting for SemPOS and BLEUn.
89
Metric Avg Best Worst
Voidpar 0.75 0.89 0.60
Voidsons 0.75 0.90 0.54
Void 0.72 0.91 0.59
Functorsons 0.72 1.00 0.43
GTM 0.71 0.90 0.54
4?SemPOS+1?BLEU2 0.70 0.93 0.43
SemPOSpar 0.70 0.93 0.30
1?SemPOS+4?BLEU3 0.70 0.91 0.26
4?SemPOS+1?BLEU1 0.69 0.93 0.43
NIST 0.69 0.90 0.53
SemPOSsons 0.69 0.94 0.40
SemPOS 0.69 0.95 0.30
2?SemPOS+1?BLEU4 0.68 0.91 0.09
BLEU1 0.68 0.87 0.43
BLEU2 0.68 0.90 0.26
BLEU3 0.66 0.90 0.14
BLEU 0.66 0.91 0.20
TER 0.63 0.87 0.29
PER 0.63 0.88 0.32
BLEU4 0.61 0.90 -0.31
Functorpar 0.57 0.83 -0.03
Functor 0.55 0.82 -0.09
Table 2: Average, best and worst system-level cor-
relation coefficients for translation to English from
various source languages evaluated on 10 different
testsets.
ence, thus allowing to spot bad word forms. In
all cases, the linear combination puts more weight
on SemPOS. Given the negligible difference be-
tween SemPOS alone and the linear combinations,
we see that word forms are not the major issue for
humans interpreting the translation?most likely
because the systems so far often make more im-
portant errors. This is also confirmed by the obser-
vation that using BLEU alone is rather unreliable
for Czech and BLEU-1 (which judges unigrams
only) is even worse. Surprisingly BLEU-2 per-
formed better than any other n-grams for reasons
that have yet to be examined. The error metrics
PER and TER showed the lowest correlation with
human judgments for translation to Czech.
4 Conclusion
This paper documented problems of single-
reference BLEU when applied to morphologically
rich languages such as Czech. BLEU suffers from
a sparse data problem, unable to judge the quality
of tokens not confirmed by the reference. This is
confirmed for other languages as well: the lower
the BLEU score the lower the correlation to hu-
man judgments.
We introduced a refinement of SemPOS, an
automatic metric of MT quality based on deep-
syntactic representation of the sentence tackling
Metric Avg Best Worst
3?SemPOS+1?BLEU4 0.55 0.83 0.14
2?SemPOS+1?BLEU2 0.55 0.83 0.14
2?SemPOS+1?BLEU1 0.53 0.83 0.09
4?SemPOS+1?BLEU3 0.53 0.83 0.09
SemPOS 0.53 0.83 0.09
BLEU2 0.43 0.83 0.09
SemPOSpar 0.37 0.53 0.14
Functorsons 0.36 0.53 0.14
GTM 0.35 0.53 0.14
BLEU4 0.33 0.53 0.09
Void 0.33 0.53 0.09
NIST 0.33 0.53 0.09
Voidsons 0.33 0.53 0.09
BLEU 0.33 0.53 0.09
BLEU3 0.33 0.53 0.09
BLEU1 0.29 0.53 -0.03
SemPOSsons 0.28 0.42 0.03
Functorpar 0.23 0.40 0.14
Functor 0.21 0.40 0.09
Voidpar 0.16 0.53 -0.08
PER 0.12 0.53 -0.09
TER 0.07 0.53 -0.23
Table 3: System-level correlation coefficients for
English-to-Czech translation evaluated on 3 differ-
ent testsets.
the sparse data issue. SemPOS was evaluated on
translation to Czech and to English, scoring better
than or comparable to many established metrics.
References
Ondr?ej Bojar, David Marec?ek, Va?clav Nova?k, Mar-
tin Popel, Jan Pta?c?ek, Jan Rous?, and Zdene?k
Z?abokrtsky?. 2009. English-Czech MT in 2008. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, Athens, Greece, March. Asso-
ciation for Computational Linguistics.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statisti-
cal Machine Translation, pages 70?106, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
workshop on statistical machine translation. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, Athens, Greece. Association for
Computational Linguistics.
Silvie Cinkova?, Jan Hajic?, Marie Mikulova?, Lu-
cie Mladova?, Anja Nedoluz?ko, Petr Pajas, Jarmila
Panevova?, Jir??? Semecky?, Jana S?indlerova?, Josef
Toman, Zden?ka Ures?ova?, and Zdene?k Z?abokrtsky?.
2004. Annotation of English on the tectogram-
matical level. Technical Report TR-2006-35,
U?FAL/CKL, Prague, Czech Republic, December.
90
Sherri Condon, Gregory A. Sanders, Dan Parvaz, Alan
Rubenstein, Christy Doran, John Aberdeen, and
Beatrice Oshika. 2009. Normalization for Auto-
mated Metrics: English and Arabic Speech Transla-
tion. In MT Summit XII.
Jesu?s Gime?nez and Llu??s Ma?rquez. 2007. Linguis-
tic Features for Automatic Evaluation of Heteroge-
nous MT Systems. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
256?264, Prague, June. Association for Computa-
tional Linguistics.
Jan Hajic?, Silvie Cinkova?, Kristy?na C?erma?kova?, Lu-
cie Mladova?, Anja Nedoluz?ko, Petr Pajas, Jir??? Se-
mecky?, Jana S?indlerova?, Josef Toman, Kristy?na
Toms?u?, Mate?j Korvas, Magdale?na Rysova?, Kater?ina
Veselovska?, and Zdene?k Z?abokrtsky?. 2009. Prague
English Dependency Treebank 1.0. Institute of For-
mal and Applied Linguistics, Charles University in
Prague, ISBN 978-80-904175-0-2, January.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka,
Marie Mikulova?, Zdene?k Z?abokrtsky?, and Magda
S?evc???kova? Raz??mova?. 2006. Prague Dependency
Treebank 2.0. LDC2006T01, ISBN: 1-58563-370-
4.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In ACL 2007, Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics Companion Volume Proceedings of the Demo
and Poster Sessions, pages 177?180, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Kamil Kos and Ondr?ej Bojar. 2009. Evaluation of Ma-
chine Translation Metrics for Czech as the Target
Language. Prague Bulletin of Mathematical Lin-
guistics, 92.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In ACL 2002,
Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, pages 311?
318, Philadelphia, Pennsylvania.
M. Przybocki, K. Peterson, and S. Bronsart. 2008. Of-
ficial results of the NIST 2008 ?Metrics for MA-
chine TRanslation? Challenge (MetricsMATR08).
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The Meaning of the Sentence and Its Semantic
and Pragmatic Aspects. Academia/Reidel Publish-
ing Company, Prague, Czech Republic/Dordrecht,
Netherlands.
Zdene?k Z?abokrtsky? and Ondr?ej Bojar. 2008. TectoMT,
Developer?s Guide. Technical Report TR-2008-39,
Institute of Formal and Applied Linguistics, Faculty
of Mathematics and Physics, Charles University in
Prague, December.
91
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 60?66,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
2010 Failures in English-Czech Phrase-Based MT ?
Ondr?ej Bojar and Kamil Kos
Charles University in Prague, Institute of Formal and Applied Linguistics ( ?UFAL)
Malostranske? na?me?st?? 25, Praha 1, CZ-11800, Czech Republic
bojar@ufal.mff.cuni.cz, kamilkos@email.cz
Abstract
The paper describes our experiments with
English-Czech machine translation for
WMT101 in 2010. Focusing primarily
on the translation to Czech, our additions
to the standard Moses phrase-based MT
pipeline include two-step translation to
overcome target-side data sparseness and
optimization towards SemPOS, a metric
better suited for evaluating Czech. Unfor-
tunately, none of the approaches bring a
significant improvement over our standard
setup.
1 Introduction
Czech is a flective language with very rich mor-
phological system. Translation between Czech
and English poses different challenges for each of
the directions.
When translating from Czech, the word order
usually needs only minor changes (despite the is-
sue of non-projectivity, a phenomenon occurring
at 2% of words but in 23% of Czech sentences,
see Hajic?ova? et al (2004) and Holan (2003)). A
much more severe issue is caused by the Czech vo-
cabulary size. Fortunately, this can be to a certain
extent mitigated by backing-off to Czech lemmas
if the exact forms are not available.
We are primarily interested in the harder task of
translating to Czech and most of the paper deals
with this direction. After a brief specification of
data sets, pre-processing and evaluation method
in this section, we provide details on the issue
of Czech vocabulary size (Section 2). We de-
scribe our current attempts at generating Czech
?The work on this project was supported by the grants
EuroMatrixPlus (FP7-ICT-2007-3-231720 of the EU and
7E09003 of the Czech Republic), GA ?CR P406/10/P259, and
MSM 0021620838. Thanks to David Kolovratn??k for the help
with manual evaluation.
1http://www.statmt.org/wmt10/
word forms in Section 3. Partly due to the large
vocabulary size of Czech, BLEU score (Papineni
et al, 2002) correlates rather poorly with human
judgments. We summarize our efforts to use a bet-
ter metric in the model optimization in Section 4.
The final Section 5 lists the exact configurations
of our English?Czech primary submissions for
WMT10, including the back-off to lemmas we use
for Czech-to-English.
1.1 Data and Pre-Processing Pipeline
Throughout the paper, we use CzEng 0.9 (Bojar
and ?Zabokrtsky?, 2009)2 as our main parallel cor-
pus. Following CzEng authors? request, we did
not use sections 8* and 9* reserved for evaluation
purposes.
As the baseline training dataset (?Small? in the
following) only the news domain of CzEng (126k
parallel sentences) is used. For large-scale ex-
periments (?Large? in the following) and our pri-
mary WMT10 submissions, we use all CzEng do-
mains except navajo and add the EMEA corpus
(Tiedemann, 2009)3 ,4 of 7.5M parallel sententes.
As our monolingual data we use by default only
the target side of the parallel corpus. For experi-
ments reported here, we also use the monolingual
data provided by WMT10 organizers for Czech.
Our primary WMT10 submission includes further
monolingual data, see Section 5.1.
We use a slightly modified tokenization rules
compared to CzEng export format. Most notably,
we normalize English abbreviated negation and
auxiliary verbs (?couldn?t? ? ?could not?) and
attempt at normalizing quotation marks to distin-
guish between the opening and closing one follow-
2http://ufal.mff.cuni.cz/czeng
3http://urd.let.rug.nl/tiedeman/OPUS
4Unfortunately, the EMEA corpus is badly tokenized on
the Czech side. Most frequently, fractional numbers are split
into several tokens (e.g. ?3, 14?). We attempted to reconstruct
the original detokenized form using a small set of regular ex-
pressions.
60
Large Small Dev
Sents 7.5M 126.1k 2.5k
Czech Tokens 79.2M 2.6M 55.8k
English Tokens 89.1M 2.9M 49.9k
Czech Vocabulary 923.1k 138.7k 15.4k
English Vocabulary 646.3k 64.7k 9.4k
Czech Lemmas 553.5k 60.3k 9.5k
English Lemmas 611.4k 53.8k 7.7k
Table 1: Corpus and vocabulary sizes.
ing proper typesetting rules.
The rest of our pre-processing pipeline matches
the processing employed in CzEng (Bojar and
?Zabokrtsky?, 2009).5 We use ?supervised truecas-
ing?, meaning that we cast the case of the lemma
to the form, relying on our morphological analyz-
ers and taggers to identify proper names, all other
words are lowercased.
The differences in relations between Czech and
English Large and Small datasets can be attributed
either to domain differences or possibly due to
noise in CzEng.
1.2 Evaluation
We use WMT10 development sets for tuning
(news-test2008) and evaluation (news-test2009).
The official scores on news-test2010 are given
only in the main WMT10 paper and not here.
The BLEU scores reported in this paper are
based on truecased word forms in the original to-
kenization as provided by the decoder. Therefore
they are likely to differ from figures reported else-
where.
The ? value given with each BLEU score is the
average of the distances to the lower and upper
empirical 95% confidence bounds estimated using
bootstrapping (Koehn, 2004).
2 Issues of Czech Vocabulary Size
Table 1 summarizes the differences of Czech and
English vocabulary sizes in our parallel corpora.
We see that the vocabulary size of Czech forms
(truecased) is more than double compared to En-
glish in the Small dataset and significantly larger
in the Large dataset as well. On the other hand,
the number of distinct Czech and English lemmas
is nearly identical.
5Due to the subsequent processing, incl. parsing, the tok-
enization of English follows PennTreebenk style. The rather
unfortunate convention of treating hyphenated words as sin-
gle tokens increases our out-of-vocabulary rate. Next time,
we will surely post-tokenize the parsed text.
Distortion Limit
TOpts 3 6 10 30 40
1 0.2 0.3 0.3 0.3 0.3
5 0.8 0.9 1.0 1.0 1.0
10 1.1 1.3 1.5 1.5 1.5
20 1.2 1.5 1.7 1.7 1.7
50 1.2 1.5 1.7 1.7 1.7
100 1.2 1.5 1.7 1.7 1.7
Table 3: Percentage of sentences reachable in
Czech-to-English small setting with various dis-
tortion limits and translation options per coverage
(TOpts) (BLEU score 14.76?0.44).
2.1 Out-of-Vocabulary Rates
Table 2 lists out-of-vocabulary (OOV) rates of our
Small and Large data setting given the develop-
ment corpus. We calculate the rates for both the
complete corpus and the restricted set of phrases
extracted from the corpus. (Note that higher-order
n-gram rates are estimated using phrases as inde-
pendent units, no combination of phrases is per-
formed.) We also list the effective OOV rate for
English-to-Czech translation where all (English)
words from each source sentence can be also pro-
duced in the hypothesis.
We see that in the small setting, the OOV rate
is almost double for Czech than for English. The
OOV is significantly decreased by enlarging the
corpus or lemmatizing the word forms.
If we consider only the words available in the
phrase tables, the issue of Czech with limited data
is striking: 10?12% of devset tokens are not avail-
able in the training data.
2.2 Reachability of Training and Reference
Translations
Schwartz (2008) extended Moses to support ?con-
straint decoding?, that is to perform an exhaustive
search through the space of hypotheses in order to
reach the reference translation (and get its score).
The current implementation of the exhaustive
search in Moses is in fact subject to several con-
figuration parameters, most importantly the num-
ber of translation options considered for each span
(-max-trans-opt-per-coverage) and the
distortion limit (-distortion-limit).
Given his aim, Schwartz (2008) uses the output
of four MT systems translating from different lan-
guages to English as the references and notes that
only around 10% of the reference translations are
reachable by an independent Swedish-English MT
system.
61
n-grams Out of Corpus Voc. n-grams Out of Phrase-Table Voc.
Dataset Language 1 2 3 4 1 2 3 4
Large Czech 2.2% 30.5% 70.2% 90.3% 3.9% 44.1% 82.2% 95.6%
Large English 1.5% 13.7% 47.3% 78.8% 2.1% 22.4% 63.5% 89.1%
Large Czech + English input sent 1.5% 29.4% 69.6% 90.1% 3.1% 42.8% 81.5% 95.3%
Small Czech 6.7% 48.1% 83.0% 95.5% 12.5% 65.4% 91.9% 98.6%
Small English 3.6% 28.1% 68.3% 90.9% 6.3% 45.4% 84.3% 97.0%
Small Czech + English input sent 5.2% 46.6% 82.4% 95.2% 10.6% 63.7% 91.2% 98.3%
Small Czech lemmas 4.1% 36.3% 75.8% 92.8% 5.8% 52.6% 87.7% 97.4%
Small English lemmas 3.4% 24.6% 64.6% 89.4% 6.9% 53.2% 87.9% 97.5%
Small Czech + English input sent lemmas 3.1% 35.7% 75.6% 92.8% 5.1% 38.1% 80.8% 96.2%
Table 2: Out-of-vocabulary rates.
Distortion Limit
TOpts 3 6 10 30 40
1 0.4 0.4 0.4 0.4 0.4
5 1.5 1.9 2.0 2.0 2.0
10 2.5 3.2 3.5 3.5 3.5
20 3.7 5.0 5.5 5.6 5.6
50 4.9 6.7 8.0 8.6 8.6
100 5.3 7.6 9.1 9.4 9.4
Table 4: Percentage of sentences reachable in
Czech-to-English large setting, two alternative de-
coding paths to translate from Czech lemma if
the form is not available in the translation table
(BLEU score 18.70?0.46).
We observe that reaching man-made reference
translations in Czech-to-English translation is far
harder. Table 3 provides the figures for small data
setting (and no phrase table filtering). The best
reachability we can hope for is given in Table 4
where we allow to use source word lemmas if the
exact form is not available. We see that the default
limits (50 translation options per span and distor-
tion limit of 6) leave us with only 6.7% sentences
reachable.
While not directly important for your training,
the figures still underpin the issue of sparse data in
Czech-English translation.
3 Targetting Czech Word Forms
Bojar (2007) experimented with several transla-
tion scenarios, including what we will call Mor-
phG, i.e. the independent translation of lemma to
lemma and tag to tag followed by a generation step
to produce target-side word form. With the small
training set available then, the MorphG model per-
formed equally well as a simpler direct translation
followed by target-side tagging and an additional
n-gram model over morphological tags. Koehn
and Hoang (2007) reports even a large loss with
MorphG for German-to-English if the alternative
of direct form-to-form translation is not available.
Bojar et al (2009b) applied the two alternative
decoding paths (direct form-to-form and MorphG,
labelled ?T+C+C&T+T+G?) to English-Czech but
they were able to use only 84k sentences. For
the full training set of 2.2M sentences, the model
was too big to fit in reasonable disk limits. More
importantly, already in the small data setting, the
complex model suffered from little stability due
to abundance of features (5 features per phrase-
table plus tree features for three LMs), so nearly
the same performance on the development set gave
largely varying quality on the independent test set.
The most important issue of the MorphG setup,
however, is the explosion of translation options.
Due to the ?synchronous factors? approach of
Moses (Koehn and Hoang, 2007), all translation
options have to be fully constructed before the
main search begins. The MorphG model how-
ever licenses too many possible combinations of
lemmas, tags and final word forms, so the prun-
ing of translation options strikes hard, causing
search errors. For more details, see Bojar et al
(2009a) where a similar issue occurs for treelet-
based translation.
3.1 Two-Step Translation
In order to avoid the explosion of the translation
options6, we experimented with two-step transla-
tion.
The first step translates from English to lemma-
tized Czech augmented to preserve important se-
mantic properties known from the source phrase.
The second step is a monotone translation from
the lemmas to fully inflected Czech. The idea be-
hind the delimitation is that all the morphological
properties of Czech words that can be established
6and also motivated when we noticed that reading MT
output to lemmatized Czech is sometimes more pleasant and
informative than regular phrase-based output
62
Data Size Simple Two-Step
Parallel Mono BLEU SemPOS BLEU SemPOS
Small Small 10.28?0.40 29.92 10.38?0.38 30.01
Small Large 12.50?0.44 31.01 12.29?0.47 31.40
Large Large 14.17?0.51 33.07 14.06?0.49 32.57
Table 5: Performance of direct (Simple) and two-step factored translation in small and large data setting.
regardless the English source should not cause par-
allel data sparseness and clutter the search. In-
stead, they should be decided based on context in
the second phase only.
Specifically, the intermediate Czech represents
most words as tuples containing only: lemma,
negation, grade (of adjectives and adverbs), num-
ber (of nouns, adjectives, verbs) and detailed part
of speech (constraining also e.g. verb tense of
Czech verbs). Some words are handled separately:
? Pronouns, punctuation and the verbs ?by?t? (to
be) and ?m??t? (to have) are represented using
their lowecased full forms because they are very
frequent, often auxiliary to other words and
their exact form best captures the available and
necessary detail of many morphological and
syntactic properties.
? Prepositions are represented using their lemmas
and case because the case of a noun phrase is
actually introduced by the governing word (e.g.
the verb that subcategorized for the noun phrase
or the preposition for prepositional phrases).
Table 5 compares the scores of the simple
phrase-based and the two-step translation via aug-
mented Czech lemmas as described above. The
small and large parallel data denote the datasets
described in Section 1.1. The small monolingual
set means just the news domain of CzEng, while
the large monolingual set means WMT10 mono-
lingual Czech texts (and no CzEng data). Note
that the monolingual data serve three purposes in
the two-step approach: the language model for the
first phase, the translation model in the second
phase (monotone and restricted to phrase-length
of 1; longer phrases did not bring significant im-
provement either), and the language model of the
second phase. Ignoring the opportunity to use the
monolingual set as the language model in the first
phase already hurts the performance.
We see that the results as evaluated both by
BLEU and SemPOS (see Section 4 below) are
rather mixed but not that surprising. There is a
negligible gain in the Small-Small setting, a mixed
outcome in the Small-Large and a little loss in the
Two- Both Both
-Step Fine Wrong Simple Total
Two-Step 23 4 8 - 35
Both Fine 7 14 17 5 43
Both Wrong 8 1 28 2 39
Simple - 3 7 23 33
Total 38 22 60 30 150
Table 6: Manual micro-evaluation of Simple
(12.50?0.44) vs. Two-step (12.29?0.47) model
in the Small-Large setting.
Large-Large setting.
The most interesting result is the Small-Large
setting: BLEU (insignificantly) prefers the simple
and SemPOS the two-step model. It thus seems
that a large target-side LM is sufficient to improve
the BLEU score, despite the untackled issue of
bilingual data sparseness.
We carried out a quick manual evaluation of
150 sentences by two annotators (one of the au-
thors and a third person; systems anonymized):
for each input segment, either one of the outputs
is distinguishably better or both are equally wrong
or equally acceptable. As listed in the confusion
matrix in Table 6, each annotator independently
marginally prefers the two-step approach but the
intersection does not confirm that.7 One good
thingis that the annotators do not completely con-
tradict each other?s preference.
Ultimately, we did not use the two-step ap-
proach in our primary submission, but we feel
there is still some unexploited potential in this
phrase-based approximation of the technique sep-
arating properties of words handled in the transla-
tion phase from properties implied by the target-
side (grammatical) context only. Certainly, the
representation of the intermediate language can
7Of the 23 sentences improved by the two-step setup,
about three quarters indeed had an improvement in lexical
coverage or better morphological choice of a word. Of the
23 sentences where the two-step model hurts, about a half
suffered from errors related to superfluous auxiliary words in
Czech that seem to be introduced by a bias towards word-
for-word translation. This bias is not inherent to the model,
only the (normalized) phrase penalty weight happened to get
nearly three times bigger than in the simple model.
63
be still improved, and more importantly, the sec-
ond phase of monotone decoding could be handled
by a more appropriate model capable of including
more additional (source) context features.8
4 Optimizing towards SemPOS
In our setup, we use minimum error-rate training
(MERT, Och (2003)) to optimize weights of model
components. In the standard implementation in
Moses, BLEU (Papineni et al, 2002) is used as
the objective function, despite its rather disputable
correlation with human judgments of MT quality.
Kos and Bojar (2009) introduced SemPOS, a
metric that performs much better in terms of cor-
relation to human judgments when translating to
Czech. Naturally, we wanted to optimize towards
SemPOS.
SemPOS computes the overlapping of autose-
mantic (content-bearing) word lemmas in the can-
didate and reference translations given a fine-
grained semantic part of speech (sempos9), as de-
fined in Hajic? et al (2006), and outputs average
overlapping score over all sempos types.
The SemPOS metric outperformed common
metrics as BLEU, TER (Snover et al, 2006) or an
adaptation of Meteor (Lavie and Agarwal, 2007)
for Czech on test sets from WMT08 (Callison-
Burch et al, 2008).
4.1 Integrating SemPOS to MERT
In our experiments we used Z-MERT (Zaidan,
2009), a recent implementation of the MERT al-
gorithm, to optimize model parameters.
The SemPOS metric requires to remove all aux-
iliary words and to identify the (deep-syntactic)
lemmas and semantic part of speech for autose-
mantic words. When employed in MERT train-
ing, the whole n-best list of candidates has to pro-
cessed like this at each iteration.
We use the TectoMT platform ( ?Zabokrtsky? and
Bojar, 2008)10 for the linguistic processing. Tec-
toMT follows the complete pipeline of tagging,
surface-syntactic analysis and deep-syntactic anal-
ysis, which is the best but rather costly way to ob-
tain the required information.
Therefore, we use two different ways of obtain-
ing lemmas and semantic parts of speech in the
8We are grateful to Trevor Cohn for the suggestion.
9In the following text we will use SemPOS to denote the
SemPOS metric. When speaking about the semantic part of
speech, we will write sempos type or sempos tag.
10http://ufal.mff.cuni.cz/tectomt/
BLEU SemPOS Iters Time
TectoMT 10.11?0.40 29.69 20 2d12.0h
in MERT 9.53?0.39 29.69 10 1d12.0h
Factored 9.46?0.37 29.36 10 2.4h
translation 8.20?0.37 29.68 - -
6.96?0.33 27.79 9 1.7h
Table 7: Five independent MERT runs optimizing
towards SemPOS with semantic parts of speech
and lemmas provided either by TectoMT on the
fly or by Moses factored translation.
MERT loop:
? indeed apply TectoMT processing to the n-best
list at each iteration (parallelized to 15 CPUs),
? apply TectoMT to the training data, express the
(deep) lemma and sempos as additional factors
using a blank value for auxiliary words, and us-
ing Moses factored translation to translate from
English forms to triplets of Czech form, deep
lemma and sempos.
Table 7 lists several ZMERT runs when opti-
mizing a simple form?form phrase-based model
(small data setting) towards SemPOS. One obser-
vation is that using TectoMT in the MERT loop
is unbearably costly and we avoided it in the sub-
sequent experiments. More importantly, from the
huge differences in the final BLEU as well as Sem-
POS scores (evaluated on the independent test set),
we see how unstable the search is.
SemPOS, while good at comparing different
MT systems, is very bad at comparing candidates
from a single system in an n-best list. This can be
easily explained by its low sensitivity to precision:
SemPOS disregards word forms as well as all aux-
iliary words. This is a good thing to compare very
different candidates (where each of the systems al-
ready struggled to produce a coherent output) but
is of very little help when comparing candidates of
a single system, because these candidates tend to
differ rather in forms than in lexical choice.
4.2 Combination of SemPOS and BLEU
To compensate for some of the shortcomings of
SemPOS, we also attempted to optimize towards
a linear combination of SemPOS and BLEU.
This should increase the suitability of the metric
for MERT optimization because BLEU will take
correct word forms into account while SemPOS
should promote better lexical choice (possibly not
confirmed by BLEU due to a different word form
than in the reference).
Table 8 provides the results of various weight
64
W. BLEU SemPOS W. BLEU SemPOS
1:0 10.42?0.38 29.91 3:1 10.30?0.39 30.03
1:1 10.15?0.39 29.81 10:1 10.17?0.40 29.58
1:1 9.42?0.37 29.30 1:2 10.11?0.38 29.80
2:1 10.37?0.38 29.95 1:10 9.44?0.40 29.74
Table 8: Optimizing towards a linear combina-
tion of BLEU and SemPOS (weights in this order),
small data setting.
BLEU SemPOS
BLEU alone 14.08?0.50 32.44
SemPOS-BLEU (1:1) 13.79?0.55 33.17
Table 9: Optimizing towards BLEU and/or Sem-
POS in large data setting.
settings, including the optimization towards
BLEU alone using ZMERT implementation. We
see that the stability is much better, only few runs
suffered a minor loss (including 1:1 in one case).
Unfortunately, the differences in final BLEU and
SemPOS scores are all within confidence intervals
when trained on the small dataset.
Table 9 documents that in our large data set-
ting, MERT indeed achieves slightly higher Sem-
POS (and lower BLEU) when optimizing towards
it. This corresponds with the intuition that with
more variance in lexical choices available in the
phrase tables, SemPOS can help to balance model
features. The current set of weights is rather lim-
ited, so our future experiments should focus on ac-
tually providing means to e.g. domain adaptation
by using features indicating the applicability of a
phrase in a specific domain.
5 Our Primary Submissions to WMT10
5.1 English-to-Czech Translation
Given the little or no improvements achieved by
the many configurations we tried, our English-to-
Czech primary submission is rather simple:
? Standard GIZA++ word alignment based on both source
and target lemmas.
? Two alternative decoding paths; forms always truecased:
form+tag?form & form?form.
The first path is more specific and helps to preserve core
syntactic elements in the sentence. Without the tag, am-
biguous English words could often all translate as e.g.
nouns, leading to no verb in the Czech sentence. The de-
fault path serves as a back-off.
? Significance filtering of the phrase tables (Johnson et al,
2007) implemented for Moses by Chris Dyer; default set-
tings of filter value a+e and the cut-off 30.
? Two separate 5-gram Czech LMs of truecased forms each
of which interpolates models trained on the following
datasets; the interpolation weights were set automatically
using SRILM (Stolcke, 2002) based on the target side of
Large Small
Backed-off by source lemmas 18.95?0.45 14.95?0.48
form?form only 18.41?0.44 14.73?0.47
Table 10: Translation from Czech better when
backed-off by source lemmas.
the development set:11
? Interpolated CzEng domains: news, web, fiction. The
rationale behind the selection of the domains is that we
prefer prose-like texts for LM estimation (and not e.g.
technical documentation) while we want as much paral-
lel data as possible.
? Interpolated monolingual corpora: WMT09
monolingual, WMT10 monolingual, Czech
National Corpus (Kocek et al, 2000) sections
SYN2000+2005+2006PUB.
? Lexicalized reordering (or-bi-fe) based on forms.
? Standard Moses MERT towards BLEU.
5.2 Czech-to-English Translation
For Czech-to-English translation we experimented
with far fewer configuration options. Our primary
submission is configured as follows:
? Two alternative decoding paths; forms always truecased:
form?form & lemma?form.
? Significance filtering as in Section 5.1.
? 5-gram English LM based on CzEng English side only.12
? Lexicalized reordering (or-bi-fe) based on forms.
? Standard Moses MERT towards BLEU.
Table 10 documents the utility of the additional
decoding path from Czech lemmas in both small
and large setting, surprisingly less significant in
the small setting. Later experiments with system
combination by Kenneth Heafield indicated that
while our system is not among the top three, it
brings an advantage to the combination.
6 Conclusion
We provided an extensive documentation of Czech
data sparseness issue for machine translation. We
attempted to tackle the problem of constructing
the target-side form by a two-step translation setup
and the problem of unreliable automatic evalua-
tion by employing a new metric in MERT loop,
neither with much success so far. Both of the at-
tempts however deserve further exploration. Ad-
ditionally, we provide the exact configurations of
our WMT10 primary submissions.
11The subsequent MERT training using the same develop-
ment test may suffer from overestimating the language model
weights, but we did not observe the issue, possibly due to
only moderate overlap of the datasets.
12We attempted to use a second LM trained on English Gi-
gaword by Chris Callison-Burch, but we observed a drop in
BLEU score from 18.95?0.45 to 18.03?0.44 probably due
to different tokenization guidelines applied.
65
References
Ondr?ej Bojar and Zdene?k ?Zabokrtsky?. 2009. CzEng
0.9: Large Parallel Treebank with Rich Annotation.
Prague Bulletin of Mathematical Linguistics, 92.
Ondr?ej Bojar, Miroslav Jan??c?ek, and Miroslav
Ty?novsky?. 2009a. Evaluation of Tree Transfer Sys-
tem. Project Euromatrix - Deliverable 3.4, Institute
of Formal and Applied Linguistics, Charles Univer-
sity in Prague.
Ondr?ej Bojar, David Marec?ek, Va?clav Nova?k, Mar-
tin Popel, Jan Pta?c?ek, Jan Rous?, and Zdene?k
?Zabokrtsky?. 2009b. English-Czech MT in 2008.
In Proc. of Fourth Workshop on Statistical Machine
Translation, ACL, Athens, Greece.
Ondr?ej Bojar. 2007. English-to-Czech Factored Ma-
chine Translation. In Proc. of the Second Workshop
on Statistical Machine Translation, ACL, Prague,
Czech Republic, June.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further Meta-Evaluation of Machine Translation. In
Proc. of the Third Workshop on Statistical Machine
Translation, ACL, Columbus, Ohio.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan ?Ste?pa?nek, Jir??? Havelka,
Marie Mikulova?, Zdene?k ?Zabokrtsky?, and Magda
?Sevc???kova? Raz??mova?. 2006. Prague Dependency
Treebank 2.0. LDC2006T01, ISBN: 1-58563-370-
4.
Eva Hajic?ova?, Jir??? Havelka, Petr Sgall, Kater?ina Vesela?,
and Daniel Zeman. 2004. Issues of Projectivity in
the Prague Dependency Treebank. The Prague Bul-
letin of Mathematical Linguistics, 81.
Toma?s? Holan. 2003. K syntakticke? analy?ze c?esky?ch(!)
ve?t. In MIS 2003. MATFYZPRESS.
Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving translation qual-
ity by discarding most of the phrasetable. In Proc.
of EMNLP-CoNLL, Prague, Czech Republic.
Jan Kocek, Marie Kopr?ivova?, and Karel Kuc?era, edi-
tors. 2000. ?Cesky? na?rodn?? korpus - u?vod a pr???ruc?ka
uz?ivatele. FF UK - ?U ?CNK, Prague.
Philipp Koehn and Hieu Hoang. 2007. Factored Trans-
lation Models. In Proc. of EMNLP.
Philipp Koehn. 2004. Statistical Significance Tests
for Machine Translation Evaluation. In Proc. of
EMNLP, Barcelona, Spain.
Kamil Kos and Ondr?ej Bojar. 2009. Evaluation of
Machine Translation Metrics for Czech as the Tar-
get Language. The Prague Bulletin of Mathematical
Linguistics, 92.
Alon Lavie and Abhaya Agarwal. 2007. Meteor:
An Automatic Metric for MT Evaluation with High
Levels of Correlation with Human Judgments. In
Proc. of the Second Workshop on Statistical Ma-
chine Translation, ACL, Prague, Czech Republic.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of ACL,
Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proc. of ACL,
Philadelphia, Pennsylvania.
Lane Schwartz. 2008. Multi-source translation meth-
ods. In Proc. of AMTA.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proc. of AMTA.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of Intl. Conf. on
Spoken Language Processing, volume 2.
Jo?rg Tiedemann. 2009. News from OPUS - A Col-
lection of Multilingual Parallel Corpora with Tools
and Interfaces. In Proc. of Recent Advances in NLP
(RANLP).
Zdene?k ?Zabokrtsky? and Ondr?ej Bojar. 2008. TectoMT,
Developer?s Guide. Technical Report TR-2008-39,
Institute of Formal and Applied Linguistics, Charles
University in Prague.
Omar F. Zaidan. 2009. Z-MERT: A Fully Config-
urable Open Source Tool for Minimum Error Rate
Training of Machine Translation Systems. The
Prague Bulletin of Mathematical Linguistics, 91.
66
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 1?11,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
A Grain of Salt for the WMT Manual Evaluation?
Ondr?ej Bojar, Milos? Ercegovc?evic?, Martin Popel
Charles University in Prague
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
{bojar,popel}@ufal.mff.cuni.cz
ercegovcevic@hotmail.com
Omar F. Zaidan
Department of Computer Science
Johns Hopkins University
ozaidan@cs.jhu.edu
Abstract
The Workshop on Statistical Machine
Translation (WMT) has become one of
ACL?s flagship workshops, held annually
since 2006. In addition to soliciting pa-
pers from the research community, WMT
also features a shared translation task for
evaluating MT systems. This shared task
is notable for having manual evaluation as
its cornerstone. The Workshop?s overview
paper, playing a descriptive and adminis-
trative role, reports the main results of the
evaluation without delving deep into ana-
lyzing those results. The aim of this paper
is to investigate and explain some interest-
ing idiosyncrasies in the reported results,
which only become apparent when per-
forming a more thorough analysis of the
collected annotations. Our analysis sheds
some light on how the reported results
should (and should not) be interpreted, and
also gives rise to some helpful recommen-
dation for the organizers of WMT.
1 Introduction
The Workshop on Statistical Machine Translation
(WMT) has become an annual feast for MT re-
searchers. Of particular interest is WMT?s shared
translation task, featuring a component for man-
ual evaluation of MT systems. The friendly com-
petition is a source of inspiration for participating
teams, and the yearly overview paper (Callison-
Burch et al, 2010) provides a concise report of the
state of the art. However, the amount of interest-
ing data collected every year (the system outputs
? This work has been supported by the grants EuroMa-
trixPlus (FP7-ICT-2007-3-231720 of the EU and 7E09003 of
the Czech Republic), P406/10/P259, MSM 0021620838, and
DARPA GALE program under Contract No. HR0011-06-2-
0001. We are grateful to our students, colleagues, and the
three reviewers for various observations and suggestions.
and, most importantly, the annotator judgments)
is quite large, exceeding what the WMT overview
paper can afford to analyze with much depth.
In this paper, we take a closer look at the data
collected in last year?s workshop, WMT101, and
delve a bit deeper into analyzing the manual judg-
ments. We focus mainly on the English-to-Czech
task, as it included a diverse portfolio of MT sys-
tems, was a heavily judged language pair, and also
illustrates interesting ?contradictions? in the re-
sults. We try to explain such points of interest,
and analyze what we believe to be the positive and
negative aspects of the currently established eval-
uation procedure of WMT.
Section 2 examines the primary style of man-
ual evaluation: system ranking. We discuss how
the interpretation of collected judgments, the com-
putation of annotator agreement, and document
that annotators? individual preferences may render
two systems effectively incomparable. Section 3
is devoted to the impact of embedding reference
translations, while Section 4 and Section 5 discuss
some idiosyncrasies of other WMT shared tasks
and manual evaluation in general.
2 The System Ranking Task
At the core of the WMT manual evaluation is the
system ranking task. In this task, the annotator
is presented with a source sentence, a reference
translation, and the outputs of five systems over
that source sentence. The instructions are kept
minimal: the annotator is to rank the presented
translations from best to worst. Ties are allowed,
but the scale provides five rank labels, allowing the
annotator to give a total order if desired.
The five assigned rank labels are submitted at
once, making the 5-tuple a unit of annotation. In
the following, we will call this unit a block. The
blocks differ from each other in the choice of the
1http://www.statmt.org/wmt10
1
Language Pair Systems Blocks Labels Comparisons Ref ? others Intra-annot. ? Inter-annot. ?
German-English 26 1,050 5,231 10,424 0.965 0.607 0.492
English-German 19 1,407 6,866 13,694 0.976 0.560 0.512
Spanish-English 15 1,140 5,665 11,307 0.989 0.693 0.508
English-Spanish 17 519 2,591 5,174 0.935 0.696 0.594
French-English 25 837 4,156 8,294 0.981 0.722 0.452
English-French 20 801 3,993 7,962 0.917 0.636 0.449
Czech-English 13 543 2,691 5,375 0.976 0.700 0.504
English-Czech 18 1,395 6,803 13,538 0.959 0.620 0.444
Average 19 962 4,750 9,471 0.962 0.654 0.494
Table 1: Statistics on the collected rankings, quality of references and kappas across language pairs. In
general, a block yields a set of five rank labels, which yields a set of
(5
2
)
= 10 pairwise comparisons.
Due to occasional omitted labels, the Comparisons/Blocks ratio is not exactly 10.
source sentence and the choice of the five systems
being compared. A couple of tricks are introduced
in the sampling of the source sentences, to en-
sure that a large enough number of judgments is
repeated across different screens for meaningful
computation of inter- and intra-annotator agree-
ment. As for the sampling of systems, it is done
uniformly ? no effort is made to oversample or un-
dersample a particular system (or a particular pair
of systems together) at any point in time.
In terms of the interface, the evaluation utilizes
the infrastructure of Amazon?s Mechanical Turk
(MTurk)2, with each MTurk HIT3 containing three
blocks, corresponding to three consecutive source
sentences.
Table 1 provides a brief comparison of the vari-
ous language pairs in terms of number of MT sys-
tems compared (including the reference), number
of blocks ranked, the number of pairwise com-
parisons extracted from the rankings (one block
with 5 systems ranked gives 10 pairwise compar-
isons, but occasional unranked systems are ex-
cluded), the quality of the reference (the percent-
age of comparisons where the reference was better
or equal than another system), and the ? statistic,
which is a measure of agreement (see Section 2.2
for more details).4
We see that English-to-Czech, the language pair
on which we focus, is not far from the average in
all those characteristics except for the number of
collected comparisons (and blocks), making it the
second most evaluated language pair.
2http://www.mturk.com/
3?HIT? is an acronym for human intelligence task, which
is the MTurk term for a single screen presented to the anno-
tator.
4We only use the ?expert? annotations of WMT10, ignor-
ing the data collected from paid annotators on MTurk, since
they were not part of the official evaluation.
2.1 Interpreting the Rank Labels
The description in the WMT overview paper says:
?Relative ranking is our official evaluation met-
ric. [Systems] are ranked based on how frequently
they were judged to be better than or equal to
any other system.? (Emphasis added.) The WMT
overview paper refers to this measure as ?? oth-
ers?, with a variant of it called ?> others? that does
not reward ties.
We first note that this description is somewhat
ambiguous, and an uninformed reader might in-
terpret it in one of two different ways. For some
system A, each block in which A appears includes
four implicit pairwise comparisons (against the
other presented systems). How is A?s score com-
puted from those comparisons?
The correct interpretation is that A is re-
warded once for each of the four comparisons in
which A wins (or ties).5 In other words, A?s score
is the number of pairwise comparisons in which
A wins (or ties), divided by the total number of
pairwise comparisons involving A. We will use
?? others? (resp. ?> others?) to refer to this inter-
pretation, in keeping with the terminology of the
overview paper.
The other interpretation is that A is rewarded
only if A wins (or ties) all four comparisons. In
other words, A?s score is the number of blocks in
whichA wins (or ties) all comparisons, divided by
the number of blocks in which A appears. We will
use ?? all in block? (resp. ?> all in block?) to
refer to this interpretation.6
5Personal communication with WMT organizers.
6There is yet a third interpretation, due to a literal read-
ing of the description, where A is rewarded at most once per
block if it wins (or ties) any one of its four comparisons. This
is probably less useful: it might be good at identifying the
bottom tier of systems, but would fail to distinguish between
all other systems.
2
REF C
U
-B
O
JA
R
C
U
-T
E
C
T
O
E
U
R
O
T
R
A
N
S
O
N
L
IN
E
B
P
C
-T
R
A
N
S
U
E
D
IN
? others 95.9 65.6 60.1 54.0 70.4 62.1 62.2
> others 90.5 45.0 44.1 39.3 49.1 49.4 39.6
? all in block 93.1 32.3 30.7 23.4 37.5 32.5 28.1
> all in block 81.3 13.6 19.0 13.3 15.6 18.7 10.6
Table 2: Sentence-level ranking scores for the
WMT10 English-Czech language pair. The ??
others? and ?> others? scores reproduced here
exactly match numbers published in the WMT10
overview paper. A boldfaced score marks the best
system in a given row (besides the reference).
For quality control purposes, the WMT organiz-
ers embed the reference translations as a ?system?
alongside the actual entries (the idea being that an
annotator clicking randomly would be easy to de-
tect, since they would not consistently rank the
reference ?system? highly). This means that the
reference is as likely as any other system to ap-
pear in a block, and when the score for a system A
is computed, pairwise comparisons with the refer-
ence are included.
We use the publicly released human judgments7
to compute the scores of systems participating in
the English-Czech subtask, under both interpreta-
tions. Table 2 reports the scores, with our ?? oth-
ers? (resp. ?> others?) scores reproduced exactly
matching those reported in Table 21 of the WMT
overview paper. (For clarity, Table 2 is abbreviated
to include only the top six systems of twelve.)
Our first suggestion is that both measures could
be reported in future evaluations, since each tells
us something different. The first interpretation
gives partial credit for an MT system, hence distin-
guishing systems from each other at a finer level.
This is especially important for a language pair
with relatively few annotations, since ?? others?
would produce a larger number of data points (four
per system per block) than ?? all in block? (one
per system per block). Another advantage of the
official ?? others? is greater robustness towards
various factors like the number of systems in the
competition, the number of systems in one block
or the presence of the reference in the block (how-
ever, see Section 3).
As for the second interpretation, it helps iden-
tify whether or not a single system (or a small
group of systems) is strongly dominant over the
other systems. For the systems listed in Table 2,
7http://statmt.org/wmt10/results.html
-
10 0
 
10
 
20
 
30
 
40
 
50
 
60  1
0
 
20
 
30
 
40
 
50
 
60
 
70
 
80
>= All in Block
>= 
Othe
rs
Czec
h-En
glish
Engl
ish-C
zech
Engl
ish-F
renc
h
Engl
ish-G
erma
n
Engl
ish-S
pani
sh
Fren
ch-E
nglis
h
Germ
an-E
nglis
h
Span
ish-E
nglis
h
a*x
+b
Figure 1: ?? all in block? and ?? others? provide
very similar ordering of systems.
?> all in block? suggests its potential in the con-
text of system combination: CU-TECTO and PC-
TRANS win almost one fifth of the blocks in which
they appear, despite the fact that either a refer-
ence translation or a combination system already
appears alongside them. (See also Table 4 below.)
Also, note that if the ranking task were designed
specifically to cater to the ?? all in block? inter-
pretation, it would only have two ?rank? labels (ba-
sically, ?top? and ?non-top?). In that case, an-
notators would spend considerably less time per
block than they do now, since all they need to do
is identify the top system(s) per block, without dis-
tinguishing non-top systems from each other.
Even for those interested in distinguishing non-
state-of-the-art systems from each other, we point
out that the ?? all in block? interpretation ulti-
mately gives a system ordering that is very simi-
lar to that of the official ?? others? interpretation,
even for the lower-tier systems (Figure 1).
2.2 Annotator Agreement
The WMT10 overview paper reports inter- and
intra-annotator agreement over the pairwise com-
parisons, to show the validity of the evaluation
setup and the ?? others? metric. Agreement is
quantified using the following formula:
? =
P (A)? P (E)
1? P (E)
(1)
where P (A) is the proportion of times two anno-
tators are observed to agree, and P (E) is the ex-
pected proportion of times two annotators would
agree by chance. Note that ? has a value of at most
1, with higher ? values indicating higher rates of
agreement. The ? measure is more meaningful
3
 
0.1
 
0.2
 
0.3
 
0.4
 
0.5
 
0.6
 
0.7
 
0.8
 
0.9 1  
0
 
5
 
10
 
15
 
20
 
25
 
30
 
35
Kappa
Sour
ce le
ngth
Intra
. inc
l. ref
.
Intra
. exc
l. ref
.
Inter
. inc
l. ref
.
Inter
. exc
l. ref
.
Mod
erate
 agre
eme
nt
Figure 2: Intra-/inter-annotator agreement
with/without references, across various source
sentence lengths (lengths of n and n + 1 are used
to plot the point at x = n). This figure is based on
all language pairs.
than reporting P (A) as is, since it takes into ac-
count, via P (E), how ?surprising? it is for annota-
tors to agree in the first place.
In the context of pairwise comparisons, an
agreement between two annotators occurs when
they compare the same pair of systems (S1,S2),
and both agree on their relative ranking: either
S1 > S2, S1 = S2, or S1 < S2. P (E) is then:
P (E) = P 2(S1>S2)+P 2(S1=S2)+P 2(S1<S2) (2)
In the WMT overview paper, all three cate-
gories are assumed equally likely, giving P (E) =
1
9 +
1
9 +
1
9 =
1
3 . For consistency with the WMT
overview paper, and unless otherwise noted, we
also use P (E) = 13 whenever a ? value is re-
ported. (Though see Section 2.2.2 for a discussion
about P (E).)
2.2.1 Observed Agreement for Different
Sentence Lengths
In Figure 2 we plot the ? values across different
source sentence lengths. We see that the inter-
annotator agreement (when excluding references)
is reasonably high only for sentences up to 10
words in length ? according to Landis and Koch
(1977), and as cited by the WMT overview paper,
not even ?moderate? agreement can be assumed if
? is less than 0.4. Another popular (and controver-
sial) rule of thumb (Krippendorff, 1980) is more
strict and says that ? < 0.67 is not suitable even
for tentative conclusions.
For this reason, and given that a majority of sen-
tences are indeed more than 10 words in length
(the median is 20 words), we suggest that future
evaluations either include fewer outputs per block,
or divide longer sentences into shorter segments
(e.g. on clause boundaries), so these segments are
more easily and reliably comparable. The latter
suggestions assumes word alignment as a prepro-
cessing and presenting the annotators the context
of the judged segment.
2.2.2 Estimating P (E), the Expected
Agreement by Chance
Several agreement measures (usually called kap-
pas) were designed based on the Equation 1 (see
Artstein and Poesio (2008) and Eugenio and Glass
(2004) for an overview and a discussion). Those
measures differ from each other in how to de-
fine the individual components of Equation 2, and
hence differ in what the expected agreement by
chance (P (E)) would be:8
? The S measure (Bennett et al, 1954) assumes
a uniform distribution over the categories.
? Scott?s pi (Scott, 1955) estimates the distribu-
tion empirically from actual annotation.
? Cohen?s ? (Cohen, 1960) estimates the dis-
tribution empirically as well, and further as-
sumes a separate distribution for each anno-
tator.
Given that the WMT10 overview paper assumes
that the three categories (S1 > S2, S1 = S2, and
S1 < S2) are equally likely, it is using the S mea-
sure version of Equation 1, though it does not ex-
plicitly say so ? it simply calls it ?the kappa coef-
ficient? (K).
Regardless of what the measure should be
called, we believe that the uniform distribution it-
self is not appropriate, even though it seems to
model a ?random clicker? adequately. In partic-
ular, and given the design of the ranking inter-
face, 13 is an overestimate of P (S1 = S2) for
a random clicker, and should in fact be 15 : each
system receives one of five rank labels, and for
two systems to receive the same rank label, there
are only five (out of 25) label pairs that satisfy
S1 = S2. Therefore, with P (S1 = S2) = 15 ,
8These three measures were later generalized to more than
two annotators (Fleiss, 1971; Bartko and Carpenter, 1976),
Thus, without loss of generality, our examples involve two
annotators.
4
?? Others? S pi
Inter incl. ref. 0.487 0.454excl. ref. 0.439 0.403
Intra incl. ref. 0.633 0.609excl. ref. 0.601 0.575
Table 3: Summary of two variants of kappa: S
(or K as it is reported in the WMT10 paper) and
our proposed Scott?s pi. We report inter- vs. intra-
annotator agreement and collected from all com-
parisons (?incl. ref.?) vs. collected only from
comparisons without the reference (?excl. ref.?)
because it is generally easier to agree that the ref-
erence is better than the other systems. This table
is based on all language pairs.
we have P (S1 > S2) = P (S1 < S2) = 25 , and
therefore P (E) = 0.36 rather than 0.333.
Taking the discussion a step further, we actually
advocate following the idea of Scott?s pi, whereby
the distribution of each category is estimated em-
pirically from the actual annotation, rather than
assuming a random annotator ? these frequencies
are easy to compute, and reflect a more meaning-
ful P (E).9
Under this interpretation, P (S1 = S2) is cal-
culated to be 0.168, reflecting the fraction of pair-
wise comparisons that correspond to a tie. (Note
that this further supports the claim that setting
P (S1 = S2) = 13 for a random clicker, as used
in the WMT overview paper, is an overestimate.)
This results in P (E) = 0.374, yielding, for in-
stance, pi = 0.454 for ?? others? inter-annotator
agreement, somewhat lower than ? = 0.487 (re-
ported in Table 3).
We do note that the difference is rather small,
and that our aim is to be mathematically sound
above all. Carefully defining P (E) would be im-
portant when comparing kappas across different
tasks with different P (E), or when attempting
to satisfy certain thresholds (as the cited 0.4 and
0.67). Furthermore, if one is interested in mea-
suring agreement for individual annotators, such
as identifying those who have unacceptably low
intra-annotator agreement, the question of P (E) is
quite important, since annotation behavior varies
noticeably from one annotator to another. A ?con-
servative? annotator who prefers to rank systems
as being tied most of the time would have a high
9We believe that P (E) should not reflect the chance that
two random annotators would agree, but the chance that two
actual annotators would agree randomly. The two sound sub-
tly related but are actually quite different.
P (E), whereas an annotator using ties moderately
would have a low P (E). Hence, two annotators
with equal agreement rates (P (A)) are not neces-
sarily equally proficient, since their P (E) might
differ considerably.10
2.3 The ? variant vs. the > variant
Even within the same interpretation of how sys-
tems could be scored, there is a question of
whether or not to reward ties. The overview paper
reports both variants of its measure, but does not
note that there are non-trivial differences between
the two orderings. Compare for example the ??
others? ordering vs. the ?> others? ordering of
CU-BOJAR and PC-TRANS (Table 2), showing an
unexpected swing of 7.9%:
? others > others
CU-BOJAR 65.6 45.0
PC-TRANS 62.1 49.4
CU-BOJAR seems better under the? variant, but
loses out when only strict wins are rewarded. The-
oretically, this could be purely due to chance, but
the total number of pairwise comparisons in ??
others? is relatively large (about 1,500 pairwise
comparisons for each system), and ought to can-
cel such effects.
A similar pattern could be seen under the ?all in
block? interpretation as well (e.g. for CU-TECTO
and ONLINEB). Table 4 documents this effect by
looking at how often a system is the sole winner
of a block. Comparing PC-TRANS and CU-BOJAR
again, we see that PC-TRANS is up there with CU-
TECTO and DCU-COMBO as the most frequent sole
winners, winning 71 blocks, whereas CU-BOJAR
is the sole winner of only 53 blocks. This is in
spite of the fact that PC-TRANS actually appeared
in slightly fewer blocks than CU-BOJAR (385 vs.
401).
One possible explanation is that the two vari-
ants (??? and ?>?) measure two subtly different
things about MT systems. Digging deeper into Ta-
ble 2?s values, we find that CU-BOJAR is tied with
another system 65.6 ? 45.0 = 20.4% of the time,
while PC-TRANS is tied with another system only
62.1? 49.4 = 12.7% of the time. So it seems that
PC-TRANS?s output is noticeably different from
another system more frequently than CU-BOJAR,
which reduces the number of times that annotators
10Who?s more impressive: a psychic who correctly pre-
dicts the result of a coin toss 50% of the time, or a psychic
who correctly predicts the result of a die roll 50% of the time?
5
Blocks Sole Winner
305 Reference
73 CU-TECTO
71 PC-TRANS
70 DCU-COMBO
57 RWTH-COMBO
54 ONLINEB
53 CU-BOJAR
46 EUROTRANS
41 UEDIN
41 UPV-COMBO
175 One of eight other systems
409 No sole winner
1395 Total English-to-Czech Blocks
Table 4: A breakdown of the 1,395 blocks for the
English-Czech task, according to which system (if
any) is the sole winner. On average, a system ap-
pears in 388 blocks.
mark PC-TRANS as tied with another system.11 In
that sense, the ??? ranking is hurting PC-TRANS,
since it does not benefit from its small number of
ties. On the other hand, the ?>? variant would not
reward CU-BOJAR for its large number of ties.
The ?? others? score may be artificially boosted
if several very similar systems (and therefore
likely to be ?tied?) take part in the evaluation.12
One possible solution is to completely disregard
ties and calculate the final score as winswins+losses . We
recommend to use this score instead of ?? others?
( wins+tieswins+ties+losses ) which is biased toward often tied
systems, and ?> others? ( winswins+ties+losses ) which is
biased toward systems with few ties.
2.4 Surprise? Does the Number of
Evaluations Affect a System?s Score?
When examining the system scores for the
English-Czech task, we noticed a surprising pat-
tern: it seemed that the more times a system is
sampled to be judged, the lower its ?? others?
score (?? all in block? behaving similarly). A
scatter plot of a system?s score vs. the number of
blocks in which it appears (Figure 3) makes the
pattern obvious.
We immediately wondered if the pattern holds
in other language pairs. We measured Pearson?s
correlation coefficient within each language pair,
reported in Table 5. As it turns out, English-
11Indeed, PC-TRANS is a commercial system (manually)
tuned over a long period of time and based on resources very
different from what other participants in WMT use.
12In the preliminary WMT11 results, this seems to hap-
pen to four Moses-like systems (UEDIN, CU-BOJAR, CU-
MARECEK and CU-TAMCHYNA) which have better ?? oth-
ers? score but worse ?> others? score than CU-TECTO.
Correlation of Block Count
Source Target vs. ?? Others?
English Czech -0.558
English Spanish -0.434
Czech English -0.290
Spanish English -0.240
English French -0.227
English German -0.161
French English -0.024
German English 0.146
Overall -0.092
Table 5: Pearson?s correlation between the num-
ber of blocks where a system was ranked and the
system?s ?? others? score. (The reference itself is
not included among the considered systems).
 
30
 
35
 
40
 
45
 
50
 
55
 
60
 
65
 
70
 
75
 
80  35
0
 
360
 
370
 
380
 
390
 
400
 
410
 
420
>= Others
Num
ber o
f judg
ments
cmu
-hea
field
-com
bo
cu-
bojar
cu-
tecto cu
-ze
ma
n
dcu
dcu-
com
bo
eur
otra
ns
koc
koc-
com
bo
onlin
eA
onlin
eB
pc-tr
ans
pots
dam
rwth
-com
bo
sfu
uedi
n
upv-
com
bo
a*x
+b
Figure 3: A plot of ?? others? system score vs.
times judged, for English-Czech.
Czech happened to be the one language pair where
the ?correlation? is strongest, with only English-
Spanish also having a somewhat strong correla-
tion. Overall, though, there is a consistent trend
that can be seen across the language pairs. Could
it really be the case that the more often a system is
judged, the worse its score gets?
Examining plots for the other language pairs
makes things a bit clearer. Consider for example
the plot for English-Spanish (Figure 4). As one
would hope, the data points actually come together
to form a cloud, indicating a lack of correlation.
The reason that a hint of a correlation exists is the
presence of two outliers in the bottom right cor-
ner. In other words, the very worst systems are,
indeed, the ones judged quite often. We observed
this pattern in several other language pairs as well.
The correlation naturally does not imply cau-
sation. We are still not sure how to explain the
artifact. A subtle possibility lies in the MTurk
interface: annotators have the choice to accept a
HIT or skip it before actually providing their la-
6
 
10
 
20
 
30
 
40
 
50
 
60
 
70
 
80  13
0
 
135
 
140
 
145
 
150
 
155
 
160
 
165
 
170
>= Others
Num
ber o
f judg
mentscamb
ridge
cmu
-hea
field
-com
bo
cu-
zem
an
dcu dfk
ijhu
koc
koc-
com
bo
onlin
eA
onlin
eB
rwth
-com
bo
sfu
uedi
n upb-
com
bo
upv
upv-
nnlm
a*x
+b
Figure 4: A plot of ?? others? system score vs.
times judged, for English-Spanish.
bels. It might be the case that some annotators are
more willing to accept HITs when there is an ob-
viously poor system (since that would make their
task somewhat easier), and who are more prone
to skipping HITs where the systems seem hard to
distinguish from each other. So there might be a
causation effect after all, but in the reverse order:
a system gets judged more often if it is a bad sys-
tem.13 A suggestion from the reviewers is to run a
pilot annotation with deliberate inclusion of a poor
system among the ranked ones.
2.5 Issues of Pairwise Judgments
The WMT overview paper also provides pairwise
system comparisons: each cell in Table 6 indicates
the percentage of pairwise comparisons between
the two systems where the system in the column
was ranked better (>) than the system in the row.
For instance, there are 81 ranking responses where
both CU-TECTO and CU-BOJAR were present and
indeed ranked14 among the 5 systems in the block.
In 37 (45.7%) of the cases, CU-TECTO was ranked
better, in 29 (35.8%), CU-BOJAR was ranked better
and there was a tie in the remaining 15 (18.5%)
cases. The ties are not explicitly shown in Table 6
but they are implied by the total of 100%. The cell
is in bold where there was a win in the pairwise
comparison, so 45.7 is bold in our example.
An interesting ?discrepancy? in Table 6 is that
CU-TECTO wins pairwise comparisons with CU-
BOJAR and UEDIN but it scores worse than them
in the official ?? others?, cf. Table 2. Simi-
larly, UEDIN outperformed ONLINEB in the pair-
13No pun intended!
14The users sometimes did not fill any rank for a system.
Such cases are ignored.
R
E
F
C
U
-B
O
JA
R
C
U
-T
E
C
T
O
E
U
R
O
T
R
A
N
S
O
N
L
IN
E
B
P
C
-T
R
A
N
S
U
E
D
IN
REF - 4.3 4.3 5.1 3.8 3.6 2.3
CU-BOJAR 87.1 - 45.7 28.3 44.4 39.5 41.1
CU-TECTO 88.2 35.8 - 38.0 55.8 44.0 36.0
EUROTRANS 88.5 60.9 46.8 - 50.7 53.8 48.6
ONLINEB 91.2 31.1 29.1 32.8 - 43.8 39.3
PC-TRANS 88.0 45.3 42.9 28.6 49.3 - 36.6
UEDIN 94.3 39.3 44.2 31.9 32.1 49.5 -
Table 6: Pairwise comparisons extracted from
sentence-level rankings of the WMT10 English-
Czech News Task. Re-evaluated to reproduce the
numbers published in WMT10 overview paper.
Bold in column A and row B means that system
A is pairwise better than system B.
wise comparisons but it was ranked worse in both
> and ? official comparison.
In the following, we focus on the CU-BOJAR
(B) and CU-TECTO (T) pair because they are in-
teresting competitors on their own. They both use
the same parallel corpus for lexical mapping but
operate very differently: CU-BOJAR is based on
Moses while CU-TECTO transfers at a deep syn-
tactic layer and generates target text which is more
or less grammatically correct but suffers in lexical
choice.
2.5.1 Different Set of Sentences
The mismatch in the outcomes of ?? others? and
pairwise comparisons could be caused by different
set of sentences. The pairwise ranking is collected
from the set of blocks where both CU-BOJAR and
CU-TECTO appeared (and were indeed ranked).
Each of the systems however competes in other
blocks as well, which contributes to the official ??
others?.
The set of sentences underlying the comparison
is very different and more importantly that the ba-
sis for pairwise comparisons is much smaller than
the basis of the official ?? others? interpretation.
The outcome of the official interpretation however
depends on the random set of systems your system
was compared to. In our case, it is impossible to
distinguish, whether CU-TECTO had just bad luck
on sentences and systems it was compared to when
CU-BOJAR was not in the block and/or whether the
81 blocks do not provide a reliable picture.
2.5.2 Pairwise Judgments Unreliable
To complement WMT10 rankings for the two sys-
tems and avoid the possible lower reliability due
to 5-fold ranking instead of a targeted compari-
7
Author of B says:
both both
B>T T>B fine wrong Total
T
sa
ys
:
B>T 9 - 1 1 11
T>B 2 13 - 3 18
both fine 2 - 2 3 7
both wrong 10 5 1 11 27
Total 23 18 4 18 63
Table 7: Additional annotation of 63 CU-BOJAR
(B) vs. CU-TECTO (T) sentences by two annota-
tors.
Better Both
Annotator B T fine wrong
A 24 23 5 11
C 10 12 5 36
D 32 20 2 9
M 11 18 7 27
O 23 18 4 18
Z 25 27 2 9
Total 125 118 25 110
Table 8: Blurry picture of pairwise rankings of
CU-BOJAR vs. CU-TECTO. Wins in bold.
son, we asked the main authors of both CU-BOJAR
and CU-TECTO to carry out a blind pairwise com-
parison on the exact set of 63 sentences appearing
across the 81 blocks in which both systems were
ranked. As the totals in Table 7 would suggest,
each author unwittingly recognized his system and
slightly preferred it. The details however reveal a
subtler reason for the low agreement: one of the
annotators was less picky about MT quality and
accepted 10+5 sentences completely rejected by
the other annotator. In total, these two annotators
agreed on 9 + 13 + 2 + 11 = 35 (56%) of cases
and their pairwise ? is 0.387.
A further annotation of these 63 sentences by
four more people completes the blurry picture:
the pairwise ? for each pair of our five annota-
tors ranges from 0.242 to 0.615 with the aver-
age 0.407?0.106. The multi-annotator ? (Fleiss,
1971) is 0.394 and all six annotators agree on a
single label only in 24% of cases. The agree-
ment is not better even if we merge the categories
?Both fine? and ?Both wrong? into a single one:
The pairwise ? ranges from 0.212 to 0.620 with
the average 0.405?0.116, the multi-annotator ? is
0.391. Individual annotations are given in Table 8.
Naturally, the set of these 63 sentences is not a
representative sample. Even if one of the systems
SRC It?s not completely ideal.
REF Nen?? to u?plne? idea?ln??. Ranks
PC-TRANS To nen?? u?plne? idea?ln??. 2 5
CU-BOJAR To nen?? u?plne? idea?ln??. 5 4
Table 9: Two rankings by the same annotator.
SRC FCC awarded a tunnel in Slovenia for 64 million
REF FCC byl pr?ide?len tunel ve Slovinsku za 64 milionu?
Gloss FCC was awarded a tunnel in Slovenia for 64 million
HYP1 FCC pr?ide?lil tunel ve Slovinsku za 64 milio?nu?
HYP2 FCC pr?ide?lila tunel ve Slovinsku za 64 milionu?
Gloss FCC awardedmasc/fem a tunnel in Slovenia for 64 million
Figure 5: A poor reference translation confuses
human judges. The SRC and REF differ in the ac-
tive/passive form, attributing completely different
roles to ?FCC?.
actually won, such an observation could not have
been generalized to other test sets. The purpose
of the exercise was to check whether we are at all
able to agree which of the systems translates this
specific set of sentences better. As it turns out,
even a simple pairwise ranking can fail to pro-
vide an answer because different annotators sim-
ply have different preferences.
Finally, Table 9 illustrates how poor the
WMT10 rankings can be. The exact same string
produced by two systems was ranked differently
each time ? by the same annotator. (The hypothe-
sis is a plausible translation, only the information
structure of the sentence is slightly distorted so the
translation may not fit well it the surrounding con-
text.)
3 The Impact of the Reference
Translation
3.1 Bad Reference Translations
Figure 5 illustrates the impact of poor reference
translation on manual ranking as carried out in
Section 2.5.2. Of our six independent annotations,
three annotators marked the hypotheses as ?both
fine? given the match with the source and three
annotators marked them as ?both wrong? due to
the mismatch with the reference. Given the con-
struction of the WMT test set, this particular sen-
tence comes from a Spanish original and it was
most likely translated directly to both English and
Czech.
8
Correlation of
Source Target Reference vs. ?? others?
Spanish English 0.341
English French 0.164
French English 0.098
German English 0.088
Czech English -0.041
English Czech -0.145
English Spanish -0.411
English German -0.433
Overall -0.107
Table 10: Pearson?s correlation of the relative per-
centage of blocks where the reference was in-
cluded in the ranking and the final ?? others?
of the system (the reference itself is not included
among the considered systems).
 
25
 
30
 
35
 
40
 
45
 
50
 
55
 
60
 
65
 
70
 
75  0.1
9
 
0.2
 
0.21
 
0.22
 
0.23
 
0.24
 
0.25
 
0.26
>= Others
Rela
tive 
pres
ence
 of th
e ref
eren
ce
cmu
-hea
field
-com
bo
cu-
zem
an
dfki
fbk
jhu
kit
koc
koc-
com
bo
lims
i
liu
onlin
eA
onlin
eB
rwth
rwth
-com
bo sfu
uedi
n
upps
ala
upv-
com
bo
a*x
+b
Figure 6: Correlation of the presence of the ref-
erence and the official ?? others? for English-
German evaluation.
3.2 Reference Can Skew Pairwise
Comparisons
The exact set of competing systems in each 5-fold
ranking in WMT10 evaluation is random. The ??
others? however is affected by this: a system may
suffer more losses if often compared to the refer-
ence, and similarly it may benefit from being com-
pared to a poor competitor.
To check this, we calculate the correlation be-
tween the relative presence of the reference among
the blocks where a system was judged and the
system?s official ?? others? score. Across lan-
guage, there is almost no correlation (Pearson?s
coefficient: ?0.107). However, for some language
pairs, the correlation is apparent, as listed in Ta-
ble 10. Negative correlation means: the more of-
ten the system was compared along with the refer-
ence, the worse the score of the system.
Figure 6 plots the extreme case of English-
German evaluation.
Source Target Min Avg?StdDev Max
English Czech 40 65?19 115
English French 40 66?17 110
English German 10 40?16 80
English Spanish 30 54?15 85
Czech English 5 38?13 60
French English 5 37?15 70
German English 10 32?12 65
Spanish English 35 56?11 70
Table 11: The number of post-edits per system for
each language pair to complement Figure 3 (page
12) of the WMT10 overview paper.
4 Other WMT10 Tasks
4.1 Blind Post-Editing Unreliable
WMT often carries out one more type of manual
evaluation: ?Editing the output of systems without
displaying the source or a reference translation,
and then later judging whether edited translations
were correct.? (Callison-Burch et al, 2010). We
call the evaluation ?blind post-editing? for short.
We feel that blind post-editing is more infor-
mative than system ranking. First, it constitutes
a unique comprehensibility test, and after all, MT
should aim at comprehensible output in the first
place. Second, blind post-editing can be further
analyzed to search for specific errors in system
output, see Bojar (2011) for a preliminary study.
Unfortunately, the amount of post-edits col-
lected in WMT10 varied a lot across systems and
language pairs. Table 11 provides the minimum,
average and maximum number of post-edits of
outputs of a particular MT system. We see that
e.g. while English-to-Czech has many judgments
of this kind per system, Czech-to-English is one of
the worst supported directions.
It is not surprising that conclusions based on 5
observations can be extremely deceiving. For in-
stance CU-BOJAR seems to produce 60% of out-
puts comprehensible (and thus wins in Figure 3 on
page 12 in the WMT overview paper), far better
than CMU. This is not in line with the ranking re-
sults where both rank equally (Table 5 on page 10
in the WMT overview paper). In fact, CU-BOJAR
was post-edited 5 times and 3 of these post-edits
were acceptable while CMU was post-edited 30
times and 5 of these post-edits were acceptable.
4.2 A Remark on System Combination Task
One results of WMT10 not observed in previous
years was that system combinations indeed per-
formed better than individual systems. Previous
9
Dev Set Test Set
Sententes 455 2034 Diff
GOOGLE 17.32?1.25 16.76?0.60 ?
BOJAR 16.00?1.15 16.90?0.61 ?
TECTOMT 11.48?1.04 13.19?0.58 ?
PC-TRANS 10.24?0.92 10.84?0.46 ?
EUROTRAN 9.64?0.92 11.04?0.48 ?
Table 12: BLEU scores of sample five systems in
English-to-Czech combination task.
years failed to show this clearly, because Google
Translate used to be included among the combined
systems, making it hard to improve. In WMT10,
Google Translate was excluded from system com-
bination task (except for translations involving
Czech, where it was accidentally included).
Our Table 12 provides an additional explanation
why the presence of Google among combined sys-
tems leads to inconclusive results. While the test
set was easier (based on BLEU) than the develop-
ment set for most systems, it was much harder for
Google. All system combinations were thus likely
to overfit and select Google n-grams most often.
Without access to Google powerful language mod-
els, the combination systems were likely to under-
perform Google in final fluency of the output.
5 Further Issues of Manual Evaluation
We have already seen that the comprehensibility
test by blind post-editing provides a different pic-
ture of the systems than the official ranking. Berka
et al (2011) introduced a third ?quiz-based evalu-
ation?. The quiz-like evaluation used the English-
to-Czech WMT10 systems, applied to different
texts: short text snippets were translated and an-
notators were asked to answer three yes/no ques-
tions complementing each snippet. The order of
the systems was rather different from the official
WMT10 results: CU-TECTO won the quiz-based
evaluation despite being the fourth in WMT10.
Because the texts were different in WMT10 and
the quiz-based evaluation, we asked a small group
of annotators to apply the ranking technique on the
text snippets. While not exactly comparable to the
WMT10 ranking, the WMT10 ranking was con-
firmed: CU-TECTO was again among the lowest-
scoring systems and Google won the ranking.
Bojar (2011) applies the error-flagging manual
evaluation by Vilar et al (2006) to four systems
of WMT09 English-to-Czech task. Again, the
overall order of the systems is somewhat differ-
ent when ranked by the number of errors flagged.
Mireia Farru?s and Fonollosa (2010) use a coarser
but linguistically motivated error classification for
Catalan-Spanish and suggest that differences in
ranking are caused by annotators treating some
types of errors as more serious.
In short, different types of manual evaluations
lead to different results even when identical sys-
tems and texts are evaluated.
6 Conclusion
We took a deeper look at the results of the WMT10
manual evaluation, and based on our observations,
we have some recommendations for future evalu-
ations:
? We propose to use a score which ignores
ties instead of the official ?? others? metric
which rewards ties and ?> others? which pe-
nalizes ties. Another score, ?? all in block?,
could help identify which systems are more
dominant.
? Inter-annotator agreement decreases dramat-
ically with sentence length; we recommend
including fewer sentences per block, at least
for longer sentences.
? We suggest agreement be measured based on
an empirical estimate of P (E), or at least us-
ing a more correct random clicking P (E) =
0.36.
? There is evidence of a negative correlation
between the number of times a system is
judged and its score; we recommend a deeper
analysis of this issue.
? We recommend the reference be sampled at
a lower rate than other systems, so as to play
a smaller role in the evaluation. We also rec-
ommend better quality control over the pro-
duction of the references.
And to the readers of the WMT overview paper,
we point out:
? Pairwise comparisons derived from 5-fold
rankings are sometimes unreliable. Even a
targeted pairwise comparison of two systems
can shed little light as to which is superior.
? The acceptability of post-edits is sometimes
very unreliable due to the low number of ob-
servations.
10
References
R. Artstein and M. Poesio. 2008. Inter-coder agree-
ment for computational linguistics. Computational
Linguistics, 34(4):555?596.
John J. Bartko and William T. Carpenter. 1976. On the
methods and theory of reliability. Journal of Ner-
vous and Mental Disease, 163(5):307?317.
E. M. Bennett, R. Alpert, and A. C. Goldstein. 1954.
Communications through limited questioning. Pub-
lic Opinion Quarterly, 18(3):303?308.
Jan Berka, Martin C?erny?, and Ondr?ej Bojar. 2011.
Quiz-Based Evaluation of Machine Translation.
Prague Bulletin of Mathematical Linguistics, 95:77?
86, March.
Ondr?ej Bojar. 2011. Analyzing Error Types in
English-Czech Machine Translation. Prague Bul-
letin of Mathematical Linguistics, 95:63?76, March.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, pages 17?53, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20(1):37?46.
Barbara Di Eugenio and Michael Glass. 2004. The
kappa statistic: A second look. Computational lin-
guistics, 30(1):95?101.
J. L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378?382.
Klaus Krippendorff. 1980. Content Analysis: An In-
troduction to Its Methodology. Sage Publications,
Beverly Hills, CA. Chapter 12.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33:159?174.
Jose? B. Marin?o Mireia Farru?s, Marta R. Costa-jussa`
and Jose? A. R. Fonollosa. 2010. Linguistic-based
evaluation criteria to identify statistical machine
translation errors. In Proceedings of the 14th Annual
Conference of the Euoropean Association for Ma-
chine Translation (EAMT?10), pages 167?173, May.
William A. Scott. 1955. Reliability of content analy-
sis: The case of nominal scale coding. Public Opin-
ion Quarterly, 19(3):321?325.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-
mann Ney. 2006. Error Analysis of Machine Trans-
lation Output. In International Conference on Lan-
guage Resources and Evaluation, pages 697?702,
Genoa, Italy, May.
11
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 92?98,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Approximating a Deep-Syntactic Metric for MT Evaluation and Tuning?
Matous? Macha?c?ek and Ondr?ej Bojar
Charles University in Prague
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostranske? na?me?st?? 25, Prague
{bojar,machacek}@ufal.mff.cuni.cz
Abstract
SemPOS is an automatic metric of machine
translation quality for Czech and English fo-
cused on content words. It correlates well
with human judgments but it is computation-
ally costly and hard to adapt to other lan-
guages because it relies on a deep-syntactic
analysis of the system output and the refer-
ence. To remedy this, we attempt at approxi-
mating SemPOS using only tagger output and
a few heuristics. At a little expense in corre-
lation to human judgments, we can evaluate
MT systems much faster. Additionally, we de-
scribe our submission to the Tunable Metrics
Task in WMT11.
1 Introduction
SemPOS metric for machine translation quality was
introduced by Kos and Bojar (2009). It is inspired
by a set of metrics relying on various linguistic fea-
tures on syntactic and semantic level introduced by
Gime?nez and Ma?rquez (2007). One of their best
performing metrics was Semantic role overlapping:
the candidate and the reference translation are rep-
resented as bags of words and their semantic roles.
The similarity between the candidate and the refer-
ence is calculated using a general similarity measure
called Overlapping. The formal definition may be
found in Section 4.
? This work has been supported by the grants EuroMa-
trixPlus (FP7-ICT-2007-3-231720 of the EU and 7E09003 of
the Czech Republic), P406/10/P259, P406/11/1499, and MSM
0021620838.
Instead of semantic role labels (not available for
Czech), Kos and Bojar (2009) use TectoMT frame-
work (Z?abokrtsky? et al, 2008) to assign a seman-
tic part of speech defined by Sgall et al (1986). In
addition they use t-lemmas (deep-syntactic lemmas)
instead of surface word forms, which most impor-
tantly means that the metric considers content words
only. In the following, we will use ?sempos? to de-
note the semantic part of speech and ?SemPOS? to
denote the whole metric by Kos and Bojar (2009).
SemPOS correlates well with human judgments
on system level, see Section 2 for a brief summary
of how the correlation is computed. The main draw-
back of SemPOS is its computational cost because
it requires full parsing up to the deep syntactic level
to obtain t-lemmas and semposes. In Section 3 we
propose four methods which approximate t-lemmas
and semposes without the deep syntactic analysis.
These methods require only part-of-speech tagging
and therefore they are not only faster but also eas-
ier to adapt for other languages, not requiring more
advanced linguistic tools.
Gime?nez and Ma?rquez (2007) and Bojar et al
(2010) used different formulas to calculate the final
overlapping.1 In Section 4, we examine variations
of the formula, adding one version of our own.
By combining one of the approximation tech-
niques with one of the overlapping formulas, we ob-
1In fact, Gime?nez and Ma?rquez (2007) released two versions
of the paper. Both of them are nearly identical except for the
formula for overlapping, so we asked the authors which of the
two versions is correct. It turns out that Bojar et al (2010),
unaware of the second version of the paper, used the wrong one
but still obtained good results. We therefore (re-)examine both
versions.
92
Workshop Filename Sentences To English from To Czech from
WMT08 test2008 2000 de, es, fr ?
WMT08 nc-test2008 2028 cs en
WMT08 newstest2008 2051 cs, de, es, fr en
WMT09 newstest2009 2525 cs, de, es, fr en
WMT10 newssyscombtest2010 2034 cs, de, es, fr en
Table 1: Datasets used to evaluate the correlation with human judgments. For example: the testset ?test2008? was
used for translation to English from German, Spanish and French and it was not used for any translation to Czech.
tain a variant of our metric. The performance of the
individual variants is reported in Section 5.
Section 6 is devoted to our submission to the Tun-
able Metrics shared task of the Sixth Workshop on
Statistical Machine Translation (WMT11).
2 Method of Evaluation
Our primary objective is to create a good metric
for automatic MT evaluation and possibly also tun-
ing. We are not interested much in how close is our
proposed approximation to the (automatic or man-
ual) semposes and t-lemmas. Therefore, we evaluate
only how well do our metrics (the pair of a chosen
approximation and a chosen formula for the overlap-
ping) correlate with human judgments.
2.1 Test Data
We use the data collected during three Workshops on
Statistical Machine Translation: WMT08 (Callison-
Burch et al, 2008), WMT09 (Callison-Burch et al,
2009) and WMT10 (Callison-Burch et al, 2010). So
far, we study only Czech and English as the target
languages. Our test sets are summarized in Table 1:
we have four sets with Czech as the target language
and 16 sets with English as the target language.
Each testset in each translation direction gives us
for each sentence one hypothesis for each participat-
ing MT system. Human judges (repeatedly) ranked
subsets of these hypotheses comparing at most 5 hy-
potheses at once and indicating some ordering of
the hypotheses. The ordering may include ties. In
WMT, these 5-fold rankings are interpreted as ?sim-
ulated pairwise comparisons?: all pairwise compar-
isons are extracted from each ranking. The HUMAN
SCORE for each system is then the percentage of
pairs where the system was ranked better or equal
to its competitor.
2.2 Correlation with Human Judgments
For each metric we examine, the correlation to hu-
man judgments is calculated as follows: given one
of the test sets (the hypotheses and reference transla-
tions), the examined metric provides a single-figure
score for each system. We use Spearman?s rank cor-
relation coefficient between the human scores and
the scores of the given metric to see how well the
metric matches human judgments. Because tied
ranks do not exist, the correlation coefficient is given
by:
? = 1?
6
?
i
(pi ? qi)2
n(n2 ? 1)
(1)
Human scores across different test sets are not
comparable, so we compute correlations for each
test set separately and average them.
3 Approximations of SemPOS
We would like to obtain t-lemmas and semantic parts
of speech without deep syntactic analysis, assuming
only automatic tagging and lemmatization.
Except for one option (Section 3.4), we approxi-
mate t-lemmas simply by surface lemmas. For the
majority of content words, this works perfectly, but
there are several regular classes of words where the
t-lemma differs. In such cases, the t-lemma usu-
ally consists of the lemma of the main content word
and an auxiliary word that significantly changes the
meaning of the content word. These are e.g. English
phrasal verbs (?blow up? should have the t-lemma
?blow up?) and Czech reflexive verbs (?sma?t se?).
The approximation of semantic part of speech de-
serves at least some minimal treatment. The follow-
ing sections describe four variations of the approxi-
mation.
93
Morph. Tag Sempos Rel. Freq.
NN n.denot 0.989
VBZ v 0.766
VBN v 0.953
JJ adj.denot 0.975
NNP n.denot 0.999
PRP n.pron.def.pers 0.999
VB v 0.875
VBP v 0.663
VBD v 0.810
WP n.pron.indef 1.000
NNS n.denot 0.996
JJR adj.denot 0.813
Table 2: A sample of the mapping from English morpho-
logical tags to semposes, including the relative frequency,
e.g. count(NN,n.denot)count(NN) .
3.1 Sempos from Tag
We noticed that the morphological tag determines
almost uniquely the semantic part of speech. We use
the Czech-English sentence-parallel corpus CzEng
(Bojar and Z?abokrtsky?, 2009) to create a simple dic-
tionary which maps morphological tags to most fre-
quent semantic parts of speech. Some morpholog-
ical tags belong almost always to auxiliary words
which do not have a corresponding deep-syntactic
node at all, so the t-lemma and sempos are not de-
fined for them. We include these morphological tags
in the dictionary and map them to a special sempos
value ?-?. Ultimately, words with such sempos are
not included in the overlapping at all.
Table 2 shows a sample of this dictionary. The
high relative frequencies indicate that we are not los-
ing too much of the accuracy: overall 93.6 % for
English and 88.4 % for Czech on CzEng e-test.
The first approximation relies just on this
(language-specific) dictionary. The input text is au-
tomatically tagged, the morphological tags are de-
terministically mapped to semposes using the dictio-
nary and words where the mapping led to the special
value of ?-? are removed.
In the following, we label this method as APPROX.
3.2 Exclude Stop-Words
By definition, the deep syntactic layer we use repre-
sents more or less only content words. Most aux-
iliary words become only attributes of the deep-
syntactic nodes and play no role in the overlapping
between the hypothesis and the reference.
Our first approximation technique (Section 3.1)
identifies auxiliary words only on the basis of the
morphological tag. We attempt to refine the re-
call by excluding a certain number of most frequent
words in each language. The frequency list was ob-
tained from the Czech and English sides of the cor-
pus CzEng. We choose the exact cut-off for stop-
words in each language separately: 100 words in
English and 220 words in Czech. See Section 5.1
below.
In the following, the method is called APPROX-
STOPWORDS.
3.3 Restricting the Set of Examined Semposes
We noticed that the contribution of each sempos to
the overall performance of the metric in terms of cor-
relation to human judgments can differ a lot. One
of the underlying reasons may be e.g. greater or
lower tagging accuracy of certain word classes, an-
other reason may be that translation errors in certain
word classes may be more relevant for human judges
of MT quality.
Tables 3 and 4 report the correlation to human
judgments if only words in a given sempos are con-
sidered in the overlapping. Based on these obser-
vations, we assume that some sempos types raise
the correlation of the overlapping with human judg-
ments and some lower it. We therefore try one more
variant of the approximation which considers only
(language-specific) subset of semposes.
The approximation called APPROX-RESTR con-
siders only these sempos tags in Czech: v, n.denot,
adj.denot, n.pron.def.pers, n.pron.def.demon, adv.-
denot.ngrad.nneg, adv.denot.grad.nneg. The consid-
ered sempos tags for English are: v, n.denot, adj.-
denot, n.pron.indef.
3.4 T-lemma and Sempos Tagging
Our last approximation method differs a lot from the
previous three approximations. We use the sequence
labeling algorithm (Collins, 2002) as implemented
in Featurama2 to choose the t-lemma and sempos
tag. The CzEng corpus (Bojar and Z?abokrtsky?,
2009) serves to train two taggers: one for Czech and
2http://sourceforge.net/projects/
featurama/
94
Tag R. Fr. Min. Max. Avg.
v 0.236 0.403 1.000 0.735
n.denot 0.506 0.189 1.000 0.728
adj.denot 0.124 0.264 0.964 0.720
n.pron.indef 0.019 0.224 1.000 0.639
n.quant.def 0.039 -0.084 0.893 0.495
n.pron.def.pers 0.068 -0.500 0.975 0.493
adv.pron.indef 0.005 -0.382 1.000 0.432
adv.denot.grad.neg 0.003 -1.000 0.904 0.413
Table 3: English semposes and their performance in
terms of correlation with human judgments if only words
of the given sempos in APPROX are checked for match
with the reference. Averaged across all testsets. Over-
lapping CAP is used, see Section 4 below. Column R. Fr.
reports relative frequency of each sempos in the testsets.
Tag R. Fr. Min. Max. Avg.
n.pron.def.pers 0.030 0.406 0.800 0.680
n.pron.def.demon 0.026 0.308 1.000 0.651
adj.denot 0.156 0.143 0.874 0.554
adv.denot.ngrad.nneg 0.047 0.291 0.800 0.451
adv.denot.grad.nneg 0.001 0.219 0.632 0.445
adj.quant.def 0.004 -0.029 0.800 0.393
n.denot.neg 0.037 0.029 0.736 0.391
adv.denot.grad.neg 0.018 -0.371 0.800 0.313
n.denot 0.432 -0.200 0.720 0.280
adv.pron.def 0.000 -0.185 0.894 0.262
adj.pron.def.demon 0.000 0.018 0.632 0.241
n.pron.indef 0.027 -0.200 0.423 0.112
adj.quant.grad 0.006 -0.225 0.316 0.079
v 0.180 -0.600 0.706 0.076
adj.quant.indef 0.002 -0.105 0.200 0.052
adv.denot.ngrad.neg 0.000 -0.883 0.775 0.000
n.quant.def 0.000 -0.800 0.713 -0.085
Table 4: Czech semposes. See Table 3 for explanation.
one for English. At each token, each of the taggers
uses the word form, morphological tag and surface
lemma (of the current and the previous two tokens)
to choose one pair of t-lemma and sempos tag from
a given set.
The set of possible t-lemma and sempos pairs is
created as follows. At first the sempos set is ob-
tained. We simply use all semposes being seen with
the given morphological tag in the corpus. Then we
find possible t-lemmas for each sempos. For most
semposes we consider surface lemma as the only
t-lemma. For the sempos tag ?v? we also add t-
lemmas composed of the surface lemma and some
auxiliary word present in the sentence (?blow up?,
?sma?t se?). For some other sempos tags we add spe-
cial t-lemmas for negation and personal pronouns
(?#Neg?, ?#PersPron?).
The overall accuracy of the tagger on the e-test is
97.9 % for English and 94.9 % for Czech, a better re-
sult on a harder task (t-lemmas also predicted) than
the deterministic tagging in Section 3.1.
We call this approximation method TAGGER.
4 Variations of Overlapping
The original Overlapping defined by Gime?nez and
Ma?rquez (2007) is given in Equations 2 and 3:
O(t) =
?
w?ri
cnt(w, t, ci)
?
w?ri?ci
max(cnt(w, t, ri), cnt(w, t, ci))
(2)
where ci and ri denotes the candidate and refer-
ence translation of sentence i and cnt(w, t, s) de-
notes number of times t-lemma w of type (sempos)
t appears in sentence s. For each sempos type t,
Overlapping O(t) calculates the proportion of cor-
rectly translated items of type t. In this paper we
will call this overlapping BOOST.
Equation 3 describes Overlapping of all types:
O(?) =
?
t?T
?
w?ri
cnt(w, t, ci)
?
t?T
?
w?ri?ci
max(cnt(w, t, ri), cnt(w, t, ci))
(3)
where T denotes the set of all sempos types. We
will call this Overlapping BOOST-MICRO because it
micro-averages the overlappings of individual sem-
pos types.
Kos and Bojar (2009) used a slightly different
Overlapping formula, denoted CAP in this paper:
O(t) =
?
w?ri
min(cnt(w, t, ri), cnt(w, t, ci))
?
w?ri
cnt(w, t, ri)
(4)
To calculate Overlapping of all types, Kos and
Bojar (2009) used ordinary macro-averaging. We
call the method CAP-MACRO:
O(?) =
1
|T |
?
t?T
O(t) (5)
The difference between micro- and macro-
average is that in macro-average all types have
95
Reduction Overlapping Min. Max. Avg.
approx cap-micro 0.409 1.000 0.804
orig cap-macro 0.536 1.000 0.801
approx cap-macro 0.420 1.000 0.799
approx-restr cap-macro 0.476 1.000 0.798
tagger cap-micro 0.409 1.000 0.790
orig cap-micro 0.391 1.000 0.784
approx-restr cap-micro 0.391 1.000 0.782
approx-stopwords cap-micro 0.391 1.000 0.754
sempos-bleu 0.374 1.000 0.754
approx-stopwords cap-macro 0.280 1.000 0.724
tagger boost-micro 0.306 1.000 0.717
orig boost-micro 0.324 1.000 0.711
approx-stopwords boost-micro 0.133 1.000 0.697
approx-restr boost-micro 0.126 1.000 0.688
approx boost-micro 0.224 1.000 0.686
tagger cap-macro 0.118 1.000 0.669
bleu -0.143 1.000 0.628
Table 5: Metric correlations for English as a target lan-
guage
the same weight regardless of count. For exam-
ple O(n.denot) and O(adv.denot.grad.nneg) would
have the same weight, however there are many
more items of type n.denot than items of type
adv.denot.grad.nneg (see Tables 3 and 4). We con-
sider this unnatural and we suggest a new Overlap-
ping formula CAP-MICRO:
O(?) =
?
t?T
?
w?ri
min(cnt(w, t, ri), cnt(w, t, ci))
?
t?T
?
w?ri
cnt(w, t, ri)
(6)
In sum, we have three Overlappings which should
be evaluated: BOOST-MICRO (Equation 3), CAP-
MACRO (Equation 5), and CAP-MICRO (Equation 6).
5 Experiments
Table 5 shows the results for English as the target
language. The first two columns denote the combi-
nation of an approximation method and an overlap-
ping formula. For conciseness, we report only the
minimum, maximum and average value among cor-
relations of all test sets.
To compare metrics to original SemPOS, the ta-
ble includes non-approximated variant ORIG where
the t-lemmas and semposes are assigned by the Tec-
toMT framework. For the purposes of compari-
son, we also report the correlations of BLEU (Pa-
pineni et al, 2002) and a linear combination of AP-
Reduction Overlapping Min. Max. Avg.
approx-restr cap-macro 0.400 0.800 0.608
tagger cap-macro 0.143 0.800 0.428
orig cap-macro 0.143 0.800 0.423
approx-restr cap-micro 0.086 0.769 0.413
tagger cap-micro 0.086 0.769 0.413
orig cap-micro 0.086 0.741 0.406
approx-stopwords cap-micro 0.086 0.790 0.368
approx cap-micro 0.086 0.734 0.354
approx-stopwords cap-macro 0.086 0.503 0.347
sempos-bleu 0.086 0.676 0.340
approx cap-macro 0.086 0.469 0.338
tagger boost-micro 0.086 0.664 0.337
bleu 0.029 0.490 0.279
orig boost-micro -0.200 0.692 0.273
approx-stopwords boost-micro -0.200 0.685 0.271
approx boost-micro -0.200 0.664 0.266
approx-restr boost-micro -0.200 0.664 0.266
Table 6: Metric correlations for Czech as a target lan-
guage
PROX+CAP-MICRO and BLEU (even weights) under
the name SEMPOS-BLEU since this metric was used
in Tunable Metric Task (Section 6).
The best performing metric is the combination
of approximation APPROX and overlapping CAP-
MICRO. It actually slightly outperforms all non-
approximated metrics. In general, the reductions
APPROX and ORIG combined with CAP-MICRO
or CAP-MACRO perform very well. Reductions
APPROX-STOPWORDS and APPROX-RESTR do not
improve on APPROX.
The TAGGER approximation correlates similarly
to ORIG when micro-average is used.
Table 6 contains the results for Czech as the target
language. The best performing metric for Czech is
APPROX-RESTR together with CAP-MACRO. In gen-
eral approximation APPROX-RESTR is better than
APPROX-STOPWORDS which is slightly better than
APPROX.
The success of overlapping CAP-MACRO in Czech
is due to the higher contribution of less frequent
semposes to the overall correlation. While in En-
glish the best correlating semposes are also very fre-
quent (Table 3), this does not hold for Czech (Ta-
ble 4). The underlying reasons have yet to be ex-
plained.
In both languages, the overlapping BOOST-
MICRO has a very low correlation. We therefore
consider this overlapping not suitable for any met-
96
 
0.62
 
0.64
 
0.66
 
0.68 0.7
 
0.72
 
0.74
 
0.76
 
0.78 0.8
 
0.82
 
0
 
50
 
100
 
150
 
200
 
250
 
300
cap-
micr
o
cap-
mac
ro
boo
st-m
icro
Figure 1: Correlation vs. the number of most frequent
words which are thrown away for English. The big drop
for lengths 109 and 110 is caused by the words ?who? and
?how?.
ric based on semposes.
On the other hand, most of the examined com-
binations are on average better than the baseline
BLEU, sometimes by a very wide margin.
5.1 Dependency of Correlation on Stopwords
List Length
We tried various stopwords list lengths for the
approximation APPROX-STOPWORDS. Figure 5.1
shows the dependency of the correlation on stop-
words list length for all overlappings in English. We
see that the best correlation arises when no words
are thrown away. One possible explanation is that
auxiliary words are recognized by the morphologi-
cal tag well enough anyway and stopwords lists re-
move also important content words, decreasing the
overall accuracy of the overlapping.
6 Tunable Metric WMT11 Shared Task
The goal of the tunable metric task in WMT11 was
to use the custom metric in MERT optimization
(Och, 2003). The target language was English. We
choose APPROX + CAP-MICRO since this combina-
tion correlates best with human judgments.
Based on the experience of Bojar and Kos (2010),
we combine this metric with BLEU. In our opin-
ion, the SemPOS metric and its variants alone are
are good at comparing systems? outputs where sen-
tence fluency has been already ensured. On the other
hand, they fail in ranking sentences in n-best lists
Weights Devset scores
BLEU APPROX BLEU APPROX
1 0 0.246 0.546
0.75 0.25 0.242 0.584
0.5 0.5 0.229 0.594
0.25 0.75 0.215 0.602
0 1 0.025 0.631
Table 7: Results of MERT optimization. The last two
columns contain metric scores of the last iteration of the
MERT process with given combination weights.
in MERT optimization because they observe only
t-lemmas and don?t penalize wrong morphological
forms of words. We thus use BLEU to establish
sentence fluency and our metrics to prefer sentences
with correctly translated content words.
We have tried several weights for the linear com-
bination of BLEU and the chosen approximation.
See Table 7 for details. We have submitted the vari-
ant with equal weights.
The preliminary results of manual evaluation (see
the WMT11 overview paper) indicate that our sys-
tem is fairly distinct from others: we won under the
?> others? metric but we were the fifth of 8 systems
in the official ?? others? (the percentage of pairs
where the system was ranked better or equal to its
competitor).
7 Conclusions
We have introduced and evaluated several approx-
imations of a deep-syntactic MT evaluation metric
SEMPOS. This allows us to reduce the computa-
tional load by far, use only shallow tagging and still
reach reasonable correlation scores.
For English, our combination of APPROX and
CAP-MICRO performs even marginally better than
the original SEMPOS. For Czech, it is APPROX-
RESTR and TAGGER approximations with CAP-
MACRO that outperform the original SEMPOS.
The applicability of these metrics (in link with
BLEU) in model optimization was confirmed by
the manual judgments for the Tunable Metrics Task.
Our submission was surprisingly different from oth-
ers: the best one in the score excluding ties and
mediocre in the score where ties are rewarded.
97
References
Ondrej Bojar and Kamil Kos. 2010. 2010 Failures in
English-Czech Phrase-Based MT. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 60?66, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Ondr?ej Bojar and Zdene?k Z?abokrtsky?. 2009. CzEng0.9:
Large Parallel Treebank with Rich Annotation.
Prague Bulletin of Mathematical Linguistics, 92. in
print.
Ondr?ej Bojar, Kamil Kos, and David Marec?ek. 2010.
Tackling Sparse Data Issue in Machine Translation
Evaluation. In Proceedings of the ACL 2010 Con-
ference Short Papers, pages 86?91, Uppsala, Sweden,
July. Association for Computational Linguistics.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 70?106, Columbus, Ohio, June.
Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53, Uppsala, Sweden, July. Association for
Computational Linguistics. Revised August 2010.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In EMNLP ?02:
Proceedings of the ACL-02 conference on Empirical
methods in natural language processing, pages 1?8,
Morristown, NJ, USA. Association for Computational
Linguistics.
Jesu?s Gime?nez and Llu??s Ma?rquez. 2007. Linguistic
Features for Automatic Evaluation of Heterogenous
MT Systems. In Proceedings of the Second Work-
shop on Statistical Machine Translation, pages 256?
264, Prague, June. Association for Computational Lin-
guistics.
Kamil Kos and Ondr?ej Bojar. 2009. Evaluation of Ma-
chine Translation Metrics for Czech as the Target Lan-
guage. Prague Bulletin of Mathematical Linguistics,
92.
Franz Josef Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proc. of the Asso-
ciation for Computational Linguistics, Sapporo, Japan,
July 6-7.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In ACL 2002, Proceed-
ings of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 311?318, Philadel-
phia, Pennsylvania.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The Meaning of the Sentence and Its Semantic and
Pragmatic Aspects. Academia/Reidel Publishing
Company, Prague, Czech Republic/Dordrecht, Nether-
lands.
Zdene?k Z?abokrtsky?, Jan Pta?c?ek, and Petr Pajas. 2008.
TectoMT: Highly modular MT system with tectogram-
matics used as transfer layer. In ACL 2008 WMT: Pro-
ceedings of the Third Workshop on Statistical Machine
Translation, pages 167?170, Columbus, OH, USA.
Association for Computational Linguistics.
98
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 330?336,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Improving Translation Model by Monolingual Data?
Ondr?ej Bojar and Ales? Tamchyna
bojar@ufal.mff.cuni.cz, a.tamchyna@gmail.com
Institute of Formal and Applied Linguistics,
Faculty of Mathematics and Physics, Charles University in Prague
Abstract
We use target-side monolingual data to ex-
tend the vocabulary of the translation model
in statistical machine translation. This method
called ?reverse self-training? improves the de-
coder?s ability to produce grammatically cor-
rect translations into languages with morphol-
ogy richer than the source language esp. in
small-data setting. We empirically evalu-
ate the gains for several pairs of European
languages and discuss some approaches of
the underlying back-off techniques needed to
translate unseen forms of known words. We
also provide a description of the systems we
submitted to WMT11 Shared Task.
1 Introduction
Like any other statistical NLP task, SMT relies on
sizable language data for training. However the par-
allel data required for MT are a very scarce resource,
making it difficult to train MT systems of decent
quality. On the other hand, it is usually possible to
obtain large amounts of monolingual data.
In this paper, we attempt to make use of the
monolingual data to reduce the sparseness of surface
forms, an issue typical for morphologically rich lan-
guages. When MT systems translate into such lan-
guages, the limited size of parallel data often causes
the situation where the output should include a word
form never observed in the training data. Even
though the parallel data do contain the desired word
? This work has been supported by the grants EuroMatrix-
Plus (FP7-ICT-2007-3-231720 of the EU and 7E09003 of the
Czech Republic), P406/10/P259, and MSM 0021620838.
in other forms, a standard phrase-based decoder has
no way of using it to generate the correct translation.
Reverse self-training addresses this problem by
incorporating the available monolingual data in the
translation model. This paper builds upon the idea
outlined in Bojar and Tamchyna (2011), describing
how this technique was incorporated in the WMT
Shared Task and extending the experimental evalu-
ation of reverse self-training in several directions ?
the examined language pairs (Section 4.2), data size
(Section 4.3) and back-off techniques (Section 4.4).
2 Related Work
The idea of using monolingual data for improving
the translation model has been explored in several
previous works. Bertoldi and Federico (2009) used
monolingual data for adapting existing translation
models to translation of data from different domains.
In their experiments, the most effective approach
was to train a new translation model from ?fake?
parallel data consisting of target-side monolingual
data and their machine translation into the source
language by a baseline system.
Ueffing et al (2007) used a boot-strapping tech-
nique to extend translation models using mono-
lingual data. They gradually translated additional
source-side sentences and selectively incorporated
them and their translations in the model.
Our technique also bears a similarity to de Gis-
pert et al (2005), in that we try to use a back-off
for surface forms to generalize our model and pro-
duce translations with word forms never seen in the
original parallel data. However, instead of a rule-
based approach, we take advantage of the available
330
Source English Target Czech Czech Lemmatized
Parallel (small) a cat chased. . . = koc?ka honila. . . koc?ka honit. . .
I saw a cat = vide?l jsem koc?ku vide?t by?t koc?ka
I read about a dog = c?etl jsem o psovi c???st by?t o pes
Monolingual (large) ? c?etl jsem o koc?ce c???st by?t o koc?ka
I read about a cat ? Use reverse translation backed-off by lemmas.
Figure 1: The essence of reverse self-training: a new phrase pair (?about a cat? = ?o koc?ce?) is learned based on a
small parallel corpus and large target-side monolingual texts.
data and learn these forms statistically. We are there-
fore not limited to verbs, but our system is only able
to generate surface forms observed in the target-side
monolingual data.
3 Reverse Self-Training
Figure 1 illustrates the core of the method. Using
available parallel data, we first train an MT system
to translate from the target to the source language.
Since we want to gather new word forms from the
monolingual data, this reverse model needs the abil-
ity to translate them. For that purpose we use a fac-
tored translation model (Koehn and Hoang, 2007)
with two alternative decoding paths: form?form
and back-off?form. We experimented with several
options for the back-off (simple stemming by trun-
cation or full lemmatization), see Section 4.4. The
decoder can thus use a less sparse representation of
words if their exact forms are not available in the
parallel data.
We use this reverse model to translate (much
larger) target-side monolingual data into the source
language. We preserve the word alignments of the
phrases as used in the decoding so we directly ob-
tain the word alignment in the new ?parallel? cor-
pus. This gives us enough information to proceed
with the standard MT system training ? we extract
and score the phrases consistent with the constructed
word alignment and create the phrase table.
We combine this enlarged translation model with
a model trained on the true parallel data and use
Minimum Error Rate Training (Och, 2003) to find
the balance between the two models. The final
model has four separate components ? two language
models (one trained on parallel and one on monolin-
gual data) and the two translation models.
We do not expect the translation quality to im-
prove simply because more data is included in train-
ing ? by adding translations generated using known
data, the model could gain only new combinations
of known words. However, by using a back-off
to less sparse units (e.g. lemmas) in the factored
target?source translation, we enable the decoder
to produce previously unseen surface forms. These
translations are then included in the model, reducing
the data sparseness of the target-side surface forms.
4 Experiments
We used common tools for phrase-based translation
? Moses (Koehn et al, 2007) decoder and tools,
SRILM (Stolcke, 2002) and KenLM (Heafield,
2011) for language modelling and GIZA++ (Och
and Ney, 2000) for word alignments.
For reverse self-training, we needed Moses to also
output word alignments between source sentences
and their translations. As we were not able to make
the existing version of this feature work, we added a
new option and re-implemented this funcionality.
We rely on automatic translation quality eval-
uation throughout our paper, namely the well-
established BLEU metric (Papineni et al, 2002). We
estimate 95% confidence bounds for the scores as
described in Koehn (2004). We evaluated our trans-
lations on lower-cased sentences.
4.1 Data Sources
Aside from the WMT 2011 Translation Task data,
we also used several additional data sources for the
experiments aimed at evaluating various aspects of
reverse self-training.
JRC-Acquis
We used the JRC-Acquis 3.0 corpus (Steinberger
et al, 2006) mainly because of the number of avail-
able languages. This corpus contains a large amount
331
Source Target Corpus Size (k sents) Vocabulary Size Ratio Baseline +Mono LM +Mono TM
Para Mono
English Czech 94 662 1.67 40.9?1.9 43.5?2.0 *44.3?2.0
English Finnish 123 863 2.81 27.0?1.9 27.6?1.8 28.3?1.7
English German 127 889 1.83 34.8?1.8 36.4?1.8 37.6?1.8
English Slovak 109 763 2.03 35.3?1.6 37.3?1.7 37.7?1.8
French Czech 95 665 1.43 39.9?1.9 42.5?1.8 43.1?1.8
French Finnish 125 875 2.45 26.7?1.8 27.8?1.7 28.3?1.8
French German 128 896 1.58 38.5?1.8 40.2?1.8 *40.5?1.8
German Czech 95 665 0.91 35.2?1.8 37.0?1.9 *37.3?1.9
Table 1: BLEU scores of European language pairs on JRC data. Asterisks in the last column mark experiments for
which MERT had to be re-run.
of legislative texts of the European Union. The fact
that all data in the corpus come from a single, very
narrow domain has two effects ? models trained on
this corpus perform mostly very well in that domain
(as documented e.g. in Koehn et al (2009)), but fail
when translating ordinary texts such as news or fic-
tion. Sentences in this corpus also tend to be rather
long (e.g. 30 words on average for English).
CzEng
CzEng 0.9 (Bojar and ?Zabokrtsky?, 2009) is a par-
allel richly annotated Czech-English corpus. It con-
tains roughly 8 million parallel sentences from a
variety of domains, including European regulations
(about 34% of tokens), fiction (15%), news (3%),
technical texts (10%) and unofficial movie subtitles
(27%). We do not make much use of the rich anno-
tation in this paper, however we did experiment with
using Czech lemmas (included in the annotation) as
the back-off factor for reverse self-training.
4.2 Comparison Across Languages
In order to determine how successful our approach
is across languages, we experimented with Czech,
Finnish, German and Slovak as target languages. All
of them have a rich morphology in some sense. We
limited our selection of source languages to English,
French and German because our method focuses on
the target language anyway. We did however com-
bine the languages with respect to the richness of
their vocabulary ? the source language has less word
forms in almost all cases.
Czech and Slovak are very close languages, shar-
ing a large portion of vocabulary and having a very
similar grammar. There are many inflectional rules
for verbs, nouns, adjectives, pronouns and numerals.
Sentence structure is exhibited by various agreement
rules which often apply over long distance. Most of
the issues commonly associated with rich morphol-
ogy are clearly observable in these languages.
German also has some inflection, albeit much less
complex. The main source of German vocabulary
size are the compound words. Finnish serves as an
example of agglutinative languages well-known for
the abundance of word forms.
Table 1 contains the summary of our experimen-
tal results. Here, only the JRC-Acquis corpus was
used for training, development and evaluation. For
every language pair, we extracted the first 10 per-
cent of the parallel corpus and used them as the par-
allel data. The last 70 percent of the same corpus
were our ?monolingual? data. We used a separate
set of 1000 sentences for the development and an-
other 1000 for testing.
Sentence counts of the corpora are shown in the
columns Corpus Size Para and Mono. The table
also shows the ratio between observed vocabulary
size of the target and source language. Except for
the German?Czech language pair, the ratios are
higher than 1. The Baseline column contains the
BLEU score of a system trained solely on the paral-
lel data (i.e. the first 10 percent). A 5-gram language
model was used. The ?+Mono LM? scores were
achieved by adding a 5-gram language model trained
on the monolingual data as a separate component
(its weight was determined by MERT). The last col-
umn contains the scores after adding the translation
model self-trained on target monolingual data. This
model was also added as another component and the
weights associated with it were found by MERT.
332
For the back-off in the reverse self-training, we
used a simple suffix-trimming heuristic suitable for
fusional languages: cut off the last three characters
of each word always keeping at least the first three
characters. This heuristic reduces the vocabulary
size to a half for Czech and Slovak but it is much
less effective for Finish and German (Table 2), as
can be expected from their linguistic properties.
Language Vocabulary reduced to (%)
Czech 52
Finnish 64
German 73
Slovak 51
Table 2: Reduction of vocabulary size by suffix trimming
We did not use any linguistic tools, such as mor-
phological analyzers, in this set of experiments. We
see the main point of this section in illustrating the
applicability of our technique on a wide range of lan-
guages, including languages for which such tools are
not available.
We encountered problems when using MERT to
balance the weights of the four model components.
Our model consisted of 14 features ? one for each
language model, five for each translation model
(phrase probability and lexical weight for both di-
rections and phrase penalty), word penalty and dis-
tortion penalty. The extra 5 weights of the reversely
trained translation model caused MERT to diverge in
some cases. Since we used the mert-moses.pl
script for tuning and kept the default parameters,
MERT ran for 25 iterations and stopped. As a result,
even though our method seemed to improve trans-
lation performance in most language pairs, several
experiments contradicted this observation. We sim-
ply reran the final tuning procedure in these cases
and were able to achieve an improvement in BLEU
as well. These language pairs are marked with a ?*?
sign in Table 1.
A possible explanation for this behaviour of
MERT is that the alternative decoding paths add a
lot of possible derivations that generate the same
string. To validate our hypothesis we examined a
diverging run of MERT for English?Czech transla-
tion with two translation models. Our n-best lists
contained the best 100 derivations for each trans-
Figure 2: Vocabulary ratio and BLEU score
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 0.8  1  1.2 1.4 1.6 1.8  2  2.2 2.4 2.6 2.8  3
G
ai
n 
in
 B
LE
U 
(ab
so
lut
e)
Vocabulary size ratio
en-cs
en-fi
en-de
en-sk
fr-cs
fr-fi
fr-dede-cs
lated sentence from the development data. On av-
erage (over all 1000 sentences and over all runs), the
n-best list only contained 6.13 different translations
of a sentence. The result of the same calculation
applied on the baseline run of MERT (which con-
verged in 9 iterations) was 34.85 hypotheses. This
clear disproportion shows that MERT had much less
information when optimizing our model.
Overall, reverse self-training seems helpful for
translating into morphologically rich languages. We
achieved promising gains in BLEU, even over the
baseline including a language model trained on the
monolingual data. The improvement ranges from
roughly 0.3 (e.g. German?Czech) to over 1 point
(English?German) absolute. This result also indi-
cates that suffix trimming is a quite robust heuristic,
useful for a variety of language types.
Figure 2 illustrates the relationship between vo-
cabulary size ratio of the language pair and the
improvement in translation quality. Although the
points are distributed quite irregularly, a certain ten-
dency towards higher gains with higher ratios is ob-
servable. We assume that reverse self-training is
most useful in cases where a single word form in the
source language can be translated as several forms in
the target language. A higher ratio between vocab-
ulary sizes suggests that these cases happen more
often, thus providing more space for improvement
using our method.
333
4.3 Data Sizes
We conducted a series of English-to-Czech experi-
ments with fixed parallel data and a varying size of
monolingual data. We used the CzEng corpus, 500
thousand parallel sentences and from 500 thousand
up to 5 million monolingual sentences. We used
two separate sets of 1000 sentences from CzEng for
development and evaluation. Our results are sum-
marized in Figure 3. The gains in BLEU become
more significant as the size of included monolingual
data increases. The highest improvement can be ob-
served when the data are largest ? over 3 points ab-
solute. Figure 4 shows an example of the impact on
translation quality ? the ?Mono? data are 5 million
sentences.
When evaluated from this point of view, our
method can also be seen as a way of considerably
improving translation quality for languages with lit-
tle available parallel data.
We also experimented with varying size of paral-
lel data (500 thousand to 5 million sentences) and its
effect on reverse self-training contribution. The size
of monolingual data was always 5 million sentences.
We first measured the percentage of test data word
forms covered by the training data. We calculated
the value for parallel data and for the combination of
parallel and monolingual data. For word forms that
appeared only in the monolingual data, a different
form of the word had to be contained in the parallel
data (so that the model can learn it through the back-
off heuristic) in order to be counted in. The differ-
ence between the first and second value can simply
be thought of as the upper-bound estimation of re-
verse self-training contribution. Figure 5 shows the
results along with BLEU scores achieved in transla-
tion experiments following this scenario.
Our technique has much greater effect for small
parallel data sizes; the amount of newly learned
word forms declines rapidly as the size grows.
Similarly, improvement in BLEU score decreases
quickly and becomes negligible around 2 million
parallel sentences.
4.4 Back-off Techniques
We experimented with several options for the back-
off factor in English?Czech translation. Data from
training section of CzEng were used, 1 million par-
Figure 3: Relation between monolingual data size and
gains in BLEU score
 26
 27
 28
 29
 30
 31
 32
 33
 0  1  2  3  4  5
BL
EU
Monolingual data size (millions of sentences)
Mono LM and TM
Mono LM
Figure 5: Varying parallel data size, surface form cov-
erage (?Parallel?, ?Parallel and Mono?) and BLEU score
(?Mono LM?, ?Mono LM and TM?)
 26
 28
 30
 32
 34
 36
 38
 0  0.5  1  1.5  2  2.5  3  3.5  4  4.5
 90
 92
 94
 96
 98
BL
EU
Co
ve
ra
ge
 o
f s
ur
fa
ce
 fo
rm
s 
(%
)
Parallel data size (millions of sentences)
Mono LM and TM
Mono LM
BL
EU
Co
ve
ra
ge
 o
f s
ur
fa
ce
 fo
rm
s 
(%
)
Parallel and Mono
Parallel
allel sentences and another 5 million sentences as
target-side monolingual data. As in the previous
section, the sizes of our development and evaluation
sets were a thousand sentences.
CzEng annotation contains lexically disam-
biguated word lemmas, an appealing option for our
purposes. We also tried trimming the last 3 charac-
ters of each word, keeping at least the first 3 charac-
ters intact. Stemming of each word to four charac-
ters was also evaluated (Stem-4).
Table 3 summarizes our results. The last column
shows the vocabulary size compared to original vo-
cabulary size, estimated on lower-cased words.
We are not surprised by stemming performing the
334
System Translation Gloss
Baseline Jsi tak zrcadla? Are youSG so mirrors? (ungrammatical)
+Mono LM Jsi neobjedna?vejte zrcadla? Did youSG don?t orderPL mirrors? (ungrammatical)
+Mono TM Uz? sis objednal zrcadla? Have youSG orderedSG the mirrors (for yourself) yet?
Figure 4: Translation of the sentence ?Did you order the mirrors?? by baseline systems and a reversely-trained system.
Only the last one is able to generate the correct form of the word ?order?.
worst ? the equivalence classes generated by this
simple heuristic are too broad. Using lemmas seems
optimal from the linguistic point of view, however
suffix trimming outperformed this approach in our
experiments. We feel that finding well-performing
back-off techniques for other languages merits fur-
ther research.
Back-off BLEU Vocabulary Size (%)
Baseline 31.82?3.24 100
Stem-4 32.73?3.19 19
Lemma 33.05?3.40 54
Trimmed Suffix 33.28?3.32 47
Table 3: Back-off BLEU scores comparison
4.5 WMT Systems
We submitted systems that used reverse self-
training (cu-tamchyna) for English?Czech and
English?German language pairs.
Our parallel data for German were constrained to
the provided set (1.9 million sentences). For Czech,
we used the training sections of CzEng and the sup-
plied WMT11 News Commentary data (7.3 million
sentences in total).
In case of German, we only used the supplied
monolingual data, for Czech we used a large col-
lection of texts for language modelling (i.e. uncon-
strained). The reverse self-training used only the
constrained data ? 2.3 million sentences in German
and 2.2 in Czech. In case of Czech, we only used
the News monolingual data from 2010 and 2011 for
reverse self-training ? we expected that recent data
from the same domain as the test set would improve
translation performance the most.
We achieved mixed results with these systems ?
for translation into German, reverse self-training did
not improve translation performance. For Czech,
we were able to achieve a small gain, even though
the reversely translated data contained less sentences
than the parallel data. Our BLEU scores were also
affected by submitting translation outputs without
normalized punctuation and with a slightly different
tokenization.
In this scenario, a lot of parallel data were avail-
able and we did not manage to prepare a reversely
trained model from larger monolingual data. Both
of these factors contributed to the inconclusive re-
sults.
Table 4 shows case-insensitive BLEU scores as
calculated in the official evaluation.
Target Language Mono LM +Mono TM
German 14.8 14.8
Czech 15.7 15.9
Table 4: Case-insensitive BLEU of WMT systems
5 Conclusion
We introduced a technique for exploiting monolin-
gual data to improve the quality of translation into
morphologically rich languages.
We carried out experiments showing improve-
ments in BLEU when using our method for trans-
lating into Czech, Finnish, German and Slovak with
small parallel data. We discussed the issues of in-
cluding similar translation models as separate com-
ponents in MERT.
We showed that gains in BLEU score increase
with growing size of monolingual data. On the other
hand, growing parallel data size diminishes the ef-
fect of our method quite rapidly. We also docu-
mented our experiments with several back-off tech-
niques for English to Czech translation.
Finally, we described our primary submissions to
the WMT 2011 Shared Translation Task.
335
References
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
182?189, Athens, Greece, March. Association for
Computational Linguistics.
Ondr?ej Bojar and Ales? Tamchyna. 2011. Forms Wanted:
Training SMT on Monolingual Data. Abstract at
Machine Translation and Morphologically-Rich Lan-
guages. Research Workshop of the Israel Science
Foundation University of Haifa, Israel, January.
Ondr?ej Bojar and Zdene?k ?Zabokrtsky?. 2009. CzEng
0.9: Large Parallel Treebank with Rich Annotation.
Prague Bulletin of Mathematical Linguistics, 92:63?
83.
Adria` de Gispert, Jose? B. Marin?o, and Josep M. Crego.
2005. Improving statistical machine translation by
classifying and generalizing inflected verb forms. In
Eurospeech 2005, pages 3185?3188, Lisbon, Portugal.
Kenneth Heafield. 2011. Kenlm: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, UK, July. Association for Computational Lin-
guistics.
Philipp Koehn and Hieu Hoang. 2007. Factored Transla-
tion Models. In Proc. of EMNLP.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL. The
Association for Computer Linguistics.
Philipp Koehn, Alexandra Birch, and Ralf Steinberger.
2009. 462 machine translation systems for europe. In
MT Summit XII.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In EMNLP, pages
388?395. ACL.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. pages 440?447, Hongkong,
China, October.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL, pages 160?
167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In ACL, pages 311?318.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz Erjavec, Dan Tufis, and Daniel
Varga. 2006. The JRC-acquis: A multilingual
aligned parallel corpus with 20+ languages. CoRR,
abs/cs/0609058. informal publication.
Andreas Stolcke. 2002. Srilm ? an extensible language
modeling toolkit, June 06.
Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.
2007. Semi-supervised model adaptation for statistical
machine translation. Machine Translation, 21(2):77?
94.
336
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 426?432,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Two-step translation with grammatical post-processing?
David Marec?ek, Rudolf Rosa, Petra Galus?c?a?kova? and Ondr?ej Bojar
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostranske? na?me?st?? 25, Prague
{marecek,rosa,galuscakova,bojar}@ufal.mff.cuni.cz
Abstract
This paper describes an experiment in which
we try to automatically correct mistakes in
grammatical agreement in English to Czech
MT outputs. We perform several rule-based
corrections on sentences parsed to dependency
trees. We prove that it is possible to improve
the MT quality of majority of the systems par-
ticipating in WMT shared task. We made both
automatic (BLEU) and manual evaluations.
1 Introduction
This paper is a joint report on two English-to-Czech
submissions to the WMT11 shared translation task.
The main contribution is however the proposal and
evaluation of a rule-based post-processing system
DEPFIX aimed at correcting errors in Czech gram-
mar applicable to any MT system. This is somewhat
the converse of other approaches (e.g. Simard et al
(2007)) where a statistical system was applied for
the post-processing of a rule-based one.
2 Our phrase-based systems
This section briefly describes our underlying phrase-
based systems. One of them (CU-BOJAR) was sub-
mitted directly to the WMT11 manual evaluation,
the other one (CU-TWOSTEP) was first corrected by
the proposed method (Section 3 below) and then
submitted under the name CU-MARECEK.
?This research has been supported by the European Union
Seventh Framework Programme (FP7) under grant agreement
n? 247762 (Faust), n? 231720 (EuroMatrix Plus), and by the
grants GAUK 116310 and GA201/09/H057.
2.1 Data for statistical systems
Our training parallel data consists of CzEng 0.9
(Bojar and Z?abokrtsky?, 2009), the News Commen-
tary corpus v.6 as released by the WMT11 orga-
nizers, the EMEA corpus, a corpus collected from
the transcripts of TED talks (http://www.ted.com),
the parallel news and separately some of the par-
allel web pages of the European Commission
(http://ec.europa.eu), and the Official Journal of the
European Union as released by the Apertium con-
sortium (http://apertium.eu/data).
A custom web crawler was used for the European
Commission website. English and Czech websites
were matched according to their URLs. Unfortu-
nately, Czech websites very often contain untrans-
lated parts of English texts. Because of this, we
aimed especially at the news articles, which are very
often translated correctly and also more relevant for
the shared task. Texts were segmented using train-
able tokenizer (Klyueva and Bojar, 2008) and dedu-
plicated. Processed texts were automatically aligned
by Hunalign (Varga and others, 2005).
The data from the Official Journal were first con-
verted from XML to plain text. The documents were
paired according to their filenames. To better han-
dle the nature of these data, we decided to divide
the documents into two classes based on the aver-
age number of words per sentence: ?lists? are docu-
ments with less than 2.8 words per sentence, other
documents are called ?texts?. The corresponding
?lists? were aligned line by line. The corresponding
?texts? were automatically segmented by trainable
tokenizer and aligned automatically by Hunalign.
We use the following two Czech language mod-
426
els, their weights are optimized in MERT:
? 5-gram LM from the Czech side of CzEng (ex-
cluding the Navajo section). The LM was con-
structed by interpolating LMs of the individual do-
mains (news, EU legislation, technical documenta-
tion, etc.) to achieve the lowest perplexity on the
WMT08 news test set.
? 6-gram LM from the monolingual data supplied by
WMT11 organizers (news of the individual years
and News Commentary), the Czech National Cor-
pus and a web collection of Czech texts. Again, the
final LM is constructed by interpolating the smaller
LMs1 for the WMT08 news test set.
2.2 Baseline Moses (CU-BOJAR)
The system denoted CU-BOJAR for English-to-
Czech is simple phrase-based translation, i.e. Moses
without factors. We tokenized, lemmatized and
tagged all texts using the tools wrapped in TectoMT
(Popel and Z?abokrtsky?, 2010). We further tokenize
e.g. dashed words (?23-year?) after all the process-
ing is finished. Phrase-based MT is then able to
handle such expressions both at once, or decompose
them as needed to cover unseen variations. We use
lexicalized reordering (orientation-bidirectional-fe).
The translation runs in ?supervised truecase?, which
means that we use the output of our lemmatizers
to decide whether the word should be lowercased
or should preserve uppercasing. After the transla-
tion, the first letter in the output is simply upper-
cased. The model is optimized using Moses? stan-
dard MERT on the WMT09 test set.
The organizers of WMT11 encouraged partici-
pants to apply simple normalization to their data
(both for training and testing).2 The main purpose
of the normalization is to improve the consistency of
typographical rules. Unfortunately, some of the au-
tomatic changes may accidentally damage the mean-
ing of the expression.3 We therefore opted to submit
1The interpolated LM file (gzipped ARPA format) is 5.1 GB
so we applied LM pruning as implemented in SRI toolkit with
the threshold 10?14 to reduce the file size to 2.3 GB.
2http://www.statmt.org/wmt11/normalize-punctuation.perl
3Fixing the ordering of the full stop and the quote is wrong
because the order (at least in Czech typesetting) depends on
whether it is the full sentence or a final phrase that is captured
in the quotes. Even riskier are rules handling decimal and thou-
sand separators in numbers. While there are language-specific
conventions, they are not always followed and the normaliza-
tion can in such cases confuse the order of magnitude by 3.
the output based on non-normalized test sets as our
primary English-to-Czech submission.
We invested much less effort into the submission
called CU-BOJAR for Czech-to-English. The only
interesting feature there is the use of alternative de-
coding paths to translate either from the Czech form
or from the Czech lemma equipped with meaning-
bearing morphological properties, e.g. the number
of nouns. Bojar and Kos (2010) used the same setup
with simple lemmas in the fallback decoding path.
The enriched lemmas perform marginally better.
2.3 Two-step translation
Our two-step translation is essentially the same
setup as detailed by Bojar and Kos (2010): (1)
the English source is translated to simplified Czech,
and (2) the simplified Czech is monotonically trans-
lated to fully inflected Czech. Both steps are sim-
ple phrase-based models. Instead of word forms, the
simplified Czech uses lemmas enriched by a sub-
set of morphological features selected manually to
encode only properties overt both in English and
Czech such as the tense of verbs or number of nouns.
Czech-specific morphological properties indicating
various agreements (e.g. number and gender of ad-
jectives, gender of verbs) are imposed in the second
step solely on the basis of the language model.
The first step uses the same parallel and mono-
lingual corpora as CU-BOJAR, except the LMs being
trained on the enriched lemmas, not on word forms.
The second step uses exactly the same LM as CU-
BOJAR but the phrase-table is extracted from all our
Czech monolingual data (phrase length limit of 1.)
3 Grammatical post-processing
Phrase-based machine translation systems often
have problems with grammatical agreement, espe-
cially on longer dependencies. Sometimes, there is
a mistake in agreement even between adjacent words
because each one belongs to a different phrase. The
goal of our post-processing is to correct forms of
some words so that they do not violate grammatical
rules (eg. grammatical agreement).
The problem is how to find the correct syntactic
relations in the output of an MT system. Parsers
trained on correct sentences can rely on grammat-
ical agreement, according to which they determine
427
the dependencies between words. Unfortunately, the
agreement in MT outputs is often wrong and the
parser fails to produce a correct parse tree. There-
fore, we would need a parser trained on a manually
annotated treebank consisting of specific outputs of
machine translation systems. Such a treebank does
not exist and we do not even want to create one, be-
cause the MT systems are changing constantly and
also because manual annotation of texts that are of-
ten not even understandable would be almost a su-
perhuman task.
The DEPFIX system was implemented in TectoMT
framework (Popel and Z?abokrtsky?, 2010). MT out-
puts were tagged by Morc?e tagger (Spoustova? et al,
2007) and then parsed with MST parser (McDon-
ald et al, 2005) that was trained on the Prague De-
pendency Treebank (Hajic? and others, 2006), i.e.
on correct Czech sentences. We used an improved
implementation with some additional features es-
pecially tuned for Czech (Nova?k and Z?abokrtsky?,
2007). The parser accuracy is much lower on the
?noisy? MT output sentences, but a lot of dependen-
cies in which we are to correct grammatical agree-
ment are determined correctly. Adapting the parser
for outputs of MT systems will be addressed in the
coming months.
A typical example of a correction is the agreement
between the subject and the predicate: they should
share the morphological number and gender. If they
do not, we simply change the number and gender
of the predicate in agreement with the subject.4 An
example of such a changed predicate is in Figure 1.
Apart from the dependency tree of the target sen-
tence, we can also use the dependency tree of the
source sentence. Source sentences are grammat-
ically correct and the accuracy of the tagger and
the parser is accordingly higher there. Words in
the source and target sentences are aligned using
GIZA++5 (Och and Ney, 2003) but verbose outputs
of the original MT systems would be possibly a bet-
ter option. The rules for fixing grammatical agree-
ment between words can thus consider also the de-
pendency relations and morphological caregories of
their English counterparts in the input sentence.
4In this case, we suppose that the number of the subject has
a much higher chance to be correct.
5GIZA++ was run on lemmatized texts in both directions
and intersection symmetrization was used.
Some
people
came
later
Atr
Sb
Pred
Advplpl
.AuxK
p?i?liPredpl
N?kte??
lid?
p?i?el
pozd?ji
Atr
Sb
Pred
Advsg, mpl
.AuxK
Figure 1: Example of fixing subject-predicate agreement.
The Czech word pr?is?el [he came] has a wrong morpho-
logical number and gender.
3.1 Grammatical rules
We have manually devised a set of the following
rules. Their input is the dependency tree of a Czech
sentence (MT output) and its English source sen-
tence (MT input) with the nodes aligned where pos-
sible. Each of the rules fires if the specified con-
ditions (?IF?) are matched, executes the command
(?DO?) , usually changing one or more morphologi-
cal categories of the word, and generates a new word
form for any word which was changed.
The rules make use of several morphological cat-
egories of the word (node:number, node:gender...),
its syntactic relation to its parent in the dependency
tree (node:afun) and the same information for its
English counterpart (node:en) and other nodes in
the dependency trees.
The order of the rules in this paper follows the
order in which they are applied; this is important, as
often a rule changes a morphological category of a
word which is then used by a subsequent rule.
3.1.1 Noun number (NounNum)
In Czech, a word in singular sometimes has the
same form as in plural. Because the tagger often
fails to tag the word correctly, we try to correct the
tag of a noun tagged as singular if its English coun-
terpart is in plural, so that the subsequent rules can
work correctly.
We trust the form of the word but changing the
number may also require to change the morphologi-
cal case (i.e. the tagger was wrong with both number
and case). In such cases we choose the first (linearly
428
from nominative to instrumentative) case matching
the form. The rule is:
IF: node:pos = noun &
node:number = singular &
node:en:number = plural
DO: node:number := plural;
node:case := find case(node:form, plural);
3.1.2 Subject case (SubjCase)
The subject of a Czech sentence must be in the
nominative case. Since the parser often fails in
marking the correct word as a subject, we use the
English source sentence and presuppose that the
Czech counterpart of the English subject is also a
subject in the Czech sentence.
IF: node:en:afun = subject
DO: node:case := nominative;
3.1.3 Subject-predicate agreement (SubjPred)
Subject and predicate in Czech agree in their mor-
phological number. To identify a Czech Subject, we
trust the subject in the English sentence. Then we
copy the number from the (Czech) Subject to the
Czech Predicate.
IF: node:en:afun = subject &
parent:afun = predicate
DO: parent:number := node:number;
3.1.4 Subject-past participle agreement (SubjPP)
Czech past participles agree with subject in
morphological gender.
IF: node:pos = noun|pronoun &
node:en:afun = subject &
parent:pos = verb past participle
DO: parent:number := node:number;
parent:gender := node:gender;
3.1.5 Preposition without children (PrepNoCh)
In our dependency trees, the preposition is the
parent of the words it belongs to (usually a noun). A
preposition without children is incorrect so we find
nodes aligned to its English counterpart?s children
and rehang them under the preposition.
IF: node:afun = preposition &
!node:has children &
node:en:has children
DO: foreach node:en:child;
node:en:child:cs:parent := node;
3.1.6 Preposition-noun agreement (PrepNoun)
Every prepositions gets a morphological case as-
signed to it by the tagger, with which the dependent
noun should agree.
IF: parent:pos = preposition &
node:pos = noun
DO: node:case := parent:case;
3.1.7 Noun-adjective agreement (NounAdj)
Czech adjectives and nouns agree in morpholog-
ical gender, number and case. We assume that the
noun is correct and change the adjective accordingly.
IF: node:pos = adjective &
parent:pos = noun
DO: node:gender := parent:gender;
node:number := parent:number;
node:case := parent:case;
3.1.8 Reflexive particle deletion (ReflTant)
Czech reflexive verbs are accompanied by reflex-
ive particles (?se? and ?si?). We delete particles not
beloning to any verb (or adjective derived from a
verb).
IF: node:form = ?se?|?si? &
node:pos = pronoun &
parent:pos != verb|verbal adjective
DO: remove node;
4 Experiments and results
We tested our CU-TWOSTEP system with DEPFIX
post-processing on both WMT10 and WMT11 test-
ing data. This combined system was submitted to
shared translation task as CU-MARECEK. We also
ran the DEPFIX post-processing on all other partici-
pating systems.
4.1 Automatic evaluation
The achieved BLEU scores are shown in Tables 1
and 2. They show the scores before and after the
DEPFIX post-processing. It is interesting that the
improvements are quite different between the years
2010 and 2011 in terms of their BLEU score. While
the average improvement on WMT10 test set was
0.21 BLEU points, it was only 0.05 BLEU points on
the WMT11 test set. Even the results of the same
TWOSTEP system differ in a similar way, so it must
have been caused by the different data.
429
system before after improvement
cu-twostep 15.98 16.13 0.15 (0.05 - 0.26)
cmu-heaf. 16.95 17.04 0.09 (-0.01 - 0.20)
cu-bojar 15.85 16.09 0.24 (0.14 - 0.36)
cu-zeman 12.33 12.55 0.22 (0.12 - 0.32)
dcu 13.36 13.59 0.23 (0.13 - 0.37)
dcu-combo 18.79 18.90 0.11 (0.02 - 0.23)
eurotrans 10.10 10.11 0.01 (-0.04 - 0.07)
koc 11.74 11.91 0.17 (0.08 - 0.26)
koc-combo 16.60 16.86 0.26 (0.16 - 0.37)
onlineA 11.81 12.08 0.27 (0.17 - 0.38)
onlineB 16.57 16.79 0.22 (0.11 - 0.33)
potsdam 12.34 12.57 0.23 (0.14 - 0.35)
rwth-combo 17.54 17.79 0.25 (0.15 - 0.35)
sfu 11.43 11.83 0.40 (0.29 - 0.52)
uedin 15.91 16.19 0.28 (0.18 - 0.40)
upv-combo 17.51 17.73 0.22 (0.10 - 0.34)
Table 1: Depfix improvements on the WMT10 systems
in BLEU score. Confidence intervals, which were com-
puted on 1000 bootstrap samples, are in brackets.
system before after improvement
cu-twostep 16.57 16.60 0.03 (-0.07 - 0.13)
cmu-heaf. 20.24 20.32 0.08 (-0.03 - 0.19)
commerc2 09.32 09.32 0.00 (-0.04 - 0.04)
cu-bojar 16.88 16.85 -0.03 (-0.12 - 0.07)
cu-popel 14.12 14.11 -0.01 (-0.06 - 0.03)
cu-tamch. 16.32 16.28 -0.04 (-0.14 - 0.06)
cu-zeman 14.61 14.80 0.19 (0.09 - 0.29)
jhu 17.36 17.42 0.06 (-0.03 - 0.16)
online-B 20.26 20.31 0.05 (-0.06 - 0.16)
udein 17.80 17.88 0.08 (-0.02 - 0.17)
upv-prhlt. 20.68 20.69 0.01 (-0.08 - 0.11)
Table 2: Depfix improvements on the WMT11 systems
in BLEU score. Confidence intervals are in brackets.
4.2 Manual evaluation
Two independent annotators evaluated DEPFIX man-
ually on the outputs of CU-TWOSTEP and ONLINE-
B. We randomly selected 1000 sentences from the
newssyscombtest2011 data set and the appropri-
ate translations made by these two systems. The
annotators got the outputs before and after DEPFIX
post-processing and their task was to decide which
translation6 from these two is better and label it by
the letter ?a?. If it was not possible to determine
6They were also provided with the source English sentence
and the reference translation. The options were shuffled and
indentical candidate sentences were collapsed.
A / B improved worsened indefinite total
improved 273 20 15 308
worsened 12 59 7 78
indefinite 53 35 42 130
total 338 114 64 516
Table 5: Matrix of the inter-annotator agreement
rule fired impr. wors. % impr.
SubjCase 51 46 5 90.2
SubjPP 193 165 28 85.5
NounAdj 434 354 80 81.6
NounNum 156 122 34 78.2
PrepNoun 135 99 36 73.3
SubjPred 68 48 20 70.6
ReflTant 15 10 5 66.7
PrepNoCh 45 29 16 64.4
Table 6: Rules and their utility.
which is better, they labeled both by ?n?.
Table 3 below shows that about 60% of sentences
fixed by DEPFIX were improved and only about 20%
were worsened. DEPFIX worked a little better on the
ONLINE-B, making fewer changes but also fewer
wrong changes. It is probably connected with the
fact that overall better translations by ONLINE-B are
easier to parse.
The matrix of inter-annotator agreement is in Ta-
ble 5. Our two annotators agreed in 374 sentences
(out of 516), that is 72.5%. On the other hand, if
we consider only cases where both annotators chose
different translation as better (no indefinite marks),
we get only 8.8% disagreement (32 out of 364).
Using the manual evaluation, we can also measure
performance of the individual rules. Table 6 shows
the number of all, improved or worsened sentences
where a particular rule was applied. Definitely, the
most useful rule (used often and quite reliable) was
the one correcting noun-adjective agreement, fol-
lowed by the subject-pastparticiple agreement rule.
In each changed sentence, two rules (not neces-
sarily related ones) were applied on average.
4.3 Manual evaluation across data sets
The fact that the improvements in BLEU scores on
WMT10 test set are much higher has led us to one
more experiment: we compare manual annotations
of 330 sentences from each of the WMT10 and
430
system annotator changed improved worsened indefinite
count % count % count %
cu-bojar-twostep A 269 152 56.5 39 14.5 78 29.0
cu-bojar-twostep B 269 173 64.3 50 18.6 46 17.1
online-B A 247 156 63.1 39 15.9 52 21.1
online-B B 247 165 66.8 64 25.9 18 7.3
Table 3: Manual evaluation of the DEPFIX post-processing on 1000 randomly chosen sentences from WMT11 test set.
test set changed improved worsened indefinite BLEU
count % count % count % before after diff
newssyscombtest2010 104 52 50.0 20 19.2 32 30.8 16.99 17.38 0.39
newssyscombtest2011 101 66 65.3 19 18.8 16 15.8 13.99 13.87 -0.12
Table 4: Manual and automatic evaluation of the DEPFIX post-processing on CU-TWOSTEP system across different
datasets. 330 sentences were randomly selected from each of the WMT10 and WMT11 test sets. Both manual scores
and BLEU are computed only on the sentences that were changed by the DEPFIX post-processing.
WMT11 sets as translated by CU-TWOSTEP and cor-
rected by DEPFIX. Table 4 shows that WMT10 and
WMT11 are comparable in manually estimated im-
provement (50?65%). BLEU does not indicate that
and even estimates a drop in quality on this subset
WMT11. (The absolute BLEU scores differ from
BLEUs on the whole test sets but we are interested
only in the change of the scores.) BLEU is thus not
very suitable for the evaluation of DEPFIX.
5 Conclusions and future work
Manual evaluation shows that our DEPFIX approach
to improving MT output quality is sensible. Al-
though it is unable to correct many serious MT er-
rors, such as wrong lexical choices, it can improve
the grammaticality of the output in a way that the
language model often cannot, which leads to out-
put that is considered to be better by humans. We
also suggest that BLEU is not appropriate metric
for measuring changes in grammatical correctness
of sentences, especially with inflective languages.
An advantage of our method is that it is possible
to apply it on output of any MT system (although it
works better for phrase-based MT systems). While
DEPFIX has been developed using the output of CU-
BOJAR, the rules we devised are not specific to any
MT system. They simply describe several grammat-
ical rules of Czech language that can be machine-
checked and if errors are found, the output can be
corrected. Moreover, our method only requires the
source sentence and the translation output for its op-
eration ? i.e. it is not necessary to modify the MT
system itself.
We are now considering modifications of the
parser so that it is able to parse the incorrect sen-
tences produced by MT. Theoretically it would be
possible to train the parser on annotated ungrammat-
ical sentences, but we do not want to invest such an-
notation labour. Instead, when parsing the Czech
sentence we will make the parser utilize the infor-
mation contained in the parse tree of the English
sentence, which is usually correct. We will proba-
bly also have to make the parser put less weight to
the often incorrect tagger output. An alternative is
to avoid parsing of the target and project the source
parse to the target side using word alignments, if
provided by the MT system.
Because some of our rules are able to work using
only the tagger output, we will also try to apply them
before the parsing as they might help the parser by
correcting some of the tags.
We will also try several modifications of the tag-
ger, but the English sentence does not help us so
much here, because it does not contain any infor-
mation regarding the most common errors ? in-
correct assignment of morphological gender and
case. However, it could help with part of speech
and morphological number disambiguation. More-
over, it would be probably helpful for us if the tag-
ger included several most probable hypotheses, as
the single-output-only disambiguation is often erro-
neous on ungrammatical sentences.
431
References
Ondrej Bojar and Kamil Kos. 2010. 2010 Failures in
English-Czech Phrase-Based MT. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 60?66, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Ondr?ej Bojar and Zdene?k Z?abokrtsky?. 2009. CzEng0.9:
Large Parallel Treebank with Rich Annotation.
Prague Bulletin of Mathematical Linguistics, 92.
Jan Hajic? et al 2006. Prague Dependency Treebank 2.0.
CD-ROM, Linguistic Data Consortium, LDC Catalog
No.: LDC2006T0 1, Philadelphia.
Natalia Klyueva and Ondr?ej Bojar. 2008. UMC 0.1:
Czech-Russian-English Multilingual Corpus. In Pro-
ceedings of International Conference Corpus Linguis-
tics, pages 188?195, Saint-Petersburg.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency parsing
using spanning tree algorithms. In HLT ?05: Proceed-
ings of the conference on Human Language Technol-
ogy and Empirical Methods in Natural Language Pro-
cessing, pages 523?530, Vancouver, British Columbia,
Canada.
Va?clav Nova?k and Zdene?k Z?abokrtsky?. 2007. Feature
engineering in maximum spanning tree dependency
parser. In Va?clav Matous?ek and Pavel Mautner, edi-
tors, Lecture Notes in Artificial Intelligence, Proceed-
ings of the 10th I nternational Conference on Text,
Speech and Dialogue, Lecture Notes in Computer Sci-
ence, pages 92?98, Pilsen, Czech Republic. Springer
Science+Business Media Deutschland GmbH.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Martin Popel and Zdene?k Z?abokrtsky?. 2010. TectoMT:
modular NLP framework. In Proceedings of the 7th
international conference on Advances in natural lan-
guage processing, IceTAL?10, pages 293?304, Berlin,
Heidelberg. Springer-Verlag.
Michel Simard, Cyril Goutte, and Pierre Isabelle. 2007.
Statistical phrase-based post-editing. In Human Lan-
guage Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics; Proceedings of the Main Con-
ference, pages 508?515, Rochester, New York, April.
Association for Computational Linguistics.
Drahom??ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel Kr-
bec, and Pavel Kve?ton?. 2007. The best of two worlds:
Cooperation of statistical and rule-based taggers for
czech. In Proceedings of the Workshop on Balto-
Slavonic Natural Language Processing, ACL 2007,
pages 67?74, Praha.
Da?niel Varga et al 2005. Parallel corpora for medium
density languages. In Proceedings of the Recent Ad-
vances in Natural Language Processing, pages 590?
596, Borovets, Bulgaria.
432
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 64?70,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
TerrorCat: a Translation Error Categorization-based MT Quality Metric
Mark Fishel,? Rico Sennrich,? Maja Popovic?,? Ondr?ej Bojar?
? Institute of Computational Linguistics, University of Zurich
{fishel,sennrich}@cl.uzh.ch
? German Research Center for Artificial Intelligence (DFKI), Berlin
maja.popovic@dfki.de
? Charles University in Prague, Faculty of Mathematics and Physics,
Institute of Formal and Applied Linguistics
bojar@ufal.mff.cuni.cz
Abstract
We present TerrorCat, a submission to the
WMT?12 metrics shared task. TerrorCat uses
frequencies of automatically obtained transla-
tion error categories as base for pairwise com-
parison of translation hypotheses, which is in
turn used to generate a score for every trans-
lation. The metric shows high overall corre-
lation with human judgements on the system
level and more modest results on the level of
individual sentences.
1 The Idea
Recently a couple of methods of automatic trans-
lation error analysis have emerged (Zeman et al,
2011; Popovic? and Ney, 2011). Initial experiments
have shown that while agreement with human error
analysis is low, these methods show better perfor-
mance on tasks with a lower granularity, e.g. ranking
error categories by frequency (Fishel et al, 2012).
In this work we apply translation error analysis to a
task with an even lower granularity: ranking transla-
tions, one of the shared tasks of WMT?12.
The aim of translation error analysis is to identify
the errors that translation systems make and catego-
rize them into different types: e.g. lexical, reorder-
ing, punctuation errors, etc. The two tools that we
will use ? Hjerson and Addicter ? both rely on a ref-
erence translation. The hypothesis translation that is
being analyzed is first aligned to the reference on the
word level, and then mistranslated, misplaced, mis-
inflected, missing or superfluous words and other er-
rors are identified.
The main idea of our work is to quantify trans-
lation quality based on the frequencies of different
error categories. The basic assumption is that differ-
ent error categories have different importance from
the point of view of overall translation quality: for
instance, it would be natural to assume that punc-
tuation errors influence translation quality less than
missing words or lexical choice errors. Furthermore,
an error category can be more important for one out-
put language than the other: for example, word or-
der can influence the meaning in an English sentence
more than in a Czech or German one, whereas in-
flection errors are probably more frequent in the lat-
ter two and can thus cause more damage.
In the context of the ranking task, the absolute
value of a numeric score has no importance, apart
from being greater than, smaller than or equal to the
other systems? scores. We therefore start by per-
forming pairwise comparison of the translations ?
the basic task is to compare two translations and re-
port which one is better. To conform with the WMT
submission format we need to generate a numeric
score as the output ? which is obtained by compar-
ing every possible pair of translations and then using
the (normalized) total number of wins per translation
as its final score.
The general architecture of the metric is thus this:
? automatic error analysis is applied to the sys-
tem outputs, yielding the frequencies of every
error category for each sentence
? every possible pair of all system outputs is rep-
resented as a vector of features, based on the
error category frequencies
64
? a binary classifier takes these feature vectors as
input and assigns a win to one of the sentences
in every pair (apart from ties)
? the final score of a system equals to the normal-
ized total number of wins per sentence
? the system-level score is averaged out over the
individual sentence scores
An illustrative example is given in Figure 1.
We call the result TerrorCat, the translation error
categorization-based metric.
2 The Details
In this section we will describe the specifics of
the current implementation of the TerrorCat met-
ric: translation error analysis, lemmatization, binary
classifier and training data for the binary classifier.
2.1 Translation Error Analysis
Addicter (Zeman et al, 2011) and Hjerson (Popovic?
and Ney, 2011) use different methods for automatic
error analysis. Addicter explicitly aligns the hy-
pothesis and reference translations and induces error
categories based on the alignment coverage while
Hjerson compares words encompassed in the WER
(word error rate) and PER (position-independent
word error rate) scores to the same end.
Previous evaluation of Addicter shows that
hypothesis-reference alignment coverage (in terms
of discovered word pairs) directly influences er-
ror analysis quality; to increase alignment cover-
age we used Berkeley aligner (Liang et al, 2006)
and trained it on and applied it to the whole set of
reference-hypothesis pairs for every language pair.
Both tools use word lemmas for their analysis;
we used TreeTagger (Schmid, 1995) for analyzing
English, Spanish, German and French and Morc?e
(Spoustova? et al, 2007) to analyze Czech. The same
tools are used for PoS-tagging in some experiments.
2.2 Binary Classification
Pairwise comparison of sentence pairs is achieved
with a binary SVM classifier, trained via sequential
minimal optimization (Platt, 1998), implemented in
Weka (Hall et al, 2009).
The input feature vectors are composed of fre-
quency differences of every error category; since the
Source: Wir sind Meister!
Translations:
Reference: We are the champions!
HYP-1: Us champions!
HYP-2: The champions we are .
HYP-3: We are the champignons!
Error Frequencies:
HYP-1: 1?inflection, 2?missing
HYP-2: 2?order, 1?punctuation
HYP-3: 1?lex.choice
Classifier Output: (or manually created
input in the training phase)
HYP-1 < HYP-2
HYP-1 < HYP-3
HYP-2 > HYP-3
Scores:
HYP-1: 0
HYP-2: 1
HYP-3: 0.5
Figure 1: Illustration of TerrorCat?s process for a single
sentence: translation errors in the hypothesis translations
are discovered by comparing them to the reference, error
frequencies are extracted, pairwise comparisons are done
by the classifier and then converted to scores. The shown
translation errors correspond to Hjerson?s output.
maximum (normalized) frequency of any error rate
is 1, the feature value range is [?1, 1]. To include
error analysis from both Addicter and Hjerson their
respective features are used side-by-side.
2.3 Data Extraction
Training data for the SVM classifier is taken from
the WMT shared task manual ranking evaluations
of previous years (2007?2011), which consist of tu-
ples of 2 to 5 ranked sentences for every language
pair. Equal ranks are allowed, and translations of
the same sentence by the same pair of systems can
be present in several tuples, possibly having conflict-
ing comparison results.
To convert the WMT manual ranking data into
the training data for the SVM classifier, we collect
all rankings for each pair of translation hypothe-
65
2007-2010 2007-2011
fr-en 34 152 46 070
de-en 36 792 53 790
es-en 30 374 41 966
cs-en 19 268 26 418
en-fr 22 734 35 854
en-de 36 076 56 054
en-es 19 352 35 700
en-cs 31 728 52 954
Table 1: Dataset sizes for every language pair, based
on manual rankings from WMT shared tasks of previ-
ous years: the number of pairs with non-conflicting, non-
equivalent ranks.
ses. Pairs with equal ranks are discarded, conflicting
ranks for the same pairs are resolved with voting. If
the voting is tied, the pair is also discarded.
The kept translation pairs are mirrored (i.e. both
directions of every pair are added to the training set
as independent entries) to ensure no bias towards the
first or second translation in a pair. We will later
present analysis of how well that works.
2.4 TerrorCat+You
TerrorCat is distributed via GitHub; information on
downloading and using it can be found online.1 Ad-
ditionally we are planning to provide more recent
evaluations with new datasets, as well as pre-trained
models for various languages and language pairs.
3 The Experiments
In the experimental part of our work, we search for
the best performing model variant, the aim of which
is to evaluate different input features, score calcula-
tion strategies and other alternations. The search is
done empirically: we evaluate one alternation at a
time, and if it successful, it is added to the system
before proceeding to test further alternations.
Performance of the models is estimated on a held-
out development set, taken from the WMT?11 data;
the training data during the optimization phase is
composed of ranking data from WMT 2007?2010.
In the end we re-trained our system on the whole
data set (WMT 2007?2011) and applied it to the un-
1http://terra.cl.uzh.ch/terrorcat.html
labeled data from this year?s shared task. The result-
ing dataset sizes are given in Table 1.
All of the resulting scores obtained by different
variants of our metric are presented in Tables 2 (for
system-level correlations) and 3 (for sentence-level
correlations), compared to BLEU and other selected
entries in the WMT?11 evaluation shared task. Cor-
relations are computed in the same way as in the
WMT evaluations.
3.1 Model Optimization
The following is a brief description of successful
modifications to the baseline system.
Weighted Wins
In the baseline model, the score of the winning
system in each pairwise comparison is increased by
1. To reduce the impact of low-confidence decisions
of the classifier on the final score we tested replac-
ing the constant rewards to the winning system with
variable ones, proportional to the classifier?s confi-
dence ? a measure of which was obtained by fitting
a logistic regression model to the SVM output.
As the results show, this leads to minor improve-
ments in sentence-level correlation and more notice-
able improvements in system-level correlation (es-
pecially English-French and Czech-English). A pos-
sible explanation for this difference in performance
on different levels is that low classification confi-
dence on the sentence-level does not necessarily af-
fect our ranking for that sentence, but reduces the
impact of that sentence on the system-level ranking.
PoS-Split Features
The original model only makes a difference be-
tween individual error categories as produced by
Hjerson and Addicter. It seems reasonable to assume
that errors may be more or less important, depending
on the part-of-speech of the words they occur in. We
therefore tested using the number of errors per er-
ror category per PoS-tag as input features. In other
words, unlike the baseline, which relied on counts
of missing, misplaced and other erroneous words,
this alternation makes a difference between miss-
ing nouns/verbs/etc., misplaced nouns, misinflected
nouns/adjectives, and so on.
The downside of this approach is that the number
of features is multiplied by the size of the PoS tag
66
Metric fr-en de-en es-en cs-en *-en en-fr en-de en-es en-cs en-*
TerrorCat:
Baseline 0.73 0.74 0.82 0.76 0.76 0.70 0.81 0.69 0.84 0.76
Weighted wins 0.73 0.74 0.82 0.79 0.77 0.75 0.81 0.69 0.84 0.77
PoS-features 0.87 0.76 0.80 0.86 0.82 0.76 0.86 0.74 0.87 0.81
GenPoS-features 0.86 0.77 0.84 0.88 0.84 0.80 0.85 0.75 0.90 0.83
No 2007 data (GenPoS) 0.89 0.80 0.80 0.95 0.86 0.85 0.84 0.81 0.90 0.85
Other:
BLEU 0.85 0.48 0.90 0.88 0.78 0.86 0.44 0.87 0.65 0.70
mp4ibm1 0.08 0.56 0.12 0.91 0.42 0.61 0.91 0.71 0.76 0.75
MTeRater-Plus 0.93 0.90 0.91 0.95 0.92 ? ? ? ? ?
AMBER ti 0.94 0.63 0.85 0.88 0.83 0.84 0.54 0.88 0.56 0.70
meteor-1.3-rank 0.93 0.71 0.88 0.91 0.86 0.85 0.30 0.74 0.65 0.63
Table 2: System-level Spearman?s rank correlation coefficients (?) between different variants of TerrorCat and hu-
man judgements, based on WMT?11 data. Other metric submissions are shown for comparison. Highest scores per
language pair are highlighted in bold separately for TerrorCat variants and for other metrics.
set. Additionally, too specific distinctions can cause
data sparsity, especially on the sentence level.
As shown by the results, PoS-tag splitting of the
features is successful on the system level, but quite
hurtful to the sentence-level correlations. The poor
performance on the sentence level can be attributed
to the aforementioned data sparsity: the number of
different features is higher than the number of words
(and hence, the biggest possible number of errors)
in the sentences. However, we cannot quite ex-
plain, how a sum of these less reliable sentence-level
scores leads to more reliable system-level scores.
To somewhat relieve data sparsity we defined sub-
sets of the original PoS tag sets, mostly leaving out
morphological information and keeping just the gen-
eral word types (nouns, verbs, adjectives, etc.). This
reduced the number of PoS-tags (and thus, the num-
ber of input features) from 2 to 4 times and produced
further increase in system-level and a smaller de-
crease in sentence-level scores, see GenPoS results.
To avoid splitting the metric into different ver-
sions for system-level and sentence-level, we gave
priority to system-level correlations and adopted the
generalized PoS-splitting of the features.
Out-of-Domain Data
The human ranking data from WMT of previ-
ous years do not constitute a completely homo-
geneous dataset. For starters, the test sets are
taken from different domains (News/News Com-
mentary/Europarl), whereas the 2012 test set is from
the News domain only. Added to this, there might be
a difference in the manual data, coming from differ-
ent organization of the competition ? e.g. WMT?07
was the only year when manual scoring of the trans-
lations with adequacy/fluency was performed, and
ranking had just been introduced into the competi-
tion. Therefore we tested whether some subsets of
the training data can result in better overall scores.
Interestingly enough, leaving out News Commen-
tary and Europarl test sets caused decreased correla-
tions, although these account for just around 10%
of the training data. On the other hand, leaving out
the data from WMT?07 led to a significant gain in
overall performance.
3.2 Error Meta-Analysis
To better understand why sentence-level correlations
are low, we analyzed the core of TerrorCat ? its pair-
wise classifier. Here, we focus on the most success-
ful variant of the metric, which uses general PoS-
tags and was trained on the WMT manual rankings
from 2008 to 2010. Table 4 presents the confusion
matrices of the classifier (one for precision and one
for recall), taking into consideration the confidence
estimate.
Evaluation is based on the data from 2011; the
prediction data was mirrored in the same way as for
67
Metric fr-en de-en es-en cs-en *-en en-fr en-de en-es en-cs en-*
TerrorCat:
Baseline 0.20 0.22 0.33 0.25 0.25 0.30 0.19 0.24 0.20 0.23
Weighted wins 0.20 0.23 0.33 0.25 0.25 0.31 0.20 0.24 0.20 0.24
PoS-features 0.13 0.18 0.24 0.15 0.18 0.27 0.15 0.15 0.17 0.19
GenPoS-features 0.16 0.24 0.31 0.22 0.23 0.27 0.18 0.22 0.19 0.22
No 2007 data (GenPoS) 0.21 0.30 0.33 0.23 0.27 0.29 0.20 0.23 0.20 0.23
Other:
mp4ibm1 0.15 0.16 0.18 0.12 0.15 0.21 0.13 0.13 0.06 0.13
MTeRater-Plus 0.30 0.36 0.45 0.36 0.37 ? ? ? ? ?
AMBER ti 0.24 0.26 0.33 0.27 0.28 0.32 0.22 0.31 0.21 0.27
meteor-1.3-rank 0.23 0.25 0.38 0.28 0.29 0.31 0.14 0.26 0.19 0.23
Table 3: Sentence-level Kendall?s rank correlation coefficients (? ) between different variants of TerrorCat and hu-
man judgements, based on WMT?11 data. Other metric submissions are shown for comparison. Highest scores per
language pair are highlighted in bold separately for TerrorCat variants and for other metrics.
the training set. Our aim was to measure the bias
of the classifier towards first or second translations
in a pair (which is obviously an undesired effect).
It can be seen that the confusion matrices are com-
pletely symmetrical, indicating no position bias of
the classifier ? even lower-confidence decisions are
absolutely consistent.
To make sure that this can be attributed to the mir-
roring of the training set, we re-trained the classifier
on non-mirrored training sets. As a result, 9% of the
instances were labelled inconsistently, with the av-
erage confidence of such inconsistent decisions be-
ing extremely low (2.1%, compared to the overall
average of 28.4%). The resulting correlations have
slightly dropped as well ? all indicating that mirror-
ing the training sets does indeed remove the posi-
tional bias and leads to slightly better performance.
Looking at the confusion matrices overall, most
decisions fall within the main diagonals (i.e. the
cells indicating correct decisions of the classifier).
Looking strictly at the classifier?s decisions, the re-
calls and precisions of the non-tied comparison out-
puts (?<? and ?>?) are 57% precision, 69% recall.
However, such strict estimates are too pessimistic in
our case, since the effect of the classifier?s decisions
is proportional to the confidence estimate. On the
sentence level it means that low-confidence decision
errors have less effect on the total score of a system.
A definite source of error is the instability of the in-
dividual translation errors on the sentence level, an
effect both Addicter and Hjerson are known to suffer
from (Fishel et al, 2012).
The precision of the classifier predictably drops
together with the confidence, and almost half of the
misclassifications come from unrecognized equiva-
lent translations ? as a result the recall of such pairs
of equivalent translations is only 20%. This can be
explained by the fact that the binary classifier was
trained on instances with just these two labels and
with no ties allowed.
On the other hand the classifier?s 0-confidence de-
cisions have a high precision (84%) on detecting the
equivalent translations; after re-examining the data
it turned out that 96% of the 0-confidence decisions
were made on input feature vectors containing only
zero frequency differences. Such vectors represent
pairs of sentences with identical translation error
analyses, which are very often simply identical sen-
tences ? in which case the classifier cannot (and in
fact, should not) make an informed decision of one
being better than the other.
4 Related Work
Traditional MT metrics such as BLEU (Papineni et
al., 2002) are based on a comparison of the trans-
lation hypothesis to one or more human references.
TerrorCat still uses a human reference to extract fea-
tures from the error analysis with Addicter and Hjer-
son, but at the core, TerrorCat compares hypotheses
not to a reference, but to each other.
68
Manual Classifier Output and Confidence: Precision
label < < or > >
0.6?1.0 0.3?0.6 0.0?0.3 0.0 0.0?0.3 0.3?0.6 0.6?1.0
< 81% 60% 45% 8% 32% 23% 10%
= 9% 17% 23% 84% 23% 17% 9%
> 10% 23% 32% 8% 45% 60% 81%
Manual Classifier Output and Confidence: Recall
label < < or > >
0.6?1.0 0.3?0.6 0.0?0.3 0.0 0.0?0.3 0.3?0.6 0.6?1.0
< 23% 18% 28% 1% 20% 7% 3%
= 5% 9% 26% 20% 26% 9% 5%
> 3% 7% 20% 1% 28% 18% 23%
Table 4: The precision and recall confusion matrices of the classifier ? judgements on whether one hypothesis is worse
than, equivalent to or better than another hypothesis are compared to the classifier?s output and confidence.
It is thus most similar to SVM-RANK and Tesla
metrics, submissions to the WMT?10 shared met-
rics task (Callison-Burch et al, 2010) which also
used SVMs for ranking translations. However, both
metrics used SVMrank (Joachims, 2006) directly for
ranking (unlike TerrorCat, which uses a binary clas-
sifier for pairwise comparisons). Their features in-
cluded some of the metric outputs (BLEU, ROUGE,
etc.) for SVM-RANK and similarity scores between
bags of n-grams for Tesla (Dahlmeier et al, 2011).
5 Conclusions
We introduced the TerrorCat metric, which performs
pairwise comparison of translation hypotheses based
on frequencies of automatically obtained error cate-
gories using a binary classifier, trained on manually
ranked data. The comparison outcome is then con-
verted to a numeric score for every sentence or doc-
ument translation by averaging out the number of
wins per translation system.
Our submitted system achieved an average
system-level correlation with human judgements in
the WMT?11 development set of 0.86 for transla-
tion into English and 0.85 for translations from En-
glish into other languages. Particularly good per-
formance was achieved on translations from English
into Czech (0.90) and back (0.95). Sentence-level
scores are more modest: average 0.27 for transla-
tion into English and 0.23 for those out of English.
The scores remain to be checked against the human
judgments from WMT?12.
The introduced TerrorCat metric has certain de-
pendencies. For one thing, in order to apply it to
new languages, a training set of manual rankings is
required ? although this can be viewed as an advan-
tage, since it enables the user to tune the metric to
his/her own preference. Additionally, the metric de-
pends on lemmatization and PoS-tagging.
There is a number of directions to explore in the
future. For one, both Addicter and Hjerson report
MT errors related more to adequacy than fluency, al-
though it was shown last year (Parton et al, 2011)
that fluency is an important component in rating
translation quality. It is also important to test how
well the metric performs if lemmatization and PoS-
tagging are not available.
For this year?s competition, training data was
taken separately for every language pair; it remains
to be tested whether combining human judgements
with the same target language and different source
languages leads to better or worse performance.
To conclude, we have described TerrorCat, one
of the submissions to the metrics shared task of
WMT?12. TerrorCat is rather demanding to apply on
one hand, having more requirements than the com-
mon reference-hypothesis translation pair, but at the
same time correlates rather well with human judge-
ments on the system level.
69
References
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53, Uppsala, Sweden.
Daniel Dahlmeier, Chang Liu, and Hwee Tou Ng. 2011.
Tesla at wmt 2011: Translation evaluation and tunable
metric. In Proceedings of the Sixth Workshop on Sta-
tistical Machine Translation, pages 78?84, Edinburgh,
Scotland.
Mark Fishel, Ondr?ej Bojar, and Maja Popovic?. 2012.
Terra: a collection of translation error-annotated cor-
pora. In Proceedings of the 8th LREC, page in print,
Istanbul, Turkey.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
SIGKDD Explorations, 11.
Thorsten Joachims. 2006. Training linear SVMs in
linear time. In Proceedings of the ACM Conference
on Knowledge Discovery and Data Mining (KDD),
Philadelphia, USA.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the HLT-
NAACL Conference, pages 104?111, New York, NY.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic evalu-
ation of machine translation. In ACL ?02: Proceedings
of the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318, Morristown, NJ,
USA. Association for Computational Linguistics.
Kristen Parton, Joel Tetreault, Nitin Madnani, and Mar-
tin Chodorow. 2011. E-rating machine translation. In
Proceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 108?115, Edinburgh, Scot-
land.
John C. Platt. 1998. Using analytic qp and sparseness
to speed training of support vector machines. In Pro-
ceedings of Neural Information Processing Systems
11, pages 557?564, Denver, CO.
Maja Popovic? and Hermann Ney. 2011. Towards au-
tomatic error analysis of machine translation output.
Computational Linguistics, 37(4):657?688.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to german. In Proceedings
of the ACL SIGDAT-Workshop, Dublin, Ireland.
Drahom??ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel Kr-
bec, and Pavel Kve?ton?. 2007. The best of two worlds:
Cooperation of statistical and rule-based taggers for
Czech. In Proceedings of the Workshop on Balto-
Slavonic Natural Language Processing, ACL 2007,
pages 67?74, Praha.
Daniel Zeman, Mark Fishel, Jan Berka, and Ondr?ej Bo-
jar. 2011. Addicter: What is wrong with my transla-
tions? The Prague Bulletin of Mathematical Linguis-
tics, 96:79?88.
70
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 253?260,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Probes in a Taxonomy of Factored Phrase-Based Models ?
Ondr?ej Bojar, Bushra Jawaid, Amir Kamran
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostranske? na?m. 25, Praha 1, CZ-118 00, Czech Republic
{bojar,jawaid,kamran}@ufal.mff.cuni.cz
Abstract
We introduce a taxonomy of factored phrase-
based translation scenarios and conduct a
range of experiments in this taxonomy. We
point out several common pitfalls when de-
signing factored setups. The paper also de-
scribes our WMT12 submissions CU-BOJAR
and CU-POOR-COMB.
1 Introduction
Koehn and Hoang (2007) introduced ?factors? to
phrase-based MT to explicitly capture arbitrary fea-
tures in the phrase-based model. In essence, input
and output tokens are no longer atomic units but
rather vectors of atomic values encoding e.g. the lex-
ical and morphological information separately. Fac-
tored translation has been successfully applied to
many language pairs and with diverse types of infor-
mation encoded in the additional factors, i.a. (Bojar,
2007; Avramidis and Koehn, 2008; Stymne, 2008;
Badr et al, 2008; Ramanathan et al, 2009; Koehn et
al., 2010; Yeniterzi and Oflazer, 2010). On the other
hand, it happens quite frequently, that the factored
setup causes a loss compared to the phrase-based
baseline. The underlying reason is the complexity of
the search space which gets boosted when the model
explicitly includes detailed information, see e.g. Bo-
jar and Kos (2010) or Toutanova et al (2008).
? This work was supported by the project EuroMatrixPlus
(FP7-ICT-2007-3-231720 of the EU and 7E09003+7E11051 of
the Czech Republic) and the Czech Science Foundation grants
P406/11/1499 and P406/10/P259. We are grateful for review-
ers? comments but we have to obey the 6 page limit. Thanks
also to Ales? Tamchyna for supplementary material on MERT.
Number of Number of
Translation Independent Structure
Steps Searches of Searches Nickname
One One ? Direct
Several
One ? Single-Step
Several
Serial Two-Step
Complex Complex
Figure 1: A taxonomy of factored phrase-based models.
In this paper, we first provide a taxonomy of
(phrase-based) translation setups and then we exam-
ine a range of sample configurations in this taxon-
omy. We don?t state universal rules, because the ap-
plicability of each of the setups depends very much
on the particular language pair, text domain and
amount of data available, but we hope to draw at-
tention to relevant design decisions.
The paper also serves as the description of our
WMT12 submissions CU-BOJAR and CU-POOR-
COMB between English and Czech.
2 A Taxonomy of Factored P-B Models
Figure 1 suggests a taxonomy of various Moses se-
tups. Following the definitions of Koehn and Hoang
(2007), a search consists of several translation and
generation steps: translation steps map source fac-
tors to target factors and generation steps produce
target factors from other target factors.
The taxonomy is vaguely linked to the types of
problems that can be expected with a given config-
uration. Direct translation is likely to suffer from
out-of-vocabulary issues (due to insufficient gener-
alization) on either side. Single-step scenarios have
253
a very high risk of combinatorial explosion of trans-
lation options (think cartesian product of all target
side factors) and/or of spurious ambiguity (several
derivations leading to the same output). Such added
ambiguity can lead to n-best lists with way fewer
unique items than the given n, which in turn ren-
ders MERT unstable, see also Bojar and Tamchyna
(2011). Serially connected setups (two as our Two-
Step or more) can lose relevant candidates between
the searches, unless some ambiguous representation
like lattices is passed between the steps.
An independent axis on which Moses setups can
be organized consists of the number and function of
factors on the source and the target side.
We use a very succint notation for the setups ex-
cept the ?complex? one: tX-Y denotes a translation
step between the factors X in the source language
and Y in the target language. Generation steps are
denoted with gY-Z, where both Y and Z are target-
side factors. Individual mapping steps are combined
with a plus, while individual source or target factors
are combined with an ?a?.
As a simple example, tF-F denotes the direct
translation from source form (F ) to the target form.
A linguistically motivated scenario with one search
can be written as tL-L+tT-T+gLaT-F : translate (1)
the lemma (L) to lemma, (2) the morphological tag
(T) to tag independently and (3) finally generate the
target form from the lemma and the tag.
We use two more operators: ?:? delimits al-
ternative decoding paths (Birch et al, 2007) used
within one search and ?=? delimits two independent
searches. A plausible setup is e.g. tF-LaT=tLaT-
F:tL-F motivated as follows: the source word form
is translated to the lemma and tag in the target lan-
guage. Then a second search (whose translation ta-
bles can be trained on larger monolingual data) con-
sists of two alternative decoding paths: either the
pair of L and T is translated into the target form, or
as a fallback, the tag is disregarded and the target
form is guessed only from the lemma (and the con-
text as scored by the language model). The example
also illustrated the priorities of the operators.
3 Common Settings
Throughout the experiments, we use the Moses
toolkit (Koehn et al, 2007) and GIZA++ (Och
Dataset Sents (cs/en) Toks (cs/en) Source
Small 197k parallel 4.2M/4.8M CzEng 1.0 news
Large 14.8M parallel 205M/236M CzEng 1.0 all
Mono 18M/50M 317M/1.265G WMT12 mono
Table 1: Summary of training data.
Decoding Path Language Models BLEU
tF-FaLaT form + lemma + tag 13.05?0.44
tF-FaT form + tag 13.01?0.44
tF-FaLaT form + tag 12.99?0.44
tF-F (baseline) form 12.42?0.44
tF-FaT form 12.19?0.44
tF-FaLaT form 12.08?0.45
Table 2: Direct en?cs translation (a single search with
one translation step only).
and Ney, 2000). The texts were processed us-
ing the Treex platform (Popel and Z?abokrtsky?,
2010)1, which included lemmatization and tagging
by Morce (Spoustova? et al, 2007). After the tag-
ging, we tokenized further so words like ?23-year?
or ?Aktualne.cz? became three tokens.
Our training data is summarized in Table 1.2
In most experiments reported here, we use the
Small dataset only. The language model (LM) for
these experiments is a 5-gram one based on the
target-side of Small only.
Our WMT12 submissions are based on the Large
and Mono data. The language model for the large
experiments uses 6-grams of forms and optionally
8-grams of morphological tags. As in previous
years, the language models are interpolated (to-
wards the best cross entropy on WMT08 dataset)
from domain-specific LMs, e.g. czeng-news, czeng-
techdoc, wmtmono-2011, wmtmono-2012.
Except where stated otherwise, we tune on the of-
ficial WMT10 test set and report BLEU (Papineni et
al., 2002) scores on the WMT11 test set.
4 Direct Setups
Table 2 lists our experiments with direct translation,
various factors and language models in our notation.
1http://ufal.mff.cuni.cz/treex/
2We did not include the parallel en-cs data made available
by the WMT12 organizers. This probably explains our loss
compared to UEDIN but allows a direct comparison with CU
TECTOMT, a deep syntactic MT based on the same data.
254
Decoding Paths LMs Avg. BLEU Eff. Nbl. Size
tL-L+tT-T+gLaT-F:tF-FaLaT F + L + T 13.31?0.06 12.24?1.33
tL-L+tT-T+gLaT-F F + L + T 13.30?0.05 40.33?3.82
tL-L+tT-T+gLaT-F F + T 13.17?0.01 39.91?2.58
tL-L+tT-T+gLaT-F:tF-FaLaT, 200-best-list F + L + T 13.15?0.24 20.47?5.63
tF-FaLaT F + L + T 13.13?0.06 34.28?3.08
tL-L+tT-T+gLaT-F:tF-FaLaT L + T 13.09?0.06 16.65?1.07
tF-FaT F + T 13.08?0.05 39.67?2.21
tL-L+tT-T+gLaT-F:tF-FaT F + T 13.01?0.43 14.87?5.04
tF-F (baseline) F 12.38?0.03 43.13?0.48
tL-L+tT-T+gLaT-F:tF-F F 12.30?0.03 17.83?3.27
Table 3: Results of three MERT runs of several single-step configurations.
Explicit modelling of target-side morphology im-
proves translation quality, compare tF-FaLaT with
the baseline tF-F. However, two results document
that if some detailed information is distinguished in
the output, it introduces target ambiguity and leads
to a loss in BLEU, unless the detailed information is
actually used in the language model: (1) tF-FaLaT
with LM on forms is worse than the baseline tF-F
but tF-FaLaT with all the three language models is
better, (2) tF-FaLaT with two LMs (forms and tags)
is negligibly worse than tF-FaT with the same lan-
guage models.
5 Single-Step Experiments
Single-step scenarios consist of more than one trans-
lation steps within a single search. We do not distin-
guish whether all the translation steps belong to the
same decoding path or to alternative decoding paths.
Table 3 lists several single-step configurations
(and three direct translations for a compari-
son). The single-step configurations always include
the linguistically-motivated tL-L+tT-T+gLaT-F with
varying language models and optionally with an al-
ternative decoding path to serve as the fallback.
Aware of the low stability of MERT (Clark et al,
2011), we run MERT three times and report the av-
erage BLEU score including the standard deviation.
The last column in Table 3 lists the average num-
ber of distinct candidates per sentence in the n-
best lists during MERT, dubbed ?effective n-best list
size?. Unless stated otherwise, we used 100-best
lists. We see that due to spurious ambiguity, e.g.
various segmentations of the input into phrases, the
effective size does not reach even a half of the limit.
We make three observations here:
(1) In this small data setting with a very morpho-
logically rich language, the complex setup tL-L+tT-
T+gLaT-F does not even need the alternative decod-
ing path tF-F. Ramanathan et al (2009) report gains
in English-to-Hindi translation and also probably do
not use alternative decoding paths.
(2) Reducing the range of language models used
leads to worse scores, which is in line with the ob-
servation made with direct setups. We are surprised
by the relative importance of the lemma-based LM.
(3) Alternative decoding paths significantly re-
duce effective n-best list size to just 12?18 unique
candidates per sentence. However, we don?t see
an obvious relation to the stability of MERT: the
standard deviations of BLEU average are very
similar except for two outliers: 13.15?0.24 and
13.01?0.43. One of the outliers, 13.15, is actually
a repeated run of the 13.31 with n-best-list size set
to 200. Here we see a slight increase in the effec-
tive size (20 instead of 12) but also a slight loss
in BLEU. We repeated the 13.31 experiment also
with n ? {300, 400, 500, 600}, three MERT runs for
each n. All the runs reached BLEU of about 13.30
except for one (n = 600) where the score dropped
to 11.50. The low result was obtained when MERT
ended at 25 iterations, the standard limit. On the
other hand, several successful runs also exhausted
the limit.
Figure 2 plots the BLEU scores in the 25 itera-
tions of the underperforming run with n = 600. The
MERT implementation in the Moses toolkit reports
at each iteration what we call ?predicted BLEU?,
i.e. the BLEU of translations selected by the current
255
 
0
 
0.0
2
 
0.0
4
 
0.0
6
 
0.0
8
 
0.1
 
0.1
2
 
0.1
4  0
 
5
 
10
 
15
 
20
 
25
 
0.1
285
 
0.1
29
 
0.1
295
 
0.1
3
 
0.1
305
 
0.1
31
 
0.1
315
BLEU
Iter
atio
ny2: 
Pre
dict
ed
y: P
red
icte
d
y: R
eal
Figure 2: Predicted and real devset BLEU scores.
weight settings from the (accumulated) n-best list.
We plot this predicted BLEU twice: once on the y2
axis alone and for the second time on the primary
y axis together with the real BLEU, i.e. the BLEU
of the dev set when Moses is actually run with the
weight settings. The real BLEU drops several times,
indicating that the prediction was misleading. Sim-
ilar drops were observed in all runs. With bad luck
as here, the iteration limit is reached when the opti-
mization is still recovering from such a drop.
To avoid such a pitfall, one should check the real
BLEU and continue or simply rerun the optimization
if the iteration limit was reached.
6 Two-Step Experiments
The linguistically motivated setups used in the pre-
vious sections are prohibitively expensive for large
data, see also Bojar et al (2009). A number of
researchers have thus tried diving the complexity
of search into two independent phases: (1) transla-
tion and reordering, and (2) conjugation and declina-
tion. The most promising results were obtained with
the second step predicting individual morphological
features using a specialized tool (Toutanova et al,
2008; Fraser et al, 2012). Here, we simply use one
more Moses search as Bojar and Kos (2010).
In the first step, source English gets translated to
a simplified Czech and in the second step, the sim-
plified Czech gets fully inflected.
6.1 Factors in Two-Step Setups
Two-step setups can use factors in the source, middle
or the target language. We experiment with factors
only in the middle language (affecting both the first
and the second search) and use only the form in both
source and target sides.
In the middle language, we experiment with one
or two factors. For presentation purposes, we always
speak about two factors: ?LOF? (?lemma or form?,
i.e. a representation of the lexical information) and
?MOT? (?modified tag?, i.e. representing the mor-
phological properties). In the single-factor experi-
ments the LOF and MOT are simply concatenated
into a token in the shape LOF+MOT.
Figure 3 illustrates the range of LOFs and MOTs
we experimented with. LOF0 and MOT0 are identi-
cal to the standard Czech lemma and morphological
tag as used e.g. in the Prague Dependency Treebank
(Hajic? et al, 2006).
LOF1 and MOT1 together make what Bojar and
Kos (2010) call ?pluslemma?. MOT1 is less com-
plex than the full tag by disregarding morphological
attributes not generally overt in the English source
side. For most words, LOF1 is simply the lemma,
but for frequent words, the full form is used. This
includes punctuation, pronouns and the verbs ?by?t?
(to be) and ?m??t? (to have).
MOT2 uses a more coarse grained part of speech
(POS) than MOT1. Depending on the POS, dif-
ferent attributes are included: gender and number
for nouns, pronouns, adjectives and verbs; case for
nouns, pronouns, adjectives and prepositions; nega-
tion for nouns and adjectives; tense and voice for
verbs and finally grade for adjectives. The remain-
ing grammatical categories are encoded using POS,
number, grade and negation.
6.2 Decoding Paths in Two-Step Setups
Each of the searches in the two-step setup can be
as complex as the various single-step configurations.
We test just one decoding path for the one or two
factors in the middle language.
All experiments with one middle factor (i.e. ?+?)
follow this config: tF-LOF+MOT = tLOF+MOT-F,
i.e. two direct translations where the first one pro-
duces the concatenated LOF and MOT tokens and
the second one consumes them. The first step uses a
5-gram LOF+MOT language model and the second
step uses a 5-gram LM based on forms.
This setup has the capacity to improve transla-
tion quality by producing forms of words never seen
aligned with a given source form. For example the
English word green would be needed in the parallel
256
Word Form LOF0 LOF1 MOT0 MOT1 MOT2 Gloss
lide? c?love?k c?love?k NNMP1-----A---1 NPA- NMP1-A people
by by?t by Vc------------- c--- V----- would
neoc?eka?vali oc?eka?vat oc?eka?vat VpMP---XR-NA--- pPN- VMP-RA expect
Figure 3: Examples of LOFs and MOTs used in our experiments.
Middle Factors 1 2
+ |
LOF0 +/|MOT0 11.11?0.48 12.42?0.48
LOF1 +/|MOT1 12.10?0.48 11.85?0.42
LOF1 +/|MOT2 11.87?0.51 12.47?0.51
Table 4: Two-step experiments.
data with all the morphological variants of the Czech
word zeleny?. Adding the middle step with appro-
priately reduced morphological information so that
only features overt in the source are represented in
the middle tokens (e.g. negation and number but not
the case) allows the model to find the necessary form
anywhere in the target-side data only:
green? zeleny?+NSA-?
{ zelene?ho (genitive)
zelene?mu (dative)
. . .
The experiments with two middle factors (i.e. ?|?)
use this path: tF-LOFaMOT = tLOFaMOT-F:LOF-
F. The first step is identical, except that now we use
two separate LMs, one for LOFs and one for MOTs.
The second step has two alternative decoding paths:
(1) as before, producing the form from both the LOF
and the MOT, and (2) ignoring the morphological
features from the source altogether and using just
target-side context to choose an appropriate form of
the word. This setup is capable of sacrificing ade-
quacy for a more fluent output.
6.3 Experiments with Two-Step Setups
Table 4 reports the BLEU scores when changing the
number of factors (?+? vs. ?|?) in the middle lan-
guage and the type of the LOF and MOT.
We see an interesting difference between MOT1
and MOT0 or 2. The more fine-grained MOT0 or 2
work better in the two-factor ?|? setup that allows
to disregard the MOT, while MOT1 works better in
the direct translation ?+?.
Overall, we see no improvement over the tF-F
baseline (BLEU of 12.42) and this is mainly due to
to the fact that we used Small data in both steps.
7 A Complex Moses Setup
Obviously, many setups fall under the ?complex?
category of our taxonomy, including also some sys-
tem combination approaches. We tried to combine
three Moses systems: (1) CU-BOJAR as described
below, (2) same setup like CU-BOJAR but optimized
towards 1-TER (Snover et al, 2006), and (3) a large-
data two-step setup.3 The system combination is
performed using a fourth Moses search that gets a
lattice (Dyer et al, 2008) of individual systems? out-
puts, performs an identity translation and scores the
candidates by language models and other features.
The lattice is created from the individual system out-
puts in the ROVER style (Matusov et al, 2008) uti-
lizing the source-to-hypothesis word alignments as
produced by the individual systems. We use our sim-
ple implementation for constructing the confusion
networks and converting them to the lattices. The
?combination Moses? was tuned on the WMT11 test
set towards BLEU. The resulting system is called
CU-POOR-COMB, because we felt it underperformed
the individual systems not only in BLEU but also in
an informal subjective evaluation.
Surprisingly, CU-POOR-COMB won the WMT12
automatic evaluation in TER. In the retrospect, this
is caused by TER overemphasizing word-level pre-
cision. CU-POOR-COMB skipped words not con-
firmed by several systems and its hypotheses are
shorter (18.1 toks/sent) than those by CU-BOJAR
(20.1 toks/sents) or the reference (21.9 toks/sent).
A quick manual inspection of 32 sentences suggests
that about one third or quarter of CU-POOR-COMB
suffer from some information loss whereas the rest
are acceptable or even better paraphrases. Prelim-
3The large two-step setup is identical to the one by (Bojar
and Kos, 2010), except that we use only the current Large and
Mono datasets as described in Section 3.
257
Our Scoring matrix.statmt.org
Test Set newstest-2011 newstest-2012
Metric BLEU TER*100 BLEU TER*100 BLEU TER
?cs
CU-POOR-COMB ?used?for? ?tuning? 14.17?0.53 64.07?0.53 14.0 0.741
CU-BOJAR (tFaT-FaT, lex. r.) 18.10?0.55 62.84?0.71 16.07?0.55 65.52?0.59 15.9 0.759
As ? but towards 1-TER 16.10?0.54 61.64?0.59 14.13?0.54 64.28?0.55 ? ?
Large Two-Step 17.34?0.57 63.47?0.66 15.37?0.54 65.85?0.57 ? ?
Unused (tFaT-FaT, dist. reord.) 18.07?0.56 62.74?0.70 15.92?0.57 65.50?0.60 ? ?
Unused (tF-FaT, dist. reord.) 17.85?0.58 63.13?0.68 15.73?0.55 65.85?0.58 ? ?
Unused (tF-F, lex. reord.) 17.73?0.58 63.04?0.68 15.61?0.57 65.76?0.58 ? ?
Unused (tFaT-F, dist. reord.) 17.62?0.56 62.97?0.70 15.33?0.58 65.70?0.59 ? ?
Unused (tF-F, dist. reord.) 17.51?0.57 63.32?0.69 15.48?0.56 65.79?0.58 ? ?
?en
CU-BOJAR (tF-F:tL-F, dist. reord.) 24.65?0.60 58.54?0.66 23.09?0.59 61.24?0.68 21.5 0.726
Unused (tF-F, dist. reord.) 24.62?0.59 58.66?0.66 22.90?0.56 61.63?0.67 ? ?
Table 5: Summary of large data runs and systems submitted to WMT12 manual evaluation. The upper part lists the
two submissions in en?cs translation and two more systems used in CU-POOR-COMB. The lower part of the table
shows the scores for CU-BOJAR when translating to English. All systems reported here use the Large and Mono data.
inary results of WMT 12 manual ranking indicate
that overall, our system combination performs poor.
8 Overview of Systems Submitted
Table 5 summarizes the scores for our two sys-
tem submissions. We report the scores in our to-
kenization on the official test sets of WMT11 and
WMT12 and also the scores as measured by http:
//matrix.statmt.org. Note that for the lat-
ter, we use the detokenized outputs processed by the
recommended normalization script.4
8.1 Details of CU-BOJAR for en?cs
We deliberately used only direct setups for the large
data and due to time constraints, we ran just a few
configurations, see Table 5.
We knew from previous years that including En-
glish (source) POS tag improves overall target sen-
tence structure: English words are often ambiguous
between noun and verb, so without the POS infor-
mation, verbs got often translated as nouns, render-
ing the sentence incomprehensible. Tagging and in-
cluding the source tag helps, as confirmed by the
tFaT-F setup being somewhat better than tF-F.
We also knew that target-side tag LM is helpful
(esp. if we can afford up to 8-grams in the LM).
This was confirmed by tF-FaT being better than tF-
F. Ultimately, we use tags on both sides: tFaT-FaT
4http://www.statmt.org/wmt11/
normalize-punctuation.perl
and get the best scores. This confirms that our par-
allel data is sufficiently large so that even the added
sparsity due to tags does not cause any trouble.
A little gain comes from a lexicalized reorder-
ing model (or-bi-fe) based on word forms, see CU-
BOJAR reaching 18.10 BLEU on WMT11 test set.
8.2 Details of CU-BOJAR for cs?en
For the translation into English, we tested just two
setups: tF-F and tF-F:tL-T. The latter setup falls
back to the Czech lemma, if the exact form is not
available. The gain is only small, because our paral-
lel data is already quite large.
9 Conclusion
We introduced a simple taxonomy of factored
phrase-based setups and conducted several probes
for English?Czech translation. We gained small
improvements in both small and large data settings.
We also warned about some common pitfalls: (1)
all target-side factors should be accompanied with a
language model to compensate for the added sparse-
ness, (2) alternative decoding paths significantly re-
duce the effective n-best list size, and (3) the infa-
mous instability of MERT can be caused by bad luck
at exhausted iteration limit.
On a general note, we learnt that a breadth-first
search for best configurations should be automated
as much as possible so that more human effort can
be invested into analysis.
258
References
Eleftherios Avramidis and Philipp Koehn. 2008. Enrich-
ing morphologically poor languages for statistical ma-
chine translation. In Proceedings of ACL-08: HLT,
pages 763?770, Columbus, Ohio, June. Association
for Computational Linguistics.
Ibrahim Badr, Rabih Zbib, and James Glass. 2008.
Segmentation for english-to-arabic statistical machine
translation. In Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguis-
tics on Human Language Technologies: Short Pa-
pers, HLT-Short ?08, pages 153?156, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. CCG Supertags in Factored Statistical Ma-
chine Translation. In Proceedings of the Second Work-
shop on Statistical Machine Translation, pages 9?16,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Ondr?ej Bojar and Kamil Kos. 2010. 2010 Failures in
English-Czech Phrase-Based MT. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 60?66, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Ondr?ej Bojar and Ales? Tamchyna. 2011. Improving
Translation Model by Monolingual Data. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 330?336, Edinburgh, Scot-
land, July. Association for Computational Linguistics.
Ondr?ej Bojar, David Marec?ek, Va?clav Nova?k, Martin
Popel, Jan Pta?c?ek, Jan Rous?, and Zdene?k Z?abokrtsky?.
2009. English-Czech MT in 2008. In Proceedings of
the Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March. Association for Compu-
tational Linguistics.
Ondr?ej Bojar. 2007. English-to-Czech Factored Machine
Translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 232?239,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statistical
machine translation: Controlling for optimizer insta-
bility. In ACL (Short Papers), pages 176?181. The
Association for Computer Linguistics.
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In Pro-
ceedings of ACL-08: HLT, pages 1012?1020, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-
bienne Cap. 2012. Modeling Inflection and Word-
Formation in SMT. In Proc. of EACL 2012. Associa-
tion for Computational Linguistics.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, Marie
Mikulova?, Zdene?k Z?abokrtsky?, and Magda S?evc???kova?
Raz??mova?. 2006. Prague Dependency Treebank 2.0.
LDC2006T01, ISBN: 1-58563-370-4.
Philipp Koehn and Hieu Hoang. 2007. Factored Transla-
tion Models. In Proc. of EMNLP.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In ACL
2007, Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Sessions,
pages 177?180, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Philipp Koehn, Barry Haddow, Philip Williams, and Hieu
Hoang. 2010. More linguistic annotation for statis-
tical machine translation. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation and
MetricsMATR, WMT ?10, pages 115?120, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Evgeny Matusov, Gregor Leusch, Rafael E. Banchs,
Nicola Bertoldi, Daniel Dechelotte, Marcello Fed-
erico, Muntsin Kolss, Young-Suk Lee, Jose B. Marino,
Matthias Paulik, Salim Roukos, Holger Schwenk, and
Hermann Ney. 2008. System Combination for Ma-
chine Translation of Spoken and Written Language.
IEEE Transactions on Audio, Speech and Language
Processing, 16(7):1222?1237, September.
Franz Josef Och and Hermann Ney. 2000. A Comparison
of Alignment Models for Statistical Machine Transla-
tion. In Proceedings of the 17th conference on Com-
putational linguistics, pages 1086?1090. Association
for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In ACL 2002, Proceed-
ings of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 311?318, Philadel-
phia, Pennsylvania.
Martin Popel and Zdene?k Z?abokrtsky?. 2010. TectoMT:
Modular NLP Framework. In Hrafn Loftsson, Eirikur
Ro?gnvaldsson, and Sigrun Helgadottir, editors, Lec-
ture Notes in Artificial Intelligence, Proceedings of the
7th International Conference on Advances in Natu-
ral Language Processing (IceTAL 2010), volume 6233
of Lecture Notes in Computer Science, pages 293?
259
304, Berlin / Heidelberg. Iceland Centre for Language
Technology (ICLT), Springer.
Ananthakrishnan Ramanathan, Hansraj Choudhary,
Avishek Ghosh, and Pushpak Bhattacharyya. 2009.
Case markers and morphology: addressing the crux
of the fluency problem in english-hindi smt. In
Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP: Volume 2 - Volume 2, ACL ?09, pages
800?808, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings AMTA, pages 223?231, August.
Drahom??ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel Kr-
bec, and Pavel Kve?ton?. 2007. The best of two worlds:
Cooperation of statistical and rule-based taggers for
czech. In Proceedings of the Workshop on Balto-
Slavonic Natural Language Processing, ACL 2007,
pages 67?74, Praha.
Sara Stymne. 2008. German Compounds in Factored
Statistical Machine Translation. In Bengt Nordstrm
and Aarne Ranta, editors, Advances in Natural Lan-
guage Processing, volume 5221 of Lecture Notes in
Computer Science, pages 464?475. Springer Berlin /
Heidelberg.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying morphology generation models to
machine translation. In Proceedings of ACL-08: HLT,
pages 514?522, Columbus, Ohio, June. Association
for Computational Linguistics.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-to-
morphology mapping in factored phrase-based statis-
tical machine translation from english to turkish. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 454?464,
Uppsala, Sweden, July. Association for Computational
Linguistics.
260
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 374?381,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Selecting Data for English-to-Czech Machine Translation ?
Ales? Tamchyna, Petra Galus?c?a?kova?, Amir Kamran, Milos? Stanojevic?, Ondr?ej Bojar
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostranske? na?m. 25, Praha 1, CZ-118 00, Czech Republic
{tamchyna,galuscakova,kamran,bojar}@ufal.mff.cuni.cz,
milosh.stanojevic@gmail.com
Abstract
We provide a few insights on data selection for
machine translation. We evaluate the quality
of the new CzEng 1.0, a parallel data source
used in WMT12. We describe a simple tech-
nique for reducing out-of-vocabulary rate af-
ter phrase extraction. We discuss the bene-
fits of tuning towards multiple reference trans-
lations for English-Czech language pair. We
introduce a novel approach to data selection
by full-text indexing and search: we select
sentences similar to the test set from a large
monolingual corpus and explore several op-
tions of incorporating them in a machine trans-
lation system. We show that this method can
improve translation quality. Finally, we de-
scribe our submitted system CU-TAMCH-BOJ.
1 Introduction
Selecting suitable data is important in all stages of
creating an SMT system. For training, the data size
plays an essential role, but the data should also be as
clean as possible. The new CzEng 1.0 was prepared
with the emphasis on data quality and we evaluate
it against the previous version to show whether the
effect for MT is positive.
Out-of-vocabulary rate is another problem related
to data selection. We present a simple technique to
reduce it by including words that became spurious
OOVs during phrase extraction.
? This work was supported by the project EuroMatrixPlus
(FP7-ICT-2007-3-231720 of the EU and 7E09003+7E11051 of
the Czech Republic) and the Czech Science Foundation grants
P406/11/1499 and P406/10/P259.
Another topic we explore is to use multiple refer-
ences for tuning to make the procedure more robust
as suggested by Dyer et al (2011). We evaluate this
approach for translating from English into Czech.
The main focus of our paper however lies in pre-
senting a method for data selection using full-text
search. We index a large monolingual corpus and
then extract sentences from it that are similar to the
input sentences. We use these sentences in several
ways: to create a new language model, a new phrase
table and a tuning set. The method can be seen as
a kind of domain adaptation. We show that it con-
tributes positively to translation quality and we pro-
vide a thorough evaluation.
2 Data and Tools
2.1 Comparison of CzEng 1.0 and 0.9
As this year?s WMT is the first to include the new
version of CzEng (Bojar et al, 2012b), we carried
out a few experiments to compare its suitability for
MT with its predecessor, CzEng 0.9. Apart from
size (which has almost doubled), there are impor-
tant differences between the two versions. In CzEng
0.9, the largest portion by far came from movie sub-
titles (a data source of varying quality), followed by
EU legislation and technical manuals. On the other
hand, CzEng 1.0 has over 4 million sentence pairs
from fiction and nearly the same amount of data
from EU legislation. Roughly 3 million sentence
pairs come from movie subtitles. This proportion
of domains suggests a higher quality of data. More-
over, sentences in CzEng 1.0 were automatically fil-
tered using a maximum entropy classifier that uti-
374
Vocab. [k]
Corpus and Domain Sents BLEU En Cs
CzEng 0.9
all 1M
14.77?0.12 187 360
CzEng 1.0 15.23?0.18 221 396
CzEng 0.9
news 100k
14.34?0.05 53 125
CzEng 1.0 14.01?0.13 47 113
Table 1: Comparison of CzEng 0.9 and 1.0.
lized a variety of features.
We trained contrastive phrase-based Moses SMT
systems?the first one on 1 million randomly se-
lected sentence pairs from CzEng 0.9, the other on
the same amount of data from CzEng 1.0. Another
contrastive pair of MT systems was based on small
in-domain data only: 100k sentences from the news
sections of CzEng 0.9 and 1.0. For each experiment,
the random selection was done 5 times. In both
experiments, identical data were used for the LM
(News Crawl corpus from 2011), tuning (WMT10
test set) and evaluation (WMT11 test set).
Table 1 shows the results. The ? sign in this case
denotes the standard deviation over the 5 experi-
ments (each with a different random sample of train-
ing data). The results indicate that overall, CzEng
1.0 is a more suitable source of parallel data?most
likely thanks to the more favorable distribution of
domains. However in the small in-domain setting,
using CzEng 0.9 data resulted in significantly higher
BLEU scores.
The vocabulary size of the news section seems to
have dropped since 0.9. We attribute this to the filter-
ing: sentences with obscure words are hard to align
so they are likely to be filtered out (the word align-
ment score as output by Giza++ received a large
weight in the classifier training). These unusual
words then do not appear in the vocabulary.
2.2 Lucene
Apache Lucene1 is a high performance open-source
search engine library written in Java. We use Lucene
to take advantage of the information retrieval (IR)
technique for domain adaptation. Each sentence of
a large corpus is indexed as a separate document; a
document is the unit of indexing and searching in
Lucene. The sentences (documents) can then be re-
1http://lucene.apache.org
trieved based on Lucene similarity formula2, given
a ?query corpus?. Lucene uses Boolean model for
initial filtering of documents. Vector Space Model
with a refined version of Tf-idf statistic is then used
to score the remaining candidates.
In the normal IR scenario, the query is usually
small. However, for domain adaptation a query can
be a whole corpus. Lucene does not allow such
big queries. This problem is resolved by taking
the query corpus sentence by sentence and search-
ing many times. The final score of a sentence in the
index is calculated as the average of the scores from
the sentence-level queries. Methods that make use
of this functionality are discussed in Section 5.
3 Reducing OOV by Relaxing Alignments
Out-of-vocabulary (OOV) rate has been shown to
increase during phrase extraction (Bojar and Kos,
2010). This is due to unfortunate alignment of some
words?no consistent phrase pair that includes them
can be extracted. This issue can be partially over-
come by adding translations of these ?lost? words
(according to Giza++ word alignment) to the ex-
tracted phrase table. This is not our original tech-
nique, it was suggested by Mermer and Saraclar
(2011), though it is not included in the published ab-
stract.
The extraction of phrases in the (hierarchical) de-
coder Jane (Stein et al, 2011) offers a range of sim-
ilar heuristics. Tinsley et al (2009) also observes
gains when extending the set of phrases consistent
with the word alignment by phrases consistent with
aligned parses.
We evaluated this technique on two sets of train-
ing data?the news section of CzEng 1.0 and the
whole CzEng 1.0. The OOV rate of the phrase table
was reduced nearly to the corpus OOV rate in both
cases, however the improvement was negligible?
only a handful of the newly added words occurred
in the test set. Table 2 shows the results. Trans-
lation performance using the improved phrase table
was identical to the baseline.
2http://tiny.cc/ca2ccw
375
Test Set OOV % New
CzEng Sections Baseline Reduced Phrases
news (197k sents) 3.69 3.66 12034
all (14.8M sents) 1.09 1.09 154204
Table 2: Source-side phrase table OOV.
Sections 1 reference 3 references
news 11.37?0.47 11.62?0.50
all 16.07?0.55 15.90?0.57
Table 3: BLEU scores on WMT12 test set when tuning
on WMT11 test set towards one or more references.
4 Tuning to Multiple Reference
Translations
Tuning towards multiple reference translations has
been shown to help translation quality, see Dyer et
al. (2011) and the cited works. Thanks to the other
references, more possible translations of each word
are considered correct, as well as various orderings
of words.
We tried two approaches: tuning to one true refer-
ence and one pseudo-reference, and tuning to multi-
ple human-translated references.
For the first method, which resembles computer-
generated references via paraphrasing as used in
(Dyer et al, 2011), we created the pseudo-reference
by translating the development set using TectoMT,
a deep syntactic MT with rich linguistic processing
implemented in the Treex platform3. We hoped that
the very different output of this decoder would be
beneficial for tuning, however we achieved no im-
provement at all.
For the second experiment we used 3 translations
of WMT11 test set. One is the true reference dis-
tributed for the shared task and two were translated
manually from the German version of the data into
Czech. We achieved a small improvement in final
BLEU score when training on a small data set. On
the complete constrained training data for WMT12,
there was no improvement?in fact, the BLEU score
as evaluated on the WMT12 test set was negligibly
lower. Table 3 summarizes our results. The ? sign
denotes the confidence bounds estimated via boot-
strap resampling (Koehn, 2004).
3http://ufal.ms.mff.cuni.cz/treex/
Used Selected Sel. Sents Avg
Models per Trans. Total BLEU?std
None ? 0 12.39?0.06
LM ? 16k ? rand. sel. 12.18?0.06
LM 3 16k 12.73?0.04
LM 100 502k 14.21?0.11
LM 1000 3.8M 15.12?0.08
LM All Sents 18.3M 15.55?0.11
Table 4: Results of experiments with Lucene, language
model adapted.
5 Experiments with Domain Adaptation
Domain adaptation is widely recognized as a tech-
nique which can significantly improve translation
quality (Wu et al, 2008; Bertoldi and Federico,
2009; Daume? and Jagarlamudi, 2011). In our ex-
periments we tried to select sentences close to the
source side of the test set and use them to improve
the final translation.
The parallel data used in this section are only
small: the news section of CzEng 1.0 (197k sentence
pairs, 4.2M Czech words, 4.8M English words). We
tuned the models on WMT09 test set and evaluated
on WMT11 test set. The techniques examined here
rely on a large monolingual corpus to select data
from. We used all the monolingual data provided by
the organizers of WMT11 (18.3M sentences, 316M
words).
5.1 Tailoring the Language Model
Our first attempt was to tailor the language model
to the test set. Our approach is similar to Zhao et
al. (2004). In Moore and Lewis (2010), the authors
compare several approaches to selecting data for LM
and Axelrod et al (2011) extend their ideas and ap-
ply them to MT.
Naturally, we only used the source side of the test
set. First we translated the test set using a baseline
translation system. Lucene indexer was then used
to select sentences similar to the translated ones in
the large target-side monolingual corpus. Finally, a
new language model was created from the selected
sentences.
The weight of the new LM has to reflect the im-
portance of the language model during both MERT
tuning as well as final application on (a different)
test set. If the new LM were based only on the final
376
test set, MERT would underestimate its value and
vice versa. Therefore, we actually translated both
our development (WMT09) as well as final test set
(WMT11) using the baseline model and created a
LM relevant to their union.
The results of performed experiments with do-
main adaptation are in Table 4. To compensate for
low stability of MERT, we ran the optimization five
times and report the average BLEU achieved. The
? value indicates the standard deviation of the five
runs.
The first row provides the scores for the baseline
experiment with no tailored language model. We
have run the experiment for three values of selected
sentences per one sentence of the test corpus: 3,
100 and 1000 closest-matching sentences were ex-
tracted. With more and more data in the LM, the
scores increase. The second line in Table 4 confirms
the usefulness of the sentence selection. Picking the
same amount of 16k sentences randomly performs
worse. As the last row indicates, taking all available
data leads to the best score.
Note that when selecting the sentences, we used
lemmas instead of word forms to reduce data sparse-
ness. So Lucene was actually indexing the lemma-
tized version of the monolingual data and the base-
line translation translated English lemmas to Czech
lemmas when creating the ?query corpus?. The final
models were created from the original sentences, not
their lemmatized versions.
5.2 Tailoring the Translation Model
Reverse self-training is a trick that allows to improve
the translation model using (target-side) monolin-
gual data and can lead to a performance improve-
ment (Bojar and Tamchyna, 2011; Lambert et al,
2011).
In our scenario, we translated the selected sen-
tences (in the opposite direction, i.e. from the target
into the source language). Then we created a new
translation model (in the original direction) based on
the alignment of selected sentences and their reverse
translation. This new model is finally combined with
the baseline model and weighted by MERT. The
whole scenario is shown in Figure 1.
The results of our experiments are in Table 5. We
ran the experiment with translation model adaptation
for 100 most similar sentences selected by Lucene.
Each experiment was again performed five times.
Due to the low stability of tuning, we also tried in-
creasing the size of n-best lists used by MERT.
Experiments with tailored translation model are
significantly better than the baseline but the im-
provement against the experiment with only the lan-
guage model adapted (with the corresponding 100
sentences selected) is very small.
5.3 Discussion of Domain Adaptation
Experiments
According to the results, using Lucene improves
translation performance already in the case when
only three sentences are selected for each translated
sentence. Our results are further supported by the
contrastive setup that used a language model cre-
ated from a random selection of the same number of
sentences?the translation quality even slightly de-
graded.
On the other hand, adding more sentences to lan-
guage model further improves results and the best
result is achieved when the language model is cre-
ated using the whole monolingual corpus. This
could have two reasons:
Too good domain match. The domain of the
whole monolingual corpus is too close to the test
corpus. Adding the whole monolingual corpus is
thus the best option. For more diverse monolingual
data, some domain-aware subsampling like our ap-
proach is likely to actually help.
Our style of retrieval. Our queries to Lucene
represent sentences as simple bags of words. Lucene
prefers less frequent words and the structure of the
sentence is therefore often ignored. For example it
prefers to retrieve sentences with the same proper
name rather than sentences with similar phrases or
longer expressions. This may not be the best option
for language modelling.
Our method can thus be useful mainly in the case
when the data available are too large to be processed
as a whole. It can also highly reduce the compu-
tation power and time necessary to achieve good
translation quality: the result achieved using the lan-
guage model created via Lucene for 1000 selected
sentences is not significantly worse than the result
achieved using the whole monolingual corpus but
the required data are 5 times smaller.
377
Tes
t Se
t [EN
]
Tra
nsl
ate
d T
S [C
S]
Sen
ten
ces
 Sim
ilar
 to 
Tra
nsl
ate
d T
S [C
S]
Re
ver
se 
Tra
nsl
ate
d S
ent
enc
es 
Sim
ilar
 to 
Tra
nsl
ate
d T
S [E
N]
Luc
ene
Ba
sel
ine
 Tr
ans
lati
on 
[EN
->C
S]
Do
ma
in A
dap
ted
 LM
Re
ver
se 
Tra
nsl
atio
n T
M
Re
ver
se 
Tra
nsl
atio
n [C
S->
EN]
Ori
gin
al L
M
Ori
gin
al T
M
Tes
t Se
t [EN
]
Tra
nsl
ate
d T
est
 Se
t [C
S]
Fin
al T
ran
sla
tion
 [EN
->C
S]
Figure 1: Scenario of reverse self-training.
Used N-Best Sel. Sents Sel. Sents Avg
Models per Trans. Sent. Total BLEU?std
None 100 ? 0 12.39?0.06
None 200 ? 0 12.4?0.03
LM + TM 100 100 502k 14.32?0.13
LM + TM 200 100 502k 14.36?0.07
Table 5: Results of experiments with Lucene, translation model applied.
5.4 Tuning Towards Selected Data
Domain adaptation can also be done by selecting a
suitable development corpus (Zheng et al, 2010; Li
et al, 2004). The final model parameters depend on
the domain of the development corpus. By choos-
ing a development corpus that is close to our test
set we might tune in the right direction. We imple-
mented this adaptation by querying the source side
of our large parallel corpus using the source side of
the test corpus. After that, the development corpus
is constructed from the selected sentences and their
corresponding reference translations.
This experiment uses a fixed model based on the
news section of CzEng 1.0. We only use different
tuning sets and run the MERT optimization. All the
resulting systems are tested on the WMT11 test set:
Baseline system is tuned on 2489 sentence pairs
selected randomly from whole CzEng 1.0 parallel
corpus. Lucene system uses 2489 sentence pairs se-
lected from CzEng 1.0 using Lucene. The selection
is done by choosing the most similar sentences to the
source side of the final test set. WMT10 system is
System avg BLEU?std
Baseline 11.41?0.25
Lucene 12.31?0.01
WMT10 12.37?0.02
Perfect selection 12.64?0.02
Bad selection 6.37?0.64
Table 6: Results of tuning with different corpora
tuned on 2489 sentence pairs of WMT10 test set. To
identify an upper bound, we also include a Perfect
selection system which is tuned on the final WMT11
test set. Naturally, this is not a fair competitor.
In order to make the results more reliable, it is
necessary to repeat the experiment several times
(Clark et al, 2011). Lucene and the WMT10 system
were tuned 3 times while baseline system was tuned
9 times because of randomness in selection of tun-
ing corpora (3 different tuning corpora each tuned 3
times). The results are shown in Table 6.
Even though the variance of the baseline system
is high (because we randomly selected corpora 3
378
times), the difference in scores between baseline
and Lucene system is high enough to conclude that
tuning on Lucene-selected corpus helps translation
quality. Still it does not give better BLEU score
than system tuned on WMT10 corpus. One possi-
ble reason is that the whole CzEng 1.0 is of some-
what lower quality than the news section. Given that
our final test set (WMT11) is also from the news
domain, tuning towards WMT10 corpus probably
leads to a better domain adaptation that tuning to-
wards all the domains in CzEng.
The tuning set must not overlap with the training
set. To illustrate the problem, we did a small exper-
iment with the same settings as above and randomly
selected 2489 sentences from training corpora. We
again ran the random selection 3 times and tuned 3
times with each of the extracted tuning sets, see the
?Bad selection? in Table 6.
In all the experiments with badly selected sen-
tences, the distortion and language model get an
extremely low weight compared to the weights of
translation model. This is because they are not use-
ful in translation of tuning data which was already
seen during training. Instead of reordering two short
phrases A and B, system already knows the transla-
tion of the phrase A B so no distortion is needed. On
unseen sentences, such weights lead to poor results.
This amplifies a drawback of our approach:
source texts have to be known prior to system tuning
or even before phrase extraction.
There are methods available that could tackle this
problem. Wuebker et al (2010) store phrase pair
counts per sentence when extracting phrases and
thus they can reestimate the probabilities when a
sentence has to be excluded from the phrase tables.
For large parallel corpora, suffix arrays (Callison-
Burch et al, 2005) have been used. Suffix arrays
allow for a quick retrieval of relevant sentence pairs,
the phrase extraction is postponed and performed on
the fly for each input sentence. It is trivial to fil-
ter out sentences belonging to the tuning set during
this delayed extraction. With dynamic suffix arrays
(Levenberg et al, 2010), one could even simply re-
move the tuning sentences from the suffix array.
6 Submitted Systems
This paper covers the submissions CU-TAMCH-BOJ.
We translated from English into Czech. Our setup
was very similar to CU-BOJAR (Bojar et al, 2012a),
but our primary submission is tuned on multiple ref-
erence translations as described in Section 4.
Apart from the additional references, this is a con-
strained setup. CzEng 1.0 were the only parallel data
used in training. We used a factored model to trans-
late the combination of English surface form and
part-of-speech tag into Czech form+POS. We used
separate 6-gram language models trained on CzEng
1.0 (interpolated by domain) and all News Crawl
corpora (18.3M setences, interpolated by years).
Additionaly, we created an 8-gram language model
on target POS tags. For reordering, we employed a
lexicalized model trained on CzEng 1.0.
Table 7 summarizes the official result of the pri-
mary submission and a contrastive baseline (tuned to
just one reference translation). There is a slight de-
crease in BLEU, but the translation error rate (TER)
is slightly better when more references were used.
The differences are however very small, suggesting
that tuning to more references did not have any sig-
nificant effect.
System BLEU TER
multiple references 14.5 0.765
contrastive baseline 14.6 0.774
Table 7: Scores of the submitted systems.
7 Conclusion
We showed that CzEng 1.0 is of better overall qual-
ity than its predecessor. We described a technique
for reducing phrase-table OOV rate, but achieved no
improvement for WMT12. Similarly, tuning to mul-
tiple references did not prove very beneficial.
We introduced a couple of techniques that exploit
full-text search in large corpora. We showed that
adding selected sentences as an additional LM im-
proves translations. Adding a new phrase table ac-
quired via reverse self-training resulted only in small
gains. Tuning to selected sentences resulted in a
better system than tuning to a random set. How-
ever the Lucene-selected corpus fails to outperform
good-quality in-domain tuning data.
379
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011.
Domain adaptation via pseudo in-domain data selec-
tion. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11, pages 355?362, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, StatMT
?09, pages 182?189, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Ondr?ej Bojar and Kamil Kos. 2010. 2010 Failures in
English-Czech Phrase-Based MT. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 60?66, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Ondr?ej Bojar and Ales? Tamchyna. 2011. Forms Wanted:
Training SMT on Monolingual Data. Abstract at
Machine Translation and Morphologically-Rich Lan-
guages. Research Workshop of the Israel Science
Foundation University of Haifa, Israel, January.
Ondr?ej Bojar, Bushra Jawaid, and Amir Kamran. 2012a.
Probes in a Taxonomy of Factored Phrase-Based Mod-
els. In Proceedings of the Seventh Workshop on Sta-
tistical Machine Translation, Montreal, Canada, June.
Association for Computational Linguistics. Submit-
ted.
Ondr?ej Bojar, Zdene?k Z?abokrtsky?, Ondr?ej Dus?ek, Pe-
tra Galus?c?a?kova?, Martin Majlis?, David Marec?ek, Jir???
Mars???k, Michal Nova?k, Martin Popel, and Ales? Tam-
chyna. 2012b. The Joy of Parallelism with CzEng
1.0. In Proceedings of LREC2012, Istanbul, Turkey,
May. ELRA, European Language Resources Associa-
tion. In print.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of the 43rd Annual Meeting
of the ACL, pages 255?262.
Jonathan Clark, Chris Dyer, Alon Lavie, and Noah Smith.
2011. Better Hypothesis Testing for Statistical Ma-
chine Translation: Controlling for Optimizer Instabil-
ity. In Proceedings of the Association for Computa-
tional Lingustics. Association for Computational Lin-
guistics.
Hal Daume?, III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining un-
seen words. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies: short papers - Vol-
ume 2, HLT ?11, pages 407?412, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Chris Dyer, Kevin Gimpel, Jonathan H. Clark, and
Noah A. Smith. 2011. The CMU-ARK German-
English Translation System. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 337?343, Edinburgh, Scotland, July. Associa-
tion for Computational Linguistics.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
EMNLP 2004, Barcelona, Spain.
Patrik Lambert, Holger Schwenk, Christophe Servan, and
Sadaf Abdul-Rauf. 2011. Investigations on trans-
lation model adaptation using monolingual data. In
Proceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 284?293, Edinburgh, Scot-
land, July. Association for Computational Linguistics.
Abby Levenberg, Chris Callison-Burch, and Miles Os-
borne. 2010. Stream-based translation models for
statistical machine translation. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the ACL, pages 394?402.
Mu Li, Yinggong Zhao, Dongdong Zhang, and Ming
Zhou. 2004. Adaptive development data selection for
log-linear model in statistical machine translation. In
In Proceedings of COLING 2004.
Coskun Mermer and Murat Saraclar. 2011. Un-
supervised Turkish Morphological Segmentation for
Statistical Machine Translation. Abstract at Ma-
chine Translation and Morphologically-Rich Lan-
guages. Research Workshop of the Israel Science
Foundation University of Haifa, Israel, January.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, ACLShort ?10, pages 220?224, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Daniel Stein, David Vilar, Stephan Peitz, Markus Freitag,
Matthias Huck, and Hermann Ney. 2011. A Guide to
Jane, an Open Source Hierarchical Translation Toolkit.
Prague Bulletin of Mathematical Linguistics, 95:5?18,
March.
John Tinsley, Mary Hearne, and Andy Way. 2009. Ex-
ploiting parallel treebanks to improve phrase-based
statistical machine translation. In Alexander F. Gel-
bukh, editor, CICLing, volume 5449 of Lecture Notes
in Computer Science, pages 318?331. Springer.
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine translation
with domain dictionary and monolingual corpora. In
Proceedings of the 22nd International Conference on
Computational Linguistics - Volume 1, COLING ?08,
pages 993?1000, Stroudsburg, PA, USA. Association
for Computational Linguistics.
380
Joern Wuebker, Arne Mauser, and Hermann Ney. 2010.
Training phrase translation models with leaving-one-
out. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
475?484.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Proceed-
ings of the 20th international conference on Compu-
tational Linguistics, COLING ?04, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Zhongguang Zheng, Zhongjun He, Yao Meng, and Hao
Yu. 2010. Domain adaptation for statistical machine
translation in development corpus selection. In Uni-
versal Communication Symposium (IUCS), 2010 4th
International, pages 2 ?7, oct.
381
Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 30?38,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Towards a Predicate-Argument Evaluation for MT
Ondr?ej Bojar?, Dekai Wu?
? Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
? HKUST, Human Language Technology Center,
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
bojar@ufal.mff.cuni.cz, dekai@cs.ust.hk
Abstract
HMEANT (Lo and Wu, 2011a) is a man-
ual MT evaluation technique that focuses on
predicate-argument structure of the sentence.
We relate HMEANT to an established lin-
guistic theory, highlighting the possibilities of
reusing existing knowledge and resources for
interpreting and automating HMEANT. We
apply HMEANT to a new language, Czech
in particular, by evaluating a set of English-
to-Czech MT systems. HMEANT proves to
correlate with manual rankings at the sentence
level better than a range of automatic met-
rics. However, the main contribution of this
paper is the identification of several issues
of HMEANT annotation and our proposal on
how to resolve them.
1 Introduction
Manual evaluation of machine translation output is
a tricky enterprise. It has been long recognized
that different evaluation techniques lead to different
outcomes, e.g. Blanchon et al (2004) mention an
evaluation carried out in 1972 where the very same
Russian-to-English MT outputs were scored 4.5 out
of the maximum 5 points by prospective users of
the system but only 1 out of 5 by teachers of En-
glish. Throughout the years, many techniques were
explored with more or less of a success.
The two-scale scoring for adequacy and fluency
used in NIST evaluation has been abandoned by
some evaluation campaigns, most notably the WMT
shared task series, see Koehn and Monz (2006)
through Callison-Burch et al (2012)1. Since 2008,
WMT uses a simple relative ranking of MT out-
puts as its primary manual evaluation technique:
the annotator is presented with up to 5 MT out-
puts for a given input sentence and the task is to
rank them from best to worst (ties allowed) on what-
ever criteria he or she deems appropriate. While this
single-scale relative ranking is perhaps faster to an-
notate and reaches a higher inter- and intra-annotator
agreement than the (absolute) fluency and adequacy
(Callison-Burch et al, 2007), the technique and its
evaluation are still far from satisfactory. Bojar et
al. (2011) observe several discrepancies in the in-
terpretation of the rankings, partly due to the high
load on human annotators (the comparison of sev-
eral long sentences at once, among other issues) but
partly also due to technicalities of the calculation.
Lo and Wu (2011a) present an interesting evalua-
tion technique called MEANT (or HMEANT if car-
ried out by humans), the core of which lies in as-
sessing whether the key elements in the predicate-
argument structure of the sentence have been pre-
served. In other words, lay annotators are check-
ing, if they recognize who did what to whom, when,
where and why from the MT outputs and whether
the respective role fillers convey the same meaning
as in the reference translation. HMEANT has been
shown to correlate reasonably well with manual ad-
equacy and ranking evaluations. It is relatively fast
and should lend itself to full automatization. On
the other hand, HMEANT was so far tested only on
translation into English and with just three compet-
ing MT systems.
1http://www.statmt.org/wmt06 till wmt12
30
In this work, we extend the application of
HMEANT to evaluating MT into Czech, a mor-
phologically rich language with relatively free word
order. The paper is structured as follows: Sec-
tion 2 presents the technical details of HMEANT
and relates HMEANT to an established linguistic
theory that underlies the Prague dependency tree-
banks (Hajic? et al, 2006; Hajic? et al, 2012) and
several other works. We also suggest possible ben-
efits of this coupling such as the reuse of tools. In
Section 3, we describe the setup and results of our
HMEANT experiment. Since this is the first time
HMEANT is applied to a new language, Section 4
constitutes the main contribution of this work. We
point out at several problems of HMEANT and pro-
pose a remedy, the empirical evaluation of which
however remains for future work. Section 5 con-
cludes our observations.
2 Relating HMEANT and Valency Theory
of FGD
2.1 HMEANT Annotation Procedure
HMEANT is designed to be simple and fast. The
annotation consists of two steps: (1) semantic role
labelling, SRL in the sequel, and (2) alignment of
roles between the hypothesis and the reference.
The annotation guidelines are deliberately mini-
malistic, so that even inexpert people can learn them
quickly. The complete guidelines for SRL are given
in Figure 1 and it takes less than 15 minutes to train
an unskilled person.
In the alignment task, the annotators first indicate
which frames in the reference and the hypothesis
correspond to each other. In the second step, they
align all matching role fillers to each other and also
mark the translation as ?Correct? or ?Partial?.
The HMEANT calculation then evaluates the f-
score of the predicates and their role fillers in a given
sentence. An important aspect of the calculation is
that unmatched predicates with all their role fillers
are excluded from the calculation.
2.2 Functional Generative Description
The core ideas of HMEANT follow the case gram-
mar (Fillmore, 1968) or PropBank (Palmer et al,
2005) and can be also directly related to an estab-
lished linguistic theory which was primarily devel-
Semantic frames summarize a sentence using a simple event
structure that captures essential parts of the meaning like
?who did what to whom, when, where, why and how?.
Phrases or clauses that express meanings can be identified as
playing a particular semantic role in the sentence. In other
words, semantic frames are the systematic abstraction of the
meanings in a sentence.
The following is the list of the semantic roles to be used in
HMEANT evaluation:
Agent (who) Action (did)
Experiencer or Patient (what) Benefactive (whom)
Temporal (when) Locative (where)
Purpose (why) Manner (how)
Degree or Extent (how) Modal (how) [may, should, ...]
Negation (how) [not] Other adverbial argument (how)
You may consider the Action predicate to be the central
event, while the other roles modify the Action to give a more
detailed description of the event. Each semantic frame con-
tains exactly one Action and any number of other roles.
Please note that the Action predicate must be exactly ONE
single word.
There may be multiple semantic frames in one sentence, be-
cause a sentence may be constructed to describe multiple
events and each semantic frame captures only one event.
Figure 1: Semantic role labeling guidelines of HMEANT.
oped for Czech, namely the Functional Generative
Description (Sgall, 1967; Sgall et al, 1986). The
theory defines so-called ?tectogrammatical? layer (t-
layer). At the t-layer, each sentence is represented as
a dependency tree with just content words as sepa-
rate nodes. All auxiliary words are ?hidden? into
attributes of the corresponding t-nodes. Moreover,
ellipsis is restored to some extent, so e.g. dropped
subject pronouns do have a corresponding t-node.
An important element of FGD is the valency the-
ory (Panevova?, 1980) which introduces empirical
linguistic tests to distinguish between what other
theories would call complements vs. adjuncts and
postulates the relationship between the set of verb
modifiers as observed in the sentence and the set of
valency slots that should be listed in a valency dic-
tionary. This aspect could provide a further refine-
ment of HMEANT, e.g. weighing complements and
adjuncts differently.
FGD has been thoroughly tested and refined dur-
ing the development of the Prague Dependency
Treebank (Hajic? et al, 2006)2 and the parallel
Prague Czech-English Dependency Treebank (Hajic?
2http://ufal.mff.cuni.cz/pdt2.0/
31
et al, 2012)3. Note that the latter is a translation
of all the 49k sentences of the Penn Treebank WSJ
section. Both English and Czech sentences are man-
ually annotated at the tectogrammatical layer, where
the English layer is based on the Penn annotation
and manually adapted for t-layer. Both languages in-
clude their respective valency lexicons and the work
on a bilingual valency lexicon is being developed
(S?indlerova? and Bojar, 2010).
A range of automatic tools to convert plain text up
to the t-layer exist for both English and Czech. Most
of them are now part of the Treex platform (Popel
and Z?abokrtsky?, 2010)4 and they were successfully
used in automatic annotation of 15 million parallel
sentences (Bojar et al, 2012)5 as well as other NLP
tasks including English-to-Czech MT. Recently, sig-
nificant effort was also invested in parsing not quite
correct output of MT systems into Czech for the
purposes of rule-based grammar correction (Rosa et
al., 2012). Establishing the automatic pipeline for
MEANT should be relatively easy with these tools
at hand.
2.3 HMEANT vs. FGD Valency
The formulation of HMEANT in terms of FGD is
straightforward: it is the f-score of matched t-nodes
for predicates and the subtrees of their immediate
dependents in the t-trees of the hypothesis and the
reference.
HMEANT uses a simple web-based annotation
interface which operates on the surface form of the
sentence. Annotators mark the predicate and their
complementations as contiguous spans in the sen-
tence. While this seems natural when we want
lay people to annotate, it brings some problems,
see Section 4. A linguistically adequate interface
would allow to mark tectogrammatical nodes and
subtrees in the t-layer, however, the customizable
editor TrEd6 used for manual annotation of t-layer
is too heavy for our purposes both in terms of speed
and complexity of user interface.
Perhaps the best option we plan to investigate in
future research is a mixed approach: the interface
would display only the text version of the sentence
3http://ufal.mff.cuni.cz/pcedt2.0/
4http://ufal.mff.cuni.cz/treex/
5http://ufal.mff.cuni.cz/czeng/
6http://ufal.mff.cuni.cz/tred/
HMEANT 0.2833
METEOR 0.2167
WER 0.1708
CDER 0.1375
NIST 0.1167
TER 0.1167
PER 0.0208
BLEU 0.0125
Table 1: Kendall?s ? for sentence-level correlation with
human rankings.
but it would internally know the (automatic) t-layer
structure. Selecting any word that corresponds to
the t-node of a verb would automatically extend the
selection to all other belongings of the t-node, i.e.
all auxiliaries of the verb. For role fillers, select-
ing any word from the role filler would select the
whole t-layer subtree. In order to handle errors in
the automatic t-layer annotation, the interface would
certainly need to allow manual selection and de-
selection of words, providing valuable feedback to
the automatic tools.
3 An Experiment in English-Czech MT
Evaluation
In this first study, we selected 50 sentences from the
English-to-Czech WMT12 manual evaluation. The
sentences were chosen to overlap with the standard
WMT ranking procedure (see Section 3.1) as much
as possible.
In total, 13 MT systems participated in this trans-
lation direction. We allocated 14 annotators (one
annotator for the SRL of the reference) so that no-
body saw the same sentence translated by more sys-
tems. The hypotheses were shuffled so every annota-
tor got samples from all systems as well as the refer-
ence. Unfortunately, time constraints and the large
number of MT systems prevented us from collect-
ing overlapping annotations, so we cannot evaluate
inter-annotator agreement.
Following Lo and Wu (2011a) and Callison-
Burch et al (2012), we report Kendall?s ? rank cor-
relation coefficients for sentence-level rankings as
provided by a range of automatic metrics and our
HMEANT. The gold standard are the manual WMT
rankings. See Table 1.
32
We see that HMEANT achieves a better correla-
tion than all the tested automatic metrics, although
in absolute terms, the correlation is not very high.
Lo and Wu (2011b) report ? for HMEANT of up to
0.49 and Lo and Wu (2011a) observe ? in the range
0.33 to 0.43. These figures are not comparable to our
result for several reasons: we evaluated 13 and not
just 3 MT systems, the gold standard for us are over-
all system rankings, not just adequacy judgments as
for Lo and Wu (2011b), and we evaluate translation
to Czech, not English. Callison-Burch et al (2012)
report ? for several automatic metrics on the whole
WMT12 English-to-Czech dataset, the best of which
correlates at ? = 0.18. The only common metric is
METEOR and it reaches 0.16 on the whole WMT12
set.7 In line with our observation, Czech-to-English
correlations reported by Callison-Burch et al (2012)
are higher: the best metric achieves 0.28 and aver-
ages 0.25 across four source languages.
The overall low sentence-level correlation of
our HMEANT and WMT12 rankings is obviously
caused to some extent by the problems we identi-
fied, see Section 4 below. On the other hand, it is
quite possible that the WMT-style rankings taken as
the gold standard are of a disputable quality them-
selves, see Section 3.1 or the detailed report on inter-
annotator agreement and a long discussion on inter-
preting the rankings in Callison-Burch et al (2012).
Last but not least, it is likely that HMEANT and
manual ranking simply measure different properties
of MT outputs. The Kendall?s ? is thus not an ulti-
mate meta-evaluation metric for us.
3.1 WMT-Style Rankings
This section illustrates some issues with the WMT
rankings when used for system-level evaluation. Ob-
viously, at the sentence level, the rankings can be-
have differently but the system-level evaluation ben-
efits from a large number of manual labels.
In the WMT-style rankings, humans are provided
with no more than 5 system outputs for a given sen-
tence at once. The task is to rank these 5 systems
relatively to each other, ties allowed.
Following Bojar et al (2011), we report three
possible evaluation regimes (or ?interpretations?) of
7It is possible that Callison-Burch et al (2012) use some-
what different METEOR settings apart from the different subset
of the data.
these 5-fold rankings to obtain system-level scores.
The first step is shared: all pairwise comparisons
implied by the 5-fold ranking are extracted. For
each system, we then report the percentage of cases
where the system won the pairwise comparison. Our
default interpretation is to exclude all ties from the
calculation, labelled ?Ties Ignored?, i.e. winswins + losses .
The former WMT interpretation (up to 2011) was to
include ties in both the numerator and the denomi-
nator, i.e. wins + tieswins+ties+losses denoted ?? Others?. WMT
summary paper also reports ?> Others? where the
ties are included in the denominator only, thus giv-
ing credit to systems that are different.
As we see in Table 2, each of the interpretations
leads to different rankings of the systems. More im-
portantly, the underlying set of sentences also affects
the result. For instance, the system ONLINEA jumps
to the second position in ?Ties Ignored? if we con-
sider only the 50 sentences used in our HMEANT
evaluation. To some extent, the differences are
caused by the lower number of observations. While
?All-No Ties? is based on 2893?134 pairwise com-
parisons per system, ?50-No Ties? is based on just
186?30 observations. Moreover, not all systems
came up among the 5 ranked systems for a given
sentence. In our 50 sentences, only 7.3?2.1 systems
were compared per sentence. On the full set of sen-
tences, this figure drops to 5.9?1.7.
4 Problems of HMEANT Annotation
We asked our annotators to take notes and report
any problems. On the positive side, some annota-
tors familiar with the WMT ranking evaluation felt
that in both phases of HMEANT, they ?knew what
they were doing and why?. In the ranking task, it
is unfortunately quite common that the annotator is
asked to rank incomparably bad hypotheses. In such
cases, the annotator probably tries to follow some
subjective and unspoken criteria, which often leads
to a lower in inter- and intra-annotator agreement.
On the negative side, we observed many problems
of the current version of HMEANT, and we propose
a remedy for all of them. We disregard minor tech-
nical issues of the annotation interface and focus on
the design decisions. The only technical limitation
worth mentioning was the inability to return to pre-
vious sentences. In some cases, this even caused the
33
Interpretation Ties Ignored ? Others > Others
Sentences All 50 All 50 All 50
cu-depfix 66.4 72.5 73.0 77.5 53.3 59.4
onlineB 63.0 61.4 70.5 69.3 50.3 49.0
uedin-wmt12 55.8 60.3 63.6 66.3 46.0 o 51.1
cu-tamch-boj 55.6 54.6 o 64.7 62.1 44.2 45.7
cu-bojar 2012 54.3 53.2 o 64.1 o 62.2 42.6 43.0
CU TectoMT 53.1 o 54.9 60.5 59.8 o 44.6 o 49.0
onlineA 52.9 o 61.4 o 60.8 o 66.7 o 44.0 o 53.0
pctrans2010 47.7 o 54.1 55.1 o 60.1 40.9 o 47.1
commercial2 46.0 51.3 54.6 59.5 38.7 42.7
cu-poor-comb 44.1 41.6 o 54.7 50.5 35.7 35.2
uk-dan-moses 43.5 33.2 53.4 44.2 o 35.9 27.7
SFU 36.1 31.0 46.8 43.0 30.0 25.6
jhu-hiero 32.2 26.7 43.2 36.0 27.0 23.3
Table 2: WMT12 system-level ranking results in three different evaluation regimes evaluated either on all sentences
or just the 50 sentences that were subject to our HMEANT annotation. The table is sorted along the first column and
the symbol ?o? in other columns marks items out of sequence.
annotators to skip parts of the annotation altogether,
because they clicked Next Sentence instead of the
Next Frame button.
Note that the impact of the problems on the final
HMEANT reliability varies. What causes just minor
hesitations in the SRL phase can lead to complete
annotation failures in the Alignment phase and vice
versa. We list the problems in decreasing severity,
based on our observations as well as the number of
annotators who complained about the given issue.
4.1 Vague SRL Guidelines
The first group of problems is caused by the SRL
guidelines being (deliberately) too succinct and de-
veloped primarily for English.
Complex predicates. Out of the many possible
cases where predicates are described using several
words, SRL guidelines mention just modal verbs and
reserve a label for them (assuming that the main verb
will be chosen as the Action, i.e. the predicate it-
self). This goes against the syntactic properties of
Czech and other languages, where the modal verb is
the one that conjugates and it is only complemented
by the content verb in infinitive. Some annotators
thus decided to mark such cases as a pair of nested
frames.
The problem becomes more apparent for other
classes of verbs, such as phasic verbs (e.g. ?to be-
gin?), which naturally lead to nested frames.
A specific problem for Czech mentioned by al-
most all annotators, was the copula verb ?to be?.
Here, the meaning-bearing element is actually the
adjective that follows (e.g. ?to be glad to . . . ?).
HMEANT forced the annotators to use e.g. the Ex-
periencer slot for the non-verbal part of this complex
predicate. In the negated form, ?nen?? (is not)?, some
annotators even marked the copula as Negation and
the non-verbal part as the Action.
No verb at all. HMEANT does not permit to an-
notate frames with no predicate. There are however
at least two frequent cases that deserve this option:
(1) the whole sentence can be a nominal construc-
tion such as the title of a section, and (2) an MT
system may erroneously omit the verb, while the re-
maining slot fillers are understandable and the whole
meaning of the sentence can be also guessed. Giving
no credit to such a sentence at all seems too strict. In
some cases, it was possible for the annotators to find
a substitute word for the Action role, e.g. a noun that
should have been translated as the verb.
A related issue was caused by the uncertainty to
what extent the frame annotation should go. There
are many nouns derived from verbs that also bear va-
lency. FGD acknowledges this and valency lexicons
for Czech do include also many of such nouns. If the
34
Reference Oblec?ky mus??me vystr???hat z c?asopisu?
Gloss clothes we-must cut from magazines
Roles Experiencer Modal Action Locative
Meaning We must cut the clothes (assuming paper toys) from magazines
Hypothesis Mus??me vyr???znout oblec?en?? z c?asopisu?
Gloss We-must cut clothes from magazines
Roles Modal Action Experiencer
Figure 2: An example of PP-attachment mismatch. While it is (almost) obvious from the word order of the reference
that the preposition phrase ?z c?asopisu?? is a separate filler, it was marked as part of the Experiencer role in the
hypothesis. In the alignment phase, there is no way to align the single Experiencer slot of the hypothesis onto the two
slots (Experiencer, Locative) if the reference.
instructions are not clear in this respect, it is quite
possible that one annotator creates frames for such
nouns and the other does not, causing a mismatch in
the Alignment phase.
PP-attachment. The problem of attaching prepo-
sitional phrases to verbs or to other noun phrases
is well acknowledged in many languages including
English and Czech. See an example in Figure 2.
A complete solution of the problem in the SRL
phase will never be possible, because there are nat-
urally ambiguous cases where each annotator can
prefer a different reading. However, the Align-
ment phase should be somehow prepared for the in-
evitable mismatches.
Unclear role labels. Insufficient role labels.
The set of role labels of HMEANT is very simple
compared to the set of edge labels (called ?func-
tors?) in the tectogrammatical annotation. Several
annotators mentioned that the HMEANT roleset is
hard to use especially for passive constructions or
verbs with a secondary object.
Because the final HMEANT calculation requires
aligned fillers to match in their role labels, the agree-
ment on role labels is important. We suggest experi-
menting also with a variant of HMEANT that would
disregard the labels altogether.
Other problematic cases are sentences where sev-
eral role fillers appear to belong to the same type,
e.g. Locative: ?Byl pr?evezen (He was transported)
| do nemocnice (to the hospital) | v za?chranne?m vr-
tuln??ku (in a helicopter)?. While it is semantically
obvious that the hospital is not in the helicopter, so
this is not a PP-attachment problem, some annota-
tors still mark both Locatives jointly as a single slot,
causing the same slot mismatch. It is also possible
that the annotator has actually assigned the Locative
label twice but the annotation interface interpreted
all the words as belonging to one filler only.
Coreference. The SRL guidelines are not specific
on handling of slot fillers realized as pronouns (or
even dropped pronouns). If we consider a sentence
like ?It is the man who wins?, it is not clear which
words should be marked as the Agent of the Action
?wins?. There are three candidates, all equally cor-
rect from the purely semantic point of view: ?it?,
?the man? and ?who?.
A natural choice would be to select the closest
word referring to the respective object, however, in
constructions of complex verbs or in pro-drop lan-
guages the object may not be explicitly stated in
the syntactically closest position. Depending on the
annotators? decisions, this can lead to a mismatch
in the number of slots in the subsequent Alignment
phase.
Other problems. Some annotators mentioned a
few other problems. One of them were paratactic
constructions: the frame-labelling procedure does
not allow to distinguish between sentences like ?It
is windy and it rains? vs. ?It is windy but it rains?,
because neither ?and? nor ?but? are a slot filler. Sim-
ilarly, expressions like ?for example? do not seem to
constitute a slot filler but still somehow refine the
meaning of the sentence and should be preserved in
the translation.
One annotator suggested that the importance of
the SRL phase should be emphasized and the anno-
tators should be pushed towards annotating as much
as they can, e.g. also by highlighting all verbs in
the sentence, in order to provide enough frames and
fillers to align in the second phase.
35
Reference Opily? r?idic? te?z?ce zrane?n
Gloss A drunken driver seriously injured
Roles Agent Extent Action
Meaning A drunken driver is seriously injured.
Hypothesis Opily? r?idic? va?z?ne? zranil
Gloss A drunken driver seriously injured (active form)
Roles Agent Extent Action
Meaning A drunken driver seriously injured (someone).
Figure 3: A mismatch of the meanings of the predicates. Other roles in the frames match perfectly.
The following sections describe problems of the
Alignment phase.
4.2 Correctness of the Predicate
HMEANT alignment phase allows the annotators to
either align or not align a pair of frames. There is
no option to indicate that the match of the predicates
themselves is somewhat incorrect. Once the predi-
cates are aligned, the user can only match individual
fillers, possibly penalizing partial mismatches.
Figure 3 illustrates this issue on a real example
from our data. Once the annotator decides to align
the frames, there is no way to indicate that the mean-
ing was reversed by the translation.
What native speakers of Czech also feel is that
the MT output in Figure 3 is incomplete, an Ex-
periencer is missing. A similar example from the
data is the hypothesis ?Sve?dek ozna?mil policii. (The
witness informed/announced the police.)? The verb
?ozna?mit (inform/announce)? in Czech requires the
message (perhaps the Experiencer in the HMEANT
terminology), similarly to the English ?announce?
but unlike ?inform?. The valency theory of FGD for-
mally describes the problem as a missing slot filler
and given a valency dictionary, such errors can be
even identified automatically.
On the other hand, it should be noted that a mis-
match in the predicate alone does not mean that the
translation is incorrect. An example in our data was
the phrase ?dokud se souc?asne? ume?n?? nedoc?kalo ve
V??dni nove?ho sta?nku? vs. ?nez? souc?asne? ume?n?? ve
V??dni dostalo novy? domov?. Both versions mean
?until contemporary art in Vienna was given a new
home? but due to the different conjunction chosen
(?dokud/nez?, till/until?), one of the verbs has to be
negated.
4.3 Need for M:N Frame Alignment
The majority of our annotators complained that
complex predicates such as phasal verbs or copula
constructions as well as muddled MT output with
no verb often render the frame matching impossi-
ble. If the reference and the hypothesis differ in the
number of frames, then it is also almost certain that
the role fillers observed in the two sentences will be
distributed differently among the frames, prohibiting
filler alignment.
A viable solution would be allow merging of
frames during the Alignment phase, which is equiva-
lent to allowing many-to-many alignment of frames.
The sets of role fillers would be simply unioned, im-
proving the chance for filler alignment.
4.4 Need for M:N Slot Alignment
Inherent ambiguities like PP-attachment or spuri-
ous differences in SRL prevent from 1-1 slot align-
ment rather frequently. A solution would be to allow
many-to-many alignments of slot fillers.
4.5 Partial Adequacy vs. Partial Fluency
The original HMEANT Alignment guidelines say to
mark an aligned slot pair as Correct or Partial match.
(Mismatching slots should not be aligned at all.) A
Partial match is described as:
Role fillers in MT express part of the
meaning of the aligned role fillers in the
reference translation. Do NOT penalize
extra meaning unless it belongs in other
role fillers in the reference translation.
The second sentence of the instructions is prob-
ably aimed at cases where the MT expresses more
than the reference does, which is possible because
36
the translator may have removed part of the content
or because the source and the reference are both not
quite literal translations from a third language. A
clarifying example of this case in the instructions is
highly desirable.
What our annotators noticed were cases where the
translation was semantically adequate but contained
e.g. an agreement mismatch or another grammar er-
ror. The instructions should exemplify, if this is to
be treated as a Correct or Partial match. Optionally,
the Partial match could be split into three separate
cases: partially inadequate, partially disfluent, and
partially inadequate and disfluent.
4.6 Summary of Suggested HMEANT Fixes
To summarize the observations above, our experi-
ence with HMEANT was overall positive, but we
propose several changes in the design to improve the
reliability of the annotations:
SRL Phase:
? The SRL guidelines should be kept as simple as
they are, but more examples and especially ex-
amples of incorrect MT output should be pro-
vided.
? The Action should be allowed to consist of sev-
eral words, including non-adjacent ones.
? The possibility of using automatic t-layer anno-
tation tools should be explored, at least to pre-
annotate which words form a multi-word pred-
icate or role filler.
Alignment Phase:
? The annotator must be able to indicate a partial
or incorrect match of the predicates themselves.
? Both frames as well as fillers should support
M:N alignment to overcome a range of natu-
rally appearing as well as spurious mismatches
in the two SRL annotations.
? Examples of anaphoric expressions should be
included in the guidelines, stressing that any el-
ement of the anaphora chain should be treated
as an appropriate representant of the role filler.
? The Partial match could distinguish between
an error in adequacy or fluency, or rather, the
Alignment guidelines should explicitly provide
examples of both types and ask the annotators
to disregard the difference.
Technical Changes:
? The annotators need to be able to go back
within each phase. (The division between
the SRL and Alignment phases should be pre-
served.)
We do not expect any of the proposed changes to
negatively impact annotation time. Actually, some
speedup may be obtained from the suggested pre-
annotation and also from a reduced hesitation of the
annotators in the alignment phase thanks to the M:N
alignment possibility.
5 Conclusion
We applied HMEANT, a technique for manual eval-
uation of MT quality based on predicate-argument
structure, to a new language, Czech. The experiment
confirmed that HMEANT is applicable in this set-
ting, outperforming automatic metrics in sentence-
level correlation with manual rankings.
During our annotation, we identified a range of
problems in the current HMEANT design. We thus
propose a few modifications to the technique and
also suggest backing HMEANT with a linguistic
theory of deep syntax, opening the avenue to au-
tomating the metric using available tools.
Acknowledgments
We would like to thank our annotators for all the com-
ments and also Chi-kiu Lo, Karteek Addanki, Anand
Karthik Tumuluru, and Avishek Kumar Thakur for ad-
ministering the annotation interface. This work was sup-
ported by the project EuroMatrixPlus (FP7-ICT-2007-
3-231720 of the EU and 7E09003+7E11051 of the
Czech Republic) and the Czech Science Foundation
grants P406/11/1499 and P406/10/P259 (Ondr?ej Bo-
jar); and by the Defense Advanced Research Projects
Agency (DARPA) under BOLT contract no. HR0011-12-
C-0016, and GALE contract nos. HR0011-06-C-0022
and HR0011-06-C-0023; by the European Union under
the FP7 grant agreement no. 287658; and by the Hong
Kong Research Grants Council (RGC) research grant
GRF621008 (Dekai Wu). Any opinions, findings and
conclusions or recommendations expressed in this mate-
rial are those of the authors and do not necessarily reflect
the views of the RGC, EU, or DARPA.
37
References
Herve? Blanchon, Christian Boitet, and Laurent Besacier.
2004. Spoken Dialogue Translation Systems Evalua-
tion: Results, New Trends, Problems and Proposals.
In Proceedings of International Conference on Spoken
Language Processing ICSLP 2004, Jeju Island, Korea,
October.
Ondr?ej Bojar, Milos? Ercegovc?evic?, Martin Popel, and
Omar Zaidan. 2011. A Grain of Salt for the WMT
Manual Evaluation. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 1?11,
Edinburgh, Scotland, July. Association for Computa-
tional Linguistics.
Ondr?ej Bojar, Zdene?k Z?abokrtsky?, Ondr?ej Dus?ek, Pe-
tra Galus?c?a?kova?, Martin Majlis?, David Marec?ek, Jir???
Mars???k, Michal Nova?k, Martin Popel, and Ales? Tam-
chyna. 2012. The Joy of Parallelism with CzEng 1.0.
In Proceedings of the Eighth International Language
Resources and Evaluation Conference (LREC?12), Is-
tanbul, Turkey, May. ELRA, European Language Re-
sources Association. In print.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (Meta-)
Evaluation of Machine Translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, pages 136?158, Prague, Czech Republic, June.
Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical machine
translation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, Montreal, Canada,
June. Association for Computational Linguistics.
Charles J. Fillmore. 1968. The Case for Case. In E. Bach
and R. Harms, editors, Universals in Linguistic The-
ory, pages 1?90. New York.
Jan Hajic?, Eva Hajic?ova?, Jarmila Panevova?, Petr Sgall,
Ondr?ej Bojar, Silvie Cinkova?, Eva Fuc???kova?, Marie
Mikulova?, Petr Pajas, Jan Popelka, Jir??? Semecky?,
Jana S?indlerova?, Jan S?te?pa?nek, Josef Toman, Zden?ka
Ures?ova?, and Zdene?k Z?abokrtsky?. 2012. Announc-
ing Prague Czech-English Dependency Treebank 2.0.
In Proceedings of the Eighth International Language
Resources and Evaluation Conference (LREC?12), Is-
tanbul, Turkey, May. ELRA, European Language Re-
sources Association. In print.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, Marie
Mikulova?, Zdene?k Z?abokrtsky?, and Magda S?evc???kova?
Raz??mova?. 2006. Prague Dependency Treebank 2.0.
LDC2006T01, ISBN: 1-58563-370-4.
Philipp Koehn and Christof Monz. 2006. Manual and au-
tomatic evaluation of machine translation between eu-
ropean languages. In Proceedings on the Workshop on
Statistical Machine Translation, pages 102?121, New
York City, June. Association for Computational Lin-
guistics.
Chi-kiu Lo and Dekai Wu. 2011a. Meant: An inexpen-
sive, high-accuracy, semi-automatic metric for evalu-
ating translation utility based on semantic roles. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 220?229, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Chi-kiu Lo and Dekai Wu. 2011b. Structured vs. flat
semantic role representations for machine translation
evaluation. In Proceedings of the Fifth Workshop on
Syntax, Semantics and Structure in Statistical Trans-
lation, SSST-5, pages 10?20, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71?106.
Jarmila Panevova?. 1980. Formy a funkce ve stavbe? c?eske?
ve?ty [Forms and functions in the structure of the Czech
sentence]. Academia, Prague, Czech Republic.
Martin Popel and Zdene?k Z?abokrtsky?. 2010. TectoMT:
Modular NLP Framework. In Hrafn Loftsson, Eirikur
Ro?gnvaldsson, and Sigrun Helgadottir, editors, Lec-
ture Notes in Artificial Intelligence, Proceedings of the
7th International Conference on Advances in Natu-
ral Language Processing (IceTAL 2010), volume 6233
of Lecture Notes in Computer Science, pages 293?
304, Berlin / Heidelberg. Iceland Centre for Language
Technology (ICLT), Springer.
Rudolf Rosa, David Marec?ek, and Ondr?ej Dus?ek. 2012.
DEPFIX: A System for Automatic Correction of
Czech MT Outputs. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, Mon-
treal, Canada, June. Association for Computational
Linguistics. Submitted.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The Meaning of the Sentence and Its Semantic and
Pragmatic Aspects. Academia/Reidel Publishing
Company, Prague, Czech Republic/Dordrecht, Nether-
lands.
Petr Sgall. 1967. Generativn?? popis jazyka a c?eska? dek-
linace. Academia, Prague, Czech Republic.
Jana S?indlerova? and Ondr?ej Bojar. 2010. Building
a Bilingual ValLex Using Treebank Token Align-
ment: First Observations. In Proceedings of the Sev-
enth International Language Resources and Evalua-
tion (LREC?10), pages 304?309, Valletta, Malta, May.
ELRA, European Language Resources Association.
38
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 1?44,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Findings of the 2013 Workshop on Statistical Machine Translation
Ondr?ej Bojar
Charles University in Prague
Christian Buck
University of Edinburgh
Chris Callison-Burch
University of Pennsylvania
Christian Federmann
Saarland University
Barry Haddow
University of Edinburgh
Philipp Koehn
University of Edinburgh
Christof Monz
University of Amsterdam
Matt Post
Johns Hopkins University
Radu Soricut
Google
Lucia Specia
University of Sheffield
Abstract
We present the results of the WMT13
shared tasks, which included a translation
task, a task for run-time estimation of ma-
chine translation quality, and an unoffi-
cial metrics task. This year, 143 machine
translation systems were submitted to the
ten translation tasks from 23 institutions.
An additional 6 anonymized systems were
included, and were then evaluated both au-
tomatically and manually, in our largest
manual evaluation to date. The quality es-
timation task had four subtasks, with a to-
tal of 14 teams, submitting 55 entries.
1 Introduction
We present the results of the shared tasks of
the Workshop on Statistical Machine Translation
(WMT) held at ACL 2013. This workshop builds
on seven previous WMT workshops (Koehn and
Monz, 2006; Callison-Burch et al, 2007, 2008,
2009, 2010, 2011, 2012).
This year we conducted three official tasks: a
translation task, a human evaluation of transla-
tion results, and a quality estimation task.1 In
the translation task (?2), participants were asked
to translate a shared test set, optionally restrict-
ing themselves to the provided training data. We
held ten translation tasks this year, between En-
glish and each of Czech, French, German, Span-
ish, and Russian. The Russian translation tasks
were new this year, and were also the most popu-
lar. The system outputs for each task were evalu-
ated both automatically and manually.
The human evaluation task (?3) involves ask-
ing human judges to rank sentences output by
anonymized systems. We obtained large numbers
of rankings from two groups: researchers (who
1The traditional metrics task is evaluated in a separate pa-
per (Macha?c?ek and Bojar, 2013).
contributed evaluations proportional to the number
of tasks they entered) and workers on Amazon?s
Mechanical Turk (who were paid). This year?s ef-
fort was our largest yet by a wide margin; we man-
aged to collect an order of magnitude more judg-
ments than in the past, allowing us to achieve sta-
tistical significance on the majority of the pairwise
system rankings. This year, we are also clustering
the systems according to these significance results,
instead of presenting a total ordering over systems.
The focus of the quality estimation task (?6)
is to produce real-time estimates of sentence- or
word-level machine translation quality. This task
has potential usefulness in a range of settings, such
as prioritizing output for human post-editing, or
selecting the best translations from a number of
systems. This year the following subtasks were
proposed: prediction of percentage of word edits
necessary to fix a sentence, ranking of up to five al-
ternative translations for a given source sentence,
prediction of post-editing time for a sentence, and
prediction of word-level scores for a given trans-
lation (correct/incorrect and types of edits). The
datasets included English-Spanish and German-
English news translations produced by a number
of machine translation systems. This marks the
second year we have conducted this task.
The primary objectives of WMT are to evaluate
the state of the art in machine translation, to dis-
seminate common test sets and public training data
with published performance numbers, and to re-
fine evaluation methodologies for machine trans-
lation. As before, all of the data, translations,
and collected human judgments are publicly avail-
able.2 We hope these datasets serve as a valu-
able resource for research into statistical machine
translation, system combination, and automatic
evaluation or prediction of translation quality.
2http://statmt.org/wmt13/results.html
1
2 Overview of the Translation Task
The recurring task of the workshop examines
translation between English and five other lan-
guages: German, Spanish, French, Czech, and ?
new this year ? Russian. We created a test set for
each language pair by translating newspaper arti-
cles and provided training data.
2.1 Test data
The test data for this year?s task was selected from
news stories from online sources. A total of 52
articles were selected, in roughly equal amounts
from a variety of Czech, English, French, German,
Spanish, and Russian news sites:3
Czech: aktua?lne?.cz (1), CTK (1), den??k (1),
iDNES.cz (3), lidovky.cz (1), Novinky.cz (2)
French: Cyber Presse (3), Le Devoir (1), Le
Monde (3), Liberation (2)
Spanish: ABC.es (2), BBC Spanish (1), El Peri-
odico (1), Milenio (3), Noroeste (1), Primera
Hora (3)
English: BBC (2), CNN (2), Economist (1),
Guardian (1), New York Times (2), The Tele-
graph (1)
German: Der Standard (1), Deutsche Welle (1),
FAZ (1), Frankfurter Rundschau (2), Welt (2)
Russian: AIF (2), BBC Russian (2), Izvestiya (1),
Rosbalt (1), Vesti (1)
The stories were translated by the professional
translation agency Capita, funded by the EU
Framework Programme 7 project MosesCore, and
by Yandex, a Russian search engine.4 All of the
translations were done directly, and not via an in-
termediate language.
2.2 Training data
As in past years we provided parallel corpora to
train translation models, monolingual corpora to
train language models, and development sets to
tune system parameters. Some training corpora
were identical from last year (Europarl5, United
Nations, French-English 109 corpus, CzEng),
some were updated (News Commentary, mono-
lingual data), and new corpora were added (Com-
mon Crawl (Smith et al, 2013), Russian-English
3For more details see the XML test files. The docid tag
gives the source and the date for each document in the test set,
and the origlang tag indicates the original source language.
4http://www.yandex.com/
5As of Fall 2011, the proceedings of the European Parlia-
ment are no longer translated into all official languages.
parallel data provided by Yandex, Russian-English
Wikipedia Headlines provided by CMU).
Some statistics about the training materials are
given in Figure 1.
2.3 Submitted systems
We received 143 submissions from 23 institu-
tions. The participating institutions and their en-
try names are listed in Table 1; each system did
not necessarily appear in all translation tasks. We
also included three commercial off-the-shelf MT
systems and three online statistical MT systems,6
which we anonymized.
For presentation of the results, systems are
treated as either constrained or unconstrained, de-
pending on whether their models were trained only
on the provided data. Since we do not know how
they were built, these online and commercial sys-
tems are treated as unconstrained during the auto-
matic and human evaluations.
3 Human Evaluation
As with past workshops, we contend that auto-
matic measures of machine translation quality are
an imperfect substitute for human assessments.
We therefore conduct a manual evaluation of the
system outputs and define its results to be the prin-
cipal ranking of the workshop. In this section, we
describe how we collected this data and compute
the results, and then present the official results of
the ranking.
We run the evaluation campaign using an up-
dated version of Appraise (Federmann, 2012); the
tool has been extended to support collecting judg-
ments using Amazon?s Mechanical Turk, replac-
ing the annotation system used in previous WMTs.
The software, including all changes made for this
year?s workshop, is available from GitHub.7
This year differs from prior years in a few im-
portant ways:
? We collected about ten times more judgments
that we have in the past, using judgments
from both participants in the shared task and
non-experts hired on Amazon?s Mechanical
Turk.
? Instead of presenting a total ordering of sys-
tems for each pair, we cluster them and report
a ranking over the clusters.
6Thanks to Herve? Saint-Amand and Martin Popel for har-
vesting these entries.
7https://github.com/cfedermann/Appraise
2
Europarl Parallel Corpus
Spanish? English French? English German? English Czech? English
Sentences 1,965,734 2,007,723 1,920,209 646,605
Words 56,895,229 54,420,026 60,125,563 55,642,101 50,486,398 53,008,851 14,946,399 17,376,433
Distinct words 176,258 117,481 140,915 118,404 381,583 115,966 172,461 63,039
News Commentary Parallel Corpus
Spanish? English French? English German? English Czech? English Russian? English
Sentences 174,441 157,168 178,221 140,324 150,217
Words 5,116,388 4,520,796 4,928,135 4,066,721 4,597,904 4,541,058 3,206,423 3,507,249 3,841,950 4,008,949
Distinct words 84,273 61,693 69,028 58,295 142,461 61,761 138,991 54,270 145,997 57,991
Common Crawl Parallel Corpus
Spanish? English French? English German? English Czech? English Russian? English
Sentences 1,845,286 3,244,152 2,399,123 161,838 878,386
Words 49,561,060 46,861,758 91,328,790 81,096,306 54,575,405 58,870,638 3,529,783 3,927,378 21,018,793 21,535,122
Distinct words 710,755 640,778 889,291 859,017 1,640,835 823,480 210,170 128,212 764,203 432,062
United Nations Parallel Corpus
Spanish? English French? English
Sentences 11,196,913 12,886,831
Words 318,788,686 365,127,098 411,916,781 360,341,450
Distinct words 593,567 581,339 565,553 666,077
109 Word Parallel Corpus
French? English
Sentences 22,520,400
Words 811,203,407 668,412,817
Distinct words 2,738,882 2,861,836
CzEng Parallel Corpus
Czech? English
Sentences 14,833,358
Words 200,658,857 228,040,794
Distinct words 1,389,803 920,824
Yandex 1M Parallel Corpus
Russian? English
Sentences 1,000,000
Words 24,121,459 26,107,293
Distinct words 701,809 387,646
Wiki Headlines Parallel Corpus
Russian? English
Sentences 514,859
Words 1,191,474 1,230,644
Distinct words 282,989 251,328
Europarl Language Model Data
English Spanish French German Czech
Sentence 2,218,201 2,123,835 2,190,579 2,176,537 668,595
Words 59,848,044 60,476,282 63,439,791 53,534,167 14,946,399
Distinct words 123,059 181,837 145,496 394,781 172,461
News Language Model Data
English Spanish French German Czech Russian
Sentence 68,521,621 13,384,314 21,195,476 54,619,789 27,540,749 19,912,911
Words 1,613,778,461 386,014,234 524,541,570 983,818,841 456,271,247 351,595,790
Distinct words 3,392,137 1,163,825 1,590,187 6,814,953 2,655,813 2,195,112
News Test Set
English Spanish French German Czech Russian
Sentences 3000
Words 64,810 73,659 73,659 63,412 57,050 58,327
Distinct words 8,935 10,601 11,441 12,189 15,324 15,736
Figure 1: Statistics for the training and test sets used in the translation task. The number of words and the number of distinct
words (case-insensitive) is based on the provided tokenizer.
3
ID Institution
BALAGUR Yandex School of Data Analysis (Borisov et al, 2013)
CMU
CMU-TREE-TO-TREE
Carnegie Mellon University (Ammar et al, 2013)
CU-BOJAR,
CU-DEPFIX,
CU-TAMCHYNA
Charles University in Prague (Bojar et al, 2013)
CU-KAREL, CU-ZEMAN Charles University in Prague (B??lek and Zeman, 2013)
CU-PHRASEFIX,
CU-TECTOMT
Charles University in Prague (Galus?c?a?kova? et al, 2013)
DCU Dublin City University (Rubino et al, 2013a)
DCU-FDA Dublin City University (Bicici, 2013a)
DCU-OKITA Dublin City University (Okita et al, 2013)
DESRT Universita` di Pisa (Miceli Barone and Attardi, 2013)
ITS-LATL University of Geneva
JHU Johns Hopkins University (Post et al, 2013)
KIT Karlsruhe Institute of Technology (Cho et al, 2013)
LIA Universite? d?Avignon (Huet et al, 2013)
LIMSI LIMSI (Allauzen et al, 2013)
MES-* Munich / Edinburgh / Stuttgart (Durrani et al, 2013a; Weller et al, 2013)
OMNIFLUENT SAIC (Matusov and Leusch, 2013)
PROMT PROMT Automated Translations Solutions
QCRI-MES Qatar / Munich / Edinburgh / Stuttgart (Sajjad et al, 2013)
QUAERO QUAERO (Peitz et al, 2013a)
RWTH RWTH Aachen (Peitz et al, 2013b)
SHEF University of Sheffield
STANFORD Stanford University (Green et al, 2013)
TALP-UPC TALP Research Centre (Formiga et al, 2013a)
TUBITAK TU?BI?TAK-BI?LGEM (Durgar El-Kahlout and Mermer, 2013)
UCAM University of Cambridge (Pino et al, 2013)
UEDIN,
UEDIN-HEAFIELD
University of Edinburgh (Durrani et al, 2013b)
UEDIN-SYNTAX University of Edinburgh (Nadejde et al, 2013)
UMD University of Maryland (Eidelman et al, 2013)
UU Uppsala University (Stymne et al, 2013)
COMMERCIAL-1,2,3 Anonymized commercial systems
ONLINE-A,B,G Anonymized online systems
Table 1: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the
commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore
anonymized in a fashion consistent with previous years of the workshop.
4
3.1 Ranking translations of sentences
The ranking among systems is produced by col-
lecting a large number of rankings between the
systems? translations. Every language task had
many participating systems (the largest was 19,
for the Russian-English task). Rather than asking
judges to provide a complete ordering over all the
translations of a source segment, we instead ran-
domly select five systems and ask the judge to rank
just those. We call each of these a ranking task.
A screenshot of the ranking interface is shown in
Figure 2.
For each ranking task, the judge is presented
with a source segment, a reference translation,
and the outputs of five systems (anonymized and
randomly-ordered). The following simple instruc-
tions are provided:
You are shown a source sentence fol-
lowed by several candidate translations.
Your task is to rank the translations from
best to worst (ties are allowed).
The rankings of the systems are numbered from 1
to 5, with 1 being the best translation and 5 be-
ing the worst. Each ranking task has the potential
to provide 10 pairwise rankings, and fewer if the
judge chooses any ties. For example, the ranking
{A:1, B:2, C:4, D:3, E:5}
provides 10 pairwise rankings, while the ranking
{A:3, B:3, C:4, D:3, E:1}
provides just 7. The absolute value of the ranking
or the degree of difference is not considered.
We use the collected pairwise rankings to assign
each system a score that reflects how highly that
system was usually ranked by the annotators. The
score for some system A reflects how frequently it
was judged to be better than other systems when
compared on the same segment; its score is the
number of pairwise rankings where it was judged
to be better, divided by the total number of non-
tying pairwise comparisons. These scores were
used to compute clusters of systems and rankings
between them (?3.4).
3.2 Collecting the data
A goal this year was to collect enough data to
achieve statistical significance in the rankings. We
distributed the workload among two groups of
judges: researchers and Turkers. The researcher
group comprised partipants in the shared task, who
were asked to contribute judgments on 300 sen-
tences for each system they contributed. The re-
searcher evaluation was held over three weeks
from May 17?June 7, and yielded about 280k pair-
wise rankings.
The Turker group was composed of non-expert
annotators hired on Amazon?s Mechanical Turk
(MTurk). A basic unit of work on MTurk is called
a Human Intelligence Task (HIT) and included
three ranking tasks, for which we paid $0.25. To
ensure that the Turkers provided high quality an-
notations, this portion of the evaluation was be-
gun after the researcher portion had completed,
enabling us to embed controls in the form of high-
consensus pairwise rankings in the Turker HITs.
To build these controls, we collected ranking tasks
containing pairwise rankings with a high degree of
researcher consensus. An example task is here:
SENTENCE 504
SOURCE Vor den heiligen Sta?tten verbeugen
REFERENCE Let?s worship the holy places
SYSTEM A Before the holy sites curtain
SYSTEM B Before we bow to the Holy Places
SYSTEM C To the holy sites bow
SYSTEM D Bow down to the holy sites
SYSTEM E Before the holy sites pay
MATRIX
A B C D E
A - 0 0 0 3
B 5 - 0 1 5
C 6 6 - 0 6
D 6 8 5 - 6
E 0 0 0 0 -
Matrix entry Mi,j records the number of re-
searchers who judged System i to be better than
System j. We use as controls pairwise judgments
for which |Mi,j?Mj,i| > 5, i.e., judgments where
the researcher consensus ran strongly in one direc-
tion. We rejected HITs from Turkers who encoun-
tered at least 10 of these controls and failed more
than 50% of them.
There were 463 people who participated in the
Turker portion of the manual evaluation, contribut-
ing 664k pairwise rankings from Turkers who
passed the controls. Together with the researcher
judgments, we collected close to a million pair-
wise rankings, compared to 101k collected last
year: a ten-fold increase. Table 2 contains more
detail.
5
Figure 2: Screenshot of the Appraise interface used in the human evaluation campaign. The annotator is presented with a
source segment, a reference translation, and the outputs of five systems (anonymized and randomly-ordered) and has to rank
these according to their translation quality, ties are allowed. For technical reasons, annotators on Amazon?s Mechanical Turk
received all three ranking tasks for a single HIT on a single page, one upon the other.
3.3 Annotator agreement
Each year we calculate annotator agreement
scores for the human evaluation as a measure of
the reliability of the rankings. We measured pair-
wise agreement among annotators using Cohen?s
kappa coefficient (?) (Cohen, 1960), which is de-
fined as
? = P (A)? P (E)1? P (E)
where P (A) is the proportion of times that the an-
notators agree, and P (E) is the proportion of time
that they would agree by chance. Note that ? is ba-
sically a normalized version of P (A), one which
takes into account how meaningful it is for anno-
tators to agree with each other, by incorporating
P (E). The values for ? range from 0 to 1, with
zero indicating no agreement and 1 perfect agree-
ment.
We calculate P (A) by examining all pairs of
systems which had been judged by two or more
judges, and calculating the proportion of time that
they agreed that A > B, A = B, or A < B. In
other words, P (A) is the empirical, observed rate
at which annotators agree, in the context of pair-
wise comparisons.
As for P (E), it should capture the probability
that two annotators would agree randomly. There-
fore:
P (E) = P (A>B)2 + P (A=B)2 + P (A<B)2
Note that each of the three probabilities in P (E)?s
definition are squared to reflect the fact that we are
considering the chance that two annotators would
agree by chance. Each of these probabilities is
computed empirically, by observing how often an-
notators actually rank two systems as being tied.
Table 3 gives ? values for inter-annotator agree-
ment for WMT11?WMT13 while Table 4 de-
tails intra-annotator agreement scores. Due to the
change of annotation software, we used a slightly
different way of computing annotator agreement
scores. Therefore, we chose to re-compute values
for previous WMTs to allow for a fair comparison.
The exact interpretation of the kappa coefficient is
difficult, but according to Landis and Koch (1977),
0?0.2 is slight, 0.2?0.4 is fair, 0.4?0.6 is moderate,
6
LANGUAGE PAIR Systems Rankings Average
Czech-English 11 85,469 7,769.91
English-Czech 12 102,842 8,570.17
German-English 17 128,668 7,568.71
English-German 15 77,286 5,152.40
Spanish-English 12 67,832 5,652.67
English-Spanish 13 60,464 4,651.08
French-English 13 80,741 6,210.85
English-French 17 100,783 5,928.41
Russian-English 19 151,422 7,969.58
English-Russian 14 87,323 6,237.36
Total 148 942,840 6,370.54
WMT12 103 101,969 999.69
WMT11 133 63,045 474.02
Table 2: Amount of data collected in the WMT13 manual evaluation. The final two rows report summary information from the
previous two workshops.
LANGUAGE PAIR WMT11 WMT12 WMT13 WMT13r WMT13m
Czech-English 0.400 0.311 0.244 0.342 0.279
English-Czech 0.460 0.359 0.168 0.408 0.075
German-English 0.324 0.385 0.299 0.443 0.324
English-German 0.378 0.356 0.267 0.457 0.239
Spanish-English 0.494 0.298 0.277 0.415 0.295
English-Spanish 0.367 0.254 0.206 0.333 0.249
French-English 0.402 0.272 0.275 0.405 0.321
English-French 0.406 0.296 0.231 0.434 0.237
Russian-English ? ? 0.278 0.315 0.324
English-Russian ? ? 0.243 0.416 0.207
Table 3: ? scores measuring inter-annotator agreement. The WMT13r and WMT13m columns provide breakdowns for re-
searcher annotations and MTurk annotations, respectively. See Table 4 for corresponding intra-annotator agreement scores.
0.6?0.8 is substantial, and 0.8?1.0 is almost per-
fect. We find that the agreement rates are more or
less the same as in prior years.
The WMT13 column contains both researcher
and Turker annotations at a roughly 1:2 ratio. The
final two columns break out agreement numbers
between these two groups. The researcher agree-
ment rates are similar to agreement rates from past
years, while the Turker agreement are well below
researcher agreement rates, varying widely, but of-
ten comparable to WMT11 and WMT12. Clearly,
researchers are providing us with more consistent
opinions, but whether these differences are ex-
plained by Turkers racing through jobs, the partic-
ularities that inform researchers judging systems
they know well, or something else, is hard to tell.
Intra-annotator agreement scores are also on par
from last year?s level, and are often much better.
We observe better intra-annotator agreement for
researchers compared to Turkers.
As a small test, we varied the threshold of ac-
ceptance against the controls for the Turker data
alone and computed inter-annotator agreement
scores on the datasets for the Russian?English task
(the only language pair where we had enough data
at high thresholds). Table 5 shows that higher
thresholds do indeed give us better agreements,
but not monotonically. The increasing ?s sug-
gests that we can find a segment of Turkers who
do a better job and that perhaps a slightly higher
threshold of 0.6 would serve us better, while the
remaining difference against the researchers sug-
gests there may be different mindsets informing
the decisions. In any case, getting the best perfor-
mance out of the Turkers remains difficult.
3.4 System Score
Given the multitude of pairwise comparisons, we
would like to rank the systems according to a
single score computed for each system. In re-
7
LANGUAGE PAIR WMT11 WMT12 WMT13 WMT13r WMT13m
Czech-English 0.597 0.454 0.479 0.483 0.478
English-Czech 0.601 0.390 0.290 0.547 0.242
German-English 0.576 0.392 0.535 0.643 0.515
English-German 0.528 0.433 0.498 0.649 0.452
Spanish-English 0.574 1.000 0.575 0.605 0.537
English-Spanish 0.426 0.329 0.492 0.468 0.492
French-English 0.673 0.360 0.578 0.585 0.565
English-French 0.524 0.414 0.495 0.630 0.486
Russian-English ? ? 0.450 0.363 0.477
English-Russian ? ? 0.513 0.582 0.500
Table 4: ? scores measuring intra-annotator agreement, i.e., self-consistency of judges, across for the past few years of the
human evaluation. The WMT13r and WMT13m columns provide breakdowns for researcher annotations and MTurk annota-
tions, respectively. The perfect inter-annotator agreement for Spanish-English is a result of there being very little data for that
language pair.
thresh. rankings ?
0.5 16,605 0.234
0.6 9,999 0.337
0.7 3,219 0.360
0.8 1,851 0.395
0.9 849 0.336
Table 5: Agreement as a function of threshold for Turkers on
the Russian?English task. The threshold is the percentage of
controls a Turker must pass for her rankings to be accepted.
cent evaluation campaigns, we tweaked the metric
and now arrived at a intuitive score that has been
demonstrated to be accurate in ranking systems ac-
cording to their true quality (Koehn, 2012).
The score, which we call EXPECTED WINS, has
an intuitive explanation. If the system is compared
against a randomly picked opposing system, on a
randomly picked sentence, by a randomly picked
judge, what is the probability that its translation is
ranked higher?
Formally, the score for a system Si among a set
of systems {Sj} given a pool of pairwise rankings
summarized as win(A,B) ? the number of times
system A is ranked higher than system B ? is
defined as follows:
score(Si) = 1|{Sj}|
?
j,j 6=i
win(Si, Sj)
win(Si, Sj) + win(Sj , Si)
Note that this score ignores ties.
3.5 Rank Ranges and Clusters
Given the scores, we would like to rank the sys-
tems, which is straightforward. But we would also
like to know, if the obtained system ranking is
statistically significant. Typically, given the large
number of systems that participate, and the simi-
larity of the systems given a common training data
condition and often common toolsets, there will be
some systems that will be very close in quality.
To establish the reliability of the obtained sys-
tem ranking, we use bootstrap resampling. We
sample from the set of pairwise rankings an equal
sized set of pairwise rankings (allowing for multi-
ple drawings of the same pairwise ranking), com-
pute the expected wins score for each system
based on this sample, and rank each system. By
repeating this procedure a 1,000 times, we can de-
termine a range of ranks, into which system falls
at least 95% of the time (i.e., at least 950 times) ?
corresponding to a p-level of p ? 0.05.
Furthermore, given the rank ranges for each sys-
tem, we can cluster systems with overlapping rank
ranges.8
For all language pairs and all systems, Table 6
reports all system scores, rank ranges, and clus-
ters. The official interpretation of these results
is that systems in the same cluster are considered
tied. Given the large number of judgements that
we collected, it was possible to group on average
about two systems in a cluster, even though the
systems in the middle are typically in larger clus-
ters.
8Formally, given ranges defined by start(Si) and end(Si),
we seek the largest set of clusters {Cc} that satisfies:
?S ?C : S ? C
S ? Ca, S ? Cb ? Ca = Cb
Ca 6= Cb ? ?Si ? Ca, Sj ? Cb :
start(Si) > end(Sj) or start(Sj) > end(Si)
8
Czech-English
# score range system
1 0.607 1 UEDIN-HEAFIELD
2 0.582 2-3 ONLINE-B
0.573 2-4 MES
0.562 3-5 UEDIN
0.547 4-7 ONLINE-A
0.542 5-7 UEDIN-SYNTAX
0.534 6-7 CU-ZEMAN
8 0.482 8 CU-TAMCHYNA
9 0.458 9 DCU-FDA
10 0.321 10 JHU
11 0.297 11 SHEF-WPROA
English-Czech
# score range system
1 0.580 1-2 CU-BOJAR
0.578 1-2 CU-DEPFIX
3 0.562 3 ONLINE-B
4 0.525 4 UEDIN
5 0.505 5-7 CU-ZEMAN
0.502 5-7 MES
0.499 5-8 ONLINE-A
0.484 7-9 CU-PHRASEFIX
0.476 8-9 CU-TECTOMT
10 0.457 10-11 COMMERCIAL-1
0.450 10-11 COMMERCIAL-2
12 0.389 12 SHEF-WPROA
Spanish-English
# score range system
1 0.624 1 UEDIN-HEAFIELD
2 0.595 2 ONLINE-B
3 0.570 3-5 UEDIN
0.570 3-5 ONLINE-A
0.567 3-5 MES
6 0.537 6 LIMSI-SOUL
7 0.514 7 DCU
8 0.488 8-9 DCU-OKITA
0.484 8-9 DCU-FDA
10 0.462 10 CU-ZEMAN
11 0.425 11 JHU
12 0.169 12 SHEF-WPROA
English-Spanish
# rank range system
1 0.637 1 ONLINE-B
2 0.582 2-4 ONLINE-A
0.578 2-4 UEDIN
0.567 3-4 PROMT
5 0.535 5-6 MES
0.528 5-6 TALP-UPC
7 0.491 7-8 LIMSI
0.474 7-9 DCU
0.472 8-10 DCU-FDA
0.455 9-11 DCU-OKITA
0.446 10-11 CU-ZEMAN
12 0.417 12 JHU
13 0.324 13 SHEF-WPROA
German-English
# rank range system
1 0.660 1 ONLINE-B
2 0.620 2-3 ONLINE-A
0.608 2-3 UEDIN-SYNTAX
4 0.586 4-5 UEDIN
0.584 4-5 QUAERO
0.571 5-7 KIT
0.562 6-7 MES
8 0.543 8-9 RWTH-JANE
0.533 8-10 MES-REORDER
0.526 9-10 LIMSI-SOUL
11 0.480 11 TUBITAK
12 0.462 12-13 UMD
0.462 12-13 DCU
14 0.396 14 CU-ZEMAN
15 0.367 15 JHU
16 0.311 16 SHEF-WPROA
17 0.238 17 DESRT
English-German
# rank range system
1 0.637 1-2 ONLINE-B
0.636 1-2 PROMT
3 0.614 3 UEDIN-SYNTAX
0.587 3-5 ONLINE-A
0.571 4-6 UEDIN
0.554 5-6 KIT
7 0.523 7 STANFORD
8 0.507 8 LIMSI-SOUL
9 0.477 9-11 MES-REORDER
0.476 9-11 JHU
0.460 10-12 CU-ZEMAN
0.453 11-12 TUBITAK
13 0.361 13 UU
14 0.329 14-15 SHEF-WPROA
0.323 14-15 RWTH-JANE
English-Russian
# rank range system
1 0.641 1 PROMT
2 0.623 2 ONLINE-B
3 0.556 3-4 CMU
0.542 3-6 ONLINE-G
0.538 3-7 ONLINE-A
0.531 4-7 UEDIN
0.520 5-7 QCRI-MES
8 0.498 8 CU-KAREL
9 0.478 9-10 MES-QCRI
0.469 9-10 JHU
11 0.434 11-12 COMMERCIAL-3
0.426 11-13 LIA
0.419 12-13 BALAGUR
14 0.331 14 CU-ZEMAN
French-English
# rank range system
1 0.638 1 UEDIN-HEAFIELD
2 0.604 2-3 UEDIN
0.591 2-3 ONLINE-B
4 0.573 4-5 LIMSI-SOUL
0.562 4-5 KIT
0.541 5-6 ONLINE-A
7 0.512 7 MES-SIMPLIFIED
8 0.486 8 DCU
9 0.439 9-10 RWTH
0.429 9-11 CMU-T2T
0.420 10-11 CU-ZEMAN
12 0.389 12 JHU
13 0.322 13 SHEF-WPROA
English-French
# rank range system
1 0.607 1-2 UEDIN
0.600 1-3 ONLINE-B
0.588 2-4 LIMSI-SOUL
0.584 3-4 KIT
5 0.553 5-7 PROMT
0.551 5-8 STANFORD
0.547 5-8 MES
0.537 6-9 MES-INFLECTION
0.533 7-10 RWTH-PB
0.516 9-11 ONLINE-A
0.499 10-11 DCU
12 0.427 12 CU-ZEMAN
13 0.408 13 JHU
14 0.382 14 OMNIFLUENT
15 0.350 15 ITS-LATL
16 0.326 16 ITS-LATL-PE
Russian-English
# rank range system
1 0.657 1 ONLINE-B
2 0.604 2-3 CMU
0.588 2-3 ONLINE-A
4 0.562 4-6 ONLINE-G
0.561 4-6 PROMT
0.550 5-7 QCRI-MES
0.546 5-7 UCAM
8 0.527 8-9 BALAGUR
0.519 8-10 MES-QCRI
0.507 9-11 UEDIN
0.497 10-12 OMNIFLUENT
0.492 11-14 LIA
0.483 12-15 OMNIFLUENT-C
0.481 12-15 UMD
0.476 13-15 CU-KAREL
16 0.432 16 COMMERCIAL-3
17 0.417 17 UEDIN-SYNTAX
18 0.396 18 JHU
19 0.215 19 CU-ZEMAN
Table 6: Official results for the WMT13 translation task. Systems are ordered by the expected win score. Lines between
systems indicate clusters according to bootstrap resampling at p-level p ? .05. This method is also used to determine the
range of ranks into which system falls. Systems with grey background indicate use of resources that fall outside the constraints
provided for the shared task.
9
4 Understandability of English?Czech
For the English-to-Czech translation, we con-
ducted a variation of the ?understandability? test
as introduced in WMT09 (Callison-Burch et al,
2009) and used in WMT10. In order to obtain
additional reference translations, we conflated this
test with post-editing. The procedure was as fol-
lows:
1. Monolingual editing (also called blind edit-
ing). The first annotator is given just the MT
output and requested to correct it. Given er-
rors in MT outputs, some guessing of the
original meaning is often inevitable and the
annotators are welcome to try. If unable, they
can mark the sentences as incomprehensible.
2. Review. A second annotator is asked to
validate the monolingual edit given both the
source and reference translations. Our in-
structions specify three options:
(a) If the monolingual edit is an adequate
translation and acceptably fluent Czech,
confirm it without changes.
(b) If the monolingual edit is adequate but
needs polishing, modify the sentence
and prefix it with the label ?OK:?.
(c) If the monolingual edit is wrong, cor-
rect it. You may start from the origi-
nal unedited MT output, if that is eas-
ier. Avoid using the reference directly,
prefer words from MT output whenever
possible.
The motivation behind this procedure is that we
want to save the time necessary for reading the
sentence. If the reviewer has already considered
whether the sentence is an acceptable translation,
they do not need to read the MT output again in
order to post-edit it. Our approach is thus some-
what the converse of Aziz et al (2013) who ana-
lyze post-editing effort to obtain rankings of MT
systems. We want to measure the understandabil-
ity of MT outputs and obtain post-edits at the same
time.
Both annotation steps were carried out in
the CASMACAT/Matecat post-editing user inter-
face.9, modified to provide the relevant variants of
the sentence next to the main edit box. Screen-
shots of the two annotation phases are given in
Figure 3 and Figure 4.
9http://www.casmacat.eu/index.php?n=Workbench
Occurrence GOOD ALMOST BAD EMPTY Total
First 34.7 0.1 42.3 11.0 4082
Repeated 41.1 0.1 41.0 6.1 805
Overall 35.8 0.1 42.1 10.2 4887
Table 7: Distribution of review statuses.
Similarly to the traditional ranking task, we pro-
vided three consecutive sentences from the origi-
nal text, each translated with a different MT sys-
tem. The annotators are free to use this contex-
tual information when guessing the meaning or re-
viewing the monolingual edits. Each ?annotation
HIT? consists of 24 sentences, i.e. 8 snippets of 3
consecutive sentences.
4.1 Basic Statistics on Editing
In total, 21 annotators took part in the exercise, 20
of them contributed to monolingual editing and 19
contributed to the reviews.
Connecting each review with the monolingual
edit (some edits received multiple reviews), we ob-
tain one data row. We collected 4887 data rows
(i.e. sentence revisions) for 3538 monolingual ed-
its, covering 1468 source sentences as translated
by 12 MT systems (including the reference).
Not all MT systems were considered for each
sentence, we preferred to obtain judgments for
more source sentences.
Based on the annotation instructions, each data
row has one of the four possible statuses: GOOD,
ALMOST, BAD, and EMPTY. GOOD rows are
those where the reviewer accepted the monolin-
gual edit without changes, ALMOST edits were
modified by the reviewer but they were marked as
?OK?. BAD edits were changed by the reviewer
and no ?OK? mark was given. Finally, the sta-
tus EMPTY is assigned to rows where the mono-
lingual editor refused to edit the sentence. The
EMPTY rows nevertheless contain the (?regular?)
post-edit of the reviewer, so they still provide a
new reference translation for the sentence.
Table 7 summarizes the distribution of row sta-
tuses depending on one more significant distinc-
tion: whether the monolingual editor has seen the
sentence before or not. We see that EMPTY and
BAD monolingual edits together drop by about
6% absolute when the sentence is not new to the
monolingual editor. The occurrence is counted as
?repeated? regardless whether the annotator has
previously seen the sentence in an editing or re-
viewing task. Unless stated otherwise, we exclude
repeated edits from our calculations.
10
Figure 3: In this screen, the annotator is expected to correct the MT output given only the context of at most two neighbouring
machine-translated sentences.
ALMOST Pairwise
treated Comparisons Agreement ?
inter
separate 2690 56.0 0.270
as BAD 2690 67.9 0.351
as GOOD 2690 65.2 0.289
intra
separate 170 65.3 0.410
as BAD 170 69.4 0.386
as GOOD 170 71.8 0.422
Table 8: Annotator agreement when reviewing monolingual
edits.
4.2 Agreement on Understandability
Before looking at individual system results, we
consider annotator agreement in the review step.
Details are given in Table 8. Given a (non-
EMPTY) string from a monolingual edit, we
would like to know how often two acceptability
judgments by two different reviewers (inter-) or
the same reviewer (intra-) agree. The repeated ed-
its remain in this analysis because we are not in-
terested in the origin of the string.
Our annotation setup leads to three possible la-
bels: GOOD, ALMOST, and BAD. The agree-
ment on one of three classes is bound to be lower
than the agreement on two classes, so we also re-
interpret ALMOST as either GOOD or BAD. Gen-
erally speaking, ALMOST is a positive judgment,
so it would be natural to treat it as GOOD. How-
ever, in our particular setup, when the reviewer
modified the sentence and forgot to add the label
?OK:?, the item ended up in the BAD class. We
conclude that this is indeed the case: the inter-
annotator agreement appears higher if ALMOST
is treated as BAD. Future versions of the review-
ing interface should perhaps first ask for the yes/no
judgment and only then allow to post-edit.
The ? values in Table 8 are the Fleiss?
kappa (Fleiss, 1971), accounting for agreement by
chance given the observed label distributions.
In WMT09, the agreements for this task were
higher: 77.4 for inter-AA and 86.6 for intra-AA.
(In 2010, the agreements for this task were not re-
ported.) It is difficult to say whether the differ-
ence lies in the particular language pair, the dif-
ferent set of annotators, or the different user in-
terface for our reviewing task. In 2009 and 2010,
the reviewers were shown 5 monolingual edits at
once and they were asked to judge each as accept-
able or not acceptable. We show just one segment
and they have probably set their minds on the post-
editing rather than acceptability judgment. We be-
lieve that higher agreements can be reached if the
reviewers first validate one or more of the edits and
only then are allowed to post-edit it.
4.3 Understandability of English?Czech
Table 9 brings about the first main result of our
post-editing effort. For each system (including
the reference translation), we check how often a
monolingual edit was marked OK or ALMOST
by the subsequent reviewer. The average under-
standability across all MT systems into Czech is
44.2?1.6%. This is a considerable improvement
compared to 2009 where the best systems pro-
duced about 32% understandable sentences. In
11
Figure 4: In this screen, the annotator is expected to validate the monolingual edit, correcting it if necessary. The annotator is
expected to add the prefix ?OK:? if the correction was more or less cosmetic.
Rank System Total Observations % Understandable
Overall incl. ref. 4082 46.7?1.6
Overall without ref. 3808 44.2?1.6
1 Reference 274?31 80.3?4.8
2-6 CU-ZEMAN 348?34 51.7?5.1
2-6 UEDIN 332?33 51.5?5.4
2-6 ONLINE-B 337?34 50.7?5.3
2-6 CU-BOJAR 341?35 50.7?5.2
2-7 CU-DEPFIX 350?34 48.0?5.3
6-10 COMMERCIAL-2 358?36 43.6?5.2
6-11 COMMERCIAL-1 316?34 41.5?5.5
7-12 CU-TECTOMT 338?34 39.4?5.2
8-12 MES 346?36 38.4?5.2
8-12 CU-PHRASEFIX 394?40 38.1?4.8
10-12 SHEF-WPROA 348?32 34.2?5.1
2009 Reference 91
2009 Best System 32
2010 Reference 97
2010 Best System 58
Table 9: Understandability of English?Czech systems. The
? values indicate empirical confidence bounds at 95%. Rank
ranges were also obtained in the same resampling: in 95% of
observations, the system was ranked in the given range.
2010, the best systems or system combinations
reached 55%?58%. The test set across years and
the quality of references and judgments also play a
role. In our annotation setup, the references appear
to be correctly understandable only to 80.3?4.8%.
To estimate the variance of these results due
to the particular sentences chosen, we draw 1000
random samples from the dataset, preserving the
dataset size and repeating some. The exact num-
ber of judgments per system can thus vary. We
report the 95% empirical confidence interval after
the ??? signs in Table 9 (the systems range from
?4.8 to?5.5). When we drop individual blind ed-
itors or reviewers, the understandability judgments
differ by about ?2 to ?4. In other words, the de-
pendence on the test set appears higher than the
dependence on the annotators.
The limited size of our dataset alows us only
to separate two main groups of systems: those
ranking 2?6 and those ranking worse. This rough
grouping vaguely matches with WMT13 ranking
results as given in Table 6. A somewhat surpris-
ing observation is that two automatic corrections
ranked better in WMT13 ranking but score worse
in understandability: CU-DEPFIX fixes some lost
negation and some agreement errors of CU-BOJAR
and CU-PHRASEFIX is a standard statistical post-
editing of a transfer-based system CU-TECTOMT.
A detailed inspection of the data is necessary to
explain this.
5 More Reference Translations for Czech
Our annotation procedure described in Section 4
allowed us to obtain a considerable number of ad-
ditional reference translations on top of official
single reference.
12
Refs 1 2 3 4 5 6 7 8 9 10-16
Sents 233 709 174 123 60 48 40 27 25 29
Table 10: Number of source sentences with the given number
of distinct reference translations.
In total, our edits cover 1468 source sentences,
i.e. about a half of the official test set size, and pro-
vide 4311 unique references. On average, one sen-
tence in our set has 2.94?2.17 unique reference
translations. Table 10 provides a histogram.
It is well known that automatic MT evalua-
tion methods perform better with more references,
because a single one may not confirm a correct
part of MT output. This issue is more severe
for morphologically rich languages like Czech
where about 1/3 of MT output was correct but not
confirmed by the reference (Bojar et al, 2010).
Advanced evaluation methods apply paraphras-
ing to smooth out some of the lexical divergence
(Kauchak and Barzilay, 2006; Snover et al, 2009;
Denkowski and Lavie, 2010). Simpler techniques
such as lemmatizing are effective for morphologi-
cally rich languages (Tantug et al, 2008; Kos and
Bojar, 2009) but they will lose resolution once the
systems start performing generally well.
WMTs have taken the stance that a big enough
test set with just a single reference should compen-
sate for the lack of other references. We use our
post-edited reference translations to check this as-
sumption for BLEU and NIST as implemented in
mteval-13a (international tokenization switched
on, which is not the default setting).
We run many probes, randomly picking the test
set size (number of distinct sentences) and the
number of distinct references per sentence. Note
that such test sets are somewhat artificially more
diverse; in narrow domains, source sentences can
repeat and even appear verbatim in the training
data, and in natural test sets with multiple refer-
ences, short sentences can receive several identical
translations.
For each probe, we measure the Spearman?s
rank correlation coefficient ? of the ranks pro-
posed by BLEU or NIST and the manual ranks.
We use the same implementation as applied in the
WMT13 Shared Metrics Task (Macha?c?ek and Bo-
jar, 2013). Note that the WMT13 metrics task still
uses the WMT12 evaluation method ignoring ties,
not the expected wins. As Koehn (2012) shows,
the two methods do not differ much.
Overall, the correlation is strongly impacted by
Figure 5: Correlation of BLEU and WMT13 manual ranks
for English?Czech translation
Figure 6: Correlation of NIST and WMT13 manual ranks
for English?Czech translation
the particular choice of test sentences and refer-
ence translations. By picking sentences randomly,
similarly or equally sized test sets can reach dif-
ferent correlations. Indeed, e.g. for a test set of
about 1500 distinct sentences selected from the
3000-sentence official test set (1 reference trans-
lation), we obtain correlations for BLEU between
0.86 and 0.94.
Figure 5 plots the correlations of BLEU and the
system rankings, Figure 6 provides the same pic-
ture for NIST. The upper triangular part of the plot
contains samples from our post-edited reference
translations, the lower rectangular part contains
probes from the official test set of 3000 sentences
with 1 reference translation.
To interpret the observations, we also calculate
the average and standard deviation of correlations
for each cell in Figures 5 and 6. Figures 7 and
8 plot the values for 1, 6, 7 and 8 references for
13
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 10  100  1000C
o
r
r
e
l
a
t
i
o
n
 
o
f
 
B
L
E
U
 
a
n
d
 
m
a
n
u
a
l
 
r
a
n
k
i
n
g
Test set size
Refs: official 1Refs: postedited 1Refs: postedited 6Refs: postedited 7Refs: postedited 8
Figure 7: Projections from Figure 5 of BLEU and WMT13
manual ranks for English?Czech translation
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 10  100  1000C
o
r
r
e
l
a
t
i
o
n
 
o
f
 
N
I
S
T
 
a
n
d
 
m
a
n
u
a
l
 
r
a
n
k
i
n
g
Test set size
Refs: official 1Refs: postedited 1Refs: postedited 6Refs: postedited 7Refs: postedited 8
Figure 8: Projections from Figure 6 of NIST and WMT13
manual ranks for English?Czech translation
BLEU and NIST, resp. The projections confirm
that the average correlations grow with test set
size, the growth is however sub-logarithmic.
Starting from as few as a dozen of sentences, we
see that using more references is better than using
a larger test set. For BLEU, we however already
seem to reach false positives at 7 references for
one or two hundred sentences: larger sets with just
one reference may correlate slightly better.
Using one reference obtained by post-editing
seems better than using the official (independent)
reference translations. BLEU is more affected
than NIST by this difference even at relatively
large test set size. Note that our post-edits are in-
spired by all MT systems, the good as well as the
bad ones. This probably provides our set with a
certain balance.
Overall, the best balance between the test set
size and the number of references seems to lie
somewhere around 7 references and 100 or 200
sentences. Creating such a test set could be even
cheaper than the standard 3000 sentences with just
one reference. However, the wide error bars re-
mind us that even this setting can lead to correla-
tions anywhere between 0.86 and 0.96. For other
languages, data sets types or other MT evaluation
methods, the best setting can be quite different and
has to be sought for.
6 Quality Estimation Task
Machine translation quality estimation is the task
of predicting a quality score for a machine trans-
lated text without access to reference translations.
The most common approach is to treat the problem
as a supervised machine learning task, using stan-
dard regression or classification algorithms. The
second edition of the WMT shared task on qual-
ity estimation builds on the previous edition of the
task (Callison-Burch et al, 2012), with variants to
this previous task, including both sentence-level
and word-level estimation, with new training and
test datasets, along with evaluation metrics and
baseline systems.
The motivation to include both sentence- and
word-level estimation come from the different po-
tential applications of these variants. Some inter-
esting uses of sentence-level quality estimation are
the following:
? Decide whether a given translation is good
enough for publishing as is.
? Inform readers of the target language only
whether or not they can rely on a translation.
? Filter out sentences that are not good enough
for post-editing by professional translators.
? Select the best translation among options
from multiple MT and/or translation memory
systems.
Some interesting uses of word-level quality es-
timation are the following:
? Highlight words that need editing in post-
editing tasks.
? Inform readers of portions of the sentence
which are not reliable.
? Select the best segments among options from
multiple translation systems for MT system
combination.
The goals of this year?s shared task were:
14
? To explore various granularity levels for the
task (sentence-level and word-level).
? To explore the prediction of more objective
scores such as edit distance and post-editing
time.
? To explore the use of quality estimation tech-
niques to replace reference-based MT evalua-
tion metrics in the task of ranking alternative
translations generated by different MT sys-
tems.
? To identify new and effective quality indica-
tors (features) for all variants of the quality
estimation task.
? To identify effective machine learning tech-
niques for all variants of the quality estima-
tion task.
? To establish the state of the art performance
in the field.
Four subtasks were proposed, as we discuss in
Sections 6.1 and 6.2. Each subtask provides spe-
cific datasets, annotated for quality according to
the subtask (Section 6.3), and evaluates the system
submissions using specific metrics (Section 6.6).
When available, external resources (e.g. SMT
training corpus) and translation engine-related re-
sources were given to participants (Section 6.4),
who could also use any additional external re-
sources (no distinction between open and close
tracks is made). Participants were also provided
with a software package to extract quality esti-
mation features and perform model learning (Sec-
tion 6.5), with a suggested list of baseline features
and learning method (Section 6.7). Participants
could submit up to two systems for each subtask.
6.1 Sentence-level Quality Estimation
Task 1.1 Predicting Post-editing Distance This
task is similar to the quality estimation task in
WMT12, but with one important difference in the
scoring variant: instead of using the post-editing
effort scores in the [1-5] range, we use HTER
(Snover et al, 2006) as quality score. This score
is to be interpreted as the minimum edit distance
between the machine translation and its manually
post-edited version, and its range is [0, 1] (0 when
no edit needs to be made, and 1 when all words
need to be edited). Two variants of the results
could be submitted in the shared task:
? Scoring: A quality score for each sentence
translation in [0,1], to be interpreted as an
HTER score; lower scores mean better trans-
lations.
? Ranking: A ranking of sentence translations
for all source test sentences from best to
worst. For this variant, it does not matter how
the ranking is produced (from HTER predic-
tions, likert predictions, or even without ma-
chine learning). The reference ranking is de-
fined based on the true HTER scores.
Task 1.2 Selecting Best Translation This task
consists in ranking up to five alternative transla-
tions for the same source sentence produced by
multiple MT systems. We use essentially the same
data provided to participants of previous years
WMT?s evaluation metrics task ? where MT eval-
uation metrics are assessed according to how well
they correlate with human rankings. However, ref-
erence translations produced by humans are not be
used in this task.
Task 1.3 Predicting Post-editing Time For this
task systems are required to produce, for each
translation, the expected time (in seconds) it
would take a translator to post-edit such an MT
output. The main application for predictions of
this type is in computer-aided translation where
the predicted time can be used to select among dif-
ferent hypotheses or even to omit any MT output
in cases where no good suggestion is available.
6.2 Word-level Quality Estimation
Based on the data of Task 1.3, we define Task 2, a
word-level annotation task for which participants
are asked to produce a label for each token that
indicates whether the word should be changed by
a post-editor or kept in the final translation. We
consider the following two sets of labels for pre-
diction:
? Binary classification: a keep/change label,
the latter meaning that the token should be
corrected in the post-editing process.
? Multi-class classification: a label specifying
the edit action that should be performed on
the token (keep as is, delete, or substitute).
6.3 Datasets
Task 1.1 Predicting post-editing distance For
the training of models, we provided the WMT12
15
quality estimation dataset: 2,254 English-
Spanish news sentences extracted from previous
WMT translation task English-Spanish test sets
(WMT09, WMT10, and WMT12). These were
translated by a phrase-based SMT Moses system
trained on Europarl and News Commentaries cor-
pora as provided by WMT, along with their source
sentences, reference translations, post-edited
translations, and HTER scores. We used TERp
(default settings: tokenised, case insensitive,
etc., but capped to 1)10 to compute the HTER
scores. Likert scores in [1,5] were also provided,
as participants may choose to use them for the
ranking variant.
As test data, we use a subset of the WMT13
English-Spanish news test set with 500 sentences,
whose translations were produced by the same
SMT system used for the training set. To com-
pute the true HTER labels, the translations were
post-edited under the same conditions as those on
the training set. As in any blind shared task, the
HTER scores were solely used to evaluate the sub-
missions, and were only released to participants
after they submitted their systems.
A few variations of the training and test data
were provided, including a version with cases re-
stored and a version detokenized. In addition,
we provided a number of engine-internal informa-
tion from Moses for glass-box feature extraction,
such as phrase and word alignments, model scores,
word graph, n-best lists and information from the
decoder?s search graph.
Task 1.2 Selecting best translation As training
data, we provided a large set of up to five alter-
native machine translations produced by different
MT systems for each source sentence and ranked
for quality by humans. This was the outcome of
the manual evaluation of the translation task from
WMT09-WMT12. It includes two language pairs:
German-English and English-Spanish, with 7,098
and 4,592 source sentences and up to five ranked
translations, totalling 32,922 and 22,447 transla-
tions, respectively.
As test data, a set of up to five alternative ma-
chine translations per source sentence from the
WMT08 test sets was provided, with 365 (1,810)
and 264 (1,315) source sentences (translations)
for German-English and English-Spanish, respec-
tively. We note that there was some overlap be-
tween the MT systems used in the training data
10http://www.umiacs.umd.edu/?snover/terp/
and test datasets, but not all systems were the
same, as different systems participate in WMT
over the years.
Task 1.3 and Task 2 Predicting post-editing
time and word-level edits For Tasks 1.3 and 2
we provides a new dataset consisting of 22 English
news articles which were translated into Span-
ish using Moses and post-edited during a CAS-
MACAT11 field trial. Of these, 15 documents have
been processed repeatedly by at least 2 out of 5
translators, resulting in a total of 1,087 segments.
For each segment we provided:
? English source and Spanish translation.
? Spanish MT output which was used as basis
for post-editing.
? Document and translator ID.
? Position of the segment within the document.
The metadata about translator and document was
made available as we expect that translator perfor-
mance and normalisation over document complex-
ity can be helpful when predicting the time spend
on a given segment.
For the training portion of the data we also pro-
vided:
? Time to post-edit in seconds (Task 1.3).
? Binary (Keep, Change) and multiclass (Keep,
Substitute, Delete) labels on word level along
with explicit tokenization (Task 2).
The labels in Task 2 are derived by comput-
ing WER between the original machine translation
and its post-edited version.
6.4 Resources
For all tasks, we provided resources to extract
quality estimation features when these were avail-
able:
? The SMT training corpus (WMT News and
Europarl): source and target sides of the cor-
pus used to train the SMT engines for Tasks
1.1, 1.3, and 2, and truecase models gener-
ated from these. These corpora can also be
used for Task 1.2, but we note that some of
the MT systems used in the datasets of this
task were not statistical or did not use (only)
the training corpus provided by WMT.
11http://casmacat.eu/
16
? Language models: n-gram language models
of source and target languages generated us-
ing the SMT training corpora and standard
toolkits such as SRILM Stolcke (2002), and
a language model of POS tags for the target
language. We also provided unigram, bigram
and trigram counts.
? IBM Model 1 lexical tables generated by
GIZA++ using the SMT training corpora.
? Phrase tables with word alignment informa-
tion generated by scripts provided by Moses
from the parallel corpora.
? For Tasks 1.1, 1.3 and 2, the Moses config-
uration file used for decoding or the code to
re-run the entire Moses system.
? For Task 1.1, both English and Spanish re-
sources for a number of advanced features
such as pre-generated PCFG parsing models,
topic models, global lexicon models and mu-
tual information trigger models.
We refer the reader to the QUEST website12 for
a detailed list of resources provided for each task.
6.5 QUEST Framework
QUEST (Specia et al, 2013) is an open source
framework for quality estimation which provides a
wide variety of feature extractors from source and
translation texts and external resources and tools.
These range from simple, language-independent
features, to advanced, linguistically motivated fea-
tures. They include features that rely on informa-
tion from the MT system that generated the trans-
lations (glass-box features), and features that are
oblivious to the way translations were produced
(black-box features).
QUEST also integrates a well-known machine
learning toolkit, scikit-learn,13 and other algo-
rithms that are known to perform well on this task
(e.g. Gaussian Processes), providing a simple and
effective way of experimenting with techniques
for feature selection and model building, as well
as parameter optimisation through grid search.
From QUEST, a subset of 17 features and an
SVM regression implementation were used as
baseline for Tasks 1.1, 1.2 and 1.3. The software
was made available to all participants.
12http://www.quest.dcs.shef.ac.uk/
13http://scikit-learn.org/
6.6 Evaluation Metrics
Task 1.1 Predicting post-editing distance
Evaluation is performed against the HTER and/or
ranking of translations using the same metrics as
in WMT12. For the scoring variant of the task,
we use two standard metrics for regression tasks:
Mean Absolute Error (MAE) as a primary metric,
and Root of Mean Squared Error (RMSE) as a
secondary metric. To improve readability, we
report these error numbers by first mapping the
HTER values to the [0, 100] interval, to be read
as percentage-points of the HTER metric. For a
given test set S with entries si, 1 ? i ? |S|, we
denote by H(si) the proposed score for entry si
(hypothesis), and by V (si) the reference value for
entry si (gold-standard value):
MAE =
?N
i=1 |H(si)? V (si)|
|S|
RMSE =
??N
i=1(H(si)? V (si))2
|S|
Both these metrics are non-parametric, auto-
matic and deterministic (and therefore consistent),
and extrinsically interpretable. For instance, a
MAE value of 10 means that, on average, the ab-
solute difference between the hypothesized score
and the reference score value is 10 percentage
points (i.e., 0.10 difference in HTER scores). The
interpretation of RMSE is similar, with the differ-
ence that RMSE penalises larger errors more (via
the square function).
For the ranking variant of the task, we use the
DeltaAvg metric proposed in the 2012 edition of
the task (Callison-Burch et al, 2012) as our main
metric. This metric assumes that each reference
test instance has an extrinsic number associated
with it that represents its ranking with respect to
the other test instances. For completeness, we
present here again the definition of DeltaAvg.
The goal of the DeltaAvg metric is to measure
how valuable a proposed ranking (which we call a
hypothesis ranking) is, according to the true rank-
ing values associated with the test instances. We
first define a parametrised version of this metric,
called DeltaAvg[n]. The following notations are
used: for a given entry sentence s, V (s) represents
the function that associates an extrinsic value to
that entry; we extend this notation to a set S, with
V (S) representing the average of all V (s), s ? S.
17
Intuitively, V (S) is a quantitative measure of the
?quality? of the set S, as induced by the extrinsic
values associated with the entries in S. For a set
of ranked entries S and a parameter n, we denote
by S1 the first quantile of set S (the highest-ranked
entries), S2 the second quantile, and so on, for n
quantiles of equal sizes.14 We also use the nota-
tion Si,j = ?jk=i Sk. Using these notations, wedefine:
DeltaAvgV [n] =
?n?1
k=1 V (S1,k)
n? 1 ? V (S)
When the valuation function V is clear from the
context, we write DeltaAvg[n] for DeltaAvgV [n].
The parameter n represents the number of quan-
tiles we want to split the set S into. For instance,
n = 2 gives DeltaAvg[2] = V (S1)?V (S), hence it
measures the difference between the quality of the
top quantile (top half) S1 and the overall quality
(represented by V (S)). For n = 3, DeltaAvg[3] =
(V (S1)+V (S1,2)/2?V (S) = ((V (S1)?V (S))+
(V (S1,2?V (S)))/2, hence it measures an average
difference across two cases: between the quality of
the top quantile (top third) and the overall quality,
and between the quality of the top two quantiles
(S1 ? S2, top two-thirds) and the overall quality.
In general, DeltaAvg[n] measures an average dif-
ference in quality across n ? 1 cases, with each
case measuring the impact in quality of adding an
additional quantile, from top to bottom. Finally,
we define:
DeltaAvgV =
?N
n=2 DeltaAvgV [n]
N ? 1
where N = |S|/2. As before, we write DeltaAvg
for DeltaAvgV when the valuation function V is
clear from the context. The DeltaAvg metric is an
average across all DeltaAvg[n] values, for those
n values for which the resulting quantiles have at
least 2 entries (no singleton quantiles).
We present results for DeltaAvg using as valu-
ation function V the HTER scores, as defined in
Section 6.3. We also use Spearman?s rank correla-
tion coefficient ? as a secondary metric.
Task 1.2 Selecting best translation The perfor-
mance on the task of selecting the best transla-
tion from a pool of translation candidates is mea-
14If the size |S| is not divisible by n, then the last quantile
Sn is assumed to contain the rest of the entries.
sured by comparing proposed (hypothesis) rank-
ings against human-produced rankings. The met-
ric used is Kendall?s ? rank correlation coefficient,
computed as follows:
? = |concordant pairs| ? |discordant pairs||total pairs|
where a concordant pair is a pair of two transla-
tions for the same source segment in which the
ranking order proposed by a human annotator and
the ranking order of the hypothesis agree; in a dis-
cordant pair, they disagree. The possible values of
? range between 1 (where all pairs are concordant)
and ?1 (where all pairs are discordant). Thus a
system with ranking predictions having a higher
? value makes predictions that are more similar
to human judgements than a system with ranking
predictions having a lower ? . Note that, in general,
being able to predict rankings with an accuracy
of ? = ?1 is as difficult as predicting rankings
with an accuracy of ? = 1, whereas a completely
random ranking would have an expected value of
? = 0. The range is therefore said to be symmet-
ric.
However, there are two distinct ways of mea-
suring rank correlation using Kendall?s ? , related
to the way ties are treated. They greatly affect how
Kendall?s ? numbers are to be interpreted, and es-
pecially the symmetry property. We explain the
difference in detail in what follows.
Kendall?s ? with ties penalised If the goal is
to measure to what extent the difference in qual-
ity visible to a human annotator has been captured
by an automatically produced hypothesis (recall-
oriented view), then proposing a tie between t1
and t2 (t1-equal-to-t2) when the pair was judged
(in the reference) as t1-better-than-t2 is treated as
a failure-to-recall. In other words, it is as bad as
proposing t1-worse-than-t2. Henceforth, we call
this recall-oriented measure ?Kendall?s ? with ties
penalised?. This metric has the following proper-
ties:
? it is completely fair when comparing differ-
ent methods to produce ranking hypotheses,
because the denominator (number of total
pairs) is the same (it is the number of non-
tied pairs under the human judgements).
? it is non-symmetric, in the sense that a value
of ? = ?1 is not as difficult to obtain as ? =
18
1 (simply proposing only ties gets a ? = ?1);
hence, the sign of the ? value matters.
? the expected value of a completely random
ranking is not necessarily ? = 0, but rather
depends on the number of ties in the refer-
ence rankings (i.e., it is test set dependent).
Kendall?s ? with ties ignored If the goal
is to measure to what extent the difference in
quality signalled by an automatically produced
hypothesis is reflected in the human annota-
tion (precision-oriented view), then proposing t1-
equal-to-t2 when the pair was judged differently
in the reference does no harm the metric.
Henceforth, we call this precision-oriented
measure ?Kendall?s ? with ties ignored?. This
metric has the following properties:
? it is not completely fair when comparing dif-
ferent methods to produce ranking hypothe-
ses, because the denominator (number of to-
tal pairs) may not be the same (it is the num-
ber of non-tied pairs under each system?s pro-
posal).
? it is symmetric, in the sense that a value of
? = ?1 is as difficult to obtain as ? = 1;
hence, the sign of the ? value may not mat-
ter. 15
? the expected value of a completely random
ranking is ? = 0 (test-set independent).
The first property is the most worrisome from
the perspective of reporting the results of a shared
task, because a system may fare very well on this
metric simply because it choses not to commit
(proposes ties) most of the time. Therefore, to
give a better understanding of the systems? perfor-
mance, for Kendall?s ? with ties ignored we also
provide the number of non-ties proposed by each
system.
Task 1.3 Predicting post-editing time Submis-
sions are evaluated in terms of Mean Average Er-
ror (MAE) against the actual time spent by post-
editors (in seconds). By using a linear error mea-
sure we limit the influence of outliers: sentences
that took very long to edit or where the measure-
ment taken is questionable.
15In real life applications this distinction matters. Even
if, from a computational perspective, it is as hard to get ?
close to?1 as it is to get it close to 1, knowing the sign is the
difference between selecting the best or the worse translation.
To further analyse the influence of extreme val-
ues, we also compute Spearman?s rank correlation
? coefficient which does not depend on the abso-
lute values of the predictions.
We also give RMSE and Pearson?s correlation
coefficient r for reference.
Task 2 Predicting word-level scores The word-
level task is primarily evaluated by macro-
averaged F-measure. Because the class distribu-
tion is skewed ? in the test data about one third
of the tokens are marked as correct ? we compute
precision and recall and F1 for each class individ-
ually. Consider the following confusion matrix for
the two classes Keep and Change:
predicted
(K)eep (C)hange
expected (K)eep 10 20(C)hange 30 40
For the given example we derive true-positive
(tp), true-negative (tn), false-positive (fp), and
false-negative (fn) counts:
tpK = 10 fpK = 30 fnK = 20
tpC = 40 fpC = 20 fnC = 30
precisionK =
tpK
tpK + fpK
= 10/40
recallK =
tpK
tpK + fnK
= 10/30
F1,K =
2 ? precisionK ? recallK
precisionK +recallK
A single cumulative statistic can be computed
by averaging the resulting F-measures (macro av-
eraging) or by micro averaging in which case pre-
cision and recall are first computed by accumulat-
ing the relevant values for all classes (O?zgu?r et al,
2005), e.g.
precision = tpK + tpC(tpK + fpK) + (tpC + fpC)
The latter gives equal weight to each exam-
ple and is therefore dominated by performance on
the largest class while macro-averaged F-measure
gives equal weight to each class.
The same setup is used to evaluate the perfor-
mance in the multiclass setting. Please note that
here the test data only contains 4% examples for
class (D)elete.
19
ID Participating team
CMU Carnegie Mellon University, USA (Hildebrand and Vogel, 2013)
CNGL Centre for Next Generation Localization, Ireland (Bicici, 2013b)
DCU Dublin City University, Ireland (Almaghout and Specia, 2013)
DCU-SYMC Dublin City University & Symantec, Ireland (Rubino et al, 2013b)
DFKI German Research Centre for Artificial Intelligence, Germany (Avramidis and
Popovic, 2013)
FBK-UEdin Fondazione Bruno Kessler, Italy & University of Edinburgh, UK (Camargo de
Souza et al, 2013)
LIG Laboratoire d?Informatique Grenoble, France (Luong et al, 2013)
LIMSI Laboratoire d?Informatique pour la Me?canique et les Sciences de l?Inge?nieur,
France (Singh et al, 2013)
LORIA Lorraine Laboratory of Research in Computer Science and its Applications,
France (Langlois and Smaili, 2013)
SHEF University of Sheffield, UK (Beck et al, 2013)
TCD-CNGL Trinity College Dublin & CNGL, Ireland (Moreau and Rubino, 2013)
TCD-DCU-CNGL Trinity College Dublin, Dublin City University & CNGL, Ireland (Moreau and
Rubino, 2013)
UMAC University of Macau, China (Han et al, 2013)
UPC Universitat Politecnica de Catalunya, Spain (Formiga et al, 2013b)
Table 11: Participants in the WMT13 Quality Estimation shared task.
6.7 Participants
Table 11 lists all participating teams submitting
systems to any subtask in this shared task. Each
team was allowed up to two submissions for each
subtask. In the descriptions below participation in
specific tasks is denoted by a task identifier: T1.1,
T1.2, T1.3, and T2.
Sentence-level baseline system (T1.1, T1.3):
QUEST was used to extract 17 system-
independent features from the source and
translation files and the SMT training cor-
pus that were found to be relevant in previous
work (same features as in the WMT12 shared
task):
? number of tokens in the source and tar-
get sentences.
? average source token length.
? average number of occurrences of the
target word within the target sentence.
? number of punctuation marks in source
and target sentences.
? Language model probability of source
and target sentences using language
models provided by the task.
? average number of translations per
source word in the sentence: as given
by IBM 1 model thresholded so that
P (t|s) > 0.2, and so that P (t|s) > 0.01
weighted by the inverse frequency of
each word in the source side of the SMT
training corpus.
? percentage of unigrams, bigrams and tri-
grams in frequency quartiles 1 (lower
frequency words) and 4 (higher fre-
quency words) in the source side of the
SMT training corpus
? percentage of unigrams in the source
sentence seen in the source side of the
SMT training corpus.
These features are used to train a Support
Vector Machine (SVM) regression algorithm
using a radial basis function kernel within the
SCIKIT-LEARN toolkit. The ?,  and C pa-
rameters were optimized using a grid-search
and 5-fold cross validation on the training
set. We note that although the system is re-
ferred to as a ?baseline?, it is in fact a strong
system. For tasks of the same type as 1.1
and 1.3, it has proved robust across a range
of language pairs, MT systems, and text do-
mains for predicting post-editing effort, as it
has also been shown in the previous edition
of the task (Callison-Burch et al, 2012).
The same features could be useful for a base-
line system for Task 1.2. In our official re-
20
sults, however, the baseline for Task 1.2 is
simpler than that: it proposes random ranks
for each pair of alternative translations for a
given source sentence, as we will discuss in
Section 6.8.
CMU (T1.1, T1.2, T1.3): The CMU quality
estimation system was trained on features
based on language models, the MT sys-
tem?s distortion model and phrase table fea-
tures, statistical word lexica, several sentence
length statistics, source language word and
bi-gram frequency statistics, n-best list agree-
ment and diversity, source language parse,
source-target word alignment and a depen-
dency parse based cohesion penalty. These
features were extracted using GIZA++, a
forced alignment algorithm and the Stanford
parser (de Marneffe et al, 2006). The pre-
diction models were trained using four clas-
sifiers in the Weka toolkit (Hall et al, 2009):
linear regression, M5P trees, multi layer per-
ceptron and SVM regression. In addition to
main system submission, a classic n-best list
re-ranking approach was used for Task 1.2.
CNGL (T1.1, T1.2, T1.3, T2): CNGL systems
are based on referential translation machines
(RTM) (Bic?ici and van Genabith, 2013), par-
allel feature decay algorithms (FDA) (Bicici,
2013a), and machine translation performance
predictor (MTPP) (Bic?ici et al, 2013), all
of which allow to obtain language and MT
system-independent predictions. For each
task, RTM models were developed using the
parallel corpora and the language model cor-
pora distributed by the WMT13 translation
task and the language model corpora pro-
vided by LDC for English and Spanish.
The sentence-level features are described in
MTPP (Bic?ici et al, 2013); they include
monolingual or bilingual features using n-
grams defined over text or common cover
link (CCL) (Seginer, 2007) structures as the
basic units of information over which sim-
ilarity calculations are made. RTMs use
308 features about coverage and diversity,
IBM1, and sentence translation performance,
retrieval closeness and minimum Bayes re-
trieval risk, distributional similarity and en-
tropy, IBM2 alignment, character n-grams,
and sentence readability. The learning mod-
els are Support Vector Machines (SVR) and
SVR with partial least squares (SVRPLS).
The word-level features include CCL links,
word length, location, prefix, suffix, form,
context, and alignment, totalling 511K fea-
tures for binary classification, and 637K for
multiclass classification. Generalised lin-
ear models (GLM) (Collins, 2002) and GLM
with dynamic learning (GLMd) were used.
DCU (T1.2): The main German-English submis-
sion uses six Combinatory Categorial Gram-
mar (CCG) features: CCG supertag lan-
guage model perplexity and log probability,
the number of maximal CCG constituents in
the translation output which are the highest-
probability minimum number of CCG con-
stituents that span the translation output, the
percentage of CCG argument mismatches be-
tween each subsequent CCG supertags, the
percentage of CCG argument mismatches be-
tween each subsequent CCG maximal cate-
gories and the minimum number of phrases
detected in the translation output. A second
submission uses the aforementioned CCG
features combined with 80 features from
QUEST as described in (Specia, 2011). For
the CCG features, the C&C parser was used
to parse the translation output. Moses was
used to build the phrase table from the SMT
training corpus with maximum phrase length
set to 7. The language model of supertags
was built using the SRILM toolkit. As learn-
ing algorithm, Logistic Regression as pro-
vided by the SCIKIT-LEARN toolkit was used.
The training data was prepared by converting
each ranking of translation outputs to a set
of pairwise comparisons according to the ap-
proach proposed by Avramidis et al (2011).
The rankings were generated back from pair-
wise comparisons predicted by the model.
DCU-SYMC (T1.1): The DCU-Symantec team
employed a wide set of features which in-
cluded language model, n-gram counts and
word-alignment features as well as syntac-
tic features, topic model features and pseudo-
reference features. The main learning algo-
rithm was SVR, but regression tree learning
was used to perform feature selection, re-
ducing the initial set of 442 features to 96
features (DCU-Symantec alltypes) and 134
21
(DCU-Symantec combine). Two methods
for feature selection were used: a best-first
search in the feature space using regression
trees to evaluate the subsets, and reading bi-
narised features directly from the nodes of
pruned regression trees.
The following NLP tools were used in feature
extraction: the Brown English Wall-Street-
Journal-trained statistical parser (Charniak
and Johnson, 2005), a Lexical Functional
Grammar parser (XLE), together with a
hand-crafted Lexical Functional Grammar,
the English ParGram grammar (Kaplan et al,
2004), and the TreeTagger part-of-speech
tagger (Schmidt, 1994) with off-the-shelf
publicly available pre-trained tagging mod-
els for English and Spanish. For pseudo-
reference features, the Bing, Moses and Sys-
tran translation systems were used. The Mal-
let toolkit (McCallum, 2002) was used to
build the topic models and features based on
a grammar checker were extracted with Lan-
guageTool.16
DFKI (T1.2, T1.3): DFKI?s submission for Task
1.2 was based on decomposing rankings into
pairs (Avramidis, 2012), where the best sys-
tem for each pair was predicted with Lo-
gistic Regression (LogReg). For German-
English, LogReg was trained with Stepwise
Feature Selection (Hosmer, 1989) on two
feature sets: Feature Set 24 includes ba-
sic counts augmented with PCFG parsing
features (number of VPs, alternative parses,
parse probability) on both source and tar-
get sentences (Avramidis et al, 2011), and
pseudo-reference METEOR score; the most
successful set, Feature Set 33 combines those
24 features with the 17 baseline features. For
English-Spanish, LogReg was used with L2
Regularisation (Lin et al, 2007) and two fea-
ture sets were devised after scoring features
with ReliefF (Kononenko, 1994) and Infor-
mation Gain (Hunt et al, 1966). Feature Set
431 combines 30 features with highest abso-
lute Relief-F and Information Gain (15 from
each). features with the highest
Task 1.3 was modelled using feature sets
selected after Relief-F scoring of external
black-box and glass-box features extracted
16http://www.languagetool.org/
from the SMT decoding process. The most
successful submission (linear6) was trained
with Linear Regression including the 17 fea-
tures with highest positive Relief-F. Most
prominent features include the alternative
possible parses of the source and target sen-
tence, the positions of the phrases with the
lowest and highest probability and future
cost estimate in the translation, the counts of
phrases in the decoding graph whose prob-
ability or whether the future cost estimate
is higher/lower than their standard deviation,
counts of verbs and determiners, etc. The
second submission (pls8) was trained with
Partial Least Squares regression (Stone and
Brooks, 1990) including more glass-box fea-
tures.
FBK-Uedin (T1.1, T1.3):
The submissions explored features built on
MT engine resources including automatic
word alignment, n-best candidate translation
lists, back-translations and word posterior
probabilities. Information about word align-
ments is used to extract quantitative (amount
and distribution of the alignments) and qual-
itative (importance of the aligned terms) fea-
tures under the assumption that alignment
information can help tasks where sentence-
level semantic relations need to be identified
(Souza et al, 2013). Three similar English-
Spanish systems are built and used to provide
pseudo-references (Soricut et al, 2012) and
back-translations, from which automatic MT
evaluation metrics could be computed and
used as features.
All features were computed over a concatena-
tion of several publicly available parallel cor-
pora for the English-Spanish language pair
such as Europarl, News Commentary, and
MultiUN. The models were developed using
supervised learning algorithms: SVMs (with
feature selection step prior to model learning)
and extremely randomized trees.
LIG (T2): The LIG systems are designed to
deal with both binary and multiclass variants
of the word level task. They integrate sev-
eral features including: system-based (graph
topology, language model, alignment con-
text, etc.), lexical (Part-of-Speech tags), syn-
tactic (constituent label, distance to the con-
22
stituent tree root) and semantic (target and
source polysemy count). Besides the exist-
ing components of the SMT system, feature
extraction requires further external tools and
resources, such as: TreeTagger (for POS tag-
ging), Bekerley Parser trained with AnCora
treebank (for generating constituent trees in
Spanish), WordNet and BabelNet (for pol-
ysemy count), Google Translate. The fea-
ture set is then combined and trained using
a Conditional Random Fields (CRF) learn-
ing method. During the labelling phase, the
optimal threshold is tuned using a small de-
velopment set split from the original training
set. In order to retain the most informative
features and eliminate the redundant ones, a
Sequential Backward Selection algorithm is
employed over the all-feature systems. With
the binary classifier, the Boosting technique
is applied to allow a number of sub feature
sets to complement each other, resulting in
the ?stronger? combined system.
LIMSI (T1.1, T1.3): The two tasks were treated
as regression problems using a simple elas-
tic regression, a linear model trained with L1
and L2 regularisers. Regarding features, the
submissions mainly aimed at evaluating the
usefulness for quality estimation of n-gram
posterior probabilities (Gispert et al, 2013)
that quantify the probability for a given n-
gram to be part of the system output. Their
computation relies on all the hypotheses con-
sidered by a SMT system during decoding:
intuitively, the more hypotheses a n-gram ap-
pears in, the more confident the system is
that this n-gram is part of the correct trans-
lation, and the higher its posterior probabil-
ity is. The feature set contains 395 other fea-
tures that differs, in two ways, from the tra-
ditional features used in quality estimation.
First, it includes several features based on
large span continuous space language mod-
els (Le et al, 2011) that have already proved
their efficiency both for the translation task
and the quality estimation task. Second, each
feature was expanded into two ?normalized
forms? in which their value was divided ei-
ther by the source length or the target length
and, when relevant, into a ?ratio form? in
which the feature value computed on the tar-
get sentence is divided by its value computed
in the source sentence.
LORIA (T1.1): The system uses the 17 baseline
features, plus several numerical and boolean
features computed from the source and target
sentences (Langlois et al, 2012). These are
based on language model information (per-
plexity, level of back-off, intra-lingual trig-
gers), translation table (IBM1 table, inter-
lingual triggers). For language models, for-
ward and backward models are built. Each
feature gives a score to each word in the sen-
tence, and the score of the sentence is the av-
erage of word scores. For several features,
the score of a word depends on the score of its
neighbours. This leads to 66 features. Sup-
port Vector Machines are used to learn a re-
gression model. In training is done in a multi-
stage procedure aimed at increasing the size
of the training corpus. Initially, the train-
ing corpus with machine translated sentences
provided by the task is used to train an SVM
model. Then this model is applied to the post-
edited and reference sentences (also provided
as part of the task). These are added to the
quality estimation training corpus using as la-
bels the SVM predictions. An algorithm to
tune the predicted scores on a development
corpus is used.
SHEF (T1.1, T1.3): These submissions use
Gaussian Processes, a non-parametric prob-
abilistic learning framework for regression,
along with two techniques to improve predic-
tion performance and minimise the amount
of resources needed for the problem: feature
selection based on optimised hyperparame-
ters and active learning to reduce the training
set size (and therefore the annotation effort).
The initial set features contains all black box
and glass box features available within the
QUEST framework (Specia et al, 2013) for
the dataset at hand (160 in total for Task 1.1,
and 80 for Task 1.3). The query selection
strategy for active learning is based on the
informativeness of the instances using Infor-
mation Density, a measure that leverages be-
tween the variance among instances and how
dense the region (in the feature space) where
the instance is located is. To perform fea-
ture selection, following (Shah et al, 2013)
features are ranked by the Gaussian Process
23
algorithm according to their learned length
scales, which can be interpreted as the rel-
evance of such feature for the model. This
information was used for feature selection
by discarding the lowest ranked (least use-
ful) ones. based on empirical results found
in (Shah et al, 2013), the top 25 features for
both models were selected and used to retrain
the same regression algorithm.
UPC (T1.2): The methodology used a broad set
of features, mainly available through the last
version of the Asiya toolkit for MT evalua-
tion (Gonza`lez et al, 2012)17. Concretely,
86 features were derived for the German-to-
English and 97 features for the English-to-
Spanish tasks. These features cover differ-
ent approaches and include standard qual-
ity estimation features, as provided by the
above mentioned Asiya and QUEST toolk-
its, but also a variety of features based on
pseudo-references, explicit semantic analy-
sis and specialised language models trained
on the parallel and monolingual corpora pro-
vided by the WMT Translation Task.
The system selection task is approached by
means of pairwise ranking decisions. It uses
Random Forest classifiers with ties, expand-
ing the work of 402013cFormiga et al), from
which a full ranking can be derived and the
best system per sentence is identified. Once
the classes are given by the Random Forest,
one can build a graph by means of the adja-
cency matrix of the pairwise decision. The fi-
nal ranking is assigned through a dominance
scheme similar to Pighin et al (2012).
An important remark of the methodology is
the feature selection process, since it was no-
ticed that the learner was sensitive to the fea-
tures used. Selecting the appropriate set of
features was crucial to achieve a good per-
formance. The best feature combination was
composed of: i) a baseline quality estimation
feature set (Asiya or Quest) but not both of
them, ii) Length Model, iii) Pseudo-reference
aligned based features, and iv) adapted lan-
guage models. However, within the de-en
task, substituting Length Model and Aligned
Pseudo-references by the features based on
17http://asiya.lsi.upc.edu/
Semantic Roles could bring marginally bet-
ter accuracy.
TCD-CNGL (T1.1) and TCD-DCU-CNGL
(T1.3): The system is based on features
which are commonly used for style classifi-
cation (e.g. author identification). The as-
sumption is that low/high quality translations
can be characterised by some patterns which
are frequent and/or differ significantly from
the opposite category. Such features are in-
tended to focus on striking patterns rather
than to capture the global quality in a sen-
tence, but they are used in conjunction with
classical features for quality estimation (lan-
guage modelling, etc.). This requires two
steps in the training process: first the refer-
ence categories against which sentences will
be compared are built, then the standard qual-
ity estimation model training stage is per-
formed. Both datasets (Tasks 1.1 and 1.3)
were used for both tasks. Since the number
of features can be very high (up to 65,000),
a combination of various heuristics for se-
lecting features was used before the training
stage (the submitted systems were trained us-
ing SVM with RBF kernels).
UMAC (T1.1, T1.2, T2): For Task 1.1, the fea-
ture set consists in POS sequences of the
source and target languages, using 12 uni-
versal tags that are common in both lan-
guages. The algorithm is an enhanced ver-
sion of the BLEU metric (EBLEU) designed
with a modified length penalty and added re-
call factor, and having the precision and re-
call components grouped using the harmonic
mean. For Task 1.2, in addition to the uni-
versal POS sequences of the source and tar-
get languages, features include the scores of
length penalty, precision, recall and rank.
Variants of EBLEU with different strategies
for alignment are used, as well as a Na??ve
Bayes classification algorithm. For Task 2,
the features used are unigrams (from previous
4th to following 3rd tokens), bigrams (from
previous 2nd to following 2nd tokens), skip
bigrams (previous and next token), trigrams
(from previous 2nd to following 2nd tokens).
The learning algorithms are Conditional Ran-
dom Fields and Na??ve Bayes.
24
6.8 Results
In what follows we give the official results for all
tasks followed by a discussion that highlights the
main findings for each of the tasks.
Task 1.1 Predicting post-editing distance
Table 12 summarises the results for the ranking
variant of the task. They are sorted from best to
worse using the DeltaAvg metric scores as primary
key and the Spearman?s rank correlation scores as
secondary key.
The winning submissions for the ranking vari-
ant of Task 1.1 are CNGL SVRPLS, with a
DeltaAvg score of 11.09, and DCU-SYMC all-
types, with a DeltaAvg score of 10.13. While the
former holds the higher score, the difference is not
significant at the p ? 0.05 level as estimated by a
bootstrap resampling test.
Both submissions are better than the baseline
system by a very wide margin, a larger relative im-
provement than that obtained in the corresponding
WMT12 task. In addition, five submissions (out
of 12 systems) scored significantly higher than the
baseline system (systems above the middle gray
area), which is a larger proportion than that in last
year?s task (only 3 out of 16 systems), indicat-
ing that this shared task succeeded in pushing the
state-of-the-art performance to new levels.
In addition to the performance of the official
submission, we report results obtained by two or-
acle methods: the gold-label HTER metric com-
puted against the post-edited translations as ref-
erence (Oracle HTER), and the BLEU metric (1-
BLEU to obtain the same range as HTER) com-
puted against the same post-edited translations as
reference (Oracle HBLEU). The ?Oracle HTER?
DeltaAvg score of 16.38 gives an upperbound in
terms of DeltaAvg for the test set used in this eval-
uation. It indicates that, for this set, the differ-
ence in post-editing effort between the top quality
quantiles and the overall quality is 16.38 on aver-
age. The oracle based on HBLEU gives a lower
DeltaAvg score, which is expected since HTER
was our actual gold label. However, it is still
significantly higher than the score of the winning
submission, which shows that there is significant
room for improvement even by the highest scor-
ing submissions.
The results for the scoring variant of the task
are presented in Table 13, sorted from best to
worse by using the MAE metric scores as primary
key and the RMSE metric scores as secondary key.
According to MAE scores, the winning submis-
sion is SHEF FS (MAE = 12.42), which uses fea-
ture selection and a novel learning algorithm for
the task, Gaussian Processes. The baseline sys-
tem is measured to have an MAE of 14.81, with
six other submissions having performances that
are not different from the baseline at a statisti-
cally significant level, as shown by the gray area
in the middle of Table 13). Nine submissions (out
of 16) scored significantly higher than the base-
line system (systems above the middle gray area),
a considerably higher proportion of submissions
as compared to last year (5 out of 19), which indi-
cates that this shared task also succeeded in push-
ing the state-of-the-art performance to new levels
in terms of absolute scoring. Only one (6%) sys-
tem scored significantly lower than the baseline,
as opposed to 8 (42%) in last year?s task.
For the sake of completeness, we also show or-
acles figures using the same methods as for the
ranking variant of the task. Here the lowerbound
in error (Oracle HTER) will clearly be zero, as
both MAE and RMSE are measured against the
same gold label used for the oracle computation.
?Oracle HBLEU? is also not indicative in this
case, as the although the values for the two metrics
(HTER and HBLEU) are within the same ranges,
they are not directly comparable. This explains the
larger MAE/RMSE figures for ?Oracle HBLEU?
than those for most submissions.
Task 1.2 Selecting the best translation
Below we present the results for this task for each
of the two Kendall?s ? flavours presented in Sec-
tion 6.6, for the German-English test set (Tables 14
and 16) and the English-Spanish test set (Tables 15
and 17). The results are sorted from best to worse
using each of the Kendall?s ? metric flavours.
For German-English, the winning submission is
DFKI?s logRegFss33 entry, for both Kendall?s ?
with ties penalised and ties ignored, with ? = 0.31
(since this submission has no ties, the two met-
rics give the same ? value). A trivial baseline that
proposes random ranks (with ties allowed) has a
Kendall?s ? with ties penalised of -0.12 (as this
metric penalises the system?s ties that were non-
ties in the reference), and a Kendall?s ? with ties
ignored of 0.08. Most of the submissions per-
formed better than this simple baseline. More in-
terestingly perhaps is the comparison between the
best submission and the performance by an ora-
25
System ID DeltaAvg Spearman ?
? CNGL SVRPLS 11.09 0.55
? DCU-SYMC alltypes 10.13 0.59
SHEF FS 9.76 0.57
CNGL SVR 9.88 0.51
DCU-SYMC combine 9.84 0.59
CMU noB 8.98 0.57
SHEF FS-AL 8.85 0.50
Baseline bb17 SVR 8.52 0.46
CMU full 8.23 0.54
LIMSI 8.15 0.44
TCD-CNGL open 6.03 0.33
TCD-CNGL restricted 5.85 0.31
UMAC 2.74 0.11
Oracle HTER 16.38 1.00
Oracle HBLEU 15.74 0.93
Table 12: Official results for the ranking variant of the WMT13 Quality Estimation Task 1.1. The winning submissions are
indicated by a ? (they are significantly better than all other submissions according to bootstrap resampling (10k times) with
95% confidence intervals). The systems in the gray area are not different from the baseline system at a statistically significant
level according to the same test. Oracle results that use human-references are also shown for comparison purposes.
System ID MAE RMSE
? SHEF FS 12.42 15.74
SHEF FS-AL 13.02 17.03
CNGL SVRPLS 13.26 16.82
LIMSI 13.32 17.22
DCU-SYMC combine 13.45 16.64
DCU-SYMC alltypes 13.51 17.14
CMU noB 13.84 17.46
CNGL SVR 13.85 17.28
FBK-UEdin extra 14.38 17.68
FBK-UEdin rand-svr 14.50 17.73
LORIA inctrain 14.79 18.34
Baseline bb17 SVR 14.81 18.22
TCD-CNGL open 14.81 19.00
LORIA inctraincont 14.83 18.17
TCD-CNGL restricted 15.20 19.59
CMU full 15.25 18.97
UMAC 16.97 21.94
Oracle HTER 0.00 0.00
Oracle HBLEU (1-HBLEU) 16.85 19.72
Table 13: Official results for the scoring variant of the WMT13 Quality Estimation Task 1.1. The winning submission is
indicated by a ? (it is significantly better than the other submissions according to bootstrap resampling (10k times) with 95%
confidence intervals). The systems in the gray area are not different from the baseline system at a statistically significant level
according to the same test. Oracle results that use human-references are also shown for comparison purposes.
26
German-English System ID Kendall?s ? with ties penalised
? DFKI logRegFss33 0.31
DFKI logRegFss24 0.28
CNGL SVRPLSF1 0.17
CNGL SVRF1 0.17
DCU CCG 0.15
UPC AQE+SEM+LM 0.11
UPC AQE+LeM+ALGPR+LM 0.10
DCU baseline+CCG 0.00
Baseline Random-ranks-with-ties -0.12
UMAC EBLEU-I -0.39
UMAC NB-LPR -0.49
Oracle Human 1.00
Oracle BLEU (margin 0.00) 0.19
Oracle BLEU (margin 0.01) 0.05
Oracle METEOR-ex (margin 0.00) 0.23
Oracle METEOR-ex (margin 0.01) 0.06
Table 14: Official results for the Task 1.2 of the WMT13 Quality Estimation shared task for German-English, using as metric
Kendall?s ? with ties penalised. The winning submissions are indicated by a ?. Oracle results that use human-references are
also shown for comparison purposes.
English-Spanish System ID Kendall?s ? with ties penalised
? CNGL SVRPLSF1 0.15
CNGL SVRF1 0.13
DFKI logRegL2-411 0.09
DFKI logRegL2-431 0.04
UPC QQE+LeM+ALGPR+LM -0.03
UPC AQE+LeM+ALGPR+LM -0.06
CMU BLEUopt -0.11
Baseline Random-ranks-with-ties -0.23
UMAC EBLEU-A -0.27
UMAC EBLEU-I -0.35
CMU cls -0.63
Oracle Human 1.00
Oracle BLEU (margin 0.00) 0.17
Oracle BLEU (margin 0.02) -0.06
Oracle METEOR-ex (margin 0.00) 0.19
Oracle METEOR-ex (margin 0.02) 0.05
Table 15: Official results for the Task 1.2 of the WMT13 Quality Estimation shared task for English-Spanish, using as metric
Kendall?s ? with ties penalised. The winning submissions are indicated by a ?. Oracle results that use human-references are
also shown for comparison purposes.
27
German-English System ID Kendall?s ? with ties ignored Nr. of non-ties / Nr. of decisions
? DFKI logRegFss33 0.31 882/882
DFKI logRegFss24 0.28 882/882
UPC AQE+SEM+LM 0.27 768/882
UPC AQE+LeM+ALGPR+LM 0.24 788/882
DCU CCG 0.18 862/882
CNGL SVRPLSF1 0.17 882/882
CNGL SVRF1 0.17 881/882
Baseline Random-ranks-with-ties 0.08 718/882
DCU baseline+CCG 0.01 874/882
UMAC NB-LPR 0.01 447/882
UMAC EBLEU-I -0.03 558/882
Oracle Human 1.00 882/882
Oracle BLEU (margin 0.00) 0.22 859/882
Oracle BLEU (margin 0.01) 0.27 728/882
Oracle METEOR-ex (margin 0.00) 0.20 869/882
Oracle METEOR-ex (margin 0.01) 0.24 757/882
Table 16: Official results for the Task 1.2 of the WMT13 Quality Estimation shared task for German-English, using as metric
Kendall?s ? with ties ignored. The winning submissions are indicated by a ?. Oracle results that use human-references are also
shown for comparison purposes.
English-Spanish System ID Kendall?s ? with ties ignored Nr. of non-ties / Nr. of decisions
? CMU cls 0.23 192/633
CNGL SVRPLSF1 0.16 632/633
CNGL SVRF1 0.13 631/633
DFKI logRegL2-411 0.13 610/633
UPC QQE+LeM+ALGPR+LM 0.11 554/633
UPC AQE+LeM+ALGPR+LM 0.08 554/633
UMAC EBLEU-A 0.07 430/633
DFKI logRegL2-431 0.04 633/633
Baseline Random-ranks-with-ties 0.03 507/633
UMAC EBLEU-I 0.02 407/633
CMU BLEUopt -0.11 633/633
Oracle Human 1.00 633/633
Oracle BLEU (margin 0.00) 0.19 621/633
Oracle BLEU (margin 0.02) 0.26 474/633
Oracle METEOR-ex (margin 0.00) 0.25 623/633
Oracle METEOR-ex (margin 0.02) 0.28 517/633
Table 17: Official results for the Task 1.2 of the WMT13 Quality Estimation shared task for English-Spanish, using as metric
Kendall?s ? with ties ignored. The winning submissions are indicated by a ?. Oracle results that use human-references are also
shown for comparison purposes.
28
cle method that has access to human-created refer-
ences. This oracle uses human references to com-
pute BLEU and METEOR scores for each trans-
lation segment, and consequently computes rank-
ings for the competing translations based on these
scores. To reflect the impact of ties on the two
versions of Kendall?s ? metric we use, we allow
these ranks to be tied if the difference between the
oracle BLEU or METEOR scores is smaller than
a margin (see lower section of Tables 14 and 16,
with margins of 0 and 0.01 for the scores). For ex-
ample, under a regime of BLEU with margin 0.01,
a translation with BLEU score of 0.172 would get
the same rank as a translation with BLEU score of
0.164 (difference of 0.008), but a higher rank than
a translation with BLEU score of 0.158 (difference
of 0.014). Not surprisingly, under the Kendall?s
? with ties penalised the best Oracle BLEU or
METEOR performance happens for a 0.0 mar-
gin (which makes ties possible only for exactly-
matching scores), for a value of ? = 0.19 and
? = 0.23, respectively. Under the Kendall?s ? with
ties ignored, the Oracle BLEU performance for a
0.01 margin (i.e, translations under 1 BLEU point
should be considered as having the same rank)
achieves ? = 0.27, while Oracle METEOR for a
0.01 margin achieves ? = 0.24. These values are
lower than the ? = 0.31 of the winning submis-
sion without access to reference translations, sug-
gesting that quality estimation models are capable
of better modelling translation differences com-
pared to traditional, human reference-based MT
evaluation metrics.
For English-Spanish, under Kendall?s ? with
ties penalised the winning submission is CNGL?s
SVRPLSF1, with ? = 0.15. Under Kendall?s ?
with ties ignored, the best scoring submission is
CMU?s cls with ? = 0.23, but this is achieved
by offering non-tie judgements only for 192 of the
633 total judgements (30% of them). As we dis-
cussed in Section 6.6, the ?Kendall?s ? with ties
ignored? metric is weak with respect to compar-
ing different submissions, since it favours systems
that are do not commit to a given rank and rather
produce a large number of ties. This becomes even
clearer when we look at the performance of the or-
acle methods (Tables 15 and 17). Under Kendall?s
? with ties penalised, ?Oracle BLEU? (margin
0.00) achieves ? = 0.17, while under Kendall?s
? with ties ignored, ?Oracle BLEU? (margin 0.02)
has a ? = 0.26. This results in 474 non-tie deci-
sions (75% of them), and a better ? value com-
pared to ?Oracle BLEU? (margin 0.00), with a
? = 0.19 under the same metric. The oracle values
for both BLEU and METEOR are close to the ?
values of the winning submissions, supporting the
conclusion that quality estimation techniques can
successfully replace traditional, human reference-
based MT evaluation metrics.
Task 1.3 Predicting post-editing time
Results for this task are presented in Table 18.
A third of the submissions was able to beat the
baseline. Among these FBK-UEDIN?s submission
ranked best in terms of MAE, our main metric for
this task, and also achieved the lowest RMSE.
Only three systems were able to beat our base-
line in terms of MAE. Please note that while all
features were available to the participants, our
baseline is actually a competitive system.
The second-best entry, CNGL SVR, reached
the highest Spearman?s rank correlation, our sec-
ondary metric. Furthermore, in terms of this met-
ric all four top-ranking entries, two by CNGL and
FBK-UEDIN respectively, are significantly better
than the baseline (10k bootstrap resampling test
with 95% confidence intervals). As high ranking
submissions also yield strong rank correlation to
the observed post-editing time, we can be confi-
dent that improvements in MAE are not only due
to better handling of extreme cases.
Many participants submitted two variants of
their systems with different numbers of features
and/or machine learning approaches. In Table 18
we can see these are grouped closely together giv-
ing rise to the assumption that the general pool of
available features and thereby the used resources
and strongest features are most relevant for a sys-
tem?s performance. Another hint in that direction
is the observation the top-ranked systems rely on
additional data and resources to generate their fea-
tures.
Task 2 Predicting word-level scores
Results for this task are presented in Table 19 and
20, sorted by macro average F1. Since this is a
new task, we have yet to establish a strong base-
line. For reference we provide a trivial baseline
that predicts the dominant class ? (K)eep ? for ev-
ery token.
The first observation in Table 19 is that this triv-
ial baseline is difficult to beat in terms of accuracy.
However, considering our main metric ? macro-
29
System ID MAE RMSE Pearson?s r Spearman?s ?
? FBK-UEDIN Extra 47.5 82.6 0.65 0.75
? FBK-UEDIN Rand-SVR 47.9 86.7 0.66 0.74
CNGL SVR 49.2 90.4 0.67 0.76
CNGL SVRPLS 49.6 86.6 0.68 0.74
CMU slim 51.6 84.7 0.63 0.68
Baseline bb17 SVR 51.9 93.4 0.61 0.70
DFKI linear6 52.4 84.3 0.64 0.68
CMU full 53.6 92.2 0.58 0.60
DFKI pls8 53.6 88.3 0.59 0.67
TCD-DCU-CNGL SVM2 55.8 98.9 0.47 0.60
TCD-DCU-CNGL SVM1 55.9 99.4 0.48 0.60
SHEF FS 55.9 103.1 0.42 0.61
SHEF FS-AL 64.6 99.1 0.57 0.60
LIMSI elastic 70.6 114.4 0.58 0.64
Table 18: Official results for the Task 1.3 of the WMT13 Quality Estimation shared-task. The winning submissions are
indicated by a ? (they are significantly better than all other submissions according to bootstrap resampling (10k times) with
95% confidence intervals). The systems in the gray area are not different from the baseline system at a statistically significant
level according to the same test.
Keep Change
System ID Accuracy Prec. Recall F1 Prec. Recall F1 Macro F1
? LIG FS BIN 0.74 0.79 0.86 0.82 0.56 0.43 0.48 0.65
? LIG BOOST BIN 0.74 0.78 0.88 0.83 0.57 0.37 0.45 0.64
CNGL GLM 0.70 0.76 0.86 0.80 0.47 0.31 0.38 0.59
UMAC NB 0.56 0.82 0.49 0.62 0.37 0.73 0.49 0.55
CNGL GLMd 0.71 0.74 0.93 0.82 0.51 0.19 0.28 0.55
UMAC CRF 0.71 0.72 0.98 0.83 0.49 0.04 0.07 0.45
Baseline (one class) 0.71 0.71 1.00 0.83 0.00 0.00 0.00 0.42
Table 19: Official results for Task 2: binary classification on word level of the WMT13 Quality Estimation shared-task. The
winning submissions are indicated by a ?.
System ID F1 Keep F1 Substitute F1 Delete Micro-F1 Macro-F1
? LIG FS MULT 0.83 0.44 0.072 0.72 0.45
? LIG ALL MULT 0.83 0.45 0.064 0.72 0.45
UMAC NB 0.62 0.43 0.042 0.52 0.36
CNGL GLM 0.83 0.18 0.028 0.71 0.35
CNGL GLMd 0.83 0.14 0.034 0.72 0.34
UMAC CRF 0.83 0.04 0.012 0.71 0.29
Baseline (one class) 0.83 0.00 0.000 0.71 0.28
Table 20: Official results for Task 2: multiclass classification on word level of the WMT13 Quality Estimation shared-task.
The winning submissions are indicated by a ?.
30
average F1 ? it is clear that all systems outperform
the baseline. The winning systems by LIG for the
binary task are also the top ranking systems on the
multiclass task.
While promising results are found for the bi-
nary variant of the task where systems are able to
achieve an F1 of almost 0.5 for the relevant class
? Change, the multiclass prediction variant of the
task seem to suffer from its severe class imbalance.
In fact, none of the systems shows good perfor-
mance when predicting deletions.
6.9 Discussion
In what follows, we discuss the main accomplish-
ments of this shared task starting from the goals
we had previously identified for it.
Explore various granularity levels for the
quality-prediction task The decision on which
level of granularity quality estimation is applied
depends strongly on the intended application. In
Task 2 we tested binary word-level classification
in a post-editing setting. If such annotation is pre-
sented through a user interface we imagine that
words marked as incorrect would be hidden from
the editor, highlighted as possibly wrong or that a
list of alternatives would we generated.
With respect to the poor improvements over
trivial baselines, we consider that the results for
word-level prediction could be mostly connected
to limitations of the datasets provided, which are
very small for word-level prediction, as compared
to successful previous work such as (Bach et al,
2011). Despite the limited amount of training
data, several systems were able to predict dubious
words (binary variant of the task), showing that
this can be a promising task. Extending the granu-
larity even further by predicting the actual editing
action necessary for a word yielded less positive
results than the binary setting.
We cannot directly compare sentence- and
word-level results. However, since sentence-level
predictions can benefit from more information
available and therefore more signal on which the
prediction is based, the natural conclusion is that,
if there is a choice in the prediction granularity,
to opt for the coarser one possible (i.e., sentence-
level over word-level). But certain applications
may require finer granularity levels, and therefore
word-level predictions can still be very valuable.
Explore the prediction of more objective scores
Given the multitude of possible applications for
quality estimation we must decide which predicted
values are both useful and accurate. In this year?s
task we have attempted to address the useful-
ness criterion by moving from the subjective, hu-
man judgement-based scores, to the prediction of
scores that can be more easily interpreted for prac-
tical applications: post-editing distance or types of
edits (word-level), post-editing time, and ranking
of alternative translations.
The general promise of using objective scores is
that predicting a value that is related to the use case
will make quality estimation more applicable and
yield lower deviance compared to the use of proxy
metrics. The magnitude of this benefit should be
sufficient to account for the possible additional ef-
fort related to collecting such scores.
While a direct comparison between the differ-
ent types of scores used for this year?s tasks is not
possible as they are based on different datasets, if
we compare last year?s task on predicting 1-5 lik-
ert scores (and generating an overall ranking of all
translations in the test set) with this year?s Task
1.1, which is virtually the same, but using post-
editing distance as gold-label, we see that the num-
ber of systems that outperform the baseline 18 is
proportionally larger this year. We can also notice
a higher relative improvement of these submis-
sions over the baseline system. While this could
simply be a consequence of progress in the field, it
may also provide an indication that objective met-
rics are more suitable for the problem.
Particularly with respect to post-editing time,
given that this label has a long tailed distribution
and is not trivial to measure even in a controlled
environment, the results of Task 1.3 are encour-
aging. Comparison with the better results seen
on Tasks 1.1 and 1.2, however, suggests that, for
Task 1.3, additional data processing, filtering, and
modelling (including modelling translator-specific
traits such as their variance in time) is required, as
evidenced in (Cohn and Specia, 2013).
Explore the use of quality estimation tech-
niques to replace reference-based MT evalua-
tion metrics When it comes to the task of au-
tomatically ranking alternative translations gener-
ated by different MT systems, the traditional use
of reference-based MT evaluation metrics is chal-
lenged by the findings of this task.
The top ranking quality estimation submissions
18The two baselines are exactly the same, and therefore the
comparison is meaningful.
31
to Task 1.2 have performances that outperform or
are at least at the same level with the ones that
involve the use of human references. The most in-
teresting property of these techniques is that, be-
ing reference-free, they can be used for any source
sentences, and therefore are ready to be deployed
for arbitrary texts.
An immediate application for this capability is
a procedure by which MT system-selection is per-
formed, based on the output of such quality esti-
mators. Additional measurements are needed to
determine the level of improvement in translation
quality that the current performance of these tech-
niques can achieve in a system-selection scenario.
Identify new and effective quality indicators
Quality indicators, or features, are core to the
problem of quality estimation. One significant dif-
ference this year with respect to previous year was
the availability of QUEST, a framework for the ex-
traction of a large number of features. A few sub-
missions used these larger sets ? as opposed to the
17 baseline features used in the 2012 edition ? as
their starting point, to which they added other fea-
tures. Most features available in this framework,
however, had already been used in previous work.
Novel families of features used this year which
seems to have played an important role are those
proposed by CNGL. They include a number of
language and MT-system independent monolin-
gual and bilingual similarity metrics between the
sentences for prediction and corpora of the lan-
guage pair under consideration. Based on standard
regression algorithm (the same used by the base-
line system), the submissions from CNGL using
such feature families topped many of the tasks.
Another interesting family of features is that
used by TCD-CNGL and TCD-DCU-CNGL for
Tasks 1.1 and 1.3. These were borrowed from
work on style or authorship identification. The as-
sumption is that low/high quality translations can
be characterised by some patterns which are fre-
quent and/or differ significantly from patterns be-
longing to the opposite category.
Like in last year?s task, the vast majority of
the participating systems used external resources
in addition to those provided for the task, par-
ticularly for linguistically-oriented features, such
as parsers, part-of-speech taggers, named entity
recognizers, etc. A novel set of syntactic fea-
tures based on Combinatory Categorial Grammar
(CCG) performed reasonably well in Task 1.2:
with six CCG-based features and no additional
features, the system outperformed the baseline
system and also a second submission where the
17 baseline features were added. This highlights
the potential of linguistically-motivated features
for the problem.
As expected, different feature sets were used
for different tasks. This is essential for Task 2,
where word-level features are certainly necessary.
For example, LIG used a number of lexical fea-
tures such as part-of-speech tag, word-posterior
probabilities, syntactic (constituent label, distance
to the constituent tree root, and target and source
polysemy count). For submissions where a se-
quence labelling algorithm such as a Conditional
Random Fields was used for prediction, the inter-
dependencies between adjacent words and labels
was also modelled though features.
Pseudo-references, i.e., scores from standard
evaluation metrics such as BLEU based on trans-
lations generated by an alternative MT system as
?reference?, featured in more than half of the sub-
missions for sentence-level tasks. This is not sur-
prising given their performance in previous work
on quality estimation.
Identify effective machine learning techniques
for all variants of the quality estimation task
For the sentence-level tasks, standard regression
methods such as SVR performed well as in the
previous edition of the shared task, topping the
results for the ranking variant of Task 1.1, both
first and second place. In fact this algorithm was
used by most submissions that outperformed the
baseline. An alternative algorithm to SVR with
very promising results and which was introduced
for the problem this year is that of Gaussian Pro-
cesses. It was used by SHEF, the winning submis-
sion in the scoring variant of Task 1.1, which also
performed well in the ranking variant, despite its
hyperparameters having been optimised for scor-
ing only. Algorithms behave similarly for Task
1.3, with SVR performing particularly well.
For Task 1.2, logistic regression performed the
best or among the best, along with SVR. One of
the most effective approach for this task, however,
appears to be one that is better tailored for the
task, namely pair-wise decomposition for ranking.
This approach benefits from transforming a k-way
ranking problem into a series of simpler, 2-way
ranking problems, which can be more accurately
solved. Another approach that shows promise is
32
that of ensemble of regressors, in which the output
is the results combining the predictions of differ-
ent regression models.
Linear-chain Conditional Random Fields are a
popular model of choice for sequence labelling
tasks and have been successfully used by several
participants in Task 2, along with discriminatively
trained Hidden Markov Models and Na??ve Bayes.
As in the previous edition, feature engineer-
ing and feature selection prior to model learning
were important components in many submissions.
However, the role of individual features is hard
to judge separately from the role of the machine
learning techniques employed.
Establish the state of the art performance All
four tasks addressed in this shared task have
achieved a dual role that is important for the re-
search community: (i) to make publicly available
new data sets that can serve to compare different
approaches and contributions; and (ii) to estab-
lish the present state-of-the-art performance in the
field, so that progress can be easily measured and
tracked. In addition, the public availability of the
scoring scripts makes evaluation and direct com-
parison straightforward.
Many participants submitted predictions for
several tasks. Comparison of the results shows
that there is little overlap between the best sys-
tems when the predicted value is varied. While
we did not formally require the participants to use
similar systems across tasks, these results indicate
that specialised systems with features selected de-
pending on the predicted variable can in fact be
beneficial.
As we mentioned before, compared to the pre-
vious edition of the task, we noticed (for Task
1.1) a larger relative improvement of scores over
the baseline system, as well as a larger propor-
tion of systems outperforming the baseline sys-
tems, which are a good indication that the field is
progressing over the years. For example, in the
scoring variant of Task 1.1, last year only 5 out of
20 systems (i.e. 25% of the systems) were able to
significantly outperform the baseline. This year, 9
out 16 systems (i.e. 56%) outperformed the same
baseline. Last year, the relative improvement of
the winning submission with respect to the base-
line system was 13%, while this year the relative
improvement is of 19%.
Overall, the tables of results presented in Sec-
tion 6.8 give a comprehensive view of the current
state-of-the-art on the data sets used for this shared
task, as well as indications on how much room
there still is for improvement via figures from ora-
cle methods. As a result, people interested in con-
tributing to research in these machine translation
quality estimation tasks will be able to do so in a
principled way, with clearly established state-of-
the-art levels and straightforward means of com-
parison.
7 Summary
As in previous incarnations of this workshop we
carried out an extensive manual and automatic
evaluation of machine translation performance,
and we used the human judgements that we col-
lected to validate automatic metrics of translation
quality. We also refined last year?s quality estima-
tion task, asking for methods that predict sentence-
level post-editing effort and time, rank translations
from alternative systems, and pinpoint words in
the output that are more likely to be wrong.
As in previous years, all data sets generated by
this workshop, including the human judgments,
system translations and automatic scores, are pub-
licly available for other researchers to analyze.19
Acknowledgments
This work was supported in parts by the
MosesCore, Casmacat, Khresmoi, Matecat and
QTLaunchPad projects funded by the European
Commission (7th Framework Programme), and by
gifts from Google, Microsoft and Yandex.
We would also like to thank our colleagues Ma-
tous? Macha?c?ek and Martin Popel for detailed dis-
cussions.
References
Allauzen, A., Pe?cheux, N., Do, Q. K., Dinarelli,
M., Lavergne, T., Max, A., Le, H.-S., and Yvon,
F. (2013). LIMSI @ WMT13. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 60?67, Sofia, Bulgaria. As-
sociation for Computational Linguistics.
Almaghout, H. and Specia, L. (2013). A CCG-
based Quality Estimation Metric for Statistical
Machine Translation. In Proceedings of MT
Summit XIV (to appear), Nice, France.
Ammar, W., Chahuneau, V., Denkowski, M., Han-
neman, G., Ling, W., Matthews, A., Murray,
19http://statmt.org/wmt13/results.html
33
K., Segall, N., Lavie, A., and Dyer, C. (2013).
The CMU machine translation systems at WMT
2013: Syntax, synthetic translation options, and
pseudo-references. In Proceedings of the Eighth
Workshop on Statistical Machine Translation,
pages 68?75, Sofia, Bulgaria. Association for
Computational Linguistics.
Avramidis, E. (2012). Comparative Quality Es-
timation: Automatic Sentence-Level Ranking
of Multiple Machine Translation Outputs. In
Proceedings of 24th International Conference
on Computational Linguistics, pages 115?132,
Mumbai, India.
Avramidis, E. and Popovic, M. (2013). Selecting
feature sets for comparative and time-oriented
quality estimation of machine translation out-
put. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, pages 327?
334, Sofia, Bulgaria. Association for Computa-
tional Linguistics.
Avramidis, E., Popovic?, M., Vilar, D., and Bur-
chardt, A. (2011). Evaluate with confidence es-
timation: Machine ranking of translation out-
puts using grammatical features. In Proceed-
ings of the Sixth Workshop on Statistical Ma-
chine Translation.
Aziz, W., Mitkov, R., and Specia, L. (2013).
Ranking Machine Translation Systems via Post-
Editing. In Proc. of Text, Speech and Dialogue
(TSD), Lecture Notes in Artificial Intelligence,
Berlin / Heidelberg. Za?padoc?eska? univerzita v
Plzni, Springer Verlag.
Bach, N., Huang, F., and Al-Onaizan, Y. (2011).
Goodness: A method for measuring machine
translation confidence. In Proceedings of the
49th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies, pages 211?219, Portland, Ore-
gon, USA.
Beck, D., Shah, K., Cohn, T., and Specia, L.
(2013). SHEF-Lite: When less is more for
translation quality estimation. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 335?340, Sofia, Bulgaria.
Association for Computational Linguistics.
Bic?ici, E., Groves, D., and van Genabith, J. (2013).
Predicting sentence translation quality using ex-
trinsic and language independent features. Ma-
chine Translation.
Bic?ici, E. and van Genabith, J. (2013). CNGL-
CORE: Referential translation machines for
measuring semantic similarity. In *SEM 2013:
The Second Joint Conference on Lexical and
Computational Semantics, Atlanta, Georgia,
USA. Association for Computational Linguis-
tics.
Bicici, E. (2013a). Feature decay algorithms
for fast deployment of accurate statistical ma-
chine translation systems. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, pages 76?82, Sofia, Bulgaria. Associa-
tion for Computational Linguistics.
Bicici, E. (2013b). Referential translation ma-
chines for quality estimation. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 341?349, Sofia, Bulgaria.
Association for Computational Linguistics.
B??lek, K. and Zeman, D. (2013). CUni multilin-
gual matrix in the WMT 2013 shared task. In
Proceedings of the Eighth Workshop on Statis-
tical Machine Translation, pages 83?89, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Bojar, O., Kos, K., and Marec?ek, D. (2010). Tack-
ling Sparse Data Issue in Machine Translation
Evaluation. In Proceedings of the ACL 2010
Conference Short Papers, pages 86?91, Upp-
sala, Sweden. Association for Computational
Linguistics.
Bojar, O., Rosa, R., and Tamchyna, A. (2013).
Chimera ? three heads for English-to-Czech
translation. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
90?96, Sofia, Bulgaria. Association for Compu-
tational Linguistics.
Borisov, A., Dlougach, J., and Galinskaya, I.
(2013). Yandex school of data analysis ma-
chine translation systems for WMT13. In Pro-
ceedings of the Eighth Workshop on Statistical
Machine Translation, pages 97?101, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Callison-Burch, C., Fordyce, C., Koehn, P., Monz,
C., and Schroeder, J. (2007). (Meta-) evaluation
of machine translation. In Proceedings of the
Second Workshop on Statistical Machine Trans-
lation (WMT07), Prague, Czech Republic.
Callison-Burch, C., Fordyce, C., Koehn, P., Monz,
C., and Schroeder, J. (2008). Further meta-
34
evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Ma-
chine Translation (WMT08), Colmbus, Ohio.
Callison-Burch, C., Koehn, P., Monz, C., Pe-
terson, K., Przybocki, M., and Zaidan, O. F.
(2010). Findings of the 2010 joint workshop
on statistical machine translation and metrics
for machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Trans-
lation (WMT10), Uppsala, Sweden.
Callison-Burch, C., Koehn, P., Monz, C., Post, M.,
Soricut, R., and Specia, L. (2012). Findings of
the 2012 workshop on statistical machine trans-
lation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 10?
51, Montre?al, Canada. Association for Compu-
tational Linguistics.
Callison-Burch, C., Koehn, P., Monz, C., and
Schroeder, J. (2009). Findings of the 2009
workshop on statistical machine translation. In
Proceedings of the Fourth Workshop on Sta-
tistical Machine Translation (WMT09), Athens,
Greece.
Callison-Burch, C., Koehn, P., Monz, C., and
Zaidan, O. (2011). Findings of the 2011 work-
shop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical
Machine Translation, pages 22?64, Edinburgh,
Scotland.
Camargo de Souza, J. G., Buck, C., Turchi, M.,
and Negri, M. (2013). FBK-UEdin participa-
tion to the WMT13 quality estimation shared
task. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, pages 350?
356, Sofia, Bulgaria. Association for Computa-
tional Linguistics.
Charniak, E. and Johnson, M. (2005). Coarse-to-
fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual
Meeting on Association for Computational Lin-
guistics, pages 173?180. Association for Com-
putational Linguistics.
Cho, E., Ha, T.-L., Mediani, M., Niehues, J., Her-
rmann, T., Slawik, I., and Waibel, A. (2013).
The Karlsruhe Institute of Technology transla-
tion systems for the WMT 2013. In Proceed-
ings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 102?106, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Cohen, J. (1960). A coefficient of agreement for
nominal scales. Educational and Psychological
Measurment, 20(1):37?46.
Cohn, T. and Specia, L. (2013). Modelling Anno-
tator Bias with Multi-task Gaussian Processes:
An Application to Machine Translation Quality
Estimation. In Proceedings of the 51st Annual
Meeting of the Association for Computational
Linguistics (to appear).
Collins, M. (2002). Discriminative training meth-
ods for hidden markov models: theory and ex-
periments with perceptron algorithms. In Pro-
ceedings of the ACL-02 conference on Empir-
ical methods in natural language processing -
Volume 10, EMNLP ?02, pages 1?8, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
de Marneffe, M.-C., MacCartney, B., and Man-
ning, C. D. (2006). Generating typed depen-
dency parses from phrase structure parses. In
Proceedings of LREC-06, pages 449?454.
Denkowski, M. and Lavie, A. (2010). Meteor-next
and the meteor paraphrase tables: improved
evaluation support for five target languages. In
Proceedings of the Joint Fifth Workshop on Sta-
tistical Machine Translation and MetricsMATR,
WMT ?10, pages 339?342, Stroudsburg, PA,
USA. Association for Computational Linguis-
tics.
Durgar El-Kahlout, I. and Mermer, C. (2013).
TU?btak-blgem german-english machine trans-
lation systems for w13. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, pages 107?111, Sofia, Bulgaria. Associ-
ation for Computational Linguistics.
Durrani, N., Fraser, A., Schmid, H., Sajjad, H.,
and Farkas, R. (2013a). Munich-Edinburgh-
Stuttgart submissions of OSM systems at
WMT13. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
120?125, Sofia, Bulgaria. Association for Com-
putational Linguistics.
Durrani, N., Haddow, B., Heafield, K., and Koehn,
P. (2013b). Edinburgh?s machine translation
systems for European language pairs. In Pro-
ceedings of the Eighth Workshop on Statisti-
cal Machine Translation, pages 112?119, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
35
Eidelman, V., Wu, K., Ture, F., Resnik, P., and Lin,
J. (2013). Towards efficient large-scale feature-
rich statistical machine translation. In Proceed-
ings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 126?131, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Federmann, C. (2012). Appraise: An Open-
Source Toolkit for Manual Evaluation of Ma-
chine Translation Output. The Prague Bulletin
of Mathematical Linguistics (PBML), 98:25?
35.
Fleiss, J. L. (1971). Measuring nominal scale
agreement among many raters. Psychological
Bulletin, 76(5):378?382.
Formiga, L., Costa-jussa`, M. R., Marin?o, J. B.,
Fonollosa, J. A. R., Barro?n-Ceden?o, A., and
Marquez, L. (2013a). The TALP-UPC phrase-
based translation systems for WMT13: System
combination with morphology generation, do-
main adaptation and corpus filtering. In Pro-
ceedings of the Eighth Workshop on Statisti-
cal Machine Translation, pages 132?138, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Formiga, L., Gonza`lez, M., Barro?n-Ceden?o, A.,
Fonollosa, J. A. R., and Marquez, L. (2013b).
The TALP-UPC approach to system selection:
Asiya features and pairwise classification using
random forests. In Proceedings of the Eighth
Workshop on Statistical Machine Translation,
pages 357?362, Sofia, Bulgaria. Association for
Computational Linguistics.
Formiga, L., Ma`rquez, L., and Pujantell, J.
(2013c). Real-life translation quality estimation
for mt system selection. In Proceedings of MT
Summit XIV (to appear), Nice, France.
Galus?c?a?kova?, P., Popel, M., and Bojar, O. (2013).
PhraseFix: Statistical post-editing of TectoMT.
In Proceedings of the Eighth Workshop on Sta-
tistical Machine Translation, pages 139?145,
Sofia, Bulgaria. Association for Computational
Linguistics.
Gispert, A., Blackwood, G., Iglesias, G., and
Byrne, W. (2013). N-gram posterior probabil-
ity confidence measures for statistical machine
translation: an empirical study. Machine Trans-
lation, 27:85?114.
Gonza`lez, M., Gime?nez, J., and Ma`rquez, L.
(2012). A graphical interface for mt evaluation
and error analysis. In Proceedings of the ACL
2012 System Demonstrations, pages 139?144,
Jeju Island, Korea.
Green, S., Cer, D., Reschke, K., Voigt, R., Bauer,
J., Wang, S., Silveira, N., Neidert, J., and Man-
ning, C. D. (2013). Feature-rich phrase-based
translation: Stanford University?s submission to
the WMT 2013 translation task. In Proceed-
ings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 146?151, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Hall, M., Frank, E., Holmes, G., Pfahringer,
B., Reutemann, P., and Witten, I. H. (2009).
The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18.
Han, A. L.-F., Wong, D. F., Chao, L. S., Lu, Y., He,
L., Wang, Y., and Zhou, J. (2013). A descrip-
tion of tunable machine translation evaluation
systems in WMT13 metrics task. In Proceed-
ings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 412?419, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Hildebrand, S. and Vogel, S. (2013). MT quality
estimation: The CMU system for WMT?13. In
Proceedings of the Eighth Workshop on Statisti-
cal Machine Translation, pages 371?377, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Hosmer, D. (1989). Applied logistic regression.
Wiley, New York, 8th edition.
Huet, S., Manishina, E., and Lefe`vre, F.
(2013). Factored machine translation systems
for Russian-English. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, pages 152?155, Sofia, Bulgaria. Associ-
ation for Computational Linguistics.
Hunt, E., Martin, J., and Stone, P. (1966). Experi-
ments in Induction. Academic Press, New York.
Kaplan, R., Riezler, S., King, T., Maxwell, J.,
Vasserman, A., and Crouch, R. (2004). Speed
and accuracy in shallow and deep stochastic
parsing. In Proceedings of the Human Lan-
guage Technology Conference and the 4th An-
nual Meeting of the North American Chapter of
the Association for Computational Linguistics
(HLT/NAACL 04).
36
Kauchak, D. and Barzilay, R. (2006). Paraphras-
ing for automatic evaluation. In Proceedings
of the main conference on Human Language
Technology Conference of the North American
Chapter of the Association of Computational
Linguistics, HLT-NAACL ?06, pages 455?462,
Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Koehn, P. (2012). Simulating human judgment in
machine translation evaluation campaigns. In
International Workshop on Spoken Language
Translation (IWSLT).
Koehn, P. and Monz, C. (2006). Manual and au-
tomatic evaluation of machine translation be-
tween European languages. In Proceedings of
NAACL 2006 Workshop on Statistical Machine
Translation, New York, New York.
Kononenko, I. (1994). Estimating attributes: anal-
ysis and extensions of RELIEF. In Proceedings
of the European conference on machine learn-
ing on Machine Learning, pages 171?182, Se-
caucus, NJ, USA. Springer-Verlag New York,
Inc.
Kos, K. and Bojar, O. (2009). Evaluation of Ma-
chine Translation Metrics for Czech as the Tar-
get Language. Prague Bulletin of Mathematical
Linguistics, 92:135?147.
Landis, J. R. and Koch, G. G. (1977). The mea-
surement of observer agreement for categorical
data. Biometrics, 33:159?174.
Langlois, D., Raybaud, S., and Sma??li, K. (2012).
Loria system for the wmt12 quality estimation
shared task. In Proceedings of the Seventh
Workshop on Statistical Machine Translation,
pages 114?119, Montre?al, Canada.
Langlois, D. and Smaili, K. (2013). LORIA sys-
tem for the WMT13 quality estimation shared
task. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, pages 378?
383, Sofia, Bulgaria. Association for Computa-
tional Linguistics.
Le, H. S., Oparin, I., Allauzen, A., Gauvain, J.-L.,
and Yvon, F. (2011). Structured output layer
neural network language model. In ICASSP,
pages 5524?5527.
Lin, C.-J., Weng, R. C., and Keerthi, S. S. (2007).
Trust region Newton methods for large-scale lo-
gistic regression. In Proceedings of the 24th
international conference on Machine learning
- ICML ?07, pages 561?568, New York, New
York, USA. ACM Press.
Luong, N. Q., Lecouteux, B., and Besacier, L.
(2013). LIG system for WMT13 QE task: In-
vestigating the usefulness of features in word
confidence estimation for MT. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 384?389, Sofia, Bulgaria.
Association for Computational Linguistics.
Macha?c?ek, M. and Bojar, O. (2013). Results of the
WMT13 metrics shared task. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 43?49, Sofia, Bulgaria. As-
sociation for Computational Linguistics.
Matusov, E. and Leusch, G. (2013). Omnifluent
English-to-French and Russian-to-English sys-
tems for the 2013 Workshop on Statistical Ma-
chine Translation. In Proceedings of the Eighth
Workshop on Statistical Machine Translation,
pages 156?161, Sofia, Bulgaria. Association for
Computational Linguistics.
McCallum, A. K. (2002). MALLET: a machine
learning for language toolkit.
Miceli Barone, A. V. and Attardi, G. (2013).
Pre-reordering for machine translation using
transition-based walks on dependency parse
trees. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, pages 162?
167, Sofia, Bulgaria. Association for Computa-
tional Linguistics.
Moreau, E. and Rubino, R. (2013). An approach
using style classification features for quality es-
timation. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
427?432, Sofia, Bulgaria. Association for Com-
putational Linguistics.
Nadejde, M., Williams, P., and Koehn, P. (2013).
Edinburgh?s syntax-based machine translation
systems. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
168?174, Sofia, Bulgaria. Association for Com-
putational Linguistics.
Okita, T., Liu, Q., and van Genabith, J. (2013).
Shallow semantically-informed PBSMT and
HPBSMT. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
175?182, Sofia, Bulgaria. Association for Com-
putational Linguistics.
O?zgu?r, A., O?zgu?r, L., and Gu?ngo?r, T. (2005). Text
37
categorization with class-based and corpus-
based keyword selection. In Proceedings of
the 20th International Conference on Computer
and Information Sciences, ISCIS?05, pages
606?615, Berlin, Heidelberg. Springer.
Peitz, S., Mansour, S., Huck, M., Freitag, M.,
Ney, H., Cho, E., Herrmann, T., Mediani,
M., Niehues, J., Waibel, A., Allauzen, A.,
Khanh Do, Q., Buschbeck, B., and Wand-
macher, T. (2013a). Joint WMT 2013 submis-
sion of the QUAERO project. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 183?190, Sofia, Bulgaria.
Association for Computational Linguistics.
Peitz, S., Mansour, S., Peter, J.-T., Schmidt, C.,
Wuebker, J., Huck, M., Freitag, M., and Ney,
H. (2013b). The RWTH aachen machine trans-
lation system for WMT 2013. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 191?197, Sofia, Bulgaria.
Association for Computational Linguistics.
Pighin, D., Formiga, L., and Ma`rquez, L.
(2012). A graph-based strategy to streamline
translation quality assessments. In Proceed-
ings of the Tenth Conference of the Associa-
tion for Machine Translation in the Americas
(AMTA?2012), San Diego, USA.
Pino, J., Waite, A., Xiao, T., de Gispert, A., Flego,
F., and Byrne, W. (2013). The University of
Cambridge Russian-English system at WMT13.
In Proceedings of the Eighth Workshop on Sta-
tistical Machine Translation, pages 198?203,
Sofia, Bulgaria. Association for Computational
Linguistics.
Post, M., Ganitkevitch, J., Orland, L., Weese, J.,
Cao, Y., and Callison-Burch, C. (2013). Joshua
5.0: Sparser, better, faster, server. In Proceed-
ings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 204?210, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Rubino, R., Toral, A., Corte?s Va??llo, S., Xie, J.,
Wu, X., Doherty, S., and Liu, Q. (2013a). The
CNGL-DCU-Prompsit translation systems for
WMT13. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
211?216, Sofia, Bulgaria. Association for Com-
putational Linguistics.
Rubino, R., Wagner, J., Foster, J., Roturier, J.,
Samad Zadeh Kaljahi, R., and Hollowood, F.
(2013b). DCU-Symantec at the WMT 2013
quality estimation shared task. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 390?395, Sofia, Bulgaria.
Association for Computational Linguistics.
Sajjad, H., Smekalova, S., Durrani, N., Fraser, A.,
and Schmid, H. (2013). QCRI-MES submis-
sion at WMT13: Using transliteration mining
to improve statistical machine translation. In
Proceedings of the Eighth Workshop on Statisti-
cal Machine Translation, pages 217?222, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Schmidt, H. (1994). Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
the International Conference on New Methods
in Natural Language Processing.
Seginer, Y. (2007). Learning Syntactic Structure.
PhD thesis, University of Amsterdam.
Shah, K., Cohn, T., and Specia, L. (2013). An In-
vestigation on the Effectiveness of Features for
Translation Quality Estimation. In Proceedings
of MT Summit XIV (to appear), Nice, France.
Singh, A. K., Wisniewski, G., and Yvon, F.
(2013). LIMSI submission for the WMT?13
quality estimation task: an experiment with n-
gram posteriors. In Proceedings of the Eighth
Workshop on Statistical Machine Translation,
pages 396?402, Sofia, Bulgaria. Association for
Computational Linguistics.
Smith, J., Saint-Amand, H., Plamada, M., Koehn,
P., Callison-Burch, C., and Lopez, A. (2013).
Dirt cheap web-scale parallel text from the
Common Crawl. In Proceedings of the 2013
Conference of the Association for Computa-
tional Linguistics (ACL 2013), Sofia, Bulgaria.
Association for Computational Linguistics.
Snover, M., Dorr, B., Schwartz, R., Micciulla, L.,
and Makhoul, J. (2006). A study of transla-
tion edit rate with targeted human annotation.
In Proceedings of the 7th Biennial Conference
of the Association for Machine Translation in
the Americas (AMTA-2006), Cambridge, Mas-
sachusetts.
Snover, M., Madnani, N., Dorr, B. J., and
Schwartz, R. (2009). Fluency, adequacy, or
hter?: exploring different human judgments
with a tunable mt metric. In Proceedings of the
Fourth Workshop on Statistical Machine Trans-
38
lation, StatMT ?09, pages 259?268, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
Soricut, R., Bach, N., and Wang, Z. (2012). The
SDL Language Weaver Systems in the WMT12
Quality Estimation Shared Task. In Proceed-
ings of the 7th Workshop on Statistical Machine
Translation, pages 145?151.
Souza, J. G. C. d., Espl-Gomis, M., Turchi, M.,
and Negri, M. (2013). Exploiting qualitative in-
formation from automatic word alignment for
cross-lingual nlp tasks. In The 51st Annual
Meeting of the Association for Computational
Linguistics - Short Papers (ACL Short Papers
2013).
Specia, L. (2011). Exploiting Objective Annota-
tions for Measuring Translation Post-editing Ef-
fort. In Proceedings of the 15th Conference of
the European Association for Machine Transla-
tion, pages 73?80, Leuven.
Specia, L., Shah, K., de Souza, J. G. C., and Cohn,
T. (2013). QuEst - A Translation Quality Esti-
mation Framework. In Proceedings of the 51th
Conference of the Association for Computa-
tional Linguistics (ACL), Demo Session, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Stolcke, A. (2002). SRILM ? An extensible lan-
guage modeling toolkit. In Proceedings of the
7th International Conference on Spoken Lan-
guage Processing (ICSLP 2002), pages 901?
904.
Stone, M. and Brooks, R. J. (1990). Contin-
uum regression: cross-validated sequentially
constructed prediction embracing ordinary least
squares, partial least squares and principal com-
ponents regression. Journal of the Royal
Statistical Society Series B Methodological,
52(2):237?269.
Stymne, S., Hardmeier, C., Tiedemann, J., and
Nivre, J. (2013). Tunable distortion limits and
corpus cleaning for SMT. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, pages 223?229, Sofia, Bulgaria. Associ-
ation for Computational Linguistics.
Tantug, A. C., Oflazer, K., and El-Kahlout, I. D.
(2008). BLEU+: a Tool for Fine-Grained BLEU
Computation. In (ELRA), E. L. R. A., edi-
tor, Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC?08),
Marrakech, Morocco.
Weller, M., Kisselew, M., Smekalova, S., Fraser,
A., Schmid, H., Durrani, N., Sajjad, H., and
Farkas, R. (2013). Munich-Edinburgh-Stuttgart
submissions at WMT13: Morphological and
syntactic processing for SMT. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 230?237, Sofia, Bulgaria.
Association for Computational Linguistics.
39
A Pairwise System Comparisons by Human Judges
Tables 21?30 show pairwise comparisons between systems for each language pair. The numbers in each
of the tables? cells indicate the percentage of times that the system in that column was judged to be better
than the system in that row, ignoring ties. Bolding indicates the winner of the two systems.
Because there were so many systems and data conditions the significance of each pairwise compar-
ison needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine
differences (rather than differences that are attributable to chance). In the following tables ? indicates sta-
tistical significance at p ? 0.10, ? indicates statistical significance at p ? 0.05, and ? indicates statistical
significance at p ? 0.01, according to the Sign Test.
Each table contains final rows showing how likely a system would win when paired against a randomly
selected system (the expected win ratio score) and the rank range according bootstrap resampling (p ?
0.05). Gray lines separate clusters based on non-overlapping rank ranges.
UE
DI
N-
HE
AF
IE
LD
ON
LI
NE
-B
M
ES
UE
DI
N
ON
LI
NE
-A
UE
DI
N-
SY
NT
AX
CU
-ZE
M
AN
CU
-TA
M
CH
YN
A
DC
U-
FD
A
JH
U
SH
EF
-W
PR
OA
UEDIN-HEAFIELD ? .50 .48? .43? .47? .43? .44? .38? .32? .25? .26?
ONLINE-B .50 ? .46? .48? .47? .49 .44? .40? .39? .29? .27?
MES .52? .54? ? .49 .47? .44? .45? .42? .41? .27? .25?
UEDIN .57? .52? .51 ? .51 .48? .47? .42? .39? .28? .25?
ONLINE-A .53? .53? .53? .49 ? .48 .51 .44? .42? .31? .30?
UEDIN-SYNTAX .57? .51 .56? .52? .52 ? .51 .43? .41? .29? .26?
CU-ZEMAN .56? .56? .55? .53? .49 .49 ? .45? .42? .32? .29?
CU-TAMCHYNA .62? .60? .58? .58? .56? .57? .55? ? .46? .35? .32?
DCU-FDA .68? .61? .59? .61? .58? .59? .58? .54? ? .32? .32?
JHU .75? .71? .73? .72? .69? .71? .68? .65? .68? ? .46?
SHEF-WPROA .74? .73? .75? .75? .70? .74? .71? .68? .68? .54? ?
score .60 .58 .57 .56 .54 .54 .53 .48 .45 .32 .29
rank 1 2-3 2-4 3-5 4-7 5-7 6-7 8 9 10 11
Table 21: Head to head comparison, ignoring ties, for Czech-English systems
CU
-B
OJ
AR
CU
-D
EP
FI
X
ON
LI
NE
-B
UE
DI
N
CU
-ZE
M
AN
M
ES
ON
LI
NE
-A
CU
-PH
RA
SE
FI
X
CU
-TE
CT
OM
T
CO
M
M
ER
CI
AL
-1
CO
M
M
ER
CI
AL
-2
SH
EF
-W
PR
OA
CU-BOJAR ? .51 .47? .44? .42? .43? .48 .41? .37? .39? .38? .33?
CU-DEPFIX .49 ? .48? .42? .43? .41? .47? .42? .40? .40? .39? .34?
ONLINE-B .53? .52? ? .47? .44? .44? .44? .44? .44? .41? .36? .34?
UEDIN .56? .58? .53? ? .47? .47? .48 .45? .44? .42? .43? .38?
CU-ZEMAN .58? .57? .56? .53? ? .49 .49 .48? .46? .47? .47? .35?
MES .57? .59? .56? .53? .51 ? .50 .47? .46? .43? .44? .42?
ONLINE-A .52 .53? .56? .52 .51 .50 ? .52 .47? .47? .47? .46?
CU-PHRASEFIX .59? .58? .56? .55? .52? .53? .48 ? .49 .48? .49 .42?
CU-TECTOMT .63? .60? .56? .56? .54? .54? .53? .51 ? .46? .46? .40?
COMMERCIAL-1 .61? .60? .59? .58? .53? .57? .53? .52? .54? ? .49 .42?
COMMERCIAL-2 .62? .61? .64? .57? .53? .56? .53? .51 .54? .51 ? .43?
SHEF-WPROA .67? .66? .66? .62? .65? .58? .54? .58? .60? .58? .57? ?
score .58 .57 .56 .52 .50 .50 .49 .48 .47 .45 .45 .38
rank 1-2 1-2 3 4 5-7 5-7 5-8 7-9 8-9 10-11 10-11 12
Table 22: Head to head comparison, ignoring ties, for English-Czech systems
40
ON
LI
NE
-B
ON
LI
NE
-A
UE
DI
N-
SY
NT
AX
UE
DI
N
QU
AE
RO
KI
T
M
ES
RW
TH
-JA
NE
M
ES
-SZ
EG
ED
-R
EO
RD
ER
-SP
LI
T
LI
M
SI
-N
CO
DE
-SO
UL
TU
BI
TA
K
UM
D
DC
U
CU
-ZE
M
AN
JH
U
SH
EF
-W
PR
OA
DE
SR
T
ONLINE-B ? .48 .44? .37? .44? .41? .42? .40? .35? .37? .32? .31? .31? .27? .23? .18? .16?
ONLINE-A .52 ? .47 .45? .47 .43? .42? .41? .44? .40? .35? .36? .34? .31? .27? .25? .21?
UEDIN-SYNTAX .56? .53 ? .48 .46? .48? .46? .46? .45? .45? .35? .35? .34? .28? .25? .20? .19?
UEDIN .63? .55? .52 ? .51 .46? .47? .49 .44? .43? .39? .34? .35? .32? .28? .24? .22?
QUAERO .56? .53 .54? .49 ? .49 .52 .44? .46? .44? .39? .38? .37? .30? .31? .25? .21?
KIT .59? .57? .52? .54? .51 ? .45? .51 .43? .46? .37? .38? .41? .35? .31? .25? .21?
MES .58? .58? .54? .53? .48 .55? ? .49 .49 .46? .44? .37? .40? .34? .30? .26? .20?
RWTH-JANE .60? .59? .54? .51 .56? .49 .51 ? .46? .50 .45? .46? .47? .38? .33? .28? .20?
MES-SZEGED-REORDER-SPLIT .65? .56? .55? .56? .54? .57? .51 .54? ? .53? .44? .41? .41? .36? .34? .31? .21?
LIMSI-NCODE-SOUL .63? .60? .55? .57? .56? .54? .54? .50 .47? ? .51 .45? .43? .37? .34? .30? .22?
TUBITAK .68? .65? .65? .61? .61? .63? .56? .55? .56? .49 ? .48? .49 .39? .41? .30? .25?
UMD .69? .64? .65? .66? .62? .62? .63? .54? .59? .55? .52? ? .48? .41? .40? .33? .27?
DCU .69? .66? .66? .65? .63? .59? .60? .53? .59? .57? .51 .52? ? .41? .38? .37? .25?
CU-ZEMAN .73? .69? .72? .68? .70? .65? .66? .62? .64? .63? .61? .59? .59? ? .44? .43? .29?
JHU .77? .73? .75? .72? .69? .69? .70? .67? .66? .66? .59? .60? .62? .56? ? .43? .30?
SHEF-WPROA .82? .75? .80? .76? .75? .75? .74? .72? .69? .70? .70? .67? .63? .57? .57? ? .41?
DESRT .84? .79? .81? .78? .79? .79? .80? .80? .79? .78? .75? .73? .75? .71? .70? .59? ?
score .66 .62 .60 .58 .58 .57 .56 .54 .53 .52 .48 .46 .46 .39 .36 .31 .23
rank 1 2-3 2-3 4-5 4-5 5-7 6-7 8-9 8-10 9-10 11 12-13 12-13 14 15 16 17
Table 23: Head to head comparison, ignoring ties, for German-English systems
ON
LI
NE
-B
PR
OM
T
UE
DI
N-
SY
NT
AX
ON
LI
NE
-A
UE
DI
N
KI
T
ST
AN
FO
RD
LI
M
SI
-N
CO
DE
-SO
UL
M
ES
-R
EO
RD
ER
JH
U
CU
-ZE
M
AN
TU
BI
TA
K
UU SH
EF
-W
PR
OA
RW
TH
-JA
NE
ONLINE-B ? .55? .50 .45? .45? .34? .37? .37? .37? .32? .32? .33? .24? .21? .26?
PROMT .45? ? .48? .50 .43? .40? .39? .36? .37? .31? .31? .32? .27? .24? .27?
UEDIN-SYNTAX .50 .52? ? .57? .45? .43? .38? .41? .39? .38? .33? .33? .26? .25? .22?
ONLINE-A .55? .50 .43? ? .51 .42? .48 .41? .36? .44? .44? .38? .32? .27? .29?
UEDIN .55? .57? .55? .49 ? .52 .45? .45? .42? .43? .37? .34? .29? .27? .31?
KIT .66? .60? .57? .58? .48 ? .48 .45? .42? .36? .39? .40? .30? .29? .26?
STANFORD .63? .61? .62? .52 .55? .52 ? .50 .44? .48 .44? .43? .34? .29? .32?
LIMSI-NCODE-SOUL .63? .64? .59? .59? .55? .55? .50 ? .44? .44? .44? .47? .40? .34? .33?
MES-REORDER .63? .63? .61? .64? .58? .58? .56? .56? ? .50 .46? .49 .38? .37? .34?
JHU .68? .69? .62? .56? .57? .64? .52 .56? .50 ? .48? .45? .36? .37? .34?
CU-ZEMAN .68? .69? .67? .56? .63? .61? .56? .56? .54? .52? ? .48 .40? .33? .34?
TUBITAK .67? .68? .67? .62? .66? .60? .57? .53? .51 .55? .52 ? .38? .40? .32?
UU .76? .73? .74? .68? .71? .70? .66? .60? .62? .64? .60? .62? ? .44? .46?
SHEF-WPROA .79? .76? .75? .73? .73? .71? .71? .66? .63? .63? .67? .60? .56? ? .47?
RWTH-JANE .74? .73? .78? .71? .69? .74? .68? .67? .66? .66? .66? .68? .54? .53? ?
score .63 .63 .61 .58 .57 .55 .52 .50 .47 .47 .46 .45 .36 .32 .32
rank 1-2 1-2 3 3-5 4-6 5-6 7 8 9-11 9-11 10-12 11-12 13 14-15 14-15
Table 24: Head to head comparison, ignoring ties, for English-German systems
41
UE
DI
N-
HE
AF
IE
LD
UE
DI
N
ON
LI
NE
-B
LI
M
SI
-N
CO
DE
-SO
UL
KI
T
ON
LI
NE
-A
M
ES
-SI
M
PL
IF
IE
DF
RE
NC
H
DC
U
RW
TH
CM
U-
TR
EE
-TO
-TR
EE
CU
-ZE
M
AN
JH
U
SH
EF
-W
PR
OA
UEDIN-HEAFIELD ? .45? .46? .46? .42? .42? .34? .34? .29? .33? .31? .28? .24?
UEDIN .55? ? .52? .43? .45? .46? .40? .38? .33? .36? .33? .32? .23?
ONLINE-B .54? .48? ? .49 .46? .44? .45? .40? .38? .34? .36? .31? .26?
LIMSI-NCODE-SOUL .54? .57? .51 ? .52? .47 .45? .42? .38? .36? .34? .31? .28?
KIT .58? .55? .54? .48? ? .47 .46? .44? .39? .38? .37? .33? .28?
ONLINE-A .58? .54? .56? .53 .53 ? .47 .45? .40? .40? .39? .34? .32?
MES-SIMPLIFIEDFRENCH .66? .60? .55? .55? .54? .53 ? .48? .44? .40? .39? .39? .32?
DCU .66? .62? .60? .58? .56? .55? .52? ? .45? .45? .42? .41? .36?
RWTH .71? .67? .62? .62? .61? .60? .56? .55? ? .48? .47? .47? .38?
CMU-TREE-TO-TREE .67? .64? .66? .64? .62? .60? .60? .55? .52? ? .50 .48 .37?
CU-ZEMAN .69? .67? .64? .66? .63? .61? .61? .58? .53? .50 ? .47? .39?
JHU .72? .68? .69? .69? .67? .66? .61? .59? .53? .52 .53? ? .45?
SHEF-WPROA .76? .77? .74? .72? .72? .68? .68? .64? .62? .63? .61? .55? ?
score .63 .60 .59 .57 .56 .54 .51 .48 .43 .42 .42 .38 .32
rank 1 2-3 2-3 4-5 4-5 5-6 7 8 9-10 9-11 10-11 12 13
Table 25: Head to head comparison, ignoring ties, for French-English systems
UE
DI
N
ON
LI
NE
-B
LI
M
SI
-N
CO
DE
-SO
UL
KI
T
PR
OM
T
ST
AN
FO
RD
M
ES
M
ES
-IN
FL
EC
TI
ON
RW
TH
-PH
RA
SE
-B
AS
ED
-JA
NE
ON
LI
NE
-A
DC
U
CU
-ZE
M
AN
JH
U
OM
NI
FL
UE
NT
IT
S-L
AT
L
IT
S-L
AT
L-P
E
UEDIN ? .49 .47? .48 .50 .44? .41? .40? .47? .39? .41? .35? .29? .30? .27? .24?
ONLINE-B .51 ? .46? .47? .47? .44? .49 .43? .43? .43? .38? .35? .36? .28? .25? .25?
LIMSI-NCODE-SOUL .53? .54? ? .45? .48 .48 .45? .43? .44? .45? .41? .32? .34? .30? .27? .27?
KIT .52 .53? .55? ? .48 .46? .45? .43? .45? .46? .38? .30? .33? .31? .29? .29?
PROMT .50 .53? .52 .52 ? .50 .48 .52? .45? .47 .48? .38? .36? .36? .34? .31?
STANFORD .56? .56? .52 .54? .50 ? .52 .48 .44? .49 .44? .39? .34? .36? .30? .29?
MES .59? .51 .55? .55? .52 .48 ? .52 .51 .45? .45? .36? .37? .34? .29? .29?
MES-INFLECTION .60? .57? .57? .57? .48? .52 .48 ? .54? .51 .46? .37? .35? .31? .33? .31?
RWTH-PHRASE-BASED-JANE .53? .57? .56? .55? .55? .56? .49 .46? ? .53 .49 .38? .36? .34? .35? .31?
ONLINE-A .61? .57? .55? .54? .53 .51 .55? .49 .47 ? .50 .45? .38? .38? .39? .35?
DCU .59? .62? .59? .62? .52? .56? .55? .54? .51 .50 ? .42? .40? .40? .36? .35?
CU-ZEMAN .65? .65? .68? .70? .62? .61? .64? .63? .62? .55? .58? ? .50 .42? .41? .37?
JHU .71? .64? .66? .67? .64? .66? .63? .65? .64? .62? .60? .50 ? .47? .42? .38?
OMNIFLUENT .70? .72? .70? .69? .64? .64? .66? .69? .66? .62? .60? .58? .53? ? .43? .42?
ITS-LATL .73? .75? .72? .71? .66? .70? .71? .67? .65? .61? .64? .59? .58? .57? ? .45?
ITS-LATL-PE .76? .75? .73? .71? .69? .71? .71? .69? .69? .65? .65? .63? .62? .58? .55? ?
score .60 .60 .58 .58 .55 .55 .54 .53 .53 .51 .49 .42 .40 .38 .35 .32
rank 1-2 1-3 2-4 3-4 5-7 5-8 5-8 6-9 7-10 9-11 10-11 12 13 14 15 16
Table 26: Head to head comparison, ignoring ties, for English-French systems
42
UE
DI
N-
HE
AF
IE
LD
ON
LI
NE
-B
UE
DI
N
ON
LI
NE
-A
M
ES
LI
M
SI
-N
CO
DE
-SO
UL
DC
U
DC
U-
OK
IT
A
DC
U-
FD
A
CU
-ZE
M
AN
JH
U
SH
EF
-W
PR
OA
UEDIN-HEAFIELD ? .49 .42? .45? .43? .40? .34? .43? .37? .34? .31? .15?
ONLINE-B .51 ? .49 .44? .46? .47? .42? .39? .40? .37? .37? .16?
UEDIN .58? .51 ? .55? .50 .47? .43? .42? .39? .39? .35? .14?
ONLINE-A .55? .56? .45? ? .50 .44? .45? .42? .42? .41? .37? .18?
MES .57? .54? .50 .50 ? .47? .45? .41? .41? .40? .38? .15?
LIMSI-NCODE-SOUL .60? .53? .53? .56? .53? ? .46? .45? .44? .43? .38? .18?
DCU .66? .58? .57? .55? .55? .54? ? .44? .47? .42? .41? .16?
DCU-OKITA .57? .61? .58? .58? .59? .55? .56? ? .49 .46? .46? .18?
DCU-FDA .63? .60? .61? .58? .59? .56? .53? .51 ? .48? .43? .18?
CU-ZEMAN .66? .63? .61? .59? .60? .57? .58? .54? .52? ? .43? .18?
JHU .69? .63? .65? .63? .62? .62? .59? .54? .57? .57? ? .22?
SHEF-WPROA .85? .84? .86? .82? .85? .82? .84? .82? .82? .82? .78? ?
score .62 .59 .57 .57 .56 .53 .51 .48 .48 .46 .42 .16
rank 1 2 3-5 3-5 3-5 6 7 8-9 8-9 10 11 12
Table 27: Head to head comparison, ignoring ties, for Spanish-English systems
ON
LI
NE
-B
ON
LI
NE
-A
UE
DI
N
PR
OM
T
M
ES
TA
LP
-U
PC
LI
M
SI
-N
CO
DE
DC
U
DC
U-
FD
A
DC
U-
OK
IT
A
CU
-ZE
M
AN
JH
U
SH
EF
-W
PR
OA
ONLINE-B ? .49 .45? .43? .38? .35? .34? .35? .37? .34? .33? .32? .23?
ONLINE-A .51 ? .49 .48 .38? .46? .42? .41? .43? .38? .38? .37? .31?
UEDIN .55? .51 ? .49 .46? .45? .43? .42? .36? .38? .38? .38? .26?
PROMT .57? .52 .51 ? .46? .48 .43? .43? .40? .37? .39? .34? .29?
MES .62? .62? .54? .54? ? .46? .44? .44? .41? .40? .43? .36? .32?
TALP-UPC .65? .54? .55? .52 .54? ? .50 .45? .44? .40? .40? .37? .32?
LIMSI-NCODE .66? .58? .57? .57? .56? .50 ? .46? .51 .48 .44? .45? .35?
DCU .65? .59? .58? .57? .56? .55? .54? ? .50 .48 .48 .45? .36?
DCU-FDA .63? .57? .64? .60? .59? .56? .49 .50 ? .53? .49 .42? .32?
DCU-OKITA .66? .62? .62? .63? .60? .60? .52 .52 .47? ? .50 .47? .36?
CU-ZEMAN .67? .62? .62? .61? .57? .60? .56? .52 .51 .50 ? .46? .40?
JHU .68? .63? .62? .66? .64? .63? .55? .55? .58? .53? .54? ? .37?
SHEF-WPROA .77? .69? .74? .71? .68? .68? .65? .64? .68? .64? .60? .63? ?
score .63 .58 .57 .56 .53 .52 .49 .47 .47 .45 .44 .41 .32
rank 1 2-4 2-4 3-4 5-6 5-6 7-8 7-9 8-10 9-11 10-11 12 13
Table 28: Head to head comparison, ignoring ties, for English-Spanish systems
43
ON
LI
NE
-B
CM
U
ON
LI
NE
-A
ON
LI
NE
-G
PR
OM
T
QC
RI
-M
ES
UC
AM
-M
UL
TI
FR
ON
TE
ND
BA
LA
GU
R
M
ES
-Q
CR
I
UE
DI
N
OM
NI
FL
UE
NT
-U
NC
NS
TR
LI
A
OM
NI
FL
UE
NT
-C
NS
TR
UM
D
CU
-K
AR
EL
CO
M
M
ER
CI
AL
-3
UE
DI
N-
SY
NT
AX
JH
U
CU
-ZE
M
AN
ONLINE-B ? .40? .42? .41? .37? .37? .41? .33? .33? .37? .33? .33? .35? .38? .34? .33? .29? .28? .14?
CMU .60? ? .50 .46? .43? .47? .42? .42? .39? .43? .41? .41? .40? .38? .36? .30? .30? .29? .17?
ONLINE-A .58? .50 ? .50 .51 .43? .47? .44? .40? .41? .43? .38? .40? .38? .38? .39? .34? .30? .19?
ONLINE-G .59? .54? .50 ? .55? .50 .51 .48 .42? .41? .44? .43? .46? .40? .44? .36? .34? .33? .19?
PROMT .63? .57? .49 .45? ? .43? .47? .43? .47? .47? .43? .39? .44? .43? .37? .41? .40? .38? .25?
QCRI-MES .63? .53? .57? .50 .57? ? .48 .46? .47? .45? .43? .45? .45? .38? .42? .37? .33? .40? .19?
UCAM-MULTIFRONTEND .59? .58? .53? .49 .53? .52 ? .47? .48 .46? .46? .42? .45? .46? .45? .40? .39? .33? .17?
BALAGUR .67? .58? .56? .52 .57? .54? .53? ? .47? .49 .45? .53? .40? .44? .44? .41? .36? .33? .23?
MES-QCRI .67? .61? .60? .58? .53? .53? .52 .53? ? .49 .47? .47? .43? .43? .44? .38? .42? .39? .17?
UEDIN .63? .57? .59? .59? .53? .55? .54? .51 .51 ? .48 .52 .44? .52 .49 .42? .43? .35? .21?
OMNIFLUENT-UNCNSTR .67? .59? .57? .56? .57? .57? .54? .55? .53? .52 ? .51 .46? .48 .48 .44? .40? .39? .25?
LIA .67? .59? .62? .57? .61? .55? .58? .47? .53? .48 .49 ? .51 .49 .48 .50 .41? .39? .20?
OMNIFLUENT-CNSTR .65? .60? .60? .54? .56? .55? .55? .60? .57? .56? .54? .49 ? .51 .48 .47? .40? .40? .25?
UMD .62? .62? .62? .60? .57? .62? .54? .56? .57? .48 .52 .51 .49 ? .53? .42? .46? .42? .19?
CU-KAREL .66? .64? .62? .56? .63? .58? .55? .56? .56? .51 .52 .52 .52 .47? ? .44? .40? .47? .24?
COMMERCIAL-3 .67? .70? .61? .64? .59? .63? .60? .59? .62? .58? .56? .50 .53? .58? .56? ? .51 .44? .32?
UEDIN-SYNTAX .71? .70? .66? .66? .60? .67? .61? .64? .58? .57? .60? .59? .60? .54? .60? .49 ? .45? .25?
JHU .72? .71? .70? .67? .62? .60? .67? .67? .61? .65? .61? .61? .60? .58? .53? .56? .55? ? .24?
CU-ZEMAN .86? .83? .81? .81? .75? .81? .83? .77? .83? .79? .75? .80? .75? .81? .76? .68? .75? .76? ?
score .65 .60 .58 .56 .56 .55 .54 .52 .51 .50 .49 .49 .48 .48 .47 .43 .41 .39 .21
rank 1 2-3 2-3 4-6 4-6 5-7 5-7 8-9 8-10 9-11 10-12 11-14 12-15 12-15 13-15 16 17 18 19
Table 29: Head to head comparison, ignoring ties, for Russian-English systems
PR
OM
T
ON
LI
NE
-B
CM
U
ON
LI
NE
-G
ON
LI
NE
-A
UE
DI
N
QC
RI
-M
ES
CU
-K
AR
EL
M
ES
-Q
CR
I
JH
U
CO
M
M
ER
CI
AL
-3
LI
A
BA
LA
GU
R
CU
-ZE
M
AN
PROMT ? .44? .39? .47 .46? .36? .37? .37? .32? .35? .28? .30? .32? .24?
ONLINE-B .56? ? .44? .41? .44? .38? .37? .35? .33? .39? .33? .31? .35? .24?
CMU .61? .56? ? .52 .49 .47? .43? .41? .39? .44? .44? .40? .35? .28?
ONLINE-G .53 .59? .48 ? .48 .50 .48 .46 .46? .42? .38? .43? .38? .36?
ONLINE-A .54? .56? .51 .52 ? .47 .49 .49 .48 .44? .38? .40? .40? .34?
UEDIN .64? .62? .53? .50 .53 ? .49 .46? .42? .39? .44? .41? .38? .29?
QCRI-MES .63? .63? .57? .52 .51 .51 ? .48 .45? .44? .42? .39? .40? .29?
CU-KAREL .63? .65? .59? .54 .51 .54? .52 ? .50 .46? .43? .40? .42? .34?
MES-QCRI .68? .67? .61? .54? .52 .58? .55? .50 ? .48? .47? .43? .45? .34?
JHU .65? .61? .56? .58? .56? .61? .56? .54? .52? ? .51 .44? .44? .33?
COMMERCIAL-3 .72? .67? .56? .62? .62? .56? .58? .57? .53? .49 ? .52 .48 .44?
LIA .70? .69? .60? .57? .60? .59? .61? .60? .57? .56? .48 ? .47? .41?
BALAGUR .68? .65? .65? .62? .60? .62? .60? .58? .55? .56? .52 .53? ? .41?
CU-ZEMAN .76? .76? .72? .64? .66? .71? .71? .66? .66? .67? .56? .59? .59? ?
score .64 .62 .55 .54 .53 .53 .52 .49 .47 .46 .43 .42 .41 .33
rank 1 2 3-4 3-6 3-7 4-7 5-7 8 9-10 9-10 11-12 11-13 12-13 14
Table 30: Head to head comparison, ignoring ties, for English-Russian systems
44
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 45?51,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Results of the WMT13 Metrics Shared Task
Matous? Macha?c?ek and Ondr?ej Bojar
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
machacekmatous@gmail.com and bojar@ufal.mff.cuni.cz
Abstract
This paper presents the results of the
WMT13 Metrics Shared Task. We asked
participants of this task to score the
outputs of the MT systems involved in
WMT13 Shared Translation Task. We
collected scores of 16 metrics from 8 re-
search groups. In addition to that we com-
puted scores of 5 standard metrics such as
BLEU, WER, PER as baselines. Collected
scores were evaluated in terms of system
level correlation (how well each metric?s
scores correlate with WMT13 official hu-
man scores) and in terms of segment level
correlation (how often a metric agrees with
humans in comparing two translations of a
particular sentence).
1 Introduction
Automatic machine translation metrics play a very
important role in the development of MT systems
and their evaluation. There are many different
metrics of diverse nature and one would like to
assess their quality. For this reason, the Metrics
Shared Task is held annually at the Workshop of
Statistical Machine Translation (Callison-Burch et
al., 2012). This year, the Metrics Task was run
by different organizers but the only visible change
is hopefully that the results of the task are pre-
sented in a separate paper instead of the main
WMT overview paper.
In this task, we asked metrics developers to
score the outputs of WMT13 Shared Translation
Task (Bojar et al, 2013). We have collected the
computed metrics? scores and use them to evalu-
ate quality of the metrics.
The systems? outputs, human judgements and
evaluated metrics are described in Section 2. The
quality of the metrics in terms of system level cor-
relation is reported in Section 3. Segment level
correlation is reported in Section 4.
2 Data
We used the translations of MT systems involved
in WMT13 Shared Translation Task together with
reference translations as the test set for the Metrics
Task. This dataset consists of 135 systems? out-
puts and 6 reference translations in 10 translation
directions (5 into English and 5 out of English).
Each system?s output and the reference translation
contain 3000 sentences. For more details please
see the WMT13 main overview paper (Bojar et al,
2013).
2.1 Manual MT Quality Judgements
During the WMT13 Translation Task a large scale
manual annotation was conducted to compare the
systems. We used these collected human judge-
ments for evaluating the automatic metrics.
The participants in the manual annotation were
asked to evaluate system outputs by ranking trans-
lated sentences relative to each other. For each
source segment that was included in the procedure,
the annotator was shown the outputs of five sys-
tems to which he or she was supposed to assign
ranks. Ties were allowed. Only sentences with 30
or less words were ranked by humans.
These collected rank labels were then used to
assign each system a score that reflects how high
that system was usually ranked by the annotators.
Please see the WMT13 main overview paper for
details on how this score is computed. You can
also find inter- and intra-annotator agreement esti-
mates there.
2.2 Participants of the Shared Task
Table 1 lists the participants of WMT13 Shared
Metrics Task, along with their metrics. We have
collected 16 metrics from a total of 8 research
groups.
In addition to that we have computed the fol-
lowing two groups of standard metrics as base-
lines:
45
Metrics Participant
METEOR Carnegie Mellon University (Denkowski and Lavie, 2011)
LEPOR, NLEPOR University of Macau (Han et al, 2013)
ACTA, ACTA5+6 Idiap Research Institute (Hajlaoui, 2013) (Hajlaoui and Popescu-Belis, 2013)
DEPREF-{ALIGN,EXACT} Dublin City University (Wu et al, 2013)
SIMPBLEU-{RECALL,PREC} University of Shefield (Song et al, 2013)
MEANT, UMEANT Hong Kong University of Science and Technology (Lo and Wu, 2013)
TERRORCAT German Research Center for Artificial Intelligence (Fishel, 2013)
LOGREGFSS, LOGREGNORM DFKI (Avramidis and Popovic?, 2013)
Table 1: Participants of WMT13 Metrics Shared Task
? Moses Scorer. Metrics BLEU (Papineni et
al., 2002), TER (Snover et al, 2006), WER,
PER and CDER (Leusch et al, 2006) were
computed using the Moses scorer which is
used in Moses model optimization. To tok-
enize the sentences we used the standard tok-
enizer script as available in Moses Toolkit. In
this paper we use the suffix *-MOSES to label
these metrics.
? Mteval. Metrics BLEU (Papineni et
al., 2002) and NIST (Doddington,
2002) were computed using the script
mteval-v13a.pl 1 which is used in
OpenMT Evaluation Campaign and includes
its own tokenization. We use *-MTEVAL
suffix to label these metrics. By default,
mteval assumes the text is in ASCII,
causing poor tokenization around curly
quotes. We run mteval in both the
default setting as well as with the flag
--international-tokenization
(marked *-INTL).
We have normalized all metrics? scores such
that better translations get higher scores.
3 System-Level Metric Analysis
We measured the quality of system-level metrics?
scores using the Spearman?s rank correlation coef-
ficient ?. For each direction of translation we con-
verted the official human scores into ranks. For
each metric, we converted the metric?s scores of
systems in a given direction into ranks. Since there
were no ties in the rankings, we used the simplified
formula to compute the Spearman?s ?:
? = 1? 6
?
d2i
n(n2 ? 1) (1)
1http://www.itl.nist.gov/iad/mig/
/tools/
where di is the difference between the human rank
and metric?s rank for system i and n is number of
systems. The possible values of ? range between
1 (where all systems are ranked in the same order)
and -1 (where the systems are ranked in the re-
verse order). A good metric produces rankings of
systems similar to human rankings. Since we have
normalized all metrics such that better translations
get higher score we consider metrics with values
of Spearman?s ? closer to 1 as better.
We also computed empirical confidences of
Spearman?s ? using bootstrap resampling. Since
we did not have direct access to participants? met-
rics (we received only metrics? scores for the com-
plete test sets without the ability to run them on
new sampled test sets), we varied the ?golden
truth? by sampling from human judgments. We
have bootstrapped 1000 new sets and used 95 %
confidence level to compute confidence intervals.
The Spearman?s ? correlation coefficient is
sometimes too harsh: If a metric disagrees with
humans in ranking two systems of a very similar
quality, the ? coefficient penalizes this equally as
if the systems were very distant in their quality.
Aware of how uncertain the golden ranks are in
general, we do not find the method very fair. We
thus also computed three following correlation co-
efficients besides the Spearman?s ?:
? Pearson?s correlation coefficient. This co-
efficient measures the strength of the linear
relationship between metric?s scores and hu-
man scores. In fact, Spearman?s ? is Pear-
son?s correlation coefficient applied to ranks.
? Correlation with systems? clusters. In the
Translation Task (Bojar et al, 2013), the
manual scores are also presented as clus-
ters of systems that can no longer be signifi-
cantly distinguished from one another given
the available judgements. (Please see the
WMT13 Overview paper for more details).
46
We take this cluster information as a ?rank
with ties? for each system and calculate its
Pearson?s correlation coefficient with each
metric?s scores.
? Correlation with systems? fuzzy ranks. For
a given system the fuzzy rank is computed
as an average of ranks of all systems which
are not significantly better or worse than the
given system. The Pearson?s correlation co-
efficient of a metric?s scores and systems?
fuzzy ranks is then computed.
You can find the system-level correlations for
translations into English in Table 2 and for transla-
tions out of English in Table 3. Each row in the ta-
bles contains correlations of a metric in each of the
examined translation directions. The metrics are
sorted by average Spearman?s ? correlation across
translation directions. The best results in each di-
rection are in bold.
As in previous years, a lot of metrics outper-
formed BLEU in system level correlation. The
metric which has on average the strongest corre-
lation in directions into English is METEOR. For
the out of English direction, SIMPBLEU-RECALL
has the highest system-level correlation. TER-
RORCAT achieved even a higher average corre-
lation but it did not participate in all language
pairs. The implementation of BLEU in mteval
is slightly better than the one in Moses scorer
(BLEU-MOSES). This confirms the known truth
that tokenization and other minor implementation
details can considerably influence a metric perfor-
mance.
4 Segment-Level Metric Analysis
We measured the quality of metrics? segment-
level scores using Kendall?s ? rank correlation
coefficient. For this we did not use the official
WMT13 human scores but we worked with raw
human judgements: For each translation direction
we extracted all pairwise comparisons where one
system?s translation of a particular segment was
judged to be (strictly) better than the other sys-
tem?s translation. Formally, this is a list of pairs
(a, b) where a segment translation a was ranked
better than translation b:
Pairs := {(a, b) | r(a) < r(b)} (2)
where r(?) is human rank. For a given metricm(?),
we then counted all concordant pairwise compar-
isons and all discordant pairwise comparisons. A
concordant pair is a pair of two translations of
the same segment in which the comparison of hu-
man ranks agree with the comparison of the met-
ric?s scores. A discordant pair is a pair in which
the comparison of human ranks disagrees with the
metric?s comparison. Note that we totally ignore
pairs where human ranks or metric?s scores are
tied. Formally:
Con := {(a, b) ? Pairs | m(a) > m(b)} (3)
Dis := {(a, b) ? Pairs | m(a) < m(b)} (4)
Finally the Kendall?s ? is computed using the fol-
lowing formula:
? = |Con| ? |Dis||Con|+ |Dis| (5)
The possible values of ? range between -1 (a met-
ric always predicted a different order than humans
did) and 1 (a metric always predicted the same or-
der as humans). Metrics with higher ? are better.
The final Kendall?s ?s are shown in Table 4
for directions into English and in Table 5 for di-
rections out of English. Each row in the tables
contains correlations of a metric in given direc-
tions. The metrics are sorted by average corre-
lation across the translation directions. Metrics
which did not compute scores for systems in all
directions are at the bottom of the tables.
You can see that in both categories, into and out
of English, the strongest correlated segment-level
metric is SIMPBLEU-RECALL.
4.1 Details on Kendall?s ?
The computation of Kendall?s ? has slightly
changed this year. In WMT12 Metrics Task
(Callison-Burch et al, 2012), the concordant pairs
were defined exactly as we do (Equation 3) but the
discordant pairs were defined differently: pairs in
which one system was ranked better by the human
annotator but in which the metric predicted a tie
were considered also as discordant:
Dis := {(a, b) ? Pairs | m(a) ? m(b)} (6)
We feel that for two translations a and b of a seg-
ment, where a is ranked better by humans, a metric
which produces equal scores for both translations
should not be penalized as much as a metric which
47
Co
rre
lat
ion
coe
ffic
ien
t
Sp
ear
ma
n?s
?
Co
rre
lat
ion
Co
effi
cie
nt
Pe
ars
on
?s
Cl
ust
ers
Fu
zzy
Ra
nk
s
Di
rec
tio
ns
fr-
en
de
-en
es-
en
cs-
en
ru
-en
Av
era
ge
Av
era
ge
Av
era
ge
Av
era
ge
Co
nsi
de
red
sys
tem
s
12
22
11
10
17
M
ET
EO
R
.9
84
?
.0
14
.9
61
?
.0
20
.97
9?
.0
24
.9
64
?
.0
27
.7
89
?
.0
40
.93
5?
.0
12
.95
0
.92
4
.93
6
DE
PR
EF
-A
LI
GN
.99
5?
.0
11
.96
6?
.0
18
.9
65
?
.0
31
.9
64
?
.0
23
.7
68
?
.0
41
.9
31
?
.0
12
.92
6
.90
9
.92
4
UM
EA
NT
.9
89
?
.0
11
.9
46
?
.0
18
.9
58
?
.0
28
.97
3?
.0
32
.7
75
?
.0
37
.9
28
?
.0
12
.90
9
.90
3
o.
93
0
M
EA
NT
.9
73
?
.0
14
.9
26
?
.0
21
.9
44
?
.0
38
.97
3?
.0
32
.7
65
?
.0
38
.9
16
?
.0
13
.90
1
.89
1
.91
8
SE
M
PO
S
.9
38
?
.0
14
.9
19
?
.0
28
.9
30
?
.0
31
.9
55
?
.0
18
.82
3?
.0
37
.9
13
?
.0
12
o.
93
4
o.
89
4
.90
1
DE
PR
EF
-EX
AC
T
.9
84
?
.0
11
.9
61
?
.0
17
.9
37
?
.0
38
.9
36
?
.0
27
.7
44
?
.0
46
.9
12
?
.0
15
o.
92
4
o.
89
2
.90
1
SIM
PB
LE
U-
RE
CA
LL
.9
78
?
.0
14
.9
36
?
.0
20
.9
23
?
.0
52
.9
09
?
.0
27
.7
98
?
.0
43
.9
09
?
.0
17
o.
92
3
.87
4
.88
6
BL
EU
-M
TE
VA
L-I
NT
L
.9
89
?
.0
14
.9
02
?
.0
17
.8
95
?
.0
49
.9
36
?
.0
32
.6
95
?
.0
42
.8
83
?
.0
15
.86
6
.84
3
.87
4
BL
EU
-M
TE
VA
L
.9
89
?
.0
14
.8
95
?
.0
20
.8
88
?
.0
45
.9
36
?
.0
32
.6
70
?
.0
41
.8
76
?
.0
15
.85
4
.83
5
.86
5
BL
EU
-M
OS
ES
.9
93
?
.0
14
.9
02
?
.0
17
.8
79
?
.0
51
.9
36
?
.0
36
.6
51
?
.0
41
.8
72
?
.0
16
o.
85
6
.82
6
.86
1
CD
ER
-M
OS
ES
.99
5?
.0
14
.8
77
?
.0
17
.8
88
?
.0
49
.9
27
?
.0
36
.6
59
?
.0
45
.8
69
?
.0
17
o.
87
7
o.
83
1
.85
9
SIM
PB
LE
U-
PR
EC
.9
89
?
.0
08
.8
46
?
.0
20
.8
32
?
.0
59
.9
18
?
.0
23
.7
04
?
.0
42
.8
58
?
.0
17
o.
87
1
.81
5
.84
7
NL
EP
OR
.9
45
?
.0
22
.9
49
?
.0
25
.8
25
?
.0
56
.8
45
?
.0
41
.7
05
?
.0
43
.8
54
?
.0
18
o.
86
7
.80
4
o.
85
3
LE
PO
R
V3
.10
0
.9
45
?
.0
19
.9
34
?
.0
27
.7
48
?
.0
77
.8
00
?
.0
36
.7
79
?
.0
41
.8
41
?
.0
20
o.
86
9
.78
0
o.
85
0
NI
ST
-M
TE
VA
L
.9
51
?
.0
19
.8
75
?
.0
22
.7
69
?
.0
77
.8
91
?
.0
27
.6
49
?
.0
45
.8
27
?
.0
20
.85
2
.77
4
.82
4
NI
ST
-M
TE
VA
L-I
NT
L
.9
51
?
.0
19
.8
75
?
.0
22
.7
62
?
.0
77
.8
82
?
.0
32
.6
58
?
.0
45
.8
26
?
.0
21
o.
85
6
.77
4
o.
82
6
TE
R-
M
OS
ES
.9
51
?
.0
19
.8
33
?
.0
23
.8
25
?
.0
77
.8
00
?
.0
36
.5
81
?
.0
45
.7
98
?
.0
21
.80
3
.73
3
.79
7
W
ER
-M
OS
ES
.9
51
?
.0
19
.6
72
?
.0
26
.7
97
?
.0
70
.7
55
?
.0
41
.5
91
?
.0
42
.7
53
?
.0
20
.78
5
.68
2
.74
9
PE
R-
M
OS
ES
.8
52
?
.0
27
.8
58
?
.0
25
.3
57
?
.0
91
.6
97
?
.0
43
.6
77
?
.0
40
.6
88
?
.0
24
.75
7
.63
7
.70
6
TE
RR
OR
CA
T
.9
84
?
.0
11
.9
61
?
.0
23
.9
72
?
.0
28
n/a
n/a
.97
2?
.0
12
.97
7
.95
8
.95
9
Ta
ble
2:
Sy
ste
m-
lev
el
co
rre
lat
ion
so
fa
uto
ma
tic
eva
lua
tio
nm
etr
ics
an
dt
he
offi
cia
lW
MT
hu
ma
ns
co
res
wh
en
tra
nsl
ati
ng
int
oE
ng
lis
h.
Th
es
ym
bo
l?
o?
ind
ica
tes
wh
ere
the
oth
er
ave
rag
es
are
ou
to
fs
eq
ue
nc
ec
om
pa
red
to
the
ma
in
Sp
ear
ma
n?s
?
ave
rag
e.
48
Co
rre
lat
ion
coe
ffic
ien
t
Sp
ear
ma
n?s
?
Co
rre
lat
ion
Co
effi
cie
nt
Pe
ars
on
?s
Cl
ust
ers
Fu
zzy
Ra
nk
s
Di
rec
tio
ns
en
-fr
en
-de
en
-es
en
-cs
en
-ru
Av
era
ge
Av
era
ge
Av
era
ge
Av
era
ge
Co
nsi
de
red
sys
tem
s
14
14
12
11
12
SIM
PB
LE
U-
RE
CA
LL
.9
24
?
.0
22
.92
5?
.0
20
.8
30
?
.0
47
.8
67
?
.0
31
.7
10
?
.0
53
.85
1?
.0
18
.84
4
.85
6
.84
9
LE
PO
R
V3
.10
0
.9
04
?
.0
34
.9
00
?
.0
27
.8
41
?
.0
49
.7
48
?
.0
56
.85
5?
.0
48
.8
50
?
.0
20
o.8
54
.83
3
.84
4
NI
ST
-M
TE
VA
L-I
NT
L
.92
9?
.0
32
.8
46
?
.0
29
.7
97
?
.0
60
.9
02
?
.0
45
.7
71
?
.0
48
.8
49
?
.0
20
.80
8
o.8
63
o.
84
5
CD
ER
-M
OS
ES
.9
21
?
.0
29
.8
67
?
.0
29
.85
7?
.0
58
.8
88
?
.0
24
.7
01
?
.0
59
.8
47
?
.0
19
.79
6
o.
86
1
.84
3
NL
EP
OR
.9
19
?
.0
28
.9
04
?
.0
27
.8
52
?
.0
49
.8
18
?
.0
45
.7
27
?
.0
64
.8
44
?
.0
21
o.
84
9
o.
84
6
.84
0
NI
ST
-M
TE
VA
L
.9
14
?
.0
34
.8
25
?
.0
30
.7
80
?
.0
66
.9
16
?
.0
31
.7
23
?
.0
48
.8
32
?
.0
21
.79
4
o.
85
1
.82
8
SIM
PB
LE
U-
PR
EC
.9
09
?
.0
26
.8
79
?
.0
25
.7
80
?
.0
71
.8
81
?
.0
35
.6
97
?
.0
51
.8
29
?
.0
20
o.
84
0
o.
85
2
.82
7
M
ET
EO
R
.9
24
?
.0
27
.8
79
?
.0
30
.7
80
?
.0
60
.93
7?
.0
24
.5
69
?
.0
66
.8
18
?
.0
22
o.
80
6
.82
5
.81
4
BL
EU
-M
TE
VA
L-I
NT
L
.9
17
?
.0
33
.8
32
?
.0
30
.7
64
?
.0
71
.8
95
?
.0
28
.6
57
?
.0
62
.8
13
?
.0
22
o.
80
2
.82
1
.80
8
BL
EU
-M
TE
VA
L
.8
95
?
.0
37
.7
86
?
.0
34
.7
64
?
.0
71
.8
95
?
.0
28
.6
31
?
.0
53
.7
94
?
.0
22
o.
79
9
.80
9
.79
0
TE
R-
M
OS
ES
.9
12
?
.0
38
.8
54
?
.0
32
.7
53
?
.0
66
.8
60
?
.0
59
.5
38
?
.0
68
.7
83
?
.0
23
.74
6
.80
6
.77
8
BL
EU
-M
OS
ES
.8
97
?
.0
34
.7
86
?
.0
34
.7
59
?
.0
78
.8
95
?
.0
28
.5
74
?
.0
57
.7
82
?
.0
22
o.
80
2
.79
2
o.
77
9
W
ER
-M
OS
ES
.9
14
?
.0
34
.8
25
?
.0
34
.7
14
?
.0
77
.8
60
?
.0
56
.5
52
?
.0
66
.7
73
?
.0
24
.73
7
o.
79
6
.76
6
PE
R-
M
OS
ES
.8
73
?
.0
40
.6
86
?
.0
45
.7
75
?
.0
47
.7
97
?
.0
49
.5
91
?
.0
62
.7
44
?
.0
24
o.
75
8
.74
7
.73
9
TE
RR
OR
CA
T
.92
9?
.0
22
.94
6?
.0
18
.91
2?
.0
41
n/a
n/a
.92
9?
.0
17
.95
2
.93
3
.92
3
SE
M
PO
S
n/a
n/a
n/a
.6
99
?
.0
45
n/a
.6
99
?
.0
45
.71
7
.61
5
.69
6
AC
TA
5?
6
.8
09
?
.0
46
-.5
26
?
.0
34
n/a
n/a
n/a
.1
41
?
.0
29
.16
6
.19
6
.17
6
AC
TA
.8
09
?
.0
46
-.5
26
?
.0
34
n/a
n/a
n/a
.1
41
?
.0
29
.16
6
.19
6
.17
6
Ta
ble
3:
Sy
ste
m-
lev
el
co
rre
lat
ion
so
fa
uto
ma
tic
eva
lua
tio
nm
etr
ics
an
dt
he
offi
cia
lW
MT
hu
ma
ns
co
res
wh
en
tra
nsl
ati
ng
ou
to
fE
ng
lis
h.
Th
es
ym
bo
l?
o?
ind
ica
tes
wh
ere
the
oth
er
ave
rag
es
are
ou
to
fs
eq
ue
nc
ec
om
pa
red
to
the
ma
in
Sp
ear
ma
n?s
?
ave
rag
e.
49
Directions fr-en de-en es-en cs-en ru-en Average
Extracted pairs 80741 128668 67832 85469 151422
SIMPBLEU-RECALL .193 .318 .279 .260 .234 .257
METEOR .178 .293 .236 .265 .239 .242
DEPREF-ALIGN .161 .267 .234 .228 .200 .218
DEPREF-EXACT .167 .263 .228 .227 .195 .216
SIMPBLEU-PREC .154 .236 .214 .208 .174 .197
NLEPOR .149 .240 .204 .176 .172 .188
SENTBLEU-MOSES .150 .218 .198 .197 .170 .187
LEPOR V3.100 .149 .221 .161 .187 .177 .179
UMEANT .101 .166 .144 .160 .108 .136
MEANT .101 .160 .145 .164 .109 .136
LOGREGFSS-33 n/a .272 n/a n/a n/a .272
LOGREGFSS-24 n/a .270 n/a n/a n/a .270
TERRORCAT .161 .298 .230 n/a n/a .230
Table 4: Segment-level Kendall?s ? correlations of automatic evaluation metrics and the official WMT
human judgements when translating into English.
Directions en-fr en-de en-es en-cs en-ru Average
Extracted pairs 100783 77286 60464 102842 87323
SIMPBLEU-RECALL .158 .085 .231 .065 .126 .133
SIMPBLEU-PREC .138 .065 .187 .055 .095 .108
METEOR .147 .049 .175 .058 .111 .108
SENTBLEU-MOSES .133 .047 .171 .052 .095 .100
LEPOR V3.100 .126 .058 .178 .023 .109 .099
NLEPOR .124 .048 .163 .048 .097 .096
LOGREGNORM-411 n/a n/a .136 n/a n/a .136
TERRORCAT .116 .074 .186 n/a n/a .125
LOGREGNORMSOFT-431 n/a n/a .033 n/a n/a .033
Table 5: Segment-level Kendall?s ? correlations of automatic evaluation metrics and the official WMT
human judgements when translating out of English.
50
strongly disagrees with humans. The method we
used this year does not harm metrics which often
estimate two segments as equally good.
5 Conclusion
We carried out WMT13 Metrics Shared Task in
which we assessed the quality of various au-
tomatic machine translation metrics. We used
the human judgements as collected for WMT13
Translation Task to compute system-level and
segment-level correlations with human scores.
While most of the metrics correlate very well
on the system-level, the segment-level correlations
are still rather poor. It was shown again this year
that a lot of metrics outperform BLEU, hopefully
one of them will attract a wider use at last.
Acknowledgements
This work was supported by the grants
P406/11/1499 of the Grant Agency of the
Czech Republic and FP7-ICT-2011-7-288487
(MosesCore) of the European Union.
References
Eleftherios Avramidis and Maja Popovic?. 2013. Ma-
chine learning methods for comparative and time-
oriented Quality Estimation of Machine Translation
output. In Proceedings of the Eight Workshop on
Statistical Machine Translation.
Ondr?ej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 workshop
on statistical machine translation. In Proceedings of
the Eight Workshop on Statistical Machine Transla-
tion.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montre?al, Canada, June. Association for
Computational Linguistics.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization
and Evaluation of Machine Translation Systems. In
Proceedings of the EMNLP 2011 Workshop on Sta-
tistical Machine Translation.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the sec-
ond international conference on Human Language
Technology Research, HLT ?02, pages 138?145, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Mark Fishel. 2013. Ranking Translations using Error
Analysis and Quality Estimation. In Proceedings of
the Eight Workshop on Statistical Machine Transla-
tion.
Najeh Hajlaoui and Andrei Popescu-Belis. 2013. As-
sessing the accuracy of discourse connective transla-
tions: Validation of an automatic metric. In 14th In-
ternational Conference on Intelligent Text Process-
ing and Computational Linguistics, page 12. Uni-
versity of the Aegean, Springer, March.
Najeh Hajlaoui. 2013. Are ACT?s scores increasing
with better translation quality. In Proceedings of the
Eight Workshop on Statistical Machine Translation.
Aaron Li-Feng Han, Derek F. Wong, Lidia S. Chao,
Yi Lu, Liangye He, Yiming Wang, and Jiaji Zhou.
2013. A Description of Tunable Machine Transla-
tion Evaluation Systems in WMT13 Metrics Task.
In Proceedings of the Eight Workshop on Statistical
Machine Translation.
Gregor Leusch, Nicola Ueffing, and Hermann Ney.
2006. Cder: Efficient mt evaluation using block
movements. In In Proceedings of EACL, pages 241?
248.
Chi-Kiu Lo and Dekai Wu. 2013. MEANT @
WMT2013 metrics evaluation. In Proceedings of
the Eight Workshop on Statistical Machine Transla-
tion.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. pages 311?318.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In In Proceedings of Association for Machine
Translation in the Americas, pages 223?231.
Xingyi Song, Trevor Cohn, and Lucia Specia. 2013.
BLEU deconstructed: Designing a better MT evalu-
ation metric. March.
Xiaofeng Wu, Hui Yu, and Qun Liu. 2013. DCU Par-
ticipation in WMT2013 Metrics Task. In Proceed-
ings of the Eight Workshop on Statistical Machine
Translation.
51
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 92?98,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Chimera ? Three Heads for English-to-Czech Translation
Ondr?ej Bojar and Rudolf Rosa and Ales? Tamchyna
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostranske? na?me?st?? 25, Prague, Czech Republic
surname@ufal.mff.cuni.cz
Abstract
This paper describes our WMT submis-
sions CU-BOJAR and CU-DEPFIX, the lat-
ter dubbed ?CHIMERA? because it com-
bines on three diverse approaches: Tec-
toMT, a system with transfer at the deep
syntactic level of representation, factored
phrase-based translation using Moses, and
finally automatic rule-based correction of
frequent grammatical and meaning errors.
We do not use any off-the-shelf system-
combination method.
1 Introduction
Targeting Czech in statistical machine transla-
tion (SMT) is notoriously difficult due to the
large number of possible word forms and com-
plex agreement rules. Previous attempts to resolve
these issues include specific probabilistic models
(Subotin, 2011) or leaving the morphological gen-
eration to a separate processing step (Fraser et al,
2012; Marec?ek et al, 2011).
TectoMT (CU-TECTOMT, Galus?c?a?kova? et al
(2013)) is a hybrid (rule-based and statistical) MT
system that closely follows the analysis-transfer-
synthesis pipeline. As such, it suffers from many
issues but generating word forms in proper agree-
ments with their neighbourhood as well as the
translation of some diverging syntactic structures
are handled well. Overall, TectoMT sometimes
even ties with a highly tuned Moses configuration
in manual evaluations, see Bojar et al (2011).
Finally, Rosa et al (2012) describes Depfix, a
rule-based system for post-processing (S)MT out-
put that corrects some morphological, syntactic
and even semantic mistakes. Depfix was able to
significantly improve Google output in WMT12,
so now we applied it on an open-source system.
Our WMT13 system is thus a three-headed
creature where, hopefully: (1) TectoMT provides
missing word forms and safely handles some non-
parallel syntactic constructions, (2) Moses ex-
ploits very large parallel and monolingual data,
and boosts better lexical choice, (3) Depfix at-
tempts to fix severe flaws in Moses output.
2 System Description
TectoMT
Moses
cu-tectomt
Depfix
cu-bojar
cu-depfix = Chimera
Input
Figure 1: CHIMERA: three systems combined.
CHIMERA is a sequential combination of three
diverse MT systems as depicted in Figure 1. Each
of the intermediate stages of processing has been
submitted as a separate primary system for the
WMT manual evalution, allowing for a more thor-
ough analysis.
Instead of an off-the-shelf system combination
technique, we use TectoMT output as synthetic
training data for Moses as described in Section 2.1
and finally we process its output using rule-based
corrections of Depfix (Section 2.2). All steps di-
rectly use the source sentence.
2.1 Moses Setup for CU-BOJAR
We ran a couple of probes with reduced training
data around the setup of Moses that proved suc-
cessful in previous years (Bojar et al, 2012a).
2.1.1 Pre-processing
We use a stable pre-processing pipeline that in-
cludes normalization of quotation marks,1 tok-
enization, tagging and lemmatization with tools
1We do not simply convert them to unpaired ASCII quotes
but rather balance them and use other heuristics to convert
most cases to the typographically correct form.
92
Case recaser lc?form utc stc
BLEU 9.05 9.13 9.70 9.81
Table 1: Letter Casing
included in the Treex platform (Popel and
Z?abokrtsky?, 2010).
This year, we evaluated the end-to-end effect of
truecasing. Ideally, English-Czech SMT should be
trained on data where only names are uppercased
(and neither the beginnings of sentences, nor all-
caps headlines or exclamations etc). For these ex-
periments, we trained a simple baseline system on
1 million sentence pairs from CzEng 1.0.
Table 1 summarizes the final (case-sensitive!)
BLEU scores for four setups. The standard ap-
proach is to train SMT lowercase and apply a re-
caser, e.g. the Moses one, on the output. Another
option (denoted ?lc?form?) is to lowercase only
the source side of the parallel data. This more
or less makes the translation model responsible
for identifying names and the language model for
identifying beginnings of sentences.
The final two approaches attempt at ?truecas-
ing? the data, i.e. the ideal lowercasing of ev-
erything except names. Our simple unsupervised
truecaser (?utc?) uses a model trained on monolin-
gual data (1 million sentences in this case, same
as the parallel training data used in this experi-
ment) to identify the most frequent ?casing shape?
of each token type when it appears within a sen-
tence and then converts its occurrences at the be-
ginnings of sentences to this shape. Our super-
vised truecaser (?stc?) casts the case of the lemma
on the form, because our lemmatizers for English
and Czech produce case-sensitive lemmas to indi-
cate names. After the translation, only determinis-
tic uppercasing of sentence beginnings is needed.
We confirm that ?stc? as we have been using it
for a couple of years is indeed the best option, de-
spite its unpleasingly frequent omissions of names
(incl. ?Spojene? sta?ty?, ?the United States?). One
of the rules in Depfix tries to cast the case from
the source to the MT output but due to alignment
errors, it is not perfect in fixing these mistakes.
Surprisingly, the standard recasing worked
worse than ?lc?form?, suggesting that two Moses
runs in a row are worse than one joint search.
We consider using a full-fledged named entity
recognizer in the future.
Tokens [M]
Corpus Sents [M] English Czech
CzEng 1.0 14.83 235.67 205.17
Europarl 0.65 17.61 15.00
Common Crawl 0.16 4.08 3.63
Table 2: Basic Statistics of Parallel Data.
2.1.2 Factored Translation for Morphological
Coherence
We use a quite standard factored configuration of
Moses. We translate from ?stc? to two factors:
?stc? and ?tag? (full Czech positional morpholog-
ical tag). Even though tags on the target side make
the data somewhat sparser (a single Czech word
form typically represents several cases, numbers
or genders), we do not use any back-off or alterna-
tive decoding path. A high-order language model
on tags is used to promote grammatically correct
and coherent output. Our system is thus less prone
to errors in local morphological agreement.
2.1.3 Large Parallel Data
The main source of our parallel data was CzEng
1.0 (Bojar et al, 2012b). We also used Europarl
(Koehn, 2005) as made available by WMT13 orga-
nizers.2 The English-Czech part of the new Com-
mon Crawl corpus was quite small and very noisy,
so we did not include it in our training data. Ta-
ble 2 provides basic statistics of the data.
Processing large parallel data can be challeng-
ing in terms of time and computational resources
required. The main bottlenecks are word align-
ment and phrase extraction.
GIZA++ (Och and Ney, 2000) has been the
standard tool for computing word alignment in
phrase-based MT. A multi-threaded version exists
(Gao and Vogel, 2008), which also supports incre-
mental extensions of parallel data by applying a
saved model on a new sentence pair. We evaluated
these tools and measured their wall-clock time3 as
well as the final BLEU score of a full MT system.
Surprisingly, single-threaded GIZA++ was con-
siderably faster than single-threaded MGIZA. Us-
ing 12 threads, MGIZA outperformed GIZA++
but the difference was smaller than we expected.
Table 3 summarizes the results. We checked the
difference in BLEU using the procedure by Clark
et al (2011) and GIZA++ alignments were indeed
2http://www.statmt.org/wmt13/
translation-task.html
3Time measurements are only indicative, they were af-
fected by the current load in our cluster.
93
Alignment Wallclock Time BLEU
GIZA++ 71 15.5
MGIZA 1 thread 114 15.4
MGIZA 12 threads 51 15.4
Table 3: Rough wallclock time [hours] of word
alignment and the resulting BLEU scores.
Corpus Sents [M] Tokens [M]
CzEng 1.0 14.83 205.17
CWC Articles 36.72 626.86
CNC News 28.08 483.88
CNA 47.00 830.32
Newspapers 64.39 1040.80
News Crawl 24.91 444.84
Total 215.93 3631.87
Table 4: Basic Statistics of Monolingual Data.
little but significantly better than MGIZA in three
MERT runs.
We thus use the standard GIZA++ aligner.
2.1.4 Large Language Models
We were able to collect a very large amount of
monolingual data for Czech: almost 216 million
sentences, 3.6 billion tokens. Table 4 lists the
corpora we used. CWC Articles is a section of
the Czech Web Corpus (Spoustova? and Spousta,
2012). CNC News refers to a subset of the Czech
National Corpus4 from the news domain. CNA
is a corpus of Czech News Agency stories from
1998 to 2012. Newspapers is a collection of ar-
ticles from various Czech newspapers from years
1998 to 2002. Finally, News Crawl is the mono-
lingual corpus made available by the organizers of
WMT13.
We created an in-domain language model from
all the corpora except for CzEng (where we only
used the news section). We were able to train a 4-
gram language model using KenLM (Heafield et
al., 2013). Unfortunately, we did not manage to
use a model of higher order. The model file (even
in the binarized trie format with probability quan-
tization) was so large that we ran out of memory
in decoding.5 We also tried pruning these larger
models but we did not have enough RAM.
To cater for a longer-range coherence, we
trained a 7-gram language model only on the News
Crawl corpus (concatenation of all years). In this
case, we used SRILM (Stolcke, 2002) and pruned
n-grams so that (training set) model perplexity
4http://korpus.cz/
5Due to our cluster configuration, we need to pre-load lan-
guage models.
Token Order Sents Tokens ARPA.gz Trie
[M] [M] [GB] [GB]
stc 4 201.31 3430.92 28.2 11.8
stc 7 24.91 444.84 13.1 8.1
tag 10 14.83 205.17 7.2 3.0
Table 5: LMs used in CU-BOJAR.
does not increase more than 10?14. The data for
this LM exactly match the domain of WMT test
sets.
Finally, we model sequences of morphological
tags on the target side using a 10-gram LM es-
timated from CzEng. Individual sections of the
corpus (news, fiction, subtitles, EU legislation,
web pages, technical documentation and Navajo
project) were interpolated to match WMT test sets
from 2007 to 2011 best. This allows even out-of-
domain data to contribute to modeling of overall
sentence structure. We filtered the model using the
same threshold 10?14.
Table 5 summarizes the resulting LM files as
used in CU-BOJAR and CHIMERA.
2.1.5 Bigger Tuning Sets
Koehn and Haddow (2012) report benefits from
tuning on a larger set of sentences. We experi-
mented with a down-scaled MT system to com-
pare a couple of options for our tuning set: the
default 3003 sentences of newstest2011, the de-
fault and three more Czech references that were
created by translating from German, the default
and two more references that were created by post-
editing a variant of our last year?s Moses system
and also a larger single-reference set consisting
of several newstest years. The preliminary re-
sults were highly inconclusive: negligibly higher
BLEU scores obtained lower manual scores. Un-
able to pick the best configuration, we picked the
largest. We tune our systems on ?bigref?, as spec-
ified in Table 6. The dataset consists of 11583
source sentences, 3003 of which have 4 reference
translations and a subset (1997 sents.) of which
has 2 reference translations constructed by post-
editing. The dataset does not include 2010 data as
a heldout for other foreseen experiments.
2.1.6 Synthetic Parallel Data
Galus?c?a?kova? et al (2013) describe several possi-
bilities of combining TectoMT and phrase-based
approaches. Our CU-BOJAR uses one of the sim-
pler but effective ones: adding TectoMT output on
the test set to our training data. As a contrast to
94
English Czech # Refs # Snts
newstest2011 official + 3 more from German 4 3003
newstest2011 2 post-edits of a system 2 1997
similar to (Bojar et al, 2012a)
newstest2009 official 1 2525
newstest2008 official 1 2051
newstest2007 official 1 2007
Total 4 11583
Table 6: Our big tuning set (bigref).
CU-BOJAR, we also examine PLAIN Moses setup
which is identical but lacks the additional syn-
thetic phrase table by TectoMT.
In order to select the best balance between
phrases suggested by TectoMT and our parallel
data, we provide these data as two separate phrase
tables. Each phrase table brings in its own five-
tuple of scores, one of which, the phrase-penalty
functions as an indicator how many phrases come
from which of the phrase tables. The standard
MERT is then used to optimize the weights.6,7
We use one more trick compared to
Galus?c?a?kova? et al (2013): we deliberately
overlap our training and tuning datasets. When
preparing the synthetic parallel data, we use the
English side of newstests 08 and 10?13. The
Czech side is always produced by TectoMT. We
tune on bigref (see Table 6), so the years 08, 11
and 12 overlap. (We could have overlapped also
years 07, 09 and 10 but we had them originally
reserved for other purposes.) Table 7 summarizes
the situation and highlights that our setup is fair:
we never use the target side of our final evaluation
set newstest2013. Some test sets are denoted
?could have? as including them would still be
correct.
The overlap allows MERT to estimate how use-
ful are TectoMT phrases compared to the standard
phrase table not just in general but on the spe-
cific foreseen test set. This deliberate overfitting
to newstest 08, 11 and 12 then helps in translating
newstest13.
This combination technique in its current state
is rather expensive as a new phrase table is re-
quired for every new input document. However,
if we fix the weights for the TectoMT phrase ta-
6Using K-best batch MIRA (Cherry and Foster, 2012) did
not work any better in our setup.
7We are aware of the fact that Moses alternative decoding
paths (Birch and Osborne, 2007) with similar phrase tables
clutter n-best lists with identical items, making MERT less
stable (Eisele et al, 2008; Bojar and Tamchyna, 2011). The
issue was not severe in our case, CU-BOJAR needed 10 itera-
tions compared to 3 iterations needed for PLAIN.
Used in
Test Set Training Tuning Final Eval
newstest07 could have en+cs ?
newstest08 en+TectoMT en+cs ?
newstest09 could have en+cs ?
newstest10 en+TectoMT could have ?
newstest11 en+TectoMT en+cs ?
newstest12 en+TectoMT en+cs ?
newstest13 en+TectoMT ? en+cs
Table 7: Summary of test sets usage. ?en? and
?cs? denote the official English and Czech sides,
resp. ?TectoMT? denotes the synthetic Czech.
ble, we can avoid re-tuning the system (whether
this would degrade translation quality needs to be
empirically evaluated). Moreover, if we use a dy-
namic phrase table, we could update it with Tec-
toMT outputs on the fly, thus bypassing the need
to retrain the translation model.
2.2 Depfix
Depfix is an automatic post-editing tool for cor-
recting errors in English-to-Czech SMT. It is ap-
plied as a post-processing step to CU-BOJAR, re-
sulting in the CHIMERA system. Depfix 2013 is an
improvement of Depfix 2012 (Rosa et al, 2012).
Depfix focuses on three major types of language
phenomena that can be captured by employing lin-
guistic knowledge but are often hard for SMT sys-
tems to get right:
? morphological agreement, such as:
? an adjective and the noun it modifies have to
share the same morphological gender, num-
ber and case
? the subject and the predicate have to agree in
morphological gender, number and person, if
applicable
? transfer of meaning in cases where the same
meaning is expressed by different grammatical
means in English and in Czech, such as:
? a subject in English is marked by being a left
modifier of the predicate, while in Czech a
subject is marked by the nominative morpho-
logical case
? English marks possessiveness by the preposi-
tion ?of?, while Czech uses the genitive mor-
phological case
? negation can be marked in various ways in
English and Czech
? verb-noun and noun-noun valency?see (Rosa
et al, 2013)
Depfix first performs a complex lingustic anal-
95
System BLEU TER WMT Ranking
Appraise MTurk
CU-TECTOMT 14.7 0.741 0.455 0.491
CU-BOJAR 20.1 0.696 0.637 0.555
CU-DEPFIX 20.0 0.693 0.664 0.542
PLAIN Moses 19.5 0.713 ? ?
GOOGLE TR. ? ? 0.618 0.526
Table 8: Overall results.
ysis of both the source English sentence and its
translation to Czech by CU-BOJAR. The anal-
ysis includes tagging, word-alignment, and de-
pendency parsing both to shallow-syntax (?analyt-
ical?) and deep-syntax (?tectogrammatical?) de-
pendency trees. Detection and correction of errors
is performed by rule-based components (the va-
lency corrections use a simple statistical valency
model). For example, if the adjective-noun agree-
ment is found to be violated, it is corrected by
projecting the morphological categories from the
noun to the adjective, which is realized by chang-
ing their values in the Czech morphological tag
and generating the appropriate word form from the
lemma-tag pair using the rule-based generator of
Hajic? (2004).
Rosa (2013) provides details of the current ver-
sion of Depfix. The main additions since 2012 are
valency corrections and lost negation recovery.
3 Overall Results
Table 8 reports the scores on the WMT13 test
set. BLEU and TER are taken from the evalu-
ation web site8 for the normalized outputs, case
insensitive. The normalization affects typeset-
ting of punctuation only and greatly increases
automatic scores. ?WMT ranking? lists results
from judgments from Appraise and Mechanical
Turk. Except CU-TECTOMT, the manual evalua-
tion used non-normalized MT outputs. The fig-
ure is the WMT12 standard interpretation as sug-
gested by Bojar et al (2011) and says how often
the given system was ranked better than its com-
petitor across all 18.6k non-tying pairwise com-
parisons extracted from the annotations.
We see a giant leap from CU-TECTOMT to CU-
BOJAR, confirming the utility of large data. How-
ever, CU-TECTOMT had something to offer since it
improved over PLAIN, a very competitive baseline,
by 0.6 BLEU absolute. Depfix seems to slightly
worsen BLEU score but slightly improve TER; the
8http://matrix.statmt.org/
System # Tokens % Tokens
All 22920 76.44
Moses 3864 12.89
TectoMT 2323 7.75
Other 877 2.92
Table 9: CHIMERA components that contribute
?confirmed? tokens.
System # Tokens % Tokens
None 21633 79.93
Moses 2093 7.73
TectoMT 2585 9.55
Both 385 1.42
CU-BOJAR 370 1.37
Table 10: Tokens missing in CHIMERA output.
manual evaluation is similarly indecisive.
4 Combination Analysis
We now closely analyze the contributions of
the individual engines to the performance of
CHIMERA. We look at translations of the new-
stest2013 sets produced by the individual systems
(PLAIN, CU-TECTOMT, CU-BOJAR, CHIMERA).
We divide the newstest2013 reference tokens
into two classes: those successfully produced by
CHIMERA (Table 9) and those missed (Table 10).
The analysis can suffer from false positives as well
as false negatives, a ?confirmed? token can violate
some grammatical constraints in MT output and
an ?unconfirmed? token can be a very good trans-
lation. If we had access to more references, the
issue of false negatives would decrease.
Table 9 indicates that more than 3/4 of to-
kens confirmed by the reference were available
in all CHIMERA components: PLAIN Moses, CU-
TECTOMT alone but also in the subsequent combi-
nations CU-BOJAR and the final CU-DEPFIX.
PLAIN Moses produced 13% tokens that Tec-
toMT did not provide and TectoMT output
roughly 8% tokens unknown to Moses. However,
note that it is difficult to distinguish the effect of
different model weights: PLAIN might have pro-
duced some of those tokens as well if its weights
were different. The row ?Other? includes cases
where e.g. Depfix introduced a confirmed token
that none of the previous systems had.
Table 10 analyses the potential of CHIMERA
components. These tokens from the reference
were not produced by CHIMERA. In almost 80%
of cases, the token was not available in any 1-best
output; it may have been available in Moses phrase
96
tables or the input sentence.
TectoMT offered almost 10% of missed tokens,
but these were not selected in the subsequent com-
bination. The potential of Moses is somewhat
lower (about 8%) because our phrase-based com-
bination is likely to select wordings that score well
in a phrase-based model. 385 tokens were sug-
gested by both TectoMT and Moses alone, but the
combination in CU-BOJAR did not select them, and
finally 370 tokens were produced by the combina-
tion while they were not present in 1-best output of
neither TectoMT nor Moses. Remember, all these
tokens eventually did not get to CHIMERA output,
so Depfix must have changed them.
4.1 Depfix analysis
Table 11 analyzes the performance of the individ-
ual components of Depfix. Each evaluated sen-
tence was either modified by a Depfix component,
or not. If it was modified, its quality could have
been evaluated as better (improved), worse (wors-
ened), or the same (equal) as before. Thus, we can
evaluate the performance of the individual compo-
nents by the following measures:9
precision = #improved#improved+#worsened (1)
impact = #modified#evaluated (2)
useless = #equal#modified (3)
Please note that we make an assumption that if
a sentence was modified by multiple Depfix com-
ponents, they all have the same effect on its qual-
ity. While this is clearly incorrect, it is impossible
to accurately determine the effect of each individ-
ual component with the evaluation data at hand.
This probably skews especially the reported per-
formance of ?high-impact? components, which of-
ten operate in combination with other components.
The evaluation is computed on 871 hits in which
CU-BOJAR and CHIMERA were compared.
The results show that the two newest compo-
nents ? Lost negation recovery and Valency model
? both modify a large number of sentences. Va-
lency model seems to have a slightly negative ef-
fect on the translation quality. As this is the only
statistical component of Depfix, we believe that
this is caused by the fact that its parameters were
not tuned on the final CU-BOJAR system, as the
9We use the term precision for our primary measure for
convenience, even though the way we define it does not match
exactly its usual definition.
Depfix component Prc. Imp. Usl.
Aux ?be? agr. ? 1.4% 100%
No prep. without children ? 0.5% 100%
Sentence-initial capitalization 0% 0.1% 0%
Prepositional morph. case 0% 2.1% 83%
Preposition - noun agr. 40% 3.8% 70%
Noun number projection 41% 7.2% 65%
Valency model 48% 10.6% 66%
Subject - nominal pred. agr. 50% 3.8% 76%
Noun - adjective agr. 55% 17.8% 75%
Subject morph. case 56% 8.5% 57%
Tokenization projection 56% 3.0% 38%
Verb tense projection 58% 5.2% 47%
Passive actor with ?by? 60% 1.0% 44%
Possessive nouns 67% 0.9% 25%
Source-aware truecasing 67% 2.8% 50%
Subject - predicate agr. 68% 5.1% 57%
Pro-drop in subject 73% 3.4% 63%
Subject - past participle agr. 75% 6.3% 42%
Passive - aux ?be? agr. 77% 4.8% 69%
Possessive with ?of? 78% 1.5% 31%
Present continuous 78% 1.5% 31%
Missing reflexive verbs 80% 1.6% 64%
Subject categories projection 83% 3.7% 62%
Rehang children of aux verbs 83% 5.5% 62%
Lost negation recovery 90% 7.2% 38%
Table 11: Depfix components performance analy-
sis on 871 sentences from WMT13 test set.
tuning has to be done semi-manually and the fi-
nal system was not available in advance. On the
other hand, Lost negation recovery seems to have
a highly positive effect on translation quality. This
is to be expected, as a lost negation often leads to
the translation bearing an opposite meaning to the
original one, which is probably one of the most
serious errors that an MT system can make.
5 Conclusion
We have reached our chimera to beat Google
Translate. We combined all we have: a deep-
syntactic transfer-based system TectoMT, very
large parallel and monolingual data, factored setup
to ensure morphological coherence, and finally
Depfix, a rule-based automatic post-editing sys-
tem that corrects grammaticality (agreement and
valency) of the output as well as some features vi-
tal for adequacy, namely lost negation.
Acknowledgments
This work was partially supported by the grants
P406/11/1499 of the Grant Agency of the Czech
Republic, FP7-ICT-2011-7-288487 (MosesCore)
and FP7-ICT-2010-6-257528 (Khresmoi) of the
European Union and by SVV project number 267
314.
97
References
Alexandra Birch and Miles Osborne. 2007. CCG Su-
pertags in Factored Statistical Machine Translation.
In In ACL Workshop on Statistical Machine Trans-
lation, pages 9?16.
Ondr?ej Bojar and Ales? Tamchyna. 2011. Improving
Translation Model by Monolingual Data. In Proc.
of WMT, pages 330?336. ACL.
Ondr?ej Bojar, Milos? Ercegovc?evic?, Martin Popel, and
Omar Zaidan. 2011. A Grain of Salt for the WMT
Manual Evaluation. In Proc. of WMT, pages 1?11.
ACL.
Ondr?ej Bojar, Bushra Jawaid, and Amir Kamran.
2012a. Probes in a Taxonomy of Factored Phrase-
Based Models. In Proc. of WMT, pages 253?260.
ACL.
Ondr?ej Bojar, Zdene?k Z?abokrtsky?, Ondr?ej Dus?ek, Pe-
tra Galus?c?a?kova?, Martin Majlis?, David Marec?ek, Jir???
Mars???k, Michal Nova?k, Martin Popel, and Ales? Tam-
chyna. 2012b. The Joy of Parallelism with CzEng
1.0. In Proc. of LREC, pages 3921?3928. ELRA.
Colin Cherry and George Foster. 2012. Batch Tun-
ing Strategies for Statistical Machine Translation. In
Proc. of NAACL/HLT, pages 427?436. ACL.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for opti-
mizer instability. In Proc. of ACL/HLT, pages 176?
181. ACL.
Andreas Eisele, Christian Federmann, Herve? Saint-
Amand, Michael Jellinghaus, Teresa Herrmann, and
Yu Chen. 2008. Using Moses to Integrate Multi-
ple Rule-Based Machine Translation Engines into a
Hybrid System. In Proc. of WMT, pages 179?182.
ACL.
Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-
bienne Cap. 2012. Modeling Inflection and Word-
Formation in SMT. In Proc. of EACL 2012. ACL.
Petra Galus?c?a?kova?, Martin Popel, and Ondr?ej Bojar.
2013. PhraseFix: Statistical Post-Editing of Tec-
toMT. In Proc. of WMT13. Under review.
Qin Gao and Stephan Vogel. 2008. Parallel Implemen-
tations of Word Alignment Tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, SETQA-NLP ?08, pages 49?
57. ACL.
Jan Hajic?. 2004. Disambiguation of rich inflection:
computational morphology of Czech. Karolinum.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable Modified
Kneser-Ney Language Model Estimation. In Proc.
of ACL.
Philipp Koehn and Barry Haddow. 2012. Towards Ef-
fective Use of Training Data in Statistical Machine
Translation. In Proc. of WMT, pages 317?321. ACL.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Machine Trans-
lation Summit X, pages 79?86.
David Marec?ek, Rudolf Rosa, Petra Galus?c?a?kova?, and
Ondr?ej Bojar. 2011. Two-step translation with
grammatical post-processing. In Proc. of WMT,
pages 426?432. ACL.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In ACL. ACL.
Martin Popel and Zdene?k Z?abokrtsky?. 2010. Tec-
toMT: Modular NLP Framework. In Hrafn Lofts-
son, Eirikur Ro?gnvaldsson, and Sigrun Helgadottir,
editors, IceTAL 2010, volume 6233 of Lecture Notes
in Computer Science, pages 293?304. Iceland Cen-
tre for Language Technology (ICLT), Springer.
Rudolf Rosa, David Marec?ek, and Ondr?ej Dus?ek.
2012. DEPFIX: A system for automatic correction
of Czech MT outputs. In Proc. of WMT, pages 362?
368. ACL.
Rudolf Rosa, David Marec?ek, and Ales? Tamchyna.
2013. Deepfix: Statistical Post-editing of Statistical
Machine Translation Using Deep Syntactic Analy-
sis. Ba?lgarska akademija na naukite, ACL.
Rudolf Rosa. 2013. Automatic post-editing of phrase-
based machine translation outputs. Master?s thesis,
Charles University in Prague, Faculty of Mathemat-
ics and Physics, Praha, Czechia.
Johanka Spoustova? and Miroslav Spousta. 2012. A
High-Quality Web Corpus of Czech. In Proc. of
LREC. ELRA.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. Intl. Conf. on
Spoken Language Processing, volume 2, pages 901?
904.
Michael Subotin. 2011. An exponential translation
model for target language morphology. In Proc. of
ACL/HLT, pages 230?238. ACL.
98
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 141?147,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
PhraseFix: Statistical Post-Editing of TectoMT
Petra Galu?c??kov?, Martin Popel, and Ondr?ej Bojar
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostransk? n?me?st? 25, Prague, Czech Republic
{galuscakova,popel,bojar}@ufal.mff.cuni.cz
Abstract
We present two English-to-Czech systems
that took part in the WMT 2013 shared
task: TECTOMT and PHRASEFIX. The
former is a deep-syntactic transfer-based
system, the latter is a more-or-less stan-
dard statistical post-editing (SPE) applied
on top of TECTOMT. In a brief survey, we
put SPE in context with other system com-
bination techniques and evaluate SPE vs.
another simple system combination tech-
nique: using synthetic parallel data from
TECTOMT to train a statistical MT sys-
tem (SMT). We confirm that PHRASEFIX
(SPE) improves the output of TECTOMT,
and we use this to analyze errors in TEC-
TOMT. However, we also show that ex-
tending data for SMT is more effective.
1 Introduction
This paper describes two submissions to the
WMT 2013 shared task:1 TECTOMT ? a deep-
syntactic tree-to-tree system and PHRASEFIX ?
statistical post-editing of TECTOMT using Moses
(Koehn et al, 2007). We also report on exper-
iments with another hybrid method where TEC-
TOMT is used to produce additional (so-called
synthetic) parallel training data for Moses. This
method was used in CU-BOJAR and CU-DEPFIX
submissions, see Bojar et al (2013).
2 Overview of Related Work
The number of approaches to system combination
is enormous. We very briefly survey those that
form the basis of our work reported in this paper.
2.1 Statistical Post-Editing
Statistical post-editing (SPE, see e.g. Simard et al
(2007), Dugast et al (2009)) is a popular method
1http://www.statmt.org/wmt13
for improving outputs of a rule-based MT sys-
tem. In principle, SPE could be applied to any
type of first-stage system including a statistical
one (Oflazer and El-Kahlout, 2007; B?chara et al,
2011), but most benefit could be expected from
post-editing rule-based MT because of the com-
plementary nature of weaknesses and advantages
of rule-based and statistical approaches.
SPE is usually done with an off-the-shelf SMT
system (e.g. Moses) which is trained on output of
the first-stage system aligned with reference trans-
lations of the original source text. The goal of SPE
is to produce translations that are better than both
the first-stage system alone and the second-stage
SMT trained on the original training data.
Most SPE approaches use the reference trans-
lations from the original training parallel corpus
to train the second-stage system. In contrast,
Simard et al (2007) use human-post-edited first-
stage system outputs instead. Intuitively, the lat-
ter approach achieves better results because the
human-post-edited translations are closer to the
first-stage output than the original reference trans-
lations. Therefore, SPE learns to perform the
changes which are needed the most. However, cre-
ating human-post-edited translations is laborious
and must be done again for each new (version of
the) first-stage system in order to preserve its full
advantage over using the original references.2
Rosa et al (2013) have applied SPE on
English?Czech SMT outputs. They have used
the approach introduced by B?chara et al (2011),
but no improvement was achieved. However, their
rule-based post-editing were found helpful.
Our SPE setting (called PHRASEFIX) uses
TECTOMT as the first-stage system and Moses as
the second-stage system. Ideally, TECTOMT pre-
2If more reference translations are available, it would be
beneficial to choose such references for training SPE which
are most similar to the first-stage outputs. However, in our
experiments only one reference is available.
141
serves well-formed syntactic sentence structures,
and the SPE (Moses) fixes low fluency wordings.
2.2 MT Output Combination
An SPE system is trained to improve the output
of a single first-stage system. Sometimes, more
(first-stage) systems are available, and we would
like to combine them. In MT output selection,
for each sentence one system?s translation is se-
lected as the final output. In MT output combi-
nation, the final translation of each sentence is a
combination of phrases from several systems. In
both approaches, the systems are treated as black
boxes, so only their outputs are needed. In the
simplest setting, all systems are supposed to be
equally good/reliable, and the final output is se-
lected by voting, based on the number of shared n-
grams or language model scores. The number and
the identity of the systems to be combined there-
fore do not need to be known in advance. More so-
phisticated methods learn parameters/weights spe-
cific for the individual systems. These methods
are based e.g. on confusion networks (Rosti et al,
2007; Matusov et al, 2008) and joint optimization
of word alignment, word order and lexical choice
(He and Toutanova, 2009).
2.3 Synthetic Data Combination
Another way to combine several first-stage sys-
tems is to employ a standard SMT toolkit, e.g.
Moses. The core of the idea is to use the n first-
stage systems to prepare synthetic parallel data
and include them in the training data for the SMT.
Corpus Combination (CComb) The easiest
method is to use these n newly created paral-
lel corpora as additional training data, i.e. train
Moses on a concatenation of the original paral-
lel sentences (with human-translated references)
and the new parallel sentences (with machine-
translated pseudo-references).
Phrase Table Combination (PTComb) An-
other method is to extract n phrase tables in
addition to the original phrase table and ex-
ploit the Moses option of multiple phrase tables
(Koehn and Schroeder, 2007). This means that
given the usual five features (forward/backward
phrase/lexical log probability and phrase penalty),
we need to tune 5 ? (n+1) features. Because such
MERT (Och, 2003) tuning may be unstable for
higher n, several methods were proposed where
the n+1 phrase tables are merged into a single one
(Eisele et al, 2008; Chen et al, 2009). Another is-
sue of phrase table combination is that the same
output can be achieved with phrases from several
phrase tables, leading to spurious ambiguity and
thus less diversity in n-best lists of a given size
(see Chen et al (2009) for one possible solution).
CComb does not suffer from the spurious ambi-
guity issue, but it does not allow to tune special
features for the individual first-stage systems.
In our experiments, we use both CComb and
PTComb approaches. In PTComb, we use TEC-
TOMT as the only first-stage system and Moses as
the second-stage system. We use the two phrase
tables separately (the merging is not needed; 5 ? 2
is still a reasonable number of features in MERT).
In CComb, we concatenate English?Czech par-
allel corpus with English??synthetic Czech? cor-
pus translated from English using TECTOMT. A
single phrase table is created from the concate-
nated corpus.
3 TECTOMT
TECTOMT is a linguistically-motivated tree-to-
tree deep-syntactic translation system with trans-
fer based on Maximum Entropy context-sensitive
translation models (Marec?ek et al, 2010) and
Hidden Tree Markov Models (?abokrtsk? and
Popel, 2009). It employs some rule-based compo-
nents, but the most important tasks in the analysis-
transfer-synthesis pipeline are based on statistics
and machine learning. There are three main rea-
sons why it is a suitable candidate for SPE and
other hybrid methods.
? TECTOMT has quite different distribution
and characteristics of errors compared to
standard SMT (Bojar et al, 2011).
? TECTOMT is not tuned for BLEU using
MERT (its development is rather driven by hu-
man inspection of the errors although different
setups are regularly evaluated with BLEU as an
additional guidance).
? TECTOMT uses deep-syntactic dependency
language models in the transfer phase, but it
does not use standard n-gram language mod-
els on the surface forms because the current syn-
thesis phase supports only 1-best output.
The version of TECTOMT submitted to WMT
2013 is almost identical to the WMT 2012 version.
Only a few rule-based components (e.g. detection
of surface tense of English verbs) were refined.
142
Corpus Sents TokensCzech English
CzEng 15M 205M 236M
tmt(CzEng) 15M 197M 236M
Czech Web Corpus 37M 627M ?
WMT News Crawl 25M 445M ?
Table 1: Statistics of used data.
4 Common Experimental Setup
All our systems (including TECTOMT) were
trained on the CzEng (Bojar et al, 2012) par-
allel corpus (development and evaluation sub-
sets were omitted), see Table 1 for statistics.
We translated the English side of CzEng with
TECTOMT to obtain ?synthetic Czech?. This
way we obtained a new parallel corpus, denoted
tmt(CzEng), with English? synthetic Czech sen-
tences. Analogically, we translated the WMT
2013 test set (newstest2013) with TECTOMT and
obtained tmt(newstest2013). Our baseline SMT
system (Moses) trained on CzEng corpus only was
then also used for WMT 2013 test set transla-
tion, and we obtained smt(newstest2013). For all
MERT tuning, newstest2011 was used.
4.1 Alignment
All our parallel data were aligned with GIZA++
(Och and Ney, 2003) and symmetrized with
the ?grow-diag-final-and? heuristics. This ap-
plies also to the synthetic corpora tmt(CzEng),
tmt(newstest2013),3 and smt(newstest2013).
For the SPE experiments, we decided to base
alignment on (genuine and synthetic Czech) lem-
mas, which could be acquired directly from the
TECTOMT output. For the rest of the experiments,
we approximated lemmas with just the first four
lowercase characters of each (English and Czech)
token.
4.2 Language Models
In all our experiments, we used three language
models on truecased forms: News Crawl as pro-
vided by WMT organizers,4 the Czech side of
CzEng and the Articles section of the Czech Web
3Another possibility was to adapt TECTOMT to output
source-to-target word alignment, but GIZA++ was simpler to
use also due to different internal tokenization in TECTOMT
and our Moses pipeline.
4The deep-syntactic LM of TECTOMT was trained only
on this News Crawl data ? http://www.statmt.org/
wmt13/translation-task.html (sets 2007?2012).
BLEU 1-TER
TECTOMT 14.71?0.53 35.61?0.60
PHRASEFIX 17.73?0.54 35.63?0.65
Filtering 14.68?0.50 35.47?0.57
Mark Reliable Phr. 17.87?0.55 35.57?0.66
Mark Identities 17.87?0.57 35.85?0.68
Table 2: Comparison of several strategies of SPE.
Best results are in bold.
Corpus (Spoustov? and Spousta, 2012).
We used SRILM (Stolcke, 2002) with modified
Kneser-Ney smoothing. We trained 5-grams on
CzEng; on the other two corpora, we trained 7-
grams and pruned them if the (training set) per-
plexity increased by less than 10?14 relative. The
domain of the pruned corpora is similar to the test
set domain, therefore we trained 7-grams on these
corpora. Adding CzEng corpus can then increase
the results only very slightly ? training 5-grams on
CzEng is therefore sufficient and more efficient.
Each of the three LMs got its weight as-
signed by MERT. Across the experiments, Czech
Web Corpus usually gained the largest portion of
weights (40?17% of the total weight assigned to
language models), WMT News Crawl was the sec-
ond (32?15%), and CzEng was the least useful
(15?7%), perhaps due to its wide domain mixture.
5 SPE Experiments
We trained a base SPE system as described in Sec-
tion 2.1 and dubbed it PHRASEFIX.
First two rows of Table 2 show that the first-
stage TECTOMT system (serving here as the base-
line) was significantly improved in terms of BLEU
(Papineni et al, 2002) by PHRASEFIX (p < 0.001
according to the paired bootstrap test (Koehn,
2004)), but the difference in TER (Snover et
al., 2006) is not significant.5 The preliminary
results of WMT 2013 manual evaluation show
only a minor improvement: TECTOMT=0.476
vs. PHRASEFIX=0.484 (higher means better, for
details on the ranking see Callison-Burch et al
(2012)).
5The BLEU and TER results reported here slightly differ
from the results shown at http://matrix.statmt.
org/matrix/systems_list/1720 because of differ-
ent tokenization and normalization. It seems that statmt.org
disables the --international-tokenization
switch, so e.g. the correct Czech quotes (?word?) are not
tokenized, hence the neighboring tokens are never counted
as matching the reference (which is tokenized as " word ").
143
Despite of the improvement, PHRASEFIX?s
phrase table (synthetic Czech ? genuine Czech)
still contains many wrong phrase pairs that worsen
the TECTOMT output instead of improving it.
They naturally arise in cases where the genuine
Czech is a too loose translation (or when the
English-Czech sentence pair is simply misaligned
in CzEng), and the word alignment between gen-
uine and synthetic Czech struggles.
Apart from removing such garbage phrase pairs,
it would also be beneficial to have some control
over the SPE. For instance, we would like to gen-
erally prefer the original output of TECTOMT ex-
cept for clear errors, so only reliable phrase pairs
should be used. We examine several strategies:
Phrase table filtering. We filter out all phrase
pairs with forward probability ? 0.7 and all sin-
gleton phrase pairs. These thresholds were set
based on our early experiments. Similar filtering
was used by Dugast et al (2009).
Marking of reliable phrases. This strategy is
similar to the previous one, but the low-frequency
phrase pairs are not filtered-out. Instead, a special
feature marking these pairs is added. The subse-
quent MERT of the SPE system selects the best
weight for this indicator feature. The frequency
and probability thresholds for marking a phrase
pair are the same as in the previous case.
Marking of identities A special feature indicat-
ing the equality of the source and target phrase in
a phrase pair is added. In general, if the output
of TECTOMT matched the reference, then such
output was probably good and does not need any
post-editing. These phrase pairs should be perhaps
slightly preferred by the SPE.
As apparent from Table 2, marking either reli-
able phrases or identities is useful in our SPE set-
ting in terms of BLEU score. In terms of TER
measure, marking the identities slightly improves
PHRASEFIX. However, none of the improvements
is statistically significant.
6 Data Combination Experiments
We now describe experiments with phrase table
and corpus combination. In the training step, the
source-language monolingual corpus that serves
as the basis of the synthetic parallel data can
be:
? the source side of the original parallel training
corpus (resulting in tmt(CzEng)),
? a huge source-language monolingual corpus for
which no human translations are available (we
have not finished this experiment yet),
? the source side of the test set (resulting in
tmt(newstest2013) if translated by TECTOMT
or smt(newstest2013) if translated by baseline
configuration of Moses trained on CzEng), or
? a combination of the above.
There is a trade-off in the choice: the source
side of the test set is obviously most useful for
the given input, but it restricts the applicability (all
systems must be installed or available online in the
testing time) and speed (we must wait for the slow-
est system and the combination).
So far, in PTComb we tried adding the full
synthetic CzEng (?CzEng + tmt(CzEng)?), adding
the test set (?CzEng + tmt(newstest2013)? and
?CzEng + smt(newstest2013)?), and adding both
(?CzEng + tmt(CzEng) + tmt(newstest2013)?). In
CComb, we concatenated CzEng and full syn-
thetic CzEng (?CzEng + tmt(CzEng)?).
There are two flavors of PTComb: either the
two phrase tables are used both at once as alter-
native decoding paths (?Alternative?), where each
source span is equipped with translation options
from any of the tables, or the synthetic Czech
phrase table is used only as a back-off method if a
source phrase is not available in the primary table
(?Back-off?). The back-off model was applied to
source phrases of up to 5 tokens.
Table 3 summarizes our results with phrase ta-
ble and corpus combination. We see that adding
synthetic data unrelated to the test set does bring
only a small benefit in terms of BLEU in the case
of CComb, and we see a small improvement in
TER in two cases. Adding the (synthetic) transla-
tion of the test set helps. However, adding trans-
lated source side of the test set is helpful only if
it is translated by the TECTOMT system. If our
baseline system is used for this translation, the re-
sults even slightly drop.
Somewhat related experiments for pivot lan-
guages by Galu?c??kov? and Bojar (2012) showed
a significant gain when the outputs of a rule-based
system were added to the training data of Moses.
In their case however, the genuine parallel corpus
was much smaller than the synthetic data. The
benefit of unrelated synthetic data seems to van-
ish with larger parallel data available.
144
Training Data for Moses Decoding Type BLEU 1-TER
baseline: CzEng ? 18.52?0.57 36.41?0.66
tmt(CzEng) ? 15.96?0.53 33.67?0.63
CzEng + tmt(CzEng) CComb 18.57?0.57 36.47?0.64
CzEng + tmt(CzEng) PTComb Alternative 18.42?0.58 36.47?0.65
CzEng + tmt(CzEng) PTComb Back-off 18.38?0.57 36.25?0.65
CzEng + tmt(newstest2013) PTComb Alternative 18.68?0.57 37.00?0.65
CzEng + smt(newstest2013) PTComb Alternative 18.46?0.54 36.59?0.65
CzEng + tmt(CzEng) + tmt(newstest2013) PTComb Alternative 18.85?0.58 37.03?0.66
Table 3: Comparison of several strategies used for Synthetic Data Combination (PTComb ? phrase table
combination and CComb ? corpus combination).
BLEU Judged better
SPE 17.73?0.54 123
PTComb 18.68?0.57 152
Table 4: Automatic (BLEU) and manual (number
of sentences judged better than the other system)
evaluation of SPE vs. PTComb.
7 Discussion
7.1 Comparison of SPE and PTComb
Assuming that our first-stage system, TECTOMT,
guarantees the grammaticality of the output (sadly
often not quite true), we see SPE and PTComb
as two complementary methods that bring in the
goods of SMT but risk breaking the grammati-
cality. Intuitively, SPE feels less risky, because
one would hope that the post-edits affect short se-
quences of words and not e.g. the clause structure.
With PTComb, one relies purely on the phrase-
based model and its well-known limitations with
respect to grammatical constraints.
Table 4 compares the two approaches empir-
ically. For SPE, we use the default PHRASE-
FIX; for PTComb, we use the option ?CzEng +
tmt(newstest2013)?. The BLEU scores are re-
peated.
We ran a small manual evaluation where three
annotators judged which of the two outputs was
better. The identity of the systems was hidden,
but the annotators had access to both the source
and the reference translation. Overall, we col-
lected 333 judgments over 120 source sentences.
Of the 333 judgments, 17 marked the two systems
as equally correct, and 44 marked the systems as
incomparably wrong. Across the remaining 275
non-tying comparisons, PTComb won ? 152 vs.
123.
We attribute the better performance of PTComb
to the fact that, unlike SPE, it has direct access to
the source text. Also, the risk of flawed sentence
structure in PTComb is probably not too bad, but
this can very much depend on the language pair.
English?Czech translation does not need much
reordering in general.
Based on the analysis of the better marked re-
sults of the PTComb system, the biggest problem
is the wrong selection of the word and word form,
especially for verbs. PTComb also outperforms
SPE in processing of frequent phrases and sub-
ordinate clauses. This problem could be solved
by enhancing fluency in SPE or by incorporat-
ing more training data. Another possibility would
be to modify TECTOMT system to produce more
than one-best translation as the correct word or
word form may be preserved in sequel transla-
tions.
7.2 Error Analysis of TECTOMT
While SPE seems to perform worse, it has a
unique advantage: it can be used as a feedback
for improving the first stage system. We can either
inspect the filtered SPE phrase table or differences
in translated sentences.
After submitting our WMT 2013 systems, this
comparison allowed us to spot a systematic error
in TECTOMT tagging of latin-origin words:
source pancreas
TECTOMT slinivek [plural]
PHRASEFIX slinivky [singular] br?i?n?
The part-of-speech tagger used in TECTOMT in-
correctly detects pancreas as plural, and the wrong
morphological number is used in the synthesis.
PHRASEFIX correctly learns that the plural form
slinivek should be changed to singular slinivky,
which has also a higher language model score.
Moreover, PHRASEFIX also learns that the trans-
145
lation of pancreas should be two words (br?i?n?
means abdominal). TECTOMT currently uses a
simplifying assumption of 1-to-1 correspondence
between content words, so it is not able to produce
the correct translation in this case.
Another example shows where PHRASEFIX
recovered from a lexical gap in TECTOMT:
source people who are strong-willed
TECTOMT lid? , kter?? jsou siln? willed
PHRASEFIX lid? , kter?? maj? silnou vu?li
TECTOMT?s primary translation model considers
strong-willed an OOV word, so a back-off dictio-
nary specialized for hyphen compounds is used.
However, this dictionary is not able to translate
willed. PHRASEFIX corrects this and also the
verb jsou = are (the correct Czech translation is
maj? silnou vu?li = have a strong will).
Finally, PHRASEFIX can also break things:
source You won?t be happy here
TECTOMT Nebudete ?t?astn? tady
PHRASEFIX Vy tady ?t?astn? [you here happy]
Here, PHRASEFIX damaged the translation by
omitting the negative verb nebudete = you won?t.
8 Conclusion
Statistical post-editing (SPE) and phrase table
combination (PTComb) can be seen as two com-
plementary approaches to exploiting the mutual
benefits of our deep-transfer system TECTOMT
and SMT.
We have shown that SPE improves the results of
TECTOMT. Several variations of SPE have been
examined, and we have further improved SPE re-
sults by marking identical and reliable phrases us-
ing a special feature. However, SMT still out-
performs SPE according to BLEU and TER mea-
sures. Finally, employing PTComb, we have im-
proved the baseline SMT system by utilizing ad-
ditional data translated by the TECTOMT system.
A small manual evaluation suggests that PTComb
is on average better than SPE, though in about one
third of sentences SPE was judged better. In our
future experiments, we plan to improve SPE by
applying techniques suited for monolingual align-
ment, e.g. feature-based aligner considering word
similarity (Rosa et al, 2012) or extending the par-
allel data with vocabulary identities to promote
alignment of the same word form (Dugast et al,
2009). Marking and filtering methods for SPE also
deserve a deeper study. As for PTComb, we plan
to combine several sources of synthetic data (in-
cluding a huge source-language monolingual cor-
pus).
Acknowledgements
This research is supported by the grants
GAUK 9209/2013, FP7-ICT-2011-7-288487
(MosesCore) of the European Union and SVV
project number 267 314. We thank the two
anonymous reviewers for their comments.
References
Hanna B?chara, Yanjun Ma, and Josef van Genabith.
2011. Statistical post-editing for a statistical MT
system. MT Summit XIII, pages 308?315.
Ondr?ej Bojar, Milo? Ercegovc?evic?, Martin Popel, and
Omar Zaidan. 2011. A Grain of Salt for the WMT
Manual Evaluation. In Proc. of WMT, pages 1?11,
Edinburgh, Scotland. ACL.
Ondr?ej Bojar, Zdene?k ?abokrtsk?, Ondr?ej Du?ek, Pe-
tra Galu?c??kov?, Martin Majli?, David Marec?ek, Jir??
Mar??k, Michal Nov?k, Martin Popel, and Ale? Tam-
chyna. 2012. The Joy of Parallelism with CzEng
1.0. In Proc. of LREC, pages 3921?3928, Istanbul,
Turkey. ELRA.
Ondr?ej Bojar, Rudolf Rosa, and Ale? Tamchyna. 2013.
Chimera ? Three Heads for English-to-Czech Trans-
lation. In Proc. of WMT.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proc. of WMT, Montreal,
Canada. ACL.
Yu Chen, Michael Jellinghaus, Andreas Eisele,
Yi Zhang, Sabine Hunsicker, Silke Theison, Chris-
tian Federmann, and Hans Uszkoreit. 2009. Com-
bining Multi-Engine Translations with Moses. In
Proc. of WMT, pages 42?46, Athens, Greece. ACL.
Lo?c Dugast, Jean Senellart, and Philipp Koehn.
2009. Statistical Post Editing and Dictionary Ex-
traction: Systran/Edinburgh Submissions for ACL-
WMT2009. In Proc. of WMT, pages 110?114,
Athens, Greece. ACL.
Andreas Eisele, Christian Federmann, Herv? Saint-
Amand, Michael Jellinghaus, Teresa Herrmann, and
Yu Chen. 2008. Using Moses to Integrate Multi-
ple Rule-Based Machine Translation Engines into a
Hybrid System. In Proc. of WMT, pages 179?182,
Columbus, Ohio. ACL.
Petra Galu?c??kov? and Ondr?ej Bojar. 2012. Improving
SMT by Using Parallel Data of a Closely Related
Language. In Proc. of HLT, pages 58?65, Amster-
dam, Netherlands. IOS Press.
146
Xiaodong He and Kristina Toutanova. 2009. Joint Op-
timization for Machine Translation System Combi-
nation. In Proc. of EMNLP, pages 1202?1211, Sin-
gapore. ACL.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in Domain Adaptation for Statistical Machine Trans-
lation. In Proc. of WMT, pages 224?227, Prague,
Czech Republic. ACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proc. of ACL, pages 177?180, Prague, Czech Re-
public. ACL.
Philipp Koehn. 2004. Statistical Significance Tests
for Machine Translation Evaluation. In Proc. of
EMNLP, Barcelona, Spain.
David Marec?ek, Martin Popel, and Zdene?k ?abokrt-
sk?. 2010. Maximum entropy translation model
in dependency-based MT framework. In Proc. of
MATR, pages 201?206. ACL.
Evgeny Matusov, Gregor Leusch, Rafael E. Banchs,
Nicola Bertoldi, Daniel Dechelotte, Marcello Fed-
erico, Muntsin Kolss, Young-Suk Lee, Jose B.
Marino, Matthias Paulik, Salim Roukos, Holger
Schwenk, and Hermann Ney. 2008. System Combi-
nation for Machine Translation of Spoken and Writ-
ten Language. IEEE, 16(7):1222?1237.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of ACL,
Sapporo, Japan.
Kemal Oflazer and Ilknur Durgar El-Kahlout. 2007.
Exploring different representational units in
English-to-Turkish statistical machine translation.
In Proc. of WMT, pages 25?32. ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. of ACL,
pages 311?318, Stroudsburg, PA, USA. ACL.
Rudolf Rosa, Ondr?ej Du?ek, David Marec?ek, and Mar-
tin Popel. 2012. Using Parallel Features in Parsing
of Machine-Translated Sentences for Correction of
Grammatical Errors. In Proc. of SSST, pages 39?48,
Jeju, Republic of Korea. ACL.
Rudolf Rosa, David Marec?ek, and Ale? Tamchyna.
2013. Deepfix: Statistical Post-editing of Statistical
Machine Translation Using Deep Syntactic Analy-
sis. Sofia, Bulgaria. ACL.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
Dorr. 2007. Combining Outputs from Multiple
Machine Translation Systems. In Proc. of NAACL,
pages 228?235, Rochester, New York. ACL.
Michel Simard, Cyril Goutte, and Pierre Isabelle.
2007. Statistical Phrase-Based Post-Editing. In
Proc. of NAACL, pages 508?515, Rochester, New
York. ACL.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. of Association for Machine Translation in
the Americas, pages 223?231.
Johanka Spoustov? and Miroslav Spousta. 2012. A
High-Quality Web Corpus of Czech. In Proc. of
LREC, Istanbul, Turkey. ELRA.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proc. of ICSLP, pages
257?286.
Zdene?k ?abokrtsk? and Martin Popel. 2009. Hidden
Markov Tree Model in Dependency-based Machine
Translation. In Proc. of IJCNLP, pages 145?148,
Suntec, Singapore.
147
