Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 490?494, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
REACTION: A naive machine learning approach for sentiment
classification
Silvio Moreira
IST/INESC-ID
Rua Alves Redol, 9
1000-029 Lisboa
Portugal
samir@inesc-id.pt
Joa?o Filgueiras
INESC-ID
Rua Alves Redol, 9
1000-029 Lisboa
Portugal
jfilgueiras@inesc-id.pt
Bruno Martins
IST/INESC-ID
Rua Alves Redol, 9
1000-029 Lisboa
Portugal
bruno.g.martins@ist.utl.pt
Francisco Couto
LASIGE - FCUL
Edif??cio C6 Piso 3
Campo Grande
1749 - 016 Lisboa
Portugal
fcouto@di.fc.ul.pt
Ma?rio J. Silva
IST/INESC-ID
Rua Alves Redol, 9
1000-029 Lisboa
Portugal
mjs@inesc-id.pt
Abstract
We evaluate a naive machine learning ap-
proach to sentiment classification focused
on Twitter in the context of the sentiment
analysis task of SemEval-2013. We employ
a classifier based on the Random Forests al-
gorithm to determine whether a tweet ex-
presses overall positive, negative or neu-
tral sentiment. The classifier was trained
only with the provided dataset and uses as
main features word vectors and lexicon word
counts. Our average F-score for all three
classes on the Twitter evaluation dataset
was 51.55%. The average F-score of both
positive and negative classes was 45.01%.
For the optional SMS evaluation dataset our
overall average F-score was 58.82%. The
average between positive and negative F-
scores was 50.11%.
1 Introduction
Sentiment Analysis is a growing research field, es-
pecially on web social networks. In this setting,
users share very diverse messages such as real-
time reactions to news, events and daily experi-
ences. The ability to tap on a vast repository of
opinions, such as Twitter, where there is great di-
versity of topics, has become an important goal
for many different applications. However, due to
the nature of the text, NLP systems face additional
challenges in this context. Shared messages, such
as tweets, are very short and users tend to resort to
highly informal an noisy speech.
Following this trend, the 2013 edition of Se-
mEval1 included a sentiment analysis on Twitter
task (SemEval-2013 Task 2). Participants were
asked to implement a system capable of determin-
ing whether a given tweet expresses positive, neg-
ative or neutral sentiment. To help in the develop-
ment of the system, an annotated training corpus
was released. Systems that used only the given
corpus for training were considered constrained,
while others were considered unconstrained. The
submitted prototypes were evaluated in a dataset
consisting of around 3700 tweets of several topics.
The metric used was the average F-score between
the positive and negative classes.
Our goal with this participation was to create a
baseline system from which we can build upon and
perform experiments to compare new approaches
with the state-of-the-art.
2 Related Work
The last decade saw a growing interest in systems
to automatically process sentiment in text. Many
approaches to detect subjectivity and determine
1Proceedings of the 7th International Workshop on Se-
mantic Evaluation (SemEval 2013), in conjunction with the
Second Joint Conference on Lexical and Computational Se-
mantics (*SEM 2013)
490
polarity of opinions in news articles, weblogs and
product reviews have been proposed (Pang et al,
2002; Pang et al, 2004; Wiebe et al, 2005; Wil-
son et al, 2005). This sub-field of NLP, known as
Sentiment Analysis is presented in great depth in
(Liu, 2012).
The emergence and proliferation of microblog
platforms created a medium where people express
and convey all kinds of information. In particu-
lar, these platforms are a rich source of subjec-
tive and opinionated text, which has motivated
the application of similar techniques to this do-
main. However, in this context, messages tend
to be very short and highly informal, full of ty-
pos, slang and unconventional spelling, posing ad-
ditional challenges to NLP systems. In fact, early
experiments in Sentiment Analysis in the context
of Twitter (Barbosa et al, 2010; Davidov et al,
2010; Koulompis et al, 2011; Pak et al, 2010;
Bifet et al, 2010) show that the techniques that
proved effective in other domains are not sufficient
in the microblog setting. In the spirit of these ap-
proaches, we included a preprocessing step, fol-
lowed by feature extraction focusing on word,
lexical and Twitter-specific features. Finally, we
use annotated data to train an automatic classifier
based on the Random Forests (Breiman, 2001) and
BESTrees (Sun et al, 2011) learning algorithms.
3 Resources
Two annotated datasets were made available to
participants of SemEval-2013 Task 2: one for
training purposes which was to contain 8000 to
12000 tweets; and another, for development, con-
taining 2000. The combined datasets ended up
amounting to a little over 7500 tweets. The distri-
bution of positives, negatives and neutrals for the
combined datasets can be found in Table 1. Nearly
half of all tweets belonged to the neutral class, and
negatives represent just 15% of these datasets.
Class Number
Positive 37%
Negative 15%
Neutral 48%
Table 1: Class distribution of annotated data.
Random examples of each class drawn from the
datasets are shown in Table 2.
Positive:
1 Louis inspired outfit on Monday and Zayn
inspired outfit today..4/5 done just need Harry
2 waking up to a Niners win, makes Tuesday
get off to a great start! 21-3 over the cards
and 2 games clear in the NFC West.
Negative:
3 Sitting at home on a Saturday night doing
absolutely nothing... Guess I?ll just watch
Greys Anatomy all night. #lonerproblems
#greysanatomy
4 Life just isn?t the same when there is no
Pretty Little Liars on Tuesday nights.
Neutral:
5 Won the match #getin . Plus,
tomorrow is a very busy day, with
Awareness Day?s and debates. Gulp. Debates
6 @ Nenaah oh cause my friend got something
from china and they said it will take at least 6
to 8 weeks and it came in the 2nd week :P
Table 2: Random examples of annotated tweets.
4 Approach
Given our goal of creating a baseline system, we
experimented with a common set of features used
in sentiment analysis. The messages were mod-
elled as a combination of binary (or presence) uni-
grams, lexical features and Twitter-specific fea-
tures. We decided to follow a supervised approach
by learning a Random Forests classifier from the
annotated data provided by the organisers of the
workshop (see Section 3). In summary, the devel-
opment of our system consisted of four steps: 1)
preprocessing of the data, 2) feature extraction, 3)
learning the classifier, and 4) applying the classi-
fier to the test set.
4.1 Preprocessing
The lexical variation introduced by typos, ab-
breviations, slang and unconventional spelling,
leads to very large vocabularies. The resulting
491
sparse vector representations with few non-zero
values hamper the learning process. In order to
tackle this problem, we replaced user mentions
(@<username>) with a fixed tag <USER> and
URLs with the tag <URL>. Then, each sentence
was normalised by converting to lower-case and
reducing character repetitions to at most 3 charac-
ters (e.g. ?heelloooooo!? would be normalised to
?heellooo!?). Finally, we performed the lemma-
tisation of the sentence using the Morphadorner2
software.
4.2 Feature Extraction
After the preprocessing step, we extract a vector
consisting of the top uni-grams present in the train-
ing set and represent individual messages in terms
of this vector. For each message we also compute
the frequency of smileys and words with prior sen-
timent polarity using a sentiment lexicon. Finally,
we include the harmonic mean of positive and neg-
ative words. Next we explain each feature in more
detail.
Word vector: a sparse word vector containing
the top 25.000 most frequent words that occur in
the training set. This feature aims at capturing re-
lations between certain words and overall message
polarity. The vector was extracted using the Weka
toolkit (Hall et al, 2009) with the stop word list
option.
Lexicon word count: positive and negative sen-
timent word counts. When the word is preceded by
a negation particle we invert the polarity. We used
Bing Liu?s Opinion Lexicon3 that includes 2006
positive and 4783 negative words and is especially
tailored for social media because it considers mis-
spellings, slang and other domain specific varia-
tions.
Smileys count: a count of positive and negative
smileys that appear in the tweet. We take advan-
tage of these constructs being especially indicative
of the overall expressed sentiment in a text (Davi-
dov et al, 2010). Although there are smiley lexi-
cons, such as the one used on SentiStrength4, we
used regular expressions to capture most common
2http://morphadorner.northwestern.edu/
3http://www.cs.uic.edu/?liub/FBS/
sentiment-analysis.html
4http://sentistrength.wlv.ac.uk
smileys in a flexible way.
Hashtag count: a count of positive and negative
hashtags. This feature also uses Bing Liu?s lexicon
to determine wether a word contained in an hash-
tag is positive or negative. The rationale behind
this feature is that positive or negative words in the
form of hashtags can have a stronger meaning than
regular words (Davidov et al, 2010).
Positive/negative harmonic mean: harmonic
mean between positive and negative token counts,
including words and hashtags.
In an attempt to further reduce the dimensional-
ity of the feature space we computed the principal
components of the word vector using the Principal
Components Analysis filter in Weka but observed
that this yielded worse results.
4.3 Learning the classifier
To implement our classifier we used the Weka ma-
chine learning framework and experimented with
two ensemble algorithms: Random Forests and
BESTrees. We eventually dropped the use of BE-
STrees as initial results were worse.
We attempted to use most of the data while be-
ing able to effectively measure the performance of
the classifier. Therefore we used the totality of
both sets for training and evaluated using 10 fold
cross-validation.
Since we used only the annotated dataset that
was provided for this task, our approach is consid-
ered constrained.
5 Results
Our results with 10 fold cross-validation using the
submitted classifier, are presented in Table 3.
Class Precision Recall F-score
positive 61.0% 63.9% 62.4%
negative 54.1% 26.8% 35.8%
neutral 64.7% 72.4% 68.3%
average F-score (pos/neg) 49.1%
Table 3: Cross-validation results using the training set.
Task evaluation results are presented in Table 4
for tweets. Our approach ranked 44th out of 48
participants. The evaluation dataset had a sim-
ilar class distribution to the annotated datasets,
492
with almost half being neutral, and just 14% neg-
ative. Preliminary results with cross-validation
were similar to those of the final evaluation for
Twitter.
Class Precision Recall F-score
positive 62.52% 55.28% 58.68%
negative 55.74% 21.80% 31.34%
neutral 56.54% 75.43% 64.63%
average F-score (pos/neg) 45.01%
Table 4: Task evaluation results for Tweets.
Also included in SemEval-2013 Task 2 was an
evaluation using a SMS dataset to understand if a
classifier trained using tweets could be applied to
SMS messages. SMS results are shown in Table 5.
In this case our approach ranked 23th out of 42 par-
ticipants. The SMS evaluation dataset was com-
posed of more than half neutral messages (58%),
and similarly distributed positives (23%) and neg-
atives (19%).
Class Precision Recall F-score
positive 53.66% 59.50% 56.45%
negative 60.54% 34.26% 43.76%
neutral 72.91% 79.90% 76.27%
average F-score (pos/neg) 50.11%
Table 5: Task evaluation results for SMS.
6 Discussion and Conclusions
As expected, our naive approach performs poorly
in the context of Twitter messages. The obtained
results are in line with similar approaches de-
scribed in the literature and we found that Ran-
dom Forests achieve the same performance as
other learning algorithms tried for the same task
(Koulompis et al, 2011).
The uneven distribution of classes in the data
may have also contributed to the low performance
of the classifier. Although the neutral class was
not considered in the evaluation, the datasets had
a great predominance of neutral messages whereas
the negative examples only accounted for 15% of
the corpus. This suggests that it could be useful to
use a minority class over-sampling method, such
as SMOTE (Chawla, 2002), to reduce the effect
of this imbalance on the data. We used n-grams
to model the words that compose each message.
However, this approach leads to very sparse rep-
resentations, thus becoming important to consider
techniques that reduce feature space. We experi-
mented with PCA, without success, but we still be-
lieve that applying feature selection algorithms or
denser word representations (Turian et al, 2010)
could improve performance in this task.
We find that our classifier performs better on the
SMS dataset. This might be explained by the fact
that SMS messages tend to be more direct, whereas
the same tweet can express, or show signs of, con-
tradictory sentiments. In fact, our naive approach
outperforms other systems that had better results
in the Twitter dataset, but it is difficult to say why,
given that we do not have access to the SMS test
set annotations.
Despite the poor ranking results, we achieved
our goal of performing basic experiments in the
task of sentiment analysis in Twitter and developed
a baseline system that will serve as a starting point
for future research.
Acknowledgments
This work was partially supported by FCT
(Portuguese research funding agency) under
project grants UTA-Est/MAI/0006/2009 (RE-
ACTION) and PTDC/CPJ-CPO/116888/2010
(POPSTAR). FCT also supported scholarship
SFRH/BD/89020/2012. This research was also
funded by the PIDDAC Program funds (INESC-
ID multi annual funding) and the LASIGE multi
annual support.
References
Barbosa, L., and Feng, J. 2010. Robust sentiment de-
tection on twitter from biased and noisy data. Pro-
ceedings of the 23rd International Conference on
Computational Linguistics: Posters, pp. 36-44.
Bifet, A., and Frank, E. 2010. Sentiment knowledge
discovery in twitter streaming data. Discovery Sci-
ence.
Breiman, L. 2001. Random forests. Machine learning,
45(1), 5-32.
Chawla, N. V., Bowyer, K. W., Hall, L. O., and
Kegelmeyer, W. P. 2002 SMOTE: synthetic minority
493
over-sampling technique. Journal of Artificial Intel-
ligence Research, 16, 321-357.
Davidov, D., Tsur, O., and Rappoport, A. 2010 En-
hanced sentiment learning using twitter hashtags
and smileys. Proceedings of the 23rd International
Conference on Computational Linguistics: Posters.
Pages 241-249. Association for Computational Lin-
guistics.
Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reute-
mann, P., and Witten, I. H. 2009. The WEKA
Data Mining Software: An Update SIGKDD Ex-
plorations, Volume 11, Issue 1.
Kouloumpis, E., Wilson, T., and Moore, J. 2011. Twit-
ter sentiment analysis: The good the bad and the
omg. Proceedings of the Fifth International AAAI
Conference on Weblogs and Social Media, 538541.
Liu, B. 2012. Sentiment Analysis and Opinion Mining.
Synthesis Lectures on Human Language Technolo-
gies, 5(1), 1167.
Pak, A., and Paroubek, P. 2010. Twitter as a corpus for
sentiment analysis and opinion mining. Proceedings
of LREC.
Pang, B., Lee, L., and Vaithyanathan, S. 2002. Thumbs
up?: sentiment classification using machine learning
techniques. Proceedings of the ACL-02 conference
on Empirical methods in natural language process-
ing. Volume 10, pp. 79-86. Association for Compu-
tational Linguistics.
Pang, B. and Lee, L. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. Proceedings of the 42nd an-
nual meeting on Association for Computational Lin-
guistics.
Sun, Q. and Pfahringer, B. 2011. Bagging Ensemble
Selection. Proceedings of the 24th Australasian Joint
Conference on Artificial Intelligence (AI?11), Perth,
Australia, pages 251-260. Springer.
Turian, J., Ratinov, L., and Bengio, Y. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics (pp. 384-394). Association for Computa-
tional Linguistics.
Wiebe, J. and Riloff, E. 2005. Creating subjective and
objective sentence classifiers from unannotated texts.
Computational Linguistics and Intelligent Text Pro-
cessing, pages 486-497, Springer.
Wilson, T., Wiebe, J., and Hoffmann, P. 2005. Rec-
ognizing contextual polarity in phrase-level senti-
ment analysis. Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pp. 347-354. Asso-
ciation for Computational Linguistics.
494
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 673?677,
Dublin, Ireland, August 23-24, 2014.
TUGAS: Exploiting Unlabelled Data for Twitter Sentiment Analysis
Silvio Amir
+
, Miguel Almeida
??
, Bruno Martins
+
, Jo
?
ao Filgueiras
+
, and M
?
ario J. Silva
+
+
INESC-ID, Instituto Superior T?ecnico, Universidade de Lisboa, Portugal
?
Priberam Labs, Alameda D. Afonso Henriques, 41, 2
o
, 1000-123 Lisboa, Portugal
?
Instituto de Telecomunicac??oes, Instituto Superior T?ecnico, Universidade de Lisboa, Portugal
samir@inesc-id.pt, miguel.almeida@priberam.pt, bruno.g.martins@tecnico.ulisboa.pt
jfilgueiras@inesc-id.pt, mjs@inesc-id.pt
Abstract
This paper describes our participation in
the message polarity classification task of
SemEval 2014. We focused on exploiting
unlabeled data to improve accuracy, com-
bining features leveraging word represen-
tations with other, more common features,
based on word tokens or lexicons. We
analyse the contribution of the different
features, concluding that unlabeled data
yields significant improvements.
1 Introduction
Research in exploiting social media for mea-
suring public opinion, evaluating popularity of
products and brands, anticipating stock-market
trends, or predicting elections showed promising
results (O?Connor et al., 2010; Mitchell et al.,
2013). However, this type of content poses a par-
ticularly challenging problem for text analysis sys-
tems. Typical messages show heavy use of Inter-
net slang, emoticons and other abbreviations and
discourse conventions. The lexical variation intro-
duced by this creative use of language, together
with the unconventional spelling and occasional
typos, leads to very large vocabularies. On the
other hand, messages are very short, and there-
fore word feature representations tend to become
very sparse, degrading the performance of ma-
chine learned classifiers.
The growing interest in this problem motivated
the creation of a shared task for Twitter Sentiment
Analysis in the 2013 edition of SemEval. The
Message Polarity Classification task was formal-
ized as follows: Given a message, decide whether
the message is of positive, negative, or neutral sen-
timent. For messages conveying both a positive
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
Positive Neutral Negative
Train 2014 3230 4109 1265
Tweets 2013 1572 1640 601
Tweets 2014 982 669 202
SMS 2013 492 1207 394
Tweets Sarcasm 2014 33 13 40
LiveJournal 2014 427 411 304
Table 1: Number of examples per class in each
SemEval dataset. The first row represents all train-
ing data; the other rows are sets used for testing.
and negative sentiment, whichever is the stronger
sentiment should be chosen (Nakov et al., 2013).
We describe our participation on the 2014 edi-
tion of this task, for which a set of manually la-
belled messages was created. Complying with the
Twitter policies for data access, the corpus was
distributed as a list of message IDs and each par-
ticipant was responsible for downloading the ac-
tual tweets. Using the provided script, we col-
lected a training set with 8604 tweets. After sub-
mission, the 2014 test sets were also made avail-
able. Along with the Tweets 2014 test set, evalu-
ation was also performed on a set of tweets with
sarcasm, on a set of LiveJournal blog entries, and
on sets of tweets and SMS messages from the 2013
edition of the task. Table 1 shows the class distri-
bution for each of these datasets.
In the 2013 edition (task 2B), the NRC-Canada
system (Mohammad et al., 2013) earned first place
by scoring 69.02% on the Official SemEval metric
(see Section 4) with a significant margin with re-
spect to the other systems: the second (G?unther
and Furrer, 2013) and third (Reckman et al., 2013)
best systems scored 65.27% and 64.86%, respec-
tively. The main novelty in the NRC-Canada sys-
tem was the use of sentiment lexicons, specific
for the Twitter domain, generated from unlabeled
tweets using emoticons and hashtags as indicators
of sentiment. They found that these lexicons had a
strong impact on the results ? more than word and
673
character n-grams.
The automatically induced lexicons are a way
to use information from unlabeled data to aid in
the classification task. In our approach, we take
this reasoning further, and focus on the impact of
various ways to incorporate knowledge from un-
labeled data. This allows us to mimic many real-
world scenarios where labelled data is scarce but
unlabeled data is plentiful.
2 Word Representations
In text classification it is common to represent doc-
uments as bags-of-words, i.e., as unordered col-
lections of words. However, in the case of very
short social media texts, these representations be-
come less effective, as they lead to increased data
sparseness. We focused our experiments in com-
paring and complementing these approaches with
denser representations, which we now describe.
2.1 Bag-Of-Words and ?BM25
In a representation based on bags-of-words,
each message is represented as a vector m =
{w
1
, w
2
, ..., w
n
} ? R
V
, where V is the size of
the vocabulary. In order to have weights that re-
flect how relevant a word is to each of the classes,
we weighted the individual terms according to the
?BM25 heuristic (Paltoglou and Thelwall, 2010):
?BM25(w
i
) = tf
i
? log
(
(N
p
?df
i,p
+s)?df
i,n
+s
(N
n
?df
i,n
+s)?df
i,p
+s
)
, (1)
where tf
i
represents the frequency of term i in the
message, N
a
is the size of corpus a, df
i,a
is the
document frequency of term i in the corpus a (i.e.,
in one of two subsets for the training data, corre-
sponding to either positive or negative messages),
and s is a smoothing constant, which we set to
0.5. This term weighting function was previously
shown to be effective for sentiment analysis.
2.2 Brown Clusters
Brown et al. (1992) proposed a greedy agglomer-
ative hierarchical clustering procedure that groups
words to maximize the mutual information of bi-
grams. Clusters are initialized as consisting of a
single word each, and are then greedily merged ac-
cording to a mutual information criterion, to form
a lower-dimensional representation of a vocabu-
lary. The hierarchical nature of the clustering al-
lows words to be represented at different levels in
the hierarchy. This approach provides a denser
representation of the messages, mitigating the fea-
ture sparseness problem. We used a publicly avail-
able
1
set of 1000 Brown clusters induced from a
corpus of 56 million Twitter messages.
We leveraged the word clusters by mapping
each word to the corresponding cluster, and we
then represented each message as a bag-of-clusters
vector in R
K
, where K = 1000 is the number
of clusters. These word cluster features were also
weighted with the ?BM25 scheme.
2.3 Concise Semantic Analysis
Concise Semantic Analysis is a form of term
and document representation that assigns, to each
term, its weight on each of the classes (Li et al.,
2011). These weights, computed from the fre-
quencies of the term on the training data, reflect
how associated the term is to each class. The
weight of term j in class c is given by (Lopez-
Monroy et al., 2013):
w
cj
=
?
k?P
c
log
2
(
1 +
tf
kj
len(k)
)
, (2)
where P
c
is the set of documents with label c
and tf
kj
is the term frequency of term j in doc-
ument k. To prevent labels with a higher number
of examples, or terms with higher frequencies, to
have stronger weights, an additional normalization
step is performed to obtain nw
cj
, the normalized
weight of term j in class c:
nw
cj
=
w
cj
?
l?L
w
lj
?
?
t?T
w
ct
. (3)
In the formula, L is the set of class labels and T is
the set of terms, making w
lj
the weight of term
j for a class l, and w
ct
the weight of a term t
in class c. After defining every term as a vector
t
j
= {nw
1j
, . . . , nw
Cj
} ? R
C
, where C is the
number of classes, each message m is represented
by summing each of its terms? weight vectors:
m
csa
=
?
j?m
tf
j
len(m)
? t
j
. (4)
In the formula, tf
j
is the frequency of term j in m.
2.4 Dense Word Vectors
Efficient approaches have recently been intro-
duced to train neural networks capable of produc-
ing continuous representations of words (Mikolov
1
http://www.ark.cs.cmu.edu/TweetNLP/
674
Lexicon #1-grams #2-grams #pairs
Bing Liu 6789 - -
MPQA 8222 - -
SentiStrength 2546 - -
NRC EmoLex 14177 - -
Sentiment140 62468 677698 480010
NRC HashSent 54129 316531 308808
Table 2: Number of unigrams, bigrams, and collo-
cation pairs, in the lexicons used in our system.
et al., 2013). These approaches allow fast train-
ing of projections from a representation based on
bags-of-words, where vectors have very high di-
mension (of the order of 10
4
), but are also very
sparse and integer-valued, to vectors of much
lower dimensions (of the order of 10
2
), with full
density and continuous values.
To induce word embeddings, a corpus of 17 mil-
lion Twitter messages was collected with the Twit-
ter crawler of Boanjak et al. (2012). Then, us-
ing word2vec
2
, we induced representations for the
word tokens occurring in the messages. All the to-
kens were represented as vectors w
j
? R
n
, with
n = 100. A message was modeled as the sum of
the vector representations of the individual words:
m
vec
=
?
j?m
w
j
. (5)
We also created a polarity class vector p
c
for each
class c, defined as:
p
c
=
1
N
c
?
m?c
m
vec
, (6)
where m is a message of class c and N
c
is the total
number of instances in class c. These vectors can
be interpreted as prototypes of their classes, and
are used in the classVec features described below.
3 The TUGAS System
We now describe the TUGAS approach, detailing
the considered features and our modeling choices.
3.1 Word Features
To reduce the feature space of the model,
messages were lower-cased, Twitter user men-
tions (@username) were replaced with the to-
ken <USER> and URLs were replaced with
the <URL> token. We also normalized words
to include at most 3 repeated characters (e.g.,
2
https://code.google.com/p/word2vec/
?helloooooo!? to ?hellooo!?). Following Pang et
al. (2002), negation was directly integrated into
the word representations. All the tokens occurring
between a negation word and the next punctuation
mark, were suffixed with the NEG annotation.
We used the following groups of features:
? bow-uni: vector of word unigrams
? bow-bc: vector of Brown word clusters
? csa: Concise Semantic Analysis vector m
csa
? wordVec: word2vec message vector m
vec
? classVec: Euclidean distance between mes-
sage vector m
vec
and each class vector p
c
3.2 Lexicon Features
The document model was enriched with features
that take into account the presence of words with a
known prior polarity, such as happy or sad. We in-
cluded words from manually annotated sentiment
lexicons: Bing Liu Opinion Lexicon (Hu and Liu,
2004), MPQA (Wilson et al., 2005) and the NRC
Emotion Lexicon (Mohammad and Turney, 2013).
We also used the two automatically generated lex-
icons from Mohammad et al. (2013): the NRC
Hashtag Sentiment Lexicon and the Sentiment140
Lexicon. Table 2 summarizes the number of terms
of each lexicon.
As Mohammad et al. (2013), we added the fol-
lowing set of lexicon features, for each lexicon,
and for each combination of negated/non-negated
words and positive/negative polarity.
? The sum of the sentiment scores of all
(negated/non-negated) terms with (posi-
tive/negative) sentiment
? The largest of those scores
? The sentiment score of the last word in the
message that is also present in the lexicon
? The number of terms within the lexicon
Notice that terms can be unigrams, bigrams, and
collocations pairs. A group of these features was
computed for each of the sentiment lexicons.
3.3 Syntactic Features
We extracted syntactic features aimed at the Twit-
ter domain, such as the use of heavy punctuation,
emoticons and character repetition. Concretely,
the following features were computed from the
original Twitter messages:
? Number of words originally with more than 3
repeated characters
? Number of sequences of exclamation marks
and/or question marks
675
Tweets Test 2013 Tweets Test 2014 SMS 2013 Live Journal 2014 Tweets Sarcasm 2014
Features Acc F1 Official Acc F1 Official Acc F1 Official Acc F1 Official Acc F1 Official
bow-uni 65.62 59.30 54.60 69.94 66.30 65.60 68.80 62.40 54.90 60.42 58.30 56.60 47.67 43.90 41.50
submitted 69.55 67.50 65.60 71.45 69.00 69.00 70.57 67.60 62.70 68.21 68.20 69.80 53.49 50.10 52.90
- lexicons 66.90 64.30 61.70 70.37 67.00 66.40 66.46 63.50 58.30 64.27 64.20 65.50 48.84 45.10 47.00
- classVec 69.37 67.30 65.40 71.83 69.30 69.60 69.14 66.60 62.10 67.51 67.50 69.30 53.49 50.10 52.90
- wordVec 69.63 67.70 66.00 70.32 67.70 68.00 66.79 64.90 60.90 68.04 68.00 69.70 53.49 50.50 53.50
- bow-bc 68.06 66.40 65.10 67.40 64.30 65.30 67.89 65.20 60.40 68.30 68.30 70.00 52.33 49.90 49.90
+ syntactic 69.58 67.60 65.70 71.24 68.30 68.50 70.38 67.40 62.40 67.95 68.00 69.70 52.33 48.80 50.00
+ csa 67.45 63.70 60.50 70.10 67.30 67.50 71.48 67.60 62.10 66.11 66.00 68.30 53.49 51.30 50.30
+ bow-uni 67.69 62.50 58.50 70.64 67.30 66.70 72.77 67.10 60.40 67.60 67.20 67.10 51.16 48.00 43.90
Table 3: Impact of removing or adding groups of features. The row marked as submitted, in bold, is the
one that we submitted to the shared task. The bold column is the official score used to rank participants.
? Number of positive/negative emoticons, de-
tected with a pre-existing regular expression
3
? Number of capitalized words
3.4 Model Training
We used the L2-regularized logistic regression im-
plementation from scikit-learn
4
. Given a set of m
instance-label pairs (x
i
, y
i
), with i = 1, . . . ,m,
x
i
? R
n
, and y
i
? {?1,+1}, learning the clas-
sifier involves solving the following optimization
problem, where C > 0 is a penalty parameter.
min
w
1
2
w
?
w + C
m
?
i=1
log(1 + e
?y
i
w
?
x
i
). (7)
In scikit-learn, the problem is solved through
a trust region Newton method, using a wrapper
over the implementation available in the liblin-
ear
5
package. For multi-class problems, scikit-
learn uses the one-vs-the-rest strategy. This par-
ticular implementation also suports the introduc-
tion of class weights, which we set to be inversely
proportional to the class frequency in the training
data, thus making each class equally important.
The selection of groups of features to be in-
cluded in the submitted run, as well as the tun-
ing of the regularization constant, were obtained
by cross-validation on the training dataset.
4 Results
We report results using the following metrics:
? Accuracy, defined as the percentage of
tweets correctly classified.
? Overall F1, computed by averaging the F1
score of all three classes.
? The Official SemEval score, computed by
averaging the F1 scores of the positive and
negative classes (Nakov et al., 2013).
3
http://sentiment.christopherpotts.net/
4
http://scikit-learn.org/
5
http://www.csie.ntu.edu.tw/
?
cjlin/liblinear/
Feature group Acc F1 Official
bow-bc 66.33 63.30 60.30
wordVec 62.34 60.00 57.90
bow-uni 65.62 59.30 54.60
csa 61.58 56.70 52.90
Table 4: Performance comparison using different
word representations in isolation.
We tried including or excluding various groups
of features, and obtained the best results on the
training set using Brown clusters (bow-bc), lexi-
con features (lexicon), word2vec word represen-
tations (wordVec), and the Euclidean distance be-
tween the word2vec representation and each class
vector (classVec). These features were the ones
used in our submission. Inclusion of syntactic
features (syntactic), Concise Semantic Analysis
(csa), and word unigrams (bow-uni) was found to
decrease performance during cross-validation, and
thus these features were not included.
Table 4 shows the results on the Twitter 2014
test set using only a single group of word represen-
tation features to train the model, from each of the
techniques introduced in Section 2. This table sug-
gests that exploiting unlabeled data is beneficial,
as representing words through their Brown clus-
ters (bow-bc) or through word2vec (wordVec)
yields better results than unigrams or CSA.
Table 3 shows results on five different test sets,
including two from the 2013 challenge (Nakov et
al., 2013), when features are added or removed
from the official submission, one group at a time.
Adding representations like bow-uni or csa actu-
ally hurts the performance, suggesting that, given
the relatively small set of training instances, using
coarse-level features in isolation, such as Brown
clusters, can yield better results.
More importantly, we verify that lexicon-based
and Brown cluster features have the largest impact
676
(2.6% and 3.7%, respectively, in the official met-
ric). These results indicate that leveraging unla-
beled data yields significant improvements.
5 Conclusions
This paper describes the participation of the
TUGAS team in the message polarity classifica-
tion task of SemEval 2014. We showed that there
are significant gains in leveraging unlabeled data
for the task of classifying the sentiment of Twit-
ter texts. Our score of 69% ranks at fifth place in
42 submissions, roughly 2% points below the top
score of 70.96%. We believe that the direction of
leveraging unlabeled data is still vastly unexplored
and, for future work, we intend to: (a) experi-
ment with semi-supervised learning approaches,
further exploiting unlabeled tweets; and (b) make
use of domain adaptation strategies to leverage on
labelled non-Twitter data.
Acknowledgements
This work was partially supported by the
EU/FEDER programme, QREN/POR Lis-
boa (Portugal), under the Intelligo project
(contract 2012/24803). The researchers from
INESC-ID were supported by Fundac??ao para
a Ci?encia e Tecnologia (FCT), through con-
tracts Pest-OE/EEI/LA0021/2013, EXCL/EEI-
ESS/0257/2012 (DataStorm), project grants
PTDC/CPJ-CPO/116888/2010 (POPSTAR), and
EXPL/EEI-ESS/0427/2013 (KD-LBSN), and
Ph.D. scholarship SFRH/BD/89020/2012.
References
Matko Boanjak, Eduardo Oliveira, Jos?e Martins, Ed-
uarda Mendes Rodrigues, and Lu??s Sarmento. 2012.
Twitterecho: a distributed focused crawler to sup-
port open research with twitter data. In 21st Interna-
tional Conference Companion on World Wide Web,
pages 1233?1240.
Peter F. Brown, Peter V. Desouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467?479.
Tobias G?unther and Lenz Furrer. 2013. GU-MLT-
LT: Sentiment analysis of short messages using lin-
guistic features and stochastic gradient descent. In
7th International Workshop on Semantic Evaluation,
pages 328?332.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In 10th ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining, pages 168?177.
Zhixing Li, Zhongyang Xiong, Yufang Zhang, Chuny-
ong Liu, and Kuan Li. 2011. Fast text categorization
using concise semantic analysis. Pattern Recogni-
tion Letters, 32(3):441?448.
?
Adrian Pastor Lopez-Monroy, Manuel Montes-y
Gomez, Hugo Jair Escalante, Luis Villasenor-
Pineda, and Esa?u Villatoro-Tello. 2013. INAOE?s
participation at PAN?13: Author profiling task. In
CLEF 2013 Evaluation Labs and Workshop.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In 27th Annual Conference on Neural Infor-
mation Processing Systems, pages 3111?3119.
Lewis Mitchell, Kameron Decker Harris, Morgan R
Frank, Peter Sheridan Dodds, and Christopher M
Danforth. 2013. The geography of happiness:
connecting twitter sentiment and expression, de-
mographics, and objective characteristics of place.
PLoS ONE, 8(5):e64417.
Saif M Mohammad and Peter D Turney. 2013. Crowd-
sourcing a word?emotion association lexicon. Com-
putational Intelligence, 29(3):436?465.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: building the state-of-
the-art in sentiment analysis of tweets. In 7th Inter-
national Workshop on Semantic Evaluation, pages
321?327.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. SemEval-2013 task 2: Sentiment analysis in
Twitter. In 7th International Workshop on Semantic
Evaluation, pages 312?320.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R Routledge, and Noah A Smith. 2010.
From tweets to polls: Linking text sentiment to
public opinion time series. In 4th International
AAAI Conference on Weblogs and Social Media,
pages 122?129.
Georgios Paltoglou and Mike Thelwall. 2010. A study
of information retrieval weighting schemes for sen-
timent analysis. In 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1386?
1395.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In ACL-02 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 79?86.
Hilke Reckman, Baird Cheyanne, Jean Crawford,
Richard Crowell, Linnea Micciulla, Saratendu Sethi,
and Fruzsina Veress. 2013. teragram: Rule-based
detection of sentiment phrases using SAS sentiment
analysis. In 7th International Workshop on Seman-
tic Evaluation, pages 513?519.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 347?354.
677
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 711?715,
Dublin, Ireland, August 23-24, 2014.
ULisboa: Identification and Classification of Medical Concepts
Andr
?
e Leal
+
, Diogo Gonc?alves
+
, Bruno Martins
?
, and Francisco M. Couto
+
+
LASIGE, Faculdade de Ci?encias, Universidade de Lisboa, 1749-016 Lisboa, Portugal.
?
INESC-ID, Instituto Superior T?ecnico, Universidade de Lisboa, Portugal
{aleal,dgoncalves}@lasige.di.fc.ul.pt , bruno.g.martins@ist.ul.pt , fcouto@di.fc.ul.pt
Abstract
This paper describes our participation on
Task 7 of SemEval 2014, which fo-
cused on the recognition and disambigua-
tion of medical concepts. We used an
adapted version of the Stanford NER sys-
tem to train CRF models to recognize tex-
tual spans denoting diseases and disor-
ders, within clinical notes. We consid-
ered an encoding that accounts with non-
continuous entities, together with a rich
set of features (i) based on domain spe-
cific lexicons like SNOMED CT, or (ii)
leveraging Brown clusters inferred from a
large collection of clinical texts. Together
with this recognition mechanism, we used
a heuristic similarity search method, to
assign an unambiguous identifier to each
concept recognized in the text.
Our best run on Task A (i.e., in the recog-
nition of medical concepts in the text)
achieved an F-measure of 0.705 in the
strict evaluation mode, and a promising
F-measure of 0.862 in the relaxed mode,
with a precision of 0.914. For Task B (i.e.,
the disambiguation of the recognized con-
cepts), we achieved less promising results,
with an accuracy of 0.405 in the strict
mode, and of 0.615 in the relaxed mode.
1 Introduction
Currently, many off-the-shelf named entity recog-
nition solutions are available, and these can be
used to recognize mentions in clinical notes de-
noting diseases and disorders. We decided to use
the Stanford NER tool (Finkel et al., 2005) to train
CRF models based on annotated biomedical text.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
The use of unsupervised methods for inferring
word representations is nowadays also known to
increase the accuracy of entity recognition mod-
els (Turian et al., 2010). Thus, we also used Brown
clusters (Brown et al., 1992; Turian et al., 2009)
inferred from a large collection of non-annotated
clinical texts, together with domain specific lexi-
cons, to build features for our CRF models.
An important challenge in entity recognition re-
lates to the recognition of overlapping and non-
continuous entities (Alex et al., 2007). In this
paper, we describe how we modified the Stan-
ford NER system to be able to recognize non-
continuous entities, through an adapted version of
the SBIEO scheme (Ratinov and Roth, 2009).
Besides the recognition of medical concepts, we
also present the strategy used to map each of the
recognized concepts into a SNOMED CT identi-
fier (Cornet and de Keizer, 2008). This task is
particularly challenging, since there are many am-
biguous cases. We describe our general approach
to address the aforementioned CUI mapping prob-
lem, based on similarity search and on the infor-
mation content of SNOMED CT concept names.
2 Task and Datasets
Task 7 of SemEval 2014 actually consisted of two
smaller tasks: recognition of mentions of medi-
cal concepts (Task A) and mapping each medical
concept, recognized in clinical notes, to a unique
UMLS CUI (Task B). In the first task, recogni-
tion of medical concepts, systems have to detect
continuous and discontinuous medical concepts
that belong to the UMLS semantic group disor-
ders. The second task, concerning with normal-
ization and mapping, is limited to UMLS CUIs
of SNOMED CT codes (i.e., although the UMLS
meta-thesaurus integrates several resources, we
are only interested in SNOMED CT). Each con-
cept that was previously recognized can have a
unique CUI associated to it, or none at all (CUI-
711
LESS). The goal here is to disambiguate the con-
cepts and choose the right CUI for each case. For
supporting the recognition and CUI mapping of
medical concepts, we retrieved the disorders sub-
set of SNOMED CT directly from UMLS
1
.
The evaluation can be done in a strict or a in
a relaxed way. For the case of strict evaluation,
an exact match must be achieved in the recogni-
tion, by having correct start and end offsets, within
the text, for the continuous concepts, and a cor-
rect set of start and end offsets for the discontin-
uous concepts. In the relaxed evaluation, there is
some space for errors in the offset values from the
recognition task. If there is some overlap between
the concepts, then the result is considered a partial
match, otherwise it is a recognition error.
A set of annotated biomedical texts was given
to the participants, separated in three categories:
trial, development and training. We also received a
final test set, and a large set of non-annotated texts.
All the provided texts were initially converted into
a common tokenized format, to be used as input
to the tools that we considered for developing our
approach. After processing, we converted the re-
sults back into the format used by SemEval 2014,
this way generating the official runs.
3 Entity Recognition
Our entity recognition approach was based on the
usage of the Stanford NER software, which em-
ploys a linear chain Conditional Random Field
(CRF) approach for building probabilistic mod-
els based on training data (Finkel et al., 2005).
In Stanford NER, model training is based on the
L-BFGS algorithm, and model decoding is made
through the Viterbi algorithm.
This tool requires all input texts to be tokenized
and encoded according to a named entity recog-
nition scheme such as SBIEO (Ratinov and Roth,
2009), characterized by only being able to recog-
nize continuous entities. As we also need to rec-
ognize non-continuous entities, we modified the
Stanford NER software to use a SBIEON encod-
ing scheme. This new scheme has the following
specific token-tag associations:
S: Single, that indicates if the token individually
constitutes an entity to be recognized.
B: Begin, identifying the beginning of the entity.
This tag is only given to the first word of the
1
http://www.nlm.nih.gov/research/umls/
entity, being followed is most cases by tokens
labeled as being inside the entity.
I: Inside, representing the continuation of a non
single word entity (i.e., the middle tokens).
E: Ending, representing the last word in the case
of entities composed by more than one word.
N: Non-Continuous, which identifies all the
words that are between the beginning and the
end of an entity, but that do not belong to it.
This label specifically allows us to model the
in-between tokens of non-continuous entities.
O: Other, which is associated to all other words
that are not part of entities.
We developed a Java parser that converts the
biomedical text, provided to the participants, into
a tokenized format. This tokenized format, in the
case of the annotated texts, associates individual
tokens to their SBIEON or SBIEO tags, so that the
datasets can be used as input to train CRF models.
3.1 Concept Recognition Models
As we said, SBIEON tokenization differs from
SBIEO by the fact that the first one gives support
to non-continuous entities. Based on these two in-
put schemes, we generated two different models:
Only continuous entities: A 2nd-order CRF
model was trained based on the SBIOE entity en-
coding scheme, which only recognizes continuous
entities. Non-continuous and overlapping entities
will thus, in this case, only be partially modeled
(i.e., we only considered the initial span of text as-
sociated to the non-continuous entities).
Non-continuous entities: A 2nd-order CRF
model was trained based on the SBIOEN entity
encoding scheme, accepting continuous and non-
continuous entities, although still not supporting
the case of overlapping entities. In these last cases,
only the first entity in each of the overlapping
groups will be modeled correctly, while the oth-
ers will only be partially modeled (i.e., by only
considering the non-overlapping spans).
Our CRF models relied on a standard set of fea-
ture templates that includes (i) word tokens within
a window of size 2, (ii) the token shape (e.g., if it
is uppercased, capitalized, numeric, etc.), (iii) to-
ken prefixes and suffixes, (iv) token position (e.g.,
at the beginning or ending of a sentence), and (v)
conjunctions of the current token with the previ-
ous 2 tags. Besides these standard features, we
also considered (a) domain-specific lexicons, and
(b) word representations based on Brown clusters.
712
3.2 Word Clusters
In addition to the annotated training dataset, par-
ticipants were also provided with 403876 non-
annotated texts, containing a total of 828509 to-
kens. We used this information to induce general-
ized cluster-based word representations.
Brown et al. proposed a greedy agglomera-
tive hierarchical clustering procedure that groups
words to maximize the mutual information of bi-
grams (Brown et al., 1992). According to Brown?s
clustering procedure, clusters are initialized as
consisting of a single word each, and are then
greedily merged according to a mutual informa-
tion criterion, based on bi-grams, to form a lower-
dimensional representation of a vocabulary that
can mitigate the feature sparseness problem. In
the context of named entity recognition studies,
several authors have previously noted that using
these types of cluster-based word representations
can indeed result in improvements (Turian et al.,
2009). The hierarchical nature of the clustering
allows words to be represented at different levels
in the hierarchy and, in our case, we considered
1000 different clusters of similar words.
We specifically used the set of training docu-
ments, together with the non-annotated documents
that were provided by the organizers, to induce
word representations based on Brown?s clustering
procedure, using an open-source implementation
that follows the description given by (Turian et al.,
2010). The word clusters are latter used as features
within the Stanford NER package, by considering
that each word can be replaced by the correspond-
ing cluster, this way adding some other back-off
features to the models (i.e., features that are less
sparse, in the sense that they will appear more fre-
quently associated to some of the instances).
4 Disambiguating Concepts
For mapping entities to concept IDs (Task B), we
used a heuristic method based on similarity search,
supported on Lucene indexes (MacCandless et al.,
2010). We look for SNOMED CT concepts that
have a high n-gram overlap with the entity name
occurring in the text, together with the information
content of each SNOMED CT concept.
In our implementation, we used Lucene to re-
trieve candidate SNOMED CT concepts according
to different string distance algorithms: the NGram
distance (Kondrak, 2005) first, then according to
the Jaro-Winkler distance (Winkler, 1990), and fi-
nally according to the Levenshtein distance. The
most similar candidate is chosen as the disam-
biguation. The specific order for the similar-
ity metrics was based on the intuition that met-
rics based on individual character-level matches
are probably not as informative as metrics based
on longer sequences of characters, although they
can be useful for dealing with spelling variations.
However, for future work, we plan to explore more
systematic approaches (e.g., based on learning to
rank) for combining multiple similarity metrics.
Additionally to the aforementioned similarity
metrics, a measure of the Information Content (IC)
of each SNOMED CT concept was also employed,
to further disambiguate the mappings (i.e., to se-
lect the SNOMED CT identifier that is more gen-
eral, and thus more likely to be associated to a par-
ticular concept descriptor). Notice that the IC of
a concept corresponds to a measure of its speci-
ficity, where higher values correspond to more
specific concepts, and lower values to more gen-
eral ones. Given the frequency freq(c) for each
concept c in a corpus (i.e., the same corpus that
was used to infer the word clusters), the informa-
tion content of this concept can be computed from
the ratio between its frequency (including its de-
scendants) and the maximum frequency of all con-
cepts (Resnik, 1995):
IC(c) = ? log
(
freq(c)
maxFreq
)
In the formula, maxFreq represents the maximum
frequency of a concept, i.e. the frequency of the
root concept, when it exists. The frequency of a
concept can be computed using an extrinsic ap-
proach that counts the exact matches of the con-
cept names on a large text corpus.
5 Evaluation Experiments
We submitted three distinct runs to the SemEval
competition. These runs were as follows:
Run 1: A SBIOEN model was used to recog-
nize non-continuous entities. This model was
trained using only the annotated texts from
the provided training set. We also used some
domain specific lexicons like SNOMED CT,
or lists with names for drugs and diseases
retrieved from DBPedia. Finally, the recog-
nition model also used Brown clusters gen-
erated from the non-annotated datasets pro-
vided in the competition.
713
For assigning a SNOMED CT identifier to
each entity, we used the disambiguation tech-
nique supported by Lucene indexes. In
this specific run we used all the considered
heuristics for similarity search.
Run 2: A simpler model based on the SBIOE
scheme was used in this case, which can only
recognize continuous entities. The same fea-
tures from Run 1 were used for training the
recognition model.
For assigning the SNOMED CT identifier to
each entity, we also used the same strategy
that was presented for Run 1.
Run 3: A similar SBIOE model to that from Run
2 was used for the recognition.
For assigning the corresponding SNOMED
CT identifier to each entity, we in this case
limited the heuristic rules that were used.
Instead of using the string similarity algo-
rithms, we used only exact matches, together
with the information content measure and the
neighboring terms for disambiguation.
6 Results and Discussion
We present our official results in Table 1, which
highlights our best results for each task.
Specifically in Task A, we achieved encourag-
ing results. Run 1 achieved an F-measure of 0.705
in the strict evaluation, and of 0.862 in the relaxed
evaluation. Since Runs 2 and 3 used the same
recognition strategy (i.e., models that attempted to
capture only the continuous entities), we obtained
the same results for Task A in both these runs. Ta-
ble 1 also shows that our performance in Task B
was significantly lower than in Task A.
As we can see in the table, our first run was the
one with the best results for Task A. The model
used on this run recognizes non-continuous enti-
ties, and this is perhaps the main reason for the
higher results (i.e., the other two runs used the
same features for the recognition models).
On what concerns the results of Task B, it is im-
portant to notice the distinct results from the first
and second runs, which used exactly the same dis-
ambiguation strategy. The differences in the re-
sults are a consequence from the use of a different
recognition model in Task A. We can see that the
ability to recognize non-continuous entities leads
to the generation of worse mappings, when con-
sidering our specific disambiguation strategy. Our
last run is the best in terms of the performance over
Task B, but the difference is subtle.
7 Conclusions and Future Work
This paper described our participation in Task 7 of
the SemEval 2014 competition, which was divided
into two subtasks, namely (i) the recognition of
continuous and non-continuous medical concepts,
and (ii) the mapping of each recognized concept to
a SNOMED CT identifier.
For the first task, we used the Stanford NER
software (Finkel et al., 2005), modified by us
to recognize not only continuous, but also non-
continuous entities. This was possible by intro-
ducing the SBIEON scheme, derived from the tra-
ditional SBIEO encoding. To increase the accu-
racy and precision of the recognition we have also
used domain specific lexicons and Brown clusters
inferred from non-annotated documents.
For the second task, we used a heuristic method
based on similarity search, for matching concepts
in the text against concepts from SNOMED CT,
together with a measure of information content
to disambiguate the cases of term polysemy in
SNOMED CT. We implemented our disambigua-
tion approach through the Lucene software frame-
work (MacCandless et al., 2010).
In the first task (Task A) we achieved some par-
ticularly encouraging results, showing that an off-
the-shelf NER system can be easily adapted to
the recognition of medical concepts in biomedi-
cal text. Our specific modifications to the Stanford
NER system, in order to support the recognition of
non-continuous entity names, indeed increased the
precision and recall on Task A. However, our ap-
proach for the disambiguation of the recognized
concepts (Task B) performed much worse, achiev-
ing an accuracy of 0.615 in the case of the relaxed
evaluation. Future developments will therefore fo-
cus on improving the component that addressed
the entity disambiguation subtask.
Specifically on what regards future work, we
plan to experiment with the usage of machine
learning methods for the disambiguation subtask,
instead of relying on a purely heuristic approach.
We are interested in experimenting with the us-
age of Learning to Rank (L2R) methods, similar to
those employed on the DNorm system (Leaman et
al., 2013), to optimally combine different heuris-
tics such as the ones that were used in our current
approach. A L2R model can be used to rank candi-
714
Task A Task B
Strict Evaluation Relaxed Evaluation Strict Relaxed
Run Precision Recall F-measure Precision Recall F-measure Accuracy Accuracy
1 0.753 0.663 0.705 0.914 0.815 0.862 0.402 0.606
2 0.752 0.660 0.703 0.909 0.806 0.855 0.404 0.612
3 0.752 0.660 0.703 0.909 0.806 0.855 0.405 0.615
Table 1: Our official results for Tasks A and B of the SemEval challenge focusing on clinical text.
date disambiguations (e.g., retrieved through sim-
ilarity search with basis on Lucene) according to a
combination of multiple criteria, and we can then
choose the top candidate as the disambiguation.
Additionally, we plan to use ontology-based
similarity measures to validate and improve the
mappings (Couto and Pinto, 2013). For example,
by assuming that all entities in a given span of text
are semantically related with each other, we can
use ontology relations to filter likely misannota-
tions (Grego and Couto, 2013; Grego et al., 2013).
Acknowledgments
The authors would like to thank Fundac??ao para a
Ci?encia e Tecnologia (FCT) for the financial sup-
port of SOMER (PTDC/EIA-EIA/119119/2010),
LASIGE (PEst-OE/EEI/UI0408/2014) and
INESC-ID (Pest-OE/EEI/LA0021/2013).
We also would like to thank our colleagues
Berta Alves, for her support in evaluating devel-
opment errors, and Lu??s Atalaya, for the develop-
ment of some of the data pre-processing scripts.
References
Beatrice Alex, Barry Haddow, and Claire Grover.
2007. Recognising nested named entities in biomed-
ical text. In Proceedings of the ACL-07 Workshop
on Biological, Translational, and Clinical Language
Processing, pages 65?72.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational linguistics, 18(4):467?479.
Ronald Cornet and Nicolette de Keizer. 2008. Forty
years of SNOMED: A literature review. BMC
Medical Informatics and Decision Making, 8(Suppl
1:S2):1?6.
Francisco M. Couto and Helena Sofia Pinto. 2013. The
next generation of similarity measures that fully ex-
plore the semantics in biomedical ontologies. Jour-
nal of Bioinformatics and Computational Biology,
11(05):1?11.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 363?370.
Tiago Grego and Francisco M Couto. 2013. Enhance-
ment of chemical entity identification in text using
semantic similarity validation. PloS ONE, 8(5):1?9.
Tiago Grego, Francisco Pinto, and Francisco Couto.
2013. LASIGE: using conditional random fields and
chebi ontology. In Proceedings of the 7th Inter-
national Workshop on Semantic Evaluation, pages
660?666.
Grzegorz Kondrak. 2005. N-gram similarity and dis-
tance. In Proceedings of the 12th International
Conference String Processing and Information Re-
trieval, pages 115?126.
Robert Leaman, Rezarta Islamaj Do?gan, and Zhiy-
ong Lu. 2013. DNorm: disease name normaliza-
tion with pairwise learning to rank. Bioinformatics,
29(22):2909?2917.
Michael MacCandless, Erik Hatcher, and Otis Gospod-
neti?c. 2010. Lucene in Action. Manning Publica-
tions Company.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of the 13th Conference on Computa-
tional Natural Language Learning, pages 147?155.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In Pro-
ceedings of the 14th International Joint Conference
on Artificial Intelligence, pages 448?453.
Joseph Turian, Lev Ratinov, Yoshua Bengio, and Dan
Roth. 2009. A preliminary evaluation of word rep-
resentations for named-entity recognition. In Pro-
ceedings of the NIPS-09 Workshop on Grammar In-
duction, Representation of Language and Language
Learning, pages 1?8.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394.
William E Winkler. 1990. String comparator metrics
and enhanced decision rules in the Fellegi-Sunter
model of record linkage. In Proceedings of the Sec-
tion on Survey Research of the American Statistical
Association, pages 354?359.
715
