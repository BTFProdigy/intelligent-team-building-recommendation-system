Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 400?409,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
A Dependency-based Word Subsequence Kernel
Rohit J. Kate
Department of Computer Sciences
The University of Texas at Austin
1 University Station C0500
Austin, TX 78712-0233, USA
rjkate@cs.utexas.edu
Abstract
This paper introduces a new kernel which
computes similarity between two natural lan-
guage sentences as the number of paths shared
by their dependency trees. The paper gives a
very efficient algorithm to compute it. This
kernel is also an improvement over the word
subsequence kernel because it only counts
linguistically meaningful word subsequences
which are based on word dependencies. It
overcomes some of the difficulties encoun-
tered by syntactic tree kernels as well. Ex-
perimental results demonstrate the advantage
of this kernel over word subsequence and syn-
tactic tree kernels.
1 Introduction
Kernel-based learning methods (Vapnik, 1998) are
becoming increasingly popular in natural language
processing (NLP) because they allow one to work
with potentially infinite number of features with-
out explicitly constructing or manipulating them. In
most NLP problems, the data is present in structured
forms, like strings or trees, and this structural infor-
mation can be effectively passed to a kernel-based
learning algorithm using an appropriate kernel, like
a string kernel (Lodhi et al, 2002) or a tree kernel
(Collins and Duffy, 2001). In contrast, feature-based
methods require reducing the data to a pre-defined
set of features often leading to some loss of the use-
ful structural information present in the data.
A kernel is a measure of similarity between ev-
ery pair of examples in the data and a kernel-based
machine learning algorithm accesses the data only
through these kernel values. For example, the string
kernel (Lodhi et al, 2002; Cancedda et al, 2003)
computes the similarity between two natural lan-
guage strings as the number of common word sub-
sequences between them. A subsequence allows
gaps between the common words which are penal-
ized according to a parameter. Each word subse-
quence hence becomes an implicit feature used by
the kernel-based machine learning algorithm. A
problem with this kernel is that many of these word
subsequences common between two strings may not
be semantically expressive or linguistically mean-
ingful1. Another problem with this kernel is that
if there are long-range dependencies between the
words in a common word subsequence, then they
will unfairly get heavily penalized because of the
presence of word gaps.
The syntactic tree kernel presented in (Collins and
Duffy, 2001) captures the structural similarity be-
tween two syntactic trees as the number of syntac-
tic subtrees common between them. However, of-
ten syntactic parse trees may share syntactic sub-
trees which correspond to very different semantics
based on what words they represent in the sentence.
On the other hand, some subtrees may differ syn-
tactically but may represent similar underlying se-
mantics. These differences can become particularly
problematic if the tree kernel is to be used for tasks
which require semantic processing.
This paper presents a new kernel which computes
similarity between two sentences as the the number
of paths common between their dependency trees.
1(Lodhi et al, 2002) use character subsequences instead of
word subsequences which are even less meaningful.
400
(a) A fat cat was chased by a dog.
(b) A cat with a red collar was chased two days ago
by a fat dog.
Figure 1: Two natural language sentences.
It improves over the word subsequence kernel be-
cause it only counts the word subsequences which
are linked by dependencies. It also circumvents
some of the difficulties encountered with the syntac-
tic tree kernel when applied for semantic processing
tasks.
Although several dependency-tree-based kernels
and modifications to syntactic tree kernels have been
proposed which we briefly discuss in the Related
Work section, to our best knowledge no previous
work has presented a kernel based on dependency
paths which offers some unique advantages. We also
give a very efficient algorithm to compute this ker-
nel. We present experimental results on the task of
domain-specific semantic parsing demonstrating the
advantage of this kernel over word subsequence and
syntactic tree kernels.
The following section gives some background on
string and tree kernels. Section 3 then introduces
the dependency-based word subsequence kernel and
gives an efficient algorithm to compute it. Some of
the related work is discussed next, followed by ex-
periments, future work and conclusions.
2 String and Tree Kernels
2.1 Word-Subsequence Kernel
A kernel between two sentences measures the simi-
larity between them. Lodhi et al (2002) presented a
string kernel which measures the similarity between
two sentences, or two documents in general, as the
number of character subsequences shared between
them. This was extended by Cancedda et al (2003)
to the number of common word subsequences be-
tween them. We will refer to this kernel as the word
subsequence kernel.
Consider the two sentences shown in Figure 1.
Some common word subsequences between them
are ?a cat?, ?was chased by?, ?by a dog?, ?a cat
chased by a dog?, etc. Note that the subsequence
?was chased by? is present in the second sentence
but it requires skipping the words ?two days ago? or
has a gap of three words present in it. The kernel
downweights the presence of gaps by a decay fac-
tor ??(0, 1]. If g1 and g2 are the sum totals of gaps
for a subsequence present in the two sentences re-
spectively, then the contribution of this subsequence
towards the kernel value will be ?g1+g2 . The ker-
nel can be normalized to have values in the range
of [0, 1] to remove any bias due to different sen-
tence lengths. Lodhi et al (2002) give a dynamic
programming algorithm to compute string subse-
quence kernels in O(nst) time where s and t are the
lengths of the two input strings and n is the maxi-
mum length of common subsequences one wants to
consider. Rousu and Shawe-Taylor (2005) present
an improved algorithm which works faster when the
vocabulary size is large. Subsequence kernels have
been used with success in NLP for text classification
(Lodhi et al, 2002; Cancedda et al, 2003), informa-
tion extraction (Bunescu and Mooney, 2005b) and
semantic parsing (Kate and Mooney, 2006).
There are, however, some shortcomings of this
word subsequence kernel as a measure of similarity
between two sentences. Firstly, since it considers all
possible common subsequences, it is not sensitive
to whether the subsequence is linguistically mean-
ingful or not. For example, the meaningless sub-
sequences ?cat was by? and ?a was a? will also be
considered common between the two sentences by
this kernel. Since these subsequences will be used as
implicit features by the kernel-based machine learn-
ing algorithm, their presence can only hurt the per-
formance. Secondly, if there are long distance de-
pendencies between the words of the subsequence
present in a sentence then the subsequence will get
unfairly penalized. For example, the most important
word subsequence shared between the two sentences
shown in Figure 1 is ?a cat was chased by a dog?
which will get penalized by total gap of eight words
coming from the second sentence and a gap of one
word from the first sentence. Finally, the kernel is
not sensitive to the relations between the words, for
example, the kernel will consider ?a fat dog? as a
common subsequence although in the first sentence
?a fat? relates to the cat and not to the dog.
2.2 Syntactic Tree Kernel
Syntactic tree kernels were first introduced by
Collins and Duffy (2001) and were also used by
401
SNP
NP
DT
A
NN
cat
PP
IN
with
NP
DT
a
JJ
red
NN
collar
VP
AUX
was
VP
VBD
chased
ADVP
NP
CD
two
NNS
days
RB
ago
PP
IN
by
NP
DT
a
JJ
fat
NN
dog
Figure 3: Syntactic parse tree of the sentence shown in Figure 1 (b).
S
NP
DT
A
JJ
fat
NN
cat
VP
AUX
was
VP
VBD
chased
PP
IN
by
NP
DT
a
NN
dog
Figure 2: Syntactic parse tree of the sentence shown in
Figure 1 (a).
Collins (2002) for the task of re-ranking syntactic
parse trees. They define a kernel between two trees
as the number of subtrees shared between them. A
subtree is defined as any subgraph of the tree which
includes more than one node, with the restriction
that entire productions must be included at every
node. The kernel defined this way captures most
of the structural information present in the syntac-
tic parse trees in the form of tree fragments which
the kernelized learning algorithms can then implic-
itly use as features. The kernel can be computed
in O(|N1||N2|) time, where |N1| and |N2| are the
number of nodes of the two trees. An efficient al-
gorithm to compute tree kernels was given by Mos-
chitti (2006a) which runs in close to linear time in
the size of the input trees.
One drawback of this tree kernel, though, partic-
ularly when used for any task requiring semantic
processing, is that it may match syntactic subtrees
between two trees even though they represent very
dissimilar things in the sentence. For example, be-
tween the syntactic parse trees shown in Figures 2
and 3 for the two sentences shown in Figure 1, the
syntactic tree kernel will find (NP (DT a) JJ NN) as a
common subtree but in the first sentence it represents
?cat? while in the second it represents ?collar? and
?dog?. It will also find ?(NP (DT a) (JJ fat) NN)?
as a common subtree which again refers to ?cat? in
the first sentence and ?dog? in the second sentence.
As another example, consider two simple sentences:
(S (NP Chip) (VP (V saw) (NP Dale))) and (S (NP
Mary) (VP (V heard) (NP Sally))). Even though se-
mantically nothing is similar between them, the syn-
tactic tree kernel will still find common subtrees (S
NP VP), (VP N NP) and (S NP (VP V NP)). The
underlying problem is that the syntactic tree kernel
tends to overlook the words of the sentences which,
in fact, carry the essential semantics. On the other
hand, although (NP (DT a) (NN cat)) and (NP (DT
a) (JJ fat) (NN cat)) represent very similar concepts
but the kernel will not capture this high level sim-
ilarity between the two constituents, and will only
find (DT a) and (NN cat) as the common substruc-
tures. Finally, the most important similarity between
the two sentences is ?a cat was chased by a dog?
which will not be captured by this kernel because
402
was
cat
a fat
chased
by
dog
a
Figure 4: Dependency tree of the sentence shown in Fig-
ure 1 (a).
(b)
was
cat
a with
collar
a red
chased
by
dog
a fat
ago
days
two
Figure 5: Dependency tree of the sentence shown in Fig-
ure 1 (b).
there is no common subtree which covers it. The
Related Work section discusses some modifications
that have been proposed to the syntactic tree kernel.
3 A Dependency-based Word Subsequence
Kernel
A dependency tree encodes functional relationships
between the words in a sentence (Hudson, 1984).
The words of the sentence are the nodes and if a
word complements or modifies another word then
there is a child to parent edge from the first word to
the second word. Every word in a dependency tree
has exactly one parent except for the root word. Fig-
ures 4 and 5 show dependency trees for the two sen-
tences shown in Figure 1. There has been a lot of
progress in learning dependency tree parsers (Mc-
Donald et al, 2005; Koo et al, 2008; Wang et al,
2008). They can also be obtained indirectly from
syntactic parse trees utilizing the head words of the
constituents.
We introduce a new kernel which takes the words
into account like the word-subsequence kernel and
also takes the syntactic relations between them into
account like the syntactic tree kernel, however, it
does not have the shortcomings of the two kernels
pointed out in the previous section. This kernel
counts the number of common paths between the de-
pendency trees of the two sentences. Another way
to look at this kernel is that it counts all the common
word subsequences which are linked by dependen-
cies. Hence we will call it a dependency-based word
subsequence kernel. Since the implicit features it
uses are dependency paths which are enumerable, it
is a well defined kernel. In other words, an example
gets implicitly mapped to the feature space in which
each dependency path is a dimension.
The dependency-based word subsequence kernel
will find the common paths ?a ? cat?, ?cat ? was
? chased?, ?chased? by? dog? among many oth-
ers between the dependency trees shown in Figures 4
and 5. The arrows are always shown from child node
to the parent node. A common path takes into ac-
count the direction between the words as well. Also
note that it will find the important subsequence ?a
? cat ? was ? chased ? by ? dog ? a? as a
common path.
It can be seen that the word subsequences this
kernel considers as common paths are linguistically
meaningful. It is also not affected by long-range de-
pendencies between words because those words are
always directly linked in a dependency tree. There
is no need to allow gaps in this kernel either because
related words are always linked. It also won?t find
?a fat? as a common path because in the first tree
?cat? is between the two words and in the second
sentence ?dog? is between them. Thus it does not
have the shortcomings of the word subsequence ker-
nel. It also avoids the shortcomings of the syntac-
tic tree kernel because the common paths are words
themselves and syntactic labels do not interfere in
capturing the similarity between the two sentences.
It will not find anything common between depen-
dency trees for the sentences ?Chip saw Dale? and
?Mary heard Sally?. But it will find ?a ? cat? as a
common path between ?a cat? and ?a fat cat?. We
however note that this kernel does not use general
syntactic categories, unlike the syntactic tree kernel,
which will limit its applicability to the tasks which
depend on the syntactic categories, like re-ranking
syntactic parse trees.
403
We now give an efficient algorithm to compute
all the common paths between two trees. To our
best knowledge, no previous work has considered
this problem. The key observation for this algo-
rithm is that a path in a tree always has a structure in
which nodes (possibly none) go up to a highest node
followed by nodes (possibly none) coming down.
Based on this observation we compute two quanti-
ties for every pair of nodes between the two trees.
We call the first quantity common downward paths
(CDP ) between two nodes, one from each tree, and
it counts the number of common paths between the
two trees which originate from those two nodes and
which always go downward. For example, the com-
mon downward paths between the ?chased? node of
the tree in Figure 4 and the ?chased? node of the
tree in Figure 5 are ?chased ? by?, ?chased ? by
? dog? and ?chased ? by ? dog ? a?. Hence
CDP (chased, chased) = 3. A word may occur
multiple times in a sentence so the CDP values will
be computed separately for each occurrence. We
will shortly give a fast recursive algorithm to com-
pute CDP values.
Once these CDP values are known, using these
the second quantity is computed which we call com-
mon peak paths (CPP ) between every two nodes,
one from each tree. This counts the number of com-
mon paths between the two trees which peak at those
two nodes, i.e. these nodes are the highest nodes in
those paths. For example, ?was? is the peak for the
path ?a ? cat ? was ? chased?. Since every com-
mon path between the two trees has a unique highest
node, once these CPP values have been computed,
the number of common paths between the two trees
is simply the sum of all these CPP values.
We now describe how all these values are effi-
ciently computed. The CDP values between every
two nodes n1 and n2 of the trees T1 and T2 respec-
tively, is recursively computed as follows:
CDP (n1, n2) = 0 if n1.w 6= n2.w
otherwise,
CDP (n1, n2) =
?
c1?C(n1)
c2?C(n2)
c1.w = c2.w
(1 + CDP (c1, c2))
In the first equation, n.w stands for the word at
the node n. If the words are not equal then there
cannot be any common downward paths originating
from the nodes. In the second equation, C(n) rep-
resents the set of children nodes of the node n in a
tree. If the words at two children nodes are the same,
then the number of common downward paths from
the parent will include all the common downward
paths at the two children nodes incremented with the
link from the parent to the children. In addition the
path from parent to the child node is also a common
downward path. For example, in the trees shown
in Figures 4 and 5, the nodes with word ?was? have
?chased? as a common child. Hence all the common
downward paths originating from ?chased? (namely
?chased ? by?, ?chased ? by ? dog? and ?chased
? by ? dog ? a?) when incremented with ?was
? chased? become common downward paths orig-
inating from ?was?. In addition, the path ?was ?
chased? itself is a common downward path. Since
?cat? is also a common child at ?was?, it?s common
downward paths will also be added.
The CDP values thus computed are then used to
compute the CPP values as follows:
CPP (n1, n2) = 0 if n1.w 6= n2.w
otherwise,
CPP (n1, n2) = CDP (n1, n2) +
?
c1, c?1?C(n1)
c2, c?2?C(n2)
c1.w = c2.w
c?1.w = c?2.w
( 1 + CDP (c1, c2) + CDP (c?1, c?2)+CDP (c1, c2) ? CDP (c?1, c?2) )
If the two nodes are not equal then the number of
common paths that peak at them will be zero. If
the nodes are equal, then all the common downward
paths between them will also be the paths that peak
at them, hence it is the first term in the above equa-
tion. Next, the remaining paths that peak at them
can be counted by considering every pair of common
children nodes represented by c1 & c2 and c?1 & c?2.
For example, for the common node ?was? in Figures
4 and 5, the children nodes ?cat? and ?chased? are
common. The path ?cat ? was ? chased? is a path
that peaks at ?was?, hence 1 is added in the second
404
term. All the downward paths from ?cat? when in-
cremented up to ?was? and down to ?chased? are also
the paths that peak at ?was? (namely ?a? cat?was
? chased?). Similarly, all the downward paths from
?chased? when incremented up to ?was? and down to
?cat? are also paths that peak at ?was? (?cat ? was
? chased ? by?, ?cat ? was ? chased ? by ?
dog?, etc.). Hence the next two terms are present in
the equation. Finally, all the downward paths from
?cat? when incremented up to ?was? and down to ev-
ery downward path from ?chased? are also the paths
that peak at ?was? (?a ? cat ? was ? chased ?
by?, ?a ? cat ? was ? chased ? by ? dog? etc.).
Hence there is the product term present in the equa-
tion. It is important not to re-count a path from the
opposite direction hence the two pairs of common
children are considered only once (i.e. not reconsid-
ered symmetrically).
The dependency word subsequence kernel be-
tween two dependency trees T1 and T2 is then sim-
ply:
K(T1, T2) =
?
n1?T1
n2?T2
n1.w = n2.w
(1 + CPP (n1, n2))
We also want to count the number of common
words between the two trees in addition to the num-
ber of common paths, hence 1 is added in the equa-
tion. The kernel is normalized to remove any bias
due to different tree sizes:
Knormalized(T1, T2) =
K(T1, T2)
?
K(T1, T1) ?K(T2, T2)
Since for any long path common between two
trees, there will be many shorter paths within it
which will be also common between the two trees,
it is reasonable to downweight the contribution of
long paths. We do this by introducing a parameter
??(0, 1] and by downweighting a path of length l by
?l. A similar mechanism was also used in the syn-
tactic tree kernel (Collins and Duffy, 2001).
The equations for computing CDP and CPP
are accordingly modified as follows to accommodate
this downweighting.
CDP (n1, n2) = 0 if n1.w 6= n2.w
otherwise,
CDP (n1, n2) =
?
c1?C(n1)
c2?C(n2)
c1.w = c2.w
(? + ? ? CDP (c1, c2))
CPP (n1, n2) = 0 if n1.w 6= n2.w
otherwise,
CPP (n1, n2) = CDP (n1, n2) +
?
c1, c?1?C(n1)
c2, c?2?C(n2)
c1.w = c2.w
c?1.w = c?2.w
(
?2 + ? ? CDP (c1, c2)+
? ? CDP (c?1, c?2)+
?2 ? CDP (c1, c2) ? CDP (c?1, c?2)
)
This algorithm to compute all the common paths
between two trees has worst time complexity of
O(|T1||T2|), where |T1| and |T2| are the number of
nodes of the two trees T1 and T2 respectively. This
is because CDP computations are needed for every
pairs of nodes between the two trees and is recur-
sively computed. Using dynamic programming their
recomputations can be easily avoided. The CPP
computations then simply add the CDP values2. If
the nodes common between the two trees are sparse
then the algorithm will run much faster. Since the
algorithm only needs to store the CDP values, its
space complexity is O(|T1||T2|). Also note that this
algorithm computes the number of common paths
of all lengths unlike the word subsequence kernel
in which the maximum subsequence length needs to
be specified and the time complexity then depends
on this length.
4 Related Work
Several modifications to the syntactic tree kernels
have been proposed to overcome the type of prob-
lems pointed out in Subsection 2.2. Zhang et al
(2007) proposed a grammar-driven syntactic tree
kernel which allows soft matching between the sub-
trees of the trees if that is deemed appropriate by
the grammar. For example, their kernel will be able
2This analysis uses the fact that any node in a tree on average
has O(1) number of children.
405
to match the subtrees (NP (DT a) (NN cat)) and
(NP (DT a ) (JJ fat) (NN cat)) with some penalty.
Moschitti (2006b) proposed a partial tree kernel
which can partially match subtrees. Moschitti et
al. (2007) proposed a tree kernel over predicate-
argument structures of sentences based on the Prob-
Bank labels. Che et al (2006) presented a hy-
brid tree kernel which combines a constituent and
a path kernel. We however note that the paths in this
kernel link predicates and their arguments and are
very different from general paths in a tree that our
dependency-based word subsequence kernel uses.
Shen et al (2003) proposed a lexicalized syntac-
tic tree kernel which utilizes LTAG-based features.
Toutanova et al (2004) compute similarity between
two HPSG parse trees by finding similarity between
the leaf projection paths using string kernels.
A few kernels based on dependency trees have
also been proposed. Zelenko et al (2003) pro-
posed a tree kernel over shallow parse tree represen-
tations of sentences. This tree kernel was slightly
generalized by Culotta and Sorensen (2004) to com-
pute similarity between two dependency trees. In
addition to the words, this kernel also incorporates
word classes into the kernel. The kernel is based
on counting matching subsequences of children of
matching nodes. But as was also noted in (Bunescu
and Mooney, 2005a), this kernel is opaque i.e. it is
not obvious what the implicit features are and the
authors do not describe it either. In contrast, our
dependency-based word subsequence kernel, which
also computes similarity between two dependency
trees, is very transparent with the implicit features
being simply the dependency paths. Their kernel is
also very time consuming and in their more general
sparse setting it requires O(mn3) time and O(mn2)
space, where m and n are the number of nodes of
the two trees (m >= n) (Zelenko et al, 2003).
Bunescu and Mooney (2005a) give a shortest path
dependency kernel for relation extraction. Their ker-
nel, however, does not find similarity between two
sentences but between the shortest dependency paths
connecting the two entities of interests in the sen-
tences. This kernel uses general dependency graphs
but if the graph is a tree then the shortest path is
the only path between the entities. Their kernel also
uses word classes in addition to the words them-
selves.
5 Experiments
We show that the new dependency-based word sub-
sequence kernel performs better than word subse-
quence kernel and syntactic tree kernel on the task
of domain-specific semantic parsing.
5.1 Semantic Parsing
Semantic parsing is the task of converting natu-
ral language sentences into their domain-specific
complete formal meaning representations which an
application can execute, for example, to answer
database queries or to control a robot. A learn-
ing system for semantic parsing induces a seman-
tic parser from the training data of natural language
sentences paired with their respective meaning rep-
resentations. KRISP (Kate and Mooney, 2006)
is a semantic parser learning system which uses
word subsequence kernel based SVM (Cristianini
and Shawe-Taylor, 2000) classifiers and was shown
to be robust to noise compared to other semantic
parser learners. The system learns an SVM classi-
fier for every production of the meaning representa-
tion grammar which tells the probability with which
a substring of the sentence represents the semantic
concept of the production. Using these classifiers
a complete meaning representation of an input sen-
tence is obtained by finding the most probable parse
which covers the whole sentence. For details please
refer to (Kate and Mooney, 2006).
The key operation in KRISP is to find the sim-
ilarity between any two substrings of two natural
language sentences. Word subsequence kernel was
employed in (Kate and Mooney, 2006) to compute
the similarity between two substrings. We modi-
fied KRISP so that the similarity between two sub-
strings can also be computed using the syntactic tree
kernel and the dependency-based word subsequence
kernel. For applying the syntactic tree kernel, the
syntactic subtree over a substring of a sentence is de-
termined from the syntactic tree of the sentence by
finding the lowest common ancestor of the words in
this substring and then considering the smallest sub-
tree rooted at this node which includes all the words
of the substring. For applying the dependency-based
word subsequence kernel to two substrings of a sen-
tence, the kernel computation was suitably modified
so that the common paths between the two depen-
406
dency trees always begin and end with the words
present in the substrings. This is achieved by in-
cluding only those downward paths in computations
of CDP which end with words within the given
substrings. These paths relate the words within the
substrings perhaps using words outside of these sub-
strings.
5.2 Methodology
We measure the performance of KRISP obtained us-
ing the three types of kernels on the GEOQUERY
corpus which has been used previously by several
semantic parsing learning systems. It contains 880
natural language questions about the US geogra-
phy paired with their executable meaning represen-
tations in a functional query language (Kate et al,
2005). Since the purpose of the experiments is to
compare different kernels and not different seman-
tic parsers, we do not compare the performance with
other semantic parser learning systems. The train-
ing and testing was done using standard 10-fold
cross-validation and the performance was measured
in terms of precision (the percentage of generated
meaning representations that were correct) and re-
call (the percentage of all sentences for which cor-
rect meaning representations were obtained). Since
KRISP assigns confidences to the meaning represen-
tations it outputs, an entire range of precision-recall
trade-off can be obtained. We measure the best F-
measure (harmonic mean of precision and recall) ob-
tained when the system is trained using increasing
amounts of training data.
Since we were not interested in the accuracy of
dependency trees or syntactic trees but in the com-
parison between various kernels, we worked with
gold-standard syntactic trees. We did not have gold-
standard dependency trees available for this cor-
pus so we obtained them indirectly from the gold-
standard syntactic trees using the head-rules from
(Collins, 1999). We however note that accurate syn-
tactic trees can be obtained by training a syntac-
tic parser on WSJ treebank and gold-standard parse
trees of some domain-specific sentences (Kate et al,
2005).
In the experiments, the ? parameter of the
dependency-based word subsequence kernel was set
to 0.25, the ? parameter of the word subsequence
kernel was fixed to 0.75 and the downweighting pa-
Examples Dependency Word Syntactic
40 25.62 21.51 23.65
80 45.30 42.77 43.14
160 63.78 61.22 59.66
320 72.44 70.36 67.05
640 77.32 77.82 74.26
792 79.79 79.09 76.62
Table 1: Results on the semantic parsing task with in-
creasing number of training examples using dependency-
based word subsequence kernel, word subsequence ker-
nel and syntactic tree kernel.
rameter for the syntactic tree kernel was fixed to 0.4.
These were determined through pilot experiments
with a smaller portion of the data set. The maxi-
mum length of subsequences required by the word
subsequence kernel was fixed to 3, a longer length
was not found to improve the performance and was
only increasing the running time.
5.3 Results
Table 1 shows the results. The dependency-based
word subsequence kernel always performs better
than the syntactic tree kernel. All the numbers
under the dependency kernel were found statisti-
cally significant (p < 0.05) over the correspond-
ing numbers under the syntactic tree kernel based on
paired t-tests. The improvement of the dependency-
based word subsequence kernel over the word sub-
sequence kernel is greater with less training data,
showing that the dependency information is more
useful when the training data is limited. The per-
formance converges with higher amounts of training
data. The numbers shown in bold were found statis-
tically significant over the corresponding numbers
under the word subsequence kernel.
It may be noted that syntactic tree kernel is mostly
doing worse than the word subsequence kernel. We
believe this is because of the shortcomings of the
syntactic tree kernel pointed out in Subsection 2.2.
Since this is a semantic processing task, the words
play an important role and the generalized syntactic
categories are not very helpful.
6 Future Work
In future, the dependency-based word subsequence
kernel could be extended to incorporate word classes
407
like the kernels presented in (Bunescu and Mooney,
2005a; Zelenko et al, 2003). It should be possible to
achieve this by incorporating matches between word
classes in addition to the exact word matches in the
kernel computations similar to the way in which the
word subsequence kernel was extended to incorpo-
rate word classes in (Bunescu and Mooney, 2005b).
This will generalize the kernel and make it more ro-
bust to data sparsity.
The dependency-based word subsequence kernel
could be tested on other tasks which require comput-
ing similarity between sentences or texts, like text
classification, paraphrasing, summarization etc. We
believe this kernel will help improve performance on
those tasks.
7 Conclusions
We introduced a new kernel which finds similarity
between two sentences as the number of common
paths shared between their dependency trees. This
kernel can also be looked upon as an improved word
subsequence kernels which only counts the common
word subsequences which are related by dependen-
cies. We also gave an efficient algorithm to compute
this kernel. The kernel was shown to out-perform
the word subsequence kernel and the syntactic tree
kernel on the task of semantic parsing.
References
Razvan C. Bunescu and Raymond J. Mooney. 2005a.
A shortest path dependency kernel for relation extrac-
tion. In Proc. of HLT/EMNLP-05, pages 724?731,
Vancouver, BC, October.
Razvan C. Bunescu and Raymond J. Mooney. 2005b.
Subsequence kernels for relation extraction. In
Y. Weiss, B. Scho?lkopf, and J. Platt, editors, Advances
in Neural Information Processing Systems 18, Vancou-
ver, BC.
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and
Jean Michel Renders. 2003. Word sequence kernels.
Journal of Machine Learning Research, Special Issue
on Machine Learning Methods for Text and Images,
3:1059?1082, February.
Wanxiang Che, Min Zhang, Ting Liu, and Sheng Li.
2006. A hybrid convolution tree kernel for semantic
role labeling. In Proc. of COLING/ACL-06, pages 73?
80, Sydney, Australia, July.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proc. of NIPS-2001.
Michael Collins. 1999. Head-driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Michael Collins. 2002. New ranking algorithms for pars-
ing and tagging: Kernels over discrete structures, and
the voted perceptron. In Proc. of ACL-2002, pages
263?270, Philadelphia, PA, July.
Nello Cristianini and John Shawe-Taylor. 2000. An
Introduction to Support Vector Machines and Other
Kernel-based Learning Methods. Cambridge Univer-
sity Press.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proc. of ACL-
04, pages 423?429, Barcelona, Spain, July.
Richard Hudson. 1984. Word Grammar. Blackwell.
Rohit J. Kate and Raymond J. Mooney. 2006. Us-
ing string-kernels for learning semantic parsers. In
Proc. of COLING/ACl-06, pages 913?920, Sydney,
Australia, July.
Rohit J. Kate, Yuk Wah Wong, and Raymond J. Mooney.
2005. Learning to transform natural to formal lan-
guages. In Proc. AAAI-2005, pages 1062?1068, Pitts-
burgh, PA, July.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Proc.
of ACL-08, pages 595?603, Columbus, Ohio, June.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini, and Chris Watkins. 2002. Text classifica-
tion using string kernels. Journal of Machine Learning
Research, 2:419?444.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proc. of
HLT/EMNLP-05, pages 523?530, Vancouver, BC.
Alessandro Moschitti, Silvia Quarteroni, Roberto Basili,
and Suresh Manandhar. 2007. Exploiting syntactic
and shallow semantic kernels for question answer clas-
sification. In Proc. of ACL-07, pages 776?783, Prague,
Czech Republic, June.
Alessandro Moschitti. 2006a. Making tree kernels prac-
tical for natural language learning. In Proc. of EACL-
06, pages 113?120, Trento, Italy, April.
Alessandro Moschitti. 2006b. Syntactic kernels for natu-
ral language learning: the semantic role labeling case.
In Proc. of HLT/NAACL-06, short papers, pages 97?
100, New York City, USA, June.
Juho Rousu and John Shawe-Taylor. 2005. Efficient
computation of gapped substring kernels on large al-
phabets. Journal of Machine Learning Research,
6:1323?1344.
Libin Shen, Anoop Sarkar, and Aravind Joshi. 2003. Us-
ing ltag based features in parse reranking. In Proc. of
EMNLP-2003, pages 89?96, Sapporo, Japan, July.
408
Kristina Toutanova, Penka Markova, and Christopher
Manning. 2004. The leaf projection path view
of parse trees: Exploring string kernels for HPSG
parse selection. In Proc. EMNLP-04, pages 166?173,
Barcelona, Spain, July.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
John Wiley & Sons.
Qin Iris Wang, Dale Schuurmans, and Dekang Lin. 2008.
Semi-supervised convex training for dependency pars-
ing. In Proceedings of ACL-08: HLT, pages 532?540,
Columbus, Ohio, June.
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel
methods for relation extraction. Journal of Machine
Learning Research, 3:1083?1106.
Min Zhang, Wanxiang Che, Aiti Aw, Chew Lim Tan,
Guodong Zhou, Ting Liu, and Sheng Li. 2007. A
grammar-driven convolution tree kernel for semantic
role classification. In Proc. of ACL-2007, pages 200?
207, Prague, Czech Republic, June.
409
Proceedings of NAACL HLT 2007, Companion Volume, pages 81?84,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Semi-Supervised Learning for Semantic Parsing
using Support Vector Machines
Rohit J. Kate and Raymond J. Mooney
Department of Computer Sciences
The University of Texas at Austin
1 University Station C0500
Austin, TX 78712-0233, USA
frjkate,mooneyg@cs.utexas.edu
Abstract
We present a method for utilizing unan-
notated sentences to improve a semantic
parser which maps natural language (NL)
sentences into their formal meaning rep-
resentations (MRs). Given NL sentences
annotated with their MRs, the initial su-
pervised semantic parser learns the map-
ping by training Support Vector Machine
(SVM) classifiers for every production in
the MR grammar. Our new method ap-
plies the learned semantic parser to the
unannotated sentences and collects unla-
beled examples which are then used to
retrain the classifiers using a variant of
transductive SVMs. Experimental results
show the improvements obtained over
the purely supervised parser, particularly
when the annotated training set is small.
1 Introduction
Semantic parsing is the task of mapping a natu-
ral language (NL) sentence into a complete, for-
mal meaning representation (MR) which a computer
program can execute to perform some task, like
answering database queries or controlling a robot.
These MRs are expressed in domain-specific unam-
biguous formal meaning representation languages
(MRLs). Given a training corpus of NL sentences
annotated with their correct MRs, the goal of a learn-
ing system for semantic parsing is to induce an ef-
ficient and accurate semantic parser that can map
novel sentences into their correct MRs.
Several learning systems have been developed for
semantic parsing, many of them recently (Zelle and
Mooney, 1996; Zettlemoyer and Collins, 2005; Ge
and Mooney, 2005; Kate and Mooney, 2006). These
systems use supervised learning methods which
only utilize annotated NL sentences. However, it
requires considerable human effort to annotate sen-
tences. In contrast, unannotated NL sentences are
usually easily available. Semi-supervised learning
methods utilize cheaply available unannotated data
during training along with annotated data and of-
ten perform better than purely supervised learning
methods trained on the same amount of annotated
data (Chapelle et al, 2006). In this paper we present,
to our knowledge, the first semi-supervised learning
system for semantic parsing.
We modify KRISP, a supervised learning sys-
tem for semantic parsing presented in (Kate and
Mooney, 2006), to make a semi-supervised system
we call SEMISUP-KRISP. Experiments on a real-
world dataset show the improvements SEMISUP-
KRISP obtains over KRISP by utilizing unannotated
sentences.
2 Background
This section briefly provides background needed for
describing our approach to semi-supervised seman-
tic parsing.
2.1 KRISP: The Supervised Semantic Parsing
Learning System
KRISP (Kernel-based Robust Interpretation for Se-
mantic Parsing) (Kate and Mooney, 2006) is a su-
pervised learning system for semantic parsing which
81
takes NL sentences paired with their MRs as train-
ing data. The productions of the formal MRL
grammar are treated like semantic concepts. For
each of these productions, a Support-Vector Ma-
chine (SVM) (Cristianini and Shawe-Taylor, 2000)
classifier is trained using string similarity as the ker-
nel (Lodhi et al, 2002). Each classifier can then
estimate the probability of any NL substring rep-
resenting the semantic concept for its production.
During semantic parsing, the classifiers are called to
estimate probabilities on different substrings of the
sentence to compositionally build the most probable
meaning representation (MR) of the sentence.
KRISP trains the classifiers used in semantic pars-
ing iteratively. In each iteration, for every produc-
tion  in the MRL grammar, KRISP collects pos-
itive and negative examples. In the first iteration,
the set of positive examples for production  con-
tains all sentences whose corresponding MRs use
the production  in their parse trees. The set of neg-
ative examples includes all of the other training sen-
tences. Using these positive and negative examples,
an SVM classifier is trained for each production 
using a string kernel. In subsequent iterations, the
parser learned from the previous iteration is applied
to the training examples and more refined positive
and negative examples, which are more specific sub-
strings within the sentences, are collected for train-
ing. Iterations are continued until the classifiers con-
verge, analogous to iterations in EM (Dempster et
al., 1977). Experimentally, KRISP compares favor-
ably to other existing semantic parsing systems and
is particularly robust to noisy training data (Kate and
Mooney, 2006).
2.2 Transductive SVMs
SVMs (Cristianini and Shawe-Taylor, 2000) are
state-of-the-art machine learning methods for clas-
sification. Given positive and negative training ex-
amples in some vector space, an SVM finds the
maximum-margin hyperplane which separates them.
Maximizing the margin prevents over-fitting in very
high-dimensional data which is typical in natural
language processing and thus leads to better general-
ization performance on test examples. When the un-
labeled test examples are also available during train-
ing, a transductive framework for learning (Vapnik,
1998) can further improve the performance on the
test examples.
Transductive SVMs were introduced in
(Joachims, 1999). The key idea is to find the
labeling of the test examples that results in the
maximum-margin hyperplane that separates the
positive and negative examples of both the training
and the test data. This is achieved by including
variables in the SVM?s objective function repre-
senting labels of the test examples. Finding the
exact solution to the resulting optimization problem
is intractable, however Joachims (1999) gives an
approximation algorithm for it. One drawback of
his algorithm is that it requires the proportion of
positive and negative examples in the test data be
close to the proportion in the training data, which
may not always hold, particularly when the training
data is small. Chen et al (2003) present another
approximation algorithm which we use in our
system because it does not require this assumption.
More recently, new optimization methods have been
used to scale-up transductive SVMs to large data
sets (Collobert et al, 2006), however we did not
face scaling problems in our current experiments.
Although transductive SVMs were originally de-
signed to improve performance on the test data by
utilizing its availability during training, they can also
be directly used in a semi-supervised setting (Ben-
nett and Demiriz, 1999) where unlabeled data is
available during training that comes from the same
distribution as the test data but is not the actual data
on which the classifier is eventually to be tested.
This framework is more realistic in the context of se-
mantic parsing where sentences must be processed
in real-time and it is not practical to re-train the
parser transductively for every new test sentence. In-
stead of using an alternative semi-supervised SVM
algorithm, we preferred to use a transductive SVM
algorithm (Chen et al, 2003) in a semi-supervised
manner, since it is easily implemented on top of an
existing SVM system.
3 Semi-Supervised Semantic Parsing
We modified the existing supervised system KRISP,
described in section 2.1, to incorporate semi-
supervised learning. Supervised learning in KRISP
involves training SVM classifiers on positive and
negative examples that are substrings of the anno-
82
function TRAIN SEMISUP KRISP(Annotated corpus A = f(s
i
;m
i
)ji = 1::Ng, MRL grammar G,
Unannotated sentences T = ft
i
ji = 1::Mg)
C  fC

j 2 Gg = TRAIN KRISP(A,G) // classifiers obtained by training KRISP
Let
P = fp

= Set of positive examples used in training C

j 2 Gg
N = fn

= Set of negative examples used in training C

j 2 Gg
U = fu

= j 2 Gg // set of unlabeled examples for each production, initially all empty
for i = 1 to M do
fu
i

j 2 Gg =COLLECT CLASSIFIER CALLS(PARSE(t
i
; C))
U = fu

= u

[ u
i

j 2 Gg
for each  2 G do
C

=TRANSDUCTIVE SVM TRAIN(p

; n

; u

) // retrain classifiers utilizing unlabeled examples
return classifiers C = fC

j 2 Gg
Figure 1: SEMISUP-KRISP?s training algorithm
tated sentences. In order to perform semi-supervised
learning, these classifiers need to be given appropri-
ate unlabeled examples. The key question is: Which
substrings of the unannotated sentences should be
given as unlabeled examples to which productions?
classifiers? Giving all substrings of the unannotated
sentences as unlabeled examples to all of the clas-
sifiers would lead to a huge number of unlabeled
examples that would not conform to the underly-
ing distribution of classes each classifier is trying to
separate. SEMISUP-KRISP?s training algorithm, de-
scribed below and shown in Figure 1, addresses this
issue.
The training algorithm first runs KRISP?s exist-
ing training algorithm and obtains SVM classifiers
for every production in the MRL grammar. Sets of
positive and negative examples that were used for
training the classifiers in the last iteration are col-
lected for each production. Next, the learned parser
is applied to the unannotated sentences. During the
parsing of each sentence, whenever a classifier is
called to estimate the probability of a substring rep-
resenting the semantic concept for its production,
that substring is saved as an unlabeled example for
that classifier. These substrings are representative of
the examples that the classifier will actually need to
handle during testing. Note that the MRs obtained
from parsing the unannotated sentences do not play
a role during training since it is unknown whether
or not they are correct. These sets of unlabeled ex-
amples for each production, along with the sets of
positive and negative examples collected earlier, are
then used to retrain the classifiers using transductive
SVMs. The retrained classifiers are finally returned
and used in the final semantic parser.
4 Experiments
We compared the performance of SEMISUP-KRISP
and KRISP in the GEOQUERY domain for semantic
parsing in which the MRL is a functional language
used to query a U.S. geography database (Kate et
al., 2005). This domain has been used in most of
the previous work. The original corpus contains 250
NL queries collected from undergraduate students
and annotated with their correct MRs (Zelle and
Mooney, 1996). Later, 630 additional NL queries
were collected from real users of a web-based inter-
face and annotated (Tang and Mooney, 2001). We
used this data as unannotated sentences in our cur-
rent experiments. We also collected an additional
407 queries from the same interface, making a total
of 1; 037 unannotated sentences.
The systems were evaluated using standard 10-
fold cross validation. All the unannotated sentences
were used for training in each fold. Performance
was measured in terms of precision (the percent-
age of generated MRs that were correct) and recall
(the percentage of all sentences for which correct
MRs were obtained). An output MR is considered
correct if and only if the resulting query retrieves
the same answer as the correct MR when submit-
ted to the database. Since the systems assign confi-
dences to the MRs they generate, the entire range of
the precision-recall trade-off can be obtained for a
system by measuring precision and recall at various
confidence levels. We present learning curves for the
best F-measure (harmonic mean of precision and re-
83
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  20  40  60  80  100  120  140  160  180  200  220  240
Be
st
 F
-m
ea
su
re
No. of annotated training sentences
SEMISUP-KRISP
KRISP
GEOBASE
Figure 2: Learning curves for the best F-measures
on the GEOQUERY corpus.
call) obtained across the precision-recall trade-off as
the amount of annotated training data is increased.
Figure 2 shows the results for both systems.
The results clearly show the improvement
SEMISUP-KRISP obtains over KRISP by utilizing
unannotated sentences, particularly when the num-
ber of annotated sentences is small. We also show
the performance of a hand-built semantic parser
GEOBASE (Borland International, 1988) for com-
parison. From the figure, it can be seen that, on
average, KRISP achieves the same performance as
GEOBASE when it is given 126 annotated examples,
while SEMISUP-KRISP reaches this level given only
94 annotated examples, a 25:4% savings in human-
annotation effort.
5 Conclusions
This paper has presented a semi-supervised ap-
proach to semantic parsing. Our method utilizes
unannotated sentences during training by extracting
unlabeled examples for the SVM classifiers it uses to
perform semantic parsing. These classifiers are then
retrained using transductive SVMs. Experimental
results demonstrated that this exploitation of unla-
beled data significantly improved the accuracy of the
resulting parsers when only limited supervised data
was provided.
Acknowledgments
This research was supported by a Google research
grant. The experiments were run on the Mastodon
cluster provided by NSF grant EIA-0303609.
References
K. Bennett and A. Demiriz. 1999. Semi-supervised support
vector machines. Advances in Neural Information Process-
ing Systems, 11:368?374.
Borland International. 1988. Turbo Prolog 2.0 Reference
Guide. Borland International, Scotts Valley, CA.
O. Chapelle, B. Scho?lkopf, and A. Zien, editors. 2006. Semi-
Supervised Learning. MIT Press, Cambridge, MA.
Y. Chen, G. Wang, and S. Dong. 2003. Learning with progres-
sive transductive support vector machine. Pattern Recogni-
tion Letters, 24:1845?1855.
R. Collobert, F. Sinz, J. Weston, and L. Bottou. 2006. Large
scale transductive SVMs. Journal of Machine Learning Re-
search, 7(Aug):1687?1712.
N. Cristianini and J. Shawe-Taylor. 2000. An Introduction to
Support Vector Machines and Other Kernel-based Learning
Methods. Cambridge University Press.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maxi-
mum likelihood from incomplete data via the EM algorithm.
Journal of the Royal Statistical Society B, 39:1?38.
R. Ge and R. J. Mooney. 2005. A statistical semantic parser
that integrates syntax and semantics. In Proc. of CoNLL-05,
pages 9?16, Ann Arbor, MI, July.
T. Joachims. 1999. Transductive inference for text classifica-
tion using support vector machines. In Proc. of ICML-99,
pages 200?209, Bled, Slovenia, June.
R. J. Kate and R. J. Mooney. 2006. Using string-kernels for
learning semantic parsers. In Proc. of COLING/ACL-06,
pages 913?920, Sydney, Australia, July.
R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005. Learning to
transform natural to formal languages. In Proc. of AAAI-05,
pages 1062?1068, Pittsburgh, PA, July.
H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini, and
C. Watkins. 2002. Text classification using string kernels.
Journal of Machine Learning Research, 2:419?444.
L. R. Tang and R. J. Mooney. 2001. Using multiple clause con-
structors in inductive logic programming for semantic pars-
ing. In Proc. of ECML-01, pages 466?477, Freiburg, Ger-
many.
V. N. Vapnik. 1998. Statistical Learning Theory. John Wiley
& Sons.
J. M. Zelle and R. J. Mooney. 1996. Learning to parse database
queries using inductive logic programming. In Proc. of
AAAI-96, pages 1050?1055, Portland, OR, August.
L. S. Zettlemoyer and M. Collins. 2005. Learning to map sen-
tences to logical form: Structured classification with proba-
bilistic categorial grammars. In Proc. of UAI-05, Edinburgh,
Scotland, July.
84
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 913?920,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Using String-Kernels for Learning Semantic Parsers
Rohit J. Kate
Department of Computer Sciences
The University of Texas at Austin
1 University Station C0500
Austin, TX 78712-0233, USA
rjkate@cs.utexas.edu
Raymond J. Mooney
Department of Computer Sciences
The University of Texas at Austin
1 University Station C0500
Austin, TX 78712-0233, USA
mooney@cs.utexas.edu
Abstract
We present a new approach for mapping
natural language sentences to their for-
mal meaning representations using string-
kernel-based classifiers. Our system learns
these classifiers for every production in the
formal language grammar. Meaning repre-
sentations for novel natural language sen-
tences are obtained by finding the most
probable semantic parse using these string
classifiers. Our experiments on two real-
world data sets show that this approach
compares favorably to other existing sys-
tems and is particularly robust to noise.
1 Introduction
Computational systems that learn to transform nat-
ural language sentences into formal meaning rep-
resentations have important practical applications
in enabling user-friendly natural language com-
munication with computers. However, most of the
research in natural language processing (NLP) has
been focused on lower-level tasks like syntactic
parsing, word-sense disambiguation, information
extraction etc. In this paper, we have considered
the important task of doing deep semantic parsing
to map sentences into their computer-executable
meaning representations.
Previous work on learning semantic parsers
either employ rule-based algorithms (Tang and
Mooney, 2001; Kate et al, 2005), or use sta-
tistical feature-based methods (Ge and Mooney,
2005; Zettlemoyer and Collins, 2005; Wong and
Mooney, 2006). In this paper, we present a
novel kernel-based statistical method for learn-
ing semantic parsers. Kernel methods (Cristianini
and Shawe-Taylor, 2000) are particularly suitable
for semantic parsing because it involves mapping
phrases of natural language (NL) sentences to se-
mantic concepts in a meaning representation lan-
guage (MRL). Given that natural languages are so
flexible, there are various ways in which one can
express the same semantic concept. It is difficult
for rule-based methods or even statistical feature-
based methods to capture the full range of NL con-
texts which map to a semantic concept because
they tend to enumerate these contexts. In contrast,
kernel methods allow a convenient mechanism to
implicitly work with a potentially infinite number
of features which can robustly capture these range
of contexts even when the data is noisy.
Our system, KRISP (Kernel-based Robust In-
terpretation for Semantic Parsing), takes NL sen-
tences paired with their formal meaning represen-
tations as training data. The productions of the for-
mal MRL grammar are treated like semantic con-
cepts. For each of these productions, a Support-
Vector Machine (SVM) (Cristianini and Shawe-
Taylor, 2000) classifier is trained using string sim-
ilarity as the kernel (Lodhi et al, 2002). Each
classifier then estimates the probability of the pro-
duction covering different substrings of the sen-
tence. This information is used to compositionally
build a complete meaning representation (MR) of
the sentence.
Some of the previous work on semantic pars-
ing has focused on fairly simple domains, primar-
ily, ATIS (Air Travel Information Service) (Price,
1990) whose semantic analysis is equivalent to fill-
ing a single semantic frame (Miller et al, 1996;
Popescu et al, 2004). In this paper, we have
tested KRISP on two real-world domains in which
meaning representations are more complex with
richer predicates and nested structures. Our exper-
iments demonstrate that KRISP compares favor-
913
NL: ?If the ball is in our goal area then our player 1 should
intercept it.?
CLANG: ((bpos (goal-area our))
(do our {1} intercept))
Figure 1: An example of an NL advice and its
CLANG MR.
ably to other existing systems and is particularly
robust to noise.
2 Semantic Parsing
We call the process of mapping natural language
(NL) utterances into their computer-executable
meaning representations (MRs) as semantic pars-
ing. These MRs are expressed in formal languages
which we call meaning representation languages
(MRLs). We assume that all MRLs have deter-
ministic context free grammars, which is true for
almost all computer languages. This ensures that
every MR will have a unique parse tree. A learn-
ing system for semantic parsing is given a training
corpus of NL sentences paired with their respec-
tive MRs from which it has to induce a semantic
parser which can map novel NL sentences to their
correct MRs.
Figure 1 shows an example of an NL sentence
and its MR from the CLANG domain. CLANG
(Chen et al, 2003) is the standard formal coach
language in which coaching advice is given to soc-
cer agents which compete on a simulated soccer
field in the RoboCup 1 Coach Competition. In the
MR of the example, bpos stands for ?ball posi-
tion?.
The second domain we have considered is the
GEOQUERY domain (Zelle and Mooney, 1996)
which is a query language for a small database of
about 800 U.S. geographical facts. Figure 2 shows
an NL query and its MR form in a functional query
language. The parse of the functional query lan-
guage is also shown along with the involved pro-
ductions. This example is also used later to illus-
trate how our system does semantic parsing. The
MR in the functional query language can be read
as if processing a list which gets modified by vari-
ous functions. From the innermost expression go-
ing outwards it means: the state of Texas, the list
containing all the states next to the state of Texas
and the list of all the rivers which flow through
these states. This list is finally returned as the an-
swer.
1http://www.robocup.org/
NL: ?Which rivers run through the states bordering Texas??
Functional query language:
answer(traverse(next to(stateid(?texas?))))
Parse tree of the MR in functional query language:
ANSWER
answer RIVER
TRAVERSE
traverse
STATE
NEXT TO
next to
STATE
STATEID
stateid ?texas?
Productions:
ANSWER ? answer(RIVER) RIVER ? TRAVERSE(STATE)
STATE ? NEXT TO(STATE) STATE ? STATEID
TRAVERSE ? traverse NEXT TO ? next to
STATEID ? stateid(?texas?)
Figure 2: An example of an NL query and its MR
in a functional query language with its parse tree.
KRISP does semantic parsing using the notion
of a semantic derivation of an NL sentence. In
the following subsections, we define the seman-
tic derivation of an NL sentence and its probabil-
ity. The task of semantic parsing then is to find
the most probable semantic derivation of an NL
sentence. In section 3, we describe how KRISP
learns the string classifiers that are used to obtain
the probabilities needed in finding the most prob-
able semantic derivation.
2.1 Semantic Derivation
We define a semantic derivation, D, of an NL sen-
tence, s, as a parse tree of an MR (not necessarily
the correct MR) such that each node of the parse
tree also contains a substring of the sentence in
addition to a production. We denote nodes of the
derivation tree by tuples (pi, [i..j]), where pi is its
production and [i..j] stands for the substring s[i..j]
of s (i.e. the substring from the ith word to the jth
word), and we say that the node or its production
covers the substring s[i..j]. The substrings cov-
ered by the children of a node are not allowed to
overlap, and the substring covered by the parent
must be the concatenation of the substrings cov-
ered by its children. Figure 3 shows a semantic
derivation of the NL sentence and the MR parse
which were shown in figure 2. The words are num-
bered according to their position in the sentence.
Instead of non-terminals, productions are shown
in the nodes to emphasize the role of productions
in semantic derivations.
Sometimes, the children of an MR parse tree
914
(ANSWER? answer(RIVER), [1..9])
(RIVER? TRAVERSE(STATE), [1..9])
(TRAVERSE?traverse, [1..4])
which1 rivers2 run3 through4
(STATE? NEXT TO(STATE), [5..9])
(NEXT TO? next to, [5..7])
the5 states6 bordering7
(STATE? STATEID, [8..9])
(STATEID? stateid ?texas?, [8..9])
Texas8 ?9
Figure 3: Semantic derivation of the NL sentence ?Which rivers run through the states bordering Texas??
which gives MR as answer(traverse(next to(stateid(texas)))).
node may not be in the same order as are the sub-
strings of the sentence they should cover in a se-
mantic derivation. For example, if the sentence
was ?Through the states that border Texas which
rivers run??, which has the same MR as the sen-
tence in figure 3, then the order of the children of
the node ?RIVER ? TRAVERSE(STATE)? would
need to be reversed. To accommodate this, a se-
mantic derivation tree is allowed to contain MR
parse tree nodes in which the children have been
permuted.
Note that given a semantic derivation of an NL
sentence, it is trivial to obtain the corresponding
MR simply as the string generated by the parse.
Since children nodes may be permuted, this step
also needs to permute them back to the way they
should be according to the MRL productions. If a
semantic derivation gives the correct MR of the
NL sentence, then we call it a correct semantic
derivation otherwise it is an incorrect semantic
derivation.
2.2 Most Probable Semantic Derivation
Let Ppi(u) denote the probability that a production
pi of the MRL grammar covers the NL substring
u. In other words, the NL substring u expresses
the semantic concept of a production pi with prob-
ability Ppi(u). In the next subsection we will de-
scribe how KRISP obtains these probabilities using
string-kernel based SVM classifiers. Assuming
these probabilities are independent of each other,
the probability of a semantic derivationD of a sen-
tence s is then:
P (D) =
?
(pi,[i..j])?D
Ppi(s[i..j])
The task of the semantic parser is to find the
most probable derivation of a sentence s. This
task can be recursively performed using the no-
tion of a partial derivation En,s[i..j], which stands
for a subtree of a semantic derivation tree with n
as the left-hand-side (LHS) non-terminal of the
root production and which covers s from index
i to j. For example, the subtree rooted at the
node ?(STATE ? NEXT TO(STATE),[5..9])? in
the derivation shown in figure 3 is a partial deriva-
tion which would be denoted as ESTATE,s[5..9].
Note that the derivation D of sentence s is then
simply Estart,s[1..|s|], where start is the start sym-
bol of the MRL?s context free grammar, G.
Our procedure to find the most probable par-
tial derivation E?n,s[i..j] considers all possible sub-
trees whose root production has n as its LHS non-
terminal and which cover s from index i to j.
Mathematically, the most probable partial deriva-
tion E?n,s[i..j] is recursively defined as:
E?n,s[i..j] =
makeTree( argmax
pi = n ? n1..nt ? G,
(p1, .., pt) ?
partition(s[i..j], t)
(Ppi(s[i..j])
?
k=1..t
P (E?nk,pk )))
where partition(s[i..j], t) is a function which re-
turns the set of all partitions of s[i..j] with t el-
ements including their permutations. A parti-
tion of a substring s[i..j] with t elements is a
t?tuple containing t non-overlapping substrings
of s[i..j] which give s[i..j] when concatenated.
For example, (?the states bordering?, ?Texas ??)
is a partition of the substring ?the states bor-
dering Texas ?? with 2 elements. The proce-
duremakeTree(pi, (p1, .., pt)) constructs a partial
derivation tree by making pi as its root production
and making the most probable partial derivation
trees found through the recursion as children sub-
trees which cover the substrings according to the
partition (p1, .., pt).
The most probable partial derivation E?n,s[i..j]
is found using the above equation by trying all
productions pi = n ? n1..nt in G which have
915
n as the LHS, and all partitions with t elements
of the substring s[i..j] (n1 to nt are right-hand-
side (RHS) non-terminals of pi, terminals do not
play any role in this process and are not shown
for simplicity). The most probable partial deriva-
tion E?STATE,s[5..9] for the sentence shown in fig-
ure 3 will be found by trying all the productions
in the grammar with STATE as the LHS, for ex-
ample, one of them being ?STATE ? NEXT TO
STATE?. Then for this sample production, all parti-
tions, (p1, p2), of the substring s[5..9] with two el-
ements will be considered, and the most probable
derivations E?NEXT TO,p1 and E
?
STATE,p2 will be
found recursively. The recursion reaches base
cases when the productions which have n on the
LHS do not have any non-terminal on the RHS or
when the substring s[i..j] becomes smaller than
the length t.
According to the equation, a production pi ? G
and a partition (p1, .., pt) ? partition(s[i..j], t)
will be selected in constructing the most probable
partial derivation. These will be the ones which
maximize the product of the probability of pi cov-
ering the substring s[i..j] with the product of prob-
abilities of all the recursively found most proba-
ble partial derivations consistent with the partition
(p1, .., pt).
A naive implementation of the above recursion
is computationally expensive, but by suitably ex-
tending the well known Earley?s context-free pars-
ing algorithm (Earley, 1970), it can be imple-
mented efficiently. The above task has some re-
semblance to probabilistic context-free grammar
(PCFG) parsing for which efficient algorithms are
available (Stolcke, 1995), but we note that our task
of finding the most probable semantic derivation
differs from PCFG parsing in two important ways.
First, the probability of a production is not inde-
pendent of the sentence but depends on which sub-
string of the sentence it covers, and second, the
leaves of the tree are not individual terminals of
the grammar but are substrings of words of the NL
sentence. The extensions needed for Earley?s al-
gorithm are straightforward and are described in
detail in (Kate, 2005) but due to space limitation
we do not describe them here. Our extended Ear-
ley?s algorithm does a beam search and attempts
to find the ? (a parameter) most probable semantic
derivations of an NL sentence s using the probabil-
ities Ppi(s[i..j]). To make this search faster, it uses
a threshold, ?, to prune low probability derivation
trees.
3 KRISP?s Training Algorithm
In this section, we describe how KRISP learns
the classifiers which give the probabilities Ppi(u)
needed for semantic parsing as described in the
previous section. Given the training corpus of
NL sentences paired with their MRs {(si,mi)|i =
1..N}, KRISP first parses the MRs using the MRL
grammar, G. We represent the parse of MR, mi,
by parse(mi).
Figure 4 shows pseudo-code for KRISP?s train-
ing algorithm. KRISP learns a semantic parser it-
eratively, each iteration improving upon the parser
learned in the previous iteration. In each itera-
tion, for every production pi of G, KRISP collects
positive and negative example sets. In the first
iteration, the set P(pi) of positive examples for
production pi contains all sentences, si, such that
parse(mi) uses the production pi. The set of nega-
tive examples,N (pi), for production pi includes all
of the remaining training sentences. Using these
positive and negative examples, an SVM classi-
fier 2, Cpi, is trained for each production pi using
a normalized string subsequence kernel. Follow-
ing the framework of Lodhi et al (2002), we de-
fine a kernel between two strings as the number of
common subsequences they share. One difference,
however, is that their strings are over characters
while our strings are over words. The more the
two strings share, the greater the similarity score
will be.
Normally, SVM classifiers only predict the class
of the test example but one can obtain class proba-
bility estimates by mapping the distance of the ex-
ample from the SVM?s separating hyperplane to
the range [0,1] using a learned sigmoid function
(Platt, 1999). The classifier Cpi then gives us the
probabilities Ppi(u). We represent the set of these
classifiers by C = {Cpi|pi ? G}.
Next, using these classifiers, the extended
Earley?s algorithm, which we call EX-
TENDED EARLEY in the pseudo-code, is invoked
to obtain the ? best semantic derivations for each
of the training sentences. The procedure getMR
returns the MR for a semantic derivation. At this
point, for many training sentences, the resulting
most-probable semantic derivation may not give
the correct MR. Hence, next, the system collects
more refined positive and negative examples
to improve the result in the next iteration. It
2We use the LIBSVM package available at: http://
www.csie.ntu.edu.tw/?cjlin/libsvm/
916
function TRAIN KRISP(training corpus {(si,mi)|i = 1..N}, MRL grammar G)
for each pi ?G // collect positive and negative examples for the first iteration
for i = 1 to N do
if pi is used in parse(mi) then
include si in P(pi)
else include si in N (pi)
for iteration = 1 to MAX ITR do
for each pi ?G do
Cpi = trainSVM(P(pi),N (pi)) // SVM training
for each pi ?G P(pi) = ? // empty the positive examples, accumulate negatives though
for i = 1 to N do
D =EXTENDED EARLEY(si, G, P ) // obtain best derivations
if 6 ? d ? D such that parse(mi) = getMR(d) then
D = D ? EXTENDED EARLEY CORRECT(si, G, P,mi) // if no correct derivation then force to find one
d? = argmaxd?D&getMR(d)=parse(mi) P (d)
COLLECT POSITIVES(d?) // collect positives from maximum probability correct derivation
for each d ? D do
if P (d) > P (d?) and getMR(d) 6= parse(mi) then
// collect negatives from incorrect derivation with larger probability than the correct one
COLLECT NEGATIVES(d, d?)
return classifiers C = {Cpi|pi ? G}
Figure 4: KRISP?s training algorithm
is also possible that for some sentences, none
of the obtained ? derivations give the correct
MR. But as will be described shortly, the most
probable derivation which gives the correct MR is
needed to collect positive and negative examples
for the next iteration. Hence in these cases, a
version of the extended Earley?s algorithm, EX-
TENDED EARLEY CORRECT, is invoked which
also takes the correct MR as an argument and
returns the best ? derivations it finds, all of
which give the correct MR. This is easily done by
making sure all subtrees derived in the process are
present in the parse of the correct MR.
From these derivations, positive and negative
examples are collected for the next iteration. Pos-
itive examples are collected from the most prob-
able derivation which gives the correct MR, fig-
ure 3 showed an example of a derivation which
gives the correct MR. At each node in such a
derivation, the substring covered is taken as a pos-
itive example for its production. Negative exam-
ples are collected from those derivations whose
probability is higher than the most probable cor-
rect derivation but which do not give the cor-
rect MR. Figure 5 shows an example of an in-
correct derivation. Here the function ?next to?
is missing from the MR it produces. The fol-
lowing procedure is used to collect negative ex-
amples from incorrect derivations. The incorrect
derivation and the most probable correct deriva-
tion are traversed simultaneously starting from the
root using breadth-first traversal. The first nodes
where their productions differ is detected, and all
of the words covered by the these nodes (in both
derivations) are marked. In the correct and incor-
rect derivations shown in figures 3 and 5 respec-
tively, the first nodes where the productions differ
are ?(STATE ? NEXT TO(STATE), [5..9])? and
?(STATE ? STATEID, [8..9])?. Hence, the union
of words covered by them: 5 to 9 (?the states
bordering Texas??), will be marked. For each
of these marked words, the procedure considers
all of the productions which cover it in the two
derivations. The nodes of the productions which
cover a marked word in the incorrect derivation
but not in the correct derivation are used to col-
lect negative examples. In the example, the node
?(TRAVERSE?traverse,[1..7])? will be used
to collect a negative example (i.e. the words 1
to 7 ??which rivers run through the states border-
ing? will be a negative example for the produc-
tion TRAVERSE?traverse) because the pro-
duction covers the marked words ?the?, ?states?
and ?bordering? in the incorrect derivation but
not in the correct derivation. With this as a neg-
ative example, hopefully in the next iteration, the
probability of this derivation will decrease signif-
icantly and drop below the probability of the cor-
rect derivation.
In each iteration, the positive examples from
the previous iteration are first removed so that
new positive examples which lead to better cor-
rect derivations can take their place. However,
negative examples are accumulated across iter-
ations for better accuracy because negative ex-
amples from each iteration only lead to incor-
rect derivations and it is always good to include
them. To further increase the number of nega-
tive examples, every positive example for a pro-
duction is also included as a negative example for
all the other productions having the same LHS.
After a specified number of MAX ITR iterations,
917
(ANSWER? answer(RIVER), [1..9])
(RIVER? TRAVERSE(STATE), [1..9])
(TRAVERSE?traverse, [1..7])
Which1 rivers2 run3 through4 the5 states6 bordering7
(STATE? STATEID, [8..9])
(STATEID? stateid texas, [8..9])
Texas8 ?9
Figure 5: An incorrect semantic derivation of the NL sentence ?Which rivers run through the states
bordering Texas?? which gives the incorrect MR answer(traverse(stateid(texas))).
the trained classifiers from the last iteration are
returned. Testing involves using these classifiers
to generate the most probable derivation of a test
sentence as described in the subsection 2.2, and
returning its MR.
The MRL grammar may contain productions
corresponding to constants of the domain, for e.g.,
state names like ?STATEID ? ?texas??, or river
names like ?RIVERID ? ?colorado?? etc. Our
system allows the user to specify such produc-
tions as constant productions giving the NL sub-
strings, called constant substrings, which directly
relate to them. For example, the user may give
?Texas? as the constant substring for the produc-
tion ?STATEID ? ?texas?. Then KRISP does
not learn classifiers for these constant productions
and instead decides if they cover a substring of the
sentence or not by matching it with the provided
constant substrings.
4 Experiments
4.1 Methodology
KRISP was evaluated on CLANG and GEOQUERY
domains as described in section 2. The CLANG
corpus was built by randomly selecting 300 pieces
of coaching advice from the log files of the 2003
RoboCup Coach Competition. These formal ad-
vice instructions were manually translated into
English (Kate et al, 2005). The GEOQUERY cor-
pus contains 880 English queries collected from
undergraduates and from real users of a web-based
interface (Tang and Mooney, 2001). These were
manually translated into their MRs. The average
length of an NL sentence in the CLANG corpus
is 22.52 words while in the GEOQUERY corpus it
is 7.48 words, which indicates that CLANG is the
harder corpus. The average length of the MRs is
13.42 tokens in the CLANG corpus while it is 6.46
tokens in the GEOQUERY corpus.
KRISP was evaluated using standard 10-fold
cross validation. For every test sentence, only the
best MR corresponding to the most probable se-
mantic derivation is considered for evaluation, and
its probability is taken as the system?s confidence
in that MR. Since KRISP uses a threshold, ?, to
prune low probability derivation trees, it some-
times may fail to return any MR for a test sen-
tence. We computed the number of test sentences
for which KRISP produced MRs, and the number
of these MRs that were correct. For CLANG, an
output MR is considered correct if and only if it
exactly matches the correct MR. For GEOQUERY,
an output MR is considered correct if and only if
the resulting query retrieves the same answer as
the correct MR when submitted to the database.
Performance was measured in terms of precision
(the percentage of generated MRs that were cor-
rect) and recall (the percentage of all sentences for
which correct MRs were obtained).
In our experiments, the threshold ? was fixed
to 0.05 and the beam size ? was 20. These pa-
rameters were found through pilot experiments.
The maximum number of iterations (MAX ITR) re-
quired was only 3, beyond this we found that the
system only overfits the training corpus.
We compared our system?s performance with
the following existing systems: the string and tree
versions of SILT (Kate et al, 2005), a system that
learns transformation rules relating NL phrases
to MRL expressions; WASP (Wong and Mooney,
2006), a system that learns transformation rules
using statistical machine translation techniques;
SCISSOR (Ge and Mooney, 2005), a system that
learns an integrated syntactic-semantic parser; and
CHILL (Tang and Mooney, 2001) an ILP-based
semantic parser. We also compared with the
CCG-based semantic parser by Zettlemoyer et al
(2005), but their results are available only for the
GEO880 corpus and their experimental set-up is
also different from ours. Like KRISP, WASP and
SCISSOR also give confidences to the MRs they
generate which are used to plot precision-recall
curves by measuring precisions and recalls at vari-
918
 50
 60
 70
 80
 90
 100
 0  10  20  30  40  50  60  70  80  90  100
Pr
ec
is
io
n
Recall
KRISP
WASP
SCISSOR
SILT-tree
SILT-string
Figure 6: Results on the CLANG corpus.
 50
 60
 70
 80
 90
 100
 0  10  20  30  40  50  60  70  80  90  100
Pr
ec
is
io
n
Recall
KRISP
WASP
SCISSOR
SILT-tree
SILT-string
CHILL
Zettlemoyer et al (2005)
Figure 7: Results on the GEOQUERY corpus.
ous confidence levels. The results of the other sys-
tems are shown as points on the precision-recall
graph.
4.2 Results
Figure 6 shows the results on the CLANG cor-
pus. KRISP performs better than either version
of SILT and performs comparable to WASP. Al-
though SCISSOR gives less precision at lower re-
call values, it gives much higher maximum recall.
However, we note that SCISSOR requires more su-
pervision for the training corpus in the form of se-
mantically annotated syntactic parse trees for the
training sentences. CHILL could not be run be-
yond 160 training examples because its Prolog im-
plementation runs out of memory. For 160 training
examples it gave 49.2% precision with 12.67% re-
call.
Figure 7 shows the results on the GEOQUERY
corpus. KRISP achieves higher precisions than
WASP on this corpus. Overall, the results show
that KRISP performs better than deterministic
rule-based semantic parsers like CHILL and SILT
and performs comparable to other statistical se-
mantic parsers like WASP and SCISSOR.
4.3 Experiments with Other Natural
Languages
We have translations of a subset of the GEOQUERY
corpus with 250 examples (GEO250 corpus) in
 50
 60
 70
 80
 90
 100
 0  10  20  30  40  50  60  70  80  90  100
Pr
ec
is
io
n
Recall
English
Japanese
Spanish
Turkish
Figure 8: Results of KRISP on the GEO250 corpus
for different natural languages.
three other natural languages: Spanish, Turkish
and Japanese. Since KRISP?s learning algorithm
does not use any natural language specific knowl-
edge, it is directly applicable to other natural lan-
guages. Figure 8 shows that KRISP performs com-
petently on other languages as well.
4.4 Experiments with Noisy NL Sentences
Any real world application in which semantic
parsers would be used to interpret natural language
of a user is likely to face noise in the input. If the
user is interacting through spontaneous speech and
the input to the semantic parser is coming form
the output of a speech recognition system then
there are many ways in which noise could creep
in the NL sentences: interjections (like um?s and
ah?s), environment noise (like door slams, phone
rings etc.), out-of-domain words, grammatically
ill-formed utterances etc. (Zue and Glass, 2000).
As opposed to the other systems, KRISP?s string-
kernel-based semantic parsing does not use hard-
matching rules and should be thus more flexible
and robust to noise. We tested this hypothesis by
running experiments on data which was artificially
corrupted with simulated speech recognition er-
rors.
The interjections, environment noise etc. are
likely to be recognized as real words by a speech
recognizer. To simulate this, after every word in
a sentence, with some probability Padd, an ex-
tra word is added which is chosen with proba-
bility proportional to its word frequency found in
the British National Corpus (BNC), a good rep-
resentative sample of English. A speech recog-
nizer may sometimes completely fail to detect
words, so with a probability of Pdrop a word is
sometimes dropped. A speech recognizer could
also introduce noise by confusing a word with a
high frequency phonetically close word. We sim-
919
 0
 20
 40
 60
 80
 100
 0  1  2  3  4  5
F-
m
ea
su
re
Noise level
KRISP
WASP
SCISSOR
Figure 9: Results on the CLANG corpus with in-
creasing amounts of noise in the test sentences.
ulate this type of noise by substituting a word in
the corpus by another word, w, with probability
ped(w)?P (w), where p is a parameter, ed(w) isw?s
edit distance (Levenshtein, 1966) from the original
word and P (w) is w?s probability proportional to
its word frequency. The edit distance which calcu-
lates closeness between words is character-based
rather than based on phonetics, but this should not
make a significant difference in the experimental
results.
Figure 9 shows the results on the CLANG cor-
pus with increasing amounts of noise, from level
0 to level 4. The noise level 0 corresponds to no
noise. The noise parameters, Padd and Pdrop, were
varied uniformly from being 0 at level 0 and 0.1 at
level 4, and the parameter p was varied uniformly
from being 0 at level 0 and 0.01 at level 4. We
are showing the best F-measure (harmonic mean
of precision and recall) for each system at differ-
ent noise levels. As can be seen, KRISP?s perfor-
mance degrades gracefully in the presence of noise
while other systems? performance degrade much
faster, thus verifying our hypothesis. In this exper-
iment, only the test sentences were corrupted, we
get qualitatively similar results when both training
and test sentences are corrupted. The results are
also similar on the GEOQUERY corpus.
5 Conclusions
We presented a new kernel-based approach to
learn semantic parsers. SVM classifiers based on
string subsequence kernels are trained for each of
the productions in the meaning representation lan-
guage. These classifiers are then used to com-
positionally build complete meaning representa-
tions of natural language sentences. We evaluated
our system on two real-world corpora. The re-
sults showed that our system compares favorably
to other existing systems and is particularly robust
to noise.
Acknowledgments
This research was supported by Defense Ad-
vanced Research Projects Agency under grant
HR0011-04-1-0007.
References
Mao Chen et al 2003. Users manual: RoboCup soccer server manual for soc-
cer server version 7.07 and later. Available at http://sourceforge.
net/projects/sserver/.
Nello Cristianini and John Shawe-Taylor. 2000. An Introduction to Support
Vector Machines and Other Kernel-based Learning Methods. Cambridge
University Press.
Jay Earley. 1970. An efficient context-free parsing algorithm. Communica-
tions of the Association for Computing Machinery, 6(8):451?455.
R. Ge and R. J. Mooney. 2005. A statistical semantic parser that integrates
syntax and semantics. In Proc. of 9th Conf. on Computational Natural
Language Learning (CoNLL-2005), pages 9?16, Ann Arbor, MI, July.
R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005. Learning to transform natural
to formal languages. In Proc. of 20th Natl. Conf. on Artificial Intelligence
(AAAI-2005), pages 1062?1068, Pittsburgh, PA, July.
Rohit J. Kate. 2005. A kernel-based approach to learning semantic parsers.
Technical Report UT-AI-05-326, Artificial Intelligence Lab, University of
Texas at Austin, Austin, TX, November.
V. I. Levenshtein. 1966. Binary codes capable of correcting insertions and
reversals. Soviet Physics Doklady, 10(8):707?710, February.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cristianini, and Chris
Watkins. 2002. Text classification using string kernels. Journal of Ma-
chine Learning Research, 2:419?444.
Scott Miller, David Stallard, Robert Bobrow, and Richard Schwartz. 1996. A
fully statistical approach to natural language interfaces. In Proc. of the 34th
Annual Meeting of the Association for Computational Linguistics (ACL-
96), pages 55?61, Santa Cruz, CA.
John C. Platt. 1999. Probabilistic outputs for support vector machines and
comparisons to regularized likelihood methods. In Alexander J. Smola, Pe-
ter Bartlett, Bernhard Scho?lkopf, and Dale Schuurmans, editors, Advances
in Large Margin Classifiers, pages 185?208. MIT Press.
Ana-Maria Popescu, Alex Armanasu, Oren Etzioni, David Ko, and Alexander
Yates. 2004. Modern natural language interfaces to databases: Composing
statistical parsing with semantic tractability. In Proc. of 20th Intl. Conf. on
Computational Linguistics (COLING-04), Geneva, Switzerland, August.
Patti J. Price. 1990. Evaluation of spoken language systems: The ATIS do-
main. In Proc. of 3rd DARPA Speech and Natural Language Workshop,
pages 91?95, June.
Andreas Stolcke. 1995. An efficient probabilistic context-free parsing al-
gorithm that computes prefix probabilities. Computational Linguistics,
21(2):165?201.
L. R. Tang and R. J. Mooney. 2001. Using multiple clause constructors in
inductive logic programming for semantic parsing. In Proc. of the 12th
European Conf. on Machine Learning, pages 466?477, Freiburg, Germany.
Yuk Wah Wong and Raymond J. Mooney. 2006. Learning for semantic pars-
ing with statistical machine translation. In Proc. of Human Language Tech-
nology Conf. / North American Association for Computational Linguistics
Annual Meeting (HLT/NAACL-2006), New York City, NY. To appear.
John M. Zelle and Raymond J. Mooney. 1996. Learning to parse database
queries using inductive logic programming. In Proc. of 13th Natl. Conf. on
Artificial Intelligence (AAAI-96), pages 1050?1055, Portland, OR, August.
Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to
logical form: Structured classification with probabilistic categorial gram-
mars. In Proc. of 21th Conf. on Uncertainty in Artificial Intelligence (UAI-
2005), Edinburgh, Scotland, July.
Victor W. Zue and James R. Glass. 2000. Conversational interfaces: Advances
and challenges. In Proc. of the IEEE, volume 88(8), pages 1166?1180.
920
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 546?554,
Beijing, August 2010
Learning to Predict Readability using Diverse Linguistic Features
Rohit J. Kate1 Xiaoqiang Luo2 Siddharth Patwardhan2 Martin Franz2
Radu Florian2 Raymond J. Mooney1 Salim Roukos2 Chris Welty2
1Department of Computer Science
The University of Texas at Austin
{rjkate,mooney}@cs.utexas.edu
2IBM Watson Research Center
{xiaoluo,spatward,franzm,raduf,roukos,welty}@us.ibm.com
Abstract
In this paper we consider the problem of
building a system to predict readability
of natural-language documents. Our sys-
tem is trained using diverse features based
on syntax and language models which are
generally indicative of readability. The
experimental results on a dataset of docu-
ments from a mix of genres show that the
predictions of the learned system are more
accurate than the predictions of naive hu-
man judges when compared against the
predictions of linguistically-trained expert
human judges. The experiments also com-
pare the performances of different learn-
ing algorithms and different types of fea-
ture sets when used for predicting read-
ability.
1 Introduction
An important aspect of a document is whether it
is easily processed and understood by a human
reader as intended by its writer, this is termed
as the document?s readability. Readability in-
volves many aspects including grammaticality,
conciseness, clarity, and lack of ambiguity. Teach-
ers, journalists, editors, and other professionals
routinely make judgements on the readability of
documents. We explore the task of learning to
automatically judge the readability of natural-
language documents.
In a variety of applications it would be useful to
be able to automate readability judgements. For
example, the results of a web-search can be or-
dered taking into account the readability of the
retrieved documents thus improving user satisfac-
tion. Readability judgements can also be used
for automatically grading essays, selecting in-
structional reading materials, etc. If documents
are generated by machines, such as summariza-
tion or machine translation systems, then they are
prone to be less readable. In such cases, a read-
ability measure can be used to automatically fil-
ter out documents which have poor readability.
Even when the intended consumers of text are
machines, for example, information extraction or
knowledge extraction systems, a readability mea-
sure can be used to filter out documents of poor
readability so that the machine readers will not ex-
tract incorrect information because of ambiguity
or lack of clarity in the documents.
As part of the DARPA Machine Reading Pro-
gram (MRP), an evaluation was designed and con-
ducted for the task of rating documents for read-
ability. In this evaluation, 540 documents were
rated for readability by both experts and novice
human subjects. Systems were evaluated based on
whether they were able to match expert readabil-
ity ratings better than novice raters. Our system
learns to match expert readability ratings by em-
ploying regression over a set of diverse linguistic
features that were deemed potentially relevant to
readability. Our results demonstrate that a rich
combination of features from syntactic parsers,
language models, as well as lexical statistics all
contribute to accurately predicting expert human
readability judgements. We have also considered
the effect of different genres in predicting read-
ability and how the genre-specific language mod-
els can be exploited to improve the readability pre-
dictions.
546
2 Related Work
There is a significant amount of published work
on a related problem: predicting the reading diffi-
culty of documents, typically, as the school grade-
level of the reader from grade 1 to 12. Some early
methods measure simple characteristics of docu-
ments like average sentence length, average num-
ber of syllables per word, etc. and combine them
using a linear formula to predict the grade level of
a document, for example FOG (Gunning, 1952),
SMOG (McLaughlin, 1969) and Flesh-Kincaid
(Kincaid et al, 1975) metrics. These methods
do not take into account the content of the doc-
uments. Some later methods use pre-determined
lists of words to determine the grade level of a
document, for example the Lexile measure (Sten-
ner et al, 1988), the Fry Short Passage measure
(Fry, 1990) and the Revised Dale-Chall formula
(Chall and Dale, 1995). The word lists these
methods use may be thought of as very simple
language models. More recently, language mod-
els have been used for predicting the grade level
of documents. Si and Callan (2001) and Collins-
Thompson and Callan (2004) train unigram lan-
guage models to predict grade levels of docu-
ments. In addition to language models, Heilman
et al (2007) and Schwarm and Ostendorf (2005)
also use some syntactic features to estimate the
grade level of texts.
Pitler and Nenkova (2008) consider a differ-
ent task of predicting text quality for an educated
adult audience. Their system predicts readabil-
ity of texts from Wall Street Journal using lex-
ical, syntactic and discourse features. Kanungo
and Orr (2009) consider the task of predicting
readability of web summary snippets produced by
search engines. Using simple surface level fea-
tures like the number of characters and syllables
per word, capitalization, punctuation, ellipses etc.
they train a regression model to predict readability
values.
Our work differs from this previous research in
several ways. Firstly, the task we have consid-
ered is different, we predict the readability of gen-
eral documents, not their grade level. The doc-
uments in our data are also not from any single
domain, genre or reader group, which makes our
task more general. The data includes human writ-
ten as well as machine generated documents. The
task and the data has been set this way because it
is aimed at filtering out documents of poor quality
for later processing, like for extracting machine-
processable knowledge from them. Extracting
knowledge from openly found text, such as from
the internet, is becoming popular but the quality
of text found ?in the wild?, like found through
searching the internet, vary considerably in qual-
ity and genre. If the text is of poor readability then
it is likely to lead to extraction errors and more
problems downstream. If the readers are going
to be humans instead of machines, then also it is
best to filter out poorly written documents. Hence
identifying readability of general text documents
coming from various sources and genres is an im-
portant task. We are not aware of any other work
which has considered such a task.
Secondly, we note that all of the above ap-
proaches that use language models train a lan-
guage model for each difficulty level using the
training data for that level. However, since the
amount of training data annotated with levels
is limited, they can not train higher-order lan-
guage models, and most just use unigram models.
In contrast, we employ more powerful language
models trained on large quantities of generic text
(which is not from the training data for readabil-
ity) and use various features obtained from these
language models to predict readability. Thirdly,
we use a more sophisticated combination of lin-
guistic features derived from various syntactic
parsers and language models than any previous
work. We also present ablation results for differ-
ent sets of features. Fourthly, given that the doc-
uments in our data are not from a particular genre
but from a mix of genres, we also train genre-
specific language models and show that including
these as features improves readability predictions.
Finally, we also show comparison between var-
ious machine learning algorithms for predicting
readability, none of the previous work compared
learning algorithms.
3 Readability Data
The readability data was collected and re-
leased by LDC. The documents were collected
547
from the following diverse sources or genres:
newswire/newspaper text, weblogs, newsgroup
posts, manual transcripts, machine translation out-
put, closed-caption transcripts and Wikipedia arti-
cles. Documents for newswire, machine transla-
tion and closed captioned genres were collected
automatically by first forming a candidate pool
from a single collection stream and then randomly
selecting documents. Documents for weblogs,
newsgroups and manual transcripts were also col-
lected in the same way but were then reviewed
by humans to make sure they were not simply
spam articles or something objectionable. The
Wikipedia articles were collected manually, by
searching through a data archive or the live web,
using keyword and other search techniques. Note
that the information about genres of the docu-
ments is not available during testing and hence
was not used when training our readability model.
A total of 540 documents were collected in this
way which were uniformly distributed across the
seven genres. Each document was then judged
for its readability by eight expert human judges.
These expert judges are native English speakers
who are language professionals and who have
specialized training in linguistic analysis and an-
notation, including the machine translation post-
editing task. Each document was also judged for
its readability by six to ten naive human judges.
These non-expert (naive) judges are native En-
glish speakers who are not language professionals
(e.g. editors, writers, English teachers, linguistic
annotators, etc.) and have no specialized language
analysis or linguistic annotation training. Both ex-
pert and naive judges provided readability judg-
ments using a customized web interface and gave
a rating on a 5-point scale to indicate how readable
the passage is (where 1 is lowest and 5 is highest
readability) where readability is defined as a sub-
jective judgment of how easily a reader can extract
the information the writer or speaker intended to
convey.
4 Readability Model
We want to answer the question whether a
machine can accurately estimate readability as
judged by a human. Therefore, we built a
machine-learning system that predicts the read-
ability of documents by training on expert hu-
man judgements of readability. The evaluation
was then designed to compare how well machine
and naive human judges predict expert human
judgements. In order to make the machine?s pre-
dicted score comparable to a human judge?s score
(details about our evaluation metrics are in Sec-
tion 6.1), we also restricted the machine scores to
integers. Hence, the task is to predict an integer
score from 1 to 5 that measures the readability of
the document.
This task could be modeled as a multi-class
classification problem treating each integer score
as a separate class, as done in some of the previ-
ous work (Si and Callan, 2001; Collins-Thompson
and Callan, 2004). However, since the classes
are numerical and not unrelated (for example, the
score 2 is in between scores 1 and 3), we de-
cided to model the task as a regression problem
and then round the predicted score to obtain the
closest integer value. Preliminary results verified
that regression performed better than classifica-
tion. Heilman et al (2008) also found that it
is better to treat the readability scores as ordinal
than as nominal. We take the average of the ex-
pert judge scores for each document as its gold-
standard score. Regression was also used by Ka-
nungo and Orr (2009), although their evaluation
did not constrain machine scores to be integers.
We tested several regression algorithms avail-
able in the Weka1 machine learning package, and
in Section 6.2 we report results for several which
performed best. The next section describes the
numerically-valued features that we used as input
for regression.
5 Features for Predicting Readability
Good input features are critical to the success of
any regression algorithm. We used three main cat-
egories of features to predict readability: syntac-
tic features, language-model features, and lexical
features, as described below.
5.1 Features Based on Syntax
Many times, a document is found to be unreadable
due to unusual linguistic constructs or ungram-
1http://www.cs.waikato.ac.nz/ml/weka/
548
matical language that tend to manifest themselves
in the syntactic properties of the text. There-
fore, syntactic features have been previously used
(Bernth, 1997) to gauge the ?clarity? of written
text, with the goal of helping writers improve their
writing skills. Here too, we use several features
based on syntactic analyses. Syntactic analyses
are obtained from the Sundance shallow parser
(Riloff and Phillips, 2004) and from the English
Slot Grammar (ESG) (McCord, 1989).
Sundance features: The Sundance system is a
rule-based system that performs a shallow syntac-
tic analysis of text. We expect that this analysis
over readable text would be ?well-formed?, adher-
ing to grammatical rules of the English language.
Deviations from these rules can be indications of
unreadable text. We attempt to capture such de-
viations from grammatical rules through the fol-
lowing Sundance features computed for each text
document: proportion of sentences with no verb
phrases, average number of clauses per sentence,
average sentence length in tokens, average num-
ber of noun phrases per sentence, average number
of verb phrases per sentence, average number of
prepositional phrases per sentence, average num-
ber of phrases (all types) per sentence and average
number of phrases (all types) per clause.
ESG features: ESG uses slot grammar rules to
perform a deeper linguistic analysis of sentences
than the Sundance system. ESG may consider
several different interpretations of a sentence, be-
fore deciding to choose one over the other inter-
pretations. Sometimes ESG?s grammar rules fail
to produce a single complete interpretation of a
sentence, in which case it generates partial parses.
This typically happens in cases when sentences
are ungrammatical, and possibly, less readable.
Thus, we use the proportion of such incomplete
parses within a document as a readability feature.
In case of extremely short documents, this propor-
tion of incomplete parses can be misleading. To
account for such short documents, we introduce
a variation of the above incomplete parse feature,
by weighting it with a log factor as was done in
(Riloff, 1996; Thelen and Riloff, 2002).
We also experimented with some other syn-
tactic features such as average sentence parse
scores from Stanford parser and an in-house maxi-
mum entropy statistical parer, average constituent
scores etc., however, they slightly degraded the
performance in combination with the rest of the
features and hence we did not include them in
the final set. One possible explanation could be
that averaging diminishes the effect of low scores
caused by ungrammaticality.
5.2 Features Based on Language Models
A probabilistic language model provides a predic-
tion of how likely a given sentence was generated
by the same underlying process that generated a
corpus of training documents. In addition to a
general n-gram language model trained on a large
body of text, we also exploit language models
trained to recognize specific ?genres? of text. If a
document is translated by a machine, or casually
produced by humans for a weblog or newsgroup,
it exhibits a character that is distinct from docu-
ments that go through a dedicated editing process
(e.g., newswire and Wikipedia articles). Below
we describe features based on generic as well as
genre-specific language models.
Normalized document probability: One obvi-
ous proxy for readability is the score assigned to
a document by a generic language model (LM).
Since the language model is trained on well-
written English text, it penalizes documents de-
viating from the statistics collected from the LM
training documents. Due to variable document
lengths, we normalize the document-level LM
score by the number of words and compute the
normalized document probability NP (D) for a
document D as follows:
NP (D) =
(
P (D|M)
) 1
|D| , (1)
where M is a general-purpose language model
trained on clean English text, and |D| is the num-
ber of words in the document D.
Perplexities from genre-specific language mod-
els: The usefulness of LM-based features in
categorizing text (McCallum and Nigam, 1998;
Yang and Liu, 1999) and evaluating readability
(Collins-Thompson and Callan, 2004; Heilman
et al, 2007) has been investigated in previous
work. In our experiments, however, since doc-
uments were acquired through several different
channels, such as machine translation or web logs,
549
we also build models that try to predict the genre
of a document. Since the genre information for
many English documents is readily available, we
trained a series of genre-specific 5-gram LMs us-
ing the modified Kneser-Ney smoothing (Kneser
and Ney, 1995; Stanley and Goodman, 1996). Ta-
ble 1 contains a list of a base LM and genre-
specific LMs.
Given a document D consisting of tokenized
word sequence {wi : i = 1, 2, ? ? ? , |D|}, its per-
plexity L(D|Mj) with respect to a LM Mj is
computed as:
L(D|Mj) = e
(
? 1|D|
P|D|
i=1 logP (wi|hi;Mj)
)
, (2)
where |D| is the number of words in D and hi are
the history words for wi, and P (wi|hi;Mj) is the
probability Mj assigns to wi, when it follows the
history words hi.
Posterior perplexities from genre-specific lan-
guagemodels: While perplexities computed from
genre-specific LMs reflect the absolute probabil-
ity that a document was generated by a specific
model, a model?s relative probability compared to
other models may be a more useful feature. To this
end, we also compute the posterior perplexity de-
fined as follows. Let D be a document, {Mi}Gi=1
be G genre-specific LMs, and L(D|Mi) be the
perplexity of the document D with respect to Mi,
then the posterior perplexity, R(Mi|D), is de-
fined as:
R(Mi|D) =
L(D|Mi)?G
j=1 L(D|Mj)
. (3)
We use the term ?posterior? because if a uni-
form prior is adopted for {Mi}Gi=1,R(Mi|D) can
be interpreted as the posterior probability of the
genre LM Mi given the document D.
5.3 Lexical Features
The final set of features involve various lexical
statistics as described below.
Out-of-vocabulary (OOV) rates: We conjecture
that documents containing typographical errors
(e.g., for closed-caption and web log documents)
may receive low readability ratings. Therefore,
we compute the OOV rates of a document with re-
spect to the various LMs shown in Table 1. Since
modern LMs often have a very large vocabulary,
to get meaningful OOV rates, we truncate the vo-
cabularies to the top (i.e., most frequent) 3000
words. For the purpose of OOV computation, a
document D is treated as a sequence of tokenized
words {wi : i = 1, 2, ? ? ? , |D|}. Its OOV rate
with respect to a (truncated) vocabulary V is then:
OOV (D|V) =
?D
i=1 I(wi /? V)
|D| , (4)
where I(wi /? V) is an indicator function taking
value 1 if wi is not in V , and 0 otherwise.
Ratio of function words: A characteristic of doc-
uments generated by foreign speakers and ma-
chine translation is a failure to produce certain
function words, such as ?the,? or ?of.? So we pre-
define a small set of function words (mainly En-
glish articles and frequent prepositions) and com-
pute the ratio of function words over the total
number words in a document:
RF (D) =
?D
i=1 I(wi ? F)
|D| , (5)
where I(wi ? F) is 1 ifwi is in the set of function
words F , and 0 otherwise.
Ratio of pronouns: Many foreign languages that
are source languages of machine-translated docu-
ments are pronoun-drop languages, such as Ara-
bic, Chinese, and romance languages. We conjec-
ture that the pronoun ratio may be a good indica-
tor whether a document is translated by machine
or produced by humans, and for each document,
we first run a POS tagger, and then compute the
ratio of pronouns over the number of words in the
document:
RP (D) =
?D
i=1 I(POS(wi) ? P)
|D| , (6)
where I(POS(wi) ? F) is 1 if the POS tag of wi
is in the set of pronouns, P , and 0 otherwise.
Fraction of known words: This feature measures
the fraction of words in a document that occur
either in an English dictionary or a gazetteer of
names of people and locations.
6 Experiments
This section describes the evaluation methodol-
ogy and metrics and presents and discusses our
550
Genre Training Size(M tokens) Data Sources
base 5136.8 mostly LDC?s GigaWord set
NW 143.2 newswire subset of base
NG 218.6 newsgroup subset of base
WL 18.5 weblog subset of base
BC 1.6 broadcast conversation subset of base
BN 1.1 broadcast news subset of base
wikipedia 2264.6 Wikipedia text
CC 0.1 closed caption
ZhEn 79.6 output of Chinese to English Machine Translation
ArEn 126.8 output of Arabic to English Machine Translation
Table 1: Genre-specific LMs: the second column contains the number of tokens in LM training data (in million tokens).
experimental results. The results of the official
evaluation task are also reported.
6.1 Evaluation Metric
The evaluation process for the DARPAMRP read-
ability test was designed by the evaluation team
led by SAIC. In order to compare a machine?s
predicted readability score to those assigned by
the expert judges, the Pearson correlation coef-
ficient was computed. The mean of the expert-
judge scores was taken as the gold-standard score
for a document.
To determine whether the machine predicts
scores closer to the expert judges? scores than
what an average naive judge would predict, a
sampling distribution representing the underlying
novice performance was computed. This was ob-
tained by choosing a random naive judge for every
document, calculating the Pearson correlation co-
efficient with the expert gold-standard scores and
then repeating this procedure a sufficient number
of times (5000). The upper critical value was set
at 97.5% confidence, meaning that if the machine
performs better than the upper critical value then
we reject the null hypothesis that machine scores
and naive scores come from the same distribution
and conclude that the machine performs signifi-
cantly better than naive judges in matching the ex-
pert judges.
6.2 Results and Discussion
We evaluated our readability system on the dataset
of 390 documents which was released earlier dur-
ing the training phase of the evaluation task. We
Algorithm Correlation
Bagged Decision Trees 0.8173
Decision Trees 0.7260
Linear Regression 0.7984
SVM Regression 0.7915
Gaussian Process Regression 0.7562
Naive Judges
Upper Critical Value 0.7015
Distribution Mean 0.6517
Baselines
Uniform Random 0.0157
Proportional Random -0.0834
Table 2: Comparing different algorithms on the readability
task using 13-fold cross-validation on the 390 documents us-
ing all the features. Exceeding the upper critical value of the
naive judges? distribution indicates statistically significantly
better predictions than the naive judges.
used stratified 13-fold cross-validation in which
the documents from various genres in each fold
was distributed in roughly the same proportion as
in the overall dataset. We first conducted experi-
ments to test different regression algorithms using
all the available features. Next, we ablated various
feature sets to determine how much each feature
set was contributing to making accurate readabil-
ity judgements. These experiments are described
in the following subsections.
6.2.1 Regression Algorithms
We used several regression algorithms available
in theWeka machine learning package and Table 2
shows the results obtained. The default values
551
Feature Set Correlation
Lexical 0.5760
Syntactic 0.7010
Lexical + Syntactic 0.7274
Language Model based 0.7864
All 0.8173
Table 3: Comparison of different linguistic feature sets.
in Weka were used for all parameters, changing
these values did not show any improvement. We
used decision tree (reduced error pruning (Quin-
lan, 1987)) regression, decision tree regression
with bagging (Breiman, 1996), support vector re-
gression (Smola and Scholkopf, 1998) using poly-
nomial kernel of degree two,2 linear regression
and Gaussian process regression (Rasmussen and
Williams, 2006). The distribution mean and the
upper critical values of the correlation coefficient
distribution for the naive judges are also shown in
the table.
Since they are above the upper critical value, all
algorithms predicted expert readability scores sig-
nificantly more accurately than the naive judges.
Bagged decision trees performed slightly better
than other methods. As shown in the following
section, ablating features affects predictive accu-
racy much more than changing the regression al-
gorithm. Therefore, on this task, the choice of re-
gression algorithm was not very critical once good
readability features are used. We also tested two
simple baseline strategies: predicting a score uni-
formly at random, and predicting a score propor-
tional to its frequency in the training data. As
shown in the last two rows of Table 2, these base-
lines perform very poorly, verifying that predict-
ing readability on this dataset as evaluated by our
evaluation metric is not trivial.
6.2.2 Ablations with Feature Sets
We evaluated the contributions of different fea-
ture sets through ablation experiments. Bagged
decision-tree was used as the regression algorithm
in all of these experiments. First we compared
syntactic, lexical and language-model based fea-
tures as described in Section 5, and Table 3 shows
2Polynomial kernels with other degrees and RBF kernel
performed worse.
the results. The language-model feature set per-
forms the best, but performance improves when it
is combined with the remaining features. The lex-
ical feature set by itself performs the worst, even
below the naive distribution mean (shown in Ta-
ble 2); however, when combined with syntactic
features it performs well.
In our second ablation experiment, we com-
pared the performance of genre-independent and
genre-based features. Since the genre-based fea-
tures exploit knowledge of the genres of text used
in the MRP readability corpus, their utility is
somewhat tailored to this specific corpus. There-
fore, it is useful to evaluate the performance of the
system when genre information is not exploited.
Of the lexical features described in subsection 5.3,
the ratio of function words, ratio of pronoun words
and all of the out-of-vocabulary rates except for
the base language model are genre-based features.
Out of the language model features described in
the Subsection 5.2, all of the perplexities except
for the base language model and all of the poste-
rior perplexities3 are genre-based features. All of
the remaining features are genre-independent. Ta-
ble 4 shows the results comparing these two fea-
ture sets. The genre-based features do well by
themselves but the rest of the features help fur-
ther improve the performance. While the genre-
independent features by themselves do not exceed
the upper critical value of the naive judges? dis-
tribution, they are very close to it and still out-
perform its mean value. These results show that
for a dataset like ours, which is composed of a mix
of genres that themselves are indicative of read-
ability, features that help identify the genre of a
text improve performance significantly.4 For ap-
plications mentioned in the introduction and re-
lated work sections, such as filtering less readable
documents from web-search, many of the input
documents could come from some of the common
genres considered in our dataset.
In our final ablation experiment, we evaluated
3Base model for posterior perplexities is computed using
other genre-based LMs (equation 3) hence it can not be con-
sidered genre-independent.
4We note that none of the genre-based features were
trained on supervised readability data, but were trained on
readily-available large unannotated corpora as shown in Ta-
ble 1.
552
Feature Set Correlation
Genre-independent 0.6978
Genre-based 0.7749
All 0.8173
Table 4: Comparison of genre-independent and genre-
based feature sets.
Feature Set By itself Ablated
from All
Sundance features 0.5417 0.7993
ESG features 0.5841 0.8118
Perplexities 0.7092 0.8081
Posterior perplexities 0.7832 0.7439
Out-of-vocabulary rates 0.3574 0.8125
All 0.8173 -
Table 5: Ablations with some individual feature sets.
the contribution of various individual feature sets.
Table 5 shows that posterior perplexities perform
the strongest on their own, but without them, the
remaining features also do well. When used by
themselves, some feature sets perform below the
naive judges? distribution mean, however, remov-
ing them from the rest of the feature sets de-
grades the performance. This shows that no indi-
vidual feature set is critical for good performance
but each further improves the performance when
added to the rest of the feature sets.
6.3 Official Evaluation Results
An official evaluation was conducted by the eval-
uation team SAIC on behalf of DARPA in which
three teams participated including ours. The eval-
uation task required predicting the readability of
150 test documents using the 390 training docu-
ments. Besides the correlation metric, two addi-
tional metrics were used. One of them computed
for a document the difference between the aver-
age absolute difference of the naive judge scores
from the mean expert score and the absolute dif-
ference of the machine?s score from the mean ex-
pert score. This was then averaged over all the
documents. The other one was ?target hits? which
measured if the predicted score for a document
fell within the width of the lowest and the highest
expert scores for that document, and if so, com-
System Correl. Avg. Diff. Target Hits
Our (A) 0.8127 0.4844 0.4619
System B 0.6904 0.3916 0.4530
System C 0.8501 0.5177 0.4641
Upper CV 0.7423 0.0960 0.3713
Table 6: Results of the systems that participated in the
DARPA?s readability evaluation task. The three metrics used
were correlation, average absolute difference and target hits
measured against the expert readability scores. The upper
critical values are for the score distributions of naive judges.
puted a score inversely proportional to that width.
The final target hits score was then computed by
averaging it across all the documents. The upper
critical values for these metrics were computed in
a way analogous to that for the correlation met-
ric which was described before. Higher score is
better for all the three metrics. Table 6 shows the
results of the evaluation. Our system performed
favorably and always scored better than the up-
per critical value on each of the metrics. Its per-
formance was in between the performance of the
other two systems. The performances of the sys-
tems show that the correlation metric was the most
difficult of the three metrics.
7 Conclusions
Using regression over a diverse combination of
syntactic, lexical and language-model based fea-
tures, we built a system for predicting the read-
ability of natural-language documents. The sys-
tem accurately predicts readability as judged by
linguistically-trained expert human judges and
exceeds the accuracy of naive human judges.
Language-model based features were found to be
most useful for this task, but syntactic and lexical
features were also helpful. We also found that for
a corpus consisting of documents from a diverse
mix of genres, using features that are indicative
of the genre significantly improve the accuracy of
readability predictions. Such a system could be
used to filter out less readable documents for ma-
chine or human processing.
Acknowledgment
This research was funded by Air Force Contract
FA8750-09-C-0172 under the DARPA Machine
Reading Program.
553
References
Bernth, Arendse. 1997. Easyenglish: A tool for improv-
ing document quality. In Proceedings of the fifth con-
ference on Applied Natural Language Processing, pages
159?165, Washington DC, April.
Breiman, Leo. 1996. Bagging predictors. Machine Learn-
ing, 24(2):123?140.
Chall, J.S. and E. Dale. 1995. Readability Revisited: The
New Dale-Chall Readability Formula. Brookline Books,
Cambridge, MA.
Collins-Thompson, Kevyn and James P. Callan. 2004. A
language modeling approach to predicting reading diffi-
culty. In Proc. of HLT-NAACL 2004, pages 193?200.
Fry, E. 1990. A readability formula for short passages. Jour-
nal of Reading, 33(8):594?597.
Gunning, R. 1952. The Technique of Clear Writing.
McGraw-Hill, Cambridge, MA.
Heilman, Michael, Kevyn Collins-Thompson, Jamie Callan,
and Maxine Eskenazi. 2007. Combining lexical and
grammatical features to improve readability measures for
first and second language texts. In Proc. of NAACL-HLT
2007, pages 460?467, Rochester, New York, April.
Heilman, Michael, Kevyn Collins-Thompson, and Maxine
Eskenazi. 2008. An analysis of statistical models and fea-
tures for reading difficulty prediction. In Proceedings of
the Third Workshop on Innovative Use of NLP for Build-
ing Educational Applications, pages 71?79, Columbus,
Ohio, June. Association for Computational Linguistics.
Kanungo, Tapas and David Orr. 2009. Predicting the read-
ability of short web summaries. In Proc. of WSDM 2009,
pages 202?211, Barcelona, Spain, February.
Kincaid, J. P., R. P. Fishburne, R. L. Rogers, and B.S.
Chissom. 1975. Derivation of new readability formulas
for navy enlisted personnel. Technical Report Research
Branch Report 8-75, Millington, TN: Naval Air Station.
Kneser, Reinhard and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Proc. of
ICASSP-95, pages 181?184.
McCallum, Andrew and Kamal Nigam. 1998. A comparison
of event models for naive Bayes text classification. In Pa-
pers from the AAAI-98 Workshop on Text Categorization,
pages 41?48, Madison, WI, July.
McCord, Michael C. 1989. Slot grammar: A system for
simpler construction of practical natural language gram-
mars. In Proceedings of the International Symposium on
Natural Language and Logic, pages 118?145, May.
McLaughlin, G. H. 1969. Smog: Grading: A new readabil-
ity formula. Journal of Reading, 12:639?646.
Pitler, Emily and Ani Nenkova. 2008. Revisiting
readability: A unified framework for predicting text
quality. In Proc. of EMNLP 2008, pages 186?195,
Waikiki,Honolulu,Hawaii, October.
Quinlan, J. R. 1987. Simplifying decision trees. Interna-
tional Journal of Man-Machine Studies, 27:221?234.
Rasmussen, Carl and Christopher Williams. 2006. Gaussian
Processes for Machine Leanring. MIT Press, Cambridge,
MA.
Riloff, E. and W. Phillips. 2004. An introduction to the Sun-
dance and Autoslog systems. Technical Report UUCS-
04-015, University of Utah School of Computing.
Riloff, Ellen. 1996. Automatically generating extraction
patterns from untagged text. In Proc. of 13th Natl. Conf.
on Artificial Intelligence (AAAI-96), pages 1044?1049,
Portland, OR.
Schwarm, Sarah E. andMari Ostendorf. 2005. Reading level
assessment using support vector machines and statistical
language models. In Proc. of ACL 2005, pages 523?530,
Ann Arbor, Michigan.
Si, Luo and James P. Callan. 2001. A statistical model for
scientific readability. In Proc. of CIKM 2001, pages 574?
576.
Smola, Alex J. and Bernhard Scholkopf. 1998. A tutorial
on support vector regression. Technical Report NC2-TR-
1998-030, NeuroCOLT2.
Stanley, Chen and Joshua Goodman. 1996. An empirical
study of smoothing techniques for language modeling. In
Proc. of the 34th Annual Meeting of the Association for
Computational Linguistics (ACL-96), pages 310?318.
Stenner, A. J., I. Horabin, D. R. Smith, and M. Smith. 1988.
The Lexile Framework. Durham, NC: MetaMetrics.
Thelen, M. and E. Riloff. 2002. A bootstrapping method for
learning semantic lexicons using extraction pattern con-
texts. In Proc. of EMNLP 2002, Philadelphia, PA, July.
Yang, Yiming and Xin Liu. 1999. A re-examination of text
cateogrization methods. In Proc. of 22nd Intl. ACM SI-
GIR Conf. on Research and Development in Information
Retrieval, pages 42?48, Berkeley, CA.
554
Tutorial Abstracts of ACL 2010, page 6,
Uppsala, Sweden, 11 July 2010. c?2010 Association for Computational Linguistics
Semantic Parsing: The Task, the State-of-the-Art and the Future
Rohit J. Kate
Department of Computer Science
The University of Texas at Austin
Austin, TX 78712, USA
rjkate@cs.utexas.edu
Yuk Wah Wong
Google Inc.
Pittsburgh, PA 15213, USA
ywwong@google.com
1 Introduction
Semantic parsing is the task of mapping natural
language sentences into complete formal mean-
ing representations which a computer can exe-
cute for some domain-specific application. This
is a challenging task and is critical for develop-
ing computing systems that can understand and
process natural language input, for example, a
computing system that answers natural language
queries about a database, or a robot that takes
commands in natural language. While the im-
portance of semantic parsing was realized a long
time ago, it is only in the past few years that the
state-of-the-art in semantic parsing has been sig-
nificantly advanced with more accurate and ro-
bust semantic parser learners that use a variety
of statistical learning methods. Semantic parsers
have also been extended to work beyond a single
sentence, for example, to use discourse contexts
and to learn domain-specific language from per-
ceptual contexts. Some of the future research di-
rections of semantic parsing with potentially large
impacts include mapping entire natural language
documents into machine processable form to en-
able automated reasoning about them and to con-
vert natural language web pages into machine pro-
cessable representations for the Semantic Web to
support automated high-end web applications.
This tutorial will introduce the semantic pars-
ing task and will bring the audience up-to-date
with the current research and state-of-the-art in se-
mantic parsing. It will also provide insights about
semantic parsing and how it relates to and dif-
fers from other natural language processing tasks.
It will point out research challenges and some
promising future directions for semantic parsing.
2 Content Overview
The proposed tutorial on semantic parsing will
start with an introduction to the task, giving ex-
amples of some application domains and meaning
representation languages. It will also point out its
distinctions from and relations to other NLP tasks.
Next, it will talk in depth about various semantic
parsers that have been built, starting with earlier
hand-built systems to the current state-of-the-art
statistical semantic parser learners. It will point
out the underlying commonalities and differences
between the learners. The next section of the tuto-
rial will talk about the recent advances in extend-
ing semantic parsing to work beyond parsing a sin-
gle sentence. Finally, the tutorial will point out
the current research challenges and some promis-
ing future directions for semantic parsing.
3 Outline
1. Introduction to the task of semantic parsing
(a) Definition of the task
(b) Examples of application domains and meaning
representation languages
(c) Distinctions from and relations to other NLP
tasks
2. Semantic parsers
(a) Earlier hand-built systems
(b) Learning for semantic parsing
i. Semantic parsing learning task
ii. Non-statistical semantic parser learners
iii. Statistical semantic parser learners
iv. Exploiting syntax for semantic parsing
v. Various forms of supervision: semi-
supervision, ambiguous supervision
(c) Underlying commonality and differences be-
tween different semantic parser learners
3. Semantic parsing beyond a sentence
(a) Using discourse contexts for semantic parsing
(b) Learning language from perceptual contexts
4. Research challenges and future directions
(a) Machine reading of documents: Connecting with
knowledge representation
(b) Applying semantic parsing techniques to the Se-
mantic Web
(c) Future research directions
5. Conclusions
6
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 823?827,
Dublin, Ireland, August 23-24, 2014.
UWM: Applying an Existing Trainable Semantic Parser to Parse Robotic
Spatial Commands
Rohit J. Kate
University of Wisconsin-Milwaukee
Milwaukee, WI
katerj@uwm.edu
Abstract
This paper describes Team UWM?s sys-
tem for the Task 6 of SemEval 2014
for doing supervised semantic parsing of
robotic spatial commands. An existing
semantic parser, KRISP, was trained us-
ing the provided training data of natural
language robotic spatial commands paired
with their meaning representations in the
formal robot command language. The en-
tire process required very little manual ef-
fort. Without using the additional annota-
tions of word-aligned semantic trees, the
trained parser was able to exactly parse
new commands into their meaning repre-
sentations with 51.18% best F-measure at
72.67% precision and 39.49% recall. Re-
sults show that the parser was particularly
accurate for short sentences.
1 Introduction
Semantic parsing is the task of converting natu-
ral language utterances into their complete formal
meaning representations which are executable for
some application. Example applications of seman-
tic parsing include giving natural language com-
mands to robots and querying databases in natu-
ral language. Some old semantic parsers were de-
veloped manually to work for specific applications
(Woods, 1977; Warren and Pereira, 1982). How-
ever, such semantic parsers were generally brittle
and building them required a lot of manual effort.
In addition, these parsers could not be ported to
any other application without again putting signif-
icant manual effort.
More recently, several semantic parsers have
been developed using machine learning (Zelle and
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
Mooney, 1996; Ge and Mooney, 2005; Zettle-
moyer and Collins, 2005; Wong and Mooney,
2006; Kate and Mooney, 2006; Lu et al., 2008;
Kwiatkowski et al., 2011). In this approach, train-
ing data is first created for the domain of inter-
est. Then using one of the many machine learn-
ing methods and semantic parsing frameworks, a
semantic parser is automatically learned from the
training data (Mooney, 2007). The trained seman-
tic parser is then capable of parsing new natu-
ral language utterances into their meaning repre-
sentations. Semantic parsers built using machine
learning tend to be more robust and can be easily
ported to other application domains with appropri-
ate domain-specific training data.
The Task 6 of SemEval 2014 provided a new ap-
plication domain for semantic parsing along with
training and test data. The domain involved giv-
ing natural language commands to a robotic arm
which would then move blocks on a board (Dukes,
2013). The domain was inspired from the classic
AI system SHRDLU (Winograd, 1972). The train-
ing data contained 2500 examples of sentences
paired with their meaning representations in the
Robot Command Language (RCL) which was de-
signed for this domain (Dukes, 2013). The test
data contained 909 such example pairs.
We trained an existing and freely available
1
se-
mantic parser KRISP (Kate and Mooney, 2006)
using the training data for this domain. Besides
changing the format of the data for running KRISP
and writing a context-free grammar for the mean-
ing representation language RCL, the entire pro-
cess required minimal manual effort. The author
spent less than a week?s time for participating in
the Task 6, and most of it was spent in running
the experiments. This demonstrates that train-
able semantic parsers like KRISP can be rapidly
adopted to new domains. In the Results section
we show different precisions and recalls it ob-
1
http://www.cs.utexas.edu/users/ml/krisp/
823
tained at different confidence levels in the form of
a precision-recall curve. The results also show that
the parser was particularly accurate on shorter sen-
tences. Two major reasons that prevented KRISP
from performing better on this domain were - its
high computational demand for memory which
prevented it from being trained beyond 1500 train-
ing examples, and some variability in the mean-
ing representation language RCL that negatively
affected training as well as evaluation.
2 Background: KRISP Semantic Parser
KRISP (Kernel-based Robust Interpretation for Se-
mantic Parsing) is a trainable semantic parser
(Kate and Mooney, 2006) that uses Support Vector
Machines (SVMs) (Cristianini and Shawe-Taylor,
2000) as the machine learning method with string-
subsequence kernel (Lodhi et al., 2002). It takes
natural language utterances and their correspond-
ing formal meaning representation as the training
data along with the context-free grammar of the
meaning representation language (MRL). The key
idea in KRISP is that every production of the MRL
is treated as a semantic concept. For every MRL
production, an SVM classifier is trained so that it
can give for any input natural language substring
of words the probability that it expresses the corre-
sponding semantic concept. Once these classifiers
are trained, parsing a sentence reduces to finding
the most probable semantic derivation of the sen-
tence in which different productions cover differ-
ent parts of the sentence and together form a com-
plete meaning representation. Figure 1 shows an
example semantic derivation of a robotic spatial
command. Productions of RCL grammar (Table 1)
are shown at tree nodes depicting different parts of
the sentence they cover.
Since the training data is not in the form of such
semantic derivations, an EM-like iterative algo-
rithm is used to collect appropriate positive and
negative examples in order to train the classifiers
(Kate and Mooney, 2006). Positive examples are
collected from correct semantic derivations de-
rived by the parser learned in the previous itera-
tion, and negative examples are collected from the
incorrect semantic derivations.
KRISP was shown to work well on the US geog-
raphy database query domain (Tang and Mooney,
2001) as well as on the RoboCup Coach Lan-
guage (CLang) domain (Kate et al., 2005). It was
also shown to be particularly robust to noise in
Figure 1: Semantic derivation of the robotic spatial com-
mand ?pick up the turquoise pyramid? obtained by KRISP
during testing which gives the correct RCL representation
(event: (action: take) (entity: (color: cyan) (type: prism))).
the natural language utterances (Kate and Mooney,
2006). KRISP was later extended to do semi-
supervised semantic parsing (Kate and Mooney,
2007b), to learn from ambiguous supervision in
which multiple sentences could be paired with a
single meaning representation in the training data
(Kate and Mooney, 2007a), and to transform the
MRL grammar to improve semantic parsing (Kate,
2008).
3 Methods
In order to apply KRISP to the Task 6 of SemEval
2014, the format of the provided data was first
changed to the XML-type format that KRISP ac-
cepts. The data contained several instances of
co-references which was also part of RCL, but
KRISP was not designed to handle co-references
and expects them to be pre-resolved. We ob-
served that almost all co-references in the mean-
ing representations, indicated by ?reference-id?
token, resolved to the first occurrence of an ?en-
tity? element in the meaning representation. This
was found to be true for more than 99% of the
cases. We used this observation to resolve co-
references during semantic parsing in the follow-
ing way. As a pre-processing step, we first remove
from the meaning representations all the ?id:? to-
kens (these resolve the references) but keep the
?reference-id:? tokens (these encode presence of
co-references). The natural language sentences
are not modified in any way and the parser learns
from the training data to relate words like ?it?
and ?one? to the RCL token ?reference-id?. After
KRISP generates a meaning representation during
testing, as a post-processing step, ?id: 1? is added
to the first ?entity? element in the meaning repre-
sentation if it contains the ?reference-id:? token.
The context-free grammar for RCL was not pro-
vided by the Task organizers. There are multi-
824
ple ways to write a context-free grammar for a
meaning representation language and those that
conform better to natural language work better
for semantic parsing (Kate, 2008). We manu-
ally wrote grammar for RCL which mostly fol-
lowed the structure of the meaning representa-
tions as they already conformed highly to natural
language commands and hence writing the gram-
mar was straightforward. KRISP runs faster if
there are fewer non-terminals on the right-hand-
side (RHS) of the grammar because that makes
the search for the most probable semantic deriva-
tion faster. Hence we kept non-terminals on RHS
as few as possible while writing the grammar.
Table 1 shows the entire grammar for RCL that
we wrote which was given to KRISP. The non-
terminals are indicated with a ?*? in their front.
We point out that KRISP needs grammar only for
the meaning representation language (an applica-
tion will need it anyway if the statements are to be
executed) and not for the natural language.
KRISP?s training algorithm could be aided by
providing it with information about which natu-
ral language words are usually used to express the
concept of a production. For example, word ?red?
usually expresses ?*color: ? ( color: red )?. The
data provided with the Task 6 came with the word-
aligned semantic trees which indicated which nat-
ural language words corresponded to which mean-
ing representation components. This information
could have been used to aid KRISP, however, we
found many inconsistencies and errors in the pro-
vided word-aligned semantic trees and chose not
to use them. In addition, KRISP seemed to learn
most of that information on its own anyway.
The Task 6 also included integrating semantic
parsing with spatial planning. This meant that if
the semantic parser generates an RCL representa-
tion that does not make sense for the given block
configuration on the board, then it could be dis-
missed and the next best RCL representation could
be considered. Besides generating the best mean-
ing representation for a natural language utterance,
KRISP is also capable of generating multiple pos-
sible meaning representations sorted by their prob-
abilities. We could have used this capability to
output only the best RCL representation that is
valid for the given board configuration. Unfortu-
nately, unfamiliarity with the provided Java API
for the spatial planner and lack of time prevented
us from doing this.
*action: ? ( action: move )
*action: ? ( action: drop )
*action: ? ( action: take )
*cardinal: ? ( cardinal: 1 )
*cardinal: ? ( cardinal: 2 )
*cardinal: ? ( cardinal: 3 )
*cardinal: ? ( cardinal: 4 )
*color: ? ( color: magenta )
*color: ? ( color: red )
*color: ? ( color: white )
*color: ? ( color: cyan )
*color: ? ( color: green )
*color: ? ( color: yellow )
*color: ? ( color: blue )
*color: ? ( color: gray )
*indicator: ? ( indicator: rightmost )
*indicator: ? ( indicator: back )
*indicator: ? ( indicator: center )
*indicator: ? ( indicator: right )
*indicator: ? ( indicator: leftmost )
*indicator: ? ( indicator: individual )
*indicator: ? ( indicator: nearest )
*indicator: ? ( indicator: front )
*indicator: ? ( indicator: left )
*reference-id: ? ( reference-id: 1 )
*relation: ? ( relation: right )
*relation: ? ( relation: forward )
*relation: ? ( relation: within )
*relation: ? ( relation: above )
*relation: ? ( relation: nearest )
*relation: ? ( relation: adjacent )
*relation: ? ( relation: front )
*relation: ? ( relation: left )
*relation: ? ( relation: backward )
*type: ? ( type: type-reference-group )
*type: ? ( type: board )
*type: ? ( type: prism )
*type: ? ( type: cube )
*type: ? ( type: type-reference )
*type: ? ( type: cube-group )
*type: ? ( type: corner )
*type: ? ( type: robot )
*type: ? ( type: stack )
*type: ? ( type: edge )
*type: ? ( type: region )
*type: ? ( type: tile )
*type: ? ( type: reference )
*indicator: ? *indicator: *indicator:
*color: ? *color: *color:
*ct: ? *color: *type:
*ict: ? *indicator: *ct:
*ctr: ? *ct: *reference-id:
*cct: ? *cardinal: *ct:
*ed: ? *entity: ( destination: *spatial-relation: )
*entity: ? ( entity: *type: )
*entity: ? ( entity: *type: *reference-id: )
*entity: ? ( entity: *type: *spatial-relation: )
*entity: ? ( entity: *ct: )
*entity: ? ( entity: *indicator: *type: )
*entity: ? ( entity: *ict: )
*entity: ? ( entity: *ict: *spatial-relation: )
*entity: ? ( entity: *cardinal: *type: )
*entity: ? ( entity: *cct: )
*entity: ? ( entity: *cct: *spatial-relation: )
*entity: ? ( entity: *ctr: )
*entity: ? ( entity: *ct: *spatial-relation: )
*entity: ? ( entity: *ctr: *spatial-relation: )
*measure: ? ( measure: *entity: )
*mr: ? *measure: *relation:
*spatial-relation: ? ( spatial-relation: *relation: *entity: )
*spatial-relation: ? ( spatial-relation: *mr: )
*spatial-relation: ? ( spatial-relation: *mr: *entity: )
*S? ( sequence: *S *S )
*S? ( event: *action: *ed: )
*S? ( event: *action: *entity: )
Table 1: Grammar for the Robot Command Lan-
guage (RCL) given to KRISP for semantic parsing.
The non-terminals are indicated with a ?*? in their
front. The start symbol is *S.
825
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  5  10  15  20  25  30  35  40  45  50
P
r
e
c
i
s
i
o
n
 
(
%
)
Recall (%)
Figure 2: Precision-recall curve for the semantic
parsing output on test sentences.
4 Results
We found that KRISP could not be trained beyond
1500 examples in this domain because the num-
ber of negative examples that are generated during
the training process would become too large for
the available memory size. This is something that
could be fixed in the future by suitably sampling
negative examples. Using the first 1500 train-
ing examples, we evaluated KRISP?s performance
on the provided 909 test examples. A generated
RCL representation is considered correct only if
it exactly matches the correct answer; no partial
credit is given. In order to avoid generating incor-
rect meaning representations when it is not confi-
dent, KRISP uses a threshold and if the confidence
(probability) of the best semantic derivation is be-
low this threshold, it does not generate any mean-
ing representation. This threshold was set to 0.05
as was previously done for other domains.
Performance was measured in terms of preci-
sion (the percentage of generated meaning repre-
sentations that were correct) and recall (the per-
centage of all sentences for which correct meaning
representations were obtained). Given that KRISP
also gives confidences with its output meaning
representations, we can compute precisions and
recalls at various confidence levels. Figure 2
shows the entire precision-recall curve thus ob-
tained. The best F-measure (harmonic mean of
precision and recall) on this curve is 51.18% pdf
at 72.67% precision and 39.49% recall. The pre-
cision at the highest recall was 45.98% which we
had reported as our official evaluation result for
the SemEval Task 6.
We further analyzed the results according to the
lengths of the sentences and found that KRISP was
Sentence length Accuracy (Correct/Total)
1-3 100.00% (15/15)
4-7 71.20% (136/191)
8-11 51.76% (147/284)
12-15 41.80% (79/189)
16-19 22.22% (28/126)
20-23 15.71% (11/70)
24-27 3.23% (1/31)
28-31 33.33% (1/3)
All 45.98% (418/909)
Table 2: Accuracy of semantic parsing across test
sentences of varying lengths.
very accurate with shorter sentences and became
progressively less accurate as the lengths of the
sentences increase. Table 2 shows these results.
This could be simply because the longer the sen-
tence, the more the likelihood of making an error,
and since no partial credit is given, the entire out-
put meaning representation is deemed incorrect.
On further error analysis we observed that there
was some variability in the meaning representa-
tions. The ?move? and ?drop? actions seemed
to mean the same thing and were used alterna-
tively. For example in the training data, the ut-
terance ?place the red block on single blue block?
had ?(action: drop)? in the corresponding mean-
ing representation, while ?place red cube on grey
cube? had ?(action: move)?, but there is no ap-
parent difference between the two cases. There
were many such instances. This was confusing
KRISP?s training algorithm because it would col-
lect the same phrase sometimes as a positive ex-
ample and sometimes as a negative example. This
also affected the evaluation, because KRISP would
generate ?move? which won?t match ?drop?, or
vice-versa, and the evaluator will call it an error.
5 Conclusions
We participated in the SemEval 2014 Task 6 of su-
pervised semantic parsing of robotic spatial com-
mands. We used an existing semantic parser
learner, KRISP, and trained it on this domain
which required minimum time and effort from our
side. The trained parser was able to map natu-
ral language robotic spatial commands into their
formal robotic command language representations
with good accuracy, particularly for shorter sen-
tences.
826
References
Nello Cristianini and John Shawe-Taylor. 2000. An
Introduction to Support Vector Machines and Other
Kernel-based Learning Methods. Cambridge Uni-
versity Press.
Kais Dukes. 2013. Semantic annotation of robotic
spatial commands. In Proceedings of the Language
and Technology Conference (LTC-2013), Poznan,
Poland.
Ruifang Ge and Raymond J. Mooney. 2005. A sta-
tistical semantic parser that integrates syntax and
semantics. In Proceedings of the Ninth Confer-
ence on Computational Natural Language Learning
(CoNLL-2005), pages 9?16, Ann Arbor, MI, July.
Rohit J. Kate and Raymond J. Mooney. 2006. Us-
ing string-kernels for learning semantic parsers. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics
(COLING/ACL-06), pages 913?920, Sydney, Aus-
tralia, July.
Rohit J. Kate and Raymond J. Mooney. 2007a. Learn-
ing language semantics from ambiguous supervi-
sion. In Proceedings of the Twenty-Second Con-
ference on Artificial Intelligence (AAAI-07), pages
895?900, Vancouver, Canada, July.
Rohit J. Kate and Raymond J. Mooney. 2007b.
Semi-supervised learning for semantic parsing us-
ing support vector machines. In Proceedings of
Human Language Technologies: The Conference of
the North American Chapter of the Association for
Computational Linguistics (NAACL-HLT-07), pages
81?84, Rochester, NY, April.
Rohit J. Kate, Yuk Wah Wong, and Raymond J.
Mooney. 2005. Learning to transform natural to for-
mal languages. In Proceedings of the Twentieth Na-
tional Conference on Artificial Intelligence (AAAI-
05), pages 1062?1068, Pittsburgh, PA, July.
Rohit J. Kate. 2008. Transforming meaning represen-
tation grammars to improve semantic parsing. In
Proceedings of the Twelfth Conference on Computa-
tional Natural Language Learning (CoNLL-2008),
pages 33?40. Association for Computational Lin-
guistics.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2011. Lexical generaliza-
tion in CCG grammar induction for semantic pars-
ing. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2011), pages 1512?1523. Association for Computa-
tional Linguistics.
Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, and Chris Watkins. 2002. Text
classification using string kernels. 2:419?444.
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S.
Zettlemoyer. 2008. A generative model for pars-
ing natural language to meaning representations. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing (EMNLP-
08), Honolulu, HI, October.
Raymond J. Mooney. 2007. Learning for semantic
parsing. In A. Gelbukh, editor, Computational Lin-
guistics and Intelligent Text Processing: Proceed-
ings of the 8th International Conference (CICLing-
2007), Mexico City, pages 311?324. Springer Ver-
lag, Berlin.
Lappoon R. Tang and Raymond J. Mooney. 2001. Us-
ing multiple clause constructors in inductive logic
programming for semantic parsing. In Proceedings
of the 12th European Conference on Machine Learn-
ing (ECML-2001), pages 466?477, Freiburg, Ger-
many.
David H. D. Warren and Fernando C. N. Pereira. 1982.
An efficient easily adaptable system for interpret-
ing natural language queries. American Journal of
Computational Linguistics, 8(3-4):110?122.
Terry Winograd. 1972. Understanding Natural Lan-
guage. Academic Press, Orlando, FL.
Yuk Wah Wong and Raymond J. Mooney. 2006.
Learning for semantic parsing with statistical ma-
chine translation. In Proceedings of Human Lan-
guage Technology Conference / North American
Chapter of the Association for Computational Lin-
guistics Annual Meeting (HLT-NAACL-06), pages
439?446, New York City, NY.
William A. Woods. 1977. Lunar rocks in natural
English: Explorations in natural language question
answering. In Antonio Zampoli, editor, Linguistic
Structures Processing. Elsevier North-Holland, New
York.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the Thirteenth Na-
tional Conference on Artificial Intelligence (AAAI-
96), pages 1050?1055, Portland, OR, August.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of 21st Conference on
Uncertainty in Artificial Intelligence (UAI-2005),
Edinburgh, Scotland, July.
827
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 828?832,
Dublin, Ireland, August 23-24, 2014.
UWM: Disorder Mention Extraction from Clinical Text Using CRFs and
Normalization Using Learned Edit Distance Patterns
Omid Ghiasvand
University of Wisconsin-Milwaukee
Milwaukee, WI
ghiasva2@uwm.edu
Rohit J. Kate
University of Wisconsin-Milwaukee
Milwaukee, WI
katerj@uwm.edu
Abstract
This paper describes Team UWM?s sys-
tem for the Task 7 of SemEval 2014 that
does disorder mention extraction and nor-
malization from clinical text. For the dis-
order mention extraction (Task A), the sys-
tem was trained using Conditional Ran-
dom Fields with features based on words,
their POS tags and semantic types, as well
as features based on MetaMap matches.
For the disorder mention normalization
(Task B), variations of disorder mentions
were considered whenever exact matches
were not found in the training data or in
the UMLS. Suitable types of variations
for disorder mentions were automatically
learned using a new method based on edit
distance patterns. Among nineteen partic-
ipating teams, UWM ranked third in Task
A with 0.755 strict F-measure and second
in Task B with 0.66 strict accuracy.
1 Introduction
Entity mention extraction is an important task in
processing natural language clinical text. Disor-
ders, medications, anatomical sites, clinical pro-
cedures etc. are among the entity types that pre-
dominantly occur in clinical text. Out of these,
the Task 7 of SemEval 2014 concentrated on ex-
tracting (Task A) and normalizing (Task B) dis-
order mentions. Disorder mention extraction is
particularly challenging because disorders are fre-
quently found as discontinuous phrases in clinical
sentences. The extracted mentions were then to be
normalized by mapping them to their UMLS CUIs
if they were in the SNOMED-CT part of UMLS
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
and belonged to the ?disorder? UMLS seman-
tic group, otherwise they were to be declared as
?CUI-less?. This normalization task is challeng-
ing because disorder names are frequently men-
tioned in modified forms which prevents their ex-
act matching with concept descriptions in UMLS.
Our team, UWM, participated in both Task
A and Task B. We modelled disorder mention
extraction as a standard sequence labeling task.
The model was trained using Conditional Ran-
dom Fields (Lafferty et al., 2001) with various
types of lexical and semantic features that in-
cluded MetaMap (Aronson, 2001) matches. The
model was also inherently capable of extracting
discontinuous disorder mentions. To normalize
disorder mentions, our system first looked for ex-
act matches with disorder mentions in the train-
ing data and in the UMLS. If no exact match
was found, then suitable variations of the disorder
mentions were generated based on the commonly
used variations of disorder mentions learned from
the training data as well as from the UMLS syn-
onyms. We developed a novel method to automat-
ically learn such variations based on edit distances
(Levenshtein, 1966) which is described in the next
section.
Our Team ranked third on Task A and second on
Task B in the official SemEval 2014 Task 7 evalua-
tion (considering only the best run for each team).
We also present results of ablation studies we did
on the development data in order to determine the
contributions of various features and components
of our system.
2 Methods
2.1 Task A: Disorder Mention Extraction
We modelled disorder mention extraction as a se-
quence labeling task with the standard ?BIO? (Be-
gin, Inside, Outside) scheme of output labels for
sentence tokens. The tokens labelled ?I? follow-
ing the latest ?B? token are extracted together as
828
a disorder. For example, in the following labelled
sequence ?the/O left/B atrium/I is/O moderately/O
dilated/I?, ?left atrium dilated? will be extracted
as a disorder. The labeling scheme thus natu-
rally models discontinuously mentioned disorders
which is one challenging aspect of the disorder
mention extraction task.
The sequence labeling model is trained using
Condition Random Fields (CRFs) (Lafferty et al.,
2001) using the five group of features shown in Ta-
ble 1. The clinical reports are first pre-processed
using Stanford CoreNLP
1
for tokenization, sen-
tence segmentation and part-of-speech (POS) tag-
ging which help in obtaining the lexical features
(Group 1). The semantic features (Group 2) are
obtained by matching the tokens, along with bi-
grams and trigrams in UMLS. For the first three
features in Group 2, only the eleven semantic
types under the ?disorder? semantic group are con-
sidered.
2
If a token is a concept in UMLS with
?disorder? semantic group then its feature is as-
signed the value of its semantic type (for example
?congenital abnormality?, ?Neoplastic process?,
etc.) otherwise it is assigned the value ?Null?. The
next three features in Group 2 take Boolean values
depending upon whether the bigram or trigram is
present in UMLS as a concept or not. The last fea-
ture in Group 2 takes CUI as its value if the word
is a concept in UMLS otherwise it takes ?Null? as
the value.
The features in Group 3 are obtained by running
MetaMap (Aronson, 2001). The lemmatized ver-
sion of word obtained using Stanford CoreNLP is
used as an additional feature in Group 4. Finally,
if the word is an abbreviation according to a list
of clinical abbreviations
3
then its full-form is ob-
tained.
4
The full-form, whether it is in UMLS,
and its semantic type (out of ?disorder group?) are
used as features under Group 5. We used the CRF-
suite (Okazaki, 2007) implementation of CRFs.
2.2 Task B: Disorder Mention Normalization
The extracted disease mentions from Task A are
normalized in Task B as follows. As a first step,
1
http://nlp.stanford.edu/software/
corenlp.shtml
2
We found that using all semantic groups negatively af-
fected the performance.
3
http://en.wikipedia.org/wiki/List_of_
medical_abbreviations
4
If multiple full-forms were present then only the first one
was used. In the future, one could improve this through ab-
breviation disambiguation (Xu et al., 2012).
Group 1: Lexical
Word
Next word
Previous word
POS tag of word
POS tag of next word
POS tag of previous word
Next to next word
Previous to previous word
Length of the word
Group 2: Semantic
UMLS semantic type of word
UMLS semantic type of next word
UMLS semantic type of previous word
Bigram with next word is in UMLS
Reverse bigram with next word is in UMLS
Trigram with next two words is in UMLS
CUI of the word
Group 3: MetaMap
Word tagged as disorder by MetaMap
Next word tagged as disorder by MetaMap
Previous word tagged as disorder by MetaMap
Group 4: Lemmatization
Lemmatized version of the word
Group 5: Abbreviation
Full-form
Full-form is in UMLS
UMLS semantic type of full-form
Table 1: Features used to train the CRF model for disorder
mention extraction.
our system tries to exactly match the disease men-
tions in the training data. If they match, then the
corresponding CUI or CUI-less is the output. If
no match is found in the training data, then the
system tries to exactly match names of concepts
in UMLS including their listed synonyms.
5
If a
match is found then the corresponding CUI is the
output. If the mention does not match either in the
training data or in the UMLS and if it is an ab-
breviation according to the abbreviation list (same
as used in Task A), then its full-form is used to
exactly match in the training data and in UMLS.
However, what makes the normalization task chal-
lenging is that exact matching frequently fails. We
employed a novel method that learns to do approx-
imate matching for this task.
We found that most failures in exact matching
were because of minor typographical variations
due to morphology, alternate spellings or typos.
In order to automatically learn such variations, we
developed a new method based on edit distance
which is a measure of typographical similarity be-
tween two terms. We used a particular type of
well-known edit distance called Levenshtein dis-
5
In accordance to the task definition, only the concepts
listed in SNOMED-CT and of the UMLS semantic group
?disorder? are considered in this step.
829
Learned Edit Distance Pattern Comments
SAME o INSERT u SAME r Change American spelling to British
INSERT s SAME space Pluralize by adding ?s? before space
DELETE i DELETE e SUBSTITUTE s/y Example: ?Z-plasties?? ?Z-plasty?
START SAME h INSERT a SAME e SAME m SAME o Variation: ?hemo...?? ?haemo...?
DELETE space DELETE n DELETE o DELETE s END Drop ? nos? in the end
SAME s SUBSTITUTE i/e SAME s Example: ?metastasis?? ?metastases?
Table 2: A few illustrative edit distance patterns that were automatically learned from UMLS and the training data.
Data used for training Task A Task B
Strict Relaxed Strict Relaxed
P R F P R F Accuracy Accuracy
Training + Development 0.787 0.726 0.755 0.911 0.856 0.883 0.660 0.909
Training 0.775 0.679 0.724 0.909 0.812 0.858 0.617 0.908
Table 3: SemEval 2014 Task 7 evaluation results for our system. Precision (P), recall (R) and F-measure (F) were measured
for Task A while accuracy was measured for Task B.
tance (Levenshtein, 1966) which is defined as the
minimum number of edits needed to convert one
term into another. The edits are in the form of in-
sertions, deletions and substitution of characters.
For example, the term ?cyanotic? can be converted
into ?cyanosis? in minimum two steps by substi-
tuting ?t? for ?s? and ?c? for ?s?, hence the Lev-
enshtein edit distance between these terms is two.
There is a fast dynamic programming based algo-
rithm to compute this. The algorithm also gives
the steps to change one term into another, which
for the above example will be ?START SAME c
SAME y SAME a SAME n SAME o SUBSTI-
TUTE t/s SAME i SUBSTITUTE c/s END?. We
will call such a sequence of steps as an edit dis-
tance pattern.
Our method first computes edit distance pat-
terns between all synonyms of the disorder con-
cepts is UMLS
6
as well as between their men-
tions in the training data and the corresponding
tagged concepts in UMLS. But these patterns are
very specific to the terms they are derived from
and will not directly apply to other terms. Hence
these patterns are generalized next. We define gen-
eralization of two edit distance patterns as their
largest contiguous common part that includes all
the edit operations of insertions, deletions and sub-
stitutions (i.e. generalization can only remove
?SAME?, ?START? and ?END? steps). For exam-
ple, the generalized edit distance pattern of ?cyan-
otic ? cyanosis? and ?thrombotic ? thrombo-
sis? will be ?SAME o SUBSTITUTE t/s SAME i
SUBSTITUTE c/s END?, essentially meaning that
a term that ends with ?otic? can be changed to end
6
Due to the large size of UMLS, we restricted to the sec-
ond of the two concept files in the 2013 UMLS distribution.
with ?osis?. Our method generalizes every pair of
edit distance patterns as well as repeatedly further
generalizes every pair of generalization patterns.
Not all generalization patterns may be good be-
cause some may change the meaning of terms
when applied. Hence our method also evaluates
the goodness of these patterns by counting the
number of positives and negatives. When a pat-
tern is applied to a UMLS term and the resultant
term has the same CUI then it is counted as a pos-
itive. But if the resultant term has a different CUI
then it is counted as a negative. Our system heuris-
tically only retains patterns that have the number
of positives more than the number of negatives and
have at least five positives. Our method learned to-
tal 554 edit distance patterns, Table 2 shows a few
illustrative ones.
These patterns are used as follows to normalize
disease mentions. When exact matching for a dis-
ease mention in the training data and the UMLS
fails, then our system generates its variations by
applying the learned edit distance patterns. These
variations are then searched for exact matching in
the UMLS. If even the variations fail to match then
the variations of possible full-forms (according to
the abbreviation list) are tried, otherwise the men-
tion is declared CUI-less. Note that while our
method learns variations only for disorder men-
tions, it is general and could be used to learn vari-
ations for terms of other types. Finally, because it
is a learning method and it also learns variations
used in the training data, it is capable of learning
variations that are specific to the style or genre of
the clinical notes that constitute the training data.
We note that the problem of matching variations
is analogous to the duplicate detection problem
830
in database records (Bilenko and Mooney, 2003).
But to the our best knowledge, no one has used an
approach to learn patterns of variations based on
edit distances. We used the edit-distance patterns
only for Task B in this work, in future we plan to
also use them in Task A for the features that in-
volve matching with UMLS.
3 Results
The organizers of the SemEval 2014 Task 7 pro-
vided the training, the development and the test
data containing 199, 99 and 133 clinical notes
respectively that included de-identified discharge
summaries, electrocardiogram, echocardiogram
and radiology reports (Pradhan et al., 2013). The
extraction performance in Task A was evaluated
in terms of precision, recall and F-measure for
strict (exact boundaries) and relaxed (overlapping
boundaries) settings. The normalization perfor-
mance in Task B was evaluated in terms of strict
accuracy (fraction of correct normalizations out
of all gold-standard disease mentions) and relaxed
accuracy (fraction of correct normalizations out of
the correct disease mentions extracted in Task A).
Note that a system?s strict accuracy in Task B de-
pends on its performance in Task A because if it
misses to extract a disease mention in Task A then
it will get zero score for its normalization.
Table 3 shows the performance of our system
as determined through the official evaluation by
the organizers. The systems were evaluated on the
test data when trained using both the training and
the development data as well as when trained us-
ing just the training data. When trained using both
the training and the development data, our team
ranked third in Task A and second in Task B con-
sidering the best run of each team if they submit-
ted multiple runs. The ranking was according to
the strict F-measure for Task A and according to
the strict accuracy for Task B. When trained using
just the training data, our team ranked second in
Task A and first in Task B.
We also performed ablation study to determine
the contribution of different components of our
system towards its performance. Since the gold-
standard annotations for the test data were not
made available to the participants, we used the de-
velopment data for testing for the ablation study.
Table 4 shows the results (strict) for Task A when
various groups of features (shown in Table 1) are
excluded one at a time. It can be noted that lex-
ical group of features were most important with-
Features P R F
All 0.829 0.673 0.743
All - Lexical 0.779 0.569 0.658
All - Semantic 0.824 0.669 0.738
All - MetaMap 0.810 0.648 0.720
All - Lemmatization 0.825 0.666 0.737
All - Abbreviations 0.828 0.668 0.740
Table 4: Ablation study results for Task A showing how the
performance is affected by excluding various feature groups
(shown in Table 1). Development data was used for testing.
Only strict precision (P), recall (R) and F-measure (F) are
shown.
Component Accuracy
Training 78.1
UMLS 83.8
Training + UMLS 88.8
Training + Patterns 86.3
UMLS + Patterns 85.2
Training + UMLS + Patterns 89.5
Table 5: Performance on Task B obtained by combinations
of exactly matching the mentions in the training data, exactly
matching in the UMLS and using learned edit distance pat-
terns for approximately matching in the UMLS. Development
data was used for testing with gold-standard disease men-
tions.
out which the performance drops significantly.
MetaMap matches were the next most important
group of features. Each of the remaining feature
groups improves the performance by only small
amount.
Table 5 shows the performance on Task B when
disease mentions are exactly matched in the train-
ing data, exactly matched in the UMLS and ap-
proximately matched in the UMLS using edit dis-
tance patterns, as well as their combinations. In
order to evaluate the performance of our system
on Task B independent of its performance on Task
A, we used gold-standard disease mentions in the
development data as input for Task B in which
case the strict and relaxed accuracies are equal. It
may be noted that adding edit distance patterns im-
proves the performance in each case.
4 Conclusions
We participated in the SemEval 2014 Task 7
of disorder mention extraction and normalization
from clinical text. Our system used conditional
random fields as the learning method for the ex-
traction task with various lexical, semantic and
MetaMap based features. We introduced a new
method to do approximate matching for normal-
ization that learns general patterns of variations
using edit distances. Our system performed com-
petitively on both the tasks.
831
References
Alan R Aronson. 2001. Effective mapping of biomed-
ical text to the UMLS Metathesaurus: the MetaMap
program. In Proceedings of the AMIA Symposium,
page 17. American Medical Informatics Associa-
tion.
Mikhail Bilenko and Raymond J Mooney. 2003.
Adaptive duplicate detection using learnable string
similarity measures. In Proceedings of the ninth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 39?48.
ACM.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of 18th International
Conference on Machine Learning (ICML-2001),
pages 282?289, Williamstown, MA.
Vladimir I Levenshtein. 1966. Binary codes capable
of correcting deletions, insertions and reversals. In
Soviet physics doklady, volume 10, page 707.
Naoaki Okazaki. 2007. CRFsuite: A fast imple-
mentation of Conditional Random Fields (CRFs),
http://www.chokkan.org/software/crfsuite/.
Sameer Pradhan, Noemie Elhadad, B South, David
Martinez, Lee Christensen, Amy Vogel, Hanna
Suominen, W Chapman, and Guergana Savova.
2013. Task 1: ShARe/CLEF eHealth Evaluation
Lab 2013. Online Working Notes of CLEF, CLEF,
230.
Hua Xu, Peter D Stetson, and Carol Friedman. 2012.
Combining corpus-derived sense profiles with esti-
mated frequency information to disambiguate clini-
cal abbreviations. In AMIA Annual Symposium Pro-
ceedings, volume 2012, page 1004. American Med-
ical Informatics Association.
832
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 33?40
Manchester, August 2008
Transforming Meaning Representation Grammars to Improve Semantic
Parsing
Rohit J. Kate
Department of Computer Sciences?
The University of Texas at Austin
1 University Station C0500
Austin, TX 78712-0233, USA
rjkate@cs.utexas.edu
Abstract
A semantic parser learning system learns
to map natural language sentences into
their domain-specific formal meaning rep-
resentations, but if the constructs of the
meaning representation language do not
correspond well with the natural language
then the system may not learn a good se-
mantic parser. This paper presents ap-
proaches for automatically transforming a
meaning representation grammar (MRG)
to conform it better with the natural lan-
guage semantics. It introduces grammar
transformation operators and meaning rep-
resentation macros which are applied in an
error-driven manner to transform an MRG
while training a semantic parser learning
system. Experimental results show that the
automatically transformed MRGs lead to
better learned semantic parsers which per-
form comparable to the semantic parsers
learned using manually engineered MRGs.
1 Introduction
Semantic parsing is the task of converting natural
language (NL) sentences into their meaning repre-
sentations (MRs) which a computer program can
execute to perform some domain-specific task, like
controlling a robot, answering database queries
etc. These MRs are expressed in a formal mean-
ing representation language (MRL) unique to the
domain to suit the application, like some specific
command language to control a robot or some
?Alumnus at the time of submission.
?c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
query language to execute database queries. A ma-
chine learning system for semantic parsing takes
NL sentences paired with their respective MRs as
training data and induces a semantic parser which
can then map novel NL sentences into their MRs.
The grammar of an MRL, which we will call
meaning representation grammar (MRG), is as-
sumed to be deterministic and context-free which
is true for grammars of almost all the computer
executable languages. A semantic parsing learn-
ing system typically exploits the given MRG of the
MRL to learn a semantic parser (Kate andMooney,
2006; Wong and Mooney, 2006). Although in dif-
ferent ways, but the systems presented in these pa-
pers learn how the NL phrases relate to the pro-
ductions of the MRG, and using this information
they parse a test sentence to compositionally gen-
erate its best MR. In order to learn a good seman-
tic parser, it is necessary that the productions of
the MRG accurately represent the semantics be-
ing expressed by the natural language. However,
an MRL and its MRG are typically designed to
best suit the application with little consideration
for how well they correspond to the semantics of
a natural language.
Some other semantic parser learning systems
which need MRL in the form of Prolog (Tang
andMooney, 2001) or ?-calculus (Zettlemoyer and
Collins, 2007; Wong and Mooney, 2007) do not
use productions of the MRG but instead use pred-
icates of the MRL. However, in order to learn a
good semantic parser, they still require that these
predicates correspond well with the semantics of
the natural language. There are also systems which
learn semantic parsers from more detailed train-
ing data in the form of semantically augmented
parse trees of NL sentences in which each inter-
nal node has a syntactic and a semantic label (Ge
33
(a) NL: If the ball is in our midfield then player 5 should go to (-5,0).
MR: (bpos (rec (pt -32 -35)(pt 0 35))
(do (player our {5})(pos (pt -5 0))))
(b) NL: Which is the longest river in Texas?
MR: answer(longest(river(loc_2(stateid(?Texas?)))))
(c) NL: Which is the longest river in Texas?
MR: select river.name from river where
river.traverse=?Texas? and river.length=
(select max(river.length) from river
where river.traverse=?Texas?);
Figure 1: Examples of NL sentences and their MRs from
(a) the CLANG domain (b) GEOQUERY domain with func-
tional MRL (c) GEOQUERY domain with SQL.
and Mooney, 2005; Nguyen et al, 2006). For these
systems to work well, it is also necessary that the
semantic labels of the MRL correspond well with
natural language semantics.
If the MRG of a domain-specific MRL does not
correspond well with natural language semantics
then manually re-engineering the MRG to work
well for semantic parsing is a tedious task and re-
quires considerable domain expertise. In this pa-
per, we present methods to automatically trans-
form a given MRG to make it more suitable for
learning semantic parsers. No previous work ad-
dresses this issue to our best knowledge. We intro-
duce grammar transformation operators and mean-
ing representation macros to transform an MRG.
We describe how these are applied in an error-
driven manner using the base semantic parsing
learning algorithm presented in (Kate andMooney,
2006) resulting in a better learned semantic parser.
Our approach, however, is general enough to im-
prove any semantic parser learning system which
uses productions of the MRG. We present exper-
imental results with three very different MRLs to
show how these grammar transformations improve
the semantic parsing performance.
2 Background
The following subsection gives some examples of
semantic parsing domains and their corresponding
MRLs and illustrates why incompatibility between
MRGs and natural language could hurt semantic
parsing. The next subsection then briefly describes
a base semantic parser learning system which we
use in our experiments.
2.1 MRLs and MRGs for Semantic Parsing
Figure 1 (a) gives an example of a natural lan-
guage sentence and its corresponding MR in an
MRL called CLANG which is a formal declar-
REGION? ( rec POINT POINT )
POINT? ( pt NUM NUM )
NUM? -32 NUM? -35
POINT? ( pt NUM NUM )
NUM? 0 NUM? 35
Figure 2: The parse for the CLANG expression ?(rec (pt
-32 -35) (pt 0 35))? corresponding to the natural language ut-
terance ?our midfield? using its original MRG.
ative language with LISP-like prefix notation
designed to instruct simulated soccer players in
the RoboCup1 Coach Competition. The MRL
and its MRG was designed by the Coach Com-
petition community (Chen et al, 2003) to suit
the requirements of their application independent
of how well the MRG conforms with the natural
language semantics. They were, in fact, not
aware that later (Kate et al, 2005) this will be
introduced as a test domain for learning semantic
parsers. In this original MRG for CLANG, there
are several constructs which do not correspond
well with their meanings in the natural language.
For example, the MR expression of the rectangle
(rec (pt -32 -35) (pt 0 35)) from
the example MR in Figure 1 (a), whose parse ac-
cording to the original MRG is shown in Figure 2,
corresponds to the NL utterance ?our midfield?. In
the parse tree, the nodes are the MRG productions
and the tokens in upper-case are non-terminals
of the MRG while the tokens in lower-case are
terminals of the MRG, this convention will be
used throughout the paper. As can be seen,
the numbers as well as the productions in the
parse of the MR expression do not correspond to
anything in its natural language utterance. It is
also impossible to derive a semantic parse tree
of this MR expression over its natural language
utterance because there are not enough words in
it to cover all the productions present in the MR
parse at the lowest level. To alleviate this problem,
the provided MRG was manually modified (Kate
et al, 2005) to make it correspond better with
the natural language by replacing such long MR
expressions for soccer regions by shorter expres-
sions like (midfield our)2. This new MRG
was used in all the previous work which uses the
CLANG corpus. In the next sections of the paper,
we will present methods to automatically obtain a
1http://www.robocup.org
2The names for the new tokens introduced were chosen for
readability and their similarity to the natural language words
is inconsequential for learning semantic parsers.
34
(a) ANSWER? answer ( RIVER )
RIVER? longest ( RIVER )
RIVER? river ( LOCATIONS )
LOCATIONS? loc 2 ( STATE )
STATE? STATEID
STATEID? stateid ( ?Texas? )
(b) ANSWER? answer ( RIVER )
RIVER? QUALIFIER ( RIVER )
QUALIFIER? longest RIVER? river ( LOCATIONS )
LOCATIONS? LOC 2 ( STATE)
LOC 2? loc 2 STATE? STATEID
STATEID? stateid ( ?Texas? )
Figure 3: Different parse trees obtained for the MR
?answer(longest(river(loc 2(stateid(?Texas?)))))? correspond-
ing to the NL sentence ?Which is the longest river in Texas??
using (a) a simple MRG (b) a manually designed MRG.
better MRG which corresponds well with the NL
semantics.
Figure 1 (b) shows an NL sentence and its MR
from the GEOQUERY domain (Zelle and Mooney,
1996) which consists of a database of U.S. geo-
graphical facts about which a user can query. The
MRL used for GEOQUERY in some of the previ-
ous work is a variable-free functional query lan-
guage, that was constructed from the original MRs
in Prolog (Kate et al, 2005). From this MRL, the
MRG was then manually written so that its pro-
ductions were compatible with the semantics ex-
pressible in natural language. This MRG was dif-
ferent from some simple MRG one would other-
wise design for the MRL. Figure 3 (a) shows the
parse tree obtained using a simple MRG for the
MR shown in Figure 1 (b). The MR parse ob-
tained using the simple MRG is more like a linear
chain which means that in a semantic parse of the
NL sentence each production will have to corre-
spond to the entire sentence. But ideally, different
productions should correspond to the meanings of
different substrings of the sentence. Figure 3 (b)
shows a parse tree obtained using the manually de-
signed MRG in which the productions QUALIFIER
? longest and LOC 2 ? loc 2 would correspond to
the semantic concepts of ?longest? and ?located
in? that are expressible in natural language.
Finally, Figure 1 (c) shows the same NL sen-
tence from the GEOQUERY domain but the MR
in SQL which is the standard database query lan-
guage. The inner expression finds the length of the
longest river in Texas and then the outer expres-
sion finds the river in Texas which has that length.
Due to space restriction, we are not showing the
parse tree for this SQL MR, but its incompatibil-
ity with the NL sentence can be seen from the MR
itself because part of the query repeats itself with
?Texas? appearing twice while in the NL sen-
tence everything is said only once.
2.2 KRISP: A Semantic Parser Learning
System
We very briefly describe the semantic parser learn-
ing system, KRISP (Kate and Mooney, 2006),
which we will use as a base system for transform-
ing MRGs, we however note that the MRG trans-
formation methods presented in this paper are gen-
eral enough to work with any system which learns
semantic parser using MRGs. KRISP (Kernel-
based Robust Interpretation for Semantic Parsing)
is a supervised learning system for semantic pars-
ing which takes NL sentences paired with their
MRs as training data. The productions of the MRG
are treated like semantic concepts. For each of
these productions, a Support-Vector Machine clas-
sifier is trained using string similarity as the ker-
nel (Lodhi et al, 2002). Each classifier can then
estimate the probability of any NL substring rep-
resenting the semantic concept for its production.
During semantic parsing, the classifiers are called
to estimate probabilities on different substrings of
the sentence to compositionally build the most
probable MR parse over the entire sentence with
its productions covering different substrings of the
sentence. KRISP was shown to perform competi-
tively with other existing semantic parser learning
systems and was shown to be particularly robust to
noisy NL input.
3 Transforming MRGs Using Operators
This section describes an approach to transform
an MRG using grammar transformation operators
to conform it better with the NL semantics. The
following section will present another approach
for transforming an MRG using macros which is
sometimes more directly applicable.
The MRLs used for semantic parsing are always
assumed to be context-free which is true for al-
most all executable computer languages. There
has been some work in learning context-free gram-
mars (CFGs) for a language given several exam-
35
ples of its expressions (Lee, 1996). Most of the
approaches directly learn a grammar from the ex-
pressions but there also have been approaches that
first start with a simple grammar and then trans-
form it using suitable operators to a better gram-
mar (Langley and Stromsten, 2000). The goodness
for a grammar is typically measured in terms of its
simplicity and coverage. Langley and Stromsten
(2000) transform syntactic grammars for NL sen-
tences. To our best knowledge, there is no previous
work on transforming MRGs for semantic parsing.
For this task, since an initial MRG is always given
with the MRL, there is no need to first learn it from
its MRs. The next subsection describes the opera-
tors our method uses to transform an initial MRG.
The subsection following that then describes how
and when the operators are applied to transform the
MRG during training. Our criteria for goodness of
an MRG is the performance of the semantic parser
learned using that MRG.
3.1 Transformation Operators
We describe five transformation operators which
are used to transform an MRG. Each of these op-
erators preserves the coverage of the grammar,
i.e. after application of the operator, the trans-
formed grammar generates the same language that
the previous grammar generated3. The MRs do
not change but only the way they are parsed may
change because of grammar transformations. This
is important because the MRs are to be used in an
application and hence should not be changed.
1. Create Non-terminal from a Terminal
(CreateNT): Given a terminal symbol t in the
grammar, this operator adds a new production
T ? t to it and replaces all the occurrences of
the terminal t in all the other productions by the
new non-terminal T . In the context of seman-
tic parsing learning algorithm, this operator intro-
duces a new semantic concept the previous gram-
mar was not explicit about. For example, this oper-
ator may introduce a production (a semantic con-
cept) LONGEST ? longest to the simple grammar
whose parse was shown in Figure 3 (a). This is
close to the production QUALIFIER? longest of the
manual grammar used in the parse shown in Fig-
ure 3 (b).
2. Merge Non-terminals (MergeNT): This op-
erator merges n non-terminals T
1
, T
2
, ..., T
n
, by
introducing n productions T ? T
1
, T ? T
2
, ...,
3This is also known as weak equivalence of grammars.
T ? T
n
where T is a new non-terminal. All the
occurrences of the merged non-terminals on the
right-hand-side (RHS) of all the remaining produc-
tions are then replaced by the non-terminal T . In
order to ensure that this operator preserves the cov-
erage of the grammar, before applying it, it is made
sure that if one of these non-terminals, say T
1
, oc-
curs on the RHS of a production pi
1
then there also
exist productions pi
2
, ..., pi
n
which are same as pi
1
except that T
2
, ..., T
n
respectively occur in them
in place of T
1
. If this condition is violated for any
production of any of the n non-terminals then this
operator is not applicable. This operator enables
generalization of some non-terminals which occur
in similar contexts leading to generalization of pro-
ductions in which they occur on the RHS. For ex-
ample, this operator may generalize non-terminals
LONGEST and SHORTEST in GEOQUERY MRG to
form QUALIFIER4 ? LONGEST and QUALIFIER ?
SHORTEST productions.
3. Combine Two Non-terminals (Combi-
neNT): This operator combines two non-terminals
T
1
and T
2
into one new non-terminal T by intro-
ducing a new production T ? T
1
T
2
. All the
instances of T
1
and T
2
occurring adjacent in this
order on the RHS (with at least one more non-
terminal5) of all the other productions are replaced
by the new non-terminal T . For example, the pro-
duction A? a B T
1
T
2
will be changed to A? a
B T . This operator will not eliminate other occur-
rences of T
1
and T
2
on the RHS of other produc-
tions in which they do not occur adjacent to each
other. In the context of semantic parsing, this op-
erator adds an extra level in the MR parses which
does not seem to be useful in itself, but later if
the non-terminals T
1
and T
2
get eliminated (by the
application of the DeleteProd operator described
shortly), this operator will be combining the con-
cepts represented by the two non-terminals.
4. Remove Duplicate Non-terminals (Re-
moveDuplNT): If a production has the same non-
terminal appearing twice on its RHS then this op-
erator adds an additional production which differs
from the first production in that it has only one oc-
currence of that non-terminal. For example, if a
production is A ? b C D C, then this operator
will introduce a new production A ? b C D re-
4A system generated name will be given to the new non-
terminal.
5Without the presence of an extra non-terminal on the
RHS, this change will merely add redundancy to the parse
trees using this production.
36
moving the second occurrence of the non-terminal
C. This operator is applied only when the subtrees
under the duplicate non-terminals of the produc-
tion are often found to be the same in the parse
trees of the MRs in the training data. As such this
operator will change the MRL the new MRG will
generate, but this can be easily reverted by appro-
priately duplicating the subtrees in its generated
MR parses in accordance to the original produc-
tion. This operator is useful during learning a se-
mantic parser because it eliminates the type of in-
compatibility between MRs and NL sentences il-
lustrated with Figure 1 (c) in Subsection 2.1.
5. Delete Production (DeleteProd): This last
operator deletes a production and replaces the oc-
currences of its left-hand-side (LHS) non-terminal
with its RHS in the RHS of all the other produc-
tions. In terms of semantic parsing, this operator
eliminates the need to learn a semantic concept. It
can undo the transformations obtained by the other
operators by deleting the new productions they in-
troduce.
We note that the CombineNT and MergeNT op-
erators are same as the two operators used by Lan-
gley and Stromsten (2000) to search a good syntac-
tic grammar for natural language sentences from
the space of its possible grammars. We also note
that the applications of CreateNT and CombineNT
operators can reduce a CFG to its Chomsky nor-
mal form6, and conversely, because of the reverse
transformations achieved by the DeleteProd opera-
tor, a Chomsky normal form of a CFG can be con-
verted into any other CFG which accepts the same
language.
3.2 Applying Transformation Operators
In order to transform an MRG to improve semantic
parsing, since a simple hill-climbing type approach
to search the space of all possible MRGs will be
computationally very intensive, we use the follow-
ing error-driven heuristic search which is faster al-
though less thorough.
First, using the provided MRG and the training
data, a semantic parser is trained using KRISP. The
trained semantic parser is applied to each of the
training NL sentences. Next, for each production pi
in the MRG, two values total
pi
and incorrect
pi
are
computed. The value total
pi
counts how many MR
parses from the training examples use the produc-
6In which all the productions are of the form A ? a or
A? B C.
tion pi. The value incorrect
pi
counts the number
of training examples for which the semantic parser
incorrectly uses the production pi, i.e. it either did
not include the production pi in the parse of the MR
it produces when the correct MR?s parse included
it, or it included the production pi when it was not
present in the correct MR?s parse. These two statis-
tics for a production indicate how well the seman-
tic parser was able to use the production in seman-
tic parsing. If it was not able to use a production pi
well, then the ratio incorrect
pi
/total
pi
, which we
call mistakeRatio
pi
, will be high indicating that
some change needs to be made to that production.
After computing these values for all the produc-
tions, the procedure described below for applying
the first type of operator is followed. After this,
the MRs in the training data are re-parsed using
the newMRG, the semantic parser is re-trained and
the total
pi
and incorrect
pi
values are re-computed.
Next, the procedure for applying the next operator
is followed and so on. The whole process is re-
peated for a specified number of iterations. In the
experiments, we found that the performance does
not improve much after two iterations.
1. Apply CreateNT: For each terminal t in the
grammar, total
t
and incorrect
t
values are com-
puted by summing up the corresponding values for
all the productions in which t occurs on the RHS
with at least one non-terminal7. If total
t
is greater
than ? (a parameter) and mistakeRatio
t
=
incorrect
t
/total
t
is greater than ? (another pa-
rameter), then the CreateNT operator is applied,
provided the production T ? t is not already
present.
2. Apply MergeNT: All the non-terminals oc-
curring on the RHS of all those productions pi are
collected whose mistakeRatio
pi
value is greater
than ? and whose total
pi
value is greater than ?.
The set of these non-terminals is then partitioned
such that the criteria for applying the MergeNT
is satisfied by the non-terminals in each partition
with size at least two. The MergeNT operator is
then applied to the non-terminals in each partition
with size at least two.
3. Apply CombineNT: For every non-terminal
pair T
1
and T
2
, total
T
1
T
2
and incorrect
T
1
T
2
val-
ues are computed by summing their correspond-
ing values for the productions in which the two
non-terminals are adjacent in the RHS in the
7Without a non-terminal on the RHS, the operator will
only add a redundant level to the parses which use this pro-
duction.
37
presence of at least one more non-terminal. If
mistakeRatio
T
1
T
2
= incorect
T
1
T
2
/total
T
1
T
2
is
greater than ? and total
T
1
T
2
is greater than ?, then
the CombineNT operator is applied to these two
non-terminals.
4. Apply RemoveDuplNT: If a production
pi has duplicate non-terminals on the RHS under
which the same subtrees are found in the MR parse
trees of the training data more than once then this
operator is applied provided its mistakeRatio
pi
is
greater than ? and total
pi
is greater than ?.
5. Apply DeleteProd: The DeleteProd opera-
tor is applied to all the productions pi and whose
mistakeRatio
pi
is greater than ? and total
pi
is
greater than ?. This step simply deletes the pro-
ductions which are mostly incorrectly used.
For the experiments, we set the ? parameter to
0.75 and ? parameter to 5, these values were de-
termined through pilot experiments.
4 Transforming MRGs Using Macros
As was illustrated with Figure 2 in Subsection 2.1,
sometimes there can be large parses for MR ex-
pressions which do not correspond well with their
semantics in the natural language. While it is pos-
sible to transform the MRG using the operators
described in the previous section to reduce a sub-
tree of the parse to just one production which will
then correspond directly to its meaning in the nat-
ural language, it will require a particular sequence
of transformation operators to achieve this which
may rarely happen during the heuristic search used
in the MRG transformation algorithm. In this sec-
tion, we describe a more direct way of obtaining
such transformations using macros.
4.1 Meaning Representation Macros
A meaning representation macro for an MRG is a
production formed by combining two or more ex-
isting productions of the MRG. For example, for
the CLANG example shown in Figure 2, the pro-
duction REGION? (rec(pt -32 -35)(pt 0 35)) is a mean-
ing representation macro. There could also be non-
terminals on its RHS. From an MR parse drawn
with non-terminals at the internal nodes (instead of
productions), a macro can be derived from a sub-
tree8 rooted at any of the internal nodes by making
its root as the LHS non-terminal and the left-to-
right sequence formed by its leaves (which could
8Each node of a subtree must either include all the chil-
dren nodes of the corresponding node from the original tree
or none of them.
be non-terminals) as the RHS. We use the follow-
ing error-driven procedure to introduce macros in
the MRG in order to improve the performance of
semantic parsing.
4.2 Learning Meaning Representation
Macros
A semantic parser is first learned from the train-
ing data using KRISP and the given MRG. The
learned semantic parser is then applied to the train-
ing sentences and if the system can not produce
any parse for a sentence then the parse tree of its
corresponding MR is included in a set called failed
parse trees. Common subtrees in these failed parse
trees are likely to be good candidates for introduc-
ing macros. Then a set of candidate trees is cre-
ated as follows. This set is first initialized to the
set of failed parse trees. The largest common sub-
tree of every pair of trees in the candidate trees is
then also included in this set if it is not an empty
tree. The process continues with the newly added
trees until no new tree can be included. This pro-
cess is similar to the repeated bottom-up general-
ization of clauses used in the inductive logic pro-
gramming system GOLEM (Muggleton and Feng,
1992). Next, the trees in this set are sorted based
on the number of failed parse trees of which they
are a subtree. The trees which are part of fewer
than ? subtrees are removed. Then in highest to
lowest order, the trees are selected one-by-one to
form macros, provided their height is greater than
two (otherwise it will be an already existing MRG
production) and an already selected tree is not its
subtree. A macro is formed from a tree by mak-
ing the non-terminal root of the tree as its LHS
non-terminal and the left-to-right sequence of the
leaves as its RHS.
These newly formed macros (productions) are
then included in the MRG. The MRs in the train-
ing data are re-parsed and the semantic parser is
re-trained using the new MRG. In order to delete
the macros which were not found useful, a pro-
cedure similar to the application of DeleteProd is
used. The total
pi
and incorrect
pi
values for all the
macros are computed in a manner similar to de-
scribed in the previous section. The macros for
which mistakeRatio
pi
= total
pi
/incorrect
pi
is
greater than ? and total
pi
is greater than ? are re-
moved. This whole procedure of adding and delet-
ing macros is repeated a specified number of it-
erations. In the experiments, we found that two
38
 30
 40
 50
 60
 70
 80
 90
 100
 0  10  20  30  40  50  60  70  80  90  100
Pr
ec
is
io
n
Recall
Manual grammar
Transformed grammar
Initial grammar
Figure 4: The results comparing the performances of the
learned semantic parsers on the GEOUQERY domain with the
functional query language using different MRGs.
 30
 40
 50
 60
 70
 80
 90
 100
 0  10  20  30  40  50  60  70  80  90  100
Pr
ec
is
io
n
Recall
Transformed grammar
Initial grammar
Figure 5: The results comparing the performances of
the learned semantic parsers on the GEOUQERY domain with
SQL as the MRL using different MRGs.
iterations are usually sufficient.
5 Experiments
We tested our MRG transformation methods with
MRGs of three different MRLs which were de-
scribed in the Background section. In each case,
we first transformed the given MRG using macros
and then using grammar transformation operators.
The training and testing was done using standard
10-fold cross-validation and the performance was
measured in terms of precision (the percentage of
generated MRs that were correct) and recall (the
percentage of all sentences for which correct MRs
were obtained). Since we wanted to evaluate how
the grammar transformation changes the perfor-
mance on the semantic parsing task, in each of
the experiments, we used the same system, KRISP,
and compared how it performs when trained using
different MRGs for the same MRL. Since KRISP
assigns confidences to the MRs it generates, an en-
tire range of precision-recall trade-off was plotted
by measuring precision and recall values at various
confidence levels.
Figure 4 shows the results on the GEOQUERY
domain using the functional query language whose
 30
 40
 50
 60
 70
 80
 90
 100
 0  10  20  30  40  50  60  70  80  90  100
Pr
ec
is
io
n
Recall
Manual grammar
Transformed grammar
Initial grammar
Figure 6: The results comparing the performances of the
learned semantic parsers on the CLANG corpus using different
MRGs.
corpus contained total 880 NL-MR pairs. As can
be seen, the performance of the semantic parser
that KRISP learns when trained using the initial
simple MRG for the MRL is not good. But
when that MRG is transformed, the performance
of the semantic parser dramatically improves and
is very close to the performance obtained with the
manually-engineered grammar. The macro trans-
formations did not help improve the performance
with this MRG, and most of the the performance
gain was obtained because of the CreateNT and
DeleteProd operators.
We next tested our MRG transformation algo-
rithm on SQL as the MRL for the GEOQUERY do-
main. This corpus contains 700 NL-MR pairs in
which the NL sentences were taken from the orig-
inal 880 examples. This corpus was previously
used to evaluate the PRECISION system (Popescu
et al, 2003), but since that system is not a machine
learning system, its results cannot be directly com-
pared with ours. The initial MRG we used con-
tained the basic SQL productions. Figure 5 shows
that results improve by a large amount after MRG
transformations. We did not have any manually-
engineered MRG for SQL for this domain avail-
able to us. With this MRG, most of the improve-
ment was obtained using the macros and the Re-
moveDuplNT transformation operator.
Finally, we tested our MRG transformation al-
gorithm on the CLANG domain using its origi-
nal MRG in which all the chief regions of the
soccer field were in the form of numeric MR ex-
pressions which do not correspond to their mean-
ings in the natural language. Its corpus contains
300 examples of NL-MR pairs. Figure 6 shows
the results. After applying the MRG transforma-
tions the performance improved by a large margin.
The gain was due to transformations obtained us-
39
ing macros while the grammar transformation op-
erators did not help with this MRG. Although the
precision was lower for low recall values, the re-
call increased by a large quantity and the best F-
measure improved from 50% to 63%. But the per-
formance still lagged behind that obtained using
the manually-engineered MRG. The main reason
for this is that the manual MRG introduced some
domain specific expressions, like left, right,
left-quarter etc., which correspond directly
to their meanings in the natural language. On
the other hand, the only way to specify ?left? of
a region using the original CLANG MRG is by
specifying the coordinates of the left region, like
(rec(pt -32 -35)(pt 0 0)) is the left of
(rec (pt -32 -35) (pt 0 35)) etc. It
is not possible to learn the concept of ?left? from
such expressions even with MRG transformations.
6 Conclusions
Ameaning representation grammar which does not
correspond well with the natural language seman-
tics can lead to a poor performance by a learned
semantic parser. This paper presented grammar
transformation operators and meaning representa-
tion macros using which the meaning representa-
tion grammar can be transformed to make it better
conform with the semantics of natural language.
Experimental results on three different grammars
demonstrated that the performance on semantic
parsing task can be improved by large amounts by
transforming the grammars.
Acknowledgments
I would like to thank Raymond Mooney for many
useful discussions regarding the work described in
this paper.
References
Chen et al 2003. Users manual: RoboCup soccer server manual for soc-
cer server version 7.07 and later. Available at http://sourceforge.
net/projects/sserver/.
Ge, R. and R. J. Mooney. 2005. A statistical semantic parser that integrates
syntax and semantics. In Proc. of CoNLL-2005, pages 9?16, Ann Arbor,
MI.
Kate, R. J. and R. J. Mooney. 2006. Using string-kernels for learning se-
mantic parsers. In Proc. of COLING/ACL-2006, pages 913?920, Sydney,
Australia.
Kate, R. J., Y. W.Wong, and R. J. Mooney. 2005. Learning to transform natural
to formal languages. In Proc. of AAAI-2005, pages 1062?1068, Pittsburgh,
PA.
Langley, Pat and Sean Stromsten. 2000. Learning context-free gramamr with a
simplicity bias. In Proc. of ECML-2000, pages 220?228, Barcelona, Spain.
Lee, Lillian. 1996. Learning of context-free languages: A survey of the lit-
erature. Technical Report TR-12-96, Center for Research in Computing
Technology, Harvard University.
Lodhi, Huma, Craig Saunders, John Shawe-Taylor, Nello Cristianini, and Chris
Watkins. 2002. Text classification using string kernels. Journal of Machine
Learning Research, 2:419?444.
Muggleton, Stephen and C. Feng. 1992. Efficient induction of logic programs.
In Muggleton, Stephen, editor, Inductive Logic Programming, pages 281?
297. Academic Press, New York.
Nguyen, Le-Minh, Akira Shimazu, and Xuan-Hieu Phan. 2006. Semantic
parsing with structured SVM ensemble classification models. In Proc. of
COLING/ACL 2006 Main Conf. Poster Sessions, pages 619?626, Sydney,
Australia.
Popescu, Ana-Maria, Oren Etzioni, and Henry Kautz. 2003. Towards a theory
of natural language interfaces to databases. In Proc. of IUI-2003, pages
149?157, Miami, FL.
Tang, L. R. and R. J. Mooney. 2001. Using multiple clause constructors in in-
ductive logic programming for semantic parsing. In Proc. of ECML-2001,
pages 466?477, Freiburg, Germany.
Wong, Y. W. and R. Mooney. 2006. Learning for semantic parsing with statis-
tical machine translation. In Proc. of HLT/NAACL-2006, pages 439?446,
New York City, NY.
Wong, Y. W. and R. J. Mooney. 2007. Learning synchronous grammars for
semantic parsing with lambda calculus. In Proc. of ACL-2007, pages 960?
967, Prague, Czech Republic.
Zelle, J. M. and R. J. Mooney. 1996. Learning to parse database queries using
inductive logic programming. In Proc. of AAAI-1996, pages 1050?1055,
Portland, OR.
Zettlemoyer, Luke S. and Michael Collins. 2007. Online learning of relaxed
CCG grammars for parsing to logical form. In Proc. of EMNLP-CoNLL-
2007, pages 678?687, Prague, Czech Republic.
40
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 203?212,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Joint Entity and Relation Extraction using Card-Pyramid Parsing
Rohit J. Kate and Raymond J. Mooney
Department of Computer Science
The University of Texas at Austin
1 University Station C0500
Austin, TX 78712-0233, USA
{rjkate,mooney}@cs.utexas.edu
Abstract
Both entity and relation extraction can
benefit from being performed jointly, al-
lowing each task to correct the errors of
the other. We present a new method for
joint entity and relation extraction using
a graph we call a ?card-pyramid.? This
graph compactly encodes all possible en-
tities and relations in a sentence, reducing
the task of their joint extraction to jointly
labeling its nodes. We give an efficient la-
beling algorithm that is analogous to pars-
ing using dynamic programming. Exper-
imental results show improved results for
our joint extraction method compared to a
pipelined approach.
1 Introduction
Information extraction (IE) is the task of extract-
ing structured information from text. The two
most common sub-tasks of IE are extracting enti-
ties (like Person, Location and Organization) and
extracting relations between them (like Work For
which relates a Person and an Organization, Org-
Based In which relates an Organization and a Lo-
cation etc.). Figure 1 shows a sample sentence an-
notated with entities and relations. The applica-
tion domain and requirements of the downstream
tasks usually dictate the type of entities and rela-
tions that an IE system needs to extract.
Most work in IE has concentrated on entity ex-
traction alone (Tjong Kim Sang, 2002; Sang and
Meulder, 2003) or on relation extraction assum-
ing entities are either given or previously extracted
(Bunescu et al, 2005; Zhang et al, 2006; Giuliano
et al, 2007; Qian et al, 2008). However, these
tasks are very closely inter-related. While iden-
tifying correct entities is essential for identifying
relations between them, identifying correct rela-
tions can in turn improve identification of entities.
For example, if the relation Work For is identified
with high confidence by a relation extractor, then
it can enforce identifying its arguments as Person
and Organization, about which the entity extractor
might not have been confident.
A brute force algorithm for finding the most
probable joint extraction will soon become in-
tractable as the number of entities in a sentence
grows. If there are n entities in a sentence, then
there are O(n2) possible relations between them
and if each relation can take l labels then there are
O(ln2) total possibilities, which is intractable even
for small l and n. Hence, an efficient inference
mechanism is needed for joint entity and relation
extraction.
The only work we are aware of for jointly ex-
tracting entities and relations is by Roth & Yih
(2004; 2007). Their method first identifies the pos-
sible entities and relations in a sentence using sep-
arate classifiers which are applied independently
and then computes a most probable consistent
global set of entities and relations using linear pro-
gramming. In this paper, we present a different ap-
proach to joint extraction using a ?card-pyramid?
graph. The labeled nodes in this graph compactly
encode the possible entities and relations in a sen-
tence. The task of joint extraction then reduces
to finding the most probable joint assignment to
the nodes in the card-pyramid. We give an ef-
ficient dynamic-programming algorithm for this
task which resembles CYK parsing for context-
free grammars (Jurafsky and Martin, 2008). The
algorithm does a beam search and gives an approx-
imate solution for a finite beam size. A natural
advantage of this approach is that extraction from
a part of the sentence is influenced by extraction
from its subparts and vice-versa, thus leading to a
joint extraction. During extraction from a part of
the sentence it also allows use of features based on
the extraction from its sub-parts, thus leading to a
more integrated extraction. We use Roth & Yih?s
203
John lives in Los Angeles , California and works there for an American company called ABC Inc .
Person Location Location
Live_In
Located_In
Work_For
OrgBased_In
OrgBased_In
Other Organization
   0      1     2     3      4        5        6          7       8        9   10  11        12            13        14      15   16 17  
Live_In
Figure 1: A sentence shown with entities and relations.
Figure 2: A pyramid built out of playing-cards.
(2004; 2007) dataset in our experiments and show
that card-pyramid parsing improves accuracy over
both their approach and a pipelined extractor.
2 Card-Pyramid Parsing for Joint
Extraction
In this section, we first introduce the card-pyramid
structure and describe how it represents entities
and their relations in a sentence. We then describe
an efficient algorithm for doing joint extraction us-
ing this structure.
2.1 Card-Pyramid Structure
We define a binary directed graph we call a card-
pyramid because it looks like a pyramid built out
of playing-cards as shown in Figure 2. A card-
pyramid is a ?tree-like? graph with one root, in-
ternal nodes, and leaves, such that if there are
n leaves, then there are exactly n levels with a
decreasing number of nodes from bottom to top,
leaves are at the lowest level (0) and the root is
at the highest level (n ? 1) (see Figure 3 for an
example). In addition, every non-leaf node at po-
Live_In
Live_In NR OrgBased_In
NR OrgBased_In
Work_For
Location Location Other Organization
Located_In NRNRLevel 1
Level 2
Level 3
Level 4
Person
0
2
10
0 1
0 1 3
(0?0) (3?4) (6?6) (12?12) (15?16)
0 1 2 3 4
2
Level 0
Figure 3: The card-pyramid for the sentence shown in Fig-
ure 1. Levels are shown by the horizontal lines under which
the positions of its nodes are indicated.
sition i in level l is the parent of exactly two nodes
at positions i and i + 1 at level l ? 1. Note that
a card-pyramid is not a tree because many of its
nodes have two parents. A useful property of a
card-pyramid is that a non-leaf node at position i
in level l is always the lowest common ancestor of
the leaves at positions i and l + i.
We now describe how entities and relations in a
sentence are easily represented in a card-pyramid.
We assume that in addition to the given entity
types, there is an extra type, Other, indicating that
the entity is of none of the given types. Similarly,
there is an extra relation type, NR, indicating that
its two entity arguments are not related.
Figure 3 shows the card-pyramid corresponding
to the annotated sentence shown in figure 1. To ob-
tain it, first, all entities present in the sentence are
made leaves of the card-pyramid in the same order
as they appear in the sentence. The label of a leaf
is the type of the corresponding entity. The leaf
also stores the range of the indices of its entity?s
words in the sentence. Note that although there is
no overlap between entities in the given example
(nor in the dataset we used for our experiments),
204
overlapping entities do not pose a problem. Over-
lapping entities can still be ordered and supplied as
the leaves of the card-pyramid. Next, the relation
between every two entities (leaves) is encoded as
the label of their lowest common ancestor. If two
entities are not related, then the label of their low-
est common ancestor is NR. This way, every non-
leaf node relates exactly two entities: the left-most
and right-most leaves beneath it.
2.2 Card-Pyramid Parsing
The task of jointly extracting entities and rela-
tions from a sentence reduces to jointly label-
ing the nodes of a card-pyramid which has all
the candidate entities (i.e. entity boundaries) of
the sentence as its leaves. We call this process
card-pyramid parsing. We assume that all candi-
date entities are given up-front. If needed, candi-
date entities can be either obtained automatically
(Punyakanok and Roth, 2001) or generated using
a simple heuristic, like including all noun-phrase
chunks as candidate entities. Or, in the worst case,
each substring of words in the sentence can be
given as a candidate entity. Liberally including
candidate entities is possible since they can sim-
ply be given the label Other if they are none of the
given types.
In this section we describe our card-pyramid
parsing algorithm whose pseudo-code is shown
in Figure 4. While the process is analogous to
context-free grammar (CFG) parsing, particularly
CYK bottom-up parsing, there are several major
differences. Firstly, in card-pyramid parsing the
structure is already known and the only task is
labeling the nodes, whereas in CFG parsing the
structure is not known in advance. This fact sim-
plifies some aspects of card-pyramid parsing. Sec-
ondly, in CFG parsing the subtrees under a node
do not overlap which simplifies parsing. How-
ever, in card-pyramid parsing there is significant
overlap between the two sub-card-pyramids under
a given node and this overlap needs to be consis-
tently labeled. This could have potentially com-
plicated parsing, but there turns out to be a simple
constant-time method for checking consistency of
the overlap. Thirdly, while CFG parsing parses the
words in a sentence, here we are parsing candi-
date entities. Finally, as described below, in card-
pyramid parsing, a production at a non-leaf node
relates the left-most and right-most leaves beneath
it, while in CFG parsing a production at a non-leaf
node relates its immediate children which could be
other non-leaf nodes.
Parsing requires specifying a grammar for the
card-pyramid. The productions in this grammar
are of two types. For leaf nodes, the produc-
tions are of the form entityLabel ? ce where ce,
which stands for candidate entity, is the only ter-
minal symbol in the grammar. We call these pro-
ductions entity productions. For non-leaf nodes,
the productions are of the form relationLabel ?
entityLabel1 entityLabel2. We call these produc-
tions relation productions. Note that their right-
hand-side (RHS) non-terminals are entity labels
and not other relation labels. From a training
set of labeled sentences, the corresponding card-
pyramids can be constructed using the procedure
described in the previous section. From these
card-pyramids, the entity productions are obtained
by simply reading off the labels of the leaves. A
relation productions is obtained from each non-
leaf node by making the node?s label the produc-
tion?s left-hand-side (LHS) non-terminal and mak-
ing the labels of its left-most and right-most leaves
the production?s RHS non-terminals. For the ex-
ample shown in Figure 3, some of the productions
are Work For? Person Organization, NR? Per-
son Other, OrgBased In? Loc Org, Person? ce,
Location ? ce etc. Note that there could be two
separate productions like Work For ? Person Or-
ganization and Work For ? Organization Person
based on the order in which the entities are found
in a sentence. For the relations which take argu-
ments of the same type, like Kill(Person,Person),
two productions are used with different LHS non-
terminals (Kill and Kill reverse) to distinguish be-
tween the argument order of the entities.
The parsing algorithm needs a classifier for ev-
ery entity production which gives the probabil-
ity of a candidate entity being of the type given
in the production?s LHS. In the pseudo-code,
this classifier is given by the function: entity-
classifier(production, sentence, range). The func-
tion range(r) represents the boundaries or the
range of the word indices for the rth candidate
entity. Similarly, we assume that a classifier is
given for every relation production which gives
the probability that its two RHS entities are re-
lated by its LHS relation. In the pseudo-code it is
the function: relation-classifier(production, sen-
tence, range1, range2, sub-card-pyramid1, sub-
card-pyramid2), where range1 and range2 are the
205
ranges of the word indices of the two entities
and sub-card-pyramid1 and sub-card-pyramid2
are the sub-card-pyramids rooted at its two chil-
dren. Thus, along with the two entities and the
words in the sentence, information from these sub-
card-pyramids is also used in deciding the relation
at a node. In the next section, we further spec-
ify these entity and relation classifiers and explain
how they are trained. We note that this use of
multiple classifiers to determine the most probable
parse is similar to the method used in the KRISP
semantic parser (Kate and Mooney, 2006).
Given the candidate entities in a sentence, the
grammar, and the entity and relation classifiers,
the card-pyramid parsing algorithm tries to find
the most probable joint-labeling of all of its nodes,
and thus jointly extracts entities and their rela-
tions. The parsing algorithm does a beam search
and maintains a beam at each node of the card-
pyramid. A node is represented by l[i][j] in the
pseudo-code which stands for the node in the jth
position in the ith level. Note that at level i, the
nodes range from l[i][0] to l[i][n? i? 1], where n
is the number of leaves. The beam at each node is
a queue of items we call beam elements. At leaf
nodes, a beam element simply stores a possible
entity label with its corresponding probability. At
non-leaf nodes, a beam element contains a possi-
ble joint assignment of labels to all the nodes in
the sub-card-pyramid rooted at that node with its
probability. This is efficiently maintained through
indices to the beam elements of its children nodes.
The parsing proceeds as follows. First, the en-
tity classifiers are used to fill the beams at the leaf
nodes. The add(beam, beam-element) function
adds the beam element to the beam while main-
taining its maximum beam-width size and sorted
order based on the probabilities. Next, the beams
of the non-leaf nodes are filled in a bottom-up
manner. At any node, the beams of its children
nodes are considered and every combination of
their beam elements are tried. To be considered
further, the two possible sub-card-pyramids en-
coded by the two beam elements must have a con-
sistent overlap. This is easily enforced by check-
ing that its left child?s right child?s beam element
is same as its right child?s left child?s beam ele-
ment. If this condition is satisfied, then those re-
lation productions are considered which have the
left-most leaf of the left child and right-most leaf
of the right child as its RHS non-terminals.1 For
every such production in the grammar, 2 the prob-
ability of the relation is determined using the re-
lation classifier. This probability is then multi-
plied by the probabilities of the children sub-card-
pyramids. But, because of the overlap between the
two children, a probability mass gets multiplied
twice. Hence the probability of the overlap sub-
card-pyramid is then suitably divided. Finally, the
estimated most-probable labeling is obtained from
the top beam element of the root node.
We note that this algorithm may not find the
optimal solution but only an approximate solu-
tion owing to a limited beam size, this is unlike
probabilistic CFG parsing algorithms in which the
optimal solution is found. A limitless beam size
will find the optimal solution but will reduce the
algorithm to a computationally intractable brute
force search. The parsing algorithm with a fi-
nite beam size keeps the search computationally
tractable while allowing a joint labelling.
3 Classifiers for Entity and Relation
Extraction
The card-pyramid parsing described in the previ-
ous section requires classifiers for each of the en-
tity and relation productions. In this section, we
describe the classifiers we used in our experiments
and how they were trained.
We use a support vector machine (SVM) (Cris-
tianini and Shawe-Taylor, 2000) classifier for each
of the entity productions in the grammar. An entity
classifier gets as input a sentence and a candidate
entity indicated by the range of the indices of its
words. It outputs the probability that the candi-
date entity is of the respective entity type. Prob-
abilities for the SVM outputs are computed using
the method by Platt (1999). We use all possible
word subsequences of the candidate entity words
as implicit features using a word-subsequence ker-
nel (Lodhi et al, 2002). In addition, we use
the following standard entity extraction features:
the part-of-speech (POS) tag sequence of the can-
didate entity words, two words before and after
the candidate entity and their POS tags, whether
any or all candidate entity words are capitalized,
1These are stored in the beam elements.
2Note that this step enforces the consistency constraint of
Roth and Yih (Roth and Yih, 2004; Roth and Yih, 2007) that
a relation can only be between the entities of specific types.
The grammar in our approach inherently enforces this con-
straint.
206
function Card-Pyramid-Parsing(Sentence,Grammar,entity-classifiers,relation-classifiers)
n = number of candidate entities in S
// Let range(r) represent the range of the indices of the words for the rth candidate entity.
// Let l[i][j] represent the jth node at ith level in the card-pyramid.
// For leaves
// A beam element at a leaf node is (label,probability).
for j = 0 to n // for every leaf
for each entityLabel ? candidate entity ? Grammar
prob = entity-classifier(entityLabel ? candidate entity, S, range(j))
add(l[0][j].beam, (entityLabel,prob))
// For non-leaf nodes
// A beam element at a non-leaf node is (label,probability,leftIndex,rightIndex,leftMostLeaf,rightMostLeaf)
// where leftIndex and rightIndex are the indices in the beams of the left and right children respectively.
for i = 1 to n // for every level above the leaves
for j = 0 to n ? i ? 1 // for every position at a level
// for each combination of beam elements of the two children
for each f ? l[i ? 1][j].beam and g ? l[i ? 1][j + 1].beam
// the overlapped part must be same (overlap happens for i > 1)
if (i == 1||f.rightIndex == g.leftIndex)
for each relationLabel ? f.leftMostLeaf g.rightMostLeaf ? Grammar
// probability of that relation between the left-most and right-most leaf under the node
prob = relation-classifier(relationLabel ? f.leftMostLeaf g.rightMostLeaf , S, range(i), range(i + j), f , g);
prob *= f.probability ? g.probability // multiply probabilities of the children sub-card-pyramids
// divide by the common probability that got multiplied twice
if (i > 1) prob /= l[i ? 2][j + 1].beam[f.rightIndex].probability
add(l[i][j].beam, (relationLabel, prob, index of f , index of g, f.leftMostLeaf , g.rightMostLeaf )
return the labels starting from the first beam element of the root i.e. l[n][0].beam[0]
Figure 4: Card-Pyramid Parsing Algorithm.
whether any or all words are found in a list of en-
tity names, whether any word has sufffix ?ment?
or ?ing?, and finally the alphanumeric pattern of
characters (Collins, 2002) of the last candidate
entity word obtained by replacing each charac-
ter by its character type (lowercase, uppercase or
numeric) and collapsing any consecutive repeti-
tion (for example, the alphanumeric pattern for
CoNLL2010 will be AaA0). The full kernel is
computed by adding the word-subsequence kernel
and the dot-product of all these features, exploit-
ing the convolution property of kernels.
We also use an SVM classifier for each of the
relation productions in the grammar which out-
puts the probability that the relation holds between
the two entities. A relation classifier is applied
at an internal node of a card-pyramid. It takes
the input in two parts. The first part is the sen-
tence and the range of the word indices of its two
entities l and r which are the left-most and the
right-most leaves under it respectively. The sec-
ond part consists of the sub-card-pyramids rooted
at the node?s two children which represent a pos-
sible entity and relation labeling for all the nodes
underneath. In general, any information from the
sub-card-pyramids could be used in the classifier.
We use the following information: pairs of rela-
tions that exist between l and b and between b and
r for every entity (leaf) b that exists between the
two entities l and r. For example, in figure 3,
the relation classifier at the root node which re-
lates Person(0-0) and Organization (15-16) will
take three pairs of relations as the information
from the two sub-card-pyramids of its children:
?Live In?OrgBased In? (with Location(3-4) as
the in-between entity), ?Live In?OrgBased In?
(with Location(6-6) as the in-between entity) and
?NR?NR? (with Other(12-12) as the in-between
entity). This information tells how the two enti-
ties are related to the entities present in between
them. This can affect the relation between the two
entities, for example, if the sentence mentions that
a person lives at a location and also mentions that
an organization is based at that location then that
person is likely to work at that organization. Note
that this information can not be incorporated in a
pipelined approach in which each relation is de-
termined independently. It is also not possible to
incorporate this in the linear programming method
presented in (Roth and Yih, 2004; Roth and Yih,
2007) because that method computes the probabil-
ities of all the relations independently before find-
ing the optimal solution through linear program-
ming. It would also not help to add hard con-
straints to their linear program relating the rela-
tions because they need not always hold.
We add the kernels for each part of the input to
compute the final kernel for the SVM classifiers.
The kernel for the second part of the input is com-
puted by simply counting the number of common
207
pairs of relations between two examples thus im-
plicitly considering every pair of relation (as de-
scribed in the last paragraph) as a feature. For the
first part of the input, we use word-subsequence
kernels which have shown to be effective for re-
lation extraction (Bunescu and Mooney, 2005b).
We compute the kernel as the sum of the word-
subsequence kernels between: the words between
the two entities (between pattern), k (a parame-
ter) words before the first entity (before pattern),
k words after the second entity (after pattern) and
the words from the beginning of the first entity to
the end of the second entity (between-and-entity
pattern). The before, between and after patterns
have been found useful in previous work (Bunescu
and Mooney, 2005b; Giuliano et al, 2007). Some-
times the words of the entities can indicate the re-
lations they are in, hence we also use the between-
and-entity pattern. When a relation classifier is
used at a node, the labels of the leaves beneath it
are already known, so we replace candidate entity
words that are in the between and between-and-
entity3 patterns by their entity labels. This pro-
vides useful information to the relation classifier
and also makes these patterns less sparse for train-
ing.
Given training data of sentences annotated with
entities and relations, the positive and negative ex-
amples for training the entity and relation clas-
sifiers are collected in the following way. First,
the corresponding card-pyramids are obtained for
each of the training sentences as described in sec-
tion 2.1. For every entity production in a card-
pyramid, a positive example is collected for its
corresponding classifier as the sentence and the
range of the entity?s word indices. Similarly, for
every relation production in a card-pyramid, a pos-
itive example is collected for its corresponding
classifier as the sentence, the ranges of the two
entities? word indices and the sub-card-pyramids
rooted at its two children. The positive examples
of a production become the negative examples for
all those productions which have the same right-
hand-sides but different left-hand-sides. We found
that for NR productions, training separate classi-
fiers is harmful because it has the unwanted side-
effect of preferring one label assignment of enti-
ties over another due to the fact that these pro-
ductions gave different probabilities for the ?not-
related? relation between the entities. To avoid
3Except for the two entities at the ends
this, we found that it suffices if all these classi-
fiers for NR productions always return 0.5 as the
probability. This ensures that a real relation will
be preferred over NR if and only if its probability
is greater than 0.5, otherwise nothing will change.
4 Experiments
We conducted experiments to compare our card-
pyramid parsing approach for joint entity and re-
lation extraction to a pipelined approach.
4.1 Methodology
We used the dataset4 created by Roth & Yih (2004;
2007) that was also used by Giuliano et el. (2007).
The sentences in this data were taken from the
TREC corpus and annotated with entities and re-
lations. As in the previous work with this dataset,
in order to observe the interaction between enti-
ties and relations, our experiments used only the
1437 sentences that include at least one relation.
The boundaries of the entities are already supplied
by this dataset. There are three types of entities:
Person (1685), Location (1968) and Organization
(978), in addition there is a fourth type Other
(705), which indicates that the candidate entity is
none of the three types. There are five types of re-
lations: Located In (406) indicates that one Loca-
tion is located inside another Location, Work For
(394) indicates that a Person works for an Orga-
nization, OrgBased In (451) indicates that an Or-
ganization is based in a Location, Live In (521)
indicates that a Person lives at a Location and Kill
(268) indicates that a Person killed another Per-
son. There are 17007 pairs of entities that are not
related by any of the five relations and hence have
the NR relation between them which thus signifi-
cantly outnumbers other relations.
Our implementation uses the LIBSVM5 soft-
ware for SVM classifiers. We kept the noise
penalty parameter of SVM very high (100) as-
suming there is little noise in our data. For the
word-subsequence kernel, we set 5 as the max-
imum length of a subsequence and 0.25 as the
penalty parameter for subsequence gaps (Lodhi et
al., 2002). We used k = 5 words for before and
after patterns for the relation classifiers. These pa-
rameter values were determined through pilot ex-
periments on a subset of the data. We used a beam
4Available at: http://l2r.cs.uiuc.edu/
?cogcomp/Data/ER/conll04.corp
5http://www.csie.ntu.edu.tw/?cjlin/
libsvm/
208
Entity Person Location Organization
Approach Rec Pre F Rec Pre F Rec Pre F
Pipeline 93.6 92.0 92.8 94.0 90.3 92.1 87.9 90.6 89.2
Card-pyramid 94.2 92.1 93.2 94.2 90.8 92.4? 88.7 90.5 89.5
RY07 Pipeline 89.1 88.7 88.6 88.1 89.8 88.9 71.4 89.3 78.7
RY07 Joint 89.5 89.1 89.0 88.7 89.7 89.1 72.0 89.5 79.2
Relation Located In Work For OrgBased In Live In Kill
Approach Rec Pre F Rec Pre F Rec Pre F Rec Pre F Rec Pre F
Pipeline 57.0 71.5 62.3 66.0 74.1 69.7 60.2 70.6 64.6 56.6 68.1 61.7 61.2 91.1 73.1
Card-pyramid 56.7 67.5 58.3 68.3 73.5 70.7 64.1 66.2 64.7 60.1 66.4 62.9? 64.1 91.6 75.2
RY07 Pipeline 56.4 52.5 50.7 44.4 60.8 51.2 42.1 77.8 54.3 50.0 58.9 53.5 81.5 73.0 76.5
RY07 Joint 55.7 53.9 51.3 42.3 72.0 53.1 41.6 79.8 54.3 49.0 59.1 53.0 81.5 77.5 79.0?
Table 1: Results of five-fold cross-validation for entity and relation extraction using pipelined and joint extraction. Boldface
indicates statistical significance (p < 0.1 using paired t-test) when compared to the corresponding value in the other row
grouped with it. Symbol ? indicates statistical significance with p < 0.05. Only statistical significance for F-measures are
indicated. RY07 stands for the ?E ? R? model in (Roth and Yih, 2007).
size of 5 in our card-pyramid parsing algorithm at
which the performance plateaus.
We note that by using a beam size of 1 and by
not using the second part of input for relation clas-
sifiers as described in section 3 (i.e. by ignoring
the relations at the lower levels), the card-parsing
algorithm reduces to the traditional pipelined ap-
proach because then only the best entity label for
each candidate entity is considered for relation ex-
traction. Hence, in our experiments we simply use
this setting as our pipelined approach.
We performed a 5-fold cross-validation to com-
pare with the previous work with this dataset by
Roth & Yih (2007), however, our folds are not
same as their folds which were not available. We
also note that our entity and relation classifiers are
different from theirs. They experimented with sev-
eral models to see the effect of joint inference on
them, we compare with the results they obtained
with their most sophisticated model which they
denote by ?E ? R?. For every entity type and
relation type, we measured Precision (percentage
of output labels correct), Recall (percentage of
gold-standard labels correctly identified) and F-
measure (the harmonic mean of Precision and Re-
call).
4.2 Results and Discussion
Table 1 shows the results of entity and relation ex-
traction. The statistical significance is shown only
for F-measures. We first note that except for the
Kill relation, all the results of our pipelined ap-
proach are far better than the pipelined approach
of (Roth and Yih, 2007), for both entities and rela-
tions. This shows that the entity and relation clas-
sifiers we used are better that the ones they used.
These strong baselines also set a higher ceiling for
our joint extraction method to improve upon.
The entity extraction results show that on all
the entities the card-pyramid parsing approach for
joint extraction obtains a better performance than
the pipelined approach. This shows that entity
extraction benefits when it is jointly done with
relation extraction. Joint extraction using card-
pyramid parsing also gave improvement in perfor-
mance on all the relations except the Located In
relation.6
The results thus show that entity and relation ex-
traction correct some of each other?s errors when
jointly performed. Roth & Yih (2004; 2007) re-
port that 5% to 25% of the relation predictions
of their pipeline models were incoherent, meaning
that the types of the entities related by the relations
are not of the required types. Their joint inference
method corrects these mistakes, hence a part of the
improvement their joint model obtains over their
pipeline model is due to the fact that their pipeline
model can output incoherent relations. Since the
types of the entities a relation?s arguments should
6Through error analysis we found that the drop in the
performance for this relation was mainly because of an un-
usual sentence in the data which had twenty Location entities
in it separated by commas. After incorrectly extracting Lo-
cated In relation between the Location entities at the lower
levels, these erroneous extractions would be taken into ac-
count at higher levels in the card-pyramid, leading to extract-
ing many more incorrect instances of this relation while do-
ing joint extraction. Since this is the only such sentence in the
data, when it is present in the test set during cross-validation,
the joint method never gets a chance to learn not to make
these mistakes. The drop occurs in only that one fold and
hence the overall drop is not found as statistically significant
despite being relatively large.
209
take are known, we believe that filtering out the
incoherent relation predictions of their pipeline
model can improve its precision without hurting
the recall. On the other hand our pipelined ap-
proach never outputs incoherent relations because
the grammar of relation productions enforce that
the relations are always between entities of the re-
quired types. Thus the improvement obtained by
our joint extraction method over our pipelined ap-
proach is always non-trivial.
5 Related Work
To our knowledge, Roth & Yih (2004; 2007) have
done the only other work on joint entity and re-
lation extraction. Their method employs inde-
pendent entity and relation classifiers whose out-
puts are used to compute a most probable consis-
tent global set of entities and relations using lin-
ear programming. One key advantage of our card-
pyramid method over their method is that the clas-
sifiers can take the output of other classifiers under
its node as input features during parsing. This is
not possible in their approach because all classi-
fier outputs are determined before they are passed
to the linear program solver. Thus our approach
is more integrated and allows greater interaction
between dependent extraction decisions.
Miller et al (2000) adapt a probabilistic
context-free parser for information extraction by
augmenting syntactic labels with entity and rela-
tion labels. They thus do a joint syntactic parsing
and information extraction using a fixed template.
However, as designed, such a CFG approach can-
not handle the cases when an entity is involved
in multiple relations and when the relations criss-
cross each other in the sentence, as in Figure 1.
These cases occur frequently in the dataset we
used in our experiments and many other relation-
extraction tasks.
Giuliano et al (2007) thoroughly evaluate the
effect of entity extraction on relation extraction us-
ing the dataset used in our experiments. However,
they employ a pipeline architecture and did not in-
vestigate joint relation and entity extraction. Carl-
son et al (2009) present a method to simultane-
ously do semi-supervised training of entity and re-
lation classifiers. However, their coupling method
is meant to take advantage of the available unsu-
pervised data and does not do joint inference.
Riedel et al (2009) present an approach for ex-
tracting bio-molecular events and their arguments
using Markov Logic. Such an approach could
also be adapted for jointly extracting entities and
their relations, however, this would restrict entity
and relation extraction to the same machine learn-
ing method that is used with Markov Logic. For
example, one would not be able to use kernel-
based SVM for relation extraction, which has been
very successful at this task, because Markov Logic
does not support kernel-based machine learning.
In contrast, our joint approach is independent of
the individual machine learning methods for en-
tity and relation extraction, and hence allows use
of the best machine learning methods available for
each of them.
6 Future Work
There are several possible directions for extend-
ing the current approach. The card-pyramid struc-
ture could be used to perform other language-
processing tasks jointly with entity and rela-
tion extraction. For example, co-reference res-
olution between two entities within a sentence
can be easily incorporated in card-pyramid pars-
ing by introducing a production like coref ?
Person Person, indicating that the two person
entities are the same.
In this work, and in most previous work, re-
lations are always considered between two enti-
ties. However, there could be relations between
more than two entities. In that case, it should
be possible to binarize those relations and then
use card-pyramid parsing. If the relations are be-
tween relations instead of between entities, then
card-pyramid parsing can handle it by considering
the labels of the immediate children as RHS non-
terminals instead of the labels of the left-most and
the right-most leaves beneath it. Thus, it would
be interesting to apply card-pyramid parsing to ex-
tract higher-order relations (such as causal or tem-
poral relations).
Given the regular graph structure of the card-
pyramid, it would be interesting to investigate
whether it can be modeled using a probabilistic
graphical model (Koller and Friedman, 2009). In
that case, instead of using multiple probabilis-
tic classifiers, one could employ a single jointly-
trained probabilistic model, which is theoretically
more appealing and might give better results.
Finally, we note that a better relation classifier
could be used in the current approach which makes
more use of linguistic information. For example,
210
by using dependency-based kernels (Bunescu and
Mooney, 2005a; Kate, 2008) or syntactic kernels
(Qian et al, 2008; Moschitti, 2009) or by includ-
ing the word categories and their POS tags in the
subsequences. Also, it will be interesting to see if
a kernel that computes the similarity between sub-
card-pyramids could be developed and used for re-
lation classification.
7 Conclusions
We introduced a card-pyramid graph structure and
presented a new method for jointly extracting enti-
ties and their relations from a sentence using it. A
card-pyramid compactly encodes the entities and
relations in a sentence thus reducing the joint ex-
traction task to jointly labeling its nodes. We pre-
sented an efficient parsing algorithm for jointly
labeling a card-pyramid using dynamic program-
ming and beam search. The experiments demon-
strated the benefit of our joint extraction method
over a pipelined approach.
Acknowledgments
This research was funded by Air Force Contract
FA8750-09-C-0172 under the DARPA Machine
Reading Program.
References
Razvan C. Bunescu and Raymond J. Mooney. 2005a. A
shortest path dependency kernel for relation extraction. In
Proc. of the Human Language Technology Conf. and Conf.
on Empirical Methods in Natural Language Processing
(HLT/EMNLP-05), pages 724?731, Vancouver, BC, Oc-
tober.
Razvan C. Bunescu and Raymond J. Mooney. 2005b. Sub-
sequence kernels for relation extraction. In Y. Weiss,
B. Scho?lkopf, and J. Platt, editors, Advances in Neural In-
formation Processing Systems 18, Vancouver, BC.
Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Edward M. Mar-
cotte, Raymond J. Mooney, Arun Kumar Ramani, and
Yuk Wah Wong. 2005. Comparative experiments on
learning information extractors for proteins and their inter-
actions. Artificial Intelligence in Medicine (special issue
on Summarization and Information Extraction from Med-
ical Documents), 33(2):139?155.
Andrew Carlson, Justin Betteridge, Estevam R. Hruschka,
and Tom M. Mitchell. 2009. Coupling semi-supervised
learning of categories and relations. In SemiSupLearn
?09: Proceedings of the NAACL HLT 2009 Workshop on
Semi-Supervised Learning for Natural Language Process-
ing, pages 1?9, Boulder, Colorado.
Michael Collins. 2002. Ranking algorithms for named-entity
extraction: Boosting and the voted perceptron. In Proc. of
the 40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2002), pages 489?496, Philadel-
phia, PA.
Nello Cristianini and John Shawe-Taylor. 2000. An Introduc-
tion to Support Vector Machines and Other Kernel-based
Learning Methods. Cambridge University Press.
Claudio Giuliano, Alberto Lavelli, and Lorenza Romano.
2007. Relation extraction and the influence of automatic
named-entity recognition. ACM Trans. Speech Lang. Pro-
cess., 5(1):1?26.
D. Jurafsky and J. H. Martin. 2008. Speech and Language
Processing: An Introduction to Natural Lan guage Pro-
cessing, Computational Linguistics, and Speech Recogni-
tion. Prentice Hall, Upper Saddle River, NJ.
Rohit J. Kate and Raymond J. Mooney. 2006. Using string-
kernels for learning semantic parsers. In Proc. of the 21st
Intl. Conf. on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguistics
(COLING/ACL-06), pages 913?920, Sydney, Australia,
July.
Rohit J. Kate. 2008. A dependency-based word subsequence
kernel. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP 2008),
pages 400?409, Waikiki,Honolulu,Hawaii, October.
Daphne Koller and Nir Friedman. 2009. Probabilistic
Graphical Models: Principles and Techniques. The MIT
Press, Cambridge, MA.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini, and Chris Watkins. 2002. Text classification
using string kernels. Journal of Machine Learning Re-
search, 2:419?444.
Scott Miller, Heidi Fox, Lance A. Ramshaw, and Ralph M.
Weischedel. 2000. A novel use of statistical parsing to
extract information from text. In Proc. of the Meeting of
the N. American Association for Computational Linguis-
tics, pages 226?233, Seattle, Washington.
Alessandro Moschitti. 2009. Syntactic and semantic ker-
nels for short text pair categorization. In Proceedings of
the 12th Conference of the European Chapter of the ACL
(EACL 2009), pages 576?584, Athens,Greece, March.
John C. Platt. 1999. Probabilistic outputs for support vec-
tor machines and comparisons to regularized likelihood
methods. In Alexander J. Smola, Peter Bartlett, Bern-
hard Scho?lkopf, and Dale Schuurmans, editors, Advances
in Large Margin Classifiers, pages 185?208. MIT Press.
Vasin Punyakanok and Dan Roth. 2001. The use of classi-
fiers in sequential inference. In Advances in Neural Infor-
mation Processing Systems 13.
Longhua Qian, Guodong Zhou, Fang Kong, Qiaoming Zhu,
and Peide Qian. 2008. Exploiting constituent depen-
dencies for tree kernel-based semantic relation extraction.
In Proceedings of the 22nd International Conference on
Computational Linguistics (Coling 2008), pages 697?704,
Manchester, UK, August.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi, and
Jun?ichi Tsujii. 2009. A Markov logic approach to
bio-molecular event extraction. In Proceedings of the
BioNLP 2009 Workshop Companion Volume for Shared
Task, pages 41?49, Boulder, Colorado, June. Association
for Computational Linguistics.
211
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Proc. of 8th Conf. on Computational Natural Language
Learning (CoNLL-2004), pages 1?8, Boston, MA.
D. Roth and W. Yih. 2007. Global inference for entity and
relation identification via a linear programming formula-
tion. In L. Getoor and B. Taskar, editors, Introduction to
Statistical Relational Learning, pages 553?580. The MIT
Press, Cambridge, MA.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Proc. of
7th Conf. on Computational Natural Language Learning
(CoNLL-2003), Edmonton, Canada.
Erik F. Tjong Kim Sang. 2002. Introduction to the CoNLL-
2002 shared task: Language-independent named entity
recognition. In Proceedings of CoNLL-2002, pages 155?
158. Taipei, Taiwan.
Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou. 2006.
A composite kernel to extract relations between entities
with both flat and structured features. In Proc. of the 21st
Intl. Conf. on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguistics
(COLING/ACL-06), Sydney, Australia, July.
212
Implementing Weighted Abduction in Markov Logic
James Blythe
USC ISI
blythe@isi.edu
Jerry R. Hobbs
USC ISI
hobbs@isi.edu
Pedro Domingos
University of Washington
pedrod@cs.washington.edu
Rohit J. Kate
University of Wisconsin-Milwaukee
katerj@uwm.edu
Raymond J. Mooney
University of Texas at Austin
mooney@cs.utexas.edu
Abstract
Abduction is a method for finding the best explanation for observations. Arguably
the most advanced approach to abduction, especially for natural language processing, is
weighted abduction, which uses logical formulas with costs to guide inference. But it
has no clear probabilistic semantics. In this paper we propose an approach that imple-
ments weighted abduction in Markov logic, which uses weighted first-order formulas to
represent probabilistic knowledge, pointing toward a sound probabilistic semantics for
weighted abduction. Application to a series of challenge problems shows the power and
coverage of our approach.
1 Introduction
Abduction is inference to the best explanation.1 Typically, one uses it to find the best hypothesis ex-
plaining a set of observations, e.g., in diagnosis and plan recognition. In natural language processing the
content of an utterance can be viewed as a set of observations, and the best explanation then constitutes
the interpretation of the utterance. Hobbs et al [7] described a variety of abduction called ?weighted
abduction? for interpreting natural language discourse. The key idea was that the best interpretation of
a text is the best explanation or proof of the logical form of the text, allowing for assumptions. What
counted as ?best? was defined in terms of a cost function which favored proofs with the fewest number of
assumptions and the most salient and plausible axioms, and in which the pervasive redundancy implicit
in natural language discourse was exploited. It was argued in that paper that such interpretation problems
as coreference and syntactic ambiguity resolution, determining the specific meanings of vague predicates
and lexical ambiguity resolution, metonymy resolution, metaphor interpretation, and the recognition of
discourse structure could be seen to ?fall out? of the best abductive proof.
Specifically, weighted abduction has the following features:
1. In a goal expression consisting of an existentially quantified conjunction of positive literals, each
literal is given a cost that represents the utility of proving that literal as opposed to assuming it.
That is, a low cost on a literal will make it more likely for it to be assumed, whereas a high cost
will result in a greater effort to find a proof.
1We are indebted to Jesse Davis, Parag Singla and Marc Sumner for discussions about this work. This research was
supported in part by the Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force
Research Laboratory (AFRL) prime contract no. FA8750-09-C-0172, in part by the Office of Naval Research under contract
no. N00014-09-1-1029, and in part by the Army Research Office under grant W911NF-08-1-0242. Any opinions, findings, and
conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of
the DARPA, AFRL, ONR, ARO, or the US government.
55
2. Costs are passed back across the implication in Horn clauses according to weights on the conjuncts
in the antecedents. Specifically, if a consequent costs $c and the weight on a conjunct in the
antecedent is v, then the cost on that conjunct will be $vc. Note that if the weights add up to less
than one, backchaining on the rule will be favored, as the cost of the antecedent will be less than
the cost of the consequent. If the weights add up to more than one, backchaining will be disfavored
unless a proof can be found for one or more of the conjuncts in the antecedent, thereby providing
partial evidence for the consequent.
3. Two literals can be factored or unified, where the result is given the minimum cost of the two,
providing no contradiction would result. This is a frequent mechanism for coreference resolution.
In practice, only a shallow or heuristic check for contradiction is done.
4. The lowest-cost proof is the best interpretation, or the best abductive proof of the goal expression.
However, there are two significant problems with weighted abduction as it was originally presented.
First, it required a large knowledge base of commonsense knowledge. This was not available when
weighted abduction was first described, but since that time there have been substantial efforts to build up
knowledge bases for various purposes, and at least two of these have been used with promising results
in an abductive setting?Extended WordNet [6] for question-answering and FrameNet [11] for textual
inference.
The second problem with weighted abduction was that the weights and costs did not have a prob-
abilistic semantics. This, for example, hampers automatic learning of weights from data or existing
resources. That is the issue we address in the present paper.
In the last decade and a half, a number of formalisms for adding uncertain reasoning to predicate logic
have been developed that are well-founded in probability theory. Among the most widely investigated
is Markov logic [14, 4]. In this paper we show how weighted abduction can be implemented in Markov
logic. This demonstrates that Markov logic networks can be used as a powerful mechanism for interpret-
ing natural language discourse, and at the same time provides weighted abduction with something like a
probabilistic semantics.
In Section 2 we briefly describe Markov logic and Markov logic networks. Section 3 then describes
how weighted abduction can be implemented in Markov logic. In Section 4 we describe experiments in
which fourteen published examples of the use of weighted abduction in natural language understanding
are implemented in Markov logic networks, with good results. Section 5 on current and future directions
briefly describes an ongoing experiment in which we are attempting to scale up to apply this procedure
to the textual inference problem with a knowledge base derived from FrameNet with tens of thousands
of axioms.
2 Markov Logic Networks and Related Work
Markov logic [14, 4] is a recently developed theoretically sound framework for combining first-order
logic and probabilistic graphical models. A traditional first-order knowledge base can be seen as a set of
hard constraints on the set of possible worlds: if a world violates even one formula, its probability is zero.
In order to soften these constraints, Markov logic attaches a weight to each first-order logic formula in
the knowledge base. Such a set of weighted first-order logic formulae is called a Markov logic network
(MLN). A formula?s weight reflects how strong a constraint it imposes on the set of possible worlds: the
higher the weight, the lower the probability of a world that violates it; however, that probability need not
be zero. An MLN with all infinite weights reduces to a traditional first-order knowledge base with only
hard constraints.
56
Formally, an MLN L is a set of formula?weight pairs (Fi, wi). Given a set of constants, it defines
a joint probability distribution over a set of boolean variables X = (X1, X2...) corresponding to the
possible groundings (using the given constants) of the literals present in the first-order formulae:
P (X = x) = 1Z exp(
?
iwini(x))
where ni(x) is the number of true groundings of Fi in x and Z is a normalization term obtained by
summing P (X = x) over all values of X .
Semantically, an MLN can be viewed as a set of templates for constructing Markov networks [12],
the undirected counterparts of Bayesian networks. An MLN and a set of constants produce a Markov
network in which each ground literal is a node and every pair of ground literals that appear together in
some grounding of some formula are connected by an edge. Different sets of constants produce different
Markov networks; however, there are certain regularities in their structure and parameters. For example,
all groundings of the same formula have the same weight.
Probabilistic inference for an MLN (such as finding the most probable truth assignment for a given
set of ground literals, or finding the probability that a particular formula holds) can be performed by
first producing the ground Markov network and then using well known inference techniques for Markov
networks, like Gibbs sampling. Given a knowledge base as a set of first-order logic formulae, and a
database of training examples each consisting of a set of true ground literals, it is also possible to learn
appropriate weights for the MLN formulae which maximize the probability of the training data. An open-
source software package for MLNs, called Alchemy 2, is also available with many built-in algorithms
for performing inference and learning.
Much of the early work on abduction was done in a purely logical framework (e.g., [13, 3, 9, 10].
Typically the choice between alternative explanations is made on the basis of parsimony; the shortest
proofs with the fewest assumptions are favored. However, a significant limitation of these purely logical
approaches is that they are unable to reason under uncertainty or estimate the likelihood of alternative
explanations. A probabilistic form of abduction is needed in order to account for uncertainty in the
background knowledge and to handle noisy and incomplete observations.
In Bayesian networks [12] background knowledge with its uncertainties is encoded in a directed
graph. Then, given a set of observations, probabilistic inference over the graph structure is done to
compute the posterior probability of alternative explanations. However, Bayesian networks are based on
propositional logic and cannot handle structured representations, hence preventing their use in situations,
characteristic of natural language processing, that involve an unbounded number of entities with a variety
of relations between them.
In recent years there have been a number of proposals attempting to combine the probabilistic nature
of Bayesian networks with structured first-order representations. It is impossible here to review this liter-
ature here. A a good review of much of it can be found in [5], and in [14] there are detailed comparisonss
of various models to MLNs.
Charniak and Shimony [2] define a variant of weighted abduction, called ?cost-based abduction? in
which weights are attached to terms rather than to rules or to antecedents in rules. Thus, the term Pi
has the same cost whatever rule it is used in. The cost of an assignment to the variables in the domain
is the sum of the costs of the variables that are true in the assignment. Charniak and Shimony provide
a probabilistic semantics for their approach by showing how to construct a Bayesian network from a
domain such that a most probable explanation solution to the Bayes net corresponds to a lowest-cost
solution to the abduction problem. However, in natural language applications the utility of proving a
proposition can vary by context; weighted abduction accomodates this, whereas cost-based abduction
does not.2http://alchemy.cs.washington.edu
57
3 Weighted Abduction and MLNs
Kate and Mooney [8] show how logical abduction can be implemented in Markov logic networks. They
use forward inference in MLNs to perform abduction by adding clauses with reverse implications. Uni-
versally quantified variables from the left hand side of rules are converted to existentially quantified
variables in the reversed clause. For example, suppose we have the following rule saying that mosquito
bites transmit malaria:
mosquito(x) ? infected(x,Malaria) ? bite(x, y) ? infected(y,Malaria)
This would be translated into the soft rule
[w] infected(y,Malaria) ? ?x[mosquito(x) ? infected(x,Malaria) ? bite(x, y)]
Where there is more than one possible explanation, they include a closure axiom saying that one of the
explanations must hold. Since blood transfusions also cause malaria, they have the hard rule
infected(y,Malaria) ?
?x[mosquito(x) ? infected(x,Malaria) ? bite(x, y)]
??x[infected(x,Malaria) ? transfuse(Blood, x, y)].
Kate and Mooney also add a soft mutual exclusivity clause that states that no more than one of the
possible explanations is true.
In translating between weighted abduction and Markov logic, we need similarly to specify the axioms
in Markov logic that correspond to a Horn clause axiom in weighted abduction. In addition, we need to
describe the relation between the numbers in weighted abduction and the weights on the Markov logic
axioms. Hobbs et al [7] give only broad, informal guidelines about how the numbers correspond to
probabilities. In this development, we elaborate on how the numbers can be defined more precisely
within these guidelines in a way that links with the weights in Markov logic, thereby pointing to a
probabilistic semantics for the weighted abduction numbers.
There are two sorts of numbers in weighted abduction?the weights on conjuncts in the antecedents
of Horn clause axioms, and the costs on conjuncts in goal expressions, which are existentially quantified
conjunctions of positive literals. We deal first with the weights, then with the costs.
The space of events over which probabilities are taken is the set of proof graphs constituting the best
interpretations of a set of texts in a corpus. Thus, by the probability of p(x) given q(x), we mean the
probability that p(x) will occur in a proof graph in which q(x) occurs.
The translation from weighted abduction axioms to Markov logic axioms can be broken into two
steps. First we consider the ?or? node case, determining the relative costs of axioms that have the same
consequent. Then we look at the ?and? node case, determining how the weights should be distributed
across the conjuncts in the antecedent of a Horn clause, given the total weight for the antecedent.
Weights on Antecedents in Axioms. First consider a set of Horn clause axioms all with the same
consequent, where we collapse the antecedent into a single literal, and for simplicity allow x to stand for
all the universally quantified variables in the antecedent, and assume the consequent to have only those
variables. That is, we convert all axioms of the form
p1(x) ? . . . ? q(x)
into axioms of the form
Ai(x) ? q(x), where p1(x) ? . . . ? Ai(x)
To convert this into Markov logic, we first introduce the hard constraint
Ai(x) ? q(x).
In addition, given a goal of proving q(x), in weighted abduction we will want to backchain on at least
(and usually at most) one of these axioms or we will want simply to assume q(x). Thus, we can introduce
another hard constraint with the disjunction of these antecedents as well as a literal AssumeQ(x) that
means q(x) is assumed rather than proved.
58
q(x) ? A1(x) ? A2(x) ? . . . ? An(x) ? AssumeQ(x).
Then we need to introduce soft constraints to indicate that each of these disjuncts is a possible explana-
tion, or proof, of q(x), with an associated probability, or weight.
[wi] q(x) ? Ai(x), . . .
[w0] q(x) ? AssumeQ(x)
The probability that AssumeQ(x) is true is the conditional probability P0 that none of the antecedents
is true given that q(x) is true.
P0 = P (?[A1(x) ? A2(x) ? . . . ? An(x)] | q(x))
In weighted abduction, when the antecedent weight is greater than one, we prefer assuming the conse-
quent to assuming the antecedent. When the antecedent weight is less than one we prefer to assume the
antecedent. If the probability that an antecedent Ai(x) is the explanation of q(x) is greater than P0, it
should be given a weighted abduction weight vi less than 1, making it more likely to be chosen.3 Cor-
respondingly, if it is less than P0, it should be given a weight vi greater than 1, making it less likely
to be chosen. In general, the weighted abduction weights should be in reverse order of the conditional
probabilities Pi that Ai(x) is the explanation of q(x).
Pi = P (Ai(x) | q(x))
If we assign the weights vi in weighted abduction to be
vi = logPilogP0
then this is consistent with informal guidelines in [7] on the meaning of these weights. We use the logs
of the probabilities rather than the probabilities themselves to moderate the effect of one axiom being
very much more probable than any of the others.
Kate and Mooney [8], in their translation of logical abduction into Markov logic, also include soft
constraints stipulating that the different possible explanations Ai(x) are normally mutually exclusive.
We do not do that here, but we get a kind of soft mutual exclusivity constraint by virtue of the axioms
below that levy a cost for any literal that is taken to be true. In general, more parimonious explanations
will be favored.
Nevertheless, in most cases a single explanation will suffice. When this is true, the probability of
Ai(x) holding when q(x) holds is e
wi
Z . Then a reasonable approximation for the relation between the
weighted abduction weights vi and the Markov logic weights wi is
wi = ?vilogP0
Weights on Conjuncts in Antecedents. Next consider how cost is spread across the conjuncts in the
antecedent of a Horn clause in weighted abduction. Here we use u?s to represent the weighted abduction
weights on the conjuncts.
p1(x)u1 ? p2(x)u2 ? ... ? A(x)
The u?s should somehow represent the semantic contribution of each conjunct to the conclusion. That is,
given that the conjunct is true, what is the probability that it is part of an explanation of the consequent?
Conjuncts with a higher such probability should be given higher weights u; they play a more significant
role in explaining A(x).
Let Pi be the conditional probability of the consequent given the ith conjunct in the antecedent.
Pi = P (A(x)|pi(x))
and let Z be a normalization factor.
Z = ?ni=1 Pi
3We use vi for these weighted abduction weights and wi for Markov logic weights.
59
Let v be the weight of the entire antecedent as determined above.
Then it is consistent with the guidelines in [7] to define the weights on the conjuncts as follows:
ui = vPiZ
The weights ui will sum to v and each will correspond to the semantic contribution of its conjunct to the
consequent.
In Markov logic, weights apply only to axioms as a whole, not parts of axioms. Thus, the single
axiom above must be decomposed into one axiom for each conjunct and the dependencies must be
written as
[wi] pi(x) ? A(x), . . .
The relation between the weighted abduction weights ui and the Markov logic weights wi can be
approximated by
ui = ve
?wi
Z
Costs on Goals. The other numbers in weighted abduction are the costs associated with the conjuncts
in the goal expression. In weighted abduction these costs function as utilities. Some parts of the goal
expression are more important to interpret correctly than others; we should try harder to prove these
parts, rather than simply assuming them. In language it is important to recognize the referential anchor
of an utterance in shared knowledge. Thus, those parts of a sentence most likely to provide this anchor
have the highest utility. If we simply assume them, we lose their connection with what is already known.
Those parts of a sentence most likely to be new information will have a lower cost, because we usually
would not be able to prove them in any case.
Consider the two sentences
The smart man is tall.
The tall man is smart.
The logical form for each of them will be
(?x)[smart(x) ? tall(x) ?man(x)]
In weighted abduction, an interpretation of the sentence is a proof of the logical form, allowing assump-
tions. In the first sentence we want to prove smart(x) to anchor the sentence referentially. Then tall(x)
is new information; it will have to be assumed. We will want to have a high cost on smart(x) to force
the proof procedure to find this referential anchor. The cost on tall(x) will be low, to allow it to be
assumed without expending too much effort in trying to locate that fact in shared knowledge.
In the second sentence, the case is the reverse.
Let?s focus on the first sentence and assume we know that educated people are smart and big people
are tall, and furthermore that John is educated and Bill is big.
educated(x)1.2 ? smart(x)
big(x)1.2 ? tall(x)
educated(J), big(B)
In weighted abduction, the best interpretation will be that the smart man is John, because he is educated,
and we pay the cost for assuming he is tall. The interpretation we want to avoid is one that says x is Bill;
he is tall because he is big, and we pay the cost of assuming he is smart. Weighted abduction with its
differential costs on conjuncts in the goal expression favors the first and disfavors the second.
In weighted abduction, only assumptions cost; literals that are proved cost nothing. When the above
axioms are translated into Markov logic, it would be natural to capture the differential costs by attaching a
negative weight to smart(x) to model the cost associated with assuming it. However, this weight would
apply to any assignment in which smart(J) is true, regardless of whether it was assumed, derived from
60
an assumed fact, or derived from a known fact. A potential solution might be to attach the negative weight
to AssumeSmart(x). But the first axiom above allows us to bypass the negative weight on smart(x).
We can hypothesize that x is Bill, pay a low cost on AssumeEducated(B), derive smart(B), and get
the wrong assignment. Thus it is not enough to attach a negative weight to high-cost conjuncts in the
goal expression. This negative weight would have to be passed back through the whole knowledge base,
making the complexity of setting the weights at problem time in the MLN knowledge base equal to the
complexity of running the inference problem.
An alternative solution, which avoids this problem when the forward inferences are exact, is to use
a set of predicates that express knowing a fact without any assumptions. In the current example, we
would add Ksmart(x) for knowing that an entity is smart. The facts asserted in the data base are now
Keducated(J) and Kbig(B). For each hard axiom involving non-K predicates, we have a correspond-
ing axiom that expresses the relation between the K-predicates, and we have a soft axiom allowing us to
cross the border between the K predicates and their non-K counterparts.
Keducated(x) ? Ksmart(x)., . . .
[w] Ksmart(x) ? smart(x), . . .
Here the positive weight w attached is chosen to counteract the negative weight we would attach to
smart(x) to reflect the high cost of assuming it.
This removes the weight associated with assuming smart(x) regardless of the inference path that
leads to knowing smart(x) (KSmart(x))). Further, this translation takes linear time in the size of
the goal expression to compute, since we do not need to know the equivalent weighted abduction cost
assigned to the possible antecedents of smart(x).
If the initial facts do not include KEducated(B) and instead educated(B) must be assumed, then
the negative weight associated with smart(B) is still present. In this solution, there is no danger that
the inference process can by-pass the cost of assuming smart(B), since it is attached to the required
predicate and can only be removed by inferring KSmart(B).
Finally, there is a tendency in Markov logic networks for assignments of high probability for proposi-
tions for which there is no evidence one way or the other. To suppress this, we associate a small negative
weight with every predicate. In practice, it has turned out that a weight of ?1 effectively suppresses this
behavior.
4 Experimental Results
We have tested our approach on a set of fourteen challenge problems from [7] and subsequent papers,
designed to exercise the principal features of weighted abduction and show its utility for solving natural
language interpretation problems. The knowledge bases used for each of these problems are sparse,
consisting of only the axioms required for solving the problems plus a few distractors.
An example of a relatively simple problem is #5 in the table below, resolving ?he? in the text
I saw my doctor last week. He told me to get more exercise.
where we are given a knowledge base that says a doctor is a person and a male person is a ?he?. Solving
the problem requires assuming the doctor is male.
(?x)[doctor(x)1.2 ? person(x)]
(?x)[male(x).6 ? person(x).6 ? he(x)]
The logical form fragment to prove is (?x)he(x), where we know doctor(D).
A problem of intermediate difficulty (#7) is resolving the three lexical ambiguities in the sentence
The plane taxied to the terminal.
61
where we are given a knowledge base saying that airplanes and wood smoothers are planes, planes
moving on the ground and people taking taxis are both described as ?taxiing?, and computer terminals
and airport terminals are both terminals.
An example of a difficult problem is #12, finding the coherence relation, thereby resolving the pro-
noun ?they?, between the sentences
The police prohibited the women from demonstrating. They feared violence.
The axioms specify relations between fearing, not wanting, and prohibiting, as well as the defeasible
transitivity of causality and the fact that a causal relation between sentences makes the discourse coher-
ent.
The weights in the axioms were mostly distributed evenly across the conjuncts in the antecedents and
summed to 1.2.
For each of these problems, we compare the performance of the method described here with a man-
ually constructed gold standard and also with a method based on Kate and Mooney?s (KM) approach.
In this method, weights were assigned to the reversed clauses based on the negative log of the sum of
weights in the original clause. This approach does not capture different weights for different antecedents
of the same rule, and so has less fidelity to weighted abduction than our approach. In each case, we used
Alchemy?s probabilistic inference to determine the most probable explanation (MPE) [12].
In some of the problems the system should make more than one assumption, so there are 22 assump-
tions in total over all 14 problems in the gold standard. Using our method, 18 of the assumptions were
found, while 15 were found using the KM method. Table 1 shows the number of correct assumptions
found and the running time for the two approaches for each problem. Our method in particular provides
good coverage, with a recall of over 80% of the assumptions made in the gold standard. It has a shorter
running time overall, approximately 5.3 seconds versus 8.7 seconds for the reversal method. This is
largely due to one problem in the test set, problem #9, where the running time for the KM method is
relatively high because the technique finds a less sparse network, leading to larger cliques. There were
two problems in the test set that neither approach could solve. One of these contains predicates that have
a large number of arguments, leading to large clique sizes.
5 Current and Future Directions
In other work [11] we are experimenting with using weighted abduction with a knowledge base with tens
of thousands of axioms derived from FrameNet for solving problems in recognizing textual entailment
(RTE2) from the Pascal dataset [1]. For a direct comparison between standard weighted abduction and
the Markov logic approach described here, we are also experimenting with using the latter on the same
task with the same knowledge base.
For each text-hypothesis pair, the sentences are parsed and a logical form is produced. The output for
the first sentence forms the specific knowledge the system has while the output for the second sentence
is used as the target to be explained. If the cost of the best explanation is below a threshold we take the
target sentence to be true given the initial information.
It is a major challenge to scale our approach to handle all the problems from the RTE2 development
and test sets. We are not yet able to address the most complex of these using inference in Markov logic
networks. However, we have devised a number of pre-processing steps to reduce the complexity of the
resultant network, which significantly increase the number of problems that are tractable.
The FrameNet knowledge base contains a large number of axioms with general coverage. For any
individual entailment problem, most of them are irrelevant and can be removed after a simple graphical
analysis. We are able to remove more irrelevant axioms and predicates with an iterative approach that in
62
Our Method KM Method Gold
Problem score seconds score seconds standard
1 3 300 3 16 3
2 1 250 1 265 1
3 1 234 1 266 1
4 2 234 2 203 2
5 1 218 1 218 1
6 1 218 0 265 1
7 3 300 3 218 3
8 1 200 1 250 1
9 2 421 0 5000 2
10 1 2500 1 1500 3
11 0 0 1
12 0 0 1
13 1 250 1 250 1
14 1 219 1 219 1
Total 18 5344 15 8670 22
Table 1: Performance on each problem in our test set, comparing two encodings of weighted abduction
into Markov logic networks and a gold standard.
each iteration both drops axioms that are shown to be irrelevant and simplifies remaining axioms in such
a way as not to change the probability of entailment.
We also simplify predications by removing unnecessary arguments. The most natural way to convert
FrameNet frames to axioms is to treat a frame as a predicate whose arguments are the frame elements for
all of its roles. After converting to Markov logic, this results in rules having large numbers of existentially
quantified variables in the consequent. This can lead to a combinatorial explosion in the number of
possible ground rules. Many of the variables in the frame predicate are for general use and can be pruned
in the particular entailment. Our approach essentially creates abstractions of the original predicates that
preserve all the information that is relevant to the current problem but greatly reduces the number of
ground instances to consider.
Before implementing these pre-processing steps, only two or three problems could be run to com-
pletion on a Macbook Pro with 8 gigabytes of RAM. After making them, 28 of the initial 100 problems
could be run to completion.
Work on this effort continues.
6 Summary
Weighted abduction is a logical reasoning framework that has been successfully applied to solve a num-
ber of interesting and important problems in computational natural-language semantics ranging from
word sense disambiguation to coreference resolution. However, its method for representing and combin-
ing assumption costs to determine the most preferred explanation is ad hoc and without a firm theoretical
foundation. Markov Logic is a recently developed formalism for combining first-order logic with prob-
abilistic graphical models that has a well-defined formal semantics in terms of specifying a probability
distribution over possible worlds. This paper has presented a method for mapping weighted abduction
63
to Markov logic, thereby providing a sound probabilistic semantics for the approach and also allowing
it to exploit the growing toolbox of inference and learning algorithms available for Markov logic. Com-
plementarily, it has also demonstrated how Markov logic can thereby be applied to help solve important
problems in computational semantics.
References
[1] Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpek-
tor. The second pascal recognising textual entailment challenge. In Proceedings of the Second PASCAL
Challenges Workshop on Recognising Textual Entailment, Venice, Italy, 2006.
[2] Eugene Charniak and Solomon E. Shimony. Cost-based abduction and map explanation. Artificial Artificial
Intelligence Journal, 66(2):345?374, 1994.
[3] P. T. Cox and T. Pietrzykowski. Causes for events: Their computation and applications. In J. Siekmann,
editor, 8th International Conference on Automated Deduction (CADE-8), Berlin, 1986. Springer-Verlag.
[4] P. Domingos and D. Lowd. Markov Logic: An Interface Layer for Artificial Intelligence. Morgan & Claypool,
San Rafael, CA, 2009.
[5] L. Getoor and B. Taskar, editors. Introduction to Statistical Relational Learning. MIT Press, Cambridge,
MA, 2007.
[6] S. Harabagiu and D.I. Moldovan. Lcc?s question answering system. In 11th Text Retrieval Conference,
TREC-11, Gaithersburg, MD., 2002.
[7] Jerry R. Hobbs, Mark E. Stickel, Douglas E. Appelt, and Paul A. Martin. Interpretation as abduction. Artifi-
cial Intelligence, 63(1-2):69?142, 1993.
[8] Rohit Kate and Ray Mooney. Probabilistic abduction using markov logic networks. In IJCAI 09 Workshop
on Plan, Activity and Intent Recognition, 2009.
[9] Hector J. Levesque. A knowledge-level account of abduction. In Eleventh International Joint Conference on
Artificial Intelligence, volume 2, pages 1061?1067, Detroit, Michigan, 1989.
[10] Hwee Tou Ng and Raymond J. Mooney. The role of coherence in constructing and evaluating abductive
explanations. In P. O?Rorke, editor, Working Notes, AAAI Spring Symposium on Automated Abduction,
Stanford, California, March 1990.
[11] E. Ovchinnikova, N. Montazeri, T. Alexandrov, J. Hobbs, M. McCord, and R. Mulkar-Mehta. Abductive
reasoning with a large knowledge base for discourse processing. In Proceedings of the 9th International
Conference on Computational Semantics, Oxford, United Kingdom, 2011.
[12] J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann,
San Francisco, CA, 1988.
[13] Harry E. Pople. On the mechanization of abductive logic. In Third International Joint Conference on Artificial
Intelligence, pages 147?152, Stanford, California, August 1973.
[14] M. Richardson and P. Domingos. Markov logic networks. Machine Learning, 62:107?136, 2006.
64
