Unsupervised Relation Extraction from Web Documents
Kathrin Eichler, Holmer Hemsen and Gu?nter Neumann
DFKI GmbH, LT-Lab, Stuhlsatzenhausweg 3 (Building D3 2), D-66123 Saarbru?cken
{FirstName.SecondName}@dfki.de
Abstract
The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system
expresses an information request in the form of a topic description, which is used for an initial search in order to retrieve
a relevant set of documents. On basis of this set of documents, unsupervised relation extraction and clustering is done by
the system. The results of these operations can then be interactively inspected by the user. In this paper we describe the
relation extraction and clustering components of the IDEX system. Preliminary evaluation results of these components are
presented and an overview is given of possible enhancements to improve the relation extraction and clustering components.
1. Introduction
Information extraction (IE) involves the process of au-
tomatically identifying instances of certain relations of
interest, e.g., produce(<company>, <product>, <lo-
cation>), in some document collection and the con-
struction of a database with information about each
individual instance (e.g., the participants of a meet-
ing, the date and time of the meeting). Currently, IE
systems are usually domain-dependent and adapting
the system to a new domain requires a high amount
of manual labour, such as specifying and implement-
ing relation?specific extraction patterns manually (cf.
Fig. 1) or annotating large amounts of training cor-
pora (cf. Fig. 2). These adaptations have to be made
offline, i.e., before the specific IE system is actually
made. Consequently, current IE technology is highly
statical and inflexible with respect to a timely adapta-
tion to new requirements in the form of new topics.
Figure 1: A hand-coded rule?based IE?system (schemat-
ically): A topic expert implements manually task?specific
extraction rules on the basis of her manual analysis of a
representative corpus.
1.1. Our goal
The goal of our IE research is the conception and im-
plementation of core IE technology to produce a new
Figure 2: A data?oriented IE system (schematically): The
task?specific extraction rules are automatically acquired by
means of Machine Learning algorithms, which are using
a sufficiently large enough corpus of topic?relevant docu-
ments. These documents have to be collected and costly
annotated by a topic?expert.
IE system automatically for a given topic. Here, the
pre?knowledge about the information request is given
by a user online to the IE core system (called IDEX)
in the form of a topic description (cf. Fig. 3). This
initial information source is used to retrieve relevant
documents and extract and cluster relations in an un-
supervised way. In this way, IDEX is able to adapt
much better to the dynamic information space, in par-
ticular because no predefined patterns of relevant re-
lations have to be specified, but relevant patterns are
determined online. Our system consists of a front-end,
which provides the user with a GUI for interactively in-
specting information extracted from topic-related web
documents, and a back-end, which contains the rela-
tion extraction and clustering component. In this pa-
per, we describe the back-end component and present
preliminary evaluation results.
1.2. Application potential
However, before doing so we would like to motivate
the application potential and impact of the IDEX ap-
Figure 3: The dynamic IE system IDEX (schematically):
a user of the IDEX IE system expresses her information
request in the form of a topic description which is used for
an initial search in order to retrieve a relevant set of doc-
uments. From this set of documents, the system extracts
and collects (using the IE core components of IDEX) a set
of tables of instances of possibly relevant relations. These
tables are presented to the user (who is assumed to be the
topic?expert), who will analyse the data further for her in-
formation research. The whole IE process is dynamic, since
no offline data is required, and the IE process is interactive,
since the topic expert is able to specify new topic descrip-
tions, which express her new attention triggered by a novel
relationship she was not aware of beforehand.
proach by an example application. Consider, e.g., the
case of the exploration and the exposure of corruptions
or the risk analysis of mega construction projects. Via
the Internet, a large pool of information resources of
such mega construction projects is available. These
information resources are rich in quantity, but also
in quality, e.g., business reports, company profiles,
blogs, reports by tourists, who visited these construc-
tion projects, but also web documents, which only
mention the project name and nothing else. One of
the challenges for the risk analysis of mega construc-
tion projects is the efficient exploration of the possibly
relevant search space. Developing manually an IE sys-
tem is often not possible because of the timely need
of the information, and, more importantly, is proba-
bly not useful, because the needed (hidden) informa-
tion is actually not known. In contrast, an unsuper-
vised and dynamic IE system like IDEX can be used
to support the expert in the exploration of the search
space through pro?active identification and clustering
of structured entities. Named entities like for example
person names and locations, are often useful indicators
of relevant text passages, in particular, if the names are
in some relationship. Furthermore, because the found
relationships are visualized using an advanced graph-
ical user interface, the user can select specific names
and find associated relationships to other names, the
documents they occur in or she can search for para-
phrases of sentences.
2. System architecture
The back-end component, visualized in Figure 4, con-
sists of three parts, which are described in detail in this
section: preprocessing, relation extraction and relation
clustering.
2.1. Preprocessing
In the first step, for a specific search task, a topic of
interest has to be defined in the form of a query. For
this topic, documents are automatically retrieved from
the web using the Google search engine. HTML and
PDF documents are converted into plain text files. As
the tools used for linguistic processing (NE recogni-
tion, parsing, etc.) are language-specific, we use the
Google language filter option when downloading the
documents. However, this does not prevent some doc-
uments written in a language other than our target
language (English) from entering our corpus. In ad-
dition, some web sites contain text written in several
languages. In order to restrict the processing to sen-
tences written in English, we apply a language guesser
tool, lc4j (Lc4j, 2007) and remove sentences not clas-
sified as written in English. This reduces errors on
the following levels of processing. We also remove sen-
tences that only contain non-alphanumeric characters.
To all remaining sentences, we apply LingPipe (Ling-
Pipe, 2007) for sentence boundary detection, named
entity recognition (NER) and coreference resolution.
As a result of this step database tables are created,
containing references to the original document, sen-
tences and detected named entities (NEs).
2.2. Relation extraction
Relation extraction is done on the basis of parsing po-
tentially relevant sentences. We define a sentence to be
of potential relevance if it at least contains two NEs.
In the first step, so-called skeletons (simplified depen-
dency trees) are extracted. To build the skeletons, the
Stanford parser (Stanford Parser, 2007) is used to gen-
erate dependency trees for the potentially relevant sen-
tences. For each NE pair in a sentence, the common
root element in the corresponding tree is identified and
the elements from each of the NEs to the root are col-
lected. An example of a skeleton is shown in Figure 5.
In the second step, information based on dependency
types is extracted for the potentially relevant sen-
tences. Focusing on verb relations (this can be ex-
tended to other types of relations), we collect for each
verb its subject(s), object(s), preposition(s) with ar-
guments and auxiliary verb(s). We can now extract
verb relations using a simple algorithm: We define a
verb relation to be a verb together with its arguments
(subject(s), object(s) and prepositional phrases) and
consider only those relations to be of interest where at
least the subject or the object is an NE. We filter out
relations with only one argument.
2.3. Relation clustering
Relation clusters are generated by grouping relation
instances based on their similarity.
web documents document
retrieval
topic specific documents plain text documents
sentence/documents+
 NE tables
languagefiltering
syntactic +typed dependencyparsing 
sov?relationsskeletons +
clustering
conversion
Preprocessing
Relation extraction
Relation clustering
sentencesrelevant
filtering of
relationfiltering
table of clustered relations
sentence boundary
resolutioncoreference
detection,NE recognition,
Figure 4: System architecture
Figure 5: Skeleton for the NE pair ?Hohenzollern? and ?Brandenburg? in the sentence ?Subsequent members of
the Hohenzollern family ruled until 1918 in Berlin, first as electors of Brandenburg.?
The comparably large amount of data in the corpus
requires the use of an efficient clustering algorithm.
Standard ML clustering algorithms such as k-means
and EM (as provided by the Weka toolbox (Witten
and Frank, 2005)) have been tested for clustering the
relations at hand but were not able to deal with the
large number of features and instances required for an
adequate representation of our dataset. We thus de-
cided to use a scoring algorithm that compares a re-
lation to other relations based on certain aspects and
calculates a similarity score. If this similarity score ex-
ceeds a predefined threshold, two relations are grouped
together.
Similarity is measured based on the output from the
different preprocessing steps as well as lexical informa-
tion from WordNet (WordNet, 2007):
? WordNet: WordNet information is used to deter-
mine if two verb infinitives match or if they are in
the same synonym set.
? Parsing: The extracted dependency information is
used to measure the token overlap of the two sub-
jects and objects, respectively. We also compare
the subject of the first relation with the object of
the second relation and vice versa. In addition,
we compare the auxiliary verbs, prepositions and
preposition arguments found in the relation.
? NE recognition: The information from this step
is used to count how many of the NEs occurring
in the contexts, i.e., the sentences in which the
two relations are found, match and whether the
NE types of the subjects and objects, respectively,
match.
? Coreference resolution: This type of information
is used to compare the NE subject (or object) of
one relation to strings that appear in the same
coreference set as the subject (or object) of the
second relation.
Manually analyzing a set of extracted relation in-
stances, we defined weights for the different similarity
measures and calculated a similarity score for each re-
lation pair. We then defined a score threshold and clus-
tered relations by putting two relations into the same
cluster if their similarity score exceeded this threshold
value.
3. Experiments and results
For our experiments, we built a test corpus of doc-
uments related to the topic ?Berlin Hauptbahnhof?
by sending queries describing the topic (e.g., ?Berlin
Hauptbahnhof?, ?Berlin central station?) to Google
and downloading the retrieved documents specifying
English as the target language. After preprocessing
these documents as described in 2.1., our corpus con-
sisted of 55,255 sentences from 1,068 web pages, from
which 10773 relations were automatically extracted
and clustered.
3.1. Clustering
From the extracted relations, the system built 306 clus-
ters of two or more instances, which were manually
evaluated by two authors of this paper. 81 of our clus-
ters contain two or more instances of exactly the same
relation, mostly due to the same sentence appearing in
several documents of the corpus. Of the remaining 225
clusters, 121 were marked as consistent, 35 as partly
consistent, 69 as not consistent. We defined consis-
tency based on the potential usefulness of a cluster to
the user and identified three major types of potentially
useful clusters:
? Relation paraphrases, e.g.,
accused (Mr Moore, Disney, In letter)
accused (Michael Moore, Walt Disney
Company)
? Different instances of the same pattern, e.g.,
operates (Delta, flights, from New York)
offers (Lufthansa, flights, from DC)
? Relations about the same topic (NE), e.g.,
rejected (Mr Blair, pressure, from Labour
MPs)
reiterated (Mr Blair, ideas, in speech, on
March)
created (Mr Blair, doctrine)
...
Of our 121 consistent clusters, 76 were classified as be-
ing of the type ?same pattern?, 27 as being of the type
?same topic? and 18 as being of the type ?relation para-
phrases?. As many of our clusters contain two instances
only, we are planning to analyze whether some clusters
should be merged and how this could be achieved.
3.2. Relation extraction
In order to evaluate the performance of the relation ex-
traction component, we manually annotated 550 sen-
tences of the test corpus by tagging all NEs and verbs
and manually extracting potentially interesting verb
relations. We define ?potentially interesting verb rela-
tion? as a verb together with its arguments (i.e., sub-
ject, objects and PP arguments), where at least two
of the arguments are NEs and at least one of them
is the subject or an object. On the basis of this crite-
rion, we found 15 potentially interesting verb relations.
For the same sentences, the IDEX system extracted 27
relations, 11 of them corresponding to the manually
extracted ones. This yields a recall value of 73% and
a precision value of 41%.
There were two types of recall errors: First, errors in
sentence boundary detection, mainly due to noisy in-
put data (e.g., missing periods), which lead to parsing
errors, and second, NER errors, i.e., NEs that were
not recognised as such. Precision errors could mostly
be traced back to the NER component (sequences of
words were wrongly identified as NEs).
In the 550 manually annotated sentences, 1300 NEs
were identified as NEs by the NER component. 402
NEs were recognised correctly by the NER, 588
wrongly and in 310 cases only parts of an NE were
recognised. These 310 cases can be divided into three
groups of errors. First, NEs recognised correctly, but
labeled with the wrong NE type. Second, only parts
of the NE were recognised correctly, e.g., ?Touris-
mus Marketing GmbH? instead of ?Berlin Tourismus
Marketing GmbH?. Third, NEs containing additional
words, such as ?the? in ?the Brandenburg Gate?.
To judge the usefulness of the extracted relations, we
applied the following soft criterion: A relation is con-
sidered useful if it expresses the main information given
by the sentence or clause, in which the relation was
found. According to this criterion, six of the eleven
relations could be considered useful. The remaining
five relations lacked some relevant part of the sen-
tence/clause (e.g., a crucial part of an NE, like the
?ICC? in ?ICC Berlin?).
4. Possible enhancements
With only 15 manually extracted relations out of 550
sentences, we assume that our definition of ?potentially
interesting relation? is too strict, and that more inter-
esting relations could be extracted by loosening the ex-
traction criterion. To investigate on how the criterion
could be loosened, we analysed all those sentences in
the test corpus that contained at least two NEs in order
to find out whether some interesting relations were lost
by the definition and how the definition would have to
be changed in order to detect these relations. The ta-
ble in Figure 6 lists some suggestions of how this could
be achieved, together with example relations and the
number of additional relations that could be extracted
from the 550 test sentences.
In addition, more interesting relations could be
found with an NER component extended by more
types, e.g., DATE and EVENT. Open domain NER
may be useful in order to extract NEs of additional
types. Also, other types of relations could be inter-
esting, such as relations between coordinated NEs,
option example additional relations
extraction of relations,
where the NE is not the
complete subject, object or
PP argument, but only part
of it
Co-operation with <ORG>M.A.X.
2001<\ORG> <V>is<\V> clearly of
benefit to <ORG>BTM<\ORG>.
25
extraction of relations with
a complex VP
<ORG>BTM<\ORG> <V>invited and or
supported<\V> more than 1,000 media rep-
resentatives in <LOC>Berlin<\LOC>.
7
resolution of relative pro-
nouns
The <ORG>Oxford Centre for Maritime
Archaeology<\ORG> [...] which will
<V>conduct<\V> a scientific symposium in
<LOC>Berlin<\LOC>.
2
combination of several of the
options mentioned above
<LOC>Berlin<\LOC> has <V>developed to
become<\V> the entertainment capital of
<LOC>Germany<\LOC>.
7
Figure 6: Table illustrating different options according to which the definition of ?potentially interesting relation?
could be loosened. For each option, an example sentence from the test corpus is given, together with the number
of relations that could be extracted additionally from the test corpus.
e.g., in a sentence like The exhibition [...] shows
<PER>Clemens Brentano<\PER>, <PER>Achim
von Arnim<\PER> and <PER>Heinrich von
Kleist<\PER>, and between NEs occurring in the
same (complex) argument, e.g., <PER>Hanns Peter
Nerger<\PER>, CEO of <ORG>Berlin Tourismus
Marketing GmbH (BTM) <\ORG>, sums it up [...].
5. Related work
Our work is related to previous work on domain-
independent unsupervised relation extraction, in par-
ticular Sekine (2006), Shinyama and Sekine (2006) and
Banko et al (2007).
Sekine (2006) introduces On-demand information ex-
traction, which aims at automatically identifying
salient patterns and extracting relations based on these
patterns. He retrieves relevant documents from a
newspaper corpus based on a query and applies a POS
tagger, a dependency analyzer and an extended NE
tagger. Using the information from the taggers, he ex-
tracts patterns and applies paraphrase recognition to
create sets of semantically similar patterns. Shinyama
and Sekine (2006) apply NER, coreference resolution
and parsing to a corpus of newspaper articles to ex-
tract two-place relations between NEs. The extracted
relations are grouped into pattern tables of NE pairs
expressing the same relation, e.g., hurricanes and their
locations. Clustering is performed in two steps: they
first cluster all documents and use this information to
cluster the relations. However, only relations among
the five most highly-weighted entities in a cluster are
extracted and only the first ten sentences of each arti-
cle are taken into account.
Banko et al (2007) use a much larger corpus, namely
9 million web pages, to extract all relations between
noun phrases. Due to the large amount of data, they
apply POS tagging only. Their output consists of mil-
lions of relations, most of them being abstract asser-
tions such as (executive, hired by, company) rather
than concrete facts.
Our approach can be regarded as a combination of
these approaches: Like Banko et al (2007), we extract
relations from noisy web documents rather than com-
parably homogeneous news articles. However, rather
than extracting relations from millions of pages we re-
duce the size of our corpus beforehand using a query in
order to be able to apply more linguistic preprocessing.
Like Sekine (2006) and Shinyama and Sekine (2006),
we concentrate on relations involving NEs, the assump-
tion being that these relations are the potentially in-
teresting ones. The relation clustering step allows us
to group similar relations, which can, for example, be
useful for the generation of answers in a Question An-
swering system.
6. Future work
Since many errors were due to the noisiness of the ar-
bitrarily downloaded web documents, a more sophisti-
cated filtering step for extracting relevant textual infor-
mation from web sites before applying NE recognition,
parsing, etc. is likely to improve the performance of
the system.
The NER component plays a crucial role for the qual-
ity of the whole system, because the relation extraction
component depends heavily on the NER quality, and
thereby the NER quality influences also the results of
the clustering process. A possible solution to improve
NER in the IDEX System is to integrate a MetaNER
component, combining the results of several NER com-
ponents. Within the framework of the IDEX project
a MetaNER component already has been developed
(Heyl, to appear 2008), but not yet integrated into the
prototype. The MetaNER component developed uses
the results from three different NER systems. The out-
put of each NER component is weighted depending on
the component and if the sum of these values for a pos-
sible NE exceeds a certain threshold it is accepted as
NE otherwise it is rejected.
The clustering step returns many clusters containing
two instances only. A task for future work is to in-
vestigate, whether it is possible to build larger clus-
ters, which are still meaningful. One way of enlarging
cluster size is to extract more relations. This could
be achieved by loosening the extraction criteria as de-
scribed in section 4. Also, it would be interesting to see
whether clusters could be merged. This would require
a manual analysis of the created clusters.
Acknowledgement
The work presented here was partially supported by a
research grant from the?Programm zur Fo?rderung von
Forschung, Innovationen und Technologien (ProFIT)?
(FKZ: 10135984) and the European Regional Develop-
ment Fund (ERDF).
7. References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Proc.
of the International Joint Conference on Artificial
Intelligence (IJCAI).
Andrea Heyl. to appear 2008. Unsupervised relation
extraction. Master?s thesis, Saarland University.
Lc4j. 2007. Language categorization library for Java.
http://www.olivo.net/software/lc4j/.
LingPipe. 2007. http://www.alias-i.com/lingpipe/.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In ACL. The Association for Computer Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted re-
lation discovery. In Proc. of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311. Association
for Computational Linguistics.
Stanford Parser. 2007. http://nlp.stanford.edu/
downloads/lex-parser.shtml.
Ian H. Witten and Eibe Frank. 2005. Data Min-
ing: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
WordNet. 2007. http://wordnet.princeton.edu/.
On UNL as the future "html of the linguistic content" & the reuse of 
existing NLP components in UNL-related applications with the 
example of a UNL-French deconverter 
Gilles St~RASSET 
GETA, CLIPS, IMAG 
385, av. de la biblioth~que, BP 53 
F-38041 Grenoble cedex 9, France 
Gilles.Serasset@ imag,fr 
Christian BOITET 
GETA, CLIPS, IMAG 
385, av. de la biblioth~que, BP 53 
F-38041 Grenoble cedex 9, France 
Christian.Boitet @imag.fr 
Abstract 
After 3 years of specifying the UNL (Universal Networking Language) language and 
prototyping deconverters I from more than 12 languages and enconverters for about 4, the 
UNL project has opened to the community by publishing the specifcations (v2.0) of the UNL 
language, intended to encode the meaning of NL utterances as semantic hypergraphs and to be 
used as a "pivot" representation in multilingual information and communication systems. 
A UNL document is an html document with special tags to delimit the utterances and their 
rendering in UNL and in all natural languages currently handled. UNL can be viewed as the 
future "html of the linguistic content". It is only an interface format, leading as well to the reuse 
of existing NLP components as to the development of original tools in a variety of possible 
applications, from automatic rough enconversion for information retrieval and information 
gathering translation to partially interactive nconversion or deconversion for higher quality. 
We illustrate these points by describing an UNL-French deconverter organized as a specific 
"localizer" followed by a classical MT transfer and an existing generator. 
Keywords 
UNL, interlingua, pivot, deconversion, UNL~French localization, transfer, generation. 
Introduction 
The UNL project of network-oriented 
multilinguat communication has proposed a 
standard for encoding the meaning of natural 
language utterances as semantic hypergraphs 
intended to be used as pivots in multilingual 
information and communication systems. In the 
first phase (1997-1999), more than 16 partners 
representing 14 languages have worked to build 
deconverters transforming an (interlingual) 
UNL hypergraph into a natural language 
utterance. 
In this project, the strategy used to achieve this 
initial objective is free. The UNL-French 
deconverter under development first performs a 
"localization" operation within the UNL format, 
and then classical transfer and generation steps, 
using the Ariane-G5 environment and some 
UNL-specifc tools. 
The use of classical transfer and generation 
steps in the context of an interlingual project 
may sound surprising. But it reflects many 
interesting issues about the status of the UNL 
language, designed as an interlingua, but 
diversely used as a linguistic pivot (disambi- 
guated abstract English), or as a purely semantic 
pivot. 
After introducing the UNL language, we present 
the architecture of the UNL-French deconverter, 
which "generates" from the UNL interlingua by 
first "localizing" the UNL form for French, 
within UNL, and then applying slightly adapted 
but classical transfer and generation techniques, 
implemented in the Ariane-G5 environlnent, 
supplemented by some UNL-specific tools. 
Then, we discuss the use of the UNL language 
as a linguistic or semantic pivot for highly 
multilingual information systems. 
1 The UNL project and language 
1.1 The project 
UNL is a project of multilingual personal 
networking communication initiated by the 
University of United Nations based in Tokyo. 
The pivot paradigm is used: the representation 
I The terms << deconvcrsion, and <~ enconvcrsion, are specific to tile UNL proiect and are defined at paragraph 2.
768 
of an utterance in the UNL interlingua (UNL 
stands for "Universal Networking Language") is 
a hyl)ergraph where normal nodes bear UWs 
CUniversal Words", or interlingual acceptions) 
with semantic attributes, and arcs bear semantic 
relations (deep cases, such as agt, obj, goal, etc.). 
Hypernodes group a subgraph defined by a set 
of connected arcs. A UW denotes a set of 
interlingual acceptions (word senses), although 
we often loosely speak of "the" word sense 
demoted by a UW. 
Because English is known by all UNL 
developers, the syntax of a tlormal WW is: 
"<Engl ish word or compound> ( <list  
of res t r i c t ions> ) ", O. Z. "look for 
( icl>action, agt>human, obj>thing)" 
Going fronl a text to the corresponding "UNL 
text" or interactively constructing a UNL text is 
called "enconversioif', while producing a text 
fiom a sequence of UNL graphs is called 
"deconversion". 
This departure fi'om the standard terms of 
analysis and generation is used to stress that this 
is not a classical M\]: projecl, bu! that UNL is 
planned to be the source format preferred for 
representing textual inl:ormation in tile 
envisaged multilingual network environment. 
Tile schedule of tile project, beginning with 
deconversion rather than cnconvcrsion, also 
reflects that difference. 
14 hmguages have been tackled during the first 
3--year phase of the prqject (1997-1999), while 
many more arc to be added in tile second 
phase. Each group is fi-ee to reuse its own 
software lools and/or lingware resources, or to 
develop directly with tools provided by tile 
UNL Center (UNU/IAS). 
Emphasis is on a very large lexical coverage, so 
that all groups spend most of their time on tile 
UNL-NL lexicons, and develop tools and 
methods for efficient lexical development. By 
contrast, gramnmrs have been initially limited to 
those necessary for deconversion, and will then 
bc gradually expanded to allow for more 
naturalness m formulating text to be 
enconverted. 
1.2 The UNL components 
1.2.1 Universal Words 
Tile nodes of a UNL utterance are called 
Universal Words (or Uws). The syntax of a 
normal UW consists of 2 parts : 
a headword, 
a list of  restrictions 
Because English is known by all UNL 
developers, tile headword is an English word or 
compound. The restrictions are given as all 
attribute value pail" where attributes are semantic 
relation labels (as the ones used in the graphs) 
and wllues are other UWs (restricted or not). 
A UW denotes a collection of interlingual 
acceptions (word senses), although we often 
loosely speak of "the" word sense denoted by an 
UW. For example, the unrestricted UW " look  
for" denotes all the word-senses associated to 
tile English compound word "look for". Tile 
restricted UW " look for ( ic l>action, 
agt>human, obj>thing) " represents all tile 
word senses of the English word "look fo r "  
that are an action, perl%rmed by a human that 
affects a thing. In this case this leads to the word 
sense: "look fo r -  to try to find". 
1.2.2 UNL hypergraph 
A UNL expression is a hypergraph (a graph 
where a node is simple or recursively contains a 
hypergraph). Tile arcs bear semantic relation 
labels (deep cases, such as agt, obj, goal, etc.). 
score(icl>event agt>human,tld>sport) \[ I @ entry. @ past. @ complete \[ 
7 i \ 
agt ....... ' /  ins \~  
i Rona~do 1 ?b~ / \ pit head(p~ol>body) ~ "\\ 
/ ,~  corner / 
goal i~cl>thing) \[.41 obj mod \[ 
~left : 
Figm'e I. 1: A UNL graph deconvertible as "Ronaldo 
has headed the ball into the left corner of the net" 
In a UNL graph, UWs appear with attributes 
describing what is said from tile speaker's point 
of view. This includes phenomena like speech 
acts, truth wllues, time, etc. 
Hypernodes may also be used ill UNL 
expressions. 
agt ...... i 
I driver.~Pl \] 
aoj 
I reckless \] 
01.@entry 
\[ drink\] 
\ 
drive \] 
Figure 1.2: A UNL Io,pergraph that may be 
deconverted as "Reckless drivers drink and drive" 
Graphs and subgraphs nmst contain one special 
node, called the entry of tile graph. 
1.2.3 Denoting a UNL  graph 
These hypergraphs are denoted using the UNL 
language per se. In the UNL hmguagc, an 
769 
expression consists in a set of arcs, connecting 
the different nodes. As an example, the graph 
presented in figure 1.1 will be denoted as: 
agt(score(...).@entry.@past.@complete, 
Ronaldo) 
obj(score(_.).@entry.@past.@complete, 
goal(icl>thing)) 
ins(score(...) .@entry.@past.@complete, 
head(pof>body)) 
plt(score(...) .@entry.@past.@complete, 
corner) 
obj (corner, goal(icl>thing)) 
mod(corner, left) 
Hypernodes are denoted by numbers. The 
graph contained by a hypernode is denoted as a 
set of arcs colored by this number as in: 
agt (:Ol.@entry, driver. @pl) 
aoj (reckless, driver.@pl) 
and:Ol (drive, drink.@entry) 
Entries of the graph and subgraphs are denoted 
with the ".@entry" attribute. 
2 Inside the French deconverter  
2.1 Overv iew 
Deconversion is the process of transforming a 
UNL graph into one (or possibly several) 
utterance in a natural language. Any means 
may be used to achieve this task. Many UNL 
project partners use a specialized tool called 
DeCo but, like several other partners, we choose 
to use our own tools for this purpose. 
One reason is that DeCo realizes the 
deconversion in one step, as in some transfer- 
based MT systems such as METAL \[17\]. We 
prefer to use a more modular architecture and 
to split deconversion into 2 steps, transfer and 
generation, each divided into several phases, 
most of them written in Arlene-G5. 
Another reason for not using DeCo is that it is 
not well suited for the morphological gene- 
ration of inflected languages (several thousands 
rules are needed for Italian, tens of thousands 
for Russian, but only about 20 rules and 350 
affixes suffice to build an exhaustive GM for 
French in Sygmor). Last, but not least, this 
choice allows us to reuse modules already 
developed for French generation. 
This strategy is illustrated by figure 2.1. 
/~;~,; o_;.,,.~,', Transfer 
~ . . . . . .  v" Ge I~ati0n her 
/ \,4v 
French utterance 
Fig. 2.1:2 possible deconversqon strategies 
Using this approach, we segment the decon- 
version process into 7 phases, as illustrated by 
figure 2.2. 
The third phase (graph-to-tree) produces a 
decorated tree which is fed into an Ariane-G5 
TS (structural transfer). 
Valklatiolff l,exicaI l'lansl~.'r (h~,li~h 1otree 
\[ .ocalization COllversion 
,Z~ "UNL Tree" 
l'araphra~c choice 
UMA structure?N\ 
Syntactic ~gcnerali(ln 
,t 
UMC structure~ 
Morl~lml.gic\[ll generation 
't 
French utterance 
Fig. 2.2: architecture of the French deconverter 
2.2  Trans fer  
2.2.1 Validation 
When we receive a UNL Graph for decon- 
version, we first check it for correctness. A UNL 
graph has to be connected, and the different 
features handled by the nodes have to be 
defined in UNL. 
If the graph proves incorrect, an explicit error 
message is sent back. This validation has to be 
performed to ilaprove robustness of the 
deconverter, as there is no hypothesis on the 
way a graph is created. When a graph proves 
valid, it is accepted for deconversion. 
2.2.2 Loeal&ation 
In order to be correctly deconverted, tile graph 
has to be slightly modified. 
2.2.2.1 Lexical localization 
Some lexical units used in the graph may not be 
present in the French deconversion dictionary. 
This problem may appear under different 
circumstances. First, the French dictionary 
(which is still under development) may be 
incomplete. Second, the UW nmy use an 
unknown notation to represent a known French 
word sense, and third, the LAV may represent a 
non-French word sense. 
We solve these problems with the same method :
Let w be a UWin  the graph G. Let D be the 
French dictionary (a set of UWs). We substitute 
w in G by w' such that: w' e D and 
VxeD d(w, w', G) = d(w, x, G). where d is a 
pseudo-distance function. 
770 
If different French UWs are at the same pseudo- 
distance of w, w' is chosen at random among 
these UWs (default in non-interactive mode). 
2.2.2.2 "Cultural" localization 
Some crucial information may be missing, 
depending on the language of the source 
utterance (sex, modality, number, determination, 
politeness, kinship...). 
It is in general impossible to solve this problem 
fully automatically in a perfect manner, as we 
do not know anything about the document, its 
c:ontext, and its intended usage: FAHQDC 2 is no 
more possible than FAHQMT on arbitrary texts. 
We have to rely on necessarily imperfect 
heuristics. 
ttowever, we can specialize tile general French 
deconverter to produce specialized servers for 
different tasks and different (target) 
sublanguages. It is possible to assign priorities 
not only to various parts of the dictionaries 
(e.g., specialized vs. general), but also to 
equivalents of the same UW within a given 
dictionary. We can then define several user 
profiles. It is also possible to build a memory of 
deconverted and possibly postedited utterances 
for each specialized French deconversion 
server. 
2.2.3 Lexical Transfer 
After the localization phase, we have to perform 
the lexical transfer. It would seem natural to do 
ill within Ariane-G5, after converting the graph 
into a tree. But lexical transfer is context- 
sensitive, and we want to avoid the possibility of 
transferring differently two tree nodes 
corresponding to one and the same graph node. 
Each graph node is replaced by a French lcxical 
unit (LU), along with some variables. A lexical 
unit used in tile French dictionary denotes a 
derivational family (e.g. in English: destroy 
denotes destroy, destruction, destructible, 
destructive .... in French: d6truire for d6truire, 
destruction, destructible, indestructible, 
destructif, destructeur). 
There may be several possible lexical units for 
one UW. This happens when there is a real 
synonymy or when different erms are used in 
different domains to denote the same word 
sense  3. In  that case, we currently choose tile 
lexical unit at random as we do not have any 
information on tile task the deconverter is used 
for. 
Tile same problem also appears because of tile 
slrategy used to build the French dictionary. In 
order to obtain a good coverage from the 
beginning, we have underspecified tile UWs and 
linked them to dift'ercnt lexical units. This way, 
we considered a UW as tile denotation of a set 
of word senses in French. 
Hence, we were able to reuse previous 
dictionaries and we can use the dictionary even 
if it is still under development and incolnplete. 
In our first version, we also solve this problem 
by a random selection of a lexical unit. 
2.2.4 Graph to tree conversion 
The subsequent deconversion phases are 
performed in Ariane-G5. Hence, it is necessary 
to convert he UNL hypergraph into an Ariane- 
G5 decorated tree. 
The UNL graph is directed. Each arc is labelled 
by a semantic relation (agt, obj, ben, con...) and 
each node is decorated by a UW and a set of 
features, or is a hypernode. One node is 
distinguished as the "entry" of the graph. 
An ARIANE tree is a general (non binary) tree 
with decorations on its nodes. Each decoration 
is a set of wlriable-value pairs. 
The graph-to-tree conversion algorithln has to 
lnaintain the direction and labelling of the 
graph along with the decoration ot' the nodes. 
Our algorithm splits tile nodes that are the target 
of more than one arc, and reverses the direction 
of as few arcs as possible. An example of such a 
conversion is shown in figure 2.3. 
! a \] 
\[5E3 / \  
J x, 
x y 
I~!  I ce  
z t 
b (x  
I I => l 
d : z +~ 
c :Y  i 
? :1  
Fig. 2.3: example graph to tree convel:vion 
Let Z be the set of nodes of G, A the set of 
labels, T the created tree, and N is the set of 
nodes of T. 
Tile graph G={ (a,b,l) l ac  Y.,b6 Z , I~  A} is 
defined as a set of directed labelled arcs. We use 
an association list A = { (n,;,n.r) I ,,,+ ~ r,, U. r E 
N }, where we memorize the correspondence 
between nodes of the tree and nodes of the 
graph. 
2 fully autonmtic high quality dcconvcrsion. 
3 strictly speaking, tile same collection of intcrlingual 
woM senses (acccptions). 
771 
l e t  e(; e  such that e is the entry  of G 
e r 6- new tree-node (ed, entry) 
inT  +- er ( ) ;  N 6- {e,r\]; A <-- {(ec;,eT)} 
whi le G :~ O do 
if there  is (a,b,l) in  G such  that  
G ~- G \ (a ,b , l ) ;  
b r 6- new tree-node(b, i) ; 
A 6- A <J {(b,b,,)); 
l e t  a, r e N such that  (a,a, r) e A 
in  add b r to the daughters of a,r; 
else if there  is (a,b,l) in G such  that  (b,br) 6 
G e- G \ (a ,b , l ) ;  
a T ( -new tree-node(a, l  i); 
A <--- A U {(a ,a . r )} ;  
l e t  brl,e N such  that  (b,br)  e A 
i n  add a,, to the daughters of br; 
else exi t  on  er ro r  ("non connected  graph") ;  
(a ,  a. r) e A then  
A then  
2.2.5 Structural transfer 
The purpose of the structural transfer is to 
transform the tree obtained so far into a 
Generating Multilevel Abstract (GMA) structure 
\[4\]. 
In this structure, non-interlingual linguistic 
levels (syntactic functions, syntagmatic 
categories...) are underspecified, and (if 
present), are used only as a set of hints for the 
generation stage. 
2.3 Generation 
2.3.1 Paraphrase choice 
The next phase is in charge of the paraphrase 
choice. During this phase, decisions are taken 
regarding the derivation applied to each lexical 
unit in order to obtain the correct syntaglnatic 
category for each node. During this phase, the 
order of appearance and the syntactic functions 
of each parts of the utterance is also decided. 
The resulting structure is called Unique 
Multilevel Abstract (UMA) structure. 
2.3.2 Syntactic and morphological generation 
The UMA structure is still lacking the syntactic 
sugar used in French to realize the choices 
made in the previous phase by generating 
articles, auxiliaries, and non connected 
compunds uch as ne...pas, etc. 
The role of this phase is to create a Unique 
Multilevel Concrete (UMC) structure. By 
concrete, we mean that the structure ~s 
projective, hence the corresponding French text 
may be obtained by a standard left to right 
traversal of the leaves and simple morphological 
and graphemic rules. The result of these phases 
is a surface French utterance. 
3 Different uses of the UNL language 
3.1 Hypergraphs  vs colored graphs 
As presented in section 1.2.3, the syntax of the 
UNL language is based on the description of a 
graph, arc by arc. Some of these arcs are 
"coloured" by a number. This colouring is 
currently interpreted as hypernodes (nodes 
containing a graph, rather than a classical UW). 
This interpretation is arbitrary and imposes 
semantic onstraints on a UNL utterance: 
the subgraph (the set of arcs labeled with 
the same colour) is connected, 
arcs with different colours cannot be 
connected to the same node. 
However, even if one uses the UNL language 
for a particular kind of application, a different 
interpretation may be chosen. By adding new 
semantic constraints to UNL expressions, one 
may restrict o the use of trees. On the contrary, 
by loosening semantic onstraint, one may use 
colored graphs instead of the more restrictive 
hypergraphs. 
This flexibility of UNL may lead to uses that 
differ from the computer science point of view 
(different structures leading to different kinds 
of methods and applications) as well as from the 
linguistic point of view (different ways to 
represent the linguistic content of a utterance). 
This kind of structure is very useful to represent 
some utterances like "Christian pulls Gilles' 
leg". Using a colored graph, one can represent 
the utterance with the graph shown in figure 
3.1, which is not a hypergraph. 
772 
01 .@entry 
ag t.. i\[ pull.@entry i 
\[ Chns~lian \] I ~,obj 
pos  
G es \] 
Figure 3.1: this graph is not cut hypergral)h, it can 
however be represented in UNL htnguage 
When using normal hypergraphs, one could 
only represent the utterance as shown in figure 
3.2. 
agt  .... \[ make fun of i 
i Chns'~tan I , i ob j  
i i 
Figure 3.2: this graph is a valid hyperglztph 
Heuce, keeping backward compatibility with 
other UNL based systems, one may develop an 
entirely new and more powerfld kind of 
application. 
3.2 L inguist ic  vs senmnt ie  pivot 
The UNL language defines the interface 
structure to be used by applications (either a 
hypergraph or a colored graph). However, it 
does not restrict the choice of the data to be 
encoded. 
Since tile beginning, two possible and wflid 
apl~roaches has been mentioned. During the 
kickoff meeting of tile UNL prelect, Pr. Tsujii 
prolnoted the use of UNL as a linguistic pivot. 
With this approach, a UNL utterance should be 
the encoding of the deep structure of a valid 
English utterance that reflects the meaning of 
the source utterance. With this approach, the 
German sentence "Hans schwimt sehr gern"  
should be encoded as shown in figure 3.3. 
agt.. _ - like.@entry ~.  .. 
\[ Ha-'~s \[ ' "-. man 
I ob  j "-. "A, 
"~--agt ........ \[ s~wim \] i much , 
Figmv 3.3: a linguistic encoding of "ltcms schwimt 
sehr gern " 
On the opposite, Hiroshi Uchida promotes the 
use of UNL as a semantic pivot. With this 
second approach, the same sentence should be 
encoded as shown in figure 3.4. 
agt /zswim.l@entry 
/ / /  I ~ iman 
H wil~lgly 
l ined 
Figure 3.4: a semantic encoding of "ltans schwimt sehr 
gem" 
Each approach has its advantages and 
drawbacks and the choice between them can 
only be made with an application in mind. The 
linguistic approach leads to a better quality ill 
the produced results and is an answer to highly 
multilingual machine translation projects. With 
this approach, the UNL graphs can only be 
produced by people mastering English or by 
(partially) automatic enconverters. 
With the semantic approach, subtle differences 
in source utterances (indefinite, reflexivity...) 
can not be expressed, leading to a lower quality. 
However, using this approach, the UNL 
encoding is much more natural and easy to 
perform by a non English speaker (as the 
semantic relations and UWs are expressed at the 
source level). Hence, this approach is to be used 
for multilingual casual communication where 
users may express themselves by directly 
encoding UNL expressions with an appropriate 
editing tool. 
Conc lus ion  
Working oil tile French deconvel-ter has led to 
im interestiug architecture where deconversion, 
in principle a "generation from interlingua", is 
implemented as transfer + generation from all 
abstract structure (UNL hypergraph) produced 
from a NL utterance. The idea to use UNL for 
directly creating documents gets here an 
indirect and perhaps paradoxical support, 
although it is clear that considerable progress 
and innovative interface design will be needed 
to make it practical. 
However, the UNL language proves flexible 
enough to be used by very different proiects. 
Moreover, with deconverters currently 
developed for 14 languages, joining the UNL 
project is really attractive. Let's hope that this 
effort will help breaking the language barriers. 
Acknowledgements  
We would like to thank the sponsors of the UNL 
project, especially UNU/IAS (T. Della Senta) & 
ASCII (K.Nishi) and of the UNL-FR subproject, 
especially UJF (C. Feuerstein), IMAG 
(J. Voiron), CLIPS (Y. Chiaramella), and the 
773 
French Ministery of Foreign Affairs (Ph. Perez), 
as well as the members of UNL Center, 
especially project leader H. Uchida, M. L. Zhu, 
and K. Sakai. Last but not least, other members 
of GETA have contributed in many ways to the 
research reported here, in particular N. N6deau, 
E. Blanc, M. Mangeot, J. Sitko, L. Fischer, 
M. Tomokiyo, and K. Fort. 
References  
\[1\] Blanc l~. & Guillaume P. (1997) 
Developing MT lingware through hlternet : 
ARIANE and the CASH interface. Proc. Pacific 
Association for Computational Linguistics 1997 
Conference (PACLING'97), Ohme, Japon, 2-5 
September 1997, vol. 1/1, pp. 15-22. 
\[2\] Blanehon H. (1994) Persl)ectives of DBMT 
for monolingual uthors on the basis of LIDIA-I, an 
implemented mockup. Proc. 15th International 
Conference on Computational Linguistics, 
COLING-94, 5-9 Aug. 1994, vol. 1/2, pp. 
115--119. 
\[3\] Boitet C., R6d. (1982) "DSE-I"--  Le point 
sur ARIANE-78 ddbut 1982. Contrat ADI/CAP- 
Sogcti/Champollion (3 vol.), GETA, Grenoble, 
fdvrier 1982, 400 p. 
\[4\] Boitet C. (1994) Dialogue-Based MT attd 
se(f exl)lahting documents as atl alternative to 
MAHT and MT of controlled languages. Proc. 
Machine Translation 10 Years On, 11 - 14 Nov. 1994, 
Cranfield University Press, pp. 22.1--9. 
\[5\] Boitet C. (1997) GETA's MT methodology 
attd its current development towards petwonal 
networking communication attd speech translation in 
the context of the UNL and C-STAR projects. Proc. 
PACLING-97, Ohme, 2-5 September 1997, Meisei 
University, pp. 23-57. (invited communication) 
\[6\] Boitet C. & Blanehon H. (1994) 
Multilingual Dialogue-Based MT for monolingual 
authotw: the LIDIA project arm a fil:s't mockup. 
Machine Translation, 9/2, pp. 99--132. 
\[7\] Boitet C., Guillaume P. & Qu6zel- 
Ambrunaz M. (1982) ARIANE-78, an 
hltegrated environment for atttomated translation attd 
human revision. Proc. COL1NG-82, Prague, July 
1982, pp. 19--27. 
\[18\] Brown R. D. (1989) Augmentation. 
Machine Translation, 4, pp. 1299-1347. 
\[19\] Ducrot J.-M. (1982) TITUS IV. In 
"Information research in Europe. Proc. ot' tile 
EURIM 5 conf. (Versailles)", P. J. Taylor, cd., 
ASLIB, London. 
\[10\] Kay M. (1973) The MIND system. In 
"Courant Computer Science Symposium 8: Natural 
Language Processing", R. Rustin, ed., Algorithmics 
Press, Inc., New York, pp. 155-188. 
\[11\] Maruyama H., Watanabe H. & Oglno S. 
(1990) An Interactive Japanese Parser for Machine 
Translation. Proc. COLING-90, Helsinki, 20-25 
aofit 1990, ACL, vol. 2/3, pp. 257-262. 
\[12\] Melby A. K., Smith M. R. & Peterson 
J. (1980) ITS : An Interactive Translation 
System. Proc. COLING-80, Tokyo, 30/9-4/10/80, 
pp. 424---429. 
1113\] Moneinme W. (1989) (159 p. 
+annexes)7;40 vet's" l'arabe. Sp&'ification d'une 
gdudration sfatldard e l'arabe. Rdalisation d'un 
l)romO'l)e anglais'-ambe ?t partir d'un attalyseur 
existant. Nouvelle thbse, UJF. 
\[114\] Nirenburg S. & al. (1989) KBMT-89 
Project Report. Center for Machine Translation, 
Carnegie Mellon University, Pittsburg, April 1989, 
286 p. 
\[115\] Nyberg E. H. & Mitamura T. (1992) 
The KANT system: Fast, Accurate, High-Quality 
Translation in Practical Domains. Proc. COLING- 
92, Nantes, 23-28 July 92, ACL, vol. 3/4, pp. 
1069--1073. 
\[16\] Qu6zel-Ambrunaz M. (1990) Ariane-G5 
v.3 - Le moniteut: GETA, IMAG, juin 1990, 206 p. 
\[17\] Sloeum J. (1984) METAL: the LRC 
Machine Translation O,stem. In "Machine 
Translation today: the state of the art (Proc. third 
Lugano Tutorial, 2-7 April 1984)", M. King, cd., 
Edinburgh University Press (1987). 
\[18\] Wehrli E. (1992) The IPS System. Proc. 
COLING-92, Nantes, 23-28 July 1992, vol. 3/4, pp. 
870-874. 
774 
Coedition to share text revision across languages
and improve MT a posteriori
Christian BOITET
GETA, CLIPS, IMAG
385 rue de la Biblioth?que, BP 53
38041 Grenoble cedex 9, France
Christian.Boitet@imag.fr
TSAI Wang-Ju
GETA, CLIPS, IMAG
385 rue de la Biblioth?que, BP 53
38041 Grenoble cedex 9, France
Wang-Ju.Tsai@imag.fr
Abstract
Coedition of a natural language text and its representation in some interlingual form seems the best and simplest way to
share text revision across languages. For various reasons, UNL graphs are the best candidates in this context. We are
developing a prototype where, in the simplest sharing scenario, naive users interact directly with the text in their
language (L0), and indirectly with the associated graph. The modified graph is then sent to the UNL-L0 deconverter and
the result shown. If is is satisfactory, the errors were probably due to the graph, not to the deconverter, and the graph is
sent to deconverters in other languages. Versions in some other languages known by the user may be displayed, so that
improvement sharing is visible and encouraging. As new versions are added with appropriate tags and attributes in the
original multilingual document, nothing is ever lost, and cooperative working on a document is rendered feasible. On the
internal side, liaisons are established between elements of the text and the graph by using broadly available resources
such as a L0-English or better a L0-UNL dictionary, a morphosyntactic parser of L0, and a canonical graph2tree
transformation. Establishing a "best" correspondence between the "UNL-tree+L0" and the "MS-L0 structure", a lattice,
may be done using the dictionary and trying to align the tree and the selected trajectory with as few crossing liaisons as
possible. A central goal of this research is to merge approaches from pivot MT, interactive MT, and multilingual text
authoring.
Keywords: revision sharing, interlingual representation, text / UNL coedition, multilingual communication
R?sum?
La co?dition d'un texte en langue naturelle et de sa repr?sentation dans une forme interlingue semble le moyen le
meilleur et le plus simple de partager la r?vision du texte vers plusieurs langues. Pour diverses raisons, les graphes UNL
sont les meilleurs candidats dans ce contexte. Nous d?veloppons un prototype o?, dans le sc?nario avec partage le plus
simple, des utilisateurs "na?fs" interagissent directement avec le texte dans leur langue (L0), et indirectement avec le
graphe associ?. Le graphe modifi? est ensuite envoy? au d?convertisseur UNL-L0 et le r?sultat est affich?. S'il est
satisfaisant, les erreurs ?taient probablement dues au graphe et non au d?convertisseur, et le graphe est envoy? aux
d?convertisseurs vers d'autres langues. Les versions dans certaines autres langues connues de l'utilisateur peuvent ?tre
affich?es, de sorte que le partage de l'am?lioration soit visible et encourageant. Comme les nouvelles versions sont
ajout?es dans le document multilingue original avec des balises et des attributs appropri?s, rien n'est jamais perdu, et le
travail coop?ratif sur un m?me document est rendu possible. Du c?t? interne, des liaisons sont ?tablies entre des
?l?ments du texte et du graphe en utilisant des ressources largement disponibles comme un dictionnaire L0-anglais, ou
mieux L0-UNL, un analyseur morphosyntaxique de L0, et une transformation canonique de graphe UNL ? arbre. On
peut ?tablir une "meilleure" correspondance entre "l'arbre-UNL+L0" et la "structure MS-L0", une treille, en utilisant le
dictionnaire et en cherchant ? aligner l'arbre et une trajectoire avec aussi peu que possible de croisements de liaisons. Un
but central de cette recherche est de fusionner les approches de la TA par pivot, de la TA interactive, et de la g?n?ration
multilingue de texte.
Mots-cl?s: r?vision partag?e, repr?sentation interlingue, co?dition texte / UNL, communication multilingue
Introduction
Creating and maintaining aligned multilingual
documents is a growing necessity. In the current
practice, a multilingual document consists in many
parallel monolingual files, which may be technical
documentation as well as help files, message files,
or simply thematic information put on the web and
intended for a multilingual audience (medicine,
cooking, travel?). The task is difficult even for a
document managed in a centralized manner.
Ususally, it is first created in a unique source
language, and translated into several target
languages. There must be a way to keep trak of
modifications, possibly done at various places on
different linguistic versions. From time to time,
somebody has to decide which modifications to
integrate in the next release of the document. For
that, modifications done in target languages have to
be translated back into the source language. The
new and the old source versions are then compared
using (fuzzy) matching techniques, so that only
really new segments are sent for translation.
The problem arises even more if the documents are
not managed centrally, so that the monolingual files
are often in various formats (Word, EgWord,
Interleaf, FileMaker, DBMS formats, etc.).
A. Assimi [1, 2] has shown how to "realign"
parallel decentralized documents and apply the
methodology sketched above. However, in both
cases, human translators have to retranslate the
modified or new source segments, or to revise them
if they are retranslated by a quality MT system.
Contrary to what is often said, quality MT exists,
but for specific contexts only. (See [14]).
What we would like to do is to make it possible to
share the revision work across languages, whatever
the domain and the context. It is clearly impossible
to reflect changes on a file in language L0 into files
in L1,? Ln automatically and faithfully, without
any intermediate structure to bridge the gap,
because that would necessitate at least a perfect
fine-grained aligner in case of changing articles or
common nouns (provided the gender and number
stay the ame in each Li version). In case of
replacing a verb by another with a different valency
frame in a target Li, the sentence in Li would have
to be reanalyzed, transformed accordingly, and
regenerated without introducing any new error or
imprecision, thereby keeping the manual
improvements coming from previous manual
revisions. Or we would need a more than perfect
MT system, namely one which would be able to
analyze the changed utterance in L0, and to transfer
and generate it into a sentence of Li as close as
possible as the previous sentence in Li, which again
could have been improved manually before.
The best and simplest way to go seems to use some
formalized interlingua IL and to
(1) reflect the modifications from L0 to the IL,
(2) regenerate into L1,? Ln from the IL.
We should also allow for direct manual
improvements, considering that the IL form will not
always be present, or not always improvable
enough for lack of expressivity, or that generators
will never be perfect. We choose UNL [3, 4, 10,
11] as our IL of choice for various reasons:
(1) it is specifically designed for linguistic and
semantic machine processing,
(2) it derives with many improvements from
H.Uchida's pivot used in ATLAS-II (Fujitsu)
[13], still evaluated as the best quality MT system
for English-Japanese, with a large coverage
(586,000 lexical entries in each language),
(3) participants of the UNL project1 have built
"deconverters" from UNL into about 12
languages, and at least the Arabic, Indonesian,
Italian, French, Russian, Spanish, and Thai
                                                           
1
 http://unl.ias.unu.edu
deconverters were accessible for experimentation
through a web interface at the time of writing,
(4) although formal, UNL graphs (see below) are
quite easy to understand with little training and
may be presented in a "localized" way to naive
users by translating UNL symbols (semantic
relations, attributes) and lexemes (UWs) into
symbols and lexemes of their language,
(5) the UNL project has defined a format embed-
ded in html for files containing a complete
multilingual document aligned at the level of
utterances, and produced a "visualizer" trans-
forming a UNL file into as many html files as
languages, and sending them to any web browser.
The UNL representation of a text is a list of
"semantic graphs", each expressing the meaning of
a natural language utterance. Nodes contain lexical
units and attributes, arcs bear semantic relations.
Connex subgraphs may be defined as "scopes", so
that a UNL graph may be a hypergraph.
The lexical units, called Universal Words (UW),
represent (sets of) word meanings, something less
ambitious than concepts. Their denotations are built
to be intuitively understood by developers knowing
English, that is, by all developers in NLP. AUW is
an English term or special symbol (number?)
possibly completed by semantic restrictions : the
UW "process" represents all word meanings of that
lemma, seen as citation form (verb or noun here),
and "process(icl>do, agt>person)" covers only the
meanings of processing, working on, etc.
The attributes are the (semantic) number, genre,
time, aspect, modality, etc., and the 40 or so
semantic relations are traditional "deep cases" such
as agent, (deep) object, location, goal, time, etc.
One way of looking at a UNL graph corresponding
to an utterance in language L is to say that it
represents the abstract structure of an equivalent
English utterance "seen from L", that is, where
semantic attributes not necessarily expressed in L
may be absent (e.g., aspect coming from French,
determination or number from Japanese, etc.).
We will first present scenarios of increasing internal
complexity for the situation where somebody reads
a UNL document in her language, corrects it, and
wants the corrections to carry over to the corres-
ponding fragment in other languages. We will then
study more precisely the correspondence between a
text in language L0 and its representation in UNL,
and show the advantage of breaking it into 3 parts:
text ? morpho-syntactic lattice or chart ? abstract
"UNL-tree" ? UNL graph. Finally, we present the
current status of this work: an experimentation web
site, a method to establish the second part of the
correspondence, and related research.
1 .  Scenarios for sharing revision across
languages
Suppose a collection of multilingual documents is
stored on a server as multilingual files in UNL-html
format, or in any other form, e.g. in a data base,
provided (1) it is possible to easily produce the
version in any language contained in the document,
(2) the versions are aligned at the level of utterance-
like segments (a segment may contain more than 1
utterance), (3) UNL-graphs may be stored and
aligned with the segments. Here is a slightly
simplified example of a file in UNL-html format.
<HTML><HEAD><TITLE>Example 1  El/UNL
</TITLE></HEAD><BODY>
[D:dn=Mar Example 1, on= UNL French,
mid=First.Author@here.com]
[P][S:1]{org:el}I ran in the park yesterday.{/org}
{unl}agt(run(icl>do).@entry.@past,i(icl>person))
plc(run(icl>do).@entry.@past,park(icl>place).@def)
tim(run(icl>do).@entry.@past,yesterday){/unl}
{cn dtime=20020130-2030, deco=man}
+1'*&8:7, {/cn}
{de dtime=20020130-2035, deco=man}
Ich lief gestern im Park. {/de}
{es dtime=20020130-2031, deco=UNL-SP}
Yo corri ayer en el parque.{/es}
{fr dtime=20020131-0805, deco=UNL-FR}
J?ai couru dans le parc hier. {/fr}[/S]
[S:2]{org:el}My dog barked at me.{/org}{unl}
agt(bark(icl>do).@entry.@past,dog(icl>animal))
gol(bark(icl>do).@entry.@past,i(icl>person))
pos(dog(icl>animal),i(icl>person))
{/unl}{de dtime=20020130-2036, deco=man}
Mein Hund bellte zu mir.{/de}
{fr dtime=20020131-0806, deco=UNL-FR}
Mon chien aboya pour moi. [/S] [/P][/D]
</BODY></HTML>
The French versions have been produced
automatically, the German and Chinese manually.
The output of the UNL viewer for French is:
<HTML><HEAD><TITLE>
Example 1  El/UNL
</TITLE></HEAD><BODY>
J?ai couru dans le parc hier.
Mon chien aboya pour moi.
</BODY></HTML>
and will probably be displayed by a browser as:
Example 1  El/UNL
J?ai couru dans le parc hier. Mon chien aboya pour
moi.
and similarly for all other languages. In all
scenarios, the user is reading the text in the normal
display, not seing any tags, and wants to make some
modification, such as moving "hier" after "couru"
and changing "pour" to "vers". Activating some
button or menu item, she enters a revision interface.
1.1 Multiple revision without sharing
In this first scenario, we don't suppose that there are
UNL graphs associated with the segments. The
problem is to transmit and add the user's modifi-
cations to the original form of the multilingual
document. That is impossible by editing the html
documents displayed, because they have no links to
the original form. The UNL-html format predates
XML, hence the special tags like [S] and {unl}, but
we may transform it into an equivalent "UNL-xml"
format. Then, using DOM and javaScript, it is
possible to produce various views: that of a viewer,
a bilingual or multilingual editable presentation,
and a revision (coedition) interface.
This is an example from an experiment performed
for the "Forum Barcelona 2004" on Spanish,
Italian, Russian, French and Hindi.
Hindi and Russian are not shown, but
Japanese has been added by hand. The
XML form is simplified.
Correct sentences are produced by the
deconverters from correct and
complete UNL graphs. We suppose
here that the UNL graph has been
produced from a Chinese version, and
does not countain definiteness and
aspectual information. Now all results
are wrong wrt articles, and some wrt
aspect.
<unl:S num="1">
'/20$*")&<unl:org lg="cn"> -1.#%+(,  </unl:org>
<unl:unl>
<unl:arc> agt(retrieve(icl>do).@entry.@future, city) </unl:arc>
<unl:arc> tim(retrieve(icl>do).@entry.@future, after) </unl:arc>
<unl:arc> obj(after, Forum) </unl:arc>
<unl:arc> obj(retrieve(icl>do).@entry.@future, zone(icl>place).@indef) </unl:arc>
<unl:arc> mod(zone(icl>place).@indef, coastal) </unl:arc> </unl:unl>
<unl:cn> '/20$*")& -1.#%+(,  </unl:cn>
<unl:el> After a Forum, a city will retrieve a coastal zone.</unl:el>
<unl:es> Ciudad recobrar? una zona de costal despu?s Foro. </unl:es>
<unl:fr> Une cit? retrouvera une zone c?ti?re apr?s un forum. </unl:fr>
<unl:it> Citt? ricuperar? une zona costiera dopo Forum. </unl:it>
<unl:jp 	?> </unl:jp>
</unl:S>
The following interface, designed to be used with sharing, may also be used by a reader knowing several
languages, displayed on demand.
For example, a
nat ive  Spanish
speaker knowing
French and English
would put the
correct articles ("La
ciudad", "La cit?",
"The city", etc.) and
the perfective as-
pect ("habra reco-
brado", "will have
recovered"), but a
native French spea-
ker would probably
not correct the
aspect in English
a n d  S p a n i s h ,
because aspect is
often underspecified
in French, e.g. in
"retrouvera".
Original text
Possible Modifications
Second Deconversion
Manual Insertion
Une cit? retrouvera une zone c?ti?re apr?s un forum.
Show Graph
La cit? retrouvera une zone c?ti?re apr?s le Forum.
Deconversion Find Lemma
Une cit? retrouvera une zone c?ti?re apr?s un forum.
English
After a Forum, a city will
retrieve a coastal zone.
Spanish
Ciudad recobrar? una zona
de costal despu?s Foro.
Italian
Citt? ricuperar? une zona
costiera dopo Forum.
Japanese
	?

Chinese
'/20$*")&-1.
#%+(,
 
QuitSave
Find Correspondence Save Graph
After the Forum, the city will
have recovered a coastal zone.
La ciudad habr? recobrado una
zona de costal despu?s el Foro
La citt? ha ricuperar? une
zona costiera dopo il Forum.
	?
PolyphraZ : a tool for the management of parallel corpora
Najeh HAJLAOUI
GETA, CLIPS, IMAG
Universit? Joseph Fourier, BP 53
38041 Grenoble, France
Najeh.Hajlaoui@imag.fr
Christian BOITET
GETA, CLIPS, IMAG
Universit? Joseph Fourier, BP 53
38041 Grenoble, France
Christian.Boitet@imag.fr
Abstract
The PolyphraZ tool is being developed in the
framework of the TraCorpEx project
(Translation of Corpora of Examples), to
manage parallel multilingual corpora through
the web. Corpus files (monolingual or
multilingual) are firstly converted to a
standard coding (CXM.dtd, UTF8). Then, they
are assembled (CPXM.dtd) to visualize them
in parallel through the web. In a third stage,
they are put in a Multilingual Polyphraz
Memory (MPM). A "polyphrase" is a structure
containing an original sentence and various
proposals of equivalent sentences, in the same
and other languages. An MPM stores one or
more corpora of polyphrazes. The MPM part
of PolyphraZ has 3 main web interfaces. One
is a web-oriented translator workstation
(TWS), where suggestions or translations
come from the MPM itself, which functions as
its own translation memory, and from calls to
MT systems. Another serves to send sentences
to MT systems with appropriate parameters,
and to run various evaluation measures (NIST,
BLEU, and distance computations) in order to
propose to the translator a "best" proposal. A
third interface is planned for giving feedbacks
to the developers of the MT systems, in the
form of lists of unknown or wrongly translated
words, with suggestions for correct
translations, and of parallel presentation of
pairs of translations showing the "editing
work" to be done to get one from the other.
The first 2 stages are operational, and used for
experimentation and MT evaluation on the
CSTAR 5-lingual BTEC corpus and on the
Japanese-English Tanaka corpus used as a
source of examples in electronic dictionaries
(JDict, Papillon). A main goal of this effort is
to offer occasional and volunteer translators
and posteditors access to a free TWS and to
sharable translation memories put in the MPM
format.
1 Introduction
Due to Internet grow, the number of available
documents grows dramatically. There is a strategic
need for companies to produce and manage
information written in more than 30 languages
(HP, IBM, MS, Caterpillar). This requires
powerful tools to manage multilingual documents.
Current techniques for handling multilingual
documents use large-grained linking (at the level
of HTML pages), but don't allow fine-grained
synchronization (at paragraph or sentence level)
and don't permit bilingual or multilingual editing
through the Web.
The interest to synchronize at least at the level of
sentences is double:
? make it possible to use Machine Aided Human
Translation (MAHT) techniques, in particular
translation memories, for translating and
postediting multilingual documents.
? add UNL tags at sentence level to store the
translations as well as UNL hypergraphs
(anglosemantic interlingual representations),
from which raw (or rough!) translations into
other languages can be obtained from distant
"deconversion" servers.
Here, we are not concerned with the problem of
aligning parallel monolingual documents, or
realigning them after they have been modified, a
frequent need in the case of leaflets and booklets.
(Assimi,2000) proposed a tool to handle the non-
centralized management of the evolution of
multilingual parallel documents. We consider the
case, frequent in the industry, where documents are
managed centrally, even if they are distributed on
several sites. What happens in general is that they
are aligned at the level of large blocks, with one
file per block and language (fileXXX.en.htm,
fileXXX.fr.htm etc. for HTML pages).
What we propose is to align them at the level of
sentences, but of course not to have one file per
sentence. Rather, if there are N languages, for a
given "block" corresponding to some unit of
processing (e.g. visualization), we will have either
N monolingual sentence-aligned files, or 1
multilingual file. In both cases, sentences or place
holders for sentences will be linked to a MPM to
manage translation and postedition.
We began to build PolyphraZ in the context of
the TraCorpEx project (Translation of Corpora of
Examples). A more recent motivation is to extend
the BTEC corpus of CSTAR III (163000 sentences
in tourism) to French and Arabic, and to evaluate
various Chinese-English MT systems on it.
We will first present the data we start with, and
our goals in more detail. In a second part, we will
describe the architecture of PolyphraZ, starting
from scenarios of use and types of users. Lastly,
we will describe the current status of this work.
2 TraCorpEx and PolyphraZ
2.1 Context
The TraCorpEx project has several contexts: the
Papillon project (Papillon) of co-operative
construction of a large multilingual lexical base on
the Web, the C-STAR III project (C-STAR III) of
translation of spoken dialogues, a French and
Tunisian project (Hajlaoui, Boitet, 2003b), the
UNL project (UNL) of communication and
multilingual information system, and the PhD
research of the various participants in this project.
2.2 Current data and problems
We have initially 2 "parallel" corpora, structured
differently.
? The BTEC corpus of C-STAR is made of 5
sets of 163 files of 12K to 40K, each
containing 1000 sentences, in English,
Japanese (coded in EUC), Chinese and
Korean, for a total of 6.1 Mo per language.
? The TANAKA corpus (Japanese-English),
given to the Papillon project a few months
before the death of its author in 2002, is made
of 45 files for a total of 18.4 Mo. It contains
sentences of newspapers or teaching works of
NHK for the training of English by the
Japanese. Each file is bilingual.
We have also corpora from the UNL project,
where each document is a multilingual file
containing for each sentence its text in source
language, a UNL graph, the result of
deconversions in a certain number of languages,
and possibly their revisions, or direct manual
translations.
All these "parallel" corpora are aligned at the
level of sentences. As it would be interesting to
show correspondences at finer levels (syntagms,
chunks, words), we design PolyphraZ to later add
tools for subsential alignement such as the one
developed by Ch. Chenon for his Ph.D.
In other corpora, we may be obliged to go up to
the level of paragraphs, because sentences will not
be aligned perfectly. That will not be done
completely in PolyphraZ, but at the level of the
structure of the multilingual document itself: if 2
sentences are translated by 3, each of the 5
sentences will be in a different polyphrase, with
their individual translations, and there will be
another polyphrase, of "n-m" type, to contain the 2
complete segements.
The first problem we encounter with the
available parallel corpora it is that there is no tool
to visualize their contents at a glance, sentence by
sentence, nor to show the fine correspondences
between subsentential segments. In addition, in the
case of UNL documents, we cannot visualize at the
same time a sentences in several languages and its
corresponding UNL graph. Lastly, it is not possible
to see successive versions in parallel.
When it comes to evaluation, we can only see
the monolingual files, and associated statistical
measurements (NIST, BLEU...), but we can never
confront them with the real translations and make a
direct subjective evaluation.
2.3 Detailed objectives
The objectives of TraCorpEx project are as
follows.
2.3.1 Construction of a software platform
We want to build an environment, which
supports the import and the export of parallel
corpora, the preparation of the data for automatic
translators, the postedition (HAMT), the evaluation
(various feedbacks methods) and finally a
preparation of "feedbacks" to the developers of
used MT systems.
2.3.2 Addition of new languages
Starting from parallel corpora, we want to add
one or more languages (those of the Papillon
project for the Tanaka corpus, French and Arabic
for the BTEC corpus).
2.3.3 Evaluation of MT systems
 We also wish that the same platform makes it
possible to evaluate automatic translators with
automatic methods such as NIST, BLEU, PER, and
to use this possibility in CSTAR, to evaluate the
Chinese-English and Japanese-English translations.
To evaluate the results of various MT systems will
also enable us to determine "the best" (or less bad!)
translation, proposable to a contributor as a starting
point for revision.
We also want to test a hypothesis by the second
author: the quality of the translations could also be
evaluated using calculations of distances between
sentences and reverse translations.
2.3.4 Feedbacks to developers of MT systems
We also want to give feedbacks to the
developers of the systems used (unknown words,
badly translated sentences...), and a comparative
presentation between the various translation
systems.
 The whole of the objectives of this project led
us to propose interactive Web interfaces allowing
us to chooses, use, compare, publish machine
translations corresponding to several language
pairs, and to contribute to the improvement of the
results by sending feedbacks to the developers of
these systems.
2.4 The PolyphraZ platform
PolyphraZ is a software platform making it
possible at the same time to visualize the available
corpora on the Web by showing several languages,
with the choice of the user and to work on a basis
of "polyphrases" initialised from these corpora
while making it possible to control all functions
described above (call of MT systems, distance
computation,  collaborative postedit ion,
evaluation).
2.4.1 General architecture
We follow the software architecture of the
Papillon platform.
 We classify the objects to handle in three types
?  Raw corpus sources
? Sources transformed into our XML format
CXM. (Common Example Markup) and
coded in UTF-8, for visualization "just as
they are", then in CPXM format, DTD for
parallel visualization.
? MPM: multilingual polyphrase memory
Figure 1: objects of the PolyphraZ platform
2.4.2 Intended users of PolyphraZ
We distinguish four principal users: the preparer,
the reader ("normal" user), the posteditor and the
manager.
?  The preparer
His role consists in calling translation systems,
thereby parameterizing them as well as pos-
sible, which supposes a certain linguistic
ability (to compare the results of various
parameter settings, and of various segmenta-
tions in "blocks", each corresponding to some
parameter settings).
The preparer can also call objective evaluation
methods (NIST, BLEU...) on the results of
translation, tune with parameters to compute
distances between sentences (results of
translation and/or reverse translations), and
post the results. The distance computation
produces, in addition to a value, a XML string
from which a ?track changes? presentation can
be generated. The preparer can also set the
parameters determining "the best" suggestion
among the various translation candidates.
? The reader (normal user)
A reader can visualize the data (the original,
various translations, and distances between the
character strings) through Web interfaces, but
is not allowed to edit the translations.
?  The translator-posteditor
The translator-posteditor is a contributor who
translates from scratch or revises proposed
translations (MT results or translations of
similar sentences found in the MPM or in other
TM put in CPXM or MPM format). There is
an editable area to modify the active sentence.
One can also ask for global modifications (ex:
"SVP" changed into "s'il vous plait" in tran-
scribed spoken utterances) and correct or sup-
plement the local dictionary attached to the
MPM. The system uses the reference sentences
already produced like a translation memory.
PolyphraZ is thus also a system of assistance
to the translator, limited to the translation of
sets of sentences (or titles), with less function-
alities than commercial TWS, but usable for
collaborative volunteer work by non-
professionals.
? The manager
The last type of user is the manager, who will
produce from a MPM "feedbacks" for the de-
velopers of the MT systems used. A manager
cans himself be a developer of an MT system.
He can draw up a list of unknown words and
words badly translated by each system (pro-
duced from the traces of distance computa-
tions). A second function is to propose for
these words suggestions of translation from the
"reference" translations obtained after human
!
 
Corpus
=
Tanaka
Corpus
=
CSTAR
Corpus
=
BTEC-CH
Corpus
=
BTEC-EN
Raw corpus
sources
initial versions
Various formats
 Various codings
 Visualization on 
several documents 
CXM 
  (Commun   Example  
 Markup)
Single format XML
 coding = UTF-8
Parallel visualisation
CXM
MPM!:
 (Multilingual Polyphase
 Memory 
)
Correspondence
Versioning
LD : Local Dictionary
MPM
LD
Export
Import
External
 resources
Internet
CPXM
CPXM 
(Common Parallel Example 
   Markup)
Corpus
 =
 set of polyphrases
  
BTEC-JPN
Format= texte
Coding= EUC
BTEC-JPN
Format =XML
DTD=CXM
Coding= UTF-8
revision. Finally, it is possibile to provide a
presentation of the evaluations and compari-
sons between the results of the various systems
used and/or their various parameter settings.
2.4.3 Implementation of PolyphraZ
Programmed in standard Java under the Enhydra
development environment used for the dynamic
and multilingual Papillon web site, PolyphraZ is
multi-platform (MacOS-X/Unix/Linux, Windows).
2.5 Scenarios
The use of  PolyphraZ can be divided in 3 parts:
setting of the data under three different formats
(CXM, CPXM, MPM).
Figure 2 : scenarios for using PolyphraZ
2.5.1 CXM (Common eXample Markup)
In order to manipulate a single format (XML)
and a single encoding (UTF-8), we automatically
convert into the CXM format the imported data
(corpus, text aligned...). CDM is defined in the
same spirit as the CDM (Common Dictionnary
Markup) of the Papillon project.
Figure 3: example XML file conforming to the
CXM.dtd
2.5.2 CPXM.dtd (Common Parallel eXample
Markup)
A second Java program transforms all CXM files
corresponding to a given multilingual parallel
corpus of sentences to the CPXM format (see
appendix 2). In this format, we introduce the
"polyphrase" XML element, which is a set of
monolingual components, each containing possibly
one or more proposals.
2.5.3 MPM.dtd (Multilingual Polyphrase
Memory)
The MPM data structure is under construction. It
is intended for the management of the
correspondences between the various linguistic
versions as well as the modifications which can be
made, and to keep the history of the modified files.
As shown in the following figure, a MPM of
PolyphraZ can contain a set of versions and
alternatives of the sentences, as well as the results
of various computations.
Figure 4 : logical view of a MPM
We give a first version of the MPM DTD in
appendix 3.
2.5.4 Parallel visualization
PolyphraZ can visualize polyphrases in parallel
from corpora in CPXM or MPM formats. This
functionality is useful to compare translations, and
is made available to readers; translators revisors,
and managers.
Initialisation 
Of LD
On the Web
local (server)
preparer
Contributors
 (translator or
posteditor)
Visitors 
Manager (on local or on
the Web)
ftp
Initial 
document
 
CXM
Recovery
Basic tools 
TextEdit, BBEdit
Tools ?
Methods ?
multilinguals 
corpora in 
CXM
CPXM
 
Corpora
 in
CPXM
LD(Local
Dictionary)
Initialisation 
of the MPM
monolinguals 
corpora in 
CXM
Papillon
Parallel Visualisation of data
MPM (Multilingual
Polyphrase Memory )
Distance
calculation
Translation
Automatic
evaluation 
2.2
1.3
1.2
2.1
1.1
Proposal
2
11.11
2.2
1.3
1.2
2.12
Proposal N?of
 version
N?of
version
XL1,L2(1.1,1.1)
XL1,L2(1.2,1.2)
??..
D
L1
(1.1,1.2)
D
L2
(1.1,1.3)
???.
VersionVersionVersion
CorespondancesDistance
calculation
?
?
.
Language 3
(L3)
Language 2
(L2)
Language 1
(L1)
Hierarchy 1
H
i
e
r
a
r
c
h
y
 
 
2
Figure 5: parallel visualisation of the BTEC (extract)
2.6 Evaluation of translation results
 We have programmed and integrad in
PolyphraZ three evaluation methods (NIST, BLEU
and distance calculation). NIST and BLEU are
well known. Let us give more details about
distance calculation between 2 sentences.
The distance we compute between two strings is
a linear combination of two edit distances, one at
the level of characters, the other at the level of
words. In general, the edit distance between two
strings P1 and P2 of atoms (characters or words
here) is the minimal number of suppressions,
insertions or replacements of atoms necessary to
transform P1 into P2 or, equivalently, P2 into P1.
To compute the edit distance between P1 and P2 at
the level of words, one segments them into words,
computes the character distances between words of
P1 and words of P2, and then computes the word
distance using words as "large characters".
We use the well-known dynamic programming
algorithm of (Wagner, Fischer, 1974). To combine
the two levels (characters and words), we use the
formula:
D = (aD
char
 +bD
word
)/(a+b)   ; a +b=1
Figure 6:trace of the Wagner and Fischer
algorithm
2.6.1 ?Track changes? visualisation
This representation corresponds to the
presentation used by Microsoft Word in
"Track changes" mode. It is very readable. In
certain cases, the representation at the level of
the characters is more compact and readable
that at the level of words, while it is the
opposite in other cases. In fact, this
representation is not "faithful" to the trace,
because a sequence of exchanges is
transformed into a sequence of suppressions
and a sequence of insertions.
aisympablthique
Figure 7:?Track changes? display
One interesting and today unsolved problem is
how to merge the 2 levels: given 2 sentences and
their character and word edit distances, necessarily
both minimal, how to produce a trace which would
be "the best" or "a best" combination of the 2
traces?
2.6.2 Representation with 3 lines
Figure 8 : 3 lines representation
This representation is simpler to understand, but
takes more space.
?  represents the exchange of a character by
another,
 || represents the equality between two characters
 ? represent the suppression of the 1st character,
Figure 9 : XML representation
3 Conclusion
The CXM and CPXM levels of PolyphraZ are
already used. They have allow us to import the
BTEC multilingual corpus of parallel sentences
(into the common CPM format), to transform it
(163000 sentenes in 5 languages) into files in
CPXM formats, and to visualize it
1
 on the web.
The Tanaka corpus should be available when this
paper will be presented. The "inner" level of MPM
(Multilingual Polyphrase Memory) is almost
completed. It will also support versioning.
In the future, we plan to use MPMs not only to
handle multilingual corpora of parallel sentences,
but also like "pivots", to establish the sentence-
level correspondence between parallel monolingual
structured documents. If no high quality TWS (like
Trados, TM2, D?j? Vu; Transit, etc.) is available,
PolyphraZ could be used as a "bare bone" TWS,
directly through the web, in the Montaigne
2
 spirit.
We are also studying how to integrate into a
MPM structure "generators" specifying classes of
sentences (automata for messages with variables
and variants, regular expressions for CSTAR IF
expressions, etc.), and to use them to extend a
MPM not only "in width" (addition of new
languages), but also "in height", by the automatic
creation of new "statements", natural and/or
formal.
R?f?rences
A-B.Assimi (Assimi,2000). Gestion de l??volution
non centralis?e de documents parall?les
multilingues, Nouvelle th?se, UJF, Grenoble,
31/10/00, 200!p.
A-B.Assimi & C.Boitet (Assimi&Boitet,2001)
Management of Non-Centralized Evolution of
Parallel Multilingual Documents. P r o c .
Internationalization Track, 10th International
World Wide Web Conference, Hong Kong, May
1-5, 2001, 7 p.
Ch.Boitet (Boitet, 2003) Approaches to enlarge
bilingual corpora of example sentences to more
languages,  Papillon-03 seminar, Sapporo, 3-5
July 2003 12!p.
Ch .Boitet & Tsai W.-J (Boitet & W-J 2002).
Coedition to share text revision across
languages. Proc. COLING-02 WS on MT,
Taipeh, 1/9/2002, 8 p.
                                                       
1
  The full corpus is only accessible to members of
CSTAR-III, so that we show only extracts
corresponding to parts which are or will be published
for the open evaluation of various MT systems to be
presented at IWSLT-04.
2
 Mutualization Of Nomadic Translation Aids
for Groups on the NEt (Mutualisation d'Outils
Nomades de Traduction avec Aides Informatiques
pour des Groupes sur le NEt).
H.Vo-trung (Vo-trung, 2004) R?utilisation de
traducteurs gratuits pour d?velopper des
syst?mes multilingues, accept? ? la conf?rence
RECITAL 2004, avril 2004, F?s, Maroc.
N.Hajlaoui, Ch .Boitet (Hajlaoui, Boitet, 2003a), A
"pivot" XML-based architecture for multilingual,
multiversion documents!: parallel monolingual
documents aligned through a central
correspondence descriptor and possible use of
U N L , Convergences?03, Alexandria, 2-6
December 2003.
N.Hajlaoui, Ch.Boitet (Hajlaoui, Boitet, 2003b),
Mod?lisation de la production de phrases,
projet franco-tunisien entre l??quipe GETA,
CLIPS, UJF, Grenoble et universit? de Sousse,
Tunisie, 25 p.
N.Hajlaoui (2002) Gestion des versions des
composants ?lectroniques virtuels. Rapport de
DEA, CSI, INPG, juin 2002, 80 p.
R.Wagner & Michael.Fischer (Wagner, Fischer
,1974)  The String-to-String Correction Problem
ACM Journal of the Association for Computing
Machinery, Vol. 21, No 1, Janvier 1974.
W.-J.Tsai (Tsai,2001) SWIIVRE a web site for the
Initiation, Information, Validation, Research and
Experimentation on UNL. Proc. First UNL Open
Conference - Building Global Knowledge with
UNL, Suzhou, China, 18-20 Nov. 2001, 8 p.
(C-STAR-III) C-STAR project, http://www.c-
star.org/
(Papillon) Projet PAPILLON de construction
coop?rative d'une base lexicale multilingue et de
c o n s t r u c t i o n  d e  d i c t i o n n a i r e s ,
http://www.papillon-dictionary.org/
(TraCorpEx) projet TraCorpEx
http://www-
clips.imag.fr/geta/User/najeh.hajlaoui/tracorpex/i
ndex.html
(UNL) Universal Networking Langage (UNL)
project, http://www.undl.org/
Appendices
<!-- CXM.dtd  (Common eXample Markup ) is a
DTD  which  describes the corpora
(multilingual or monolingual), it is the
simplest format for imported data.
$Author:  Najeh Hajlaoui
najeh.hajlaoui@imag.fr
$Date: 2003/12/10 01:28:30 $ -->
<!ELEMENT document (information, sentence*) >
<!ELEMENT information (#PCDATA) >
<!ATTLIST information document-name  CDATA
#REQUIRED>
<!ATTLIST information   creation-date
CDATA #IMPLIED>
<!ATTLIST information modification-date
CDATA #IMPLIED>
<!ATTLIST information   coding-set    CDATA
#IMPLIED>
<!ATTLIST information number-of-languages
CDATA     #IMPLIED>
<!ATTLIST information   number-of-sentences
CDATA #IMPLIED>
<!ATTLIST sentence   sentence-id    CDATA
#REQUIRED>
<!ATTLIST sentence xml:lang CDATA #REQUIRED>
<!ELEMENT sentence (segment*) >
<!ATTLIST segment   segment-id    CDATA
#REQUIRED>
<!ELEMENT segment (#PCDATA) >
<!-- Document is a set of sentences, each
sentence is defined
by an identifier called sentence-id and also
by an attribute which indicates the
language -->
<!-- number-of-languages is the total number
of languages constituting the document; if
the document is monolingual, number-of-
languages =1   -->
<!-- number-of-sentences is the total number
of sentences constituting the document -->
<!-- Each sentence is a set of one or more
possible  segment; each segment is
identified by an attribute called  segment-
id -->
Appendix 1 : CXM.dtd (Common eXample
Markup)
<!-- CPXM.dtd  (Common Parallel eXample
Markup ) is a  DTD  which  describes the
multilingual documents (m languages),
multiversions (n versions) (n>m), it
allows the description of a collection of
polyphrases in a single format and
encoding.
$Author:  Najeh Hajlaoui
najeh.hajlaoui@imag.fr
$Date: 2003/06/10 01:28:30 $ -->
<!ELEMENT document (information,
polyphrase*) >
<!ELEMENT information (#PCDATA) >
<!ATTLIST information document-name  CDATA
#REQUIRED>
<!ATTLIST information   creation-date
CDATA #IMPLIED>
<!ATTLIST information modification-date
CDATA #IMPLIED>
<!ATTLIST information   coding-set
CDATA #IMPLIED>
<!ATTLIST information number-of-languages
CDATA                 #IMPLIED>
<!ATTLIST information number-of-
polyphrases CDATA #IMPLIED>
<!ELEMENT polyphrase (monolingual-
component*) >
<!ATTLIST polyphrase   polyphrase-id
CDATA #REQUIRED>
<!ELEMENT  monolingual-component
(segment*) >
<!ATTLIST monolingual-component xml:lang
CDATA #REQUIRED>
<!ELEMENT  segment (proposal) >
<!ATTLIST proposal   proposal-id    CDATA
#REQUIRED>
<!ELEMENT  proposal (#PCDATA) >
<!-- number-of-languages is the total
number of languages appearing in the
document; if the document is monolingual,
number-of-languages =1   -->
<!-- number-of-polyphrases is the total
number of polyphrases constituting the
document -->
<!-- A polyphrase is a set of monolingual
components, each containing 1 or more
possible proposals. Every polyphrase is
identified by a number called polyphrase-
id -->
<!-- Each monolingual component is a set
of one or more possible renderings of the
segment in question; it is identified by
an attribute which indicates the language
-->
<!-- Segment represents the level of
alignment, it is usually a sentence -->
Appendix 2 : CPXM.dtd (Common Parallel
eXample Markup)
<!-- MPM.dtd  (Multilingual Polyphrases
Memory ) is a  DTD  which  allows the
generation of sentences aligned in
several languages and the management of
the correspondence between these
sentences.
$Author:  Najeh Hajlaoui
najeh.hajlaoui@imag.fr
$Date: 2003/01/28 21:28:30 $ -->
<!ELEMENT document (information,
generator*, node-of-correspondence*) >
<!ELEMENT information (#PCDATA) >
<!ATTLIST information document-name
CDATA #REQUIRED>
<!ATTLIST information   creation-date
CDATA #IMPLIED>
<!ATTLIST information modification-date
CDATA #IMPLIED>
<!ATTLIST information   coding-set
CDATA #IMPLIED>
<!ATTLIST information number-of-languages
CDATA #IMPLIED>
<!ATTLIST information number-of-generator
CDATA #IMPLIED>
<!ELEMENT generator (instance*) >
<!ATTLIST generator   original    CDATA
#REQUIRED>
<!ATTLIST generator   context    CDATA
#REQUIRED>
<!ELEMENT  instance (segment*) >
<!ATTLIST instance xml:lang CDATA
#REQUIRED>
<!ATTLIST segment node-of-corespondance-
id CDATA #REQUIRED>
<!ELEMENT  segment (proposal) >
<!ELEMENT  proposal (#PCDATA) >
<!-- number-of-languages is the total
number of languages appearing in the
document; if the document is
monolingual, number-of-languages = 1 -->
<!-- number-of-generator is the total
number of generator appearing in the
document -->
<!-- A generator is a set of original
sentences and  their instance  -->
<!-- A instance is a set of one or more
possible renderings of the segment in
question; it is identified by an
attribute which indicates the language
-->
<!-- Segment represents the level of
alignment, it is usually a sentence -->
<!-- A node-of-correspondence-id
represents the link of corespondance
between the dif?rents proposals of
translation -->
Appendix 3 : MPM.dtd (Multilingual Polyphrase
Memory)
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 537?544
Manchester, August 2008
Hindi Urdu Machine Transliteration using Finite-state Transducers 
M G Abbas Malik   Christian Boitet 
GTALP, Laboratoire d?Informatique Grenoble 
Universit? Joseph Fourier, France 
abbas.malik@imag.fr, 
Christian.Boitet@imag.fr 
Pushpak Bhattacharyya 
Dept. of Computer Science and Engineering, 
IIT Bombay, India 
pb@cse.iitb.ac.in 
 
Abstract 
Finite-state Transducers (FST) can be 
very efficient to implement inter-dialectal 
transliteration. We illustrate this on the 
Hindi and Urdu language pair. FSTs can 
also be used for translation between sur-
face-close languages. We introduce UIT 
(universal intermediate transcription) for 
the same pair on the basis of their com-
mon phonetic repository in such a way 
that it can be extended to other languages 
like Arabic, Chinese, English, French, etc. 
We describe a transliteration model based 
on FST and UIT, and evaluate it on Hindi 
and Urdu corpora. 
1 Introduction 
Transliteration is mainly used to transcribe a 
word written in one language in the writing sys-
tem of the other language, thereby keeping an 
approximate phonetic equivalence. It is useful for 
MT (to create possible equivalents of unknown 
words) (Knight and Stall, 1998; Paola and San-
jeev, 2003), cross-lingual information retrieval 
(Pirkola et al 2003), the development of multi-
lingual resources (Yan et al 2003) and multilin-
gual text and speech processing. Inter-dialectal 
translation without lexical changes is quite useful 
and sometimes even necessary when the dialects 
in question use different scripts; it can be 
achieved by transliteration alone. That is the case 
of HUMT (Hindi-Urdu Machine Transliteration) 
where each word has to be transliterated from 
Hindi to Urdu and vice versa, irrespective of its 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
type (noun, verb, etc. and not only proper noun 
or unknown word). 
?One man?s Hindi is another man?s Urdu? 
(Rai, 2000). The major difference between Hindi 
and Urdu is that the former is written in Devana-
gari script with a more Sanskritized vocabulary 
and the latter is written in Urdu script (derivation 
of Persio-Arabic script) with more vocabulary 
borrowed from Persian and Arabic. In contrast to 
the transcriptional difference, Hindi and Urdu 
share grammar, morphology, a huge vocabulary, 
history, classical literature, cultural heritage, etc. 
Hindi is the National language of India with 366 
million native speakers. Urdu is the National and 
one of the state languages of Pakistan and India 
respectively with 60 million native speakers 
(Rahman, 2004). Table 1 gives an idea about the 
size of Hindi and Urdu. 
 Native Speakers 
2nd Language 
Speakers Total 
Hindi 366,000,000 487,000,000 853,000,000 
Urdu 60,290,000 104,000,000 164,290,000 
Total 426,290,000 591,000,000 1,017,000,000 
Table 1: Hindi and Urdu speakers 
Hindi and Urdu, being varieties of the same 
language, cover a huge proportion of world?s 
population. People from Hindi and Urdu com-
munities can understand the verbal expressions 
of each other but not the written expressions. 
HUMT is an effort to bridge this scriptural divide 
between India and Pakistan. 
Hindi and Urdu scripts are briefly introduced 
in section 2. Universal Intermediate Transcrip-
tion (UIT) is described in section 3, and UIT 
mappings for Hindi and Urdu are given in sec-
tion 4. Contextual HUMT rules are presented and 
discussed in section 5. An HUMT system im-
plementation and its evaluation are provided in 
section 6 and 7. Section 8 is on future work and 
conclusion. 
537
2 HUMT 
There exist three languages at the border between 
India and Pakistan: Kashmiri, Punjabi and Sindhi. 
All of them are mainly written in two scripts, one 
being a derivation of the Persio-Arabic script and 
the other being Devanagari script. A person us-
ing the Persio-Arabic script cannot understand 
the Devanagari script and vice versa. The same is 
true for Hindi and Urdu which are varieties or 
dialects of the same language, called Hindustani 
by Platts (1909). 
PMT (Punjabi Machine Transliteration) (Ma-
lik, 2006) was a first effort to bridge this scrip-
tural divide between the two scripts of Punjabi 
namely Shahmukhi (a derivation of Perio-Arabic 
script) and Gurmukhi (a derivation of Landa, 
Shardha and Takri, old Indian scripts). HUMT is 
a logical extension of PMT. Our HUMT system 
is generic and flexible such that it will be extend-
able to handle similar cases like Kashmiri, Pun-
jabi, Sindhi, etc. HUMT is also a special type of 
machine transliteration like PMT. 
A brief account of Hindi and Urdu is first giv-
en for unacquainted readers. 
2.1 Hindi 
The Devanagari (literally ?godly urban?) script, a 
simplified version of the alphabet used for San-
skrit, is a left-to-right script. Each consonant 
symbol inherits by default the vowel sound [?]. 
Two or more consonants may be combined to-
gether to form a cluster called Conjunct that 
marks the absence of the inherited vowel [?] be-
tween two consonants (Kellogg, 1872; Montaut, 
2004). A sentence illustrating Devanagari is giv-
en below: 
?????? ?????????? ?? ???? ????? ??. 
[h?n?i h?n?ust?n ki q?mi zub?n h?] 
(Hindi is the national language of India) 
2.2 Urdu 
Urdu is written in an alphabet derived from the 
Persio-Arabic alphabet. It is a right-to-left script 
and the shape assumed by a character in a word 
is context-sensitive, i.e. the shape of a character 
is different depending on whether its position is 
at the beginning, in the middle or at the end of a 
word (Zia, 1999). A sentence illustrating Urdu is 
given below: 
?? y6?36 G?6[  zEegEZ F? ?X? y6[ Ei ??? 
[?r?u p?k?st?n ki q?mi zub?n h?] 
(Urdu is the National Language of Pakistan.) 
3 Universal Intermediate Transcription 
UIT (Universal Intermediate Transcription) is a 
scheme to transcribe texts in Hindi, Urdu, Punja-
bi, etc. in an unambiguous way encoded in AS-
CII range 32 ? 126, since a text in this range is 
portable across computers and operating systems 
(James 1993; Wells, 1995). SAMPA (Speech 
Assessment Methods Phonetic Alphabet) is a 
widely accepted scheme for encoding the IPA 
(International Phonetic Alphabet) into ASCII. It 
was first developed for Danish, Dutch, French, 
German and Italian, and since then it has been 
extended to many languages like Arabic, Czech, 
English, Greek, Hebrew, Portuguese, Russian, 
Spanish, Swedish, Thai, Turkish, etc. 
We define UIT as a logical extension of 
SAMPA. The UIT encoding for Hindi and Urdu 
is developed on the basis of rules and principles 
of SAMPA and X-SAMPA (Wells, 1995), that 
cover all symbols on the IPA chart. Phonemes 
are the most appropriate invariants to mediate 
between the scripts of Hindi, Punjabi, Urdu, etc., 
so that the encoding choice is logical and suitable. 
4 Analysis of Scripts and UIT Mappings 
For the analysis and comparison, scripts of Hindi 
and Urdu are divided into different groups on the 
basis of character types. 
4.1 Consonants 
These are grouped into two categories: 
Aspirated Consonants: Hindi and Urdu both 
have 15 aspirated consonants. In Hindi, 11 aspi-
rated consonants are represented by separate cha-
racters e.g. ? [k?], ? [b?], etc. The remaining 4 
consonants are represented by combining a sim-
ple consonant to be aspirated and the conjunct 
form of HA ?[h], e.g. ? [l] + ?? + ? [h] = ??? [l?]. 
In Urdu, all aspirated consonants are 
represented by a combination of a simple conso-
nant to be aspirated and Heh Doachashmee (?) 
[h], e.g. ? [k] + ? [h] = ?? [k?], ? [b] + ? [h] = ?? 
[b?],  ? [l] + ? [h] = ?? [l?], etc.  
The UIT mapping for aspirated consonants is 
given in Table 2. 
Hindi Urdu UIT Hindi Urdu UIT 
? ?? [b?] b_h ??? ?? [r?] r_h 
? ?? [p?] p_h ? ?? [??] r`_h 
? ?? [??] t_d_h ? ?? [k?] k_h 
? ?? [??] t`_h ? ?? [g?] g_h 
? ?? [??] d_Z_h ??? ?? [l?] l_h 
538
? ?? [??] t_S_h ??? ?? [m?] m_h 
? ?? [??] d_d_h ??? ?? [n?] n_h 
? ?? [??] d`_h    
Table 2: Hindi Urdu aspirated consonants 
Non-aspirated Consonants: Hindi has 29 
non-aspirated consonant symbols representing 28 
consonant sounds as both SHA (?) and SSA (?) 
represent the same sound [?]. Similarly Urdu has 
35 consonant symbols representing 27 sounds as 
multiple characters are used to represent the 
same sound e.g. Heh (?) and Heh-Goal (?) 
represent the sound [h] and Theh (?), Seen (?) 
and Sad (?) represent the sound [s], etc. 
UIT mapping for non-aspirated consonants is 
given in Table 3. 
Hindi Urdu UIT Hindi Urdu UIT 
? ? [b] b ? ? [s] s2 
? ? [p] p ? ? [z] z2 
? ? [?] t_d ? ? [?] t_d1 
? ? [?] t` ? ? [z] z3 
? ? [s] s1 - ? [?] ? 
? ? [?] d_Z ? ? [?] X 
? ? [?] t_S ? ? [f] f 
? ? [h] h1 ? ? [q] q 
? ? [x] x ? ? [k] k 
? ? [?] d_d ? ? [g] g 
? ? [?] d` ? ? [l] l 
? ? [z] z1 ? ? [m] m 
? ? [r] r ? ? [n] n 
? ? [?] r` ? ? [v] v 
? ? [z] z ? ? [h] h 
? ? [?] Z ? ? [j] j 
? ? [s] s ? ? [?] t_d2 
? ? [?] S ? - [?] n` 
? ? [?] S1 ? ? ? [?] ~ 
Table 3: Hindi Urdu non-aspirated consonants 
4.2 Vowels 
Hindi has 11 vowels and 10 of them have nasa-
lized forms. They are represented by 11 indepen-
dent vowel symbols e.g. ? [?], ? [u], ? [?], 
etc. and 10 dependent vowel symbols e.g. ?? 
[?], ?? [u], ?? [?], etc. called maatraas. When a 
vowel comes at the start of a word or a syllable, 
the independent form is used; otherwise the de-
pendent form is used (Kellogg, 1872; Montaut, 
2004). 
Urdu contains 10 vowels and 7 of them have 
nasalized forms (Hussain, 2004; Khan, 1997). 
Urdu vowels are represented using four long vo-
wels (Alef Madda (?), Alef (?), Vav (?) and Choti 
Yeh (?)) and three short vowels (Arabic Fatha ? 
Zabar -?, Arabic Damma ? Pesh -? and Arabic Ka-
sra ? Zer -?). Vowel representation is context-
sensitive in Urdu. Vav (?) and Choti Yeh (?) are 
also used as consonants. 
Hamza (?) is a place holder between two suc-
cessive vowel sounds, e.g. in ?????  [k?m?i] 
(earning), Hamza (?) separates the two vowel 
sounds Alef (?) [?] and Choti Yeh (?) [i]. Noon-
ghunna (?) is used as nasalization marker. Anal-
ysis and mapping of Hindi Urdu vowels is given 
in Table 5. 
4.3 Diacritical Marks 
Urdu contains 15 diacritical marks. They 
represent vowel sounds, except Hamza-e-Izafat  -? 
and Kasr-e-Izafat -? that are used to build com-
pound words, e.g. ??????? ????? [???r?h?s??ns] (In-
stitute of Science), ??????? ?????? [t?rix?ped???] 
(date of birth), etc. Shadda -? is used to geminate 
a consonant e.g. ??? [r?bb] (God), ????? [?????] 
(good), etc. Jazm  -? is used to mark the absence of 
a vowel after the base consonant (Platts, 1909). 
In Hindi, the conjunct form is used to geminate a 
consonant. Urdu diacritical marks mapping is 
given in Table 4. 
Hindi Urdu UIT Hindi Urdu UIT 
- F? [?] @ ?? G? [?] A 
?? G? [?] I ? F? [?n] @n 
?? E? [?] U ??? E? [?n] Un 
?? E? [u] u ??? F? [?n] In 
?? G? [i] i    
Table 4: Diacritical Marks of Urdu 
Diacritical marks are present in Urdu but spa-
ringly used by people. They are very important 
for the correct pronunciation and understanding 
the meanings of a word. For example, 
 ??? ?????? ??? ???  
[je s???k b?h?? ???i h?] (This is a wide road.) 
 ??? ??? ???????? 
[meri ?u?i s?r?x h?] (My bangle is red.) 
In the first sentence, the word ???? is pro-
nounced as [???i] (wide) and in the second, it is 
539
pronounced as [?u?i] (bangle). There should be 
Zabar (??) and Pesh (??) after Cheh (?) in above 
words and correct transcriptions are ????? (wide) 
and ????? (bangle). Thus diacritical marks are 
essential for removing ambiguities, natural lan-
guage processing and speech synthesis. 
 
Vowel Urdu Hindi (UIT) 
? It is represented by Alef (?) + Zabar -? at the start of a word e.g. ??? [?b] (now) and by Zabar -? in the middle of a word respectively e.g. ???? [r?bb] (God). It never comes at the end of a word. ? (@) 
? 
It is represented by Alef Madda (?) at the start of a word e.g. ???? [?d?mi] (man) and by Alef (?) or Alef 
Madda (?) in the middle of a word e.g. ???? [??n?] (go), ?????? [b?l?x?r] (at last). At the end of a word, it is 
represented by Alef (?). In some Arabic loan words, it is represented by Choti Yeh (?) + Khari Zabar ?- at 
the end of a word e.g. ????? [??l?] (Superior) and by Khari Zabar ?- in the middle of a word e.g. ????? [?l?hi] 
(God). 
? or ?? (A) 
e 
It is represented by Alef (?) + Choti Yeh (?) at the start of a word e.g. ????? [es?r] (sacrifice), ??? [ek] (one), 
etc. and by Choti Yeh (?) or Baree Yeh (?) in the middle of a word e.g. ???? [mer?] (mine), ??????? 
[?nd??er?] (darkness), ????? [beg??r] (homeless) etc. At the end of a word, It is represented by Baree Yeh 
(?) e.g. ???? [s?re] (all). 
? or ?? (e) 
? 
It is represented by Alef (?) + Zabar -? + Choti Yeh (?) at the start of a word e.g. ????? [?h] (this) and by Zabar 
-? + Choti Yeh (?) in the middle of a word e.g. ???? [m?l] (dirt). At the end of a word, it is represented by 
Zabar -? + Baree Yeh (?) e.g. ??? [h?] (is). 
? or ?? ({) 
? 
It is represented by Alef (?) + Zer -? at the start of a word e.g. ??? [?s] (this) and by Zer -? in the middle of a 
word e.g. ????? [b?r??] (rain). It never comes at the end of a word. At the end of a word, it is used as Kasr-e-
Izafat to build compound words. 
? or ?? (I) 
i 
It is represented by Alef (?) + Zer -? + Choti Yeh (?) at the start of a word e.g. ?????? [im?n] (belief) and by 
Zer -? + Choti Yeh (?) in the middle or at the end of a word e.g. ?????? [?miri] (richness), ????? [q?rib] (near), 
etc. 
? or ?? (i) 
? 
It is represented by Alef (?) + Pesh -? at the start of a word e.g. ?????? [?d?d???r] (there) and by Pesh -? in the 
middle of a word e.g. ???? [m?ll] (price). It never comes at the end of a word. 
? or ?? (U) 
u 
It is represented by Alef (?) + Pesh -? + Vav (?) at the start of a word e.g. ???????? [?g??t??] (dozzing) and by 
Pesh -? + Vav (?) in the middle or at the end of a word e.g. ????? [sur?t ?] (face), ?????? [t??r?zu] (physical bal-
ance), etc. 
? or ?? (u) 
o It is represented by Alef (?) + Vav (?) at the start of a word e.g. ????? [o???] (nasty) and by Vav (?) in the 
middle or at the end of a word e.g. ???? [holi] (slowly), ??? [k?ho] (say), etc. 
? or ?? (o) 
? 
It is represented by Alef (?) + Zabar -? + Vav (?) at the start of a word e.g. ???? [??] (hindrance) and by Zabar -? 
+ Vav (?) in the middle or at the end of a word e.g. ???? [m?t ?] (death). 
? or ?? (O) 
r ? 
It is represented by a consonant symbol Reh (?) [r] as this vowel is only present in Sanskrit loan words. It is 
almost not used in modern standard Hindi. It is not present in Urdu as it is used only in Sanskrit loan words. ? or ?? (r1) 
Note: In Hindi, Nasalization of a vowel is done by adding Anunasik (??) or Anusavar (??) after the vowel. Anusavar (??) is used when 
the vowel graph goes over the upper line; otherwise Anunasik (??) is used (Kellogg, 1872; Montaut, 2004). In UIT, ~ is added at end of 
UIT encoding for nasalization of all above vowels except the last one that do not have a nasalized form. 
Table 5: Analysis and Mapping of Hindi Urdu Vowels 
5 HUMT Rules 
In this section, UIT mappings of Hindi Urdu al-
phabets and contextual rules that are necessary 
for Hindi-Urdu transliteration are discussed. 
5.1 UIT Mappings 
UIT mappings for Hindi and Urdu alphabets and 
their vowels are given in Table 2 ? 5. In Hindi, 
SHA (?) and SSA (?) both represent the sound 
[?] and have one equivalent symbol in Urdu, i.e. 
Sheen (?). To make distinction between SHA 
(?) and SSA (?) in UIT, they are mapped on S 
and S1 respectively. Similarly in Urdu, Seh (?), 
Seen (?) and Sad (?) represent the sound [s] 
and have one equivalent symbol in Hindi, i.e. SA 
(?). To make distinction among them in UIT, 
they are mapped on s1, s and s2 respectively. All 
similar cases are shown in Table 6. 
IPA Urdu (UIT) Hindi (UIT) 
? ? (t_d), ? (t_d1), ? (t_d2) ? (t_d) 
s ? (s1), ? (s), ? (s2) ? (s) 
H ? (h1), ? (h) ? (h) 
540
z ? (z1), ? (z), ? (Z), ? (z2), ? (z3) ? (z) 
? ? (S) ? (S), ? (S1) 
r ? (r) ? (r), ? (r1) 
Table 6: Multiple Characters for one IPA 
Multi-equivalences are problematic for Hindi-
Urdu transliteration. 
UIT is extendable to other languages like Eng-
lish, French, Kashmiri, Punjabi, Sindhi, etc. For 
example, Punjabi has one extra character than 
Urdu i.e. Rnoon [?] (?), it is mapped on ?n`? in 
UIT. Similarly, UIT, a phonetic encoding 
scheme, can be extended to other languages. 
All these mappings can be implemented by 
simple finite-state transducers using XEROX?s 
XFST (Beesley and Karttunen, 2003) language. 
A sample XFST code is given in Figure 1. 
read regex [? -> b, ? -> p, ? -> [d ?_? Z] ]; 
read regex [[? ?] -> [d ?_? Z ?_? h]]; 
read regex [? -> v, ? -> j || .#. _ ]; 
read regex [? -> v, ? -> j || _ [? | ?]]; 
read regex [? -> e || CONSONANTS _ ]; 
read regex [ ? -> i || _ [ ?| .#.]]; 
? 
read regex [? -> b, ? -> p, ? -> z, ? -> [d ?_? Z ?_? h]]; 
read regex [? -> ?@?, ? -> A, ? -> i || .#. _ ] 
? 
Figure 1: Sample XFST code 
Finite-state transducers are robust and time 
and space efficient (Mohri, 1997). They are a 
logical choice for Hindi-Urdu transliteration via 
UIT as this problem could also be seen as string 
matching and producing an analysis string as an 
output like finite-state morphological analysis. 
5.2 Contextual HUMT Rules 
UIT mappings need to be accompanied by neces-
sary contextual HUMT rules for correct Hindi to 
Urdu transliteration and vice versa. 
For example, Vav (?) and Choti Yeh (?) are 
used to represent vowels like [o], [?], [i], [e], etc. 
but they are also used as consonants. Vav (?) and 
Choti Yeh (?) are consonants when they come at 
the beginning of a word or when they are fol-
lowed by Alef mada (?) or Alef (?). Also, Choti 
Yeh (?) represents the vowel [e] when it is pre-
ceded by a consonant but when it comes at the 
end of a word and is preceded by a consonant 
then it represents the vowel [i]. These rules are 
shown in red colour in Figure 1. 
Thus HUMT contextual rules are necessary for 
Hindi-Urdu transliteration and they can also be 
implemented as finite-state transducer using 
XFST. All these rules can?t be given here due to 
shortage of space. 
6 HUMT System 
The HUMT system exploits the simplicity, ro-
bustness, power and time and space efficiency of 
finite-state transducers. Exactly the same trans-
ducer that encodes a Hindi or Urdu text into UIT 
can be used in the reverse direction to generate 
Hindi or Urdu text from the UIT encoded text. 
This two-way power of the finite-state transducer 
(Mohri, 1997) has significantly reduced the 
amount of efforts to build the HUMT system. 
Another very important and powerful strength of 
finite-state transducers, they can be composed 
together to build a single transducer that can per-
form the same task that could be done with help 
of two or more transducers when applied sequen-
tially (Mohri, 1997), not only allows us to build a 
direct Hindi ? Urdu transducer, but also helps to 
divide difficult and complex problems into sim-
ple ones, and has indeed simplified the process of 
building the HUMT system. A direct Hindi ? 
Urdu transducer can be used in applications 
where UIT encoding is not necessary like Hindi-
Urdu MT system. 
The HUMT system can be extended to per-
form transliteration between two or more differ-
ent scripts used for the same languages like 
Kashmiri, Kazakh, Malay, Punjabi, Sindhi, etc. 
or between language pairs like English?Hindi, 
English?Urdu, English?French, etc. by just in-
troducing the respective transducers in the Fi-
nite-state Transducer Manager of 
the HUMT system to build a multilingual ma-
chine transliteration system. 
 
Figure 2: HUMT System 
In the HUMT system, Text Tokenizer 
takes the input Hindi or Urdu Unicode text, toke-
nizes it into Hindi or Urdu words and passes 
541
them to UIT Enconverter. The enconverter 
enconverts Hindi or Urdu words into UIT words 
using the appropriate transducer from Finite-
state Transducers Manager, e.g. for 
Hindi words, it uses the Hindi ? UIT transducer. 
It passes these UIT encoded words to UIT De-
converter, which deconverts them into Hindi 
or Urdu words using the appropriate transducer 
from Finite-state Transducers Man-
ager in reverse and generates the target Hindi 
or Urdu text. 
6.1 Enconversion of Hindi-Urdu to UIT 
Hindi ? UIT transducer is a composition of the 
mapping rules transducers and the contextual 
rules transducers. This is clearly shown in figure 
3 with a sample XFST code. 
clear stack 
set char-encoding UTF-8 
define CONSONANTS [? | ? | ? | ? | ? | ? | ?]; 
read regex [?? -> J, ?? -> h, ?? -> 0]; 
read regex [? -> k, ? -> [k ?_? h],  ? -> g, ? -> [g ?_? 
h],  ? -> [n ?@? g], ? -> [t ?_? S], ? -> [t ?_? S ?_? h]]; 
read regex [[? ?? ?] -> [k k]?, [? ?? ?] -> [k k ?_? h],  
[? ?? ?] -> [g g]?, [? ?? ?] -> [g g ?_? h]]; 
? 
read regex [[? ??] -> [k h], [?] -> [n A], [? ??] -> [j h], 
[? ??] -> [v h] || .#. _ .#.]; 
compose net 
Figure 3: Sample code for Hindi ? UIT Transducer 
How the HUMT system works is shown with 
the help of an example. Take the Hindi sentence: 
????? ????? ?? ??? ?? ????? ?? 
[f?x??? m?h?b?? ?r ?m?n k? n???n h?] 
(Dove is symbol of love and peace) 
This sentence is received by the Text To-
kenizer and is tokenized into Hindi words, 
which are enconverted into UIT words using the 
mapping and the contextual rules of Hindi ? 
UIT transducer by the UIT Enconverter. 
The Hindi Words and the UIT enconversions are 
given in Table 7. 
Hindi Words UIT 
????? [f?x???] fAx@t_dA 
????? [m?h?b??] mUh@b@t_d 
?? [?r] Or 
??? [?m?n] @m@n 
?? [k?] kA 
????? [n???n] nISAn 
?? [h?] H{ 
Table 7: Hindi Words with UIT 
6.2 Deconversion of UIT to Hindi-Urdu 
For the deconversion, Hindi ? UIT or Urdu ? 
UIT transducer is applied in reverse on the UIT 
enconverted words to generate Hindi or Urdu 
words. To continue with the example in the pre-
vious section, the UIT words are deconverted 
into the Urdu words by the UIT Deconver-
ter using Urdu ? UIT transducer in reverse. 
The Urdu words are given in table 8 with the 
Hindi and the UIT words. 
Hindi UIT Urdu 
????? [f?x???] fAx@t_dA ????? 
????? [m?h?b??] mUh@b@t_d ????? 
?? [?r] Or ???? 
??? [?m?n] @m@n ??? 
?? [k?] kA ?? 
????? [n???n] nISAn ????? 
?? [h?] H{ ??? 
Table 8: Hindi, UIT and Urdu Words 
Finally, the following Urdu sentence is gener-
ated from Urdu words. 
????? ????? ???? ??? ?? ????? ???  
Here the word ????? [f?x???] (Dove) is 
transliterated wrongly into ??????? because the 
vowel [?] at the end of some Urdu words (bor-
rowed from Persian language) is transcribed with 
help of Heh-gol [h] (?). This phenomenon is a 
problem for Hindi to Urdu transliteration but not 
for Urdu to Hindi transliteration. 
7 Evaluation Experiments and Results 
For evaluation purpose, we used a Hindi corpus, 
containing 374,150 words, and an Urdu corpus 
with 38,099 words. The Hindi corpus is extracted 
from the Hindi WordNet2 developed by the Re-
source Center for Indian Language Technology 
Solutions, CSE Department, Indian Institute of 
Technology (IIT) Bombay, India and from the 
project CIFLI (GETALP-LIG 3 , University Jo-
seph Fourier), a project for building resources 
and tools for network-based ?linguistic survival? 
communication between French, English and 
Indian languages like Hindi, Tamil, etc. The Ur-
du corpus was developed manually from a book 
titled ?????? ???? [z?lm?? k?d?]. The Hindi-Urdu 
corpus contains in total 412,249 words. 
The HUMT system is an initial step to build 
Urdu resources and add Urdu to the languages of 
                                                 
2 http://www.cfilt.iitb.ac.in 
3 http://www.liglab.fr 
542
SurviTra-CIFLI (Survival Translation) (Boitet et 
al, 2007), a multilingual digital phrase-book to 
help tourists for communication and enquiries 
like restaurant, hotel reservation, flight enquiry, 
etc. 
To reduce evaluation and testing efforts, 
unique words are extracted from the Hindi-Urdu 
corpus and are transliterated using the HUMT 
system. These unique words and their translitera-
tions are checked for accuracy with the help of 
dictionaries (Platts, 1911; Feroz). 
7.1 Urdu ? Hindi Transliteration Results 
While transliterating Urdu into Hindi, multiple 
problems occur like multi-equivalences, no equi-
valence, missing diacritical marks in Urdu text. 
For example, Sheen [?] (?) can be transliterated 
in Hindi into SHA [?] (?) or SSA [?] (?) that are 
present in 7,917 and 6,399 corpus words respec-
tively. Sheen [?] (?) is transliterated into SHA 
[?] (?) by default. Thus, 6,399 words containing 
SSA [?] (?) are wrongly transliterated into Hindi 
using HUMT. Urdu to Hindi multi-equivalences 
cases are given in Table 9 with their frequencies. 
Urdu Hindi (corpus Frequency) 
? [?] ? (7917), ? (6399) 
? [r] ? (79,345), ? (199) 
Table 9: Urdu ? Hindi Multi-equivalences 
Some Hindi characters do not have equivalent 
characters in Urdu, e.g. NNA [?] (?), retroflexed 
version of [n], has approximately mapped onto 
Noon [n] (?). This creates a problem when a 
word actually containing NNA [?] (?) is transli-
terated from Urdu to Hindi. No-equivalence cas-
es are given in Table 10. 
Urdu Hindi (corpus Frequency) 
- ? (4744) 
- ? (0) 
- ? (532) 
Table 10: Urdu ? Hindi No-equivalences 
Missing diacritical marks is the major problem 
when transliterating Urdu into Hindi. The impor-
tance of diacritical marks has already been ex-
plained in section 4.3. This work assumed that all 
necessary diacritical marks are present in Urdu 
text because they play a vital role in Urdu to 
Hindi transliterations. Results of Urdu to Hindi 
transliteration are given in Table 11. 
 Error Words Accuracy 
Corpus 11,874 97.12% 
Unique Words 123 98.54% 
Table 11: Urdu ? Hindi Transliteration Results 
7.2 Hindi ? Urdu Transliteration Results 
Hindi ? Urdu transliteration also have multi-
equivalences and no-equivalence problems that 
are given in Table 12. 
 
Hindi Urdu (corpus Frequency) 
? 1312) ? ,(41,751) ?) 
? 86) ? ,(751) ? ,(53,289) ?) 
? 1800) ? ,(72,850) ?) 
? 2) ? ,(215) ? ,(228) ? ,(1489) ? ,(2551) ?) 
- 2857) ?) 
Table 12: Hindi ? Urdu Multi & No equivalences 
Results of Hindi to Urdu transliteration are 
given in Table 13. 
 Error Words Accuracy 
Corpus 8,740 97.88% 
Unique Words 1400 83.41% 
Table 13: Hindi ? Urdu Transliteration Results 
Interestingly, Hindi to Urdu conversion is 
14.47% less accurate on the unique words as 
compared to its result on the corpus data that is a 
contrasting fact for the reverse conversion. 
The HUMT system gives 97.12% accuracy for 
Urdu to Hindi and 97.88% accuracy for Hindi to 
Urdu. Thus, the HUMT system works with 
97.50% accuracy. 
8 Future Implications 
Hindi-Urdu transliteration is one of the cases 
where one language is written in two or more 
mutually incomprehensible scripts like Kazakh, 
Kashmiri, Malay, Punjabi, Sindhi, etc. The 
HUMT system can be enhanced by extending 
UIT and introducing the respective finite-state 
transducers. It can similarly be enhanced to 
transliterate between language pairs, e.g. Eng-
lish-Arabic, English-Hindi, English-Urdu, 
French-Hindi, etc. Thus, it can be enhanced to 
build a multilingual machine transliteration sys-
tem that can be used for cross-scriptural transli-
teration and MT. 
We are intended to resolve the problems of 
multi-equivalences, no-equivalences and the 
most importantly the restoration of diacritical 
marks in Urdu text that are observed but left un-
attended in the current work. Restoration of dia-
critical marks in Urdu, Sindhi, Punjabi, Kashmi-
ri, etc. texts is essential for word sense disambig-
uation, natural language processing and speech 
synthesis of the said languages. 
The HUMT system will also provide a basis 
for the development of Inter-dialectal translation 
system and MT system for surface-close lan-
guages like Indonesian-Malay, Japanese-Korean, 
543
Hindi-Marathi, Hindi-Urdu, etc. Translation of 
the surface-close languages or inter-dialectal 
translation can be performed by using mainly 
transliteration and some lexical translations. 
Thus HUMT will also provide basis for Cross-
Scriptural Transliteration, Cross-scriptural In-
formation Retrieval, Cross-scriptural Applica-
tion Development, inter-dialectal translation and 
translation of surface-close languages. 
9 Conclusion 
Finite-state transducers are very efficient, robust, 
and simple to use. Their simplicity and powerful 
features are exploited in the HUMT model to 
perform Hindi-Urdu transliteration using UIT 
that is a generic and flexible encoding scheme to 
uniquely encode natural languages into ASCII. 
The HUMT system gives 97.50% accuracy when 
it is applied on the Hindi-Urdu corpora contain-
ing 412,249 words in total. It is an endeavor to 
bridge the scriptural, ethnical, cultural and geo-
graphical division between 1,017 millions people 
around the globe. 
Acknowledgement 
This study is partially supported by the project CIFLI 
funded under ARCUS-INDIA program by Ministry of 
Foreign Affairs and Rh?ne-Alpes region. 
References 
Beesley, Kenneth R. and Karttunen, Lauri. 2003. Fi-
nite State Morphology. CSLI Publications, USA. 
Boitet, Christian. Bhattacharayya, Pushpak. Blanc, 
Etienne. Meena, Sanjay. Boudhh, Sangharsh. Fafiotte, 
Georges. Falaise, Achille. Vacchani, Vishal. 2007. 
Building Hindi-French-English-UNL Resources for 
SurviTra-CIFLI, a linguistic survival system under 
construction. Proceedings of the Seventh Symposium 
on NLP, 13 ? 15 December, Chonburi, Thailand. 
Feroz ul Din. ????????????? ????? Feroz Sons Publishers, 
Lahore, Pakistan. 
Hussain, Sarmad. 2004. Letter to Sound Rules for 
Urdu Text to Speech System. Proceedings of Work-
shop on Computational Approaches to Arabic Script-
based Languages, COLING 2004, Geneva, Switzer-
land. 
James, L. Hieronymus. 1993. ASCII Phonetic Symbols 
for the World?s Languages: Worldbet. AT&T Bell 
Laboratories, Murray Hill, NJ 07974, USA. 
Kellogg, Rev. S. H. 1872. A Grammar of Hindi Lan-
guage. Delhi, Oriental Book Reprints. 
Khan, Mehboob Alam. 1997. ????? ?? ???? ???? (Sound 
System in Urdu) National Language Authority, Pakis-
tan. 
Knight, K. and Graehl, J. 1998. Machine Translitera-
tion. Computational Linguistics, 24(4). 
Knight, K. and Stall, B G. 1998. Translating Names 
and Technical Terms in Arabic Tex. Proceedings of 
the COLING/ACL Workshop on Computational Ap-
proaches to Semitic Languages. 
Malik, M. G. Abbas. 2006. Punjabi Machine Transli-
teration. Proceedings of the 21st International Confe-
rence on Computational Linguistics and 44th Annual 
Meeting of the ACL, July 2006, Sydney.  
Mohri, Mehryar. 1997. Finite-state Transducers in 
Language and Speech Processing. Computational 
Linguistics, 23(2). 
Montaut A. 2004. A Linguistic Grammar of Hindi. 
Studies in Indo-European Linguistics Series, M?n-
chen, Lincom Europa. 
Paola, V. and Sanjeev, K. 2003. Transliteration of 
proper names in cross-language applications. Pro-
ceedings of the 26th annual International ACM SIGIR 
conference on research and development in informa-
tion retrieval. 
Pirkola, A. Toivonen, J. Keskustalo, H. Visala, K. and 
J?rvelin, K. 2003. Fuzzy translation of cross-lingual 
spelling variants. Proceedings of the 26th Annual 
international ACM SIGIR Conference on Research 
and Development in informaion Retrieval, Toronto, 
Canada. 
Platts, John T. 1909. A Grammar of the Hindustani or 
Urdu Language. Crosby Lockwood and Son, 7 Sta-
tioners Hall Court, Ludgate hill, London. E.C. 
Platts, John T. 1911. A Dictionary of Urdu, Classical 
Hindi and English. Crosby Lockwood and Son, 7 Sta-
tioners Hall Court, Ludgate hill, London, E.C. 
Rahman, Tariq. 2004. Language Policy and Localiza-
tion in Pakistan: Proposal for a Paradigmatic Shift. 
Crossing the Digital Divide, SCALLA Conference on 
Computational Linguistics. 
Rai, Alok. 2000. Hindi Nationalism. Orient Longman 
Private Limited, New Delhi. 
Wells, J C. 1995. Computer-coding the IPA: A Pro-
posed Extension of SAMPA. University College Lon-
don. http://www.phon.ucl.ac.uk/home/sampa/ipasam-
x.pdf. 
Yan Qu, Gregory Grefenstette, David A. Evans. 2003. 
Automatic transliteration for Japanese-to-English text 
retrieval. Proceedings of the 26th annual interntional 
ACM SIGIR conference on Research and develop-
ment in information retrieval. 
Zia, Khaver. 1999a. Standard Code Table for Urdu. 
Proceedings of 4th Symposium on Multilingual In-
formation Processing (MLIT-4), Yangon, Myanmar, 
CICC, Japan. 
544
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 177?185,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
A Hybrid Model for Urdu Hindi Transliteration 
 
 
Abbas Malik Laurent Besacier Christian Boitet 
GETALP, Laboratoire d?Informatique Grenoble (LIG) 
Universit? Joseph Fourier 
Abbas.Malik, Laurent.Besacier, 
Christian.Boitet@imag.fr 
Pushpak Bhattacharyya 
IIT Bombay 
 
pb@cse.iitb.ac.in 
 
  
 
Abstract 
We report in this paper a novel hybrid ap-
proach for Urdu to Hindi transliteration that 
combines finite-state machine (FSM) based 
techniques with statistical word language 
model based approach. The output from the 
FSM is filtered with the word language model 
to produce the correct Hindi output. The main 
problem handled is the case of omission of di-
acritical marks from the input Urdu text. Our 
system produces the correct Hindi output even 
when the crucial information in the form of di-
acritic marks is absent. The approach improves 
the accuracy of the transducer-only approach 
from 50.7% to 79.1%. The results reported 
show that performance can be improved using 
a word language model to disambiguate the 
output produced by the transducer-only ap-
proach, especially when diacritic marks are not 
present in the Urdu input. 
1 Introduction 
Transliteration is a process to transcribe a word 
written in one language, in another language by 
preserving its articulation. It is crucial for han-
dling out-of-vocabulary (OOV) words in differ-
ent domains of Natural Language Processing 
(NLP), especially in Machine Translation 
(Knight and Graehl, 1998; Knight and Stall, 
1998; Paola and Sanjeev, 2003), Cross-Lingual 
Information Retrieval (Pirkola et al, 2003), the 
development of multi-lingual resources (Yan et 
al., 2003) and multi-lingual text and speech 
processing. It is also useful for Inter-dialectal 
translation without lexical changes and some-
times it is mandatory when the dialects in ques-
tion use mutually incomprehensible writing sys-
tems. Such cases exists in Malay (written in 2 
different scripts), Turkish (2 scripts), Kurdish (3 
scripts), Hindi/Urdu (2 scripts), Punjabi (2 
scripts), etc., where words are transliterated from 
one script to the other, irrespective of their type 
(noun, verb, etc., and not only proper nouns and 
unknown words). In this study, we will focus on 
Hindi/Urdu example. 
Hindi and Urdu are written in two mutually 
incomprehensible scripts, Devanagari and Urdu 
script ? a derivative of Persio-Arabic script re-
spectively. Hindi and Urdu are the official lan-
guages of India and the later is also the National 
language of Pakistan (Rahman, 2004). Table 1 
gives an idea about the number of speakers of 
Hindi and Urdu. 
 
 Native Speaker
2nd Lang. 
Speaker Total 
Hindi 366 487 853 
Urdu 60.29 104 164.29 
Total 426.29 591 1,017.29 
Source: (Grimes, 2000) all numbers are in millions 
Table 1: Hindi and Urdu Speakers 
Notwithstanding the transcriptional differences, 
Hindi and Urdu share phonology, grammar, 
morphology, literature, cultural heritage, etc. 
People from Hindi and Urdu communities can 
understand the verbal expressions of each other 
but the written expression of one community is 
alien to the other community. 
A finite-state transliteration model for Hindi 
and Urdu transliteration using the Universal In-
termediate Transcription (UIT ? a pivot between 
the two scripts) was proposed by Malik et al 
(2008). The non-probabilistic finite-state model 
is not powerful enough to solve all problems of 
Hindi ? Urdu transliteration. We visit and ana-
lyze Hindi ? Urdu transliteration problems in 
the next section and show that the solution of 
these problems is beyond the scope of a non-
probabilistic finite-state transliteration model. 
177
Following this, we show how a statistical model 
can be used to solve some of these problems, 
thereby enhancing the capabilities of the finite-
state model. 
Thus, we propose a hybrid transliteration 
model by combining the finite-state model and 
the statistical word language model for solving 
Hindi ? Urdu transliteration problems, dis-
cussed in section 2. Section 3 will throw light on 
the proposed model, its different components and 
various steps involved in its construction. In sec-
tion 4, we will report and various aspects of dif-
ferent experiments and their results. Finally, we 
will conclude this study in section 5. 
2 Hindi Urdu Transliteration 
In this section, we will analyze Hindi ? Urdu 
transliteration problems and will concentrate on 
Urdu to Hindi transliteration only due to shortage 
of space and will discuss the reverse translitera-
tion later. Thus, the remainder of the section ana-
lyzes the problems from Urdu to Hindi translite-
ration. 
2.1 Vowel, Yeh (?) and Waw (?) 
Urdu is written in a derivation of Persio-Arabic 
script. Urdu vowels are represented with the help 
of four long vowels Alef-madda (?), Alef (?), 
Waw (?), Yeh (?) and diacritical marks. One 
vowel can be represented in many ways depend-
ing upon its context or on the origin of the word, 
e.g. the vowel [?] is represented by Alef-madda 
(?) at the beginning of a word, by Alef (?) in the 
middle of a word and in some Persio-Arabic loan 
word, it is represented by the diacritical mark 
Khari Zabar (G?). Thus Urdu has very complex 
vowel system, for more details see Malik et al 
(2008). Urdu contains 10 vowels, and 7 of them 
also have their nasalization forms (Hussain, 
2004; Khan, 1997) and 15 diacritical marks. 
Thou diacritical marks form the cornerstone of 
the Urdu vowel system, but are sparingly used 
(Zia, 1999). They are vital for the correct Urdu to 
Hindi transliteration using the finite-state transli-
teration model. The accuracy of the finite-state 
transliteration model decreases from above 80% 
to 50% in the absence of diacritical marks. Fig-
ure 1 shows two example Urdu phrases (i) with 
and (ii) without the diacritical marks and their 
Hindi transliteration using the finite-state transli-
teration model. Due to the absence of Zabar (F?) 
in the first and the last words in (1)(ii) and in the 
5th word in (2)(ii), vowels ? ? [?] and ? [?] are 
transliterated into vowels ?? [e] and ? [o] re-
spectively. Similarly, due to the absence of Pesh 
( E?) and Zer (G?) in 3rd and 4th words respectively 
in (1)(ii), both vowels ? ? [?] and ?? [?] are con-
verted into the vowel [?]. All wrongly converted 
words are underlined. 
 
(1)  (i) ??? ???? ?? ???? ????? ??? ????? ???? 
 (ii) ?? ??? ???? ??? ??? ?? ??? ???? 
(i) ??? ?? ???? ???? ??? ???? ???? ?? 
?? ??? ???? ??? ??? ??? ?? ??? (ii) 
I have not done a lot of work 
(2) (i) ????? ?? ?? ?????? ???? ??? ?? ???? ???????????  
  (ii) ????? ??? ?? ??? ??????? ??? ?? ??? ???  
(i) ??????? ??? ?? ?? ?? ????? ??? ?? ?? 
?? ?? ??? ???? ?? ?? ?? ??? ??? ??? (ii) 
Both at the central level and at the state level 
Figure 1: Example Urdu Phrases 
In Hindi, each vowel is represented by a cha-
racter and a vowel sign except the vowel [?], 
which is only represented by the character ? and 
do not have a vowel sign (Malik et al, 2008). 
Table 2 gives all vowel conversion problems. 
 
Sr. IPA 
Vowel 
Conversion 
Problems 
Hindi 
1 ? ? ? ? ? or ?? ? ? or 0* 
2 ? ? ? ? ? or ?? ? ? or 0* 
3 i i ? e ? or ?? ? ? or ?? 
4 ? ? ? e ? or ?? ? ? or ?? 
5 u u ? o ? or ?? ? ? or ?? 
6 ? ? ? o ? or ?? ? ? or ??
7 j j ? e ? ? ?? 
8 v v ? o ? ? ?? 
* Zero (0) means deleted. 
Table 2: Vowel Problems from Urdu to Hindi 
Long vowels Yeh (?) [j] and Waw (?) [v] are 
also used as consonants and certain contextual 
rules help us to decide whether they are used as a 
consonant or as a vowel, e.g., Yeh (?) and Waw 
(?) are used as consonants at the start of a word 
and after the long vowel Alef-madda (?), etc. Fi-
178
nite-state transliteration model can exploit such 
contextual rules but it is not possible to decide 
Yeh (?) and Waw (?) as consonants in the ab-
sence of diacritics. Thus a finite-state translitera-
tion model wrongly converts consonant Yeh (?) 
and Waw (?) into vowels ?? [e] and ?? [o], also 
given in Table 2, instead of consonants Ya (?) 
and Wa (?) respectively, e.g., in the word ????? 
(prince) [k??vr], Waw is wrongly converted into 
the vowel [o] due to the absence of Zabar ( F?) 
after it and the word becomes [k?nor], which is 
not a valid word of Hindi/Urdu. 
2.2 Native Sounds 
The Hindi writing system contains some native 
sounds/characters, e.g., vocalic R (?) [r?], retrof-
lex form of Na (?) [?], etc. On the other hand 
Urdu does not have their equivalents. Thus 
words containing such sounds are transcribed in 
Urdu with their approximate phonetic equiva-
lents. All such cases are problematic for Urdu to 
Hindi transliteration and are given in Table 3. 
 
Sr. IPA Hindi Urdu 
1 r? ? or ?? ? [r] 
2 ? ? ? [n] 
3 ? ? ? [?] 
4 Half h ?? ? [h] 
Table 3: Sounds of Sanskrit Origin 
2.3 Conjunct Form 
The Hindi alphabet is partly syllabic because 
each consonant inherits the vowel [?]. Two or 
more consonants may be combined together to 
form a cluster called Conjunct that marks the 
absence of the inherited vowel [?] between con-
sonants (Kellogg, 1872; Montaut, 2004). Con-
junction is also used to represent the gemination 
of a consonant, e.g., ?[k]+??+?[k]=???[kk] 
where ?? is the conjunct marker and aspiration of 
some consonants like ? [n], ? [m], ? [r] and ? 
[l] when used as conjunction with ? [h], e.g., 
?[n] + ?? + ?[h] = ???[nh]. Conjunction has a spe-
cial meaning but native speakers use conjunct 
forms without any explicit rule (Montaut, 2004). 
On the other hand, Urdu uses Jazam ( H? ? a 
diacritic) and Shadda (H?) to mark the absence of 
a vowel between two consonants and gemination 
of a consonant respectively. In the absence of 
these diacritics in the input Urdu text, it is not 
possible to decide on the conjunct form of con-
sonants except in the case of aspiration. In Urdu, 
aspiration of a consonant is marked with the spe-
cial character Heh-Doachashmee (?) (Malik et 
al., 2008), thus a finite-state transducer can easi-
ly decide about the conjunction for aspiration 
with a simple contextual rule, e.g. the word ????? 
(bride) [d??lhn] is correctly transliterated by our 
finite-state transliteration model into ??????. 
2.4 Native Hindi Spellings and Sanskritized 
Vocabulary 
Sanskrit highly influences Hindi and especially 
its vocabulary. In some words of Sanskrit origin, 
the vowel ?? [i] and ?? [u] are transcribed as ?? 
[?] and ?? [?] respectively at the end of a word. 
Javaid and Ahmed (2009) have pointed to this 
issue in these words ?Hindi language can have 
words that end on short vowel??. Table 4 gives 
some examples of such native words. On the 
other hand in Urdu, short vowels can never come 
at the end of a word (Javaid and Ahmed, 2009; 
Malik et al, 2008). 
 
Vowel Examples 
?? [i] 
??????? ? ????? (person) [vj?kti] 
??????? ? ??????? (culture) [s??skr?t?i] 
???????? ? ??????? (high) [???ko?i] 
?? [u] 
???? ? ????? (for) [het?u] 
????? ?? ????? (but) [k?nt?u] 
???? ? ?????? (metal) [d??t?u] 
Table 4: Hindi Word with Short vowel at End 
It is clear from above examples that short vowels 
at the end of a Hindi word can easily be translite-
rated in Urdu using a contextual rule of a finite-
state transducer, but it is not possible to do so for 
Urdu to Hindi transliteration using a non-
probabilistic finite-state transliteration model. 
Thus Urdu to Hindi transliteration can also be 
179
considered as a special case of Back Translitera-
tion. 
In some words, the vowel ?? [u] is written as 
the vowel ?? [?], e.g., ??? ? ????? or ??? ? ???? (to be) 
[hue], ??????? (name of a city) [r???npur]. 
Some of these cases are regular and can be im-
plemented as contextual rules in a finite-state 
transducer but it is not possible in every case. 
2.5 Ain (?) 
Ain (? ? glottal stop) exists in the Arabic alpha-
bet and native Arabic speakers pronounce it 
properly. Urdu also has adopted Ain (?) in its 
alphabet as well as Arabic loan words but native 
speakers of the sub-continent cannot produce its 
sound properly, rather they produce a vowel 
sound by replacing Ain (?) with Alef (?). The 
Hindi alphabet follows one character for one 
sound rule and it does not have any equivalent of 
Ain (?). Then, Ain (?) in Urdu words is tran-
scribed in Hindi by some vowel representing the 
pronunciation of the word by native sub-
continent speakers. Thus it is always translite-
rated in some vowel in Hindi. For example, Ain 
(?) gives the sound of the vowel [?] in ????  ? 
???? (strange) [??ib] and the vowel [?] with 
and without Alef (?) in words ???  ? ?? (com-
mon) [?m] and ???  ? ??? (after) [b?d?] respective-
ly. In some words, Ain (?) is not pronounced at 
all and should be deleted while transliterating 
from Urdu to Hindi, e.g., ??????  ? ???? (to start) 
[??ru], etc. Conversion of Ain (?) is a big prob-
lem for transliteration. 
2.6 Nasalization 
Noonghunna (?) [?] is the nasalization marker of 
vowels in Urdu. Interestingly, it is only used to 
nasalize a vowel at the end of a word. In the 
middle of a word, Noon (?) [n] is used to mark 
the nasalization of a vowel and it is also used as 
a consonant. It is difficult to differentiate be-
tween nasalized and consonant Noon (?). There 
are certain contextual rules that help to decide 
that Noon (?) is used as a consonant or a nasali-
zation marker, but it not possible in all cases. 
2.7 Persio-Arabic Vocabulary 
Urdu borrows a considerable portion of it voca-
bulary from Persian and Arabic and translitera-
tion of these words in Hindi is not regular. Table 
5 explains it with few examples. 
 
Urdu 
Hindi 
FST Conversion Correct 
??????  
?????? 
(surely) 
?????? 
[b?lk?l] 
???????? 
???????? 
(with reference of) 
???????? 
[b?lv?st??] 
?? ????????
????????? 
(in fact) 
???????? 
[f?lh?qiq?t]
Table 5: Persio-Arabic Vocabulary in Urdu 
3 Hybrid Transliteration Model 
The analysis of the previous section clearly 
shows that solution of these problems is beyond 
the scope of the non-probabilistic Hindi Urdu 
Finite-state transliteration model (Malik et al, 
2008). We propose a hybrid transliteration model 
that takes the input Urdu text and converts it in 
Hindi using the Finite-state Transliteration Mod-
el (Malik et al 2008). After that, it tries to cor-
rect the orthographic errors in the transducer-
only Hindi output string using a statistical word 
language model for Hindi with the help of a 
Hindi Word Map described later. The approach 
used is rather similar to what is done in text re-
capitalization (Stolcke et al 1998) for instance. 
 
Figure 2: Hybrid Transliteration Model for Urdu 
Hindi 
Normally, the Urdu text does not contain neces-
sary diacritical marks that are mandatory for the 
correct transliteration by the finite-state compo-
nent Urdu Hindi Transliteration 
180
Finite-state Machine (UHT-FSM), 
described by Malik et al (2008). The proposed 
hybrid model focuses on the correct translitera-
tion of Urdu texts without diacritical marks. Fig-
ure 2 gives the proposed Model architecture. 
3.1 Preprocessing UHT-FSM Output 
The goal of this pre-processing is to generate a 
more ?normalized? (and consequently more am-
biguous) form of Hindi, e.g. pre-processing 
transforms both corpus words ?? (this) [?s] and 
?? (that) [?s] (if encountered in the UHT-FSM 
Hindi output) into the default input Hindi word 
??* [?s] (not a valid Hindi word but is a finite-
state transliteration of the input Urdu word ??, a 
word without diacritical marks). Thus pre-
processing is vital for establishing connections 
between the UHT-FSM Hindi output words 
(from the Urdu input without diacritical marks) 
and the Hindi corpus words. In the example 
above, the word ??* [?s] is aligned to two Hin-
di corpus words. All such alignments are record-
ed in the Hindi Word Map. This ambiguity will 
be solved by the Hindi word language 
model, trained on a large amount of Hindi data. 
Thus pre-processing is a process that establishes 
connections between the most likely expected 
input Hindi word forms (UHT-FSM Hindi output 
from the Urdu input without diacritical marks) 
and the correct Hindi word forms (words that are 
present in the Hindi corpus). 
The Preprocessing component is a finite-
state transducer that normalizes the Hindi output 
of UHT-FSM component for the Hindi word 
language model. The transducer converts all 
cases of gemination of consonants into a simple 
consonant. For example, the UHT-FSM converts 
the Urdu word ??? (God) [r?bb] into ???? and the 
Preprocessing converts it into ?? [rb]. The 
transducer also removes the conjunct marker (??) 
from the output of the UHT-FSM except when it 
is preceded by one of the consonant from the set 
{? [r], ? [l], ? [m], ? [n]} and also followed by 
the consonant ? [h] (first 3 lines of Figure 3), 
e.g., UHT-FSM converts the Urdu words ?????? 
(Hindi) [h?ndi] and ????? (bride) [d??lhn] into ?????? 
and ?????? respectively and the Preprocess-
ing component converts them into ????? (re-
moves ??) and ?????? (no change). Actually, Pre-
processing deteriorates the accuracy of the output 
of the UHT-FSM component. We will come back 
to this point with exact figures in the next sec-
tion. 
The code of the finite-state transducer is given 
in XFST (Beesley and Karttunen, 2003) style in 
Figure 3. In XFST, the rules are applied in re-
verse order due to XFST?s transducer stack, i.e. a 
rule written at the end of the XFST script file 
will apply first and so on. 
 
read regex [? ?-> 0 || [? - [? | ? | ? | ?]] _ [? - 
?]]; 
read regex [?? -> 0 || [? | ? | ? | ?] _ [? - ?]]; 
read regex [?? -> 0 || [? - [? | ? | ? | ?]] _ [?]]; 
read regex [[? ? ??] -> ?, [? ? ??] -> ?, 
[? ? ??] -> ?, [? ? ??] -> ?, [? ? ??] -> ?, 
[? ?? ?] -> ?, [? ? ??] -> ?, [? ? ??] -> ?, 
[? ? ??] -> ?, [? ? ??] -> ?, [? ? ??] -> ?, [? ? ?
?] -> ?, [? ?? ?] -> ?, [? ? ??] -> ?, [? ? ??] 
-> ?, [? ? ??] -> ?, [? ? ??] -> ?, [? ? ??] -> 
?, [? ? ??] -> ?, [? ? ??] -> ?, [? ? ??] -> ?, 
[? ? ??] -> ?, [? ? ??] -> ?, [? ? ??] -> ?, 
[? ?? ?] -> ?, [? ?? ?] -> ?, [? ?? ?] -> ?, 
[? ?? ?] -> ?, [? ?? ?] -> ?, [? ?? ?] -> ?, 
[? ?? ?] -> ?, [? ?? ?] -> ?, [? ?? ?] -> ?, 
[? ?? ?] -> ?, [? ?? ?] -> ?, [? ? ??] -> ?]; 
Figure 3: Preprocessing Transducer 
3.2 Hindi Word Language Model 
The Hindi Word Language Model is an 
important component of the hybrid transliteration 
model. For the development of our statistical 
word language model, we have used the Hindi 
Corpus freely available from the Center for In-
dian Language Technology1, Indian Institute of 
Technology Bombay (IITB), India. 
First, we extracted all Hindi sentences from 
the Hindi corpus. Then we removed all punctua-
tion marks from each sentence. Finally, we add-
ed ?<s>? and ?</s>? tags at the start and at the 
end of each sentence. We trained a tri-gram 
Hindi Word Language Model with the 
SRILM (Stolcke, 2002) tool. The processed Hin-
di corpus data contains total 173,087 unique sen-
                                                 
1 http://www.cfilt.iitb.ac.in/ 
181
tences and more than 3.5 million words. The 
SRILM toolkit command ?disambig? is used to 
generate the final Hindi output using the statis-
tical word language model for Hindi and the 
Hindi Word Map described in the next section.  
3.3 Hindi Word Map 
The Hindi Word Map is another very important 
component of the proposed hybrid transliteration 
model. It describes how each ?normalized? Hindi 
word that can be seen after the Preprocess-
ing step and can be converted to one or several 
correct Hindi words, the final decision being 
made by the statistical word language model for 
Hindi. We have developed it from the same 
processed Hindi corpus data that was used to 
build the Hindi Word Language Model. 
We extracted all unique Hindi words (120,538 
unique words in total). 
The hybrid transliteration model is an effort to 
correctly transliterate the input Urdu text without 
diacritical marks in Hindi. Thus we take each 
unique Hindi word and try to generate all possi-
ble Hindi word options that can be given as input 
to the Hindi Word Language Model 
component for the said word. Consider the Urdu 
word ??? (God) [r?bb]; its correct Hindi spel-
lings are ????. If we remove the diacritical mark 
Shadda (H?) after the last character of the word, 
then the word becomes ?? and UHT-FSM trans-
literates it in ??*. Thus the Hindi Word 
Language Model will encounter either ???? or 
??* for the Hindi word ???? (two possible word 
options). In other words, the Hindi Word Map is 
a computational model that records all possible 
alignments between the ?normalized? or pre-
processed words (most likely input word forms) 
and the correct Hindi words from the corpus. 
We have applied a finite-state transducer that 
generates all possible word options for each 
unique Hindi word. We cannot give the full 
XFST code of the ?Default Input Creator? due to 
space shortage, but a sample XFST code is given 
in Figure 4. If the Urdu input contains all neces-
sary diacritical marks, then pre-processing of the 
output of the UHT-FSM tries to remove the effect 
of some of these diacritical marks from the Hindi 
output. In the next section, we will show that 
actually it increases the accuracy at the end. 
 
 
define CONSONANTS [? | ? | ? | ? | ? | ? | 
? | ? | ? | ? | ? | ? | ? | ? | ? | ? | ? | ? | ? | 
? | ? | ? | ? | ? | ? | ? | ? | ? | ? | ? | ? | ? | 
? | ? | ? | ? | ? | ? | ? | ?]; 
? 
read regex [?? (->) ?,? ?? (->) ?,? ?? (->) ??, ?? 
(->) ??, ?? (->) 0, ? ?(->) 0 || [CONSONANTS] 
_ ]; 
read regex [?? (->) ? ?|| [CONSONANTS] _ [? -
 .#.]]; 
read regex [?? -> ??, ? ? -> ??, ? ? -> ? ? || 
[CONSONANTS] _ .#.]; 
? 
Figure 4: Default Input Creator Transducer 
Practically, the Hindi Word Map is a file in 
which each line contains a possible input word to 
Hindi Word Language Model, followed 
by a list of one (see line 3 of Figure 5) or more 
(see line 1 of Figure 5) words from the corpus 
that are associated with this possible input word. 
The ?Default Input Creator? transducer has 
generated in total 961,802 possible input words 
for 120,538 unique Hindi words. For implemen-
tation reasons, we also added non-ambiguous 
pair entries in the word map (see line 2 of Figure 
5), thus the initial word map contains in total 
1,082,340 entries. We extract unique option 
words and finally, Hindi Word Map contains in 
total 962,893 entries. Some examples from Hindi 
Word Map file are given in Table 5. 
 
(1) ???? ???? ???? 
(2) ???? ???? 
(3) ?? ???? 
(4) ???????? ????????? ?????????? 
(5) ?? ?? ?? 
Figure 5: Sample Hindi Word Map 
4 Test and Results 
For testing purposes, we extracted 200 Hindi 
sentences from the Hindi corpus before removing 
punctuation marks. These sentences were of 
course removed from the training corpus used to 
build the statistical word language model for 
Hindi. First we converted these 200 Hindi sen-
tences in Urdu using Hindi Urdu Finite-state 
transliteration model (Malik et al, 2008). Trans-
182
literated Urdu sentences were post edited ma-
nually for any error and we also made sure that 
the Urdu text contained all diacritical marks. 200 
original Hindi sentences served as Hindi refer-
ence for evaluation purposes. 
From the post-edited Urdu sentences, we de-
veloped two test corpora. The first test corpus 
was the Urdu test with all diacritical marks. In 
the second test corpus, all diacritical marks were 
removed. We calculated both word level and 
character level accuracy and error rates using the 
SCLITE 2  tool. Our 200 sentence test contains 
4,250 words and 16,677 characters in total. 
4.1 Test: UHT-FSM 
First we converted both Urdu test data using 
UHT-FSM only and compared the transliterated 
Hindi texts with the Hindi reference. UHT-FSM 
shows a word error rate of 21.5% and 51.5% for 
the Urdu test data with and without diacritics 
respectively. Results are given in Table 6, row 1. 
 
Urdu Test Data With diacritics 
Without 
diacritics 
UHT-FSM 
Accuracy/Error 
80.7% / 
21.5% 
50.7% / 
51.5% 
UHT-FSM + 
HLM 
82.6% / 
19.6% 
79.1% / 
23.1% 
UHT-FSM + 
PrePro 
67.5% / 
32.4% 
50.7% / 
51.5% 
UHT-FSM + 
PrePro + HLM 
85.8% / 
16.4% 
79.1% / 
23.1% 
Table 6: Word Level Results 
These results support our claims that the absence 
of diacritical marks considerably increases the 
error rate. 
4.2 Test: UHT-FSM + Hindi Language 
Model 
Both outputs of UHT-FSM are first passed direct-
ly to Hindi Word Language Model with-
out preprocessing. The Hindi Word Lan-
guage Model converts UHT-FSM Hindi out-
put in the final Hindi output with the help of 
Hindi Word Map. 
Two final outputs were again compared with 
the Hindi reference and results are given in Table 
6, row 2. For Urdu test data without diacritics, 
error rate decreased by 28.4% due to the Hindi 
Word Language Model and Hindi Word 
                                                 
2 http://www.itl.nist.gov/iad/mig//tools/ 
Map as compared to the UHT-FSM error rate. 
The Hindi Word Language Model also decreases 
the error rate by 1.9% for the Urdu test data with 
diacritics. 
4.3 Test: UHT-FSM + Preprocessing 
In this test, both outputs of UHT-FSM were pre-
processed and the intermediate Hindi outputs 
were compared with the Hindi reference. Results 
are given in Table 6, row 3. After the comparison 
of results of row 1 and row 3, it is clear that pre-
processing deteriorates the accuracy of Urdu test 
data with diacritics and does not have any effect 
on Urdu test data without diacritics. 
4.4 Test: UHT-FSM + Preprocessing + 
Hindi Language Model 
Preprocessed UHT-FSM Hindi outputs of the test 
of Section 4.3 were passed to the Hindi Word 
Language Model that produced final Hindi 
outputs with the help of the Hindi Word Map. 
Results are given in Table 6, row 4. They show 
that the Hindi Word Language Model 
increases the accuracy by 5.1% and 18.3% when 
compared with the accuracy of UHT-FSM and 
UHT-FSM + Preprocessing tests respectively, for 
the Urdu test data with diacritical marks. 
For the Urdu test data without diacritical 
marks, the Hindi Word Language Model 
increases the accuracy rate by 28.3% in compari-
son to the accuracy of the UHT-FSM output 
(whether pre-processed or not). 
4.5 Character Level Results 
All outputs of tests of Sections 4.1, 4.2, 4.3 and 
4.4 and the Hindi reference are processed to cal-
culate the character level accuracy and error 
rates. Results are given in Table 7. 
 
Urdu Test 
Data 
With 
diacritics 
Without 
diacritics 
UHT-FSM 94.1% / 6.5% 77.5% / 22.6%
UHT-FSM + 
HLM 94.6% / 6.1% 89.8% / 10.7 
UHT-FSM + 
PreP 87.5% / 13.0% 77.5% / 22.6 
UHT-FSM + 
PreP + HLM 94.5% / 6.1% 89.8% / 10.7 
Table 7: Character Level Results 
183
4.6 Results and Examples 
The Hindi Word Language Model in-
creases the accuracy of Urdu Hindi translitera-
tion, especially for the Urdu input without dia-
critical marks. 
Consider the examples of Figure 7. Figure 1 is 
reproduced here by adding the Hindi translitera-
tion of example sentences using the proposed 
hybrid transliteration model and Hindi reference. 
 
(1)  (i) ?? ???? ????? ??? ????? ???? ??? ???? 
 (ii) ??? ?? ??? ???? ??? ???? ??? ?? 
(i) ??? ?? ???? ???? ??? ???? ???? ?? 
?? ??? ???? ??? ??? ??? ?? ???(ii)  
I have not done a lot of work
Output of Hybrid Transliteration Model 
(i) ??? ?? ???? ???? ??? ???? ???? ?? 
(ii) ??? ?? ???? ???? ??? ???? ???? ?? 
Hindi Reference 
????? ???? ???? ??? ???? ???? ?? 
(2) (i) ????? ?? ?? ?????? ???? ????? ?? ?? ???????????  
  (ii) ??????? ??? ?? ??? ??? ????? ??? ?? ???  
(i) ??????? ??? ?? ?? ?? ????? ??? ?? ?? 
 ?? ??? ???? ?? ?? ?? ??? ??? ???(ii) 
?? 
Both at the central level and at the state level
Output of Hybrid Transliteration Model 
(i) ??????? ??? ?? ?? ?? ????? ??? ?? ?? 
(ii) ??? ??? ??? ?? ?? ?? ????? ??? ?? ?? 
Hindi Reference 
??????? ??? ?? ?? ?? ????? ??? ?? ?? 
Figure 7: Examples 
By comparing Hindi outputs of Hindi Word 
Language Model with the Hindi reference, 
only the first word of (2)(ii) is wrong and other 
errors due to the absence of diacritical marks in 
the source Urdu sentences are corrected properly. 
5 Conclusion 
From the test results of the previous section we 
can conclude that the statistical word language 
model increases the accuracy of Urdu to Hindi 
transliteration, especially for Urdu input text 
without diacritical marks. The proposed Hybrid 
Transliteration Model improves the accuracy and 
produces the correct Hindi output even when the 
crucial information in the form of diacritical 
marks is absent. It increases the accuracy by 
28.3% in comparison to our previous Finite-state 
Transliteration Model. This study also shows that 
diacritical marks are crucial and necessary for 
Hindi Urdu transliteration. 
References  
Beesley, Kenneth R. and Karttunen, Lauri. 2003. Fi-
nite State Morphology, CSLI Publication, USA. 
Grimes, Barbara F. (ed). 2000. Pakistan, in Ethnolo-
gue: Languages of the World, 14th Edition Dallas, 
Texas; Summer Institute of Linguistics, pp: 588-
598. 
Hussain, Sarmad. 2004. Letter to Sound Rules for 
Urdu Text to Speech System, proceedings of Work-
shop on Computational Aproaches to Arabic 
Script-based Languages, COLING 2004, Geneva, 
Switzerland. 
Jawaid, Bushra and Tafseer Ahmed. 2009. Hindi to 
Urdu Conversion: Beyond Simple Transliteration, 
in proceedings of Conference on Language & 
Technology, Lahore, Pakistan. 
Kellogg, Rev. S. H. 1872. A Grammar of Hindi Lan-
guage, Delhi, Oriental Book reprints. 
Khan, Mehboob Alam. 1997. ????? ?? ???? ???? (Sound 
System in Urdu), National Language Authority, 
Pakistan 
Knight, K. and Graehl, J. 1998. Machine Translitera-
tion, Computational Linguistics, 24(4). 
Knight, K. and Stall, B. G. 1998. Transliterating 
Names and Technical Terms in Arabic Text, pro-
ceedings of COLING/ACL Workshop on Compu-
tational Approaches to Semitic Languages. 
Malik, M. G. Abbas. Boitet, Christian. Bhattcharyya, 
Pushpak. 2008. Hindi Urdu Machine Translitera-
tion using Finite-state Transducers, proceedings of 
COLING 2008, Manchester, UK. 
Montaut, A. 2004. A Linguistic Grammar of Hindi, 
Studies in Indo-European Linguistics Series, Mun-
chen, Lincom Europe. 
Paola, V. and Sanjeev, K. 2003. Transliteration of 
Proper Names in Cross-language Application, pro-
ceedings of 26th Annual International ACM SIGIR 
Conference on Research and Development in In-
formation Retrieval, Toronto, Canada. 
Pirkola, A. Toivonen, J. Keshustalo, H. Visala, K. and 
Jarvelin, K. 2003. Fuzzy Translation of Cross-
lingual Spelling Variants, proceedings of 26th An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval, 
Toronto, Canada. 
Rahman, Tariq. 2004. Language Policy and Localiza-
tion in Pakistan: Proposal for a Paradigmatic 
184
Shift, Crossing the Digital Divide, SCALLA Con-
ference on Computational Linguistics. 
Stolcke, A. 2002. SRILM ? An Extensible Language 
Modeling Toolkit, in proceedings of International 
Conference on Spoken Language Processing. 
Stolcke, A. Shriberg, E. Bates, R. Ostendorf, M. Hak-
kani, D. Plauche, M. Tur, G. and Lu, Y. 1998. Au-
tomatic Detection of Sentence Boundaries and Dis-
fluencies based on Recognized Words. Proceedings 
of International Conference on Spoken Language 
Processing (ICSLP), Sydney, Australia. 
Yan, Qu. Gregory, Grefenstette. and David A. Evans. 
2003. Automatic Transliteration for Japanese-to-
English Text Retrieval. In proceedings of the 26th 
annual international ACM SIGIR conference on 
Research and Development in Information Retriev-
al, pp: 353 ? 360. 
Zia, Khaver. 1999. Standard Code Table for Urdu. 
Proceedings of 4th Symposium on Multilingual In-
formation Processing (MILIT-4), Yangon, Myan-
mar, CICC, Japan. 
 
185
The PAPILLON project: cooperatively building a multilingual lexical
data-base to derive open source dictionaries & lexicons
Christian BOITET(1), Mathieu MANGEOT(2) & Gilles S?RASSET(1)
 (1) GETA, CLIPS, IMAG
385, av. de la biblioth?que, BP 53
F-38041 Grenoble cedex 9, France
Christian.Boitet@imag.fr
(2) National Institute of Informatics (NII)
2-1-2-1314, Hitotsubashi
Chiyoda-ku Tokyo 101-8430, Japan
Mathieu.Mangeot@imag.fr
Abstract
The PAPILLON project aims at creating a cooperative, free, permanent, web-oriented and personalizable environment for the
development and the consultation of a multilingual lexical database. The initial motivation is the lack of dictionaries, both for
humans and machines, between French and many Asian languages. In particular, although there are large F-J paper usage
dictionaries, they are usable only by Japanese literates, as they never contain both original (kanji/kana) and romaji writing.
This applies as well to Thai, Vietnamese, Lao, etc.
Introduction
The project was initiated in 2000 and launched
with the support of the French Embassy and NII
(Tokyo) in July 2000, and took really shape in
2001, with a technical seminar in July 2001 at
Grenoble, and concrete work (data gathering,
tool building, etc.).
The macrostructure of Papillon is a set of
monolingual dictionaries (one for each language)
of word senses, called "lexies", linked through a
central set of interlingual links, called "axies".
This pivot macrostructure has been defined by
S?rasset (1994) and experimented by Blanc
(1994) in the PARAX mockup.
The microstructure of the monolingual
dictionaries is the "DiCo" structure, which is a
simplification of Mel'tchuk's (1981;1987;1995)
DEC (Explanatory and Combinatory
Dictionary) designed by Polgu?re (2000) &
Mel'tchuk !"# make it possible to construct
large, detailed and principled dictionaries in
tractable time.
1. Languages included in the project
In 2000, the initial languages of the Papillon
project were English, French, Japanese and Thai.
Thai was included because there had been a
successful project, SAIKAM (Ampornaramveth
et al 1998; 2000), supported by NII and
NECTEC, of building a Japanese-Thai lexicon by
volunteers on the web. Lao, Vietnamese, and
Malay have been added in 2001 because of active
interest of labs and individuals.
The star-like macrostructure of Papillon makes
it easy to add a new language. Also, the DiCo
microstructure of each monolingual dictionary is
defined by an XML schema, containing a large
common core and a small specialization part
(morphosyntactic categories, language usage).
2. Interlingual links
Axies, also called "interlingual acceptions", are
not concepts, but simply interlingual links
between lexies, motivated by translations found
in existing dictionaries or proposed by the
contributors.
In case of discrepancies, 1 axie may be linked
with lexies of some languages only, e.g.
FR(mur#1), EN(wall#1), RU(stena#1), and t o
other axies by refinement links:
Example:
Axie#234 --lg--> FR(mur#1),
EN(wall#1), RU(stena#1)
--rf--> Axie#235, Axie#236
Axie#235 --lg--> DE(Wand#2),
IT(muro#1), ES(muro#1)
Axie#236 --lg--> DE(Mauer#2),
IT(parete#1), ES(pared#1)
It is also possible to have 2 axies for the same
"concept" at a certain stage in the life of the
database, because the monolingual information is
not yet detailed enough.
Suppose the level of language (usual, specialized,
vulgar, familiar?) is not yet given for
FR(maladie#1), FR(affection#2), EN(disease#1),
EN(affection#3).
Then we might have, for translational reasons:
Axie#456
--lg-->
FR(maladie#1),
EN(disease#1)
Axie#457
--lg-->
FR(affection#2),
EN(affection#3)
When this information will be put in each of the
above 4 monolingual entries, we may merge the
2 axies and get:
Axie#500
--lg-->
FR(maladie#1, affection#2),
EN(disease#1, affection#3)
Axies may also be linked to "external" systems
of semantic description. Each axie contains a
(possibly empty) list for each such system, and
the list of systems is open. The following are
included at this stage; UNL UWs (universal
words), ONTOS concepts, WordNet synsets,
NTT semantic categories.
3. Building the content
3.1. Recuperating existing resources
Building the content of the data base has several
aspects. To initiate it, the project starts from
open source computerized data, called "raw
dictionaries", which may be monolingual (4,000
French DiCo entries from UdM, 10,000 Thai
entries from Kasetsart Univ.), bilingual (70,000
Japanese-English entries and 10,000 Japanese-
French entries in J.Breen's JDICT XML format,
8000 Japanese-Thai entries in SAIKAM XML
format, 120,000 English-Japanese entries in
KDD-KATE LISP format), or multilingual
(50,000 French-English-Malay entries in FEM
XML format).
3.2. Integrating the data into Papillon
In the second phase, the "raw dictionaries" are
transformed into a "lexical soup" in
M.Mangeot's (2001) intermediary DML format
(an XML schema and namespace). The
transformation into almost empty DiCo entries
and the creation of axies for the translational
equivalences is semi-automatic. A tool has been
programmed at NII for that task.
3.3. Enriching the data with contributions
After that, it is hoped that many contributors
will "fill in" the missing information. The basis
for that third and continuous phase is a server
for cooperative building of the data base, where
each contributor has his/her own space, so that
contributions can be validated and integrated
into the DB by a group experts. Users can
establish user groups with specific read and write
access rights on their spaces.
4. Consultation of the resulting data
4.1. Online consultation
Consultation is meant to be free for anybody,
and open source. Requests produce personalizable
views of the data base, the most classical of
which are fragments of bilingual dictionaries.
However, it is also possible to produce
multitarget entries, on the fly and offline. Users
(consumers) are encouraged to become
contributors. To contribute, one may propose a
new word sense, a definition, an example of use,
a translation, the translation of an example, a
correction, etc., or an annotation on any
accessible information: Every user can
contribute with his own knowledge level.
4.2. Download of entire files
Users can also retrieve files, and can contribute
to define new output formats. The files retrieved
can contain structural, content-oriented tags.
This open source orientation contrasts with the
current usage of allowing users to retrieve files
containing only presentation-oriented tags.
4.3. Coverage of the dictionary
An interesting point is that the project wants t o
cover both general terms and terminological
terms.
Another one is that it contains a translation
subproject, because definitions, examples,
citations, etc. have to be translated into all
languages of the collection. For this, the notion
of complex lexie, already present to account for
lexical collocations such as compound predicates
(e.g. "to kick the bucket"), is extended to cover
full sentences. Axies relating them are special
because they can't in general relate them t o
external semantic systems such as WordNet. An
exception is UNL: the UNL list for an axie may
contain one UNL graph, produced automatically,
manually, or semi-automatically.  This graph
may be automatically sent to available UNL
"deconverters" to get draft translations.
5. Project organisation
In the current stage, the project has no legal
implementation as a fundation, association,
company, etc., although many participants have
already established official MOUs and other
types of agreements on which to base their
cooperative work.
There is a steering committee of about 10-12
members, who represent Papillon where they
are, and not the converse. There is a set of
tasks, and for each task a working group and an
advisory committee. One of the tasks is the
management of the project. In between, there is
a coordinating group containing the heads of the
tasks and chaired by the head of the
management task.
Sponsors may not donate money to the project,
which has no bank account. Rather, they are
encouraged to donate data, to assign personal
part time to the project, and to fund
participating organizations and persons as they
see fit.
Conclusion
The theoretical frameworks for the whole
database, the macrostructure and the
microstructure are very well defined. I t
constitutes a solid basis for the implementation.
A lot of open problems still have to be addressed
for the Papillon project to be a success. In this
respect, the Papillon project appears to be a
very interesting experimentation platform for a
lot of NLP research as data acquisition or human
access to lexical data, among others.
All these research will improve the attraction of
such a project to the Internet users. This
attraction is necessary for the project to go on,
as it is highly dependent on its users
motivations.
This way, we will be able to provide a very
interesting multilingual lexical database that we
hope useful for a lot of persons.
Rerefences
Ampornaramveth V., Aizawa A. & Oyama K. (2000)
An Internet-based Collaborative Dictionary
Development Project: SAIKAM. Proc. of 7th Intl.
Workshop on Academic Information Networks and
Systems (WAINS'7), Bangkok, 7-8 December 2000,
Kasetsart University.
Blanc ?., S?rasset G. & Tch?ou F. (1994) Designing
an Acception-Based Multilingual Lexical Data Base
under HyperCard: PARAX. Research Report, GETA,
IMAG (UJF & CNRS), Aug. 1994, 10 p.
Connolly, Dan (1997) XML Principles, Tools and
Techniques World Wide Web Journal, Volume 2,
Issue 4, Fall 1997, O'REILLY & Associates, 250 p.
Ide, N. & Veronis, J. (1995) Text Encoding Initiative,
background and context. Kluwer Academic
Publishers, 242 p.
Mangeot-Lerebours M. (2000) Papillon Lexical
Database Project: Monolingual Dictionaries &
Interlingual Links. Proc. of 7th Workshop on
Advanced Information Network and System Pacific
Association for Computational Linguistics 1997
Conference (WAINS'7), Bangkok, Thailande, 7-8
d?cembre 2000, Kasetsart University, 6 p.
Mangeot-Lerebours M. (2001) Environnements
centralis?s et distribu?s pour lexicographes et
lexicologues en contexte multilingue. Nouvelle th?se,
Universit? Joseph Fourier (Grenoble I), 27 September
2001, 280 p.
Mel?tchuk I., Clas A. & Polgu?re A. (1995)
Introduction ? la lexicologie explicative et
combinatoire. AUPELF-UREF/Duculot, Louvain-la-
Neuve, 256 p.
Polgu?re, A. (2000) Towards a theoretically-motivated
general public dictionary of semantic derivations
and collocations for French. Proc. EURALEX'2000,
Stuttgart, pp 517-527.
S?rasset G. (1994a) Interlingual Lexical Organisation
for Multilingual Lexical Databases. Proc. of 15th
International Conference on Computational
Linguistics, COLING-94, 5-9 Aug. 1994, 6 p.
S?rasset G. (1994b) SUBLIM, un syst?me universel de
bases lexicales multilingues; et NADIA, sa
sp?cialisation aux bases lexicales interlingues par
acceptions. Nouvelle th?se, UJF (Grenoble 1), d?c.
1994.
S?rasset G. (1997) Le projet NADIA-DEC : vers un
dictionnaire explicatif et combinatoire informatis? ?
Proc. of La m?moire des mots, 5?me journ?es
scientifiques du r?seau LTT, Tunis, 25-27 septembre
1997, AUPELF?UREF, 7 p.
S?rasset G. & Mangeot-Lerebours M. (2001) Papillon
Lexical Database Project: Monolingual Dictionaries
& Interlingual Links. Proc. NLPRS'2001,
Hitotsubashi Memorial Hall, National Center of
Sciences, Tokyo, Japan, 27-30 November 2001, vol
1/1, pp. 119-125.
Tomokiyo M., Mangeot-Lerebours M. & Planas E.
(2000) Papillon : a Project of Lexical Database for
English, French and Japanese, using Interlingual
Links. Proc. of Journ?es des Sciences et Techniques
de l'Ambassade de France au Japon, Tokyo, Japon,
13-14 novembre 2000, Ambassade de France au
Japon, 3 p.
-o-o-o-o-o-o-o-o-o-
Coling 2010: Poster Volume, pages 791?800,
Beijing, August 2010
Finite-state Scriptural Translation 
M. G. Abbas Malik Christian Boitet Pushpak Bhattacharyya 
GETALP ? LIG (Grenoble Informatics Lab.) 
University of Grenoble 
 
IIT Bombay 
Abbas.Malik Christian.Boitet@imag.fr pb@iitb.ac.in 
  
 
Abstract 
We use robust and fast Finite-State Machines 
(FSMs) to solve scriptural translation prob-
lems. We describe a phonetico-morphotactic 
pivot UIT (universal intermediate transcrip-
tion), based on the common phonetic reposito-
ry of Indo-Pak languages. It is also extendable 
to other language groups. We describe a finite-
state scriptural translation model based on fi-
nite-state transducers and UIT. We report its 
performance on Hindi, Urdu, Punjabi and Se-
raiki corpora. For evaluation, we design two 
classification scales based on the word and 
sentence accuracies for translation system 
classifications. We also show that subjective 
evaluations are vital for real life usage of a 
translation system in addition to objective 
evaluations. 
1 Introduction 
Transliteration refers to phonetic translation 
across two languages with different writing sys-
tems, such as Arabic to English (Arbabi et al, 
1994; Stall and Knight, 1998; Al-Onaizan and 
Knight, 2002; AbdulJaleel and Larkey, 2003). 
Most prior work on transliteration has been done 
for MT of English, Arabic, Japanese, Chinese, 
Korean, etc., for CLIR (Lee and Choi., 1998; 
Jeong et al, 1999; Fujii and Ishikawa, 2001; 
Sakai et al, 2002; Pirkola et al, 2003; Virga and 
Khudanpur, 2003; Yan et al, 2003), and for the 
development of multilingual resources (Kang 
and Choi, 2000; Yan, Gregory et al, 2003). 
The terms transliteration and transcription are 
often used as generic terms for various processes 
like transliteration, transcription, romanization, 
transcribing and technography (Halpern, 2002). 
In general, the speech processing community 
uses the term transcription to denote a process of 
conversion from the script or writing system to 
the sound (phonetic representation). For exam-
ple, the transcription of the word ?love? in the 
International Phonetic Alphabet (IPA) is [l?v]. 
While the text processing community uses the 
term transliteration and defines it as a process of 
converting a word written in one writing system 
into another writing system while preserving the 
sound of the original word (Al-Onaizan and 
Knight, 2002; AbdulJaleel and Larkey, 2003). 
More precisely, the text processing community 
defines the term transliteration as two transcrip-
tion processes ?source script to sound transcrip-
tion? and ?sound to target script transcription? 
and sometimes as one transcription process 
?source script to target script transcription?. 
We propose a new term Scriptural Translation 
for this combined process. Scriptural translation 
is a process of transcribing a word written in the 
source language script into the target language 
script by preserving its articulation in the original 
language in such a way that the native speaker of 
the target language can produce the original pro-
nunciation. 
FSMs have been successfully used in various 
domains of Computational Linguistics and Natu-
ral Language Processing (NLP). The successful 
use of FSMs have already been shown in various 
fields of computational linguistics (Mohri, 1997; 
Roche and Schabes, 1997; Knight and Al-
Onaizan, 1998). Their practical and advanta-
geous features make them very strong candidates 
to be used for solving scriptural translation 
problems. 
First, we describe scriptural translation and 
identify its problems that fall under weak transla-
tion problems. Then, we analyze various chal-
lenges for solving weak scriptural translation 
problems. We describe our finite-state scriptural 
translation model and report our results on Indo-
Pak languages. 
791
2 Scriptural Translation ? a weak 
translation problem 
A weak translation problem is a translation prob-
lem in which the number of possible valid trans-
lations, say N, is either very small, less than 5, or 
almost always 1. 
Scriptural Translation is a sub-problem of 
general translation and almost always a weak 
translation problem. For example, French-IPA 
and Hindi-Urdu scriptural translation problems 
are weak translation problems due to their small 
number of valid translations. On the other hand, 
Japanese-English and French-Chinese scriptural 
translation problems are not weak. 
Scriptural translation is not only vital for 
translation between different languages, but also 
becomes inevitable when the same language is 
written in two or more mutually incomprehensi-
ble scripts. For example, Punjabi is written in 
three different scripts: Shahmukhi (a derivation 
of the Perso-Arabic script), Gurmukhi and Deva-
nagari. Kazakh and Kurdish are also written in 
three different scripts, Arabic, Latin and Cyrillic. 
Malay has two writing systems, Latin and Jawi 
(a derivation of the Arabic script), etc. Figure 1 
shows an example of scriptural divide between 
Hindi and Urdu. 
6 ?3[ 3e ?? ??Z ?? ]gzu[? X? 
?????? ?? ??? ?? ???? ??? 
[??n?j? ko ?m?n ki z?rur?? h?.] 
The world needs peace. 
Figure 1: Example of scriptural divide 
Thus, solving the scriptural translation prob-
lem is vital to bridge the scriptural divide be-
tween the speakers of different languages as well 
as of the same language. 
Punjabi, Sindhi, Seraiki and Kashmiri exist on 
both sides of the common border between India 
and Pakistan and all of them are written in two or 
more mutually incomprehensible scripts. The 
Hindi?Urdu pair exists both in India and Pakis-
tan. We call all these languages the Indo-Pak 
languages. 
3 Challenges of Scriptural Translation 
In this section, we describe the main challenges 
of scriptural translation. 
3.1 Scriptural divide 
There exists a written communication gap be-
tween people who can understand each other 
verbally but cannot read each other. They are 
virtually divided and become scriptural aliens. 
Examples are the Hindi & Urdu communities, 
the Punjabi/Shahmukhi & Punjabi/Gurmukhi 
communities, etc. An example of scriptural di-
vide is shown in Figure 1. Such a gap also ap-
pears when people want to read some foreign 
language or access a bilingual dictionary and are 
not familiar with the writing system. For exam-
ple, Japanese?French or French?Urdu dictiona-
ries are useless for French learners because of the 
scriptural divide. Table 1 gives some figures on 
how this scriptural divide affects a large popula-
tion of the world. 
Sr. Language Number of Speakers 
1 Hindi 853,000,000 
2 Urdu 164,290,000 
3 Punjabi 120,000,000 
4 Sindhi 21,382,120 
5 Seraiki 13,820,000 
6 Kashmir 5,640,940 
Total 1178,133,060 
Table 1: Number of Speakers of Indo-Pak languages 
3.2 Under-resourced languages 
Under-resourced and under-written features of 
the source or target language are the second big 
challenge for scriptural translation. The lack of 
standard writing practices or even the absence of 
a standard code page for a language makes trans-
literation or transcription very hard. The exis-
tence of various writing styles and systems for a 
language leads towards a large number of va-
riants and it becomes difficult and complex to 
handle them. 
In the case of Indo-Pak languages, Punjabi is 
the largest language of Pakistan (more than 70 
million) and is more a spoken language than a 
written one. There existed only two magazines 
(one weekly and one monthly) in 1992 (Rahman, 
1997). In the words of (Rahman, 2004), ?? 
there is little development in Punjabi, Pashto, 
Balochi and other languages??. (Malik, 2005) 
reports the first effort towards establishing a 
standard code page for Punjabi-Shahmukhi and 
till date, a standard code page for Shahmukhi 
does not exist. Similar problems also exist for the 
Kashmiri and Seraiki languages. 
792
3.3 Absence of necessary information 
There are cases where the necessary and indis-
pensable information for scriptural translation 
are missing in the source text. For example, the 
first word ???? [??n?j?] (world) of the example sen-
tence of Figure 1 misses crucial diacritical in-
formation, mandatory to perform Urdu to Hindi 
scriptural translation. Like in Arabic, diacritical 
marks are part of the Urdu writing system but are 
sparingly used in writings (Zia, 1999; Malik et 
al., 2008; Malik et al, 2009). 
Figure 2(a) shows the example word without 
diacritical marks and its wrong Hindi conversion 
according to conversion rules (explained later). 
The Urdu community can understand the word in 
its context or without the context because people 
are tuned to understand the Urdu text or word 
without diacritical marks, but the Hindi conver-
sion of Figure 2(a) is not at all acceptable or 
readable in the Hindi community. 
Figure 2(b) shows the example word with dia-
critical marks and its correct Hindi conversion 
according to conversion rules. Similar problems 
also arise for the other Indo-Pak languages. 
Therefore, missing information in the source text 
makes the scriptural translation problem compu-
tationally complex and difficult. 
 ? = ?? ????]?]  ?[? [ ?]n]  ?[? [ ?]j [ ?]?[ 
??????  = ?  ]? [ ??  ]? [ ?  ]n [ ??  ]? [ ?  ]j [ ??  ]?[  
(b) with necessary information 
 ? = ????]? [ ?]n [ ?]j [ ?]?[ 
????   = ?  ]? [ ?  ]n [ ?  ]j [ ??  ]?[  
(a) without necessary information 
Figure 2: Example of missing information 
3.4 Different spelling conventions 
Different spelling conventions exist across dif-
ferent scripts used for the same language or for 
different languages because users of a script are 
tuned to write certain words in a traditional way. 
For example, the words ?? [je] (this) = ? [j] + ? [h] 
and ?? [vo] (that) = ? [v] + ? [h] are used in Urdu 
and Punjabi/Shahmukhi. The character ? [h] pro-
duces the vowel sounds [e] and [o] in the exam-
ple words respectively. On the other hand, the 
example words are written as ?? [je] & ?? [vo] and 
? ? [je] & ?? [vo] in Devanagari and Gurmukhi, 
respectively. There exist a large number of such 
conventions between Punjabi/Shahmukhi?
Punjabi Gurmukhi, Hindi?Urdu, etc. 
Different spelling conventions are also driven 
by different religious influences on different 
communities. In the Indian sub-continent, Hindi 
is a part of the Hindu identity, while Urdu is a 
part of the Muslim identity1 (Rahman, 1997; Rai, 
2000). Hindi derives its vocabulary from San-
skrit, while Urdu borrows its literary and scien-
tific vocabulary from Persian and Arabic. Hindi 
and Urdu not only borrow from Sanskrit and Per-
sian/Arabic, but also adopt the original spellings 
of the borrowed word due the sacredness of the 
original language. These differences make scrip-
tural translation across scripts, dialects or lan-
guages more challenging and complex. 
3.5 Transcriptional ambiguities 
Character level scriptural translation across dif-
ferent scripts is ambiguous. For example, the 
Sindhi word ?????? [??s?n] (human being) can be 
converted into Devanagari either as ????? [??s?n] or 
????* [?ns?n] (* means wrong spellings). The trans-
literation process of the example word from 
Sindhi to Devanagari is shown in Figure 3(a). 
The transliteration of the third character from the 
left, Noon (?) [n], is ambiguous because in the 
middle of a word, Noon may represent a conso-
nant [n] or the nasalization [?] of a vowel. 
 
Figure 3: Sindhi transliteration example 
In the reverse direction, the Sindhi Devanagari 
word ????? [??s?n] can be converted into a set of 
possible transliterations [?????? ,*?????? ,??????*]. All 
these possible transliterations have the same pro-
nunciation [??s?n] but have different spellings in 
                                                 
1 The Hindi movement of the late 19th century played 
a central role in the ideologization of Hindi. The 
movement started in reaction to the British Act 29 of 
1837 by which Persian was replaced by Hindusta-
ni/Urdu, written in Persian script, as the official ver-
nacular of the courts of law in North India. It is the 
moment in history, when Hindi and Urdu started to 
emerge as Hindu and Muslim identities. 
793
the Perso-Arabic script, as shown in Figure 3(b). 
Similar kinds of ambiguities also arise for other 
pairs of scripts, dialects or languages. Thus these 
ambiguities increase the complexity and difficul-
ty of scriptural translation. 
3.6 Distinctive sound inventories 
Sound inventories across dialects or languages 
can be different. Consider the English?Japanese 
pair. Japanese make no distinction between the 
?L? [l] and ?R? [r] sounds so that these two Eng-
lish sounds collapse onto the same Japanese 
sound (Knight and Al-Onaizan, 1998). 
For Indo-Pak languages, Punjabi/Gurmukhi (a 
dialect of Punjabi spoken in India) possesses two 
additional sounds than Punjabi/Shahmukhi (a 
dialect of Punjabi spoken in Pakistan). Similarly, 
Hindi, Punjabi, Sindhi and Seraiki have the re-
troflex form [?], but Urdu and Kashmiri do not. 
Marathi has 14 vowels in contrast to Hindi?s 11 
vowels, shown in Table 2. 
Hindi Vowels 
? [?] ? [?] ? [?] ? [i] ? [?] ? [u] ? [r]? ? [e] ? [?] 
? [o] ? [?] 
Marathi Vowels 
? [?] ? [?] ? [?] ? [i] ? [?] ? [u] ? [r]? ? [e] ? [?] 
? [o] ? [?] ?? [??] ?? [?h] ? [l]? 
Table 2: Hindi and Marathi vowel comparison 
Scriptural translation approximates the pro-
nunciation of the source language or dialect in 
the target due to different sound inventories. 
Thus a distinctive sound inventory across scripts, 
dialects or languages increases ambiguities and 
adds to the complexity of the scriptural transla-
tion problem. 
4 Universal Intermediate Transcription 
UIT (Universal Intermediate Transcription) is a 
multipurpose pivot. In the current study, it is 
used as a phonetico-morphotactic pivot for the 
surface morphotactic translation or scriptural 
translation. 
Although we have not used IPA as encoding 
scheme, we have used the IPA coding associated 
with each character as the encoding principle for 
our ASCII encoding scheme. We selected the 
printable ASCII characters to base the UIT en-
coding scheme because it is universally portable 
to all computer systems and operating systems 
without any problem (Boitet and Tch?ou, 1990; 
Hieronymus, 1993; Wells, 1995). UIT is a de-
terministic and unambiguous scheme of tran-
scription for Indo-Pak languages in ASCII range 
32?126, since a text in this rage is portable 
across computers and operating systems 
(Hieronymus, 1993; Wells, 1995). 
Speech Assessment Methods Phonetic Alpha-
bet (SAMPA)2 is a widely accepted scheme for 
encoding IPA into ASCII. The purpose of SAM-
PA was to form the basis of an international 
standard machine-readable phonetic alphabet for 
the purpose of international collaboration in 
speech research (Wells, 1995). The UIT encod-
ing of Indo-Pak languages is developed as an 
extension of the SAMPA and X-SAMPA that 
covers all symbols on the IPA chart (Wells, 
1995). 
4.1 UIT encodings 
All characters of the Indo-Pak languages are 
subdivided into three categories, consonants, 
vowels and other symbols (punctuations and di-
gits). 
Consonants are further divided into aspirated 
consonants and non-aspirated consonants. For 
aspiration, in phonetic transcription a simple ?h? 
following the base consonant symbol is consi-
dered adequate (Wells, 1995). In the Indo-Pak 
languages, we have two characters with IPA [h]. 
Thus to distinguish between the ?h? consonants 
and the aspiration, we use underscore ?_? to 
mark the aspirate and we encode an aspiration as 
?_h?. For example, the aspirated consonants J[J 
[??], J ?J [p?] and J ?Y [??] of the Indo-Pak languages 
are encoded as ?t`_h?, ?p_h? and ?t_S_h? respec-
tively. Similarly for the dental consonants, we 
use the ?_d? marker. For example, the characters 
? [?] and ? [?] are encoded as ?d_d? and ?t_d? in 
UIT. Table 3 shows the UIT encodings of Hindi 
and Urdu aspirated consonants. 
Hindi Urdu UIT Hindi Urdu UIT 
? J [J [b?] b_h ??? |g [r?] r_h 
? J ?J [p?] p_h ? |g [??] r`_h 
? J[J [??] t_d_h ? J? [k?] k_h 
? J[J [??] t`_h ? J? [g?] g_h 
? J [Y [??] d_Z_h ?? J? [l?] l_h 
? J ?Y [??] t_S_h ?? Jb [m?] m_h 
                                                 
2 http://www.phon.ucl.ac.uk/home/sampa/ 
794
? |e [??] d_d_h ?? J [J [n?] n_h 
? |e [??] d`_h    
Table 3: UIT encodings of Urdu aspirated consonants 
Similarly, we can encode all characters of In-
do-Pak languages. Table 4 gives UIT encodings 
of Hindi and Urdu non-aspirated consonants. We 
cannot give all encoding tables here due to short-
age of space. 
Hindi Urdu UIT Hindi Urdu UIT 
? ? [b] b ? ? [s] s2 
? ? [p] p ? ? [z] z2 
? ? [?] t_d ? ? [?] t_d1 
? ? [?] t` ? ? [z] z3 
? ? [s] s1 - ? [?] ? 
? ? [?] d_Z ? ? [?] X 
? ? [?] t_S ? ? [f] f 
? ? [h] h1 ? ? [q] q 
? ? [x] x ? ? [k] k 
? ? [?] d_d ? ? [g] g 
? ? [?] d` ? ? [l] l 
? ? [z] z1 ? ? [m] m 
? ? [r] r ? ? [n] n 
? ? [?] r` ? ? [v] v 
? ? [z] z ? ? [h] h 
? ? [?] Z ? ? [j] j 
? ? [s] s ? ? [?] t_d2 
? ? [?] S ? - [?] n` 
? ? [?] S1 ?? ? [?] ~ 
Table 4: UIT encodings of Urdu non-aspirated conso-
nants 
5 Finite-state Scriptural Translation 
Model 
Figure 4 shows the system architecture of our 
finite-state scriptural translation system. 
Text Tokenizer receives and converts the 
input source language text into constituent words 
or tokens. This list of the source language tokens 
is then passed to the UIT Encoder that en-
codes these tokens into a list of UIT tokens using 
the source language to UIT conversion transduc-
er from the repertoire of Finite-State Transduc-
ers. These UIT tokens are given to the UIT De-
coder that decodes them into target language 
tokens using the UIT to target language conver-
sion transducer from the repertoire of Transduc-
ers. Finally, Text Generator generates the 
target language text from the translated target 
language tokens. 
 
Figure 4: System Architecture of fintie-state scriptural 
translation 
5.1 Finite-state Transducers 
Both conversions of the source language text into 
the UIT encoded text and from the UIT encoded 
text into the target language text are regular rela-
tions on strings. Moreover, regular relations are 
closed under serial composition and a finite set 
of conversion relations when applied to each 
other?s output in a specific order, also defines a 
regular expression (Kaplan and Kay, 1994). Thus 
we model the conversions from the source lan-
guage to UIT and from UIT to the target lan-
guage as finite-state transducers. These transla-
tional transducers can be deterministic and non-
deterministic. 
Character Mappings: Table 5 shows regular 
relations for converting Hindi and Urdu aspirated 
consonants into UIT. 
IPA Hindi to UIT Urdu to UIT 
b? ? ? b_h J [J ? b_h 
p? ? ? p_h J ?J ? p_h 
?? ? ? t_d_h J[J ? t_d_h 
?? ? ? t`_h J[J ? t`_h 
?? ? ? d_Z_h J [Y ? d_Z_h 
?? ? ? t_S_h J ?Y ? t_S_h 
795
?? ? ? d_d_h |e ? d_d_h 
?? ? ? d`_h |e ? d`_h 
r? ??? ? r_h |g ? r_h 
?? ? ? r`_h |g ? r`_h 
k? ? ? k_h J? ? k_h 
g? ? ? g_h J? ? g_h 
l? ?? ? l_h J? ? l_h 
m? ?? ? m_h Jb ? m_h 
n? ?? ? n_h J [J ? n_h 
Table 5: Regular rules for aspirated consonants of 
Hindi and Urdu 
By interchanging the UIT encodings before 
the arrow sign and the respective characters of 
Hindi and Urdu after the arrow, we can construct 
regular conversion relations from UIT to Hindi 
and Urdu. We have used XFST (Xerox finite-
state engine) to build finite-state transducers. 
Table 6 shows a sample XFST code. 
Contextual Mappings: A contextual mapping 
is a contextual rule that determines a desired out-
put when a character appears in a certain context. 
The third command of Table 6 models another 
contextual mapping saying that ????? is translated 
by ?_h? when it is preceded by any of the charac-
ters ?, ?, ?, and ?. The second last rule of Table 6 
models the contextual mapping rule that ?A1? is 
translated into ??? when it is at the end of a word 
and preceded by a consonant. 
clear stack 
set char-encoding UTF-8 
read regex [?? -> I]; 
read regex [? -> [k "_" h], ? -> [g 
"_" h], ? -> [t "_" S "_" h], ? -
> [d "_" Z "_" h], ? -> [t "`" "_" 
h], ? -> [d "`" "_" h], ? -> [t 
"_" d "_" h], ? -> [d "_" d "_" 
h], ? -> [p "_" h], ? -> [b "_" 
h], ? -> [r "`" "_" h], ? -> s, ?
-> [t "_" d], ? -> r, ? -> l, ? -> 
m, ? -> n, ? -> v, ? -> h]; 
read regex [[?? ?] -> ["_" h] || [? | 
? | ? | ?] _ ]; 
compose net 
Table 6: Sample XFST code 
Vowel representations in Urdu, Punja-
bi/Shahmukhi, Sindhi, Seraiki/Shahmukhi and 
Kashmiri are highly context-sensitive (Malik et 
al., 2010). 
6 Experiments and Results 
A sample run of our finite-state scriptural trans-
lation system on the Hindi to Urdu example sen-
tence of Figure 1 is shown in Table 7. 
Text 
Tokenizer 
UIT 
Encoder 
UIT Decoder 
Unique 
output 
Ambiguous 
outputs 
?????? dUnIjA1 ?????? , ?????? ] ?????? ]  
?? ko ?? , ?? ] ?? ] 
??? @mn ??? ] ??? ] 
?? ki ?? , ?? ] ?? ] 
???? zrurt_d ?????  [ ????? , 
?????? , 
?????? , 
?????? , 
?????? , 
?] 
 
?? 
 
h{  
??? , ?? ] ??? ] 
Table 7: Sample run of finite-state scriptural transla-
tion model on Hindi to Urdu example 
Text Generator converts the unique out-
put of the UIT Decoder into an Urdu sentence 
with one error in the fifth word (highlighted), 
shown in Figure 5. 
? ]gzEgi ?? ??Z ?? 6 ?3G[ 3 Ee 
Figure 5: Unique output of the sample run by deter-
ministic FSTs 
On the other hand, from the ambiguous output 
of the UIT Decoder, we can generate 240 output 
sentences, but only one is the correct scriptural 
translation of the source Hindi sentence in Urdu. 
The correct sentence is shown in Figure 6. The 
sole difference between the output of the deter-
ministic FST and the correct scriptural transla-
tion is highlighted in both sentences shown in 
Figure 5 and 6. 
? ]gzEu [? ?? ??Z ?? 6 ?3G[ 3 Ee 
Figure 6: Correct scriptural translation of the example 
6.1 Test Data 
Table 8 shows test sets for the evaluation of our 
finite-state scriptural translation system. 
796
Data 
set Language pair 
No. of 
words 
No. of 
sentences Source 
HU 
1 Hindi?Urdu 52,753 - 
Platts 
dictionary
HU 
2 Hindi?Urdu 4,281 200 
Hindi 
corpus 
HU 
3 Hindi?Urdu 4,632 226 
Urdu 
corpus 
PU Punjabi/Shahmukhi?Punjabi/Gurmukhi 5,069 500 
Classical 
poetry 
SE Seraiki/Shahmukhi?Seraiki/Devanagari 2,087 509 
Seraiki 
poetry 
Table 8: Test Sets of Hindi, Urdu, Punjabi and Seraiki 
HU 1 is a word list obtained from the Platts 
dictionary3 (Platts, 1884). 
6.2 Results 
For Hindi to Urdu scriptural translation, we have 
applied the finite-state model to all Hindi inputs 
of HU Test sets 1, 2 and 3. In general, it gives us 
an Urdu output with the necessary diacritical 
marks. To evaluate the performance of Hindi to 
Urdu scriptural translation of our finite-state sys-
tem against the Urdu without diacritics, we have 
created a second Urdu output by removing all 
diacritical marks from the default Urdu output of 
the finite-state system. We have calculated the 
Word Accuracy Rate (WAR) and Sentence Accu-
racy Rate (SAR) for the default and the 
processed Urdu outputs by comparing them with 
the Urdu references with and without diacritics 
respectively. To compute WAR and SAR, we 
have used the SCLITE utility from the Speech 
Recognition Scoring Toolkit (SCTK)4 of NIST. 
The results of Hindi to Urdu scriptural transla-
tion are given in Table 24. 
Test Set 
Default output Processed output 
Word 
Level 
Sentence 
Level 
Word 
Level 
Sentence 
Level 
HU 1 32.5% - 78.9% - 
HU 2 90.8% 26.5% 91.0% 27% 
HU 3 81.2% 8.8% 82.8% 9.7% 
Table 9:Hindi to Urdu scriptural translation restuls 
The finite-state scriptural translation system 
for Hindi to Urdu produces an Urdu output with 
diacritics. However, we know that the Urdu 
community is used to see the Urdu text without 
diacritics. Thus, we removed all diacritical marks 
from the Urdu output text that is more acceptable 
to the Urdu community. By this post-processing, 
                                                 
3 Shared by University of Chicago for research pur-
poses. 
4 http://www.itl.nist.gov/iad/mig//tools/ 
we gain more than 40% accuracy in case of HU 
Test Set 1. We also gain in accuracy for the other 
test sets. 
For the classification of our scriptural transla-
tion systems, we have devised two scales. One 
corresponds to the word accuracy rate and the 
other corresponds to the sentence level accuracy. 
They are shown in Figure 7 and 8. 
 
Figure 7: Classification scale based on the word 
accuracy rate for scriptural transaltion 
 
Figure 8: Classification scale based on the sentence 
accucary rate for scriptural translation 
According to the scale of Figure 7 and 8, the 
Hindi to Urdu scriptural translation system is 
classified as ?Good? and ?Good Enough?, respec-
tively. 
The subjective evaluations like usability, ef-
fectiveness and adequacy depend on several fac-
tors. A user with a good knowledge of Hindi and 
Urdu languages would rate our Hindi to Urdu 
system quite high and would also rate the Urdu 
output very usable. Another user who wants to 
read a Hindi text, but does not know Hindi, 
would also rate this system and the Urdu output 
quite high and very usable respectively, because 
it serves its purpose. 
On the other hand, a user who wants to pub-
lish a Hindi book in Urdu, would rate this system 
not very good. This is because he has to localize 
the Hindi vocabulary of Sanskrit origin as the 
acceptance of the Hindi vocabulary in the Urdu 
797
community, target of his published book, is very 
low. Thus the subjective evaluation depends on 
various factors and it is not easy to compute such 
measures for the evaluation of a scriptural trans-
lation system, but they are vital in real life. 
For Urdu to Hindi scriptural translation, we 
have two inputs for each HU Test Set. One input 
contains all diacritical marks and the other does 
not contain any. On Hindi side, we have a single 
Hindi reference with which we will compare 
both Hindi outputs. We already know that it will 
give us less accuracy rates for the Urdu input 
without diacritical marks that are mandatory for 
correct Urdu to Hindi scriptural translation. The 
results for Urdu to Hindi scriptural translation 
are given in Table 10. 
Test Set 
With diacritics Without diacritics 
Word 
Level 
Sentence 
Level 
Word 
Level 
Sentence 
Level 
HU 1 68.0% - 31.2% - 
HU 2 83.9% 10% 53.0% 1% 
HU 3 98.4% 73.9% 58.9% 0.4% 
Table 10: Urdu to Hindi scriptural translation results 
For the Urdu input with diacritics, the accura-
cy of the Urdu to Hindi finite-state scriptural 
translation system is 83.9% at word level for HU 
Test Set 2 and it is classified as ?GOOD? the 
classification scale of Figure 7. On the other 
hand, it shows a sentence-level accuracy of 10% 
for the same test set and is classified as ?AVER-
AGE? by the classification scale of Figure 8. 
For the Urdu input without diacritics, the Urdu 
to Hindi scriptural translation system is classified 
as ?OK? by the scale of Figure 7 for HU Test set 
2 and 3. It is classifies as ?NULL? for HU Test 
Set 1. According to the scale of Figure 8, it is 
classified as ?NULL? for all three test sets. 
For Punjabi scriptural translation, we also de-
veloped two types of output default and 
processed for Gurmukhi to Shahmukhi transla-
tion. In the reverse direction, it has two types of 
inputs, one with diacritics and the other without 
diacritics. Table 11 and 12 shows results of Pun-
jabi scriptural translation. 
Test Set 
Default output Processed output 
Word 
Level 
Sentence 
Level 
Word 
Level 
Sentence 
Level 
PU 84.2% 27.8% 85.2% 29.9% 
Table 11: Gurmukhi to Shahmukhi scriptural transla-
tion results 
 
 
Test Set 
With diacritics Without diacritics 
Word 
Level 
Sentence 
Level 
Word 
Level 
Sentence 
Level 
PU 98.8% 90.3% 67.3% 6.4% 
Table 12: Shahmukhi to Gurmukhi scriptural 
translation results 
Compared to the Hindi?Urdu pair, the Punja-
bi/Shahmukhi?Punjabi/Gurmukhi pair is compu-
tationally less hard. The post-processing to the 
default out of the finite-state scriptural transla-
tion systems for Punjabi/Gurmukhi to Punja-
bi/Shahmukhi also helps to gain an increase of 
approximately 1% and 2% at word and sentence 
levels respectively. The Shahmukhi to Gurmukhi 
scriptural translation system is classified as 
?GOOD? by both scales of Figure 7 and 8. Thus 
the usability of the Punjabi finite-state scriptural 
translation system is higher than the Hindi?Urdu 
finite-state scriptural translation system. 
In the reverse direction, the Shahmukhi to 
Gurmukhi scriptural translation system gives an 
accuracy of 98.8% and 67.3% for the Shahmukhi 
input text with and without diacritics respective-
ly. For the Shahmukhi input text with diacritics, 
the scriptural translation system is classified as 
?EXCELLENT? by both scales. On the other 
hand, it is classified as ?NULL? according to the 
scale of Figure 8 for the Shahmukhi input text 
without diacritical marks. 
Similar to Hindi?Urdu and Punjabi finite-state 
scriptural translation, we have applied our finite-
state system to the Seraiki test set. Here again, 
we have developed a processed Serai-
ki/Shahmukhi output from the default output of 
our finite-state system by removing the diacrit-
ics. The results are given in Table 13 and 14. 
Test Set 
Default output Processed output 
Word 
Level 
Sentence 
Level 
Word 
Level 
Sentence 
Level 
SE 81.3% 19.4% 83.7% 20.3% 
Table 13: Seraiki/Devanagari to Seraiki/Shahmukhi 
scriptural translation results 
 
Test Set 
With diacritics Without diacritics 
Word 
Level 
Sentence 
Level 
Word 
Level 
Sentence 
Level 
SE 95.2% 76.4% 58.6% 8.6% 
Table 14: Seraiki/Shahmukhi to Seraiki/Devanagari 
scriptural translation results 
In the case of the Seraiki/Devanagari to Serai-
ki/Shahmukhi scriptural translation system, the 
post-processing also helps to gain an increase in 
word accuracy of approximately 1 to 2 percent 
798
both at the word and the sentence levels. The 
accuracy for both the default and the processed 
Seraiki/Shahmukhi outputs is also more than 
80% at word level. The system is classified as 
?GOOD? and ?GOOD ENOUGH? according to 
the scale of Figure 7 and 8 respectively. 
The absence of diacritical marks in the Serai-
ki/Shahmukhi has a very bad effect on the accu-
racy of the finite-state scriptural translation sys-
tem. The scriptural translation system is classi-
fied as ?NULL? for the Seraiki/Shahmukhi input 
text without diacritics. 
7 Conclusion 
Finite-state methods are robust and efficient to 
implement scriptural translation rules in a very 
precise and compact manner. 
The missing information and the diacritical 
marks in the source text proved to be very criti-
cal, crucial and important for achieving high and 
accurate results. The above results support our 
hypothesis that lack of important information in 
the source texts considerably lowers the quality 
of scriptural translation. They are crucial and 
their absence in the input texts decreases the per-
formance considerably, from more than 80% to 
less than 60% at word level. Thus restoration of 
the missing information and the diacritical marks 
or reducing the effect of their absence on the 
scriptural translation is one of the major ques-
tions for further study and work. 
In general, only word accuracy rates are re-
ported. We have observed that only word accura-
cy rates may depict a good performance, but the 
performance of the same system at sentence-
level may be not very good. Therefore, subjec-
tive evaluations and usage of translation results 
in real life should also be considered while eva-
luating the translation quality. 
Acknowledgments 
This study is supported by Higher Education Com-
mission (HEC), Government of Pakistan under its 
overseas PhD scholarship scheme. We are also thank-
ful to Digital South Asian Library, University of Chi-
cago for sharing Platts dictionary data (Platts, 1884). 
References  
AbdulJaleel, N. and L. S. Larkey. 2003. Statistical 
Transliteration for English-Arabic Cross Language 
Information Retrieval. 12th international 
Conference on information and Knowledge 
Management (CIKM 03), New Orleans. 139-146. 
Al-Onaizan, Y. and K. Knight. 2002. Machine 
Transliteration of Names in Arabic Text. 
Workshop on Computational Approaches To 
Semitic Languages, the 40th Annual Meeting of 
the ACL, Philadelphia, Pennsylvania, 1-13. 
Arbabi, M., S. M. Fischthal, V. C. Cheng and E. Bart. 
1994. Algorithms for Arabic Name 
Transliteration. IBM J. Res. Dev. 38(2): 183-193. 
Boitet, C. and F. X. Tch?ou. 1990. On a Phonetic and 
Structural Encoding of Chinese Characters in 
Chinese texts. ROCLING III, Taipeh. 73-80. 
Fujii, A. and T. Ishikawa. 2001. Japanese/English 
Cross-Language Information Retrieval: 
exploration of query translation and transliteration. 
Computers and the Humanities 35(4): 389-420. 
Halpern, J. 2002. Lexicon-based Orthographic 
Disambiguation in CJK Intelligent Information 
Retrieval. 3rd workshop on Asian language 
resources and international standardization, the 
19th International Conference on Computational 
Linguistics (COLING), Taipei, Taiwan. 1-7. 
Hieronymus, J. 1993. ASCII Phonetic Symbols for the 
World's Languages: Worldbet. AT&T Bell 
Laboratories. 
Jeong, K. S., S. H. Myaeng, J. S. Lee and K.-S. Choi. 
1999. Automatic Identification and Back-
transliteration of Foreign Words for Information 
Retrieval. Information Processing and 
Management 35: 523-540. 
Kang, B. and K. Choi. 2000. Automatic 
Transliteration and Back Transliteration by 
Decision Tree Learning. 2nd International 
Conference on Evaluation and Language 
Resources (ELRC), Athens. 
Kaplan, R. M. and M. Kay. 1994. Regular Models of 
Phonological Rule Systems.  20(3). 
Knight, K. and Y. Al-Onaizan. 1998. Translation with 
Finite-State Devices 3rd Conference of the 
Association for Machine Translation in the 
Americas on Machine Translation and the 
Information Soup (AMTA-98), Pennsylvania. 
421-437. 
Lee, J. S. and K. S. Choi. 1998. English to Korean 
Statistical Transliteration for Information 
Retrieval. Computer Processing of Oriental 
languages 12(1): 17-37. 
Malik, M. G. A. 2005. Towards a Unicode 
Compatible Punjabi Character Set. 27th 
Internationalization and Unicode Conference, 
Berlin. 
Malik, M. G. A., L. Besacier, C. Boitet and P. 
Bhattacharyya. 2009. A Hybrid Model for Urdu 
Hindi Transliteration. Joint conference of the 47th 
Annual Meeting of the Association of 
Computational Linguistics and the 4th 
799
International Joint Conference on Natural 
Language Processing of the Asian Federation of 
NLP ACL/IJCNLP Workshop on Named Entities 
(NEWS-09), Singapore, 177?185. 
Malik, M. G. A., C. Boitet and P. Bhattacharyya. 
2008. Hindi Urdu Machine Transliteration using 
Finite-state Transducers. 22nd International 
Conference on Computational Linguistics 
(COLING), Manchester, 537-544. 
Malik, M. G. A., C. Boitet and P. Bhattacharyya. 
2010. Analysis of Noori Nast'aleeq for Major 
Pakistani Languages. 2nd Workshop on Spoken 
Language Technologies for Under-resourced 
Languages SLTU-2010, Penang, Malaysia. 
Mohri, M. 1997. Finite-state Transducers in Language 
and Speech Processing.  23(2). 
Pirkola, A., J. Toivonen, H. Keskustalo, K. Visala and 
K. J?rvelin. 2003. Fuzzy Translation of Cross-
lingual Spelling Variants. 26th Annual 
international ACM SIGIR Conference on 
Research and Development in informaion 
Retrieval, Toronto. 
Platts, J. T. 1884. A Dictionary of Urdu, Classical 
Hindi and Englsih. W. H. Allen & Co. 
Rahman, T. 1997. Language and Politics in Pakistan. 
Oxford University Press, Lahore.  
Rahman, T. 2004. Language Policy and Localization 
in Pakistan: Proposal for a Paradigmatic Shift. 
Crossing the Digital Divide, SCALLA Conference 
on Computational Linguistics, Katmandu. 
Rai, A. 2000. Hindi Nationalism. Orient Longman 
Private Limited, New Delhi.  
Roche, E. and Y. Schabes, Eds. 1997. Finite-state 
Language Processing. MIT Press,  Cambridge. 
Sakai, T., A. Kumano and T. Manabe. 2002. 
Generating Transliteration Rules for Cross-
language Information Retrieval from Machine 
Translation Dictionaries. IEEE Conference on 
Systems, Man and Cybernatics. 
Stall, B. and K. Knight. 1998. Translating Names and 
Technical Terms in Arabic Text. Workshop on 
Computational Approaches to Semitic Languages, 
COLING/ACL, Montreal, 34-41. 
Virga, P. and S. Khudanpur. 2003. Transliteration of 
Proper Names in Cross-language Applications. 
26th Annual international ACM SIGIR 
Conference on Research and Development in 
informaion Retrieval, Toronto. 
Wells, J. C. 1995. Computer-coding the IPA: a 
proposed extension of SAMPA. University 
College London. 
Yan, Q., G. Gregory and A. E. David. 2003. 
Automatic Transliteration for Japanese-to-English 
Text Retrieval. 26th annual international ACM 
SIGIR conference on Research and development 
in information retrieval, 353-360. 
Zia, K. 1999. Standard Code Table for Urdu. 4th 
Symposium on Multilingual Information 
Processing (MLIT-4), Yangon. 
800
Coling 2010: Poster Volume, pages 1041?1049,
Beijing, August 2010
Web-based and combined language models:
a case study on noun compound identification
Carlos Ramisch?? Aline Villavicencio? Christian Boitet?
? GETALP ? Laboratory of Informatics of Grenoble, University of Grenoble
? Institute of Informatics, Federal University of Rio Grande do Sul
{ceramisch,avillavicencio}@inf.ufrgs.br Christian.Boitet@imag.fr
Abstract
This paper looks at the web as a corpus
and at the effects of using web counts
to model language, particularly when we
consider them as a domain-specific versus
a general-purpose resource. We first com-
pare three vocabularies that were ranked
according to frequencies drawn from
general-purpose, specialised and web cor-
pora. Then, we look at methods to com-
bine heterogeneous corpora and evaluate
the individual and combined counts in the
automatic extraction of noun compounds
from English general-purpose and spe-
cialised texts. Better n-gram counts can
help improve the performance of empiri-
cal NLP systems that rely on n-gram lan-
guage models.
1 Introduction
Corpora have been extensively employed in sev-
eral NLP tasks as the basis for automatically
learning models for language analysis and gener-
ation. In theory, data-driven (empirical or statis-
tical) approaches are well suited to take intrinsic
characteristics of human language into account. In
practice, external factors also determine to what
extent they will be popular and/or effective for a
given task, so that they have shown different per-
formances according to the availability of corpora,
to the linguistic complexity of the task, etc.
An essential component of most empirical sys-
tems is the language model (LM) and, in partic-
ular, n-gram language models. It is the LM that
tells the system how likely a word or n-gram is in
that language, based on the counts obtained from
corpora. However, corpora represent a sample of
a language and will be sparse, i.e. certain words or
expressions will not occur. One alternative to min-
imise the negative effects of data sparseness and
account for the probability of out-of-vocabulary
words is to use discounting techniques, where a
constant probability mass is discounted from each
n-gram and assigned to unseen n-grams. Another
strategy is to estimate the probability of an un-
seen n-gram by backing off to the probability of
the smaller n-grams that compose it.
In recent years, there has also been some ef-
fort in using the web to overcome data sparseness,
given that the web is several orders of magnitude
larger than any available corpus. However, it is
not straightforward to decide whether (a) it is bet-
ter to use the web than a standard corpus for a
given task or not, and (b) whether corpus and web
counts should be combined and how this should
be done (e.g. using interpolation or back-off tech-
niques). As a consequence there is a strong need
for better understanding of the impacts of web fre-
quencies in NLP systems and tasks.
More reliable ways of combining word counts
could improve the quality of empirical NLP sys-
tems. Thus, in this paper we discuss web-based
word frequency distributions (? 2) and investigate
to what extent ?web-as-a-corpus? approaches can
be employed in NLP tasks compared to standard
corpora (? 3). Then, we present the results of
two experiments. First, we compare word counts
drawn from general-purpose corpora, from spe-
cialised corpora and from the web (? 4). Second,
we propose several methods to combine data from
heterogeneous corpora (? 5), and evaluate their ef-
fectiveness in the context of a specific multiword
1041
expression task: automatic noun compound iden-
tification. We close this paper with some conclu-
sions and future work (? 6).
2 The web as a corpus
Conventional and, in particular, domain-specific
corpora, are valuable resources which provide a
closed-world environment where precise n-gram
counts can be obtained. As they tend to be smaller
than general purpose corpora, data sparseness can
considerably hinder the results of statistical meth-
ods. For instance, in the biomedical Genia cor-
pus (Ohta et al, 2002), 45% of the words occur
only once (so-called hapax legomena), and this is
a very poor basis for a statistical method to decide
whether this is a significant event or just random
noise.
One possible solution is to see the web as a
very large corpus containing pages written in sev-
eral languages and being representative of a large
fraction of human knowledge. However, there are
some differences between using regular corpora
and the web as a corpus, as discussed by Kilgar-
riff (2003). One assumption, in particular, is that
page counts can approximate word counts, so that
the total number of pages is used as an estimator
of the n-gram count, regardless of how many oc-
currences of the n-gram they contain.
This simple underlying assumption has been
employed for several tasks. For example, Grefen-
stette (1999), in the context of example-based ma-
chine translation, uses web counts to decide which
of a set of possible translations is the most natural
one for a given sequence of words (e.g. groupe de
travail as work group vs labour collective). Like-
wise, Keller and Lapata (2003) use the web to esti-
mate the frequencies of unseen nominal bigrams,
while Nicholson and Baldwin (2006) look at the
interpretation of noun compounds based on the
individual counts of the nouns and on the global
count of the compound estimated from the web as
a large corpus.
Villavicencio et al (2007) show that the web
and the British National Corpus (BNC) could be
used interchangeably to identify general-purpose
and type-independent multiword expressions. La-
pata and Keller (2005) perform a careful and
systematic evaluation of the web as a corpus in
other general-purpose tasks both for analysis and
generation, comparing it with a standard corpus
(the BNC) and using two different techniques to
combine them: linear interpolation and back-off.
Their results show that, while web counts are not
as effective for some tasks as standard counts, the
combined counts can generate results, for most
tasks, that are as good as the results produced by
the best individual corpus between the BNC and
the web. Nakov (2007) further investigates these
tasks and finds that, for many of them, effective
attribute selection can produce results that are at
least comparable to those from the BNC using
counts obtained from the web.
On the one hand, the web can minimise the
problem of sparse data, helping distinguish rare
from invalid cases. Moreover, a search engine al-
lows access to ever increasing quantities of data,
even for rare constructions and words, which
counts are usually equated to the number of pages
in which they occur. On the other hand, n-
grams in the highest frequency ranges, such as
the words the, up and down, are often assigned
the estimated size of the web, uniformly. While
this still gives an idea of their massive occur-
rence, it does not provide a finer grained distinc-
tion among them (e.g. in the BNC, the, down and
up occur 6,187,267, 84,446 and 195,426 times,
respectively, while in Yahoo! they all occur in
2,147,483,647 pages).
3 Standard vs web corpora
When we compare n-gram counts estimated from
the web with counts taken from a well-formed
standard corpus, we notice that web counts are
?estimated? or ?approximated? as page counts,
whereas standard corpus counts are the exact
number of occurrences of the n-gram. In this way,
web counts are dependent on the particular search
engine?s algorithms and representations, and these
may perform approximations to handle the large
size of their indexing structures and procedures,
such as ignoring punctuation and using stopword
lists (Kilgarriff, 2007). This assumption, as well
as the following discussion, are not valid for for
controlled data sets derived from Web data, such
1042
as the Google 1 trillion n-grams1. Thus, our re-
sults cannot be compared to those using this kind
of data (Bergsma et al, 2009).
In data-driven techniques, some statistical mea-
sures are based on contingency tables, and the
counts for each of the table cells can be straight-
forwardly computed from a standard corpus.
However, this is not the case for the web, where
the occurrences of an n-gram are not precisely
calculated in relation to the occurrences of the
(n? 1)-grams composing it. For instance, the
n-gram the man may appear in 200,000 pages,
while the words the and man appear in respec-
tively 1,000,000 and 200,000 pages, implying that
the word man occurs with no other word than the2.
In addition, the distribution of words in a stan-
dard corpus follows the well known Zipfian dis-
tribution (Baayen, 2001) while, in the web, it is
very difficult to distinguish frequent words or n-
grams as they are often estimated as the size of the
web. For instance, the Yahoo! frequencies plotted
in figure 1(a) are flattened in the upper part, giv-
ing the same page counts for more than 700 of the
most frequent words. Another issue is the size of
the corpus, which is an important information, of-
ten needed to compute frequencies from counts or
to estimate probabilities in n-gram models. Un-
like the size of a standard corpus, which is easily
obtained, it is very difficult to estimate how many
pages exist on the web, especially as this number
is always increasing.
But perhaps the biggest advantage of the web is
its availability, even for resource-poor languages
and domains. It is a free, expanding and easily ac-
cessible resource that is representative of language
use, in the sense that it contains a great variability
of writing styles, text genres, language levels and
knowledge domains.
4 Analysing n-gram frequencies
In this section, we describe an experiment to com-
pare the probability distribution of the vocabulary
of two corpora, Europarl (Koehn, 2005) and Ge-
nia (Ohta et al, 2002), that represent a sample
of general-purpose and specialised English. In
1This dataset is released through LDC and is not freely
available. Therefore, we do not consider it in our evaluation.
2In practice, this procedure can lead to negative counts.
Vep Vgenia Vinter
types 104,144 20,876 6,798
hapax 41,377 9,410 ?
tokens 39,595,352 486,823 ?
Table 1: Some characteristics of general vs
domain-specific corpora.
addition to both corpora, we also considered the
counts from the web as a corpus, using Google
and Yahoo! APIs, and these four corpora act as n-
gram count sources. To do that, we preprocessed
the data (? 4.1), extracted the vocabularies from
each corpus and calculated their counts in our
four n-gram count sources (? 4.2), analysing their
rank plots to compare how each of these sources
models general-purpose and specialised language
(? 4.3). The experiments described in this sec-
tion were implemented in the mwetoolkit and
are available at http://sf.net/projects/
mwetoolkit/.
4.1 Preprocessing
The Europarl corpus v3.0 (ep) contains transcrip-
tions of the speeches held at the European Par-
liament, with more than 1.4M sentences and
39,595,352 words. The Genia corpus (genia) con-
tains abstracts of scientific articles in biomedicine,
with around 1.8K sentences and 486,823 words.
These standard corpora were preprocessed in the
following way:
1. conversion to XML, lemmatisation and POS
tagging3;
2. case homogenisation, based on the following
criteria:
? all-uppercase and mixed case words
were normalised to their predominant
form, if it accounts for at least 80% of
the occurrences;
? uppercase words at the beginning of
sentences were lowercased;
? other words were not modified.
3Genia contains manual POS tag annota-
tion. Europarl was tagged using the TreeTagger
(www.ims.uni-stuttgart.de/projekte/
corplex/TreeTagger).
1043
This lowercasing algorithm helps to deal with the
massive use of abbreviations, acronyms, named
entities, and formulae found in specialised cor-
pora, such as those containing biomedical (and
other specialised) scientific articles.
For calculating arbitrary-sized n-grams in large
textual corpora efficiently, we implemented a
structure based on suffix arrays (Yamamoto and
Church, 2001). While suffix trees are often used
in LM tools, where n-grams have a fixed size, they
are not fit for arbitrary length n-gram searches and
can consume quite large amounts of memory to
store all the node pointers. Suffix arrays, on the
other hand, allow for arbitrary length n-grams to
be counted in a time that is proportional to log(N),
where N is the number of words (which is equiva-
lent to the number of suffixes) in the corpus. Suf-
fix arrays use a constant amount of memory pro-
portional to N. In our implementation, where ev-
ery word and every word position in the corpus are
encoded as a 4-byte integer, it corresponds pre-
cisely to 4?2?N plus the size of the vocabulary,
which is generally very small if compared to N,
given a typical token/type ratio. The construction
of the suffix array takes O(N log2 N) operations,
due to a sorting step at the end of the process.
4.2 Vocabulary creation
After preprocessing, we extracted all the unigram
surface forms (i.e. all words) from ep and from ge-
nia, generating two vocabularies, Vep and Vgenia,
where the words are ranked in descending fre-
quency order with respect to the corpus itself seen
as a n-gram count source. Formally, we can model
a vocabulary as a set V of words vi ?V taken from
a corpus. A word count is the value c(vi) = n of a
function that goes from words to natural numbers,
c : V ? N. Therefore, there is always an implicit
word order relation?r in a vocabulary, that can be
generated from V and c by using the order relation
? in N4. Thus, a rank is defined as a partially-
ordered set formed by a vocabulary?word order
pair relation: ?V,?r?.
Table 1 summarises some measures of the ex-
tracted vocabularies, where Vinter denotes the in-
tersection of Vep and Vgenia. Notice that Vinter
4That is, ?v1,v2 ?V , suppose c(v1) = n1 and c(v2) = n2,
then v1 ?r v2 if and only if n1 ? n2.
n-gram genia ep google yahoo
642 1 4 8090K 220M
African 2 2028 15400K 916M
fatty 16 22 2550K 59700K
medicine 4 643 21900K 934M
Mac 15 3 34500K 1910M
SH2 27 1 113K 3270K
advances 4 646 6200K 173M
thereby 29 2370 8210K 145M
Table 2: Distribution of some words in Vinter.
contains considerably less entries than the small-
est vocabulary (Vgenia). This shows to what ex-
tent both types of text differ and how important
it is to use the correct techniques when work-
ing with domain-specific data in empirical ap-
proaches. The table also shows the number of ha-
pax legomena (i.e. words that occur only once) in
each corpus, and in this aspect both corpora are
similar5. It also shows how sparseness affects lan-
guage, since a vocabulary that is 400% bigger has
only 5% less hapax legomena.
For each entry in each vocabulary, we ob-
tained a count estimated from four different n-
gram count sources: ep, genia, Google as a cor-
pus (google) and Yahoo! as a corpus (yahoo). The
latter were configured to return only results for
pages in English. Table 2 shows an example of
entries extracted from Vinter. Notice that there are
no zeroes in columns genia and ep, since this vo-
cabulary only contains words that occur at least
once in these corpora. Also, some words like Mac
and SH2, that are probably specialised terms, oc-
cur more in genia than in ep even if the latter is
more than 80 times larger than the former.
4.3 Rank analyses
For each vocabulary, we want to estimate how
similar the ranks generated by each of the four
count sources are. Figure 1 shows the rank po-
sition (x) against the frequency (y) of words in
Vgenia, Vep and Vinter, where each plotted point rep-
resents a rank position according to corpus fre-
5The percentual difference in the proportion of hapax
legomena can be explained by the fact that genia is much
smaller than ep.
1044
10-5
10-4
10-3
10-2
10-1
1 10 100 1000 10000
No
rm
ali
ze
d f
req
ue
nc
y
Rank
Rankplot of vgenia
genia
epgoogleyahoo
(a) Rank plot of Vgenia.
10-5
10-4
10-3
10-2
10-1
1 10 100 1000 10000
No
rm
ali
ze
d f
req
ue
nc
y
Rank
Rankplot of vep
genia
epgoogleyahoo
(b) Rank plot of Vep.
10-5
10-4
10-3
10-2
10-1
1 10 100 1000 10000
No
rm
ali
ze
d f
req
ue
nc
y
Rank
Rankplot of vinter
genia
epgoogleyahoo
(c) Rank plot of Vinter.
Figure 1: Plot of normalised frequencies of vocabularies according to rank positions, log-log scale.
quencies and may correspond to several different
words.6 The four sources have similar shaped
curves for each of the three vocabularies: ep
and genia could be reasonably approximated by
a linear regression curve (in the log-log domain).
google and yahoo present Zipfian curves for low
frequency ranges but have a flat line for higher
frequencies, and the phenomenon seems consis-
tent in all vocabularies and more intense on yahoo.
This is related to the problem discussed in sec-
tion 3 which is that web-based frequencies are not
accurate to model common words because web
counts correspond to page counts and not to word
counts, and that a common word will probably ap-
pear dozens of times in a single page. Nonethe-
less, google seems more robust to this effect,
and indeed yahoo returns exactly the same value
(roughly 2 billion pages) for a large number of
common words, producing the perfectly straight
line in the rank plots. Moreover, the problem
seems less serious in Vinter, but this could be due
to its much smaller size. These results show that
google is incapable of distinguishing among the
top-100 words while yahoo is incapable of distin-
guishing among the top-1000 words, and this can
be a serious drawback for web-based counts both
in general-purpose and specialised NLP tasks.
The curves agree in a large portion of the fre-
quency range, and the only interval in which ge-
nia and ep disagree is in lower frequencies (shown
in the bottom right corner). This happens be-
6Given the Zipfian behaviour of word probability distri-
butions, a log-log scale was used to plot the curves.
cause general-purpose ep frequencies are much
less accurate to model the specialised genia vo-
cabulary, specially in low frequency ranges when
sparseness becomes more marked (figure 1(a)),
and vice-versa (figure 1(b)). This effect is min-
imised in figure 1(c), corresponding to Vinter.
Although both vocabularies present the same
word frequency distributions, it does not mean
that their ranks are similar for the four count
sources. Tables 3 and 4 show the correlation
scores for the compared count sources and for the
two vocabularies, using Kendall?s ? . The ? corre-
lation index estimates the probability that a word
pair in a given rank has the same respective po-
sition in another rank, in spite of the distance be-
tween the words7.
In the two vocabularies, correlation is low,
which indicates that the ranks tend to order words
differently even if there are some similarities in
terms of the shape of the frequency distribution.
When we compare genia with google and with
yahoo, we observe that yahoo is slightly less cor-
related with genia than google, probably because
of its uniform count estimates for frequent words.
However, both seem to be more similar to genia
than ep.
A comparison of ep with google and with yahoo
shows that web frequencies are much more similar
to a general-purpose count source like ep than to
a specialised source like genia. Additionally, both
yahoo and google seem equally correlated to ep.
7For all correlation values, p < 0.001 for the alternative
hypothesis that ? is greater than 0.
1045
Vgenia Vgenia Vgenia Vgenia
top middle bottom
genia-ep 0.26 0.24 0.13 0.06
genia-google 0.28 0.24 0.18 0.09
genia-yahoo 0.27 0.22 0.17 0.09
ep-google 0.57 0.68 0.53 0.49
ep-yahoo 0.57 0.68 0.53 0.49
google-yahoo 0.90 0.90 0.89 0.89
Table 3: Kendall?s ? for count sources in Vgenia.
Vep Vep Vep Vep
top middle bottom
genia-ep 0.26 0.36 0.07 0.04
genia-google 0.27 0.39 0.15 0.12
genia-yahoo 0.24 0.35 0.12 0.10
ep-google 0.40 0.45 0.22 0.09
ep-yahoo 0.38 0.44 0.20 0.08
google-yahoo 0.86 0.89 0.84 0.83
Table 4: Kendall?s ? for count sources in Vep.
Surprisingly, this correlation is higher for Vgenia
than for Vep, as web frequencies and ep frequen-
cies are more similar for a specialised vocabulary
than for a general-purpose vocabulary. This could
mean that the three perform similarly (poorly) at
estimating frequencies for the biomedical vocab-
ulary (Vgenia) whereas they differ considerably at
estimating general-purpose frequencies.
The correlation of the rank (first column) is also
decomposed into the correlation for top words
(more than 10 occurrences), middle words (10 to
3 occurrences) and bottom words (2 and 1 occur-
rences). Except for the pair google-yahoo, the cor-
relation is much higher in the top portion of the
vocabulary and is close to zero in the long tail.
In spite of the logarithmic scale of the graphics
in figure 1, that show the largest difference in the
top part, the bottom part is actually the most ir-
regular. The only exception is ep compared with
the web count sources in Vgenia: these two pairs do
not present the high variability of the other com-
pared pairs, and this means that using ep counts
(general-purpose) to estimate genia counts (spe-
cialised) is similar to using web counts, indepen-
dently of the position of the word in the rank.
Counts from google and from yahoo are also very
similar, specially if we also consider Spearman?s
? , that is very close to total correlation. Web ranks
are also more similar for a specialised vocabulary
than for a general-purpose one, providing further
evidence for the hypothesis that the higher corre-
lation is a consequence of both sources being poor
frequency estimators. That is, for a given vocabu-
lary, when web count sources are good estimators,
they will be more distinct (e.g. having less zero
frequencies).
5 Combining corpora frequencies
In our second experiment, the goal is to propose
and to evaluate techniques for the combination
of n-gram counts from heterogeneous sources.
Therefore, we will use the insights about the vo-
cabulary differences presented in the previous sec-
tion. In this evaluation, we measure the impact
of the suggested techniques in the identification
of noun?noun compounds in corpora. Noun com-
pounds are very frequent in general-purpose and
specialised texts (e.g. bus stop, European Union
and gene activation). We extract them automat-
ically from ep and from genia using a standard
method based on POS patterns and association
measures (Evert and Krenn, 2005; Pecina, 2008;
Ramisch et al, 2010).
5.1 Experimental setup
The evaluation task consists of, given a corpus
of N words, extract all occurrences of adjacent
pairs of nouns8 and then rank them using a stan-
dard statistical measure that estimates the asso-
ciation strength between the two nouns. Analo-
gously to the formalism adopted in section 4.2,
we assume that, for each corpus, we generate a
set NN containing n-grams v1...n ? NN9 for which
we obtain n-gram counts from four sources. The
elements in NN are generated by comparing the
POS pattern noun?noun against all the bigrams in
the corpus and keeping only those pairs of adja-
cent words that match the pattern. The calculation
of the association measure, considering a bigram
v1v2, is based on a contingency table which cells
8We ignore other types of compounds, e.g. adjective?
noun pairs.
9We abbreviate a sequence v1 . . .vn as v1...n.
1046
contain all possible outcomes a1a2,ai ? {vi,?vi}.
For web-based counts, we corrected up to 2% of
them by forcing the frequency of a unigram to be
at least equal to the frequency of the bigram in
which it occurs. Such inconsistencies are incom-
patible with statistical approaches based on con-
tingency table, as discussed in section 2.
The log-likelihood association measure (LL, al-
ternatively called expected mutual information),
estimates the difference between the observed ta-
ble and the expected table under the assumption of
independent events, where E(a1 . . .an) =
n?
i=1
c(ai)
Nn?1is calculated using maximum likelihood:
LL(v1v2) = ?
a1a2
c(a1a2)? log2
c(a1a2)
E(a1a2)
The evaluation of the NN lists is performed au-
tomatically with the help of existing noun com-
pound dictionaries. The general-purpose gold
standard, used to evaluate NNep, is composed of
bigram noun compounds extracted from several
resources: 6,212 entries from the Cambridge In-
ternational Dictionary of English, 22,981 from
Wordnet and 2,849 from the data sets of MWE
200810. Those were merged into a single general-
purpose gold standard that contains 28,622 bi-
gram noun compounds. The specialised gold stan-
dard, used to evaluate NNgenia, is composed of
7,441 bigrams extracted from constituent annota-
tion of the genia corpus with respect to concepts
in the Genia ontology (Kim et al, 2006).
True positives (TPs) are the n-grams of NN
that are contained in the respective gold standard,
while n-grams that do not appear in the gold stan-
dard are considered false positives11. While this
is a simplification that underestimates the perfor-
mance of the method, it is appropriate for the pur-
pose of this evaluation because we compare only
the mean average precision (MAP) between two
NN ranks, in order to verify whether improve-
ments obtained by the combined frequencies are
10420 entries provided by Timothy Baldwin, 2,169 en-
tries provided by Su Nam Kim and 250 entries provided by
Preslav Nakov, freely available at http://multiword.
sf.net/
11In fact, nothing can be said about an n-gram that is not
in a (limited-coverage) dictionary, further manual annotation
would be necessary to asses its relevance.
significant. Additionaly, MWEs are complex lin-
guistic phenomena, and their annotation, specially
in a domain corpus, is a difficult task that reaches
low agreement rates, sometimes even for expert
native speakers. Therefore, not only for theo-
retical reasons but also for practical reasons, we
adopted an automatic evaluation procedure rath-
ern than annotating the top candidates in the lists
by hand.
Since the log-likelihood measure is a function
that assigns a real value to each n-gram, there is
a rank relation ?r that will be used to calculate
MAP as follows:
MAP(NN,?r) =
?
v1...n?NN
P(v1...n)? p(v1...n)
|TPs in NN| ,
where p = 1 if v1...n is a TP, 0 else, and the preci-
sion P(v1...n) of a given n-gram corresponds to the
number of TPs before v1...n in ?NN,?r? over the
total number of n-grams before v1...n in ?NN,?r?.
5.2 Combination heuristics
From the initial list of 176,552 lemmatised n-
grams in NNep and 14,594 in NNgenia, we fil-
tered out all hapax legomena in order to remove
noise and avoid useless computations. Then, we
counted the occurrences of v1, v2 and v1v2 in our
four sources, and those were used to calculate the
four LL values of n-grams in both lists. We also
propose three heuristics to combine a set of m
count sources c1 through cm into a single count
source ccomb:
ccomb(v1...n) =
m?
i=1
wi(v1...n)? ci(v1...n),
where w(v1...n) is a function that assigns a weight
between 0 and 1 for each count source accord-
ing to the n-gram v1...n. Three different func-
tions were used in our experiments: uniform
linear interpolation assumes a constant and uni-
form weight w(v1...n) = 1/m for all n-grams; pro-
portional linear interpolation assumes a constant
weight wi(v1...n) = ((?mj=1 N j)?Ni)/?mj=1 N j that
is proportional to the inverse size of the corpus;
and back-off uses the uniform interpolation of
web frequencies whenever the n-gram count in the
original corpus falls below a threshold (empiri-
cally defined as log2(N/100,000)).
1047
MAP of rank NNgenia NNep
LLgenia 0.4400 0.0462
LLep 0.4351 0.0371
LLgoogle 0.4297 0.0532
LLyahoo 0.4209 0.0508
LLuni f orm 0.4254 0.0508
LLproportional 0.4262 0.0520
LLbacko f f 0.3719 0.0370
Table 5: Performance of compound extraction.
Table 5 shows that the performance of back-
off is below all other techniques for both vocab-
ularies, thus excluding it as a successful combina-
tion heuristic. The large difference between MAP
scores for NNep and for NNgenia is explained by
the relative size of the gold standards: while the
general-purpose reference accounts for 16% of the
size of the NNep set, the specialised reference has
as many entries as 50% of NNgenia. Moreover, the
former was created by joining heterogeneous re-
sources while the latter was compiled by human
annotators from the Genia corpus itself. The goal
of our evaluation, however, is not to compare the
difficulty of each task, but to compare the com-
bination heuristics presented in each row of the
table.
The best MAP for NNgenia was obtained with
genia, that significantly outperforms all other
sources except ep12. On the other hand, the use
of web-based or interpolated counts in extracting
specialised noun?noun compounds does not im-
prove the performance of results based on sparse
but reliable counts drawn from well-formed cor-
pora. Nonetheless, the performance of ep in spe-
cialised extraction is surprising and could only be
explained by some overlap between the corpora.
Moreover, the interpolated counts are not signif-
icantly different from google counts, even if this
corpus should have the weakest weight in propor-
tional interpolation.
General-purpose compound extraction, how-
ever, benefits from the counts drawn from large
corpora as google and yahoo. Indeed, the former
12Significance was assessed through a standard one-tailed
t test for equal sample sizes and variances, ? = 0.005.
significantly outperforms all other count sources,
closely followed by proportional counts. In
both vocabularies, proportional interpolation per-
forms very similar to the best count source, but,
strangely enough, it still does not outperform
google. Further data inspection would be needed
to explain these results for the interpolated combi-
nation and to try to shed some light on the reason
why the backoff method performs so poorly.
6 Future perspectives
In this work, we presented a detailed evalua-
tion of the use of web frequencies as estima-
tors of corpus frequencies in general-purpose and
specialised tasks, discussing some important as-
pects of corpus-based versus web-based n-gram
frequencies. The results indicate that they are
not only very distinct but they are so in different
ways. The importance of domain-specific data for
modelling a specialised vocabulary is discussed in
terms of using ep to get Vgenia counts. Further-
more, the web corpora were more similar to genia
than to ep, which can be explained by the fact that
?similar? is different from ?good?, i.e. they might
be equally bad in modelling genia while they are
distinctly better for ep.
We also proposed heuristics to combine count
sources inspired by standard interpolation and
back-off techniques. Results show that we can-
not use web-based or combined counts to identify
specialised noun compounds, since they do not
help minimise data sparseness. However, general-
purpose extraction is improved with the use of
web counts instead of counts drawn from standard
corpora.
Future work includes extending this research
to other languages and domains in order to es-
timate how much of these results depend on the
corpora sizes. Moreover, as current interpolation
techniques usually combine two corpora, weights
are estimated in a more or less ad hoc proce-
dure (Lapata and Keller, 2005). Interpolating sev-
eral corpora would need a more controlled learn-
ing technique to obtain optimal weights for each
frequency function. Additionally, the evaluation
shows that corpora perform differently according
to the frequency range. This insight could be used
to define weight functions for interpolation.
1048
Acknowledgements
This research was partly supported by CNPq
(Projects 479824/2009-6 and 309569/2009-5),
FINEP and SEBRAE (COMUNICA project
FINEP/SEBRAE 1194/07). Special thanks to
Fl?vio Brun for his thorough work as volunteer
proofreader.
References
Baayen, R. Harald. 2001. Word Frequency Distri-
butions, volume 18 of Text, Speech and Language
Technology. Springer.
Bergsma, Shane, Dekang Lin, and Randy Goebel.
2009. Web-scale N-gram models for lexical disam-
biguation. In Boutilier, Craig, editor, Proceedings
of the 21st International Joint Conference on Arti-
ficial Intelligence (IJCAI 2009), pages 1507?1512,
Pasadena, CA, USA, July.
Evert, Stefan and Brigitte Krenn. 2005. Using small
random samples for the manual evaluation of sta-
tistical association measures. Computer Speech &
Language Special issue on Multiword Expression,
19(4):450?466.
Grefenstette, Gregory. 1999. The World Wide Web
as a resource for example-based machine translation
tasks. In Proceedings of the Twenty-First Interna-
tional Conference on Translating and the Computer,
London, UK, November. ASLIB.
Keller, Frank and Mirella Lapata. 2003. Using
the web to obtain frequencies for unseen bigrams.
Computational Linguistics Special Issue on the Web
as Corpus, 29(3):459?484.
Kilgarriff, Adam and Gregory Grefenstette. 2003. In-
troduction to the special issue on the web as corpus.
Computational Linguistics Special Issue on the Web
as Corpus, 29(3):333?347.
Kilgarriff, Adam. 2007. Googleology is bad science.
Computational Linguistics, 33(1):147?151.
Kim, Jin-Dong, Tomoko Ohta, Yuka Teteisi, and
Jun?ichi Tsujii. 2006. GENIA ontology. Techni-
cal report, Tsujii Laboratory, University of Tokyo.
Koehn, Philipp. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of
the Tenth Machine Translation Summit(MT Summit
2005), Phuket, Thailand, September. Asian-Pacific
Association for Machine Translation.
Lapata, Mirella and Frank Keller. 2005. Web-
based models for natural language processing. ACM
Transactions on Speech and Language Processing
(TSLP), 2(1):1?31.
Nakov, Preslav. 2007. Using the Web as an Implicit
Training Set: Application to Noun Compound Syn-
tax and Semantics. Ph.D. thesis, EECS Department,
University of California, Berkeley, CA, USA.
Nicholson, Jeremy and Timothy Baldwin. 2006. Inter-
pretation of compound nominalisations using cor-
pus and web statistics. In Moir?n, Bego?a Villada,
Aline Villavicencio, Diana McCarthy, Stefan Ev-
ert, and Suzanne Stevenson, editors, Proceedings of
the COLING/ACL Workshop on Multiword Expres-
sions: Identifying and Exploiting Underlying Prop-
erties (MWE 2006), pages 54?61, Sidney, Australia,
July. Association for Computational Linguistics.
Ohta, Tomoko, Yuka Tateishi, and Jin-Dong Kim.
2002. The GENIA corpus: an annotated research
abstract corpus in molecular biology domain. In
Proceedings of the Second Human Language Tech-
nology Conference (HLT 2002), pages 82?86, San
Diego, CA, USA, March. Morgan Kaufmann Pub-
lishers.
Pecina, Pavel. 2008. Reference data for czech collo-
cation extraction. In Gregoire, Nicole, Stefan Ev-
ert, and Brigitte Krenn, editors, Proceedings of the
LREC Workshop Towards a Shared Task for Multi-
word Expressions (MWE 2008), pages 11?14, Mar-
rakech, Morocco, June.
Ramisch, Carlos, Aline Villavicencio, and Christian
Boitet. 2010. mwetoolkit: a framework for mul-
tiword expression identification. In Calzolari, Nico-
letta, Khalid Choukri, Bente Maegaard, Joseph Mar-
iani, Jan Odjik, Stelios Piperidis, Mike Rosner, and
Daniel Tapias, editors, Proceedings of the Seventh
International Conference on Language Resources
and Evaluation (LREC 2010), Valetta, Malta, May.
European Language Resources Association.
Villavicencio, Aline, Valia Kordoni, Yi Zhang, Marco
Idiart, and Carlos Ramisch. 2007. Validation
and evaluation of automatically acquired multi-
word expressions for grammar engineering. In
Eisner, Jason, editor, Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL 2007), pages
1034?1043, Prague, Czech Republic, June. Associ-
ation for Computational Linguistics.
Yamamoto, Mikio and Kenneth W. Church. 2001. Us-
ing suffix arrays to compute term frequency and
document frequency for all substrings in a corpus.
Computational Linguistics, 27(1):1?30.
1049
Coling 2010: Demonstration Volume, pages 57?60,
Beijing, August 2010
Multiword Expressions in the wild?
The mwetoolkit comes in handy
Carlos Ramisch?? Aline Villavicencio? Christian Boitet?
? GETALP ? Laboratory of Informatics of Grenoble, University of Grenoble
? Institute of Informatics, Federal University of Rio Grande do Sul
{ceramisch,avillavicencio}@inf.ufrgs.br Christian.Boitet@imag.fr
Abstract
The mwetoolkit is a tool for auto-
matic extraction of Multiword Expres-
sions (MWEs) from monolingual corpora.
It both generates and validates MWE can-
didates. The generation is based on sur-
face forms, while for the validation, a se-
ries of criteria for removing noise are pro-
vided, such as some (language indepen-
dent) association measures.1 In this paper,
we present the use of the mwetoolkit
in a standard configuration, for extracting
MWEs from a corpus of general-purpose
English. The functionalities of the toolkit
are discussed in terms of a set of selected
examples, comparing it with related work
on MWE extraction.
1 MWEs in a nutshell
One of the factors that makes Natural Language
Processing (NLP) a challenging area is the fact
that some linguistic phenomena are not entirely
compositional or predictable. For instance, why
do we prefer to say full moon instead of total moon
or entire moon if all these words can be consid-
ered synonyms to transmit the idea of complete-
ness? This is an example of a collocation, i.e. a
sequence of words that tend to occur together and
whose interpretation generally crosses the bound-
aries between words (Smadja, 1993). More gen-
erally, collocations are a frequent type of mul-
tiword expression (MWE), a sequence of words
that presents some lexical, syntactic, semantic,
pragmatic or statistical idiosyncrasies (Sag et al,
2002). The definition of MWE also includes a
wide range of constructions like phrasal verbs (go
1The first version of the toolkit was presented in
(Ramisch et al, 2010b), where we described a language- and
type-independent methodology.
ahead, give up), noun compounds (ground speed),
fixed expressions (a priori) and multiword termi-
nology (design pattern). Due to their heterogene-
ity, MWEs vary in terms of syntactic flexibility
(let alne vs the moon is at the full) and semantic
opaqueness (wheel chair vs pass away).
While fairly studied and analysed in general
Linguistics, MWEs are a weakness in current
computational approaches to language. This is
understandable, since the manual creation of lan-
guage resources for NLP applications is expen-
sive and demands a considerable amount of ef-
fort. However, next-generation NLP systems need
to take MWEs into account, because they corre-
spond to a large fraction of the lexicon of a na-
tive speaker (Jackendoff, 1997). Particularly in
the context of domain adaptation, where we would
like to minimise the effort of porting a given sys-
tem to a new domain, MWEs are likely to play a
capital role. Indeed, theoretical estimations show
that specialised lexica may contain between 50%
and 70% of multiword entries (Sag et al, 2002).
Empirical evidence confirms these estimations: as
an example, we found that 56.7% of the terms
annotated in the Genia corpus are composed by
two or more words, and this is an underestimation
since it does not include general-purpose MWEs
such as phrasal verbs and fixed expressions.
The goal of mwetoolkit is to aid lexicog-
raphers and terminographers in the task of creat-
ing language resources that include multiword en-
tries. Therefore, we assume that, whenever a tex-
tual corpus of the target language/domain is avail-
able, it is possible to automatically extract inter-
esting sequences of words that can be regarded as
candidate MWEs.
2 Inside the black box
MWE identification is composed of two phases:
first, we automatically generate a list of candi-
57
mle = c(w1 . . .wn)N
dice = n? c(w1 . . .wn)?ni=1 c(wi)
pmi = log2
c(w1 . . .wn)
E(w1 . . .wn)
t-score = c(w1 . . .wn)?E(w1 . . .wn)?c(w1 . . .wn)
Figure 1: A candidate is a sequence of words w1 to
wn, with word counts c(w1) . . .c(wn) and n-gram
count c(w1 . . .wn) in a corpus with N words. The
expected count if words co-occurred by chance is
E(w1 . . .wn)? c(w1)...c(wn)Nn?1 .
dates from the corpus; then we filter them, so that
we can discard as much noise as possible. Can-
didate generation uses flat linguistic information
such as surface forms, lemmas and parts of speech
(POS).2 We can then define target sequences of
POS, such as VERB NOUN sequences, or even
more fine-grained constraints which use lemmas,
like take NOUN and give NOUN, or POS patterns
that include wildcards that stand for any word or
POS.3 The optimal POS patterns for a given do-
main, language and MWE type can be defined
based on the analysis of the data.
For the candidate filtering a set of association
measures (AMs), listed in figure 1, are calculated
for each candidate. A simple threshold can sub-
sequently be applied to filter out all the candidates
for which the AMs fall below a user-defined value.
If a gold standard is available, the toolkit can build
a classifier, automatically annotating each candi-
date to indicate whether it is contained in the gold
standard (i.e. it is regarded as a true MWE) or
not (i.e. it is regarded as a non-MWE).4 This
annotation is not used to filter the lists, but only
2If tools like a POS tagger are not available for a lan-
guage/domain, it is possible to generate simple n-gram lists
(n = 1..10), but the quality will be inferior. A possible solu-
tion is to filter out candidates on a keyword basis, e.g. from
a list of stopwords).
3Although syntactic information can provide better re-
sults for some types of MWEs, like collocations (Seretan,
2008), currently no syntactic information is allowed as a cri-
terion for candidate generation, keeping the toolkit as simple
and language independent as possible.
4The gold standard can be a dictionary or a manually an-
notated list of candidates.
candidate fEP fgoogle class
status quo 137 1940K True
US navy 4 1320K False
International Cooperation 2 1150K False
Cooperation Agreement 188 115K True
Panama Canal 2 753K True
security institution 5 8190 False
lending institution 4 54800 True
human right 2 251K True
Human Rights 3067 3400K False
pro-human right 2 34 False
Table 1: Example of MWE candidates extracted
by mwetoolkit.
by the classifier to learn the relation between the
AMs and the MWE class of the candidate. This
is particularly useful because, to date, it remains
unclear which AM performs better for a partic-
ular type or language, and the classifier applies
measures according to their efficacy in filtering
the candidates.Some examples of output are pre-
sented in table 1.
3 Getting started
The toolkit is open source software that can
be freely downloaded (sf.net/projects/
mwetoolkit). As a demonstration, we present
the extraction of noun-noun compounds from the
general-purpose English Europarl (EP) corpus5.
To preprocess the corpus, we used the sen-
tence splitter and tokeniser provided with EP, fol-
lowed by a lowercasing treatment (integrated in
the toolkit), and lemmatisation and POS tagging
using the TreeTagger6. The tagset was simplified
since some distinctions among plural/singular and
proper nouns were irrelevant.
From the preprocessed corpus, we obtained all
sequences of 2 nouns, which resulted in 176,552
unique noun compound candidates. Then, we ob-
tained the corpus counts for the bigrams and their
component unigrams in the EP corpus. Adopt-
ing the web as a corpus, we also use the number
of pages retrieved by Google and by Yahoo! as
5www.statmt.org/europarl.
6http://www.ims.uni-stuttgart.de/
projekte/corplex/TreeTagger/.
58
raw EN 
Europar
l
sentenc
e-split, 
lowerca
sed,
tokenis
ed, POS
-tagged
,
lemmat
ised Eu
roparl  Preproce
ssing
noun-no
un 
candida
tes
filtered-
1 
candida
tes
filtered-
2 
candida
tes  Count ca
ndidate
s
  Thresh
old
 Associa
tion me
as.
 Sort an
d thresh
old
mwetoo
lkit
Figure 2: Step-by-step demonstration on the EP
corpus.
counts. The mwetoolkit implements a cache
mechanism to avoid redundant queries, but to
speed up the process7, we filtered out all candi-
dates occurring less than two times in EP, which
reduced the list of candidates to 64,551 entries
(filtered-1 candidates in figure 2).
For the second filtering step, we calculated
four AMs for each of the three frequency sources
(EP, Google and Yahoo!). Some results on ma-
chine learning applied to the candidate lists of
the mwetoolkit can be found in Ramisch et al
(2010b). Here, we will limit ourselves to a dis-
cussion on some advantages and inconvenients of
the chosen approach by analysing a list of selected
examples.
4 Pros and cons
One of the biggest advantages of our approach is
that, since it is language independent, it is straight-
forward to apply it on corpora in virtually any
language. Moreover, it is not dependent on a
specific type of construction or syntactic formal-
ism. Of course, since it only uses limited linguis-
tic information, the accuracy of the resulting lists
can always be further improved with language-
dependent tools. In sum, the toolkit allows users
to perform systematic MWE extraction with con-
sistent intermediary files and well defined scripts
and arguments (avoiding the need for a series of ad
hoc separate scripts). Even if some basic knowl-
edge about how to run Python scripts and how to
7Yahoo! limits the queries to 5,000/day.
pass arguments to the command line is necessary,
the user is not required to be a programmer.
Nested MWEs are a problem in the current
approach. Table 1 shows two bigrams Interna-
tional Cooperation and Cooperation Agreement,
both evaluated as False candidates. However, they
could be considered as parts of a larger MWE In-
ternational Cooperation Agreement, but with the
current methodology it is not possible to detect
this kind of situation. Another case where the
candidate contains a MWE is the example pro-
human right, and in this case it would be neces-
sary to separate the prefix from the MWE, i.e. to
re-tokenise the words around the MWE candidate.
Indeed, tools for consistent tokenisation, specially
concerning dashes and slashes, could improve the
quality of the results, in particular for specialised
corpora.
The toolkit provides full integration with web
search engine APIs. The latter, however, are of
limited utility because search engines are not only
slow but also return more or less arbitrary num-
bers, some times even inconsistent (Ramisch et
al., 2010c). When large corpora like EP are avail-
able, we suggest that it is better to use its counts
rather than web counts. The toolkit provides an
efficient indexing mechanism, allowing for arbi-
trary n-grams to be counted in linear time.
The automatic evaluation of the candidates will
always be limited by the coverage of the reference
list. In the examples, Panama Canal is consid-
ered as a true MWE whereas US navy is not, but
both are proper names and the latter should also
be included as a true candidate. The same happens
for the candidates Human Rights and human right.
The mwetoolkit is an early prototype whose
simple design allows fine tuning of knowledge-
poor methods for MWE extraction. However, we
believe that there is room for improvement at sev-
eral points of the extraction methodology.
5 From now on
One of our goals for future versions is to be able
to extract bilingual MWEs from parallel or com-
parable corpora automatically. This could be done
through the inclusion of automatic word align-
ment information. Some previous experiments
show, however, that this may not be enough, as
59
automatic word alignment uses almost no lin-
guistic information and its output is often quite
noisy (Ramisch et al, 2010a). Combining align-
ment and shallow linguistic information seems a
promising solution for the automatic extraction
of bilingual MWEs. The potential uses of these
lexica are multiple, but the most obvious appli-
cation is machine translation. On the one hand,
MWEs could be used to guide the word align-
ment process. For instance, this could solve the
problem of aligning a language where compounds
are separate words, like French, with a language
that joins compound words together, like Ger-
man. In statistical machine translation systems,
MWEs could help to filter phrase tables or to boost
the scores of phrases which words are likely to
be multiwords.Some types of MWE (e.g. collo-
cations) could help in the semantic disambigua-
tion of words in the source language. The sense
of a word defined by its collocate can allow to
chose the correct target word or expression (Sere-
tan, 2008).
We would also like to improve the techniques
implemented for candidate filtering. Related work
showed that association measures based on con-
tingency tables are more robust to data sparseness
(Evert and Krenn, 2005). However, they are pair-
wise comparisons and their application on arbi-
trarily long n-grams is not straightforward. An
heuristics to adapt these measures is to apply them
recursively over increasing n-gram length. Other
features that could provide better classification
are context words, linguistic information coming
from simple word lexica, syntax, semantic classes
and domain-specific keywords. While for poor-
resourced languages we can only count on shallow
linguistic information, it is unreasonable to ignore
available information for other languages. In gen-
eral, machine learning performs better when more
information is available (Pecina, 2008).
We would like to evaluate our toolkit on several
data sets, varying the languages, domains and tar-
get MWE types. This would allow us to assign
its quantitative performance and to compare it to
other tools performing similar tasks. Additionally,
we could evaluate how well the classifiers perform
across languages and domains. In short, we be-
lieve that the mwetoolkit is an important first
step toward robust and reliable MWE treatment.
It is a freely available core application providing
flexible tools and coherent up-to-date documenta-
tion, and these are essential characteristics for the
extension and support of any computer system.
Acknowledgements
This research was partly supported by CNPq
(Projects 479824/2009-6 and 309569/2009-5),
FINEP and SEBRAE (COMUNICA project
FINEP/SEBRAE 1194/07).
References
Evert, Stefan and Brigitte Krenn. 2005. Using small
random samples for the manual evaluation of statis-
tical association measures. Comp. Speech & Lang.
Special issue on MWEs, 19(4):450?466.
Jackendoff, Ray. 1997. Twistin? the night away. Lan-
guage, 73:534?559.
Pecina, Pavel. 2008. Reference data for czech colloca-
tion extraction. In Proc. of the LREC Workshop To-
wards a Shared Task for MWEs (MWE 2008), pages
11?14, Marrakech, Morocco, Jun.
Ramisch, Carlos, Helena de Medeiros Caseli, Aline
Villavicencio, Andr? Machado, and Maria Jos? Fi-
natto. 2010a. A hybrid approach for multiword ex-
pression identification. In Proc. of the 9th PROPOR
(PROPOR 2010), volume 6001 of LNCS (LNAI),
pages 65?74, Porto Alegre, RS, Brazil. Springer.
Ramisch, Carlos, Aline Villavicencio, and Christian
Boitet. 2010b. mwetoolkit: a framework for multi-
word expression identification. In Proc. of the Sev-
enth LREC (LREC 2010), Malta, May. ELRA.
Ramisch, Carlos, Aline Villavicencio, and Christian
Boitet. 2010c. Web-based and combined language
models: a case study on noun compound identifica-
tion. In Proc. of the 23th COLING (COLING 2010),
Beijing, China, Aug.
Sag, Ivan, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multi-
word expressions: A pain in the neck for NLP.
In Proc. of the 3rd CICLing (CICLing-2002), vol-
ume 2276/2010 of LNCS, pages 1?15, Mexico City,
Mexico, Feb. Springer.
Seretan, Violeta. 2008. Collocation extraction based
on syntactic parsing. Ph.D. thesis, University of
Geneva, Geneva, Switzerland.
Smadja, Frank A. 1993. Retrieving collocations from
text: Xtract. Comp. Ling., 19(1):143?177.
60
Proceedings of the 6th Workshop on Ontologies and Lexical Resources (Ontolex 2010), pages 19?27,
Beijing, August 2010
Multilingual Lexical Network from the Archives of the Digital 
Silk Road 
Mohammad Daoud  
LIG, GETALP 
Universit? Joseph Fourier 
Mohammad.Daoud@imag.fr 
Kyo Kageura 
Graduate School of Education 
The University of Tokyo  
kyo@p.u-tokyo.ac.jp 
Christian Boitet 
LIG, GETALP 
Universit? Joseph Fourier 
Christian.Boitet@imag.fr 
Asanobu Kitamoto 
The National Institute of Informat-
ics (Tokyo) 
Kitamoto@nii.ac.jp 
Mathieu Mangeot 
LIG, GETALP 
Universit? Joseph Fourier 
Mathieu.Mangeot@imag.fr 
 
 
Abstract 
We are describing the construction 
process of a specialized multilingual 
lexical resource dedicated for the ar-
chive of the Digital Silk Road DSR. The 
DSR project creates digital archives of 
cultural heritage along the historical Silk 
Road; more than 116 of basic references 
on Silk Road have been digitized and 
made available online. These books are 
written in various languages and attract 
people from different linguistic back-
ground, therefore, we are trying to build 
a multilingual repository for the termi-
nology of the DSR to help its users, and 
increase the accessibility of these books. 
The construction of a terminological da-
tabase using a classical approach is dif-
ficult and expensive. Instead, we are in-
troducing specialized lexical resources 
that can be constructed by the commu-
nity and its resources; we call it Multi-
lingual Preterminological Graphs 
MPGs.  We build such graphs by ana-
lyzing the access log files of the website 
of the Digital Silk Road. We aim at 
making this graph as a seed repository 
so multilingual volunteers can contrib-
ute.  We have used the access log files 
of the DSR since its beginning in 2003, 
and obtained an initial graph of around 
116,000 terms. As an application, We 
have used this graph to obtain a preter-
minological multilingual database that 
has a number of applications. 
1 Introduction 
This paper describes the design and develop-
ment of a specialized multilingual lexical re-
source for the archive constructed and main-
tained by the Digital Silk Road project. The 
Digital Silk Road project (NII 2003) is an initia-
tive started by the National Institute of Infor-
matics (Tokyo/Japan) in 2002, to archive cul-
tural historical resources along the Silk Road, 
by digitizing them and making them available 
and accessible online.  
One of the most important sub-projects is the 
Digital Archive of Toyo Bunko Rare Books 
(NII 2008) where 116 (30,091 pages) of old rare 
books available at Toyo Bunko library have 
been digitized using OCR (Optical Character 
Recognition) technology. The digitized collec-
tion contains books from nine languages includ-
ing English. The website of the project attracts 
visitors from the domain of history, archeology, 
and people who are interested in cultural heri-
tage. It provides services of reading and search-
ing the books of Toyo Bunko, along with vari-
ety of services. Table 1 shows the countries 
from which DSR is being accessed. The table 
19
shows that around 60% of visitors are coming 
from countries other than Japan. The diversity 
of the visitors? linguistic backgrounds suggests 
two things: 1) Monolingual translation service is 
not enough. 2) It shows that we can benefit from 
allowing them to contribute to a multilingual 
repository. So we design and build a collabora-
tive multilingual terminological database and 
seed using the DSR project and its resources 
(Daoud, Kitamoto et al 2008). However, Dis-
covering and translating domain specific termi-
nology is a very complicated and expensive 
task, because (1) traditionally, it depends on 
human terminologists (Cabre and Sager 1999) 
which increases the cost, (2) terminology is dy-
namic (Kageura 2002), thousands of terms are 
coined each year, and (3) it is difficult to in-
volve domain experts in the construction proc-
ess. That will not only increase the cost, but it 
will reduce the quality, and the coverage (num-
ber of languages and size). Databases like (UN-
Geo 2002; IATE 2008; UN 2008) are built by 
huge organizations, and it is difficult for a 
smaller community to produce its own multilin-
gual terminological database. 
Country Visitors language Books in the same language  
Japan 117782 JA 2 books 
China 30379 CH 5 books 
USA 15626 EN 44 books 
Germany 8595 GE 14 books 
Spain 7076 SP - 
Australia 5239 EN See USA  
  Italy  4136 IT 1 book 
  France  3875 FR 14 books 
  Poland  2236  PO - 
  Russia  1895  RU 7 books 
other  87573 Other There are many books in 
different language 
Total 284412 
Table 1. Countries of the DSR visitors (from 
jan/2007 to dec/2008) 
In the next section we will give definitions 
for the basic concepts presented in this article, 
in particular, the preterminology and its lexical 
network (graph). Then, in the third section we 
will show the automatic approach to seed the 
multilingual preterminological graph based on 
the resources of the DSR. And then, we will 
discuss the human involvement in the develop-
ment of such a resource by providing a study of 
the possible contributors through analyzing the 
multilinguality and loyalty of the DSR visitors. 
In the fifth section we will show the experimen-
tal results. And finally, we will draw some con-
clusions.   
2 Multilingual Preterminological 
Graphs 
2.1 Preterminology 
Terminological sphere of a domain is the set of 
terms related to that domain. A smaller set of 
that sphere is well documented and available in 
dictionaries and terminological databases such 
as (FAO 2008; IEC 2008; IDRC 2009)... How-
ever, the majority of terms are not multilingual-
ized, nor stored into a database, even though, 
they may be used and translated by the commu-
nity and domain experts. This situation is shown 
in Figure 1, where the majority of terms are in 
area B. Preterminological sphere (area B) of a 
domain is a set of terms (preterms) related to 
the domain and used by the community but it 
might not be documented and included in tradi-
tional lexical databases. 
Multilingual Terminological Sphere
Preterminology
MTDB
B
A
C
 
Figure  1. Preterminological sphere 
Every year thousands of terms are coined and 
introduced in correspondence to new concepts, 
scientific discoveries or social needs. Most of 
these terms are produced in the top dominant 
languages, i.e. English. Interested people from 
different linguistic backgrounds would find 
suitable translations to new terms and use it 
amongst them. For example, the term ?status 
update? is used by people who visit social net-
working websites like facebook.com. Transla-
tion of this term to Arabic might not be avail-
able in area A of Figure 1. However the Arabic 
community found a translation that is acceptable 
which is  ????? ??????. So this term is in the area B. 
We are trying to use what is in area A, and what 
can be contributed from B to build preterminol-
ogy (Daoud, Boitet et al 2009).  
20
2.2 Structure of MPG 
We are building preterminological resource as a 
lexical network (graph) to handle the diversity 
of the resources that we use. A multilingual pre-
terminological graph MPG(N,E) is a finite non-
empty set N={n1,n2, ?} of objects called 
Nodes together with a set E={e1,e2, ?} of un-
ordered pairs of distinct nodes of MPG called 
edges. This definition is based on the general 
definition of a graph at the following references 
(Even 1979; Loerch 2000).  MPG of domain X, 
contains possible multilingual terms related to 
that domain connected to each other with rela-
tions. A multilingual lexical unit and its transla-
tions in different languages are represented as 
connected nodes with labels.  
In an MPG the set of nodes N consists of p,l, 
s, occ, where p is the string of the preterm, l is 
the language, s is the code of the first source of 
the preterm, and occ is the number of occur-
rences. Note that l could be undefined. For ex-
ample: N={[silk road, en, log],[Great Wall of China, en, 
,wikipedia, 5], [?????, ar, contributorx,6]}, here we have 
three nodes, 2 of them are English and one in 
Arabic, each term came from a different source. 
Note that English and Arabic terms belong to 
the same N thus, the same MPG. 
An Edge e={n, v} is a pair of nodes adjacent in 
an MPG. An edge represents a relation between 
two preterms represented by their nodes. The 
nature of the relation varies. However, edges are 
weighted with several weights (described be-
low) to indicate the possible nature of this rela-
tion. 
The following are the weights that label the 
edges on an MPG: Relation Weights rw: For an 
edge e={[p1,l1,s1], [p2,l2,s2]}, rw indicates 
that there is a relation between the preterm p1 
and p2. The nature of the relation could not be 
assumed by rw. Translation Weights tw: For an 
edge e={[p1,l1,s1], [p2,l2,s2]}, tw suggests that 
p1 in language l1 is a translation of p2 in lan-
guage l2. Synonym Weights sw: For an edge 
e={[p1,l1,s1], [p2,l1,s2]}, sw suggests that p1 
and p2 are synonyms. 
3 Automatic Initialization of DSR-
MPG  
Basically we seeded DSR-MPG, through two 
steps, the firs one is the automatic seeding, 
which consists of the following: 1) Initialization 
by finding interesting terms used to search the 
website of the DSR. 2) Multilingualization, us-
ing online resources. 3) Graph Expansion using 
the structure of the graph it self. The second 
step is the progressive enhancement, by receiv-
ing contributions from users, through set of use-
ful applications. In this section we will discuss 
the first three steps. In section 4, we will discuss 
the human factor in the development of DSR-
MPG. 
3.1 Analyzing Access Log Files 
We analyze two kinds of access requests that 
can provide us with information to enrich the 
MPG: (1) requests made to the local search en-
gine of DSR (2) requests from web-based 
search engine (like Google, Yahoo!?). These 
requests provide the search terms that visitors 
used to access the website. Moreover, we can 
understand the way users interpret a concept 
into lexical units. For example, if we find that 
five different users send two search requests t1 
and t2, then there is a possibility that t1 and t2 
have a relation. The graph constructor analyzes 
the requests to make the initial graph by creat-
ing edges between terms in the same session. 
rw(x,y), is set to the number of sessions contain-
ing x and y within the log file. 
For example, rw(x,y) = 10 means that 10 
people thought about x and y within the same 
search session. Figure 2 shows an example of a 
produced graph. The method did not discover 
the kind of relation between the terms. But it 
discovered that there is a relation, for example, 
three users requested results for ?yang? fol-
lowed by ?yin? within the same session. Hence, 
edge with weight of 2 was constructed based on 
this. 
21
 Figure  2. Example of constructing an MPG 
from an access log file 
3.2 Multilingualization Using Online Re-
sources 
Many researchers focused on the usage of dic-
tionaries in digital format to translate lexical 
resources automatically (Gopestake, Briscoe et 
al. 1994) (Etzioni, Reiter et al 2007). We are 
concerned with the automatic utilization of 
these resources to acquire multilingual preter-
minological resources through the following: 1) 
Wikipedia 2) online MT systems 3) online dic-
tionaries. 
Wikipedia (Wikipedia-A 2008) is a rich 
source of preterminology, it has good linguistic 
and lexical coverage. As of December, 2009, 
there are 279 Wikipedias in different languages, 
and 14,675,872 articles. There are 29 Wikipe-
dias with more that 100000 articles and 91 lan-
guages have more than 10,000 articles. Beside, 
Wikipedia is built by domain experts. We ex-
ploit the structure of Wikipedia to seed an 
MPG, by selecting a root set of terms, for each 
one of them we fetch its wikipedia article, and 
then we use the language roll of the article. For 
example, we fetch the article (Cuneiform script) 
En: http://en.wikipedia.org/wiki/Cuneiform_script, to reach its 
translation in Arabic from this url:  
http://ar.wikipedia.org/wiki/ ???????_?????  
We use also online machine translation sys-
tems as general purpose MRDs. One of the 
main advantages of MT systems is the good 
coverage even for multiword terms. The agree-
ment of some MT systems with other resources 
on the translation of one term enhanced the con-
fidence of the translation. Another positive 
point is that the results of MT provide a first 
draft to be post edited later. We used 3 MT sys-
tems: 
? Google Translate (Google 2008) (50 
languages) 
? Systran (Systran 2009) (14 languages) 
? Babylon (Babylon 2009) (26 languages) 
Here is an example of translating the term 
?great wall of China? into Arabic. 
 
Figure  3. MPG sample nodes 
In a similar way, we used several online re-
positories; to make good use of what is avail-
able and standardized, to initializing the MPG 
with various resources, and to construct a meta-
system to call online dictionaries automatically. 
We used IATE (IATE 2008)  as an example of a 
terminological db, and Google dictionary 
(Google 2008). The concept is similar to the 
concept of using online translations, where we 
construct an http request, to receive the result as 
html page. 
3.3 Graph Expansion 
 And then, the Graph is expanded by finding the 
synonyms according to formula (1) described at 
(Daoud, Boitet et al 2009). After finding syno-
nyms we assume that synonyms share the same 
translations. As Figure 4 shows, X1 and X2 have 
translations overlaps, and relatively high rw, so 
that suggest they are synonyms. Therefore we 
constructed heuristic edges between the transla-
tions of X1 and X2. 
Systran 
wight=1 
Wikipedia 
Google 
 Babylon 
wight=3 
great wall 
of China 
 ??? ?????
?????? 
 ?????? ????
????? 
22
 Figure  4. Graph expansion 
4 Human Involvement in the Develop-
ment of DSR-MPG 
After initializing the graph, we target contribu-
tions from the visitors to the DSR website. In 
this section we will start by analyzing the possi-
bility of receiving contributions from the visi-
tors, and then we will introduce some useful 
applications on the DSR-MPG that can help the 
visitors and attract them to get involved. 
4.1 Analyzing Possible Contributors of the 
DSR 
We are trying to analyze access log files to find 
out the possible contributors to a pretermi-
nological multilingual graph dedicated to an 
online community. This kind of information is 
necessary for the following reasons: 1) it pro-
vide feasibility analysis predicting the possibil-
ity of receiving contribution to a multilingual 
preterminological repository. 2) it gives infor-
mation that can be used by the collaborative 
environment to personalize the contribution 
process for those who prove to be able to con-
tribute. 
In the analysis process we are using the fol-
lowing information that can be easily extracted 
the access records: 
? Key terms to access the historical resources of 
the Digital Silk Road, whether it is the local 
search engine, or any external search engine. 
? Access frequency: number of access requests 
by a visitor over a period of time. 
? Language preferences 
? Period of visits 
Knowing these points helps determining the 
possible users who might be willing to contrib-
ute. A contributor should satisfy the following 
characteristics: 1) Loyalty 2) Multilinguality.  A 
multilingual user is a visitor who uses multilin-
gual search terms to access the online resources. 
We rank users based on their linguistic compe-
tence, we measure that by tracking users? search 
requests, and matching them with the multilin-
gual preterminological graph, users with higher 
matches in certain pair of languages are ranked 
higher. A loyal user is a user who visits the web 
site frequently and stays longer than other users. 
Users based on how many months they accessed 
the website more that k times. 
4.2 DSR-MPG Applications 
For a historical archive like the DSR, we find 
that reading and searching where the most im-
portant for users. Log files since 2003 shows 
that 80% of the project visitors were interested 
in reading the historical records. Moreover, 
around 140000 search requests have been sent 
to the internal search engine. So we imple-
mented two applications (1) ?contribute-while-
reading? and (2) ?contribute-while-searching?. 
4.2.1 Contribute While Searching 
Physical books have been digitized and indexed 
into a search engine. We expect users to send 
monolingual search requests in any language 
supported by our system to get multilingual an-
swers. Having a term base of multilingual 
equivalences could achieve this (Chen 2002). A 
bilingual user who could send a bilingual search 
request could be a valid candidate to contribute. 
We plan that users who use our search engine 
will use the DSR-pTMDB to translate their re-
quests and will contribute to the graph sponta-
neously. As Figure 5 shows, a user would trans-
late the search request, during the searching 
process; the user can ask to add new translation 
if s/he was not happy with the suggested transla-
tion, by clicking on ?Add Suggestions? to view 
a contribution page. 
 
Figure  5. A Japanese user translating his re-
quest 
23
4.2.2 Contribute While Reading 
The other application is trying to help users 
from different linguistic backgrounds to trans-
late some of the difficult terms into their lan-
guages while they are reading, simply by select-
ing a term from the screen. As shown in Figure 
6, readers will see a page from a book as an im-
age, with its OCR text. Important terms will be 
presented with yellow background. Once a term 
is clicked, a small child contribution/lookup 
window will be open, similar. Also user can 
lookup/translate any term from the screen by 
selecting it. This application helps covering all 
the important terms of each book. 
 
Figure 6. Translate while reading 
5 Experimental Results 
In this section present we will present the ex-
periment of seeding DSR-MPG, and the results 
of discovering possible contributors from the 
visitors of the DSR. 
5.1 DSR-MPG Initialization 
To build the initial DSR-MPG, we used the ac-
cess log files of the DSR website (dsr.nii.ac.jp) 
from December 2003 to January 2009. The ini-
tial graph after normalization contained 89,076 
nodes.  Also we extracted 81,204 terms using 
Yahoo terms. 27,500 of them were not discov-
ered from the access files. So, the total number 
of nodes in the initial graph was 116,576 nodes, 
see Figure 7 for sample nodes. 
After multilingualization, the graph has 210,781 
nodes containing terms from the most important 
languages. The graph has now 779,765 edges 
with tw > 0.  The important languages are the 
languages of the majority of the visitors, the 
languages of the archived books, and represen-
tative languages a long the Silk Road. DSR-
MPG achieved high linguistic coverage as 20 
languages have more than 1000 nodes on the 
graph. To evaluate the produced graph, we ex-
tracted 350 English terms manually from the 
index pages of the following books: 
Ancient Khotan, vol.1: 
http://dsr.nii.ac.jp/toyobunko/VIII-5-B2-7/V-1/ 
On Ancient Central-Asian Tracks, 
vol.1:http://dsr.nii.ac.jp/toyobunko/VIII-5-B2-
19/V-1 
Memoir on Maps of Chinese Turkistan and 
Kansu, vol.1: 
http://dsr.nii.ac.jp/toyobunko/VIII-5-B2-11/V-1 
0
5 0
10 0
15 0
2 0 0
2 5 0
3 0 0
DS R- M P G 2 D S R - M P G 1 P a n Ima g e s W i ki t io n a ry B i- d i c t io n a ry DS R1
En-Ar (only correct tranlstions) En-Fr (only correct translations)
 
Figure  7. A comparison between DSR-MPG, 
and other dictionaries. The En-Ar bi-dictionary 
is Babylon (Babylon 2009), and the En-Fr bi-
dictionary was IATE. 
We assume that the terms available in these 
books are strongly related to the DSR. Hence, 
we tried to translate them into Arabic and 
French. Figure 7 compares between DSR-MPG, 
and various general purpose dictionaries. Out of 
the 350 terms, we found 189 correct direct 
translations into Arabic. However, the number 
reached 214 using indirect translations.  On the 
other hand, the closest to our result was PanI-
mages, which uses Wikitionaries and various 
dictionaries, with only 83 correct translations. 
DSR-MPG1 is the translations obtained from 
formula 1, DSR-MPG2 represents the transla-
tions obtained from indirect translations, which 
increased the amount of correct translation by 
24
25 terms in the case of En-Ar. The result can be 
progressively enhanced by accepting contribu-
tions from volunteers through the applications 
we described in the section three and the generic 
nature of MPG makes it easy to accept contribu-
tions from any dictionary or terminological da-
tabase. 
Around 55200 root English terms were used 
as a seed set of terms; these terms were selected 
from the initial DSR-MPG. Around 35000 
terms have been translated from Wikipedia into 
at least 1 language, mostly in French, German. 
Wikipedia increased the density of the graph by 
introducing around 113,000 edges (with tw). 
Translations
0
2000
4000
6000
8000
10000
12000
fr de ja it zh es ru ar
 
Figure 8. Number of translated terms in sam-
ple languages using Wikipedia 
Naturally MT would achieve better coverage; 
we checked the results for Arabic, we selected 
60 terms randomly from the root set, around 25 
terms were translated correctly. 13 terms needed 
slight modification to be correct. 
0
2000
4000
6000
8000
10000
12000
fr de ja it zh es ru ar
Wikipedia
Google Translate confirmations
 
Figure 9. Terms translated by Google MT 
and matched the translation of Wikipedia 
5.2 DSR Possible Contributors 
With K=2, meaning that a multilinguality com-
petence is counted only if the two terms sent by 
a user has to have more than 2 points of transla-
tion weight on the MPG. 
The highest score was 33, achieved by this 
IP: p27250-adsao05douji-acca.osaka.ocn.ne.jp. 
That means that this user sent 33 multilingual 
search requests. We have another 115 users with 
score higher than 5.  
For example, the following two request, sent by 
one user: 
p27250-adsao05douji-acca.osaka.ocn.ne.jp
 &input=peshawar 
p27250-adsao05douji-acca.osaka.ocn.ne.jp
  &input=?????? 
On the DSR-MPG the translation weight be-
tween peshawer and ?????? = 5, thus 
this IP earned a point. With k=10, means that a 
user should send 10 requests to earn a loyalty 
point, only 309 users earned 12 point (for 12 
months), 43 of them has more than 3 points. 
6 Conclusions 
We presented our work in constructing a new 
lexical resource that can handle multilingual 
terms based on the historical archive of the 
Digital Silk Road. Multilingual Preterminologi-
cal Graphs (MPGs) are constructed based on 
domain dedicated resources, and based on vol-
unteer contributions.  
DSR Terminology
DSR-MPG (200,000 nodes)
previous DSR
dictionary (500
entries)
 
Figure  10. DSR preterminology 
It compiles terms available in the pretermi-
nological sphere of a domain. In this article we 
defined the framework of the construction of 
preterminology, and we described the approach 
for using access log files to initialize such pre-
terminological resource by finding the trends in 
the search requests used to access the resources 
of an online community. Aiming at a standard-
ized multilingual repository is very expensive 
25
and difficult.  Instead of that, MPGs tries to use 
all available contributions.  This way will en-
hance the linguistic and informational coverage, 
and tuning the weights (tw, rw, and sw) will 
give indications for the confidence of the trans-
lation equivalences, as the tedges accumulate 
the agreements of the contributors and MDRs 
(online resources). 
We used the resources of the Digital Silk 
Road Project to construct a DSR-MPG and 
some applications that attract further contribu-
tion to the MPG.  DSR-MPG achieved high lin-
guistic and informational coverage compared to 
other general purpose dictionaries, Figure 10. 
Furthermore, the generic structure of the MPG 
makes it possible to accept volunteer contribu-
tions, and it facilitates further study of comput-
ing more lexical functions and ontological rela-
tions between the terms. We made a study on 
the possibility of receiving contributions from 
users, by analyzing the access log file to find 
multilinguality and loyalty of the DSR visitors; 
we found 115 users with the needed linguistic 
capacity 43 of them scored high loyalty points. 
This gives an indication of the future of the con-
tributions. These measures are just estimations 
and expected to go high with the help of the 
MPG-DSR applications. 
References 
Babylon. (2009). "Babylon Dictionary."   Retrieved 
5/5/2009, 2009, from 
http://www.babylon.com/define/98/English-
Arabic-Dictionary.html. 
Cabre, M. T. and J. C. Sager (1999). Terminology: 
Theory, methods, and applications, J. Benjamins 
Pub. Co. 
Chen, A. (2002). "Cross-Language Retrieval Ex-
periments at CLEF 2002." in CLEF-2002 working 
notes,. 
Daoud, M., C. Boitet, et al (2009). Constructing 
multilingual preterminological graphs using vari-
ous online-community resources. the Eighth In-
ternational Symposium on Natural Language 
Processing (SNLP2009), Thailand. 
Daoud, M., C. Boitet, et al (2009). Building a 
Community-Dedicated Preterminological Multi-
lingual Graphs from Implicit and Explicit User In-
teractions. Second International Workshop on 
REsource Discovery (RED 2009), co-located with 
VLDB 2009, Lyon, France. 
Daoud, M., A. Kitamoto, et al (2008). A CLIR-
Based Collaborative Construction of Multilingual 
Terminological Dictionary for Cultural Resources. 
Translating and the Computer 30, London-UK. 
Etzioni, O., K. Reiter, et al (2007). Lexical transla-
tion with application to image searching on the 
web. MT Summit XI, Copenhagen, Denmark. 
Even, S. (1979). Graph Algorithms, Computer Sci-
ence Press. 
FAO. (2008). "FAO TERMINOLOGY."   Retrieved 
1/9/2008, 2008, from http://www.fao.org/faoterm. 
Google. (2008). "Google Dictionary."   Retrieved 
1/9/2008, 2008, from 
http://www.google.com/dictionary. 
Google. (2008). "Google Translate."   Retrieved 1 
June 2008, 2008, from http://translate.google.com. 
Gopestake, A., T. Briscoe, et al (1994). "Acquisition 
of lexical translation relations from MRDS." Ma-
chine Translation Volume 9, Numbers 3-4 / Sep-
tember, 1994: 183-219. 
IATE. (2008). "Inter-Active Terminology for 
Europe."   Retrieved 10/10/2008, 2008, from 
http://iate.europa.eu. 
IDRC. (2009, 10 January 2009). "The Water De-
mand Management Glossary (Second Edition)." 
from 
http://www.idrc.ca/WaterDemand/IDRC_Glossar
y_Second_Edition/index.html. 
IEC. (2008). "Electropedia."   Retrieved 10/10/2008, 
2008, from 
http://dom2.iec.ch/iev/iev.nsf/welcome?openform. 
Kageura, K. (2002). The Dynamics of Terminology: 
A descriptive theory of term formation and termi-
nological growth. 
Loerch, U. (2000). An Introduction to Graph Algo-
rithms Auckland, New Zealand, University of 
Auckland. 
NII. (2003). "Digital Silk Road."   Retrieved 
1/9/2008, 2008, from 
http://dsr.nii.ac.jp/index.html.en. 
NII. (2008). "Digital Archive of Toyo Bunko Rare 
Books."   Retrieved 1 June 2008, 2008, from 
http://dsr.nii.ac.jp/toyobunko/. 
Systran. (2009). "Systran Web Tranlstor."   Re-
trieved 20/12/2009, 2009, from 
www.systransoft.com/. 
UN-Geo (2002). Glossary of Terms for the Stan-
dardization of Geographical Names, UN, New 
York. 
26
UN. (2008). "United Nations Multilingual Terminol-
ogy Database."   Retrieved 10/10/2008, 2008, 
from http://unterm.un.org/. 
Wikipedia-A. (2008). "Wikipedia."   Retrieved 1 
June 2008, 2008, from http://www.wikipedia.org/. 
 
 
 
27
Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 52?60,
Beijing, August 2010
Ontology driven content extraction using interlingual annotation of
texts in the OMNIA project
Achille Falaise, David Rouquet, Didier Schwab, Herve? Blanchon, Christian Boitet
LIG-GETALP, University of Grenoble
{Firstname}.{Lastname}@imag.fr
Abstract
OMNIA is an on-going project that aims
to retrieve images accompanied with
multilingual texts. In this paper, we pro-
pose a generic method (language and do-
main independent) to extract conceptual
information from such texts and sponta-
neous user requests. First, texts are la-
belled with interlingual annotation, then
a generic extractor taking a domain on-
tology as a parameter extract relevant
conceptual information. Implementation
is also presented with a first experiment
and preliminary results.
1 Introduction
The OMNIA project (Luca Marchesotti et al,
2010) aims to retrieve images that are described
with multilingual free companion texts (cap-
tions, comments, etc.) in large Web datasets.
Images are first classified with formal descrip-
tors in a lightweight ontology using automatic
textual and visual analysis. Then, users may ex-
press spontaneous queries in their mother tongue
to retrieve images. In order to build both formal
descriptors and queries for the ontology, a con-
tent extraction in multilingual texts is required.
Multilingual content extraction does not im-
ply translation. It has been shown in (Daoud,
2006) that annotating words or chunks with in-
terlingual lexemes is a valid approach to initiate
a content extraction. We thus skip syntactical
analysis, an expensive and low quality process,
and get language-independent data early in our
flow, allowing further treatments to be language-
independent. We use the lightweight ontology
for image classifications as the formal knowl-
edge representation tha determines relevant in-
formation to extract. This ontology is considered
as a domain parameter for the content extractor.
We are testing this method on a database pro-
vided for the image retrieval challenge CLEF09
by the Belgium press agency Belga. The
database contains 500K images with free com-
panion texts of about 50 words (about 25M
words in total). The texts in the database are in
English only, and we ?simulate? multilinguism
with partially post-edited machine translation.
The rest of the paper is organized as fol-
low. We first depict our general architecture de-
ployed for CLIA and then detail the various pro-
cesses involved : interlingual annotation, con-
ceptual vector based disambiguation and ontol-
ogy driven content extraction. We conclude
with the first results of experimentations on the
CLEF09 data.
2 General architecture
2.1 General process
In our scenario, there are two types of tex-
tual data to deal with : companion texts in the
database (captions), but also user requests. The
two are processed in a very similar way.
The general architecture is depicted in figure
1. The main components, that will be described
in detail, may be summarized as follows:
? Texts (both companions and requests) are
first lemmatised with a language-dependent
piece of software. Ambiguities are pre-
served in a Q-graph structure presented in
section 3.1.2.
52
ConceptsQ-Graph Conceptextraction Lemmatisation Disamb
Companiontexts
NL Requests
NL-UWdictionnary UW-ConceptMap Ontology
Interlingualannotation
Figure 1: General architecture of CLIA in the OMNIA project
? Then, the lemmatised texts are annotated
with interlingual (ideally unambiguous)
lexemes, namely Universal Words (UW)
presented in section 3.1.1. This adds a lot
of ambiguities to the structure, as an ac-
tual lemma may refer to several semanti-
cally different lexemes.
? The possible meanings for lemmas are then
weighted in the Q-graph through a disam-
biguation process.
? Finally, relevant conceptual information is
extracted using an alignment between a do-
main ontology and the interlingual lexemes.
The conceptual information in the output may
adopt different shapes, such as a weighted con-
ceptual vector, statements in the A-Box of the
ontology or annotations in the original text, etc.
In the case of OMNIA, conceptual informa-
tion extracted from companion texts is stored
in a database, while conceptual information ex-
tracted from users requests are transformed into
formal requests for the database (such as SQL,
SPARQL, etc.).
2.2 Implementation
The general process is implemented following a
Service Oriented Architecture (SOA). Each part
of the process corresponds to a service.
This allowed us to reuse part of existing re-
sources developed on heterogeneous platforms
using web interfaces (in the best case REST in-
terfaces (Fielding, 2000), but frequently only
HTML form-based interfaces). A service su-
pervisor has been built to deal with such an
heterogeneity and address normalization issues
(e.g. line-breaks, encoding, identification, cook-
ies, page forwarding, etc.).
This architecture is able to process multiple
tasks concurrently, allowing to deal with users
requests in real time while processing compan-
ion texts in the background.
3 Interlingual annotation
We present in this section the preliminary treat-
ments of multilingual texts (image companion
texts or user requests) that are required for
our content extraction process (Rouquet and
Nguyen, 2009a).
In order to allow a content extraction in multi-
lingual texts, we propose to represent texts with
the internal formalism of the Q-Systems and
to annotate chunks with UNL interlingual lex-
emes (UW) . Roughly, we are making an inter-
lingual lemmatisation, containing more informa-
tion than simple tagging, that is not currently
proposed by any lemmatisation software.
3.1 Resources and data structures
3.1.1 The Universal Network Language
UNL (Boitet et al, 2009; Uchida Hiroshi et
al., 2009) is a pivot language that represents the
meaning of a sentence with a semantic abstract
structure (an hyper-graph) of an equivalent En-
glish sentence.
The vocabulary of UNL consists in a set of
Universal Words (UW). An UW consists of:
1. a headword, if possible derived from En-
glish, that can be a word, initials, an expres-
sion or even an entire sentence. It is a label
for the concepts it represents in its original
language ;
2. a list of restrictions that aims to precisely
specify the concept the UW refers to. Re-
strictions are semantic relations with other
53
UW. The most used is the ?icl? relation that
points to a more general UW.
Examples :
? book(icl>do, agt>human, obj>thing)
and book(icl>thing).
Here, the sense of the headword is focused
by the attributes.
? ikebana(icl>flower arrangement).
Here, the headword comes from Japanese.
? go down.
Here, the headword does not need any re-
finement.
Ideally, an UW refers unambiguously to a con-
cept, shared among several languages. However,
UW are designed to represent acceptions in a
language ; we therefore find distinct UW that
refer to the same concept as for ?affection? and
?disease?.
We are mainly using the 207k UW built by the
U++ Consortium (Jesus Carden?osa et al, 2009)
from the synsets of the Princeton WordNet, that
are linked to natural languages via bilingual dic-
tionaries. The storage of these dictionaries can
be supported by a suitable platform like PIVAX
(Nguyen et al, 2007) or a dedicated database.
The gain of a pivot language is illustrated in fig-
ure 2. If we want to add a new language in the
multilingual system, we just need to create the
links with the pivot but not with all the other lan-
guages.
3.1.2 The Q-Systems
We can think of inserting the UW annotations
with tags (e.g. XML) directly along the source
text as in table 1. However, this naive approach is
not adequate to represent the segmentation am-
biguities that can occur in the text interpretation
(in the example of table 1, we list the different
possible meanings for ?in?, but cannot represent
?waiting?, ?room? and ?waiting room? as three
possible lexical units).
In order to allow the representation of segmen-
tation and other ambiguities, that can occur in
a text interpretation, we propose to use the Q-
Systems. They represent texts in an adequate
Interlingual
UW volume
French 
volume
English 
volume
Chinese
 volume
Figure 2: Multilingual architecture with a pivot
in a waiting room
<tag uw=?in(icl-sup-how),
in(icl-sup-adj),
in(icl-sup-linear unit,
equ-sup-inch)?>in</tag>
<tag uw=?unk?>a</tag> <tag
uw=?waiting room(icl-sup-room,
equ-sup-lounge)?>waiting
room</tag>
Table 1: Naive annotation of a text fragment
graph structure decorated with bracketed expres-
sions (trees) and, moreover, allow processing on
this structure via graph rewriting rules (a set of
such rewriting rules is a so called Q-System).
An example of the Q-System formalism is
given in figure 3 of section 3.2.3. It presents
successively : the textual input representing a Q-
graph, a rewriting rule and a graphical view of
the Q-graph obtained after the application of the
rule (and others).
The Q-Systems were proposed by Alain
Colmeraurer at Montreal University (Colmer-
auer, 1970). For our goal, they have three main
advantages :
? they provide the formalized internal struc-
ture for linguistic portability that we men-
tioned in the introduction (Hajlaoui and
Boitet, 2007) ;
? they unify text processing with powerful
graph rewriting systems ;
54
? they allow the creation or the edition of
a process by non-programmers (e.g. lin-
guists) using SLLP (Specialized Language
for Linguistic Programming).
We are actually using a reimplementation of
the Q-Systems made in 2007 by Hong-Thai
Nguyen during his PhD in the LIG-GETALP
team (Nguyen, 2009).
3.2 Framework of the annotation process
3.2.1 Overview
The annotation process is composed by the
following steps :
1. splitting the text in fragments if too long ;
2. lemmatisation with a specialized software ;
3. transcription to the Q-Systems format ;
4. creation of local bilingual dictionaries
(source language - UW) for each fragment
with PIVAX ;
5. execution of those dictionaries on the frag-
ments ;
3.2.2 Lemmatisation
As we want to use dictionaries where entries
are lemmas, the first step is to lemmatise the in-
put text (i.e. to annotate occurrences with possi-
ble lemmas). This step is very important because
it although gives the possible segmentations of
the text in lexical units. It brings two kinds of
ambiguities into play : on one hand, an occur-
rence can be interpreted as different lemmas, on
the other, there can be several possible segmen-
tations (eventually overlapping) to determine the
lexical units.
For content extraction or information retrieval
purpose, it is better to preserve an ambiguity than
to badly resolve it. Therefore we expect from a
lemmatiser to keep all ambiguities and to repre-
sent them in a confusion network (a simple tag-
ger is not suitable). Several lemmatiser can be
used to cover different languages. For each of
them, we propose to use a dedicated ANTLR
grammar (Terence Parr et al, 2009) in order to
soundly transform the output in a Q-graph.
To process the Belga corpus, we developed a
lemmatiser that produce natively Q-graphs. It
is based on the morphologic dictionary DELA1
available under LGPL licence.
3.2.3 Local dictionaries as Q-Systems
Having the input text annotated with lemmas,
with the Q-System formalism, we want to use the
graph rewriting possibilities to annotate it with
UW. To do so, we use PIVAX export features to
produce rules that rewrite a lemma in an UW (see
figure 3). Each rule correspond to an entry in
the bilingual dictionary. To obtain a tractable Q-
Systems (sets of rules), we built local dictionar-
ies that contain the entries for fragments of the
text (about 250 words in the first experiment).
Figure 3: Creation and execution of a Q-System
Considering the significant quantity of ambi-
guities generated by this approach (up to a dozen
UW for a single word), we need to include a
disambiguation process. This process, based on
conceptual vectors, is presented in the next sec-
tion.
4 Conceptual vector based
disambiguation
Vectors have been used in NLP for over 40 years.
For information retrieval, the standard vector
model (SVM) was invented by Salton (Salton,
1991) during the late 60?s, while for meaning
representation, latent semantic analysis (LSA)
1http://infolingu.univ-mlv.fr/DonneesLinguistiques/
Dictionnaires/telechargement.html
55
was developed during the late 80?s (Deerwester
et al, 1990). These approaches are inspired
by distributional semantics (Harris et al, 1989)
which hypothesises that a word meaning can be
defined by its co-text. For example, the mean-
ing of ?milk? could be described by {?cow?, ?cat?,
?white?, ?cheese?, ?mammal?, . . . }. Hence, distribu-
tional vector elements correspond directly (for
SVM) or indirectly (for LSA) to lexical items
from utterances.
The conceptual vector model is different as it
is inspired by componential linguistics (Hjelm-
lev, 1968) which holds that the meaning of words
can be described with semantic components.
These can be considered as atoms of meaning
(known as primitives (Wierzbicka, 1996)), or
also only as constituents of the meaning (known
as semes, features (Greimas, 1984), concepts,
ideas). For example, the meaning of ?milk?
could be described by {LIQUID, DAIRY PRODUCT, WHITE,
FOOD, . . .}. Conceptual vectors model a formal-
ism for the projection of this notion in a vectorial
space. Hence, conceptual vector elements corre-
spond to concepts indirectly, as we will see later.
For textual purposes2, conceptual vectors can
be associated to all levels of a text (word, phrase,
sentence, paragraph, whole texts, etc.). As they
represent ideas, they correspond to the notion of
semantic field3 at the lexical level, and to the
overall thematic aspects at the level of the entire
text.
Conceptual vectors can also be applied to
lexical meanings. They have been studied in
word sense disambiguation (WSD) using iso-
topic properties in a text, i.e. redundancy of ideas
(Greimas, 1984). The basic idea is to maximise
the overlap of shared ideas between senses of
lexical items. This can be done by computing the
angular distance between two conceptual vectors
(Schwab and Lafourcade, 2007).
In our case, conceptual vectors are used for
automatic disambiguation of texts. Using this
method, we calculate confidence score for each
UW hypothesis appearing in the Q-Graph.
2Conceptual vectors can be associated with any content,
not only text: images, videos, multimedia, Web pages, etc.
3The semantic field is the set of ideas conveyed by a
term.
5 Ontology driven content extraction
The content extraction has to be leaded by a
?knowledge base? containing the informations
we want to retrieve.
5.1 Previous works in content extraction
This approach has its roots in machine trans-
lation projects such as C-Star II (1993-1999)
(Blanchon and Boitet, 2000) and Nespole!
(2000-2002) (Metze et al, 2002), for on the fly
translation of oral speech acts in the domain of
tourism. In these projects, semantic transfer was
achieved through an IF (Inter-exchange Format),
that is a semantic pivot dedicated to the domain.
This IF allows to store information extracted
from texts but is although used to lead the con-
tent extraction process by giving a formal repre-
sentation of the relevant informations to extract,
according to the domain.
The Nespole! IF consists of 123 concepts
from the tourism domain, associated with sev-
eral arguments and associable with speech acts
markers. The extraction process is based on pat-
terns. As an example, the statement ?I wish a
single room from September 10th to 15th? may
be represented as follows:
{ c:give-information+disposition+room
( disposition=(desire, who=i),
room-spec=
( identifiability=no,single_room ),
time=
( start-time=(md=10),
end-time(md=15, month=9)
)
)
}
5.2 Ontologies as parameter for the domain
In the project OMNIA, the knowledge base has
the form of a lightweight ontology for image
classification 4. This ontology contains 732 con-
cepts in the following domains : animals, pol-
itics, religion, army, sports, monuments, trans-
ports, games, entertainment, emotions, etc. To
us, using an ontology has the following advan-
tages :
? Ontologies give an axiomatic description
of a domain, based on formal logics (usu-
4http://kaiko.getalp.org/kaiko/ontology/OMNIA/OMNIA current.owl
56
ally description logics (Baader et al, 2003))
with an explicit semantic. Thus, the knowl-
edge stored in them can be used soundly by
software agents;
? Ontological structures are close to the or-
ganisation of ideas as semantic networks in
human mind (Aitchenson, 2003) and are la-
beled with strings derived from natural lan-
guages. Thus humans can use them (brows-
ing or contributing) in a pretty natural way;
? Finally, with the advent of the Semantic
Web and normative initiatives such as the
W3C5, ontologies come with a lot of shared
tools for editing, querying, merging, etc.
As the content extractor might only process
UW annotations, it is necessary that the knowl-
edge base is whether expressed using UW or
linked to UW. The ontology is here considered
as a domain parameter of content extraction
and can be changed to improve preformances
on specific data collections. Therefore, given
any OWL ontology6, we must be able to link it
with a volume of UW considering the following
constraints :
Creating manually such correspondences
is costly due to the size of resources so an
automatic process is requiered.
Ontologies and lexicons evolve over the time
so an alignment must be adaptable to incremen-
tal evolutions of resources.
The correspondences must be easily manip-
ulated by users so they can manually improve
the quality of automatically created alignments
with post-edition.
Constructing and maintaining an alignment
between an ontology and an UW lexicon is a
challenging task (Rouquet and Nguyen, 2009b).
Basically, any lexical resource can be repre-
sented in an ontology language as a graph. We
propose to use an OWL version of the UW vol-
ume available on Kaiko website 7. It allows us
5http://www.w3.org/
6http://www.w3.org/2004/OWL/
7http://kaiko.getalp.org
to benefit of classical ontology matching tech-
niques and tools (Euzenat and Shvaiko, 2007)
to represent, compute and manipulate the align-
ment. We implemented two string based match-
ing techniques on top of the alignment API (Eu-
zenat, 2004). Specific disambiguation methods
are in development to improve the alignment
precision. Some of them are based on conceptual
vectors presented in section 4, others will adapt
structural ontology matching techniques. This
approach to match an ontology with a lexical re-
source is detailled in (Rouquet et al, 2010).
5.3 The generic extractor
In the case of the OMNIA project, the system
output format is constraint by the goal of an in-
tegration with visual analysis results, in a larger
multimodal system. The visual analysis systems
are also based on concept extraction, but does not
need an ontology to organise concepts. There-
fore, our results has to remain autonaumous,
which means without references to the ontology
used to extract concepts. So, we use a simple
concept vector as output, with intensity weights;
practically, a simple data-value pairs sequence
formatted in XML.
Concept extraction is achieved through a 3
steps process, has shown in figure 4.
1. Concept matching: each UW in the Q-
Graph, that matches a concept according to
the UW-concept map, is labelled with this
concept.
2. Confidence calculation: each concept la-
bel is given a confidence score, in accor-
dance with the score of the UW carrying the
concept, obtained after disambiguation, and
pondered according to the number of UWs
in the Q-Graph. It is planed to take into ac-
count a few linguistics hints here, such as
negations, and intensity adverbs.
3. Score propagation: because we need au-
tonomous results, we have to perform all
ontology-based calculation before releasing
them. The confidence scores are propagated
in the ontology concept hierarchy: for each
57
labelled concept, its score is added to the
super-concept, and so on.
The ontology and the derivated UW-concept
map are considered as parameters for the treat-
ments, and may be replaced in accordance with
the domain, and the relevance of the concepts
and their hierarchy, according to the task.
ConceptnstsQ-sG-Qreon
ConrtnrtarhQsreons xiQrs enL
msohtxhoxQLQreon
DbNhQx 
ConstxrR
qubConstxrUQx
Wnro-oLd
Figure 4: Detail of concept extraction.
6 Experiments
For a first experiment, we used a small dataset,
containing:
? a sub-corpus of 1046 English companion
texts from CLEF09 corpus (press pictures
and captions of about 50 words),
? a 159 concepts ontology, designed for pic-
ture and emotions depiction,
? a UW-concept map comprising 3099 UW.
It appeared that, with this parameters, con-
cepts where extracted for only 25% of the texts.
This preliminary result stressed the importance
of recall for such short texts. However, there
were many ways to improve recall in the system:
? improve the ontology, in order to better
cover the press domain;
? significantly increase the quantity of UW
linked to concepts (only 3099 obtained for
this experiment), by considering synonyms
during the linking process;
? using UW restrictions during concept
matching for UW that are not directly
linked to a concept, as these restrictions are
a rich source of refined semantic informa-
tion.
A second experiment with an improved on-
tology, including 732 concepts, and the use of
UW restrictions, showed very promising results.
Concepts were retrieved from 77% of texts. The
remaining texts were very short (less than 10
words, sometime just date or name).
For example, we extracted the following con-
cepts from the picture and companion text repro-
duced in figure 5.
CoCncepetnntnstQepe-CGraCahexiC eLexmDbNeRquUWedNyeMWOUmeDelqmymDNyeqgexmDbNeImUdNOUWye?OODuerMddUNWeNWeDeRDNyNW?mqqueNWe-D?ODO?deD??WdMme?dlNyD?tQe?lyUu?metnnt??OODuerMddUNWeNdeOq?UO?elMmdMNW?ey?UeOU??qluUWyeqgeRUDlqWdeqgeuDddeOUdymM?NqWeDWOeRN?eOqe?de?Udyeyqe?NOUey?uegmque?eNWdlU?qmdhey?e-mNyNd???mWuUWye?DNuUOeNWeDeccplD?eOqddNUmeuDOUelM????dye?qMmde?UgqmUeDedlU?D?erqMdUeqge?quuqWdeOU?DyUeqWexmDb?xmDbNe?M?MmUe?NWNdyUmerDuDOe?MddUgerDuuDONe???Oey?e-mNyNd?D?U?yNqWde??DdU?dd??eeeee?CeIr??C?xeCoCaeCoCa
Figure 5: Picture document and companion text
example.
CONCEPT WEIGHT
BUILDING 0.098
HOSPITAL 0.005
HOUSE 0.043
MINISTER 0.016
OTHER BUILDING 0.005
PEOPLE 0.142
PERSON 0.038
POLITICS 0.032
PRESIDENT 0.016
RESIDENTIAL BUILDING 0.043
WOMAN 0.005
As this results were more consistent, we could
have a preliminary survey about precision, on a
30 texts sample. While disambiguation imple-
mentation is still at an early stage, weights were
not yet taken into account. A concept match can
be considered correct following two criterons :
1. Visual relevance considers a concept as
correct if carried by an element of the pic-
ture; for instance, the match of concept
58
?SPORT? is regarded as correct for a pic-
ture containing a minister of sports, even if
not actually performing any sport.
2. Textual relevance considers a concept as
correct if carried by a word of the text,
as parts of texts may involve concepts that
are not actually present in the picture, such
as contextual information, previous events,
etc.
124 concepts were found in 23 texts (7 texts had
no concept match):
1. 99 concepts were correct according to the
visual relevance,
2. 110 were correct according to the textual
relevance,
3. 14 were totally incorrect.
We thus have an overall precision score of 0.798
according to the visual relevance and 0.895 ac-
cording to the textual relevance. Most of the er-
rors where caused by ambiguity problems, and
may be addressed with disambiguation process
that are not fully implemented yet.
7 Conclusion and perspectives
We exposed a generic system designed to extract
content (in the form of concepts) from multi-
lingual texts. Our content extraction process is
generic regarding to two aspects :
? it is language independent, as it process an
interlingual representation of the texts
? the content to be extracted can be specified
using a domain ontology as a parameter
This is an ongoing work, and disambiguation
through conceptual vectors is expected to im-
prove accuracy, giving significant weights to the
hypothetical meanings of words.
In the long run, we will focus on integration
with visual content extractors, speed optimiza-
tion to achieve a real-time demonstrator and de-
tailled evaluation of the method.
References
Aitchenson, J. 2003. Words in the Mind. An Intro-
duction to the Mental Lexicon. Blackwell Publish-
ers.
Baader, De Franz, Diego Calvanese, Deborah
McGuinness, Peter Patel-Schneider, and Daniele
Nardi. 2003. The Description Logic Handbook.
Cambridge University Press.
Blanchon, H. and C. Boitet. 2000. Speech translation
for french within the C-STAR II consortium and
future perspectives. In Proc. ICSLP 2000, pages
412?417, Beijing, China.
Boitet, Christian, Igor Boguslavskij, and Jesus
Carden?osa. 2009. An evaluation of UNL usabil-
ity for high quality multilingualization and projec-
tions for a future UNL++ language. In Computa-
tional Linguistics and Intelligent Text Processing,
pages 361?373.
Colmerauer, A. 1970. Les syste`mes-q ou un for-
malisme pour analyser et synthe?tiser des phrases
sur ordinateur. de?partement d?informatique de
l?Universite? de Montre?al, publication interne, 43,
September.
Daoud, Daoud. 2006. Il faut et on peut constru-
ire des syste`mes de commerce e?lectronique a` inter-
face en langue naturelle restreints (et multilingues)
en utilisant des me?thodes oriente?es vers les sous-
langages et le contenu. Ph.D. thesis, UJF, Septem-
ber.
Deerwester, Scott C., Susan T. Dumais, Thomas K.
Landauer, George W. Furnas, and Richard A.
Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society of In-
formation Science, 41(6).
Euzenat, Je?ro?me and Pavel Shvaiko. 2007. Ontology
matching. Springer, Heidelberg (DE).
Euzenat, Je?ro?me. 2004. An API for ontology align-
ment. In Proceedings of the 3rd International
Semantic Web Conference, pages 698?7112, Hi-
roshima, Japan.
Fielding, Roy T. 2000. Architectural styles and the
design of network-based software architectures.
Ph.D. thesis, University of California.
Greimas, Algirdas Julien. 1984. Structural Seman-
tics: An Attempt at a Method. University of Ne-
braska Press.
Hajlaoui, Najeh and Christian Boitet. 2007. Portage
linguistique d?applications de gestion de contenu.
In TOTh07, Annecy.
59
Harris, Zellig S., Michael Gottfried, Thomas Ryck-
man, Paul Mattick Jr., Anne Daladier, T.N. Har-
ris, and S. Harris. 1989. The form of Information
in Science, Analysis of Immunology Sublanguage,
volume 104 of Boston Studies in the Philosophy of
Science. Kluwer Academic Publisher, Dordrecht.
Hjelmlev, Louis. 1968. Prole?gole`me a` une the?orie
du langage. e?ditions de minuit.
Jesus Carden?osa et al 2009. The U++ con-
sortium (accessed on september 2009).
http://www.unl.fi.upm.es/consorcio/index.php,
September.
Luca Marchesotti et al 2010. The Omnia project
(accessed on may 2010). http://www.omnia-
project.org, May.
Max Silberztein. 2009. NooJ linguistic
software (accessed on september 2009).
http://www.nooj4nlp.net/pages/nooj.html,
September.
Metze, F., J. McDonough, H. Soltau, A. Waibel,
A. Lavie, S. Burger, C. Langley, L. Levin,
T. Schultz, F. Pianesi, R. Cattoni, G. Lazzari,
N. Mana, and E. Pianta. 2002. The Nespole!
speech-to-speech translation system. In Proceed-
ings of HLT-2002 Human Language Technology
Conference, San Diego, USA, march.
Nguyen, H.T., C. Boitet, and G. Se?rasset. 2007. PI-
VAX, an online contributive lexical data base for
heterogeneous MT systems using a lexical pivot.
In SNLP, Bangkok, Thailand.
Nguyen, Hong-Thai. 2009. EMEU w,
a simple interface to test the Q-
Systems (accessed on september 2009).
http://sway.imag.fr/unldeco/SystemsQ.po?localhost=
/home/nguyenht/SYS-Q/MONITEUR/, Septem-
ber.
Rouquet, David and Hong-Thai Nguyen. 2009a.
Interlingual annotation of texts in the OMNIA
project. Poznan, Poland.
Rouquet, David and Hong-Thai Nguyen. 2009b.
Multilingu??sation d?une ontologie par des core-
spondances avec un lexique pivot. In TOTh09, An-
necy, France, May.
Rouquet, David, Cassia Trojahn, Didier Scwab, and
Gilles Se?rasset. 2010. Building correspondences
between ontologies and lexical resources. In to be
published.
Salton, Gerard. 1991. The Smart document re-
trieval project. In Proc. of the 14th Annual Int?l
ACM/SIGIR Conf. on Research and Development
in Information Retrieval, Chicago.
Schwab, Didier and Mathieu Lafourcade. 2007. Lex-
ical functions for ants based semantic analysis. In
ICAI?07- The 2007 International Conference on
Artificial Intelligence, Las Vegas, Nevada, USA,
juin.
Terence Parr et al 2009. ANTLR parser
generator (accessed on september 2009).
http://www.antlr.org/, September.
Uchida Hiroshi et al 2009. The UNDL
foundation (accessed on september 2009).
http://www.undl.org/, September.
Wierzbicka, Anna. 1996. Semantics: Primes and
Universals. Oxford University Press.
60
Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 79?87,
Beijing, August 2010
Multilinguization and Personalization of NL-based Systems Najeh Hajlaoui GETALP, LIG, UJF 385 rue de la Biblioth?que, BP n? 53 38041 Grenoble, cedex 9, France Najeh.Hajlaoui@imag.fr Christian Boitet GETALP, LIG, UJF 385 rue de la Biblioth?que, BP n? 53 38041 Grenoble, cedex 9, France Christian.Boitet@imag.fr  Abstract Linguistic porting of content manage-ment services processing spontaneous ut-terances in natural language has become important. In most situations, such utter-ances are noisy, but are constrained by the situation, thus constituting a restricted sublangage. In previous papers, we have presented three methods to port such sys-tems to other languages. In this paper, we study how to also personalize them by making them capable of automatic per-ception adaptation, using fuzzy evalu-ation functions. We have reengineered IMRS, a music retrieval NL-based sys-tem, to implement that idea, and ported it to French, English and Arabic using an enhanced version of our external porting method, building a unique content extrac-tor for these three languages. More than 30 persons participated in a preliminary on-line qualitative evaluation of the sys-tem.  1 Introduction Multilingualizing systems handling content ex-pressed in spontaneous natural language is an important but difficult problem, and very few multilingual services are available today.  The choice of a particular multilingualization process depends on the translational situation: types and levels of possible accesses, available resources, and linguistic competences of participants in-volved in the multilingualization task. Three main strategies are possible in principle for multilingualization, by translation, and by inter-nal or external adaptation.  We consider here the 
subproblem of linguistic porting, where the con-tent is adapted to another language, but not ne-cessarily to a different cultural environment. We also try to add some level of personalization, by automatic perception adaptation, based on the use of fuzzy evaluation functions. We use the example of IMRS, an Impression-based Music-Retrieval System (Kumamoto, 2004), with a na-tive interface in Japanese, which we have reengi-neered and ported to French, English and Arabic.   The context and objectives of our work are pre-sented in the second section. The third section presents the IMRS original prototype and the possible strategies to achieve porting and person-alization. In the fourth section, we give detailed specifications of our reengineered music retrieval system, IMRS-g. In the fifth section, we present the implementation of five music retrieval modes. Finally, we report on the multilingual porting of this system. 2 Methods for porting NL-based con-tent processing systems The choice of a method for multilingualizing e-commerce services based on content extraction from spontaneous texts depends on two aspects of the translational situation: ? The level of access to resources of the initial application. Four cases are possible: complete access to the source code, access limited to the internal representation, access limited to the dictionary, and no access. In the case of IMRS, the access was limited to the internal representation, visible as a non-linguistic interface in the original prototype (a set of 10 impressions manipulate by a set of 7 check-box).  ? The linguistic qualification level of the per-sons involved in the process (level of know-
79
ledge of the source language, competence in NLP) and the resources (corpora, dictionaries) available for the new language(s), in particu-lar for the sublanguages at hand. We concentrate on NLP-based systems that perform specific tasks in restricted domains. Figure 1 shows the general structure of these sys-tems. Examples of such applications and services are: categorization of various documents such as AFP (Agence France Presse) flash reports or cus-tomer messages on an ASS (After Sale Service) server, and information extraction to feed or con-sult a database (e.g. classified ads, FAQ, auto-mated hotlines). 
 Figure 1: general structure of an NLP-based CMS  We first studied linguistic porting of e-commerce systems handling spontaneous utter-ances in natural languages, that are often noisy, but constrained by the situation, and constitute a more or less restricted sub-language (Kittredge, 1982), (Harris, 1968) (Grishman and Kittredge, 1986). This kind of system uses a specific content representation on which the functional kernel works. In most cases, this content representation is generated from the native language L1 by a content extractor. In our PhD, we have identified three possible methods of linguistic porting, and have illustrated them by porting to French CATS (Daoud, 2006), a Classified Ads Transaction System in SMS (Arabic) deployed in Amman on Fastlink, as well as IMRS, mentioned above. The three possible strategies for linguistic porting are internal porting, external porting and porting by machine translation. Figure 2 shows an example of the car domain with the output of the content extractor (CRL-CATS).  In CRL-CATS (Content Representation Lan-guage for CATS), a posted SMS is represented as a set of binary relations between objects. It is a kind of semantic graph with a UNL-like syntax (Uchida and Zhu 2005-2006). There are no vari-
ables, but the dictionary is used as a type lattice allowing specialization and generalization.  ;Selling Renault Megane m 2000 [S] sal(saloon:00,sale:00) mak(saloon:00,RENAULT(country<France, county<europe):07) mod(saloon:00,Megane(country<France, country <europe,make<RENAULT):0C) yea(saloon:00,2000:0K) [/S] Figure 2: Example of SMS  2.1 Internal porting The first possibility consists in adapting the ori-ginal content extractor of the application from L1 to the target language L2 (see Figure 3); but that is viable only if : ? the developers agree to open their code and tools,  ? the code and tools are relatively easy to un-derstand, ? the resources are not too heavy to create (in particular the dictionary).  That method requires of course training the lo-calization team with the tools and methods used. Under these conditions, adaptation can be done at a very reasonable cost, and further main-tenance. 
 Figure 3: internal porting We have previously experimented this method (Hajlaoui, 2008) by porting CATS from Arabic to French: for that, we adapted its native Arabic content extractor, written in EnCo1 (Uchida and Zhu 1999), by translating its dictionary, and modifying a few analysis rules.                                                  1 EnCo is a tool based on rules and dictionaries used for content extraction in original version of CATS system. 
80
2.2 External porting If there is access only to the internal content rep-resentation, the solution consists in adapting an available content extractor for L2 to the sublan-guage at hand, and to compile its results into the original content representation (see Figure 4). For a company wanting to offer multilinguali-zation services, it would indeed be an ideal situa-tion to have a generic content extractor, and to adapt it to each situation (language, sublanguage, domain, content representation, task, other con-straints).  However, there is still no known ge-neric content extractor of that power, and not even a generic content extractor for particular languages, so that this approach cannot be con-sidered at present. Our approach is then to adapt an existing content extractor, developed for L2 and a different domain/task, or for another lan-guage and the same domain/task.  We also applied this method to port CATS from Arabic to French, and experimentation are described in (Hajlaoui, 2008). 
 Figure 4: external porting 2.3 Porting by machine translation If there is no access to the code, dictionary, and internal content representation of the original application, the only possible approach to port it from L1 to L2 is to develop an MT system to automatically translate its (spontaneous) inputs from L2 into L1 (see Figure 5).   Porting CATS from Arabic to French by stat-istical translation gave a very good performance, and that with a very small training corpus (less than 10 000 words). This proves that, in the case of very small sub-languages, statistical transla-tion may be of sufficient quality, starting from a corpus 100 to 500 smaller than for the general language. 
 Figure 5: porting by machine translation 2.4 Results and evaluation  We translated manually the evaluation corpus used for the evaluation of CATS Arabic version. It contains 200 real SMS (100 SMS to buy + 100 SMS to sale) posted by real users in Jordan.  We spent 289 mn to translate the 200 Arabic SMS (2082 words is equivalent to 10 words/SMS, approximately 8 standard pages2) into a French translation, or about 35 mn per page, and 10 mn per standard page to pass from raw translation to functional translation.  We obtained 200 French SMS considered to be functional (1361 words, or about 6,8 words/SMS, approximately 5 standard pages). We then computed the recall R, the precision P and the F-measure F for each most important property (action ?sale or buy?, ?make?, ?model?, ?year?, ?price?). P = |correct entities identified by the system| / |entities identified by the system|;  R = |correct entities identified by the system| / |entities identified by the human|;  F-measure = 2PR/(P+R) Table 1 summarizes the percentage (F-measure ratio) of the Arabic-French porting of CATS and shows details in (Hajlaoui, 2008). Properties having numbers as values, like price and year, lower the percentage of porting by ex-ternal porting, but the advantage is that method requires only accessing the internal representa-tion of the application.  Minimum Average Maximum Internal porting 95% 98% 100% External porting 46% 77% 99% Porting by statis-tical translation 85% 93% 98% Table 1: evaluation of three methods used for porting CATS_Cars from Arabic to French.                                                  2 Standard page = 250 words 
81
In the third part of this article, we describe the multilinguization of IMRS, IMRS-g, which in-cludes a module of queries management, where the queries are expressed either in a natural lan-guage or in a graphical interface showing 10 vec-tors corresponding to the internal content repre-sentation. In response to a query, the user re-ceives a set of music pieces that correspond to her/his desired selection criteria.  In addition to the original design, where the NL expressions of the 10 measures are mapped in a fixed way to the integers in the real interval [1, 7], we have tried to apply a small part of the theory of fuzzy sets to improve the representa-tion and evaluation of human perceptions. 3 Multilinguization of IMRS To port IMRS to several languages, we used the external porting method and we built a new con-tent extractor, which treats simple utterances re-lated to the music domain. 3.1 IMRS IMRS (Kumamoto and Ohta, 2003) is a non-deployed Web service prototype, developed as an experimental base for a PhD. It allows to re-trieve music pieces either by using Japanese queries, or by manipulating a graphical interface with 10 criteria settable by knobs (speed, noise, rhythm...), and showing remarkable values (inte-gers between 1 and 7) expressed by English la-bels. In IMRS, an utterance processed by the sys-tem is a spontaneous sentence or fragment of a sentence. The content extractor transforms it into a vector of 10 real numbers in the interval [1, 7]. The symbol nil means don?t care.  The 10 components are called Noisy-Quiet, Calm-Agitated, Bright-Dark, Refreshing-Depressing, Solemn-Flippant, Leisurely-Restricted, Pretty-unattractive, Happy-Sad, Re-laxed-Aroused, The mind is restored-The mind is vulnerable. Each has associated grades (inter-preted as "concepts" below). For example, the component Happy-Sad is characterized by the seven grades: very happy, happy, a little happy, medium, a little sad, sad and very sad. In the ori-ginal IMRS, these values always correspond to the integers in the [1, 7] interval, respectively 7.0, 6.0, 5.0, 4.0, 3.0, 2.0 and 1.0.  
A request to find a piece of music that gives a happy impression (happy) corresponds to the 10-dimensional vector as follows: (nil nil nil nil nil nil nil 6.0 nil nil) (Kumamoto, 2007), but the music pieces can be described by vectors having non-integer components.  Although we had a quite precise description of the internal representation used by IMRS. We could not find information on the rest of the sys-tem. Hence, we recreated it to emulate the func-tions described in the original publications. That includes the system architecture, the design and implementation of the database, the management of requests, and the programming of actually much more than the originally proposed service. By definition, linguistic porting consists in making an application existing in some language L1 available in another language L2, within the same context. Evaluation of the linguistic porting of a content management application can be done at two levels.  ? Evaluation at the internal representation level. It is an evaluation at the level of com-ponents.  ? Evaluation at the task level. It is an end-to-end evaluation of the new version (in L2) of the application.   To make an end-to-end evaluation of IMRS, an IMRS Web-based simulator was developed. It makes it possible to evaluate in context the result of linguistic porting (Japanese ! French, Arabic, English). A real database with real music pieces, characterized by 10-dimensional vectors as in IMRS, was also created.  The aim of the multilinguization was however not to develop an application strictly equivalent to IMRS, with the addition of being able to han-dle queries expressed in French, English and Arabic, but to develop an upward compatible, extended application. In particular, we wanted to add other dimensions corresponding to the type of music, the composer, the period of compo-sition, the instruments used, etc. We also wanted to experiment the possibility to associate to each impression such as happy a fuzzy set over [1,7] expressed by a membership function (into [0,1]). More details are given below. 3.2 Our IMRS-g system  With the help of a Master student in computer science, Xiang Yin, we have programmed in 
82
PHP/MySQL a Web service called IMRS-g, re-implementing as accurately as possible the sys-tem IMRS, and generalizing it. Not having sufficient expertise in Japanese, we replaced Japanese by French. We also ad-apted the NLP part to English and Arabic, using the same strategy to handle the three languages. We then generalized the internal representa-tion by adding other search criteria (such as the type of music, the composer, the period of com-position, and the instruments used), and using fuzzy sets. A large set of music pieces was loaded into the database, and labelled by vectors in a collabor-ative way. An evaluation of the French version was then conducted as part of a realistic use, with students listening to music. The first part of the linguistic porting has been very rapid, since it consisted only in translating into French and Arabic the NL labels expressing impressions (Noisy/Quiet, Calm/Agitated, Sad/Happy, etc.), by associating them the same values as in IMRS. The content extractor processes simple utter-ances and extracts from them a 10-dimensional IMRS vector, and the additional information in the form (lists of) of items. As in IMRS, a request for a music piece can be made either by typing a query in natural lan-guage, or through a graphical interface allowing to manipulate a 10-dimensional vector, and to fill fields for the other types of information.  In response, the user receives a list of links to music pieces corresponding to its selection cri-teria. Clicking on a link starts the playing of the corresponding music piece. 3.3 Generalization by fuzzying the interpre-tation of the NL labels  The original representation of IMRS seems too rigid to express utterances like quite calm or to change the current request using an utterance like a little slower. Even if we agree that each term corresponds to an interval of length 1 centred on its reference value, e.g. [5.5, 6.5[ for happy, [6.5, 7.5] for very happy, etc., there are problems at the extremities. Therefore we studied the possi-bility of better modelling and better processing the requests by using fuzzy logic (Zadeh, 1965).  In order to reason from imperfect knowledge, in contrast to classical logic, fuzzy logic pro-
poses to replace the Boolean variables used in classical logic by fuzzy variables, and the classi-cal crisp sets by fuzzy sets.  Let U be a universe of elements. A fuzzy set A over U is defined by its membership function (fA). An element x of U is in A with a degree of membership fA(x) " [0, 1]. If fA(x) " {0, 1}, A reduces to a classic set, where x " A if fA(x)=1 and x # A if fA(x)=0 (fA is then simply the char-acteristic function of A).  In a fuzzy set, an element x more or less be-longs to the concept associated to A, or to the concept attached to A (such as happy). A fuzzy set is defined by all values of its membership function on its definition domain (which may be discrete or continuous). For example, the concept young might be de-fined over the universe U of possible (integer) ages U = [0, 120] by the discrete fuzzy set A = ((10 1), (20 0.8), (30 0.6), (40 0.2), (50, 0.1), (60 0), (70 0), (80 0)). The first pair means that a 10-year old is 100% young, and the fifth that a 50-year old is 10% young.  Using fuzzy logic, we could say that a piece of music is 100% rapid if its tempo is 100 (100 crotchets (quarter notes) per minute), with a bell-shaped curve for the membership function, rising from 0 to 1 and then falling, in the range [84, 112]. Then, rapid might be understood as im-pression of rapidity. As the impression of ra-pidity may differ from person to person, that curve may differ accordingly.   Figure 6. Representation of the rapidity impression  We propose to incorporate the possibility to move from the perceptions to digital measure-ments and to personalize the system by learning parameters of each curve of this type for each user. Such a curve can be characterized by a small number of significant points, such as the maximum, 2 points at the 10% below the maxi-mum, 2 minima on each side, and 2 points at 10% above the global minimum.  
 
rapide  0  40 60 84 100 112 140 160 176 1   0 
83
To find the criteria for each piece of music, we have developed a website to ask a group of peo-ple to listen to music pieces and to give their opinions in terms of impressions, knowing that they will not have the same taste and the same perception. For example, for the same piece, a first listener will say that it is rapid, and a second will find it very rapid. The question here is how to merge these different views into a single im-pression. We propose two solutions: (1) con-struct a fuzzy set which is the average of those of annotators, possibly by giving greater weight to the annotations of confirmed annotators, (2) build several perception types, i.e. several fuzzy sets corresponding to subgroups of users with similar perceptions. We know that the Japanese persons find only slow pieces of music that Wes-terners find very slow. In this work, we have taken into account pre-vious queries of users or the history of users. For example, if a user requests a piece a little less noisy, or a little more calm, we should be able to use the information saved in his history, and cal-culate the new request taking into account the perceptions associated to the last piece of music listened to.  4 Specification and implementation We specified and implemented a multimedia database for storing music pieces, as well as in-formation representing the impressions of each piece. As said above, we added to the 10 features of IMRS other information types:  singer, poet, composer, genre, album and duration, for each music piece. Moreover, to evaluate music, we stored the values of the impressions recorded by contributing users for each piece. These values were used to produce the final values stored in the database. To analyze the impressions of users, we requested further information from each user, as gender and age. We loaded our database with a set of 354 pieces (89 Western, 265 Eastern) and all infor-mation related to each piece (artist, album, genre...). The duration of individual pieces varies between 48 seconds and 22 minutes. The website has a login page that allows a se-cured access for each user. For a first connection, the user must register and fill some information from which we compute and store a profile.  
If the connection is successful, a list of pieces is displayed. For each piece, a link allows listen-ing to the music and also opens a new page pro-viding an adapted evaluation interface appropri-ate to the evaluation task.  In the evaluation phase, the user can listen to the selected piece and evaluate it according to the 10 IMRS basic criteria (soft, calm, happy...). For each criterion, we offer a range of values and the user can move a cursor and put it on the value that represents its perception. Next, we propose several ways to search for music pieces. The cost of multilinguization of the IMRS sys-tem was 3 man-months. To this cost, we add 1 man-month for the development and integration task of the content extractor for the three lan-guages (French, Arabic, English). 5 Music retrieval modes  After registering and connecting, users listen to and evaluate music. The evaluation information is recorded directly in the database.  For each dimension, we compute the average of the obtained values. This phase is temporary pending further evaluations to draw the curves associated to each dimension and to each piece. We defined and implemented five possibilities to search music: by user profile, by impressions, by selecting criteria, by input utterances, and by history. 5.1 Search by user profile  We propose to users music adapted to their pre-ferences recorded in their profiles. The method follows the following steps:  ? Find the user profile (ten values that represent his basic impressions) in the database.  ? Compute the Euclidean distance between the two vectors formed by the 10 values of profile and the 10 values of each music piece (see ex-ample below).  ? Sort pieces by distances in ascending order.  ? View the nearest 10 pieces.  Here is an example: User profile: impressions vector  Profile = (Nil 6  3 Nil 2  1  3  5 Nil Nil)  Piece impressions (existing impressions vector):  Piece1 =  (3.5 Nil 2.3 5.0 3.2 Nil 2.6 Nil 6.0  1.4)  
84
Euclidian distance (d):  d= ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( )2222222222 4.1464456.23412.32543.23465.34 !+!+!+!+!+!+!+!+!+!   => d = 5.3,  Note: if  value = ?Nil?, we put value :=  4. 5.2 Search by impressions We ask the user to place the cursors on the values that represent his perception. We can limit ourselves to a particular type of music (Western music, Eastern music or light music). The search method has the following steps:  ? Choose the kind of music (Western music, Eastern music or light music).  ? Place one or more cursors on the values that represent user?s perception.  ? Compute the Euclidean distance between the two vectors formed by the 10 values of search and the 10 values of each piece. ? Sort pieces by distances in ascending order. ? View the nearest 10 pieces.  Here is an example: we search a noisy (fort) and somewhat calm (assez calme) piece (see Figure 7).   
 Figure 7. Example of search by impressions  The result of the previous request is a set of 10 music pieces. 5.3 Search by selection criteria  We offer four search criteria: artist, album, genre and creation date. The search methods for each of these criteria are similar.  For example, the search by artist follows the following steps:  ? Search all artists (singers) existing in the database.  ? Choose an artist from this list.  ? Search all pieces performed by this artist and show them (by links). 
5.4 Search by input utterances  The content extractor works for French, Arabic and English, and handles simple utterances re-lated to the music domain. This program takes as input a corpus of music pieces and gives as out-put a file containing the corresponding vector representations.  The search method has the following steps:  ? Enter an utterance in natural language repre-senting impressions of music search.  ? Call a content extractor. The result, which contains a vector representing the desired per-ceptions, is stored in a text file.  ? Extract the vector from the text file.  ? For each value of the vector (Vv), if one of the symbols (+, + +, -, --,?) appears, then we extract the value of the last search of the con-cerned user (Vo: old value) from the database to compute the new value of search (Vn: new value).  Here are some examples of utterances that cor-respond to the precedent symbols. +: more noisy, ++: still more noisy, -: less noisy, --: still less noisy, ?: not noisy.  We treat these symbols with the following rules:  If (Vv == ?+?) {Vn = Vo + $ ;} If (Vv == ?++?) {Vn = Vo + 2$ ;} If (Vv == ?-?) {Vn = Vo - $ ;} If (Vv == ?--?) {Vn = Vo - 2$ ;} If (Vv == ??x?) {Vn = 7 - x ;} If (Vn > 7) {Vn = 7 ;} ? Compute the Euclidean distance between the two vectors formed by the 10 desired values and the 10 values of each piece. ? Sort music by distances in ascending order. ? View the nearest 10 pieces. 5.5 Search by history We extract from the history of each user five types of information:  (a) the kind of desired pieces, (b) their creation date, (c) the artists (per-formers), (d) the liked albums, (e) the favourite impressions.  The search method has the following steps:  ? Search the user?s history in the database and check if the user has already searched with the five previous conditions. ?  If the user has searched for condition (a) or (b) or (c) or (d), we extract the last value of found for each of them. 4 values are obtained. 
85
 If the user searches by impressions (condition (e)), we compute for each dimension the aver-age that represents the history of the searches.  ? Search for music using the values obtained at step 2.   If (e) is verified, we compute the Euclidean distance between the average of impressions representing the history and impressions exist-ing in the database.  If (e) is not verified, we look for pieces, using only the 4 values obtained by the conditions ((a), (b), (c), (d)).  Here an example of a history of one user. For condition (a), the latest search value is ?Pop?. For condition (b), there is no value, i.e. the user did not search by creation date. For condition (c), the latest search value is ?1? (number of the ar-tist). For condition (d), the latest search value is ?2? (number of the album). For condition (e), there are 3 vectors in the search history: V1=(2 5 Nil 3 Nil 2 7 1 Nil Nil) V2=(3 Nil 4.5 2.5 Nil 3.1 6.4 Nil 5 2) V3=(3.5 4.3 Nil 2.1 Nil Nil Nil 3 Nil Nil)  We compute the average of the history, Vm:  Vm=(2.83 4.65 4.5 2.53 Nil 2.55 6.7 2 5 2)  We search for pieces that verify the complex condition: (Kind of music = 'Pop') AND (Num-ber of the artist ='1') AND (Album ID ='2') AND (Impressions vector is closest to Vm according to the Euclidean distance). If the search is successful, then the result is optimal. Otherwise, we search pieces that corres-pond to the second condition: ((Kind of music = 'Pop') OR (Number of the artist ='1') OR (Album ID ='2')) AND (Impressions vector is closest to Vm according to the Euclidean distance). We refined this search through other combina-tions formed by the conditions (a), (b), (c), (d) and (e) and differentiated by the OR and AND operators. 6 Multilingual porting To build our content extractor, we started from a content extractor for French we had previously develop for the same domain, integrated it into IMRS-g, and extended it as explained above (more information type, and fuzzy sets). We then ported it to English and to Arabic, using the sec-
ond technique of external porting (when one has access to the internal representation). Seeing the large percentage of common code to the 3 content extractors obtained, we factor-ised it and obtained a unique content extractor handling input utterances in the music domain in our 3 target languages: French, English and Ara-bic. This technique is perhaps not generalizable, but it works for this sub-language, which is very small, and for the simple task of extracting in-formation representable in very small textual fragments. Here are some examples of results for Arabic, French and English: Exemple_Ar 1 : //je veux un morceau de musique tr?s calme Musique_Ar 1: musique-spec=(nil 7,0 nil nil nil nil nil nil nil nil) Exemple_Ar 2 : //je veux un morceau de musique un peu bruit? Musique_Ar 2: musique-spec=(3,0 nil nil nil nil nil nil nil nil nil) Exemple_Fr 1:je veux un morceau de musique calme et tr?s solennel Musique_Fr 1: musique-spec=(nil 6,0 nil nil 7,0 nil nil nil nil nil) Exemple_Fr 2:je veux un morceau de musique assez fort et clair Musique_Fr 2: musique-spec=(3,0 nil nil 6,0 nil nil nil nil nil nil) Exemple_En 1:I want a calm and very solemn music Musique_En 1: music-spec=(nil 6,0 nil nil 7,0 nil nil nil nil nil) Exemple_En 2:I want a little noisy and bright music Musique_En 2: music-spec=(3,0 nil nil 6,0 nil nil nil nil nil nil) Tableau 1: Examples of results of IMRS-g for Arabic, French and English Conclusion We have presented several possible methods for "porting" applications based on handling the con-tent of spontaneous NL messages in a "native" language L1 into another language, L2.  In a pre-vious paper, we described experiments and evaluations of these methods. We tried to do an ?end-to-end? evaluation of porting IMRS by building a website that pro-poses to engage people in evaluation of a set of music pieces, thereby offering them to search for 
86
music in different possible modes. To that effect, we have produced a functional Web site  (http://www-clips.imag.fr/geta/User/najeh.hajlaoui/Musique/). To date, the evaluation has been done only for French. More than 30 users have participated, perhaps because they were rewarded in a sense: as a kind of compensation, each user could listen to appropriate music adapted to his way of per-ception and taste. The use of fuzzy logic proved useful and was perhaps even necessary to give some freedom of choice of impressions to users. Acknowledgments  We thank Yin Xiang, who dedicated his TER internship during his master in CS to the study and implemention of the IMRS-g system. We would also like to thank our reviewers, who made many constructive remarks and sug-gestions, which we gladly incorporated in this new version of the paper. References  Daoud, D. M. (2006). It is necessary and possible to build (multilingual) NL-based restricted e-commerce systems with mixed sublanguage and contend-oriented methods. Th?se. Universit? Jo-seph Fourier. Grenoble, France. September 23, 2006. 296 p. Grishman, R. and R. Kittredge. (1986). Analyzing language in restricted domains. Hillsdale NJ. Law-rence Erlbaum Associates. 248 p.  Hajlaoui, N. (2008) Multilingu?sation de syst?mes de e-commerce traitant des ?nonc?s spontan?s en langue naturelle. Th?se. Universit? Joseph Fourier. Grenoble. 25 septembre 2008. 318 p. Harris, Z. (1968). Mathematical structures of lan-guage. in The Mathematical Gazette. Vol. 54(388): pp. 173-174. May, 1970. Kittredge, R. (1978). Textual cohesion within sublan-guages: implications for automatic analysis and synthesis. Proc. Coling-78. Bergen, Norv?ge. Au-gust 14-18, 1978. Vol. 1/1.  Kittredge, R. (1982). Variation and Homogeneity of Sublanguages. in Sublanguage - Studies of Lan-guage in Restricted Semantic Domains. Walter de Gruyter. Berlin / New York. 20 p.   Kittredge, R. (1993). Sublanguage Analysis for Natu-ral Language Processing. Proc. First Symposium 
on Natural Language Processing. Thailand, Bang-kok pp. 69-83.  Kittredge, R. and J. Lehrberger (1982a). Sublanguage - Studies of language in restricted semantic do-main. Walter de Gruyter. Berlin / New York. Kumamoto, T. (2004). Design and Implementation of Natural Language Interface for Impression-based Music-Retrieval Systems. Knowledge-Based Intel-ligent Information and Engineering Systems. Springer Berlin / Heidelberg. October 14, 2004. Vol. 3214/2004:  pp. 139-147.  Kumamoto, T. (2007). A Natural Language Dialogue System for Impression-based Music-Retrieval.  Proc. CICLING-07 (Computational Linguistics and Intelligent Text Processing). Mexico. February 12-24, 2007. 12 p.  Kumamoto, T. and K. Ohta (2003). Design and De-velopment of Natural Language Interface for an Impression-based Music Retrieval System. in Joho Shori Gakkai Kenkyu Hokoku. Vol. 4(NL-153): pp. 97-104. Kurohashi, S. and M. Nagao (1999) Manual for Japa-nese Morphological Analysis System JUMAN. Rap. Language Media Lab. School of Informatics, Kyoto University. Kyoto, Japan. November 1999. Uchida, H., M. Zhu, et al (2005-2006). Universal Networking Language 10 2-8399-0128-5. 218 p. Uchida, H. and M. Zhu (1999). Enconverter Specifications, UNU/IAS UNL Center, 33 p. Zadeh, L. A. (1965). Fuzzy sets. Information and Con-trol 8: pp. 338-353. 
87
Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 87?98,
Dublin, Ireland, August 23, 2014.
Jibiki-LINKS: a Tool between Traditional Dictionaries and Lexical Networks for Modelling Lexical Resources  
ZHANG Ying1, 2 Mathieu MANGEOT1 Val?rie BELLYNCK1 Christian BOITET1 1. GETALP-LIG, 41 rue des Math?matiques BP53, 38041 Grenoble Cedex 2. SAS Lingua et Machina, Domaine de Voluceau, Rocquencourt, 78153 Le Chesnay  {ying.zhang, mathieu.mangeot, valerie.bellynck, christian.boitet}@imag.fr  
 Abstract  Between simple electronic dictionaries such as the TLFi (computerized French Language Treasure)1 and lexical networks like WordNet2 (Diller et al., 1990; Vossen, 1998), the lexical databases are growing at high speed. Our work is about the addition of rich links to lexical databases, in the context of the parallel development of lexical networks. Current research on management tools for lexical databases is strongly influenced by the field of massive data ("big data") and by the Web of data ("linked data"). In lexical networks, one can build and use arbitrary links, but possible queries cannot model all the usual interactions with lexicographers-developers and users, that are needed, and derive from the paper world. Our work aims to find a solution that allows for the main advantages of lexical networks, while providing the equivalent of paper dictionaries by doing the lexicographic work in lexical DBs.  1 Introduction  The growing importance of IT in all human activities extends and expands the needs and usages of all key digital resources that include lexical resources. Thus, while applications valuing the linguistic processes rely on increasingly abstract representations, modelled for computer operations, it remains that models coming from the historical construction of resources foster human understanding, and therefore, the building of tools for studies centring on the humanities.  In this this section, we place the emergence of the concept of lexical database between electronic dictionaries and lexical networks. We show that this concept is still valid, that it is still necessary to enrich it, and that our work on improving tools for lexical databases helps solve real problems.  To do this, we analyse in the second section the evolution of lexical resources in 4 main steps (simple electronic dictionaries, simple lexical databases, multilevel and multiversion lexical databases, and lexical networks) and present the associated problems. In the third section, we present Jibiki-LINKS, a platform for building multilingual lexical databases that enriches the Jibiki generic platform by introducing the concept of rich link between the components it manages (dictionary entries and dictionary volumes). Finally, we show that it allows the construction of lexical databases such as Pivax-UNL, which support scaling up.  2 From computerized dictionaries to lexical databases with rich links  The first computerized lexical resources are electronic versions of printed dictionaries, mainly monolingual or bilingual. The use of computers has helped to overcome the constraints of the paper form. The impossibility to inverse bilingual dictionaries led to a model having a "pivot" consisting of axies3. Lexical pivot-based databases are invertible and transitive, but rooted on the form of the 
                                                This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http:// creativecommons.org/licenses/by/4.0/ 1 http://atilf.atilf.fr 2 http://wordnet.princeton.edu 3 "Axie" = "interlingual meaning," by analogy with "lexie". 
87
symbols, while the lexical networks allow a move towards the direct manipulation of semantic tokens, regardless of their surface form, and thus of the language.  In this section, we present the evolution of approaches, distinguishing four main types of lexical resources, the limitations that motivated this evolution, and the remaining hard problems.  2.1 Simple electronic dictionaries  A simple electronic dictionary is an electronic version of a printed dictionary, or the computer representation of a new kind of the same type of dictionary, for example, the TLFi4, the morphological and bilingual dictionaries of Apertium5, etc. A simple electronic dictionary contains either one volume or two volumes. The electronic version of a monolingual paper dictionary is (usually implicitly) based on its microstructure, that is to say, on the organization of its entries in the form of a small tree organizing the information it contains. In a paper dictionary, the presentation of an entry reflects the microstructure, but the microstructure is not always directly retrievable from it (for example, parts in italics can correspond to different types of information units, such as idiom or example of use). In absolute terms, it is always possible to represent the information specified in each entry of a dictionary according to a common structure. In reality, the structures of paper dictionaries are less rigorous than what would be required for automatic processing, so that manual editing is required.  A bilingual paper dictionary is generally based on a structure in two volumes, one for each language pair, each volume conforming to the same microstructure. There are therefore generally one volume from language A (Lg A) to language B (Lg B) and a mirror volume from Lg B to Lg A. We define the macrostructure of a dictionary as the organization of the volumes that make up its structure. These macrostructures constitute the bulk of the printed dictionaries.  2.2 Lexical databases  A lexical database is a tool for unifying any set of dictionaries, where each dictionary can be monolingual, bilingual or multitarget. A multilingual lexical database is composed of volumes that are monolingual, direct multilingual, or indirect multilingual, i.e. connecting the entries of different languages via a pivot structure. It has an overall macrostructure, and a microstructure for each of its volumes. A link between 2 entries is realized by the software tool as a direct link, or as 2 links going through an intermediate language, or as a semantic link, etc.  The lack of symmetry of the correspondence between the entries of bilingual dictionaries (from word senses to words, not word senses) led to the concept of interlingual pivot. In the pivot macrostructure developed and used for the Papillon-NADIA multilingual lexical database (S?rasset and Mangeot, 2001), there is only one monolingual volume for each language. Lexies are word senses (of a lexeme or an idiom) and make up the entries of these volumes. To group the lexies of different languages together, there is a pivot volume of axies (interlingual acceptions). An axie connects synonymous lexies. The links are established only between lexies and axies. This is the simplest macrostructure for a pivot-based multilingual lexical resource that allows for the extraction of usage dictionaries for all pairs in all directions. The concept of axie-based pivot structure has been validated by the Papillon project and then included in the Lexical Markup Framework standard (Francopoulo et al. 2009).  2.3 Multilevel and multiversion databases  In this type of lexical database, several monolingual volumes are allowed for each lexical space6, A volume of axemes (monolingual acceptions) is introduced to link synonymous lexies of the considered lexical space. Also, various levels are introduced to tag entries according to different points of view (sublanguage, version, type of link, reliability, preference). The simple links of previous versions are replaced by rich links that can be established not only between lexies, axemes and axies, but also between entries and subentries, monolingually (lexicosemantic functions) or bilingually (translations).                                                  4 Tr?sor de la Langue Fran?aise informatis?, http://atilf.atilf.fr/ 5 http://wiki.apertium.org/wiki/User:Alessiojr/Easy_dictionary_-_Application-GSOC2010 6 A lexical space of a natural language contains various levels (wordform, lemma, lexie, prolexeme, proaxeme); it can also contain the lexical symbols of an artificial semantic representation language (e.g., the UWs of UNL). 
88
For example, there is a 3-level macrostructure (lexie, axeme, axie) in PIVAX (Nguyen & al., 2007) and a 4-level macrostructure (lexie, prolexeme, proaxie, axie) in ProAxie (Zhang & Mangeot, 2013), described in more detail in section 4.1. Both allow us to manage one or more monolingual volumes for each lexical space. That has been quite useful in the ANR Traouiero GBDLex-UW++ subproject, during which we stored the UNL part of many UNL-Li dictionaries (the UW interlingual lexemes, built with slightly different conventions by different UNL groups for their languages), and tried then to unify them in a new monolingual UNL dictionary (using a set of "UW++" built from WordNet and from the previous UNL dictionaries).  2.4 Lexical network  A lexical network brings together the set of words that denote ideas or realities that refer to the same theme, as well as all the words that, because of the context and certain aspects of their meaning, also evoke this theme7. The theme may possibly be very broad. It is possible to represent the full vocabulary of a language in a lexical network, such as, for French, the JeuxDeMots network (Lafourcade and Joubert, 2010) or RFL (Lexical Network of French (Lux-Pogodalla, Polgu?re 2011)).  Lexical networks are traditionally represented as graphs. Nodes represent the lexemes of one or more languages, and links represent the relationships between these lexemes (translation, synonymy, etc.). A lexical network can be monolingual or multilingual. One can create syntactic, morphological and semantic relations between lexemes.  Although lexical networks have many advantages, they are not suitable for all usages. For example, lexical networks like WordNet (Diller & al., 1990; Vossen, 1998), HowNet (Dong et al., 2010) and MindNet (Dolan and Richardson, 1996) (Richardson et al., 1998) are not browsable in alphabetical order. But we need that possibility to have an idea of the content of a lexical repository, whatever its nature, or to play word games, or to find a word one has on the tip of the tongue8. On the other hand, in a lexical network, the concept of volume is missing, which prevents to create a resource in a simple way when studying a new language.  For example, the lexical network DBNary (S?rasset, 2012), which is based on the Lemon model (McCrae et al., 2011), contains millions of terms, but does not allow labelling the links. To navigate in this system, one must write SPARQL queries, which is not within the reach of everyone.  2.5 Conclusion: features, limitations and hard problems  Research efforts focus today mainly on lexical networks, but much remains to be done on the preceding types (pivot, multilevel). In particular, the import of lexical databases in lexical networks causes a loss of information, especially information born by the attributes of rich links. For example, what concerns the history, the etymology or the evolution of word senses is not systematically imported into lexical networks. They therefore cannot meet the needs of the humanities, nor allow the transition to "digital humanities."  A lexical network is actually the type of structure that enables the greatest freedom of representation. Indeed, we can create entries and links arbitrarily. But the possible queries cannot model all the usual interactions with lexicographers-developers and users, which come from the world of paper, and are felt necessary. They allow us to represent all categories of lexical resources, but the analogy with the real world is lost. Thus, the practical expertise of linguists-lexicographers is lost.  We must continue to equip lexical databases, because that is the right level to transfer the techniques used by lexicographers-linguists. Also, modelling by a volume-based macrostructure allows keeping a link to the original paper world. Moreover, there are already reusable resources of these types. That is why we focus on the management of resources having multiversion and multilevel macrostructures.  3 Reuse of rich links  In this section, we present an improvement that consists in introducing into lexical databases relational                                                 7 http://ddata.over-blog.com/xxxyyy/3/12/82/15/GRAMMAIRE/champs-et-reseaux-lexicaux.pdf 8 For that kind of functionality, multiple sorting on subsets of inflected forms and on arbitray types of information seems to be a necessary first level of computer aid. 
89
information in the form of rich links that will bring them closer to lexical networks. An important point is that these links may bear arbitrary labels.  3.1 Presentation of the Jibiki platform  Jibiki is a generic platform that enables the construction of contributive websites dedicated to the construction of multilingual lexical databases. That platform has been developed mainly by Mathieu Mangeot (Mangeot & Chalvin, 2006) and Gilles S?rasset (S?rasset & Mangeot, 2001). It has been used in various projects (EU LexALP project, Papillon project, GDEF project, etc.). The code is available in open source, and freely downloadable by SVN from ligforge.imag.fr. With this platform, one can perform import, export, edit and search operations in lexical databases. One can also manage the contributions. Jibiki allows handling almost all lexical resources of XML type, by using different microstructures and macrostructures.  In the Jibiki approach, resources are organized in volumes, which makes it easier to achieve the equivalent of paper dictionaries, keeping the mental image of the representation of the dictionary, while offering new interactions allowed in the digital world. Usages of dictionaries in Jibiki are also similar to those of paper dictionaries. For example, one can consult a database in alphabetical order, indicate a source and/or target language, group lexies in vocables, navigate in a volume, etc.  3.2 Classical Common Dictionary Markup  Version 1 of Jibiki uses "CDM pointers" (Common Dictionary Markup (Mangeot, 2002)) to import, view and edit any type of microstructure without modifying it. CDM pointers are also used to index specific parts of the information, and then allow a multi-criteria search.  Each CDM pointer indicates the path (XPath) to the corresponding element in the XML microstructure of the described resource (see Figure 1). Its description is stored in a XML metadata file. When the resource is imported in the Jibiki platform, the pointers are computed, and the result is stored in a table of the (postgresql) database, for each volume. This table is considered as an indexing table.  
 Figure 1: CDM pointers for the French volume of the GDEF9 resource (Mangeot and Chalvin, 2006) 
CDM tags FeM10 (Gut et al., 1996) OHD11  JMdict12 (Breen, 2004) Volume /volume /volume /JMdict Entry /volume/entry /volume/se /JMdict/entry Entry ID /volume/entry/@id  /JMdict/entry/ent_seq/text() Headword /volume/entry/headword/text() /volume/se/hw/text() /JMdict/entry/k_ele/keb/text() Pron /volume/entry/prnc/text() /volume/se/pr/ph/text()  PoS //sense-list/sense/pos-list/text() /volume/se/hg/ps/text() /JMdict/entry/sense/pos/text() Domain  //u/text()  Example //sense1/expl-list/expl/fra //le/text() /JMdict/entry/sense/gloss/text() Table 1: Examples of Common Dictionary Markup                                                  9 GDEF is a large Estonian-French dictionary that is being created by the Franco-Estonian lexicography association (see http://estfra.ee/GDEF.po).  10 FeM is a French-English-Malay dictionary (30000 entries, 50000 lexies, 8000 idioms, 10000 examples of use). 11 OHD is abbreviation of Oxford-Hachette Dictionary, which is a French-English dictionary. 12 JMdict is a Japanese-multilingual dictionary. 
90
The translation links are treated at this stage with conventional CDM pointers, as classical information elements. It is not possible to index other information carried by the links, such as weights or labels.  Hence, multilevel macrostructures cannot be modelled in a generic manner with Jibiki-v1 and traditional CDM pointers. For example, it is not possible to link the same volume to several volumes at different levels. This has forced us initially to use palliatives that did not scale up. It became necessary to modify the conceptual model. We addressed these shortcomings in a new version, Jibiki-LINKS.  Table 1 above is an example of CDM for the different resources.  3.3 New version of Jibiki with CDM LINKS  To manage multilevel macrostructures, we enriched the CDM with a richer description of the links (see Figure 2). For each link, more information can be indexed:  ? the identifier of the source entry.  ? the identifier of the target entry.  ? the identifier of the XML element of the source entry containing the link. For example, the sense number in a polysemous entry having a translation link for each translation direction. That allows us to precisely retrieve the origin of the link.  ? the link name. It is used to distinguish between different types of links in a single entry, such as a translation link and a synonymy link.  ? the target language (three-letter code ISO 639-2 / T).  ? the target volume.  ? the type of link. Some types are predefined, because they are used by the algorithms that compute the rich links (translation, axeme, axie), but it is possible to use other types of links.  ? a label whose text is arbitrary.  ? a weight whose value must be a real number.  These links can be established between two entries of the same volume or between two different volumes. The same volume may group entries connected to several volumes.  To realize the implementation of rich links, we separated the module processing the links from the module processing other CDM pointers. It means we have two CDM tables in the database associated to each volume. The first stores CDM traditional pointers, and the second CDM LINKS. All information of LINKS can be found in this table.  
 Figure 2: CDM-LINKS for the English volume of the CommonUNLDict resource  3.4 Approach by rich links in searching in a complex lexical network  To explain how we create arbitrary links, let us give an example. A free label is available for each link. For example, in a lexical resource including SMS, in French "A+" has a link to "Over" with a "SMS" label, in English "L8R" corresponds to "later" with a "SMS" label, and the label of the link between "Over" and "later" is "translation."  A ProAxie macrostructure (Zhang & Mangeot, 2013) has been implemented on the Jibiki-Links platform. We present another example of rich links for semantic search in section 4.1.  
91
3.5 Algorithms for computing rich links  The computer implementation is based on two algorithms. The first collects the links, and the second builds the result. More precisely, the first looks for all possible links in the set of all rich links of all volumes, for a desired entry. The second recursively performs the following steps: (1) selection of the start entry; (2) search of the links to other entries; (3) treatment of labels; (4) recursive call of the algorithm on the connected entry; (5) integration of the XML code of the entry connected to the start entry; (6) display.  4 Experimentation  4.1 Examples of multilevel macrostructures  We have already installed several multilevel macrostructures on Jibiki-LINKS. Here are 3 examples.  Mot?Mot: trilingual lexical database with a pivot structure (Mangeot & Touche, 2010) This project (2009-2012) has computerized a French-Khmer classical dictionary, initially in Word, into a Jibiki database (see http://jibiki.univ-savoie.fr/motamot/). The macrostructure is composed of a monolingual volume for each language and a central pivot volume. However, in order not to confuse users, the contributing interface shows a classic view of a bilingual dictionary. Each bilingual link language A ? language B added via this interface is actually translated into the background by creating two interlingual links as well as an axie link representing the original translation, to finally get: language A ? pivot axie ? language B (see Figure 3). If a contributor wants to add a translation link between a vocable Va of language A and a vocable Vb of language B, s/he can establish this link at different levels. The ideal solution is to connect a word meaning (lexie) La of the vocable Va to another word meaning Lb of the vocable Vb. In this case, the link is bijective and Lb is also connected to La. If the contributor cannot choose between word meanings, s/he can connect directly the word meaning La to the vocable Vb and the link is tagged for refinement. With the pivot macrostructure, if two links language A ? language B and language B ? language C exist, then it will automatically create a link language A ? language C tagged for refinement. 
 Figure 3: Example of Mot?Mot ProAxie: multilingual extension of ProxlexBase (Tran, 2006) The ProAxie macrostructure aims at solving the problem of linking several terms that refer to one and the same referent, in particular for the management of acronyms (Zhang et Mangeot, 2013). In this macrostructure, there are two different layers. The base layer consists of two types of volume: volumes of lexies and volumes of axies. The axies are used to connect the lexies that match each other exactly. For example, one translates "ONU" by "UN" (see Figure 4) from French into English.  The "Pro" layer allows us to propose to users translations having the same referential meanings. This layer includes the volumes of prolexemes (Tran, 2006) and one volume of proaxies. A prolexeme entry links lexies having the same meaning with a label (aka, acronym, definition, etc.). A proaxie entry connects prolexemes of different languages. If one cannot find the translations directly using the lower layer, one will get the translations proposed by the "Pro" layer.  For example, for "Nations-Unies", translations by "United Nations" and "UN" will be proposed, with the "alias" label.   
92
 Figure 4: Example of ProAxie  For each natural language, there are one or more volumes of lexies, and a single volume of prolexemes. For each dictionary, there is a volume of axies and a volume of proaxies.  This gives three levels of translation, classified according to the precision obtained.  (1) The system finds a lexie directly, using the volume of axies. That is the first and most accurate level of translation.  (2) The system searches a link to the prolexemes volume of the source language with a certain label. When it finds the link in the proaxies volume, it follows the prolexeme link of the target language, and finally arrives at the volume of lexies in the target language, and finds a lexie that has the same label. That is the second, intermediate level.  (3) The system finds the lexies going through prolexemes and proaxies, without a corresponding label. These proposed lexies constitute the third and least accurate level.  Pivax: lexical multilingual multiversion database with 3 levels  The Pivax macrostructure has three levels: lexie, axeme and axie (Nguyen & al., 2007). Axemes are monolingual acceptions, and group monolingual lexies having the same meaning. Axies group synonymous axemes of different languages in a central "hub". In some situations, a lexical database has several volumes for a single language. For example, when there are several editions, or when the lexical resource is created for a machine translation system: one may have one volume coming from Systran, one from Ariane/H?lo?se, one from IATE13, etc. This macrostructure allows us to manage multiple volumes in the same language. Given a language, there are one or more volumes of lexies and a single volume of axemes. For any Pivax database, there is only one volume of axies. The links between the lexies and the axemes and between the axemes and the axies are rich links with attributes such as type, target volume, target language, free label, weight, etc.  4.2 CommonUNLDict: toward scaling up with a resource of Pivax type  In this section, we present the CommonUNLDict resource that uses the Pivax macrostructure. We have implemented this resource on the Pivax-UNL platform, which is an instance of Jibiki-Links. Users can easily use this resource via the link http://getalp.imag.fr/pivax/Home.po.  Resource created by linguists  Thanks to CDM-LINKS, all types of XML formats can be used in an instance of Jibiki-LINKS without modification. One needs only simple knowledge about XML to create a resource for Jibiki-LINKS. In addition, very useful available tools can be used to create an XML file, such as oXygen14 that allows the creation of a DTD using a graphical interface.   
                                                13 "A single database for all EU-related terminology (InterActive Terminology for Europe) in 23 languages opens to the public", 2007) 14 http://www.oxygenxml.com 
93
The CommonUNLDict resource has been created by the Russian lexicographer  and linguist Viacheslav Dikonov (Dikonov & Boguslavsky, 2009).  Figure 5 shows the graph of a monolingual volume structure using oXygen. In this example, each volume contains a large quantity of vocables, and each vocable includes one or more lexie. We will explain this structure in section 3.2.3. 
 Figure 5: Structure of a monolingual volume 
 Figure 6: Macrostructure of CommonUNLDict   
94
Macrostructure of CommonUNLDict  CommonUNLDict contains 8 languages (7 natural languages, French, English, Hindi, Malay, Russian, Spanish, Vietnamese, and the UNL language) and 17 volumes (8 volumes of monolingual data, 8 volumes of monolingual axemes, and 1 volume of axies ("interlingual meanings"). The macrostructure of CommonUNLDict is diagrammed in Figure 6. For each language, there is only one volume of monolingual data (vocables and lexical items) and a single volume of axemes. For the whole CommonUNLDict, there is only one volume of axies.  Microstructure of CommonUNLDict  The microstructure is the structure of the entries (Mangeot, 2001). In the CommonUNLDict resource, there are three types of entries (vocables, axemes and axies) and 720 K entries in total.  See Table 2.  Volume  Language  Entries  CommonUNLDict_axi  axi  82804  CommonUNLDict_eng  English  45471  CommonUNLDict_eng-axemes  English  82069  CommonUNLDict_esp  Spanish  7080  CommonUNLDict_esp-axemes  Spanish  22254  CommonUNLDict_fra  French  27537  CommonUNLDict_fra-axemes  French  48312  CommonUNLDict_hin  Hindi  31255  CommonUNLDict_hin-axemes  Hindi  50380  CommonUNLDict_msa  Malay  37342  CommonUNLDict_msa-axemes  Malay  31699  CommonUNLDict_rus  Russian  28475  CommonUNLDict_rus-axemes  Russian  45020  CommonUNLDict_unl  unl  82804  CommonUNLDict_unl-axemes  unl  82804  CommonUNLDict_vie  Vietnamese  6585  CommonUNLDict_vie-axemes  Vietnamese  8819  Table 2: Number of entries of CommonUNLDict  All volumes of the same type have the same microstructure. The example below (see Figure 7) shows the microstructure of a volume of vocables. Each entry of vocable type allows us to describe all detailed information, such as part of speech (POS), pronunciation, etc. Each vocable includes one or more lexies (word senses). Figure 2 shows an example. Therefore the number of axemes is greater than or equal to the number of vocables. In this microstructure, the "entryref" attribute allows us to manage the links between lexies and the entries of axeme type.  
 Figure 7: Microstructure of a volume of lexies  ? In this example, the value of "type" is the type of link, the value of "volume" is the target volume, the value of "idref" is the identifier of the axeme entry, the value of "lang" is the target language, and the value of "relationship-mono" is the label.  
95
? The microstructure of the entries of axeme type allows us to describe the links with entries of lexie type and the links with entries of axie type. The microstructure of the axies allows us to describe the links with the entries of axeme type.  Response time and use case The tests were performed with an instance of Jibiki-LINKS installed on a machine with an Intel Core i3 processor at 3.3 GHz with 8 GB of RAM.  The tool used to perform queries is wget. The command is run directly on the server to avoid the latency due to the network. We give three examples in Table 3, which show the number of links computed by the system, of entries displayed, of queries, of different languages, and the average response time. The response time, less than 1 second in these cases, is generally satisfactory. For better understanding, there is some details about the example "manger" (see Figure 8). We search "manger" in French, and find one entry with id "fra.manger.v" in the French vocable volume. The search direction is "up". This entry links to another entry of the volume of French axemes, whose id is CommonUNLDict.axeme.fra.eat(icl>consume>do, agt>living_thing, obj>concrete_thing, ins>thing) 15. This axeme entry links with one axie entry and the vocable entry fra.manger.v. Because the search direction is "up", we just go to the axie entry. When we arrive in the volume of axies, the search direction is changed to "down". The axie entry links to 6 different axeme entries. We search each axeme entry and its links. Because the search direction is "down", we only take into account vocable entries links. For each axeme entry, we find at least one vocable entry. In other cases, one vocable entry has more than one lexie, so it links to one or several axeme entries, and there are more links.  Search argument Links  Displayed entries  Number of requests  Different languages  Average time (ms)  French vocable "manger"  14  6  10  6  19.7  French vocable "recherche"  66  27  10  6  73.5  UNL "search(icl>action)"  51  20  10  6  56  Table 3: Response time on three examples  
 Figure 8: Links in the case of "manger"  Figure 9 shows the display of the interface for a classical search in a Web browser.  
                                                15 In order to better display figure, we have simplified the id in figure 8. 
96
 Figure 9: Display of the interface for a classical search  5 Conclusion and perspectives  In this article, we analysed the different types of lexical resource and presented a method of modelling lexical resources using volumes. This method allows us to manage complex resources while providing facilities for manipulation and treatment equivalent to those of a paper dictionary.  Jibiki-LINKS is a new version of the Jibiki platform, which can manage resources based on multilevel macrostructures using rich links, bearing attributes such as target volume, weight, type, language, open label, etc. To realize the implementation of rich links, we separated the module processing the links from the module processing other CDM pointers. Jibiki-LINKS has been used to implement the Mot?Mot, ProAxie and Pivax macrostructures.  On the Pivax-UNL platform, another instance of the Jibiki-LINKS-based Pivax macrostructure, we have installed the volumes corresponding to the CommonUNLDict resource of V. Dikonov, and tested our platform with that resource.  There is also a UW (UNL interlingual lexemes) resource of 8G entries that was created from DBpedia by David Rouquet. In that resource, there are several volumes for the same language. As links were poorly structured, we are currently working on this resource in order to recompute them. We hope to be able to import this resource, and to make tests at that very large scale in the near future.  To sum up, lexical databases equipped with rich links allow for importing XML-based electronic dictionaries without loss of information, whether they have been elaborated from source or printable forms (such as Word, rtf, ps, pdf) or directly produced in XML from a relational database, or using a dedicated editor knowing their microstructures. They also allow us to automatically produce from them a pivot-based macrostructure organised in volumes, and after that to edit and improve them, using a mixed textual and graphical interface to merge or split lexies, axemes or axies, or to enrich the links with appropriate labels. The introduction of rich links to multilevel lexical databases enhances them with a very interesting aspect of the lexical networks while keeping the classical ways of using dictionaries and of performing lexicographic work. References  EU-IATE (2007) A single database for all EU-related terminology (InterActiveTerminology for Europe) in 23 languages opens to the public. Press release. Brussels. 2007-06-28. Breen, J. W., (2004) JMdict : a Japanese-Multilingual Dictionary. In Gilles S?rasset, Susan Armstrong, Christian Boitet, Andrei Pospescu-Belis, and Dan Tufis, editors, post COLING Workshop on Multilingual Linguistic Resources, Geneva, Switzerland, 28th August. International Committee on Computational Linguistics.  
97
Dikonov V., Boguslavsky I., (2009) Semantic Network of the UNL Dictionary of Concepts. Proceedings of the SENSE Workshop on conceptual Structures for Extracting Natural language SEmantics Moscow, Russia, July 2009, 7 p. Diller, G.A., Beckwith, R., Fellbaum, C., Gross, D., and Miller, K.J. (1990) Introduction to WordNet: an on-line lexical database, International Journal of Lexicography 3(4), pp. 235-244. Dolan, W.B. & Richardson, S.D., (1996) Interactive Lexical Priming for Disambiguation. Proc. MIDDIM'96, Post-COLING seminar on Interactive Disambiguation, C. Boitet ed. Le Col de Porte, Is?re, France. 12-14 ao?t 1996. vol. 1/1 : pp. 54-56. Dong, Z.D., Dong, Q., Hao, C.L., (2010). HowNet and Its Computation of Meaning. In Actes de COLING-2010, Beijing, 4 p. Francopoulo, G., Bel, N., George, M., Calzolari, N., Monachini, M., Pet, M. and Soria, C. (2009). Multilingual resources for NLP in the lexical markup framework (LMF). In journal de Language Resources and Evaluation, March 2009, Volume 43, pp. 55-57. Gut, Y., Ramli, P. R. M., Yusoff, Z., Kim, Ch. Ch., Samat, S. A., Boitet, Ch., N?dobejkine, N., Lafourcade, M., Gaschler, J. and Levenbach, D. (1996). Kamus Perancis-Melayu Dewan, dictionnaire fran?ais-malais. Dewan Bahasa Dan Pustaka, Kuala Lumpur, 667 p. Lafourcade, M., Joubert, A. (2010). Computing trees of named word usages from a crowdsourced lexical network. Investigationes Linguisticae, vol. XXI, pp. 39-56  Lux-Pogodalla, V., Polgu?re, A. (2011) Construction of a French Lexical Network: Methodological Issues. Proceedings of the First International Workshop on Lexical Resources, WoLeR 2011. An ESSLLI-2011 Workshop. Ljubljana, 2011, pp. 54-61.  Mangeot, M. (2002). An XML Markup Language Framework for Lexical Databases Environments: the Dictionary Markup Language. In Actes de LREC-2002, pp. 37-44. Mangeot, M & Chalvin, A.  (2006). Dictionary Building with the Jibiki Platform: the GDEF case. In Actes de LREC-2006, Genoa, pp. 1666-1669.  Mangeot, M. & Touch, S., (2010) Mot?Mot project: building a multilingual lexical system via bilingual dictionaries. Proc. SLTU 2010: Second International Workshop on Spoken Languages Technologies for Under-Resourced Languages, Penang, Malaysia, 2010, 6 p. McCrae, J., Spohr, D. and Cimiano, P., (2011)  Linking lexical resources and ontologies on the semantic web with lemon. Proc. ESWC?11, Berlin, pp. 245-259. Nguyen, H.T., Boitet, C. and S?rasset, G. (2007). PIVAX, an online contributive lexical data base for heterogeneous MT systems using a lexical pivot. In Actes de SNLP-2007, Bangkok, 6 p. Richardson, S.D., Dolan, W.B. and Vanderwende, L. (1998) MindNet: acquiring and structuring semantic information from text, no. MSR-TR-98-23. S?rasset, G. (2012) Dbnary: Wiktionary as a LMF-based Multilingual RDF network. In Actes de LREC-2012, Istanbul, 7 p. S?rasset, G. & Mangeot, M. (2001). Papillon Lexical Database Project: Monolingual Dictionaries and Interlingual Links. In Proc. NLPRS-2011, Tokyo, pp. 119-125. Tran, M. (2006). Prolexbase : Un dictionnaire relationnel multilingue de noms propres : conception, impl?mentation et gestion en ligne. Th?se de doctorat, Tours, pp. 54-57. Vossen, P., (1998) EuroWordNet: A Multilingual Database with Lexical Semantic Networks, Computers and the Humanities, 32(2-3). Zhang, Y. & Mangeot, M., (2013).  Gestion des terminologies riches : l'exemple des acronymes. In Actes de TALN-2013, Les Sables d?Olonne, 8 p.  
98
