Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 367?376,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
User Participation Prediction in Online Forums
Zhonghua Qu and Yang Liu
The University of Texas at Dallas
{qzh,yangl@hlt.utdallas.edu}
Abstract
Online community is an important source
for latest news and information. Accurate
prediction of a user?s interest can help pro-
vide better user experience. In this paper,
we develop a recommendation system for
online forums. There are a lot of differ-
ences between online forums and formal me-
dia. For example, content generated by users
in online forums contains more noise com-
pared to formal documents. Content topics
in the same forum are more focused than
sources like news websites. Some of these
differences present challenges to traditional
word-based user profiling and recommenda-
tion systems, but some also provide oppor-
tunities for better recommendation perfor-
mance. In our recommendation system, we
propose to (a) use latent topics to interpo-
late with content-based recommendation; (b)
model latent user groups to utilize informa-
tion from other users. We have collected
three types of forum data sets. Our experi-
mental results demonstrate that our proposed
hybrid approach works well in all three types
of forums.
1 Introduction
Internet is an important source of information. It
has become a habit of many people to go to the in-
ternet for latest news and updates. However, not all
articles are equally interesting for different users.
In order to intelligently predict interesting articles
for individual users, personalized news recommen-
dation systems have been developed. There are in
general two types of approaches upon which rec-
ommendation systems are built. Content based rec-
ommendation systems use the textual information
of news articles and user generated content to rank
items. Collaborative filtering, on the other hand,
uses co-occurrence information from a collection
of users for recommendation.
During the past few years, online community
has become a large part of internet. More often,
latest information and knowledge appear at on-
line community earlier than other formal media.
This makes it a favorable place for people seeking
timely update and latest information. Online com-
munity sites appear in many forms, for example,
online forums, blogs, and social networking web-
sites. Here we focus our study on online forums. It
is very helpful to build an automatic system to sug-
gest latest information a user would be interested
in. However, unlike formal news media, user gen-
erated content in forums is usually less organized
and not well formed. This presents a great chal-
lenge to many existing news article recommenda-
tion systems. In addition, what makes online fo-
rums different from other media is that users of
online communities are not only the information
consumers but also active providers as participants.
Therefore in this study we develop a recommen-
dation system to account for these characteristics
of forums. We propose several improvements over
previous work:
? Latent topic interpolation: This is to address
the issue with the word-based content repre-
sentation. In this paper we used Latent Dirich-
let Allocation (LDA), a generative multino-
mial mixture model, for topic inference inside
threads. We build a system based on words
367
and latent topics, and linearly interpolate their
results.
? User modeling: We model users? participa-
tion inside threads as latent user groups. Each
latent group is a multinomial distribution on
users. Then LDA is used to infer the group
mixture inside each thread, based on which
the probability of a user?s participation can be
derived.
? Hybrid system: Since content and user-
based methods rely on different information
sources, we combine the results from them for
further improvement.
We have evaluated our proposed method using
three data sets collected from three representative
forums. Our experimental results show that in all
forums, by using latent topics information, system
can achieve better accuracy in predicting threads
for recommendation. In addition, by modeling la-
tent user groups in thread participation, further im-
provement is achieved in the hybrid system. Our
analysis also showed that each forum has its nature,
resulting in different optimal parameters in the dif-
ferent forums.
2 Related Work
Recommendation systems can help make informa-
tion retrieving process more intelligent. Generally,
recommendation methods are categorized into two
types (Adomavicius and Tuzhilin, 2005), content-
based filtering and collaborative filtering.
Systems using content-based filtering use the
content information of recommendation items a
user is interested in to recommend new items to
the user. For example, in a news recommendation
system, in order to recommend appropriate news
articles to a user, it finds the most prominent fea-
tures (e.g., key words, tags, category) in the docu-
ment that a user likes, then suggests similar articles
based on this ?personal profile?. In Fabs system
(Balabanovic and Shoham, 1997), Skyskill & We-
bert system (Pazzani et al 1997), documents are
represented using a set of most important words
according to a weighting measure. The most popu-
lar measure of word ?importance? is TF-IDF (term
frequency, inverse document frequency) (Salton
and Buckley, 1988), which gives weights to words
according to its ?informativeness?. Then, base on
this ?personal profile? a ranking machine is applied
to give a ranked recommendation list. In Fabs sys-
tem, Rocchio? algorithm (Rocchio, 1971) is used
to learn the average TF-IDF vector of highly rated
documents. Skyskill & Webert?s system uses Naive
Bayes classifiers to give the probability of docu-
ments being liked. Winnow?s algorithm (Little-
stone, 1988), which is similar to perception algo-
rithm, has been shown to perform well when there
are many features. An adaptive framework is intro-
duced in (Li et al 2010) using forum comments
for news recommendation. In (Wu et al 2010),
a topic-specific topic flow model is introduced to
rank the likelihood of user participating in a thread
in online forums.
Collaborative-filtering based systems, unlike
content-based systems, predict the recommending
items using co-occurrence information between
users. For example, in a news recommendation
system, in order to recommend an article to user
c, the system tries to find users with similar taste
as c. Items favored by similar users would be rec-
ommended. Grundy (Rich, 1979) is known to be
one of the first collaborative-filtering based sys-
tems. Collaborative filtering systems can be ei-
ther model based or memory based (Breese et al
1998). Memory-based algorithms, such as (Del-
gado and Ishii, 1999; Nakamura and Abe, 1998;
Shardanand and Maes, 1995), use a utility function
to measure the similarity between users. Then rec-
ommendation of an item is made according to the
sum of the utility values of active users that partic-
ipate in it. Model-based algorithms, on the other
hand, try to formulate the probability function of
one item being liked statistically using active user
information. (Ungar et al 1998) clustered sim-
ilar users into groups for recommendation. Dif-
ferent clustering methods have been experimented,
including K-means and Gibbs Sampling. Other
probabilistic models have also been used to model
collaborative relationships, including a Bayesian
model (Chien and George, 1999), linear regres-
sion model (Sarwar et al 2001), Gaussian mix-
ture models (Hofmann, 2003; Hofmann, 2004). In
(Blei et al 2001) a collaborative filtering appli-
cation is discussed using LDA. However in this
model, re-estimation of parameters for the whole
system is needed when a new item comes in. In
368
this paper, we formulate users? participation differ-
ently using the LDA mixture model.
Some previous work has also evaluated using
a hybrid model with both content and collabora-
tive features and showed outstanding performance.
For example, in (Basu et al 1998), hybrid features
are used to make recommendation using inductive
learning.
3 Forum Data
We have collected data from three forums in this
study.1 Ubuntu community forum is a technical
support forum; World of Warcraft (WoW) forum is
about gaming; Fitness forum is about how to live
a healthy life. These three forums are quite rep-
resentative of online forums on the internet. Us-
ing three different types of forums for task eval-
uation helps to demonstrate the robustness of our
proposed method. In addition, it can show how the
same method could have substantial performance
difference on forums of different nature. Users?
behaviors in these three forums are very differ-
ent. Casual forums like ?Wow gaming? have much
more posts in each thread. However its posts are
the shortest in length. This is because discussions
inside these types of forums are more like casual
conversation, and there is not much requirement
on the user?s background, and thus there is more
user participation. In contrast, technical forums
like ?Ubuntu? have fewer average posts in each
thread, and have the longest post length. This is
because a Question and Answer (QA) forum tends
to be very goal oriented. If a user finds the thread
is unrelated, then there will be no motivation for
participation.
Inside forums, different boards are created to
categorize the topics allowed for discussion. From
the data we find that users tend to participate in a
few selected boards of their choices. To create a
data set for user interest prediction in this study,
we pick the most popular boards in each forum.
Even within the same board, users tend to partici-
pate in different threads base on their interest. We
use a user?s participation information as an indica-
tion whether a thread is interesting to a user or not.
Hence, our task is to predict the user participation
in forum threads. Note this approach could intro-
1Please contact the authors to obtain the data.
duce some bias toward negative instances in terms
of user interests. A users? absence from a thread
does not necessarily mean the user is not interested
in that thread; it may be a result of the user being
offline by that time or the thread is too behind in
pages. As a matter of fact, we found most users
read only the threads on the first page during their
time of visit of a forum. This makes participation
prediction an even harder task than interest predic-
tion.
In online forums, threads are ordered by the time
stamp of their last participating post. Provided with
the time stamp for each post, we can calculate the
order of a thread on its board during a user?s par-
ticipation. Figure 1 shows the distribution of post
location during users? participation. We found that
most of the users read only the posts on the first
page. In order to minimize the false negative in-
stances from the data set, we did thread location
filtering. That is, we want to filter out messages
that actually interest the user but do not have the
user?s participation because they are not on the first
page. For any user, only those threads appearing in
the first 10 entries on a page during a user?s visit
are included in the data set.
Figure 1: Thread position during users? participation.
In the pre-processing step of the experiment, first
we use online status filtering discussed above to
remove threads that a user does not see while of-
fline. The statistics of the boards we have used in
each forum are shown in Table 1. The statistics
are consistent with the full forum statistics. For
example, users in technical forums tend to post
less than casual forums. We define active users as
those who have participated in 10 or more threads.
Column ?Part. @300? shows the average number
369
of threads the top 300 users have participated in.
?Filt. Threads@300? shows the average number of
threads after using online filtering with a window
of 10. Thread participation in ?Ubuntu? forum is
very sparse for each user, having only 10.01% par-
ticipating threads for each user after filtering. ?Fit-
ness? and ?Wow Forum? have denser participation,
at 18.97% and 13.86% respectively.
4 Interesting Thread Prediction
In the task of interesting thread prediction, the sys-
tem generates a ranked list of threads a user is
likely to be interested in based on users? past his-
tory of thread participation. Here, instead of pre-
dicting the true interestedness, we predict the par-
ticipation of the user, which is a sufficient condi-
tion for interestedness. This approach is also used
by (Wu et al 2010) for their task evaluation. In
this section, we describe our proposed approaches
for thread participation prediction.
4.1 Content-based Filtering
In the content-based filtering approach, only con-
tent of a thread is used as features for prediction.
Recommendation through content-based filtering
has its deep root in information retrieval. Here we
use a Naive Bayes classifier for ranking the threads
using information based on the words and the la-
tent topic analysis.
4.1.1 Naive Bayes Classification
In (Pazzani et al 1997) Naive Bayesian classi-
fier showed outstanding performance in web page
recommendation compared to several other clas-
sifiers. A Naive Bayes classifier is a generative
model in which words inside a document are as-
sumed to be conditionally independent. That is,
given the class of a document, words are generated
independently. The posterior probability of a test
instance in Naive Bayes classifier takes the follow-
ing form:
P (Ci|f1..k) =
1
Z
P (Ci)
?
j
P (fj |Ci) (1)
where Z is the class label independent normaliza-
tion term, f1..k is the bag-of-word feature vector
for the document. Naive Bayes classifier is known
for not having a well calibrated posterior probabil-
ity (Bennett, 2000). (Pavlov et al 2004) showed
that normalization by document length yielded
good empirical results in approximating a well cal-
ibrated posterior probability for Naive Bayes clas-
sifier. The normalized Naive Bayes classifier they
used is as follows:
P (Ci|f1..k) =
1
Z
P (Ci)
?
j
P (fj |Ci)
1
|f | (2)
In this equation, the probability of generat-
ing each word is normalized by the length of
the feature vector |f |. The posterior probabil-
ity P (interested|f1..k) from (normalized) Naive
Bayes classifier is used for recommendation item
ranking.
4.1.2 Latent Topics based Interpolation
Because of noisy forum writing and limited
training data, the above bag-of-word model used in
naive Bayes classifier may suffer from data sparsity
issues. We thus propose to use latent topic model-
ing to alleviate this problem. Latent Dirichlet Allo-
cation (LDA) is a generative model based on latent
topics. The major difference between LDA and
previous methods such as probabilistic Latent Se-
mantic Analysis (pLSA) is that LDA can efficiently
infer topic composition of new documents, regard-
less of the training data size (Blei et al 2001). This
makes it ideal for efficiently reducing the dimen-
sion of incoming documents.
In an online forum, words contained in threads
tend to be very noisy. Irregular words, such as
abbreviation, misspelling and synonyms, are very
common in an online environment. From our ex-
periments, we observe that LDA seems to be quite
robust to these phenomena and able to capture
word relationship semantically. To illustrate the
words inside latent topics in the LDA model in-
ferred from online forums, we show in Table 2 the
top words in 3 out of 20 latent topics inferred from
?Ubuntu? forum according to its multinomial dis-
tribution. We can see that variations of the same
words are grouped into the same topic.
Since each post could be very short and LDA is
generally known not to work well with short docu-
ments, we concatenated the content of posts inside
each thread to form documents. In order to build
a valid evaluation configuration, only posts before
the first time the testing user participated are used
for model fitting and inference.
370
Forum Name Threads Posts Active Users Part. @300 Filt. Threads @300
Ubuntu 185,747 940,230 1,700 464.72 4641.25
Fitness 27,250 529,201 2,808 613.15 3231.04
Wow Gaming 34,187 1,639,720 19,173 313.77 2264.46
Table 1: Data statistics after filtering.
Topic 1 Topic 2 Topic 3
lol?d wine email
lol. Wine mail
imo. game Thunderbird
,? fixme evolution
-, stub send
lulz. not emails
lmao. WINE gmail
rofl. play postfix
Table 2: Example of LDA topics that capture words
with different variations.
After model fitting for LDA, the topic distri-
butions on new threads can be inferred using the
model. Compared to the original bag-of-word fea-
ture vector, the topic distribution vector is not only
more robust against noise, but also closer to hu-
man interpretation of words. For example in topic
3 in Table 2, people who care about ?Thunder-
bird?, an email client, are also very likely to show
interest in ?postfix?, which is a Linux email ser-
vice. These closely related words, however, might
not be captured using the bag-of-word model since
that would require the exact words to appear in the
training set.
In order to take advantage of the topic level in-
formation while not losing the ?fine-grained? word
level feature, we use the topic distribution as ad-
ditional features in combination with the bag-of-
word features. To tune the contribution of topic
level features in classifiers like Naive Bayes clas-
sifiers, we normalize the topic level feature to a
length of Lt = ?|f | and bag-of-word feature to
Lw = (1??)|f |. ? is a tuning parameter from 0 to
1 that determines the proportion of the topic infor-
mation used in the features. |f | is from the original
bag-of-word feature vector. The final feature vec-
tor for each thread can be represented as:
F = Lww1, ..., Lwwk ? Lt?1, ..., Lt?T (3)
where ?1, ..., ?t is the multinomial distribution of
topics for the thread.
4.2 Collaborative Filtering
Collaborative filtering techniques make prediction
using information from similar users. It has ad-
vantages over content-based filtering in that it can
correctly predict items that are vastly different in
content but similar in concepts indicated by users?
participation.
In some previous work, clustering methods were
used to partition users into several groups, Then,
predictions were made using information from
users in the same group. However, in the case
of thread recommendation, we found that users?
interest does not form clean clusters. Figure 2
shows the mutual information between users after
doing an average-link clustering on their pairwise
mutual information. In a clean clustering, intra-
cluster mutual information should be high, while
inter-cluster mutual information is very low. If so,
we would expect that the figure shows clear rect-
angles along the diagonal. Unfortunately, from this
figure it appears that users far away in the hierarchy
tree still have a lot of common thread participation.
Here, we propose to model user similarity based on
latent user groups.
4.2.1 Latent User Groups
In this paper, we model users? participation in-
side threads as an LDA generative model. We
model each user group as a multinomial distribu-
tion. Users inside each group are assumed to have
common interests in certain topic(s). A thread in an
online forum typically contains several such top-
ics. We could model a user?s participation in a
thread as a mixture of several different user groups.
Since one thread typically attracts a subset of user
groups, it is reasonable to add a Dirichlet prior on
the user group mixture.
The generative process is the same as the LDA
used above for topic modeling, except now users
371
Figure 2: Mutual information between users in Average
Link Hierarchical clustering.
are ?words? and user groups are ?topics?. Using
LDA to model user participation can be viewed
as soft-clustering of users in a sense that one user
could appear in multiple groups at the same time.
The generative process for participating users is as
follows.
1. Choose ? ? Dir(?)
2. For each of N participating users, un:
(a) Choose a group zn ?Multinomial(?)
(b) Choose a user un ? p(un|zn)
One thing worth noting is that in LDA model a
document is assumed to consist of many words. In
the case of modeling user participation, a thread
typically has far fewer users than words inside a
document. This could potentially cause problem
during variable estimation and inference. How-
ever, we show that this approach actually works
well in practice (experimental results in Section 5).
4.2.2 Using Latent User Groups for
Prediction
For an incoming new thread, first the latent
group distribution is inferred using collapsed Gibbs
Sampling (Griffiths and Steyvers, 2004). The pos-
terior probability of a user ui participating in thread
j given the user group distribution is as follows.
P (ui|?j , ?) =
?
k?T
P (ui|?k)P (k|?j) (4)
In the equation, ?k is the multinomial distribution
of users in group k, T is the number of latent user
groups, and ?j is the group composition in thread
j after inference using the training data. In gen-
eral, the probability of user ui appearing in thread
j is proportional to the membership probabilities
of this user in the groups that compose the partici-
pating users.
4.3 Hybrid System
Up to this point we have two separate systems that
can generate ranked recommendation lists based on
different factors of threads. In order to generate the
final ranked list, we give each item a score accord-
ing to the ranked lists from the two systems. Then
the two scores are linearly interpolated using a tun-
ing parameter ? as shown in Equation 5. The final
ranked list is generated accordingly.
Ci =(1? ?)Scorecontent
+ ?Scorecollaborative
(5)
We propose several different rescoring methods
to generate the scores in the above formula for the
two individual systems.
? Posterior: The posterior probabilities of each
item from the two systems are used directly as
the score.
Scoredir = p(clike|itemi) (6)
This way the confidence of ?how likely? an
item is interesting is preserved. However,
the downside is that the two different sys-
tems have different calibration on its posterior
probability, which could be problematic when
directly adding them together.
? Linear rescore: To counter the problem asso-
ciated with posterior probability calibration,
we use linear rescoring based on the ranked
list:
Scorelin = 1?
posi
N
(7)
In the formula, posi is the position of item i
in the ranked list, and N is the total number
of items being ranked. The resulting score is
between 0 and 1, 1 being the first item on the
list and 0 being the last.
? Sigmoid rescore: In a ranked list, usually
items on the top and bottom of the list have
372
higher confidence than those in the middle.
That is to say more ?emphasis? should be put
on both ends of the list. Hence we use a sig-
moid function on the Scorelinear to capture
this.
Scoresig =
1
1 + e?l(Scorelin?0.5)
(8)
A sigmoid function is relatively flat on both
ends while being steep in the middle. In the
equation, l is a tuning parameter that decides
how ?flat? the score of both ends of the list is
going to be. Determining the best value for l
is not a trivial problem. Here we empirically
assign l = 10.
5 Experiment and Evaluation
In this section, we evaluate our approach empiri-
cally on the three forum data sets described in Sec-
tion 3. We pick the top 300 most active users from
each forum for the evaluation. Among the 300
users, 100 of them are randomly selected as the de-
velopment set for parameter tuning, while the rest
is test set. All the data sets are filtered using an on-
line filter as previously described, with a window
size of 10 threads.
Threads are tokenized into words and filtered us-
ing a simple English stop word list. All words
are then ordered by their occurrences multiplied by
their inverse document frequencies (IDF).
idfw = log
|D|
|{d : w ? d}|
(9)
The top 4,000 words from this list are then used to
form the vocabulary.
We used standard mean average precision
(MAP) as the evaluation metric. This standard in-
formation retrieval evaluation metric measures the
quality of the returned rank lists from a system.
Entries higher in the rank are more accurate than
lower ones. For an interesting thread recommenda-
tion system, it is preferable to provide a short and
high-quality list of recommendation; therefore, in-
stead of reporting full-range MAP, we report MAP
on top 10 relevant threads (MAP@10). The reason
why we picked 10 as the number of relevant doc-
ument for MAP evaluation is that users might not
have time to read too many posts, even if they are
relevant.
During evaluation, a 3-fold cross-validation is
performed for each user in the test set. In each fold,
MAP@10 score is calculated from the ranked list
generated by the system. Then the average from all
the folds and all the users is computed as the final
result.
To make a proper evaluation configuration, for
each user, only posts up to the first participation of
the testing user are used for the test set.
5.1 Content-based Results
Here we evaluate the performance of interest
thread prediction using only features from text.
First we use the ranking model with latent topic
information only on the development set to deter-
mine an optimal number of topics. Empirically,
we use hyper parameter ? = 0.1 and ? = 1/K
(K is the number of topics). We use the perfor-
mance of content-based recommendation directly
to determine the optimal topic number K. We var-
ied the latent topic number K from 10 to 100, and
found that the best performance was achieved us-
ing 30 topics in all three forums. Hence we use
K = 30 for content based recommendation unless
otherwise specified.
Next, we show how topic information can help
content-based recommendation achieve better re-
sults. We tune the parameter ? described in Sec-
tion 4.1.2 and show corresponding performances.
We compare the performance using Naive Bayes
classifier, before and after normalization. The
MAP@10 results on the test set are shown in Fig-
ure 3 for three forums. When ? = 0, no latent topic
information is used, and when ? = 1, latent topics
are used without any word features.
When using Naive Bayes classifier without nor-
malization, we find relatively larger performance
gain from adding topic information for the ? val-
ues of close to 0. This phenomenon is probably
because of the poor posterior probabilities of the
Naive Bayes classifier, which are close to either 1
or 0.
For normalized Naive Bayes classifier, interpo-
lating with latent topics based ranking yields per-
formance improvement compared to word-based
results consistently for the three forums. In
?Wow Gaming? corpus, the optimal performance
is achieved with a relatively high ? value (at around
0.5), and it is even higher for the ?Fitness? forum.
373
This means that the system relies more on the la-
tent topics information. This is because in these fo-
rums, casual conversation contains more irregular
words, causing more severe data sparsity problem
than others.
Between the two naive Bayes classifiers, we
can see that using normalized probabilities out-
performs the original one in ?Wow Gaming? and
?Ubuntu? forums. This observation is consistent
with previous work (e.g., (Pavlov et al 2004)).
However, we found that in ?Fitness Forum?, the
performance degrades with normalization. Further
work is still needed to understand why this is the
case.
5.2 Latent User Group Classification
In this section, collaborative filtering using latent
user groups is evaluated. First, participating users
from the training set are used to estimate an LDA
model. Then, users participating in a thread are
used to infer the topic distribution of the thread.
Candidate threads are then sorted by the proba-
bility of a target user?s participation according to
Equation 4. Note that all the users in the forum are
used to estimate the latent user groups, but only the
top 300 active users are used in evaluation. Here,
we vary the number of latent user groups G from
5 to 100. Hyper parameters were set empirically:
? = 1/G, ? = 0.1.
Figure 4 shows the MAP@10 results using dif-
ferent numbers of latent groups for the three fo-
rums. We compare the performance using latent
groups with a baseline using SVM ranking. In
the baseline system, users? participation in a thread
is used as a binary feature. LibSVM with radius
based function (RBF) kernel is used to estimate the
probability of a user?s participation.
From the results, we find that ranking using la-
tent groups information outperforms the baseline
in almost all non-trivial cases. In the case of
?Ubuntu? forum, the performance gain is less com-
pared to other forums. We believe this is because
in this technical support forum, the average user
participation in threads is much less, thus making
it hard to infer a reliable group distribution in a
thread. In addition, the optimal number of user
groups differs greatly between ?Fitness? forum and
?Wow Gaming? forum. We conjecture the reason
behind this is that in the ?Fitness? forum, users
XVHU

Z
R
U
G
Figure 5: Position of items with different #users and
#words in a ranked list. (red=0 being higher on the
ranked list and green being lower)
may be interested in a larger variety of topics and
thus the user distribution in different topics is not
very obvious. In contrast, people in the gaming
forum are more specific to the topics they are inter-
ested in.
It is known that LDA tends to perform poorly
when there are too few words/users. To have a
general idea of how much user participation is
?enough? for decent prediction, we show a graph
(Figure 5) depicting the relationships among the
number of users, the number of words, and the po-
sition of the positive instances in the ranked lists.
In this graph, every dot is a positive thread instance
in ?Wow Gaming? forum. Red color shows that
the positive thread is indeed getting higher ranks
than others. We observe that threads with around
16 participants can already achieve a decent perfor-
mance.
5.3 Hybrid System Performance
In this section, we evaluate the performance of the
hybrid system output. Parameters used in each fo-
rum data set are the optimal parameters found in
the previous sections. Here we show the effect of
the tuning parameter ? (described in Section 4.3).
Also, we compare three different scoring schemes
used to generate the final ranked list. Performance
of the hybrid system is shown in Table 3.
We can see that the combination of the two sys-
tems always outperforms any one model alone.
374
 0.36
 0.39
 0.42
 0.45
 0.48
 0.51
 0.54
 0  0.2  0.4  0.6  0.8  1
MA
P 
10
Gamma
Ubuntu Forum
Naive Bayes
Normalized NB
 0.2
 0.22
 0.24
 0.26
 0.28
 0.3
 0  0.2  0.4  0.6  0.8  1
MA
P 
10
Gamma
Wow Gaming
Naive Bayes
Normalized NB
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.2  0.4  0.6  0.8  1
MA
P 
10
Gamma
Fitness Forum
Naive Bayes
Normalized NB
Figure 3: Content-based filtering results: MAP@10 vs. ? (contribution of topic-based features).
 0.14
 0.16
 0.18
 0.2
 0.22
 1  10  100
MA
P 
10
Number of Groups
Ubuntu Forum
Latent Group
SVM
 0.15
 0.2
 0.25
 0.3
 0.35
 1  10  100
MA
P 
10
Number of Groups
Wow Gaming
Latent Group
SVM
 0.2
 0.3
 0.4
 0.5
 0.6
 1  10  100
MA
P 
10
Number of Groups
Fitness Forum
Latent Group
SVM
Figure 4: Collaborative filtering results: MAP@10 vs. user group number.
Forum
Contribution Factor ?
0.0 1.0 Optimal
Ubuntu 0.523 0.198 0.534 (? = 0.9)
Wow 0.278 0.283 0.304 (? = 0.1)
Fitness 0.545 0.457 0.551 (? = 0.85)
Table 3: Performance of the hybrid system with differ-
ent ? values.
This is intuitive since the two models use differ-
ent information sources. A MAP@10 score of 0.5
means that around half of the suggested results do
have user participation. We think this is a good re-
sult considering that this is not a trivial task.
We also notice that based on the nature of differ-
ent forums, the optimal ? value could be substan-
tially different. For example, in ?Wow gaming?
forum where people participate in more threads, a
higher ? value is observed which favors collabo-
rative filtering score. In contrast, in ?Ubuntu? fo-
rum, where people participate in far fewer threads,
the content-based system is more reliable in thread
prediction, hence a lower ? is used. This observa-
tion also shows that the hybrid system is more ro-
bust against differences among forums compared
with single model systems.
6 Conclusion
In this paper, we proposed a new system that can
intelligently recommend threads from online com-
munity according to a user?s interest. The system
uses both content-based filtering and collaborative-
filtering techniques. In content-based filtering, we
solve the problem of data sparsity in online con-
tent by smoothing using latent topic information.
In collaborative filtering, we model users? partici-
pation in threads with latent groups under an LDA
framework. The two systems compliment each
other and their combination achieves better per-
formance than individual ones. Our experiments
across different forums demonstrate the robustness
of our methods and the difference among forums.
In the future work, we plan to explore how social
information could help further refine a user?s inter-
est.
References
Gediminas Adomavicius and Alexander Tuzhilin.
2005. Toward the next generation of recommender
systems: A survey of the state-of-the-art and possi-
ble extensions. IEEE TRANSACTIONS ON KNOWL-
EDGE AND DATA ENGINEERING, 17(6):734?749.
Marko Balabanovic and Yoav Shoham. 1997.
375
Fab: Content-based, collaborative recommendation.
Communications of the ACM, 40:66?72.
Chumki Basu, Haym Hirsh, and William Cohen. 1998.
Recommendation as classification: Using social and
content-based information in recommendation. In In
Proceedings of the Fifteenth National Conference on
Artificial Intelligence, pages 714?720. AAAI Press.
Paul N. Bennett. 2000. Assessing the calibration of
naive bayes? posterior estimates.
David Blei, Andrew Y. Ng, and Michael I. Jordan.
2001. Latent dirichlet alcation. Journal of Ma-
chine Learning Research, 3:2003.
John S. Breese, David Heckerman, and Carl Kadie.
1998. Empirical analysis of predictive algorithms for
collaborative filtering. pages 43?52. Morgan Kauf-
mann.
Y H Chien and E I George, 1999. A bayesian model for
collaborative filtering. Number 1.
Joaquin Delgado and Naohiro Ishii. 1999. Memory-
based weighted-majority prediction for recom-
mender systems.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of Amer-
ica, 101(Suppl 1):5228?5235, April.
Thomas Hofmann. 2003. Collaborative filtering via
gaussian probabilistic latent semantic analysis. In
Proceedings of the 26th annual international ACM
SIGIR conference on Research and development in
informaion retrieval, SIGIR ?03, pages 259?266,
New York, NY, USA. ACM.
Thomas Hofmann. 2004. Latent semantic models
for collaborative filtering. ACM Trans. Inf. Syst.,
22(1):89?115.
Qing Li, Jia Wang, Yuanzhu Peter Chen, and Zhangxi
Lin. 2010. User comments for news recom-
mendation in forum-based social media. Inf. Sci.,
180:4929?4939, December.
Nick Littlestone. 1988. Learning quickly when irrele-
vant attributes abound: A new linear-threshold algo-
rithm. In Machine Learning, pages 285?318.
Atsuyoshi Nakamura and Naoki Abe. 1998. Collab-
orative filtering using weighted majority prediction
algorithms. In Proceedings of the Fifteenth Interna-
tional Conference on Machine Learning, ICML ?98,
pages 395?403, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
Dmitry Pavlov, Ramnath Balasubramanyan, Byron
Dom, Shyam Kapur, and Jignashu Parikh. 2004.
Document preprocessing for naive bayes classifica-
tion and clustering with mixture of multinomials. In
Proceedings of the tenth ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, KDD ?04, pages 829?834, New York, NY, USA.
ACM.
Michael Pazzani, Daniel Billsus, S. Michalski, and
Janusz Wnek. 1997. Learning and revising user pro-
files: The identification of interesting web sites. In
Machine Learning, pages 313?331.
Elaine Rich. 1979. User modeling via stereotypes.
Cognitive Science, 3(4):329?354.
J. Rocchio, 1971. Relevance Feedback in Information
Retrieval.
Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval.
In INFORMATION PROCESSING AND MANAGE-
MENT, pages 513?523.
Badrul Sarwar, George Karypis, Joseph Konstan, and
John Reidl. 2001. Item-based collaborative fil-
tering recommendation algorithms. In WWW ?01:
Proceedings of the 10th international conference on
World Wide Web, pages 285?295, New York, NY,
USA. ACM.
Upendra Shardanand and Pattie Maes. 1995. So-
cial information filtering: Algorithms for automating
?word of mouth?. In CHI, pages 210?217.
Lyle Ungar, Dean Foster, Ellen Andre, Star Wars,
Fred Star Wars, Dean Star Wars, and Jason Hiver
Whispers. 1998. Clustering methods for collabo-
rative filtering. AAAI Press.
Hao Wu, Jiajun Bu, Chun Chen, Can Wang, Guang Qiu,
Lijun Zhang, and Jianfeng Shen. 2010. Modeling
dynamic multi-topic discussions in online forums. In
AAAI.
376
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 519?523,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Interactive Group Suggesting for Twitter
Zhonghua Qu, Yang Liu
The University of Texas at Dallas
{qzh,yangl}@hlt.utdallas.edu
Abstract
The number of users on Twitter has drasti-
cally increased in the past years. However,
Twitter does not have an effective user group-
ing mechanism. Therefore tweets from other
users can quickly overrun and become in-
convenient to read. In this paper, we pro-
pose methods to help users group the peo-
ple they follow using their provided seeding
users. Two sources of information are used to
build sub-systems: textural information cap-
tured by the tweets sent by users, and social
connections among users. We also propose
a measure of fitness to determine which sub-
system best represents the seed users and use
it for target user ranking. Our experiments
show that our proposed framework works well
and that adaptively choosing the appropriate
sub-system for group suggestion results in in-
creased accuracy.
1 Introduction
Twitter is a well-known social network service that
allows users to post short 140 character status update
which is called ?Tweet?. A twitter user can ?follow?
other users to get their latest updates. Twitter cur-
rently has 19 million active users. These users fol-
lows 80 other users on average. Default Twitter ser-
vice displays ?Tweets? in the order of their times-
tamps. It works well when the number of tweets
the user receives is not very large. However, the
flat timeline becomes tedious to read even for av-
erage users with less than 80 friends. As Twitter
service grows more popular in the past few years,
users? ?following? list starts to consist of Twitter ac-
counts for different purposes. Take an average user
?Bob? for example. Some people he follows are his
?Colleagues?, some are ?Technology Related Peo-
ple?, and others could be ?TV show comedians?.
When Bob wants to read the latest news from his
?Colleagues?, because of lacking effective ways to
group users, he has to scroll through all ?Tweets?
from other users. There have been suggestions from
many Twitter users that a grouping feature could be
very useful. Yet, the only way to create groups is
to create ?lists? of users in Twitter manually by se-
lecting each individual user. This process is tedious
and could be sometimes formidable when a user is
following many people.
In this paper, we propose an interactive group cre-
ating system for Twitter. A user creates a group by
first providing a small number of seeding users, then
the system ranks the friend list according to how
likely a user belongs to the group indicated by the
seeds. We know in the real world, users like to group
their ?follows? in many ways. For example, some
may create groups containing all the ?computer sci-
entists?, others might create groups containing their
real-life friends. A system using ?social informa-
tion? to find friend groups may work well in the lat-
ter case, but might not effectively suggest correct
group members in the former case. On the other
hand, a system using ?textual information? may be
effective in the first case, but is probably weak in
finding friends in the second case. Therefore in
this paper, we propose to use multiple information
sources for group member suggestions, and use a
cross-validation approach to find the best-fit sub-
519
system for the final suggestion. Our results show
that automatic group suggestion is feasible and that
selecting approximate sub-system yields additional
gain than using individual systems.
2 Related Work
There is no previous research on interactive sug-
gestion of friend groups on Twitter to our knowl-
edge; however, some prior work is related and can
help our task. (Roth et al, 2010) uses implicit so-
cial graphs to help suggest email addresses a person
is likely to send to based on the addresses already
entered. Also, using the social network informa-
tion, hidden community detection algorithms such
as (Palla et al, 2005) can help suggest friend groups.
Besides the social information, what a user tweets is
also a good indicator to group users. To character-
ize users? tweeting style, (Ramage et al, 2010) used
semi-supervised topic modeling to map each user?s
tweets into four characteristic dimensions.
3 Interactive Group Creation
Creating groups manually is a tedious process.
However, creating groups in an entirely un-
supervised fashion could result in unwanted results.
In our system, a user first indicates a small number
of users that belong to a group, called ?seeds?, then
the system suggests other users that might belong to
this group. The general structure of the system is
shown in Figure 1.
[ Social Sub-System
??
Textual Sub-System
Sub-System 
Selector
Seed Users
Target Users Ranks
Figure 1: Overview of the system architecture
As mentioned earlier, we use different informa-
tion sources to determine user/group similarity, in-
cluding textual information and social connections.
A module is designed for each information source to
rank users based on their similarity to the provided
seeds. In our approach, the system first tries to detect
what sub-system can best fit the seed group. Then,
the corresponding system is used to generate the fi-
nal ranked list of users according to the likelihood of
belonging to the group.
After the rank list is given, the user can adjust the
size of the group to best fit his/her needs. In addition,
a user can correct the system by specifically indicat-
ing someone as a ?negative seed?, which should not
be on the top of the list. In this paper, we only con-
sider creating one group at a time with only ?positive
seed? and do not consider the relationships between
different groups.
Since determining the best fitting sub-system or
the group type from the seeds needs the use of the
two sub-systems, we describe them first. Each sub-
system takes a group of seed users and unlabeled
target users as the input, and provides a ranked list
of the target users belonging to the group indicated
by the seeds.
3.1 Tweet Based Sub-system
In this sub-system, user groups are modeled using
the textual information contained in their tweets. We
collected all the tweets from a user and grouped
them together.
To represent the tweets information, we could use
a bag-of-word model for each user. However, since
Twitter messages are known to be short and noisy,
it is very likely that traditional natural language pro-
cessing methods will perform poorly. Topic mod-
eling approaches, such as Latent Dirichlet Alloca-
tion (LDA) (Blei et al, 2003), model document as a
mixture of multinomial distribution of words, called
topics. They can reduce the dimension and group
words with similar semantics, and are often more
robust in face of data sparsity or noisy data. Be-
cause tweet messages are very short and hard to infer
topics directly from them, we merge all the tweets
from a user to form a larger document. Then LDA
is applied to the collection of documents from all
the users to derive the topics. Each user?s tweets
can then be represented using a bag-of-topics model,
where the ith component is the proportion of the ith
520
topic appearing in the user?s tweet.
Given a group of seed users, we want to find target
users that are similar to the seeds in terms of their
tweet content. To take multiple seed instances into
consideration, we use two schemes to calculate the
similarity between one target user and a seed group.
? centroid: we calculate the centroid of seeds,
then use the similarity between the centroid and
the target user as the final similarity value.
? average: we calculate the similarity between
the target and each individual seed user, then
take the average as the final similarity value.
In this paper, we explore using two different sim-
ilarity functions between two vectors (ui and vi),
cosine similarity and inverse Euclidean distance,
shown below respectively.
dcosine(u, v) =
1
| u || v |
n?
i=1
ui ? vi (1)
deuclidean(u, v) =
1
??n
i=1(ui ? vi)2
(2)
After calculating similarity for all the target users,
this tweet-based sub-system gives the ranking ac-
cordingly.
3.2 Friend Based Sub-system
As an initial study, we use a simple method to model
friend relationship in user groups. In the future, we
will replace it with other better performing meth-
ods. In this sub-system, we model people using
their social information. In Twitter, social informa-
tion consists of ?following? relation and ?mentions?.
Unlike other social networks like ?Facebook? or
?Myspace?, a ?following? relation in Twitter is di-
rected. In Twitter, a ?mention? happens when some-
one refers to another Twitter user in their tweets.
Usually it happens in replies and retweets. Because
this sub-system models the real-life friend groups,
we only consider bi-directional following relation
between people. That is, we only consider an edge
between users when both of them follow each other.
There are many hidden community detection algo-
rithms that have been proposed for network graphs
(Newman, 2004; Palla et al, 2005). Our task is how-
ever different in that we know the seed of the target
group and the output needs to be a ranking. Here, we
use the count of bi-directional friends and mentions
between a target user and the seed group as the score
for ranking. The intuition is that the social graph be-
tween real life friends tends to be very dense, and
people who belong to the clique should have more
edges to the seeds than others.
3.3 Group Type Detection
The first component in our system is to determine
which sub-system to use to suggest user groups. We
propose to evaluate the fitness of each sub-system
base on the seeds provided using a cross-validation
approach. The assumption is that if a sub-system
(information source used to form the group) is a
good match, then it will rank the users in the seed
group higher than others not in the seed.
The procedure of calculating the fitness score of
each sub-system is shown in Algorithm 1. In the in-
put, S is the seed users (with more than one user),
U is the target users to be ranked, and subrank is
a ranking sub-system (two systems described above,
each taking seed users and target users as input, and
producing the ranking of the target users). This pro-
cedure loops through the seed users. Each time, it
takes one seed user Si out and puts it together with
other target users. Then it calls the sub-system to
rank the new list and finds out the resulting rank for
Si. The final fitness score is the sum of all the ranks
for the seed instances. The system with the highest
score is then selected and used to rank the original
target users.
Algorithm 1 Fitness of a sub-system for a seed
group
proc fitness(S,U, subrank) ?
ranks := ?
for i := 1 to size(S) do
U ? := Si ? U
S? := S \ Si
r := subrank(U ?, S?);
t := rankOf(Si, r);
ranks := ranks ? t; od
fitness := sum(ranks);
print(fitness);
end
4 Data
Our data set is collected from Twitter website using
its Web API. Because twitter does not provide direct
functions to group friends, we use lists created by
521
twitter users as the reference friend group in testing
and evaluation. We exclude users that have less than
20 or more than 150 friends; that do not have a qual-
ified list (more than 20 and less than 200 list mem-
bers); and that do not use English in their tweets.
After applying these filtering criteria, we found 87
lists from 12 users. For these qualified users, their
1, 383 friends information is retrieved, again using
Twitter API. For the friends that are retrieved, their
180, 296 tweets and 584, 339 friend-of-friend infor-
mation are also retrieved. Among all the retrieved
tweets, there are 65, 329 mentions in total.
5 Experiment
In our experiment, we evaluate the performance of
each sub-system and then use group type detection
algorithm to adaptively combine the systems. We
use the Twitter lists we collected as the reference
user groups for evaluation. For each user group, we
randomly take out 6 users from the list and use as
seed candidate. The target user consists of the rest of
the list members and other ?friends? that the list cre-
ator has. From the ranked list for the target users, we
calculate the mean average precision (MAP) score
with the rank position of the list members. For each
group, we run the experiment 10 times using ran-
domly selected seeds. Then the average MAP on all
runs on all groups is reported. In order to evaluate
the effect of the seed size on the final performance,
we vary the number of seeds from 2 to 6 using the 6
taken-out list members.
In the tweet based sub-system, we optimize its hy-
per parameter automatically based on the data. After
trying different numbers of topics in LDA, we found
optimal performance with 50 topics (? = 0.5 and
? = 0.04).
System Seed Size2 3 5 6
Tweet Sub
CosCent 28.45 29.34 29.54 31.18
CosAvg 28.37 29.51 30.01 31.45
EucCent 27.32 28.12 28.97 29.75
EucAvg 27.54 28.74 29.12 29.97
Social Sub 26.45 27.78 28.12 30.21
Adaptive 30.17 32.43 33.01 34.74
BOW baseline 23.45 24.31 24.73 24.93
Random Baseline 17.32
Table 1: Ranking Result (Mean Average Precision) using
Different Systems.
Table 1 shows the performance of each sub-
system as well as the adaptive system. We include
the baseline results generated using random ranking.
As a stronger baseline (BOW baseline), we used co-
sine similarity between users? tweets as the similar-
ity measure. In this baseline, we used a vocabulary
of 5000 words that have the highest TF-IDF values.
Each user?s tweet content is represented using a bag-
of-words vector using this vocabulary. The ranking
of this baseline is calculated using the average simi-
larity with the seeds.
In the tweet-based sub-system, ?Cos? and ?Euc?
mean cosine similarity and inverse Euclidean dis-
tance respectively as the similarity measure. ?Cent?
and ?Avg? mean using centroid vector and average
similarity respectively to measure the similarities
between a target user and the seed group. From the
results, we can see that in general using a larger seed
group improves performance since more informa-
tion can be obtained from the group. The ?CosAvg?
scheme (which uses cosine similarity with average
similarity measure) achieves the best result. Using
cosine similarity measure gives better performance
than inverse Euclidean distance. This is not surpris-
ing since cosine similarity has been widely adopted
as an appropriate similarity measure in the vector
space model for text processing. The bag-of-word
baseline is much better than the random baseline;
however, using LDA topic modeling to collapse the
dimension of features achieves even better results.
This confirms that topic modeling is very useful in
representing noisy data, such as tweets.
In the adaptive system, we also used ?CosAvg?
scheme in the tweet based sub-system. After the au-
tomatic sub-system selection, we observe increased
performance. This indicates that users form lists
based on different factors and thus always using
one single system is not the best solution. It also
demonstrates that our proposed fitness measure us-
ing cross-validation works well, and that the two in-
formation sources used to build sub-systems can ap-
propriately capture the group characteristics.
6 Conclusion
In this paper, we have proposed an interactive group
creation system for Twitter users to organize their
?followings?. The system takes friend seeds pro-
vided by users and generates a ranked list according
522
to the likelihood of a test user being in the group.
We introduced two sub-systems, based on tweet text
and social information respectively. We also pro-
posed a group type detection procedure that is able
to use the most appropriate system for group user
ranking. Our experiments show that by using differ-
ent systems adaptively, better performance can be
achieved compared to using any single system, sug-
gesting this framework works well. In the future, we
plan to add more sophisticated sub-systems in this
framework, and also explore combining ranking out-
puts from different sub-systems. Furthermore, we
will incorporate negative seeds into the process of
interactive suggestion.
References
David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent dirichlet alocation. Jour-
nal of Machine Learning Research, 3:2003.
Mark Newman. 2004. Analysis of weighted networks.
Physical Review E, 70(5), November.
Gergely Palla, Imre Derenyi, Illes Farkas, and Tamas Vic-
sek. 2005. Uncovering the overlapping community
structure of complex networks in nature and society.
Nature, 435(7043):814?818, June.
Daniel Ramage, Susan Dumais, and Dan Liebling. 2010.
Characterizing microblogs with topic models. In
ICWSM.
Maayan Roth, Assaf Ben-David, David Deutscher, Guy
Flysher, Ilan Horn, Ari Leichtberg, Naty Leiser, Yossi
Matias, and Ron Merom. 2010. Suggesting friends
using the implicit social graph. In SIGKDD, KDD ?10,
pages 233?242. ACM.
523
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 554?562,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Sentence Dependency Tagging in Online Question Answering Forums
Zhonghua Qu and Yang Liu
The University of Texas at Dallas
{qzh,yangl@hlt.utdallas.edu}
Abstract
Online forums are becoming a popular re-
source in the state of the art question answer-
ing (QA) systems. Because of its nature as an
online community, it contains more updated
knowledge than other places. However, go-
ing through tedious and redundant posts to
look for answers could be very time consum-
ing. Most prior work focused on extracting
only question answering sentences from user
conversations. In this paper, we introduce the
task of sentence dependency tagging. Finding
dependency structure can not only help find
answer quickly but also allow users to trace
back how the answer is concluded through
user conversations. We use linear-chain con-
ditional random fields (CRF) for sentence type
tagging, and a 2D CRF to label the depen-
dency relation between sentences. Our ex-
perimental results show that our proposed ap-
proach performs well for sentence dependency
tagging. This dependency information can
benefit other tasks such as thread ranking and
answer summarization in online forums.
1 Introduction
Automatic Question Answering (QA) systems rely
heavily on good sources of data that contain ques-
tions and answers. Question answering forums, such
as technical support forums, are places where users
find answers through conversations. Because of
their nature as online communities, question answer-
ing forums provide more updated answers to new
problems. For example, when the latest release of
Linux has a bug, we can expect to find solutions
in forums first. However, unlike other structured
knowledge bases, often it is not straightforward to
extract information such as questions and answers in
online forums because such information spreads in
the conversations among multiple users in a thread.
A lot of previous work has focused on extract-
ing the question and answer sentences from forum
threads. However, there is much richer information
in forum conversations, and simply knowing a sen-
tence is a question or answer is not enough. For
example, in technical support forums, often it takes
several iterations of asking and clarifications to de-
scribe the question. The same happens to answers.
Usually several candidate answers are provided, and
not all answers are useful. In this case users? feed-
back is needed to judge the correctness of answers.
Figure 1 shows an example thread in a technical
support forum. Each sentence is labeled with its type
(a detailed description of sentence types is provided
Table 1). We can see from the example that ques-
tions and answers are not expressed in a single sen-
tence or a single post. Only identifying question and
answering sentences from the thread is not enough
for automatic question answering. For this example,
in order to get the complete question, we would need
to know that sentence S3 is a question that inquires
for more details about the problem asked earlier, in-
stead of stating its own question. Also, sentence S5
should not be included in the correct answer since
it is not a working solution, which is indicated by a
negative feedback in sentence S6. The correct solu-
tion should be sentence S7, because of a user?s posi-
tive confirmation S9. We define that there is a depen-
dency between a pair of sentences if one sentence
554
A: [S1:M-GRET] Hi everyone. [S2:P-STAT] I
have recently purchased USB flash and I am having
trouble renaming it, please help me.
B: [S3:A-INQU] What is the size and brand of this
flash?
A: [S4:Q-CLRF] It is a 4GB SanDisk flash.
B: [S5:A-SOLU] Install gparted, select flash drive
and rename.
A: [S6:M-NEGA] I got to the Right click on
partition and the label option was there but grayed
out.
B: [S7:A-SOLU] Sorry again, I meant to right click
the partition and select Unmount and then select
Change name while in gparted.
A: [S8:C-GRAT] Thank you so much. [S9:M-
POST] I now have an ?Epic USB? You Rock!
Figure 1: Example of a Question Answering Thread in
Ubuntu Support Forum
exists as a result of another sentence. For example,
question context sentences exist because of the ques-
tion itself; an answering sentence exists because of
a question; or a feedback sentence exists because of
an answer. The sentence dependency structure of
this example dialog is shown in Figure 2.
S1: M-GRET
S2: P-STAT
S3: A-INQU S4:Q-CLRF
S5:A-SOLU S6:M-NEGA S7:A-SOLU
S8:C-GRAT
S9:M-POST
Figure 2: Dependency Structure of the Above Example
This example shows that in order to extract in-
formation from QA forums accurately, we need to
understand the sentence dependency structure of a
QA thread. Towards this goal, in this paper, we de-
fine two tasks: labeling the types for sentences, and
finding the dependency relations between sentences.
For the first task of sentence type labeling, we de-
fine a rich set of categories representing the purpose
of the sentences. We use linear-chain conditional
random fields (CRF) to take advantage of many
long-distance and non-local features. The second
task is to identify relations between sentences. Most
previous work only focused on finding the answer-
question relationship between sentences. However,
other relations can also be useful for information ex-
traction from online threads, such as user?s feed-
backs on the answers, problem detail inquiry and
question clarifications. In this study, we use two
approaches for labeling of dependency relation be-
tween sentences. First each sentence is considered
as a source, and we run a linear-chain CRF to la-
bel whether each of the other sentences is its tar-
get. Because multiple runs of separate linear-chain
CRFs ignore the dependency between source sen-
tences, the second approach we propose is to use a
2D CRF that models all pair relationships jointly.
The data we used was collected from Ubuntu
forum general help section. Our experimental re-
sults show that our proposed sentence type tagging
method works very well, even for the minority cate-
gories, and that using 2D CRF further improves per-
formance over linear-chain CRFs for identifying de-
pendency relation between sentences.
The paper is organized as follows. In the follow-
ing section, we discuss related work on finding ques-
tions and answers in online environment as well as
some dialog act tagging techniques. In Section 3, we
introduce the use of CRFs for sentence type and de-
pendency tagging. Section 4 describes data collec-
tion, annotation, and some analysis. In Section 5, we
show that our approach achieves promising results
in thread sentence dependency tagging. Finally we
conclude the paper and suggest some possible future
extensions.
2 Related Work
There is a lot of useful knowledge in the user gener-
ated content such as forums. This knowledge source
could substantially help automatic question answer-
ing systems. There has been some previous work
focusing on the extraction of question and corre-
sponding answer pairs in online forums. In (Ding
et al, 2008), a two-pass approach was used to find
relevant solutions for a given question, and a skip-
chain CRF was adopted to model long range de-
555
pendency between sentences. A graph propagation
method was used in (Cong et al, 2008) to rank
relevant answers to questions. An approach using
email structure to detect and summarize question an-
swer pairs was introduced in (Shrestha and Mck-
eown, 2004). These studies focused primarily on
finding questions and answers in an online envi-
ronment. In this paper, in order to provide a bet-
ter foundation for question answer detection in on-
line forums, we investigate tagging sentences with a
much richer set of categories, as well as identifying
their dependency relationships. The sentence types
we use are similar to dialog acts (DA), but defined
specifically for question answering forums. Work of
(Clark and Popescu-Belis, 2004) defined a reusable
multi-level tagset that can be mapped from conversa-
tional speech corpora such as the ICSI meeting data.
However, it is hard to reuse any available corpus or
DA tagset because our task is different, and also on-
line forum has a different style from speech data.
Automatic DA tagging has been studied a lot previ-
ously. For example, in (Stolcke et al, 2000), Hidden
Markov Models (HMMs) were used for DA tagging;
in (Ji and Bilmes, 2005), different types of graphical
models were explored.
Our study is different in several aspects: we are
using forum domains, unlike most work of DA tag-
ging on conversational speech; we use CRFs for sen-
tence type tagging; and more importantly, we also
propose to use different CRFs for sentence relation
detection. Unlike the pair-wise sentence analysis
proposed in (Boyer et al, 2009) in which HMM
was used to model the dialog structure, our model is
more flexible and does not require related sentences
to be adjacent.
3 Thread Structure Tagging
As described earlier, we decompose the structure
analysis of QA threads into two tasks, first deter-
mine the sentence type, and then identify related
sentences. This section provides details for each
task.
3.1 Sentence Type Tagging
In human conversations, especially speech conver-
sations, DAs have been used to represent the pur-
pose or intention of a sentence. Different sets of
DAs have been adopted in various studies, ranging
from very coarse categories to fine grained ones. In
this study, we define 13 fine grained sentence types
(corresponding to 4 coarse categories) tailored to our
domain of QA forum threads. Table 1 shows the cat-
egories and their description. Some tags such as P-
STAT and A-SOLU are more important in that users
try to state a problem and provide solutions accord-
ingly. These are the typical ones used in previous
work on question answering. Our set includes other
useful tags. For example, C-NEGA and C-POSI can
evaluate how good an answer is. Even though C-
GRAT does not provide any direct feedback on the
solutions, existence of such a tag often strongly im-
plies a positive feedback to an answer. These sen-
tence types can be grouped into 4 coarse categories,
as shown in Table 1.
Types Category Description
Problems
P-STAT question of problem
P-CONT problem context
P-CLRF problem clarification
Answers
A-SOLU solution sentence
A-EXPL explanation on solutions
A-INQU inquire problem details
Confirm.
C-GRAT gratitude
C-NEGA negative feedback
C-POSI positive feedback
Misc.
M-QCOM question comment
M-ACOM comment on the answer
M-GRET greeting and politeness
M-OFF off-topic sentences
Table 1: Sentence Types for QA Threads
To automatically label sentences in a thread with
their types, we adopt a sequence labeling approach,
specifically linear-chain conditional random fields
(CRFs), which have shown good performance in
many other tasks (Lafferty, 2001). Intuitively there
is a strong dependency between adjacent sentences.
For example, in our data set, 45% sentences follow-
ing a greeting sentence (M-GRET) are question re-
lated sentences; 53% sentences following a question
inquiry sentence (Q-INQ) are solution related sen-
tences. The following describes our modeling ap-
proaches and features used for sentence type tag-
ging.
556
3.1.1 Linear-chain Conditional Random Field
Linear-chain CRFs is a type of undirected graphi-
cal models. Distribution of a set of variables in undi-
rected graphical models can be written as
p(x, y) =
1
Z
?
A
?A(xA, yA) (1)
Z is the normalization constant to guarantee valid
probability distributions. CRFs is a special case
of undirected graphical model in which ? are log-
linear functions:
?A(xA, yA) = exp
{
?
k
?AkfAk(xA, yA)
}
(2)
?A is a real value parameter vector for feature
function set fA. In the sequence labeling task, fea-
ture functions across the sequence are often tied to-
gether. In other words, feature functions at different
locations of the sequence share the same parameter
vector ?.
Figure 3: Graphical Structure of Linear-chain CRFs.
Linear-chain CRF is a special case of the general
CRFs. In linear-chain CRF, cliques only involve two
adjacent variables in the sequence. Figure 3 shows
the graphical structure of a linear-chain CRF. In our
case of sentence tagging, cliques only contain two
adjacent sentences. Given the observation x, the
probability of label sequence y is as follows:
p(y|x) =
1
Z
|y|?
i=1
?e(x, y, i)
|y|?
j=0
?v(x, y, j) (3)
?e(x, y, i) = exp
{
?
k
?ekfek(yi?1, yi, x, i)
}
(4)
?v(x, y, j) = exp
{
?
k
?vkfvk(yj , x, j)
}
(5)
where feature templates fek and fvk correspond to
edge features and node features respectively.
Feature Description
Cosine similarity with previous sentence.
Quote segment within two adjacent sentences?
Code segment within two adjacent sentences?
Does this sentence belong to author?s post?
Is it the first sentence in a post?
Post author participated thread before?
Does the sentence contain any negative words?
Does the sentence contain any URL?
Does the sentence contain any positive words?
Does the sentence contain any question mark?
Length of the sentence.
Presence of verb.
Presence of adjective.
Sentence perplexity based on a background LM.
Bag of word features.
Table 2: Features Used in Sentence Type Tagging.
3.1.2 Sentence Type Tagging Features
We used various types of feature functions in sen-
tence type tagging. Table 2 shows the complete list
of features we used. Edge features are closely re-
lated to the transition between sentences. Here we
use the cosine similarity between sentences, where
each sentence is represented as a vector of words,
with term weight calculated using TD-IDF (term fre-
quency times inverse document frequency). High
similarity between adjacent sentences suggests sim-
ilar or related types. For node features, we explore
different sources of information about the sentence.
For example, the presence of a question mark indi-
cates that a sentence may be a question or inquiry.
Similarly, we include other cues, such as positive
or negative words, verb and adjective words. Since
technical forums tend to contain many system out-
puts, we include the perplexity of the sentence as a
feature which is calculated based on a background
language model (LM) learned from common En-
glish documents. We also use bag-of-word features
as in many other text categorization tasks.
Furthermore, we add features to represent post
level information to account for the structure of QA
threads, for example, whether or not a sentence be-
longs to the author?s post, or if a sentence is the be-
ginning sentence of a post.
557
3.2 Sentence Dependency Tagging
Knowing only the sentence types without their de-
pendency relations is not enough for question an-
swering tasks. For example, correct labeling of an
answer without knowing which question it actually
refers to is problematic; not knowing which answer
a positive or negative feedback refers to will not be
helpful at all. In this section we describe how sen-
tence dependency information is determined. Note
that sentence dependency relations might not be a
one-to-one relation. A many-to-many relation is also
possible. Take question answer relation as an ex-
ample. There could be potentially many answers
spreading in many sentences, all depending on the
same question. Also, it is very likely that a question
is expressed in multiple sentences too.
Dependency relationship could happen between
many different types of sentences, for example, an-
swer(s) to question(s), problem clarification to ques-
tion inquiry, feedback to solutions, etc. Instead of
developing models for each dependency type, we
treat them uniformly as dependency relations be-
tween sentences. Hence, for every two sentences,
it becomes a binary classification problem, i.e.,
whether or not there exists a dependency relation
between them. For a pair of sentences, we call the
depending sentence the source sentence, and the de-
pended sentence the target sentence. As described
earlier, one source sentence can potentially depend
on many different target sentences, and one target
sentence can also correspond to multiple sources.
The sentence dependency task is formally defined
as, given a set of sentences St of a thread, find the
dependency relation {(s, t)|s ? St, t ? St}, where s
is the source sentence and t is the target sentence that
s depends on.
We propose two methods to find the dependency
relationship. In the first approach, for each source
sentence, we run a labeling procedure to find the de-
pendent sentences. From the data, we found given a
source sentence, there is strong dependency between
adjacent target sentences. If one sentence is a tar-
get sentence of the source, often the next sentence
is a target sentence too. In order to take advantage
of such adjacent sentence dependency, we use the
linear-chain CRFs for the sequence labeling. Fea-
tures used in sentence dependency labeling are listed
in Table 3. Note that a lot of the node features used
here are relative to the source sentence since the task
here is to determine if the two sentences are related.
For a thread of N sentences, we need to perform N
runs of CRF labeling, one for each sentence (as the
source sentence) in order to label the target sentence
corresponding to this source sentence.
Feature Description
* Cosine similarity with previous sentence.
* Is adjacent sentence of the same type?
* Pair of types of the adjacent target sentences.
Pair of types of the source and target sentence.
Is target in the same post as the source?
Do target and source belong to the same author?
Cosine similarity between target and source sentence.
Does target sentence happen before source?
Post distance between source and target sentence.
* indicates an edge feature
Table 3: Features Used in Sentence Dependency Labeling
The linear-chain CRFs can represent the depen-
dency between adjacent target sentences quite well.
However they cannot model the dependency be-
tween adjacent source sentences, because labeling
is done for each source sentence individually. To
model the dependency between both the source sen-
tences and the target sentences, we propose to use
2D CRFs for sentence relation labeling. 2D CRFs
are used in many applications considering two di-
mension dependencies such as object recognitions
(Quattoni et al, 2004) and web information extrac-
tion (Zhu et al, 2005). The graphical structure of
a 2D CRF is shown in Figure 4. Unlike one di-
mensional sequence labeling, a node in 2D environ-
ment is dependent on both x-axis neighbors and y-
axis neighbors. In the sentence relation task, the
source and target pair is a 2D relation in which its
label depends on labels of both its adjacent source
and its adjacent target sentence. As shown in Fig-
ure 4, looking from x-axis is the sequence of target
sentences with a fixed source sentence, and from y-
axis is the sequence of source sentences with a fixed
target sentence. This model allows us to model all
the sentence relationships jointly. 2D CRFs contain
3 templates of features: node template, x-axis edge
template, and y-axis edge template. We use the same
edge features and node features as in linear-chain
CRFs for node features and y-axis edge features in
558
2D CRFs. For the x-axis edge features, we use the
same feature functions as for y-axis, except that now
they represent the relation between adjacent source
sentences.
y i y i + 1 . . .. . .
X
y 0 0 . . .
. . .
. . . . . . . . .
y 10
y 0 1 y 11 X
So u r c e
T
arg
et
Figure 4: Graphical Structure of 2D CRF for Sentence
Relation Labeling.
In a thread containing N sentences, we would
have a 2D CRF containing N2 nodes in a N ? N
grid. Exact inference in such a graph is intractable.
In this paper we use loopy belief propagation algo-
rithm for the inference. Loopy belief propagation is
a message passing algorithm for graph inference. It
calculates the marginal distribution for each node in
the graph. The result is exact in some graph struc-
tures (e.g., linear-chain CRFs), and often converges
to a good approximation for general graphs.
4 Data
We used data from ubuntu community forum gen-
eral help section for the experiments and evalua-
tion. This is a technical support section that provides
answers to the latest problems in Ubuntu Linux.
Among all the threads that we have crawled, we se-
lected 200 threads for this initial study. They con-
tain between 2? 10 posts and at least 2 participants.
Sentences inside each thread are segmented using
Apache OpenNLP tools. In total, there are 706 posts
and 3,483 sentences. On average, each thread con-
tains 3.53 posts, and each post contains around 4.93
sentences. Two annotators were recruited to anno-
tate the sentence type and the dependency relation
between sentences. Annotators are both computer
science department undergraduate students. They
are provided with detailed explanation of the anno-
tation standard. The distribution of sentence types
in the annotated data is shown in Table 4, along with
inter-annotator Kappa statistics calculated using 20
common threads annotated by both annotators. We
can see that the majority of the sentences are about
problem descriptions and solutions. In general, the
agreement between the two annotators is quite good.
General Type Category Percentage Kappa
Problems
P-STAT 12.37 0.88
P-CONT 37.30 0.77
P-CLRF 1.01 0.98
Answers
A-SOLU 9.94 0.89
A-EXPL 11.60 0.89
A-INQU 1.38 0.99
Confirmation
C-GRAT 5.06 0.98
C-NEGA 1.98 0.96
C-POSI 1.84 0.96
Miscellaneous
M-QCOM 1.98 0.93
M-ACOM 1.47 0.96
M-GRET 1.01 0.96
M-OFF 7.92 0.96
Table 4: Distribution and Inter-annotator Agreement of
Sentence Types in Data
There are in total 1, 751 dependency relations
identified by the annotators among those tagged sen-
tences. Note that we are only dealing with intra-
thread sentence dependency, that is, no dependency
among sentences in different threads is labeled.
Considering all the possible sentence pairs in each
thread, the labeled dependency relations represent a
small percentage. The most common dependency
is problem description to problem question. This
shows that users tend to provide many details of
the problem. This is especially true in technical fo-
rums. Seeing questions without their context would
be confusing and hard to solve. The relation of an-
swering solutions and question dependency is also
very common, as expected. The third common re-
lation is the feedback dependency. Even though the
number of feedback sentences is small in the data
set, it plays a vital role to determine the quality of
answers. The main reason for the small number is
that, unlike problem descriptions, much fewer sen-
tences are needed to give feedbacks.
5 Experiment
In the experiment, we randomly split annotated
threads into three disjoint sets, and run a three-fold
cross validation. Within each fold, first sentence
types are labeled using linear-chain CRFs, then the
559
resulting sentence type tagging is used in the sec-
ond pass to determine dependency relations. For
part-of-speech (POS) tagging of the sentences, we
used Stanford POS Tagger (Toutanova and Man-
ning, 2000). All the graphical inference and estima-
tions are done using MALLET package (McCallum,
2002).
In this paper, we evaluate the results using stan-
dard precision and recall. In the sentence type tag-
ging task, we calculate precision, recall, and F1
score for each individual tag. For the dependency
tagging task, a pair identified by the system is cor-
rect only if the exact pair appears in the reference an-
notation. Precision and recall scores are calculated
accordingly.
5.1 Sentence Type Tagging Results
The results of sentence type tagging using linear-
chain CRFs are shown in Table 5. For a comparison,
we include results using a basic first-order HMM
model. Because HMM is a generative model, we
use only bag of word features in the generative pro-
cess. The observation probability is the probabil-
ity of the sentence generated by a unigram language
model, trained for different sentence types. Since
for some applications, fine grained categories may
not be needed, for example, in the case of finding
questions and answers in a thread, we also include
in the table the tagging results when only the gen-
eral categories are used in both training and testing.
We can see from the table that using CRFs
achieves significantly better performance than
HMMs for most categories, except greeting and off-
topic types. This is mainly because of the advantage
of CRFs, allowing the incorporation of rich discrimi-
native features. For the two major types of problems
and answers, in general, our system shows very good
performance. Even for minority types like feed-
backs, it also performs reasonably well. When using
coarse types, the performance on average is better
compared to the finer grained categories, mainly be-
cause of the fewer classes in the classification task.
Using the fine grained categories, we found that the
system is able to tell the difference between ?prob-
lem statement? (P-STAT) and ?problem context? (P-
CONT). Note that in our task, a problem statement is
not necessarily a question sentence. Instead it could
be any sentence that expresses the need for a solu-
Linear-chain CRF First-order HMM
13 Fine Grained Types
Tag Prec. / Rec. F1 Prec. / Rec. F1
M-GRET 0.45 / 0.58 0.51 0.73 / 0.57 0.64
P-STAT 0.79 / 0.72 0.75 0.35 / 0.34 0.35
P-CONT 0.80 / 0.74 0.77 0.58 / 0.18 0.27
A-INQU 0.37 / 0.48 0.42 0.11 / 0.25 0.15
A-SOLU 0.78 / 0.64 0.71 0.27 / 0.29 0.28
A-EXPL 0.4 / 0.76 0.53 0.24 / 0.19 0.21
M-POST 0.5 / 0.41 0.45 0.04 / 0.1 0.05
C-GRAT 0.43 / 0.53 0.48 0.01 / 0.25 0.02
M-NEGA 0.67 / 0.5 0.57 0.09 / 0.31 0.14
M-OFF 0.11 / 0.23 0.15 0.20 / 0.23 0.21
P-CLRF 0.15 / 0.33 0.21 0.10 / 0.12 0.11
M-ACOM 0.27 / 0.38 0.32 0.09 / 0.1 0.09
M-QCOM 0.34 / 0.32 0.33 0.08 / 0.23 0.11
4 General Types
Tag Prec. / Rec. F1 Prec. / Rec. F1
Problem 0.85 / 0.76 0.80 0.73 / 0.27 0.39
Answers 0.65 / 0.72 0.68 0.45 / 0.36 0.40
Confirm. 0.80 / 0.74 0.77 0.06 / 0.26 0.10
Misc. 0.43 / 0.61 0.51 0.04 / 0.36 0.08
Table 5: Sentence Type Tagging Performance Using
CRFs and HMM.
tion.
We also performed some analysis of the features
using the feature weights in the trained CRF mod-
els. We find that some post level information is rela-
tively important. For example, the feature represent-
ing whether the sentence is before a ?code? segment
has a high weight for problem description classifica-
tion. This is because in linux support forum, people
usually put some machine output after their problem
description. We also notice that the weights for verb
words are usually high. This is intuitive since the
?verb? of a sentence can often determine its purpose.
5.2 Sentence Dependency Tagging Results
Table 6 shows the results using linear-chain CRFs
(L-CRF) and 2D CRFs for sentence dependency tag-
ging. We use different settings in our experiments.
For the categories of sentence types, we evaluate us-
ing both the fine grained (13 types) and the coarse
categories (4 types). Furthermore, we examine two
ways to obtain the sentence types. First, we use the
output from automatic sentence type tagging. In the
second one, we use the sentence type information
from the human annotated data in order to avoid the
error propagation from automatic sentence type la-
560
beling. This gives an oracle upper bound for the
second pass performance.
Using Oracle Sentence Type
Setup Precision Recall F1
13 types
L-CRF 0.973 0.453 0.618
2D-CRF 0.985 0.532 0.691
4 general
L-CRF 0.941 0.124 0.218
2D-CRF 0.956 0.145 0.252
Using System Sentence Type
Setup Precision Recall F1
13 types
L-CRF 0.943 0.362 0.523
2D-CRF 0.973 0.394 0.561
4 general
L-CRF 0.939 0.101 0.182
2D-CRF 0.942 0.127 0.223
Table 6: Sentence Dependency Tagging Performance
From the results we can see that 2D CRFs out-
perform linear-chain CRFs for all the conditions.
This shows that by modeling the 2D dependency in
source and target sentences, system performance is
improved. For the sentence types, when using auto-
matic sentence type tagging systems, there is a per-
formance drop. The performance gap between us-
ing the reference and automatic sentence types sug-
gests that there is still room for improvement from
better sentence type tagging. Regarding the cate-
gories used for the sentence types, we observe that
they have an impact on dependence tagging perfor-
mance. When using general categories, the perfor-
mance is far behind that using the fine grained types.
This is because some important information is lost
when grouping categories. For example, a depen-
dency relation can be: ?A-EXPL? (explanation for
solutions) depends on ?A-SOLU? (solutions); how-
ever, when using coarse categories, both are mapped
to ?Solution?, and having one ?Solution? depending
on another ?Solution? is not very intuitive and hard
to model properly. This shows that detailed cate-
gory information is very important for dependency
tagging even though the tagging accuracy from the
first pass is far from perfect.
Currently our system does not put constraints on
the sentence types for which dependencies exist. In
the system output we find that sometimes there are
obvious dependency errors, such as a positive feed-
back depending on a negative feedback. We may
improve our models by taking into account different
sentence types and dependency relations.
6 Conclusion
In this paper, we investigated sentence dependency
tagging of question and answer (QA) threads in on-
line forums. We define the thread tagging task as a
two-step process. In the first step, sentence types
are labeled. We defined 13 sentence types in or-
der to capture rich information of sentences to bene-
fit question answering systems. Linear chain CRF
is used for sentence type tagging. In the second
step, we label actual dependency between sentences.
First, we propose to use a linear-chain CRF to label
possible target sentences for each source sentence.
Then we improve the model to consider the depen-
dency between sentences along two dimensions us-
ing a 2D CRF. Our experiments show promising
performance in both tasks. This provides a good
pre-processing step towards automatic question an-
swering. In the future, we plan to explore using
constrained CRF for more accurate dependency tag-
ging. We will also use the result from this work in
other tasks such as answer quality ranking and an-
swer summarization.
7 Acknowledgment
This work is supported by DARPA under Contract
No. HR0011-12-C-0016 and NSF No. 0845484.
Any opinions expressed in this material are those of
the authors and do not necessarily reflect the views
of DARPA or NSF.
References
Kristy Elizabeth Boyer, Robert Phillips, Eun Young Ha,
Michael D. Wallis, Mladen A. Vouk, and James C.
Lester. 2009. Modeling dialogue structure with ad-
jacency pair analysis and hidden markov models. In
Proc. NAACL-Short, pages 49?52.
Alexander Clark and Andrei Popescu-Belis. 2004.
Multi-level dialogue act tags. In Proc. SIGDIAL,
pages 163?170.
Gao Cong, Long Wang, Chinyew Lin, Youngin Song, and
Yueheng Sun. 2008. Finding question-answer pairs
from online forums. In Proc. SIGIR, pages 467?474.
Shilin Ding, Gao Cong, Chinyew Lin, and Xiaoyan Zhu.
2008. Using conditional random fields to extract con-
texts and answers of questions from online forums. In
Proc. ACL-HLT.
Gang Ji and J Bilmes. 2005. Dialog Act Tagging Using
Graphical Models. In Proc. ICASSP.
561
John Lafferty. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proc. ICML, pages 282?289.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Ariadna Quattoni, Michael Collins, and Trevor Darrell.
2004. Conditional random fields for object recogni-
tion. In Proc. NIPS, pages 1097?1104.
Lokesh Shrestha and Kathleen Mckeown. 2004. Detec-
tion of question-answer pairs in email conversations.
In Proc. Coling, pages 889?895.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Rachel Martin, Carol Van Ess-Dykema, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26:339?373.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In Proc. EMNLP/VLC,
pages 63?70.
Jun Zhu, Zaiqing Nie, Ji R. Wen, Bo Zhang, and Wei Y.
Ma. 2005. 2D Conditional Random Fields for Web
information extraction. In Proc. ICML, pages 1044?
1051, New York, NY, USA.
562
