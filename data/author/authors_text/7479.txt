Chunking Japanese Compound Functional Expressions
by Machine Learning
Masatoshi Tsuchiya? and Takao Shime? and Toshihiro Takagi?
Takehito Utsuro?? and Kiyotaka Uchimoto?? and Suguru Matsuyoshi?
Satoshi Sato?? and Seiichi Nakagawa??
?Computer Center / ??Department of Information and Computer Sciences,
Toyohashi University of Technology, Tenpaku-cho, Toyohashi, 441?8580, JAPAN
?Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto, 606?8501, JAPAN
??Graduate School of Systems and Information Engineering, University of Tsukuba,
1-1-1, Tennodai, Tsukuba, 305-8573, JAPAN
??National Institute of Information and Communications Technology,
3?5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619?0289 JAPAN
??Graduate School of Engineering, Nagoya University,
Furo-cho, Chikusa-ku, Nagoya, 464?8603, JAPAN
Abstract
The Japanese language has various types
of compound functional expressions,
which are very important for recogniz-
ing the syntactic structures of Japanese
sentences and for understanding their
semantic contents. In this paper, we
formalize the task of identifying Japanese
compound functional expressions in a
text as a chunking problem. We apply a
machine learning technique to this task,
where we employ that of Support Vector
Machines (SVMs). We show that the pro-
posed method significantly outperforms
existing Japanese text processing tools.
1 Introduction
As in the case of other languages, the Japanese
language has various types of functional words
such as post-positional particles and auxiliary
verbs. In addition to those functional words,
the Japanese language has much more compound
functional expressions which consist of more than
one words including both content words and func-
tional words. Those single functional words as
well as compound functional expressions are very
important for recognizing the syntactic structures
of Japanese sentences and for understanding their
semantic contents. Recognition and understanding
of them are also very important for various kinds
of NLP applications such as dialogue systems, ma-
chine translation, and question answering. How-
ever, recognition and semantic interpretation of
compound functional expressions are especially
difficult because it often happens that one com-
pound expression may have both a literal (in other
words, compositional) content word usage and
a non-literal (in other words, non-compositional)
functional usage.
For example, Table 1 shows two example sen-
tences of a compound expression ?? (ni)???
(tsuite)?, which consists of a post-positional par-
ticle ?? (ni)?, and a conjugated form ????
(tsuite)? of a verb ??? (tsuku)?. In the sentence
(A), the compound expression functions as a case-
marking particle and has a non-compositional
functional meaning ?about?. On the other hand,
in the sentence (B), the expression simply corre-
sponds to a literal concatenation of the usages of
the constituents: the post-positional particle ??
(ni)? and the verb ???? (tsuite)?, and has a
content word meaning ?follow?. Therefore, when
considering machine translation of those Japanese
sentences into English, it is necessary to precisely
judge the usage of the compound expression ??
(ni)??? (tsuite)?, as shown in the English trans-
lation of the two sentences in Table 1.
There exist widely-used Japanese text process-
ing tools, i.e., pairs of a morphological analysis
tool and a subsequent parsing tool, such as JU-
MAN1+ KNP2 and ChaSen3+ CaboCha4. How-
ever, they process those compound expressions
only partially, in that their morphological analy-
sis dictionaries list only limited number of com-
pound expressions. Furthermore, even if certain
expressions are listed in a morphological analysis
1http://www.kc.t.u-tokyo.ac.jp/
nl-resource/juman-e.html
2http://www.kc.t.u-tokyo.ac.jp/
nl-resource/knp-e.html
3http://chasen.naist.jp/hiki/ChaSen/
4http://chasen.org/?taku/software/
cabocha/
25
Table 1: Translation Selection of a Japanese Compound Expression ?? (ni)??? (tsuite)?
? (watashi) ? (ha) ? (kare) ? (ni)??? (tsuite) ??? (hanashita)
(A) (I) (TOP) (he) (about) (talked)
(I talked about him.)
? (watashi) ? (ha) ? (kare) ? (ni) ??? (tsuite) ??? (hashitta)
(B) (I) (TOP) (he) (ACC) (follow) (ran)
(I ran following him.)
Table 2: Classification of Functional Expressions based on Grammatical Function
# of major # of
Grammatical Function Type expressions variants Example
subsequent to predicate 36 67 ????
post-positional / modifying predicate (to-naru-to)
particle subsequent to nominal 45 121 ?????
type / modifying predicate (ni-kakete-ha)
subsequent to predicate, nominal 2 3 ???
/ modifying nominal (to-iu)
auxiliary verb type 42 146 ??? (te-ii)
total 125 337 ?
dictionary, those existing tools often fail in resolv-
ing the ambiguities of their usages, such as those
in Table 1. This is mainly because the frame-
work of those existing tools is not designed so as
to resolve such ambiguities of compound (possi-
bly functional) expressions by carefully consider-
ing the context of those expressions.
Considering such a situation, it is necessary
to develop a tool which properly recognizes and
semantically interprets Japanese compound func-
tional expressions. In this paper, we apply a ma-
chine learning technique to the task of identify-
ing Japanese compound functional expressions in
a text. We formalize this identification task as a
chunking problem. We employ the technique of
Support Vector Machines (SVMs) (Vapnik, 1998)
as the machine learning technique, which has been
successfully applied to various natural language
processing tasks including chunking tasks such
as phrase chunking (Kudo and Matsumoto, 2001)
and named entity chunking (Mayfield et al, 2003).
In the preliminary experimental evaluation, we fo-
cus on 52 expressions that have balanced distribu-
tion of their usages in the newspaper text corpus
and are among the most difficult ones in terms of
their identification in a text. We show that the pro-
posed method significantly outperforms existing
Japanese text processing tools as well as another
tool based on hand-crafted rules. We further show
that, in the proposed SVMs based framework, it is
sufficient to collect and manually annotate about
50 training examples per expression.
2 Japanese Compound Functional
Expressions and their Example
Database
2.1 Japanese Compound Functional
Expressions
There exist several collections which list Japanese
functional expressions and examine their usages.
For example, (Morita and Matsuki, 1989) examine
450 functional expressions and (Group Jamashii,
1998) also lists 965 expressions and their exam-
ple sentences. Compared with those two collec-
tions, Gendaigo Hukugouji Youreishu (National
Language Research Institute, 2001) (henceforth,
denoted as GHY) concentrates on 125 major func-
tional expressions which have non-compositional
usages, as well as their variants5 (337 expressions
in total), and collects example sentences of those
expressions. As a first step of developing a tool for
identifying Japanese compound functional expres-
sions, we start with those 125 major functional ex-
pressions and their variants. In this paper, we take
an approach of regarding each of those variants as
a fixed expression, rather than a semi-fixed expres-
sion or a syntactically-flexible expression (Sag et
al., 2002). Then, we focus on evaluating the ef-
fectiveness of straightforwardly applying a stan-
5For each of those 125 major expressions, the differences
between it and its variants are summarized as below: i) in-
sertion/deletion/alternation of certain particles, ii) alternation
of synonymous words, iii) normal/honorific/conversational
forms, iv) base/adnominal/negative forms.
26
Table 3: Examples of Classifying Functional/Content Usages
Expression Example sentence (English translation) Usage
(1) ???? ????????????? ????
??????
functional
(to-naru-to) (The situation is serious if it is not effec-
tive against this disease.)
(???? (to-naru-to) = if)
(2) ???? ???????????????
???? ????????
content
(to-naru-to) (They think that it will become a require-
ment for him to be the president.)
(????? (to-naru-to)
= that (something) becomes ?)
(3) ????? ???????? ????? ????
??????????
functional
(ni-kakete-ha) (He has a great talent for earning money.) (?????? (ni-kakete-ha)
= for ?)
(4) ????? ???? ????? ???? content
(ni-kakete-ha) (I do not worry about it.)
( (??)??????
((?)-wo-ki-ni-kakete-ha)
= worry about ?)
(5) ??? ??????? ??? ??????
??
functional
(to-iu) (I heard that he is alive.) (???? (to-iu) = that ?)
(6) ??? ????????????? ????? content
(to-iu) (Somebody says ?Please visit us.?.) (???? (to-iu)
= say (that) ?)
(7) ??? ???????????? ??? ? functional
(te-ii) (You may have a break after we finish this
discussion.)
(???? (te-ii) = may ?)
(8) ??? ????????? ??? ? content
(te-ii) (This bag is nice because it is big.) (???? (te-ii)
= nice because ?)
dard chunking technique to the task of identifying
Japanese compound functional expressions.
As in Table 2, according to their grammat-
ical functions, those 337 expressions in total
are roughly classified into post-positional particle
type, and auxiliary verb type. Functional expres-
sions of post-positional particle type are further
classified into three subtypes: i) those subsequent
to a predicate and modifying a predicate, which
mainly function as conjunctive particles and are
used for constructing subordinate clauses, ii) those
subsequent to a nominal, and modifying a predi-
cate, which mainly function as case-marking parti-
cles, iii) those subsequent to a nominal, and modi-
fying a nominal, which mainly function as adnom-
inal particles and are used for constructing adnom-
inal clauses. For each of those types, Table 2 also
shows the number of major expressions as well as
that of their variants listed in GHY, and an exam-
ple expression. Furthermore, Table 3 gives exam-
ple sentences of those example expressions as well
as the description of their usages.
2.2 Issues on Identifying Compound
Functional Expressions in a Text
The task of identifying Japanese compound func-
tional expressions roughly consists of detecting
candidates of compound functional expressions in
a text and of judging the usages of those can-
didate expressions. The class of Japanese com-
pound functional expressions can be regarded as
closed and their number is at most a few thousand.
27
Table 4: Examples of Detecting more than one Candidate Expression
Expression Example sentence (English translation) Usage
(9) ??? ????? ??? ???????? functional
(to-iu) (That?s why a match is not so easy.) (NP1??? (to-iu)NP2
= NP
2
called as NP
1
)
(10) ?????? ??? ?????? ???????? functional
(to-iu-mono-no) (Although he won, the score is bad.)
(???????
(to-iu-mono-no)
= although ?)
Therefore, it is easy to enumerate all the com-
pound functional expressions and their morpheme
sequences. Then, in the process of detecting can-
didates of compound functional expressions in a
text, the text are matched against the morpheme
sequences of the compound functional expressions
considered.
Here, most of the 125 major functional expres-
sions we consider in this paper are compound ex-
pressions which consist of one or more content
words as well as functional words. As we intro-
duced with the examples of Table 1, it is often
the case that they have both a compositional con-
tent word usage as well as a non-compositional
functional usage. For example, in Table 3, the
expression ????? (to-naru-to)? in the sen-
tence (2) has the meaning ? that (something) be-
comes ??, which corresponds to a literal concate-
nation of the usages of the constituents: the post-
positional particle ???, the verb ????, and the
post-positional particle ???, and can be regarded
as a content word usage. On the other hand, in
the case of the sentence (1), the expression ???
?? (to-naru-to)? has a non-compositional func-
tional meaning ?if?. Based on this discussion, we
classify the usages of those expressions into two
classes: functional and content. Here, functional
usages include both non-compositional and com-
positional functional usages, although most of the
functional usages of those 125 major expressions
can be regarded as non-compositional. On the
other hand, content usages include compositional
content word usages only.
More practically, in the process of detecting
candidates of compound functional expressions in
a text, it can happen that more than one can-
didate expression is detected. For example, in
Table 4, both of the candidate compound func-
tional expressions ???? (to-iu)? and ????
??? (to-iu-mono-no)? are detected in the sen-
tence (9). This is because the sequence of the two
morphemes ?? (to)? and ??? (iu)? constituting
the candidate expression ???? (to-iu)? is a sub-
sequence of the four morphemes constituting the
candidate expression ??????? (to-iu-mono-
no)? as below:
Morpheme sequence
? (to) ?? (iu) ?? (mono) ? (no)
Candidate expression??? (to-iu)
? (to) ?? (iu) ?? (mono) ? (no)
Candidate expression?????? (to-iu-mono-no)
? (to) ?? (iu) ?? (mono) ? (no)
This is also the case with the sentence (10).
Here, however, as indicated in Table 4, the sen-
tence (9) is an example of the functional usage of
the compound functional expression ???? (to-
iu)?, where the sequence of the two morphemes ?
? (to)? and ??? (iu)? should be identified and
chunked into a compound functional expression.
On the other hand, the sentence (10) is an ex-
ample of the functional usage of the compound
functional expression ??????? (to-iu-mono-
no)?, where the sequence of the four morphemes ?
? (to)?, ??? (iu)?, ??? (mono)?, and ?? (no)?
should be identified and chunked into a compound
functional expression. Actually, in the result of
our preliminary corpus study, at least in about 20%
of the occurrences of Japanese compound func-
tional expressions, more than one candidate ex-
pression can be detected. This result indicates that
it is necessary to consider more than one candidate
expression in the task of identifying a Japanese
compound functional expression, and also in the
task of classifying the functional/content usage of
a candidate expression. Thus, in this paper, based
on this observation, we formalize the task of iden-
tifying Japanese compound functional expressions
as a chunking problem, rather than a classification
problem.
28
Table 5: Number of Sentences collected from
1995 Mainichi Newspaper Texts (for 337 Expres-
sions)
# of expressions
50 ? # of sentences 187 (55%)
0 < # of sentences < 50 117 (35%)
# of sentences = 0 33 (10%)
2.3 Developing an Example Database
We developed an example database of Japanese
compound functional expressions, which is used
for training/testing a chunker of Japanese com-
pound functional expressions (Tsuchiya et al,
2005). The corpus from which we collect example
sentences is 1995 Mainichi newspaper text corpus
(1,294,794 sentences, 47,355,330 bytes). For each
of the 337 expressions, 50 sentences are collected
and chunk labels are annotated according to the
following procedure.
1. The expression is morphologically analyzed
by ChaSen, and its morpheme sequence6 is
obtained.
2. The corpus is morphologically analyzed by
ChaSen, and 50 sentences which include the
morpheme sequence of the expression are
collected.
3. For each sentence, every occurrence of the
337 expressions is annotated with one of the
usages functional/content by an annotator7.
Table 5 classifies the 337 expressions accord-
ing to the number of sentences collected from the
1995 Mainichi newspaper text corpus. For more
than half of the 337 expressions, more than 50 sen-
tences are collected, although about 10% of the
377 expressions do not appear in the whole cor-
pus. Out of those 187 expressions with more than
50 sentences, 52 are those with balanced distribu-
tion of the functional/content usages in the news-
paper text corpus. Those 52 expressions can be re-
garded as among the most difficult ones in the task
of identifying and classifying functional/content
6For those expressions whose constituent has conjugation
and the conjugated form also has the same usage as the ex-
pression with the original form, the morpheme sequence is
expanded so that the expanded morpheme sequences include
those with conjugated forms.
7For the most frequent 184 expressions, on the average,
the agreement rate between two human annotators is 0.93 and
the Kappa value is 0.73, which means allowing tentative con-
clusions to be drawn (Carletta, 1996; Ng et al, 1999). For
65% of the 184 expressions, the Kappa value is above 0.8,
which means good reliability.
usages. Thus, this paper focuses on those 52 ex-
pressions in the training/testing of chunking com-
pound functional expressions. We extract 2,600
sentences (= 52 expressions ? 50 sentences) from
the whole example database and use them for
training/testing the chunker. The number of the
morphemes for the 2,600 sentences is 92,899. We
ignore the chunk labels for the expressions other
than the 52 expressions, resulting in 2,482/701
chunk labels for the functional/content usages, re-
spectively.
3 Chunking Japanese Compound
Functional Expressions with SVMs
3.1 Support Vector Machines
The principle idea of SVMs is to find a separate
hyperplane that maximizes the margin between
two classes (Vapnik, 1998). If the classes are not
separated by a hyperplane in the original input
space, the samples are transformed in a higher di-
mensional features space.
Giving x is the context (a set of features) of
an input example; xi and yi(i = 1, ..., l, xi ?
Rn, yi?{1,?1}) indicate the context of the train-
ing data and its category, respectively; The deci-
sion function f in SVM framework is defined as:
f(x) = sgn
( l
?
i=1
?iyiK(xi,x) + b
)
(1)
where K is a kernel function, b ? R is a thresh-
old, and ?i are weights. Besides, the weights ?i
satisfy the following constraints:
0 ? ?i ? C (i = 1, ..., l) (2)
?l
i=1 ?iyi = 0 (3)
where C is a misclassification cost. The xi with
non-zero ?i are called support vectors. To train
an SVM is to find the ?i and the b by solving the
optimization problem; maximizing the following
under the constraints of (2) and (3):
L(?) =
l
?
i=1
?i?
1
2
l
?
i,j=1
?i?jyiyjK(x
i
,x
j
) (4)
The kernel function K is used to transform the
samples in a higher dimensional features space.
Among many kinds of kernel functions available,
we focus on the d-th polynomial kernel:
K(x,y) = (x ? y + 1)d (5)
29
Through experimental evaluation on chunking
Japanese compound functional expressions, we
compared polynomial kernels with d = 1, 2, and
3. Kernels with d = 2 and 3 perform best, while
the kernel with d = 3 requires much more compu-
tational cost than that with d = 2. Thus, through-
out the paper, we show results with the quadratic
kernel (d = 2).
3.2 Chunking with SVMs
This section describes details of formalizing the
chunking task using SVMs. In this paper, we use
an SVMs-based chunking tool YamCha8 (Kudo
and Matsumoto, 2001). In the SVMs-based
chunking framework, SVMs are used as classi-
fiers for assigning labels for representing chunks
to each token. In our task of chunking Japanese
compound functional expressions, each sentence
is represented as a sequence of morphemes, where
a morpheme is regarded as a token.
3.2.1 Chunk Representation
For representing proper chunks, we employ
IOB2 representation, one of those which have
been studied well in various chunking tasks of nat-
ural language processing (Tjong Kim Sang, 1999;
Kudo and Matsumoto, 2001). This method uses
the following set of three labels for representing
proper chunks.
I Current token is a middle or the end of a
chunk consisting of more than one token.
O Current token is outside of any chunk.
B Current token is the beginning of a chunk.
As we described in section 2.2, given a candi-
date expression, we classify the usages of the ex-
pression into two classes: functional and content.
Accordingly, we distinguish the chunks of the two
types: the functional type chunk and the content
type chunk. In total, we have the following five la-
bels for representing those chunks: B-functional,
I-functional, B-content, I-content, and O. Ta-
ble 6 gives examples of those chunk labels rep-
resenting chunks.
Finally, as for exending SVMs to multi-class
classifiers, we experimentally compare the pair-
wise method and the one vs. rest method, where
the pairwise method slightly outperformed the one
vs. rest method. Throughout the paper, we show
results with the pairwise method.
8http://chasen.org/?taku/software/
yamcha/
3.2.2 Features
For the feature sets for training/testing of
SVMs, we use the information available in the sur-
rounding context, such as the morphemes, their
parts-of-speech tags, as well as the chunk labels.
More precisely, suppose that we identify the chunk
label ci for the i-th morpheme:
?? Parsing Direction ??
Morpheme m
i?2
m
i?1
m
i
m
i+1
m
i+2
Feature set F
i?2
F
i?1
F
i
F
i+1
F
i+2
at a position
Chunk label c
i?2
c
i?1
c
i
Here, mi is the morpheme appearing at i-th po-
sition, Fi is the feature set at i-th position, and ci
is the chunk label for i-th morpheme. Roughly
speaking, when identifying the chunk label ci for
the i-th morpheme, we use the feature sets Fi?2,
Fi?1, Fi, Fi+1, Fi+2 at the positions i ? 2, i ? 1,
i, i + 1, i + 2, as well as the preceding two chunk
labels ci?2 and ci?1.
The detailed definition of the feature set Fi at i-
th position is given below. The feature set Fi is de-
fined as a tuple of the morpheme feature MF (mi)
of the i-th morpheme mi, the chunk candidate fea-
ture CF (i) at i-th position, and the chunk context
feature OF (i) at i-th position.
Fi = ? MF (mi), CF (i), OF (i) ?
The morpheme feature MF (mi) consists of the
lexical form, part-of-speech, conjugation type and
form, base form, and pronunciation of mi.
The chunk candidate feature CF (i) and the
chunk context feature OF (i) are defined consid-
ering the candidate compound functional expres-
sion, which is a sequence of morphemes includ-
ing the morpheme mi at the current position i. As
we described in section 2, the class of Japanese
compound functional expressions can be regarded
as closed and their number is at most a few thou-
sand. Therefore, it is easy to enumerate all the
compound functional expressions and their mor-
pheme sequences. Chunk labels other than O
should be assigned to a morpheme only when it
constitutes at least one of those enumerated com-
pound functional expressions. Suppose that a se-
quence of morphemes mj . . . mi . . . mk including
mi at the current position i constitutes a candidate
functional expression E as below:
m
j?2
m
j?1
m
j
. . . m
i
. . . m
k
m
k+1
m
k+2
candidate E of
a compound
functional expression
where the morphemes mj?2, mj?1, mk+1, and
mk+2 are at immediate left/right contexts of E.
Then, the chunk candidate feature CF (i) at i-th
position is defined as a tuple of the number of mor-
phemes constituting E and the position of mi in
E. The chunk context feature OF (i) at i-th posi-
tion is defined as a tuple of the morpheme features
30
Table 6: Examples of Chunk Representation and Chunk Candidate/Context Features
(a) Sentence (7) of Table 3
(English Chunk candidate Chunk context
Morpheme translation) Chunk label feature feature
?? (kono) (this) O ? ?
?? (giron) (discussion) O ? ?
? (ga) (NOM) O ? ?
???(owatt) (finish) O ? ?
?? (tara) (after) O ? ?
?? (kyuukei) (break) O ? ?
? (shi) (have) O ? ?
? (te) (may) B-functional ?2, 1? ? MF (?? (kyuukei)), ?, MF (? (shi)), ?,
?? (ii) I-functional ?2, 2? MF (?(period)), ?, ?, ? ?
?(period) (period) O ? ?
(b) Sentence (8) of Table 3
(English Chunk candidate Chunk context
Morpheme translation) Chunk label feature feature
?? (kono) (this) O ? ?
??? (bag) (discussion) O ? ?
? (ha) (TOP) O ? ?
??? (ookiku) (big) O ? ?
? (te) (because) B-content ?2, 1? ? MF (? (ha)), ?, MF (??? (ookiku)), ?,
?? (ii) (nice) I-content ?2, 2? MF (?(period)), ?, ?, ? ?
?(period) (period) O ? ?
as well as the chunk candidate features at immedi-
ate left/right contexts of E.
CF (i) = ? length of E, position of m
i
in E ?
OF (i) = ? MF (m
j?2
), CF (j ? 2),
MF (m
j?1
), CF (j ? 1),
MF (m
k+1
), CF (k + 1),
MF (m
k+2
), CF (k + 2) ?
Table 6 gives examples of chunk candidate fea-
tures and chunk context features
It can happen that the morpheme at the cur-
rent position i constitutes more than one candidate
compound functional expression. For example,
in the example below, the morpheme sequences
mi?1mimi+1, mi?1mi, and mimi+1mi+2 consti-
tute candidate expressions E
1
, E
2
, and E
3
, respec-
tively.
Morpheme sequence m
i?1
m
i
m
i+1
m
i+2
Candidate E
1
m
i?1
m
i
m
i+1
Candidate E
2
m
i?1
m
i
Candidate E
3
m
i
m
i+1
m
i+2
In such cases, we prefer the one starting with the
leftmost morpheme. If more than one candidate
expression starts with the leftmost morpheme, we
prefer the longest one. In the example above, we
prefer the candidate E
1
and construct the chunk
candidate features and chunk context features con-
sidering E
1
only.
4 Experimental Evaluation
The detail of the data set we use in the experimen-
tal evaluation was presented in section 2.3. As we
show in Table 7, performance of our SVMs-based
chunkers as well as several baselines including ex-
isting Japanese text processing tools is evaluated
in terms of precision/recall/F?=1 of identifying
functional chunks. Performance is evaluated also
in terms of accuracy of classifying detected can-
didate expressions into functional/content chunks.
Among those baselines, ?majority ( = functional)?
always assigns functional usage to the detected
candidate expressions. ?Hand-crafted rules? are
manually created 145 rules each of which has con-
ditions on morphemes constituting a compound
functional expression as well as those at immedi-
ate left/right contexts. Performance of our SVMs-
based chunkers is measured through 10-fold cross
validation.
As shown in Table 7, our SVMs-based chunkers
significantly outperform those baselines both in
F?=1 and classification accuracy9. We also evalu-
ate the effectiveness of each feature set, i.e., the
morpheme feature, the chunk candidate feature,
and the chunk context feature. The results in the
table show that the chunker with the chunk candi-
date feature performs almost best even without the
chunk context feature10.
9Recall of existing Japanese text processing tools is low,
because those tools can process only 50?60% of the whole
52 compound functional expressions, and for the remaining
40?50% expressions, they fail in identifying all of the occur-
rences of functional usages.
10It is also worthwhile to note that training the SVMs-
based chunker with the full set of features requires computa-
tional cost three times as much as training without the chunk
31
Table 7: Evaluation Results (%)
Identifying Acc. of classifying
functional chunks functional/content
Prec. Rec. F?=1 chunks
majority ( = functional) 78.0 100 87.6 78.0
Baselines Juman/KNP 89.2 49.3 63.5 55.8
ChaSen/CaboCha 89.0 45.6 60.3 53.2
hand-crafted rules 90.7 81.6 85.9 79.1
SVM morpheme 88.0 91.0 89.4 86.5
(feature morpheme + chunk-candidate 91.0 93.2 92.1 89.0
set) morpheme + chunk-candidate/context 91.1 93.6 92.3 89.2
Figure 1: Change of F?=1 with Different Number
of Training Instances
For the SVMs-based chunker with the chunk
candidate feature with/without the chunk context
feature, Figure 1 plots the change of F?=1 when
training with different number of labeled chunks
as training instances. With this result, the increase
in F?=1 seems to stop with the maximum num-
ber of training instances, which supports the claim
that it is sufficient to collect and manually annotate
about 50 training examples per expression.
5 Concluding Remarks
The Japanese language has various types of com-
pound functional expressions, which are very im-
portant for recognizing the syntactic structures of
Japanese sentences and for understanding their se-
mantic contents. In this paper, we formalized
the task of identifying Japanese compound func-
tional expressions in a text as a chunking prob-
lem. We applied a machine learning technique
to this task, where we employed that of Sup-
port Vector Machines (SVMs). We showed that
the proposed method significantly outperforms ex-
isting Japanese text processing tools. The pro-
context feature.
posed framework has advantages over an approach
based on manually created rules such as the one in
(Shudo et al, 2004), in that it requires human cost
to manually create and maintain those rules. On
the other hand, in our framework based on the ma-
chine learning technique, it is sufficient to collect
and manually annotate about 50 training examples
per expression.
References
J. Carletta. 1996. Assessing agreement on classification
tasks: the Kappa statistic. Computational Linguistics,
22(2):249?254.
Group Jamashii, editor. 1998. Nihongo Bunkei Jiten.
Kuroshio Publisher. (in Japanese).
T. Kudo and Y. Matsumoto. 2001. Chunking with support
vector machines. In Proc. 2nd NAACL, pages 192?199.
J. Mayfield, P. McNamee, and C. Piatko. 2003. Named entity
recognition using hundreds of thousands of features. In
Proc. 7th CoNLL, pages 184?187.
Y. Morita and M. Matsuki. 1989. Nihongo Hyougen Bunkei,
volume 5 of NAFL Sensho. ALC. (in Japanese).
National Language Research Institute. 2001. Gendaigo
Hukugouji Youreishu. (in Japanese).
H. T. Ng, C. Y. Lim, and S. K. Foo. 1999. A case study on
inter-annotator agreement for word sense disambiguation.
In Proc. ACL SIGLEXWorkshop on Standardizing Lexical
Resources, pages 9?13.
I. Sag, T. Baldwin, F. Bond, A. Copestake, and D. Flickinger.
2002. Multiword expressions: A pain in the neck for NLP.
In Proc. 3rd CICLING, pages 1?15.
K. Shudo, T. Tanabe, M. Takahashi, and K. Yoshimura. 2004.
MWEs as non-propositional content indicators. In Proc.
2nd ACL Workshop on Multiword Expressions: Integrat-
ing Processing, pages 32?39.
E. Tjong Kim Sang. 1999. Representing text chunks. In
Proc. 9th EACL, pages 173?179.
M. Tsuchiya, T. Utsuro, S. Matsuyoshi, S. Sato, and S. Nak-
agawa. 2005. A corpus for classifying usages of Japanese
compound functional expressions. In Proc. PACLING,
pages 345?350.
V. N. Vapnik. 1998. Statistical Learning Theory. Wiley-
Interscience.
32
Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 65?72,
Prague, June 2007. c?2007 Association for Computational Linguistics
Learning Dependency Relations of
Japanese Compound Functional Expressions
Takehito Utsuro? and Takao Shime? and Masatoshi Tsuchiya??
Suguru Matsuyoshi?? and Satoshi Sato??
?Graduate School of Systems and Information Engineering, University of Tsukuba,
1-1-1, Tennodai, Tsukuba, 305-8573, JAPAN
?NEC Corporation
??Computer Center, Toyohashi University of Technology,
Tenpaku-cho, Toyohashi, 441?8580, JAPAN
??Graduate School of Engineering, Nagoya University,
Furo-cho, Chikusa-ku, Nagoya, 464?8603, JAPAN
Abstract
This paper proposes an approach of process-
ing Japanese compound functional expressions
by identifying them and analyzing their depen-
dency relations through a machine learning tech-
nique. First, we formalize the task of identify-
ing Japanese compound functional expressions
in a text as a machine learning based chunking
problem. Next, against the results of identify-
ing compound functional expressions, we apply
the method of dependency analysis based on the
cascaded chunking model. The results of ex-
perimental evaluation show that, the dependency
analysis model achieves improvements when ap-
plied after identifying compound functional ex-
pressions, compared with the case where it is ap-
plied without identifying compound functional
expressions.
1 Introduction
In addition to single functional words, the Japanese
language has many more compound functional ex-
pressions which consist of more than one word in-
cluding both content words and functional words.
They are very important for recognizing syntactic
structures of Japanese sentences and for understand-
ing their semantic content. Recognition and under-
standing of them are also very important for vari-
ous kinds of NLP applications such as dialogue sys-
tems, machine translation, and question answering.
However, recognition and semantic interpretation of
compound functional expressions are especially dif-
ficult because it often happens that one compound
expression may have both a literal (i.e. compo-
sitional) content word usage and a non-literal (i.e.
non-compositional) functional usage.
For example, Table 1 shows two example sen-
tences of a compound expression ?? (ni) ???
(tsuite)?, which consists of a post-positional particle
?? (ni)?, and a conjugated form ???? (tsuite)? of
a verb ??? (tsuku)?. In the sentence (A), the com-
pound expression functions as a case-marking parti-
cle and has a non-compositional functional meaning
?about?. On the other hand, in the sentence (B), the
expression simply corresponds to a literal concate-
nation of the usages of the constituents: the post-
positional particle ?? (ni)? and the verb ????
(tsuite)?, and has a content word meaning ?follow?.
Therefore, when considering machine translation of
these Japanese sentences into English, it is neces-
sary to judge precisely the usage of the compound
expression ?? (ni)??? (tsuite)?, as shown in the
English translation of the two sentences in Table 1.
There exist widely-used Japanese text processing
tools, i.e. combinations of a morphological analy-
sis tool and a subsequent parsing tool, such as JU-
MAN1+ KNP2 and ChaSen3+ CaboCha4. However,
they process those compound expressions only par-
tially, in that their morphological analysis dictionar-
ies list only a limited number of compound expres-
sions. Furthermore, even if certain expressions are
listed in a morphological analysis dictionary, those
existing tools often fail in resolving the ambigui-
1http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/juman-e.html
2http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp-e.html
3http://chasen.naist.jp/hiki/ChaSen/
4http://chasen.org/?taku/software/
cabocha/
65
? (watashi) ? (ha) ? (kare) ? (ni)??? (tsuite) ??? (hanashita)
(A) (I) (TOP) (he) (about) (talked)
(I talked about him.)
? (watashi) ? (ha) ? (kare) ? (ni) ??? (tsuite) ??? (hashitta)
(B) (I) (TOP) (he) (ACC) (follow) (ran)
(I ran following him.)
Table 1: Translation Selection of a Japanese Compound Expression ?? (ni)??? (tsuite)?
Correct English Translation:
( As a means of solving the problem, USA recommended the activity of OSCE in which Russia participates.)
(1) Correct Dependency Relation by Identifying Compound Functional Expression: ??????
with a Case Marking Particle Usage.
(2)  Incorrect Dependency Relation without Identifying Compound Functional Expression: ??????,
which Literally Consists of a Post-positional Particle ??? (with) and a Conjugation Form ????
of a Verb ???? (do).
??? ???? ? ??? ??? ? ?? ?? ?? ????????? ??? ?????
USA-TOP as a means for solution       Russia-NOM also             participate in                                of  OSCE activity-ACC          recommended
??? ???? ? ??? ??? ? ?? ?? ?? ????????? ??? ?????
USA-TOP with a means for Russia-NOM also             participate in                            of  OSCE activity-ACC       recommended
solution
Figure 1: Example of Improving Dependency Analysis of Compound Functional Expressions by Identifying
them before Dependency Analysis
ties of their usages, such as those in Table 1. This
is mainly because the framework of these existing
tools is not designed so as to resolve such ambigu-
ities of compound (possibly functional) expressions
by carefully considering the context of those expres-
sions.
Actually, as we introduce in the next section, as a
first step towards studying computational processing
of compound functional expressions, we start with
125 major functional expressions which have non-
compositional usages, as well as their variants (337
expressions in total). Out of those 337 expressions,
111 have both a content word usage and a functional
usage. However, the combination of JUMAN+KNP
is capable of distinguishing the two usages only for
43 of the 111 expressions, and the combination of
ChaSen+CaboCha only for 40 of those 111 expres-
sions. Furthermore, the failure in distinguishing the
two usages may cause errors of syntactic analysis.
For example, (1) of Figure 1 gives an example of
identifying a correct modifiee of the second bunsetsu
segment 5 ???????? (as a means for solu-
tion)? including a Japanese compound functional ex-
pression ???? (as)?, by appropriately detecting
the compound functional expression before depen-
dency analysis. On the other hand, (2) of Figure 1
gives an example of incorrectly indicating an erro-
neous modifiee of the third bunsetsu ????, which
actually happens if we do not identify the compound
functional expression ???? (as)? before depen-
dency analysis of this sentence.
Considering such a situation, it is necessary to
develop a tool which properly recognizes and se-
mantically interprets Japanese compound functional
expressions. This paper proposes an approach of
processing Japanese compound functional expres-
sions by identifying them and analyzing their de-
pendency relations through a machine learning tech-
nique. The overall flow of processing compound
functional expressions in a Japanese sentence is il-
5A Japanese bunsetsu segment is a phrasal unit which con-
sits of at least one content word and zero or more functional
words.
66
( As a means of solving the 
problem, USA recommended the 
activity of OSCE in which Russia 
participates.)
???????????
????????????
??????????
?????
??
(solution)
??
(means)
?
(with)
?
(do)
?
(and)
? ? ?
? ? ?
??
(solution)
??
(means)
???
(as)
? ? ?
? ? ?
??
(solution)
??
(means)
???
(as)
? ? ?
? ? ?
morphological 
analysis
by ChaSen
??
(solution)
??
(means)
?
(with)
?
(do)
?
(and)
? ? ?
? ? ?
compound
functional 
expression
Identifying
compound
functional
expression
chunking
bunsetsu
segmentation
&
dependency
analysis
bunsetsu
segment
dependency
relation
Figure 2: Overall Flow of Processing Compound Functional Expressions in a Japanese Sentence
lustrated in Figure 2. First of all, we assume a
sequence of morphemes obtained by a variant of
ChaSen with all the compound functional expres-
sions removed from its outputs, as an input to our
procedure of identifying compound functional ex-
pressions and analyzing their dependency relations.
We formalize the task of identifying Japanese com-
pound functional expressions in a text as a machine
learning based chunking problem (Tsuchiya et al,
2006). We employ the technique of Support Vec-
tor Machines (SVMs) (Vapnik, 1998) as the ma-
chine learning technique, which has been success-
fully applied to various natural language process-
ing tasks including chunking tasks such as phrase
chunking and named entity chunking. Next, against
the results of identifying compound functional ex-
pressions, we apply the method of dependency anal-
ysis based on the cascaded chunking model (Kudo
and Matsumoto, 2002), which is simple and efficient
because it parses a sentence deterministically only
deciding whether the current bunsetsu segment mod-
ifies the one on its immediate right hand side. As
we showed in Figure 1, identifying compound func-
tional expressions before analyzing dependencies in
a sentence does actually help deciding dependency
relations of compound functional expressions.
In the experimental evaluation, we focus on 59
expressions having balanced distribution of their us-
ages in the newspaper text corpus and are among the
most difficult ones in terms of their identification in
a text. We first show that the proposed method of
chunking compound functional expressions signifi-
cantly outperforms existing Japanese text processing
tools. Next, we further show that the dependency
analysis model of (Kudo and Matsumoto, 2002) ap-
plied to the results of identifying compound func-
tional expressions significantly outperforms the one
applied to the results without identifying compound
functional expressions.
2 Japanese Compound Functional
Expressions
There exist several collections which list Japanese
functional expressions and examine their usages.
For example, (Morita and Matsuki, 1989) exam-
ine 450 functional expressions and (Group Jamashii,
1998) also lists 965 expressions and their example
sentences. Compared with those two collections,
Gendaigo Hukugouji Youreishu (National Language
Research Institute, 2001) (henceforth, denoted as
GHY) concentrates on 125 major functional expres-
sions which have non-compositional usages, as well
as their variants6, and collects example sentences of
those expressions. As we mentioned in the previous
section, as a first step towards developing a tool for
identifying Japanese compound functional expres-
sions, we start with those 125 major functional ex-
pressions and their variants (337 expressions in to-
6For each of those 125 major expressions, the differences
between it and its variants are summarized as below: i) inser-
tion/deletion/alternation of certain particles, ii) alternation of
synonymous words, iii) normal/honorific/conversational forms,
iv) base/adnominal/negative forms.
67
(a) Classification of Compound Functional Expressions based on Grammatical Function
Grammatical Function Type # of major expressions # of variants Example
post-positional conjunctive particle 36 67 ??? (kuse-ni)
particle type case-marking particle 45 121 ??? (to-shite)
adnominal particle 2 3 ??? (to-iu)
auxiliary verb type 42 146 ??? (te-ii)
total 125 337 ?
(b) Examples of Classifying Functional/Content Usages
Expression Example sentence (English translation) Usage
(1) ??? ??????? ??? ???????????????? functional
(kuse-ni) (To my brother, (someone) gave money, while (he/she) did noth-
ing to me but just sent a letter.)
(??? (kuse-ni) = while)
(2) ??? ???? ??? ??????? content
(kuse-ni) (They all were surprised by his habit.) (???? (kuse-ni)
= by one?s habit
(3) ??? ?????????? ??? ??????? functional
(to-shite) (He is known as an expert of the problem.) (???? (to-shite)
= as ?)
(4) ??? ?????????????? ??? ???? content
(to-shite) (Please make it clear whether this is true or not.) (?? ???? (to-shite)
= make ? ?
(5) ??? ??????? ??? ???????? functional
(to-iu) (I heard that he is alive.) (???? (to-iu) = that ?)
(6) ??? ?????????? ??? ????? content
(to-iu) (Somebody says ?Please visit us.?.) (???? (to-iu)
= say (that) ?)
(7) ??? ???????????? ??? ? functional
(te-ii) (You may have a break after we finish this discussion.) (???? (te-ii) = may ?)
(8) ??? ????????? ??? ? content
(te-ii) (This bag is nice because it is big.) (???? (te-ii)
= nice because ?)
Table 2: Classification and Example Usages of Compound Functional Expressions
tal). In this paper, following (Sag et al, 2002), we
regard each variant as a fixed expression, rather than
a semi-fixed expression or a syntactically-flexible
expression 7. Then, we focus on evaluating the
effectiveness of straightforwardly applying a stan-
dard chunking technique to the task of identifying
Japanese compound functional expressions.
As in Table 2 (a), according to their grammat-
ical functions, those 337 expressions in total are
roughly classified into post-positional particle type,
and auxiliary verb type. Functional expressions of
post-positional particle type are further classified
into three subtypes: i) conjunctive particle types,
which are used for constructing subordinate clauses,
ii) case-marking particle types, iii) adnominal parti-
cle types, which are used for constructing adnominal
7Compound functional expressions of auxiliary verb types
can be regarded as syntactically-flexible expressions.
clauses. Furthermore, for examples of compound
functional expressions listed in Table 2 (a), Table 2
(b) gives their example sentences as well as the de-
scription of their usages.
3 Identifying Compound Functional
Expressions by Chunking with SVMs
This section describes summaries of formalizing the
chunking task using SVMs (Tsuchiya et al, 2006).
In this paper, we use an SVMs-based chunking tool
YamCha8 (Kudo and Matsumoto, 2001). In the
SVMs-based chunking framework, SVMs are used
as classifiers for assigning labels for representing
chunks to each token. In our task of chunking
Japanese compound functional expressions, each
8http://chasen.org/?taku/software/
yamcha/
68
sentence is represented as a sequence of morphemes,
where a morpheme is regarded as a token.
3.1 Chunk Representation
For representing proper chunks, we employ IOB2
representation, which has been studied well in var-
ious chunking tasks of natural language processing.
This method uses the following set of three labels
for representing proper chunks.
I Current token is a middle or the end of a
chunk consisting of more than one token.
O Current token is outside of any chunk.
B Current token is the beginning of a chunk.
Given a candidate expression, we classify the us-
ages of the expression into two classes: functional
and content. Accordingly, we distinguish the chunks
of the two types: the functional type chunk and the
content type chunk. In total, we have the follow-
ing five labels for representing those chunks: B-
functional, I-functional, B-content, I-content, and
O. Finally, as for extending SVMs to multi-class
classifiers, we experimentally compare the pairwise
method and the one vs. rest method, where the pair-
wise method slightly outperformed the one vs. rest
method. Throughout the paper, we show results with
the pairwise method.
3.2 Features
For the feature sets for training/testing of SVMs, we
use the information available in the surrounding con-
text, such as the morphemes, their parts-of-speech
tags, as well as the chunk labels. More precisely,
suppose that we identify the chunk label c
i
for the
i-th morpheme:
?? Parsing Direction ??
Morpheme m
i?2
m
i?1
m
i
m
i+1
m
i+2
Feature set F
i?2
F
i?1
F
i
F
i+1
F
i+2
at a position
Chunk label c
i?2
c
i?1
c
i
Here, m
i
is the morpheme appearing at i-th posi-
tion, F
i
is the feature set at i-th position, and c
i
is
the chunk label for i-th morpheme. Roughly speak-
ing, when identifying the chunk label c
i
for the i-th
morpheme, we use the feature sets F
i?2
, F
i?1
, F
i
,
F
i+1
, F
i+2
at the positions i ? 2, i ? 1, i, i + 1,
i+2, as well as the preceding two chunk labels c
i?2
and c
i?1
. The detailed definition of the feature set
F
i
at i-th position is given in (Tsuchiya et al, 2006),
which mainly consists of morphemes as well as in-
formation on the candidate compound functional ex-
pression at i-th position.
4 Learning Dependency Relations of
Japanese Compound Functional
Expressions
4.1 Japanese Dependency Analysis using
Cascaded Chunking
4.1.1 Cascaded Chunking Model
First of all, we define a Japanese sen-
tence as a sequence of bunsetsu segments
B = ?b
1
, b
2
, . . . , b
m
? and its syntactic struc-
ture as a sequence of dependency patterns
D = ?Dep(1), Dep(2), . . . , Dep(m ? 1)?, where
Dep(i) = j means that the bunsetsu segment b
i
depends on (modifies) bunsetsu segment b
j
. In
this framework, we assume that the dependency
sequence D satisfies the following two constraints:
1. Japanese is a head-final language. Thus, except
for the rightmost one, each bunsetsu segment
modifies exactly one bunsetsu segment among
those appearing to its right.
2. Dependencies do not cross one another.
Unlike probabilistic dependency analysis models
of Japanese, the cascaded chunking model of Kudo
and Matsumoto (2002) does not require the proba-
bilities of dependencies and parses a sentence de-
terministically. Since Japanese is a head-final lan-
guage, and the chunking can be regarded as the cre-
ation of a dependency between two bunsetsu seg-
ments, this model simplifies the process of Japanese
dependency analysis as follows: 9
1. Put an O tag on all bunsetsu segments. The O
tag indicates that the dependency relation of the
current segment is undecided.
2. For each bunsetsu segment with an O tag, de-
cide whether it modifies the bunsetsu segment
on its immediate right hand side. If so, the O
tag is replaced with a D tag.
3. Delete all bunsetsu segments with a D tag that
immediately follows a bunsetsu segment with
an O tag.
9The O and D tags used in this section have no relation to
those chunk reppresentation tags introduced in section 3.1.
69
Initialization
?? ??? ??? ??? ?????
( He was moved by her warm heart. )
He her warm heart be moved
Input:
Tag:
?? ??? ??? ??? ?????
O O O O O
Input:
Tag:
?? ??? ??? ??? ?????
O O D D O
Deleted
Input:
Tag:
?? ??? ??? ?????
O D D O
Deleted
Input:
Tag:
?? ??? ?????
O D O
Input:
Tag:
?? ?????
O
Deleted
Input:
Tag:
?????
O
Finish
D
Deleted
Figure 3: Example of the Parsing Process with Cas-
caded Chunking Model
4. Terminate the algorithm if a single bunsetsu
segment remains, otherwise return to the step
2 and repeat.
Figure 3 shows an example of the parsing process
with the cascaded chunking model.
4.1.2 Features
As a Japanese dependency analyzer based on the
cascaded chunking model, we use the publicly avail-
able version of CaboCha (Kudo and Matsumoto,
2002), which is trained with the manually parsed
sentences of Kyoto text corpus (Kurohashi and Na-
gao, 1998), that are 38,400 sentences selected from
the 1995 Mainichi newspaper text.
The standard feature set used by CaboCha con-
sists of static features and dynamic features. Static
features are those solely defined once the pair
of modifier/modifiee bunsetsu segments is speci-
fied. For the pair of modifier/modifiee bunsetsu
segments, the following are used as static fea-
tures: head words and their parts-of-speech tags,
inflection-types/forms, functional words and their
parts-of-speech tags, inflection-types/forms, inflec-
tion forms of the words that appear at the end
of bunsetsu segments. As for features between
modifier/modifiee bunsetsu segments, the distance
of modifier/modifiee bunsetsu segments, existence
of case-particles, brackets, quotation-marks, and
punctuation-marks are used as static features. On the
other hand, dynamic features are created during the
parsing process, so that, when a certain dependency
relation is determined, it can have some influence
on other dependency relations. Dynamic features in-
clude bunsetsu segments modifying the current can-
didate modifiee (see Kudo and Matsumoto (2002)
for the details).
4.2 Coping with Compound Functional
Expressions
As we show in Figure 2, a compound functional ex-
pression is identified as a sequence of several mor-
phemes and then chunked into one morpheme. The
result of this identification process is then trans-
formed into the sequence of bunsetsu segments. Fi-
nally, to this modified sequence of bunsetsu seg-
ments, the method of dependency analysis based on
the cascaded chunking model is applied.
Here, when chunking a sequence of several mor-
phemes constituting a compound functional expres-
sion, the following two cases may exist:
(A) As in the case of the example (A) in Table 1, the
two morphemes constituting a compound func-
tional expression ?? (ni)??? (tsuite)? over-
laps the boundary of two bunsetsu segments.
In such a case, when chunking the two mor-
phemes into one morpheme corresponding to
a compound functional expression, those two
bunsetsu segments are concatenated into one
bunsetsu segment.
? ?
kare ni
(he)
???
tsuite
=?
? ????
kare ni-tsuite
(he) (about)
(B) As we show below, a compound functional ex-
pression ??? (koto)? (ga)?? (aru)? over-
laps the boundary of two bunsetsu segments,
though the two bunsetsu segments concatenat-
ing into one bunsetsu segment does include no
content words. In such a case, its immedi-
ate left bunsetsu segment (???(itt)? (ta)? in
the example below), which corresponds to the
content word part of ??? (koto)? (ga)??
(aru)?, has to be concatenated into the bunsetsu
segment ??? (koto)? (ga)?? (aru)?.
70
?? ?
itt ta
(went)
?? ?
koto ga
??
aru
=?
?? ? ?????
itt ta koto-ga-aru
(have been ?)
Next, to the compound functional expression, we
assign one of the four grammatical function types
listed in Table 2 as its POS tag. For example,
the compound functional expression ?? (ni)???
(tsuite)? in (A) above is assigned the grammatical
function type ?case-marking particle type?, while ?
?? (koto) ? (ga) ?? (aru)? in (B) is assigned
?auxiliary verb type?.
These modifications cause differences in the final
feature representations. For example, let us compare
the feature representations of the modifier bunsetsu
segments in (1) and (2) of Figure 1. In (1), the mod-
ifier bunsetsu segment is ????????? which
has the compound functional expression ?????
in its functional word part. On the other hand, in
(2), the modifier bunsetsu segment is ????, which
corresponds to the literal verb usage of a part of the
compound functional expression ?????. In the
final feature representations below, this causes the
following differences in head words and functional
words / POS of the modifier bunsetsu segments:
(1) of Figure 1 (2) of Figure 1
head word ?? (means) ?? (do)
functional word ??? (as) ? (and)
POS subsequent to nominal conjunctive
/ modifying predicate particle
5 Experimental Evaluation
5.1 Training/Test Data Sets
For the training of chunking compound functional
expressions, we collected 2,429 example sentences
from the 1995 Mainichi newspaper text corpus. For
each of the 59 compound functional expressions for
evaluation mentioned in section 1, at least 50 ex-
amples are included in this training set. For the
testing of chunking compound functional expres-
sions, as well as training/testing of learning depen-
dencies of compound functional expressions, we
used manually-parsed sentences of Kyoto text cor-
pus (Kurohashi and Nagao, 1998), that are 38,400
sentences selected from the 1995 Mainichi newspa-
per text (the 2,429 sentences above are selected so
that they are exclusive of the 37,400 sentences of
Kyoto text corpus.). To those data sets, we manually
annotate usage labels of the 59 compound functional
expressions (details in Table 3).
Usages # of
functional content total sentences
for chunker
training 1918 1165 3083 2429
Kyoto text corpus 5744 1959 7703 38400
Table 3: Statistics of Data Sets
Identifying
functional chunks
Acc. of
classifying
functional /
content
Prec. Rec. F
?=1
chunks
majority ( = functional) 74.6 100 85.5 74.6
Juman/KNP 85.8 40.5 55.0 58.4
ChaSen/CaboCha 85.2 26.7 40.6 51.1
SVM 91.4 94.6 92.9 89.3
Table 4: Evaluation Results of Chunking (%)
5.2 Chunking
As we show in Table 4, performance of our SVMs-
based chunkers as well as several baselines includ-
ing existing Japanese text processing tools is evalu-
ated in terms of precision/recall/F
?=1
of identifying
all the 5,744 functional chunks included in the test
data (Kyoto text corpus in Table 3). Performance is
evaluated also in terms of accuracy of classifying de-
tected candidate expressions into functional/content
chunks. Among those baselines, ?majority ( = func-
tional)? always assigns functional usage to the de-
tected candidate expressions. Performance of our
SVMs-based chunkers is measured through 10-fold
cross validation. Our SVMs-based chunker signif-
icantly outperforms those baselines both in F
?=1
and classification accuracy. As we mentioned in
section 1, existing Japanese text processing tools
process compound functional expressions only par-
tially, which causes damage in recall in Table 4.
5.3 Analyzing Dependency Relations
We evaluate the accuracies of judging dependency
relations of compound functional expressions by the
variant of CaboCha trained with Kyoto text cor-
pus annotated with usage labels of compound func-
tional expressions. This performance is measured
through 10-fold cross validation with the modified
version of the Kyoto text corpus. In the evaluation
phase, according to the flow of Figure 2, first we ap-
ply the chunker of compound functional expressions
trained with all the 2,429 sentences in Table 3 and
obtain the results of chunked compound functional
expressions with about 90% correct rate. Then, bun-
setsu segmentation and dependency analysis are per-
71
modifier modifiee
baselines CaboCha (w/o FE) 72.5 88.0
CaboCha (public) 73.9 87.6
chunker + CaboCha (proposed) 74.0 88.0
reference + CaboCha (proposed) 74.4 88.1
Table 5: Accuracies of Identifying Modi-
fier(s)/Modifiee (%)
formed by our variant of CaboCha, where accu-
racies of identifying modifier(s)/modifiee of com-
pound functional expressions are measured as in Ta-
ble 5 (?chunker + CaboCha (proposed)? denotes that
inputs to CaboCha (proposed) are with 90% correct
rate, while ?reference + CaboCha (proposed)? de-
notes that they are with 100% correct rate). Here,
?CaboCha (w/o FE)? denotes a baseline variant of
CaboCha, with all the compound functional expres-
sions removed from its inputs (which are outputs
from ChaSen), while ?CaoboCha (public)? denotes
the publicly available version of CaboCha, which
have some portion of the compound functional ex-
pressions included in its inputs.
For the modifier accuracy, the difference of
?chunker + CaboCha (proposed)? and ?CaboCha
(w/o FE)? is statistically significant at a level of
0.05. Identifying compound functional expressions
typically contributes to improvements when the lit-
eral constituents of a compound functional expres-
sion include a verb. In such a case, for bunsetsu
segments which usually modifies a verb, an incor-
rect modifee candidate is removed, which results in
improvements in the modifier accuracy. The dif-
ference between ?CaoboCha (public)? and ?chunker
+ CaboCha (proposed)? is slight because the pub-
licly available version of CaboCha seems to include
compound functional expressions which are dam-
aged in identifying their modifiers with ?CaboCha
(w/o FE)?. For the modifiee accuracy, the difference
of ?chunker + CaboCha (proposed)? and ?CaboCha
(w/o FE)? is zero. Here, more than 100 instances of
improvements like the one in Figure 1 are observed,
while almost the same number of additional fail-
ures are also observed mainly because of the sparse-
ness problem. Furthermore, in the case of the modi-
fiee accuracy, it is somehow difficult to expect im-
provement because identifying modifiees of func-
tional/content bunsetsu segments mostly depends on
features other than functional/content distinction.
6 Concluding Remarks
We proposed an approach of processing Japanese
compound functional expressions by identifying
them and analyzing their dependency relations
through a machine learning technique. This ap-
proach is novel in that it has never been applied
to any language so far. Experimental evaluation
showed that the dependency analysis model applied
to the results of identifying compound functional ex-
pressions significantly outperforms the one applied
to the results without identifying compound func-
tional expressions. The proposed framework has ad-
vantages over an approach based on manually cre-
ated rules such as the one in (Shudo et al, 2004), in
that it requires human cost to create manually and
maintain those rules. Related works include Nivre
and Nilsson (2004), which reports improvement of
Swedish parsing when multi word units are manu-
ally annotated.
References
Group Jamashii, editor. 1998. Nihongo Bunkei Jiten. Kuroshio
Publisher. (in Japanese).
T. Kudo and Y. Matsumoto. 2001. Chunking with support vec-
tor machines. In Proc. 2nd NAACL, pages 192?199.
T. Kudo and Y. Matsumoto. 2002. Japanese dependency ana-
lyisis using cascaded chunking. In Proc. 6th CoNLL, pages
63?69.
S. Kurohashi and M. Nagao. 1998. Building a Japanese parsed
corpus while improving the parsing system. In Proc. 1st
LREC, pages 719?724.
Y. Morita and M. Matsuki. 1989. Nihongo Hyougen Bunkei,
volume 5 of NAFL Sensho. ALC. (in Japanese).
National Language Research Institute. 2001. Gendaigo Huku-
gouji Youreishu. (in Japanese).
J. Nivre and J. Nilsson. 2004. Multiword units in syntactic
parsing. In Proc. LRECWorkshop, Methodologies and Eval-
uation of Multiword Units in Real-World Applications, pages
39?46.
I. Sag, T. Baldwin, F. Bond, A. Copestake, and D. Flickinger.
2002. Multiword expressions: A pain in the neck for NLP.
In Proc. 3rd CICLING, pages 1?15.
K. Shudo, T. Tanabe, M. Takahashi, and K. Yoshimura. 2004.
MWEs as non-propositional content indicators. In Proc. 2nd
ACL Workshop on Multiword Expressions: Integrating Pro-
cessing, pages 32?39.
M. Tsuchiya, T. Shime, T. Takagi, T. Utsuro, K. Uchimoto,
S. Matsuyoshi, S. Sato, and S. Nakagawa. 2006. Chunk-
ing Japanese compound functional expressions by machine
learning. In Proc. Workshop on Multi-Word-Expressions in
a Multilingual Context, pages 25?32.
V. N. Vapnik. 1998. Statistical Learning Theory. Wiley-
Interscience.
72
