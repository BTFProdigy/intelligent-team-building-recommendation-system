Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1128?1136,
Beijing, August 2010
Efficient Statement Identification for Automatic Market Forecasting
Henning Wachsmuth
Universita?t Paderborn
Software Quality Lab
hwachsmuth@slab.upb.de
Peter Prettenhofer and Benno Stein
Bauhaus-Universita?t Weimar
Web Technology & Information Systems
benno.stein@uni-weimar.de
Abstract
Strategic business decision making in-
volves the analysis of market forecasts.
Today, the identification and aggregation
of relevant market statements is done by
human experts, often by analyzing doc-
uments from the World Wide Web. We
present an efficient information extrac-
tion chain to automate this complex nat-
ural language processing task and show
results for the identification part. Based
on time and money extraction, we iden-
tify sentences that represent statements on
revenue using support vector classifica-
tion. We provide a corpus with German
online news articles, in which more than
2,000 such sentences are annotated by do-
main experts from the industry. On the
test data, our statement identification al-
gorithm achieves an overall precision and
recall of 0.86 and 0.87 respectively.
1 Introduction
Touch screen market to hit $9B by 2015. 50 sup-
pliers provide multi-touch screens, and that num-
ber is likely to rise.1
Strategic business decision making is a highly
complex process that requires experience as well
as an overall view of economics, politics, and
technological developments. Clearly, for the time
being this process cannot be done by a computer at
the level of a human expert. However, important
tasks may be automated such as market forecast-
ing, which relies on identifying and aggregating
relevant information from the World Wide Web
(Berekoven et. al., 2001). An analyst who inter-
prets the respective data can get a reasonable idea
about the future market volume, for example. The
1Adapted from http://industry.bnet.com.
problem is that a manually conducted Web search
is time-consuming and usually far from being ex-
haustive. With our research we seek to develop
an efficient system that finds and analyzes market
forecast information with retrieval, extraction and
natural language processing (NLP) techniques.
We contribute to the following situation. For a
given product, technology, or industry sector we
identify and aggregate statements on its market
development found on relevant websites. In par-
ticular, we extract time information (?by 2015?)
and money information (?$9B?) and use support
vector classification to identify sentences that rep-
resent market statements. The statements? sub-
jects (?touch screen?) are found by relating recog-
nized named entities to the time and money infor-
mation, which we then normalize and aggregate.
In this paper we report on results for the statement
identification. To the best of our knowledge no
data for the investigation of such market analysis
tasks has been made publicly available until now.
We provide such a corpus with statements on rev-
enue annotated in news articles from the Web; the
corpus was created in close collaboration with our
industry partner Resolto Informatik GmbH.
We pursue two objectives, namely, to support
human experts with respect to the effectiveness
and completeness of their analysis, and to estab-
lish a technological basis upon which more intri-
cate analysis tasks can be automated. To summa-
rize, the main contributions of this paper are:
1. We show how to decompose the identifi-
cation and aggregation of forecasts into re-
trieval, extraction, and normalization tasks.
2. We introduce a manually annotated German
corpus for computational linguistics research
on market information.
3. We offer empirical evidence that classifica-
tion and extraction techniques can be com-
1128
bined to precisely identify statements on rev-
enue.
1.1 Related Work
Stein et. al. (2005) were among the first to con-
sider information extraction for automatic mar-
ket forecasting. Unlike us, the authors put much
emphasis on retrieval aspects and applied depen-
dency grammar parsing to identify market state-
ments. As a consequence their approach suffers
from the limitation to a small number of prede-
fined sentence structures.
While we obtain market forecasts by extract-
ing expert statements from the Web, related ap-
proaches derive them from past market behavior
and quantitative news data. Koppel and Shtrim-
berg (2004) studied the effect of news on finan-
cial markets. Lavrenko et al (2000) used time-
series analysis and language models to predict
stock market prices and, similarly, Lerman et al
(2008) proposed a system for forecasting public
opinion based on concurrent modeling of news ar-
ticles and market history. Another related field is
opinion mining in the sense that it relies on the ag-
gregation of individual statements. Glance et al
(2005) inferred marketing intelligence from opin-
ions in online discussions. Liu et al (2007) exam-
ined the effect of Weblogs on box office revenues
and combined time-series with sentiment analysis
to predict the sales performance of movies.
The mentioned approaches are intended to re-
flect or to predict present developments and,
therefore, primarily help for operative decision
making. In contrast, we aim at predicting long-
term market developments, which are essential for
strategic decision making.
2 The Problem
Market forecasts depend on two parameters, the
topic of interest and the criterion to look at. A
topic is either an organization or a market. Under
a market we unite branches, products, and tech-
nologies, because the distinction between these is
not clear in general (e.g., for semiconductors). In
contrast, we define a criterion to be a metric at-
tribute that can be measured over time. Here we
are interested in financial criteria such as revenue,
profit, and the like. The ambitious overall task that
we want to solve is as follows:
Task description: Given a topic ? and a finan-
cial criterion ?, find information for ? on the de-
velopment of ?. Aggregate the found values on ?
with respect to time.
We omit the limitation to forecasts because we
could miss useful information otherwise:
(1) In 2008, the Egyptian automobile industry
achieved US$ 9.96bn in sales.
(2) Egypt?s automotive sales will rise by 97%
from 2008 to 2013.
Both sentences have the same topic. In Particu-
lar, the 2008 amount of money from example (1)
can be aggregated with the forecast in (2) to infer
the predicted amount in 2013.
As in these examples, market information can
often only be found in running text; the major
source for this is the Web. Thus, we seek to
find web pages with sentences that represent state-
ments on a financial criterion ? and to make
these statements processable. Conceptually, such
a statement is a 5-tuple S? = (S, g, T,M, td),
where S is the topical subject, which may have a
geographic scope g, T is a period of time, M con-
sists of a growth rate and/or an amount of money
to be achieved during T with respect to ?, and td
is the statement time, i.e., the point in time when
the statement was made.
3 Approach
Our goal is to find and aggregate statements on
a criterion ? for a topic ? . In close collaboration
with two companies from the semantic technology
field, we identified eight high-level subtasks in the
overall process as explained in the following. An
overview is given in Table 1.
3.1 Find Candidate Documents
To find web pages that are likely to contain state-
ments on ? and ? , we propose to perform a meta-
search by starting from a set of characteristic
terms of the domain and then using query expan-
sion techniques such as local context analysis (Xu
and Croft, 2000). As Stein et. al. (2005) describe,
1129
Subtask Applied technologies
1 Find candidate documents meta-search, query expansion, genre analysis
2 Preprocess content content extraction, sentence splitting, tokenization, POS tagging and chunking
3 Extract entities time and money extraction, named entity recognition of organizations and markets
4 Identify statements statistical classification based on lexical and distance features
5 Determine statement type relation extraction based on dependency parse trees, matching of word lists
6 Fill statement templates template filling, anaphora resolution, matching of word lists
7 Normalize values time and money normalization, coreference resolution
8 Aggregate information chronological merging and averaging, inference from subtopic to topic
Table 1: Subtasks of the identification and aggregation of market statements for a specified topic.
Experiments in this paper cover the subtasks written in black.
a genre analysis, which classifies a document with
respect to its form, style, and targeted audience,
may be deployed afterwards to further improve
the quality of the result list efficiently. In this way,
we only maintain candidate documents that look
promising on the surface.
3.2 Preprocess Content
Preprocessing is needed for accurate access to the
document text. Our overall task incorporates re-
lating information from different document areas,
so mixing up a web page?s main frame and side-
bars should be avoided. We choose Document
Slope Curve (DSC) for content detection, which
looks for plateaus in the HTML tag distribution.
Gottron (2007) has offered evidence that DSC
is currently the best algorithm in terms of pre-
cision. Afterwards, the sentences are split with
rules that consider the specific characteristics of
reports, press releases and the like, such as head-
lines between short paragraphs. In succeeding
subtasks, tokens as well as their Part-of-Speech
and chunk tags are also used, but we see no point
in not relying on standard algorithms here.
3.3 Extract Entities
The key to identify a statement S? on a finan-
cial criterion ? is the extraction of temporal and
monetary entities. Recent works report that sta-
tistical approaches to this task can compete with
hand-crafted rules (Ahn et. al., 2005; Cramer et.
al., 2007). In the financial domain, however, the
focus is only on dates and periods as time infor-
mation, along with currency numbers, currency
terms, or fractions as money information. We
found that with regular expressions, which rep-
resent the complex but finite structures of such
phrases, we can achieve nearly perfect recall in
recognition (see Section 5).
We apply named entity recognition (NER) of
organizations and markets in this stage, too, so we
can relate statements to the appropriate subjects,
later on. Note that market names do not follow a
unique naming scheme, but we observed that they
often involve similar phrase patterns that can be
exploited as features. NER is usually done by se-
quence labeling, and we use heuristic beam search
due to our effort to design a highly efficient overall
system. Ratinov and Roth (2009) have shown for
the CoNLL-2003 shared task that Greedy decod-
ing (i.e., beam search of width 1) is competitive
to the widely used Viterbi algorithm while being
over 100 times faster at the same time.
3.4 Identify Statements
Based on time and money information, sentences
that represent a statement S? can be identified.
Such a sentence gives us valuable hints on which
temporal and monetary entity stick together and
how to interpret them in relation. Additionally,
it serves as evidence for the statement?s correct-
ness (or incorrectness). Every sentence with at
least one temporal and one monetary entity is a
candidate. Criteria such as revenue usually imply
small core vocabularies Lpos, which indicate that
a sentence is on that criterion or which often ap-
pear close to it. On the contrary, there are sets of
words Lneg that suggest a different criterion. For
a given text collection with known statements on
?, both Lpos and Lneg can be found by computing
the most discriminant terms with respect to ?. A
reasonable first approach is then to filter sentences
1130
that contain terms from Lpos and lack terms from
Lneg, but problems arise when terms from differ-
ent vocabularies co-occur or statements on differ-
ent criteria are attached to one another.
Instead, we propose a statistical learning ap-
proach. Support Vector Machines (SVMs) have
been proven to yield very good performance in
both general classification and sentence extraction
while being immune to overfitting (Steinwart and
Christmann, 2008; Hirao et. al., 2001). For our
candidates, we compute lexical and distance fea-
tures based on Lpos, Lneg, and the time and money
information. Then we let an SVM use these fea-
tures to distinguish between sentences with state-
ments on ? and others. At least for online news
articles, this works reasonably well as we demon-
strate in Section 5. Note that classification is not
used to match the right entities, but to filter the
small set of sentences on ?.
3.5 Determine Statement Type
The statement type implies what information we
can process. If a sentence contains more than one
temporal or monetary entity, we need to relate the
correct T and M to each S?, now. The type of S?
then depends on the available money information,
its trend and the time direction.
We consider four types of money information.
? refers to a period of time that results in a new
amount A of money in contrast to its preceding
amount Ap. The difference between A and Ap
may be specified as an incremental amount ?A
or as a relative growth rate r. M can span any
combination of A, Ap, ?A and r, and at least A
and r constitute a reasonable entity on their own.
Sometimes the trend of r (i.e. decreasing or in-
creasing) cannot be derived from the given val-
ues. However, this information can mostly be ob-
tained from a nearby indicator word (e.g. ?plus? or
?decreased?) and, therefore, we address this prob-
lem with appropriate word lists. Once the trend is
known, any two types imply the others.
Though we are predominantly interested in
forecasts, statements also often represent a decla-
ration on achieved results. This distinction is es-
sential and can be based on time-directional indi-
cators (e.g. ?next?) and the tense of leading verbs.
For this, we test both feature and kernel methods
on dependency parse trees, thereby determining T
and M at the same time. We only parse the iden-
tified sentences, though. Hence, we avoid running
into efficiency problems.
3.6 Fill Statement Templates
The remaining subtasks are ongoing work, so we
only present basic concepts here.
Besides T and M , the subject S and the state-
ment time td have to be determined. S may be
found within the previously extracted named enti-
ties using the dependency parse tree from Section
3.5 or by anaphora resolution. Possible limitations
to a geographic scope g can be recognized with
word lists. In market analysis, the approximate
td suffices, and for most news articles td is simi-
lar to their release date. Thus, if no date is in the
parse tree, we search the extracted temporal enti-
ties for the release date, which is often mentioned
at the beginning or end of the document?s content.
We fill one template (S, g, T,M, td) for each S?
where we have at least S, T , and M .
3.7 Normalize Values
Since we base the extraction on regular expres-
sions, we can normalize most monetary entities
with a predefined set of rules. Section 3.5 implies
that M? = (A?, r?) is a reasonable normalized
form where A? is A specified in million US-$ and
r? is r as percentage with a fixed number of deci-
mals.2 Time normalization is more complex. Any
period should be transformed to T ? = (t?s, t?e)
consisting of the start date t?s and end date t?e .
Following Ahn et. al. (2005), we consider fully
qualified, deictic and anaphoric periods. While
normalization of fully qualified periods like ?from
Apr to Jun 1999? is straightforward, deictic (e.g.
?since 2005?, ?next year?) and anaphoric men-
tions (e.g. ?in the reported time?) require a refer-
ence time. Approaches to resolve such references
rely on dates or fully qualified periods in the pre-
ceding text (Saquete et. al., 2003; Mani and Wil-
son, 2000).3
2Translating the currency requires exchange rates at state-
ment time. We need access to such information or omit the
translation if only one currency is relevant.
3References to fiscal years even involve a whole search
problem if no look-up table on such data is available.
1131
without interference
with interference
0
2
4
A
A?p
A?
Ap
t 0
2
4
t
0
2
4
A
A?Ap=A?p
t 0
2
4
t
mill.US-$ mill.US-$
mill.US-$ mill.US-$
Figure 1: Example for merging monetary values.
9
10
11
A?
A??
A
t
mill.US-$
-10%
0%
10%
t
r
Figure 2: Example for the inference of relative in-
formation from absolute values.
If we cannot normalize M or T , we discard the
corresponding statement templates. For the oth-
ers, we have to resolve synonymous co-references
(e.g. ?Loewe AG? and ?Loewe?) before we can
proceed to the last step.
3.8 Aggregate Information
We can aggregate the normalized values in either
two or three dimensions depending on whether
to separate statements with respect to td. Aggre-
gation then incorporates two challenges, namely,
how to merge values and how to infer information
on a topic from values of a subtopic.
We say that two statements on the same topic
? and criterion ? interfere if the contained peri-
ods of time intersect and the according monetary
values do not coincide. In case of declarations,
this means that we extracted incorrect values or
extracted values incorrectly. For forecasts, on the
contrary, we are exactly onto such information.
In both cases, an intuitive solution is to compute
the average (or median) and deviations. Figure 1
graphically illustrates such merging. The subtopic
challenge is based on the assumption that a mean-
ingful number of statements on a certain subtopic
of ? implies relative information on ? , as shown in
Figure 2. One of the most interesting relations are
organizations as subtopics of markets they pro-
duce for, because it is quite usual to search for
Statements Total Forecasts Declarations
Complete corpus 2075 523 (25.2%) 1552 (74.8%)
Training set 1366 306 (22.4%) 1060 (77.6%)
Validation set 362 113 (31.2%) 249 (68.8%)
Test set 347 104 (30.0%) 243 (70.0%)
Table 2: Statements on revenue in the corpus.
information on a market, but only receive state-
ments on companies. Approaches to this relation
may rely e.g. on the web page co-occurrence and
term frequencies of the markets and companies.
Altogether, we return the aggregated values
linked to the sentences in which we found them.
In this way, we make the results verifiable and,
thereby, compensate for possible inaccuracies.
4 Corpus
To evaluate the given and related tasks, we built
a manually annotated corpus with online news ar-
ticles on the revenues of organizations and mar-
kets. The compilation aims at being representa-
tive for target documents, a search engine returns
to queries on revenue. The purpose of the corpus
is to investigate both the structure of sentences on
financial criteria and the distribution of associated
information over the text.
The corpus consists of 1,128 German news ar-
ticles from the years 2003 to 2009, which were
taken from 29 news websites like www.spiegel.de
or www.capital.de. The content of each document
comes as unicode plain text with appended URL
for access to the HTML source code. Annotations
are given in a standard XMI file preformatted for
the Unstructured Information Management Archi-
tecture (Ferrucci and Lally, 2004). We created a
split, in which 2/3 of the documents constitute the
training set and each 1/6 refers to the validation
and test set. To simulate real conditions, the train-
ing documents were randomly chosen from only
the seven most represented websites, while the
validation and test data both cover all 29 sources.
Table 2 shows some corpus statistics, which give
a hint that the validation and test set differ sig-
nificantly from the training set. The corpus is
free for scientific use and can be downloaded at
http://infexba.upb.de.
1132
Loewe AG: Vorla?ufige Neun-Monats-Zahlen
Kronach, [6. November 2007]REF ? Das Ergebnis vor
Zinsen und Steuern (EBIT) des Loewe Konzerns konnte
in den ersten 9 Monaten 2007 um 41% gesteigert wer-
den. Vor diesem Hintergrund hebt die [Loewe AG]ORG
ihre EBIT-Prognose fu?r das laufende Gescha?ftsjahr auf
20 Mio. Euro an. Beim Umsatz strebt Konzernchef
[Rainer Hecker]AUTH [fu?r das Gesamtjahr]TIME ein
ho?her als urspru?nglich geplantes [Wachstum]TREND
[von 10% auf ca. 380 Mio. Euro]MONEY an. (...)
Figure 3: An annotated document in the corpus.
The text is taken from www.boerse-online.de, but
has been modified for clarification.
4.1 Annotations
In each document, every sentence that includes a
temporal entity T and a monetary entity M and
that represents a forecast or declaration on the
revenue of an organization or market is marked
as such. T and M are annotated themselves and
linked to the sentence. Accordingly, the subject
is tagged (and linked) within the sentence bound-
aries if available, otherwise its last mention in the
preceding text. The same holds for optional en-
tities, namely a reference time, a trend indicator
and the author of a statement. Altogether, 2,075
statements are tagged in this way. As in Figure
3, only information that refers to a statement on
revenue (typed in bold face) is annotated. These
annotations may be spread across the text.
The source documents were manually selected
and prepared by our industrial partners, and two
of their employees annotated the plain document
text. With respect to the statement annotations,
a preceding pilot study yielded substantial inter-
annotator agreement, as indicated by the value
? = 0.79 of the conservative measure Cohen?s
Kappa (Carletta, 1996). Additionally, we per-
formed a manual correction process for each an-
notated document to improve consistency.
5 Experiments
We now present experiments for the statement
identification, which were conducted on our cor-
pus. The goal was to evaluate whether our com-
bined extraction and classification approach suc-
ceeds in the precise identification of sentences that
comprise a statement on revenue, while keeping
recall high. Only exact matches of the annotated
text spans were considered to be correct identifi-
cations. Unlike in Section 3, we only worked on
plain text, though.
5.1 Experimental Setup
To find candidate sentences, we implemented a
sentence splitter that can handle article elements
such as subheadings, URLs, or bracketed sen-
tences. We then constructed sophisticated, but
efficient regular expressions for time and money.
They do not represent correct language, in gen-
eral, but model the structure of temporal and mon-
etary entities, and use word lists provided by do-
main experts on the lowest level.4 For feature
computation, we assumed that the closest pair of
temporal and monetary entity refers to the enclos-
ing candidate sentence.5 Since only positive in-
stances IP of statements on revenue are annotated
in our corpus, we declared all candidates, which
have no counterpart in the annotated data, to con-
stitute the negative class IN , and balanced IP and
IN by ?randomly? (seed 42) removing instances
from IN .6
For the vocabularies Lpos = {P1, P2} we first
counted the frequencies of all words in the unbal-
anced sets IP and IN . From these, we deleted
named entities, numbers and adjectives. If the pre-
fix (e.g. ?Umsatz?) of a word (?Umsatzplus?) oc-
curred, we only kept the prefix. We then filtered
all terms that appeared in at least 1.25% of the in-
stances in IP and more than 3.5 times as much in
IP as in IN . The remaining words were manually
partitioned into two lists:
P1 = {umgesetzt, Umsatz, Umsa?tze, setzte} (all
of these are terms for revenue)
P2 = {Billionen, meldet, Mitarbeiter, Verband}
(trillions, announce, employee, association)
Lneg = {N1, N2} was built accordingly. In ad-
dition, we set up a list G1 with genitive pronouns
4More details are given at http://infexba.upb.de.
555% of the candidate sentences in the training set con-
tain more than one temporal and/or monetary entity, so this
assumption may lead to errors.
6We both tested undersampling and oversampling tech-
niques but saw no effective differences in the results.
1133
and determiners. Based on Lpos, Lneg and G1,
we computed the following 43 features for every
candidate sentence s:
? 1-8: Number of terms from P1 (N1) in s as
well as in the two preceding sentences and in
the following sentence.
? 9-10: Number of terms from P2 (N2) in s.
? 11: Occurrence of term from G1 next to the
monetary entity.
? 12-19: Forward (backward) distance in to-
kens between the monetary (temporal) entity
in s and a term from P1 (N1).
? 20-27: Forward (backward) distance in num-
ber of symbols from O1 = {?.?,???,?!?} be-
tween the monetary (temporal) entity in s
and a term from P1 (N1).
? 28-43: Same as 20-27 for O2 = {?:?,?;?} and
O3 = {?,?}, respectively.
We trained a linear SVM with cost parameter
C = 0.3 (selected during validation) on these fea-
tures using the Weka integration of LibSVM (Hall
et. al., 2009; Fan et. al., 2001). Further features
were evaluated, e.g. occurrences of contraposi-
tions or comparisons, but they did not improve the
classifier. Instead, we noticed that we can avoid
some complex cases when we apply two rules af-
ter entity extraction:
R1: Delete temporal and monetary entities that
are directly surrounded by brackets.
R2: Delete temporal entities that contain the
word ?Vorjahr? (?preceding year?).
Now, we evaluated the following five statement
identification algorithms:
? Na??ve: Simply return all candidate sentences
(to estimate the relative frequency of state-
ments on revenue in the corpus).
? Baseline: Return all candidate sentences that
contain a term from the list P1.
? NEG: Use the results from Baseline. Return
all sentences that lack terms from N1.
Recall Training Validation Test
Sentences 0.98 0.98 0.96
Temporal entities 0.97 (0.95) 0.97 (0.94) 0.98 (0.96)
Monetary entities 0.96 (0.96) 0.96 (0.96) 0.95 (0.94)
Table 3: Recall of sentence and entity extraction.
In brackets: Recall after applying R1 and R2.
? RB: Filter candidates using R1 and R2. Then
apply NEG.
? SVM: Filter candidates using R1 and R2.
Then classify sentences with the SVM.
5.2 Results
Table 3 shows that we found at least 95% of the
sentences, time and money information, which re-
fer to a statement on revenue, in all datasets.7 We
could not measure precision for these since not all
sentences and entities are annotated in the corpus,
as mentioned in Section 4.
Results for the statement identification are
given in Figure 4. Generally, the test values are
somewhat lower than the validation values, but
analog in distribution. Nearly all statements were
recognized by the Na??ve algorithm, but only with
a precision of 0.35. In contrast, both for Baseline
and NEG already around 80% of the found state-
ments were correct. The latter paid a small gain in
precision with a significant loss in recall. While
RB and SVM both achieved 86% precision on the
test set, SVM tends to be a little more precise as
suggested by the validation results. In terms of re-
call, SVM clearly outperformed RB with values
of 89% and 87% and was only a little worse than
the Baseline. Altogether, the F1-Measure values
show that SVM was the best performing algorithm
in our evaluation.
5.3 Error Analysis
To assess the influence of the sentence, time and
money extraction, we compared precision and re-
call of the classifier on the manually annotated and
the extracted data, respectively. Table 4 shows
7We intentionally did not search for unusual entities like
?am 1. Handelstag nach dem Erntedankfest? (?the 1st trading
day after Thanksgiving?) in order not to develop techniques
that are tailored to individual cases. Also, money amounts
that lack a currency term were not recognized.
1134
0,75
0,80
0,85
0,90
0,95
1,00
TestValidation
N
a?
ve
N
a?
ve
B
as
el
in
e
B
as
el
in
e
N
E
G
N
E
G
R
B
R
BSV
M
SV
M
Recall
.89
.87
.83.83
0,3
0,4
0,5
0,6
0,7
0,8
0,9
TestValidation
N
a?
ve
N
a?
ve
B
as
el
in
e
B
as
el
in
e
N
E
G
N
E
G
R
B
R
BSV
M
SV
M
Precision
.89 .90
.86 .86
0,5
0,6
0,7
0,8
0,9
TestValidation
N
a?
ve
N
a?
ve
B
as
el
in
e
B
as
el
in
e
N
E
G
N
E
G
R
B
R
BSV
M
SV
M
F1-Measure
.89
.86
.84
.86
.79 .77
.92
.89
.85
.83
Figure 4: Precision, recall and F1-Measure of the five evaluated statement identification algorithms.
SVM is best in precision both on validation and test data and outperforms RB in recall significantly.
that only recall differs significantly. We found that
false statement identifications referred to the fol-
lowing noteworthy error cases.
False match: Most false positives result from
matchings of temporal and monetary entities that
actually do not refer to the same statement.
Missing criterion: Some texts describe the de-
velopment of revenue without ever mentioning
revenue. Surrogate words like ?market? may be
used, but they are not discriminative enough.
Multiple criteria: Though we aimed at dis-
carding sentences, in which revenue is mentioned
without comprising a statement on it, in some
cases our features did not work out, mainly due
to intricate sentence structure.
Traps: Some sentences contain numeric values
on revenue, but not the ones looked for, as in ?10%
of the revenue?. We tackled these cases, but had
still some false classifications left.
Hidden boundaries: Finally, we did not find
all correct sentence boundaries, which can lead to
both false positives and false negatives. The pre-
dominant problem was to separate headlines from
paragraph beginnings and is partly caused by the
missing access to markup tags.
5.4 Efficiency
We ran the identification algorithm on the whole
corpus using a 2 GHz Intel Core 2 Duo MacBook
with 4 GB RAM. The 1,128 corpus documents
contain 33,370 sentences as counted by our algo-
rithm itself. Tokenization, sentence splitting, time
and money extraction took only 55.2 seconds, i.e.,
more than 20 documents or 600 sentences each
second. Since our feature computation is not op-
timized yet, the complete identification process is
a little less efficient with 7.35 documents or 218
Candidates Data Precision Recall
Annotated validation data 0.91 0.94
test data 0.87 0.93
Extracted validation data 0.90 0.89
test data 0.86 0.87
Table 4: Precision and recall of the statement
identification on manually annotated data and on
automatically extracted data, respectively.
sentences per second. However, it is fast enough
to be used in online applications, which was our
goal in the end.
6 Conclusion
We presented a multi-stage approach for the au-
tomatic identification and aggregation of market
statements and introduced a manually annotated
German corpus for related tasks. The approach
has been influenced by industry and is oriented
towards practical applications, but is, in general,
not specific to the German language. It relies on
efficient retrieval, extraction and NLP techniques.
By now, we can precisely identify most sentences
that represent statements on revenue. This already
allows for the support of strategists, e.g. by high-
lighting such sentences in web pages, which we
currently implement as a Firefox extension. The
overall problem is complex, though, and we are
aware that human experts can do better at present.
Nevertheless, time-consuming tasks can be auto-
mated and, in this respect, the results on our cor-
pus are very promising.
Acknowledgement: This work was funded by
the project ?InfexBA? of the German Federal Min-
istry of Education and Research (BMBF) under
contract number 01IS08007A.
1135
References
Ahn, David, Sisay F. Adafre, and Maarten de Rijke.
2005. Extracting Temporal Information from Open
Domain Text: A Comparative Exploration. Journal
of Digital Information Management, 3(1): 14?20.
Berekoven, Ludwig, Werner Eckert, and Peter El-
lenrieder. 2001. Marktforschung: Methodische
Grundlagen und praktische Anwendung, 9th Edi-
tion, Gabler, Wiesbaden, Germany.
Carletta, Jean. 1996. Assessing Agreement on Classi-
fication Tasks: The Kappa Statistic. Computational
Linguistics, 22: 249?254.
Cramer, Irene M., Stefan Schacht, and Andreas
Merkel. 2007. Classifying Number Expressions in
German Corpora. In Proceedings of the 31st An-
nual Conference of the German Classification Soci-
ety on Data Analysis, Machine Learning, and Appli-
cations, pages 553?560.
Fan, Rong-En, Pai-Hsuen Chen, and Chih-Jen Lin.
2001. Working Set Selection Using Second Order
Information for Training Support Vector Machines.
Journal of Machine Learning Research, 6: 1889?
1918.
Ferrucci, David and Adam Lally. 2004. UIMA:
An Architectural Approach to Unstructured Infor-
mation Processing in the Corporate Research Envi-
ronment. Natural Language Engineering, 10(3?4):
pages 327?348.
Glance, Natalie, Matthew Hurst, Kamal Nigam,
Matthew Siegler, Robert Stockton, and Takashi
Tomokiyo. 2005. Deriving Marketing Intelligence
from Online Discussion. In Proceedings of the
Eleventh International Conference on Knowledge
Discovery in Data Mining, pages 419?428.
Gottron, Thomas. 2007. Evaluating Content Extrac-
tion on HTML Documents. In Proceedings of the
2nd International Conference on Internet Technolo-
gies and Applications, pages 123?132.
Hall, Mark, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Up-
date. SIGKDD Explorations, 11(1).
Hirao, Tsutomu, Hideki Isozaki, Eisaku Maeda and
Yuji Matsumoto. 2002. Extracting Important Sen-
tences with Support Vector Machines. In Proceed-
ings of the 19th International Conference on Com-
putational linguistics, pages 342?348.
Koppel, Moshe and Itai Shtrimberg. 2004. Good
News or Bad News? Let the Market Decide. In Pro-
ceedings of the AAAI Spring Symposium on Explor-
ing Attitude and Affect in Text: Theories and Appli-
cations, pages 86?88.
Lavrenko, Victor, Matt Schmill, Dawn Lawrie, Paul
Ogilvie, David Jensen, and James Allan. 2000.
Mining of Concurrent Text and Time Series. In
Proceedings of the 6th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and
Data Mining Workshop on Text Mining, pages 37?
44.
Lerman, Kevin, Ari Gilder, Mark Dredze, and Fer-
nando Pereira. 2008. Reading the Markets: Fore-
casting Public Opinion of Political Candidates by
News Analysis. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics,
pages 473?480.
Liu, Yang, Xiangji Huang, Aijun An, and Xiaohui Yu.
2007. Arsa: A Sentiment-Aware Model for Predict-
ing Sales Performance Using Blogs. In Proceedings
of the 30th Annual International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval, pages 607?614.
Mani, Inderjeet and George Wilson. 2000. Ro-
bust Temporal Processing of News. In Proceedings
of the 38th Annual Meeting of the Association for
Computational Linguistics, pages 69?76.
Ratinov, Lev and Dan Roth. 2009. Design Chal-
lenges and Misconceptions in Named Entity Recog-
nition. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learn-
ing, pages 147?155.
Saquete, Estela, Rafael Mun?oz, and Patricio Mart??nez-
Barco. 2003. TERSEO: Temporal Expression Res-
olution System Applied to Event Ordering. Text,
Speech and Dialogue, Springer, Berlin / Heidelberg,
Germany, pages 220?228.
Stein, Benno, Sven Meyer zu Eissen, Gernot Gra?fe,
and Frank Wissbrock. 2005. Automating Market
Forecast Summarization from Internet Data. Fourth
International Conference on WWW/Internet, pages
395?402.
Steinwart, Ingo and Andreas Christmann. 2008. Sup-
port Vector Machines, Springer, New York, NY.
Xu, Jinxi and Bruce W. Croft 2000. Improving the ef-
fectiveness of information retrieval with local con-
text analysis. ACM Transactions on Information
Systems, 18(1): 79-112.
1136
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1118?1127,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Cross-Language Text Classification
using Structural Correspondence Learning
Peter Prettenhofer and Benno Stein
Bauhaus-Universita?t Weimar
D-99421 Weimar, Germany
{peter.prettenhofer,benno.stein}@uni-weimar.de
Abstract
We present a new approach to cross-
language text classification that builds on
structural correspondence learning, a re-
cently proposed theory for domain adap-
tation. The approach uses unlabeled doc-
uments, along with a simple word trans-
lation oracle, in order to induce task-
specific, cross-lingual word correspon-
dences. We report on analyses that reveal
quantitative insights about the use of un-
labeled data and the complexity of inter-
language correspondence modeling.
We conduct experiments in the field
of cross-language sentiment classification,
employing English as source language,
and German, French, and Japanese as tar-
get languages. The results are convincing;
they demonstrate both the robustness and
the competitiveness of the presented ideas.
1 Introduction
This paper deals with cross-language text classifi-
cation problems. The solution of such problems
requires the transfer of classification knowledge
between two languages. Stated precisely: We are
given a text classification task ? in a target lan-
guage T for which no labeled documents are avail-
able. ? may be a spam filtering task, a topic cate-
gorization task, or a sentiment classification task.
In addition, we are given labeled documents for
the identical task in a different source language S.
Such type of cross-language text classification
problems are addressed by constructing a clas-
sifier fS with training documents written in S
and by applying fS to unlabeled documents writ-
ten in T . For the application of fS under lan-
guage T different approaches are current practice:
machine translation of unlabeled documents from
T to S, dictionary-based translation of unlabeled
documents from T to S , or language-independent
concept modeling by means of comparable cor-
pora. The mentioned approaches have their pros
and cons, some of which are discussed below.
Here we propose a different approach to cross-
language text classification which adopts ideas
from the field of multi-task learning (Ando and
Zhang, 2005a). Our approach builds upon struc-
tural correspondence learning, SCL, a recently
proposed theory for domain adaptation in the
field of natural language processing (Blitzer et al,
2006).
Similar to SCL, our approach induces corre-
spondences among the words from both languages
by means of a small number of so-called pivots. In
our context a pivot is a pair of words, {wS , wT },
from the source language S and the target lan-
guage T , which possess a similar semantics. Test-
ing the occurrence of wS or wT in a set of unla-
beled documents from S and T yields two equiv-
alence classes across these languages: one class
contains the documents where eitherwS orwT oc-
cur, the other class contains the documents where
neither wS nor wT occur. Ideally, a pivot splits
the set of unlabeled documents with respect to the
semantics that is associated with {wS , wT }. The
correlation between wS or wT and other words w,
w 6? {wS , wT } is modeled by a linear classifier,
which then is used as a language-independent pre-
dictor for the two equivalence classes. As we will
see, a small number of pivots can capture a suffi-
ciently large part of the correspondences between
S and T in order to (1) construct a cross-lingual
representation and (2) learn a classifier fST for the
task ? that operates on this representation. Several
advantages follow from our approach:
? Task specificity. The approach exploits the
words? pragmatics since it considers?during
the pivot selection step?task-specific char-
acteristics of language use.
1118
? Efficiency in terms of linguistic resources.
The approach uses unlabeled documents
from both languages along with a small num-
ber (100 - 500) of translated words, instead
of employing a parallel corpus or an exten-
sive bilingual dictionary.
? Efficiency in terms of computing resources.
The approach solves the classification prob-
lem directly, instead of resorting to a more
general and potentially much harder problem
such as machine translation. Note that the use
of such technology is prohibited in certain sit-
uations (market competitors) or restricted by
environmental constraints (offline situations,
high latency, bandwidth capacity).
Contributions Our contributions to the outlined
field are threefold: First, the identification and uti-
lization of the theory of SCL to cross-language
text classification, which has, to the best of our
knowledge, not been investigated before. Sec-
ond, the further development and adaptation of
SCL towards a technology that is competitive with
the state-of-the-art in cross-language text classifi-
cation. Third, an in-depth analysis with respect
to important hyperparameters such as the ratio
of labeled and unlabeled documents, the number
of pivots, and the optimum dimensionality of the
cross-lingual representation. In this connection we
compile extensive corpora in the languages En-
glish, German, French, and Japanese, and for dif-
ferent sentiment classification tasks.
The paper is organized as follows: Section 2
surveys related work. Section 3 states the termi-
nology for cross-language text classification. Sec-
tion 4 describes our main contribution, a new ap-
proach to cross-language text classification based
on structural correspondence learning. Section 5
presents experimental results in the context of
cross-language sentiment classification.
2 Related Work
Cross-Language Text Classification Bel et al
(2003) belong to the first who explicitly consid-
ered the problem of cross-language text classi-
fication. Their research, however, is predated
by work in cross-language information retrieval,
CLIR, where similar problems are addressed
(Oard, 1998). Traditional approaches to cross-
language text classification and CLIR use linguis-
tic resources such as bilingual dictionaries or par-
allel corpora to induce correspondences between
two languages (Lavrenko et al, 2002; Olsson et
al., 2005). Dumais et al (1997) is considered as
seminal work in CLIR: they propose a method
which induces semantic correspondences between
two languages by performing latent semantic anal-
ysis, LSA, on a parallel corpus. Li and Taylor
(2007) improve upon this method by employing
kernel canonical correlation analysis, CCA, in-
stead of LSA. The major limitation of these ap-
proaches is their computational complexity and,
in particular, the dependence on a parallel cor-
pus, which is hard to obtain?especially for less
resource-rich languages. Gliozzo and Strappar-
ava (2005) circumvent the dependence on a par-
allel corpus by using so-called multilingual do-
main models, which can be acquired from com-
parable corpora in an unsupervised manner. In
(Gliozzo and Strapparava, 2006) they show for
particular tasks that their approach can achieve a
performance close to that of monolingual text clas-
sification.
Recent work in cross-language text classifica-
tion focuses on the use of automatic machine
translation technology. Most of these methods in-
volve two steps: (1) translation of the documents
into the source or the target language, and (2) di-
mensionality reduction or semi-supervised learn-
ing to reduce the noise introduced by the ma-
chine translation. Methods which follow this two-
step approach include the EM-based approach by
Rigutini et al (2005), the CCA approach by For-
tuna and Shawe-Taylor (2005), the information
bottleneck approach by Ling et al (2008), and the
co-training approach by Wan (2009).
Domain Adaptation Domain adaptation refers
to the problem of adapting a statistical classifier
trained on data from one (or more) source domains
(e.g., newswire texts) to a different target domain
(e.g., legal texts). In the basic domain adaptation
setting we are given labeled data from the source
domain and unlabeled data from the target domain,
and the goal is to train a classifier for the target
domain. Beyond this setting one can further dis-
tinguish whether a small amount of labeled data
from the target domain is available (Daume, 2007;
Finkel and Manning, 2009) or not (Blitzer et al,
2006; Jiang and Zhai, 2007). The latter setting is
referred to as unsupervised domain adaptation.
1119
Note that, cross-language text classification
can be cast as an unsupervised domain adapta-
tion problem by considering each language as a
separate domain. Blitzer et al (2006) propose
an effective algorithm for unsupervised domain
adaptation, called structural correspondence learn-
ing. First, SCL identifies features that general-
ize across domains, which the authors call pivots.
SCL then models the correlation between the piv-
ots and all other features by training linear clas-
sifiers on the unlabeled data from both domains.
This information is used to induce correspon-
dences among features from the different domains
and to learn a shared representation that is mean-
ingful across both domains. SCL is related to the
structural learning paradigm introduced by Ando
and Zhang (2005a). The basic idea of structural
learning is to constrain the hypothesis space of a
learning task by considering multiple different but
related tasks on the same input space. Ando and
Zhang (2005b) present a semi-supervised learning
method based on this paradigm, which generates
related tasks from unlabeled data. Quattoni et al
(2007) apply structural learning to image classifi-
cation in settings where little labeled data is given.
3 Cross-Language Text Classification
This section introduces basic models and termi-
nology.
In standard text classification, a document d
is represented under the bag-of-words model as
|V |-dimensional feature vector x ? X , where V ,
the vocabulary, denotes an ordered set of words,
xi ? x denotes the normalized frequency of word
i in d, and X is an inner product space. DS
denotes the training set and comprises tuples of
the form (x, y), which associate a feature vector
x ? X with a class label y ? Y . The goal is to
find a classifier f : X ? Y that predicts the la-
bels of new, previously unseen documents. With-
out loss of generality we restrict ourselves to bi-
nary classification problems and linear classifiers,
i.e., Y = {+1, -1} and f(x) = sign(wTx). w is a
weight vector that parameterizes the classifier, [?]T
denotes the matrix transpose. The computation of
w from DS is referred to as model estimation or
training. A common choice for w is given by a
vector w? that minimizes the regularized training
error:
w? = argmin
w?R|V |
?
(x,y)?DS
L(y, wTx) +
?
2
?w?2 (1)
L is a loss function that measures the quality
of the classifier, ? is a non-negative regulariza-
tion parameter that penalizes model complexity,
and ?w?2 = wTw. Different choices for L entail
different classifier types; e.g., when choosing the
hinge loss function for L one obtains the popular
Support Vector Machine classifier (Zhang, 2004).
Standard text classification distinguishes be-
tween labeled (training) documents and unlabeled
(test) documents. Cross-language text classifica-
tion poses an extra constraint in that training doc-
uments and test documents are written in different
languages. Here, the language of the training doc-
uments is referred to as source language S, and
the language of the test documents is referred to as
target language T . The vocabulary V divides into
VS and VT , called vocabulary of the source lan-
guage and vocabulary of the target language, with
VS ? VT = ?. I.e., documents from the training
set and the test set map on two non-overlapping
regions of the feature space. Thus, a linear classi-
fier fS trained on DS associates non-zero weights
only with words from VS , which in turn means that
fS cannot be used to classify documents written
in T .
One way to overcome this ?feature barrier? is
to find a cross-lingual representation for docu-
ments written in S and T , which enables the trans-
fer of classification knowledge between the two
languages. Intuitively, one can understand such
a cross-lingual representation as a concept space
that underlies both languages. In the following,
we will use ? to denote a map that associates the
original |V |-dimensional representation of a doc-
ument d written in S or T with its cross-lingual
representation. Once such a mapping is found the
cross-language text classification problem reduces
to a standard classification problem in the cross-
lingual space. Note that the existing methods for
cross-language text classification can be character-
ized by the way ? is constructed. For instance,
cross-language latent semantic indexing (Dumais
et al, 1997) and cross-language explicit semantic
analysis (Potthast et al, 2008) estimate ? using a
parallel corpus. Other methods use linguistic re-
sources such as a bilingual dictionary to obtain ?
(Bel et al, 2003; Olsson et al, 2005).
1120
4 Cross-Language
Structural Correspondence Learning
We now present a novel method for learning a
map ? by exploiting relations from unlabeled doc-
uments written in S and T . The proposed method,
which we call cross-language structural corre-
spondence learning, CL-SCL, addresses the fol-
lowing learning setup (see also Figure 1):
? Given a set of labeled training documentsDS
written in language S, the goal is to create a
text classifier for documents written in a dif-
ferent language T . We refer to this classifi-
cation task as the target task. An example for
the target task is the determination of senti-
ment polarity, either positive or negative, of
book reviews written in German (T ) given a
set of training reviews written in English (S).
? In addition to the labeled training docu-
ments DS we have access to unlabeled doc-
uments DS,u and DT ,u from both languages
S and T . Let Du denote DS,u ?DT ,u.
? Finally, we are given a budget of calls to a
word translation oracle (e.g., a domain ex-
pert) to map words in the source vocabu-
lary VS to their corresponding translations in
the target vocabulary VT . For simplicity and
without loss of applicability we assume here
that the word translation oracle maps each
word in VS to exactly one word in VT .
CL-SCL comprises three steps: In the first step,
CL-SCL selects word pairs {wS , wT }, called piv-
ots, where wS ? VS and wT ? VT . Pivots have to
satisfy the following conditions:
Confidence Both words, wS and wT , are predic-
tive for the target task.
Support Both words, wS and wT , occur fre-
quently in DS,u and DT ,u respectively.
The confidence condition ensures that, in the
second step of CL-SCL, only those correlations
are modeled that are useful for discriminative
learning. The support condition, on the other
hand, ensures that these correlations can be es-
timated accurately. Considering our sentiment
classification example, the word pair {excellentS ,
exzellentT } satisfies both conditions: (1) the
words are strong indicators of positive sentiment,
Words in V
S
Class
label
term frequencies
Negative class label
Positive class label
Words in V
T
... , x|V|)x = (x1 , ...
D
S
D
S,u
D
T,u
Du
No value
y
Figure 1: The document sets underlying CL-SCL.
The subscripts S , T , and u designate ?source lan-
guage?, ?target language?, and ?unlabeled?.
and (2) the words occur frequently in book reviews
from both languages. Note that the support of wS
andwT can be determined from the unlabeled data
Du. The confidence, however, can only be deter-
mined for wS since the setting gives us access to
labeled data from S only.
We use the following heuristic to form an or-
dered set P of pivots: First, we choose a subset
VP from the source vocabulary VS , |VP |  |VS |,
which contains those words with the highest mu-
tual information with respect to the class label of
the target task in DS . Second, for each word
wS ? VP we find its translation in the target vo-
cabulary VT by querying the translation oracle; we
refer to the resulting set of word pairs as the can-
didate pivots, P ? :
P ? = {{wS , TRANSLATE(wS)} | wS ? VP }
We then enforce the support condition by elim-
inating in P ? all candidate pivots {wS , wT } where
the document frequency of wS in DS,u or of wT
in DT ,u is smaller than some threshold ?:
P = CANDIDATEELIMINATION(P ?, ?)
Let m denote |P |, the number of pivots.
In the second step, CL-SCL models the corre-
lations between each pivot {wS , wT } ? P and all
other words w ? V \ {wS , wT }. This is done by
training linear classifiers that predict whether or
not wS or wT occur in a document, based on the
other words. For this purpose a training set Dl is
created for each pivot pl ? P :
Dl = {(MASK(x, pl), IN(x, pl)) | x ? Du}
1121
MASK(x, pl) is a function that returns a copy of
x where the components associated with the two
words in pl are set to zero?which is equivalent
to removing these words from the feature space.
IN(x, pl) returns +1 if one of the components of x
associated with the words in pl is non-zero and -1
otherwise. For each Dl a linear classifier, charac-
terized by the parameter vector wl, is trained by
minimizing Equation (1) on Dl. Note that each
training set Dl contains documents from both lan-
guages. Thus, for a pivot pl = {wS , wT } the vec-
tor wl captures both the correlation between wS
and VS \ {wS} and the correlation between wT
and VT \ {wT }.
In the third step, CL-SCL identifies correlations
across pivots by computing the singular value de-
composition of the |V |?m-dimensional parameter
matrix W, W =
[
w1 . . . wm
]
:
U?VT = SVD(W)
Recall that W encodes the correlation structure
between pivot and non-pivot words in the form
of multiple linear classifiers. Thus, the columns
of U identify common substructures among these
classifiers. Choosing the columns of U associated
with the largest singular values yields those sub-
structures that capture most of the correlation in
W. We define ? as those columns of U that are
associated with the k largest singular values:
? = UT[1:k, 1:|V |]
Algorithm 1 summarizes the three steps of CL-
SCL. At training and test time, we apply the pro-
jection ? to each input instance x. The vector v?
that minimizes the regularized training error for
DS in the projected space is defined as follows:
v? = argmin
v?Rk
?
(x,y)?DS
L(y, vT ?x) +
?
2
?v?2 (2)
The resulting classifier fST , which will operate
in the cross-lingual setting, is defined as follows:
fST (x) = sign(v
?T ?x)
4.1 An Alternative View of CL-SCL
An alternative view of cross-language structural
correspondence learning is provided by the frame-
work of structural learning (Ando and Zhang,
2005a). The basic idea of structural learning is
Algorithm 1 CL-SCL
Input: Labeled source data DS
Unlabeled data Du = DS,u ?DT ,u
Parameters: m, k, ?, and ?
Output: k ? |V |-dimensional matrix ?
1. SELECTPIVOTS(DS ,m)
VP = MUTUALINFORMATION(DS )
P ? = {{wS , TRANSLATE(wS)} | wS ? VP }
P = CANDIDATEELIMINATION(P ?, ?)
2. TRAINPIVOTPREDICTORS(Du,P )
for l = 1 to m do
Dl = {(MASK(x, pl), IN(x, pl)) | x ? Du}
wl= argmin
w?R|V |
?
(x,y)?Dl
L(y,wTx)) + ?2 ?w?
2
end for
W =
[
w1 . . . wm
]
3. COMPUTESVD(W, k)
U?VT = SVD(W)
? = UT[1:k, 1:|V |]
output {?}
to constrain the hypothesis space, i.e., the space of
possible weight vectors, of the target task by con-
sidering multiple different but related prediction
tasks. In our context these auxiliary tasks are rep-
resented by the pivot predictors, i.e., the columns
of W. Each column vector wl can be considered
as a linear classifier which performs well in both
languages. I.e., we regard the column space of W
as an approximation to the subspace of bilingual
classifiers. By computing SVD(W) one obtains
a compact representation of this column space in
the form of an orthonormal basis ?T .
The subspace is used to constrain the learning of
the target task by restricting the weight vector w to
lie in the subspace defined by ?T . Following Ando
and Zhang (2005a) and Quattoni et al (2007) we
choose w for the target task to be w? = ?Tv?,
where v? is defined as follows:
v? = argmin
v?Rk
?
(x,y)?DS
L(y, (?Tv)Tx) +
?
2
?v?2 (3)
Since (?Tv)T = vT ? it follows that this view
of CL-SCL corresponds to the induction of a new
feature space given by Equation 2.
1122
5 Experiments
We evaluate CL-SCL for the task of cross-
language sentiment classification using English
as source language and German, French, and
Japanese as target languages. Special emphasis is
put on corpus construction, determination of upper
bounds and baselines, and a sensitivity analysis of
important hyperparameters. All data described in
the following is publicly available from our project
website.1
5.1 Dataset and Preprocessing
We compiled a new dataset for cross-language
sentiment classification by crawling product re-
views from Amazon.{de | fr | co.jp}. The crawled
part of the corpus contains more than 4 million
reviews in the three languages German, French,
and Japanese. The corpus is extended with En-
glish product reviews provided by Blitzer et al
(2007). Each review contains a category label,
a title, the review text, and a rating of 1-5 stars.
Following Blitzer et al (2007) a review with >3
(<3) stars is labeled as positive (negative); other
reviews are discarded. For each language the la-
beled reviews are grouped according to their cate-
gory label, whereas we restrict our experiments to
three categories: books, dvds, and music.
Since most of the crawled reviews are posi-
tive (80%), we decide to balance the number of
positive and negative reviews. In this study, we
are interested in whether the cross-lingual repre-
sentation induced by CL-SCL captures the differ-
ence between positive and negative reviews; by
balancing the reviews we ensure that the imbal-
ance does not affect the learned model. Balancing
is achieved by deleting reviews from the major-
ity class uniformly at random for each language-
specific category. The resulting sets are split into
three disjoint, balanced sets, containing training
documents, test documents, and unlabeled docu-
ments; the respective set sizes are 2,000, 2,000,
and 9,000-50,000. See Table 1 for details.
For each of the nine target-language-category-
combinations a text classification task is created
by taking the training set of the product category in
S and the test set of the same product category in
T . A document d is described as normalized fea-
ture vector x under a unigram bag-of-words docu-
ment representation. The morphological analyzer
1http://www.webis.de/research/corpora/
webis-cls-10/
MeCab is used for Japanese word segmentation.2
5.2 Implementation
Throughout the experiments linear classifiers are
employed; they are trained by minimizing Equa-
tion (1), using a stochastic gradient descent (SGD)
algorithm. In particular, the learning rate schedule
from PEGASOS is adopted (Shalev-Shwartz et al,
2007), and the modified Huber loss, introduced by
Zhang (2004), is chosen as loss function L.3
SGD receives two hyperparameters as input: the
number of iterations T , and the regularization pa-
rameter ?. In our experiments T is always set to
106, which is about the number of iterations re-
quired for SGD to converge. For the target task,
? is determined by 3-fold cross-validation, testing
for ? all values 10?i, i ? [0; 6]. For the pivot pre-
diction task, ? is set to the small value of 10?5, in
order to favor model accuracy over generalizabil-
ity.
The computational bottleneck of CL-SCL is the
SVD of the dense parameter matrix W. Here we
follow Blitzer et al (2006) and set the negative
values in W to zero, which yields a sparse repre-
sentation. For the SVD computation the Lanczos
algorithm provided by SVDLIBC is employed.4
We investigated an alternative approach to obtain
a sparse W by directly enforcing sparse pivot pre-
dictors wl through L1-regularization (Tsuruoka et
al., 2009), but didn?t pursue this strategy due to
unstable results. Since SGD is sensitive to fea-
ture scaling the projection ?x is post-processed as
follows: (1) Each feature of the cross-lingual rep-
resentation is standardized to zero mean and unit
variance, where mean and variance are estimated
on DS ?Du. (2) The cross-lingual document rep-
resentations are scaled by a constant ? such that
|DS |
?1?
x?DS
???x? = 1.
We use Google Translate as word translation or-
acle, which returns a single translation for each
query word.5 Though such a context free transla-
tion is suboptimum we do not sanitize the returned
words to demonstrate the robustness of CL-SCL
with respect to translation noise. To ensure the re-
producibility of our results we cache all queries to
the translation oracle.
2http://mecab.sourceforge.net
3Our implementation is available at http://github.
com/pprett/bolt
4http://tedlab.mit.edu/?dr/SVDLIBC/
5http://translate.google.com
1123
T Category
Unlabeled data Upper Bound CL-MT CL-SCL
|DS,u| |DT ,u| ? ? ? ? ? ? ? ?
books 50,000 50,000 83.79 (?0.20) 79.68 (?0.13) 4.11 79.50 (?0.33) 4.29
German dvd 30,000 50,000 81.78 (?0.27) 77.92 (?0.25) 3.86 76.92 (?0.07) 4.86
music 25,000 50,000 82.80 (?0.13) 77.22 (?0.23) 5.58 77.79 (?0.02) 5.00
books 50,000 32,000 83.92 (?0.14) 80.76 (?0.34) 3.16 78.49 (?0.03) 5.43
French dvd 30,000 9,000 83.40 (?0.28) 78.83 (?0.19) 4.57 78.80 (?0.01) 4.60
music 25,000 16,000 86.09 (?0.13) 75.78 (?0.65) 10.31 77.92 (?0.03) 8.17
books 50,000 50,000 79.39 (?0.27) 70.22 (?0.27) 9.17 73.09 (?0.07) 6.30
Japanese dvd 30,000 50,000 81.56 (?0.28) 71.30 (?0.28) 10.26 71.07 (?0.02) 10.49
music 25,000 50,000 82.33 (?0.13) 72.02 (?0.29) 10.31 75.11 (?0.06) 7.22
Table 1: Cross-language sentiment classification results. For each task, the number of unlabeled docu-
ments from S and T is given. Accuracy scores (mean ? and standard deviation ? of 10 repetitions of
SGD) on the test set of the target language T are reported. ? gives the difference in accuracy to the
upper bound. CL-SCL uses m = 450, k = 100, and ? = 30.
5.3 Upper Bound and Baseline
To get an upper bound on the performance of
a cross-language method we first consider the
monolingual setting. For each target-language-
category-combination a linear classifier is learned
on the training set and tested on the test set. The
resulting accuracy scores are referred to as upper
bound; it informs us about the expected perfor-
mance on the target task if training data in the tar-
get language is available.
We chose a machine translation baseline
to compare CL-SCL to another cross-language
method. Statistical machine translation technol-
ogy offers a straightforward solution to the prob-
lem of cross-language text classification and has
been used in a number of cross-language senti-
ment classification studies (Hiroshi et al, 2004;
Bautin et al, 2008; Wan, 2009). Our baseline
CL-MT works as follows: (1) learn a linear clas-
sifier on the training data, and (2) translate the test
documents into the source language,6 (3) predict
6Again we use Google Translate.
the sentiment polarity of the translated test doc-
uments. Note that the baseline CL-MT does not
make use of unlabeled documents.
5.4 Performance Results and Sensitivity
Table 1 contrasts the classification performance of
CL-SCL with the upper bound and with the base-
line. Observe that the upper bound does not ex-
hibit a great variability across the three languages.
The average accuracy is about 82%, which is con-
sistent with prior work on monolingual sentiment
analysis (Pang et al, 2002; Blitzer et al, 2007).
The performance of CL-MT, however, differs con-
siderably between the two European languages
and Japanese: for Japanese, the average difference
between the upper bound and CL-MT (9.9%) is
about twice as much as for German and French
(5.3%). This difference can be explained by the
fact that machine translation works better for Eu-
ropean than for Asian languages such as Japanese.
Recall that CL-SCL receives three hyperparam-
eters as input: the number of pivots m, the di-
mensionality of the cross-lingual representation k,
Pivot
English German
Semantics Pragmatics Semantics Pragmatics
{beautifulS , scho?nT } amazing, beauty, picture, pattern, poetry, scho?ner (more beautiful), bilder (pictures),
lovely photographs, paintings traurig (sad) illustriert (illustrated)
{boringS , langweiligT } plain, asleep, characters, pages, langatmig (lengthy), charaktere (characters),
dry, long story einfach (plain), handlung (plot),
entta?uscht (disappointed) seiten (pages)
Table 2: Semantic and pragmatic correlations identified for the two pivots {beautifulS , scho?nT } and
{boringS , langweiligT } in English and German book reviews.
1124
Figure 2: Influence of unlabeled data and hyperparameters on the performance of CL-SCL. The rows
show the performance of CL-SCL as a function of (1) the ratio between labeled and unlabeled documents,
(2) the number of pivots m, and (3) the dimensionality of the cross-lingual representation k.
and the minimum support ? of a pivot in DS,u
and DT ,u. For comparison purposes we use fixed
values of m = 450, k = 100, and ? = 30.
The results show the competitiveness of CL-SCL
compared to CL-MT. Although CL-MT outper-
forms CL-SCL on most tasks for German and
French, the difference in accuracy can be consid-
ered as small (<1%); merely for French book and
music reviews the difference is about 2%. For
Japanese, however, CL-SCL outperforms CL-MT
on most tasks with a difference in accuracy of
about 3%. The results indicate that if the dif-
ference between the upper bound and CL-MT is
large, CL-SCL can circumvent the loss in accu-
racy. Experiments with language-specific settings
revealed that for Japanese a smaller number of piv-
ots (150<m<250) performs significantly better.
Thus, the reported results for Japanese can be con-
sidered as pessimistic.
Primarily responsible for the effectiveness of
CL-SCL is its task specificity, i.e., the ways in
which context contributes to meaning (pragmat-
ics). Due to the use of task-specific, unlabeled
data, relevant characteristics are captured by the
pivot classifiers. Table 2 exemplifies this with two
pivots for German book reviews. The rows of the
table show those words which have the highest
correlation with the pivots {beautifulS , scho?nT }
and {boringS , langweiligT }. We can distinguish
between (1) correlations that reflect similar mean-
ing, such as ?amazing?, ?lovely?, or ?plain?, and
(2) correlations that reflect the pivot pragmatics
with respect to the task, such as ?picture?, ?po-
etry?, or ?pages?. Note in this connection that au-
thors of book reviews tend to use the word ?beau-
tiful? to refer to illustrations or poetry. While the
first type of word correlations can be obtained by
methods that operate on parallel corpora, the sec-
ond type of correlation requires an understanding
of the task-specific language use.
In the following we discuss the sensitivity of
each hyperparameter in isolation while keeping
1125
the others fixed atm = 450, k = 100, and ? = 30.
The experiments are illustrated in Figure 2.
Unlabeled Data The first row of Figure 2 shows
the performance of CL-SCL as a function of the
ratio of labeled and unlabeled documents. A ratio
of 1 means that |DS,u| = |DT ,u| = 2,000, while
a ratio of 25 corresponds to the setting of Table 1.
As expected, an increase in unlabeled documents
results in an improved performance, however, we
observe a saturation at a ratio of 10 across all nine
tasks.
Number of Pivots The second row shows the in-
fluence of the number of pivots m on the perfor-
mance of CL-SCL. Compared to the size of the
vocabularies VS and VT , which is in 105 order
of magnitude, the number of pivots is very small.
The plots show that even a small number of piv-
ots captures a significant amount of the correspon-
dence between S and T .
Dimensionality of the Cross-Lingual Represen-
tation The third row shows the influence of the
dimensionality of the cross-lingual representation
k on the performance of CL-SCL. Obviously the
SVD is crucial to the success of CL-SCL if m
is sufficiently large. Observe that the value of k
is task-insensitive: a value of 75<k<150 works
equally well across all tasks.
6 Conclusion
The paper introduces a novel approach to cross-
language text classification, called cross-language
structural correspondence learning. The approach
uses unlabeled documents along with a word
translation oracle to automatically induce task-
specific, cross-lingual correspondences. Our con-
tributions include the adaptation of SCL for the
problem of cross-language text classification and
a well-founded empirical analysis. The analy-
sis covers performance and robustness issues in
the context of cross-language sentiment classifica-
tion with English as source language and German,
French, and Japanese as target languages. The re-
sults show that CL-SCL is competitive with state-
of-the-art machine translation technology while
requiring fewer resources.
Future work includes the extension of CL-SCL
towards a general approach for cross-lingual adap-
tation of natural language processing technology.
References
Rie-K. Ando and Tong Zhang. 2005a. A framework
for learning predictive structures from multiple tasks
and unlabeled data. J. Mach. Learn. Res., 6:1817?
1853.
Rie-K. Ando and Tong Zhang. 2005b. A high-
performance semi-supervised learning method for
text chunking. In Proceedings of ACL-05, pages 1?
9, Ann Arbor.
Mikhail Bautin, Lohit Vijayarenu, and Steven Skiena.
2008. International sentiment analysis for news and
blogs. In Proceedings of ICWSM-08, pages 19?26,
Seattle.
Nuria Bel, Cornelis H. A. Koster, and Marta Villegas.
2003. Cross-lingual text categorization. In Proceed-
ings of ECDL-03, pages 126?139, Trondheim.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural corre-
spondence learning. In Proceedings of EMNLP-06,
pages 120?128, Sydney.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Proceedings of ACL-07, pages 440?447,
Prague.
Hal Daume? III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of ACL-07, pages 256?263,
Prague.
Susan T. Dumais, Todd A. Letsche, Michael L.
Littman, and Thomas K. Landauer. 1997. Auto-
matic cross-language retrieval using latent semantic
indexing. In AAAI Symposium on CrossLanguage
Text and Speech Retrieval.
Jenny-R. Finkel and Christopher-D. Manning. 2009.
Hierarchical bayesian domain adaptation. In Pro-
ceedings of HLT/NAACL-09, pages 602?610, Boul-
der.
Blaz? Fortuna and John Shawe-Taylor. 2005. The use
of machine translation tools for cross-lingual text
mining. In Proceedings of the ICML Workshop on
Learning with Multiple Views.
Alfio Gliozzo and Carlo Strapparava. 2005. Cross lan-
guage text categorization by acquiring multilingual
domain models from comparable corpora. In Pro-
ceedings of the ACL Workshop on Building and Us-
ing Parallel Texts.
Alfio Gliozzo and Carlo Strapparava. 2006. Exploit-
ing comparable corpora and bilingual dictionaries
for cross-language text categorization. In Proceed-
ings of ACL-06, pages 553?560, Sydney.
Kanayama Hiroshi, Nasukawa Tetsuya, and Watanabe
Hideo. 2004. Deeper sentiment analysis using
machine translation technology. In Proceedings of
COLING-04, pages 494?500, Geneva.
1126
Jing Jiang and Chengxiang Zhai. 2007. A two-stage
approach to domain adaptation for statistical classi-
fiers. In Proceedings of CIKM-07, pages 401?410,
Lisbon.
Victor Lavrenko, Martin Choquette, and W. Bruce
Croft. 2002. Cross-lingual relevance models. In
Proceedings of SIGIR-02, pages 175?182, Tampere.
Yaoyong Li and John S. Taylor. 2007. Advanced
learning algorithms for cross-language patent re-
trieval and classification. Inf. Process. Manage.,
43(5):1183?1199.
Xiao Ling, Gui-R. Xue, Wenyuan Dai, Yun Jiang,
Qiang Yang, and Yong Yu. 2008. Can chinese web
pages be classified with english data source? In Pro-
ceedings of WWW-08, pages 969?978, Beijing.
Douglas W. Oard. 1998. A comparative study of query
and document translation for cross-language infor-
mation retrieval. In Proceedings of AMTA-98, pages
472?483, Langhorne.
J. Scott Olsson, Douglas W. Oard, and Jan Hajic?. 2005.
Cross-language text classification. In Proceedings
of SIGIR-05, pages 645?646, Salvador.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification us-
ing machine learning techniques. In Proceedings of
EMNLP-02, pages 79?86, Philadelphia.
Martin Potthast, Benno Stein, and Maik Anderka.
2008. A wikipedia-based multilingual retrieval
model. In Proceedings of ECIR-08, pages 522?530,
Glasgow.
Ariadna Quattoni, Michael Collins, and Trevor Darrell.
2007. Learning visual representations using images
with captions. In Proceedings of CVPR-07, pages
1?8, Minneapolis.
Leonardo Rigutini, Marco Maggini, and Bing Liu.
2005. An em based training algorithm for cross-
language text categorization. In Proceedings of WI-
05, pages 529?535, Compie`gne.
Shai Shalev-Shwartz, Yoram Singer, and Nathan Sre-
bro. 2007. Pegasos: Primal estimated sub-gradient
solver for svm. In Proceedings of ICML-07, pages
807?814, Corvalis.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent training
for l1-regularized log-linear models with cumulative
penalty. In Proceedings of ACL/AFNLP-09, pages
477?485, Singapore.
Xiaojun Wan. 2009. Co-training for cross-
lingual sentiment classification. In Proceedings of
ACL/AFNLP-09, pages 235?243, Singapore.
Tong Zhang. 2004. Solving large scale linear predic-
tion problems using stochastic gradient descent al-
gorithms. In Proceedings of ICML-04, pages 116?
124, Banff.
1127
