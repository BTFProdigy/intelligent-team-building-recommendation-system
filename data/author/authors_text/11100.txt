Coling 2008: Companion volume ? Posters and Demonstrations, pages 23?26
Manchester, August 2008
A Scalable MMR Approach to Sentence Scoring
for Multi-Document Update Summarization
Florian Boudin
\
and Marc El-B
`
eze
\
\
Laboratoire Informatique d?Avignon
339 chemin des Meinajaries, BP1228,
84911 Avignon Cedex 9, France.
florian.boudin@univ-avignon.fr
marc.elbeze@univ-avignon.fr
Juan-Manuel Torres-Moreno
\,[
[
?
Ecole Polytechnique de Montr?eal
CP 6079 Succ. Centre Ville H3C 3A7
Montr?eal (Qu?ebec), Canada.
juan-manuel.torres@univ-avignon.fr
Abstract
We present SMMR, a scalable sentence
scoring method for query-oriented up-
date summarization. Sentences are scored
thanks to a criterion combining query rele-
vance and dissimilarity with already read
documents (history). As the amount of
data in history increases, non-redundancy
is prioritized over query-relevance. We
show that SMMR achieves promising re-
sults on the DUC 2007 update corpus.
1 Introduction
Extensive experiments on query-oriented multi-
document summarization have been carried out
over the past few years. Most of the strategies
to produce summaries are based on an extrac-
tion method, which identifies salient textual seg-
ments, most often sentences, in documents. Sen-
tences containing the most salient concepts are se-
lected, ordered and assembled according to their
relevance to produce summaries (also called ex-
tracts) (Mani and Maybury, 1999).
Recently emerged from the Document Under-
standing Conference (DUC) 2007
1
, update sum-
marization attempts to enhance summarization
when more information about knowledge acquired
by the user is available. It asks the following ques-
tion: has the user already read documents on the
topic? In the case of a positive answer, producing
an extract focusing on only new facts is of inter-
est. In this way, an important issue is introduced:
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1
Document Understanding Conferences are conducted
since 2000 by the National Institute of Standards and Tech-
nology (NIST), http://www-nlpir.nist.gov
redundancy with previously read documents (his-
tory) has to be removed from the extract.
A natural way to go about update summarization
would be extracting temporal tags (dates, elapsed
times, temporal expressions...) (Mani and Wilson,
2000) or to automatically construct the timeline
from documents (Swan and Allan, 2000). These
temporal marks could be used to focus extracts on
the most recently written facts. However, most re-
cently written facts are not necessarily new facts.
Machine Reading (MR) was used by (Hickl et
al., 2007) to construct knowledge representations
from clusters of documents. Sentences contain-
ing ?new? information (i.e. that could not be in-
ferred by any previously considered document)
are selected to generate summary. However, this
highly efficient approach (best system in DUC
2007 update) requires large linguistic resources.
(Witte et al, 2007) propose a rule-based system
based on fuzzy coreference cluster graphs. Again,
this approach requires to manually write the sen-
tence ranking scheme. Several strategies remain-
ing on post-processing redundancy removal tech-
niques have been suggested. Extracts constructed
from history were used by (Boudin and Torres-
Moreno, 2007) to minimize history?s redundancy.
(Lin et al, 2007) have proposed a modified Max-
imal Marginal Relevance (MMR) (Carbonell and
Goldstein, 1998) re-ranker during sentence selec-
tion, constructing the summary by incrementally
re-ranking sentences.
In this paper, we propose a scalable sentence
scoring method for update summarization derived
from MMR. Motivated by the need for relevant
novelty, candidate sentences are selected accord-
ing to a combined criterion of query relevance and
dissimilarity with previously read sentences. The
rest of the paper is organized as follows. Section 2
23
introduces our proposed sentence scoring method
and Section 3 presents experiments and evaluates
our approach.
2 Method
The underlying idea of our method is that as the
number of sentences in the history increases, the
likelihood to have redundant information within
candidate sentences also increases. We propose
a scalable sentence scoring method derived from
MMR that, as the size of the history increases,
gives more importance to non-redundancy that to
query relevance. We define H to represent the pre-
viously read documents (history), Q to represent
the query and s the candidate sentence. The fol-
lowing subsections formally define the similarity
measures and the scalable MMR scoring method.
2.1 A query-oriented multi-document
summarizer
We have first started by implementing a simple
summarizer for which the task is to produce query-
focused summaries from clusters of documents.
Each document is pre-processed: documents are
segmented into sentences, sentences are filtered
(words which do not carry meaning are removed
such as functional words or common words) and
normalized using a lemmas database (i.e. inflected
forms ?go?, ?goes?, ?went?, ?gone?... are replaced
by ?go?). An N -dimensional term-space ? , where
N is the number of different terms found in the
cluster, is constructed. Sentences are represented
in ? by vectors in which each component is the
term frequency within the sentence. Sentence scor-
ing can be seen as a passage retrieval task in Infor-
mation Retrieval (IR). Each sentence s is scored by
computing a combination of two similarity mea-
sures between the sentence and the query. The first
measure is the well known cosine angle (Salton et
al., 1975) between the sentence and the query vec-
torial representations in ? (denoted respectively ~s
and
~
Q). The second similarity measure is based
on the Jaro-Winkler distance (Winkler, 1999). The
original Jaro-Winkler measure, denoted JW, uses
the number of matching characters and transposi-
tions to compute a similarity score between two
terms, giving more favourable ratings to terms that
match from the beginning. We have extended this
measure to calculate the similarity between the
sentence s and the query Q:
JW
e
(s,Q) =
1
|Q|
?
?
q?Q
max
m?S
?
JW(q,m) (1)
where S
?
is the term set of s in which the terms
m that already have maximized JW(q,m) are re-
moved. The use of JW
e
smooths normalization and
misspelling errors. Each sentence s is scored using
the linear combination:
Sim
1
(s,Q) = ? ? cosine(~s,
~
Q)
+ (1? ?) ? JW
e
(s,Q) (2)
where ? = 0.7, optimally tuned on the past DUCs
data (2005 and 2006). The system produces a list
of ranked sentences from which the summary is
constructed by arranging the high scored sentences
until the desired size is reached.
2.2 A scalable MMR approach
MMR re-ranking algorithm has been successfully
used in query-oriented summarization (Ye et al,
2005). It strives to reduce redundancy while main-
taining query relevance in selected sentences. The
summary is constructed incrementally from a list
of ranked sentences, at each iteration the sentence
which maximizes MMR is chosen:
MMR = argmax
s?S
[ ? ? Sim
1
(s,Q)
? (1? ?) ?max
s
j
?E
Sim
2
(s, s
j
) ] (3)
where S is the set of candidates sentences and E
is the set of selected sentences. ? represents an
interpolation coefficient between sentence?s rele-
vance and non-redundancy. Sim
2
(s, s
j
) is a nor-
malized Longest Common Substring (LCS) mea-
sure between sentences s and s
j
. Detecting sen-
tence rehearsals, LCS is well adapted for redun-
dancy removal.
We propose an interpretation of MMR to tackle
the update summarization issue. Since Sim
1
and
Sim
2
are ranged in [0, 1], they can be seen as prob-
abilities even though they are not. Just as rewriting
(3) as (NR stands for Novelty Relevance):
NR = argmax
s?S
[ ? ? Sim
1
(s,Q)
+ (1? ?) ? (1? max
s
h
?H
Sim
2
(s, s
h
)) ] (4)
We can understand that (4) equates to an OR com-
bination. But as we are looking for a more intu-
itive AND and since the similarities are indepen-
dent, we have to use the product combination. The
24
scoring method defined in (2) is modified into a
double maximization criterion in which the best
ranked sentence will be the most relevant to the
query AND the most different to the sentences in
H .
SMMR(s) = Sim
1
(s,Q)
?
(
1? max
s
h
?H
Sim
2
(s, s
h
)
)
f(H)
(5)
Decreasing ? in (3) with the length of the sum-
mary was suggested by (Murray et al, 2005) and
successfully used in the DUC 2005 by (Hachey
et al, 2005), thereby emphasizing the relevance
at the outset but increasingly prioritizing redun-
dancy removal as the process continues. Sim-
ilarly, we propose to follow this assumption in
SMMR using a function denoted f that as the
amount of data in history increases, prioritize non-
redundancy (f(H)? 0).
3 Experiments
The method described in the previous section has
been implemented and evaluated by using the
DUC 2007 update corpus
2
. The following subsec-
tions present details of the different experiments
we have conducted.
3.1 The DUC 2007 update corpus
We used for our experiments the DUC 2007 up-
date competition data set. The corpus is composed
of 10 topics, with 25 documents per topic. The up-
date task goal was to produce short (?100 words)
multi-document update summaries of newswire ar-
ticles under the assumption that the user has al-
ready read a set of earlier articles. The purpose
of each update summary will be to inform the
reader of new information about a particular topic.
Given a DUC topic and its 3 document clusters: A
(10 documents), B (8 documents) and C (7 doc-
uments), the task is to create from the documents
three brief, fluent summaries that contribute to sat-
isfying the information need expressed in the topic
statement.
1. A summary of documents in cluster A.
2. An update summary of documents in B, un-
der the assumption that the reader has already
read documents in A.
2
More information about the DUC 2007 corpus is avail-
able at http://duc.nist.gov/.
3. An update summary of documents in C, un-
der the assumption that the reader has already
read documents in A and B.
Within a topic, the document clusters must be pro-
cessed in chronological order. Our system gener-
ates a summary for each cluster by arranging the
high ranked sentences until the limit of 100 words
is reached.
3.2 Evaluation
Most existing automated evaluation methods work
by comparing the generated summaries to one or
more reference summaries (ideally, produced by
humans). To evaluate the quality of our generated
summaries, we choose to use the ROUGE
3
(Lin,
2004) evaluation toolkit, that has been found to be
highly correlated with human judgments. ROUGE-
N is a n-gram recall measure calculated between
a candidate summary and a set of reference sum-
maries. In our experiments ROUGE-1, ROUGE-2
and ROUGE-SU4 will be computed.
3.3 Results
Table 1 reports the results obtained on the DUC
2007 update data set for different sentence scor-
ing methods. cosine + JW
e
stands for the scor-
ing method defined in (2) and NR improves it
with sentence re-ranking defined in equation (4).
SMMR is the combined adaptation we have pro-
posed in (5). The function f(H) used in SMMR is
the simple rational function
1
H
, where H increases
with the number of previous clusters (f(H) = 1
for cluster A,
1
2
for cluster B and
1
3
for cluster C).
This function allows to simply test the assumption
that non-redundancy have to be favoured as the
size of history grows. Baseline results are obtained
on summaries generated by taking the leading sen-
tences of the most recent documents of the cluster,
up to 100 words (official baseline of DUC). The
table also lists the three top performing systems at
DUC 2007 and the lowest scored human reference.
As we can see from these results, SMMR out-
performs the other sentence scoring methods. By
ways of comparison our system would have been
ranked second at the DUC 2007 update competi-
tion. Moreover, no post-processing was applied to
the selected sentences leaving an important margin
of progress. Another interesting result is the high
performance of the non-update specific method
(cosine+ JW
e
) that could be due to the small size
3
ROUGE is available at http://haydn.isi.edu/ROUGE/.
25
of the corpus (little redundancy between clusters).
ROUGE-1 ROUGE-2 ROUGE-SU4
Baseline 0.26232 0.04543 0.08247
3
rd
system 0.35715 0.09622 0.13245
2
nd
system 0.36965 0.09851 0.13509
cosine+ JW
e
0.35905 0.10161 0.13701
NR 0.36207 0.10042 0.13781
SMMR 0.36323 0.10223 0.13886
1
st
system 0.37032 0.11189 0.14306
Worst human 0.40497 0.10511 0.14779
Table 1: ROUGE average recall scores computed
on the DUC 2007 update corpus.
4 Discussion and Future Work
In this paper we have described SMMR, a scal-
able sentence scoring method based on MMR that
achieves very promising results. An important as-
pect of our sentence scoring method is that it does
not requires re-ranking nor linguistic knowledge,
which makes it a simple and fast approach to the
issue of update summarization. It was pointed out
at the DUC 2007 workshop that Question Answer-
ing and query-oriented summarization have been
converging on a common task. The value added
by summarization lies in the linguistic quality. Ap-
proaches mixing IR techniques are well suited for
query-oriented summarization but they require in-
tensive work for making the summary fluent and
coherent. Among the others, this is a point that we
think is worthy of further investigation.
Acknowledgments
This work was supported by the Agence Nationale
de la Recherche, France, project RPM2.
References
Boudin, F. and J.M. Torres-Moreno. 2007. A Co-
sine Maximization-Minimization approach for User-
Oriented Multi-Document Update Summarization.
In Recent Advances in Natural Language Processing
(RANLP), pages 81?87.
Carbonell, J. and J. Goldstein. 1998. The use of MMR,
diversity-based reranking for reordering documents
and producing summaries. In 21st annual interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 335?336.
ACM Press New York, NY, USA.
Hachey, B., G. Murray, and D. Reitter. 2005. The
Embra System at DUC 2005: Query-oriented Multi-
document Summarization with a Very Large Latent
Semantic Space. In Document Understanding Con-
ference (DUC).
Hickl, A., K. Roberts, and F. Lacatusu. 2007. LCC?s
GISTexter at DUC 2007: Machine Reading for Up-
date Summarization. In Document Understanding
Conference (DUC).
Lin, Z., T.S. Chua, M.Y. Kan, W.S. Lee, L. Qiu, and
S. Ye. 2007. NUS at DUC 2007: Using Evolu-
tionary Models of Text. In Document Understanding
Conference (DUC).
Lin, C.Y. 2004. Rouge: A Package for Automatic
Evaluation of Summaries. In Workshop on Text Sum-
marization Branches Out, pages 25?26.
Mani, I. and M.T. Maybury. 1999. Advances in Auto-
matic Text Summarization. MIT Press.
Mani, I. and G. Wilson. 2000. Robust temporal pro-
cessing of news. In 38th Annual Meeting on Asso-
ciation for Computational Linguistics, pages 69?76.
Association for Computational Linguistics Morris-
town, NJ, USA.
Murray, G., S. Renals, and J. Carletta. 2005. Extractive
Summarization of Meeting Recordings. In Ninth Eu-
ropean Conference on Speech Communication and
Technology. ISCA.
Salton, G., A. Wong, and C. S. Yang. 1975. A vector
space model for automatic indexing. Communica-
tions of the ACM, 18(11):613?620.
Swan, R. and J. Allan. 2000. Automatic generation
of overview timelines. In 23rd annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 49?56.
Winkler, W. E. 1999. The state of record linkage and
current research problems. In Survey Methods Sec-
tion, pages 73?79.
Witte, R., R. Krestel, and S. Bergler. 2007. Generat-
ing Update Summaries for DUC 2007. In Document
Understanding Conference (DUC).
Ye, S., L. Qiu, T.S. Chua, and M.Y. Kan. 2005. NUS
at DUC 2005: Understanding documents via con-
cept links. In Document Understanding Conference
(DUC).
26
Coling 2010: Poster Volume, pages 1059?1067,
Beijing, August 2010
Multilingual Summarization Evaluation without Human Models
Horacio Saggion
TALN - DTIC
Universitat Pompeu Fabra
horacio.saggion@upf.edu
Juan-Manuel Torres-Moreno
LIA/Universite? d?Avignon
E?cole Polytechnique de Montre?al
juan-manuel.torres@univ-avignon.fr
Iria da Cunha
IULA/Universitat Pompeu Fabra
LIA/Universite? d?Avignon
iria.dacunha@upf.edu
Eric SanJuan
LIA/Universite? d?Avignon
eric.sanjuan@univ-avignon.fr
Patricia Vela?zquez-Morales
VM Labs
patricia vazquez@yahoo.com
Abstract
We study correlation of rankings of text
summarization systems using evaluation
methods with and without human mod-
els. We apply our comparison frame-
work to various well-established content-
based evaluation measures in text sum-
marization such as coverage, Responsive-
ness, Pyramids and ROUGE studying their
associations in various text summarization
tasks including generic and focus-based
multi-document summarization in English
and generic single-document summariza-
tion in French and Spanish. The research
is carried out using a new content-based
evaluation framework called FRESA to
compute a variety of divergences among
probability distributions.
1 Introduction
Text summarization evaluation has always been a
complex and controversial issue in computational
linguistics. In the last decade, significant ad-
vances have been made in the summarization eval-
uation field. Various evaluation frameworks have
been established and evaluation measures devel-
oped. SUMMAC (Mani et al, 2002), in 1998,
provided the first system independent framework
for summary evaluation; the Document Under-
standing Conference (DUC) (Over et al, 2007)
was the main evaluation forum from 2000 until
2007; nowadays, the Text Analysis Conference
(TAC)1 provides a forum for assessment of dif-
ferent information access technologies including
text summarization.
Evaluation in text summarization can be extrin-
sic or intrinsic (Spa?rck-Jones and Galliers, 1996).
In an extrinsic evaluation, the summaries are as-
sessed in the context of an specific task a human
or machine has to carry out; in an intrinsic eval-
uation, the summaries are evaluated in reference
to some ideal model. SUMMAC was mainly ex-
trinsic while DUC and TAC followed an intrinsic
evaluation paradigm. In order to intrinsically eval-
uate summaries, the automatic summary (peer)
has to be compared to a model summary or sum-
maries. DUC used an interface called SEE to al-
low human judges compare a peer summary to a
model summary. Using SEE, human judges give a
coverage score to the peer summary representing
the degree of overlap with the model summary.
Summarization systems obtain a final coverage
score which is the average of the coverage?s scores
associated to their summaries. The system?s cov-
erage score can then be used to rank summariza-
tion systems. In the case of query-focused sum-
marization (e.g. when the summary has to re-
spond to a question or set of questions) a Respon-
siveness score is also assigned to each summary
which indicates how responsive the summary is to
the question(s).
Because manual comparison of peer summaries
with model summaries is an arduous and costly
1http://www.nist.gov/tac
1059
process, a body of research has been produced in
the last decade on automatic content-based eval-
uation procedures. Early studies used text simi-
larity measures such as cosine similarity (with or
without weighting schema) to compare peer and
model summaries (Donaway et al, 2000), vari-
ous vocabulary overlap measures such as set of
n-grams overlap or longest common subsequence
between peer and model have also been pro-
posed (Saggion et al, 2002; Radev et al, 2003).
The Bleu machine translation evaluation measure
(Papineni et al, 2002) has also been tested in
summarization (Pastra and Saggion, 2003). The
DUC conferences adopted the ROUGE package
for content-based evaluation (Lin, 2004). It im-
plements a series of recall measures based on n-
gram co-occurrence statistics between a peer sum-
mary and a set of model summaries. ROUGE mea-
sures can be used to produce systems ranks. It
has been shown that system rankings produced
by some ROUGE measures (e.g., ROUGE-2 which
uses bi-grams) correlate with rankings produced
using coverage. In recent years the Pyramids eval-
uation method (Nenkova and Passonneau, 2004)
was introduced. It is based on the distribution
of ?content? in a set of model summaries. Sum-
mary Content Units (SCUs) are first identified in
the model summaries, then each SCU receives
a weight which is the number of models con-
taining or expressing the same unit. Peer SCUs
are identified in the peer, matched against model
SCUs, and weighted accordingly. The Pyramids
score given to the peer is the ratio of the sum
of the weights of its units and the sum of the
weights of the best possible ideal summary with
the same number of SCUs as the peer. The Pyra-
mids scores can be used for ranking summariza-
tion systems. Nenkova and Passonneau (2004)
showed that Pyramids scores produced reliable
system rankings when multiple (4 or more) mod-
els were used and that Pyramids rankings cor-
relate with rankings produced by ROUGE-2 and
ROUGE-SU2 (i.e. ROUGE with skip bi-grams).
Still this method requires the creation of models
and the identification, matching, and weighting of
SCUs in both models and peers.
Donaway et al (2000) put forward the idea of
using directly the full document for comparison
purposes, and argued that content-based measures
which compare the document to the summary may
be acceptable substitutes for those using model
summaries. A method for evaluation of sum-
marization systems without models has been re-
cently proposed (Louis and Nenkova, 2009). It is
based on the direct content-based comparison be-
tween summaries and their corresponding source
documents. Louis and Nenkova (2009) evalu-
ated the effectiveness of the Jensen-Shannon (Lin,
1991b) theoretic measure in predicting systems
ranks in two summarization tasks query-focused
and update summarization. They have shown that
ranks produced by Pyramids and ranks produced
by the Jensen-Shannon measure correlate. How-
ever, they did not investigate the effect of the mea-
sure in past summarization tasks such as generic
multi-document summarization (DUC 2004 Task
2), biographical summarization (DUC 2004 Task
5), opinion summarization (TAC 2008 OS), and
summarization in languages other than English.
We think that, in order to have a better under-
standing of document-summary evaluation mea-
sures, more research is needed. In this paper we
present a series of experiments aimed at a better
understanding of the value of the Jensen-Shannon
divergence for ranking summarization systems.
We have carried out experimentation with the
proposed measure and have verified that in cer-
tain tasks (such as those studied by (Louis and
Nenkova, 2009)) there is a strong correlation
among Pyramids and Responsiveness and the
Jensen-Shannon divergence, but as we will show
in this paper, there are datasets in which the cor-
relation is not so strong. We also present exper-
iments in Spanish and French showing positive
correlation between the Jensen-Shannon measure
and ROUGE.
The rest of the paper is organized in the follow-
ing way: First in Section 2 we introduce related
work in the area of content-based evaluation iden-
tifying the departing point for our inquiry; then in
Section 3 we explain the methodology adopted in
our work and the tools and resources used for ex-
perimentation. In Section 4 we present the experi-
ments carried out together with the results. Sec-
tion 5 discusses the results and Section 6 con-
cludes the paper.
1060
2 Related Work
One of the first works to use content-based mea-
sures in text summarization evaluation is due to
(Donaway et al, 2000) who presented an evalu-
ation framework to compare rankings of summa-
rization systems produced by recall and cosine-
based measures. They showed that there was
weak correlation between rankings produced by
recall, but that content-based measures produce
rankings which were strongly correlated, thus
paving the way for content-based measures in text
summarization evaluation.
Radev et al (2003) also compared various eval-
uation measures based on vocabulary overlap. Al-
though these measures were able to separate ran-
dom from non-random systems, no clear conclu-
sion was reached on the value of each of the mea-
sures studied.
Nowadays, a widespread summarization evalu-
ation framework is ROUGE (Lin and Hovy, 2003)
which, as we have mentioned before, offers a set
of statistics that compare peer summaries with
models. Various statistics exist depending on the
used n-gram and on the type of text processing ap-
plied to the input texts (e.g., lemmatization, stop-
word removal).
Lin et al (2006) proposed a method of evalua-
tion based on the use of ?distances? or divergences
between two probability distributions (the distri-
bution of units in the automatic summary and the
distribution of units in the model summary). They
studied two different Information Theoretic mea-
sures of divergence: the Kullback-Leibler (KL)
(Kullback and Leibler, 1951) and Jensen-Shannon
(JS) (Lin, 1991a) divergences. In this work we
use the Jensen-Shannon (JS) divergence that is
defined as follows:
DJS(P ||Q) = 12
?
w
Pw log2
2Pw
Pw +Qw
+ Qw log2
2Qw
Pw +Qw
(1)
This measure can be applied to the distribu-
tion of units in system summaries P and refer-
ence summaries Q and the value obtained used
as a score for the system summary. The method
has been tested by (Lin et al, 2006) over the
DUC 2002 corpus for single and multi docu-
ment summarization tasks showing good correla-
tion among divergence measures and both cover-
age and ROUGE rankings.
Louis and Nenkova (2009) went even further
and, as in (Donaway et al, 2000), proposed to
directly compare the distribution of words in full
documents with the distribution of words in auto-
matic summaries to derive a content-based eval-
uation measure. They found high correlation
among rankings produced using models and rank-
ings produced without models. This work is the
departing point for our inquiry into the value of
measures that do not rely on human models.
3 Methodology
The methodology of this paper mirrors the one
adopted in past work (Donaway et al, 2000;
Louis and Nenkova, 2009). Given a particular
summarization task T , p data points to be sum-
marized with input material {Ii}p?1i=0 (e.g. doc-
ument(s), questions, topics), s peer summaries
{SUMi,k}s?1k=0 for input i, and m model sum-
maries {MODELi,j}m?1j=0 for input i, we will com-
pare rankings of the s peer summaries produced
by various evaluation measures. Some measures
we use compare summaries with n out of the m
models:
MEASUREM (SUMi,k, {MODELi,j}nj=0) (2)
while other measures compare peers with all or
some of the input material:
MEASUREM (SUMi,k, I ?i) (3)
where I ?i is some subset of input Ii. The val-
ues produced by the measures for each sum-
mary SUMi,k are averaged for each system k =
0, . . . , s ? 1 and these averages are used to pro-
duce a ranking. Rankings are compared using
Spearman Rank correlation (Spiegel and Castel-
lan, 1998) used to measure the degree of associa-
tion between two variables whose values are used
to rank objects. We use this correlation to directly
compare results to those presented in (Louis and
Nenkova, 2009). Computation of correlations is
1061
done using the CPAN Statistics-RankCorrelation-
0.12 package2, which computes the rank correla-
tion between two vectors.
3.1 Tools
We carry out experimentation using a new sum-
marization evaluation framework: FRESA
?FRamework for Evaluating Summaries
Automatically? which includes document-
based summary evaluation measures based on
probabilities distribution. As in the ROUGE
package, FRESA supports different n-grams
and skip n-grams probability distributions.
The FRESA environment can be used in the
evaluation of summaries in English, French,
Spanish and Catalan, and it integrates filtering
and lemmatization in the treatment of summaries
and documents. It is developed in Perl and will be
made publicly available. We also use the ROUGE
package to compute various ROUGE statistics in
new datasets.
3.2 Summarization Tasks and Data Sets
We have conducted our experimentation with the
following summarization tasks and data sets:
Generic multi-document-summarization in En-
glish (i.e. production a short summary of a cluster
of related documents) using data fromDUC 20043
corpus task 2: 50 clusters (10 documents each) ?
294,636 words.
Focused-based summarization in English (i.e.
production a short focused multi-document sum-
mary focused on the question ?who is X??, where
X is a person?s name) using data from the DUC
2004 task 5: 50 clusters ( 10 documents each plus
a target person name) ? 284,440 words.
Update-summarization task that consists of cre-
ating a summary out of a cluster of documents and
a topic. Two sub-tasks are considered here: A)
an initial summary has to be produced based on
an initial set of documents and topic; B) an up-
date summary has to be produced from a differ-
ent (but related) cluster assuming documents used
in A) are known. The English TAC 2008 Update
2http://search.cpan.org/?gene/
Statistics-RankCorrelation-0.12/
3http://www-nlpir.nist.gov/projects/
duc/guidelines/2004.html
Summarization dataset is used which consists of
48 topics with 20 documents each ? 36,911 words.
Opinion summarization where systems have to
analyze a set of blog articles and summarize the
opinions about a target in the articles. The TAC
2008 Opinion Summarization in English4 data set
(taken from the Blogs06 Text Collection) is used:
25 clusters and targets (i.e., target entity and ques-
tions) were used ? 1,167,735 words.
Generic single-document summarization in
Spanish using the ?Spanish Medicina Cl??nica?5
corpus which is composed of 50 biomedical ar-
ticles in Spanish, each one with its corresponding
author abstract ? 124,929 words.
Generic single document summarization in
French using the ?Canadien French Sociologi-
cal Articles? corpus from the journal Perspec-
tives interdisciplinaires sur le travail et la sante?
(PISTES)6. It contains 50 sociological articles in
French with their corresponding author abstracts
? 381,039 words.
3.3 Summarization Systems
For experimentation in the TAC and the DUC
datasets we directly use the peer summaries
produced by systems participating in the eval-
uations. For experimentation in Spanish and
French (single-document summarization) we
have created summaries at the compression rates
of the model summaries using the following
summarization systems:
? CORTEX (Torres-Moreno et al, 2002), a
single-document sentence extraction system
for Spanish and French that combines vari-
ous statistical measures of relevance (angle
between sentence and topic, various Ham-
ming weights for sentences, etc.) and applies
an optimal decision algorithm for sentence
selection;
? ENERTEX (Fernandez et al, 2007), a sum-
marizer based on a theory of textual energy;
4http://www.nist.gov/tac/data/index.
html
5http://www.elsevier.es/revistas/
ctl servlet? f=7032&revistaid=2
6http://www.pistes.uqam.ca/
1062
? SUMMTERM (Vivaldi et al, 2010), a
terminology-based summarizer that is used
for summarization of medical articles and
uses specialized terminology for scoring and
ranking sentences;
? JS summarizer, a summarization system that
scores and ranks sentences according to their
Jensen-Shannon divergence to the source
document;
? a lead-based summarization system that se-
lects the lead sentences of the document;
? a random-based summarization system that
selects sentences at random;
? the multilingual word-frequency Open Text
Summarizer (Yatsko and Vishnyakov, 2007);
? the AutoSummarize program of Microsoft
Word;
? the commercial SSSummarizer7;
? the Pertinence summarizer8;
? the Copernic summarizer9.
3.4 Evaluation Measures
The following measures derived from human
assessment of the content of the summaries are
used in our experiments:
? Coverage is understood as the degree to
which one peer summary conveys the same
information as a model summary (Over et al,
2007). Coverage was used in DUC evalua-
tions.
? Responsiveness ranks summaries in a 5-point
scale indicating how well the summary sat-
isfied a given information need (Over et al,
2007). It is used in focused-based summa-
rization tasks. Responsiveness was used in
DUC-TAC evaluations.
7http://www.kryltech.com/summarizer.
htm
8http://www.pertinence.net
9http://www.copernic.com/en/products/
summarizer
? Pyramids (briefly introduced in Section 1)
(Nenkova and Passonneau, 2004) is a content
assessment measure which compares content
units in a peer summary to weighted content
units in a set of model summaries. Pyramids
is the adopted metric for content-based eval-
uation in the TAC evaluations.
For DUC and TAC datasets the values of these
measures are available and we used them directly.
We used the following automatic evaluation
measures in our experiments:
? We use the Rouge package (Lin, 2004) to
compute various statistics. For the experi-
ments presented here we used uni-grams, bi-
grams, and the skip bi-grams with maximum
skip distance of 4 (ROUGE-1, ROUGE-2 and
ROUGE-SU4). ROUGE is used to compare a
peer summary to a set of model summaries
in our framework.
? Jensen-Shannon divergence formula given in
Equation 1 is implemented in our FRESA
package with the following specification for
the probability distribution of words w.
Pw =
CTw
N (4)
Qw =
{
CSw
NS if w ? S
CTw+?
N+??B elsewhere
(5)
Where P is the probability distribution of
words w in text T and Q is the probabil-
ity distribution of words w in summary S;
N is the number of words in text and sum-
mary N = NT + NS , B = 1.5|V |, CTw is
the number of words in the text and CSw is
the number of words in the summary. For
smoothing the summary?s probabilities we
have used ? = 0.005.
4 Experiments and Results
We first replicated the experiments presented in
(Louis and Nenkova, 2009) to verify that our im-
plementation of JS produced correlation results
compatible with that work. We used the TAC
2008 Update Summarization data set and com-
puted JS and ROUGE measures for each peer
1063
summary. We produced two system rankings (one
for each measure), which were compared to rank-
ings produced using the manual Pyramids and Re-
sponsiveness scores. Spearman correlations were
computed among the different rankings. The re-
sults are presented in Table 1. These results con-
firm a high correlation among Pyramids, Respon-
siveness, and JS. We also verified high corre-
lation between JS and ROUGE-2 (0.83 Spearman
correlation, not shown in the table) in this task and
dataset.
Measure Pyr. p-value Resp. p-value
ROUGE-2 0.96 p < 0.005 0.92 p < 0.005
JS 0.85 p < 0.005 0.74 p < 0.005
Table 1: Spearman system rank correlation of
content-based measures in TAC 2008 Update
Summarization task
Then, we experimented with data from DUC
2004, TAC 2008 Opinion Summarization pilot
and with single document summarization in Span-
ish and French. In spite of the fact that the exper-
iments for French and Spanish corpora use less
data points (i.e., less summarizers per task) than
for English, results are still quite significant.
For DUC 2004, we computed the JS measure
for each peer summary in tasks 2 and 5 and we
used JS and the official ROUGE, coverage, and
Responsiveness scores to produce systems? rank-
ings. The various Spearman?s rank correlation
values for DUC 2004 are presented in Tables 2
(for task 2) and 3 (for task 5). For task 2, we have
verified a strong correlation between JS and cov-
erage. For task 5, the correlation between JS and
coverage is weak, and the correlation between JS
and Responsiveness weak and negative.
Measure Cov. p-value
ROUGE-2 0.79 p < 0.0050
JS 0.68 p < 0.0025
Table 2: Spearman system rank correlation of
content-based measures with coverage in DUC
2004 Task 2
Although the Opinion Summarization task is a
new type of summarization task and its evaluation
is a complicated issue, we have decided to com-
pare JS rankings with those obtained using Pyra-
Measure Cov. p-value Resp. p-value
ROUGE-2 0.78 p < 0.001 0.44 p < 0.05
JS 0.40 p < 0.050 -0.18 p < 0.25
Table 3: Spearman system rank correlation of
content-based measures in DUC 2004 Task 5
mids and Responsiveness in TAC 2008. Spear-
man?s correlation values are listed in Table 4. As
can be seen, there is weak and negative correla-
tion of JS with both Pyramids and Responsive-
ness. Correlation between Pyramids and Respon-
siveness rankings is high for this task (0.71 Spear-
man?s correlation value).
Measure Pyr. p-value Resp. p-value
JS -0.13 p < 0.25 -0.14 p < 0.25
Table 4: Spearman system rank correlation of
content-based measures in TAC 2008 Opinion
Summarization task
For experimentation in Spanish and French, we
have run 11 multi-lingual summarization systems
over each of the documents in the two corpora,
producing summaries at a compression rate close
to the compression rate of the provided authors?
abstracts. We have computed JS and ROUGE
measures for each summary and we have aver-
aged the measure?s values for each system. These
averages were used to produce rankings per each
measure. We computed Spearman?s correlations
for all pairs of rankings. Results are presented in
Tables 5-6. All results show medium to strong
correlation between JS and ROUGE measures.
However the JS measure based on uni-grams has
lower correlation than JSs which use n-grams of
higher order.
5 Discussion
The departing point for our inquiry into text sum-
marization evaluation has been recent work on the
use of content-based evaluation metrics that do
not rely on human models but that compare sum-
mary content to input content directly (Louis and
Nenkova, 2009). We have some positive and some
negative results regarding the direct use of the full
document in content-based evaluation. We have
verified that in both generic muti-document sum-
1064
Measure ROUGE-1 p-value ROUGE-2 p-value ROUGE-SU4 p-value
JS 0.56 p < 0.100 0.46 p < 0.100 0.45 p < 0.200
JS2 0.88 p < 0.001 0.80 p < 0.002 0.81 p < 0.005
JS4 0.88 p < 0.001 0.80 p < 0.002 0.81 p < 0.005
JSM 0.82 p < 0.005 0.71 p < 0.020 0.71 p < 0.010
Table 5: Spearman system rank correlation of content-based measures with ROUGE in the Medicina
Clinica Corpus (Spanish)
Measure ROUGE-1 p-value ROUGE-2 p-value ROUGE-2 p-value
JS 0.70 p < 0.050 0.73 p < 0.05 0.73 p < 0.500
JS2 0.93 p < 0.002 0.86 p < 0.01 0.86 p < 0.005
JS4 0.83 p < 0.020 0.76 p < 0.05 0.76 p < 0.050
JSM 0.88 p < 0.010 0.83 p < 0.02 0.83 p < 0.010
Table 6: Spearman system rank correlation of content-based measures with ROUGE in the PISTES
Sociological Articles Corpus (French)
marization and in topic-based multi-document
summarization in English correlation among mea-
sures that use human models (Pyramids, Respon-
siveness, and ROUGE) and a measure that does
not use models (the Jensen Shannon divergence)
is strong. We have found that correlation among
the same measures is weak for summarization of
biographical information and summarization of
opinions in blogs. We believe that in these cases
content-based measures should consider in addi-
tion to the input document, the summarization
task (i.e. its text-based representation) to better
assess the content of the peers, the task being a
determinant factor in the selection of content for
the summary. Our multi-lingual experiments in
generic single-document summarization confirm a
strong correlation among the Jensen-Shannon di-
vergence and ROUGE measures. It is worth not-
ing that ROUGE is in general the chosen frame-
work for presenting content-based evaluation re-
sults in non-English summarization. For the ex-
periments in Spanish, we are conscious that we
only have one model summary to compare with
the peers. Nevertheless, these models are the cor-
responding abstracts written by the authors of the
articles and this is in fact the reason for choosing
this corpus. As the experiments in (da Cunha et
al., 2007) show, the professionals of a specialized
domain (as, for example, the medical domain)
adopt similar strategies to summarize their texts
and they tend to choose roughly the same content
chunks for their summaries. Because of this, the
summary of the author of a medical article can be
taken as reference for summaries evaluation. It is
worth noting that there is still debate on the num-
ber of models to be used in summarization evalu-
ation (Owkzarzak and Dang, 2009). In the French
corpus PISTES, we suspect the situation is similar
to the Spanish case.
6 Conclusions and Future Work
This paper has presented a series of experiments
in content evaluation in text summarization to as-
sess the value of content-based measures that do
not rely on the use of model summaries for com-
parison purposes. We have carried out exten-
sive experimentation with different summariza-
tion tasks drawing a clearer picture of tasks where
the measures could be applied. This paper makes
the following contributions:
? We have shown that if we are only interested
in ranking summarization systems according
to the content of their automatic summaries,
there are tasks where models could be sub-
stituted by the full document in the computa-
tion of the Jensen-Shannon divergence mea-
sure obtaining reliable rankings. However,
we have also found that the substitution of
models by full-documents is not always ad-
visable. We have found weak correlation
among different rankings in complex sum-
marization tasks such as the summarization
of biographical information and the summa-
1065
Measure ROUGE-1 p-value ROUGE-2 p-value ROUGE-2 p-value
JS 0.83 p < 0.002 0.66 p < 0.05 0.741 p < 0.01
JS2 0.80 p < 0.005 0.59 p < 0.05 0.68 p < 0.02
JS4 0.75 p < 0.010 0.52 p < 0.10 0.62 p < 0.05
JSM 0.85 p < 0.002 0.64 p < 0.05 0.74 p < 0.01
Table 7: Spearman system rank correlation of content-based measures with ROUGE in the RPM2 Cor-
pus (French)
rization of opinions about an ?entity?.
? We have also carried out large-scale exper-
iments in Spanish and French which show
positive medium to strong correlation among
system?s ranks produced by ROUGE and di-
vergence measures that do not use the model
summaries.
? We have also presented a new framework,
FRESA, for the computation of measures
based on Jensen-Shannon divergence. Fol-
lowing the ROUGE approach, FRESA imple-
ments word uni-grams, bi-grams and skip n-
grams for the computation of divergences.
The framework is being made available to the
community for research purposes.
Although we have made a number of contribu-
tions, this paper leaves many questions open that
need to be addressed. In order to verify correlation
between ROUGE and JS, in the short term we in-
tend to extend our investigation to other languages
and datasets such as Portuguese and Chinese for
which we have access to data and summarization
technology. We also plan to apply our evaluation
framework to the rest of the DUC and TAC sum-
marization tasks to have a full picture of the corre-
lations among measures with and without human
models. In the long term we plan to incorporate a
representation of the task/topic in the computation
of the measures.
Acknowledgements
We thank three anonymous reviewers for their
valuable and enthusiastic comments. Horacio
Saggion is grateful to the Programa Ramo?n y Ca-
jal from the Ministerio de Ciencia e Innovacio?n,
Spain and to a Comenc?a grant from Universitat
Pompeu Fabra (COMENC?A10.004). This work
is partially supported by a postdoctoral grant (Na-
tional Program for Mobility of Research Human
Resources; National Plan of Scientific Research,
Development and Innovation 2008-2011) given to
Iria da Cunha by the Ministerio de Ciencia e In-
novacio?n, Spain.
References
da Cunha, Iria, Leo Wanner, and M. Teresa Cabre?.
2007. Summarization of specialized discourse: The
case of medical articles in spanish. Terminology,
13(2):249?286.
Donaway, Robert L., Kevin W. Drummey, and
Laura A. Mather. 2000. A comparison of rank-
ings produced by summarization evaluation mea-
sures. In NAACL-ANLP 2000 Workshop on Au-
tomatic Summarization, pages 69?78, Morristown,
NJ, USA. ACL.
Fernandez, Silvia, Eric SanJuan, and Juan-Manuel
Torres-Moreno. 2007. Textual Energy of Associa-
tive Memories: performants applications of Enertex
algorithm in text summarization and topic segmen-
tation. In MICAI?07, pages 861?871.
Kullback, S. and R.A. Leibler. 1951. On information
and sufficiency. Annals of Mathematical Statistics,
22(1):79?86.
Lin, C.-Y. and E. Hovy. 2003. Automatic evaluation
of summaries using n-gram co-occurrence statistics.
In Proceedings of HLT-NAACL 2003, pages 71?78,
Morristown, NJ, USA. ACL.
Lin, Chin-Yew, Guihong Cao, Jianfeng Gao, and Jian-
Yun Nie. 2006. An information-theoretic approach
to automatic evaluation of summaries. In Confer-
ence on Human Language Technology Conference
of the North American Chapter of the Association
of Computational Linguistics, pages 463?470, Mor-
ristown, NJ, USA. ACL.
Lin, J. 1991a. Divergence measures based on the
shannon entropy. IEEE Transactions on Informa-
tion Theory, 37(145-151).
1066
Lin, Jianhua. 1991b. Divergence measures based on
the shannon entropy. IEEE Transactions on Infor-
mation theory, 37:145?151.
Lin, Chin-Yew. 2004. ROUGE: A Package for
Automatic Evaluation of Summaries. In Marie-
Francine Moens, Stan Szpakowicz, editor, Text
Summarization Branches Out: ACL-04 Workshop,
pages 74?81, Barcelona, Spain, July.
Louis, Annie and Ani Nenkova. 2009. Automati-
cally Evaluating Content Selection in Summariza-
tion without Human Models. In Conference on Em-
pirical Methods in Natural Language Processing,
pages 306?314, Singapore, August. ACL.
Mani, I., G. Klein, D. House, L. Hirschman, T. Firmin,
and B. Sundheim. 2002. Summac: a text summa-
rization evaluation. Natural Language Engineering,
8(1):43?68.
Nenkova, Ani and Rebecca Passonneau. 2004. Eval-
uating Content Selection in Summarization: The
Pyramid Method. In Proceedings of NAACL-HLT
2004.
Over, Paul, Hoa Dang, and Donna Harman. 2007. Duc
in context. Information Processing & Management,
43(6):1506?1520.
Owkzarzak, Karolina and Hoa Trang Dang. 2009.
Evaluation of automatic summaries: Metrics under
varying data conditions. In Proceedings of the 2009
Workshop on Language Generation and Summari-
sation (UCNLG+Sum 2009), pages 23?30, Suntec,
Singapore, August. ACL.
Papineni, K., S. Roukos, T. Ward, , and W. J. Zhu.
2002. BLEU: a method for automatic evaluation
of machine translation. In ACL?02: 40th Annual
meeting of the Association for Computational Lin-
guistics, pages 311?318.
Pastra, K. and H. Saggion. 2003. Colouring sum-
maries Bleu. In Proceedings of Evaluation Initia-
tives in Natural Language Processing, Budapest,
Hungary, 14 April. EACL.
Radev, Dragomir R., Simone Teufel, Horacio Sag-
gion, Wai Lam, John Blitzer, Hong Qi, Arda C?elebi,
Danyu Liu, and Elliott Dra?bek. 2003. Evaluation
challenges in large-scale document summarization.
In ACL, pages 375?382.
Saggion, H., D. Radev, S. Teufel, and W. Lam. 2002.
Meta-evaluation of Summaries in a Cross-lingual
Environment using Content-based Metrics. In Pro-
ceedings of COLING 2002, pages 849?855, Taipei,
Taiwan, August 24-September 1.
Spa?rck-Jones, Karen and Julia Rose Galliers, editors.
1996. Evaluating Natural Language Processing
Systems, An Analysis and Review, volume 1083 of
Lecture Notes in Computer Science. Springer.
Spiegel, S. and N.J. Castellan, Jr. 1998. Nonparamet-
ric Statistics for the Behavioral Sciences. McGraw-
Hill International.
Torres-Moreno, Juan-Manuel, Patricia Velz?quez-
Morales, and Jean-Guy Meunier. 2002. Condenss?
de textes par des me?thodes numr?iques. In JADT?02,
volume 2, pages 723?734, St Malo, France.
Vivaldi, Jorge, Iria da Cunha, Juan-Manuel Torres-
Moreno, and Patricia Vela?zquez-Morales. 2010.
Automatic summarization using terminological and
semantic resources. In LREC?10, volume 2,
page 10, Malta.
Yatsko, V.A. and T.N. Vishnyakov. 2007. A method
for evaluating modern systems of automatic text
summarization. Automatic Documentation and
Mathematical Linguistics, 41(3):93?103.
1067
Proceedings of the Fifth Law Workshop (LAW V), pages 1?10,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
On the Development of the RST Spanish Treebank 
Iria da Cunha Juan-Manuel Torres-Moreno Gerardo Sierra 
Institute for Applied 
Linguistics (UPF), Spain 
Laboratoire Informatique 
d?Avignon (UAPV), France 
Instituto de Ingenier?a (UNAM), 
Mexico 
Instituto de Ingenier?a 
(UNAM), Mexico 
Instituto de Ingenier?a (UNAM), 
Mexico 
gsierram@iingen.unam.
mx 
Laboratoire Informatique 
d?Avignon (UAPV), France 
?cole Polytechnique de Montr?al, 
Canada 
 
iria.dacunha@upf.edu juan-manuel.torres@univ-
avignon.fr 
 
 
 
Abstract 
In this article we present the RST Spanish 
Treebank, the first corpus annotated with 
rhetorical relations for this language. We 
describe the characteristics of the corpus, 
the annotation criteria, the annotation 
procedure, the inter-annotator agreement, 
and other related aspects. Moreover, we 
show the interface that we have developed 
to carry out searches over the corpus? 
annotated texts. 
1 Introduction 
The Rhetorical Structure Theory (RST) (Mann and 
Thompson, 1988) is a language independent theory 
based on the idea that a text can be segmented into 
Elementary Discourse Units (EDUs) linked by 
means of nucleus-satellite or multinuclear 
rhetorical relations. In the first case, the satellite 
gives additional information about the other one, 
the nucleus, on which it depends (ex. Result, 
Condition, Elaboration or Concession). In the 
second case, several elements, all nuclei, are 
connected at the same level, that is, there are no 
elements dependent on others and they all have the 
same importance with regard to the intentions of 
the author of the text (ex. Contrast, List, Joint or 
Sequence). The rhetorical analysis of a text by 
means of RST includes 3 phases: segmentation, 
detection of relations and building of hierarchical 
rhetorical trees. For more information about RST 
we recommend the original article of Mann and 
Thompson (1988), the web site of RST1 and the 
RST review by Taboada and Mann (2006a). 
RST has been used to develop several 
applications, like automatic summarization, 
information extraction (IE), text generation, 
question-answering, automatic translation, etc. 
(Taboada and Mann, 2006b). Nevertheless, most of 
these works have been developed for English, 
German or Portuguese. This is due to the fact that 
at present corpora annotated with RST relations are 
available only for these languages (for English: 
Carlson et al, 2002, Taboada and Renkema, 2008; 
for German: Stede, 2004; for Portuguese: Pardo et 
al., 2008) and there are automatic RST parsers for 
two of them (for English: Marcu, 2000; for 
Portuguese: Pardo et al, 2008) or automatic RST 
segmenters (for English: Tofiloski et al, 2009). 
Scientific community working on RST applied to 
Spanish is very small. For example, Bouayad-Agha 
et al (2006) apply RST to text generation in 
several languages, Spanish among them. Da Cunha 
et al (2007) develop a summarization system for 
medical texts in Spanish based on RST. Da Cunha 
and Iruskieta (2010) perform a contrastive analysis 
of Spanish and Basque texts. Romera (2004) 
analyzes coherence relations by means of RST in 
spoken Spanish. Taboada (2004) applies RST to 
analyze the resources used by speakers to elaborate 
conversations in English and Spanish.  
We consider that it is necessary to build a 
Spanish corpus annotated by means of RST. This 
corpus should be useful for the development of a 
rhetorical parser for this language and several other 
applications related to computational linguistics, 
like those developed for other languages 
                                                           
1 http://www.sfu.ca/rst/index.html 
1
(automatic translation, automatic summarization, 
IE, etc.). And that is what we pretend to achieve 
with our work. We present the development of the 
RST Spanish Treebank, the first Spanish corpus 
annotated by means of RST. 
In Section 2, we present the state of the art 
about RST annotated corpora. In Section 3, we 
explain the characteristics of the RST Spanish 
Treebank. In Section 4, we show the search 
interface we have developed. In Section 5, we 
establish some conclusions and future work. 
2 State of the Art 
The most known RST corpus is the RST Discourse 
Treebank, for English (Carlson et al, 2002a, 
2002b). It includes 385 texts of the journalistic 
domain, extracted from the Penn Treebank 
(Marcus et al, 1993), such as cultural reviews, 
editorials, economy articles, etc. 347 texts are used 
as a learning corpus and 38 texts are used as a test 
corpus. It contains 176,389 words and 21,789 
EDUs. 13.8% of the texts (that is, 53) were 
annotated by two people with a list of 78 relations. 
For annotation, the annotation tool RSTtool 2 
(O'Donnell, 2000) was used, with some 
adaptations. The principal advantages of this 
corpus stand on the high number of annotated texts 
(for the moment it is the biggest RST corpus) and 
the clarity of the annotation method (specified in 
the annotation manual by Carlson and Marcu, 
2001). However, some drawbacks remain. The 
corpus is not free, it is not on-line and it only 
includes texts of one domain (journalistic).  
For English there is also the Discourse 
Relations Reference Corpus (Taboada and 
Renkema, 2008). This corpus includes 65 texts 
(each one tagged by one annotator) of several types 
and from several sources: 21 articles from the Wall 
Street Journal extracted from the RST Discourse 
Treebank, 30 movies and books? reviews extracted 
from the epinions.com website, and 14 diverse 
texts, including letters, webs, magazine articles, 
newspaper editorials, etc. The tool used for 
annotation was also the RSTtool. The advantages 
of this corpus are that it is free and on-line, and it 
includes texts of several types and domains. The 
disadvantages are that the amount of texts is not 
very high, the annotation methodology is not 
                                                           
2 http://www.wagsoft.com/RSTTool/ 
specified and it does not include texts annotated by 
several people. 
Another well-known corpus is the Potsdam 
Commentary Corpus, for German (Stede, 2004; 
Reitter and Stede, 2003). This corpus includes 173 
texts on politics from the on-line newspaper 
M?rkische Allgemeine Zeitung. It contains 32,962 
words and 2,195 sentences. It is annotated with 
several data: morphology, syntax, rhetorical 
structure, connectors, correference and informative 
structure. Nevertheless, only a part of this corpus 
(10 texts), which the authors name "core corpus", 
is annotated with all this information. The texts 
were annotated with the RSTtool. This corpus has 
several advantages: it is annotated at different 
levels (the annotation of connectors is especially 
interesting); all the texts were annotated by two 
people (with a previous RST training phase); it is 
free for research purposes, and there is a tool for 
searching over the corpus (although it is not 
available on-line). The disadvantages are: the 
genre and domain of all the texts are the same, the 
methodology of annotation was quite intuitive 
(without a manual or specific criteria) and the 
inter-annotator agreement is not given. 
For Portuguese, there are 2 corpora, built in 
order to develop a rhetorical parser (Pardo et al, 
2008). The first one, the CorpusTCC (Pardo et al, 
2008), was used as learning corpus for detection of 
linguistic patterns indicating rhetorical relations. It 
contains 100 introduction sections of computer 
science theses (53,000 words and 1,350 sentences). 
To annotate the corpus a list of 32 rhetorical 
relations was used. The annotation manual by 
Carlson and Marcu (2001) was adapted to 
Portuguese. The annotation tool was the ISI RST 
Annotation Tool3 , an extension of the RSTtool. 
The advantages of this corpus are: it is free, it 
contains an acceptable number of texts and words 
and it follows a specific annotation methodology. 
The disadvantage is: it only includes texts of one 
genre and domain, only annotated by one person. 
The second one, Rhetalho (Pardo and Seno, 
2005), was used as reference corpus for the parser 
evaluation. It contains 50 texts: 20 introduction 
sections and 10 conclusion sections from computer 
science scientific articles, and 20 texts from the on-
line newspaper Folha de S?o Paulo (7 from the 
Daily section, 7 from the World section and 6 from 
                                                           
3 http://www.isi.edu/~marcu/discourse/ 
2
the Science section). It includes approximately 
5,000 words. The relations and the annotation tool 
are the same as those used in the CorpusTCC. The 
advantages of this corpus are that it is free, it was 
annotated by 2 people (they both were RST experts 
and followed an annotation manual) and it contains 
texts of several genres and domains. The main 
disadvantage is the scarce amount of texts. 
The Penn Discourse Treebank (Rashmi et al, 
2008)f for English includes texts annotated with 
information related to discourse structure and 
semantics (without a specific theoretical approach). 
Its advantages are: its big size (it contains 40,600 
annotated discourse relations) allows to apply 
machine learning, and the discourse annotations 
are aligned with the syntactic constituency 
annotations of the Penn Treebank. Its limitations 
are: dependencies across relations are not marked, 
it only includes texts of the journalistic domain, 
and it is not free. Although there are several 
corpora annotated with discourse relations, there is 
not a corpus of this type for Spanish. 
3 The RST Spanish Treebank  
As Sierra (2008) states, a corpus consists of a 
compilation of a set of written and/or spoken texts 
sharing some characteristics, created for certain 
investigation purposes. According to Hovy (2010), 
we use 7 core questions in corpus design, detailed 
in the next subsections. 
3.1 Selecting a Corpus 
For the RST Spanish Treebank, we wanted to 
include short texts (finally, the average is 197 
words by text; the longest containing 1,051 words 
and the shortest, 25) in order to get a best on-line 
visualization of the RST trees. Moreover, in the 
first stage of the project, we preferred to select 
specialized texts of very different areas, although 
in the future we plan to include also non-
specialized texts (ex. blogs, news, websites) in 
order to guarantee the representativity of the 
corpus. We did not find a pre-existing Spanish 
corpus with these characteristics, so we decided to 
build our own corpus. Following Cabr? (1999), we 
consider that a text is specialized if it is written by 
a professional in a given domain. According to this 
work, specialized texts can be divided in three 
levels: high (both the author and the potential 
reader of the text are specialists), average (the 
author of the text is a specialist, and the potential 
reader of that text is a student or someone 
interested in or possessing some prior knowledge 
about the subject) and low (the author of the text is 
a specialist, and the potential reader is the general 
public). The RST Spanish Treebank includes 
specialized texts of the three mentioned levels: 
high (scientific articles, conference proceedings, 
doctoral theses, etc.), average (textbooks) and low 
(articles and reports from popular magazines, 
associations? websites, etc.). The texts have been 
divided in 9 domains (some of them including 
subdivisions): Astrophysics, Earthquake 
Engineering, Economy, Law, Linguistics (Applied 
Linguistics, Language Acquisition, PLN, 
Terminology), Mathematics (Primary Education, 
Secondary Education, Scientific Articles), 
Medicine (Administration of Health Services, 
Oncology, Orthopedy), Psychology and Sexuality 
(Clinical Perspective, Psychological Perspective). 
The size of a corpus is also a polemic question. 
If the corpus is developed for machine learning, its 
size will be enough when the application we want 
to develop obtains acceptable percentages of 
precision and recall (in the context of that 
application). Nevertheless, if the corpus is built 
with descriptive purposes, it is difficult to 
determine the corpus size. In the case of a corpus 
annotated with rhetorical relations, it is even more 
difficult, because there are various factors 
involved: EDUs, SPANs (that is, a group of related 
EDUs), nuclearity and relations. In addition, 
relations are multiple (we use 28). As Hovy (2010: 
13) mentions, one of the most difficult phenomena 
to annotate is the discourse structure. Our corpus 
contains 52,746 words and 267 texts. Table 1 
includes RST Spanish Treebank statistics in terms 
of texts, words, sentences and EDUs. 
 
 Texts Words Sentences EDUs 
Learning corpus 183 41,555 1,759 2,655 
Test corpus  84 11,191 497 694 
Total corpus  267 52,746 2,256 3,349 
 
Table 1: RST Spanish Treebank statistics 
 
To increase the linear performance of a 
statistical method, it is necessary that the training 
corpus size grows exponentially (Zhao et al, 
2010). However, the RST Spanish Treebank is not 
designed only to use statistical methods; we think 
it will be useful to employ symbolic or hybrid 
3
algorithms (combining symbolic and statistical 
methods). Moreover, this corpus will be dynamic, 
so we expect to have a bigger corpus in the future, 
useful to apply machine learning methods. 
If we measure the corpus size in terms of words 
or texts, we can take as a reference the other RST 
corpora. Nevertheless, as Sierra states (2008), it is 
?absurd? to try to build an exhaustive corpus 
covering all the aspects of a language. On the 
contrary, the linguist looks for the 
representativeness of the texts, that is, tries to 
create a sample of the studied language, selecting 
examples which represent the linguistic reality, in 
order to analyze them in a pertinent way. In this 
sense and in the frame of this work, we consider 
that the size will be adequate if the rhetorical trees 
of the corpus include a representative number of 
examples of rhetorical relations, at least 20 
examples of each one (taking into account that the 
corpus contains 3115 relations, we consider that 
this quantity is acceptable; however, we expect to 
have even more examples when the corpus grows).  
Table 2 shows the number of examples of each 
relation currently included into the RST Spanish 
Treebank (N-S: nucleus-satellite relation; N-N: 
multinuclear relation). As it can be observed, it 
contains more than 20 examples of most  of the  
relations. The exceptions are the nucleus-satellite 
relations of Enablement, Evaluation, Summary,  
Otherwise and  Unless, and the multinuclear 
relations of Conjunction and Disjunction, because 
it is not so usual to find these rhetorical relations in 
the language, in comparison with others. Hovy 
(2010: 128) states that, given the lack of examples 
in the corpus, there are 2 possible strategies: a) to 
leave the corpus as it is, with few or no examples 
of some cases (but the problem will be the lack of 
training examples for machine learning systems), 
or b) to add low-frequency examples artificially to 
?enrich? the corpus (but the problem will be the 
distortion of the native frequency distribution and 
perhaps the confusion of machine learning 
systems). In the current state of our project, we 
have chosen the first option. We think that, 
including specialized texts in a second stage, we 
will get more examples of these less common 
relations. If we carry out a more granulated 
segmentation maybe we could obtain more 
examples; however, we wanted to employ the 
segmentation criteria used to develop the Spanish 
RST discourse segmenter (da Cunha et al, 2011). 
 
Quantity Relation Type 
N? % 
Elaboration N-S 765 24.56 
Preparation N-S 475 15.25 
Background N-S 204 6.55 
Result N-S 193 6.20 
Means N-S 175 5.62 
List N-N 172 5.52 
Joint N-N 160 5.14 
Circumstance N-S 140 4.49 
Purpose N-S 122 3.92 
Interpretation N-S 88 2.83 
Antithesis N-S 80 2.57 
Cause N-S 77 2.47 
Sequency N-N 74  2.38 
Evidence N-S 59 1.89 
Contrast N-N 58 1.86 
Condition N-S 53 1.70 
Concession N-S 50 1.61 
Justification N-S 39 1.25 
Solution N-S 32 1.03 
Motivation N-S 28 0.90 
Reformulation N-S 22 0.71 
Otherwise N-S 3 0.10 
Conjunction N-N 11 0.35 
Evaluation N-S 11 0.35 
Disjunction N-N 9 0.29 
Summary N-S 8 0.26 
Enablement  N-S 5 0.16 
Unless N-S 2 0.06 
 
Table 2: Rhetorical relations in RST Spanish Treebank 
 
3.2 Instantiating the Theory 
Our segmentation and annotation criteria are very 
similar to the original ones used by Mann and 
Thompson (1988) for English, and by da Cunha 
and Iruskieta (2010) for Spanish. We also explore 
the annotation manual for English by Carlon and 
Marcu (2001). Though we use some of their 
postulates, we think that their analysis is too 
meticulous in some aspects. Because of this, we 
consider that it is not adjusted to our interest, 
which is the finding of the simplest and most 
objective annotation method, orientated to the 
4
future development of a rhetorical parser for 
Spanish. To sum up, our segmentation criteria are:  
 
a) All the sentences of the text are segmented as 
EDUs (we consider that a sentence is a textual 
passage between a period and another period, a 
semicolon, a question mark or an exclamation 
point; texts? titles are also segmented). Exs.4 
 
[?stas son las razones fundamentales que motivaron 
este trabajo.] 
      [These are the fundamental reasons which motivated this 
work.] 
[Estudio de caso ?nico sobre violencia conyugal] 
      [Study of a case on conjugal violence] 
 
b) Intra-sentence EDUs are segmented, using the 
following criteria: 
 
b1) An intra-sentence EDU has to include a finite 
verb, an infinitive or a gerund. Ex.  
 
[Siendo una variante de la eliminaci?n Gaussiana,] 
[posee caracter?sticas did?cticas ventajosas.] 
      [Being a variant of Gaussian elimination,] [it possesses 
didactic profitable characteristics.] 
 
b2) Subject/object subordinate clauses or 
substantive sentences are not segmented. Ex.  
 
[Se muestra que el modelo discreto en diferencias finitas 
es convergente y que su realizaci?n se reduce a resolver 
una sucesi?n de sistemas lineales tridiagonales.] 
      [It appears that the discreet model in finite differences is 
convergent and that its accomplishment is to solve a 
succession of tridiagonal linear systems.] 
 
b3) Subordinate relative clauses are not segmented. 
Ex. 
 
[Durante el proceso, que utiliza solo aritm?tica entera, 
se obtiene el determinante de la matriz de coeficientes 
del sistema, sin necesidad de c?lculos adicionales.] 
       [During the process, which only uses entire arithmetic, the 
determinant of the system coefficient matrix is obtained, 
without  additional calculations.] 
 
b4) Elements in parentheses are only segmented if 
they follow the criterion b1. Ex.  
[Este a?o se cumple el bicentenario del nacimiento de 
Niels (Nicol?s, en nuestro idioma) Henrik Abel.] 
       [This year is the bicentenary of Niels's birth (Nicol?s, in 
our language) Henrik Abel.]     
b5) Embedded units are segmented by means of 
the non-relation Same-Unit proposed by Carlon 
and Marcu (2001). Figure 1 shows this structure. 
 
[En d?cadas precedentes se ha puesto de manifiesto,] [y 
as? lo han atestiguado muchos investigadores de la 
                                                           
4 Spanish examples were extracted from the corpus. English 
translations are ours. 
terminolog?a cient?fica serbia,] [una tendencia a 
importar pr?stamos del ingl?s.]  
        [In previous decades it has been shown,] [and it has been 
testified by many researchers of the scientific Serbian 
terminology,] [a trend to import loanwords from English.]  
 
 
Figure 1: Example of the non-relation Same-Unit 
3.3 Designing the Interface 
The annotation tool used in this work is the 
RSTtool, since it is free and easy to use. Therefore, 
we preferred to use it instead of designing a new 
one. Nevertheless, we have designed an on-line 
interface to include the corpus and to carry out 
searches over it (see Section 4). 
3.4 Selecting and Training the Annotators 
With regard to the corpus annotators, we have a 
team of 10 people (last year Bachelor?s degree 
students, Master?s degree students and PhDs) 5 . 
Before the annotation, they took a RST course of 6 
months (100 hours), where the segmentation and 
annotation methodology used for the development 
of the RST Spanish Treebank was explained.6 We 
called this period "training phase". The course had 
a theoretical and a practical part. In the theoretical 
part, some criteria with regard to the 3 phases of 
rhetorical analysis (segmentation, detection of 
relations, and rhetorical trees building) were given 
to annotators. In the practical part, firstly, it was 
explained how to use the RSTtool. Secondly, 
annotators extracted several texts from the web, 
following their personal interests, as for example, 
music, video games, cookery or art webs. They 
segmented those texts, using the established 
segmentation criteria. Once segmented, all the 
doubts and problematic examples were discussed, 
and they tried to get an agreement on the most 
complicated cases. Thirdly, the relations were 
                                                           
5  We thank annotators (Adriana Valerio, Brenda Castro, 
Daniel Rodr?guez, Ita Cruz, Jessica M?ndez, Josu? Careaga, 
Luis Cabrera, Marina Fomicheva and Paulina De La Vega) 
and interface developers (Luis Cabrera and Juan Rolland). 
6 This course was given in the framework of a last-year subject 
in the Spanish Linguistics Degree at UNAM (Mexico City).  
5
analyzed (using a given relations list) and, once 
again, annotators discussed the difficult cases. 
After the discussion, texts were re-annotated to 
verify if the difficulties were solved. This process 
was doubly interesting, since it helped to create 
common criteria for the annotation of the final 
corpus and to define the annotation criteria more 
clearly and consensually, in order to include them 
in the RST Spanish Treebank annotation manual. 
Once annotators agreed on the most difficult cases, 
we consider that the training phase finished. 
3.5 Designing and Managing the Annotation 
Procedure 
We start from the following annotation definition:  
 
Annotation (?tagging?) is the process of adding new 
information into source material by humans 
(annotators) or suitably trained machines. [...]. The 
addition process usually requires some sort of 
mental decision that depends both on the source 
material and on some theory or knowledge that the 
annotator has internalized earlier. (Hovy, 2010: 6) 
 
Exactly, after our annotators internalized the 
theory and annotation criteria during the training 
phase, the "annotation phase" of the final texts 
included in the RST Spanish Treebank started. In 
this phase, the annotation tasks were assigned to 
annotators (the number of texts assigned to each 
annotator was different, depending on their 
availability). They were asked to carry out the 
annotation individually and without questions 
among them. We calculated that the average time 
to carry out the annotation of one text was between 
15 minutes and 1 hour. This time difference is due 
to the fact that the corpus includes both short and 
long texts. The annotation process is the following: 
once a text is segmented, rhetorical relations 
between EDUs are annotated. First, EDUs inside 
the same sentence are annotated in a binary way. 
Second, sentences inside the same paragraph are 
linked. Finally, paragraphs are linked.  
Hovy (2010) states that it is difficult to 
determine if, for the same money (we add ?for the 
same time?), it is better to double-annotate less, or 
to single-annotate more. As he explains, Dligach et 
al. (2010) made an experiment with OntoNotes 
(Pradhan et al, 2007) verb sense annotation. The 
result was that, assuming the annotation is stable 
(that is, inter-annotator agreement is high), it is 
better to annotate more, even with only one 
annotator. The problem with RST annotation is 
that there are so many categories to annotate, that 
is very difficult to obtain a stable annotation. 
Therefore, we consider it is necessary to have at 
least some texts double-annotated (or even triple-
annotated), in order to have an adequate discourse 
corpus. This is the reason why, following the RST 
Discourse Treebank methodology, we use some 
texts as learning corpus and some others (from the 
Mathematics, Psychology and Sexuality domains) 
as test corpus: 69% (183 texts) and 31% (84 texts), 
respectively. The texts of the learning corpus were 
annotated by 1 person, whereas the texts of the test 
corpus were annotated by 2 people. 
3.6 Validating Results 
Da Cunha and Iruskieta (2010) measure inter-
annotator agreement by using the RST trees 
comparison methodology by Marcu (2000). This 
methodology evaluates the agreement on 4 
elements (EDUs, SPANs, Nuclearity and 
Relations), by means of precision and recall 
measures (an annotation with regard to the other 
one). Following this methodology, we have 
measured inter-annotator agreement over the test 
corpus. We employ an on-line automatic tool for 
RST trees comparison, RSTeval (Mazeiro and 
Pardo, 2009), where Marcu?s methodology has 
been implemented (for 4 languages: English, 
Portuguese, Spanish and Basque). We know that 
there are some other ways to measure agreement, 
such as Cohen's kappa (Cohen, 1960) or Fleiss's 
kappa (Fleiss, 1971), for example. Nevertheless, 
we consider that Marcu's methodology (2000) is 
suitable to compare adequately 2 annotations of the 
same original text, because it has been designed 
specifically for this task.  
For each trees pair from the test corpus, 
precision and recall were measured separately. 
Afterwards, all those individual results were put 
together to obtain general results. Table 3 shows 
global results for the 4 categories. The category 
with more agreement was EDUs (recall: 91.04% / 
precision: 87.20%), that is, segmentation. This 
result was expected, since the segmentation criteria 
given to the annotators were quite precise and the 
possibility of mistake was low. The lowest 
agreement was obtained for the category Relations 
(recall: 78.48% / precision: 76.81%). This result is 
lower than the other, but we think it is acceptable. 
In the RST Discourse Treebank the trend was 
similar to the one detected in our corpus: the 
6
highest agreement is obtained at the segmentation 
level and the lowest at the relations level. 
 
 
Category Precision Recall 
EDUs 87.20% 91.04% 
SPANs 86% 87.31% 
Nuclearity 82.46% 84.66% 
Relations 76.81% 78.48% 
 
Table 3: Inter-annotator agreement 
 
 
Precision and recall have not been calculated 
with respect to a gold standard because it does not 
exist for Spanish. Our future aim is to reach a 
consensus on the annotation of the test corpus 
(using an external "judge"), in order to establish a 
set of texts considered as a preliminary gold 
standard for this language. We consider that the 
annotations have quality at present, because inter-
annotator agreement is quite high; however, this 
consensus could solve the typical annotation 
mistakes we have detected or some ambiguities. 
We have analyzed the main discrepancy reasons 
between annotators. With regard to the 
segmentation, the main one was human mistake; 
ex. segmenting EDUs without a verb (one 
annotator segmented the following passage into 2 
EDUs because she detected a Means relation, but 
the second EDU does not include any verb): 
 
[Adem?s estudiamos el desarrollo de criterios para 
determinar si un semigrupo dado tiene dicha propiedad ] 
[mediante el estudio de desigualdades de curvatura-
dimensi?n. ]  
      [We also study the development of tests in order to 
determine if a given semi group has this property] [by means 
of curvature-dimension inequalities.]  
 
The second reason was that in the manual some 
aspects were not explained in detail. For example, 
if a substantive sentence or a direct/object clause 
(which must not be segmented, according to the 
point b2) includes two coordinated clauses, these 
must not be segmented either. Thus, we found 
some erroneous segmentations. For example: 
 
[Los hombres adultos tienen miedo de fracasar] [y no 
cumplir con el rol masculino de ser proveedores del 
hogar y de proteger a su familia.]  
      [Adult men are scared to fail] [and not to fulfill the 
masculine role of being the suppliers of the home and to 
protect their family.]  
 
This kind of mistakes allowed us to refine our 
segmentation manual a posteriori. In the future, we 
will ask the test corpus annotators to make a new 
annotation of the texts, using the refined manual, in 
order to check if the agreement increases, in the 
same way as the RST Discourse Treebank. 
With regard to rhetorical annotations, we 
detected 2 main reasons of inter-annotator 
disagreement. The first one was the ambiguity of 
some relations and their corresponding connectors; 
for example, Justification-Reason, Antithesis-
Concession or Circumstance-Means relations, like 
in the following passage (in Spanish, ?al? may 
indicate time or manner): 
 
[Los ni?os aprenden matem?ticas] [al resolver 
problemas.] 
      [Children learn mathematics] [when solving problems.] 
 
The second one is due to differences between 
annotators when determining nuclearity. For 
example, in the following passage, one annotator 
marked Background and the other one Elaboration: 
 
[Qued? un hueco en la pared de 60 x 
1.20cm.]S_Background [Norma y Andr?s quieren 
colocar en el hueco una pecera. ]N_Background 
 
[Qued? un hueco en la pared de 60 x 
1.20cm.]N_Elaboration [Norma y Andr?s quieren 
colocar en el hueco una pecera. ]S_Elaboration 
      [A hole of 60 x 1.20 cm remained in the wall.] [Norma and 
Andr?s want to place a fish tank in the hole.]  
 
It is easier to solve segmentation disagreement 
than relations disagreement, since in this case 
annotator subjectivity is more evident; we must 
consider how to refine our manual in this sense. 
3.7 Delivering and Maintaining the Product 
Hovy (2010) mentions some technical issues 
regarding these points: licensing, distribution, 
maintenance and updates. With regard to licensing 
and distribution, the RST Spanish Treebank will be 
free for research purposes. We have a data 
manager responsible for maintenance and updates.  
The description of the annotated corpus is also 
a very important issue (Ide and Pustejovsky, 2010). 
It is important to provide a high level description 
of the corpus, including the theoretical framework, 
the methodology (annotators, annotation manual 
and tool, agreement, etc.), the means for resource 
maintenance, the technical aspects, the project 
leader, the contact, the team, etc. The RST Spanish 
Treebank includes all this detailed information. 
XML (with a DTD) has been used, in order the 
corpus can be reused for several aplications. In the 
future, we plan to use the standard XCES. 
7
To know more about resources development, 
linguistic annotation or inter-annotator agreement, 
we recommend: Palmer et al (on-line), Palmer and 
Xue (2010), and Artstein and Poesio (2008). 
4 The Search Interface of the RST 
Spanish Treebank 
The RST Spanish Treebank interface is freely 
available on-line7. It allows the visualization and 
downloading of all the texts in txt format, with 
their corresponding annotated trees in RSTtool 
format (rs3), as well as in image format (jpg). Each 
text includes its title, its reference, its web link (if 
it is an on-line text) and its number of words. The 
interface shows texts by areas and allows the user 
to select a subcorpus (including individual files or 
folders containing several files). The selected 
subcorpus can be saved on local disk (generating a 
xml file) for future analyses.  
The interface includes a statistical tool which 
allows obtaining statistics of rhetorical relations in 
a subcorpus selected by the user. The RSTtool also 
offers this option but it can be only used for one 
text. We consider that it is more useful for the user 
to obtain statistics from various texts, in order to 
get significant statistical results. As the RSTtool, 
our tool allows to count the multinuclear relations 
in two ways: a) one unit for each detected 
multinuclear relation, and b) one unit for each 
detected nucleus. If we use b), the statistics of the 
multinuclear relations of Table 2 are higher: List 
(864), Joint (537), Sequence (289), Contrast (153), 
Conjunction (28) and Disjunction (24).  
We are developing another tool, aimed to 
extract information from the annotated texts, which 
we will soon include into the interface. This tool 
will allow to the user to select a subcorpus and to 
extract from it the EDUs corresponding to the 
rhetorical relations selected, like a multidocument 
specialized summarizer guided by user's interests.  
The RST Spanish Treebank interface also 
includes a screen which permits the users to send 
their own annotated texts. Our aim is for the RST 
Spanish Treebank to become a dynamic corpus, in 
constant evolution, being increased with texts 
annotated by users. This has a double advantage 
since, on the one hand, the corpus will grow and, 
on the other hand, users will profit from the 
                                                           
7 http://www.corpus.unam.mx/rst/ 
interface's applications, using their own 
subcorpora. The only requirement is to use the 
relations and the segmentation and annotation 
criteria of our project. Once the texts are sent, the 
RST Spanish Treebank data manager will verify if 
the annotation corresponds to these criteria. 
5 Conclusions and Future Work 
We think that this work means an important step 
for the RST research in Spanish, and that the RST 
Spanish Treebank will be useful to carry out 
diverse researches about RST in this language, 
from a descriptive point of view (ex. analysis of 
texts from different domains or genres) and an 
applied point of view (development of discourse 
parsers and NLP applications, like automatic 
summarization, automatic translation, IE, etc.).  
For the moment the corpus' size is acceptable 
and, though the percentage of double-annotated 
texts is not very high, we think that having 10 
annotators (using the same annotation manual) 
avoids the bias of only one annotator. In addition, 
the corpus includes texts of diverse domains and 
genres, which provides us with a heterogeneous 
Spanish corpus. Moreover, the corpus interface 
that we have designed allows the user to select a 
subcorpus and to analyze it statistically. In 
addition, we think that it is essential to release a 
free corpus, on-line and dynamic, that is, in 
continuous growth. Nevertheless, we are conscious 
that our work still has certain limitations, which we 
will try to solve in the future. In the short term, we 
have 5 aims:  
 
a) To add one more annotator for the test corpus 
and to measure inter-annotator agreement. 
b) To use more agreement measures, like kappa. 
c) To reach a consensus on the annotation of the 
test corpus, in order to establish a set of texts 
considered as a preliminary gold standard. 
d) To finish and to evaluate the IE tool. 
e) To analyze the corpus to extract linguistic 
patterns for the automatic relations detection. 
 
In the long term, we consider other aims: 
 
f) To increase the corpus, by adding non-
specialized texts, and new domains and genres. 
g) To annotate all the texts by 3 people, to get a 
representative gold-standard for Spanish (this aim 
will depend on the funding of the project). 
 
8
References  
Ron Artstein, and Massimo Poesio. 2008. Survey 
Article: Inter-Coder Agreement for Computational 
Linguistics. Computational Linguistics, 34(4):555-
596. 
Nadjet Bouayad-Agha, Leo Wanner, and Daniel 
Nicklass. 2006. Discourse structuring of dynamic 
content. Procesamiento del lenguaje natural, 37:207-
213. 
M. Teresa Cabr? (1999). La terminolog?a: 
representaci?n y comunicaci?n. Barcelona: IULA-
UPF. 
Lynn Carlson and Daniel Marcu. 2001. Discourse 
Tagging Reference Manual. ISI Technical Report 
ISITR-545. Los ?ngeles: University of Southern 
California. 
Lynn Carlson, Daniel Marcu, and Mary Ellen 
Okurowski. 2002a. RST Discourse Treebank. 
Pennsylvania: Linguistic Data Consortium. 
Lynn Carlson, Daniel Marcu, and Mary Ellen 
Okurowski. 2002b. Building a Discourse-Tagged 
Corpus in the Framework of Rhetorical Structure 
Theory. In Proceedings of the 2nd SIGDIAL 
Workshop on Discourse and Dialogue, Eurospeech 
2001. 
Jacob Cohen. 1960. A coefficient of agreement for 
nominal scales. Educational and Psychological 
Measurement, 20(1):37-46 
Iria da Cunha, Eric SanJuan, Juan-Manuel Torres-
Moreno, Marina Lloberes, and Irene Castell?n. 2010. 
Discourse Segmentation for Spanish based on 
Shallow Parsing. Lecture Notes in Computer 
Science, 6437:13-23.  
Iria da Cunha, and Mikel Iruskieta. 2010. Comparing 
rhetorical structures of different languages: The 
influence of translation strategies. Discourse Studies, 
12(5):563-598.  
Iria da Cunha, Leo Wanner, and M. Teresa Cabr?. 2007. 
Summarization of specialized discourse: The case of 
medical articles in Spanish. Terminology, 13(2):249-
286.  
Dmitriy Dligach, Rodney D. Nielsen, and Martha 
Palmer. 2010. To Annotate More Accurately or to 
Annotate More. In Proceedings of the 4th Linguistic 
Annotation Workshop (LAW-IV). 48th Annual 
Meeting of the Association for Computational 
Linguistics. 
Joseph L. Fleis. 1971. Measuring nominal scale 
agreement among many raters. Psychological 
Bulletin, 76(5):378-382. 
Eduard Hovy. 2010. Annotation. A Tutorial. Presented 
at the 48th Annual Meeting of the Association for 
Computational Linguistics. 
Nancy Ide and Pustejovsky, J. (2010). What Does 
Interoperability Mean, anyway? Toward an 
Operational Definition of Interoperability. In 
Proceedings of the Second International Conference 
on Global Interoperability for Language Resources 
(ICGL 2010).  
William C. Mann, and Sandra A. Thompson. 1988. 
Rhetorical structure theory: Toward a functional 
theory of text organization. Text, 8(3):243-281. 
Daniel Marcu. 2000. The Theory and Practice of 
Discourse Parsing Summarization. Massachusetts: 
Institute of Technology. 
Mitchell P. Marcus, Beatrice Santorini, Mary A. 
Marcinkiewicz. 1993. Building a large annotated 
corpus of English: the Penn Treenbank. 
Computational Linguistics, 19(2):313-330. 
Michael O?Donnell. 2000. RSTTOOL 2.4 ? A markup 
tool for rhetorical structure theory. In Proceedings of 
the International Natural Language Generation 
Conference. 253-256. 
Martha Palmer, and Nianwen Xue. 2010. Linguistic 
Annotation. Handbook of Computational Linguistics 
and Natural Language Processing.  
Martha Palmer, Randee Tangi, Stephanie Strassel, 
Christiane Fellbaum, and Eduard Hovy (on-line). 
Historical Development and Future Directions in 
Data Resource Development. MINDS report. 
http://www-nlpir.nist.gov/MINDS/FINAL/data.web.pdf 
Sameer Pradhan, Eduard Hovy, Mitch Marcus, Martha 
Palmer, Lance Ramshaw, Ralph Weischedel. 2007. 
OntoNotes: A Unified Relational Semantic 
Representation. In Proceedings of the First IEEE 
International Conference on Semantic Computing 
(ICSC-07). 
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni 
Miltsakaki, Livio Robaldo, Aravind Joshi, and 
Bonnie Webber. 2008. The Penn Discourse Treebank 
2.0. In Proceedings of the 6th International 
Conference on Language Resources and Evaluation 
(LREC 2008). 
David Reitter, and Mandred Stede. 2003. Step by step: 
underspecified markup in incremental rhetorical 
analysis. In Proceedings of the 4th International 
9
Workshop on Linguistically Interpreted Corpora 
(LINC-03). 
Magdalena Romera. 2004. Discourse Functional Units: 
The Expression of Coherence Relations in Spoken 
Spanish. Munich: LINCOM. 
Thiago Alexandre Salgueiro Pardo, and Lucia Helena 
Machado Rino. 2001. A summary planner based on a 
three-level discourse model. In Proceedings of 
Natural Language Processing Pacific Rim 
Symposium. 533-538. 
Thiago Alexandre Salgueiro Pardo, Maria das Gra?as 
Volpe Nunes, and Lucia Helena Machado Rino. 
2008. DiZer: An Automatic Discourse Analyzer for 
Brazilian Portuguese. Lecture Notes in Artificial 
Intelligence, 3171:224-234.  
Thiago Alexandre Salgueiro Pardo, and Eloize Rossi 
Marques Seno. 2005. Rhetalho: um corpus de 
refer?ncia anotado retoricamente. In Anais do V 
Encontro de Corpora. S?o Carlos-SP, Brasil. 
Gerardo Sierra. 2008. Dise?o de corpus textuales para 
fines ling??sticos. In Proceedings of the IX Encuentro 
Internacional de Ling??stica en el Noroeste 2. 445-
462. 
Manfred Stede. 2004. The Potsdam commentary corpus. 
In Proceedings of the Workshop on Discourse 
Annotation, 42nd Meeting of the Association for 
Computational Linguistics. 
Maite Taboada. 2004. Building Coherence and 
Cohesion: Task-Oriented Dialogue in English and 
Spanish. Amsterdam/Philadelphia: John Benjamins. 
Maite Taboada, and Jan Renkema. 2008. Discourse 
Relations Reference Corpus [Corpus]. Simon Fraser 
University and Tilburg University. 
http://www.sfu.ca/rst/06tools/discourse_relations_cor
pus.html. 
Maite Taboada, and William C. Mann. 2006a. 
Rhetorical Structure Theory: Looking Back and 
Moving Ahead. Discourse Studies, 8(3):423-459.  
Maite Taboada, and William C. Mann. 2006b. 
Applications of Rhetorical Structure Theory. 
Discourse Studies, 8(4):567-588.  
Milan Tofiloski, Julian Brooke, and Maite Taboada. 
2009. A Syntactic and Lexical-Based Discourse 
Segmenter. In Proceedings of the 47th Annual 
Meeting of the Association for Computational 
Linguistics.  
Hai Zhao, Yan Song, and Chunyu Kit. 2010. How Large 
a Corpus Do We Need: Statistical Method Versus 
Rule-based Method. In Proceedings of the Seventh 
conference on International Language Resources and 
Evaluation (LREC'10). 
 
10
