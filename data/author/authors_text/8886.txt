The NICE Fairy-tale Game System1 
 
 
Joakim Gustafson, Linda Bell, Johan Boye, Anders Lindstr?m and Mats Wir?n 
TeliaSonera AB, 12386 Farsta, Sweden 
firstname.lastname@teliasonera.com 
 
                                                          
1 The work described in this paper was supported by the EU/HLT funded project NICE (IST-2001-35293), www.niceproject.com 
Abstract 
This paper presents the NICE fairy-tale game 
system, in which adults and children can 
interact with various animated characters in a 
3D world. Computer games is an interesting 
application for spoken and multimodal 
dialogue systems. Moreover, for the 
development of future computer games, 
multimodal dialogue has the potential to 
greatly enrichen the user?s experience. In this 
paper, we also present some requirements that 
have to be fulfilled to successfully integrate 
spoken dialogue technology with  a computer 
game application. 
1 Introduction 
The goal of the NICE project is to allow users of all 
ages to interact with lifelike conversational characters 
in a fairy-tale world inspired by the Danish author     
H C Andersen. To make these characters convincing 
in a computer game scenario, they have to possess 
conversational skills as well as the ability to perform  
physical actions in an interactive 3D world.  
What primarily distinguishes the NICE fairy-tale 
game system from other spoken dialogue systems is 
that the human-computer dialogue takes place within 
the context of an interactive computer game. 
However, spoken and multimodal dialogue is not 
supposed to be just an ?add-on? to the game, but the 
user?s primary means of progression through the 
story. The rationale for this is the great potential for 
more natural interaction we see in making methods 
from multimodal dialogue systems available in 
controlling gameplay. Potentially, spoken and 
multimodal interaction will make it possible to create 
a more engaging and immersive experience, or even 
facilitate the development of new kinds of computer 
games.    
Secondly, what makes NICE differ from typical 
spoken dialogue systems is the attempt to move away 
from strictly task-oriented dialogue. Instead, the 
interaction with the characters is domain-oriented. 
This means that the dialogue concerns different 
subplots in the fairy-tales, but without a clear goal-
orientation and without other demands than it being 
entertaining to the user. Furthermore, social 
interaction plays an important role in the fairy-tale 
world where the game takes place. By engaging in 
socializing with the animated characters, the user will 
find out things necessary to overcome various 
obstacles and enable progression through the story.  
Thirdly, a feature that differentiates NICE from 
other systems is that the main target user group of the 
system is children and young users. Previous studies 
have indicated that children employ partly different 
strategies when interacting with dialogue systems 
than adults do, and that there are also differences 
between age groups. For instance, younger children 
use less overt politeness markers and verbalize their 
frustration more than older children do (Arunachalam 
et al 2001). It has also been shown that children?s 
user experience is improved if they can communicate 
with a system with a ?personality? and that they 
benefit from being able to choose from several input 
modalities (Narayanan and Potamianos 2002). 
Furthermore, since many young people have a lot of 
experience with computer games, the believability of 
the dialogue characters and natural expressions will 
be critical aspects for the system?s success.  
Thus, computer games provide an excellent 
application area for research in spoken dialogue 
technology, requiring an advance of the state-of-the-
art in several fronts. Perhaps more importantly, game 
players will have a lot to gain from a successful 
incorporation of spoken dialogue technology into 
computer games. Today?s computer games are 
limited by the user?s input options, which are often 
restricted to direct manipulation and simple 
commands. In the development of the next generation 
of computer games, we believe that multimodal 
dialogue has the potential to greatly enrichen the 
user?s experience. For instance, spoken interaction 
makes it possible to refer to past events and objects 
currently not visible on the screen. Social interaction, 
which is already part of popular games such as SIMS, 
can be improved with spoken dialogue. Furthermore, 
speech and multimodal interaction supports 
cooperative games, where the user and character 
works together in solving a mutual problem.  
2 Spoken dialogue systems 
Spoken dialogue systems have so far mostly been 
designed with an overall goal to carry out a specific 
task, e.g. accessing time table information or ordering 
tickets (e.g. Zue et al 1991; Aust et al 1995). With 
task-oriented systems, it is possible to build domain 
models that can be used to predefine the language 
models and dialogue rules. The existence of 
predefined tasks makes it rather straight-forward to 
evaluate the performance of the dialogue system.  
Recent developments have made it possible to 
modify and extend the goals of spoken dialogue 
systems. Explorative dialogues, in which users are 
encouraged to browse through information without 
pursuing a specific task, have been presented by 
(Cassell et al 1999; Bell et al 2001). These 
dialogues still contain tasks to be solved during the 
interaction, e.g. giving constraints or receiving 
information about objects. However, explorative 
dialogue systems cannot be evaluated using merely 
the number of turns between different user 
interactions. A user who continues speaking with the 
system for a long time may do so because she is 
finding a lot of interesting information.  
Yet another type of dialogue system aims to 
present its users with an engaging and entertaining 
experience, without the presence of an external 
predetermined task. Conversational kiosks, such as 
August (Gustafson and Bell 2000) and MACK 
(Cassell et al 2002), encourage users to engage in 
social dialogues with embodied characters. Such 
dialogues are amenable to handling by a correctly 
designed dialogue system, since they primarily bring 
up features from the shared context. 
3 Interactive storytelling   
Interactivity has been defined as ?a kind of drama 
where the audience can modify the course of the 
actions [?] thus having an active role? (Szilas 1999). 
In interactive scenarios, the user helps the story 
unfold and may affect its course depending on his or 
her active participation. It has been argued that 
interactive storytelling will change computer 
entertainment by introducing better narrative content 
and allowing users to interfere with the progression 
of the storyline (Cavazza et al 2002). However, 
Young (2001) suggests that the drama manager of the 
system should put a limit to the user?s actions by not 
allowing interference that violates the overall 
narrative plan. Most interactive games developed so 
far allow users to intervene in the storytelling by 
acting on physical objects on the screen using direct 
maniputation (Young 2001; Cavazza et al 2002). 
Moreover, some systems allow users to interact with 
characters by means of written text input (Mateas and 
Stern 2002). In addition, Cavazza et al (2002) 
explored using a speech interface that handled 
isolated utterances from the user.  
4 The NICE fairy-tale game scenario  
The overall goal of the project is to provide users 
with an immersive dialogue experience in a 3D fairy-
tale world, see Figure 1. To this end, we have chosen 
to make spoken and multimodal dialogue the user?s 
primary vehicle of progressing through the story. It is 
also by verbal and non-verbal communication that the 
user can gain access to the goals and desires of the 
fairy-tale characters. This will be critical as the 
characters will ask the users to help them in solving 
problems. These problems either relate to objects that 
have to be manipulated or information that has to be 
retrieved from other fairy-tale characters.  
 
Figure 1. Cloddy Hans in the fairy-tale world. 
The fairy-tale domain was chosen because of its 
classic themes and stereotypical characters, well-
known to most adults as well as children. Some of 
these familiar characters are shown in Figure 2. 
 
Figure 2. The fairy-tale characters. 
To facilitate the progression through the story, we 
introduce Cloddy Hans, the user?s faithful assistant.  
Cloddy Hans?s character is conveyed to the users in 
the following way: he is a bit slow to understand, or 
so it seems. He sometimes appears hard of hearing 
and only understands spoken utterances and graphical 
gestures at a rather simple level. Cloddy Hans does 
not take a lot of initiatives, but is honest and anxious 
to try to help the user. In spite of his limited 
intellectual and perceptual capabilities, he may 
sometimes provide important clues through sudden 
flashes of insight.  
The user can ask Cloddy Hans to manipulate objects 
by referring to them verbally and/or by using the 
mouse. To understand the reason for not allowing 
users to directly manipulate objects on the screen, we 
have to recall what distinguishes NICE from other 
games, namely, spoken multimodal dialogue. We 
thus want to ensure that multimodal dialogue is 
appreciated by the user not just as an ?add-on? but as 
the primary means of progressing in the game. Our 
key to achieving this is to deliberately limit the 
capabilities of the key actors ? the user and Cloddy 
Hans ? in such a way that they can succeed only by 
cooperating through spoken multimodal dialogue. In 
other words, the user is intelligent but cannot himself 
affect objects in the world; Cloddy Hans on the other 
hand is a bit slow but capable of physical action 
according to what he gets told (and he may 
occasionally also provide tips to the user). 
The fairy-tale game will start with an introductory 
dialogue, in which the user meets Cloddy Hans in H C 
Andersen?s fairy-tale laboratory, see Figure 3. The 
simple task the user and Cloddy have to solve 
together is to take fairy-tale objects from a shelf and 
put them in the appropriate slot in a fairy-tale 
machine. Each slot is labelled with a symbol, which 
denotes the type of object supposed to go there, but 
since Cloddy Hans is not very bright, he needs help 
understanding these labels.  
 
Figure 3. Cloddy Hans in the fairy-tale lab 
The initial scenario is a ?grounding game? set in the 
context of a narrow task. In other words, its real 
purpose is a training session in which the user and 
Cloddy Hans agree on what different objects can be 
used for and how they can be referred to. This 
process also lets the player find out (by trial-and-
error) how to adapt in order to make it easier for the 
system to understand him or her. Moreover, Cloddy 
Hans sometimes explicitly instructs the user. For 
example, one lesson might be that it is sometimes 
more efficient to use multimodal input instead of just 
spoken utterances.  
The subsequent game in the fairy-tale world 
depends on what objects have been chosen by the 
user in the initial scenario. The advantage of this is 
that the objects are already grounded; for example, a 
sack of gold will be visually recognized by the player 
and there is an already agreed way of referring to it. 
5 System characteristics  
The game scenario as presented in the preceding 
section puts a number of requirements on the system. 
The scenario involves several animated characters, 
each with its own intended distinct personality. These 
personalities must be made explicit for the game 
player, and manifest themselves on all levels: from 
the appearance of the characters, their gestures and 
voices, choice of words, to their long-term behavior 
and overall role in the fairy-tale world. Furthermore, 
the characters need to be responsive, and be able to 
engage in conversation which makes sense to the 
player of the game. 
On the surface level, then, we need to have 
beautifully crafted animated characters and 
environments (these have been designed by the 
computer-game company Liquid Media). Each 
character must have its own voice that conveys the 
nature of that character?s personality, and be able to 
use prosodic cues to signal mood and emotions. To 
this end, a unit-selection speech synthesizer has been 
developed. Cloddy Hans has been given a slow, deep 
voice that goes along with his intended dunce 
personality. His repertoire of gestures and his style of 
walking also amplifies the impression of a slow-
witted but friendly person.  
On the input side, we need to recognize 
continuous, unconstrained speech for users of all 
ages. Previous studies have shown that children?s 
speech is associated with elevated error rates 
(Potamianos et al 1997; Oviatt and Adams 2000), 
making it necessary for Scansoft to retrain the NICE 
recognizer?s acoustic models. In addition, we need to 
take into account the disfluent speech patterns that 
are likely to arise, most probably because the users 
are unused to the situation or distracted by the virtual 
environment. On the other hand, not all input needs 
to be adequately interpreted. Much of the socializing 
utterances from the user can be handled in a 
satisfactory way by using shallow methods. 
Furthermore, the interpretation of the goal oriented 
interactions is simplified by the fact that the system 
knows which objects are visible on the screen and, 
more importantly, since it already knows what 
problems the fairy-tale characters has asked the user 
to help them to solve. Finally, the user also has the 
possibility of referring to objects using a pointing 
device. The software for the interpretation of this 
graphical input has been developed by LIMSI.  
The above characteristics have led us to design the 
system?s interpretation of user input in the following 
way. The system is implemented as a set of event-
driven processes that communicate via message-
passing. The architecture is essentially an extension 
of the one described in (Bell et al 2001). This 
architecture allows, among other things, for highly 
flexible turn-taking. When the user speaks, the 
system first tries to categorize the utterance as either 
social (needing only shallow interpretation) or goal-
oriented (needing further analysis). 
Finally, the long-term behavior of a character is 
decided by its set of internal goals and rules. A goal 
is essentially a predicate (that can be either true or 
false) concerning of the state of the virtual world. For 
instance, a character may have a goal to acquire a 
certain object or visit a certain place. If a given goal 
is not fulfilled (the predicate is false), the character 
will try to fulfill it. To this end it will use its set of 
rules, that define actions and dialogue acts that are 
likely to contribute to reaching the goal. 
6  Evaluation issues  
Task-oriented spoken dialogue systems are usually 
evaluated in terms of objective and subjective 
features. Objective criteria include the technical 
robustness and core functionality of the system 
components as well as system performance measures 
such as task completion rate. Subjective usability 
evaluations estimate features like naturalness and 
quality of the interactions, as well as user satisfaction 
reported in post-experimental interviews. However, 
many of these measures are simply not relevant for 
entertainment-type applications, where user 
satisfaction increases rather than decreases with task 
completion time. It can even be difficult to define 
what the completion of the task would be. In practice, 
computer games are usually evaluated by 
professional game reviewers and by the users in 
terms of number of copies sold.  
In the evaluation of the NICE fairy-tale game sales 
figures will not be possible to use, and several of the 
traditional objective measures are less relevant due to 
the domain. Instead, subjective measures involving 
features like ?narrative progression?, ?character 
believability?, and ?entertainment value?, will be 
used. They will be obtained off-line, by interviewing 
the users after their interactions and asking them to 
fill out questionnaires. Users will be asked how they 
perceived the quality of the actual interaction, as well 
as the personality of the fairy-tale characters. Expert 
evaluators, who will be able to replay the user 
interactions and inspect the system logs, will also be 
employed. Examples of evaluation questions to the 
experts include: ?Do the characters display 
meaningful roles and believable personalities that 
contribute to the story??, ?Do they succeed in 
signaling their level of understanding?, ?To what 
extent is the user able to affect the plot?? 
In order to be able to replay the user interactions with 
the fairy-tale system, all communication between the 
system modules are logged with time stamps. This 
will be a valuable tool both in the iterative system 
development and for system evaluations. At present, 
we are in the process of collecting data with the 
introductory game scenario. The data collected will 
be used to develop the subsequent scenarios in the 
fairy-tale game.  
References 
Arunachalam, S., D. Gould, E. Andersen, D. Byrd and S. S. 
Narayanan. (2001). Politeness and frustration language 
in child-machine interactions. Proceedings of 
Eurospeech: 2675-2678. 
Aust, H., M. Oerder, F. Seide and V. Steinbiss (1995). The 
Philips automatic train timetable information system. 
Speech Communication 17(3-4): 249-262. 
Bell, L., J. Boye and J. Gustafson (2001). Real-time 
handling of fragmented utterances. Proc. NAACL 2001 
workshop on Adaptation in Dialogue Systems. 
Cassell, J., T. Bickmore, M. Billinghurst, L. Campbell, K. 
Chang, H. Vilhj?lmsson and H. Yan (1999). 
Embodiment in conversational interfaces: Rea. 
Proceedings of CHI: 520-527. 
Cassell, J., T. Stocky, T. Bickmore, Y. Gao, Y. Nakano, K. 
Ryokai, D. Tversky, C. Vaucelle and H. Vilhjlmsson 
(2002). MACK: Media lab Autonomous 
Conversational Kiosk. Imagina 02. Monte Carlo. 
Cavazza, M., F. Charles and S. J. Mead (2002). Character-
based interactive storytelling. IEEE Intelligent 
Systems, Special issue on AI in Interactive 
Entertainment: 17-24. 
Gustafson, J. and L. Bell (2000). Speech technology on 
trial - Experiences from the August system. Natural 
Language Engineering 6(3-4): 273-286. 
Mateas, M. and A. Stern (2002). Architecture, authorial 
idioms and early observations of the interactive drama 
Facade. Technical report CM-CS-02-198. 
Narayanan, S. and A. Potamianos (2002). Creating 
conversational interfaces for children. IEEE 
Transactions on Speech and Audio Proc. 10(2): 65-78. 
Oviatt, S. and B. Adams (2000). Designing and evaluating 
conversational interfaces with animated characters. 
Embodied Conversational Agents. J. Cassell, J. 
Sullivan, S. Prevost and E. Churchill. MIT Press. 
Potamianos, A., S. Narayanan and S. Lee (1997). 
Automatic speech recognition for children. 
Proceedings of Eurospeech. 5: 2371-2374. 
Szilas, N. (1999). Interactive drama on the computer: 
beyond linear narrative. AAAI 1999 Fall Symposium 
on Narrative Intelligence. 
Young, R. M. (2001). An Overview of the Mimesis 
Architecture: Integrating Intelligent Narrative Control 
into an Existing Gaming Environment. Working Notes 
of the AAAI Spring Symposium on Artificial 
Intelligence and Interactive Entertainment. 
Zue, V., J. Glass, D. Goodline, H. Leung, M. Phillips, J. 
Polifroni and S. Seneff (1991). Integration of speech 
recognition and natural language processing in the 
MIT voyager system. Proc. ICASSP'91. Toronto. 
Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 56?63,
NAACL-HLT, Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Experiences of an In-Service Wizard-of-Oz Data Collection 
for the Deployment of a Call-Routing Application 
 
Mats Wir?n,? Robert Eklund,? Fredrik Engberg? and Johan Westermark? 
?Research and Development ?Customer Integrated Solutions 
TeliaSonera TeliaSonera 
SE?123 86 Farsta, Sweden SE?751 42 Uppsala, Sweden 
firstname.lastname@teliasonera.com 
 
 
Abstract 
This paper describes our experiences of 
collecting a corpus of 42,000 dialogues 
for a call-routing application using a 
Wizard-of-Oz approach. Contrary to 
common practice in the industry, we did 
not use the kind of automated application 
that elicits some speech from the 
customers and then sends all of them to 
the same destination, such as the existing 
touch-tone menu, without paying 
attention to what they have said. Contrary 
to the traditional Wizard-of-Oz paradigm, 
our data-collection application was fully 
integrated within an existing service, 
replacing the existing touch-tone 
navigation system with a simulated call-
routing system. Thus, the subjects were 
real customers calling about real tasks, 
and the wizards were service agents from 
our customer care. We provide a detailed 
exposition of the data collection as such 
and the application used, and compare our 
approach to methods previously used. 
1 Background and introduction 
Spoken-dialogue systems for applications such as 
customer care increasingly use statistical language 
models (SLMs) and statistically-based semantic 
classification for recognition and analysis of 
utterances. A critical step in designing and 
deploying such a system is the initial data 
collection, which must provide a corpus that is 
both representative of the intended service and 
sufficiently large for development, training and 
evaluation. 
For at least 20 years, Wizard-of-Oz methodology 
has been regarded as a superior (though not 
unproblematic) method of collecting high-quality, 
machine-directed speech data in the absence of a 
runnable application.1 Normally, these data will be 
useful for several purposes such as guiding 
dialogue design and training speech recognizers. 
Still, the Wizard-of-Oz option is often dismissed in 
favour of simpler methods on the ground that it 
does not scale well in terms of cost and time (for 
example, Di Fabbrizio et al 2005). Consequently, 
Wizard-of-Oz has typically been used for data 
collections that are more limited in the number of 
subjects involved or utterances collected. One 
exception from this is the data collection for the 
original AT&T ?How May I Help You? system 
(Gorin et al 1997; Ammicht et al 1999), which 
comprised three batches of transactions with live 
customers, each involving up to 12,000 utterances. 
Other well-known instances are ?Voyager? (Zue 
et al 1989) and the individual ATIS collections 
(Hirschman et al 1993) which involved up to a 
hundred subjects or (again) up to 12,000 
utterances. 
While it is true that Wizard-of-Oz is a labour-
intensive method, the effort can often be motivated 
on the ground that it enables significant design and 
evaluation to be carried out before implementation, 
thereby reducing the amount of re-design 
necessary for the actual system. However, one 
should also bear in mind the crucial advantage 
brought about by the possibility in a production 
environment of running the Wizard-of-Oz 
collection in-service rather than in a closed lab 
setting. As we shall discuss, the fact that real 
customers with real problems are involved instead 
of role-playing subjects with artificial tasks 
circumvents the key methodological problem that 
has been raised as an argument against Wizard-of-
Oz, namely, lack of realism. 
                                                          
1 For backgrounds on Wizard-of-Oz methodology, see Dahlb?ck et al (1993) 
and Fraser & Gilbert (1991). 
56
The aim of this paper is to describe our 
experiences of running a Wizard-of-Oz collection 
in a production environment with real customers, 
with the double purpose of guiding dialogue design 
and collecting a sufficient amount of data for the 
first training of a speech recognizer. We also 
review what other options there are for the initial 
data collection and compare our Wizard-of-Oz 
approach with those. 
The rest of this paper is organized as follows: 
Section 2 describes the call-routing problem and 
our particular domain. Section 3 gives an overview 
of the options for the initial data collection and the 
major trade-offs involved in selecting a method. 
Section 4 describes the application that was 
developed for our Wizard-of-Oz data collection, 
whereas Section 5 describes the actual data 
collection, summary statistics for the collected data 
and some experimental results obtained. Section 6 
contains a discussion of our overall experiences. 
2 The call-routing task and domain 
Call routing is the task of directing a caller to a 
service agent or a self-serve application based on 
their description of the issue. Increasingly, speech-
enabled routing is replacing traditional touch-tone 
menues whereby callers have to navigate to the 
appropriate destinations. 
The domain of interest in this paper is (the 
entrance to) the TeliaSonera2 residential customer 
care in Sweden, comprising the entire range of 
services offered: fixed and mobile telephony, 
broadband and modem-based Internet, IP 
telephony, digital television, triple play, etc. 
Around 14 million calls are handled annually, and 
before the speech-enabled call-routing system was 
launched in 2006, touch-tone navigation was used 
throughout. The speech-enabled system involves 
an SLM-based speech recognizer and a 
statistically-based classifier.3 The task of the 
classifier is to map a spoken utterance to an 
application category which corresponds to a self-
serve application, (a queue to) a human agent, a 
disambiguation category or a discourse category. 
Whereas self-serve applications and service agents 
are the desired goals to reach, disambiguation and 
discourse categories correspond to intermediate 
states in the routing dialogue. More specifically, 
                                                          
2 TeliaSonera (www.teliasonera.com) is the largest telco in the Scandinavian 
?Baltic region. 
3 The speech recognizer and classifier are delivered by Nuance 
(www.nuance.com).  
disambiguation categories correspond to cases 
where the classifier has picked up some 
information about the destination, but needs to 
know more in order to route the call. Discourse 
categories correspond to domain-independent 
utterances such as greetings (?Hi, my name is John 
Doe?), channel checks (?Hello??) and meta 
questions (?Who am I talking to??). Altogether, 
there are 124 application categories used by the 
current classifier. 
3 Options for initial data collection 
Basically, there are three options for making the 
initial data collection for a call-routing application: 
to collect human?human dialogues in a call center, 
to use an automated data-collection application, or 
to use a Wizard-of-Oz approach. We shall now 
describe each of these. 
3.1 Human?human dialogues 
The simplest possible approach to the initial data 
collection is to record conversations between 
service agents and customers in a call center. This 
is an inexpensive method since it does not require 
any data-collection application to be built. Also, 
there is no customer impact. However, the data 
obtained tend not to be sufficiently representative, 
for two reasons: First, typically only a subset of the 
services of a call center is carried out by human 
agents, and hence many services will not be 
covered. Second, the characteristics of human?
human conversations differ from those of human?
machine interaction. Still, this option has 
sometimes been preferred on the grounds of 
simplicity and lack of negative customer impact. 
3.2 Automated applications 
Due to the nature of the task, it is easy to put out a 
fully automated mock-up system in a live service 
that engages in the initial part of a call-routing 
dialogue. Typically, such a system will play an 
open prompt, record the customers? speech, play 
another prompt saying that the system did not 
understand, again record the speech, and finally 
direct all calls to a single destination, such as a 
general-skills service agent or the entry to the 
existing touch-tone menu. We estimate that a 
system of this kind could be implemented and 
integrated into a call center in about a person week. 
An example of this approach is the AT&T ?Ghost 
Wizard? (referred to in Di Fabbrizio et al 2005). 
57
This basic approach can be improved upon by 
detecting silences and touch-tone events, and in 
these cases playing designated prompts that try to 
get the caller on track. Furthermore, if data from 
previous call-routing applications are available, it 
is possible to use these to handle domain-
independent utterances. Such utterances 
correspond to discourse categories as mentioned in 
Section 2, and the idea then is to play prompts that 
encourage the caller to describe the issue. A 
description of such an approach is provided by Di 
Fabbrizio et al (2005). 
A problem with the automated approach is that 
customer impact can be quite negative, since the 
application does not actually do anything except 
for recording their speech (possibly through 
several turns), and routing them to a ?dummy? 
destination where they will have to start over. Of 
course, one way of avoiding this is to include a 
human in the loop who listens to the customer?s 
speech and then routes the call to the right 
destination. Apparently, this is the approach of 
Di Fabbrizio et al (2005), which consequently is 
not fully automated. 
Apart from customer impact, the problem with an 
automated system is that we do not learn the full 
story about caller behaviour. In particular, since 
typically only a minority of callers will state their 
issue in an unambiguous way within the given few 
turns, less information about the callers? actual 
issues will be obtained. In particular, for callers 
who completely fail to speak or who give no 
details about their issue, we will have no 
possibility of finding out what they wanted and 
why they failed. Furthermore, since the system 
lacks the ability to respond intelligently to in-
domain utterances, no follow-up dialogue such as 
disambiguation can be collected. 
3.3 Wizard-of-Oz 
Although Wizard-of-Oz is arguably the best 
method for collecting machine-directed data in the 
absence of a running application, it is not without 
methodological problems. The basic critique has 
always been aimed at the lack of realism (for 
example, von Hahn 1986). In a thorough analysis, 
Allwood & Haglund (1992) point out that in a 
Wizard-of-Oz simulation, both the subjects and the 
wizard(s) are playing roles, occupied and assigned. 
The researcher acting as the wizard is occupying 
the role of a researcher interested in obtaining ?as 
natural as possible? language and speech data, 
while playing the role of the system. The subject, 
on the other hand, is occupying the role of a 
subject in a scientific study, and playing the role of 
a client (or similar), communicating with a system 
while carrying out tasks that are not genuine to the 
subject, but given to them by the experiment leader 
(who might be identical with the wizard). 
It turns out, however, that a traditional Wizard-
of-Oz approach with made-up tasks according to a 
scenario is anyway not an option when collecting 
data for deploying a call-routing system. The 
reason for this is that we want to learn not just how 
callers express themselves, but also what kind of 
tasks they have, which obviously rules out pre-
written scenarios. If the existing system uses 
touch-tone navigation, usually not too much can be 
ascertained about this, and trying to design a set of 
tasks just by looking at the existing destinations 
would miss the point. 
By instead integrating a Wizard-of-Oz 
application in an existing, live service, we can 
circumvent the key methodological problems, 
while addressing all the problems of the previously 
described approaches and even obtaining some 
independent advantages: 
1. Since the callers? experience will be like that of 
the intended application, albeit with human 
speech understanding, the customer impact will 
be at least as good. In fact, it is even possible to 
issue a kind of guarantee against maltreatment 
of customers by instructing the wizards to take 
over calls that become problematic (this is 
further discussed in Section 4). 
2.  Since real customers are involved, no role-
playing from the point of view of the subjects 
takes place, and hence the data become highly 
realistic.   
3. The fact that scenarios are superfluous?or 
even run counter to the goal of the data 
collection?means that the main source of 
methodological problems disappears, and that 
the data collection as such is considerably 
simplified compared to traditional Wizard-of-
Oz. 
4. By letting service agents be wizards, we move 
away even further from role-playing, given that 
the interaction metaphor in speech-enabled call 
routing is natural-language dialogue with a 
(general-skills) service agent. 
58
5. Service agents possess the expertise necessary 
for a call-routing wizard: they know when 
additional information is required from the 
caller, when a call is ready for routing, and 
where to actually route the call. Hence, wizard 
guidelines and training become less complex 
than in traditional Wizard-of-Oz.4 
6. Service agents have excellent skills in dealing 
with customers. Hence, during the data 
collection they will be able to provide valuable 
feedback on dialogue and prompt design that 
can be carried over to the intended application. 
In spite of these advantages, Wizard-of-Oz appears 
to have been used only very rarely for collecting 
call-routing data. The sole such data collection that 
we are aware of was made for the original AT&T 
?How May I Help you? system (Gorin et al 1997; 
Ammicht et al 1999). The one disadvantage of the 
Wizard-of-Oz approach is that it is more laborious 
than automated solutions, mainly because several 
person months of wizard work is required. On the 
other hand, as we have seen, it is still less laborious 
than a traditional Wizard-of-Oz, since there are no 
scenarios and since wizard guidelines can be kept 
simple. 
4 Data-collection application 
Our data-collection application consists of two 
parts: The first part is the Prompt Piano Client 
(PPC), which is running on the service agent?s PC. 
This is essentially a GUI with ?keys? 
corresponding to prerecorded prompts by which 
the wizard interacts with the caller, thereby 
simulating the intended system. The PPC interface 
is shown in PLATE 1. The second part is the 
Prompt Piano Server (PPS), which is an IVR 
(interactive voice response) server with a Dialogic 
telephony board, running Envox, Nuance and 
Dialogic software. This handles playing of prompts 
as well as recording of calls. Two kinds of 
recordings are made: call logs (that is, the callers? 
speech events as detected by the Nuance speech 
recognizer) and complete dialogues (?open mic?). 
To set up a data collection, the contact center 
solution is modified so that a percentage of the 
incoming calls to the customer care is diverted to 
the PPS. The PPS in turn transfers each call to a 
wizard (that is, to a PPC) using tromboning. 
                                                          
4 Furthermore, as a side effect, it is possible to facilitate the subsequent process 
of manually tagging the data by keeping track of where each call is routed. 
Allocation of the wizards is performed by the Telia 
CallGuide contact center platform using skill-
based routing. Whenever a wizard answers a call, 
two audio streams are established, one from the 
customer to the wizard so that she can hear the 
customer?s speech, and one from an audio source 
in the PPS to the customer. An initial open prompt 
is played automatically by the PPS, and the wizard 
is then free to start playback of prompts. This is 
realized by sending control messages from the PPC 
to the audio source on the PPS via TCP/IP, while 
listening to the customer throughout. 
Depending on the caller?s response, different 
things will happen: If the caller provides an 
unambiguous description of the issue, the wizard 
will transfer the call to the correct queue and end 
the recording by pressing the ?end / route 
customer? button. This signals to the PPS that the 
call should be released using the Explicit Call 
Transfer (ECT) supplementary service, freeing the 
two channels used for the tromboned call in the 
PPS. 
If, on the other hand, the caller does not provide 
an unambiguous description of the issue, the 
wizard will play a follow-up prompt aimed at 
getting more information from the caller by 
choosing from the buttons/prompts situated to the 
right (fields II and III of the GUI; see Plate 1). 
These parts of the GUI are fully configurable; the 
number and layout of buttons as well as the names 
of sound files for the corresponding prompts are 
declared separately. (Declarations include 
specifying whether the prompt associated with a 
particular button allows barge-in or not.) Thus, it is 
possible not just to vary individual prompts, but 
also to simulate call-routing dialogues to various 
depths by varying the number of buttons/prompts. 
Apart from routing the call, a possible action of 
the wizard is to enter into the call. This is realized 
by establishing a two-way direct audio stream with 
the customer, enabling the parties to talk to each 
other. As pointed out in Section 3.3, one purpose 
of this is to let wizards take over calls that are 
problematic, thereby making sure that callers do 
not get maltreated during the data collection and 
reducing the risk that they hang up. A similar 
functionality was available in the data-collection 
application for AT&Ts ?How May I Help You? 
system (Walker et al 2000). 
59
 
PLATE 1: The Prompt Piano Client interface as configured towards the end of the data collection. The interface 
is divided into three fields with buttons. I: The leftmost field provides caller information, like A-nr (the phone 
number the customer is calling from) and Cid (the phone number the customer provides as reason for the call). 
The wizard has two option buttons, Mina ?tg?rder (?my actions?), at hand: the button Bryt in / Prata med 
kund (?barge-in/talk to client?) which is used for entering into the call, and the button Avsluta / Koppla kund 
(?end/route customer?) which is used to terminate the recording prior to routing the call to the appropriate 
destination. (Both of these options are associated with prompts being played.) II: The second field, Kunden? 
(?the customer??), contains buttons corresponding to renewed open prompts for the purpose of error-handling, 
??r tyst (?? is silent?), ?trycker p? knappar (?uses the touch-tone keypad?), ?ber om hj?lp (?asks for 
help?), ?avbryter (?interrupts?), ?pratar f?r l?nge (?talks for too long?), ?s?ger inget om ?rendet (?doesn?t 
say anything about the reason for the call?), ??r sv?r att uppfatta (?is hard to understand?). III: The third field, 
Jag undrar om det g?ller? (?I would like to know if it is about??), contains buttons corresponding to 
disambiguation prompts asking for additional information, e.g. whether the customer?s reason for the call is 
about fixed (?fast?) or mobile (?mobilt?) telephony, broadband (?bredband?) or something else. All buttons also 
have hot-key possibilities for agents who prefer this over point-and-click. 
With the exception of the initial open prompt, the 
wizards have full control over when and in what 
order prompts are played and actions are executed. 
Thus, whereas an automated system will start 
playing the next prompt after an end-of-speech 
timeout typically within the range of 0.75?1.5 
seconds, a wizard may decide to impose longer 
delays if she considers that the caller has not yet 
yielded the turn. On the other hand, the wizard 
may also respond more rapidly. Thus, the problem 
of response delays, which has sometimes had 
distorting impact in Wizard-of-Oz simulations, 
does not appear in our application (cf. Oviatt et al 
1992). 
The PPS application was developed in the Envox 
graphical scripting language, which makes it 
possible to write event-driven applications 
controlled from an external source such as the 
PPC, and also supports Nuance call logging (for 
recording customer utterances) and Dialogic 
transaction recording (for recording entire 
conversations between two parties, in this case the 
customer and the PPS, or the customer and the 
wizard).5 Design, implementation and testing of 
the Prompt Piano (PPC and PPS) took four person 
weeks. 
The agents/wizards were involved in the 
development from the very start to ensure that the 
application (and in particular the GUI) was 
                                                          
5 VXML was not used since it appeared that real-time control of an IVR from 
an external source would then have been more difficult to implement. 
Furthermore, VXML browsers generally have no support for features such as 
transaction recording during tromboned transfer and delayed invocation of the 
ECT supplementary service in conjunction with call transfer. Hence, in a 
VXML framework, additional components would be required to solve these 
tasks. 
60
optimized according to their needs and wishes. The 
Prompt Piano GUI was reconfigured several times 
during the course of the data collection, both for 
the purpose of carrying out prompt-design 
experiments and in response to (individual or 
group) requests for changes by the agents/wizards. 
5 Data collection 
5.1 Overview 
The purpose of the data collection was twofold: to 
obtain speech data that could be used for initial 
training of the speech recognizer, and to obtain 
data that could be used to guide dialogue design of 
the intended application. Thus, whereas the former 
only involved caller responses to open prompts, the 
latter required access to complete call-routing 
dialogues, including error-handling and 
disambiguation.  
Organization. Ten wizards were used for the 
data collection. Initially, one week was used for 
training of the wizards and basic tuning of the 
prompts. This process required four person weeks 
(not all wizards were present all the time). After a 
break of three weeks, the data collection then went 
on for five weeks in a row, with the ten wizards 
acquiring around 42,000 call-routing dialogues. 
(This figure includes around 2,000 useable 
dialogues that were collected during the initial 
week.) This was more than had been anticipated, 
and much more than the 25,000 that had been 
projected as a minimum for training, tuning and 
evaluation of the speech recognizer. Thus, 
although 50 person weeks were used by the 
wizards for the actual collection, 32 person weeks 
would actually have been sufficient to reach the 
minimum of 25,000 dialogues. On average, 195 
dialogues were collected per working day per 
wizard (mean values ranging from 117 dialogues 
per day to 317 dialogues per day; record for a 
wizard on a single day was 477). 
Barge-in. Initially, barge-in was allowed for all 
prompts. However, it turned out to be useful to 
have one very short prompt with barge-in disabled, 
just asking the caller to state the reason for the call. 
The main usage of this was in cases where callers 
were repeatedly barging in on the system to the 
extent that the system could not get its message 
through. 
Utterance fragments. As a consequence of, on 
the one hand, wizards having full control over 
when and whether to start playing a prompt and, on 
the other hand, the speech recognizer having a 
fixed end-of-speech timeout, it would sometimes 
happen that more than one sound file would be 
recorded between two prompts in the Nuance call 
logs. An example of this would be: ?Eeh, I... I?m 
wondering whether... can you tell me the pricing of 
broadband subscriptions??, where both of the two 
silent pauses would trigger the end-of-speech 
timeout. Although this constitutes a mismatch 
between data collection and final system, in 
practice this caused no problem: on the contrary, 
the sound files were simply treated as separate 
utterances for the purpose of training the speech 
recognizer, which means that the most informative 
fragment, typically at the end, was not lost. In 
addition, these data are potentially very valuable 
for research on turn-taking (in effect, intelligent 
end-of-speech detection). 
Wizards entering into calls. The event of 
wizards taking over calls in order to sort out 
problematic dialogues occurred on average in 5% 
of the calls. The figure was initially a bit higher, 
presumably because the wizards were less skillful 
in using the prompts available, and because the 
prompts were less well-developed. As a side-effect 
of this, we have obtained potentially very valuable 
data for error-handling, with both human?machine 
and human?human data for the same callers and 
issues (compare Walker et al, 2000). 
Post-experimental interviews. We also used the 
facility of letting wizards take over calls as a way 
of conducting post-experimental interviews. This 
was achieved by having wizards route the calls to 
themselves and then handle the issue, whereupon 
the wizard would ask the caller if they would 
accept being interviewed. In this way, we were 
able to assess customer satisfaction on the fly with 
respect to the intended system and even getting 
user feedback on specific design features already 
during the data collection. 
5.2 Experiments 
Several design experiments were run during the 
data collection. Here, we shall only very briefly 
describe one of them, in which we compared two 
styles of disambiguation prompts, one completely 
open and one more directed. As can be seen in 
TABLE 1, utterances following the open 
disambiguation prompt are on average 3.6 times 
longer than utterances following the directed 
prompt.
61
Utterances and Words Disfluency Concepts Prompt 
Utts Words Words 
/Utts 
Disfl Disfl 
/Utts 
Disfl 
/Words 
Concepts 
In 
Concepts 
Out 
DIFFs 
Total 
DIFFs 
Change 
DIFFS 
/Utts 
DIFFS 
/Words 
Directed 118 216 1.8 19 0.16 0.09 136 244 108 0 0.9 0.5 
Open 121 791 6.5 72 0.6 0.09 144 248 122 18 1.01 0.15 
TABLE 1. Summary statistics for the directed prompt (?I need some additional information about the reason for 
your call. Is it for example about an order, price information or support??), and the open prompt (?Could you please 
tell me a little bit more about the reason for you call??) prompts. Totals and ratios are given for utterances/words, 
disfluencies and number of concepts acquired before the disambiguation prompt was played (?In?) and after the 
customer had replied to the disambiguation prompt (?Out?). Also, ratios are given for number of concepts compared 
to number of utterances and words, as well as totals and ratios for the differences (DIFFs) between concepts in and 
concepts out, i.e., how many concepts you ?win? by asking the disambiguation prompt.  
Furthermore, in order to see to what extent 
these prompts also made callers provide more 
information, we manually tagged the transcribed 
utterances with semantic categories. Following 
the evaluation methodology suggested by Boye 
& Wir?n (2007, Section 5), we then computed 
the difference with respect to ?concepts? for 
utterances immediately following and preceding 
the two kinds of prompts.  
Although the number of concepts gained is 
only slightly higher6 for the open prompt (as a 
function of concepts per utterance), there are 
some palpable differences between the directed 
and the open prompt. One, shown in TABLE 1, is 
that there are no instances where an already 
instantiated concept (e.g. fixedTelephony) is 
changed to something else (e.g. broadband), 
while this happens 18 times following the open 
prompt. The other, not shown in TABLE 1, is 
that, following the directed prompt, one never 
?gains? more than one new concept, while there 
are 26 instances following the open prompt 
where the gain is two concepts, and even two 
instances where the gain is three concepts 
(which also means that one concept is changed). 
Finally, when one analyses the syntactic 
characteristics following the two different types 
of prompts, there is an obvious shift from the 
telegraphic ?noun-only? responses that amount 
to more than 70% of the directed prompt 
responses, to the responses following the open 
prompt, where 40% are complete sentences and 
21% are noun phrases. Also, the syntax is more 
varied following the open prompt.7  
                                                          
6 However, the difference is not statistically significant, either using a t test 
(two-sampled, two-tailed: p=0.16 with equal variances assumed; p=0.158 
equal variances not assumed) or Mann-Whitney U test (two-tailed: 
p=0.288). 
7 The distributions are, in descending order, for the directed prompt: 
Noun=85, Sentence=11, Yes/No=8, Noun Phrase=8, no response=3, 
Yes/No+Noun=2, Adverbial Phrase=1, Adjective Phrase=1; for the 
open prompt: Sentence=49, Noun Phrase=26, Noun=24, Verb 
6 Discussion 
We claimed in Section 3.3 that by using an in-
service Wizard-of-Oz data collection, we have 
been able to effectively overcome all problems 
of the alternative methods discussed there. A 
relevant question is then if there are any 
remaining, independent problems of the 
approach described here. 
On the methodological side, there is clearly a 
certain amount of role playing left in the sense 
that service agents are acting as the system 
(albeit a system whose interaction metaphor is a 
service agent!). Interestingly, we noticed early 
on that the agents sometimes failed in properly 
simulating the intended system in one respect: 
Since they would often grasp what the caller 
wanted before he or she had finished speaking, 
they would start playing the next prompt so 
early that they were barging in on the caller. 
Thus, in their willingness to provide quick 
service, they were stepping outside of their 
assigned role. However, they soon learnt to 
avoid this, and it was never a problem except for 
the first few days. 
Apart from this, the main disadvantage of 
Wizard-of-Oz collections clearly is the amount 
of work involved compared to the other 
methods. As we have seen, the Prompt Piano 
design and implementation took four person 
weeks, training of the wizards took another four 
person weeks, and collection of 25,000 
dialogues required 32 person weeks?hence 
altogether 40 person weeks (although we 
actually used 50 person weeks, since we went on 
collecting more data). This could be compared 
with possibly a single person week required for 
the fully automated approach. The more 
                                                                                       
Phrase=11, Adjective Phrase=5, Adverbial Phrase=2, no response=2, 
Yes/No=1, Interjection=1. 
 
62
elaborate automated methods would come 
somewhere in between, also depending on 
whether a human agent is used for routing 
callers or not.  
In the TeliaSonera case, the main desiderata 
favouring Wizard-of-Oz were highly 
representative data, no negative customer impact 
and need for early evaluation and design, 
particularly because this was the first 
deployment of natural-language call routing in 
Scandinavia. In other words, it was decided to 
accept a higher initial cost in return for reduced 
costs downstream, due to higher quality and less 
re-design of the implemented system.  
It is impossible to quantify the downstream 
savings made by choosing Wizard-of-Oz since 
we have no baseline. However, one indication of 
the quality of the data is the initial performance 
of the classifier of the deployed system. (By 
?initial?, we mean the period during which no 
data from the live system had yet been used for 
training or updating of the system.) In our case, 
the initial accuracy was 75%, using 113 
application categories. We regard this as a high 
figure, also considering that it was achieved in 
spite of several new products having been 
introduced in the meantime that were not 
covered by the speech recognizer. The initial 
training of the speech recognizer and classifier 
used 25,000 utterances. As a comparison, when 
an additional 33,000 utterances (mostly from the 
live system) had been used for training, the 
accuracy increased to 85%. 
Acknowledgements 
Many colleagues have provided invaluable help 
and support throughout this project. Here we can 
only mention some of them: Johan Boye, Joakim 
Gustafson, Linda Bell, Fredrik Bystr?m, Robert 
Sandberg, Erik N?slund, Erik Demmelmaier, 
Viktoria Ahlbom, Inger Thall and Marco 
Petroni. Last but not least we thank our skilled 
wizards: Christina Carlson, Marie Hagdorn, 
Gunilla Johannisson, Ana-Maria Loriente, Maria 
Mellgren, Linda Norberg, Anne T?rk, Mikael 
Wikner, Eva Wintse and Jeanette ?berg. 
References  
Allwood, Jens & Bj?rn Haglund. 1992. 
Communicative Activity Analysis of a Wizard of 
Oz Experiment. Internal Report, PLUS ESPRIT 
project P5254. 
Ammicht, Egbert, Allen Gorin & Tirso Alonso. 1999. 
Knowledge Collection For Natural Language 
Spoken Dialog Systems. Proc. Eurospeech, 
Budapest, Hungary, Volume 3, pp. 1375?1378. 
Boye, Johan & Mats Wir?n. 2007. Multi-slot 
semantics for natural-language call routing 
systems. Proc. Bridging the Gap: Academic and 
Industrial Research in Dialog Technology. 
NAACL Workshop, Rochester, New York, USA. 
Dahlb?ck, Nils, Arne J?nsson & Lars Ahrenberg, 
Wizard of Oz Studies ? Why and How. 1993. 
Knowledge-Based Systems, vol. 6, no. 4, pp. 258?
266. Also in: Mark Maybury & Wolfgang 
Wahlster (eds.). 1998. Readings in Intelligent 
User Interfaces, Morgan Kaufmann.  
Di Fabbrizio, Giuseppe, Gokhan Tur & Dilek 
Hakkani-T?r. 2005. Automated Wizard-of-Oz for 
Spoken Dialogue Systems. Proc. Interspeech, 
Lisbon, Portugal, pp. 1857?1860. 
Fraser, Norman M. & G. Nigel Gilbert. Simulating 
speech systems. 1991. Computer Speech and 
Language, vol. 5, pp. 81?99.  
Gorin, A. L., G. Riccardi & J. H. Wright. 1997. How 
may I help you? Speech Communication, vol. 23, 
pp. 113?127. 
von Hahn, Walther. 1986. Pragmatic considerations 
in man?machine discourse. Proc. COLING, Bonn, 
Germany, pp. 520?526. 
Hirschman, L., M. Bates, D. Dahl, W. Fisher, J. 
Garofolo, D. Pallett, K. Hunicke-Smith, P. Price, 
A. Rudnicky & E. Tzoukermann. 1993. Multi-Site 
Data Collection and Evaluation in Spoken 
Language Understanding. Proc. ARPA Human 
Language Technology, Princeton, New Jersey, 
USA, pp. 19?24 .  
Oviatt, Sharon, Philip Cohen, Martin Fong & 
Michael Frank. 1992. A rapid semi-automatic 
simulation technique for investigating interactive 
speech and handwriting. Proc. ICSLP, Banff, 
Alberta, Canada, pp. 1351?1354. 
Walker, Marilyn, Irene Langkilde, Jerry Wright, 
Allen Gorin & Diane Litman. 2000. Learning to 
Predict Problematic Situations in a Spoken 
Dialogue System: Experiments with How May I 
Help You? Proc. North American Meeting of the 
Association for Computational Linguistics 
(NAACL), pp. 210?217. 
Zue, Victor, Nancy Daly, James Glass, David 
Goodine, Hong Leung, Michael Phillips, Joseph 
Polifroni, Stephanie Seneff & Michael Soclof. 
1989. The Collection and Preliminary Analysis of 
a Spontaneous Speech Database. Proc. DARPA 
Speech and Natural Language Workshop, 
pp. 126?134. 
63
Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 68?75,
NAACL-HLT, Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Multi-slot semantics for natural-language call routing systems 
 
 
Johan Boye and Mats Wir?n  
TeliaSonera R&D 
Vitsandsgatan 9 
SE-123 86 Farsta, Sweden 
johan.boye@teliasonera.com, mats.wiren@teliasonera.com 
 
 
 
 
Abstract 
Statistical classification techniques for 
natural-language call routing systems 
have matured to the point where it is pos-
sible to distinguish between several hun-
dreds of semantic categories with an 
accuracy that is sufficient for commercial 
deployments. For category sets of this 
size, the problem of maintaining consis-
tency among manually tagged utterances 
becomes limiting, as lack of consistency 
in the training data will degrade perform-
ance of the classifier. It is thus essential 
that the set of categories be structured in a 
way that alleviates this problem, and en-
ables consistency to be preserved as the 
domain keeps changing. In this paper, we 
describe our experiences of using a two-
level multi-slot semantics as a way of 
meeting this problem. Furthermore, we 
explore the ramifications of the approach 
with respect to classification, evaluation 
and dialogue design for call routing sys-
tems. 
1 Introduction 
Call routing is the task of directing callers to a ser-
vice agent or a self-service that can provide the 
required assistance. To this end, touch-tone menus 
are used in many call centers, but such menus are 
notoriously difficult to navigate if the number of 
destinations is large, resulting in many misdirected 
calls and frustrated customers. Natural-language 
call routing provides an approach to come to terms 
with these problems. The caller gets the opportu-
nity to express her reasons for calling using her 
own words, whereupon the caller?s utterance is 
automatically categorized and routed. 
This paper focuses on experiences obtained 
from the deployment of a call-routing application 
developed for the TeliaSonera residential customer 
care.1 The application was launched in 2006, re-
placing a previous system based on touch-tone 
menus. The customer care annually handles some 
14 million requests and questions concerning a 
wide range of products in fixed telephony, mobile 
telephony, modem-connected Internet, broadband, 
IP telephony and digital TV.  
The crucial step in any call routing application is 
classification, that is, the mapping of natural-
language utterances to categories that correspond 
to routing destinations. Early systems used quite 
small numbers of categories. For example, the 
original ?How May I Help You? system had 15 
categories (Gorin et al 1997), the system of Chu-
Carroll and Carpenter (1999) had 23 categories, 
and Cox and Shahshahani (2001) had 32. Nowa-
days, it is possible to distinguish between several 
hundreds of categories with high accuracy (see, for 
example, Speech Technology Magazine 2004). 
The TeliaSonera system currently distinguishes 
between 123 categories with an accuracy of 85% 
(using a speech recognizer and classifier developed 
by Nuance2). Moreover, according to our experi-
ments the same classification technology can be 
                                                          
1
 TeliaSonera (www.teliasonera.com) is the largest telecom operator in the 
Nordic?Baltic region in Europe. 
2
  www.nuance.com.  
68
used to distinguish between 1,500 categories with 
80% accuracy.3 
For large category sets like these, the problem of 
maintaining consistency among manually tagged 
utterances becomes limiting, as lack of consistency 
in the training data will degrade performance of the 
classifier. The problem is exacerbated by the fact 
that call-routing domains are always in a state of 
flux: Self-services are being added, removed, 
modified, split and merged. Organizational 
changes and product development regularly call for 
redefinitions of human expertise areas. All of these 
changes must be accommodated in the category 
set. Hence, it must be possible to update this set 
efficiently and at short intervals. 
To meet this problem, it is crucial that the set of 
categories be structured in a way that facilitates the 
task of manual tagging and enables consistency to 
be preserved. However, in spite of the fact that the 
size of category sets for call routing have increased 
dramatically since the original ?How May I Help 
You? system, we are not aware of any papers that 
systematically discuss how such large sets should 
be structured in order to be efficiently maintain-
able. Rather, many papers in the call-routing litera-
ture consider the call routing problem as an 
abstract classification task with atomic categories 
at a single level of abstraction. Such atomic cate-
gories are typically taken to correspond to depart-
ments and self-services of the organization to 
which the call center belongs. In a real-life imple-
mentation, the situation is often more complicated. 
At TeliaSonera, we have adopted a two-level 
multi-slot semantics as a way of maintaining 
modularity and consistency of a large set of cate-
gories over time. 
The aim of this paper is to share our experiences 
of this by providing a detailed description of the 
approach and its implications for classification, 
dialogue design and evaluation. The rest of the pa-
per is organized as follows: Section 2 describes the 
multi-slot category system. Sections 3?5 outline 
consequences of the multi-slot semantics for dis-
ambiguation, classification and evaluation, respec-
tively. Section 6 concludes. 
 
 
 
                                                          
3
 In both cases, the classifier was trained on 60,000 utterances. 
2 What?s in a category? 
2.1 Motivation 
As pointed out above, call-routing domains are 
always to some extent moving targets because of 
constant changes with respect to products and or-
ganization. It would be cumbersome to manually 
re-tag old data each time the category set is up-
dated. Retagging the training data for the statistical 
classifier might introduce inconsistencies into the 
training set and degrade classifier performance. 
Thus, it is a good idea to define two sets of catego-
ries at different levels; one set of semantic catego-
ries reflecting the contents of the utterance, and 
one set of application categories reflecting how the 
call should be handled. These two sets of catego-
ries are related by means of a many-to-one map-
ping from the semantic domain to the application 
domain. Figure 1 gives the general picture. 
 
 
 
Figure 1: Mapping between semantic categories and 
application categories. 
 
The utterances in the training set for the auto-
matic classifier are manually categorized using 
semantic categories. The automatic classifier can 
be trained to work either in the semantic domain or 
in the application domain (see further Section 4). 
 
 
Semantic categories Application categories 
69
2.2 Semantic categories 
In the TeliaSonera system, semantic categories are 
triples of the form 
 
( family, intention, object ) 
 
where family is the general product family which 
the call concerns (e.g. fixed telephony, mobile te-
lephony, broadband, etc.), intention represents the 
nature of the request (e.g. order, want-info, 
change-info, activate, want-support, report-error, 
etc.), and object represents more specifically what 
the call is about (e.g. particular names of products, 
or concepts like ?telephone number?, ?SIM card?, 
or ?password?). Currently there are 10 families, 
about 30 intentions, and about 170 objects that 
span the semantic domain.  
Some (in fact, the majority) of the possible tri-
ples are disallowed because they are nonsensical. 
For instance, it is not meaningful to combine 
?fixed telephony? in the family slot with ?SIM 
card? in the object slot. To cater for this, we have 
defined a set of combination rules weeding out the 
illegal combinations of values. These rules disal-
low about 80% of the possible combinations, leav-
ing about 10,000 permissible semantic triples. Of 
these 10,000 triples, about 1,500 have actually 
turned up in real data.  
The three-slot structure of categories is very use-
ful when performing manual tagging of the train-
ing material for the statistical classifier. Although 
there are 10,000 categories, the person performing 
the tagging needs only to keep track of about 210 
concepts (10 families + 30 intentions + 170 ob-
jects). In contrast, it is safe to say that an unstruc-
tured category system containing 10,000 atomic 
categories would be quite impractical to use.  
In addition, the combination rules can further al-
leviate the manual tagging task. It is straightfor-
ward to implement a tagging tool that allows the 
human tagger to select a value for one semantic 
slot, and then restrict the selection for the other 
slots only to include the possible values. For ex-
ample, if ?fixed telephony? is chosen for the family 
slot, ?SIM card? would not appear among the pos-
sible values for the object slot. This approach has 
been successfully adopted in the project. 
 
 
2.3 Application categories 
There is one application category for each type of 
action from the system. Actions come in two fla-
vors; either the call is routed (in the cases where 
the caller has given sufficient information), or the 
system asks a counter-question in order to extract 
more information from the caller. That is, applica-
tion categories can be labeled either as routing 
categories or disambiguation categories. For con-
venience, names of application categories are also 
triples, chosen among the set of semantic triples 
that map to that application category.  
2.4 Information ordering 
Each slot in a semantic triple can take the value 
unknown, representing the absence of information. 
For instance, the most accurate semantic category 
for the caller utterance ?Broadband?4 is (broad-
band, unknown, unknown), since nothing is known 
about the intention of the caller or the specific 
topic of the request. Thus, in the information order-
ing, ?unknown? is situated below all other values.  
There are also some intermediate values in the 
information ordering. The value telephony repre-
sents ?either fixed telephony or mobile telephony?, 
and has been incorporated in the category set since 
many callers tend not be explicit about this point. 
In the same vein, internet represents ?either broad-
band or modem-connected internet?, and billing 
represents the disjunction of a whole range of bill-
ing objects, some of which can be handled by a 
self-service and some can not. 
 
 
 
 
Figure 2: Parts of the semantic information ordering. 
 
 
The information ordering extends naturally to 
triples. In particular, the triple (unknown, unknown, 
                                                          
4
 Many callers express themselves in this telegraphic fashion. 
unknown 
telephony internet 
modemConnected broadband fixed mobile 
70
unknown) represents complete absence of informa-
tion. 
3 Disambiguation 
The caller?s request might be ambiguous in one 
sense or another, in which case the system will 
need to perform disambiguation by asking a fol-
low-up question. This might either be a general 
question encouraging the user to describe his re-
quest in greater detail, or a directed question of the 
type ?Would that be fixed telephony or mobile te-
lephony?? 
Ambiguous utterances might be represented in 
at least two fundamentally different ways. In vec-
tor-based approaches, routing destinations and in-
put utterances alike are represented by vectors in a 
multi-dimensional space. An input utterance is 
routed to a specific destination if the vector repre-
sentation of the utterance is close to that of the des-
tination. An ambiguous utterance is characterized 
by the fact that the Euclidean distances from the 
utterance vector to the n closest routing destination 
vectors are roughly the same.  
Chu-Carroll and Carpenter (1999) describe a 
method of disambiguation, where disambiguation 
questions are dynamically constructed on the basis 
of an analysis of the differences among the closest 
routing destination vectors. However, it is not clear 
that the disambiguation questions produced by 
their proposed method would make sense in all 
possible situations. Furthermore, their method does 
not take into account the fact that some ambiguities 
tend to be more important and arise more often 
than others. We think it is worthwhile to concen-
trate on these important cases (in terms of prompt 
design, speech recognition grammar construction, 
etc.), rather than trying to solve every conceivable 
ambiguity, most of which would never appear in 
real life.  
As previously mentioned, in the TeliaSonera 
system we have chosen another way of treating 
ambiguities, namely that certain application cate-
gories are disambiguation categories; they repre-
sent foreseen, frequently occurring, ambiguous 
input utterances. The three-slot structure of catego-
ries provides a handy way of identifying ambigu-
ous cases; they are represented by triples where 
one or more slots are unknown, or where some slot 
has an intermediate value, like telephony or inter-
net. Examples of such ambiguous utterances are 
?broadband? (broadband-unknown-unknown) and 
?I want to have a telephone subscription? (teleph-
ony-order-subscription). All categories that repre-
sent ambiguities have pre-prepared disambiguation 
questions, speech recognition grammars, and dia-
logue logic to handle the replies from the callers. 
Of course, there are still problematic cases 
where an utterance can not be assigned any unique 
category with any tolerable level of confidence, 
neither a routing category nor a disambiguation 
category. In those cases, the system simply re-
phrases the question: ?Sorry, I didn?t quite under-
stand that. Could you please rephrase??  
4 Classification 
4.1 Atomic vs. multi-slot classification 
For the purpose of automatic classification of ut-
terances, there are at least two different views one 
may adopt. In one view, the ?atomic? view, the 
three-slot structure of category names is considered 
as merely a linguistic convention, convenient only 
when manually tagging utterances (as discussed in 
Section 2.1). When adopting this view, we still 
regard the categories to be distinct atomic entities 
as concerns automatic classification. For instance, 
to the human eye it is obvious that two categories 
like (internet, order, subscription) and (broadband, 
order, subscription) are related, but the automatic 
classifier just considers them to be any two catego-
ries, each with its separate set of training examples.  
An alternative view, the ?multi-slot view?, is to 
see the category as actually consisting of three 
slots, each of which should be assigned a value 
independently. This means that a separate classifier 
is needed for each of the three slots. 
It is not clear which view is preferable. An ar-
gument in favor of the multi-slot view is the fol-
lowing: If some categories have the same value in 
one slot, then these categories are semantically 
related in some way. Most likely this semantic re-
lation is reflected by the use of common words and 
phrases; for instance, expressions like ?order? and 
?get a new? presumably are indicative for all cate-
gories having the value order in the intention slot. 
Therefore, classifying each slot separately would 
be a way to take a priori semantic knowledge into 
account.  
  To this, proponents of the atomic view may re-
spond that such similarities between categories 
71
would emerge anyway when using a single classi-
fier that decides the entire semantic triple in one go 
(provided that enough training data is available). In 
addition, if each slot is categorized separately, it is 
not certain that the resulting three values would 
constitute a permissible semantic triple (as men-
tioned in Section 2.1, about 80% of the possible 
combinations are illegal). In contrast, if a single 
classifier is used, the result will always be a legal 
triple, since only legal triples appear in the training 
material. 
The statistical classifier actually used in the live 
call routing system treats categories as atomic enti-
ties and, as mentioned in the introduction, it works 
well. The encouraging numbers bear out that the 
?atomic? view is viable when lots of data is at 
hand. On the other hand, if training data is sparse, 
one might consider using a hand-written, rule-
based classifier, and in these cases the multi-slot 
view seems more natural.  
4.2 Rule-based multi-slot classification 
To obtain a baseline for the performance of the 
statistical classifier used in the live system, we im-
plemented an alternative classifier that solves the 
classification task using hand-written rules. Thus, 
the purpose of this was to investigate the perform-
ance of a na?ve classification method, and use that 
for comparison with other methods. In addition, 
the rule-based classifier provides an example of 
how the multi-slot approach can support the inclu-
sion of human a priori domain knowledge into the 
classification process. 
The rule-based classifier has three kinds of 
rules: Firstly, phrase-spotting rules associate a 
word or a phrase with a value for a semantic slot 
(i.e. a family, an intention, or an object). Rules of 
the second kind are domain axioms that encode 
invariant relationships, such as the fact that ob-
ject=SIMcard implies family=mobileTelephony. 
Finally, rules of the third kind specify how seman-
tic values can be combined into a legal semantic 
triple (these rules are also used for manual tagging, 
as mentioned in Section 2.1). Each semantic value 
is also (manually) given a score that reflects its 
information content; a higher score means that the 
value contains more information. For instance, the 
value subscription has a lower information score 
than have the names of specific subscription types 
that TeliaSonera offers its customers.  
The classifier works in three phases, which we 
will demonstrate on a running example. In the first 
phase, it applies the phrase-spotting rules to the 
input sentence, returning a list of slot-value pairs. 
For instance, the input sentence ?I want to order a 
new SIM card? would yield the list [ inten-
tion=order, object=SIMcard ], using rules trigger-
ing on the phrases ?order? and ?SIM card? in the 
input sentence.  
Secondly, the classifier adds semantic compo-
nents as a result of applying the domain axioms to 
members of the list. Using the domain axiom men-
tioned above, the semantic component fam-
ily=mobileTelephony would be added to the list, 
due to the presence of object=SIMcard. Thus, after 
the two first phases, the intermediate result in this 
example is [intention=order, object=SIMcard,  
family=mobileTelephony]. 
In the final phase, semantic components are se-
lected from the list to form a semantic triple. In the 
example, this step is straightforward since the list 
contains exactly one value for each component, 
and these values are combinable according to the 
combination rules. The final result is: 
 
( mobileTelephony, order, SIMcard ) 
 
In cases where the semantic values in the list are 
not combinable (a situation often originating from 
a speech recognition error), one or several values 
have got to be relaxed to unknown. According to 
our experiments, the best heuristic is to first relax 
the object component and then the intention com-
ponent. For example, in the list [family = fixed-
Telephony, intention=order, object=SIMcard], the 
first and third elements are not combinable; thus 
this list yields the triple: 
 
( fixedTelephony, order, unknown ) 
 
In the case where some slots are not filled in 
with a value, the values of those slots are set to 
unknown.  Thus, the list [ family=fixedTelephony, 
intention=order ] would also yield the semantic 
triple above. 
 Finally, consider the case where the input list 
contains more than one value for one or several 
slots. In this case, the algorithm picks the value 
with the highest information content score. For 
instance, consider the utterance ?I want to have a 
broadband subscription, this eh ADSL I?ve read 
72
about?. After the first two phases, the algorithm 
has found family=broadband, intention=order, 
and two possible values for the object slot, namely 
object=subscription and object=ADSL. Since the 
latter has higher information score, the final result 
is: 
 
( broadband, order, ADSL ) 
 
The rule-based classifier was developed in about 
five man-weeks, and contains some 3,000 hand-
written rules. When evaluated on a set of 2,300 
utterances, it classified 67% of the utterances cor-
rectly. Thus, not surprisingly, its performance is 
significantly below the statistical classifier used in 
the deployed system. Still, the rule-based approach 
might be a viable alternative in less complex do-
mains. It might also be usable for data collection 
purposes in early prototypes of natural-language 
call routing systems. 
5 Evaluation of call-routing dialogues 
5.1 Motivation 
An important issue in the development of any dia-
logue system is the selection of an evaluation met-
ric to quantify performance improvements. In the 
call-routing area, there have been many technical 
papers specifically comparing the performance of 
classifiers, using standard metrics such as accuracy 
of the semantic categories obtained over a test cor-
pus (see e.g. Kuo and Lee, 2000, and Sarikaya et 
al., 2005). Accuracy is then stated as a percentage 
figure showing the degree of the categories that 
have been completely correctly classified, given 
that categories are atomic. There have also been 
some design-oriented papers that try to assess the 
effects of different prompt styles by looking at the 
proportion of routable versus unroutable calls 
given callers? first utterances. Thus, both of these 
strands of work base their evaluations on binary 
divisions between correct/incorrect and rout-
able/unroutable, respectively. Furthermore, they 
both constitute utterance-based metrics in the sense 
that they focus on the outcome of a single system?
caller turn. 
An excellent example of a design-oriented call-
routing paper is Williams and Witt (2004), which 
among other things compares open and directed 
prompt styles in the initial turn of the dialogue. 
Williams and Witt divide callers? responses into 
Routable (if the utterance contained sufficient in-
formation for the call to be routed) or Failure (if 
the utterance did not contain sufficient information 
for routing). Depending on why a call is not rout-
able, Williams and Witt further subdivide instances 
of Failure into three cases: Confusion (utterances 
such as ?Hello?? and ?Is this a real person??), 
Agent (the caller requests to speak to a human 
agent), and Unroutable (which corresponds to ut-
terances that need disambiguation). Thus, Williams 
and Witt?s performance metric uses altogether four 
labels. (In addition, they have three labels related 
to non-speech events: silence, DTMF and hang-up. 
Since such events are not handled by the classifier, 
they fall outside of the scope of this paper.) 
Although all of Williams? and Witt?s measures 
are needed in evaluating call-routing dialogue, the 
field clearly needs more in-depth evaluation. In 
particular, we need more fine-grained metrics in 
order to probe more exactly to what extent Failure 
actually means that the dialogue is off track. Fur-
thermore, given that call-routing dialogues typi-
cally consist of between one and (say) five turns, 
we need not just utterance-based metrics, but also 
dialogue-based metrics ? in other words, being 
able to evaluate the efficiency of an overall dia-
logue. 
5.2 Utterance-based metrics 
When assessing the performance of classification 
methods, it is perfectly reasonable to use the binary 
distinction correct/incorrect if only few categories 
are used. In such a context it can be assumed that 
different categories correspond to different de-
partments of the organization, and that a misclassi-
fication would lead the call being routed the wrong 
way. However, with a richer category system, it is 
important to realize that the classifier can be par-
tially correct. For instance, if the caller expresses 
that he wants technical support for his broadband 
connection, then the information that the purpose 
of the call has something to do with broadband is 
surely better than no information at all. If the sys-
tem obtains this information, it could ask a directed 
follow-up question: OK broadband. Please tell me 
if your call concerns an order, billing, deliveries, 
support, error report, or something else, or some-
thing to that effect. Otherwise, the system can only 
restate the original question. 
73
In the field of task-oriented dialogue, several 
evaluation metrics have been put forward that go 
beyond a simple division into correct/incorrect. In 
particular, concept accuracy (Boros et al 1996) is 
an attempt to find a semantic analogue of word 
accuracy as used in speech recognition. Basically, 
the idea is to compute the degree of correctness of 
a semantic analysis based on a division of the rep-
resentation into subunits, and by taking into ac-
count insertions, deletions and replacements of 
these subunits. 
Making use of our multi-slot semantics, we can 
take subunits to correspond to semantic slot values. 
An insertion has occurred if the classifier spuri-
ously has added information to some slot value 
(e.g. if the classifier outputs the value broadband 
for the family slot, when the correct value is inter-
net or unknown). Conversely, a deletion has oc-
curred when semantic triple output from the 
classifier contains a slot value which is situated 
lower than the correct value in the information or-
dering (a part of which is depicted in Figure 2). 
Finally, a replacement has occurred when the com-
puted slot value and the correct slot value are unre-
lated in the information ordering. 
By using concept accuracy as an evaluation met-
ric for classifiers rather than the binary distinction 
correct/incorrect, we can arrive at more informa-
tive assessments. This possibility is brought about 
by the multi-slot structure of categories.  
5.3 Dialogue-based metrics 
In the literature, there have also been proposals for 
dialogue-based metrics. In particular, Glass et al 
(2000) put forward two such metrics, query density 
(QD) and concept efficiency (CE). Query density is 
the mean number of new ?concepts? introduced 
per user query, assuming that each concept corre-
sponds to a slot?filler pair in the representation of 
the query. For example, a request such as ?I?d like 
a flight from Stockholm to Madrid on Sunday af-
ternoon? would introduce three new concepts, cor-
responding to departure, destination and time. 
Query density thus measures the rate at which the 
user communicates content. In contrast, concept 
efficiency measures the average number of turns it 
takes for a concept to be successfully understood 
by the system. Concept efficiency thus measures 
the rate at which the system understands content.  
Using the multi-slot semantics, we can adapt the 
notions of query density and concept efficiency in 
order to arrive at a more fine-grained performance 
metric for call routing. The basic idea is to regard 
every element in the semantic triple as one ?con-
cept?. We can then obtain a measure of how in-
formation increases in the dialogue by computing 
the difference between triples in each user utter-
ance, where ?difference? means that the values of 
two corresponding elements are not equal. 
An example of computing query density is given 
below. We assume that the value of the semantic 
triple is initially (unknown, unknown, unknown). 
 
System: Welcome to TeliaSonera. How may I help 
you? 
Caller: Fixed telephony. 
 (fixedTelephony, unknown, unknown) 
1 new concept 
System: Could you tell me some more about what 
you want to do? 
Caller: I can?t use my broadband while I?m speak-
ing on the phone.(broadband, reportProb-
lem, lineOrPhone) 
3 new concepts 
 
Note that query density and concept efficiency 
are both applicable on a per-utterance basis as well 
as on the whole dialogue (or indeed arbitrary 
stretches of the dialogue). To compute these meas-
ures for the whole dialogue, we simply compute 
the mean number of new concepts introduced per 
user utterance and the average number of turns it 
takes for a concept to be successfully understood, 
respectively. 
The principal application of this methodology is 
to measure the effectiveness of system utterances. 
When using a fine-grained system of categories, it 
is important that callers express themselves at a 
suitable level of detail. Too verbose user utterances 
are usually difficult to analyse, but too telegraphic 
user utterances are not good either, as they most 
often do not contain enough information to route 
the call directly. Therefore it is very important to 
design system utterances so as to make users give 
suitably expressive descriptions of their reasons for 
calling.  
By using the query density metric it is possible 
to asses the effectiveness (in the above sense) of 
different alternative system utterances at various 
points in the dialogue, most notably the first sys-
74
tem utterance. Again, this possibility is brought 
about by the multi-slot structure of categories. It is 
also possible to evaluate more general dialogue 
strategies over longer stretches of dialogue (e.g. 
the use of general follow-up questions like ?Could  
you please tell me some more about what you want 
to do? as opposed to more directed questions like 
?Please tell me if your call concerns an order, bill-
ing, deliveries, support, error report, or something 
else?). By calculating the average query density 
over a number of consecutive utterances, it is pos-
sible to compare the relative merits of different 
such dialogue strategies. 
We have not yet adopted this metric for evalua-
tion of dialogues from the live system. However, 
elsewhere we have applied it to dialogues from the 
initial Wizard-of-Oz data collection for the Telia-
Sonera call routing system (Wir?n et al 2007). 
Here, we used it to compare two styles of disam-
biguation prompts, one completely open and one 
more directed. 
6 Concluding remarks 
In the literature, the natural-language call routing 
problem is often presented as the problem of clas-
sifying spoken utterances according to a set of 
atomic categories. The hypothesis underlying this 
paper is that this view is inadequate, and that there 
is a need for a more structured semantics. We base 
our claims on experiences gathered from the de-
velopment and deployment of the TeliaSonera call 
center, for which we developed a multi-slot system 
of categories. 
A multi-slot semantics offers several advan-
tages. First of all, it makes the set of categories 
manageable for human taggers, and provides a 
means to break down the tagging task into sub-
tasks. Furthermore, we have shown how multi-slot 
semantics for call-routing systems allows straight-
forward division of categories into routing catego-
ries and disambiguation categories, the possibility 
of multi-slot categorization, and the use of more 
fine-grained evaluation metrics like concept accu-
racy and query density. 
Acknowledgements 
This work has benefited greatly from discussions 
on category systems and classification with Marco 
Petroni, Linda Brostr?m, Per-Olof G?llstedt, Alf 
Bergstrand and Erik Demmelmaier, and we thank 
them all. We would also like to thank Robert 
Sandberg and Erik N?slund for their support of this 
work. 
References  
Boros, M., Eckert, W., Gallwitz, F., G?rz, G., Han-
rieder, G. and Niemann, H. (1996). Towards under-
standing spontaneous speech: Word accuracy vs. 
concept accuracy. Proc. Fourth International Con-
ference on Spoken Language Processing (ICSLP), 
pp. 1009?1012.  
Chu-Carroll, J. and Carpenter, B. (1999) Vector-based 
natural language call routing. Computational linguis-
tics, 25(3), pp. 361-388. 
Cox, S. and Shahshahani, B. (2001). A comparison of 
some different techniques for vector based call-
routing. Proc. Eurospeech, Aalborg, Denmark. 
Glass, J., Polifroni, J., Seneff, S. and Zue, V. Data col-
lection and performance evaluation of spoken dia-
logue systems: The MIT experience. In Proc. Sixth 
International Conference on Spoken Language Proc-
essing (ICSLP), Beijing, China. 
Gorin, A., Riccardi, G., and Wright, J. (1997) How may 
I help you?. Journal of Speech Communication, 23, 
pp. 113-127. 
Kuo, H-K J. and Lee, C-H. (2000)  Discriminative train-
ing in natural language call routing. Proc. Sixth In-
ternational Conference on Spoken Language 
Processing (ICSLP), Beijing, China. 
Sarikaya, R, Kuo, H-K J., Goel, V. and Gao, Y. (2005) 
Exploiting unlabeled data using multiple classifiers 
for improved natural language call-routing. Proc. In-
terspeech, Lisbon, Portugal. 
Speech Technology Magazine (2004) Q&A with Bell 
Canada?s Belinda Banks, senior associate director, 
customer care. Speech Technology Magazine, vol 9, 
no 3.  
Williams, Jason D. and Witt, Silke M. (2004). A com-
parison of dialog strategies for call routing. Interna-
tional Journal of Speech Technology 7(1), pp. 9?24. 
Wir?n, M., Eklund, R., Engberg, F. and Westermark, J. 
(2007). Experiences of an in-service Wizard-of-Oz 
data collection for the deployment of a call-routing 
application. Proc. Bridging the gap: Academic and 
industrial research in dialog technology. NAACL 
workshop, Rochester, New York, USA. 
 
 
75
Book Review
Language and Computers
Markus Dickinson*, Chris Brew?, and Detmar Meurers?
(*Indiana University, ?Educational Testing Service, and ?University of Tu?bingen)
Wiley-Blackwell, 2013, xviii+232 pp; paperbound, ISBN 978-1-4051-8305-5, $34.95;
hardbound, ISBN 978-4051-8306-2, $87.95; e-book, ISBN 978-1-1183-2316-8, $22.99
Reviewed by
Mats Wire?n
Stockholm University
Any textbook on computational linguistics today must position itself relative to Jurafsky
and Martin (2009), which, by virtue of its depth and comprehensiveness, is the standard
introduction and reference for speech and language processing. Readers without sub-
stantial background may find that book too demanding, however, and then Language
and Computers, by Dickinson, Brew, and Meurers, may be a better choice. Roughly, this
is how Language and Computers distinguishes itself from Jurafsky and Martin (2009):
1. It does not presuppose any computing background. More specifically, as
stated in the Overview for Instructors, the book assumes ?no mathematical
or linguistic background beyond normal high-school experience.? The
book certainly has the ambition to explain some of the inner workings of
the technology, but does so starting from a general level.
2. It is structured according to major applications (treated in six chapters),
with the theoretical material being introduced along the way as it is
needed. Jurafsky and Martin (2009) is instead structured according
to linguistic description levels, starting with words and gradually
proceeding through higher levels (albeit with a final part on applications).
The idea in Language and Computers is that readers without a background
in computing will at least be familiar with real-world tasks in which
computers deal with language, and will find a textbook structured in
this way more accessible. It also gives the authors an opportunity to
show how the underlying techniques recur across different applications.
3. Its topic is processing of text. There is some discussion in the first chapter
of the nature and representation of speech, but the applications (with the
possible exception of dialogue) are text-oriented.
4. It is introductory, typically aimed at a quarter-length course according to
the Overview for Instructors. At 232 pages, it is about one-quarter of the
length of Jurafsky and Martin (2009), which has enough material for a
full-year course.
? 2013 Association for Computational Linguistics
doi:10.1162/COLI r 00165
Computational Linguistics Volume 39, Number 3
Chapter 1 is a prologue with a refreshingly broad perspective, dealing with how
written and spoken language are represented in computers. First, the basic writ-
ing systems are described: alphabetical (roughly, one character?one phoneme), syl-
labic (roughly, one character?one syllable), logographic (roughly, one character?one
morpheme/word), and major encodings are addressed (in particular, Unicode). A de-
scription of the nature of speech and its representation in waveforms and spectrograms
follows, including sections on how to read spectrograms and on language modeling
using n-grams. The two latter sections come under the heading of ?Under the Hood.?
Sections so headed (typically, one or two per chapter) provide digressions into selected
technical material that provide bonuses for the interested reader, but which can be
omitted without losing the gist of each chapter.
Chapter 2 brings up the first application, ?Writer?s Aids,? which consists of two
parts: checking spelling and checking grammar. The first part includes an overview of
different kinds of spelling errors, and methods for detection and correction of errors.
These include a description of dynamic programming for calculating the minimum edit
distance between a misspelled word and a set of candidate corrections. The second
part deals with grammar checking. To set the scene for this, there is first an exposi-
tion of context-free grammar for the purpose of specifying the norm (the well-formed
sentences) of the language. (The possibility of enriching the formalism with features
is mentioned later in the same chapter.) It is then stated that ?[w]hen a parser fails to
parse a sentence, we have an indication that something is grammatically wrong with
the sentence? (page 58). At the same time, it is recognized that the most one can do
is to build grammar fragments. So what about undergeneration, and the challenges of
maintaining a large, manually encoded grammar? Instead of elaborating on this, the
chapter goes on with a brief description of methods for correcting errors. These include
relaxation-based techniques that discharge grammar rules, and special-purpose rules
or n-grams that trigger directly on erroneous sequences of words. The reader is left
wondering what one could expect from a grammar fragment. Although this may not
be relevant to grammar checking, it would have been instructive to have a mention
somewhere of the possibility of augmenting context-free rules or other formalisms with
probabilities, and of how this has affected wide-coverage parsing (Clark and Curran
2007).
Chapter 3 deals with language tutoring systems and how to make them linguis-
tically aware. To this end, the concepts (but not the inner workings) of tokenization,
part-of-speech tagging, and syntactic parsing are described. An example language
tutoring system for learners of Portuguese is then addressed. Compared with traditional
workbook exercises, the system gives immediate feedback on orthographic, syntactic,
and semantic errors, and also contains audio.
The topic of Chapter 4, ?Searching,? is mainly related to information retrieval. Most
of the chapter is contained in two comprehensive sections that deal with searching in
unstructured data (typically the Web) and semi-structured data (such as Wikipedia).
The former section contains a relatively detailed description of search engines and
PageRank, as well as an overview of HTML. Evaluation of search results is mentioned,
but the measures are described in more detail in the next chapter. The section on semi-
structured data contains a description of regular expressions, Unix grep, and finite-
state automata. The chapter also contains brief sections on searching of structured data
(databases, using Boolean expressions) and of text corpora (mainly discussing corpus
annotation). Given that many of the readers of this book will be linguists, it is perhaps
surprising that the book has so little material related to corpus linguistics, but it could be
argued that this topic would require a text of its own (and that it is not an ?application?).
778
Book Review
Anyway, the ?Further Reading? section of the chapter includes some good suggestions
on this topic.
Chapter 5 brings up document classification, and begins with an overview of
concepts in machine learning. Then there is a detailed description of how to measure
success in classification, with precision/recall, true/false positives/negatives, and
the related measures of sensitivity and specificity used in medicine. After this, two
examples of document classifiers are described in some detail: naive Bayes and the
perceptron. Finally, there is a short section on sentiment analysis. This chapter has a
good balance between applications-oriented and theoretical material, and gives a good
grasp of each of them.
Chapter 6 deals with dialogue systems. After some motivation, the chapter
examines in detail an example spoken dialogue transcript from the Carnegie Mellon
University Let?s Go! Bus Information System, followed by a thorough description of
dialogue moves, speech acts, and Grice?s conversational maxims. After this ambitious
background, one would expect some (even superficial) material on the anatomy of a
dialogue system founded on these principles, but instead there is a detailed description
of Eliza. The motivation is that ?[Let?s Go!] is . . . too complicated to be explained fully in
this textbook? (page 167). The ambition to explain a working system in full is admirable,
but seems misdirected here. Why not try to work out the basic principles of a simpler
question-answering system? As it stands, the applications-oriented material (Eliza) is
not connected to the theoretical parts of the chapter. It is also potentially misleading
when the authors say: ?[Eliza] works reasonably well using simple means, and this can
be useful if your application calls for a straightforward but limited way of creating the
illusion of an intelligent being at the other end of the wire? (page 170). Here, the authors
must be referring to chatbots, but for applications in general, it would have been valu-
able to have a pointer to principles of user interface design, such as trying to maintain
consistency (Cohen, Giangola, and Balogh 2004). On a different note, this chapter would
profit from picking up the thread on speech from Chapter 1. For example, it might be
instructive to have an explanation of word-error rate and the degradation of input typi-
cally caused by a speech recognizer. This would also give an opportunity to elaborate
on one of the problems mentioned at the outset of the chapter: ?[f]ixing confusions and
misunderstandings before they cause the conversation to break down? (page 154).
Chapter 7 brings up the final application, machine translation. This is a chapter
that gives a good grasp of both applications-oriented and theoretical material. Example-
based translation and translation memories are briefly discussed, and, after some back-
ground, word alignment, IBM Model 1, the noisy channel model, and phrase-based
statistical translation are explained. Commercial translation is also addressed. In con-
nection with the translation triangle, the possibility of an interlingua is discussed, the
conclusion being that ?[d]espite great efforts, nobody has ever managed to design or
build a suitable interlingua? (page 189) and ?we probably cannot have one any time
soon? (page 190). This is true for a universal interlingua in the sense of a fully abstract
language-independent representation, but what might be more relevant is domain-
specific interlinguas, which have proved to be feasible in recent years (Ranta 2011).
Interlinguas are also mentioned later in the chapter, at the end of ?Under the Hood
12? (page 205) on phrase-based statistical translation, where it is stated that ?[t]here is
certainly no interlingua in sight.? Although this is different, Google Translate actually
uses English as an interlingua for a large majority of its language pairs (Ranta 2010).
Surely, the reason for this is that Google typically has much more parallel data for
language pairs where English is one of the members than for language pairs where
this is not the case.
779
Computational Linguistics Volume 39, Number 3
The final chapter (?Epilogue?) brings up some philosophical questions: the im-
pact of language technology on society (effects of automatization of natural-language
tasks), the human communication system as opposed to animal communication,
human self-perception when computers begin to use language to interact, and ethical
considerations.
A good index is crucial in a book like this, also because it helps to fulfill the aim of
showing the reader how the underlying techniques recur across applications. However,
for numerous terms such as n-gram, part-of-speech tagging, parsing, supervised learning,
and so forth, many (sometimes a majority) of the occurrences in the book are not covered
by the index. It would be highly desirable to improve this for a future edition.
In sum, although a couple of chapters could have had a better balance or connection
between the applications-oriented and theoretical material, the overall impression is
that Language and Computers successfully fills a niche: It serves its purpose well in being
an introductory textbook in computational linguistics for people without a computing
background. It is possible to debate both the applications chosen and the theory selected
for presentation, but it is nonetheless good to see a general introduction with a clear
ambition to explain some of the inner workings of the technology.
I teach an introductory course in computational linguistics for Master?s students
in linguistics who lack a computing background. My experience is that the primary
need of these students is corpus linguistics, because, above all, they need to learn how
to practice linguistics as an empirical science. They soon reach a point, however, where
they also need a broader introduction to the applications and methods of computational
linguistics. Such a broad introduction is also needed by Bachelor?s students in linguis-
tics, and by those studying to become language consultants, translators, and so on. This
book would then be the natural choice.
References
Clark, Stephen and James R. Curran.
2007. Wide-coverage efficient statistical
parsing with CCG and log-linear
models. Computational Linguistics
33(4):493?552.
Cohen, Michael H., James P. Giangola, and
Jennifer Balogh. 2004. Voice User Interface
Design. Addison-Wesley.
Jurafsky, Daniel and James H. Martin.
2009. Speech and Language Processing:
An Introduction to Natural Language
Processing, Speech Recognition, and
Computational Linguistics. 2nd edition.
Prentice-Hall.
Ranta, Aarne. 2010. Report from Google
EMEA Faculty Summit 8?10 February
2010. http://www.molto-project
.eu/node/842.
Ranta, Aarne. 2011. Grammatical Framework:
Programming with Multilingual Grammars.
CSLI Publications, Stanford, CA.
Mats Wire?n is Associate Professor in computational linguistics at Stockholm University. He re-
ceived his M.Sc. and Ph.D. from Linko?ping University. His current research focuses on language
acquisition, whereas a long-time interest of his is parsing. He has also carried out research in
speech translation and spoken dialogue. Wire?n?s address is Department of Linguistics, Stockholm
University, SE-106 91 Stockholm, Sweden; e-mail: mats.wiren@ling.su.se.
780
