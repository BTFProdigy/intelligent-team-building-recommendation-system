Proceedings of the 43rd Annual Meeting of the ACL, pages 379?386,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Semantic Approach to IE Pattern Induction
Mark Stevenson and Mark A. Greenwood
Department of Computer Science
University of Sheffield
Sheffield, S1 4DP, UK
marks,m.greenwood@dcs.shef.ac.uk
Abstract
This paper presents a novel algorithm for
the acquisition of Information Extraction
patterns. The approach makes the assump-
tion that useful patterns will have simi-
lar meanings to those already identified
as relevant. Patterns are compared using
a variation of the standard vector space
model in which information from an on-
tology is used to capture semantic sim-
ilarity. Evaluation shows this algorithm
performs well when compared with a
previously reported document-centric ap-
proach.
1 Introduction
Developing systems which can be easily adapted to
new domains with the minimum of human interven-
tion is a major challenge in Information Extraction
(IE). Early IE systems were based on knowledge en-
gineering approaches but suffered from a knowledge
acquisition bottleneck. For example, Lehnert et al
(1992) reported that their system required around
1,500 person-hours of expert labour to modify for
a new extraction task. One approach to this problem
is to use machine learning to automatically learn the
domain-specific information required to port a sys-
tem (Riloff, 1996). Yangarber et al (2000) proposed
an algorithm for learning extraction patterns for a
small number of examples which greatly reduced the
burden on the application developer and reduced the
knowledge acquisition bottleneck.
Weakly supervised algorithms, which bootstrap
from a small number of examples, have the advan-
tage of requiring only small amounts of annotated
data, which is often difficult and time-consuming
to produce. However, this also means that there
are fewer examples of the patterns to be learned,
making the learning task more challenging. Pro-
viding the learning algorithm with access to addi-
tional knowledge can compensate for the limited
number of annotated examples. This paper presents
a novel weakly supervised algorithm for IE pattern
induction which makes use of the WordNet ontology
(Fellbaum, 1998).
Extraction patterns are potentially useful for many
language processing tasks, including question an-
swering and the identification of lexical relations
(such as meronomy and hyponymy). In addition, IE
patterns encode the different ways in which a piece
of information can be expressed in text. For exam-
ple, ?Acme Inc. fired Jones?, ?Acme Inc. let Jones
go?, and ?Jones was given notice by his employers,
Acme Inc.? are all ways of expressing the same fact.
Consequently the generation of extraction patterns is
pertinent to paraphrase identification which is cen-
tral to many language processing problems.
We begin by describing the general process of pat-
tern induction and an existing approach, based on
the distribution of patterns in a corpus (Section 2).
We then introduce a new algorithm which makes use
of WordNet to generalise extraction patterns (Sec-
tion 3) and describe an implementation (Section 4).
Two evaluation regimes are described; one based on
the identification of relevant documents and another
which aims to identify sentences in a corpus which
379
are relevant for a particular IE task (Section 5). Re-
sults on each of these evaluation regimes are then
presented (Sections 6 and 7).
2 Extraction Pattern Learning
We begin by outlining the general process of learn-
ing extraction patterns, similar to one presented by
(Yangarber, 2003).
1. For a given IE scenario we assume the exis-
tence of a set of documents against which the
system can be trained. The documents are
unannotated and may be either relevant (con-
tain the description of an event relevant to the
scenario) or irrelevant although the algorithm
has no access to this information.
2. This corpus is pre-processed to generate the set
of all patterns which could be used to represent
sentences contained in the corpus, call this set
S. The aim of the learning process is to identify
the subset of S representing patterns which are
relevant to the IE scenario.
3. The user provides a small set of seed patterns,
Sseed, which are relevant to the scenario. These
patterns are used to form the set of currently
accepted patterns, Sacc, so Sacc ? Sseed. The
remaining patterns are treated as candidates for
inclusion in the accepted set, these form the set
Scand(= S ? Sacc).
4. A function, f , is used to assign a score to
each pattern in Scand based on those which
are currently in Sacc. This function as-
signs a real number to candidate patterns so
? c  Scand, f(c, Sacc) 7? <. A set of high
scoring patterns (based on absolute scores or
ranks after the set of patterns has been ordered
by scores) are chosen as being suitable for in-
clusion in the set of accepted patterns. These
form the set Slearn.
5. The patterns in Slearn are added to Sacc and
removed from Scand, so Sacc ? Sacc ? Slearn
and Scand ? Sacc ? Slearn
6. If a suitable set of patterns has been learned
then stop, otherwise go to step 4
2.1 Document-centric approach
A key choice in the development of such an algo-
rithm is step 4, the process of ranking the candidate
patterns, which effectively determines which of the
candidate patterns will be learned. Yangarber et al
(2000) chose an approach motivated by the assump-
tion that documents containing a large number of
patterns already identified as relevant to a particu-
lar IE scenario are likely to contain further relevant
patterns. This approach, which can be viewed as be-
ing document-centric, operates by associating confi-
dence scores with patterns and relevance scores with
documents. Initially seed patterns are given a maxi-
mum confidence score of 1 and all others a 0 score.
Each document is given a relevance score based on
the patterns which occur within it. Candidate pat-
terns are ranked according to the proportion of rele-
vant and irrelevant documents in which they occur,
those found in relevant documents far more than in
irrelevant ones are ranked highly. After new patterns
have been accepted all patterns? confidence scores
are updated, based on the documents in which they
occur, and documents? relevance according to the
accepted patterns they contain.
This approach has been shown to successfully ac-
quire useful extraction patterns which, when added
to an IE system, improved its performance (Yangar-
ber et al, 2000). However, it relies on an assump-
tion about the way in which relevant patterns are dis-
tributed in a document collection and may learn pat-
terns which tend to occur in the same documents as
relevant ones whether or not they are actually rele-
vant. For example, we could imagine an IE scenario
in which relevant documents contain a piece of in-
formation which is related to, but distinct from, the
information we aim to extract. If patterns expressing
this information were more likely to occur in rele-
vant documents than irrelevant ones the document-
centric approach would also learn the irrelevant pat-
terns.
Rather than focusing on the documents matched
by a pattern, an alternative approach is to rank pat-
terns according to how similar their meanings are
to those which are known to be relevant. This
semantic-similarity approach avoids the problem
which may be present in the document-centric ap-
proach since patterns which happen to co-occur in
the same documents as relevant ones but have dif-
ferent meanings will not be ranked highly. We now
go on to describe a new algorithm which implements
this approach.
380
3 Semantic IE Pattern Learning
For these experiments extraction patterns consist of
predicate-argument structures, as proposed by Yan-
garber (2003). Under this scheme patterns consist
of triples representing the subject, verb and object
(SVO) of a clause. The first element is the ?se-
mantic? subject (or agent), for example ?John? is a
clausal subject in each of these sentences ?John hit
Bill?, ?Bill was hit by John?, ?Mary saw John hit
Bill?, and ?John is a bully?. The second element is
the verb in the clause and the third the object (pa-
tient) or predicate. ?Bill? is a clausal object in the
first three example sentences and ?bully? in the final
one. When a verb is being used intransitively, the
pattern for that clause is restricted to only the first
pair of elements.
The filler of each pattern element can be either
a lexical item or semantic category such as per-
son name, country, currency values, numerical ex-
pressions etc. In this paper lexical items are rep-
resented in lower case and semantic categories are
capitalised. For example, in the pattern COM-
PANY+fired+ceo, fired and ceo are lexical
items and COMPANY a semantic category which
could match any lexical item belonging to that type.
The algorithm described here relies on identify-
ing patterns with similar meanings. The approach
we have developed to do this is inspired by the
vector space model which is commonly used in
Information Retrieval (Salton and McGill, 1983)
and language processing in general (Pado and La-
pata, 2003). Each pattern can be represented as
a set of pattern element-filler pairs. For exam-
ple, the pattern COMPANY+fired+ceo consists
of three pairs: subject COMPANY, verb fired
and object ceo. Each pair consists of either a
lexical item or semantic category, and pattern ele-
ment. Once an appropriate set of pairs has been es-
tablished a pattern can be represented as a binary
vector in which an element with value 1 denotes that
the pattern contains a particular pair and 0 that it
does not.
3.1 Pattern Similarity
The similarity of two pattern vectors can be com-
pared using the measure shown in Equation 1. Here
~a and~b are pattern vectors, ~bT the transpose of~b and
Patterns Matrix labels
a. chairman+resign 1. subject chairman
b. ceo+quit 2. subject ceo
c. chairman+comment 3. verb resign
4. verb quit
5. verb comment
Similarity matrix Similarity values
1 0.95 0 0 0
0.95 1 0 0 0
0 0 1 0.9 0.1
0 0 0.9 1 0.1
0 0 0.1 0.1 1
sim(~a,~b) = 0.925
sim(~a, ~c) = 0.55
sim(~b, ~c) = 0.525
Figure 1: Similarity scores and matrix for an exam-
ple vector space formed from three patterns
W a matrix that lists the similarity between each of
the possible pattern element-filler pairs.
sim(~a,~b) = ~aW
~bT
|~a||~b|
(1)
The semantic similarity matrix W contains infor-
mation about the similarity of each pattern element-
filler pair stored as non-negative real numbers and is
crucial for this measure. Assume that the set of pat-
terns, P , consists of n element-filler pairs denoted
by p1, p2, ...pn. Each row and column of W rep-
resents one of these pairs and they are consistently
labelled. So, for any i such that 1 ? i ? n, row i and
column i are both labelled with pair pi. If wij is the
element of W in row i and column j then the value
of wij represents the similarity between the pairs pi
and pj . Note that we assume the similarity of two
element-filler pairs is symmetric, so wij = wji and,
consequently, W is a symmetric matrix. Pairs with
different pattern elements (i.e. grammatical roles)
are automatically given a similarity score of 0. Di-
agonal elements of W represent the self-similarity
between pairs and have the greatest values.
Figure 1 shows an example using three patterns,
chairman+resign, ceo+quit and chair-
man+comment. This shows how these patterns are
represented as vectors and gives a sample semantic
similarity matrix. It can be seen that the first pair
of patterns are the most similar using the proposed
measure.
The measure in Equation 1 is similar to the cosine
metric, commonly used to determine the similarity
of documents in the vector space model approach
381
to Information Retrieval. However, the cosine met-
ric will not perform well for our application since it
does not take into account the similarity between el-
ements of a vector and would assign equal similarity
to each pair of patterns in the example shown in Fig-
ure 1.1 The semantic similarity matrix in Equation 1
provides a mechanism to capture semantic similar-
ity between lexical items which allows us to identify
chairman+resign and ceo+quit as the most
similar pair of patterns.
3.2 Populating the Matrix
It is important to choose appropriate values for the
elements of W . We chose to make use of the re-
search that has concentrated on computing similar-
ity between pairs of lexical items using the WordNet
hierarchy (Resnik, 1995; Jiang and Conrath, 1997;
Patwardhan et al, 2003). We experimented with
several of the measures which have been reported
in the literature and found that the one proposed by
Jiang and Conrath (1997) to be the most effective.
The similarity measure proposed by Jiang and
Conrath (1997) relies on a technique developed by
Resnik (1995) which assigns numerical values to
each sense in the WordNet hierarchy based upon
the amount of information it represents. These val-
ues are derived from corpus counts of the words in
the synset, either directly or via the hyponym rela-
tion and are used to derive the Information Content
(IC) of a synset c thus IC(c) = ? log(Pr(c)). For
two senses, s1 and s2, the lowest common subsumer,
lcs(s1, s2), is defined as the sense with the highest
information content (most specific) which subsumes
both senses in the WordNet hierarchy. Jiang and
Conrath used these elements to calculate the seman-
tic distance between a pair or words, w1 and w2, ac-
cording to this formula (where senses(w) is the set
1The cosine metric for a pair of vectors is given by the cal-
culation a.b|a||b| . Substituting the matrix multiplication in the nu-
merator of Equation 1 for the dot product of vectors ~a and ~b
would give the cosine metric. Note that taking the dot product
of a pair of vectors is equivalent to multiplying by the identity
matrix, i.e. ~a.~b = ~aI ~bT . Under our interpretation of the simi-
larity matrix, W , this equates to each pattern element-filler pair
being identical to itself but not similar to anything else.
of all possible WordNet senses for word w):
ARGMAX
s1  senses(w1),
s2  senses(w2)
IC(s1)+IC(s2)?2?IC(lcs(s1, s2))
(2)
Patwardhan et al (2003) convert this distance
metric into a similarity measure by taking its mul-
tiplicative inverse. Their implementation was used
in the experiments described later.
As mentioned above, the second part of a pattern
element-filler pair can be either a lexical item or a
semantic category, such as company. The identifiers
used to denote these categories, i.e. COMPANY, do
not appear in WordNet and so it is not possible to
directly compare their similarity with other lexical
items. To avoid this problem these tokens are man-
ually mapped onto the most appropriate node in the
WordNet hierarchy which is then used for similar-
ity calculations. This mapping process is not partic-
ularly time-consuming since the number of named
entity types with which a corpus is annotated is usu-
ally quite small. For example, in the experiments
described in this paper just seven semantic classes
were sufficient to annotate the corpus.
3.3 Learning Algorithm
This pattern similarity measure can be used to create
a weakly supervised approach to pattern acquisition
following the general outline provided in Section 2.
Each candidate pattern is compared against the set
of currently accepted patterns using the measure de-
scribed in Section 3.1. We experimented with sev-
eral techniques for ranking candidate patterns based
on these scores, including using the best and aver-
age score, and found that the best results were ob-
tained when each candidate pattern was ranked ac-
cording to its score when compared against the cen-
troid vector of the set of currently accepted patterns.
We also experimented with several schemes for de-
ciding which of the scored patterns to accept (a full
description would be too long for this paper) result-
ing in a scheme where the four highest scoring pat-
terns whose score is within 0.95 of the best pattern
are accepted.
Our algorithm disregards any patterns whose cor-
pus occurrences are below a set threshold, ?, since
these may be due to noise. In addition, a second
382
threshold, ?, is used to determine the maximum
number of documents in which a pattern can occur
since these very frequent patterns are often too gen-
eral to be useful for IE. Patterns which occur in more
than ? ?C, where C is the number of documents in
the collection, are not learned. For the experiments
in this paper we set ? to 2 and ? to 0.3.
4 Implementation
A number of pre-processing stages have to be ap-
plied to documents in order for the set of patterns to
be extracted before learning can take place. Firstly,
items belonging to semantic categories are identi-
fied by running the text through the named entity
identifier in the GATE system (Cunningham et al,
2002). The corpus is then parsed, using a ver-
sion of MINIPAR (Lin, 1999) adapted to process
text marked with named entities, to produce depen-
dency trees from which SVO-patterns are extracted.
Active and passive voice is taken into account in
MINIPAR?s output so the sentences ?COMPANY
fired their C.E.O.? and ?The C.E.O. was fired by
COMPANY? would yield the same triple, COM-
PANY+fire+ceo. The indirect object of ditran-
sitive verbs is not extracted; these verbs are treated
like transitive verbs for the purposes of this analysis.
An implementation of the algorithm described
in Section 3 was completed in addition to an im-
plementation of the document-centric algorithm de-
scribed in Section 2.1. It is important to mention
that this implementation is not identical to the one
described by Yangarber et al (2000). Their system
makes some generalisations across pattern elements
by grouping certain elements together. However,
there is no difference between the expressiveness of
the patterns learned by either approach and we do
not believe this difference has any effect on the re-
sults of our experiments.
5 Evaluation
Various approaches have been suggested for the
evaluation of automatic IE pattern acquisition.
Riloff (1996) judged the precision of patterns
learned by reviewing them manually. Yangarber et
al. (2000) developed an indirect method which al-
lowed automatic evaluation. In addition to learning
a set of patterns, their system also notes the rele-
vance of documents based on the current set of ac-
cepted patterns. Assuming the subset of documents
relevant to a particular IE scenario is known, it is
possible to use these relevance judgements to de-
termine how accurately a given set of patterns can
discriminate the relevant documents from the irrele-
vant. This evaluation is similar to the ?text-filtering?
sub-task used in the sixth Message Understanding
Conference (MUC-6) (1995) in which systems were
evaluated according to their ability to identify the
documents relevant to the extraction task. The doc-
ument filtering evaluation technique was used to al-
low comparison with previous studies.
Identifying the document containing relevant in-
formation can be considered as a preliminary stage
of an IE task. A further step is to identify the sen-
tences within those documents which are relevant.
This ?sentence filtering? task is a more fine-grained
evaluation and is likely to provide more information
about how well a given set of patterns is likely to
perform as part of an IE system. Soderland (1999)
developed a version of the MUC-6 corpus in which
events are marked at the sentence level. The set of
patterns learned by the algorithm after each iteration
can be compared against this corpus to determine
how accurately they identify the relevant sentences
for this extraction task.
5.1 Evaluation Corpus
The evaluation corpus used for the experiments was
compiled from the training and testing corpus used
in MUC-6, where the task was to extract information
about the movements of executives from newswire
texts. A document is relevant if it has a filled tem-
plate associated with it. 590 documents from a ver-
sion of the MUC-6 evaluation corpus described by
Soderland (1999) were used.
After the pre-processing stages described in Sec-
tion 4, the MUC-6 corpus produced 15,407 pattern
tokens from 11,294 different types. 10,512 patterns
appeared just once and these were effectively dis-
carded since our learning algorithm only considers
patterns which occur at least twice (see Section 3.3).
The document-centric approach benefits from a
large corpus containing a mixture of relevant and ir-
relevant documents. We provided this using a subset
of the Reuters Corpus Volume I (Rose et al, 2002)
which, like the MUC-6 corpus, consists of newswire
383
COMPANY+appoint+PERSON
COMPANY+elect+PERSON
COMPANY+promote+PERSON
COMPANY+name+PERSON
PERSON+resign
PERSON+depart
PERSON+quit
Table 1: Seed patterns for extraction task
texts. 3000 documents relevant to the management
succession task (identified using document meta-
data) and 3000 irrelevant documents were used to
produce the supplementary corpus. This supple-
mentary corpus yielded 126,942 pattern tokens and
79,473 types with 14,576 of these appearing more
than once. Adding the supplementary corpus to the
data set used by the document-centric approach led
to an improvement of around 15% on the document
filtering task and over 70% for sentence filtering. It
was not used for the semantic similarity algorithm
since there was no benefit.
The set of seed patterns listed in Table 1 are in-
dicative of the management succession extraction
task and were used for these experiments.
6 Results
6.1 Document Filtering
Results for both the document and sentence filter-
ing experiments are reported in Table 2 which lists
precision, recall and F-measure for each approach
on both evaluations. Results from the document fil-
tering experiment are shown on the left hand side
of the table and continuous F-measure scores for
the same experiment are also presented in graphi-
cal format in Figure 2. While the document-centric
approach achieves the highest F-measure of either
system (0.83 on the 33rd iteration compared against
0.81 after 48 iterations of the semantic similarity ap-
proach) it only outperforms the proposed approach
for a few iterations. In addition the semantic sim-
ilarity approach learns more quickly and does not
exhibit as much of a drop in performance after it has
reached its best value. Overall the semantic sim-
ilarity approach was found to be significantly bet-
ter than the document-centric approach (p < 0.001,
Wilcoxon Signed Ranks Test).
Although it is an informative evaluation, the doc-
ument filtering task is limited for evaluating IE pat-
0 20 40 60 80 100 120
Iteration
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
F-
m
ea
su
re
Semantic Similarity
Document-centric
Figure 2: Evaluating document filtering.
tern learning. This evaluation indicates whether the
set of patterns being learned can identify documents
containing descriptions of events but does not pro-
vide any information about whether it can find those
events within the documents. In addition, the set of
seed patterns used for these experiments have a high
precision and low recall (Table 2). We have found
that the distribution of patterns and documents in
the corpus means that learning virtually any pattern
will help improve the F-measure. Consequently, we
believe the sentence filtering evaluation to be more
useful for this problem.
6.2 Sentence Filtering
Results from the sentence filtering experiment are
shown in tabular format in the right hand side of
Table 22 and graphically in Figure 3. The seman-
tic similarity algorithm can be seen to outperform
the document-centric approach. This difference is
also significant (p < 0.001, Wilcoxon Signed Ranks
Text).
The clear difference between these results shows
that the semantic similarity approach can indeed
identify relevant sentences while the document-
centric method identifies patterns which match rel-
evant documents, although not necessarily relevant
sentences.
2The set of seed patterns returns a precision of 0.81 for this
task. The precision is not 1 since the pattern PERSON+resign
matches sentences describing historical events (?Jones resigned
last year.?) which were not marked as relevant in this corpus
following MUC guidelines.
384
Document Filtering Sentence Filtering
Number of Document-centric Semantic similarity Document-centric Semantic similarity
Iterations P R F P R F P R F P R F
0 1.00 0.26 0.42 1.00 0.26 0.42 0.81 0.10 0.18 0.81 0.10 0.18
20 0.75 0.68 0.71 0.77 0.78 0.77 0.30 0.29 0.29 0.61 0.49 0.54
40 0.72 0.96 0.82 0.70 0.93 0.80 0.40 0.67 0.51 0.47 0.64 0.55
60 0.65 0.96 0.78 0.68 0.96 0.80 0.32 0.70 0.44 0.42 0.73 0.54
80 0.56 0.96 0.71 0.61 0.98 0.76 0.18 0.71 0.29 0.37 0.89 0.52
100 0.56 0.96 0.71 0.58 0.98 0.73 0.18 0.73 0.28 0.28 0.92 0.42
120 0.56 0.96 0.71 0.58 0.98 0.73 0.17 0.75 0.28 0.26 0.95 0.41
Table 2: Comparison of the different approaches over 120 iterations
0 20 40 60 80 100 120
Iteration
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
F-
m
ea
su
re
Semantic Similarity
Document-centric
Figure 3: Evaluating sentence filtering.
The precision scores for the sentence filtering task
in Table 2 show that the semantic similarity al-
gorithm consistently learns more accurate patterns
than the existing approach. At the same time it
learns patterns with high recall much faster than the
document-centric approach, by the 120th iteration
the pattern set covers almost 95% of relevant sen-
tences while the document-centric approach covers
only 75%.
7 Discussion
The approach to IE pattern acquisition presented
here is related to other techniques but uses differ-
ent assumptions regarding which patterns are likely
to be relevant to a particular extraction task. Eval-
uation has showed that the semantic generalisa-
tion approach presented here performs well when
compared to a previously reported document-centric
method. Differences between the two approaches
are most obvious when the results of the sentence
filtering task are considered and it seems that this is
a more informative evaluation for this problem. The
semantic similarity approach has the additional ad-
vantage of not requiring a large corpus containing a
mixture of documents relevant and irrelevant to the
extraction task. This corpus is unannotated, and so
may not be difficult to obtain, but is nevertheless an
additional requirement.
The best score recorded by the proposed algo-
rithm on the sentence filtering task is an F-measure
of 0.58 (22nd iteration). While this result is lower
than those reported for IE systems based on knowl-
edge engineering approaches these results should be
placed in the context of a weakly supervised learning
algorithm which could be used to complement man-
ual approaches. These results could be improved by
manual filtering the patterns identified by the algo-
rithm.
The learning algorithm presented in Section 3 in-
cludes a mechanism for comparing two extraction
patterns using information about lexical similarity
derived from WordNet. This approach is not re-
stricted to this application and could be applied to
other language processing tasks such as question an-
swering, paraphrase identification and generation or
as a variant of the vector space model commonly
used in Information Retrieval. In addition, Sudo
et al (2003) proposed representations for IE pat-
terns which extends the SVO representation used
here and, while they did not appear to significantly
improve IE, it is expected that it will be straightfor-
ward to extend the vector space model to those pat-
385
tern representations.
One of the reasons for the success of the approach
described here is the appropriateness of WordNet
which is constructed on paradigmatic principles,
listing the words which may be substituted for one
another, and is consequently an excellent resource
for this application. WordNet is also a generic
resource not associated with a particular domain
which means the learning algorithm can make use
of that knowledge to acquire patterns for a diverse
range of IE tasks. This work represents a step to-
wards truly domain-independent IE systems. Em-
ploying a weakly supervised learning algorithm re-
moves much of the requirement for a human anno-
tator to provide example patterns. Such approaches
are often hampered by a lack of information but the
additional knowledge in WordNet helps to compen-
sate.
Acknowledgements
This work was carried out as part of the RE-
SuLT project funded by the EPSRC (GR/T06391).
Roman Yangarber provided advice on the re-
implementation of the document-centric algorithm.
We are also grateful for the detailed comments pro-
vided by the anonymous reviewers of this paper.
References
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: an Architecture for Devel-
opment of Robust HLT. In Proceedings of the 40th
Anniversary Meeting of the Association for Computa-
tional Linguistics (ACL-02), pages 168?175, Philadel-
phia, PA.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lexi-
cal Database and some of its Applications. MIT Press,
Cambridge, MA.
J. Jiang and D. Conrath. 1997. Semantic similarity based
on corpus statistics and lexical taxonomy. In Proceed-
ings of International Conference on Research in Com-
putational Linguistics, Taiwan.
W. Lehnert, C. Cardie, D. Fisher, J. McCarthy, E. Riloff,
and S. Soderland. 1992. University of Massachusetts:
Description of the CIRCUS System used for MUC-4.
In Proceedings of the Fourth Message Understanding
Conference (MUC-4), pages 282?288, San Francisco,
CA.
D. Lin. 1999. MINIPAR: a minimalist parser. In Mary-
land Linguistics Colloquium, University of Maryland,
College Park.
MUC. 1995. Proceedings of the Sixth Message Under-
standing Conference (MUC-6), San Mateo, CA. Mor-
gan Kaufmann.
S. Pado and M. Lapata. 2003. Constructing semantic
space models from parsed corpora. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL-03), pages 128?135, Sap-
poro, Japan.
S. Patwardhan, S. Banerjee, and T. Pedersen. 2003. Us-
ing measures of semantic relatedness for word sense
disambiguation. In Proceedings of the Fourth Inter-
national Conferences on Intelligent Text Processing
and Computational Linguistics, pages 241?257, Mex-
ico City.
P. Resnik. 1995. Using Information Content to evalu-
ate Semantic Similarity in a Taxonomy. In Proceed-
ings of the 14th International Joint Conference on Ar-
tificial Intelligence (IJCAI-95), pages 448?453, Mon-
treal, Canada.
E. Riloff. 1996. Automatically generating extraction
patterns from untagged text. In Thirteenth National
Conference on Artificial Intelligence (AAAI-96), pages
1044?1049, Portland, OR.
T. Rose, M. Stevenson, and M. Whitehead. 2002. The
Reuters Corpus Volume 1 - from Yesterday?s news to
tomorrow?s language resources. In LREC-02, pages
827?832, La Palmas, Spain.
G. Salton and M. McGill. 1983. Introduction to Modern
Information Retrieval. McGraw-Hill, New York.
S. Soderland. 1999. Learning Information Extraction
Rules for Semi-structured and free text. Machine
Learning, 31(1-3):233?272.
K. Sudo, S. Sekine, and R. Grishman. 2003. An Im-
proved Extraction Pattern Representation Model for
Automatic IE Pattern Acquisition. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL-03), pages 224?231.
R. Yangarber, R. Grishman, P. Tapanainen, and S. Hut-
tunen. 2000. Automatic acquisition of domain
knowledge for information extraction. In Proceed-
ings of the 18th International Conference on Compu-
tational Linguistics (COLING 2000), pages 940?946,
Saarbru?cken, Germany.
R. Yangarber. 2003. Counter-training in the discovery of
semantic patterns. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics (ACL-03), pages 343?350, Sapporo, Japan.
386
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 200?201,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
SUPPLE: A Practical Parser for Natural Language Engineering
Applications
Robert Gaizauskas, Mark Hepple, Horacio Saggion,
Mark A. Greenwood and Kevin Humphreys?
Department of Computer Science
University of Sheffield, Sheffield, UK
{robertg|hepple|saggion|m.greenwood|-}@dcs.shef.ac.uk
Abstract
We describe SUPPLE, a freely-available,
open source natural language parsing sys-
tem, implemented in Prolog, and designed
for practical use in language engineering
(LE) applications. SUPPLE can be run as
a stand-alone application, or as a compo-
nent within the GATE General Architec-
ture for Text Engineering. SUPPLE is dis-
tributed with an example grammar that has
been developed over a number of years
across several LE projects. This paper de-
scribes the key characteristics of the parser
and the distributed grammar.
1 Introduction
In this paper we describe SUPPLE1 ? the Sheffield
University Prolog Parser for Language Engineering
? a general purpose parser that produces both syn-
tactic and semantic representations for input sen-
tences, which is well-suited for a range of LE ap-
plications. SUPPLE is freely available, and is dis-
tributed with an example grammar for English that
was developed across a number of LE projects. We
will describe key characteristics of the parser and the
grammar in turn.
2 The SUPPLE Parser
SUPPLE is a general purpose bottom-up chart parser
for feature-based context free phrase structure gram-
?At Microsoft Corporation since 2000 (Speech and Natural
Language Group). Email: kevinhum@microsoft.com.
1In previous published materials and in the current GATE
release the parser is referred to as buChart. This is name is now
deprecated.
mars (CF-PSGs), written in Prolog, that has a num-
ber of characteristics making it well-suited for use
in LE applications. It is available both as a language
processing resource within the GATE General Ar-
chitecture for Text Engineering (Cunningham et al,
2002) and as a standalone program requiring vari-
ous preprocessing steps to be applied to the input.
We will here list some of its key characteristics.
Firstly, the parser allows multiword units identi-
fied by earlier processing components, e.g. named
entity recognisers (NERs), gazetteers, etc, to be
treated as non-decomposable units for syntactic pro-
cessing. This is important as the identification of
such items is an essential part of analyzing real text
in many domains.
The parser allows a layered parsing process, with
a number of separate grammars being applied in se-
ries, one on top of the other, with a ?best parse? se-
lection process between stages so that only a sub-
set of the constituents constructed at each stage is
passed forward to the next. While this may make
the parsing process incomplete with respect to the
total set of analyses licensed by the grammar rules,
it makes the parsing process much more efficient and
allows a modular development of sub-grammars.
Facilities are provided to simplify handling
feature-based grammars. The grammar representa-
tion uses flat, i.e. non-embedded, feature represen-
tations which are combined used Prolog term uni-
fication for efficiency. Features are predefined and
source grammars compiled into a full form repre-
sentation, allowing grammar writers to include only
relevant features in any rule, and to ignore feature or-
dering. The formalism also permits disjunctive and
optional right-hand-side constituents.
The chart parsing algorithm is simple but very
200
efficient, exploiting the characteristics of Prolog to
avoid the need for active edges or an agenda. In in-
formal testing, this approach was roughly ten times
faster than a related Prolog implementation of stan-
dard bottom-up active chart parsing.
The parser does not fail if full sentential parses
cannot be found, but instead outputs partial anal-
yses as syntactic and semantic fragments for user-
selectable syntactic categories. This makes the
parser robust in applications which deal with large
volumes of real text.
3 The Sample Grammar
The sample grammar distributed with SUPPLE has
been developed over several years, across a number
LE projects. We here list some key characteristics.
The morpho-syntactic and semantic information
required for individual lexical items is minimal ?
inflectional root and word class only, where the word
class inventory is basically the PTB tagset.
A conservative philosophy is adopted regarding
identification of verbal arguments and attachment of
nominal and verbal post-modifiers, such as preposi-
tional phrases and relative clauses. Rather than pro-
ducing all possible analyses or using probabilities to
generate the most likely analysis, the preference is to
offer a single analysis that spans the input sentence
only if it can be relied on to be correct, so that in
many cases only partial analyses are produced. The
philosophy is that it is more useful to produce par-
tial analyses that are correct than full analyses which
may well be wrong or highly disjunctive. Output
from the parser can be passed to further processing
components which may bring additional information
to bear in resolving attachments.
An analysis of verb phrases is adopted in which
a core verb cluster consisting of verbal head plus
auxiliaries and adverbials is identified before any at-
tempt to attach any post-verbal arguments. This con-
trasts with analyses where complements are attached
to the verbal head at a lower level than auxiliaries
and adverbials, e.g. as in the Penn TreeBank. This
decision is again motivated by practical concerns: it
is relatively easy to recognise verbal clusters, much
harder to correctly attach complements.
A semantic analysis, or simplified quasi-logical
form (SQLF), is produced for each phrasal con-
stituent, in which tensed verbs are interpreted as re-
ferring to unique events, and noun phrases as refer-
ring to unique objects. Where relations between syn-
tactic constituents are identified in parsing, semantic
relations between associated objects and events are
asserted in the SQLF.
While linguistically richer grammatical theories
could be implemented in the grammar formalism
of SUPPLE, the emphasis in our work has been on
building robust wide-coverage tools ? hence the re-
quirement for only minimal lexical morphosyntac-
tic and semantic information. As a consequence the
combination of parser and grammars developed to
date results in a tool that, although capable of return-
ing full sentence analyses, more commonly returns
results that include chunks of analysis with some,
but not all, attachment relations determined.
4 Downloading SUPPLE Resources
SUPPLE resources, including source code and the
sample grammar, and also a longer paper providing
a more detailed account of both the parser and gram-
mar, are available from the supple homepage at:
http://nlp.shef.ac.uk/research/supple
5 Conclusion
The SUPPLE parser has served as a component in
numerous LE research projects, and is currently in
use in a Question Answering system which partic-
ipated in recent TREC/QA evaluations. We hope
its availability as a GATE component will facilitate
its broader use by NLP researchers, and by others
building applications exploiting NL technology.
Acknowledgements
The authors would like to acknowledge the sup-
port of the UK EPSRC under grants R91465 and
K25267, and also the contributions of Chris Huyck
and Sam Scott to the parser code and grammars.
References
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. GATE: A framework and graphical devel-
opment environment for robust NLP tools and applica-
tions. Proceedings of the 40th Anniversary Meeting of
the Association for Computational Linguistics, 2002.
201
Proceedings of the Workshop on Information Extraction Beyond The Document, pages 12?19,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Comparing Information Extraction Pattern Models
Mark Stevenson and Mark A. Greenwood
Department of Computer Science
University of Sheffield
Sheffield, S1 4DP, UK
{marks,m.greenwood}@dcs.shef.ac.uk
Abstract
Several recently reported techniques for
the automatic acquisition of Information
Extraction (IE) systems have used depen-
dency trees as the basis of their extrac-
tion pattern representation. These ap-
proaches have used a variety of pattern
models (schemes for representing IE pat-
terns based on particular parts of the de-
pendency analysis). An appropriate model
should be expressive enough to represent
the information which is to be extracted
from text without being overly compli-
cated. Four previously reported pattern
models are evaluated using existing IE
evaluation corpora and three dependency
parsers. It was found that one model,
linked chains, could represent around 95%
of the information of interest without gen-
erating an unwieldy number of possible
patterns.
1 Introduction
A common approach to Information Extraction
(IE) is to use patterns which match against text
and identify items of interest. Patterns are applied
to text which has undergone various levels of lin-
guistic analysis, such as phrase chunking (Soder-
land, 1999) and full syntactic parsing (Gaizauskas
et al, 1996). The approaches use different defini-
tions of what constitutes a valid pattern. For exam-
ple, the AutoSlog system (Riloff, 1993) uses pat-
terns which match certain grammatical categories,
mainly nouns and verbs, in phrase chunked text
while Yangarber et al (2000) use subject-verb-
object tuples derived from a dependency parse. An
appropriate pattern language must encode enough
information about the text to be able to accurately
identify the items of interest. However, it should
not contain so much information as to be complex
and impractical to apply.
Several recent approaches to IE have used pat-
terns based on a dependency analysis of the input
text (Yangarber, 2003; Sudo et al, 2001; Sudo et
al., 2003; Bunescu and Mooney, 2005; Stevenson
and Greenwood, 2005). These approaches have
used a variety of pattern models (schemes for rep-
resenting IE patterns based on particular parts of
the dependency tree). For example, Yangarber
(2003) uses just subject-verb-object tuples while
Sudo et al (2003) allow any subpart of the tree to
act as an extraction pattern. The set of patterns al-
lowed by the first model is a proper subset of the
second and therefore captures less of the informa-
tion contained in the dependency tree. Little anal-
ysis has been carried out into the appropriateness
of each model. Sudo et al (2003) compared three
models in terms of their ability to identify event
participants.
The choice of pattern model has an effect on
the number of potential patterns. This has impli-
cations on the practical application for each ap-
proach, particularly when used for automatic ac-
quisition of IE systems using learning methods
(Yangarber et al, 2000; Sudo et al, 2003; Bunescu
and Mooney, 2005). This paper evaluates the ap-
propriateness of four pattern models in terms of
the competing aims of expressive completeness
(ability to represent information in text) and com-
plexity (number of possible patterns). Each model
is examined by comparing it against a corpus an-
notated with events and determining the propor-
tion of those which it is capable of representing.
The remainder of this paper is organised as fol-
lows: a variety of dependency-tree-based IE pat-
12
Figure 1: An example dependency tree.
tern models are introduced (Sections 2 and 3).
Section 4 describes experiments comparing each
model and the results are discussed in Section 5.
2 Pattern Models
In dependency analysis (Mel?c?uk, 1987) the syn-
tax of a sentence is represented by a set of directed
binary links between a word (the head) and one of
its modifiers. These links may be labelled to in-
dicate the grammatical relation between the head
and modifier (e.g. subject, object). In general
cyclical paths are disallowed so that the analysis
forms a tree structure. An example dependency
analysis for the sentence ?Acme Inc. hired Mr
Smith as their new CEO, replacing Mr Bloggs.?
is shown Figure 1.
The remainder of this section outlines four mod-
els for representing extraction patterns which can
be derived from dependency trees.
Predicate-Argument Model (SVO): A simple
approach, used by Yangarber (2003) and Steven-
son and Greenwood (2005), is to use subject-verb-
object tuples from the dependency parse as extrac-
tion patterns. These consist of a verb and its sub-
ject and/or direct object1. An SVO pattern is ex-
tracted for each verb in a sentence. Figure 2 shows
the two SVO patterns2 which are produced for the
dependency tree shown in Figure 1.
This model may be motivated by the assump-
tion that many IE scenarios involve the extraction
1Yangarber et al (2000) and Sudo et al (2003) used a
slightly extended version of this model in which the pattern
also included certain phrases which referred to either the sub-
ject or object.
2The formalism used for representing dependency pat-
terns is similar to the one introduced by Sudo et al (2003).
Each node in the tree is represented in the format a[b/c]
(e.g. subj[N/bomber]) where c is the lexical item
(bomber), b its grammatical tag (N) and a the dependency
relation between this node and its parent (subj). The rela-
tionship between nodes is represented as X(A+B+C) which
indicates that nodes A, B and C are direct descendents of node
X.
of participants in specific events. For example,
the MUC-6 (MUC, 1995) management succession
scenario concerns the identification of individuals
who are changing job. These events are often de-
scribed using a simple predicate argument struc-
ture, e.g. ?Acme Inc. fired Smith?. However,
the SVO model cannot represent information de-
scribed using other linguistic constructions such as
nominalisations or prepositional phrases. For ex-
ample, in the MUC6 texts it is common for job ti-
tles to be mentioned within prepositional phrases,
e.g. ?Smith joined Acme Inc. as CEO?.
Chains: A pattern is defined as a path between
a verb node and any other node in the dependency
tree passing through zero or more intermediate
nodes (Sudo et al, 2001). Figure 2 shows the eight
chains which can be extracted from the tree in Fig-
ure 1.
Chains provide a mechanism for encoding in-
formation beyond the direct arguments of predi-
cates and includes areas of the dependency tree ig-
nored by the SVO model. For example, they can
represent information expressed as a nominalisa-
tion or within a prepositional phrase, e.g. ?The
resignation of Smith from the board of Acme ...?
However, a potential shortcoming of this model is
that it cannot represent the link between arguments
of a verb. Patterns in the chain model format are
unable to represent even the simplest of sentences
containing a transitive verb, e.g. ?Smith left Acme
Inc.?.
Linked Chains: The linked chains model
(Greenwood et al, 2005) represents extraction
patterns as a pair of chains which share the same
verb but no direct descendants. This model gen-
erates 14 patterns for the verb hire in Figure 1,
examples of which are shown in Figure 2. This
pattern representation encodes most of the infor-
mation in the sentence with the advantage of being
able to link together event participants which nei-
ther of the SVO or chain model can, for example
the relation between ?Smith? and ?Bloggs?.
Subtrees: The final model to be considered is
the subtree model (Sudo et al, 2003). In this
model any subtree of a dependency tree can be
used as an extraction pattern, where a subtree is
any set of nodes in the tree which are connected to
one another. Single nodes are not considered to be
subtrees. The subtree model is a richer representa-
tion than those discussed so far and can represent
any part of a dependency tree. Each of the previ-
13
SVO Chains
[V/hire](subj[N/Acme Inc.]+obj[N/Mr Smith]) [V/hire](subj[N/Acme Inc.])
[V/replace](obj[N/Mr Bloggs]) [V/hire](obj[N/Mr Smith])
[V/hire](obj[N/Mr Smith](as[N/CEO]))
[V/hire](obj[N/Mr Smith](as[N/CEO](gen[N/their])))
[V/hire](obj[N/Mr Smith](as[N/CEO](mod[A/new])))
[V/hire](vpsc mod[V/replace])
[V/hire](vpsc mod[V/replace](obj[N/Mr Bloggs]))
[V/replace](obj[N/Mr Bloggs])
Linked Chains
[V/hire](subj[N/Acme Inc.]+obj[N/Mr Smith])
[V/hire](subj[N/Acme Inc.]+obj[N/Mr Smith](as[N/CEO]))
[V/hire](obj[N/Mr Smith]+vpsc mod[V/replace](obj[N/Mr Bloggs]))
Figure 2: Example patterns for three models
ous models form a proper subset of the subtrees.
By choosing an appropriate subtree it is possible
to link together any pair of nodes in a tree and
consequently this model can represent the relation
between any set of items in the sentence.
3 Pattern Enumeration and Complexity
In addition to encoding different parts of the de-
pendency analysis, each pattern model will also
generate a different number of potential patterns.
A dependency tree, T , can be viewed as a set
of N connected nodes. Assume that V , such that
V ? N , is the set of nodes in the dependency tree
labelled as a verb.
Predicate-Argument Model (SVO): The num-
ber of SVO patterns extracted from T is:
Nsvo (T ) = |V | (1)
Chain Model: A chain can be created between
any verb and a node it dominates (directly or indi-
rectly). Now assume that d(v) denotes the count
of a node v and all its descendents then the number
of chains is given by:
Nchains (T ) =
?
v?V
( d (v) ? 1 ) (2)
Linked Chains: Let C(v) denote the set of di-
rect child nodes of node v and vi denote the i-th
child, so C(v) =
{
v1, v2, ...v|C(v)|
}
. The number
of possible linked chains in T is given by:
Nlinked chains (T ) =
?
v?V
|C(v)|
?
i=1
|C(v)|
?
j=i+1
d (vi) d (vj)
(3)
Subtrees: Now assume that sub(n) is a func-
tion denoting the number of subtrees, including
single nodes, rooted at node n. This can be de-
fined recursively as follows:
sub(n) =
?
?
?
1 if n is a leaf node
|C(n)|
?
i=1
(sub (ni) + 1) otherwise
(4)
The total number of subtrees in a tree is given
by:
Nsubtree (T ) =
(
?
n?N
sub(n)
)
? |N | (5)
The dependency tree shown in Figure 1 gener-
ates 2, 8, 14 and 42 possible SVO, chain, linked
chain and subtree patterns respectively. The num-
ber of SVO patterns is constant on the number of
verbs in the tree. The number of chains is gener-
ally a linear function on the size of the tree but,
in the worst case, can be polynomial. The linked
chain model generates a polynomial number of
patterns while the subtree model is exponential.
There is a clear tradeoff between the complex-
ity of pattern representations and the practicality
of computation using them. Some pattern rep-
resentations are more expressive, in terms of the
amount of information from the dependency tree
they make use of, than others (Section 2) and are
therefore more likely to produce accurate extrac-
tion patterns. However, the more expressive mod-
els will add extra complexities during computation
since a greater number of patterns will be gen-
erated. This complexity, both in the number of
patterns produced and the computational effort re-
quired to produce them, limits the algorithms that
can reasonably be applied to learn useful extrac-
tion patterns.
For a pattern model to be suitable for an ex-
traction task it needs to be expressive enough to
encode enough information from the dependency
parse to accurately identify the items which need
to be extracted. However, we also aim for the
14
model to be as computationally tractable as pos-
sible. The ideal model will then be one with suffi-
cient expressive power while at the same time not
including extra information which would make its
use less practical.
4 Experiments
We carried out experiments to determine how suit-
able the pattern representations detailed in Section
2 are for encoding the information of interest to
IE systems. We chose a set of IE corpora anno-
tated with the information to be extracted (detailed
in Section 4.1), generated sets of patterns using a
variety of dependency parsers (Section 4.2) which
were then examined to discover how much of the
target information they contain (Section 4.3).
4.1 Corpora
Corpora representing different genres of text were
chosen for these experiments; one containing
newspaper text and another composed of biomed-
ical abstracts. The first corpus consisted of Wall
Street Journal texts from the Sixth Message Un-
derstanding Conference (MUC, 1995) IE evalu-
ation. These are reliably annotated with details
about the movement of executives between jobs.
We make use of a version of the corpus pro-
duced by Soderland (1999) in which events de-
scribed within a single sentence were annotated.
Events in this corpus identify relations between
up to four entities: PersonIn (the person start-
ing a new job), PersonOut (person leaving a
job), Post (the job title) and Organisation
(the employer). These events were broken down
into a set of binary relationships. For exam-
ple, the sentence ?Smith was recently made chair-
man of Acme.? contains information about the
new employee (Smith), post (chairman) and or-
ganisation (Acme). Events are represented as a
set of binary relationships, Smith-chairman,
chairman-Acme and Smith-Acme for this
example.
The second corpus uses documents taken from
the biomedical domain, specifically the train-
ing corpus used in the LLL-05 challenge task
(Ne?dellec, 2005), and a pair of corpora (Craven
and Kumlien, 1999) which were derived from the
Yeast Proteome Database (YPD) (Hodges et al,
1999) and the Online Mendelian Inheritance in
Man database (OMIM) (Hamosh et al, 2002).
Each of these corpora are annotated with binary
relations between pairs of entities. The LLL-05
corpora contains interactions between genes and
proteins. For example the sentence ?Expression
of the sigma(K)-dependent cwlH gene depended
on gerE? contains relations between sigma(K) and
cwlH and between gerE and cwlH. The YPD cor-
pus is concerned with the subcellular compart-
ments in which particular yeast proteins localize.
An example sentence ?Uba2p is located largely in
the nucleus? relates Uba2p and the nucleus. The
relations in the OMIM corpora are between genes
and diseases, for example ?Most sporadic colorec-
tal cancers also have two APC mutations? con-
tains a relation between APC and colorectal can-
cer.
The MUC6 corpus contains a total of six pos-
sible binary relations. Each of the three biomedi-
cal corpora contain a single relation type, giving a
total of nine binary relations for the experiments.
There are 3911 instances of binary relations in all
corpora.
4.2 Generating Dependency Patterns
Three dependency parsers were used for these ex-
periments: MINIPAR3 (Lin, 1999), the Machinese
Syntax4 parser from Connexor Oy (Tapanainen
and Ja?rvinen, 1997) and the Stanford5 parser
(Klein and Manning, 2003). These three parsers
represent a cross-section of approaches to produc-
ing dependency analyses: MINIPAR uses a con-
stituency grammar internally before converting
the result to a dependency tree, Machinese Syn-
tax uses a functional dependency grammar, and
the Stanford Parser is a lexicalized probabilistic
parser.
Before these parsers were applied to the various
corpora the named entities participating in rela-
tions are replaced by a token indicating their class.
For example, in the MUC6 corpus ?Acme hired
Smith? would become ?Organisation hired
PersonIn?. Each parser was adapted to deal
with these tokens correctly. The parsers were ap-
plied to each corpus and patterns extracted from
the dependency trees generated.
The analyses produced by the parsers were post-
processed to make the most of the information
they contain and ensure consistent structures from
which patterns could be extracted. It was found
3http://www.cs.ualberta.ca/
?
lindek/
4http://www.connexor.com/software/syntax/
5http://www-nlp.stanford.edu/software/
15
Parser SVO Chains Linked chains Subtrees
MINIPAR 2,980 52,659 149,504 353,778,240,702,149,000
Machinese Syntax 2,382 67,690 265,631 4,641,825,924
Stanford 2,950 76,620 478,643 1,696,259,251,073
Table 1: Number of patterns produced for each pattern model by different parsers
that the parsers were often unable to generate a de-
pendency tree which included the whole sentence
and instead generate an analysis consisting of sen-
tence fragments represented as separate tree struc-
tures. Some fragments did not include a verb so
no patterns could be extracted. To take account of
this we allowed the root node of any tree fragment
to take the place of a verb in a pattern (see Sec-
tion 2). This leads to the generation of more chain
and linked chain patterns but has no effect on the
number of SVO patterns or subtrees.
Table 1 shows the number of patterns generated
from the dependency trees produced by each of the
parsers. The number of subtrees generated from
the MINIPAR parses is several orders of magnitude
higher than the others because MINIPAR allows
certain nodes to be the modifier of two separate
nodes to deal with phenomena such as conjunc-
tion, anaphora and VP-coordination. For exam-
ple, in the sentence ?The bomb caused widespread
damage and killed three people? the bomb is the
subject of both the verbs cause and kill. We made
use of this information by duplicating any nodes
(and their descendants) with more than one head.6
Overall the figures in Table 1 are consistent with
the analysis in Section 3 but there is great variation
in the number of patterns produced by the differ-
ent parsers. For example, the Stanford parser pro-
duces more chains and linked chains than the other
parsers. (If we did not duplicate portions of the
MINIPAR parses then the Stanford parser would
also generate the most subtrees.) We found that
the Stanford parser was the most likely to gen-
erate a single dependency tree for each sentence
while the other two produced a set of tree frag-
ments. A single dependency analysis contains a
greater number of patterns, and possible subtrees,
than a fragmented analysis. One reason for this
may be that the Stanford parser is unique in allow-
ing the use of an underspecified dependency rela-
tion, dep, which can be applied when the role of
the dependency is unclear. This allows the Stan-
6One dependency tree produced by MINIPAR, expanded in
this way, contained approximately 1 ? 1064 subtrees. These
are not included in the total number of subtrees for the MINI-
PAR parses shown in the table.
ford parser to generate analyses which span more
of the sentence than the other two.
4.3 Evaluating Pattern Models
Patterns from each of the four models are exam-
ined to check whether they cover the information
which should be extracted. In this context ?cover?
means that the pattern contains both elements
of the relation. For example, an SVO pattern
extracted from the dependency parse of ?Smith
was recently made chairman of Acme.? would be
[V/make](subj[N/Smith]+obj[N/chairman])
which covers the relation between Smith and
chairman but not the relations between Smith
and Acme or chairman and Acme. The coverage
of each model is computed as the percentage of
relations in the corpus for which at least one of
the patterns contains both of the participating
entities. Coverage is related to the more familiar
IE evaluation metric of recall since the coverage
of a pattern model places an upper bound on the
recall of any system using that model. The aim
of this work is to determine the proportion of
the relations in a corpus that can be represented
using the various pattern models rather than their
performance in an IE system and, consequently,
we choose to evaluate models in terms of their
coverage rather than precision and recall.7
For practical applications parsers are required
to generate the dependency analysis but these may
not always provide a complete analysis for every
sentence. The coverage of each model is influ-
enced by the ability of the parser to produce a tree
which connects the elements of the event to be ex-
tracted. To account for this we compute the cov-
erage of each model relative to a particular parser.
The subtree model covers all events whose enti-
ties are included in the dependency tree and, con-
sequently, the coverage of this model represents
the maximum number of events that the model can
7The subtree model can be used to cover any set of items
in a dependency tree. So, given accurate dependency anal-
yses, this model will cover all events. The coverage of the
subtree model can be determined by checking if the elements
of the event are connected in the dependency analysis of the
sentence and, for simplicity, we chose to do this rather than
enumerating all subtrees.
16
represent for a given dependency tree. The cover-
age of other models relative to a dependency anal-
ysis can be computed by dividing the number of
events it covers by the number covered by the sub-
tree model (i.e. the maximum which can be cov-
ered). This measure is refered to as the bounded
coverage of the model. Bounded coverage for the
subtree model is always 100%.
5 Results
Coverage and bounded-coverage results for each
pattern representation and parser combination are
given in Table 2. The table lists the corpus, the
total number of instances within that corpus and
the results for each of the four pattern models. Re-
sults for the subtree model lists the coverage and
raw count, the bounded-coverage for this model
will always be 100% and is not listed. Results
for the other three models show the coverage and
raw count along with the bounded coverage. The
coverage of each parser and pattern representa-
tion (combined across both corpora) are also sum-
marised in Figure 3.
The simplest representation, SVO, does not per-
form well in this evaluation. The highest bounded-
coverage score is 15.1% (MUC6 corpus, Stanford
parser) but the combined average over all corpora
is less than 6% for any parser. This suggests
that the SVO representation is simply not expres-
sive enough for IE. Previous work which has used
this representation have used indirect evaluation:
document and sentence filtering (Yangarber, 2003;
Stevenson and Greenwood, 2005). While the SVO
representation may be expressive enough to allow
a classifier to distinguish documents or sentences
which are relevant to a particular extraction task it
seems too limited to be used for relation extrac-
tion. The SVO representation performs notice-
ably worse on the biomedical text. Our analysis
suggests that this is because the items of interest
are commonly described in ways which the SVO
model is unable to represent.
The more complex chain model covers a greater
percentage of the relations. However its bounded-
coverage is still less than half of the relations in ei-
ther the MUC6 corpus or the biomedical texts. Us-
ing the chain model the best coverage which can
be achieved over any corpus is 41.07% (MUC6
corpus, MINIPAR and Stanford parser) which is
unlikely to be sufficient to create an IE system.
Results for the linked chain representation are
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
MINIPAR Machinese Syntax Stanford
Co
ve
ra
ge
SVO Chains Linked Chains Subtrees
Figure 3: Coverage of various pattern representa-
tion models for each of the three parsers.
much more promising covering around 70% of all
relations using the MINIPAR and Machinese Syn-
tax parsers and over 90.64% using the Stanford
parser. For all three parsers this model achieves
a bounded-coverage of close to 95%, indicating
that this model can represent the majority of re-
lations which are included in a dependency tree.
The subtree representation covers slight more of
the relations than linked chains: around 75% us-
ing the MINIPAR or Machinese Syntax parsers and
96.62% using the Stanford parser.
A one-way repeated measures ANOVA was car-
ried out to analyse the differences between the re-
sults for each model shown in Table 2. It was
found that the differences between the SVO, chain,
linked chain and subtree models are significant
(p < 0.01). A Tukey test was then applied to iden-
tify which of the individual differences between
pairs of models were significant. Differences be-
tween two pairs of models were not found to be
significant (p < 0.01): SVO and chains; linked
chains and subtrees.
These results suggest that the linked chains and
subtree models can represent significantly more of
the relations which occur in IE scenarios than ei-
ther the SVO or chain models. However, there is
little to be gained from using the subtree model
since accuracy of the linked chain model is com-
parable and the number of patterns generated is
bounded by a polynomial rather than exponential
function.
5.1 Analysis and Discussion
Examination of the relations which were cov-
ered by the subtree model but not by linked
chains suggested that there are certain construc-
tions which cause difficulties. One such construc-
tion is the appositive, e.g. the relation between
17
# of SVO Chains Linked Chains Subtrees
Parser Corpus Relations %C %B-C %C %B-C %C %B-C %C
MUC6 1322 7.49 (99) 9.07 41.07 (543) 49.73 81.92 (1083) 99.18 82.60 (1092)
MINIPAR Biomed 2589 0.93 (24) 1.30 17.38 (450) 24.44 65.31 (1691) 91.85 71.11 (1841)
Combined 3911 3.14 (123) 4.19 25.39 (993) 33.86 70.93 (2774) 94.58 74.99 (2933)
Machinese MUC6 1322 2.12 (28) 2.75 35.70 (472) 46.41 76.32 (1009) 99.21 76.93 (1017)
Syntax Biomed 2589 0.19 (5) 0.27 14.56 (377) 20.47 65.47 (1695) 92.02 71.15 (1842)Combined 3911 0.84 (33) 1.15 21.71 (849) 29.70 69.14 (2704) 94.58 73.10 (2859)
MUC6 1322 15.05 (199) 15.10 41.07 (543) 41.20 94.78 (1253) 95.07 99.70 (1318)
Stanford Biomed 2589 0.46 (12) 0.49 16.53 (428) 17.39 88.52 (2292) 93.13 95.06 (2461)
Combined 3911 5.40 (211) 5.58 24.83 (971) 25.69 90.64 (3545) 93.81 96.62 (3779)
Table 2: Evaluation results for the three different parsers.
PersonOut and Organisation in the frag-
ment ?Organisation?s Post, PersonOut,
resigned yesterday morning?. Certain nominal-
isations may also cause problems for the linked
chains representation, e.g. in biomedical text
the relation between Agent and Target in the
nominalisation ?the Agent-dependent assembly
of Target? cannot be represented by a linked
chain. In both cases the problem is caused by the
fact that the dependency tree generated includes
the two named entities in part of the tree domi-
nated by a node marked as a noun. Since each
linked chain must be anchored at a verb (or the
root of a tree fragment) and the two chains can-
not share part of their path, these relations are not
covered. It would be possible to create another
representation which allowed these relations to be
captured but it would generate more patterns than
the linked chain model.
Our results also reveal that the choice of depen-
dency parser effects the coverage of each model
(see Figure 3). The subtree model coverage scores
for each parser shown in Table 3 represent the per-
centage of sentences for which an analysis was
generated that included both items from the bi-
nary relations. These figures are noticably higher
for the Stanford parser. We previously mentioned
(Section 4.2) that this parser allows the use of an
underspecified dependency relation and suggested
that this may be a reason for the higher cover-
age. The use of underspecified dependency re-
lations may not be useful for all applications but
is unlikely to cause problems for systems which
learn IE patterns provided the trees generated by
the parser are consistent. Differences between the
results produced by the three parsers suggest that
it is important to fully evaluate their suitability for
a particular purpose.
These experiments also provide insights into the
more general question of how suitable dependency
trees are as a basis for extraction patterns. De-
pendency analysis has the advantage of generat-
ing analyses which abstract away from the sur-
face realisation of text to a greater extent than
phrase structure grammars tend to. This leads to
the semantic information being more accessible in
the representation of the text which can be use-
ful for IE. For practical applications this approach
relies on the ability to accurately generate depen-
dency analyses. The results presented here sug-
gest that the Stanford parser (Klein and Manning,
2003) is capable of generating analyses for almost
all sentences within corpora from two very differ-
ent domains. Bunescu and Mooney (2005) have
also demonstrated that dependency graphs can be
produced using Combinatory Categorial Grammar
(CCG) and context-free grammar (CFG) parsers.
6 Conclusions
This paper compares four IE pattern models:
SVO, chains, linked chains and subtrees. Us-
ing texts from the management succession and
biomedical domains it was found that the linked
chains model can represent around 95% of the
possible relations contained in the text, given a de-
pendency parse. Subtrees can represent all the re-
lations contained within dependency trees but their
use is less practical because enumerating all pos-
sible subtrees is a more complex problem and the
large number of resulting patterns could limit the
learning algorithms that can be applied. This re-
sult should be borne in mind during the design of
IE systems.
Acknowledgements
The authors are grateful to Mike Stannet for pro-
viding the method for counting subtrees intro-
duced in Section 3 and to Connexor Oy for use
of the Machinese Syntax parser. The research
18
described in this paper was funded by the En-
gineering and Physical Sciences Research Coun-
cil via the RESuLT project (GR/T06391) and par-
tially funded by the IST 6th Framework project X-
Media (FP6-26978).
References
Razvan Bunescu and Raymond Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proceedings of the Human Language Tech-
nology Conference and Conference on Empirical
Methods in Natural Language Processing, pages
724?731, Vancouver, B.C.
Mark Craven and Johan Kumlien. 1999. Construct-
ing Biological Knowledge Bases by Extracting In-
formation from Text Sources. In Proceedings of the
Seventh International Conference on Intelligent Sys-
tems for Molecular Biology, pages 77?86, Heidel-
berg, Germany. AAAI Press.
Robert Gaizauskas, Takahiro Wakao, Kevin
Humphreys, Hamish Cunningham, and Yorick
Wilks. 1996. Description of the LaSIE system
as used for MUC-6. In Proceedings of the Sixth
Message Understanding Conference (MUC-6),
pages 207?220, San Francisco, CA.
Mark A. Greenwood, Mark Stevenson, Yikun Guo,
Henk Harkema, and Angus Roberts. 2005. Au-
tomatically Acquiring a Linguistically Motivated
Genic Interaction Extraction System. In Proceed-
ings of the 4th Learning Language in Logic Work-
shop (LLL05), Bonn, Germany.
Ada Hamosh, Alan F. Scott, Joanna Amberger, Carol
Bocchini, David Valle, and Victor A. McKusick.
2002. Online Mendelian Inheritance in Man
(OMIM), a knowledgebase of human genes and ge-
netic disorders. Nucleic Acids Research, 30(1):52?
55.
Peter E. Hodges, Andrew H. Z. McKee, Brian P. Davis,
William E. Payne, and James I. Garrels. 1999. The
Yeast Proteome Database (YPD): a model for the or-
ganization and presentation of genome-wide func-
tional data. Nucleic Acids Research, 27(1):69?73.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate Unlexicalized Parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL-03), pages 423?430, Sap-
poro, Japan.
Dekang Lin. 1999. MINIPAR: A Minimalist Parser.
In Maryland Linguistics Colloquium, University of
Maryland, College Park.
Igor Mel?c?uk. 1987. Dependency Syntax: Theory and
Practice. SUNY Press, New York.
MUC. 1995. Proceedings of the Sixth Message Un-
derstanding Conference (MUC-6), San Mateo, CA.
Morgan Kaufmann.
Claire Ne?dellec. 2005. Learning Language in Logic -
Genic Interaction Extraction Challenge. In Proceed-
ings of the 4th Learning Language in Logic Work-
shop (LLL05), Bonn, Germany, August.
Ellen Riloff. 1993. Automatically constructing a dic-
tionary for information extraction tasks. pages 811?
816.
Stephen Soderland. 1999. Learning Information Ex-
traction Rules for Semi-structured and free text. Ma-
chine Learning, 31(1-3):233?272.
Mark Stevenson and Mark A. Greenwood. 2005. A
Semantic Approach to IE Pattern Induction. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 379?386,
Ann Arbor, MI.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2001. Automatic Pattern Acquisition for Japanese
Information Extraction. In Proceedings of the Hu-
man Language Technology Conference (HLT2001).
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An Improved Extraction Pattern Representa-
tion Model for Automatic IE Pattern Acquisition. In
Proceedings of the 41st Annual Meeting of the As-
sociation for Computational Linguistics (ACL-03),
pages 224?231, Sapporo, Japan.
Pasi Tapanainen and Timo Ja?rvinen. 1997. A Non-
Projective Dependency Parser. In Proceedings of
the 5th Conference on Applied Natural Language
Processing, pages 64?74, Washington, DC.
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Automatic Acquisition of
Domain Knowledge for Information Extraction. In
Proceedings of the 18th International Conference on
Computational Linguistics (COLING 2000), pages
940?946, Saarbru?cken, Germany.
Roman Yangarber. 2003. Counter-training in the Dis-
covery of Semantic Patterns. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL-03), pages 343?350, Sap-
poro, Japan.
19
Proceedings of the Workshop on Information Extraction Beyond The Document, pages 29?35,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Improving Semi-Supervised Acquisition of Relation Extraction Patterns
Mark A. Greenwood and Mark Stevenson
Department of Computer Science
University of Sheffield
Sheffield, S1 4DP, UK
{m.greenwood,marks}@dcs.shef.ac.uk
Abstract
This paper presents a novel approach to
the semi-supervised learning of Informa-
tion Extraction patterns. The method
makes use of more complex patterns than
previous approaches and determines their
similarity using a measure inspired by re-
cent work using kernel methods (Culotta
and Sorensen, 2004). Experiments show
that the proposed similarity measure out-
performs a previously reported measure
based on cosine similarity when used to
perform binary relation extraction.
1 Introduction
A recent approach to Information Extraction (IE)
is to make use of machine learning algorithms
which allow systems to be rapidly developed or
adapted to new extraction problems. This reduces
the need for manual development which is a major
bottleneck in the development of IE technologies
and can be extremely time consuming (e.g. Riloff
(1996)).
A number of machine learning approaches have
recently been applied. One is the use of itera-
tive learning algorithms to infer extraction patterns
from a small number of seed examples (Yangar-
ber et al, 2000; Stevenson and Greenwood, 2005).
These approaches use dependency analysis as the
basis of IE patterns. Training text is parsed and a
set of candidate patterns extracted. These patterns
are then compared against the seeds with the most
similar being selected and added to the seed set.
(Optionally, a user may verify the patterns at this
point.) The process is then repeated with the re-
maining patterns being compared to the enlarged
seed set. The process continues until a suitable set
of patterns has been learned. These approaches
require only a small number of example extraction
patterns which greatly reduces the effort required
to develop IE systems.
While it has been found that these approaches
are capable of learning useful IE patterns from
a handful of examples (Yangarber et al, 2000;
Stevenson and Greenwood, 2005) they are limited
by the use of basic extraction patterns: SVO tu-
ples. The patterns used by these systems are de-
fined as a verb and its direct subject and/or object.
They could then only extract a limited set of re-
lations; those expressed using a verb and its di-
rect arguments. For example, these patterns could
identify the relation between Jones and Smith in
the sentence ?Jones replaced Smith?. However,
no pattern consisting of a verb and its arguments
could be constructed which could identify the
same relation in ?Jones was named as Smith?s suc-
cessor.?
Others have suggested alternative approaches
for generating extraction patterns from depen-
dency trees, each of which allows a particular part
of the dependency analysis to act as an extraction
pattern. For example, Sudo et al (2003) used pat-
terns consisting of a path from a verb to any of
its descendents (direct or indirect) while Bunescu
and Mooney (2005) suggest the shortest path be-
tween the items being related. However, iterative
learning algorithms, such as the ones used by Yan-
garber et al (2000) and Stevenson and Greenwood
(2005), have not made use of these more complex
extraction patterns. Part of the reason for this is
that these algorithms require a way of determining
the similarity between patterns (in order to com-
pare candidate patterns with the seeds). This pro-
cess is straightforward for simple patterns, based
on SVO tuples, but less so for more complex ex-
29
traction patterns.
In this paper we present a semi-supervised al-
gorithm for the iterative learning of relation ex-
traction patterns which makes use of a more com-
plex pattern representation than has been previ-
ously used by these approaches (Sections 2 and 3).
The algorithm makes use of a similarity function
based on those which have been proposed for use
with non-iterative learning algorithms (Zelenko et
al., 2003; Culotta and Sorensen, 2004; Bunescu
and Mooney, 2005). These are extended to include
information about lexical similarity derived from
WordNet (Section 4). We present results of using
patterns acquired through this similarity function
to perform binary relation extraction (Sections 5
and 6).
2 Semi-Supervised Learning of
Extraction Patterns
We begin by outlining the general process of learn-
ing extraction patterns using a semi-supervised al-
gorithm, similar to one presented by Yangarber
(2003).
1. For a given IE scenario we assume the ex-
istence of a set of documents against which
the system can be trained. The documents are
unannotated and may be either relevant (con-
tain the description of an event relevant to the
scenario) or irrelevant.
2. This corpus is pre-processed to generate a set
of all patterns which could be used to repre-
sent sentences contained in the corpus, call
this set P . The aim of the learning process is
to identify the subset of P representing pat-
terns which are relevant to the IE scenario.
3. The user provides a small set of seed pat-
terns, Pseed, which are relevant to the sce-
nario. These patterns are used to form the
set of currently accepted patterns, Pacc, so
Pacc ? Pseed. The remaining patterns are
treated as candidates for inclusion in the ac-
cepted set, these form the set Pcand(= P ?
Pacc).
4. A function, f , is used to assign a score to
each pattern in Pcand based on those which
are currently in Pacc. This function as-
signs a real number to candidate patterns so
? c  Pcand, f(c, Pacc) 7? R. A set of high
scoring patterns (based on absolute scores or
ranks after the set of patterns has been or-
dered by scores) are chosen as being suitable
for inclusion in the set of accepted patterns.
These form the set Plearn.
5. (Optional) The patterns in Plearn may be re-
viewed by a user who may remove any they
do not believe to be useful for the scenario.
6. The patterns in Plearn are added to Pacc and
removed from Pcand, so Pacc ? Pacc ?
Plearn and Pcand ? Pacc ? Plearn
7. Stop if an acceptable set of patterns has been
learned, otherwise goto step 4
Previous algorithms which use this approach in-
clude those described by Yangarber et al (2000)
and Stevenson and Greenwood (2005). A key
choice in the development of an algorithm using
this approach is the process of ranking candidate
patterns (step 4) since this determines the patterns
which will be learned at each iteration. Yangar-
ber et al (2000) chose an approach motivated by
the assumption that documents containing a large
number of patterns already identified as relevant to
a particular IE scenario are likely to contain further
relevant patterns. This approach operates by asso-
ciating confidence scores with patterns and rele-
vance scores with documents. Initially seed pat-
terns are given a maximum confidence score of 1
and all others a 0 score. Each document is given
a relevance score based on the patterns which oc-
cur within it. Candidate patterns are ranked ac-
cording to the proportion of relevant and irrele-
vant documents in which they occur, those found
in relevant documents far more than in irrelevant
ones are ranked highly. After new patterns have
been accepted all patterns? confidence scores are
updated, based on the documents in which they
occur, and documents? relevance according to the
accepted patterns they contain.
Stevenson and Greenwood (2005) suggested an
alternative method for ranking the candidate pat-
terns. Their approach relied on the assumption
that useful patterns will have similar meanings to
the patterns which have already been accepted.
They chose to represent each pattern as a vector
consisting of the lexical items which formed the
pattern and used a version of the cosine metric to
determine the similarity between pairs of patterns,
consequently this approach is referred to as ?co-
sine similarity?. The metric used by this approach
incorporated information from WordNet and as-
signed high similarity scores to patterns with sim-
ilar meanings expressed in different ways.
30
Figure 1: An example dependency tree.
3 Relation Extraction Patterns
Both these approaches used extraction pat-
terns which were based on dependency analysis
(Tesnie?re, 1959) of text. Under this approach the
structure of a sentence is represented by a set of di-
rected binary links between a word (the head) and
one of its modifiers. These links may be labelled
to indicate the grammatical relation between the
head and modifier (e.g. subject, object). Cycli-
cal paths are generally disallowed and the analysis
forms a tree structure. An example dependency
analysis for the sentence ?Acme Inc. hired Mr
Smith as their new CEO, replacing Mr Bloggs.?
is shown in Figure 1.
The extraction patterns used by both Yan-
garber et al (2000) and Stevenson and Green-
wood (2005) were based on SVO tuples ex-
tracted from dependency trees. The depen-
dency tree shown in Figure 1 would gener-
ate two patterns: replace obj??? Mr Bloggs
and Acme Inc. subj??? hire obj??? Mr Smith.
While these represent some of the core informa-
tion in this sentence, they cannot be used to iden-
tify a number of relations including the connection
between Mr. Smith and CEO or between Mr. Smith
and Mr. Bloggs.
A number of alternative approaches to con-
structing extraction patterns from dependency
trees have been proposed (e.g. (Sudo et al, 2003;
Bunescu and Mooney, 2005)). Previous analysis
(Stevenson and Greenwood, 2006a) suggests that
the most useful of these is one based on pairs of
linked chains from the dependency tree. A chain
can be defined as a path between a verb node and
any other node in the dependency tree passing
through zero or more intermediate nodes (Sudo
et al, 2001). The linked chains model (Green-
wood et al, 2005) represents extraction patterns
as a pair of chains which share the same verb but
no direct descendants. It can be shown that linked
Figure 2: Example linked chain patterns
chain patterns can represent the majority of rela-
tions within a dependency analysis (Stevenson and
Greenwood, 2006a). For example, the dependency
tree shown in Figure 1 contains four named enti-
ties (Acme Inc., Mr Smith, CEO and Mr. Bloggs)
and linked chains patterns can be used to repre-
sent the relation between any pair.1 Some exam-
ple patterns extracted from the analysis in Figure 1
can be seen in Figure 2. An additional advantage
of linked chain patterns is that they do not cause
an unwieldy number of candidate patterns to be
generated unlike some other approaches for rep-
resenting extraction patterns, such as the one pro-
posed by Sudo et al (2003) where any subtree of
the dependency tree can act as a potential pattern.
When used within IE systems these pat-
terns are generalised by replacing terms
which refer to specific entities with a gen-
eral semantic class. For example, the pattern
Acme Inc.
subj??? hire obj??? Mr Smithwould
become COMPANY subj??? hire obj??? PERSON.
4 Pattern Similarity
Patterns such as linked chains have not been used
by semi-supervised approaches to pattern learn-
ing. These algorithms require a method of de-
termining the similarity of patterns. Simple pat-
terns, such as SVO tuples, have a fixed structure
containing few items and tend to occur relatively
frequently in corpora. However, more complex
patterns, such as linked chains, have a less fixed
structure and occur less frequently. Consequently,
the previously proposed approaches for determin-
ing pattern similarity (see Section 2) are unlikely
to be as successful with these more complex pat-
terns. The approach proposed by Stevenson and
1Note that we allow a linked chain pattern to represent the
relation between two items when they are on the same chain,
such as Mr Smith and CEO in this example.
31
Greenwood (2005) relies on representing patterns
as vectors which is appropriate for SVO tuples
but not when patterns may include significant por-
tions of the dependency tree. Yangarber et al
(2000) suggested a method where patterns were
compared based on their distribution across doc-
uments in a corpus. However, since more complex
patterns are more specific they occur with fewer
corpus instances which is likely to hamper this
type of approach.
Another approach to relation extraction is to use
supervised learning algorithms, although they re-
quire more training data than semi-supervised ap-
proaches. In particular various approaches (Ze-
lenko et al, 2003; Culotta and Sorensen, 2004;
Bunescu and Mooney, 2005) have used kernel
methods to determine the sentences in a corpus
which contain instances of a particular relation.
Kernel methods (Vapnik, 1998) allow the repre-
sentation of large and complicated feature spaces
and are therefore suitable when the instances are
complex extraction rules, such as linked chains.
Several previous kernels used for relation extrac-
tion have been based on trees and include meth-
ods based on shallow parse trees (Zelenko et al,
2003), dependency trees (Culotta and Sorensen,
2004) and part of a dependency tree which rep-
resents the shortest path between the items be-
ing related (Bunescu and Mooney, 2005). Ker-
nels methods rely on a similarity function between
pairs of instances (the kernel) and these can be
used within semi-supervised approaches to pattern
learning such as those outlined in Section 2.
4.1 Structural Similarity Measure
The remainder of this Section describes a similar-
ity function for pairs of linked chains, based on
the tree kernel proposed by Culotta and Sorensen
(2004). The measure compares patterns by follow-
ing their structure from the root nodes through the
patterns until they diverge too far to be considered
similar.
Each node in an extraction pattern has three fea-
tures associated with it: the word, the relation to
a parent, and the part-of-speech (POS) tag. The
values of these features for node n are denoted
by nword, nreln and npos respectively. Pairs of
nodes can be compared by examining the values of
these features and also by determining the seman-
tic similarity of the words. A set of four functions,
F = {word, relation, pos, semantic}, is used to
compare nodes. The first three of these correspond
to the node features with the same name; the rel-
evant function returns 1 if the value of the feature
is equal for the two nodes and 0 otherwise. For
example, the pos function compares the values of
the part of speech feature for nodes n1 and n2:
pos (n1, n2) =
{
1 if n1, pos = n2, pos
0 otherwise
The remaining function, semantic, returns a
value between 0 and 1 to signify the semantic sim-
ilarity of lexical items contained in the word fea-
ture of each node. This similarity is computed us-
ing the WordNet (Fellbaum, 1998) similarity func-
tion introduced by Lin (1998) .
The similarity of two nodes is zero if their part
of speech tags are different and, otherwise, is sim-
ply the sum of the scores provided by the four
functions which form the set F . This is repre-
sented by the function s:
s (n1, n2) =
{ 0 if pos(n1, n2) = 0
?
f?F
f (n1, n2) otherwise
The similarity of a pair of linked chain patterns,
l1 and l2, is determined by the function sim:
sim (l1, l2) =
?
?
?
0 if s (r1, r2) = 0
s (r1, r2)+
simc (Cr1 , Cr2) otherwise
where r1 and r2 are the root nodes of patterns l1
and l2 (respectively) and Cr is the set of children
of node r.
The final part of the similarity function calcu-
lates the similarity between the child nodes of n1
and n2.2
simc (Cn1, Cn2) =
?
c1?Cn1
?
c2?Cn2
sim (c1, c2)
Using this similarity function a pair of identi-
cal nodes have a similarity score of four. Conse-
quently, the similarity score for a pair of linked
chain patterns can be normalised by dividing the
similarity score by 4 times the size (in nodes) of
the larger pattern. This results in a similarity func-
tion that is not biased towards either small or large
patterns but will select the most similar pattern to
those already accepted as representative of the do-
main.
This similarity function resembles the one in-
troduced by Culotta and Sorensen (2004) but also
2In linked chain patterns the only nodes with multiple
children are the root nodes so, in all but the first applica-
tion, this formula can be simplified to simc (Cn1 , Cn2) =
sim(c1, c2).
32
differs in a number of ways. Both functions make
use of WordNet to compare tree nodes. Culotta
and Sorensen (2004) consider whether one node is
the hypernym of the other while the approach in-
troduced here makes use of existing techniques to
measure semantic similarity. The similarity func-
tion introduced by Culotta and Sorensen (2004)
compares subsequences of child nodes which is
not required for our measure since it is concerned
only with linked chain extraction patterns.
5 Experiments
This structural similarity metric was implemented
within the general framework for semi-supervised
pattern learning presented in Section 2. At
each iteration the candidate patterns are compared
against the set of currently accepted patterns and
ranked according to the average similarity with the
set of similar accepted patterns. The four highest
scoring patterns are considered for acceptance but
a pattern is only accepted if its score is within 0.95
of the similarity of the highest scoring pattern.
We conducted experiments which compared the
proposed pattern similarity metric with the vec-
tor space approach used by Stevenson and Green-
wood (2005) (see Section 2). That approach was
originally developed for simple extraction patterns
consisting of subject-verb-object tuples but was
extended for extraction patterns in the linked chain
format by Greenwood et al (2005). We use the
measure developed by Lin (1998) to provide infor-
mation about lexical similarity. This is the same
measure which is used within the structural simi-
larity metric (Section 4).
Three different configurations of the iterative
learning algorithm were compared. (1) Cosine
(SVO) This approach uses the SVO model for ex-
traction patterns and the cosine similarity metric
to compare them (see Section 2). This version of
the algorithm acts as a baseline which represents
previously reported approaches (Stevenson and
Greenwood, 2005; Stevenson and Greenwood,
2006b). (2) Cosine (Linked chain) uses extrac-
tion patterns based on the linked chain model
along with the cosine similarity to compare them
and is intended to determine the benefit which is
gained from using the more expressive patterns.
(3) Structural (Linked chain) also uses linked
chain extraction patterns but compares them using
the similarity measure introduced in Section 4.1.
COMPANY
subj???appoint obj???PERSON
COMPANY
subj???elect obj???PERSON
COMPANY
subj???promote obj???PERSON
COMPANY
subj???name obj???PERSON
PERSON
subj???resign
PERSON
subj???depart
PERSON
subj???quit
Table 1: Seed patterns used by the learning algo-
rithm
5.1 IE Scenario
Experiments were carried out on the management
succession extraction task used for the Sixth
Message Understanding Conference (MUC-6)
(MUC, 1995). This IE scenario concerns the
movement of executives between positions and
companies. We used a version of the evaluation
data which was produced by Soderland (1999)
in which each event was converted into a set of
binary asymmetric relations. The corpus con-
tains four types of relation: Person-Person,
Person-Post, Person-Organisation,
and Post-Organisation. At each iteration
of the algorithm the related items identified by the
current set of learned patterns are extracted from
the text and compared against the set of related
items which are known to be correct. The systems
are evaluated using the widely used precision (P)
and recall (R) metrics which are combined using
the F-measure (F).
The texts used for these experiments have been
previously annotated with named entities. MINI-
PAR (Lin, 1999), after being adapted to handle the
named entity tags, was used to produce the depen-
dency analysis from which the pattersn were gen-
erated. All experiments used the seed patterns in
Table 1 which are indicative of this extraction task
and have been used in previous experiments into
semi-supervised IE pattern acquisition (Stevenson
and Greenwood, 2005; Yangarber et al, 2000).
The majority of previous semi-supervised ap-
proaches to IE have been evaluated over prelim-
inary tasks such as the identification of event par-
ticipants (Sudo et al, 2003) or sentence filtering
(Stevenson and Greenwood, 2005). These may be
a useful preliminary tasks but it is not clear to what
extent the success of such systems will be repeated
when used to perform relation extraction. Conse-
33
0 25 50 75 100 125 150 175 200 225 250
Iteration
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
F-
m
ea
su
re
Cosine (SVO)
Cosine (Linked-Chains)
Structural (Linked-Chains)
Figure 3: F-measure scores for relation extraction
over 250 iterations
quently we chose a relation extraction task to eval-
uate the work presented here.
6 Results
Results from the relation extraction evaluation can
be seen in Table 2 and Figure 3. The seven seed
patterns achieve a precision of 0.833 and recall of
0.022. The two approaches based on cosine sim-
ilarity performs poorly, irrespective of the pattern
model being used. The maximum increase in F-
measure of 0.15 (when using the cosine measure
with the linked chain model) results in a maximum
F-measure for the cosine similarity model of 0.194
(with a precision of 0.491 and recall of 0.121) after
200 iterations.
The best result is recorded when the linked
chain model is used with the similarity measure
introduced in Section 4.1, achieving a maximum
F-measure of 0.329 (with a precision of 0.434 and
recall of 0.265) after 190 iterations. This is not
a high F-measure when compared against super-
vised IE systems, however it should be remem-
bered that this represents an increase of 0.285 in
F-measure over the original seven seed patterns
and that this is achieved with a semi-supervised
algorithm.
7 Conclusions
A number of conclusions can be drawn from
the work described in this paper. Firstly, semi-
supervised approaches to IE pattern acquisition
benefit from the use of more expressive extraction
pattern models since it has been shown that the
performance of the linked chain model on the rela-
tion extraction task is superior to the simpler SVO
model. We have previously presented a theoret-
ical analysis (Stevenson and Greenwood, 2006a)
which suggested that the linked chain model was a
more suitable format for IE patterns than the SVO
model but these experiments are, to our knowl-
edge, the first to show that applying this model
improves learning performance. Secondly, these
experiments demonstrate that similarity measures
inspired by kernel functions developed for use in
supervised learning algorithms can be applied to
semi-supervised approaches. This suggests that
future work in this area should consider applying
other similarity functions, including kernel meth-
ods, developed for supervised learning algorithms
to the task of semi-supervised IE pattern acquisi-
tion. Finally, we demonstrated that this similar-
ity measure outperforms a previously proposed ap-
proach which was based on cosine similarity and a
vector space representation of patterns (Stevenson
and Greenwood, 2005).
Acknowledgements
This work was carried out as part of the RESuLT
project funded by the Engineering and Physical
Sciences Research Council (GR/T06391) and par-
tially funded by the IST 6th Framework project X-
Media (FP6-26978).
References
Razvan Bunescu and Raymond Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proceedings of the Human Language Tech-
nology Conference and Conference on Empirical
Methods in Natural Language Processing, pages
724?731, Vancouver, B.C.
Aron Culotta and Jeffery Sorensen. 2004. Dependency
Tree Kernels for Relation Extraction. In 42nd An-
nual Meeting of the Association for Computational
Linguistics, Barcelona, Spain.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database and some of its Applica-
tions. MIT Press, Cambridge, MA.
Mark A. Greenwood, Mark Stevenson, Yikun Guo,
Henk Harkema, and Angus Roberts. 2005. Au-
tomatically Acquiring a Linguistically Motivated
Genic Interaction Extraction System. In Proceed-
ings of the 4th Learning Language in Logic Work-
shop (LLL05), Bonn, Germany.
Dekang Lin. 1998. An information-theoretic def-
inition of similarity. In Proceedings of the Fif-
34
Iteration Cosine (SVO) Cosine (Linked Chains) Structural (Linked Chains)
# P R F P R F P R F
0 0.833 0.022 0.044 0.833 0.022 0.044 0.833 0.022 0.044
25 0.600 0.081 0.142 0.833 0.022 0.044 0.511 0.103 0.172
50 0.380 0.085 0.139 0.500 0.022 0.043 0.482 0.179 0.261
75 0.383 0.103 0.163 0.417 0.022 0.043 0.484 0.197 0.280
100 0.383 0.103 0.163 0.385 0.022 0.042 0.471 0.220 0.300
125 0.383 0.103 0.163 0.500 0.081 0.139 0.441 0.220 0.293
150 0.383 0.103 0.163 0.500 0.099 0.165 0.429 0.229 0.298
175 0.383 0.103 0.163 0.481 0.112 0.182 0.437 0.247 0.315
200 0.383 0.103 0.163 0.491 0.121 0.194 0.434 0.265 0.329
225 0.383 0.103 0.163 0.415 0.121 0.188 0.434 0.265 0.329
250 0.383 0.103 0.163 0.409 0.121 0.187 0.413 0.265 0.322
Table 2: Comparison of the different similarity functions when used to perform relation extraction
teenth International Conference on Machine learn-
ing (ICML-98), Madison, Wisconsin.
Dekang Lin. 1999. MINIPAR: A Minimalist Parser.
In Maryland Linguistics Colloquium, University of
Maryland, College Park.
MUC. 1995. Proceedings of the Sixth Message Un-
derstanding Conference (MUC-6), San Mateo, CA.
Morgan Kaufmann.
Ellen Riloff. 1996. Automatically generating extrac-
tion patterns from untagged text. In Thirteenth Na-
tional Conference on Artificial Intelligence (AAAI-
96), pages 1044?1049, Portland, OR.
Stephen Soderland. 1999. Learning Information Ex-
traction Rules for Semi-structured and free text. Ma-
chine Learning, 31(1-3):233?272.
Mark Stevenson and Mark A. Greenwood. 2005. A
Semantic Approach to IE Pattern Induction. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 379?386,
Ann Arbor, MI.
Mark Stevenson and Mark A. Greenwood. 2006a.
Comparing Information Extraction Pattern Mod-
els. In Proceedings of the Information Extraction
Beyond The Document Workshop (COLING/ACL
2006), Sydney, Australia.
Mark Stevenson and Mark A. Greenwood. 2006b.
Learning Information Extraction Patterns using
WordNet. In Third International Global WordNet
Conference (GWC-2006), Jeju Island, Korea.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2001. Automatic Pattern Acquisition for Japanese
Information Extraction. In Proceedings of the Hu-
man Language Technology Conference (HLT2001).
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An Improved Extraction Pattern Representa-
tion Model for Automatic IE Pattern Acquisition. In
Proceedings of the 41st Annual Meeting of the As-
sociation for Computational Linguistics (ACL-03),
pages 224?231, Sapporo, Japan.
Lucien Tesnie?re. 1959. Eleme?nts de Syntaxe Struc-
turale. Klincksiek, Paris.
Vladimir Vapnik. 1998. Statistical Learning Theory.
John Wiley and Sons.
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Automatic Acquisition of
Domain Knowledge for Information Extraction. In
Proceedings of the 18th International Conference on
Computational Linguistics (COLING 2000), pages
940?946, Saarbru?cken, Germany.
Roman Yangarber. 2003. Counter-training in the Dis-
covery of Semantic Patterns. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL-03), pages 343?350, Sap-
poro, Japan.
Dimitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation ex-
traction. Journal of Machine Learning Research,
3:1083?1106.
35
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 81?88,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Task-based Comparison of Information Extraction Pattern Models
Mark A. Greenwood and Mark Stevenson
Department of Computer Science
University of Sheffield
Sheffield, S1 4DP, UK
{m.greenwood, marks}@dcs.shef.ac.uk
Abstract
Several recent approaches to Information
Extraction (IE) have used dependency trees
as the basis for an extraction pattern repre-
sentation. These approaches have used a va-
riety of pattern models (schemes which de-
fine the parts of the dependency tree which
can be used to form extraction patterns).
Previous comparisons of these pattern mod-
els are limited by the fact that they have used
indirect tasks to evaluate each model. This
limitation is addressed here in an experiment
which compares four pattern models using
an unsupervised learning algorithm and a
standard IE scenario. It is found that there
is a wide variation between the models? per-
formance and suggests that one model is the
most useful for IE.
1 Introduction
A common approach to Information Extraction (IE)
is to (manually or automatically) create a set of pat-
terns which match against text to identify informa-
tion of interest. Muslea (1999) reviewed the ap-
proaches which were used at the time and found
that the most common techniques relied on lexico-
syntactic patterns being applied to text which has
undergone relatively shallow linguistic processing.
For example, the extraction rules used by Soderland
(1999) and Riloff (1996) match text in which syn-
tactic chunks have been identified. More recently
researchers have begun to employ deeper syntactic
analysis, such as dependency parsing (Yangarber et
al., 2000; Stevenson and Greenwood, 2005; Sudo et
al., 2001; Sudo et al, 2003; Yangarber, 2003). In
these approaches extraction patterns are essentially
parts of the dependency tree. To perform extraction
they are compared against the dependency analysis
of a sentence to determine whether it contains the
pattern.
Each of these approaches relies on a pattern
model to define which parts of the dependency tree
can be used to form the extraction patterns. A vari-
ety of pattern models have been proposed. For ex-
ample the patterns used by Yangarber et al (2000)
are the subject-verb-object tuples from the depen-
dency tree (the remainder of the dependency parse is
discarded) while Sudo et al (2003) allow any sub-
tree within the dependency parse to act as an ex-
traction pattern. Stevenson and Greenwood (2006)
showed that the choice of pattern model has impor-
tant implications for IE algorithms including signifi-
cant differences between the various models in terms
of their ability to identify information of interest in
text.
However, there has been little comparison be-
tween the various pattern models. Those which have
been carried out have been limited by the fact that
they used indirect tasks to evaluate the various mod-
els and did not compare them in an IE scenario.
We address this limitation here by presenting a di-
rect comparison of four previously described pattern
models using an unsupervised learning method ap-
plied to a commonly used IE scenario.
The remainder of the paper is organised as fol-
lows. The next section presents four pattern models
which have been previously introduced in the litera-
81
ture. Section 3 describes two previous studies which
compared these models and their limitations. Sec-
tion 4 describes an experiment which compares the
four models on an IE task, the results of which are
described in Section 5. Finally, Section 6 discusses
the conclusions which may be drawn from this work.
2 IE Pattern Models
In dependency analysis (Mel?c?uk, 1987) the syntax
of a sentence is represented by a set of directed bi-
nary links between a word (the head) and one of its
modifiers. These links may be labelled to indicate
the relation between the head and modifier (e.g. sub-
ject, object). An example dependency analysis for
the sentence ?Acme hired Smith as their new CEO,
replacing Bloggs.? is shown Figure 1.
Figure 1: An example dependency tree.
The remainder of this section outlines four mod-
els for representing extraction patterns which can be
derived from dependency trees.
Predicate-Argument Model (SVO): A simple
approach, used by Yangarber et al (2000), Yangar-
ber (2003) and Stevenson and Greenwood (2005),
is to use subject-verb-object tuples from the depen-
dency parse as extraction patterns. These consist of
a verb and its subject and/or direct object. Figure
2 shows the two SVO patterns1 which are produced
for the dependency tree shown in Figure 1.
This model can identify information which is ex-
pressed using simple predicate-argument construc-
tions such as the relation between Acme and Smith
1The formalism used for representing dependency patterns
is similar to the one introduced by Sudo et al (2003). Each
node in the tree is represented in the format a[b/c] (e.g.
subj[N/Acme]) where c is the lexical item (Acme), b its
grammatical tag (N) and a the dependency relation between this
node and its parent (subj). The relationship between nodes is
represented as X(A+B+C) which indicates that nodes A, B and
C are direct descendents of node X.
in the dependency tree shown in Figure 1. How-
ever, the SVO model cannot represent information
described using other linguistic constructions such
as nominalisations or prepositional phrases. For ex-
ample the SVO model would not be able to recog-
nise that Smith?s new job title is CEO since these
patterns ignore the part of the dependency tree con-
taining that information.
Chains: A pattern is defined as a path between a
verb node and any other node in the dependency tree
passing through zero or more intermediate nodes
(Sudo et al, 2001). Figure 2 shows examples of the
chains which can be extracted from the tree in Figure
1.
Chains provide a mechanism for encoding infor-
mation beyond the direct arguments of predicates
and includes areas of the dependency tree ignored by
the SVO model. For example, they can represent in-
formation expressed as a nominalisation or within a
prepositional phrase, e.g. ?The resignation of Smith
from the board of Acme ...? However, a potential
shortcoming of this model is that it cannot represent
the link between arguments of a verb. Patterns in the
chain model format are unable to represent even the
simplest of sentences containing a transitive verb,
e.g. ?Smith left Acme?.
Linked Chains: The linked chains model
(Greenwood et al, 2005) represents extraction pat-
terns as a pair of chains which share the same verb
but no direct descendants. Example linked chains
are shown in Figure 2. This pattern representa-
tion encodes most of the information in the sen-
tence with the advantage of being able to link to-
gether event participants which neither of the SVO
or chain model can, for example the relation be-
tween ?Smith? and ?Bloggs? in Figure 1.
Subtrees: The final model to be considered is the
subtree model (Sudo et al, 2003). In this model any
subtree of a dependency tree can be used as an ex-
traction pattern, where a subtree is any set of nodes
in the tree which are connected to one another. Sin-
gle nodes are not considered to be subtrees. The
subtree model is a richer representation than those
discussed so far and can represent any part of a de-
pendency tree. Each of the previous models form a
proper subset of the subtrees. By choosing an appro-
priate subtree it is possible to link together any pair
of nodes in a tree and consequently this model can
82
SVO
[V/hire](subj[N/Acme]+obj[N/Smith])
[V/replace](obj[N/Bloggs])
Chains
[V/hire](subj[N/Acme])
[V/hire](obj[N/Smith])
[V/hire](obj[N/Smith](as[N/CEO]))
[V/hire](obj[N/Smith](as[N/CEO](gen[N/their])))
Linked Chains
[V/hire](subj[N/Acme]+obj[N/Smith])
[V/hire](subj[N/Acme]+obj[N/Smith](as[N/CEO]))
[V/hire](obj[N/Smith]+vpsc mod[V/replace](obj[N/Bloggs]))
Subtrees
[V/hire](subj[N/Acme]+obj[N/Smith]+vpsc mod[V/replace])
[V/hire](subj[N/Acme]+vpsc mod[V/replace](obj[N/Bloggs]))
[N/Smith](as[N/CEO](gen[N/their]+mod[A/new]))
Figure 2: Example patterns for four models
represent the relation between any set of items in the
sentence.
3 Previous Comparisons
There have been few direct comparisons of the var-
ious pattern models. Sudo et al (2003) compared
three models (SVO, chains and subtrees) on two
IE scenarios using a entity extraction task. Mod-
els were evaluated in terms of their ability to iden-
tify entities taking part in events and distinguish
them from those which did not. They found the
SVO model performed poorly in comparison with
the other two models and that the performance of
the subtree model was generally the same as, or
better than, the chain model. However, they did
not attempt to determine whether the models could
identify the relations between these entities, simply
whether they could identify the entities participating
in relevant events.
Stevenson and Greenwood (2006) compared the
four pattern models described in Section 2 in terms
of their complexity and ability to represent rela-
tions found in text. The complexity of each model
was analysed in terms of the number of patterns
which would be generated from a given depen-
dency parse. This is important since several of
the algorithms which have been proposed to make
use of dependency-based IE patterns use iterative
learning (e.g. (Yangarber et al, 2000; Yangarber,
2003; Stevenson and Greenwood, 2005)) and are un-
likely to cope with very large sets of candidate pat-
terns. The number of patterns generated therefore
has an effect on how practical computations using
that model may be. It was found that the number
of patterns generated for the SVO model is a lin-
ear function of the size of the dependency tree. The
number of chains and linked chains is a polynomial
function while the number of subtrees is exponen-
tial.
Stevenson and Greenwood (2006) also analysed
the representational power of each model by measur-
ing how many of the relations found in a standard IE
corpus they are expressive enough to represent. (The
documents used were taken from newswire texts and
biomedical journal articles.) They found that the
SVO and chain model could only represent a small
proportion of the relations in the corpora. The sub-
tree model could represent more of the relations than
any other model but that there was no statistical dif-
ference between those relations and the ones cov-
ered by the linked chain model. They concluded
that the linked chain model was optional since it is
expressive enough to represent the information of
interest without introducing a potentially unwieldy
number of patterns.
There is some agreement between these two stud-
ies, for example that the SVO model performs
poorly in comparison with other models. However,
Stevenson and Greenwood (2006) also found that
the coverage of the chain model was significantly
worse than the subtree model, although Sudo et al
83
(2003) found that in some cases their performance
could not be distinguished. In addition to these dis-
agreements, these studies are also limited by the fact
that they are indirect; they do not evaluate the vari-
ous pattern models on an IE task.
4 Experiments
We compared each of the patterns models described
in Section 2 using an unsupervised IE experiment
similar to one described by Sudo et al (2003).
Let D be a corpus of documents and R a set of
documents which are relevant to a particular extrac-
tion task. In this context ?relevant? means that the
document contains the information we are interested
in identifying. D and R are such that D = R ? R?
and R?R? = ?. As assumption behind this approach
is that useful patterns will be far more likely to occur
in R than D overall.
4.1 Ranking Patterns
Patterns for each model are ranked using a technique
inspired by the tf-idf scoring commonly used in In-
formation Retrieval (Manning and Schu?tze, 1999).
The score for each pattern, p, is given by:
score(p) = tfp ?
(
N
dfp
)?
(1)
where tfp is the number of times pattern p ap-
pears in relevant documents, N is the total number
of documents in the corpus and dfp the number of
documents in the collection containing the pattern
p.
Equation 1 combines two factors: the term fre-
quency (in relevant documents) and inverse docu-
ment frequency (across the corpus). Patterns which
occur frequently in relevant documents without be-
ing too prevalent in the corpus are preferred. Sudo
et al (2003) found that it was important to find the
appropriate balance between these two factors. They
introduced the ? parameter as a way of controlling
the relative contribution of the inverse document fre-
quency. ? is tuned for each extraction task and pat-
tern model combination.
Although simple, this approach has the advantage
that it can be applied to each of the four pattern mod-
els to provide a direct comparison.
4.2 Extraction Scenario
The ranking process was applied to the IE scenario
used for the sixth Message Understanding confer-
ence (MUC-6). The aim of this task was to iden-
tify management succession events from a corpus
of newswire texts. Relevant information describes
an executive entering or leaving a position within a
company, for example ?Last month Smith resigned
as CEO of Rooter Ltd.?. This sentence described as
event involving three items: a person (Smith), po-
sition (CEO) and company (Rooter Ltd). We made
use of a version of the MUC-6 corpus described by
Soderland (1999) which consists of 598 documents.
For these experiments relevant documents were
identified using annotations in the corpus. However,
this is not necessary since Sudo et al (2003) showed
that adequate knowledge about document relevance
could be obtained automatically using an IR system.
4.3 Pattern Generation
The texts used for these experiments were parsed
using the Stanford dependency parser (Klein and
Manning, 2002). The dependency trees were pro-
cessed to replace the names of entities belonging
to specific semantic classes with a general token.
Three of these classes were used for the manage-
ment succession domain (PERSON, ORGANISA-
TION and POST). For example, in the dependency
analysis of ?Smith will became CEO next year?,
?Smith? is replaced by PERSON and ?CEO? by
POST. This process allows more general patterns to
be extracted from the dependency trees. For exam-
ple, [V/become](subj[N/PERSON]+obj[N/POST]).
In the MUC-6 corpus items belonging to the relevant
semantic classes are already identified.
Patterns for each of the four models were ex-
tracted from the processed dependency trees. For
the SVO, chain and linked chain models this was
achieved using depth-first search. However, the
enumeration of all subtrees is less straightforward
and has been shown to be a #P -complete prob-
lem (Goldberg and Jerrum, 2000). We made use of
the rightmost extension algorithm (Abe et al, 2002;
Zaki, 2002) which is an efficient way of enumerating
all subtrees. This approach constructs subtrees iter-
atively by combining together subtrees which have
already been observed. The algorithm starts with a
84
set of trees, each of which consists of a single node.
At each stage the known trees are extended by the
addition of a single node. In order to avoid dupli-
cation the extension is restricted to allowing nodes
only to be added to the nodes on the rightmost path
of the tree. Applying the process recursively creates
a search space in which all subtrees are enumerated
with minimal duplication.
The rightmost extension algorithm is most suited
to finding subtrees which occur multiple times and,
even using this efficient approach, we were unable
to generate subtrees which occurred fewer than four
times in the MUC-6 texts in a reasonable time. Sim-
ilar restrictions have been encountered within other
approaches which have relied on the generation of
a comprehensive set of subtrees from a parse for-
est. For example, Kudo et al (2005) used subtrees
for parse ranking but could only generate subtrees
which appear at least ten times in a 40,000 sentence
corpus. They comment that the size of their data set
meant that it would have been difficult to complete
the experiments with less restrictive parameters. In
addition, Sudo et al (2003) only generated subtrees
which appeared in at least three documents. Kudo
et al (2005) and Sudo et al (2003) both used the
rightmost extension algorithm to generate subtrees.
To provide a direct comparison of the pattern
models we also produced versions of the sets of pat-
terns extracted for the SVO, chain and linked chain
models in which patterns which occurred fewer than
four times were removed. Table 1 shows the num-
ber of patterns generated for each of the four mod-
els when the patterns are both filtered and unfil-
tered. (Although the set of unfiltered subtree pat-
terns were not generated it is possible to determine
the number of patterns which would be generated
using a process described by Stevenson and Green-
wood (2006).)
Model Filtered Unfiltered
SVO 9,189 23,128
Chains 16,563 142,019
Linked chains 23,452 493,463
Subtrees 369,453 1.69 ?1012
Table 1: Number of patterns generated by each
model
It can be seen that the various pattern models gen-
erate vastly different numbers of patterns and that
the number of subtrees is significantly greater than
the other three models. Previous analysis (see Sec-
tion 3) suggested that the number of subtrees which
would be generated from a corpus could be difficult
to process computationally and this is supported by
our findings here.
4.4 Parameter Tuning
The value of ? in equation 1 was set using a sep-
arate corpus from which the patterns were gener-
ated, a methodology suggested by Sudo et al (2003).
To generate this additional text we used the Reuters
Corpus (Rose et al, 2002) which consists of a year?s
worth of newswire output. Each document in the
Reuters corpus has been manually annotated with
topic codes indicating its general subject area(s).
One of these topic codes (C411) refers to man-
agement succession events and was used to identify
documents which are relevant to the MUC6 IE sce-
nario. A corpus consisting of 348 documents anno-
tated with code C411 and 250 documents without
that code, representing irrelevant documents, were
taken from the Reuters corpus to create a corpus
with the same distribution of relevant and irrelevant
documents as found in the MUC-6 corpus. Unlike
the MUC-6 corpus, items belonging to the required
semantic classes are not annotated in the Reuters
Corpus. They were identified automatically using
a named entity identifier.
The patterns generated from the MUC-6 texts
were ranked using formula 1 with a variety of val-
ues of ?. These sets of ranked patterns were then
used to carry out a document filtering task on the
Reuters corpus - the aim of which is to differentiate
documents based on whether or not they contain a
relation of interest. The various values for ? were
compared by computing the area under the curve. It
was found that the optimal value for ? was 2 for all
pattern models and this setting was used for the ex-
periments.
4.5 Evaluation
Evaluation was carried out by comparing the ranked
lists of patterns against the dependency trees for the
MUC-6 texts. When a pattern is found to match
against a tree the items which match any seman-
85
tic classes in the pattern are extracted. These items
are considered to be related and compared against
the gold standard data in the corpus to determine
whether they are in fact related.
The precision of a set of patterns is computed as
the proportion of the relations which were identified
that are listed in the gold standard data. The recall is
the proportion of relations in the gold standard data
which are identified by the set of patterns.
The ranked set of patterns are evaluated incremen-
tally with the precision and recall of the first (highest
ranked) pattern computed. The next pattern is then
added to the relations extracted by both are evalu-
ated. This process continues until all patterns are
exhausted.
5 Results
Figure 3 shows the results when the four filtered pat-
tern models, ranked using equation 1, are compared.
A first observation is that the chain model
performs poorly in comparison to the other
three models. The highest precision achieved by
this model is 19.9% and recall never increases
beyond 9%. In comparison the SVO model in-
cludes patterns with extremely high precision but
the maximum recall achieved by this model is
low. Analysis showed that the first three SVO
patterns had very high precision. These were
[V/succeed](subj[N/PERSON]+obj[N/PERSON]),
[V/be](subj[N/PERSON]+obj[N/POST]) and
[V/become](subj[N/PERSON]+obj[N/POST]),
which have precision of 90.1%, 80.8% and 78.9%
respectively. If these high precision patterns are
removed the maximum precision of the SVO model
is around 32%, which is comparable with the linked
chain and subtree models. This suggests that, while
the SVO model includes very useful extraction
patterns, the format is restrictive and is unable to
represent much of the information in this corpus.
The remaining two pattern models, linked chains
and subtrees, have very similar performance and
each achieves higher recall than the SVO model, al-
beit with lower precision. The maximum recall ob-
tained by the linked chain model is slightly lower
than the subtree model but it does maintain higher
precision at higher recall levels.
The maximum recall achieved by all four models
is very low in this evaluation and part of the reason
for this is the fact that the patterns have been filtered
to allow direct comparison with the subtree model.
Figure 4 shows the results when the unfiltered SVO,
chain and linked chain patterns are used. (Perfor-
mance of the filtered subtrees are also included in
this graph for comparison.)
This result shows that the addition of extra pat-
terns for each model improves recall without effect-
ing the maximum precision achieved. The chain
model also performs badly in this experiment. Pre-
cision of the SVO model is still high (again this is
due to the same three highly accurate patterns) how-
ever the maximum recall achieved by this model is
not particularly increased by the addition of the un-
filtered patterns. The linked chain model benefits
most from the unfiltered patterns. The extra patterns
lead to a maximum recall which is more than dou-
ble any of the other models without overly degrad-
ing precision. The fact that the linked chain model
is able to achieve such a high recall shows that it is
able to represent the relations found in the MUC-6
text, unlike the SVO and chain models. It is likely
that the subtrees model would also produce a set of
patterns with high recall but the number of poten-
tial patterns which are allowable within this model
makes this impractical.
6 Discussion and Conclusions
Some of the results reported for each model in these
experiments are low. Precision levels are generally
below 40% (with the exception of the SVO model
which achieves high precision using a small number
of patterns). One reason for this that the the patterns
were ranked using a simple unsupervised learning
algorithm which allowed direct comparison of four
different pattern models. This approach only made
use of information about the distribution of patterns
in the corpus and it is likely that results could be im-
proved for a particular pattern model by employing
more sophisticated approaches which make use of
additional information, for example the structure of
the patterns.
The results presented here provide insight into the
usefulness of the various pattern models by evaluat-
ing them on an actual IE task. It is found that SVO
patterns are capable of high precision but that the
86
0.0 0.1 0.2 0.3 0.4
Recall
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Pr
ec
isi
on
Subject-Verb-Object
Chains
Linked Chains
Subtrees
Figure 3: Comparisons of filtered pattern models.
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Recall
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Pr
ec
isi
on
Subject-Verb-Object
Chains
Linked Chains
Subtrees
Figure 4: Comparison of unfiltered models.
87
restricted set of possible patterns leads to low re-
call. The chain model was found to perform badly
with low recall and precision regardless of whether
the patterns were filtered. Performance of the linked
chain and subtree models were similar when the pat-
terns were filtered but unfiltered linked chains were
capable of achieving far higher recall than the fil-
tered subtrees.
These experiments suggest that the linked chain
model is a useful one for IE since it is simple enough
for an unfiltered set of patterns to be extracted and
able to represent a wider range of information than
the SVO and chain models.
References
Kenji Abe, Shinji Kawasoe, Tatsuya Asai, Hiroki
Arimura, and Setsuo Arikawa. 2002. Optimised Sub-
structure Discovery for Semi-Structured Data. In Pro-
ceedings of the 6th European Conference on Princi-
ples and Practice of Knowledge in Databases (PKDD-
2002), pages 1?14.
Leslie Ann Goldberg and Mark Jerrum. 2000. Counting
Unlabelled Subtrees of a Tree is #P -Complete. Lon-
don Mathmatical Society Journal of Computation and
Mathematics, 3:117?124.
Mark A. Greenwood, Mark Stevenson, Yikun Guo, Henk
Harkema, and Angus Roberts. 2005. Automati-
cally Acquiring a Linguistically Motivated Genic In-
teraction Extraction System. In Proceedings of the
4th Learning Language in Logic Workshop (LLL05),
Bonn, Germany.
Dan Klein and Christopher D. Manning. 2002. Fast
Exact Inference with a Factored Model for Natural
Language Parsing. In Advances in Neural Informa-
tion Processing Systems 15 (NIPS 2002), Vancouver,
Canada.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based Parse Reranking with Subtree Fea-
tures. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
189?196, Ann Arbour, MI.
Chritopher Manning and Hinrich Schu?tze. 1999. Foun-
dations of Statistical Natural Language Processing.
MIT Press, Cambridge, MA.
Igor Mel?c?uk. 1987. Dependency Syntax: Theory and
Practice. SUNY Press, New York.
Ion Muslea. 1999. Extraction Patterns for Information
Extraction: A Survey. In Proceedings of the AAAI-99
workshop on Machine Learning for Information Ex-
traction, Orlando, FL.
Ellen Riloff. 1996. Automatically Generating Extraction
Patterns from Untagged Text. In Thirteenth National
Conference on Artificial Intelligence (AAAI-96), pages
1044?1049, Portland, OR.
Tony Rose, Mark Stevenson, and Miles Whitehead.
2002. The Reuters Corpus Volume 1 - from Yes-
terday?s News to Tomorrow?s Language Resources.
In Proceedings of the Third International Conference
on Language Resources and Evaluation (LREC-02),
pages 827?832, La Palmas de Gran Canaria.
Stephen Soderland. 1999. Learning Information Extrac-
tion Rules for Semi-structured and Free Text. Machine
Learning, 31(1-3):233?272.
Mark Stevenson and Mark A. Greenwood. 2005. A Se-
mantic Approach to IE Pattern Induction. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics, pages 379?386, Ann Ar-
bor, MI.
Mark Stevenson and Mark A. Greenwood. 2006. Com-
paring Information Extraction Pattern Models. In Pro-
ceedings of the Information Extraction Beyond The
Document Workshop (COLING/ACL 2006), pages 12?
19, Sydney, Australia.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2001. Automatic Pattern Acquisition for Japanese
Information Extraction. In Proceedings of the Hu-
man Language Technology Conference (HLT2001),
San Diego, CA.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An Improved Extraction Pattern Representa-
tion Model for Automatic IE Pattern Acquisition. In
Proceedings of the 41st Annual Meeting of the Associ-
ation for Computational Linguistics (ACL-03), pages
224?231, Sapporo, Japan.
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Unsupervised Discov-
ery of Scenario-level Patterns for Information Extrac-
tion. In Proceedings of the Applied Natural Language
Processing Conference (ANLP 2000), pages 282?289,
Seattle, WA.
Roman Yangarber. 2003. Counter-training in the Dis-
covery of Semantic Patterns. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics (ACL-03), pages 343?350, Sapporo,
Japan.
Mohammed Zaki. 2002. Effectively Mining Frequent
Trees in a Forest. In 8th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing, pages 71?80, Edmonton, Canada.
88
Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 34?41
Manchester, UK. August 2008
A Data Driven Approach to Query Expansion in Question Answering
Leon Derczynski, Jun Wang, Robert Gaizauskas and Mark A. Greenwood
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello
Sheffield S1 4DP UK
{aca00lad, acp07jw}@shef.ac.uk
{r.gaizauskas, m.greenwood}@dcs.shef.ac.uk
Abstract
Automated answering of natural language
questions is an interesting and useful prob-
lem to solve. Question answering (QA)
systems often perform information re-
trieval at an initial stage. Information re-
trieval (IR) performance, provided by en-
gines such as Lucene, places a bound on
overall system performance. For example,
no answer bearing documents are retrieved
at low ranks for almost 40% of questions.
In this paper, answer texts from previous
QA evaluations held as part of the Text
REtrieval Conferences (TREC) are paired
with queries and analysed in an attempt
to identify performance-enhancing words.
These words are then used to evaluate the
performance of a query expansion method.
Data driven extension words were found
to help in over 70% of difficult questions.
These words can be used to improve and
evaluate query expansion methods. Sim-
ple blind relevance feedback (RF) was cor-
rectly predicted as unlikely to help overall
performance, and an possible explanation
is provided for its low value in IR for QA.
1 Introduction
The task of supplying an answer to a question,
given some background knowledge, is often con-
sidered fairly trivial from a human point of view,
as long as the question is clear and the answer is
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
known. The aim of an automated question answer-
ing system is to provide a single, unambiguous re-
sponse to a natural language question, given a text
collection as a knowledge source, within a certain
amount of time. Since 1999, the Text Retrieval
Conferences have included a task to evaluate such
systems, based on a large pre-defined corpus (such
as AQUAINT, containing around a million news
articles in English) and a set of unseen questions.
Many information retrieval systems perform
document retrieval, giving a list of potentially rel-
evant documents when queried ? Google?s and Ya-
hoo!?s search products are examples of this type of
application. Users formulate a query using a few
keywords that represent the task they are trying to
perform; for example, one might search for ?eif-
fel tower height? to determine how tall the Eiffel
tower is. IR engines then return a set of references
to potentially relevant documents.
In contrast, QA systems must return an exact an-
swer to the question. They should be confident
that the answer has been correctly selected; it is
no longer down to the user to research a set of doc-
ument references in order to discover the informa-
tion themselves. Further, the system takes a natural
language question as input, instead of a few user-
selected key terms.
Once a QA system has been provided with a
question, its processing steps can be described in
three parts - Question Pre-Processing, Text Re-
trieval and Answer Extraction:
1. Question Pre-Processing TREC questions
are grouped into series which relate to a given
target. For example, the target may be ?Hinden-
burg disaster? with questions such as ?What type
of craft was the Hindenburg?? or ?How fast could
it travel??. Questions may include pronouns ref-
34
erencing the target or even previous answers, and
as such require processing before they are suitable
for use.
2. Text Retrieval An IR component will return
a ranked set of texts, based on query terms. At-
tempting to understand and extract data from an
entire corpus is too resource intensive, and so an IR
engine defines a limited subset of the corpus that
is likely to contain answers. The question should
have been pre-processed correctly for a useful set
of texts to be retrieved ? including anaphora reso-
lution.
3. Answer Extraction (AE) Given knowledge
about the question and a set of texts, the AE sys-
tem attempts to identify answers. It should be clear
that only answers within texts returned by the IR
component have any chance of being found.
Reduced performance at any stage will have a
knock-on effect, capping the performance of later
stages. If questions are left unprocessed and full
of pronouns (e.g.,?When did it sink??) the IR com-
ponent has very little chance of working correctly
? in this case, the desired action is to retrieve
documents related to the Kursk submarine, which
would be impossible.
IR performance with a search engine such as
Lucene returns no useful documents for at least
35% of all questions ? when looking at the top
20 returned texts. This caps the AE component
at 65% question ?coverage?. We will measure the
performance of different IR component configura-
tions, to rule out problems with a default Lucene
setup.
For each question, answers are provided in the
form of regular expressions that match answer text,
and a list of documents containing these answers
in a correct context. As references to correct doc-
uments are available, it is possible to explore a
data-driven approach to query analysis. We deter-
mine which questions are hardest then concentrate
on identifying helpful terms found in correct doc-
uments, with a view to building a system than can
automatically extract these helpful terms from un-
seen questions and supporting corpus. The avail-
ability and usefulness of these terms will provide
an estimate of performance for query expansion
techniques.
There are at least two approaches which could
make use of these term sets to perform query ex-
pansion. They may occur in terms selected for
blind RF (non-blind RF is not applicable to the
TREC QA task). It is also possible to build a cata-
logue of terms known to be useful according to cer-
tain question types, thus leading to a dictionary of
(known useful) expansions that can be applied to
previously unseen questions. We will evaluate and
also test blind relevance feedback in IR for QA.
2 Background and Related Work
The performance of an IR system can be quanti-
fied in many ways. We choose and define mea-
sures pertinent to IR for QA. Work has been done
on relevance feedback specific to IR for QA, where
it is has usually be found to be unhelpful. We out-
line the methods used in the past, extend them, and
provide and test means of validating QA relevance
feedback.
2.1 Measuring QA Performance
This paper uses two principle measures to describe
the performance of the IR component. Coverage
is defined as the proportion of questions where at
least one answer bearing text appears in the re-
trieved set. Redundancy is the average number
of answer bearing texts retrieved for each ques-
tion (Roberts and Gaizauskas, 2004).
Both these measures have a fixed limit n on the
number of texts retrieved by a search engine for a
query. As redundancy counts the number of texts
containing correct answers, and not instances of
the answer itself, it can never be greater than the
number of texts retrieved.
The TREC reference answers provide two ways
of finding a correct text, with both a regular expres-
sion and a document ID. Lenient hits (retrievals of
answer bearing documents) are those where the re-
trieved text matches the regular expression; strict
hits occur when the document ID of the retrieved
text matches that declared by TREC as correct and
the text matches the regular expression. Some doc-
uments will match the regular expression but not
be deemed as containing a correct answer (this
is common with numbers and dates (Baeza-Yates
and Ribeiro-Neto, 1999)), in which case a lenient
match is found, but not a strict one.
The answer lists as defined by TREC do not in-
clude every answer-bearing document ? only those
returned by previous systems and marked as cor-
rect. Thus, false negatives are a risk, and strict
measures place an approximate lower bound on
the system?s actual performance. Similarly, lenient
35
matches can occur out of context, without a sup-
porting document; performance based on lenient
matches can be viewed as an approximate upper
bound (Lin and Katz, 2005).
2.2 Relevance Feedback
Relevance feedback is a widely explored technique
for query expansion. It is often done using a spe-
cific measure to select terms using a limited set of
ranked documents of size r; using a larger set will
bring term distribution closer to values over the
whole corpus, and away from ones in documents
relevant to query terms. Techniques are used to
identify phrases relevant to a query topic, in or-
der to reduce noise (such as terms with a low cor-
pus frequency that relate to only a single article)
and query drift (Roussinov and Fan, 2005; Allan,
1996).
In the context of QA, Pizzato (2006) employs
blind RF using the AQUAINT corpus in an attempt
to improve performance when answering factoid
questions on personal names. This is a similar ap-
proach to some content in this paper, though lim-
ited to the study of named entities, and does not
attempt to examine extensions from the existing
answer data.
Monz (2003) finds a negative result when apply-
ing blind feedback for QA in TREC 9, 10 and 11,
and a neutral result for TREC 7 and 8?s ad hoc re-
trieval tasks. Monz?s experiment, using r = 10
and standard Rocchio term weighting, also found
a further reduction in performance when r was
reduced (from 10 to 5). This is an isolated ex-
periment using just one measure on a limited set
of questions, with no use of the available answer
texts.
Robertson (1992) notes that there are issues
when using a whole document for feedback, as
opposed to just a single relevant passage; as men-
tioned in Section 3.1, passage- and document-level
retrieval sets must also be compared for their per-
formance at providing feedback. Critically, we
will survey the intersection between words known
to be helpful and blind RF terms based on initial
retrieval, thus showing exactly how likely an RF
method is to succeed.
3 Methodology
We first investigated the possibility of an IR-
component specific failure leading to impaired
coverage by testing a variety of IR engines and
configurations. Then, difficult questions were
identified, using various performance thresholds.
Next, answer bearing texts for these harder ques-
tions were checked for words that yielded a per-
formance increase when used for query expansion.
After this, we evaluated how likely a RF-based ap-
proach was to succeed. Finally, blind RF was ap-
plied to the whole question set. IR performance
was measured, and terms used for RF compared to
those which had proven to be helpful as extension
words.
3.1 IR Engines
A QA framework (Greenwood, 2004a) was origi-
nally used to construct a QA system based on run-
ning a default Lucene installation. As this only
covers one IR engine in one configuration, it is
prudent to examine alternatives. Other IR engines
should be tested, using different configurations.
The chosen additional engines were: Indri, based
on the mature INQUERY engine and the Lemur
toolkit (Allan et al, 2003); and Terrier, a newer en-
gine designed to deal with corpora in the terabyte
range and to back applications entered into TREC
conferences (Ounis et al, 2005).
We also looked at both passage-level and
document-level retrieval. Passages can be de-
fined in a number of ways, such as a sentence,
a sliding window of k terms centred on the tar-
get term(s), parts of a document of fixed (and
equal) lengths, or a paragraph. In this case,
the documents in the AQUAINT corpus contain
paragraph markers which were used as passage-
level boundaries, thus making ?passage-level?
and ?paragraph-level? equivalent in this paper.
Passage-level retrieval may be preferable for AE,
as the number of potential distracters is some-
what reduced when compared to document-level
retrieval (Roberts and Gaizauskas, 2004).
The initial IR component configuration was with
Lucene indexing the AQUAINT corpus at passage-
level, with a Porter stemmer (Porter, 1980) and an
augmented version of the CACM (Jones and van
Rijsbergen, 1976) stopword list.
Indri natively supports document-level indexing
of TREC format corpora. Passage-level retrieval
was done using the paragraph tags defined in the
corpus as delimiters; this allows both passage- and
document-level retrieval from the same index, ac-
cording to the query.
All the IR engines were unified to use the Porter
36
Coverage Redundancy
Year Len. Strict Len. Strict
Lucene
2004 0.686 0.636 2.884 1.624
2005 0.703 0.566 2.780 1.155
2006 0.665 0.568 2.417 1.181
Indri
2004 0.690 0.554 3.849 1.527
2005 0.694 0.512 3.908 1.056
2006 0.691 0.552 3.373 1.152
Terrier
2004 - - - -
2005 - - - -
2006 0.638 0.493 2.520 1.000
Table 1: Performance of Lucene, Indri and Terrier at para-
graph level, over top 20 documents. This clearly shows the
limitations of the engines.
stemmer and the same CACM-derived stopword
list.
The top n documents for each question in the
TREC2004, TREC2005 and TREC2006 sets were
retrieved using every combination of engine, and
configuration1 . The questions and targets were
processed to produce IR queries as per the default
configuration for the QA framework. Examining
the top 200 documents gave a good compromise
between the time taken to run experiments (be-
tween 30 and 240 minutes each) and the amount
one can mine into the data. Tabulated results are
shown in Table 1 and Table 2. Queries have had
anaphora resolution performed in the context of
their series by the QA framework. AE compo-
nents begin to fail due to excess noise when pre-
sented with over 20 texts, so this value is enough to
encompass typical operating parameters and leave
space for discovery (Greenwood et al, 2006).
A failure analysis (FA) tool, an early version
of which is described by (Sanka, 2005), provided
reporting and analysis of IR component perfor-
mance. In this experiment, it provided high level
comparison of all engines, measuring coverage
and redundancy as the number of documents re-
trieved, n, varies. This is measured because a per-
fect engine will return the most useful documents
first, followed by others; thus, coverage will be
higher for that engine with low values of n.
3.2 Identification of Difficult Questions
Once the performance of an IR configuration over
a question set is known, it?s possible to produce
a simple report listing redundancy for each ques-
tion. A performance reporting script accesses the
1Save Terrier / TREC2004 / passage-level retrieval;
passage-level retrieval with Terrier was very slow using our
configuration, and could not be reliably performed using the
same Terrier instance as document-level retrieval.
Coverage Redundancy
Year Len. Strict Len. Strict
Indri
2004 0.926 0.837 7.841 2.663
2005 0.935 0.735 7.573 1.969
2006 0.882 0.741 6.872 1.958
Terrier
2004 0.919 0.806 7.186 2.380
2005 0.928 0.766 7.620 2.130
2006 0.983 0.783 6.339 2.067
Table 2: Performance of Indri and Terrier at document level
IR over the AQUAINT corpus, with n = 20
FA tool?s database and lists all the questions in
a particular set with the strict and lenient redun-
dancy for selected engines and configurations. En-
gines may use passage- or document-level config-
urations.
Data on the performance of the three engines is
described in Table 2. As can be seen, the cover-
age with passage-level retrieval (which was often
favoured, as the AE component performs best with
reduced amounts of text) languishes between 51%
and 71%, depending on the measurement method.
Failed anaphora resolution may contribute to this
figure, though no deficiencies were found upon vi-
sual inspection.
Not all documents containing answers are noted,
only those checked by the NIST judges (Bilotti
et al, 2004). Match judgements are incomplete,
leading to the potential generation of false nega-
tives, where a correct answer is found with com-
plete supporting information, but as the informa-
tion has not been manually flagged, the system will
mark this as a failure. Assessment methods are
fully detailed in Dang et al (2006). Factoid per-
formance is still relatively poor, although as only
1.95 documents match per question, this may be an
effect of such false negatives (Voorhees and Buck-
land, 2003). Work has been done into creating
synthetic corpora that include exhaustive answer
sets (Bilotti, 2004; Tellex et al, 2003; Lin and
Katz, 2005), but for the sake of consistency, and
easy comparison with both parallel work and prior
local results, the TREC judgements will be used to
evaluate systems in this paper.
Mean redundancy is also calculated for a num-
ber of IR engines. Difficult questions were those
for which no answer bearing texts were found by
either strict or lenient matches in any of the top n
documents, using a variety of engines. As soon as
one answer bearing document was found by an en-
gine using any measure, that question was deemed
non-difficult. Questions with mean redundancy of
37
zero are marked difficult, and subjected to further
analysis. Reducing the question set to just diffi-
cult questions produces a TREC-format file for re-
testing the IR component.
3.3 Extension of Difficult Questions
The documents deemed relevant by TREC must
contain some useful text that can help IR engine
performance. Such words should be revealed by
a gain in redundancy when used to extend an ini-
tially difficult query, usually signified by a change
from zero to a non-zero value (signifying that rele-
vant documents have been found where none were
before). In an attempt to identify where the use-
ful text is, the relevant documents for each difficult
question were retrieved, and passages matching the
answer regular expression identified. A script is
then used to build a list of terms from each passage,
removing words in the question or its target, words
that occur in the answer, and stopwords (based on
both the indexing stopword list, and a set of stems
common within the corpus). In later runs, num-
bers are also stripped out of the term list, as their
value is just as often confusing as useful (Baeza-
Yates and Ribeiro-Neto, 1999). Of course, answer
terms provide an obvious advantage that would not
be reproducible for questions where the answer is
unknown, and one of our goals is to help query ex-
pansion for unseen questions. This approach may
provide insights that will enable appropriate query
expansion where answers are not known.
Performance has been measured with both the
question followed by an extension (Q+E), as well
as the question followed by the target and then
extension candidates (Q+T+E). Runs were also
executed with just Q and Q+T, to provide non-
extended reference performance data points. Ad-
dition of the target often leads to gains in perfor-
mance (Roussinov et al, 2005), and may also aid
in cases where anaphora resolution has failed.
Some words are retained, such as titles, as in-
cluding these can be inferred from question or tar-
get terms and they will not unfairly boost redun-
dancy scores; for example, when searching for a
?Who? question containing the word ?military?,
one may want to preserve appellations such as
?Lt.? or ?Col.?, even if this term appears in the an-
swer.
This filtered list of extensions is then used to cre-
ate a revised query file, containing the base ques-
tion (with and without the target suffixed) as well
as new questions created by appending a candidate
extension word.
Results of retrievals with these new question are
loaded into the FA database and a report describ-
ing any performance changes is generated. The
extension generation process also creates custom
answer specifications, which replicate the informa-
tion found in the answers defined by TREC.
This whole process can be repeated with vary-
ing question difficulty thresholds, as well as alter-
native n values (typically from 5 to 100), different
engines, and various question sets.
3.4 Relevance Feedback Performance
Now that we can find the helpful extension words
(HEWs) described earlier, we?re equipped to eval-
uate query expansion methods. One simplistic ap-
proach could use blind RF to determine candidate
extensions, and be considered potentially success-
ful should these words be found in the set of HEWs
for a query. For this, term frequencies can be
measured given the top r documents retrieved us-
ing anaphora-resolved query Q. After stopword
and question word removal, frequent terms are ap-
pended to Q, which is then re-evaluated. This
has been previously attempted for factoid ques-
tions (Roussinov et al, 2005) and with a limited
range of r values (Monz, 2003) but not validated
using a set of data-driven terms.
We investigated how likely term frequency (TF)
based RF is to discover HEWs. To do this, the
proportion of HEWs that occurred in initially re-
trieved texts was measured, as well as the propor-
tion of these texts containing at least one HEW.
Also, to see how effective an expansion method is,
suggested expansion terms can be checked against
the HEW list.
We used both the top 5 and the top 50 documents
in formulation of extension terms, with TF as a
ranking measure; 50 is significantly larger than the
optimal number of documents for AE (20), without
overly diluting term frequencies.
Problems have been found with using entire
documents for RF, as the topic may not be the
same throughout the entire discourse (Robertson
et al, 1992). Limiting the texts used for RF to
paragraphs may reduce noise; both document- and
paragraph-level terms should be checked.
38
Engine
Year Lucene
Para
Indri
Para
Indri
Doc
Terrier
Doc
2004 76 72 37 42
2005 87 98 37 35
2006 108 118 59 53
Table 3: Number of difficult questions, as defined by those
which have zero redundancy over both strict and lenient mea-
sures, at n = 20. Questions seem to get harder each year.
Document retrieval yields fewer difficult questions, as more
text is returned for potential matching.
Engine
Lucene Indri Terrier
Paragraph 226 221 -
Document - 121 109
Table 4: Number of difficult questions in the 2006 task, as de-
fined above, this time with n = 5. Questions become harder
as fewer chances are given to provide relevant documents.
4 Results
Once we have HEWs, we can determine if these
are going to be of significant help when chosen as
query extensions. We can also determine if a query
expansion method is likely to be fruitful. Blind RF
was applied, and assessed using the helpful words
list, as well as RF?s effect on coverage.
4.1 Difficult Question Analysis
The number of difficult questions found at n =
20 is shown in Table 3. Document-level retrieval
gave many fewer difficult questions, as the amount
of text retrieved gave a higher chance of finding
lenient matches. A comparison of strict and lenient
matching is in Table 5.
Extensions were then applied to difficult ques-
tions, with or without the target. The performance
of these extensions is shown in Table 6. Results
show a significant proportion (74.4%) of difficult
questions can benefit from being extended with
non-answer words found in answer bearing texts.
4.2 Applying Relevance Feedback
Identifying HEWs provides a set of words that
are useful for evaluating potential expansion terms.
Match type
Strict Lenient
Year
2004 39 49
2005 56 66
2006 53 49
Table 5: Common difficult questions (over all three engines
mentioned above) by year and match type; n = 20.
Difficult questions used 118
Variations tested 6683
Questions that benefited 87 (74.4%)
Helpful extension words (strict) 4973
Mean helpful words per question 42.144
Mean redundancy increase 3.958
Table 6: Using Terrier Passage / strict matching, retrieving 20
docs, with TREC2006 questions / AQUAINT. Difficult ques-
tions are those where no strict matches are found in the top 20
IRT from just one engine.
2004 2005 2006
HEW found in IRT 4.17% 18.58% 8.94%
IRT containing HEW 10.00% 33.33% 34.29%
RF words in HEW 1.25% 1.67% 5.71%
Table 7: ?Helpful extension words?: the set of extensions that,
when added to the query, move redundancy above zero. r =
5, n = 20, using Indri at passage level.
Using simple TF based feedback (see Section 3.4),
5 terms were chosen per query. These words had
some intersection (see Table 7) with the exten-
sion words set, indicating that this RF may lead to
performance increases for previously unseen ques-
tions. Only a small number of the HEWs occur in
the initially retrieved texts (IRTs), although a no-
ticeable proportion of IRTs (up to 34.29%) contain
at least one HEW. However, these terms are prob-
ably not very frequent in the documents and un-
likely to be selected with TF-based blind RF. The
mean proportion of RF selected terms that were
HEWs was only 2.88%. Blind RF for question an-
swering fails here due to this low proportion. Strict
measures are used for evaluation as we are inter-
ested in finding documents which were not pre-
viously being retrieved rather than changes in the
distribution of keywords in IRT.
Document and passage based RF term selection
is used, to explore the effect of noise on terms, and
document based term selection proved marginally
superior. Choosing RF terms from a small set of
documents (r = 5) was found to be marginally
better than choosing from a larger set (r = 50).
In support of the suggestion that RF would be un-
r
5 50 Baseline
Rank Doc Para Doc Para
5 0.253 0.251 0.240 0.179 0.312
10 0.331 0.347 0.331 0.284 0.434
20 0.438 0.444 0.438 0.398 0.553
50 0.583 0.577 0.577 0.552 0.634
Table 8: Coverage (strict) using blind RF. Both document-
and paragraph-level retrieval used to determine RF terms.
39
Question:
Who was the nominal leader after the overthrow?
Target: Pakistani government overthrown in 1999
Extension word Redundancy
Kashmir 4
Pakistan 4
Islamabad 2.5
Question: Where did he play in college?
Target: Warren Moon
Extension word Redundancy
NFL 2.5
football 1
Question: Who have commanded the division?
Target: 82nd Airborne division
Extension word Redundancy
Gen 3
Col 2
decimated 2
officer 1
Table 9: Queries with extensions, and their mean redundancy
using Indri at document level with n = 20. Without exten-
sions, redundancy is zero.
likely to locate HEWs, applying blind RF consis-
tently hampered overall coverage (Table 8).
5 Discussion
HEWs are often found in answer bearing texts,
though these are hard to identify through sim-
ple TF-based RF. A majority of difficult questions
can be made accessible through addition of HEWs
present in answer bearing texts, and work to deter-
mine a relationship between words found in initial
retrieval and these HEWs can lead to coverage in-
creases. HEWs also provide an effective means
of evaluating other RF methods, which can be de-
veloped into a generic rapid testing tool for query
expansion techniques. TF-based RF, while finding
some HEWs, is not effective at discovering exten-
sions, and reduces overall IR performance.
There was not a large performance change
between engines and configurations. Strict
paragraph-level coverage never topped 65%, leav-
ing a significant number of questions where no
useful information could be provided for AE.
The original sets of difficult questions for in-
dividual engines were small ? often less than the
35% suggested when looking at the coverage fig-
ures. Possible causes could include:
Difficult questions being defined as those for
which average redundancy is zero: This limit
may be too low. To remedy this, we could increase
the redundancy limit to specify an arbitrary num-
ber of difficult questions out of the whole set.
The use of both strict and lenient measures: It
is possible to get a lenient match (thus marking a
question as non-difficult) when the answer text oc-
curs out of context.
Reducing n from 20 to 5 (Table 4) increased
the number of difficult questions produced. From
this we can hypothesise that although many search
engines are succeeding in returning useful docu-
ments (where available), the distribution of these
documents over the available ranks is not one that
bunches high ranking documents up as those im-
mediately retrieved (unlike a perfect engine; see
Section 3.1), but rather suggests a more even dis-
tribution of such documents over the returned set.
The number of candidate extension words for
queries (even after filtering) is often in the range
of hundreds to thousands. Each of these words
creates a separate query, and there are two varia-
tions, depending on whether the target is included
in the search terms or not. Thus, a large number
of extended queries need to be executed for each
question run. Passage-level retrieval returns less
text, which has two advantages: firstly, it reduces
the scope for false positives in lenient matching;
secondly, it is easier to scan result by eye and de-
termine why the engine selected a result.
Proper nouns are often helpful as extensions.
We noticed that these cropped up fairly regularly
for some kinds of question (e.g. ?Who?). Espe-
cially useful were proper nouns associated with
locations - for example, adding ?Pakistani? to
a query containing the word Pakistan lifted re-
dundancy above zero for a question on President
Musharraf, as in Table 9. This reconfirms work
done by Greenwood (2004b).
6 Conclusion and Future Work
IR engines find some questions very difficult and
consistently fail to retrieve useful texts even with
high values of n. This behaviour is common over
many engines. Paragraph level retrieval seems to
give a better idea of which questions are hard-
est, although the possibility of false negatives is
present from answer lists and anaphora resolution.
Relationships exist between query words and
helpful words from answer documents (e.g. with
a military leadership themes in a query, adding the
term ?general? or ?gen? helps). Identification of
HEWs has potential use in query expansion. They
could be used to evaluate RF approaches, or asso-
ciated with question words and used as extensions.
Previous work has ruled out relevance feedback
40
in particular circumstances using a single ranking
measure, though this has not been based on analy-
sis of answer bearing texts. The presence of HEWs
in IRT for difficult questions shows that guided RF
may work, but this will be difficult to pursue. Blind
RF based on term frequencies does not increase IR
performance. However, there is an intersection be-
tween words in initially retrieved texts and words
data driven analysis defines as helpful, showing
promise for alternative RF methods (e.g. based on
TFIDF). These extension words form a basis for
indicating the usefulness of RF and query expan-
sion techniques.
In this paper, we have chosen to explore only
one branch of query expansion. An alternative data
driven approach would be to build associations be-
tween recurrently useful terms given question con-
tent. Question texts could be stripped of stopwords
and proper nouns, and a list of HEWs associated
with each remaining term. To reduce noise, the
number of times a particular extension has helped
a word would be counted. Given sufficient sample
data, this would provide a reference body of HEWs
to be used as an aid to query expansion.
References
Allan, J., J. Callan, K. Collins-Thompson, B. Croft,
F. Feng, D. Fisher, J. Lafferty, L. Larkey, TN Truong,
P. Ogilvie, et al 2003. The Lemur Toolkit for Lan-
guage Modeling and Information Retrieval.
Allan, J. 1996. Incremental Relevance Feedback for
Information Filtering. In Research and Development
in IR, pages 270?278.
Baeza-Yates, R. and B. Ribeiro-Neto. 1999. Modern
Information Retrieval. Addison Wesley.
Bilotti, M.W., B. Katz, and J. Lin. 2004. What Works
Better for Question Answering: Stemming or Mor-
phological Query Expansion. Proc. IR for QA Work-
shop at SIGIR 2004.
Bilotti, M.W. 2004. Query Expansion Techniques for
Question Answering. Master?s thesis, Massachusetts
Institute of Technology.
Dang, H.T., J. Lin, and D. Kelly. 2006. Overview of
the TREC 2006 QA track. Proc. 15th Text REtrieval
Conf..
Greenwood, M.A., M. Stevenson, and R. Gaizauskas.
2006. The University of Sheffield?s TREC 2006
Q&A Experiments. In Proc. 15th Text REtrieval
Conference
Greenwood, M.A. 2004a. AnswerFinder: Question
Answering from your Desktop. In Proc. 7th Annual
Colloquium for the UK SIG for Computational Lin-
guistics (CLUK ?04).
Greenwood, M.A. 2004b. Using Pertainyms to Im-
prove Passage Retrieval for Questions Requesting
Information about a Location. In Proc. Workshop
on IR for QA (SIGIR 2004).
Jones, K.S. and C.J. van Rijsbergen. 1976. IR Test
Collections. J. of Documentation, 32(1):59?75.
Lin, J. and B. Katz. 2005. Building a Reusable Test
Collection for Question Answering. J. American So-
ciety for Information Science and Technology.
Monz, C. 2003. From Document Retrieval to Question
Answering. ILLC Dissertation Series 2003, 4.
Ounis, I., G. Amati, V. Plachouras, B. He, C. Macdon-
ald, and D. Johnson. 2005. Terrier IR Platform.
Proc. 27th European Conf. on IR (ECIR 05), San-
tiago de Compostela, Spain, pages 517?519.
Pizzato, L.A., D. Molla, and C. Paris. 2006. Pseudo-
Relevance Feedback using Named Entities for Ques-
tion Answering. Australasian Language Technology
Workshop (ALTW2006), pages 83?90.
Porter, M. 1980. An Algorithm for Suffix Stripping
Program. Program, 14(3):130?137.
Roberts, I and R Gaizauskas. 2004. Evaluating Passage
Retrieval Approaches for Question Answering. In
Proc. 26th European Conf. on IR.
Robertson, S.E., S. Walker, M. Hancock-Beaulieu,
A. Gull, and M. Lau. 1992. Okapi at TREC. In
Text REtrieval Conf., pages 21?30.
Roussinov, D. and W. Fan. 2005. Discretization Based
Learning Approach to Information Retrieval. In
Proc. 2005 Conf. on Human Language Technologies.
Roussinov, D., M. Chau, E. Filatova, and J.A. Robles-
Flores. 2005. Building on Redundancy: Fac-
toid Question Answering, Robust Retrieval and the
?Other?. In Proc. 14th Text REtrieval Conf.
Sanka, Atheesh. 2005. Passage Retrieval for Question
Answering. Master?s thesis, University of Sheffield.
Tellex, S., B. Katz, J. Lin, A. Fernandes, and G. Marton.
2003. Quantitative Evaluation of Passage Retrieval
Algorithms for Question Answering. Proc. 26th An-
nual Int?l ACM SIGIR Conf. on R&D in IR, pages
41?47.
Voorhees, E. and L. P. Buckland, editors. 2003. Proc.
12th Text REtrieval Conference.
41
Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 58?65
Manchester, UK. August 2008
Evaluation of Automatically Reformulated Questions in Question Series
Richard Shaw, Ben Solway, Robert Gaizauskas and Mark A. Greenwood
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello
Sheffield S1 4DP UK
{aca04rcs, aca04bs}@shef.ac.uk
{r.gaizauskas, m.greenwood}@dcs.shef.ac.uk
Abstract
Having gold standards allows us to evalu-
ate new methods and approaches against a
common benchmark. In this paper we de-
scribe a set of gold standard question re-
formulations and associated reformulation
guidelines that we have created to support
research into automatic interpretation of
questions in TREC question series, where
questions may refer anaphorically to the
target of the series or to answers to pre-
vious questions. We also assess various
string comparison metrics for their utility
as evaluation measures of the proximity of
an automated system?s reformulations to
the gold standard. Finally we show how
we have used this approach to assess the
question processing capability of our own
QA system and to pinpoint areas for im-
provement.
1 Introduction
The development of computational systems which
can answer natural language questions using large
text collections as knowledge sources is widely
seen as both intellectually challenging and prac-
tically useful. To stimulate research and devel-
opment in this area the US National Institute of
Standards and Technology (NIST) has organized a
shared task evaluation as one track at the annual
TExt Retrieval Conference (TREC) since 19991.
These evaluations began by considering factoid-
type questions only (e.g. How many calories are
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1http://trec.nist.gov/
there in a Big Mac?) each of which was asked in
isolation to any of the others. However, in an effort
to move the challenge towards a long term vision
of interactive, dialogue-based question answer-
ing to support information analysts (Burger et al,
2002), the track introduced the notion of question
targets and related question series in TREC2004
(Voorhees, 2005), and this approach to question
presentation has remained central in each of the
subsequent TRECs. In this simulated task, ques-
tions are grouped into series where each series has
a target of a definition associated with it (see Fig-
ure 1). Each question in the series asks for some
information about the target and there is a final
?other? question which is to be interpreted as ?Pro-
vide any other interesting details about the target
that has not already been asked for explicitly?. In
this way ?each series is a (limited) abstraction of
an information dialogue in which the user is trying
to define the target. The target and earlier ques-
tions in a series provide the context for the current
question.? (Voorhees, 2005).
One consequence of putting questions into se-
ries in this way is that questions may not make
much sense when removed from the context their
series provides. For example, the question When
was he born? cannot be sensibly interpreted with-
out knowledge of the antecedent of he provided
by the context (target or prior questions). Inter-
preting questions in question series, therefore, be-
comes a critical component within a QA systems.
Many QA systems have an initial document re-
trieval stage that takes the question and derives a
query from it which is then passed to a search en-
gine whose task is to retrieve candidate answering
bearing documents for processing by the rest of
the system. Clearly a question such as When was
he born? is unlikely to retrieve documents rele-
58
Target 136: Shiite
Q136.1 Who was the first Imam of the Shiite
sect of Islam?
Q136.2 Where is his tomb?
Q136.3 What was this persons relationship to
the Prophet Mohammad?
Q136.4 Who was the third Imam of Shiite
Muslims?
Q136.5 When did he die?
Figure 1: An Example Question Series
vant to answering a question about Kafka?s date
of birth if passed directly to a search engine. This
problem can be addressed in a naive way by sim-
ply appending the target to every question. How-
ever, this has several disadvantages: (1) in some
cases co-reference in a question series is to the
answer of a previous question and not to the tar-
get, so blindly substituting the target is not ap-
propriate; (2) some approaches to query formula-
tion and to answer extraction from retrieved docu-
ments may require syntactically well-formed ques-
tions and may be able to take advantage of the extra
information, such as syntactic dependencies, pro-
vided in a fully de-referenced, syntactically correct
question.
Thus, it is helpful in general if systems can auto-
matically interpret a question in context so as to re-
solve co-references appropriately, and indeed most
TREC QA systems do this to at least a limited ex-
tent as part of their question pre-processing. Ide-
ally one would like a system to be able to reformu-
late a question as a human would if they were to re-
express the question so as to make it independent
of the context of the preceding portion of the ques-
tion series. To support the development of such
systems it would useful if there were a collection
of ?gold standard? reformulated questions against
which systems? outputs could be compared. How-
ever, to the best of our knowledge no such resource
exists.
In this paper we describe the creation of such a
corpus of manually reformulated questions, mea-
sures we have investigated for comparing system
generated reformulations against the gold stan-
dard, and experiments we have carried out com-
paring our TREC system?s automatic question re-
formulator against the gold standard and insights
we have obtained therefrom.
2 The Gold Standard Corpus
Our aim was to take the questions in a TREC
question series and re-express them as questions
that would naturally be asked by a human ask-
ing them as a single, stand-alone question outside
the context of the question series. Our intuition
was that most adult native speakers would agree
on a small number of variant forms these refor-
mulated questions would take. We explored this
intuition by having two persons iteratively refor-
mulate some questions independently, compare re-
sults and evolve a small set of guidelines for the
process.
2.1 Creating the Gold Standard
Ten question sets were randomly selected from
sets available at http://trec.nist.gov/
data/qa/t2007_qadata.html. These were
reformulated separately by two people and results
compared. From this an initial set of guidelines
was drawn up. Using these guidelines another 10
question sets from the TREC 2007 QA set were in-
dependently reformulated and then the guidelines
refined.
At this point the reformulators? outputs were
sufficiently close to each other and the guidelines
sufficiently stable that, given limited resources, it
was decided reformulation could proceed singly.
Using the guidelines, therefore, a further 48 ques-
tion sets from 2007 were reformulated, where
this time each question set was only reformulated
by a single person. Each question set contained
between 5 and 7 individual questions therefore
around 406 questions were reformulated, creating
one or more gold standard forms for each question.
In total there are approximately 448 individual re-
formulations, with a maximum number of 3 refor-
mulations for any single question and a mean of
1.103 reformulations per question.
2.2 Guidelines
Using the above method we derived a set of simple
guidelines which anyone should be able to follow
to create a set of reformulated questions.
Context independence and readability: The
reformulation of questions should be understand-
able outside of the question series context. The re-
formulation should be written as a native speaker
would naturally express it; this means, for exam-
ple, that stop words are included.
Example: ?How many people were killed 1991
59
eruption of Mount Pinatubo?? vs ?How many
people were killed in the 1991 eruption of Mount
Pinatubo?. The latter is preferred as it more read-
able due to the inclusion of stop words ?in the?.
Reformulate questions so as to maximise
search results:
Example: ?Who was William Shakespeare??
vs ?Who was Shakespeare??. William should be
added to the phrase as it adds extra information
which could allow more results to be found.
Target matches a sub-string of the question:
If the target string matches a sub-string of the ques-
tion the target string should substitute the entirety
of the substring. Stop-words should not be used
when determining if strings and target match but
should usually be substituted along with the rest of
the target.
Example: Target: ?Sony Pictures Entertainment
(SPE)?; Question: ?What U.S. company did Sony
purchase to form SPE??; Gold Standard: ?What
U.S. company did Sony purchase to form Sony Pic-
tures Entertainment (SPE)??
Rephrasing: A Question should not be unnec-
essarily rephrased.
Example: Target: ?Nissan Corp?; Question:
?What was Nissan formerly known as??; ?What
was Nissan Corp. formerly known as?? is pre-
ferred over the other possible reformulation ?Nis-
san Corp. was formerly known as what??.
Previous Questions and Answers: Questions
which include a reference to a previous ques-
tion should be reformulated to include a PREVI-
OUS ANSWER variable. Another reformulation
should also be provided should a system know it
needs the answer to the previous question but has
not found one. This should be a reformulation of
the previous question within the current question.
Example: Target: ?Harriet Miers withdraws
nomination to Supreme Court?; Question: ?What
criterion did this person cite in nominating
Miers??; Gold Standard 1: ?What criterion did
PREVIOUS ANSWER cite in nominating Harriet
Miers??; Gold Standard 2: ?What criterion did
this person who nominated Harriet Miers for the
post cite in nominating Harriet Miers??
Targets that contain brackets: Brackets in tar-
get should be dealt with in the following way. The
full target should be substituted into the question
in the correct place as one of the Gold Standards.
The target without the bracketed word and with it
should also be included in the Gold Standard.
Example: Target: ?Church of Jesus Christ
of Latter-day Saints (Mormons)?; Question:?Who
founded the Church of Jesus Christ of Latter-day
Saints??; Gold Standard 1: ?Who founded the
Church of Jesus Christ of Latter-day Saints (Mor-
mons)??; Gold Standard 2: ?Who founded the
Church of Jesus Christ of Latter-day Saints??;
Gold Standard 3 ?Who founded the Mormons??
Stemming and Synonyms: Words should not
be stemmed and synonyms should not be used un-
less they are found in the target or the current ques-
tion series. If they are found then both should be
used in the Gold Standard.
Example: Target: ?Chunnel?; Question:?How
long is the Chunnel??; Gold Standard: ?How long
is the Chunnel??; Incorrect reformulation: ?How
long is the Channel Tunnel??
As the term ?Channel Tunnel? is not referenced
in this section or hard-coded into the QA engine it
cannot be substituted for ?Chunnel?, even though
doing so may increase the probability of finding
the correct answer.
It: The word it should be interpreted as referring
to either the answer of the previous question of that
set or if no answer available to the target itself.
Example:Target: ?1980 Mount St. Helens erup-
tion?; Question: ?How many people died when
it erupted??; Gold Standard: ?How many people
died when Mt. St. Helens? erupted in 1980??
Pronouns (1): If the pronouns he or she are
used within a question and the TARGET is of type
?Person? then substitute the TARGET string for the
pronoun. If however the PREVIOUS ANSWER
is of type ?Person? then it should be substituted in-
stead as in this case the natural interpretation of the
pronoun is to the answer of the previous question.
Example: Target: ?Jay-Z?; Question: ?When
was he born??; Gold Standard: ?When was Jay-Z
born??
Pronouns (2): If the pronouns his/hers/their
are used within a question and the TARGET is of
type ?Person? then substitute the TARGET string
for the pronoun appending the string ??s? to the
end of the substitution. If however the PREVI-
OUS ANSWER is of type ?Person? then it should
be substituted as the natural interpretation of the
pronoun is to the answer of the previous question.
Example: Target: ?Jasper Fforde?; Question:
?What year was his first book written??; Gold
Standard: ?What year was Jasper Fforde?s first
book written??
60
3 Evaluation against the Gold Standard
To assess how close a system?s reformulation of a
question in a questions series is to the gold stan-
dard requires a measure of proximity. Whatever
metric we adopt should have the property that re-
formulations that are closer to our gold standard re-
formulations get a higher score. The closest possi-
ble score is achieved by getting an identical string
to that of the gold standard. Following conven-
tional practice we will adopt a metric that gives us
a value between 0 and 1, where 1 is highest (i.e. a
score of 1 is achieved when the pre-processed re-
formulation and the gold standard are identical).
Another requirement for the metric is that the
ordering of the words in the reformulation is not
as important as the content of the reformulation.
We assume this because one key use for reformu-
lated questions in the retrieval of candidate answer
bearing documents and the presence of key content
terms in a reformulation can help to find answers
when it is used as a query, regardless of their order
Ordering does still need to be taken into account
by the metric but it should alter the score less than
the content words in the reformulation.
Related to this point, is that we would like refor-
mulations that simply append the target onto the
end of the original question to score more highly
on average than the original questions on their
own, since this is a default strategy followed by
many systems that clearly helps in many cases.
These requirement can help to guide metric selec-
tion.
3.1 Choosing a metric
There are many different systems which attempt
to measure string similarity. We considered a va-
riety of tools like ROUGE (Lin, 2004) and ME-
TEOR (Lavie and Agarwal, 2007) but decided they
were unsuitable for this task. ROUGE and ME-
TEOR were developed to compare larger stretches
of text ? they are usually used to compare para-
graphs rather than sentences. We decided develop-
ing our own metric would be simpler than trying to
adapt one of these existing tools.
To explore candidate similarity measures we
created a program which would take as input a list
of reformulations to be assessed and a list of gold
standard reformulations and compare them to each
other using a selection of different string compar-
ison metrics. To find out which of these metrics
best scored reformulations in the way which we
expected, we created a set of test reformulations to
compare against the gold standard reformulations.
Three test data sets were created: one where
the reformulation was simply the original ques-
tion, one where the reformulation included the tar-
get appended to the end, and one where the refor-
mualation was identical to the gold standard. The
idea here was that the without target question set
should score less than the with target question set
and the identical target question set should have a
score of 1 (the highest possible score).
We then had to choose a set of metrics to test and
chose to use metrics from the SimMetrics library
as it is an open source extensible library of string
similarity and distance metrics 2.
3.2 Assessing Metrics
After running the three input files against the met-
rics we could see that certain metrics gave a score
which matched our requirements more closely than
others.
Table 1 shows the metrics used and the mean
scores across the data set for the different question
sets. A description of each of these metrics can be
found in the SimMetrics library.
From these results we can see that certain met-
rics are not appropriate. SmithWaterman, Jaro and
JaroWinkler all do the opposite to what we require
them to do in that they score a reformulation with-
out the target higher than one with the target. This
could be due to over-emphasis on word ordering.
These metrics can therefore be discounted.
Levenshtein, NeedlemanWunch and QGrams-
Distance can also be discounted as the difference
between With target and Without target is not large
enough. It would be difficult to measure improve-
ments in the system if the difference is this small.
MongeElkan can also be discounted as overall its
scores are too large and for this reason it would be
difficult to measure improvements using it.
Of the five remaining metrics ? DiceSimilar-
ity, JaccardSimilarity, BlockDistance, Euclidean-
Distance and CosineSimilarity ? we decided that
we should discount EuclideanDistance as it had the
smallest gap between with target and without tar-
get. We now look at the other four metrics in more
detail3:
2http://www.dcs.shef.ac.uk/
?
sam/
simmetrics.html
3Refer to Manning and Schu?tze (2001) for more details on
these algorithms.
61
Metric Without Target With Target Identical
JaccardSim. 0.798 0.911 1.0
DiceSim. 0.872 0.948 1.0
CosineSim. 0.878 0.949 1.0
BlockDistance 0.869 0.941 1.0
EuclideanDistance 0.902 0.950 1.0
MongeElkan 0.922 0.993 1.0
Levenshtein 0.811 0.795 1.0
NeedlemanWunch 0.830 0.839 1.0
SmithWaterman 0.915 0.859 1.0
QGramsDistance 0.856 0.908 1.0
JaroWinkler 0.855 0.831 0.993
Jaro 0.644 0.589 0.984
Table 1: Mean scores across the data set for each of the different question sets.
3.2.1 Block Distance
Block Distance metric is variously named block
distance, L1 distance or city block distance. It is a
vector-based approach, where q and r are defined
in n-dimensional vector space. The L
1
or block
distance is calculated from summing the edge dis-
tances.
L
1
(q, r) =
?
y
| q(y) ? r(y)|
This can be described in two dimensions with
discrete-valued vectors. When we can picture the
set of points within a grid, the distance value is
simply the number of edges between points that
must be traversed to get from q to r within the grid.
This is the same problem as getting from corner
a to b in a rectilinear street map, hence the name
?city-block metric?.
3.2.2 Dice Similarity
This is based on Dice coefficient which is a term
based similarity measure (0-1) whereby the simi-
larity measure is defined as twice the number of
terms common to compared entities divided by the
total number of terms in both. A coefficient result
of 1 indicates identical vectors while a 0 indicates
orthogonal vectors.
Dice Coefficient =
2 ? |S
1
? S
2
|
|S
1
| + |S
2
|
3.2.3 Jaccard Similarity
This is a token based vector space similarity
measure like the cosine distance. Jaccard Sim-
ilarity uses word sets from the comparison in-
stances to evaluate similarity. The Jaccard mea-
sure penalizes a small number of shared entries
(as a portion of all non-zero entries) more than
the Dice coefficient. Each instance is represented
as a Jaccard vector similarity function. The Jac-
card similarity between two vectors X and Y is
(X ? Y )/(|X||Y | ? (X ? Y )) where (X ? Y ) is the
inner product of X and Y , and |X| = (X ? X)1/2,
i.e. the Euclidean norm of X. This can more easily
be described as (|X ? Y |)/(|X ? Y |)
3.2.4 Cosine similarity
This is a common vector based similarity mea-
sure similar to the Dice Coefficient. The input
string is transformed into vector space so that the
Euclidean cosine rule can be used to determine
similarity. The cosine similarity is often paired
with other approaches to limit the dimensionality
of the problem. For instance with simple strings a
list of stopwords is used to reduce the dimension-
ality of the comparison. In theory this problem has
as many dimensions as terms exist.
cos(q, r) =
?
y
q(y)r(y)
?
?
y
q(y)
2
?
y
r(y)
2
3.3 Using bigrams and trigrams
All four of these measures appear to value the con-
tent of the strings higher than ordering which is
what we want our metric to do. However the scores
are quite large, and as a result we considered refin-
ing the metrics to give scores that are not as close
to 1. To do this we decided to try and increase the
importance of ordering by also taking into account
shared bigrams and trigrams. As we do not want
ordering to be too important in our metric we intro-
duced a weighting mechanism into the program to
62
Metric Without Target With Target ?Gap
Dice 0.872 0.948 +0.076
Cosine 0.878 0.949 +0.071
Jaccard 0.798 0.911 +0.113
Block 0.869 0.941 +0.072
Table 2: Results for Unigram weighting
Metric Without Target With Target ?Gap
Dice 0.783 0.814 -3.6
Cosine 0.789 0.816 -3.5
Jaccard 0.698 0.748 -5.5
Block 0.782 0.811 -3.5
Table 3: U:1, B:1, T:0
allow us to used a weighted combination of shared
unigrams, bigrams and trigrams.
The results for just unigram weighting is shown
in Table 2.
We began by testing the metrics by introduc-
ing just bigrams to give us an idea of what effect
they would have. A weight ratio of U:1, B:1, T:0
was used (where U:unigram, B:bigram, T:trigram).
The results are shown in Table 3.
The ? Gap column is the increase in the differ-
ence between Without Target and With Target from
the first test run which used only unigrams.
The introduction of bigrams decreases the gap
between Without Target and With Target. It also
lowers the scores which is good as it is then eas-
ier to distinguish between perfect reformulations
and reformulations which are close but not perfect.
This means that the introduction of bigrams is al-
ways going to decrease a system?s ability to dis-
tinguish between Without Target and With Target.
We had to now find the lowest decrease in this gap
whilst still lowering the score of the with target re-
sult.
From the results of the bigrams we expected that
the introduction of trigrams would further decrease
the gap (U : 1, B : 1, T : 1). The results proved
Metric Without Target With Target ?Gap
Dice 0.725 0.735 -6.4
Cosine 0.730 0.735 -6.3
Jaccard 0.639 0.663 -9.0
Block 0.724 0.733 -6.1
Table 4: U:1, B:1, T:1
Metric Without Target With Target ?Gap
Dice 0.754 0.770 -4.8
Cosine 0.759 0.771 -4.9
Jaccard 0.664 0.694 -7.4
Block 0.753 0.767 -4.6
Table 5: U:1, B:2
Metric Without Target With Target ?Gap
Dice 0.813 0.859 -2.4
Cosine 0.819 0.860 -2.4
Jaccard 0.731 0.802 -3.7
Block 0.811 0.854 -2.2
Table 6: U:2, B:1
this and are shown in Table 4.
The introduction of trigrams has caused the gaps
to significantly drop. It has also lowered the scores
too much. From this evidence we decided trigrams
are not appropriate to use to refine these metrics.
We now had to try and find the best weighting
of unigram to bigram that would lower the With
Target score from 1.0 whilst still keeping the gap
between Without Target and With Target high.
We would expect that further increasing the bi-
gram weighting would further decrease the gap
and the With Target score. The results in Table 5
show this to be the case. However this has de-
creased the gap too much. The next step was to
look at decreasing the weighting of the bigrams.
Table 6 shows that the gap has decreased slightly
but the With Target score has decreased by around
10% on average. The Jaccard score for this run is
particularly good as it has a good gap and is not
too close to 1.0. The Without Target is also quite
low which is what we want.
U : 2, B : 1 is currently the best weighting
found with the best metric being Jaccard. Fur-
ther work in this area could be directed at further
modifying these weightings using machine learn-
ing techniques to refine the weightings using linear
regression.
4 Our system against the Metric
Our current pre-processing system takes a question
and its target and looks to replace pronouns like
?he?, ?she? and certain definite nominals with the
target and also to replace parts of the target with
the full target (Gaizauskas et al, 2005). Given
our choice of metric we would hope that this strat-
63
Figure 2: Graph of Jaccard score distribution
egy gets a better score than just adding the target
on the end, as the ordering of the words is also
taken into account by our pre-processing as it tries
to achieve natural reformulations like those of our
gold standard. We would therefore expect that it
achieves at least the same score as adding the target
on the end, which is its default strategy when no
co-reference can be determined, though of course
incorrect coreference resolutions will have a neg-
ative effect. One of the aims of creating the gold
standard and a comparison metric was to quickly
identify whether strategies such as ours are work-
ing and if not where not.
A subset of the gold standard was preprocessed
by our system then compared against the results
of doing no reformulation and of reformulating by
simply appending the target.
Tables 7 and 8 shows how our system did in
comparison. Diff shows the difference between
WithTarget and Our System. Table 7 is results for
weighting U : 1, B : 0, T : 0, Table 8 is results for
U : 2, B : 1, T : 0.
Our system does do better than just adding the
target on the end, and this difference is exaggerated
(Table 8) when bigrams are taken into account, as
expected since this weighting increases the met-
ric?s sensitivity to recognising our system?s ability
to put the target in the correct place.
Mean scores across a data set tell part of the
story, but to gain more insight we need to exam-
ine the distribution of scores and then, in order to
improve the system, we need to look at questions
which have a low score and work out what has
gone wrong. Figure 2 shows the distribution of
Jaccard scores across the test data set. Looking at
the scores from the data set using the U:2,B:1,T:0
weighting we find that the minimum Jaccard score
was 0.44 and was for the following example:
Metric Score
Dice 0.574
Cosine 0.578
Jaccard 0.441
Block 0.574
Table 9: Finding Bad Reformulations
Target: ?Hindenburg disaster?; Question:
?How many of them were killed?; Our System:
?How many of Hindenburg disaster were killed?;
Gold Standard: ?How many people were killed
during the Hindenburg disaster?.
The results of comparing our system with the
gold standard for this question for all four metrics
are shown in Table 9.
The problem here is that our system has wrongly
replaced the term ?them? with the target when in
fact its antecedent was in the previous question
in the series How many people were on board?.
Once again the low score has helped us to quickly
identify a problem: the system is only interpret-
ing pronouns as references to the target, which is
clearly insufficient. Furthermore should the pre-
processing system be altered to address a problem
like this the gold system and scoring software can
be used for regression testing to ensure no previ-
ously correct reformulations have been lost.
Another example of a poor scoring reformula-
tion is:
Target: ?Hindenburg disaster?; Question:
?What type of craft was the Hindenburg?; Our
System: ?What type of craft was the Hindenburg
disaster?; Gold Standard: ?What type of craft was
the Hindenburg?.
For this example Jaccard gave our system refor-
mulation a score of 0.61. The problem here is our
system blindly expanded a substring of the target
appearing in the question to the full target without
recognizing that in this case the substring is not an
abbreviated reference to the target (an event) but to
an entity that figured in the event.
5 Conclusions and Future Work
In this paper we have presented a Gold Standard
for question reformulation and an associated set of
guidelines which can be used to reformulate other
questions in a similar fashion. We then evaluated
metrics which can be used to assess the effective-
ness of the reformulations and validated the whole
approach by showing how it could be used to help
64
Metric Without Target With Target Our System Diff
Dice 0.776 0.901 0.931 +3.1
Cosine 0.786 0.904 0.936 +3.1
Jaccard 0.657 0.834 0.890 +5.5
Block 0.772 0.888 0.920 +4.2
Table 7: How our system compared, U:1,B:0,T:0
Metric Without Target With Target Our System Diff
Dice 0.702 0.819 0.889 +8.7
Cosine 0.742 0.822 0.893 +9.2
Jaccard 0.616 0.738 0.839 +12.3
Block 0.732 0.812 0.884 +9.1
Table 8: How our system compared, U:2,B:1,T:0
improve the question pre-processing component of
a QA system.
Further work will aim to expand the Gold Stan-
dard to at least 1000 questions, refining the guide-
lines as required. The eventual goal is to incor-
porate the approach into an evaluation tool such
that a developer would have a convenient way
of evaluating any question reformulation strategy
against a large gold standard. Of course one also
needs to develop methods for observing and mea-
suring the effect of question reformulation within
question pre-processing upon the performance of
downstream components in the QA system, such
as document retrieval.
References
Burger, J., C. Cardie, V. Chaudhri, R. Gaizauskas,
S. Harabagiu, D. Israel, C. Jacquemin, C-Y.
Lin, S. Maiorano, G. Miller, D. Moldovan,
B. Ogden, J. Prager, E. Riloff, A. Singhal,
R. Shrihari, T. Strzalkowski, E. Voorhees, and
R. Weischedel. 2002. Issues, tasks and pro-
gram structures to roadmap research in question
& answering (q&a). Technical report. www-
nlpir.nist.gov/projects/duc/papers/qa.Roadmap-
paper v2.doc.
Gaizauskas, Robert, Mark A. Greenwood, Mark Hep-
ple, Henk Harkemaa, Horacio Saggion, and Atheesh
Sanka. 2005. The University of Sheffield?s TREC
2005 Q&A Experiments. In Proceedings of the 14th
Text REtrieval Conference.
Lavie, Alon and Abhaya Agarwal. 2007. METEOR:
An automatic metric for MT evaluation with high
levels of correlation with human judgments. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 228?231, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Lin, Chin-Yew. 2004. Rouge: A package for
automatic evaluation of summaries. In Marie-
Francine Moens, Stan Szpakowicz, editor, Text Sum-
marization Branches Out: Proceedings of the ACL-
04 Workshop, pages 74?81, Barcelona, Spain, July.
Association for Computational Linguistics.
Manning, Christopher D. and Hinrich Schu?tze. 2001.
Foundations of Statistical Natural Language Pro-
cessing. MIT Press.
Voorhees, E. 2005. Overview of the TREC 2004
question answering track. In Proceedings of the
Thirteenth Text Retrieval Conference (TREC 2004).
NIST Special Publication 500-261.
65
