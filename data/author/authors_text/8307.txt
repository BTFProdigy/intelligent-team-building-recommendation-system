Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 897?904
Manchester, August 2008
Training Conditional Random Fields Using Incomplete Annotations
Yuta Tsuboi, Hisashi Kashima
Tokyo Research Laboratory,
IBM Research, IBM Japan, Ltd
Yamato, Kanagawa 242-8502, Japan
{yutat,hkashima}@jp.ibm.com
Shinsuke Mori
Academic Center for Computing and
Media Studies, Kyoto University
Sakyo-ku, Kyoto 606-8501, Japan
forest@i.kyoto-u.ac.jp
Hiroki Oda
Shinagawa, Tokyo, Japan
oda@fw.ipsj.or.jp
Yuji Matsumoto
Graduate School of Information Science,
Nara Institute of Science and Technology
Takayama, Ikoma, Nara 630-0101, Japan
matsu@is.naist.jp
Abstract
We address corpus building situations,
where complete annotations to the whole
corpus is time consuming and unrealistic.
Thus, annotation is done only on crucial
part of sentences, or contains unresolved
label ambiguities. We propose a parame-
ter estimation method for Conditional Ran-
dom Fields (CRFs), which enables us to
use such incomplete annotations. We show
promising results of our method as applied
to two types of NLP tasks: a domain adap-
tation task of a Japanese word segmenta-
tion using partial annotations, and a part-
of-speech tagging task using ambiguous
tags in the Penn treebank corpus.
1 Introduction
Annotated linguistic corpora are essential for
building statistical NLP systems. Most of the
corpora that are well-known in NLP communi-
ties are completely-annotated in general. However
it is quite common that the available annotations
are partial or ambiguous in practical applications.
For example, in domain adaptation situations, it is
time-consuming to annotate all of the elements in a
sentence. Rather, it is efficient to annotate certain
parts of sentences which include domain-specific
expressions. In Section 2.1, as an example of such
efficient annotation, we will describe the effective-
ness of partial annotations in the domain adapta-
tion task for Japanese word segmentation (JWS).
In addition, if the annotators are domain experts
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
rather than linguists, they are unlikely to be confi-
dent about the annotation policies and may prefer
to be allowed to defer some linguistically complex
decisions. For many NLP tasks, it is sometimes
difficult to decide which label is appropriate in a
particular context. In Section 2.2, we show that
such ambiguous annotations exist even in a widely
used corpus, the Penn treebank (PTB) corpus.
This motivated us to seek to incorporate such
incomplete annotations into a state of the art ma-
chine learning technique. One of the recent ad-
vances in statistical NLP is Conditional Random
Fields (CRFs) (Lafferty et al, 2001) that evaluate
the global consistency of the complete structures
for both parameter estimation and structure infer-
ence, instead of optimizing the local configurations
independently. This feature is suited to many NLP
tasks that include correlations between elements
in the output structure, such as the interrelation of
part-of-speech (POS) tags in a sentence. However,
conventional CRF algorithms require fully anno-
tated sentences. To incorporate incomplete anno-
tations into CRFs, we extend the structured out-
put problem in Section 3. We focus on partial an-
notations or ambiguous annotations in this paper.
We also propose a parameter estimation method
for CRFs using incompletely annotated corpora in
Section 4. The proposed method marginalizes out
the unknown labels so as to optimize the likelihood
of a set of possible label structures which are con-
sistent with given incomplete annotations.
We conducted two types of experiments and ob-
served promising results in both of them. One was
a domain adaptation task for JWS to assess the
proposed method for partially annotated data. The
other was a POS tagging task using ambiguous an-
notations that are contained in the PTB corpus. We
summarize related work in Section 6, and conclude
897
      
cut
incised wound 
cut injury
abrasionor
file (or rasp)
infl. injuryinfl.infl.
pickpocket
Figure 1: An example of word boundary ambigui-
ties: infl. stands for an inflectional suffix of a verb.
in Section 7.
2 Incomplete Annotations
2.1 Partial Annotations
In this section, we describe an example of an effi-
cient annotation which assigns partial word bound-
aries for the JWS task.
It is not trivial to detect word boundaries for
non-segmented languages such as Japanese or Chi-
nese. For example, the correct segmentation of
the Japanese phrase ????????? (incised
wound or abrasion) is shown by the lowest boxes
segmented by the solid lines in Figure 1. How-
ever, there are several overlapping segmentation
candidates, which are shown by the other boxes,
and possible segmentation by the dashed lines.
Thus, the decisions on the word segmentation re-
quire considering the context, so simple dictionary
lookup approach is not appropriate. Therefore sta-
tistical methods have been successfully used for
JWS tasks. Previous work (Kudo et al, 2004)
showed CRFs outperform generative Markov mod-
els and discriminative history-based methods in
JWS. In practice, a statistical word segment an-
alyzer tends to perform worse for text from dif-
ferent domains, so that additional annotations for
each target domain are required. A major cause of
errors is the occurrence of unknown words. For ex-
ample, if ????? (abrasion) is an unknown word,
the system may accept the word sequence of ???
?????? as ????? (incised wound), ???
?? (file), and ??? (injury) by mistake.
On one hand, lists of new terms in the target
domain are often available in the forms of techni-
cal term dictionaries, product name lists, or other
sources. To utilize those domain word lists, Mori
(2006) proposed a KWIC (KeyWord In Context)
style annotation user interface (UI) with which a
user can delimit a word in a context with a single
user action. In Figure 2, an annotator marks the oc-
currences of ?????, a word in the domain word
??????? ??? ??????
? ??????? ??? ??????
? ??????? ??? ??????
Figure 2: An example of KWIC style annotation:
marked lines are identified as a correct segmenta-
tion.
list, if they are used as a real word in their con-
text. The ????? in the first row is a part of an-
other word ?????? (scratch), and the annotator
marks the last two rows as correctly segmented ex-
amples. This UI simplifies annotation operations
for segmentation to yes/no decisions, and this sim-
plification can also be effective for the reduction
of the annotation effort for other NLP tasks. For
example, the annotation operations for unlabeled
dependency parsing can be simplified into a series
of yes/no decisions as to whether or not given two
words have syntactic dependency. Compared with
sentence-wise annotation, the partial annotation is
not only effective in terms of control operations,
but also reduces annotation errors because it does
not require annotating the word boundaries that an
annotator is unsure of. This feature is crucial for
annotations made by domain experts who are not
linguists.
1
We believe partial annotation is effec-
tive in creating corpora for many other structured
annotations in the context of the domain adapta-
tions.
2.2 Ambiguous Annotations
Ambiguous annotations in this paper refer to a set
of candidate labels annotated for a part of a struc-
tured instance. For example, the following sen-
tence from the PTB corpus includes an ambiguous
annotation for the POS tag of ?pending?:
That/DT suit/NN is/VBZ pending/VBG|JJ ./. ,
where words are paired with their part-of-speech
tag by a forward slash (?/?).
2
Uncertainty concern-
ing the proper POS tag of ?pending? is represented
by the disjunctive POS tag (?VBG and JJ?) as in-
dicated by a vertical bar.
The existence of the ambiguous annotations is
due to the task definition itself, the procedure man-
1
The boundary policies of some words are different even
among linguists. In addition, the boundary agreement is even
lower in Chinese (Luo, 2003).
2
These POS tags used here are DT:determiner,
NN:common noun, VBZ:present tense 3rd person singular
verb, VBG:gerund or present participle verb, JJ:adjective,
NNS:plural noun, RBR:comparative adverb, IN:preposition
or subordinating conjunction, and RB:adverb.
898
frequency word POS tags
15 data NN|NNS
10 more JJR|RBR
7 pending JJ|VBG
4 than IN|RB
Table 1: Words in the PTB with ambiguous POSs.
ual for the annotators, or the inadequate knowl-
edge of the annotators. Ideally, the annotations
should be disambiguated by a skilled annotator for
the training data. However, even the PTB cor-
pus, whose annotation procedure is relatively well-
defined, includes more than 100 sentences contain-
ing POS ambiguities such as those listed in Ta-
ble 1. Although the number of ambiguous an-
notations is not considerably large in PTB cor-
pus, corpora could include more ambiguous anno-
tations when we try to build wider coverage cor-
pora. Also, ambiguous annotations are more com-
mon in the tasks that deal with semantics, such as
information extraction tasks so that learning algo-
rithms must deal with ambiguous annotations.
3 Problem Definition
In this section, we give a formal definition of the
supervised structured output problem that uses par-
tial annotations or ambiguous annotations in the
training phase. Note that we assume the input and
output structures are sequences for the purpose of
explanation, though the following discussion is ap-
plicable to other structures, such as trees.
Let x=(x
1
, x
2
, ? ? ? , x
T
) be a sequence of ob-
served variables x
t
? X and y=(y
1
, y
2
, ? ? ? , y
T
)
be a sequence of label variables y
t
? Y . Then the
supervised structured output problem can be de-
fined as learning a map X ? Y . In the Japanese
word segmentation task, x can represent a given
sequence of character boundaries and y is a se-
quence of the corresponding labels, which spec-
ify whether the current position is a word bound-
ary.
3
In the POS tagging task, x represents a word
sequence and y is a corresponding POS tag se-
quence. An incomplete annotation, then, is defined
as a sequence of subset of the label set instead of a
sequence of labels. Let L=(L
1
, L
2
, ? ? ? , L
T
) be a
sequence of label subsets for an observed sequence
3
Peng et al (2004) defined the word segmentation prob-
lem as labeling each character as whether or not the previous
character boundary of the current character is a word bound-
ary. However, we employ our problem formulation since it
is redundant to assign the first character of a sentence as the
word boundary in their formulation.
x, where L
t
? 2
Y
? {?}. The partial annotation
at position s is where L
s
is a singleton and the rest
L
t6=s
is Y . For example, if a sentence with 6 char-
acter boundaries (7 characters) is partially anno-
tated using the KWIC UI described in Section 2.1,
a word annotation where its boundary begins with
t = 2 and ends with t = 5 will be represented as:
L = ({?,?}, {?}, {?}, {?}, {?}
? ?? ?
partial annotation
, {?,?}),
where ? and ? denote the word boundary la-
bel and the non-word boundary label, respectively.
The ambiguous annotation is represented as a set
which contains candidate labels. The example sen-
tence including the ambiguous POS tag in Sec-
tion 2.2 can be represented as:
L = ({DT}, {NN}, {VBZ}, {VBG, JJ}
? ?? ?
ambiguous annotation
, {.}).
Note that, if all the elements of a given sequence
are annotated, it is the special case such that the
size of all elements is one, i.e. |L
t
| = 1 for all
t = 1, ? ? ? , T . The goal in this paper is training
a statistical model from partially or ambiguously
annotated data, D = {(x
(n)
, L
(n)
)}
N
n=1
.
4 Marginalized Likelihood for CRFs
In this section, we propose a parameter estimation
procedure for the CRFs (Lafferty et al, 2001) in-
corporating partial or ambiguous annotations. Let
?(x, y) : X ?Y ? <
d
denote a map from a pair
of an observed sequence x and a label sequence y
to an arbitrary feature vector of d dimensions, and
? ? <
d
denotes the vector of the model parame-
ters. CRFs model the conditional probability of a
label sequence y given an observed sequence x as:
P?(y|x) =
e
???(x,y)
Z?,x,Y
? (1)
where ? denotes the inner product of the vectors,
and the denominator is the normalization term that
guarantees the model to be a probability:
Z?,x,S =
?
y?S
e
???(x,y)
.
Then once ? has been estimated, the la-
bel sequence can be predicted by
?
y =
argmaxy?Y P?(y|x). Since the original CRF
learning algorithm requires a completely labeled
sequence y, the incompletely annotated data
(x, L) is not directly applicable to it.
899
Let YL denote all of the possible label sequence
consistent with L. We propose to use the condi-
tional probability of the subset YL given x:
P?(YL|x) =
?
y?Y
L
P?(y|x), (2)
which marginalizes the unknown ys out. Then
the maximum likelihood estimator for this model
can be obtained by maximizing the log likelihood
function:
LL(?) =
N
?
n=1
lnP?(YL(n) |x
(n)
) (3)
=
N
?
n=1
(
lnZ?,x(n),Y
L
(n)
? lnZ?,x(n),Y
)
.
This modeling naturally embraces label ambigui-
ties in the incomplete annotation.
4
Unfortunately, equation (3) is not a concave
function
5
so that there are local maxima in the
objective function. Although this non-concavity
prevents efficient global maximization of equation
(3), it still allows us to incorporate incomplete an-
notations using gradient ascent iterations (Sha and
Pereira, 2003). Gradient ascent methods require
the partial derivative of equation (3):
? LL(?)
??
=
N
?
n=1
?
?
?
y?Y
L
(n)
P?(y|YL(n) , x
(n)
)?(x
(n)
, y)
?
?
y?Y
P?(y|x
(n)
)?(x
(n)
, y)
?
?
, (4)
where
P?(y|YL, x) =
e
???(x,y)
Z?,x,Y
L
(5)
is a conditional probability that is normalized over
YL.
Equations (3) and (4) include the summations
of all of the label sequences in Y or YL. It is not
practical to enumerate and evaluate all of the label
configurations explicitly, since the number of all of
the possible label sequences is exponential on the
number of positions t with |L
t
| > 1. However, un-
der the Markov assumption, a modification of the
4
It is common to introduce a prior distribution over the pa-
rameters to avoid over-fitting in CRF learning. In the experi-
ments in Section 5, we used a Gaussian prior with the mean 0
and the variance ?
2
so that ?
||?||
2
2?
2
is added to equation (3).
5
Since its second order derivative can be positive.
domain #sentences #words
(A) conversation 11,700 145,925
(B) conversation 1,300 16,348
(C) medical manual 1,000 29,216
Table 2: Data statistics.
Types Template
Characters c
?1
, c
+1
,
Character types c
?2
c
?1
, c
?1
c
+1
, c
+1
c
+2
,
Term in dic. c
?2
c
?1
c
+1
, c
?1
c
+1
c
+2
Term in dic. starts at
c
?1
, c
+1
Term in dic. ends at
Table 3: Feature templates: Each subscript stands
for the relative distance from a character boundary.
Forward-Backward algorithm guarantees polyno-
mial time computation for the equations (3) and
(4). We explain this algorithm in Appendix A.
5 Experiments
We conducted two types of experiments, assessing
the proposed method in 1) a Japanese word seg-
mentation task using partial annotations and 2) a
POS tagging task using ambiguous annotations.
5.1 Japanese Word Segmentation Task
In this section, we show the results of domain
adaptation experiments for the JWS task to assess
the proposed method. We assume that only par-
tial annotations are available for the target domain.
In this experiment, the corpus for the source do-
main is composed of example sentences in a dic-
tionary of daily conversation (Keene et al, 1992).
The text data for the target domain is composed
of sentences in a medical reference manual (Beers,
2004) . The sentences of all of the source domain
corpora (A), (B) and a part of the target domain
text (C) were manually segmented into words (see
Table 2).
The performance measure in the experiments is
the standard F measure score, F = 2RP/(R + P )
where
R =
# of correct words
# of words in test data
? 100
P =
# of correct words
# of words in system output
? 100.
In this experiment, the performance was evaluated
using 2-fold cross-validation that averages the re-
sults over two partitions of the data (C) into the
900
91
91.5
92
92.5
93
93.5
94
94.5
95
0 100 200 300 400 500 600 700 800 900 1000
Number of word annotations
F
Proposed method
Argmax as training data
Point-wise classifier
Figure 3: Average performances varying the num-
ber of word annotations over 2 trials.
data for annotation and training (C1) versus the
data for testing (C2).
We implemented first order Markov CRFs. As
the features for the observed variables, we use the
characters and character type n-gram (n=1, 2, 3)
around the current character boundary. The
character types are categorized into Hiragana,
Katakana, Kanji, English alphabet, Arabic numer-
als, and symbols. We also used lexical features
consulting a dictionary: one is to check if any
of the above defined character n-grams appear in
a dictionary (Peng et al, 2004), and the other is
to check if there are any words in the dictionary
that start or end at the current character boundary.
We used the unidic
6
(281K distinct words) as the
general purpose dictionary, and the Japanese Stan-
dard Disease Code Master (JSDCM)
7
(23K dis-
tinct words) as the medical domain dictionary. The
templates for the features we used are summarized
in Table 3. To reduce the number of parameters,
we selected only frequent features in the source do-
main data (A) or in about 50K of the unsegmented
sentences of the target domain.
8
The total number
of distinct features was about 300K.
A CRF that was trained using only the source
domain corpus (A), CRF
S
, achieved F=96.84 in
the source domain validation data (B). However,
it showed the need for the domain adaptation that
this CRF
S
suffered severe performance degrada-
tion (F=92.3) on the target domain data. This
experiment was designed for the case in which a
user selects the occurrences of words in the word
list using the KWIC interface described in Sec-
tion 2.1. We employed JSDCM as a word list
in which 224 distinct terms appeared on average
over 2 test sets (C1). The number of word an-
6
Ver. 1.3.5; http://www.tokuteicorpus.jp/dist/
7
Ver. 2.63; http://www2.medis.or.jp/stdcd/byomei/
8
The data (B) and (C), which were used for validation and
test, were excluded from this feature selection process.
notations varied from 100 to 1000 in this exper-
iment. We prioritized the occurrences of each
word in the list using a selective sampling tech-
nique. We used label entropy (Anderson et al,
2006), H(y
s
t
) =
?
ys
t
?Y s
t
P
??
(y
s
t
|x) lnP
??
(y
s
t
|x)
, as importance metric of each word occurrence,
where
?
? is the model parameter of CRF
S
, and y
s
t
=
(y
t
, y
t+1
, ? ? ? , y
s
) ? Y
s
t
is a subsequence starting
at t and ending at s in y. Intuitively, this metric
represents the prediction confidence of CRF
S
.
9
As
training data, we mixed the complete annotations
(A) and these partial annotations on data (C1) be-
cause that performance was better than using only
the partial annotations.
We used conjugate gradient method to find the
local maximum value with the initial value being
set to be the parameter vector of CRF
S
. Since the
amount of annotated data for the target domain was
limited, the hyper-parameter ? was selected using
the corpus (B).
For the comparison with the proposed method,
the CRFs were trained using the most probable
label sequences consistent with L (denoted as
argmax). The most probable label sequences were
predicted by the CRF
S
. Also, we used a point-wise
classifier, which independently learns/classifies
each character boundary and just ignores the unan-
notated positions in the learning phase. As the
point-wise classifier, we implemented a maximum
entropy classifier which uses the same features and
optimizer as CRFs.
Figure 3 shows the performance comparisons
varying the number of word annotations. The
combination of both the proposed method and the
selective sampling method showed that a small
number of word annotations effectively improved
the word segmentation performance. In addi-
tion, the proposed method significantly outper-
formed argmax and point-wise classifier based on
the Wilcoxon signed rank test at the significance
level of 5%. This result suggests that the pro-
posed method maintains CRFs? advantage over the
point-wise classifier and properly incorporates par-
tial annotations.
5.2 Part-of-speech Tagging Task
In this section, we show the results of the POS tag-
ging experiments to assess the proposed method
using ambiguous annotations.
9
We selected word occurrences in a batch mode since each
training of the CRFs takes too much time for interactive use.
901
Ex.1 Ex.2
ambiguous sentences (training) 118
unique sentences (training) 1,480 2,960
unique sentences (test) 11,840
Table 4: Training and test data for POS tagging.
As mentioned in Section 2.2, there are words
which have two or more candidate POS tags in the
PTB corpus (Marcus et al, 1993). In this experi-
ment, we used 118 sentences in which some words
(82 distinct words) are annotated with ambiguous
POS tags, and these sentences are called the POS
ambiguous sentences. On the other hand, we call
sentences in which the POS tags of these terms are
uniquely annotated as the POS unique sentences.
The goal of this experiment is to effectively im-
prove the tagging performance using both these
POS ambiguous sentences and the POS unique
sentences as the training data. We assume that the
amount of training data is not sufficient to ignore
the POS ambiguous sentences, or that the POS am-
biguous sentences make up a substantial portion of
the total training data. Therefore we used a small
part (1/10 or 1/5) of the POS unique sentences for
training the CRFs and evaluated their performance
using other (4/5) POS unique sentences. We con-
ducted two experiments in which different num-
bers of unique sentences were used in the training
phases, and these settings are summarized in Ta-
ble 4.
The feature sets for each word are the case-
insensitive spelling, the orthographic features of
the current word, and the sentence?s last word. The
orthographic features are whether a spelling begins
with a number or an upper case letter; whether
it begins with an upper case letter and contains a
period (?.?); whether it is all upper case letters or
all lower case letters; whether it contains a punc-
tuation mark or a hyphen; and the last one, two,
and three letters of the word. Also, the sentence?s
last word corresponds to a punctuation mark (e.g.
?.?, ???, ?!?). We employed only features that ap-
peared more than once. The total number of re-
sulting distinct features was about 14K. Although
some symbols are treated as distinct tags in the
PTB tag definitions, we aggregated these symbols
into a symbol tag (SYM) since it is easy to restore
original symbol tags from the SYM tag. Then, the
number of the resulting tags was 36.
For the comparison with the proposed method
(mrg), we used three heuristic rules that disam-
biguated the annotated candidate POS tags in the
POS ambiguous sentences. These rules selected a
POS tag 1) at random, 2) as the first one in the
description order
10
, 3) as the most frequent tag
in the corpus. In addition, we evaluated the case
when the POS ambiguous sentences are 4) dis-
carded from the training data.
For evaluation, we employed the Precision
(P) and Average Precision for Ambiguous words
(APA):
P=
# of correctly tagged word
# of all word occurrences
?100?
APA=
1
|A|
?
w?A
# of the correctly tagged w
# of all occurrences of w
?100?
where A is a word set and is composed of the word
for which at least one of its occurrences is ambigu-
ously annotated. Here, we employed APA to eval-
uate each ambiguous words equally, and |A| was
82 in this experiment. Again, we used the conju-
gate gradient method to find the local maximum
value with the initial value being set to be the pa-
rameters obtained in the CRF learning for the dis-
carded setting.
Table 5 shows the average performance of POS
tagging over 5 different POS unique data. Since
the POS ambiguous sentences are only a fraction
of all of the training data, the overall performance
(P) was slightly improved by the proposed method.
However, according to the performance for am-
biguously annotated words (APA), the proposed
method outperformed other heuristics for POS dis-
ambiguation. The P and APA scores between
the proposed method and the comparable methods
are significantly different based on the Wilcoxon
signed rank test at the 5% significance level. Al-
though the performance improvement in this POS
tagging task was moderate, we believe the pro-
posed method will be more effective to the NLP
tasks whose corpus has a considerable number of
ambiguous annotations.
6 Related Work
Pereira and Schabes (1992) proposed a grammar
acquisition method for partially bracketed corpus.
Their work can be considered a generative model
for the tree structure output problem using partial
annotations. Our discriminative model can be ex-
tended to such parsing tasks.
10
Although the order in which the candidate tags appear
has not been standardized in the PTB corpus, we assume that
annotators might order the candidate tags with their confi-
dence.
902
mrg random first frequent discarded
Ex.1
P 94.39 94.27 94.26 94.27 94.19
APA 73.10 71.58 72.65 71.68 71.91
Ex.2
P 95.08 94.98 94.97 94.97 94.98
APA 76.70 74.27 75.28 74.32 75.16
Table 5: The average POS tagging performance over 5 trials.
Our model is interpreted as one of the CRFs
with hidden variables (Quattoni et al, 2004).
There are previous work which handles hidden
variables in discriminative parsers (Clark and Cur-
ran, 2006; Petrov and Klein, 2008). In their meth-
ods, the objective functions are also formulated as
same as equation (3).
For interactive annotation, Culotta et al (2006)
proposed corrective feedback that effectively re-
duces user operations utilizing partial annotations.
Although they assume that the users correct en-
tire label structures so that the CRFs are trained as
usual, our proposed method extends their system
when the users cannot annotate all of the labels in
a sentence.
7 Conclusions and Future Work
We are proposing a parameter estimation method
for CRFs incorporating partial or ambiguous an-
notations of structured data. The empirical results
suggest that the proposed method reduces the do-
main adaptation costs, and improves the prediction
performance for the linguistic phenomena that are
sometimes difficult for people to label.
The proposed method is applicable to other
structured output tasks in NLP, such as syntactic
parsing, information extraction, and so on. How-
ever, there are some NLP tasks, such as the word
alignment task (Taskar et al, 2005), in which it is
not possible to efficiently calculate the sum score
of all of the possible label configurations. Re-
cently, Verbeek and Triggs (2008) independently
proposed a parameter estimation method for CRFs
using partially labeled images. Although the ob-
jective function in their formulation is equivalent
to equation (3), they used Loopy Belief Propaga-
tion to approximate the sum score for their ap-
plication (scene segmentation). Their results im-
ply these approximation methods can be used for
such applications that cannot use dynamic pro-
gramming techniques.
Acknowledgments
We would like to thank the anonymous reviewers
for their comments. We also thank Noah Smith,
Ryu Iida, Masayuki Asahara, and the members
of the T-PRIMAL group for many helpful discus-
sions.
References
Anderson, Brigham, Sajid Siddiqi, and Andrew Moore.
2006. Sequence selection for active learning. Tech-
nical Report CMU-IR-TR-06-16, Carnegie Mellon
University.
Beers, Mark H. 2004. The Merck Manual of Medical
Information (in Japanese). Nikkei Business Publi-
cations, Inc, Home edition.
Clark, Stephen and James R. Curran. 2006. Par-
tial training for a lexicalized-grammar parser. In
Proceedings of the Annual Meeting of the North
American Association for Computational Linguis-
tics, pages 144?151.
Culotta, Aron, Trausti Kristjansson, Andrew McCal-
lum, and Paul Viola. 2006. Corrective feedback and
persistent learning for information extraction. Artifi-
cial Intelligence Journal, 170:1101?1122.
Keene, Donald, Hiroyoshi Hatori, Haruko Yamada, and
Shouko Irabu, editors. 1992. Japanese-English Sen-
tence Equivalents (in Japanese). Asahi Press, Elec-
tronic book edition.
Kudo, Taku, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to
Japanese morphological analysis. In Proceedings of
Empirical Methods in Natural Language Processing.
Lafferty, John, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the 18th International Con-
ference on Machine Learning.
Luo, Xiaoquan. 2003. A maximum entropy chinese
character-based parser. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 192?199.
Marcus, Mitchell P., Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of English: The Penn treebank. Computa-
tional Linguistics, 19(2).
Mori, Shinsuke. 2006. Language model adaptation
with a word list and a raw corpus. In Proceedings
of the 9th International Conference on Spoken Lan-
guage Processing.
903
Peng, Fuchun, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detection
using conditional random fields. In Proceedings of
the International Conference on Computational Lin-
guistics.
Pereira, Fernando C. N. and Yves Schabes. 1992.
Inside-outside reestimation from partially bracketed
corpora. In Proceedings of Annual Meeting Associ-
ation of Computational Linguistics, pages 128?135.
Petrov, Slav and Dan Klein. 2008. Discriminative
log-linear grammars with latent variables. In Ad-
vances in Neural Information Processing Systems,
pages 1153?1160, Cambridge, MA. MIT Press.
Quattoni, Ariadna, Michael Collins, and Trevor Darrell.
2004. Conditional random fields for object recogni-
tion. In Advances in Neural Information Processing
Systems.
Sha, Fei and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proceedings
of Human Language Technology-NAACL, Edmon-
ton, Canada.
Taskar, Ben, Simon Lacoste-Julien, and Dan Klein.
2005. A discriminative matching approach to word
alignment. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Verbeek, Jakob and Bill Triggs. 2008. Scene segmen-
tation with CRFs learned from partially labeled im-
ages. In Advances in Neural Information Processing
Systems, pages 1553?1560, Cambridge, MA. MIT
Press.
Appendix A Computation of Objective
and Derivative functions
Here we explain the effective computation proce-
dure for equation (3) and (4) using dynamic pro-
gramming techniques.
Under the first-order Markov assumption
11
, two
types of features are usually used: one is pairs of
an observed variable and a label variable (denoted
as f(x
t
, y
t
) : X ? Y ), the other is pairs of two
label variables (denoted as g(y
t?1
, y
t
) : Y ? Y )
at time t. Then the feature vector can be de-
composed as ?(x, y) =
?
T+1
t=1
?(x
t
, y
t?1
, y
t
)
where ?(x
t
, y
t?1
, y
t
) = f(x
t
, y
t
) + g(y
t?1
, y
t
).
In addition, let S and E be special label vari-
ables to encode the beginning and ending of a se-
quence, respectively. We define ?(x
t
, y
t?1
, y
t
) to
be ?(x
t
, S, y
t
) at the head t = 1 and g(y
t?1
, E) at
the tail where t = T + 1. The technique of the ef-
fective calculation of the normalization value is the
11
Note that, although the rest of the explanation based on
the first-order Markov models for purposes of illustration, the
following arguments are easily extended to the higher order
Markov CRFs and semi-Markov CRFs.
precomputation of the ??,x,L[t, j], and??,x,L[t, j]
matrices with given ?,x, and L. The matrices ?
and ? are defined as follows, and should be cal-
culated in the order of t = 1, ? ? ? , T , and t =
T + 1, ? ? ? , 1, respectively
??,x,L[t, j]
=
?
?
?
?
?
?
?
?
?
0 if j /? L
t
? ? ?(x
t
, S, j) else if t = 1
ln
?
i?L
t?1
e
?[t?1,i]+??ffi(x
t
,i,j)
else
??,x,L[t, j]
=
?
?
?
?
?
?
?
?
?
0 if j /? L
t
? ? g(j, E) else if t = T + 1
ln
?
k?L
t+1
e
??ffi(x
t
,j,k)+?[t+1,k]
else
Note that L = (Y, ? ? ? , Y ) is used to calculate all
the entries in Y . In the rest of this section, we omit
the subscripts ?, x, and L of ?, ?, Z unless mis-
understandings could occur. The time complexity
of the ?[t, j] or ?[t, j] computation is O(T |Y |
2
).
Finally, equations (3) and (4) are efficiently cal-
culated using ?, ?. The logarithm of Z in equation
(3) is calculated as:
lnZ?,Y
L
= ln
?
j?L
T
e
?
?,L
[T,j]+??g(j,E)
.
Similarly, the first and second terms of equation
(4) can be computed as:
?
y?Y
L
P?,L(y|x)?(x, y) =
?
i?L
T
?L(T, i, E)g(i, E)
+
T
?
t=1
?
j?L
t
?
?
?L(t, j)f(xt, j) +
?
i?L
t?1
?L(t, i, j)g(i, j)
?
?
where ?, x are omitted in this equation, and ??,x,L
and ??,x,L are the marginal probabilities:
??,x,L(t, j) = P?,L(yt = j|x)
= e
?[t,j]+?[t,j]?ln Z
Y
L
, and
??,x,L(t, i, j) = P?,L(yt?1 = i, yt = j|x)
= e
?[t?1,i]+??ffi(x
t
,i,j)+?[t,j]?ln Z
Y
L
.
Note that YL is replaced with Y and L =
(Y, ? ? ? , Y ) to compute the second term.
904
Learning Sequence-to-Sequence Correspondences from Parallel Corpora
via Sequential Pattern Mining
Kaoru Yamamoto? and Taku Kudo? and Yuta Tsuboi? and Yuji Matsumoto?
?Genomic Sciences Center, The Institute of Physical and Chemical Research
1-7-22-E209, Suehiro-cho, Tsurumi-ku, Yokohama, 230-0045 Japan
kaorux@gsc.riken.go.jp
?Graduate School of Information Science, Nara Institute of Science and Technology
8916-5 Takayama, Ikoma, Nara, 630-0192 Japan
taku-ku@is.aist-nara.ac.jp, matsu@is.aist-nara.ac.jp
?Tokyo Research Laboratory, IBM Japan, Ltd.
1623-14 Shimotsuruma, Yamato-shi, Kanagawa-ken, 242-8502 Japan
yutat@jp.ibm.com
Abstract
We present an unsupervised extraction of
sequence-to-sequence correspondences from
parallel corpora by sequential pattern mining.
The main characteristics of our method are
two-fold. First, we propose a systematic way
to enumerate all possible translation pair can-
didates of rigid and gapped sequences without
falling into combinatorial explosion. Second,
our method uses an efficient data structure and
algorithm for calculating frequencies in a con-
tingency table for each translation pair candi-
date. Our method is empirically evaluated us-
ing English-Japanese parallel corpora of 6 mil-
lion words. Results indicate that it works well
for multi-word translations, giving 56-84% ac-
curacy at 19% token coverage and 11% type
coverage.
1 Introduction
This paper addresses the problem of identifying ?multi-
word? (sequence-to-sequence) translation correspon-
dences from parallel corpora. It is well-known that trans-
lation does not always proceed by word-for-word. This
highlights the need for finding multi-word translation cor-
respondences.
Previous works that focus on multi-word transla-
tion correspondences from parallel corpora include noun
phrase correspondences (Kupiec, 1993), fixed/flexible
collocations (Smadja et al, 1996), n-gram word se-
quences of arbitrary length (Kitamura and Matsumoto,
1996), non-compositional compounds (Melamed, 2001),
captoids (Moore, 2001), and named entities 1.
In all of these approaches, a common problem seems to
be an identification of meaningful multi-word translation
units. There are a number of factors which make han-
dling of multi-word units more complicated than it ap-
pears. First, it is a many-to-many mapping which poten-
tially leads to a combinatorial explosion. Second, multi-
word translation units are not necessarily contiguous, so
an algorithm should not be hampered by the word adja-
cency constraint. Third, word segmentation itself is am-
biguous for non-segmented languages such as Chinese or
Japanese. We need to resolve such ambiguity as well.
In this paper, we apply sequential pattern mining to
solve the problem. First, the method effectively avoids an
inherent combinatorial explosion by concatenating pairs
of parallel sentences into single bilingual sequences and
applying a pattern mining algorithm on those sequences.
Second, it covers both rigid (gap-less) and gapped se-
quences. Third, it achieves a systematic way of enumer-
ating all possible translation pair candidates, single- or
multi-word. Note that some are overlapped to account
for word segmentation ambiguity. Our method is bal-
anced by a conservative discovery of translation corre-
spondences with the rationale that direct associations will
win over indirect ones, thereby resolving the ambiguity.
2 Our Basic Idea
Our approach is illustrated in Figure 1. We concatenate
corresponding parallel sentences into bilingual sequences
to which sequential pattern mining is applied. By doing
so, we obtain the following effects:
? It exhaustively generates all possible translation can-
1As of this writing, we learn that Moore will present his
results on named entity at EACL 2003.
Parallel Corpus
Japanese 
Preprocessing
English
Preprocessing
Sequential Pattern Mining
Bilingual Sequence Database
Monolingual Patterns with Independet Frequency
Bilingual Patterns with Co-occurrence Frequency
Sequence-to-Sequence Correspondence Discovery
Bilingual Expressions
Concatenate
Figure 1: Our Approach
didates, both rigid and gapped sequences, yet avoid-
ing combinatorial explosion.
? It achieves an efficient calculation of a contingency
table in a single running of sequential pattern min-
ing.
In what follows, we describe sequential pattern mining
and each module in Figure 1.
2.1 Sequential Pattern Mining
Sequential pattern mining discovers frequent subse-
quences as patterns in a sequence database (Agrawal
and Srikant, 1995). Here, a subsequence is an order-
preserving item sequence where some gaps between
items are allowed. In this paper, we write the support of
subsequence s in sequence database S as supportS(s),
meaning the occurrence frequency of s in S. The prob-
lem is defined as follows:
Given a set of sequences S, where each sequence con-
sists of items, and a given a user-specified minimum sup-
port ?, sequential pattern mining is to find all of the
subsequences whose occurrence frequency in the set S is
no less than ?.
A sequential pattern is different from N-gram pattern
in that the former includes a pattern with and without
gaps and does not impose any limit on its length. These
characteristics in sequential pattern mining leads us to the
idea of concatenating corresponding parallel sentences
into a bilingual sequence database from which bilingual
sequential patterns are mined efficiently.
minsup = 2
A00 C01 D02
A10 B11 C12
C20 B21 A22
A30 A31 B32
A00 C01 D02
A10 B11 C12
A22
A30 A31 B32
B11 C12
B21 A22
B32
C01 D02
C12
C20 B21 A22
A
B
C
A10 B11 C12
A30 B32
B
C
A00 C01 D02
A10 C12
A    4
AB 2
AC 2
B     3
C     3
sequence db pattern projected db pattern projected db
pattern projected db
pattern projected db
pattern projected db
sup= 4
sup= 3
sup= 3
sup= 2
sup= 2
results
D
Figure 2: A Sample Execution of PrefixSpan
2.2 Bilingual Lexicon Extraction
2.2.1 Bilingual Sequence Database
For each parallel sentence, we undergo language-
dependent preprocessing, such as word segmentation and
part-of-speech tagging. Then we concatenate the mono-
lingual sequences into a single bilingual sequence, and
a collection of bilingual sequences becomes a sequence
database S.
2.2.2 Sequential Pattern Mining
A single run of sequential pattern mining takes care of
identifying and counting translation candidate patterns ?
rigid and gapped, some of which are overlapped ? in the
bilingual sequence database. All English subsequences
satisfying the minimum support ? will be generated (e.g.,
?e1?, ?e1e2?, ?e1e3? ? ? ?, indicated by Ei ). Similarly, all
Japanese and bilingual subsequences with support ? ?
will be generated (indicated by Jj and EiJj respectively).
It is important to point out that for any bilingual pattern
EiJj , corresponding English pattern Ei and Japanese
pattern Jj that form constituents of the bilingual pattern
are always recognized and counted.
PrefixSpan
In order to realize sequential pattern mining, we use
PrefixSpan algorithm (Pei et al, 2001). The general idea
is to divide the sequence database by frequent prefix and
to grow the prefix-spanning patterns in depth-first search
fashion.
We introduce some concepts. Let ? be a sequential
pattern in the sequence database S. Then, we refer to the
?-projected database, S|?, as the collection of postfixes
of sequences in S w.r.t prefix ?.
A running example of PrefixSpan with the minimum
support ? = 2 (i.e., mining of sequential patterns with
frequency ? 2) is shown in Figure 2. Each item in a se-
function PrefixSpan (?, S|?)
begin
B ? {b|(s ? S|?, b ? s)
? (supportS |?(?b?) ? ?)
? (projectable(?, b))}
foreach b ? B
begin
(S|?)|b ? {?i, s??|(?i, s? ? S|?)
? (s? = postfix(s, b))}
call PrefixSpan (?b, (S|?)|b)
end
end
Figure 3: Pseudo Code of PrefixSpan
quence database is indicated by eij where e is an item,
i is a sequence id, j is the offset for the postfix of se-
quence id i. First, frequent sequential patterns with length
1 are selected. This gives A,B and C. The support of
D is less than the minimum support 2, so D-projected
database will not be created. For projections drawn with
bold lines in Figure 2, we proceed with a frequent prefix
A. Since A satisfies the minimum support 2, it creates a
A-projected database derived from the sequence database
S (S|A). From S|A, frequent items B,C are identified,
subsequently forming prefix patterns AB and AC, and
corresponding projected databases of the postfixes S|AB ,
S|AC . We continue with projection recursively to mine
all sequential patterns satisfying the minimum support
count 2.
PrefixSpan is described in Figure 3. The predicate pro-
jectable is designed to encode if a projection is feasible
in an application domain. The original PrefixSpan gives
a predicate that always returns true.
There are a number of possibilities for projectable to
reflect linguistic constraints. A default projectable pred-
icate covers both rigid and gapped sequences satisfying
the minimum support. If we care for word adjacency, the
projectable should return true only when the last item of
the mined pattern and the first item of a postfix sequence
in the projected database are contiguous. Another possi-
bility is to prevent a certain class of words from being an
item of a sequence. For example, we may wish to find
a sequence consisting only of content words. In such a
case, we should disallow projections involving functional
word item.
2.2.3 Sequence-to-Sequence Correspondence
The effect of sequential pattern mining from bilingual
sequence database can better be seen in a contingency
table shown in Table 1. Frequencies of a bilingual pattern
EiJj , an English pattern Ei, and a Japanese pattern Jj
correspond to a, a + b, and a + c respectively. Since
we know the total number of bilingual sequences N =
a + b + c + d, values of b, c and d can be calculated
immediately.
Table 1: Contingency Table
Jj ? Jj
Ei a b a + b
? Ei c d
a + c N
The contingency table is used for calculating a sim-
ilarity (or association) score between Ei and Jj . For
this present work, we use Dunning?s log-likelihood ratio
statistics (Dunning, 1993) defined as follows:
sim = a log a+ b log b+ c log c+ d log d
?(a+ b) log (a+ b)? (a+ c) log (a+ c)
?(b+ d) log (b+ d)? (c+ d) log (c+ d)
+(a+ b+ c+ d) log (a+ b+ c+ d)
For each bilingual pattern EiJj , we compute its similarity
score and qualify it as a bilingual sequence-to-sequence
correspondence if no equally strong or stronger associ-
ation for monolingual constituent is found. This step is
conservative and the same as step 5 in Moore (2001) or
step 6(b) in Kitamura and Matsumoto (1996). Our im-
plementation uses a digital trie structure called Double
Array for efficient storage and retrieval of sequential pat-
terns (Aoe, 1989).
For non-segmented language, a word unit depends on
results of morphological analysis. In case of Japanese
morphological analysis, ChaSen (Matsumoto et al, 2000)
tends to over-segment words, while JUMAN (Kurohashi
et al, 1994) tends to under-segment words. It is diffi-
cult to define units of correspondences only consulting
the Japanese half of parallel corpora. A parallel sentence-
pair may resolve some Japanese word segmentation am-
biguity, however, we have no way to rank for word units
with the same degree of segmentation ambiguity. In-
stead, we assume that frequently co-occurred sequence-
to-sequence pairs in the entire parallel corpora are trans-
lation pairs. Using the global frequency of monolingual
and bilingual sequences in the entire parallel corpora, we
have better chance to rank for the ties, thereby resolv-
ing ambiguity in the monolingual half. To follow this
intuition, we generate overlapped translation candidates
where ambiguity exists, and extract ones with high asso-
ciation scores.
Sequential pattern mining takes care of translation can-
didate generation as well as efficient counting of the gen-
erated candidates. This characteristic is well-suited for
our purpose in generating overlapped translation candi-
dates of which frequencies are efficiently counted.
3 Experimental Results
3.1 Data
We use the English-Japanese parallel corpora that are
automatically aligned from comparable corpora of the
news wires (Utiyama and Isahara, 2002). There are
150,000 parallel sentences which satisfy their proposed
sentence similarity. We use TnT (Brants, 2000) for En-
glish POS tagging and ChaSen (Matsumoto et al, 2000)
for Japanese morphological analysis, and label each to-
ken to either content or functional depending on its part-
of-speech.
Table 2: Statistics of 150,000 parallel sentences
Japanese English
content (token) 2,039,656 2,257,806
content (type) 47,316 57,666
functional (token) 2,660,855 1,704,189
functional (type) 1,811 386
3.2 Evaluation Criteria
We evaluate our sequence-to-sequence correspondence
by accuracy and coverage, which we believe, similar cri-
teria to (Moore, 2001) and (Melamed, 2001) 2. Let Cseq
be the set of correct bilingual sequences by a human
judge, Sseq be the set of bilingual sequences identified
by our system, Ctoken be the multiset of items covered
by Cseq , Ttoken be the multiset of items in the bilingual
sequence database, Ctype be the set of items covered by
Cseq , and Ttype be the set of items in the bilingual se-
quence database. Then, our evaluation metrics are given
by:
accuracy = |Cseq||Sseq|
token coverage = |Ctoken||Ttoken|
2We would like to examine how many distinct translation
pairs are correctly identified (accuracy) and how well the iden-
tified subsequences can be used for partial sequence alignment
in the original parallel corpora (coverage). Since all the correct
translation pairs in our parallel corpora are not annotated, the
sum of true positives and false negatives remain unknown. For
this reason, we avoid to use evaluation terms precision and re-
call to emphasize the difference. There are many variations of
evaluation criteria used in the literature. At first, we try to use
Moore?s criteria to present a direct comparison. Unfortunately,
we are unclear about frequency for multi-words in the parallel
corpora, which seems to require for the denominator of his cov-
erage formula. Further, we also did not split train/test corpus
for cross-validation. Our method is an unsupervised learning,
and the learning does not involve tuning parameters of a prob-
abilistic model for unseen events. So we believe results using
entire parallel corpora give indicative material for evaluation.
type coverage = |Ctype||Ttype|
In order to calculate accuracy, each translation pair is
compared against the EDR (Dictionary, 1995). All the
entries appeared in the dictionary were assumed to be
correct. The remaining list was checked by hand. A
human judge was asked to decide ?correct?, ?nearmiss?,
or ?incorrect? for each proposed translation pair with-
out any reference to the surrounding context. Distinc-
tion between ?nearmiss? and ?incorrect? is that the for-
mer includes translation pairs that are partially correct3.
In Tables 3, 4, and 5, accuracy is given as a range from
a combination of ?correct? and ?nearmiss? to a combi-
nation of ?nearmiss? and ?incorrect?. Having calculated
the total accuracy, accuracies for single-word translation
pairs only and for multi-word translation pairs only are
calculated accordingly.
3.3 Results
Our method is implemented in C++, and executed on
a 2.20 GHz Penntium IV processor with 2GB mem-
ory. For each experiment, we set the minimum support
(minsup) and the maximum length (maxpat) of pat-
terns. All experiments target bilingual sequences of con-
tent words only, since we feel that functional word cor-
respondences are better dealt with by consulting the sur-
rounding contexts in the parallel corpora4. An execution
of bilingual sequence databases compiled from 150,000
sentences, takes less than 5 mins with minsup = 3 and
maxpat = 3, inferring 14312 translation pairs.
Given different language pair, different genre of text,
different evaluation criteria, we find it difficult to di-
rectly compare our result with previous high-accuracy ap-
proaches such as (Moore, 2001). Below, we give an ap-
proximate comparison of our empirical results.
3.3.1 Rigid Sequences
Table 3 shows a detailed result of rigid sequences with
minsup = 3, maxpat = 3. In total, we obtain 14312
translation pairs, out of which we have 6567 single-word
3We include ?not sure? ones for a single-word translation.
Those are entries which are correct in some context, but debat-
able to include in a dictionary by itself. As for multi-word trans-
lation, we include pairs that can become ?correct? in at most 2
rewriting steps.
4Inclusion of functional word items in bilingual sequences
is debatable. We have conducted an preliminary experiment of
approx 10,000 sentences taken from a English?Japanese dic-
tionary. As sentences are shorter and more instructive, we get
grammatical collocations such as ?impressed with / ni kanmei
? and ?apologize for / koto owabi? or phrasal expressions such
as ?for your information / go sanko? and ?on behalf of / wo
daihyo shi?. However, we felt that it was not practical to in-
clude functional words in this work, since the parallel corpora
is large-scale and interesting translation pairs in newspaper are
named entities comprised of mostly content words.
Table 3: Result of Rigid Sequence Only with minsup = 3, maxpat = 3. Accuracy is given as a range from a combination
of ?correct? and ?nearmiss? to a combination of ?nearmiss? and ?incorrect?. The left side of slash gives a tigher
evaluation and the right side of slash gives a looser evaluation.
minsup maxpat extracted correct total single-word multi-word token type
sequence sequence accuracy accuracy accuracy coverage coverage
3 3 1000 927 / 988 0.927 / 0.988 0.942 / 0.988 0.824 / 0.984 0.142 0.018
3 3 2000 1836 / 1969 0.918 / 0.986 0.953 / 0.992 0.742 / 0.945 0.164 0.035
3 3 3000 2723 / 2932 0.908 / 0.977 0.951 / 0.991 0.732 / 0.923 0.174 0.050
3 3 4000 3563 / 3882 0.891 / 0.971 0.951 / 0.990 0.695 / 0.909 0.179 0.064
3 3 5000 4330 / 4825 0.866 / 0.965 0.948 / 0.989 0.656 / 0.903 0.182 0.076
3 3 6000 5052 / 5752 0.842 / 0.959 0.945 / 0.990 0.618 / 0.891 0.184 0.087
3 3 7000 5776 / 6656 0.825 / 0.951 0.941 / 0.989 0.607 / 0.879 0.186 0.098
3 3 8000 6350 / 7463 0.794 / 0.933 0.938 / 0.987 0.568 / 0.848 0.187 0.104
3 3 9000 7034 / 8345 0.782 / 0.927 0.935 / 0.985 0.562 / 0.844 0.188 0.113
Table 4: Result of Rigid Sequences Only with minsup = 10 and minsup = 5.
minsup maxpat extracted correct total single-word multi-word token type
sequence sequence accuracy accuracy accuracy coverage coverage
10 3 4467 3989 / 4341 0.893 / 0.972 0.946 / 0.988 0.712 / 0.918 0.085 0.011
5 3 7654 6325 / 7271 0.826 / 0.950 0.937 / 0.986 0.618 / 0.882 0.188 0.106
10 10 4518 4002 / 4392 0.886 / 0.972 0.947 / 0.988 0.690 / 0.921 0.183 0.073
5 10 8007 6383 / 7387 0.797 / 0.922 0.938 / 0.986 0.563 / 0.817 0.188 0.106
Table 5: Result of Rigid and Gapped Sequences with minsup = 10. A default projectable constraint in Figure 3 is used.
minsup maxpat extracted correct total single-word multi-word token type
sequence sequence accuracy accuracy accuracy coverage coverage
10 3 5792 4503 / 4979 0.777 / 0.860 0.950 / 0.989 0.530 / 0.674 0.085 0.012
Table 6: Comparison between Table 4 and Table 5 with minsup = 10, maxpat = 3
single-word single-word single-word multi-word multi-word multi-word
correct wrong all correct wrong all
Both 3239 167 3406 554 181 735
Rigid only 25 18 43 171 112 283
Gapped only 0 2 2 710 937 1649
Table 7: Length Distribution of 171 correct Rigid multi-word Sequences Only (left) vs. Length Distribution of 112
wrong Rigid multi-word Sequences Only (right)
HHHHE
J 1 2 3
1 n/a 16 0
2 15 110 6
3 5 7 12
HHHHE
J 1 2 3
1 n/a 11 0
2 19 29 19
3 3 24 7
Table 8: Length Distribution of 710 correct Rigid and Gapped multi-word Sequences (left) vs. Length Distribution of
937 wrong Rigid and Gapped multi-word Sequences (right)
HHHHE
J 1 2 3
1 n/a 17 0
2 45 546 15
3 9 43 35
HHHHE
J 1 2 3
1 n/a 30 2
2 36 229 239
3 15 162 226
translation pairs and 7745 multi-word translation pairs.
In this paper, we evaluate only the top 9000 pairs sorted
by the similarity score.
For single-word translation, we get 93-99% accuracy
at 19% token coverage and 11% type coverage. This im-
plies that about 1/5 of content word tokens in the paral-
lel corpora can find their correspondence with high ac-
curacy. We cannot compare our word alignment result
to (Moore, 2001), since the real rate of tokens that can
be aligned by single-word translation pairs is not explic-
itly mentioned. Although our main focus is sequence-to-
sequence correspondences, the critical question remains
as to what level of accuracy can be obtained when ex-
tending coverage rate, for example to 36%, 46% and
90%. Our result appears much inferior to Moore (2001)
and Melamed (2001) in this respect and may not reach
36% type coverage. A possible explanation for the poor
performance is that our algorithm has no mechanism to
check mutually exclusive constraints between translation
candidates derived from the same paired parallel sen-
tence.
For general multi-word translation, our method seems
more comparable to Moore (2001). Our method performs
56-84% accuracy at 11% type coverage. It seems bet-
ter than ?compound accuracy? which is his proposal of
hypothesizing multi-word occurrences, being 45-54% at
12% type coverage. However it is less favorable to ?mul-
tiword accuracy? provided by Microsoft parsers, being
73-76% accuracy at 12% type coverage (Moore, 2001).
The better performance could be attributed to our redun-
dant generation of overlapped translation candidates in
order to account for ambiguity. Although redundancy
introduces noisier indirect associations than one-to-one
mapping, our empirical result suggests that there is still a
good chance of direct associations being selected.
Table 4 shows results of rigid sequences with a higher
minimum support and a longer maximum length. Com-
paring with Table 3, setting a higher minimum support
produces a slightly more cost-effective results. For ex-
ample, minsup = 10,maxpat = 3, there are 4467 pairs
extracted with 89.3-97.1% accuracy, while the top 4000
pairs in minsup = 3,maxpat = 3 are extracted with
89.1-97.1% accuracy. Table 4 reveals a drop in multi-
word accuracy when extending minpat, indicating that
care should be given to the length of a pattern as well as
a cutoff threshold.
Our analysis suggests that an iterative method by con-
trolling minsup and maxpat appropriately seems bet-
ter than a single execution cycle of finding correspon-
dences. It can take mutually exclusive constraints into
account more easily which will improve the overall per-
formance. Another interesting extension is to incorporate
more linguistically motivated constraints in generation of
sequences. Yamamoto et al (2001) reports that N-gram
translation candidates that do not go beyond the chunk
boundary boosts performance. Had we performed a lan-
guage dependent chunking in preparation of bilingual se-
quences, such a chunk boundary constraint could be sim-
ply represented in the projectable predicate. The issues
are left for future research.
3.3.2 Gapped Sequences
One of advantages in our method is a uniform genera-
tion of both rigid and gapped sequences simultaneously.
Gapped sequences are generated and extracted without
recording offset and without distinguisting compositional
compounds from non-compositional compounds. Al-
though non-compositional compounds are rare and more
difficult to extract, compositional compounds are still
useful as collocational entires in bilingual dictionary.
There are positive and negarive effects in our gapped
sequences using sequential pattern mining. Suppose we
have English sequences of ?My best friend wishes your
father to visit ? ? ?? and ?? ? ? best wishes for success?.
Then, we obtain a pattern ?best wishes? that should be
counted separately. However, if we have sequences of
?staying at Hilton hotel? and ?staying at Kyoto Miyako
hotel?, then we will obtain a kind of a phrasal template
?staying at hotel? where the individual name of hotel,
Hilton or Kyoto Miyako, is abstracted. Usefulness of
such gapped sequences is still open, but we emperically
evaluate the result of gapped sequences with minsup =
10 and maxpat = 3 shown in Table 5.
Comparing Table 4 and 5, we lose the multi-word ac-
curacy substantially. Table 6 is a breakdown of rigid and
gapped sequences with minsup = 10, maxpat = 3.
The ?Both? row lists the number of pairs found, under a
category described in the column head, in both rigid and
gapped sequences. The ?Rigid only? row counts for those
only found in rigid sequences, while the ?Gapped only?
row counts for those only found in gapped sequence. We
learn that the decrease in multi-word accuracy is due to
an increase in the portion of wrong pairs in sequences;
57% (937 / 1649) in gapped sequences whilst 40% (112 /
283) in rigid sequences.
However, gapped sequences have contributed to an
increase in the absolute number of correct multi-word
translation pairs (+539 correct pairs). In order to gain a
better insight, we summarizes the length combination be-
tween English pattern and Japanese pattern as reported
in Tables 7 and 8. It reveals that the word adjacency
constraint in rigid sequences are too stringent. By relax-
ing the constraint, 436 (546 - 110) correct 2-2 translation
pairs are encountered, though 200 (229 - 29) wrong 2-2
pairs are introduced at the same time. At this particular
instance of minsup = 10 and maxpat = 3, consider-
ing gapped sequence of length 3 seems to introduce more
noise.
Admittedly, we still require further analysis as to
searching a break-even point of rigid/gapped sequences.
Our preliminary finding supports the work on collocation
by Smadja et al (1996) in that gapped sequences are also
an important class of multi-word translations.
4 Related Work
Moore (2001) presents insightful work which is closest
to ours. His method first computes an initial association
score, hypothesizes an occurrence of compounds, fuses
it to a single token, recomputes association scores as if
all translations are one-to-one mapping, and returns the
highest association pairs. As for captoids, he also com-
putes association of an inferred compound and its con-
stituent words. He also uses language-specific features
(e.g. capital letters, punctuation symbols) to identify
likely compound candidates.
Our method is quite different in dealing with com-
pounds. First, we outsource a step of hypothesizing com-
pounds to language-dependent preprocessors. The reason
is that an algorithm will become complicated if language-
specific features are directly embedded. Instead, we pro-
vide an abstract interface, namely the projectable predi-
cate in sequential pattern mining, to deal with language-
specific constraints. Second, we allow items being re-
dundantly counted and translation pair candidates being
overlapped. This sharply contrasts with Moore?s method
of replacing an identified compound to a single token for
each sentence pair. In his method, word segmentation
ambiguity must be resolved before hypothesizing com-
pounds. Our method reserves a possibility for word seg-
mentation ambiguity and resolves only when frequently
co-occured sequence-to-sequence pairs are identified.
Since we compute association scores independently, it
is difficult to impose mutually exclusive constraints be-
tween translation candidates derived from a paired par-
allel sentence. Hence, our method tends to suffer from
indirect association when the association score is low, as
pointed out by Melamed (2001). Although our method
relies on an empirical observation that ?direct associa-
tions are usually stronger than indirect association?, it
seems effective enough for multi-word translation. bal-
anced by a
As far as we know, our method is the first attempt
to make an exhaustive enumeration of rigid and gapped
translation candidates of both languages possible, yet
avoiding combinatorial explosion. Previous approaches
effectively narrow down its search space by some heuris-
tics. Kupiec (1993) focuses on noun-phrase translations
only, Smadja et al (1996) limits to find French transla-
tion of English collocation identified by his Xtract sys-
tem, and Kitamura and Matsumoto (1996) can exhaus-
tively enumerate only rigid word sequences.
Many of works mentioned in the last paragraph as
well as ours extract non-probabilistic translation lexicons.
However, there are research works which go beyond
word-level translations in statistical machine translation.
One notable work is that of Marcu and Wong (2002),
which is based on a joint probability model for statistical
machine translation where word equivalents and phrase
(rigid sequence) equivalents are automatically learned
form bilingual corpora.
Our method does not iterate an extraction process as
shown in Figure 1. This could be a cause of poor perfor-
mance in single-word translation pairs, since there is no
mechanism for imposing mutually exclusion constrains.
An interesting question then is what kind of iteration
should be performed to improve performance. Prob-
abilistic translation lexicon acquisition often uses EM
training on Viterbi alignments, e.g. (Marcu and Wong,
2002), while non-probabilistic ones employ a greedy al-
gorithm that extracts translation pairs that give higher as-
sociation scores than a predefined threshold where the
threshold is monotonically decreasing as the algorithm
proceeds, e.g. (Kitamura and Matsumoto, 1996). The
issue is left for future work.
Last but not least, no previous works give an explicit
mention to an efficient calculation of each cell in a con-
tingency table. Our approach completes the process by
a single run of sequential pattern mining. Since speed
does not affect results of accuracy and coverage, its sig-
nificance is often ignored. However, it will be important
when we handle with corpora of large size.
5 Conclusions
We have proposed an effective method to find sequence-
to-sequence correspondences from parallel corpora by se-
quential pattern mining. As far as multi-word translation
is concerned, our method seems to work well, giving 56-
84% accuracy at 19% token coverage and 11% type cov-
erage.
In this work, we choose English-Japanese pair and em-
pirically evaluate our method. However, we believe the
method is applicable to any language pair with appropri-
ate language-specific preprocessing tools. As by-product
of our experiment, we obtain Japanese-English parallel
corpora of 150,000 sentences where alignment of vali-
dated subsequence correspondences are back-annotated.
This was accomplished by looking up to a Double Array
dictionary of sequential patterns constructed in the ex-
traction method. This shows that our method can be use-
ful not only to development of semi-automatic lexicon for
data-driven machine translation, but also to annotation of
corresponding subsequences in translation memory sys-
tem.
Acknowledgement
We would like to thank Masao Utiyama of CRL for
creating a large-scale English-Japanese parallel corpora,
Thorsten Brants and ChaSen development team for mak-
ing NLP tools publicly available. In addition, we would
like to thank anonymous reviewers for useful comments
that have helped preparation of this paper.
References
R. Agrawal and R. Srikant. 1995. Mining sequential
patterns. Proc. 1995 International Conference of Very
Large DataBases (VLDB?95), pages 3?14.
J. Aoe. 1989. An Efficient Digital Search Algorithm by
Using a Double-Array Structure. IEEE Transactions
on Software Engineering Vol. 15, 9, pages 1066?1077.
T. Brants. 2000. TnT ? A Statistical Part-of-Speech Tag-
ger. 6th Applied Natural Language Processing Con-
ference, pages 224?231.
EDR Electronic Dictionary. 1995.
http://www.iijnet.or.jp/edr.
T. Dunning. 1993. Accurate Methods for the Statistics of
Surprise and Coincidence. Computational Linguistics,
Vol.19, No.1, pages 61?74.
M. Kitamura and Y. Matsumoto. 1996. Automatic Ex-
traction of Word Sequence Correspondences in Paral-
lel Corpora. Proc. of the 4th Annual Workshop on Very
Large Corpora (WVLC-4), pages 79?87.
J. Kupiec. 1993. An Algorithm for Finding Noun Phrase
Correspondences in Bilingual Corpora. 31st Annual
Meeting of the Association for Computational Linguis-
tics, pages 23?30.
S. Kurohashi, T. Nakamura, Y. Matsumoto, and M. Na-
gao. 1994. Improvements of japanese morphological
analyzer juman. SNLR: Proceedings of the Interna-
tional Workshop on Sharable Natural Language Re-
sources, pages 22?28.
D. Marcu and W. Wong. 2002. A Phrase-Based, Joint
Probability Model for Statistical Machine Translation.
Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing, pages 133?
139.
Y. Matsumoto, A. Kitamuchi, T. Yamashita, H. Matsuda,
K. Takaoka, and M. Asahara. 2000. Morphological
analysis system chasen version 2.2.1 manual. Nara In-
stitute of Science and Technology.
I.D. Melamed. 2001. Empirical Methods for Exploiting
Parallel Texts. MIT Press.
R.C. Moore. 2001. Towards a Simple and Accurate
Statistical Approach to Learning Translation Relation-
ships among Words. ACL Workshop on Data-Driven
Machine Translation, pages 79?86.
J. Pei, B. Han, J. Mortazavi-Asl, H. Pinto, Q. Chen,
U. Dayal, and M. Hau. 2001. Prefixspan: Mining se-
quential patterns efficiently by prefix-projected pattern
growth. Proc. of International Conference of Data En-
gineering (ICDE2001), pages 215?224.
F. Smadja, K.R. McKeown, and V. Hatzuvassiloglou.
1996. Translating Collocations for Bilingual Lexi-
cons: A Statistical Approach. Computational Linguis-
tics, 22(1):1?38.
M. Utiyama and H. Isahara. 2002. Alingment of
Japanese?English News Articles and Sentences (in
Japanese). IPSJ SIG-NL 151, pages 15?21.
K. Yamamoto, Y. Matsumoto, and Kitamura M. 2001. A
Comparative Study on Translation Units for Bilingual
Lexicon Extraction. ACL Workshop on Data-Driven
Machine Translation, pages 87?94.
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 938?950,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Neural Networks Leverage Corpus-wide Information
for Part-of-speech Tagging
Yuta Tsuboi
IBM Research - Tokyo
yutat@jp.ibm.com
Abstract
We propose a neural network approach to
benefit from the non-linearity of corpus-
wide statistics for part-of-speech (POS)
tagging. We investigated several types
of corpus-wide information for the words,
such as word embeddings and POS tag dis-
tributions. Since these statistics are en-
coded as dense continuous features, it is
not trivial to combine these features com-
paring with sparse discrete features. Our
tagger is designed as a combination of
a linear model for discrete features and
a feed-forward neural network that cap-
tures the non-linear interactions among the
continuous features. By using several re-
cent advances in the activation functions
for neural networks, the proposed method
marks new state-of-the-art accuracies for
English POS tagging tasks.
1 Introduction
Almost all of the approaches to NLP tasks such
as part-of-speech tagging and syntactic parsing
mainly use sparse discrete features to represent lo-
cal information such as word surfaces in a size-
limited window. The non-linearity of those dis-
crete features is often used in many NLP tasks
since the simple conjunction (AND) of discrete
features represents the co-occurrence of the fea-
tures and is intuitively understandable. In addi-
tion, the thresholding of these combinatorial fea-
tures by simple counts effectively suppresses the
combinatorial increase of the parameters. At the
same time, although global information had also
been used in several reports (Nakagawa and Mat-
sumoto, 2006; Huang and Yates, 2009; Turian et
al., 2010; Schnabel and Sch?utze, 2014), the non-
linear interactions of these features were not well
investigated since these features are often dense
continuous features and the explicit non-linear ex-
pansions are counterintuitive and drastically in-
crease the number of the model parameters. In our
work, we investigate neural networks used to rep-
resent the non-linearity of global information for
POS tagging in a compact way.
We focus on four kinds of corpus-wide infor-
mation: (1) word embeddings, (2) POS tag dis-
tributions, (3) supertag distributions, and (4) con-
text word distributions. All of them are continuous
dense features and we use a feed-forward neural
network to exploit the non-linearity of these fea-
tures. Although all of them except (3) have been
used for POS tagging in previous work (Nakamura
et al., 1990; Schmid, 1994; Schnabel and Sch?utze,
2014; Huang and Yates, 2009), we propose a neu-
ral network approach to capture the non-linear in-
teractions of these features. By feeding these fea-
tures into neural networks as an input vector, we
can expect our tagger can handle not only the non-
linearity of the N-grams of the same kinds of fea-
tures but also the non-linear interactions among
the different kind of features.
Our tagger combines a linear model using
sparse high-dimensional features and a neural net-
work using continuous dense features. Although
Collobert et al. (2011) seeks to solve NLP tasks
without depending on the feature engineering of
conventional NLP methods, our architecture is
more practical because it integrates the neural
networks into a well-tuned conventional method.
Thus, our tagger enjoys both the manually ex-
plored combinations of discrete features and the
automatically learned non-linearity of the contin-
uous features. We also studied some of the newer
activation functions: Rectified Linear Units (Nair
and Hinton, 2010), Maxout networks (Goodfel-
low et al., 2013), and L
p
-pooling (Gulcehre et al.,
2014; Zhang et al., 2014).
Deep neural networks have been a hot topic
in many application areas such as computer vi-
938
sion and voice recognition. However, although
neural networks show state-of-the-art results on
a few semantic tasks (Zhila et al., 2013; Socher
et al., 2013; Socher et al., 2011), neural net-
work approaches have not performed better than
the state-of-the-art systems for traditional syn-
tactic tasks. Our neural tagger shows state-of-
the-art results: 97.51% accuracy in the standard
benchmark on the Penn Treebank (Marcus et al.,
1993) and 98.02% accuracy in POS tagging on
CoNLL2009 (Haji?c et al., 2009). In our experi-
ments, we found that the selection of the activation
functions led to large differences in the tagging ac-
curacies. We also observed that the POS tags of
the words are effectively clustered by the hidden
activations of the intermediate layer. This obser-
vation is evidence that the neural network can find
good representations for POS tagging.
The remainder of this paper is organized as fol-
lows. Section 2 introduces our deterministic tag-
ger and its learning algorithm. Section 3 describes
the continuous features that represent corpus-wide
information and Section 4 is about the neural net-
work we used. Section 5 presents our empiri-
cal study of the effects of corpus-wide informa-
tion and neural networks on English POS tagging
tasks. Section 6 describes related work, and Sec-
tion 7 concludes and suggests items for future
work.
2 Transition-based tagging
Our tagging model is a deterministic tagger based
on Choi and Palmer (2012), which is a one-pass,
left-to-right tagging algorithm that uses well-tuned
binary features.
Let x = (x
1
, x
2
, . . . , x
T
) ? X
T
be an
input token sequence of length T and y =
(y
1
, y
2
, . . . , y
T
) ? Y
T
be a corresponding POS
tag sequence of x. We denote the predicted tags
by a tagger as
?
y and the subsequence from r to t
as y
t
r
. The prediction of the t-th tag is determinis-
tically done by the classifier:
y?
t
= argmax
y?Y
f?(zt, y), (1)
where f? is a scoring function with arbitrary pa-
rameters, ? ? R
d
, that are to be learned and z
t
is
an arbitrary feature representation of the t-th po-
sition using x and
?
y
t?1
1
which is the prediction
history of the previous tokens.
We extend Choi and Palmer (2012) in three
ways: (1) an online SVM learning algorithm with
L
1
and L
2
regularization, (2) continuous features
for corpus-wide information, and (3) the compos-
ite function of a linear model for discrete features
and a non-linear model for continuous features.
Since (2) and (3) are the main topics of this pa-
per, they are explained in detail in Sections 3 and
4 and we describe only (1) here.
First, our learning algorithm trains a multi-class
SVM with L
1
and L
2
regularization based on Fol-
low the Proximally Regularized Leader (FTRL-
Proximal) (McMahan, 2011). In the k-th iteration,
the parameter update is done by
?
k
=argmin
?
k
?
l=1
(
g
l
? ?+
1
2?
l
?
?
?
?
?
?
???
l
?
?
?
?
?
?
2
2
)
+R(?),
where g
k
? R
d
is a subgradient of the hinge loss
function and R(?) = ?
1
||?||
1
+
?
2
2
||?||
2
2
is the
composite function of the L
1
and L
2
regulariza-
tion terms with hyper-parameters ?
1
? 0 and
?
2
? 0. To incorporate an adaptive learning rate
scheduling, Adagrad (Duchi et al., 2010), we use
per-coordinate learning rates for {i|1 ? i < d}:
?
k
i
=
?
i
(
?
i
+
?
?
k
l=1
(g
l
i
)
2
)
,
where ? ? 0 and ? ? 0. Although the
naive implementation may require O(k) compu-
tation in the k-th iteration, FTRL-Proximal can
be implemented efficiently by maintaining two
length-d vectors, m =
?
k
l
g
l
?
1
2?l?
l
and n =
?
k
l
(g
l
i
)
2
(McMahan et al., 2013).
Second, to overcome the error propagation
problem, we train the classifier with a simple vari-
ant of the on-the-fly example generation algorithm
from Goldberg and Nivre (2012). Since the scor-
ing function refers to the prediction history, Choi
and Palmer (2012) uses the gold POS tags, y
t?1
1
,
to generate training examples, which means they
assume all of the past decisions are correct. How-
ever, this causes error propagation problems, since
each state depends on the history of the past deci-
sions. Therefore, at the k-th iteration and the t-th
position of the input sequence, we simply use the
predictions of the previously learned classifiers to
generate training examples, i.e.,
y?
t?r
= argmax
y?Y
f?
k?r
(z
t?r
, y)
for all {r|1 ? r < t ? 1}. Although it is
not theoretically justified, it empirically runs as a
939
stochastic version of DAGGER (Ross et al., 2011)
or SEARN (Daum?e III et al., 2009) with the speed
benefit of online learning.
Algorithm 1 Learning algorithm
function LEARN(?,?,?
1
,?
2
,m,n,?
k
)
while ? stop do
Select a random sentence (x,y)
for t = 1 to T do
u=UPDATE(?,?,?
1
,?
2
,m,n,?
k
)
y?
t
= argmax
y?Y
fu(zt, y)
y? = argmax
y ?=y
t
fu(zt, , y)
if fu(zt, yt)? fu(zt, y?) < 1 then
g=?u?(zt, yt, y?) ? Subgradient
For all i ? I compute
?
i
=
(
?
n
i
+ g
2
i
?
?
n
i
)
/?
i
m
i
? m
i
+ g
i
? ?
i
u
i
n
i
? n
i
+ g
2
i
end if
k ? k + 1
end for
end while
return ?
k
end function
function UPDATE(?,?,?
1
,?
2
,m,n,?
k
)
for i ? I do
?
k
i
=
?
?
?
0 if |m
i
| ? ?
1
?m
i
+sgn(m
i
)?
1
(
?
i
?
2
+
?
n
i
)
/?
i
+?
2
otherwise
u
i
? ?
k
i
if acceleration then
u
i
? ?
k
i
+
k
k+3
(
?
k
i
? ?
k?1
i
)
end if
end for
for i ?? I do
u
i
? ?
k
i
? ?
k?1
i
? Leaving all ? for inactive i unchanged
end for
return u
end function
Algorithm 1 summarizes our training process
where ?(z
t
, y
t
, y?) := max(0, 1 ? f?(zt, y) +
f?(zt, y?)) is the multi-class hinge loss (Crammer
and Singer, 2001). I in Algorithm 1 is a set of
parameter indexes that correspond to the non-zero
features, so the update is sparse for sparse fea-
tures. In addition, for the parameter update of the
neural networks, we also use an accelerated prox-
imal method (Parikh and Boyd, 2013), which is
considered as a variant of the momentum meth-
ods (Sutskever et al., 2013). Although u and ? are
the same when the acceleration is not used, u in
Algorithm 1 is an extrapolation step in the accel-
erated method. Although we do not focus on the
learning algorithm in this work, the algorithm con-
verges quite quickly and the speed is important be-
cause the neural network extension described later
requires a hyper-parameter search which is com-
putationally demanding.
3 Corpus-wide Information
Since typical discrete features indicate only the
occurrence in a local context and do not convey
corpus-wide statistics, we studied four kinds of
continuous features for POS tagging to represent
the corpus-wide information.
3.1 Word embeddings
Word embeddings, or distributed word represen-
tations, embed the words into a low-dimensional
continuous space. Most of the neural network ap-
plications for NLP use word embeddings (Col-
lobert et al., 2011; Socher et al., 2011; Zhila et
al., 2013; Socher et al., 2013), and even for linear
models, Turian et al. (2010) highlights the benefit
of word embeddings on sequential labeling tasks.
In particular, in our experiments, we used two
recently proposed algorithms, word2vec (Mikolov
et al., 2013) and glove (Pennington et al.,
2014), which are simple and scalable, although
our method could use other word embeddings.
Word2vec trains the word embeddings to pre-
dict the words surrounding each word, and glove
trains the word embeddings to predict the loga-
rithmic count of the surrounding words of each
word. Thus, these embeddings can be seen as
the distributed versions of the distributional fea-
tures since the word vectors compactly represent
the distribution of the context in which a word ap-
pears. We normalized the word embeddings to
unit length and used the average vector of training
vocabulary for the unknown tokens.
3.2 POS tag distribution
In a way similar to Schmid (1994), we use POS tag
distribution over a training corpus. Each word is
represented by a vector of length |Y | in which the
y-th element is the conditional probabilities with
which that word gets the y-th POS tag. We also
use the POS tag distributions of the affixes and
940
spelling binary features used in Choi and Palmer
(2012). We cite the definitions of these features.
1. Affix: c
:1
, c
:2
, c
:3
, c
n:
, c
n?1:
, c
n?2:
, c
n?3:
where c
?
is a character string in a word. For
example c
:2
is the prefix of length two of a
word and c
n?1:
is the suffix of length two of
a word.
2. Spelling: initial uppercase, all upper-
case, all lowercase, contains 1/2+ capi-
tal(s) not at the beginning, contains a (pe-
riod/number/hyphen).
The probabilities for a feature b is estimated with
additive smoothing as
P (y|b) =
C(b, y) + 1
C(b) + |Y |
, (2)
where C(b) and C(b, y) are the counts of b and
co-occurrences of b and y, respectively. In addi-
tion, an extra dimension for sentence boundaries
is added to the vector for word-forms. In total, the
POS tag distributions for each word are encoded
by a vector of dimension |Y |+1+|Y |?14 (|Y | for
lowercase simplified word-forms, 1 for sentence
boundaries, |Y | ? 7 for affixes, and |Y | ? 7 for
spellings).
3.3 Supertag distribution
We also use the distribution of supertags for de-
pendency parsing. Supertags are lexical templates
which are extracted from the syntactic dependency
structures and suppertagging is often used for the
pre-processing of a parsing task. Since the su-
pertags encode rich syntactic information, we ex-
pect the supertag distribution of a word to also
provide clues for the POS tagging. We used two
types of supertags: One is the dependency rela-
tion label of the head of the word and the other
is that of the dependents of the word. Following
Ouchi et al. (2014), we added the relative posi-
tion, left (L) or right (R), to the supertags. For
example, a word has its dependents in the left di-
rection with a label ?nn? and in the right direc-
tion with a label ?amod?, so its supertag set for
dependents is {?nn/L?, ?amod/R?}. A special su-
pertag ?NO-CHILD? is used for a word that has
no dependent. Note that, although the Model 2 su-
pertag set of Ouchi et al. (2014) is defined as the
combination of head and dependent tags, we used
them separately. The feature values for each word
are defined in the same way as Equation 2 in Sec-
tion 3.2. Since a word can have more than one
dependent, the dependent supertag features are no
longer multinomial distributions but we used them
in that way. Note that, since the feature values are
calculated using the tree annotations from training
set, our tagger does not require any dependency
parser at runtime.
3.4 Context word distribution
This is the simplest distributional features in
which each word is represented by the distribu-
tions of its left and right neighbors. Although the
context word distribution is similar to word em-
beddings, we believe they complement each other,
as reported by Levy and Goldberg (2014). Fol-
lowing Schnabel and Sch?utze (2014), we restricted
the set of indicator words to the 500 most frequent
words in the corpus, and used two special feature
entries: One is the marginal probability of the non-
indicator words and the other is the probabilities
of neighboring sentence boundaries. The condi-
tional probabilities for left and right neighbors are
estimated in the same way as Equation 2 in Sec-
tion 3.2, and there are a total of 1, 004 dimensions
of this feature for a word.
4 Neural Networks
The non-linearity of the discrete features has been
exploited in many NLP tasks, since the simple
conjunction of the discrete features is intuitive and
the thresholding of these combinatorial features
by their feature counts effectively suppresses the
combinatorial increase of the parameters.
In contrast, it is not easy to manually tune the
non-linearity of the continuous features. For ex-
ample, it is not intuitive to design the conjunc-
tion features of two kinds of word embeddings,
word2vec and glove. Although kernel methods
have been used to incorporate non-linearity in
prior research, they are rarely used now because
their tagging speed is too slow (Gim?enez and
M`arquez, 2003). Our solution is to introduce
feed-forward neural networks to capture the non-
linearity of the corpus-wide information.
4.1 Hybrid model
We designed our tagger as a hybrid of a linear
model and a non-linear model. Wang and Man-
ning (2013) reported that a neural network us-
ing both sparse discrete features and dense (low-
941
Figure 1: A hybrid architecture of a linear model
and a neural network with a pooling activation
function
dimensional) continuous features was worse than
a linear model using the same two features. At
the same time, they also reported that a neural net-
work using only the dense continuous features out-
performed a linear model using the same features.
Based on their results, we applied neural networks
only for the continuous features and used a linear
model for the discrete features.
Formally, the scoring function (1) in Section 2
is defined as the composite function of two terms:
f(z, y) := f
linear
(z, y)+f
nn
(z, y). The first f
linear
is the linear model and the second f
nn
is a neu-
ral network. Since this is a linear combination of
two functions, the subgradient of the loss function
required for Algorithm 1 is also the linear com-
bination of subgradients of two functions, which
means
???(zt, yt, y?) = ??flinear(zt, y?) + ??fnn(zt, y?)
? ??flinear(zt, yt)? ??fnn(zt, yt)
if f?(zt, yt)? f?(zt, y?) < 0.
First, the linear model can be defined as
f
linear
(z, y) := ?
d
? ?
d
(z, y),
where ?
d
(z, y) is a feature mapping for the dis-
crete part of z and a POS tag, and ?
d
is the cor-
responding parameter vector. Since this is a lin-
ear model, the gradient of this function is simply
??flinear(z, y) = ?d(z, y).
Second, each hidden layer of our neural net-
works non-linearly transforms an input vector h
?
into an output vector h and we can say h
?
is the
continuous part of z at the first layer. Let h
L
be
a hidden activation of the top layer, which is the
non-linear transformation of the continuous part
of z. The output layer of the neural network is
defined as
f
nn
(z, y) := ?
o
? ?
o
(h
L
, y),
where?
o
(h, y) is a feature mapping for the hidden
variables and a POS tag, and ?
o
is the correspond-
ing parameter vector.
4.2 Activation functions
The hidden variables h are computed by the re-
cursive application of a non-linear activation func-
tion. Since new styles of the activation functions
were recently proposed, we review several acti-
vation functions here. Let v ? R
|V |
be the in-
put of an activation function and each element is
v
j
= ?
nn,j
? h
?
+ ?
bias,j
, where ?
nn,j
is the param-
eter vector for v
j
and ?
bias,j
is the bias parameter
for v
j
. We also assume v is divided into groups
of size G, and denote the j-th element of the i-th
group as {v
ij
|1 ? i ? |V |/G ? 1 ? j ? G}. We
studied three activation functions:
1. Rectified linear units (ReLUs) (Nair and Hin-
ton, 2010):
h
j
= max(0, v
j
) for all {j|1 ? j ? |V |}.
Note that a subgradient of ReLUs is
?h
j
??
=
{
?v
j
?? if vj > 0
0 otherwise.
2. Maxout networks (MAXOUT) (Goodfellow
et al., 2013):
h
i
= max
1?j?G
v
ij
for all {i|1 ? i ?
|V |
G
}.
Note that a subgradient of MAXOUT is
?h
i
??
=
?v
i
?
j
??
, where
?
j = argmax
1?j?G
v
ij
3. Normalized L
p
-pooling (L
p
) (Gulcehre et al.,
2014):
h
i
=
?
?
1
G
G
?
j=1
|v
ij
|
p
?
?
1
p
for all {i|1 ? i ?
|V |
G
}.
Note that a subgradient of L
p
is
?h
i
??
=
G
?
j=1
?v
ij
??
v
ij
|v
ij
|
p?2
G
?
?
1
G
G
?
j=1
|v
i,j
|
p
?
?
1
p
?1
.
942
The activation inputs for each predefined group,
{v
1j
, . . . , v
Gj
}, are aggregated by a non-linear
function in MAXOUT or L
p
activation functions,
while each input is transformed into a correspond-
ing hidden variable in the ReLUs. When the
number of parameters required for these activation
functions is the same, the number of output vari-
ables h for MAXOUT and L
p
is one-G-th smaller
than that for ReLUs. Boureau et al. (2010) show
pooling operations theoretically reduce the vari-
ance of hidden activations, and our experimental
results also show MAXOUT and L
p
perform bet-
ter than the ReLUs with the same number of pa-
rameters. Note that MAXOUT is a special case
of unnormalized L
p
pooling when p = ? and
v
j
> 0 for all j (Zhang et al., 2014). Figure 1
summarizes the proposed architecture with a sin-
gle hidden layer and a pooling activation function.
4.3 Hyper-parameter search
Finally, the subgradients of the neural network,
f
nn
(z, y), can be computed through standard
back-propagation algorithms and we can apply
them in Algorithm 1. However, many of the hyper-
parameters have to be determined for the training
of the neural networks, and two stages of random
hyper-parameter searches (Bergstra and Bengio,
2012) are used in our experiments. Note that the
parameters are grouped into three sets, ?
d
,?
o
,?
nn
,
and the same values for ?
1
, ?
2
, ?, ? are used for
each parameter set.
In the first stage, we randomly select 32 combi-
nations of ?
2
for f
nn
, ?
1
, ?
2
for f
linear
, the epoch
to start the L1/L2 regularizations, and the on and
off the acceleration in Algorithm 1. Here are the
candidates of three hyper-parameters:
1. ?
1
: 0 for the update of f
nn
and
{0, 10
?8
, 10
?6
, 10
?4
, 10
?2
, 1} for the
update of f
linear
;
2. ?
2
: {0.1, 0.5, 1, 5, 10} for the update of
f
nn
and {1, 5, 10, 50, 100} for the update of
f
linear
; and
3. Epoch to start the regularizations: {0, 1, 2}.
In the second stage with each hyper-parameter
combination above, we select 8 random combina-
tions of ?, ? for both f
linear
and f
nn
and initial pa-
rameter ranges R for f
nn
. Here are the candidates
of the three hyper-parameters:
1. ?: {0.01, 0.05, 0.1, 0.5, 1, 5};
Data Set #Sent. #Tokens #Unknown
Training 38,219 912,344 0
Development 5,527 131,768 4,467
Testing 5,462 129,654 3,649
Table 1: Data set splits for PTB.
2. ?: {0.5, 1, 5};
3. R: {[?0.1, 0.1], [?0.05, 0.05], [?0.01, 0.01],
[?0.005, 0.005]}.
The values of ? for f
nn
are uniformly sampled in
the range of the randomly selected R. Note that,
according to Goodfellow et al. (2014), the biases
?
bias
are initialized as 0 for MAXOUT and L
p
, and
uniformly sampled from a range R + max(R),
i.e., always initialized with non-negative values.
The best combination for the development set is
chosen after training that uses random 20% of the
training set at the second stage, and Algorithm 1
is terminated when the all token accuracy of the
development data has been declining for 5 epochs
at the first stage. In other words, 32 ? 8 random
combinations of ?, ?, and ? for f
nn
were tested.
5 Experiments
5.1 Setup
Our experiments were mainly performed using
the Wall Street Journal from Penn Treebank
(PTB) (Marcus et al., 1993). We used tagged sen-
tences from the parse trees (Toutanova et al., 2003)
and followed the standard approach of splitting the
PTB, using sections 0?18 for training, section 19?
21 for development, and section 22?24 for testing
(Table 1). In addition, we used the CoNLL2009
data sets with the training, development, and
test splits used in the shared task (Haji?c et al.,
2009) for better comparison with a joint model of
POS tagging and dependency parsing (Bohnet and
Nivre, 2012).
Our baseline tagger was trained by Algorithm 1.
As discrete features for our tagger, we used the
same binary feature set as Choi and Palmer (2012)
which is composed of (a) 1, 2, 3-grams of the
surface word-forms and their predicted/dominated
POS tags, (b) the prefixes and suffixes of the
words, and (c) the spelling types of the words. In
the same way as Choi and Palmer (2012), we used
lowercase simplified word-forms which appeared
at least 3 times.
943
In addition to their binary features, we used con-
tinuous features which are the concatenation of the
corpus-wide features in a context window. The
window of size w = 2s + 1 is the local context
centered around x
t
: x
t?s
, ? ? ? , x
t
, ? ? ? , x
t+s
. The
experimental settings of each feature described in
Section 3 are as follows.
Word embeddings
We used two word vectors: 300-dimensional
vectors that were learned by word2vec using
a part of the Google News dataset (around
100 billion tokens)
1
, and 300-dimensional
vectors that were learned by glove using a
part of the Common Crawl dataset (840 bil-
lion tokens)
2
. For sentence boundaries, we
use the vector of the special entry ?</s>? for
the word2vec embeddings and the zero vec-
tor for the glove embeddings.
POS tag distribution
The counts are calculated using training data.
Supertag distribution
In the experiments on PTB, we used the Stan-
ford parser v2.0.4
3
to convert from phrase
structures to dependency structures so that
the dependency relation labels of the Stan-
ford dependencies are used. The size of the
supertag set is 85 for both heads and depen-
dents in our experiments. In the experiments
on CoNLL2009, the dependency structures
and labels defined in CoNLL2009 are used
and the size of supertag set is 99 for both
heads and dependents.
Context word distribution
To count the neighboring words in our exper-
iments, we used sections 0?18 of the Wall
Street Journal and all of the Brown corpus
from Penn Treebank (Marcus et al., 1993).
Since the training of the neural networks is com-
putationally demanding, first, we trained the lin-
ear classifiers using Algorithm 1 to select the best
window sizes for each corpus-wide information of
Section 3. Then the best window size setting for
the development set of PTB was used for train-
ing the neural networks described in Section 4.
1
The pre-trained vectors are available at https://
code.google.com/p/word2vec
2
The pre-trained vectors are available at http://nlp.
stanford.edu/projects/glove/
3
http://nlp.stanford.edu/software/
lex-parser.shtml
Window size Accuracy (%)
# w2v glv pos stg cw All Unk.
1 - - - - - 97.15 86.81
2 3 - - - - 97.36 88.96
3 - 3 - - - 97.34 89.55
4 3 3 - - - 97.40 90.44
5 3 3 3 - 1 97.44 90.17
6 3 3 3 1 1 97.44 90.53
7 3 3 3 3 1 97.45 90.22
8 3 3 6 - 1 97.41 90.51
9 3 3 6 3 1 97.44 90.15
Table 2: Feature and window size selection: de-
velopment accuracies of all tokens (All) and un-
known tokens (Unk.) of linear models trained on
PTB (w2v: word2vec; glv: glove; pos: POS tag
distribution; stg: supertag distribution; cw: con-
text word distribution).
We fixed the group size at 8 for MAXOUT and
L
p
, and the number of hidden variables was cho-
sen from {32, 48} for MAXOUT and L
p
and from
{32, 64, 128, 256, 384} for ReLUs according to all
token accuracy on the development data of PTB.
We report the POS tagging accuracy for both all
of the tokens and only for the unknown tokens that
do not appear in the training set.
5.2 Results
Table 2 shows the accuracies of the linear models
on PTB with different window sizes for the con-
tinuous features. The window sizes of the word
embeddings (word2vec and glove) in Section 3.1,
POS tag distributions in Section 3.2, supertag dis-
tributions in Section 3.3, and context word distri-
butions in Section 3.4 are shown in the columns
of w2v, glv, pos, stg, and cw, respectively. Note
that ?-? denotes the corresponding feature was not
used at all and the first row with all ?-? denotes the
results only using the original binary features from
Choi and Palmer (2012). The window sizes in Ta-
ble 2 are chosen mainly to investigate the effect
of the word2vec embeddings, glove embeddings,
and supertag distributions, since they had not pre-
viously been used for POS tagging.
The additions of the word embeddings improve
all token accuracy by about 0.2 points accord-
ing to the results shown in Nos. 1, 2, 3. Al-
though both word embeddings improved the ac-
curacy of the unknown tokens, the gain of the
glove embeddings (No. 3) is larger than that of the
944
Neural Network Settings Development Set Test Set
# Activation functions #Hidden Group size (G) All Unk. All Unk.
1 Linear model - - 97.45 90.22 97.46 91.39
2 ReLUs 384 1 97.45 90.87 97.42 91.04
3 L
p
(p = 2) 48 8 97.52 90.91 97.51 91.64
4 L
p
(p = 3) 32 8 97.51 90.91 97.51 91.53
5 MAXOUT 48 8 97.50 90.89 97.50 91.67
6 L
p
(p = 2) (w/o linear part) 48 8 97.39 91.18 97.40 91.23
Table 3: Development and test accuracies of all tokens and unknown tokens (%) on PTB.
Tagger All Unk.
Manning (2011) 97.32 90.79
S?gaard (2011) 97.50 N/A
L
p
(p = 2) 97.51 91.64
(a) Test accuracies on PTB
Tagger All Unk.
Bohnet and Nivre (2012) 97.84 N/A
L
p
(p = 2) 98.02 92.01
(b) Test accuracies on CoNLL2009
Table 4: Test accuracies of all tokens and unknown tokens (%) comparing with the previously reported
results
word2vec (No. 2). The reason for this difference
in the two embeddings may be because the train-
ing data for the glove vectors is 8 times larger than
that for the word2vec vectors. The usage of the
two word embeddings shows further improvement
in the tagging accuracy over single word embed-
dings (No. 4).
The addition of the POS tag distributions and
the context word distributions improves all token
accuracy (Nos. 5, 8). The comparison between the
results with stag=?-? (Nos. 5, 8) and stag = {1, 3}
(Nos. 6, 7, 9) indicates the minor but consistent
improvement by using the supertag distribution
features in Section 3.3. Finally, the 7th window-
size setting in Table 2 achieves the best all token
accuracy among the linear models, so we chose
this setting for the experiments with the neural net-
works.
In Table 3, we compare the different settings of
the neural networks with a single hidden layer
4
on the development set and test set from PTB.
Neural networks with the MAXOUT and L
p
(Nos. 3, 4, 5) significantly outperform the best lin-
ear model (No. 1)
5
, but the accuracy of the Re-
LUs (No. 2) was similar to that of the best lin-
ear model. According to these results, we argue
4
We leave the investigation of deeper neural networks as
future work.
5
For significance tests, we have used the Wilcoxon
matched-pairs signed-rank test at the 95% confidence level
dividing the data into 100 data pairs.
that the activation function selection is important,
although conventional research in NLP has used
only a single activation function. It took roughly
7 times as long to learn the hybrid models than
the linear model (No. 1). ?L
p
(p = 2) (w/o linear
part)? (No. 6) shows the result for a L
p
(p = 2)
model which does not include the linear model
f
linear
for the binary features. Comparing the test
results of No. 6 with that of No. 3, the proposed
hybrid architecture of a linear model and a neural
network enjoys the benefits of both models. Note
that No. 6?s accuracies of the unknown tokens are
relatively competitive, and this may be because the
continuous features for the neural network do not
include word surfaces.
Since it shows the best accuracy for all tokens
on the development set, we refer to L
p
(p = 2)
with 48 hidden variables and the group size of 8
(No. 3 in Table 3) as our representative tagger and
denote it as L
p
(p = 2) in the rest of discussion.
In Table 4a, we compare our result with the pre-
viously reported results and we see that our tagger
outperforms the current state-of-the-art systems on
PTB for the accuracies of all tokens and unknown
tokens.
In addition, since our tagger was trained us-
ing the dependency tree annotations as described
in Section 3.3, we compare it with the results of
Bohnet and Nivre (2012) which is also trained
using both POS tag and dependency annotations.
Although their focus is on the dependency pars-
945
PC1
?1.0 0.0 1.0 ?1.5 0.0 1.0
?
4
?
2
0
?
1.0
0.0
1.0
PC2
PC3
?
2.5
?
1.0
0.5
?4 ?2 0
?
1.5
?
0.5
0.5
?2.5 ?1.0 0.5
PC4
VB
VBD
VBG
VBN
VBP
VBZ
(a) PCA of the raw features
PC1
?0.5 0.5 ?0.8 ?0.2
?
0.8
?
0.2
0.4
?
0.5
0.0
0.5 PC2
PC3
?
1.5
?
0.5
?0.8 ?0.2 0.4
?
0.8
?
0.4
0.0
?1.5 ?0.5
PC4
VB
VBD
VBG
VBN
VBP
VBZ
(b) PCA of the hidden activations of L
p
(p = 2)
Figure 2: Scatter plots of verbs for all combinations between the first four principal components of the
raw features and the activation of hidden variables.
ing, they report state-of-the art POS accuracies
for many languages. Note that Bohnet and Nivre
(2012) also used external resources. Table 4b
gives the results for CoNLL2009 data set
6
. Our
tagger outperform Bohnet and Nivre (2012), so we
believe this is the highest POS accuracy ever re-
ported for a tagger trained on this data set.
Finally, to visualize the learned representations,
we applied principal components analysis (PCA)
to the hidden activations h
L
of the first 10, 000 to-
kens of the development set from PTB. We also
performed PCA to the raw continuous inputs of
the same data set. Figure 2 shows the data plots
for all the combinations among the first four prin-
cipal components. We plots only the verb tokens
to make the plots easier to see. Figures 2a and
2b show the PCA results of the raw features and
the hidden activations of L
p
(p = 2), respectively.
Compared to Figure 2a, the tokens with the same
POS tag are more clearly clustered in Figure 2b.
This suggests the neural network learned the good
representations for POS tagging and these hidden
activations can be used as the input of the succeed-
ing processes, such as parsing.
6
The accuracies of our tagger on the development set of
CoNLL2009 data are 97.76% for all tokens and 93.42% for
unknown tokens.
6 Related Work
There is some old work on the POS tagging by
neural networks. Nakamura et al. (1990) proposed
a neural tagger that predicts the POS tag using a
previous POS predictions. Schmid (1994) is most
similar to our work. The inputs of his neural net-
work are the POS tag distributions of a word and
its suffix in a context window, and he reports a
2% improvement over a regular hidden Markov
model. However, his tagger did not use the other
kinds of corpus-wide information as we used.
Most of the recent studies on POS tagging use
linear models (Suzuki and Isozaki, 2008; Spous-
tov?a et al., 2009) or other non-linear models, such
as k-nearest neighbor (kNN) (S?gaard, 2011).
One trend in these studies is model combinations.
Suzuki and Isozaki (2008) combined generative
and discriminative models, Spoustov?a et al. (2009)
used the combination of three taggers to gener-
ate automatically annotated corpus, and S?gaard
(2011) used the outputs of a supervised tagger and
an unsupervised tagger as the feature space of the
kNN. Our work also follows this trend since neural
networks can be considered as non-linear integra-
tion of several linear classifiers.
Apart from POS tagging, some previous studies
in parsing used the discretization method to handle
the combination of continuous features. Bohnet
and Nivre (2012) binned the difference of two con-
946
tinuous features in discrete steps of a predefined
small interval. Bansal et al. (2014) used the con-
junction of discretized features and studied two
discretization methods: One is the binning of real
values into discrete steps and the other is a hard
clustering of continuous feature vectors. It is not
easy to determine the optimal intervals for the bin-
ning method, and the clustering method is unsu-
pervised so that the clusters are not guaranteed for
good representations of the target tasks.
To capture rich syntactic information for Chi-
nese POS tagging, Sun and Uszkoreit (2012) used
the ensemble model of both a POS tagger and a
constituency parser. Sun et al. (2013) improved
the efficiency of Sun and Uszkoreit (2012) in
which a single tagging model is trained using au-
tomatically annotated corpus generated by the en-
semble tagger. Although the supertag distribution
feature in Section 3.3 is a simple way to incor-
porate syntactic information, automatically parsed
large corpora may make the estimate of the su-
pertag distributions more accurate.
7 Conclusion and Future Work
We are studying a neural network approach to han-
dle the non-linear interaction among corpus-wide
statistics. For POS tagging, we used word em-
beddings, POS tag distributions, supertag distribu-
tions, and context word distributions in a context
window. These features are beneficial, even for
linear classifiers, but the neural networks leverage
these features for improving tagging accuracies.
Our tagger with Maxout networks (Goodfellow et
al., 2013) or L
p
-pooling (Zhang et al., 2014; Gul-
cehre et al., 2014) show the state-of-the-art results
on two English benchmark sets.
Our empirical results suggest further opportu-
nities to investigate continuous features not only
for POS tagging but also for other NLP tasks.
An obvious use case for continuous features is
the N-best outputs with confidence values, which
were predicted by the previous process in a NLP
pipeline, such as the POS tags used for syntactic
parsing. Another interesting extension is the use of
on-the-fly features which reflect previous network
states, although the neural networks in our current
work do not refer to the prediction history. Recur-
rent neural networks (RNNs) may be a solution to
represent the prediction history in a compact way,
and Mesnil et al. (2013) reported that RNNs out-
perform conditional random fields (CRFs) on a se-
quential labeling task. They also show the superi-
ority of bi-directional RNNs on their task, so the
bi-directional RNNs may also be effective on the
POS tagging, since bi-directional inferences were
also used in earlier work (Tsuruoka and Tsujii,
2005).
It has a clear benefit over kernel methods in
that the test-time computational cost of neural net-
works is independent from training data. How-
ever, although the test-time speed of original ker-
nel methods is proportional to the number of train-
ing data, recent development of kernel approxima-
tion techniques achieve significant speed improve-
ments (Le et al., 2013; Pham and Pagh, 2013).
Since this work shows the non-linearity of contin-
uous features should be exploited, those approxi-
mated kernel methods may also improve the tag-
ging accuracies without sacrifice tagging speed.
Independent from our work, Ma et al. (2014)
and Santos and Zadrozny (2014) also recently pro-
posed neural network approaches for POS tagging.
Ma et al. (2014)?s approach is similar to our ap-
proach, with a combination of a linear model and
a neural network, although a direct comparison is
not easy since their focus is the Web domain adap-
tation of POS tagging. Remarkably, they report n-
gram embeddings are better than single word em-
beddings. Santos and Zadrozny (2014) proposed
character-level embedding to capture the morpho-
logical and shape information for POS tagging.
Although the reported accuracy (97.32%) on PTB
data is lower than state of the art results, their ap-
proach is promising for morphologically rich lan-
guages. We may study the integration of these em-
beddings into our approach as future work.
References
Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014. Tailoring continuous word representations for
dependency parsing. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics, Proceedings of the Conference (ACL). The
Association for Computer Linguistics.
James Bergstra and Yoshua Bengio. 2012. Random
search for hyper-parameter optimization. Journal of
Machine Learning Research, 13:281?305.
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and
labeled non-projective dependency parsing. In Pro-
ceedings of the Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
947
putational Natural Language Learning (EMNLP-
CoNLL), pages 1455?1465.
Y-Lan Boureau, Jean Ponce, and Yann LeCun. 2010.
A theoretical analysis of feature pooling in visual
recognition. In Proceedings of the International
Conference on Machine Learning (ICML), pages
111?118.
Jinho D. Choi and Martha Palmer. 2012. Fast and
robust part-of-speech tagging using dynamic model
selection. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics, Pro-
ceedings of the Conference (ACL), pages 363?367.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Koby Crammer and Yoram Singer. 2001. On the algo-
rithmic implementation of multiclass kernel-based
vector machines. Journal of Machine Learning Re-
search, 2:265?292.
Hal Daum?e III, John Langford, and Daniel Marcu.
2009. Search-based structured prediction. Machine
Learning Journal, 75(3):297?325, June.
John C. Duchi, Elad Hazan, and Yoram Singer. 2010.
Adaptive subgradient methods for online learning
and stochastic optimization. In Proceedings of
the Conference on Learning Theory (COLT), pages
257?269.
Jes?us Gim?enez and Llu??s M`arquez. 2003. Fast and ac-
curate part-of-speech tagging: The svm approach re-
visited. In Proceedings of Recent Advances in Natu-
ral Language Processing (RANLP), pages 153?163.
Yoav Goldberg and Joakim Nivre. 2012. A dynamic
oracle for arc-eager dependency parsing. In Pro-
ceedings of International Conference on Computa-
tional Linguistics (COLING), pages 959?976.
Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza,
Aaron C. Courville, and Yoshua Bengio. 2013.
Maxout networks. In Proceedings of the Interna-
tional Conference on Machine Learning (ICML),
pages 1319?1327.
Ian J. Goodfellow, Mehdi Mirza, Xia Da, Aaron C.
Courville, and Yoshua Bengio. 2014. An empirical
investigation of catastrophic forgeting in gradient-
based neural networks. In Proceedings of Inter-
national Conference on Learning Representations
(ICLR).
Caglar Gulcehre, Kyunghyun Cho, Razvan Pascanu,
and Yoshua Bengio. 2014. Learned-norm pool-
ing for deep feedforward and recurrent neural net-
works. In Proceedings of the European Con-
ference on Machine Learning and Principles and
Practice of Knowledge Discovery in Databases
(ECML/PKDD).
Jan Haji?c, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad?o, Jan
?
St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the
Conference on Computational Natural Language
Learning (CoNLL), pages 1?18.
Fei Huang and Alexander Yates. 2009. Distributional
representations for handling sparsity in supervised
sequence-labeling. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics and the International Joint Conference on
Natural Language Processing (ACL/IJCNLP), pages
495?503.
Quoc V. Le, Tam?as Sarl?os, and Alexander J. Smola.
2013. Fastfood - computing Hilbert space expan-
sions in loglinear time. In Proceedings of the Inter-
national Conference on Machine Learning (ICML),
pages 244?252.
Omer Levy and Yoav Goldberg. 2014. Linguistic
regularities in sparse and explicit word represen-
tations. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL).
Ji Ma, Yue Zhang, Tong Xiao, and Jingbo Zhu. 2014.
Tagging the Web: Building a robust web tagger with
neural network. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguis-
tics, Proceedings of the Conference (ACL). The As-
sociation for Computer Linguistics.
Christopher D. Manning. 2011. Part-of-speech tag-
ging from 97% to 100%: Is it time for some linguis-
tics? In Proceedings of Conference on Intelligent
Text Processing and Computational Linguistics (CI-
CLing), pages 171?189.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The Penn treebank. Computa-
tional Linguistics, 19(2):313?330.
H. Brendan McMahan, Gary Holt, David Sculley,
Michael Young, Dietmar Ebner, Julian Grady,
Lan Nie, Todd Phillips, Eugene Davydov, Daniel
Golovin, Sharat Chikkerur, Dan Liu, Martin Wat-
tenberg, Arnar Mar Hrafnkelsson, Tom Boulos, and
Jeremy Kubica. 2013. Ad click prediction: a
view from the trenches. In Proceedings of the ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD), pages 1222?
1230.
H. Brendan McMahan. 2011. Follow-the-regularized-
leader and mirror descent: Equivalence theorems
and L1 regularization. In Proceedings of the Four-
teenth International Conference on Artificial Intelli-
gence and Statistics (AISTATS), pages 525?533.
948
Gr?egoire Mesnil, Xiaodong He, Li Deng, and Yoshua
Bengio. 2013. Investigation of recurrent-neural-
network architectures and learning methods for spo-
ken language understanding. In Proceedings of An-
nual Conference of the International Speech Com-
munication Association (INTERSPEECH), pages
3771?3775.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013. Distributed repre-
sentations of words and phrases and their composi-
tionality. In Proceedings of Advances in Neural In-
formation Processing Systems (NIPS), pages 3111?
3119.
Vinod Nair and Geoffrey E. Hinton. 2010. Rectified
linear units improve restricted Boltzmann machines.
In Proceedings of the International Conference on
Machine Learning (ICML), pages 807?814.
Tetsuji Nakagawa and Yuji Matsumoto. 2006. Guess-
ing parts-of-speech of unknown words using global
information. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguis-
tics, Proceedings of the Conference (ACL).
Masami Nakamura, Katsuteru Maruyama, Takeshi
Kawabata, and Kiyohiro Shikano. 1990. Neural
network approach to word category prediction for
English texts. In Proceedings of International Con-
ference on Computational Linguistics (COLING),
pages 213?218.
Hiroki Ouchi, Kevin Duh, and Yuji Matsumoto. 2014.
Improving dependency parsers with supertags. In
Proceedings of Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL), pages 154?158.
Neal Parikh and Stephen P. Boyd. 2013. Proximal al-
gorithms. Foundations and Trends in Optimization,
1(3):123?231.
Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: global vectors
for word representation. In Proceedings of the Con-
ference on Empirical Methods on Natural Language
Processing (EMNLP).
Ninh Pham and Rasmus Pagh. 2013. Fast and scal-
able polynomial kernels via explicit feature maps.
In Proceedings of the ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing (KDD), pages 239?247.
St?ephane Ross, Geoffrey J. Gordon, and Drew Bagnell.
2011. A reduction of imitation learning and struc-
tured prediction to no-regret online learning. In Pro-
ceedings of the Fourteenth International Conference
on Artificial Intelligence and Statistics (AISTATS),
pages 627?635.
Cicero Dos Santos and Bianca Zadrozny. 2014.
Learning character-level representations for part-of-
speech tagging. In Proceedings of the International
Conference on Machine Learning (ICML), pages
1818?1826.
Helmut Schmid. 1994. Part-of-speech tagging with
neural networks. In Proceedings of International
Conference on Computational Linguistics (COL-
ING), pages 172?176.
Tobias Schnabel and Hinrich Sch?utze. 2014. FLORS:
Fast and simple domain adaptation for part-of-
speech tagging. Transactions of the Association for
Computational Linguistics, 2:15?26, February.
Richard Socher, Eric H. Huang, Jeffrey Pennington,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In Proceedings of
Advances in Neural Information Processing Systems
(NIPS), pages 801?809.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Chris Manning, Andrew Ng, and Chris
Potts. 2013. Recursive deep models for semantic
compositionality over a sentiment treebank. In Pro-
ceedings of the Conference on Empirical Methods
on Natural Language Processing (EMNLP), pages
1631?1642.
Anders S?gaard. 2011. Semi-supervised condensed
nearest neighbor for part-of-speech tagging. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics, Proceedings of the
Conference (ACL), pages 48?52.
Drahom??ra johanka Spoustov?a, Jan Haji?c, Jan Raab,
and Miroslav Spousta. 2009. Semi-supervised
training for the averaged perceptron pos tagger. In
Proceedings of Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL), pages 763?771.
Weiwei Sun and Hans Uszkoreit. 2012. Capturing
paradigmatic and syntagmatic lexical relations: To-
wards accurate Chinese part-of-speech tagging. In
Proceedings of the Annual Meeting of the Associa-
tion for Computational Linguistics, Proceedings of
the Conference (ACL), pages 242?252.
Weiwei Sun, Xiaochang Peng, and Xiaojun Wan.
2013. Capturing long-distance dependencies in se-
quence models: A case study of Chinese part-of-
speech tagging. In Proceedings of the International
Joint Conference on Natural Language Processing
(IJCNLP), pages 180?188.
Ilya Sutskever, James Martens, George E. Dahl, and
Geoffrey E. Hinton. 2013. On the importance of
initialization and momentum in deep learning. In
Proceedings of the International Conference on Ma-
chine Learning (ICML), pages 1139?1147.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-
word scale unlabeled data. In Proceedings of the An-
nual Meeting of the Association for Computational
Linguistics, Proceedings of the Conference (ACL),
pages 665?673.
949
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-
of-speech tagging with a cyclic dependency net-
work. In Proceedings of the Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT), pages 252?259.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy for
tagging sequence data. In Proceedings of Human
Language Technology Conference and Conference
on Empirical Methods in Natural Language Pro-
cessing (HLT-EMNLP), pages 467?474.
Joseph P. Turian, Lev-Arie Ratinov, and Yoshua Ben-
gio. 2010. Word representations: A simple and gen-
eral method for semi-supervised learning. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics, Proceedings of the
Conference (ACL), pages 384?394.
Mengqiu Wang and Christopher D. Manning. 2013.
Effect of non-linear deep architecture in sequence
labeling. In Proceedings of the International Joint
Conference on Natural Language Processing (IJC-
NLP).
Xiaohui Zhang, Jan Trmal, Daniel Povey, and San-
jeev Khudanpur. 2014. Improving deep neural
network acoustic models using generalized maxout
networks. In Proceedings of International Confer-
ence on Acoustics, Speech and Signal Processing
(ICASSP).
Alisa Zhila, Wen tau Yih, Christopher Meek, Geoffrey
Zweig, and Tomas Mikolov. 2013. Combining het-
erogeneous models for measuring relational similar-
ity. In Proceedings of the Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT), pages 1000?1009.
950
Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 2?12,
October 29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Learning from a Neighbor: Adapting a Japanese Parser for Korean
through Feature Transfer Learning
Hiroshi Kanayama
IBM Research - Tokyo
Koto-ku, Tokyo, Japan
hkana@jp.ibm.com
Youngja Park
IBM Research - T.J.Watson Research Center
Yorktown Heights, NY, USA
young park@us.ibm.com
Yuta Tsuboi
IBM Research - Tokyo
Koto-ku, Tokyo, Japan
yutat@jp.ibm.com
Dongmook Yi
Korea Software Solutions Laboratory, IBM Korea
Gangnam-gu, Seoul, Korea
dmyi@kr.ibm.com
Abstract
We present a new dependency parsing
method for Korean applying cross-lingual
transfer learning and domain adaptation
techniques. Unlike existing transfer learn-
ing methods relying on aligned corpora or
bilingual lexicons, we propose a feature
transfer learning method with minimal su-
pervision, which adapts an existing parser
to the target language by transferring the
features for the source language to the tar-
get language. Specifically, we utilize the
Triplet/Quadruplet Model, a hybrid pars-
ing algorithm for Japanese, and apply a
delexicalized feature transfer for Korean.
Experiments with Penn Korean Treebank
show that even using only the transferred
features from Japanese achieves a high
accuracy (81.6%) for Korean dependency
parsing. Further improvements were ob-
tained when a small annotated Korean cor-
pus was combined with the Japanese train-
ing corpus, confirming that efficient cross-
lingual transfer learning can be achieved
without expensive linguistic resources.
1 Introduction
Motivated by increasing demands for advanced
natural language processing (NLP) applications
such as sentiment analysis (Pang et al., 2002;
Nasukawa and Yi, 2003) and question answer-
ing (Kwok et al., 2001; Ferrucci et al., 2010), there
is a growing need for accurate syntactic parsing
and semantic analysis of languages, especially for
non-English languages with limited linguistic re-
sources. In this paper, we propose a new depen-
dency parsing method for Korean which requires
minimal human supervision. Dependency parsing
can handle long-distance relationships and coor-
dination phenomena very well, and has proven to
be very effective for parsing free-order languages
such as Korean and Japanese (K?ubler et al., 2009).
Most statistical parsing methods rely on anno-
tated corpora labeled with phrase structures or
dependency relationships, but it is very expen-
sive to create a large number of consistent anno-
tations. Recently, treebanks have become avail-
able for many languages such as English, Ger-
man, Arabic, and Chinese. However, the pars-
ing results on these treebanks vary a lot depend-
ing on the size of annotated sentences and the type
of annotations (Levy and Manning, 2003; Mc-
Donald et al., 2013). Further, many languages
lack annotated corpus, or the size of the anno-
tated corpus is too small to develop a reliable sta-
tistical method. To address these problems, there
have been several attempts at unsupervised pars-
ing (Seginer, 2007; Spitkovsky et al., 2011), gram-
mar induction (Klein and Manning, 2004; Naseem
et al., 2010), and cross-lingual transfer learning
using annotated corpora of other languages (Mc-
Donald et al., 2011). However, the accuracies of
unsupervised methods are unacceptably low, and
results from cross-lingual transfer learning show
different outcomes for different pairs of languages,
but, in most cases, the parsing accuracy is still low
for practical purposes. A recent study by McDon-
ald et al. (2013) concludes that cross-lingual trans-
fer learning is beneficial when the source and tar-
get languages were similar. In particular, it reports
that Korean is an outlier with the lowest scores
(42% or less in UAS) when a model was trained
from European languages.
In this paper, we present a new cross-lingual
2
transfer learning method that learns a new model
for the target language by transferring the fea-
tures for the source language. Unlike other ap-
proaches which rely on aligned corpora or a
bilingual lexicon, we learn a parsing model for
Korean by reusing the features and annotated
data used in the Japanese dependency parsing,
the Triplet/Quadruplet Model (Kanayama et al.,
2000), which is a hybrid approach utilizing both
grammatical knowledge and statistics.
We exploit many similarities between the two
languages, such as the head-final structure, the
noun to verb modification via case and topic mark-
ers, and the similar word-order constraints. It was
reported that the mapping of the grammar formal-
ism in the language pair was relatively easy (Kim
et al., 2003b; Kim et al., 2003a). However, as the
two languages are classified into independent lan-
guage families (Gordon and Grimes, 2005), there
are many significant differences in their morphol-
ogy and grammar (especially in the writing sys-
tems), so it is not trivial to handle the two lan-
guages in a uniform way.
We show the Triplet/Quadruplet Model is suit-
able for bilingual transfer learning, because the
grammar rules and heuristics reduce the number
of modification candidates and can mitigate the
differences between two languages efficiently. In
addition, this model can handle the relationships
among the candidates as a richer feature space,
making the model less dependent upon the lexi-
cal features of the content words that are difficult
to align between the two languages. Similarly to
the delexicalized parsing model in (McDonald et
al., 2011), we transfer only part-of-speech infor-
mation of the features for the content words. We
create new mapping rules to extract syntactic fea-
tures for Korean parsing from the Japanese anno-
tated corpus and refine the grammar rules to get
closer modification distributions in two languages.
Our experiments with Penn Korean Tree-
bank (Han et al., 2002) confirm that the
Triplet/Quadruplet Model adapted for Korean out-
performs a distance-based dependency parsing
method, achieving 81.6% accuracy when no an-
notated Korean corpus was used. Further perfor-
mance improvements were obtained when a small
size of annotated Korean corpus was added, con-
firming that our algorithm can be applied with-
out more expensive linguistic resources such as an
aligned corpora or bilingual lexicons. Moreover,
the delexicalized feature transfer method enables
the algorithm applicable to any two languages that
have similar syntactic structures.
2 Related Work
2.1 Parsing for Korean
Since Korean is a morphologically-rich language,
many efforts for Korean parsing have focused
on automatically extracting rich lexical informa-
tion such as the use of case frame patterns for
the verbs (Lee et al., 2007), the acquisition of
case frames and nominal phrases from raw cor-
pora (Park et al., 2013), and effective features
from phrases and their neighboring contexts (Choi
and Palmer, 2011). Recently, Choi et al. (2012)
discussed the transformation of eojeol-based Ko-
rean treebank to entity-based treebank to effec-
tively train probabilistic CFG parsers. We apply
similar techniques as in (Choi et al., 2012) to miti-
gate the differences between Korean and Japanese
syntactic structures.
Chung and Rim (2003) applied the
Triplet/Quadruplet Model for Korean parsing
as done in our work. They reported that the model
performed well for long-distance dependencies,
but, in their experiments, the number of modi-
fication candidates was not effectively reduced
(only 91.5% of phrases were in one of the three
positions, while it was 98.6% in Kanayama?s
work for Japanese). In this paper, we introduce
more sophisticated grammatical knowledge and
heuristics to have similar dependency distribu-
tions in the two languages. Smith and Smith
(2004) attempted a bilingual parsing for English
and Korean by combining statistical dependency
parsers, probabilistic context-free grammars, and
word translation models into a unified framework
that jointly searches for the best English parse,
Korean parse and word alignment. However, we
utilize an existing parser and align the features
from the source language to the features for
the target language, and, thus, our method is
applicable to situations where there is no aligned
corpora or word translation models.
2.2 Transfer learning and domain adaptation
Recently, transfer learning has attracted much at-
tention, as it can overcome the lack of training
data for new languages or new domains for both
classification and regression tasks (Pan and Yang,
2010). Transfer learning has also been applied to
3
???
??/NNC?/PCA
?
?/VV?/EAN
????
???/NPR?/PAN
??
??/NNC
???
??/NNC?/PCA
????
??/NNC??/PAD
???
??/VV?/ECS
??
?/VX?/EFN
?
./SFN
?wife-NOM?
?buy-PAST?
?France-GEN? ?travel?
?bag-ACC?
?friend-DAT? ?show? ?want? ?.?
e
1
e
2
e
3
e
4
e
5
e
6
e
7
e
8
e
9
? ??? ?? ? ?
Figure 1: An example of dependency structures of a Korean sentence ?????????????
? ???? ??? ??.? (?(I) want to show the French travel bag which (my) wife bought to (my)
friend?). Each box corresponds to a Korean phrasal unit eojeol.
??
?/n?/pc
???
?/v?/aux
?????
????/np?/pn
??????
??/n???/n?/pc
???
??/n?/pc
?????
??/v??/aux?/
?wife-NOM?
?buy-PAST?
?France-GEN?
?travel bag-ACC?
?friend-DAT? ?want to show?
b
1
b
2
b
3
b
4
b
5
b
6
? ?? ??
Figure 2: A dependency structure of a Japanese sentence which corresponds to the Korean sentence in
Figure 1, ??????????????????????????. Each box corresponds to a Japanese
phrasal unit bunsetsu.
syntactic parsing, where a parsing model for a tar-
get language is learned from linguistic resources
in one or more different languages (Hwa et al.,
2005; Zeman and Resnik, 2008; McDonald et al.,
2011; Durrett et al., 2012; Georgi et al., 2012;
Naseem et al., 2012). McDonald et al. (2011)
proposed a delexicalized parsing model for cross-
lingual dependency parsing and demonstrated that
a high accuracy parsing was achieved for Indo-
European languages where significant amount of
parallel texts exist. However, in more recent work,
McDonald et al. (2013) showed that, unlike trans-
fer learning within close language families, build-
ing a Korean parser from European languages was
not successful with a very low accuracy. Durrett
et al. (2012) and Georgi et al. (2012) show that
transfer parsing can be improved when additional
bilingual resources are available, such as bilingual
dictionaries and parallel corpora of glossed texts
respectively.
Our method does not require such resources and
does not have restrictions on the sentence type that
can be parsed. Instead, we use a mixture of a
small corpus in a target language (i.e., Korean) and
a larger corpus of a source language (Japanese).
This task is similar to domain adaptation, and our
objective is to outperform the training model built
on each language separately. To avoid the loss of
accuracy due to the differences between two do-
mains, we apply the domain adaptation technique
proposed by Daum?e III (2007) which duplicates
the feature space into three categories with each
of the features trained by source, by target, and by
combined domains.
3 Dependency Structures of Korean and
Japanese
A dependency structure in Korean is typically an-
alyzed in terms of eojeol units, a basic phrase
that consists of a content word agglutinated with
optional functional morphemes such as postposi-
tional particles or endings for verbs. Figure 1
shows an example Korean sentence with the de-
pendencies between the eojeols indicated by ar-
rows. Figure 2 illustrates the dependency struc-
ture between the bunsetsus in Japanese that cor-
responds to the Korean structure in Figure 1. As
these figures show, the syntactic structures are
quite similar in these languages: All of the de-
pendencies are directed from left to right, and the
postpositional particles determine if the content
word modifies a verb (??? in e
1
and ??? in b
1
)
or a noun (??? in e
3
and ??? in b
3
). The eojeols
in Korean roughly correspond to the bunsetsus in
Japanese. In the remainder of this paper, we de-
note both an eojeol or a bunsetsu as a ?PU? (phrasal
unit) when distinction is not needed.
While Korean and Japanese have similar syn-
tactic structures, the two languages have many dif-
ferences. The eojeols in Korean are separated by
white space, while the bunsetsus in Japanese are
not. Further, the statistics show several differences
in the two languages. Table 1 compares a Korean
corpus, Penn Korean Treebank (henceforth KTB)
4
Table 1: Statistics of Korean and Japanese corpora.
KTB (Korean) EDR (Japanese)
Average number of characters (except for whitespace) per sentence 73.7 28.0
Average number of PUs per sentence 25.5 8.53
Average number of morphemes per PU 1.83 2.86
Ratio of modification to the next PU 70.0% 61.8%
Table 2: Simplified examples of Japanese grammar rules.
Rightmost morpheme of the modifier PU Conditions for the modified PUs
postpositional ??? wo (accusative) verb, adjective
postpositional ??? no (genitive, nominative) noun, verb, adjective
postpositional ??? to (conjunctive) noun, verb, adjective, adverb ????? isshoni (?together?)
adverb verb, adjective, adverb, copula
(Han et al., 2002), and a Japanese corpus, EDR
Corpus (EDR, 1996). Both corpora consist of
word-level bracketed constituents, so they are con-
verted into PU-level dependency structures using
the method described in Choi and Palmer (2011).
Though both corpora consist mainly of newspaper
or magazine articles, the sentences are not aligned
with each other, so the statistics show the compar-
isons of the two corpora, rather than the theoret-
ical comparisons of the two languages. However,
we can see that Korean sentences tend to be longer
than Japanese sentences both in terms of the num-
ber of characters and PUs. More eojeols modify an
adjacent eojeol in Korean than in Japanese. For in-
stance, e
1
, e
4
, e
6
, e
7
, and e
8
modify the next eojeol
in Figure 1, but only b
1
, b
3
, and b
5
modify the next
bunsetsu in Figure 2. Those differences suggest
some of the difficulties in applying the Japanese
dependency model to Korean. The Japanese pars-
ing method that will be described in the next sec-
tion exploits these characteristics, which we apply
to Korean parsing.
4 Triplet/Quadruplet Model
This section describes the Triplet/Quadruplet
Model (Kanayama et al., 2000) which was origi-
nally designed for Japanese parsing. First, we re-
view the two main ideas of the model ? restriction
of modification candidates and feature selection
for probability calculation. Then, we describe how
we apply the Triplet/Quadruplet Model to Korean
parsing in Section 4.3.
4.1 Restriction of modification candidates
The Triplet/Quadruplet Model utilizes a small
number (about 50) of hand-crafted grammar rules
that determine whether a PU can modify each PU
to its right in a sentence. The main goal of the
grammar rules is to maximize the coverage, and
the rules are simple describing high-level syntac-
tic dependencies, and, thus, the rules can be eas-
ily created without worrying about the precision
or contradictory rules. The statistical information
is later used to select the right rules for a given
sentence to produce an accurate parsing result. Ta-
ble 2 shows several grammar rules for Japanese, in
which the modified PUs are determined depend-
ing on the conditions of the rightmost morpheme
in the modifier PU.
An analysis of the EDR corpus shows that
98.6% of the correct dependencies are either the
nearest PU, the second nearest PU, or the farthest
PU from the modifier (more details in Table 4(a)).
Therefore, the model can be simplified by restrict-
ing the candidates to these three candidates and
by ignoring the other PUs with a small sacrifice
(1.4%) of parsing accuracy.
4.2 Calculation of modification probabilities
Let u be a modifier PU in question, c
un
the u?s n-
th modification candidate PU, ?
u
and ?
c
un
the at-
tributes of u and c
un
, respectively. Then the prob-
ability that u modifies its n-th candidate is calcu-
lated by the triplet equation (1) or the quadruplet
equation (2) when u has two or three candidates,
respectively
1
.
P (u ? c
un
) = P (n | ?
u
,?
c
u1
,?
c
u2
) (1)
P (u ? c
un
) = P (n | ?
u
,?
c
u1
,?
c
u2
,?
c
u3
) (2)
1
It is trivial to show that P (u ? cu
1
) = 1, when u has
only one candidate.
5
Table 3: Simplified examples of Korean grammar rules.
Rightmost morpheme of the modifier PU Conditions for the modified PUs
PCA,PCJ,PAD,PAU (postpositional particles) V* (verb, adjective or auxiliary), CO (copula)
EAN (nominal verb ending e.g. ??? eun) N* (noun)
ADV (adverb), ADC (conjunction) N* (noun), V* (verb, adjective or auxiliary), ADV (adverb), ADC (conjunction)
postpositional ??? gwa (conjunctive) N* (noun), V* (verb, adjective or aux), adverb ???? hamkke (?together?)
N* (noun) N* (noun), V* (verb, adjective or auxiliary)
Table 4: Distribution (percentage) of the position of the correct modified PU among the candidate PUs
selected by the initial grammar rules. The column ?Sum? shows the coverage of the 1st, 2nd and last
PUs. The EDR corpus was used for Japanese, and the KTB was used for Korean in this analysis.
(a) Japanese
# of Candidates Ratio 1st 2nd Last Sum
1 32.7 100.0 ? ? 100.0
2 28.1 74.3 26.7 ? 100.0
3 17.5 70.6 12.6 16.8 100.0
4 9.9 70.4 11.1 13.8 95.3
?5 11.8 70.2 11.1 10.5 91.9
Total 100 ? ? ? 98.6
(b) Korean (with the initial grammar)
# of Candidates Ratio 1st 2nd Last Sum
1 10.5 100.0 ? ? 100.0
2 11.4 85.9 14.1 ? 100.0
3 10.4 76.2 13.4 10.4 100.0
4 9.3 74.7 11.3 8.0 93.9
?5 58.4 75.5 10.0 4.9 90.5
Total 100 ? ? ? 93.9
These probabilities are estimated by the maxi-
mum entropy method with a feature set to express
? and ?. Assuming the independence of those
modifications, the probability of the dependency
tree for an entire sentence P (T ) is calculated as
the product of the probabilities of all of the depen-
dencies in the sentence using beam search to max-
imize P (T ) under the constraints of the projected
structure.
P (T ) ?
?
u
P (u ? c
un
) (3)
In comparison, a traditional statistical parser
(Collins, 1997) uses Equation (4) to calculate the
probability of u modifying t.
P (u ? t) = P (True | ?
u
,?
t
,?
u,t
) (4)
We call the model based on Equation (4) the Dis-
tance Model, since ?
u,t
(the distance between u
and t) is typically used as the key feature. Though
other contextual information, in addition to the at-
tributes of u and t, can be added, the model calcu-
lates the probabilities of the dependencies between
u and t independently and thus often fails to incor-
porate appropriate contextual information.
Equations (1) and (2) have two major advan-
tages over the Distance Model: First, all the at-
tributes of the modifier and its candidates can be
handled simultaneously. The combination of those
attributes helps the model to express the context of
the modifications. Second, the probability of each
modification is calculated based on the relative po-
sitions of the candidates, instead of the distance
from the modifier PU in the surface sentence, and,
thus, the model is more robust.
4.3 Korean dependency parsing with the
Triplet/Quadruplet Model
We design the Korean parser by adapting the
Triplet/Quadruplet Model based on the analogous
characteristics of Japanese and Korean. First, we
created the Korean grammar rules for generating
candidate modified PUs by modifying the rules
for Japanese shown in Table 2 for Korean. The
rule set, containing fewer than 50 rules, is sim-
ple enough to be created manually, because the
rules simply describe possible dependencies, and
Japanese phenomena are good hints for Korean
phenomena. Table 3 shows some examples of the
rules for Korean based on the POS schema used in
the KTB corpus. We did not automatically extract
the rules from the annotated corpora so that the
rules are general and independent of the training
corpus. Nonetheless, 96.6% of the dependencies
in KTB are covered by the grammar rules. The re-
maining dependencies (3.4%) not covered by the
rule set are mainly due to rare modifications and
may indicate inconsistencies in the annotations,
so we do not seek any grammar rules to achieve
nearly 100%.
6
??? (?wife-NOM?) ? (?buy?) ??? (?show?) ?? (?want?)
e
1
e
2
e
7
e
8
? ? ?
?1? NNC (common noun)
?2? PCA (postpositional)
?3???? (?-NOM?)
?5? VV (verb)
?7? EAN (adnominal ending)
?8???? (past adnominal)
?5? VV (verb)
?7? ECS (ending)
?8???? (conjunctive)
?5? VX (auxiliary)
?7? EFN (final ending)
?8???? (predicative)
Figure 3: The features used to select the modified PU of e
1
among its three candidates. The full sentence
of this example is shown in Figure 1. The numbers in brackets correspond to the feature IDs in Table 5.
Table 5: The features to express attributes of a modifier and modification candidates.
Feature set ID Description
?1? PoS of the head morpheme of the modifier
?2? PoS of the last morpheme of the modifier
?3? Lex of the postpositional or endings of the modifier
?4? Lex of the adverb of the modifier
?5? PoS of the head morpheme of the modification candidate
?6? Lex of the head morpheme of the modification candidate
?7? PoS of the last morpheme of the modification candidate
?8? Lex of the postpositional or endings of the modification candidate
?9? Existence of a quotation expression ???? dago or ???? rago
?10? Number of ??? eun (TOPIC marker) between the modifier and modification candidate
?11? Number of commas between the modifier and modification candidate
combination ?1? ? ?5? / ?2? ? ?5? / ?2? ? ?7? / ?3? ? ?5? / ?3? ? ?8?
Table 4(a) and (b) show the distribution of the
numbers of candidate PUs and the position of the
correct modified PUs obtained from the analysis
of the EDR corpus and the KTB corpus respec-
tively. As we can see, the first candidate is pre-
ferred in both languages, but the preference of the
nearer candidate is stronger in Korean. For in-
stance, when there are more than one candidates,
the probability that the first candidate is the cor-
rect one is 78% for Korean but 71% for Japanese.
Further, when there are more than 2 candidates,
Japanese prefers the last candidate, while Korean
prefers the second candidate. Based on the analy-
sis results, the number of modification candidates
is restricted to at most three (the first, second and
last candidates) for Korean as well.
The next step is to design ?
u
and ?
c
un
, which
are required in Equations (1) and (2) to choose the
correct modified PU. We converted the feature set
from the Japanese study to get the Korean features
as listed in Table 5. For example, to find the mod-
ified PU of e
1
????? anae-ga (?wife-NOM?) in
the sentence shown in Figure 1, the attributes of
e
1
and the attributes of the three candidates, e
2
,
e
7
, and e
8
, are extracted as shown in Figure 3, and
their attributes are used to estimate the probability
of each candidate in Equation (2).
5 Adaptation for Bilingual Transfer
Learning
In Section 4.3, we explained how the
Triplet/Quadruplet Model can be used for
Korean. In this section, we describe the feature
adaption techniques in more detail and investigate
if the new model with transferred features works
well when a small amount of annotated corpus for
the target language is provided. Further, we study
if we can leverage the annotated corpus for the
source language in addition to the parsing model
and train a model for the target language using the
training data for the source language.
5.1 Feature Transfer
With the assumption that Korean and Japanese
have similar syntactic dependencies, we adopt
the delexicalized parsing model presented in Mc-
Donald et al. (2011). We transfer the part-of-
speech (POS) in the Japanese features to the POS
scheme in the KTB corpus, and translate Japanese
functional words to the corresponding functional
words in Korean. This transfer process is manda-
tory because we use the language specific POS
systems to capture language-specific dependency
phenomena, unlike other works using language
universal but coarser POS systems.
We do not transfer lexical knowledge on con-
7
Table 6: Example of mapping rules for parts-of-speech and functional words.
Japanese PoS Korean PoS
common noun NNC
verb VV
adjective VJ
nominal suffix XSF
case particle
???,???,???,???? PAD
others PCA
Japanese particle Korean particle
??? wo (?-ACC?) ??? eul
???? yori (?from?) ???? buteo
??? ha (?-TOPIC?) ??? eun
??? mo (?too?) ??? do
??? ga case particle (?-NOM?) ??? i
conjunctive particle (?but?) ???? jiman
tent words and exceptional cases, so feature sets
?4? and ?6? are not transferred. Table 6 shows
some examples of the feature transfer which han-
dle POS tags and functional words. We note that
the Korean features shown in Figure 3 are directly
extracted from Japanese corpus using those rules.
5.2 Adaptation of parsing rules
While Japanese and Korean are similar in terms of
syntactic dependencies, there are significant dif-
ferences between the two languages in the distri-
bution of modification as shown in the Table 4(a)
and (b): In Korean, more than half of modifiers
have 5 or more candidates, while only 12% of
Japanese modifiers do. In Japanese, 98.6% of cor-
rect modified PUs are located in one of the three
positions (1st, 2nd or last), but, in Korean, the ratio
falls to 93.9% as shown in Table 4. Another ma-
jor difference of the two languages is the different
average numbers of PUs per sentence as shown in
Table 1. Korean has 25.5 PUs per sentence, while
the number is only 8.5 in Japanese. This is mainly
caused by the difference between eojeol in Korean
and bunsetsu in Japanese. In Japanese, compound
nouns and verb phrases with an auxiliary verb are
likely to form a single bunsetsu, while, in Korean,
they are split into multiple eojeols with a whites-
pace in-between.
These differences significantly reduce the ef-
fect of transfer learning. To address these prob-
lems, we further refine the grammar rules as in
the following. We added heuristic rules for the
Korean model to effectively reduce the number of
candidates in compound nouns which consist of a
noun sequence in multiple eojeols, and verbs or
adjectives followed by auxiliary verbs. Figure 4
shows an algorithm to reduce the number of mod-
ified PUs considering the structure of compound
nouns. In this example, both PUs e
4
(?travel?) and
e
5
(?bag-ACC?) can be candidate PUs for eojeol
e
3
. However, based on the rule in Figure 4, e
4
and
e
5
are considered as a compound noun (line 1),
and e
4
is determined to modify e
5
(line 3). Sub-
sequently, e
3
?s modifiability to e
4
is rejected (line
5), and, thus, the correct modified PU of e
3
is de-
termined as e
5
. After refining the rules for com-
pound nouns and auxiliary verbs, the probability
of the correct modified PU being the 1st, 2nd or
last candidate PU increases from 93.9% to 96.3%
as shown in Table 7, and the distribution of the
candidate?s positions for Korean became closer to
the Japanese distribution shown in Table 4(a).
5.3 Learning from heterogeneous bilingual
corpora
The feature transfer and rule adaptation methods
described in previous sections generate a very ac-
curate Korean parser using only a Japanese cor-
pus as shown in the first row in Table 8. The next
question is if we can leverage bilingual corpora to
further improve the accuracy, when annotated cor-
pus for the target language (Korean) is available.
We note that the two corpora do not need to be
aligned and can come from different domains. To
mitigate the side effects of merging heterogeneous
training data in different languages, we apply the
domain adaptation method proposed by Daum?e III
(2007) and augment the feature set to a source
language-specific version, a target-specific version
and a general version. Specifically, a feature set x
in Table 5 is expanded as follows:
x
K
=< x,0,x > (5)
x
J
=< 0,x,x > (6)
where x
K
and x
J
denote the feature sets extracted
from the Korean corpus and the Japanese corpus
respectively. Then, the features specific to Korean
and Japanese get higher weights for the first part
or the second part respectively, and the character-
istics existing in both languages influence the last
part.
8
if PoS of u
i
?s last morpheme is N* and PoS of u
i+1
?s first morpheme is N*
then
u
i
must modify u
i+1
if u
i?1
?s last morpheme is not ??? then u
i?1
cannot modify u
i+1
else u
i?1
cannot modify u
i
u
1
to u
i?2
cannot modify u
i
????
???/NPR?/PAN
??
??/NNC
???
??/NNC?/PCA
?France-GEN? ?travel?
?bag-ACC?
e
3
e
4
e
5
?? ?
Figure 4: Heuristic rules to reduce the number of modification candidates surrounding compound nouns
in Korean. The example in the right figure shows that candidates in the dotted lines are removed by the
heuristics.
Table 7: Distribution of the position of correct modified PU for Korean after the refinement of the Korean
grammar rules.
# of candidates Ratio 1st 2nd Last Sum
1 46.4% 100.0% ? ? 100.0%
2 9.8% 79.0% 21.0% ? 100.0%
3 9.2% 75.5% 12.7% 11.8% 100.0%
4 8.0% 71.0% 11.8% 9.6% 92.4%
? 5 26.6% 70.4% 10.1% 7.8% 88.3%
Total 100% ? ? ? 96.3%
6 Experiments
In this section, we validate the effectiveness of
learning a Korean parser using the feature transfer
learning from the Japanese parser and compare the
Korean model with other baseline cases. We also
compare the parsing results when various sizes of
bilingual corpora were used to train the Korean
model.
6.1 Korean parsing using the
Triplet/Quadruplet Model
First, to validate the effectiveness of the
Triplet/Quadruplet Model for parsing Korean, we
built eight Korean dependency parsing models us-
ing different numbers of training sentences for Ko-
rean. The KTB corpus Version 2.0 (Han et al.,
2002) containing 5,010 annotated sentences was
used in this study. We first divide the corpus into 5
subsets by putting each sentence into its (sentence
ID mod 5)-th group. We use sentences from the
first subgroup for estimating the parameters, sen-
tences from the second subgroup for testing, and
use the remaining three subgroups for training. We
built 8 models in total, using from 0 sentence up
to 3,006 sentences selected from the training set.
The number of training sentences in each model
is shown in the first column in Table 8. The pa-
rameters were estimated by the maximum entropy
method, and the most preferable tree is selected
using each dependency probability and the beam
search. The test data set contains 1,043 sentences.
We compare the Triplet/Quadruplet Model-
based models with the Distance Model. For the
Distance Model, we used the same feature set as
in Table 5, and added the distance feature (?
u,t
)
by grouping the distance between two PUs into 3
categories (1, 2 to 5, and 6 or more). The perfor-
mances are measured by UAS (unlabeled attach-
ment score), and the results of the two methods
are shown in the second column, where Japanese
Corpus Size=0, in Table 8 (a) and (b) respectively.
The top leftmost cells (80.61% and 71.63%) show
the parsing accuracies without any training cor-
pora. In these cases the nearest candidate PU is
selected as the modified PU. The difference be-
tween two models suggests the effect of restriction
of modification candidates by the grammar rules.
We note that the Triplet/Quadruplet Model pro-
duces more accurate results and outperforms the
Distance Model by more than 2 percentage points
in all cases. The results confirm that the method
for Japanese parsing is suitable for Korean pars-
ing.
6.2 Results of bilingual transfer learning
Next, we evaluate the transfer learning when anno-
tated sentences for Japanese were also added. Ta-
ble 8(a) shows the accuracies of our model when
various numbers of training sentences from Ko-
rean and Japanese are used. The first row shows
the accuracies of Korean parsing when the models
were trained only with the Japanese corpus, and
9
Table 8: The accuracy of Korean dependency parsing with various numbers of annotated sentences in the
two languages.
?
denotes that the mixture of bilingual corpora significantly outperformed (p < .05)
the parser trained with only the Korean corpus without Japanese corpus.
(a) Triplet/Quadruplet Model
Japanese Corpus Size
0 2,500 5,000 10,000
K
o
r
e
a
n
C
o
r
p
u
s
S
i
z
e
0 80.61% 80.78% 81.23%
?
81.58%
?
50 82.21% 82.32% 82.40%
?
82.43%
?
98 82.36% 82.66%
?
82.69%
?
82.70%
?
197 83.13% 83.18% 83.30%
?
83.28%
383 83.62% 83.92%
?
83.94%
?
83.91%
?
750 84.03% 84.00% 84.06% 84.06%
1,502 84.41% 84.34% 84.32% 84.28%
3,006 84.77% 84.64% 84.64% 84.65%
(b) Distance Model
Japanese Corpus Size
0 2,500 5,000
K
o
r
e
a
n
C
o
r
p
u
s
S
i
z
e
0 71.63% 62.42% 54.92%
50 79.31% 79.55%
?
79.54%
?
98 80.53% 80.63%
?
80.72%
?
197 80.91% 80.84% 80.85%
383 81.86% 81.75% 81.76%
750 82.10% 81.92% 81.94%
1,502 82.50% 82.48% 82.50%
3,006 82.66% 82.57% 82.54%
other rows show the results when the Korean and
Japanese corpora were mixed using the method
described in Section 5.3.
As we can see from the results, the bene-
fit of transfer learning is larger when the size
of the annotated corpus for Korean (i.e., target
language) is smaller. In our experiments with
Triplet/Quadruplet Model, positive results were
obtained by the mixture of the two languages when
the Korean corpus is less than 500 sentences, that
is, the annotations in the source language success-
fully compensated the small corpus of the target
language. When the size of the Korean corpus is
relatively large (? 1, 500 sentences), adding the
Japanese corpus decreased the accuracy slightly,
due to syntactic differences between the two lan-
guages. Also the effect of the corpus from the
source language tends to saturate as the size of
the source corpus, when the target corpus is larger.
This is mainly because our mapping rules ignore
lexical features, so few new features found in the
larger corpus were incorrectly processed.
When merging the corpus in two languages,
if we simply concatenate the transferred features
from the source language and the features from
the target language (instead of using the dupli-
cated features shown in Equations (5) and (6)), the
accuracy dropped from 82.70% to 82.26% when
the Korean corpus size was 98 and Japanese cor-
pus size was 10,000, and from 83.91% to 83.40%
when Korean=383. These results support that
there are significant differences in the dependen-
cies between two languages even if we have im-
proved the feature mapping, and our approach
with the domain adaptation technique (Daum?e III,
2007) successfully solved the difficulty.
Table 8(b) shows the results of the Distance
Model. As we can see from the first row, using
only the Japanese corpus did not help the Dis-
tance Model at all in this case. The Distance
Model was not able to mitigate the differences be-
tween the two languages, because it does not use
any grammatical rules to control the modifiability.
This demonstrates that the hybrid parsing method
with the grammar rules makes the transfer learn-
ing more effective. On the other hand, the domain
adaptation method described in (5) and (6) suc-
cessfully counteracted the contradictory phenom-
ena in the two languages and increased the accu-
racy when the size of the Korean corpus was small
(size=50 and 98). This is because the interactions
among multiple candidates which cannot be cap-
tured from the small Korean corpus were provided
by the Japanese corpus.
Some of previous work reported the parsing
accuracy with the same KTB corpus; 81% with
trained grammar (Chung et al., 2010) and 83%
with Stanford parser after corpus transformation
(Choi et al., 2012), but as Choi et al. (2012) noted
it is difficult to directly compare the accuracies.
6.3 Discussion
The analysis of e
2
?s dependency in Figure 1
is a good example to illustrate how the
Triplet/Quadruplet Model and the Japanese
corpus help Korean parsing. Eojeol e
2
has three
modification candidates, e
3
, e
5
, and e
6
. In the
10
Distance Model, e
3
is chosen because the distance
between the two eojeols (?
e
2
,e
3
) was 1, which
is a very strong clue for dependency. Also, in
the Triplet/Quadruplet Model trained only with
a small Korean corpus, e
3
received a higher
probability than e
5
and e
6
. However, when a
larger Japanese corpus was combined with the
Korean corpus, e
5
was correctly selected as the
Japanese corpus provided more samples of the
dependency relation of ?verb-PAST? (e
2
) and
?common noun-ACC (?)? (e
5
) than that of
?verb-PAST? and ?proper noun-GEN (?)? (e
3
).
As we can notice, larger contextual information
is required to make the right decision for this case,
which may not exist sufficiently in a small cor-
pus due to data sparseness. The grammar rules in
the Triplet/Quadruplet Model can effectively cap-
ture such contextual knowledge even from a rel-
atively small corpus. Further, since the grammar
rules are based only on part-of-speech tags and a
small number of functional words, they are sim-
ilar to the delexicalized parser (McDonald et al.,
2011). These delexicalized rules are more robust
to linguistic idiosyncrasies, and, thus, are more ef-
fective for transfer learning.
7 Conclusion
We presented a new dependency parsing algo-
rithm for Korean by applying transfer learning
from an existing parser for Japanese. Unlike other
transfer learning methods relying on aligned cor-
pora or bilingual lexical resources, we proposed a
feature transfer method utilizing a small number
of hand-crafted grammar rules that exploit syn-
tactic similarities of the source and target lan-
guages. Experimental results confirm that the fea-
tures learned from the Japanese training corpus
were successfully applied for parsing Korean sen-
tences and mitigated the data sparseness problem.
The grammar rules are mostly delexicalized com-
prising only POS tags and a few functional words
(e.g., case markers), and some techniques to re-
duce the syntactic difference between two lan-
guages makes the transfer learning more effective.
This methodology is expected to be applied to any
two languages that have similar syntactic struc-
tures, and it is especially useful when the target
language is a low-resource language.
References
Jinho D. Choi and Martha Palmer. 2011. Statistical
dependency parsing in Korean: From corpus gener-
ation to automatic parsing. In Proceedings of the
Second Workshop on Statistical Parsing of Morpho-
logically Rich Languages, pages 1?11.
DongHyun Choi, Jungyeul Park, and Key-Sun Choi.
2012. Korean treebank transformation for parser
training. In Proceedings of the ACL 2012 Joint
Workshop on Statistical Parsing and Semantic Pro-
cessing of Morphologically Rich Languages, pages
78?88.
Hoojung Chung and Heechang Rim. 2003. A
new probabilistic dependency parsing model for
head-final, free word order languages. IEICE
TRANSACTIONS on Information and Systems, E86-
1(11):2490?2493.
Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of korean parsing. In
Proceedings of the NAACL HLT 2010 First Work-
shop on Statistical Parsing of Morphologically-Rich
Languages, pages 49?57.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Compu-
tational Linguistics and Eighth Conference of the
European Chapter of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.
Hal Daum?e III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256?263.
Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syn-
tactic transfer using a bilingual lexicon. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1?11.
EDR. 1996. EDR (Japan Electronic Dictionary Re-
search Institute, Ltd.) electronic dictionary version
1.5 technical guide.
David A. Ferrucci, Eric W. Brown, Jennifer Chu-
Carroll, James Fan, David Gondek, Aditya Kalyan-
pur, Adam Lally, J. William Murdock, Eric Nyberg,
John M. Prager, Nico Schlaefer, and Christopher A.
Welty. 2010. Building Watson: An overview of the
DeepQA project. AI Magazine, 31(3):59?79.
Ryan Georgi, Fei Xia, and William D Lewis. 2012.
Improving dependency parsing with interlinear
glossed text and syntactic projection. In Proceed-
ings of COLING 2012, pages 371?380.
Raymond GGordon and Barbara F Grimes. 2005. Eth-
nologue: Languages of the world, volume 15. SIL
international Dallas, TX.
11
Chung-hye Han, Na-Rae Han, Eon-Suk Ko, Heejong
Yi, and Martha Palmer. 2002. Penn Korean tree-
bank: Development and evaluation. In Proc. Pacific
Asian Conf. Language and Comp.
Rebecca Hwa, Philip Resnik, and Amy Weinberg.
2005. Breaking the resource bottleneck for multi-
lingual parsing. Technical report, DTIC Document.
Hiroshi Kanayama, Kentaro Torisawa, Yutaka Mitsu-
ishi, and Jun?ichi Tsujii. 2000. A hybrid Japanese
parser with hand-crafted grammar and statistics. In
Proceedings of the 18th International Conference on
Computational Linguistics, pages 411?417.
Roger Kim, Mary Dalrymple, Ronald M Kaplan, and
Tracy Holloway King. 2003a. Porting grammars
between typologically similar languages: Japanese
to korean. In Proceedings of the 17th Pacific Asia
Conference on Language, Information.
Roger Kim, Mary Dalrymple, Ronald M Kaplan,
Tracy Holloway King, Hiroshi Masuichi, and
Tomoko Ohkuma. 2003b. Multilingual grammar
development via grammar porting. In ESSLLI 2003
Workshop on Ideas and Strategies for Multilingual
Grammar Development, pages 49?56.
Dan Klein and Christopher D Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, pages 478?487.
Sandra K?ubler, Ryan McDonald, and Joakim Nivre.
2009. Dependency parsing. Synthesis Lectures on
Human Language Technologies, 1(1):1?127.
Cody Kwok, Oren Etzioni, and Daniel S Weld. 2001.
Scaling question answering to the web. ACM Trans-
actions on Information Systems (TOIS), 19(3):242?
262.
Hyeon-Yeong Lee, Yi-Gyu Hwang, and Yong-Seok
Lee. 2007. Parsing of Korean based on CFG using
sentence pattern information. International Journal
of Computer Science and Network Security, 7(7).
Roger Levy and Christopher Manning. 2003. Is it
harder to parse chinese, or the chinese treebank? In
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, ACL, pages
439?446.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 62?72.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar
T?ackstr?om, et al. 2013. Universal dependency an-
notation for multilingual parsing. In Proceedings of
ACL 2013.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1234?1244.
Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective sharing for multilingual dependency
parsing. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 629?637.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Senti-
ment analysis: Capturing favorability using natural
language processing. In Proceedings of the Second
International Conferences on Knowledge Capture,
pages 70?77.
Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. Knowledge and Data Engineer-
ing, IEEE Transactions on, 22(10):1345?1359.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in nat-
ural language processing (EMNLP), pages 79?86,
Philadelphia, Pennsylvania.
Jungyeul Park, Daisuke Kawahara, Sadao Kurohashi,
and Key-Sun Choi. 2013. Towards fully lexicalized
dependency parsing for Korean. In Proceedings of
The 13th International Conference on Parsing Tech-
nologies.
Yoav Seginer. 2007. Fast unsupervised incremental
parsing. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 384?391.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using English to
parse Korean. In Proceedings of the 2004 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 49?56.
Valentin I Spitkovsky, Hiyan Alshawi, Angel X Chang,
and Daniel Jurafsky. 2011. Unsupervised depen-
dency parsing without gold part-of-speech tags. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 1281?
1290.
Daniel Zeman and Philip Resnik. 2008. Cross-
language parser adaptation between related lan-
guages. In IJCNLP, pages 35?42.
12
