Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 705?712
Manchester, August 2008
A Method for Automatic POS Guessing of Chinese Unknown Words 
Likun Qiu 
Department of Chinese Language and 
Literature, Peking University / No.5 Yi-
heyuan Road, Haidian District, Beijing, 
China 100871 
NEC Laboratories, China 
qiulk@pku.edu.cn 
Changjian Hu, Kai Zhao 
NEC Laboratories, China / 14F, Build-
ing.A, Innovation Plaza, No.1 
Tsinghua Science Park, Zhongguancun 
East Road, Haidian District, Beijing, 
China 100084 
{huchangjian, zhaokai} 
@research.nec.com.cn 
 
Abstract 
This paper proposes a method for auto-
matic POS (part-of-speech) guessing of 
Chinese unknown words. It contains two 
models. The first model uses a machine-
learning method to predict the POS of 
unknown words based on their internal 
component features. The credibility of 
the results of the first model is then 
measured. For low-credibility words, the 
second model is used to revise the first 
model?s results based on the global con-
text information of those words. The ex-
periments show that the first model 
achieves 93.40% precision for all words 
and 86.60% for disyllabic words, which 
is a significant improvement over the 
best results reported in previous studies, 
which were 89% precision for all words 
and 74% for disyllabic words. Further, 
the second model improves the results by 
0.80% precision for all words and 1.30% 
for disyllabic words. ? 
1 Introduction 
Since written Chinese does not use blank spaces 
to denote word boundaries, Chinese word seg-
mentation becomes an essential task for natural 
language processing, as in many other Asian lan-
guages (Thai, Japanese, Tibetan, etc.). It is diffi-
cult to build a complete dictionary comprising all 
words, for new words are constantly being cre-
ated. As such, unknown words may greatly influ-
ence the effectiveness of text processing. Studies 
                                                 
?2008. Licensed under the Creative Commons Attribution-
Noncommercial-Share Alike 3.0 Unported license 
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some 
rights reserved. 
on unknown words include detection, POS 
guessing, sense classification, etc. Current meth-
ods for automatic unknown word detection have 
been relatively successful and widely used in 
many systems, yet automatic POS guessing for 
unknown words still remains a challenge for 
natural language processing research. 
The task of POS guessing is quite different 
from traditional POS tagging. Traditional POS 
tagging involves assigning a single POS tag to a 
word token, provided that it is known what POS 
tag this word can take on in principle. This task 
requires a lexicon that lists possible POS tags for 
all words. However, unknown words are not in 
the lexicon, so the task of POS guessing of un-
known words involves the guessing of a correct 
POS for an unknown word from the whole POS 
set of the current language. Obviously, tradi-
tional methods of POS tagging cannot effectively 
solve the problem of POS guessing of unknown 
words. 
In previous work, two types of features have 
been used for the task of POS guessing of un-
known Chinese words. One type is contextual 
feature, including local contextual features and 
global contextual features, and the other is inter-
nal component feature. Previous work has mainly 
used context information to guess the POS tags 
of unknown Chinese words, while a few designs 
looked at internal component features. Although 
there have been some attempts to combine the 
two types of features together, no reasonable ex-
planation of the relationship between the two 
types of features has been given. 
It is well known that the properties of a struc-
ture always depend on its internal component 
structure. As such, it is natural for us to wonder 
whether models based on internal component 
features alone can perform the POS guessing task 
for unknown Chinese words with both high pre-
705
cision and high recall. Here we present a model 
based on internal component features of un-
known words using CRFs (conditional random 
fields). The results are very good, especially for 
multi-syllabic words (excluding disyllabic 
words). 
While in the previous model the precision of 
POS guessing of disyllabic words is relatively 
high, there is still much room for further im-
provement. Considering that the usages of a 
word in real text can show the properties of a 
word, and that these features may be a useful 
complement to internal component features, we 
designed a scheme to effectively utilize the two 
types of features together. In this scheme, credi-
bility scores for former guessing results are com-
puted, and only those words with relatively lower 
credibility scores are revised by the model based 
on global context information. The model based 
on global context information simulates the be-
havior of linguists in judging the POS of a word. 
Though the recall of the latter model is low, it 
can revise some incorrect guessing results of the 
initial model. 
According to Lu (2005), there are six main 
types of unknown Chinese words: 
(1) Abbreviation (acronym): e.g., ?? zhong-
mei (China-U.S). 
(2) Proper names (person?s name, place name, 
company name): e.g., ??? Wang Anshi 
(person?s name), ?? Penang (an island in 
Malaysia; place name), ?? Huawei (com-
pany name). 
(3) Derived words (words with affixes): e.g., ?
?? zong-jingli (general manager), ??? 
xiandai-hua (modernize). 
(4) Compounds: e.g., ??  huoyun (obtain 
permission), ?? nisha (mud), ?? tubian 
(sudden change) 
(5) Numeric compounds: e.g., ???? siqian 
riyuan (four thousand Japanese yen), ??
??? 2003nian (year 2003) 
(6) Reduplicated words: e.g., ? ? ? ? 
yingbuyinggai (should or should not), ??
?? chuchujinjin (go in and go out) 
Proper names and numeric compounds are all 
nouns, so they don?t have the problem of POS 
guessing. We will focus on abbreviation, derived 
words, compounds, and reduplicated words. 
The remainder of this paper is organized as 
follows: in Section 2, we introduce some previ-
ous work on POS guessing of unknown Chinese 
words. Sections 3, 4, and 5 describe our proposed 
method, which includes two models and an addi-
tional process linking the two models together. In 
detail, Section 3 considers POS guessing for un-
known Chinese words as a sequence-labeling 
problem and proposes a model based on internal 
component features to solve this task. In Section 
4, we compute a credibility score for each guess-
ing result based on the sequence type of the 
words? internal component structure. This links 
the model in Section 3 and that of Section 5 to-
gether. Section 5 describes a model based on 
global context information to revise the guessing 
results of the initial model that have relatively 
lower credibility scores. Section 6 shows the ex-
periments and results of our methods and a com-
parison with previous work. Section 7 presents 
our conclusions. 
2 Previous Work 
Considering the features used during POS guess-
ing, we have classified previous studies on POS 
guessing of unknown words into three types. 
The first type use only contextual features, in-
cluding local context and global context.  For 
example, Nakagawa and Matsumoto (2006) pro-
posed a probabilistic model to guess the POS 
tags of unknown words by considering all the 
occurrences of unknown words with the same 
lexical form in a document. The parameters were 
estimated using Gibbs sampling. They also at-
tempted to apply the model to semi-supervised 
learning, and conducted experiments on multiple 
corpora. The highest precision in the Chinese 
corpus of their experiments was 67.85%. 
The second type use only internal component 
features, such as that of Chen and Bai (1997) and 
Wu and Jiang (2000). Chen and Bai (1997) ex-
amined all unknown nouns, verbs, and adjectives 
and reported 69.13% precision using Dice met-
rics to measure the affix-category association 
strength and an affix-dependent entropy weight-
ing scheme for determining the weightings be-
tween prefix-category and suffix-category asso-
ciations. This approach is effective in processing 
derived words such as ??? xiandai-hua (mod-
ernize), but performs poorly when encountering 
compounds such as ?? baozhi (inflation-proof). 
Wu and Jiang (2000) calculated P(Cat,Pos,Len) 
for each character, where Cat is the POS of a 
word containing the character, Pos is the position 
of the character in that word, and Len is the 
length of that word. They then calculated the 
POS probabilities for each unknown word as the 
joint probabilities of P(Cat,Pos,Len) for its com-
706
ponent characters. This approach was applied to 
unknown nouns, verbs, and adjectives of two to 
four characters in length. This approach exhibits 
lower recall for multi-syllabic words even if the 
training corpus is significantly large.  
The third type attempt to combine internal 
component features and context information, 
such as that of Lu (2005) and Goh et al (2006). 
Lu (2005) describes a hybrid model that com-
bines a rule-based model with two statistical 
models for the task of POS guessing of unknown 
Chinese words. The rule-based model includes 
35 manual rules concerning the type, length, and 
internal structure of unknown words, and the two 
statistical models utilize context information and 
the likelihood for a character to appear in a par-
ticular position of words of a particular length 
and POS category, one of which is Wu and Ji-
ang?s (2000) model. It achieves a precision of 
89.00%, a significant improvement over the best 
result reported in previous studies, which was 
69.00%. Goh et al (2006) propose a method for 
guessing the part-of-speech tags of detected un-
known words using contextual and internal com-
ponent features with maximum entropy models. 
Both Lu (2005) and Goh et al (2006) use only 
local context and not global context. As far as 
internal component features are concerned, Lu 
(2005) uses only the word category feature in his 
rule-based model while Goh et al (2006) uses 
only the first character and last character features. 
From the above studies, we may find that meth-
ods based on internal component features are 
very promising, but this kind of features still 
needs much more attention. Moreover, none of 
them has proved that methods based on context 
information can improve the results of methods 
based on internal component features. They only 
attempted to utilize different types of features 
together and to give simultaneous results for both 
types of features. 
Our method is among the third type of studies, 
but is different from the rest in the scheme of 
combining the two types of features together. In 
our method, internal component features play a 
more important role. We will prove that a model 
based on this type of features alone can perform 
very well. The other type of features acts as a 
useful supplement and can improve the results of 
some words in a certain degree. The two models 
are linked together by assigning a credibility 
score for each POS guessing result generated by 
the initial model. The results with a relatively 
lower credibility score are identified and put 
through reconsideration by a method based on 
global context information. 
3 Model Based on Internal Component 
Features 
In this model, we consider the task of POS 
guessing of unknown words as a problem of se-
quence labeling. The inspiration for this ap-
proach came from our observations of how hu-
mans understand a word. Usually an unknown 
word is regarded by human as a sequence of 
characters or morphemes that can be partitioned 
into several segments, where each segment is 
relatively coherent in meaning. 
3.1 Conditional Random Fields 
In contrast with other models for labeling se-
quences, such as HMM and MEMM, CRFs are 
good at avoiding the label bias problem. They 
condition on the entire observation sequence, 
thus avoiding the need for independent assump-
tions between observations and vastly expanding 
the set of features that can be incorporated into 
the model without violating its assumptions. One 
of the main advantages of a conditional model is 
its ability to explore a diverse range of features 
relevant to a specific task. As many studies have 
shown, CRFs are the best models for solving the 
sequence labeling problem (Lafferty et al, 2001; 
Vail et al, 2007). So we chose to use CRFs to 
solve the POS guessing problem. 
3.2 The POS Guessing Model 
While training, we use the words of a dictionary 
as training data. Those words will be considered 
as sentences and then segmented and assigned 
POS-tags by a standard word segmentation and 
POS tagging tool. By training with this data, we 
will obtain a POS guessing model for unknown 
words. While testing, we still consider an un-
known word as a sentence and process it with the 
same tool. 
In the dictionary, most words have one POS-
tag while a few have more. The monosyllabic 
words were omitted from the training. 
Feature Analysis 
In our CRFs model, we employed three main 
types of features: the components of words, the 
lengths of those components, and the POS tags of 
those components. 
Before the CRFs training, we analyzed the in-
ternal component structure of the dictionary 
words and assigned a proper POS-tag to each 
component. There are four analysis schemes that 
707
are different from each other in two aspects. The 
first aspect is the type of the component, which 
may be a character or immediate constituent (IC). 
Here, ?immediate constituent? means constitu-
ents that directly form a word. For example, ?
???? kexuejishubu (department of science 
and technology) has the following constituents: 
? ke, ? xue, ? ji, ? shu, ? bu, ?? kexue, 
and ?? jishu, in which only ?? kexue, ?? 
jishu and ? bu are the immediate constituents of 
????? kexuejishubu. The second aspect is 
regarding the consideration of the POS-tag of the 
component. The four analysis schemes are listed 
in Table 1. 
 
POS 
 
Component 
With  Without  
Character Scheme 1 Scheme 2 
Immediate 
Constituent 
Scheme 3 Scheme 4 
Table 1. Analysis Schemes 
In Scheme 1 and Scheme 2, the tool segments 
a dictionary word until all components are char-
acters, and in Scheme 1 only, each component is 
given a POS tag. In Scheme 3 and Scheme 4, the 
tool segments a dictionary word only once to get 
its immediate constituents, and in Scheme 3 only, 
each component is given a POS tag. For instance, 
?????  kexuejishubu (department of sci-
ence and technology) will be segmented as ??/N 
?/N ?/N ?/N ?/N? in Scheme 1 and ???/N 
??/N ?/N? in Scheme 3. 
Feature Template Selection 
For each type of feature, we used the five tem-
plates in Figure 1. So in Schemes 1 and 3 there 
are 15 templates, and in Schemes 2 and 4 there 
are 10 templates. 
For instance, when training, ????? will 
be transformed as ??/N/N_B??/N/N_M ?
/N/N_E in Scheme 3, in which N denotes that the 
POS of the word ????? is noun, while B, 
M and E denote the beginning, middle and end 
positions of the word. 
 
U01:%x[-1,i]: the former component?s ith feature
U02:%x[0,i]: the current component?s ith feature
U03:%x[1,i]: the next component?s ith feature 
U04:%x[-1,i]/%x[0,i]: the former component?s 
ith feature and the current component?s ith fea-
ture  
U05:%x[0,i]/%x[1,i]: the current component?s 
ith feature and the next component?s ith feature 
Figure 1. List of Templates(i=1-3) 
By using a dictionary with these feature tem-
plates for the training, we obtain a POS guessing 
model for unknown Chinese words. If an un-
known word such as ?? yongdian (electricity 
used) is tested, it would be analyzed as ?/V ?
/N (to use, electricity) for the feature extraction 
and then it would be tagged as ? /V/N_B ?
/N/N_E. That is, the word is assigned a POS of 
noun. 
4 Credibility Computation 
The initial model is based on the hypothesis that 
the syntactical properties of a word depend on its 
internal structure. But the internal structure of 
some words are ambiguous. For instance, both ?
? yongyu (vocabulary [literally, ?used words?]) 
?and ?  yongjing (exert [literally, ?use 
strength?]) both have the sequence V1N1 (which 
is combined by a POS sequence of ?VN? and a 
length sequence of ?11?), yet the former one is a 
noun and the latter one is a verb.  
There are some other sequences like V1N1. 
All the words (especially disyllabic words) 
fitting to these sequences bring difficulty for 
POS guessing model in Section 3. In this section, 
we attempt to identify those words by computing 
a credibility score for each type of sequence. The 
lower the score, less credible the result of the 
model.  
In detail, Formula 1 is used to compute the 
credibility of a word that has a certain type of 
sequence, e.g., ? ?  yongyu (vocabulary 
[literally, ?used words?])  with the sequence 
?V1N1?. 
)(
)|()|( 1
k
jkjk
k SCount
PPSCountPPSCount
C +
=?=
=
      (1) 
In Formula 1, kC  denotes the credibility score 
of words that have the kth type of sequence SK. 
SK denotes a sequence as P1L1P2L2??PnLn, in 
which n means the quantity of components in the 
sequence SK; Pn and Ln mean the POS and length 
of the nth component of any word that has the 
sequence SK, respectively; Count(SK) denotes the 
quantity of words in the dictionary that have the 
sequence SK; and Count(SK|P=Pj) and 
Count(SK|P=Pj+1) denote the quantity of words in 
the dictionary that have the sequence SK  and are 
tagged as POS Pj and Pj+1, respectively, in which 
Pj and Pj+1 are the two POSs that make the value 
of Count(SK|P=Pj) a maximum of two. Some se-
quences with lower credibility scores are listed in 
Table 2. 
708
For instance, the sequence type of the word ?
?  yongdian (electricity used) is V1N1, so its 
credibility score is 0.65. If the threshold is 0.8, 
this word will be considered a low-credibility 
word and put through reconsideration by the fol-
lowing model. 
 
 
 
Sequence Credibility 
Score 
Proportion 
Vg1Ng1 0.7 0.50% 
V1Ng1 0.67 1.90% 
Vg1N1 0.67 0.55% 
V1N1 0.65 2.99% 
V1Vn2 0.57 0.04% 
A1A1 0.55 0.22% 
N1A1 0.48 0.15% 
V1V2 0.44 0.05% 
V1Vi2 0.25 0.08% 
Table 2. Examples of Sequences with Low 
Credibility Scores 
 
5 Model Based on Global Contextual 
Features 
Words with relatively lower credibility scores 
(given in Section 4) will be revised by a model 
based on global context. In this paper, we im-
plement a model of voting by syntactical tem-
plates, which derived from research results of 
linguists. 
This process requires a relatively large corpus 
that can provide enough context instances for 
each under-processed word. It is difficult for us 
to find a corpus that can provide enough in-
stances of most unknown words, because many 
of such instances have only been used for a rela-
tively short time. In this paper, we use search 
engine as the source of corpus, i.e., throw a word 
to a search engine and pick out instances from 
the returned snippets. 
Linguists have summarized systematic rules 
for judging the POS of a Chinese word based on 
its global distribution in real text (Guo, 2002). 
For example, generally a verb or adjective can be 
modified by the word ??? (not) while a noun 
cannot. Based on this knowledge, we designed a 
set of syntactical templates listed in Table 3. The 
templates indicate whether a word can be used in 
such ways. 
For every word, we build phrases based on 
these templates (see Table 3 for instances of ?
? xihuan (like)) and send the phrases to a search 
engine as queries. For each query, the search 
engine returns some snippets, which are 
generally in sentence form. Then each word gets 
three scores through a voting process in which 
the sentences act as ?voters.? The three scores, 
Score(N), Score(V), and Score(A), denote the 
likelihood score for the word to be a noun, a verb 
or an adjective, respectively. Each voter votes by 
following the criteria given in Figure 2. In Figure 
2, Value(N), Value(V), and Value(A) are  con-
stant values that are used to balance the three 
scores. 
 
Table 3. Syntactical Templates with Instances of 
????1 
 
If the unknown word follows a transitive verb 
and is at the end of a sentence or subsentence, 
Score(N)+=Value(N); 
If the unknown word follows a quantitative word 
and is at the end of a sentence or subsentence, 
Score(N)+=Value(N); 
If the unknown word follows the word ???, 
??? or ???, Score(V)+=Value(V); 
If the unknown word follows the word ????, 
???? or ???? and is at the end of a sentence 
or subsentence, or there is a following word that 
is not a verb, Score(V)+=Value(N); 
If the unknown word follows the word ???, 
??? or ????, Score(A)+=Value(A). 
Figure 2. Criteria for Voting 
For each instance, Score(N), Score(V), and 
Score(A) will be added to the scores Value(N), 
Value(V), and Value(A), respectively. 
Although these templates are effective, there 
are some exceptions brought by morphology 
analysis errors or other reasons, so we use an 
outstanding method to filter the exceptions. We 
                                                 
1 Here ?*? means the structure is invalid. 
T
em
plates 
~ ?
+
~ 
?
?
+
~ 
?
?
+
~ 
?
?
+
~ 
?
+
~ 
?
+
~ 
?
+
~ 
?
?
+
~ Instance 
?
?
?
?
?
?
?
?
?
?
?
?
?
* 
?
?
?
?
* 
?
?
? 
?
?
? 
?
?
?
?
?
?
?
709
compute an outstanding value with Formula 2 to 
judge whether the voting result is acceptable. 
))((
))(('))((
POSScoreMax
POSScoreMaxPOSScoreMax
O
?
=
    (2) 
In Formula 2, O  means the outstanding value 
of a voting result; ))(( POSScoreMax  means 
the maximum score among the three scores and 
))((' POSScoreMax means the maximum score 
between the other scores. If O  is larger than a 
threshold, we assume the voting result to be ac-
ceptable and adopt the result to revise the POS 
guessing result of the initial model.  
For instance, Score(N), Score(V), and Score(A) 
of the word?? yongyu (vocabulary [literally, 
?used words?]) are 50, 5 and 3 respectively. So 
O(??)=(50-5)/50=0.9. 
6 Experiments and Results 
6.1 Data Preparation 
The model based on CRFs is trained on the 
Modern Chinese Grammar Information 
Dictionary (Yu, 1998) and tested on the 
Contemporary Chinese Corpus of Peking 
University (Yu et al, 2002). The corpus is seg-
mented and POS-tagged. Both the dictionary and 
corpus were constructed by the Institute of Com-
putational Linguistics, Peking University. The 
corpus was built using the content of all the news 
articles of the People?s Daily newspaper pub-
lished in China from January to June 1998. We 
selected all verbs, nouns and adjectives from the 
dictionary, excluding monosyllabic words, as 
training data. The nouns, verbs and adjectives in 
the corpus but not in the dictionary were consid-
ered to be unknown words and used as testing 
data. The distribution of word length of the train-
ing and testing data is presented in Table 4. 
 
Word Length Training Testing 
Disyllabic 40,103 11,108 
Tri-syllabic 12,167 12,901 
Four-character 1,180 1,055 
Five-character 0 279 
Total 53,450 25,343 
Table 4. Distribution of Word Length in Training 
and Testing Data 
We used ICTCLAS 1.0 (Zhang, 2002) to do 
word segmentation and POS tagging, because 
ICTCLAS is known as one of the best tools for 
those functions. ?CRF++, Yet Another CRF? 
toolkit (Kudo, 2005) was used as the implemen-
tation of CRFs model and www.Baidu.com as 
the search engine for our model based on contex-
tual features. 
6.2 Results of the Proposed Method 
The results for the four schemes of our method 
based on internal component features are listed in 
Table 52. From these results we may see that 
Scheme 1 is the best and Scheme 3 the second 
best, which means POS-tag of internal compo-
nents is very useful feature in the POS guessing 
work. The comparison between Scheme 1 and 
Scheme 3 indicates that character-based scheme 
is good for processing tri-syllabic words and 
five-character words while IC-based scheme is 
good for processing disyllabic words and four-
character words. Considering that most tri-
syllabic words and five-character words are de-
rivative words, while disyllabic words and four-
character words are compounds, the results show 
that the character-based scheme is good for proc-
essing derivative words while the IC-based 
scheme is good for processing compounds. All 
the following improvements will be based on 
Scheme 1. 
We assign the threshold of credibility score as 
0.8, and then there are 2,234 words with a credi-
bility score lower than the threshold. These 
words are then put through the revision process. 
In the revision model, we set the values of 
Value(N), Value(V), Value(A) and the out-
standing threshold as 4, 1, 1, and 0.5, respec-
tively, based on experience. All above thresholds 
are experimentally determined. Finally, 1,357 out 
of the 2,234 words pass the outstanding examina-
tion. Among them, 462 results were different 
from the former results and 302 of those were 
correctly revised, which resulted in the precision 
of disyllabic words reaching 87.90% (see Table 
6). Moreover, other 895 words, which have the 
same result in the two models, reaches the preci-
sion of 91.2%. That means the credibility will be 
very high when the two models generate the 
same result. 
Although we believe that the former method 
may have equal effectiveness to most man-made 
rules, there are still several rules that must be 
incorporated in order to simplify our machine-
learning method. Here we incorporated two 
reduplication rules to process two types of 
reduplicated unknown words, respectively. The 
form of the first kind of words is ?V1?V2,  such 
as ????  yingbuyinggai (should or should 
                                                 
2 Precision, recall, F-measure are the same. 
710
Table 7. Results of Wu & Jiang's (2000) 
Table 6. Results of Revision by Voting Model 
and Two Rules 
not) and the form of the second kind of words is 
?V1V1V2V2,? such as ???? chuchujinjin (go 
in and go out). If a four-character word is 
associated with one of the two forms and the first 
character is a verb, we revise its POS as a verb 
tag. 
The two reduplication rules correctly revised 
68 four-character words, which increased the 
precision of four-character words to 97.10%, a 
significant improvement over the previous best 
result, which was 92.89% (see Table 6). 
 
 
6.3 Comparison with Previous Work 
Wu & Jiang?s (2000)3 method is the most analo-
gous with our method, yet they did not directly 
report the results in their paper. In this paper, we 
                                                 
3 Lu (2005) implemented Wu & Jiang?s (2000) model with 
a relatively small corpus as the training data. The precision 
of Wu & Jiang?s method reported by the paper is 77.90% 
with a recall of 63.82%. 
implement their model using the same data as 
our method. The results of Wu & Jiang?s (2000) 
model are listed in Table 7. It shows that their 
model can guess the POS for disyllabic words 
with a relatively good F-measure (83.60%). How-
ever, the recall is not high for disyllabic 
(79.11%) and tri-syllabic (82.70%) words, and 
quite low for four-character (20.95%) and five-
character (0%) words. Our model in Section 3 
not only improves F-measure to 93.40%, but also 
improves recalls to 86.60%, 99.22%, 92.03% and 
100% for multi-syllabic words in turn (Table 5, 
Scheme 1).  
Lu (2005) proposed a hybrid model that 
achieved a precision of 89% for all words and 
74% for disyllabic words. Compared with that 
method, the hybrid model in this paper improves 
the precision to 94.20% for all words and 
87.90% for disyllabic words. Although the ex-
periments were not taken on the same data, the 
figures reflect the difference of power between 
methods in a certain degree. 
7 Conclusion and Future Work 
The results of this experiment show that our 
model based on internal component features can 
achieve quite good results in POS guessing for 
unknown Chinese words, both in precision and 
recall. This proves that the internal component 
Word Length Precision of 
Scheme 1 
Precision of 
Scheme 2 
Precision of  
Scheme 3 
Precision of  
Scheme 4 
Best Result
Disyllabic 86.60% 86.01 86.65% 85.21% 86.65% 
Tri-syllabic 99.22% 99.17% 98.65% 97.48% 99.22% 
Four-character 92.03% 91.47% 92.89% 89.76% 92.89% 
Five-character 100.00% 98.20% 98.92% 98.92% 100.00% 
Total 93.40% 93.08% 93.15% 91.80% 93.40% 
Word 
Length 
Total 
number 
Precision (Correspond-
ing value of before) 
Disyllabic 11,108 87.90% (86.60%) 
Tri-syllabic 12,901 99.22% (99.22%) 
Four-
character 
1,055 97.10% (92.89%) 
Five-
character 
279 100%    (100%) 
Total 25,343 94.20% (93.40%) 
Word Length Total Num-
ber 
Tagged 
Number 
Precision Recall F-measure (Corre-
sponding value of 
our method) 
Disyllabic 11,108 10,408 84.43% 79.11% 81.68%  (87.90%) 
Tri-syllabic 12,901 11,091 96.20% 82.70% 88.94%  (99.22%) 
Four-character 1,055 225 98.22% 20.95% 34.53%  (97.10%) 
Five-character 279 0 0 0 0             (100%) 
Total 25,343 21,724 90.58% 77.65% 83.60%   (94.20%) 
Table 5. Results for Four Schemes of The Model Based on Internal Component Features 
711
features of unknown words can be very useful in 
POS guessing. Moreover, the trained model 
based on internal component features is universal 
and robust. One evidence is that the model can 
identify POS correctly for most five-character 
words, even when there is no training data for 
that type of words. 
Our results also show that the contextual fea-
tures of unknown words can be an important 
complement to help improve POS guessing. Al-
though models based on contextual features 
alone can?t achieve the same precision and recall 
as models based on internal component features 
do, we may use contextual features as a comple-
ment in processing those words with ambiguous 
structure. 
In contrast with Lu (2005), we don?t use many 
manual rules. This does not mean that we believe 
those rules are useless in POS guessing. In fact, 
our initial model based on the CRFs model has 
learned the structure rules of Chinese words and 
can even give a credibility score for each rule. 
That is, most of the rules have been incorporated 
by the utilization of the CRFs model. 
In the future, to improve the results, we at-
tempt to manually revise the training data.  No-
tice that the training data was formed by seg-
menting and tagging POS of each word in a dic-
tionary using an existing tool like ICTCLAS. 
However, these tools usually generate quite a 
few errors on the words, because they are de-
signed to handle sentence but not word. These 
errors were not revised in the experiment, which 
damaged the performance. Thus, by manually 
revising the training data, we hope to improve 
the results in a certain degree. 
Although our experiments are mainly based on 
contemporary Chinese, we believe that this 
method will also be applicable to other Asian 
languages such as Japanese. 
References 
Andy Wu and Zixin Jiang. 2000. Statistically-
enhanced New Word Identification in a Rule-based 
Chinese System. In Proceedings of the 2nd Chinese 
Language Processing Workshop, pages 46?51. 
Chao-Jan Chen, Ming-Hong Bai, and Keh-Jiann Chen. 
1997. Category Guessing for Chinese Unknown 
Words. In Proceedings of the Natural Language 
Processing Pacific Rim Symposium, pages 35?40. 
Chooi-Ling Goh, Masayuki Asahara, and Yuji Ma-
tsumoto. 2006. Machine Learning-based Methods to 
Chinese Unknown Word Detection and POS Tag 
Guessing. In Journal of Chinese Language and Com-
puting 16 (4):185-206 
Douglas L. Vail, Manuela M. Veloso, and John D. 
Lafferty. 2007. Conditional Random Fields for Activ-
ity Recognition. In Proceedings of 2007 International 
Joint Conference on Autonomous Agents and Multi-
agent Systems. 
John Lafferty, Andrew McCallum, and Fernando 
Pereira. 2001. Conditional Random Fields: Probabilis-
tic Models for Segmenting and Labeling Sequence 
Data. In Proceedings of International Conference on 
Machine Learning. 
Kevin Zhang. ICTCLAS1.0. http://www.nlp.org.cn/ 
project/project.php?proj_id=6.  
Rui Guo. 2002. Studies on Part-of-speech of Contem-
porary Chinese. Commercial Press, Beijing, China. 
Shiwen Yu. 1998. Dictionary of Modern Chinese 
Grammar Information. Tsinghua University Press. 
Beijing, China. 
Shiwen Yu, Huiming Duan, Xuefeng Zhu, and Bing 
Sun. 2002. The Basic Processing of Contemporary 
Chinese Corpus at Peking University. Technical Re-
port, Institute of Computational Linguistics, Peking 
University, Beijing, China. 
T Nakagawa, Y Matsumoto. 2006. Guessing Parts-of-
speech of Unknown Words Using Global Information. 
In Proceedings of the 21st International Conference 
on Computational Linguistics and 44th Annual Meet-
ing of Association for Computational Linguistics, 
pages 705?712. 
Taku Kudo. 2005. CRF++: Yet Another CRF toolkit. 
http://chasen.org/~taku/software/CRF++. 
Xiaofei Lu. 2005. Hybrid Methods for POS Guessing 
of Chinese Unknown Words. In Proceedings of the 
43th Annual Meeting of Association for Computational 
Linguistics Student Research Workshop, pages 1?6. 
712
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 257?268, Dublin, Ireland, August 23-29 2014.
Multi-view Chinese Treebanking
Likun Qiu
1,2,3
, Yue Zhang
1
, Peng Jin
4
and Houfeng Wang
2
1
Singapore University of Technology and Design, Singapore
2
Institute of Computational Linguistics, Peking University, China
3
School of Chinese Language and Literature, Ludong University, China
4
Lab of Intelligent Information Processing and Application, Leshan Normal University, China
{qiulikun,jandp,wanghf}@pku.edu.cn, yue zhang@sutd.edu.sg
Abstract
We present a multi-view annotation framework for Chinese treebanking, which uses dependen-
cy structures as the base view and supports conversion into phrase structures with minimal loss
of information. A multi-view Chinese treebank was built under the proposed framework, and
the first release (PMT 1.0) containing 14,463 sentences is be made freely available. To verify
the effectiveness of the multi-view framework, we implemented an arc-standard transition-based
dependency parser and added phrase structure features produced by the phrase structure view.
Experimental results show the effectiveness of additional features for dependency parsing. Fur-
ther, experiments on dependency-to-string machine translation show that our treebank and parser
could achieve similar results compared to the Stanford Parser trained on CTB 7.0.
1 Introduction
Phrase structures (PS) and dependency structures (DS) are two of the most popular grammar formalisms
for statistical parsing (Collins, 2003; Charniak, 2000; McDonald et al., 2005; Nivre, 2006; Petrov and
Klein, 2007; Zhang and Clark, 2008). While DS trees emphasize the grammatical relation between heads
and dependents, PS trees stress the hierarchical constituent structures of sentences. Several researchers
have explored DS and PS simultaneously to enhance the quality of syntactic parsing (Wang and Zong,
2010; Farkas and Bohnet, 2012; Sun and Wan, 2013) and tree-to-string machine translation (Meng et al.,
2013), showing that the two types of information complement each other for NLP tasks.
Most existing Chinese and English treebanks fall into the phrase structure category, and much work
has been done to convert PS into DS (Magerman, 1994; Collins et al., 1999; Collins, 2003; Sun and
Jurafsky, 2004; Johansson and Nugues, 2007; Duan et al., 2007; Zhang and Clark, 2008). Research on
statistical dependency parsing has frequently used dependency treebanks converted from phrase structure
treebanks, such as the Penn Treebank (PTB) (Marcus et al., 1993) and Penn Chinese Treebank (CTB)
(Xue et al., 2000). However, previous research shows that dependency categories in converted tree-
banks are simplified (Johansson and Nugues, 2007), and the widely used head-table PS to DS conversion
approach encounters ambiguities and uncertainty, especially for complex coordination structures (Xue,
2007). The main reason is that the PS treebanks were designed without consideration of DS conversion,
leading to inherent ambiguities in the mapping, and loss of information in the resulting DS treebanks. To
minimize information loss during treebank conversions, a treebank could be designed by considering PS
and DS information simultaneously; such treebanks have been proposed as multi-view treebanks (Xia et
al., 2009). We develop a multi-view treebank for Chinese, which treats PS and DS as different views of
the same internal structures of a sentence.
We choose the DS view as the base view, from which PS would be derived. Our choice is based on the
effectiveness of information transfer rather than convenience of annotation (Rambow, 2010; Bhatt and
Xia, 2012). Research on Chinese syntax (Zhu, 1982; Chen, 1999; Chen, 2009) shows that the phrasal
category of a constituent can be derived from the phrasal categories of its immediate subconstituents and
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
257
PKU POS Our POS
Ag, a, ad, ia, ja, la a (adjective)
Bg,b, ib, jb, jm, lb b (distinguishing words)
Dg, d, dc, df, id, jd, ld d (adverb)
m, mq m(number)
n, an, in, jn, ln, Ng, vn, nr, kn n (noun)
Qg,q, qb, qc, qd, qe, qj, ql, qr, qt, qv, qz q (measure word)
Rg,r, rr, ry, ryw, rz, rzw r (pronoun)
Tg, t, tt t (temporal noun)
u, ud, ue, ui, ul, uo, us, uz, Ug u (auxiliary word)
v, iv, im, jv, lv, Vg, vd, vi, vl, vq,vu, vx, vt,kv v (verb)
w, wd, wf, wj, wk, wky, wkz, wm,wp, ws, wt, wu, ww, wy, wyy, wyz w (punctuation)
Table 1: Mapping from PKU POS to our POS.
the dependency categories between them (for terminal words, parts-of-speech can be used as phrasal cat-
egories). Consequently, in Chinese, the canonical PS, containing information of constituent hierarchies
and phrasal categories, can be derived naturally from the canonical DS. As Xia et al. (2009) stated, a rich
set of dependency categories should be designed to ensure lossless conversion from DS to PS. When the
information of PS has been represented in DS explicitly or implicitly, we can convert DS to PS without
ambiguity (Rambow et al., 2002).
Given our framework, a multi-view Chinese treebank, containing 14,463 sentences and 336K words,
is constructed. This main corpus is based on the Peking University People?s Daily Corpus. We name our
treebank the Peking University Multi-view Chinese Treebank (PMT) release 1.0. To verify the useful-
ness of the treebank for statistical NLP, a transition-based dependency parser is implemented to include
PS features produced in the derivation process of phrasal categories. We perform a set of empirical
evaluations, with experimental results on both dependency parsing and dependency-to-string machine
translation showing the effectiveness of the proposed annotation framework and treebank. We make the
treebank, the DS to PS conversion script and the parser freely available.
2 Annotation Framework
2.1 Part-of-speech Tagset
Our part-of-speech (POS) tagset is based on the Peking University (PKU) People?s Daily corpus, which
consists of over 100 tags (Yu et al., 2003). We simplify the PKU tagset by syntactic distribution. The
simplified tagset contains 33 POS tags. The mapping from the original PKU POS to our simplified POS is
shown in Table 1. For instance, Ag (adjective morpheme), ad (adjective acting as an adverb), ia (adjective
idioms), ja (adjective abbreviation) and la (temporary phrase acting as an adjective) are all mapped to one
tag a (adjective). A set of basic PKU POS tags, including c (conjunction), e (interjection), f (localizer),
g (morpheme), h (prefix), i (idiom), j (abbreviation), k (suffix), l (temporary phrase), nr (personal name),
nrf (family name), nrg (surname), ns (toponym), nt (organization name), nx (non-Chinese noun), nz
(other proper noun), o (onomonopeia), p (preposition), q (measure word), r (pronoun), s (locative), x
(other non-Chinese word), y (sentence final particle), z (state adjective), are left unchanged.
2.2 Dependency Category Tagset
In a DS, the modifier is tagged with a dependency category, which denotes the role the modifier plays
with regard to its head. The root word of a sentence is dependent on a virtual root node R and tagged
with the dependency category HED. Table 2 lists the 32 dependency categories used in our annotation
guideline. These categories are designed in consideration of PS conversion with minimal ambiguities,
and can be classified according to the following criteria:
(1) whether the head dominates a compound clause (i.e. has an IC modifier) in the PS view. Accord-
ing to this, dependency categories can be cross-clause or in-clause. For instance, in Figure 1, the last
punctuation (") is labeled with the cross-clause tag PUS, and its head dominates an IC modifier. (2) the
relative position of the modifier to the head. According to this, dependency categories can be left, right
or free. For instance, the LAD, SBV, ADV, COS, DE and ATT labels in Figure 1 are all left. The VOB label
258
Tag Description Tag Description
ACT action object LAD left additive
ADV adverbial MT modality and time
APP appositive element NUM number
ATT attribute POB propositional object
CMP complement PUN punctuation
COO other coordination element PUS cross-clause punctuation
COS share-right-child coordination element QUC post-positional quantity
DE de (modifier of(special function word)) QUCC non-shared post-positional quantity
DEI dei (modifier ofProceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1870?1880,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
ZORE: A Syntax-based System for Chinese Open Relation Extraction
Likun Qiu and Yue Zhang
Singapore University of Technology and Design, Singapore
qiulikun@gmail.com, yue zhang@sutd.edu.sg
Abstract
Open Relation Extraction (ORE) over-
comes the limitations of traditional IE
techniques, which train individual extrac-
tors for every single relation type. Sys-
tems such as ReVerb, PATTY, OLLIE, and
Exemplar have attracted much attention
on English ORE. However, few studies
have been reported on ORE for languages
beyond English. This paper presents a
syntax-based Chinese (Zh) ORE system,
ZORE, for extracting relations and seman-
tic patterns from Chinese text. ZORE
identifies relation candidates from auto-
matically parsed dependency trees, and
then extracts relations with their semantic
patterns iteratively through a novel double
propagation algorithm. Empirical results
on two data sets show the effectiveness of
the proposed system.
1 Introduction
Traditional Information Extraction (IE) system-
s train extractors for pre-specified relations (Kim
and Moldovan, 1993). This approach cannot scale
to the web, where target relations are not defined
in advance. Open Relation Extraction (ORE) at-
tempts to solve this problem by shallow-parsing-
based, syntax-based or semantic-role-based pat-
tern matching without pre-defined relation types,
and has achieved great success on open-domain
corpora ranging from news to Wikipedia (Banko
et al., 2007; Wu and Weld, 2010; Nakashole et
al., 2012; Etzioni et al., 2011; Moro and Nav-
igli, 2013). Many NLP and IR applications, in-
cluding selectional preference learning, common-
sense knowledge and entailment rule mining, have
benefited from ORE (Ritter et al., 2010). Howev-
er, most existing ORE systems focus on English,
and little research has been reported on other lan-
guages. In addition, existing ORE techniques are
mainly concerned with the extraction of textual re-
lations, without trying to give semantic analysis,
which is the advantage of traditional IE.
Our goal in this paper is to present a syntax-
based Chinese (Zh) ORE system, ZORE, which
extracts relations by using syntactic dependen-
cy patterns, while associating them with explic-
it semantic information. An example is shown
is Figure 1, where the relation (cn? (Oba-
ma)o? (President) , Pred[.? (graduate)],M
? (Harvard) {? (Law School)) is extract-
ed from the given sentence ?cn? (Obama) o
? (President) .? (graduate) u (from) M?
(Harvard) {? (Law School)?, and general-
ized into the syntactic-semantic pattern {nsubj-
NR(Af) Pred[.? (graduate)] prep-u (from)
pobj-NN(Di)}. Here, Af and Di stand for human
and institution, respectively, according to a Chi-
nese taxonomy Extended Cilin (Che et al., 2010).
Rather than extracting binary relations and then
generalizing them into semantic patterns, which
most previous work does (Mausam et al., 2012;
Nakashole et al., 2012; Moro and Navigli, 2012;
Moro and Navigli, 2013), we develop a novel
method that extracts relations and patterns simul-
taneously. A double propagation algorithm is used
to make relation and pattern information reinforce
each other, so that negative effects from automatic
syntactic and semantic analysis errors can be miti-
gated. In this way, semantic pattern information is
leveraged to improve relation extraction.
We manually annotate two sets of data, from
news text and Wikipedia, respectively. Experi-
ments on both data sets show that the double prop-
agation algorithm gives better precision and recall
compared to the baseline. To our knowledge, we
are one of the first to report empirical results on
Chinese ORE. The ZORE system, together with
the two sets of test data we annotated, and the sets
of 5 million relations and 344K semantic patterns
extracted from news and Wikipedia, is freely re-
1870
Figure 1: A sample sentence analyzed by ZORE.
leased
1
.
2 Basic Definitions for Open Information
Extraction
ZORE is applied to web text to extract general re-
lations and their semantic types. Our definition
of relations follow previous work on ORE (Moro
and Navigli, 2013), but with language-specific ad-
justments. In this section, we use the sentence in
Figure 1 as an instance to describe the basic defi-
nitions for ZORE.
Definition 1 (predicate phrase) A predicate
phrase is a sequence of words that contains at least
one verb or copula, and governs one or more noun
phrases syntactically. For instance, a predicate
phrase for the sentence in Figure 1 is ?.? (grad-
uate)?. Following Fader et al. (2011), Mausam et
al. (2012) and Nakashole et al. (2012), in case of
light verb constructions, the verb and its direct ob-
ject jointly serve as predicate phrase. We do not
include prepositions into the predicate phrases.
Definition 2 (argument) An argument is a base
noun phrase governed by a predicate phrase direct-
ly or indirectly with a preposition. For instance,
?cn? (Obama) o? (President)? and ?M?
(Harvard) {? (Law School)? are two argu-
ments of the predicate phrase ?.? (graduate)?.
Definition 3 (relation) A binary relation is a
triple that consists of the predicate phrase Pred
and its two arguments x and y. Accordingly, an
n-ary relation contains n arguments. For instance,
the sentence in Figure 1 contains the binary rela-
tion (cn? (Obama) o? (President), Pred[.
? (graduate)], M? (Harvard) {? (Law
School)). In English, the two arguments of a bi-
nary relation are usually positioned on the left and
right of Pred, respectively. Hence, shallow pat-
terns are highly useful for English relation extrac-
1
https://sourceforge.net/projects/zore/
tion (Banko et al., 2007). In Chinese, however, the
two arguments can be both on the left, both on the
right or one on the left and one on the right of the
predicate, and the resulting binary relation can be
either (x, y, Pred), (Pred, x, y) and (x, Pred, y), de-
pending on the sentence. This makes the detection
of relation phrases more complicated.
Definition 4 (syntactic pattern) A syntactic pat-
tern is the syntactic abstraction of a relation. A re-
lation can be generalized into the combination of
words, POS-tags and syntactic dependency labels
(Nakashole et al., 2012). For instance, the syntac-
tic pattern of the sentence in Figure 1 is {nsubj-
NR(A) Pred[.?] prep-u pobj-NN(A)}. It con-
sists of four sub-patterns. The first, nsubj-NR(A),
denotes that the current phrase acts as the sub-
ject of the predicate phrase with the POS-tag NR
(proper nouns). Here, ?(A)? means that the phrase
is an argument of the extracted relation. The sec-
ond sub-pattern denotes that the predicate phrase
of the example is ?.? (graduate)?. Note that
the words between the predicate and arguments
(e.g., prep-u) are included into the pattern direct-
ly (Nakashole et al., 2012; Mausam et al., 2012).
Definition 5 (semantic signature) The seman-
tic signature of a relation consists of the semantic
categories of the arguments. The semantic signa-
ture of Figure 1 is (Af, Di), where Af and Di de-
notes human and institute, respectively.
Definition 6 (semantic pattern) A semantic pat-
tern is the semantic abstraction of a relation. It
is the combination of a syntactic pattern and a
semantic signature. For instance, the syntactic
pattern {nsubj-NR(A) Pred[.?] prep-u pobj-
NN(A)}, combined with the semantic signature
(Af, Di), results in the semantic pattern {nsubj-
NR(Af) Pred[.?] prep-u pobj-NN(Di)}.
1871
Figure 2: Architecture of ZORE.
Figure 3: Parsing result of the example sentence
in Figure 1, in Stanford dependencies.
3 ZORE
The architecture of ZORE is shown in Figure 2. It
consists of three components. The first is a relation
candidate extractor, which consumes input tex-
t and performs sentence segmentation, word seg-
mentation, POS tagging, syntactic parsing, base
NP extraction, light verb structure (LVC) detection
and relation candidate extraction. The output is a
set of relation candidates. The second component
tags relations and extracts semantic patterns by a
double propagation algorithm. In the third compo-
nent, extracted patterns are grouped into synsets,
and relations are filtered by confidence scores.
3.1 Extracting Relation Candidates
3.1.1 Parsing and Base NP Extraction
ZORE analyzes the syntactic structures of input
texts by applying a pipeline of NLP tools. Each
sentence is segmented into a list of words by using
the Stanford segmenter (Chang et al., 2008), and
parsed by using ZPar (Zhang and Clark, 2011),
with POS tags and constituent structures by the
CTB standard (Xue et al., 2005). The result-
ing constituent trees are transformed into projec-
tive trees with Stanford dependencies by using the
Stanford parser (Chang et al., 2009). Figure 4
shows the parse tree of the sentence in Figure 1.
Next, base noun phrases (NPs) are extracted
from the dependency tree. Here a base NP is a
maximum phrase whose words can only have POS
from the first row of Table 1. The head word of a
base NP can be either a noun, a pronoun, a num-
ber or a measure word (the second row of Table
1). The dependency labels within a base NP can
only be from the third row of Table 1. Obviously,
a base NP does not contain other base NPs, and is
also not contained by any other base NP.
3.1.2 Detecting Light Verb Constructions
In linguistics, a light verb is a verb that has little
semantic content of its own, and typically form-
s a predicate with a noun (Butt, 2003). Exam-
ple predicates by light verb constructions (LVC)
include ?is a capital of? and ?claim responsibil-
ity for?, where ?is? and ?claim? are light verbs.
Improper handling of LVC can cause a significan-
t problem by uninformative extractions (Etzioni et
al., 2011). For example, if ?is? and ?claim? are ex-
tracted as predicates, the resulting relations (such
as (Hamas, claimed, responsibility) from the sen-
tence ?Hamas claimed responsibility for the Gaza
attack?) might not bare useful information. Re-
Verb (Etzioni et al., 2011) handles this problem by
hard syntactic constraints, taking the noun phrase
(e.g., responsibility) between a verb phrase (e.g.,
?claim?) and a preposition (e.g., ?for?) as a part of
the predicate phrase rather than an argument, lead-
ing to the relation (Hamas, claimed responsibility
for, the Gaza attack).
In Chinese, LVCs are highly frequent and
should be handled properly in order to ensure that
the extracted relations are informative. Howev-
er, the syntactic constraints in ReVerb can not be
transferred to Chinese directly, because the word
orders of English and Chinese are quite different.
1872
Labels
Base NP modifier NN (common noun), M (measure word), CD (cardinal number), OD (ordinal number), PN (pronoun), NR
(proper noun), NT (temporal noun), JJ (other noun-modifier), or PU (punctuation)
Base NP head NN (common noun), M (measure word), CD (cardinal number), OD (ordinal number), PN (pronoun), NR
(proper noun), NT (temporal noun)
Labels in base NPs nn (noun compound modifier), conj (conjunct), nummod (number modifier), cc (coordinating conjunction),
clf (classifier modifier), det (determiner), ordmod (ordinal number modifier), punct (punctuation), dep (other
dependencies), or amod (adjectival modifier)
Labels from base
NPs to predicate
phrase
nsubj (nominal subject), conj (conjunct), dobj (direct object), advmod (adverbial modifier), prep (preposi-
tional modifier), pobj (prepositional object), lobj (localizer object), range (dative object that is a quantifier
phrase), tmod (temporal modifier), plmod (localizer modifier of a preposition), attr (attributive), loc (local-
izer), top (topic), xsubj (controlling subject), ba (?ba? construction), nsubjpass (nominal passive subject)
Table 1: Constraints on POS-tags and dependency labels. Labels in the top three rows are used for base
NP extraction, while labels in the last row for traversing from a base NP to the predicate phrase.
In Chinese, prepositions acting as the modifier of
a verb can be on both the left and right of the ver-
b. For instance, the sentence ?cn? (Obama)o
? (President) u (from) M? (Harvard) {?
(Law School).? (graduate)? is a paraphrase of
the sentence in Figure 1, with the preposition u
(from) on the left of the predicate phrase.
Chinese LVCs can be classified into two types,
which we refer to as dummy-LVCs and common
LVCs, respectively. For the first type, the predi-
cate is a dummy verb such as ??1 (do)? and ??
? (give)?, which has a noun phrase as its object.
Since dummy verbs in Chinese are a closed set, we
detect this type of LVCs (such as ??1 (do)?!
(talk)?) by finding the dummy verb from a lexi-
con. For the second type of LVCs, the predicate is
a common verb, which has a nominalized structure
or a common noun as its object. For instance, ??
m (launch) N (investigation)? belongs to this
type of construction.
Common LVCs are more difficult to detect than
dummy-LVCs. We detect common LVCs by the
context. Besides the NPs in the LVC itself, a com-
mon LVC typically governs two NPs, with the lat-
ter being connected to the predicate phrase by an
LVC-related preposition such as ?? (for), ?u
(for), ? (for), ? (to), ? (with), ? (with), ?
(with) ?. Based on the observation, a basic idea of
identifying common LVCs is to find verb-object
structures that frequently co-occur with a LVC-
related preposition in a large-scale corpus parsed
automatically. For a given verb-object v, let f
v
and
f
p
denote the frequency of v and the frequency of v
co-occurring with an LVC-related preposition, re-
spectively. We define the statistical strength of v
to be an LVC as the ratio f
p
/f
v
. If the statistical
strength of v exceeds a threshold t
lvc
, we identify v
as a LVC. Table 2 illustrates some high-frequency
LVCs extracted by the method automatically.
3.1.3 Extracting Relation Candidates
ZORE tries to extract relation candidates from
sentences that contain two or more base NPs. Giv-
en two base NPs, we traverse the dependency tree
to obtain the shortest path that connects them. The
path can contain only dependency labels in the
fourth row of Table 1, and should contain at least
one of the labels from ?nsubj? and ?dobj? to en-
sure that a predicate phrase is included in the path.
If such a path is acquired, other base NPs governed
by the same predicate phrase are included into the
target relation, resulting in a n-ary relation candi-
dates with each base NP being an argument. Ac-
cording to the predicate phrase, relation candidates
can be classified into the following classes.
Common and dummy LVC relations. In this
type of relations, the predicate phrase of the path
is an LVC (e.g., a light verb and a nominal ob-
ject). The two base NPs can be the subject or
prepositional object of the light verb. For instance,
in the sentence ??&Z (Houdini) ? (to) ?
(my)?? (career)k (have)?? (big)K? (in-
fluence)?, ?k (have)?and ?K? (influence)? are
combined into a common LVC and taken as the
predicate phrase, resulting the relation (?&Z
(Houdini), Pred[k (have)K? (influence)],?
(my)?? (career)). In the corresponding English
sentence, the predicate phrase ?be a big influence
in? is also an LVC structure.
Verb relations. In this type of relations, a verb
acts as the predicate phrase. For instance, the rela-
tion (cn? (Obama) o? (President), Pred[.
? (graduate)], M? (Harvard) {? (Law
School)) extracted from the sentence in Figure 1
is a typical verb relation.
Relative-clause relations. In an relative-clause
1873
Verb Noun
?1 (do) (*) u1 (distribution),?? (analysis),?8 (collection),?U (modification),?? (visit),?v (punishment)
k (have) (*) K?(effect), z (contribution),, (interest),?? (help),@? (understanding),?" (expectation)
) (generate) (**) K? (effect),, (interest),~?(doubt),?? (shock),?a (good feeling),?? (fear)
E? (cause) (**) K? (effect),??(destruction),?? (harm),% (threat),?? (pressure),Z6 (distraction)
L? (express) (**) ?? (satisfaction),?H (welcome),?? (respect),?b (worry),H (mourning),a (gratitude)
?m (launch) (**) N (investigation),?? (attack),?? (offensive),1? (criticism),1 (negotiation),?z (lawsuit)
Table 2: Instances of dummy-LVCs (*) and common LVCs (**). A verb in the left column is combined
with a noun in the right column to form an LVC, which serves as the predicate phrase.
relation, the head word is a noun, modified by
an relative clause, but acting as an argument of
the predicate of the relative clause semantically.
The sentence ?.? (graduate) u (from) M?
(Harvard) {? (Law School)  (de, an aux-
iliary word) cn? (Obama) o? (president)?
is a paraphrase of the sentence in Figure 1, with
the same predicate phrase and arguments. How-
ever, the relation extracted from this phrase is an
relative-clause relation (Pred[.? (graduate)],
M? (Harvard) {? (Law School), cn?
(Obama) o? (president)), which belongs to the
same pattern synset as the relation of Figure 1.
3.2 Semantic Tagging by Double Propagation
The basic idea of our approach is to identify rela-
tions and patterns iteratively through semantical-
ly tagging the head words of arguments in rela-
tion candidates. Given a set of relation candidates
and a semantic taxonomy, the propagation consist-
s of three steps. In Step 1, monosemic arguments
in candidate relations are tagged with a seman-
tic category, such as Af and Di, to obtain seman-
tic patterns. In Step 2 and Step 3, untagged am-
biguous and unknown words are tagged by perfect
matching and partial matching, respectively. In the
end of each step, semantic patterns are generalized
from extracted and tagged relations, and then used
to help relation tagging in the next step. Because
of the two-way information exchange, we call this
method double propagation. The method can also
be treated as similar to bootstrapping (Yangarber
et al., 2000; Qiu et al., 2009).
3.2.1 Step 1: Tagging Monosemic Arguments
Each argument in a relation candidate is a base N-
P. Since base NPs are endocentric, we can take the
semantic category of the head word of a base NP
as the semantic category of the base NP. In a tax-
onomy, each word is associated with one or more
semantic categories. In this step, however, only
monosemic words are tagged, while both ambigu-
ous words and unknown words are left untagged.
Most named entities are not included in the
taxonomy. However, after POS-tagging, most of
them are detected as NR (proper noun). As a re-
sult, they are taken as ambiguous words that can
be person names, organization names or location
names. The named entities that are not included in
the taxonomy are tagged in Steps 2 and 3.
After this step, all the arguments in some re-
lation candidates have been tagged with semantic
categories. We refer to these relation candidates as
tagged relation candidates, and the remaining re-
lation candidates as untagged relation candidates.
Tagged relation candidate are generalized into se-
mantic patterns, consisting of syntactic patterns
and semantic signatures, as illustrated in Figure 1
and Section 2. We call the set of resulting seman-
tic patterns Set
SemPat
.
3.2.2 Step 2: Tagging by Perfect Pattern
Matching
In this step, the arguments in the untagged relation
candidates are tagged by semantic pattern match-
ing. Given an untagged relation candidate r, we
acquire a set of possible semantic categories for
each argument with an ambiguous head word. For
the arguments with unknown head words, we ac-
quire a set of possible semantic categories accord-
ing to their characters. Qiu et al. (2011) demon-
strate that 98% Chinese words have at least one
synonym, which shares at least one character. For
Chinese nouns, the set of synonyms usually shares
the last one or two characters. According to this,
our strategy for acquiring possible semantic cate-
gories for an unknown word is as follows.
Given an unknown word w
u
, if we find a known
word w
k
that shares the last two character with w
u
,
the semantic categories of w
k
will be used as the
possible semantic categories of w
u
. Otherwise, if
we find a known word w
k
that share the last one
character with w
u
, the semantic categories of w
k
will be used as the possible categories of w
u
.
1874
We then acquire possible semantic signatures of
untagged relation candidates, of which all the ar-
guments are tagged with possible semantic cate-
gories. As in Step 1, we generalize relation r in-
to a syntactic pattern pat
syn
, and then combine
pat
syn
with each possible semantic signature of
r to generate possible semantic patterns. In case
one or more possible semantic patterns of r ex-
ist in Set
SemPat
, if the highest frequency of these
patterns is above a threshold t
sem
, the correspond-
ing pattern will be taken as the semantic pattern
of r, from which we infer the semantic signature
for r and then the semantic category for the head
word of each argument of r. After this step, the
frequency of each semantic pattern in Set
SemPat
is updated according to the newly tagged relation
candidates.
3.2.3 Step 3: Tagging by Partial Pattern
Matching
In this step, we tag the ambiguous and unknown
words by partial matching rather than perfec-
t matching of the whole semantic pattern. This
can be treated as a back-off of the last step.
We first split n-ary semantic patterns in
Set
SemPat
into binary semantic patterns, and cal-
culate their frequencies. Second, we split each
untagged relation candidate r into several binary
sub-relations and then search for the correspond-
ing semantic patterns as in Step 2 ? for each bi-
nary sub-relation, we obtain a binary semantic sig-
nature with the highest frequency. By combining
the binary semantic signatures, we obtain one n-
ary semantic signature for r, based on which all
the unknown and ambiguous words can be tagged
with a semantic categories. If all the arguments
of a relation candidate r are tagged, r is treated as
tagged. Finally, according to the newly tagged re-
lations, statistics in Set
SemPat
are updated.
3.3 Grouping Patterns into Synsets
In this step, we group semantic patterns from
Set
SemPat
into pattern synsets, based on a
single-pass clustering process (Papka and Allan,
1998). Given two semantic patterns SemPat
i
and
SemPat
j
, we refer to their corresponding syntac-
tic pattern, semantic signature and predicate phras-
es as SynPat
i
and SynPat
j
, SemSig
i
and SemSig
j
,
Pred
i
and Pred
j
, respectively. Not taking the pred-
icate phrase into account, SynPat
i
and SynPat
j
are
identical, and we call them loosely identical (?).
The algorithm in Figure 4 is used to group
Figure 4: Algorithm for pattern synset grouping.
Type Feature Weight
Base r covers all words in c 0.96
Base There are commas within r -0.47
Base LENGTH(r)<10 words 0.35
Base 10 words[LENGTH(r)<20 words 0.11
Base 20 words[LENGTH(r) -1.06
Base COUNT(arguments)=2 0.14
Base COUNT(arguments)=3 0.33
Base COUNT(arguments)=4 -0.60
Base COUNT(arguments)> 4 -0.46
SemPat Being tagged in Step 3 0.87
SemPat Being tagged before Step 3 0.75
SemPat 50[SIZE(SemPat) and untagged -0.05
SemPat 50[SIZE(SemPat) and tagged 0.65
SemPat 10[SIZE(SemPat)<50 and untagged -0.16
SemPat 10[SIZE(SemPat)<50 and tagged 0.39
SemPat 5[SIZE(SemPat)<10 and untagged -0.22
SemPat 5[SIZE(SemPat)<10 and tagged 0.36
SemPat SIZE(SemPat) <5 and untagged -0.92
SemPat SIZE(SemPat) <5 and tagged -0.64
Table 3: Features of the logistic regression classi-
fier with weights trained on Wiki-500 dataset.
patterns, where ARGCOUNT(SynPat
j
) denotes the
number of arguments in SemPat
i
, SEMCAT(arg
1
)
indicates the semantic category of the first ar-
gument, and ISSYNONYM (Pred
i
, Pred
j
) return-
s whether two predicates are synonyms. In
similarity-based single-pass clustering, the topic
excursion problem is common (Papka and Allan,
1998). But since our similarity measure is sym-
metric, we do not suffer from this problem.
3.4 Computing the Confidence for Relations
Without filtering, the extraction algorithm in the
previous sections may yield false relations. Fol-
lowing previous ORE systems, we make a balance
between recall and precision by using a confidence
threshold (Fader et al., 2011). A logistic regres-
sion classifier is used to give a confidence score
to each relation, with features shown in Table 3.
In the table, c, r, arguments and SemPat denote
1875
Dataset Source #Sen #Rel
Wiki-500 Chinese Wikipedia 500 561
Sina-500 Sina News 500 707
Table 4: Annotated relation datasets.
clause, relation, arguments in a relation, and se-
mantic pattern, respectively. LENGTH(r), COUN-
T(arguments) and SIZE(SemPat) indicate the num-
ber of words in r, the number of arguments in
r, and the number of relations that belong to the
same semantic pattern SemPat as r. Because se-
mantic patterns from the double propagation al-
gorithm are used as features in the classifier, they
participate in relation extraction also. Their effect
on relation extraction can directly demonstrate the
effectiveness of double propagation.
4 Experiments
4.1 Experimental Setup
We run ZORE on two difference corpora: the Chi-
nese edition of Wikipedia (Wiki), which contain-
s 4.3 million sentences (as of March 29, 2014),
and a corpus from the Sina News archive (Sina
News), which includes 6.1 million sentences from
January 2013 to May 2013. The sentences that do
not end with punctuations are filtered. The Chi-
nese taxonomy Extended Cilin
2
(Cilin) (Che et al.,
2010) is used to give semantic categories for each
word. Cilin contains 77,492 Chinese words, or-
ganized into a five-level hierarchy. There are 12
categories in the top level, 94 in the second and
1492 in the third. In this paper, the second level
is used for semantic categories. We create two test
sets, containing 500 sentences from Wiki and 500
sentences from Sina News, respectively (see Table
4), annotated by two independent annotators us-
ing the annotation strategy of Fader et al. (2011).
The thresholds t
lvc
and t
sem
for pattern matching
are set as 0.4 and 5, tuned on 100 sentences from
Wiki-500 dataset, respectively.
4.2 Evaluation of Relation Extraction
First, we compare ZORE with a baseline system
to illustrate the effectiveness of the double prop-
agation algorithm. The baseline system does not
have the double propagation tagging component
in Figure 2, using the logistic regression classifier
in Section 3.4.1 with the 9 base features to filter
extracted relation candidates. It is similar to the
architecture of ReVerb (Fader et al., 2011). We
2
http://ir.hit.edu.cn/demo/ltp/Sharing Plan.htm
Figure 5: Performance on Wiki.
Figure 6: Performance on Sina News.
measure the precision and recall of the extracted
relations. An extracted relation is considered cor-
rect only when the predicate phrase and all the ar-
guments match the the gold set. On each data set,
we perform 5-fold cross-validation test and take
the average as the final precision and recall.
Figures 5 and 6 show the comparison of the two
systems on Wiki and Sina News, respectively. On
Wiki, ZORE has higher precision than the baseline
at all levels of recall. When the recall is 0.3, the
precision of ZORE is 0.77, 0.11 higher than the
baseline. The result on Sina News is similar. The
second column of Table 3 shows the weights of all
features trained on the Wiki data set, which indi-
cates that the semantic pattern features can give a
positive effect on relation filtering.
Second, we compare the intermediate results at
Steps 1, 2, and 3 in Section 3.2, respectively. The
precision, recall and F1 of the three steps with d-
ifferent numbers of Wiki sentences (from 10K to
5M sentences) are shown in Table 5. This figure
shows that Step 2 achieves higher precision than
Step 1 at all levels of recall, indicating that the
word sense tagging method in step 2 is useful for
1876
Sentences Step 1 Step 2 Step 3
P R F1 P R F1 P R F1
10K 0.947 0.032 0.062 0.960 0.043 0.082 0.933 0.075 0.139
50K 0.894 0.075 0.138 0.922 0.105 0.189 0.907 0.139 0.241
100K 0.897 0.093 0.169 0.924 0.130 0.228 0.909 0.160 0.272
200K 0.901 0.114 0.202 0.926 0.157 0.268 0.892 0.191 0.315
500K 0.891 0.146 0.251 0.909 0.196 0.322 0.860 0.230 0.363
1M 0.860 0.164 0.275 0.885 0.219 0.351 0.842 0.248 0.383
2M 0.797 0.182 0.296 0.819 0.250 0.383 0.788 0.278 0.411
3M 0.784 0.187 0.302 0.802 0.253 0.385 0.778 0.282 0.414
4M 0.739 0.178 0.287 0.801 0.258 0.390 0.778 0.287 0.419
5M 0.779 0.189 0.304 0.798 0.260 0.392 0.768 0.289 0.420
Table 5: Accuracies on different numbers Wiki sentences.
a significant boost of recall, together with a lit-
tle improvement in precision. In particular, Step
2 can extract about 20% relations with relatively
high precision (about 90%). The result of Step 3
is better to that of Step 2 in terms of F1-measure,
with the highest F1-measure achieved by this step.
4.3 Evaluation of Patterns
ZORE acquires 122K and 222K patterns from Wi-
ki and Sina News, clustered into 59K and 118K
pattern synsets, respectively. The frequency distri-
bution of the Wiki patterns is shown in Figure 7,
which conforms to Zipf?s law.
To assess the accuracy of pattern extraction, we
rank the extracted patterns by the size, and eval-
uated the precision of the top 100 and a random
set of 100 pattern synsets. Two annotators were
shown a pattern synset with its semantic signature
and a few example relations, and then asked to
judge whether it indicates a valid semantic rela-
tion or not. The results are shown in Table 6. The
averaged precision is 92% for the top 100 set, and
85% for the random 100 set.
The patterns in a pattern synset can be taken
as paraphrases (Barzilay and Lee, 2003). We ob-
serve that two synonymous patterns might differ
in three aspects. First, two patterns can differ
by the predicates, which are synonyms. For in-
stance, the verbs ???, , ?, ??, ?, ??
are synonyms, meaning ?to hold the appointment
of?. Second, two patterns in the same synset can
belong to different syntactic patterns, and there-
fore are paraphrases in the syntactic level. For in-
stance, the semantic patterns of the two sentences
?.? (graduate) u (from) M? (Harvard) {?
 (Law School) (de, an auxiliary word) cn
? (Obama)o? (president)? and ?cn? (Oba-
ma)o? (president)l(from)M? (Harvard){
? (Law School).? (graduate)? are both syn-
onymous to that of the sentence in Figure 1; al-
l the three patterns are found in the same synset
obtained by ZORE. Third, two patterns can dif-
fer only by the POS-tag. For instance, ?cn?
(Obama) l(from) M? (Harvard) {? (Law
School).? (graduate)? and ?@? (That) ??
(attorney) l(from) M? (Harvard) {? (Law
School) .? (graduate)? are synonyms with d-
ifferent POS-tags for the first argument (i.e. N-
R and NN). According to the grouping algorithm
in Section 3.3, all the three types of paraphrases
are grouped in a pattern synset, which makes some
synsets very large. The largest synset contains 110
patterns, while the top 100 synsets contain more
than 20 patterns.
4.4 Error analysis
We analyze the incorrect extractions (precision
loss) and missed correct relations (recall loss) re-
turned by Step 2, running on 500K sentences. Ta-
ble 7 summarizes the types of correct relations that
are missed by ZORE. 40% missed relations are
due to the minimum frequency constraint on se-
mantic patterns, which is used for a balance be-
tween precision and recall. Another main source
of failure is the incorrect identification of the pred-
icate phrase due to parsing errors, which account
for 37% of the total errors. Other sources of fail-
ures include redundant arguments and segmenta-
tion errors. Most redundant arguments are related
to prepositions such as ?U? (according to)? and
??? (on the basis of)?. For instance, in the sen-
tence ?U? (according to)?? (the)*: (point
of view) ? (,) ? (fundamental) ?K (prob-
lem) ? (is)?, an incorrect binary relation (??
(the)*: (point of view),? (fundamental)?
K (problem), Pred[? (is)]) is extracted, because
the prepositional object ??? (the) *: (point
of view)? is tagged as an argument of the predi-
cate phrase ?? (is)?.
Table 8 summarizes the major types of incor-
1877
Corpus Patterns Synsets Top100 Random100
Wiki 122,723 59,298 0.93 0.87
Sina 222,773 118,923 0.91 0.83
Table 6: Precision of pattern synsets.
40% Relations filtered by semantic pattern constraint
37% Could not identify correct predicates because of
preprocessing errors
12% Too many arguments because of parsing errors
11% Segmentation and POS tagging errors
Table 7: Relations missed by ZORE.
rect relations, 56% of which were caused by pars-
ing errors, and 34% of which were due to word
segmentation and POS tagging errors. Although
many errors have been filtered by ZORE, the
biggest source of errors is still syntactic analysis,
which is very important for high quality of ORE.
5 Related Work
English has been the major language on which
ORE research has been conducted. Previous
work on English ORE has evolved from shallow-
syntactic (Banko et al., 2007; Fader et al., 2011;
Merhav et al., 2012) to full-syntactic (Nakashole
et al., 2012; Mausam et al., 2012; Moro and Nav-
igli, 2013; Xu et al., 2013) and semantic (Johans-
son and Nugues, 2008) systems.
It has been shown that a full-syntactic system
based on dependency grammar can give signif-
icantly better results than shallow syntactic sys-
tems based on surface POS-patterns, yet enjoy
higher efficiency compared with semantic system-
s (Mesquita et al., 2013). Our investigation on
Chinese ORE takes root in full dependency syntax
and is hence able to identify patterns that involve
long-range dependencies. Considering the charac-
teristics of the Chinese language, such as the lack
of morphology and function words, and the high
segmentation and word sense ambiguities, we in-
corporate semantic ontology information into the
design of the system to improve the output quality
without sacrificing efficiency.
The state-of-the-art systems most closely relat-
ed to our approach are PATTY (Nakashole et al.,
2012) and the system of Moro and Navigli (2013).
Both, however, extract relations first, and then de-
fines patterns based on extracted relations. This
paper differs in that patterns and relations are ex-
tracted in a simultaneous process and so they can
improve each other. Previous studies show that
pattern generalization benefit from relation extrac-
56% Parsing errors
17% Segmentation errors
17% POS tagging errors
6% Redundant arguments
6% Other, including base NP extraction errors
Table 8: Incorrect extractions by ZORE.
Figure 7: The frequency distribution of patterns
extracted from Wiki. Size and Count denote the
number of relations that belong to a semantic pat-
tern and the logarithmic number of semantic pat-
terns that have the same size, respectively.
tion (Nakashole et al., 2012; Moro and Navigli,
2013), and relation extraction can benefit from
pattern generalization (Mausam et al., 2012). By
using double propagation, not only can we make
relation and pattern extraction benefit from each
other, but we can also tag relations and patterns
with semantic categories in a joint process.
There has been a line of research on Chinese re-
lation extraction, where both feature-based (Zhou
et al., 2005; Li et al., 2008) and kernel-based
(Zhang et al., 2006; Che et al., 2005) methods have
been applied. In addition, semantic ontologies
such as Extended Cilin have been shown useful
for Chinese relation extraction (Liu et al., 2013).
However, these studies have focused on tradition-
al IE, with pre-defined relations. In contrast, we
investigate ORE for Chinese, finding that seman-
tic ontologies useful for this task also. Tseng et al.
(2014) is the only previous research focusing on
Chinese ORE. Their system can be considered as
a pipeline of word segmentation, POS-tagging and
parsing, while our work gives semantic interpreta-
tion and explicitly deals with statistical errors in
parsing by a novel double propagation algorithm
between patterns and relations.
1878
6 Conclusion and Future Work
We presented a Chinese ORE system that inte-
grates relation extraction with semantic pattern
generalization by double propagation. Experimen-
tal results on two datasets demonstrated the ef-
fectiveness of the proposed algorithm. We make
the ZORE system, together with the large scale
relations and pattern synsets extracted by ZORE,
freely available at (https://sourceforge.
net/projects/zore/). Another version of
ZORE (ZORE-PMT), which is based on the de-
pendency tagset from PMT1.0 (Qiu et al., 2014),
is also provided.
Our error analysis demonstrates that the quali-
ty of syntactic parsing is crucial to the accuracy of
syntax-based Chinese ORE. Improvements to syn-
tactic analysis is likely to lead to improved ORE.
In addition, the idea of double propagation can be
generalized into information propagation between
relation extraction and syntactic analysis. We plan
to investigate the use of ORE in improving syntac-
tic analysis in future work.
Acknowledgments
We gratefully acknowledge the invaluable assis-
tance of Ji Ma, Wanxiang Che and Yijia Liu. We
also thank the anonymous reviewers for their con-
structive comments, and gratefully acknowledge
the support of the Singapore Ministry of Educa-
tion (MOE) AcRF Tier 2 grant T2MOE201301,
the start-up grant SRG ISTD 2012 038 from
Singapore University of Technology and Design,
the National Natural Science Foundation of Chi-
na (No. 61103089), National High Technolo-
gy Research and Development Program of Chi-
na (863 Program) (No. 2012AA011101), Ma-
jor National Social Science Fund of China (No.
12&ZD227), Scientific Research Foundation of
Shandong Province Outstanding Young Scientist
Award (No. BS2013DX020) and Humanities and
Social Science Projects of Ludong University (No.
WY2013003).
References
Michele Banko, Michael J Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction for the web. In IJCAI,
volume 7, pages 2670?2676.
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: an unsupervised approach using
multiple-sequence alignment. In Proceedings of the
2003 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology-Volume 1, pages 16?
23. Association for Computational Linguistics.
Miriam Butt. 2003. The light verb jungle. In Work-
shop on Multi-Verb Constructions, pages 1?28.
Pi-Chuan Chang, Michel Galley, and Christopher D
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 224?232. Association for
Computational Linguistics.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D Manning. 2009. Discriminative re-
ordering with Chinese grammatical relations fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation, pages
51?59. Association for Computational Linguistics.
WX Che, Jianmin Jiang, Zhong Su, Yue Pan, and T-
ing Liu. 2005. Improved-edit-distance kernel for
Chinese relation extraction. In IJCNLP, pages 132?
137.
Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp:
A Chinese language technology platform. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics: Demonstrations, pages
13?16. Association for Computational Linguistics.
Oren Etzioni, Anthony Fader, Janara Christensen,
Stephen Soderland, and Mausam Mausam. 2011.
Open information extraction: The second genera-
tion. In Proceedings of the Twenty-Second inter-
national joint conference on Artificial Intelligence-
Volume Volume One, pages 3?10. AAAI Press.
Anthony Fader, Stephen Soderland, and Oren Etzion-
i. 2011. Identifying relations for open informa-
tion extraction. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1535?1545. Association for Compu-
tational Linguistics.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based semantic role labeling of prop-
bank. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
69?78. Association for Computational Linguistics.
Jun-Tae Kim and Dan I Moldovan. 1993. Acquisition
of semantic patterns for information extraction from
corpora. In Artificial Intelligence for Application-
s, 1993. Proceedings., Ninth Conference on, pages
171?176. IEEE.
Wenjie Li, Peng Zhang, Furu Wei, Yuexian Hou, and
Qin Lu. 2008. A novel feature-based approach
to Chinese entity relation extraction. In Proceed-
ings of the 46th Annual Meeting of the Association
for Computational Linguistics on Human Language
Technologies: Short Papers, pages 89?92. Associa-
tion for Computational Linguistics.
1879
Dandan Liu, Zhiwei Zhao, Yanan Hu, and Longhua
Qian. 2013. Incorporating lexical semantic simi-
larity to tree kernel-based Chinese relation extrac-
tion. In Chinese Lexical Semantics, pages 11?21.
Springer.
Michael Schmitz Mausam, Robert Bart, Stephen
Soderland, and Oren Etzioni. 2012. Open language
learning for information extraction. pages 523?534.
Yuval Merhav, Filipe Mesquita, Denilson Barbosa,
Wai Gen Yee, and Ophir Frieder. 2012. Extracting
information networks from the blogosphere. ACM
Transactions on the Web (TWEB), 6(3):11.
Filipe Mesquita, Jordan Schmidek, and Denilson Bar-
bosa. 2013. Effectiveness and efficiency of open
relation extraction. New York Times, 500:150.
Andrea Moro and Roberto Navigli. 2012. Wisenet:
Building a wikipedia-based semantic network with
ontologized relations. In Proceedings of the 21st
ACM international conference on Information and
knowledge management, pages 1672?1676. ACM.
Andrea Moro and Roberto Navigli. 2013. Integrating
syntactic and semantic analysis into the open infor-
mation extraction paradigm. In Proceedings of the
Twenty-Third international joint conference on Arti-
ficial Intelligence, pages 2148?2154. AAAI Press.
Ndapandula Nakashole, Gerhard Weikum, and Fabi-
an Suchanek. 2012. PATTY: a taxonomy of rela-
tional patterns with semantic types. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1135?1145. As-
sociation for Computational Linguistics.
Ron Papka and James Allan. 1998. On-line new event
detection using single pass clustering. University of
Massachusetts, Amherst.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009. Expanding domain sentiment lexicon through
double propagation. In IJCAI, volume 9, pages
1199?1204.
Likun Qiu, Yunfang Wu, and Yanqiu Shao. 2011.
Combining contextual and structural information for
supersense tagging of Chinese unknown words. In
Computational Linguistics and Intelligent Text Pro-
cessing, pages 15?28. Springer.
Likun Qiu, Yue Zhang, Peng Jin, and Houfeng Wang.
2014. Multi-view Chinese treebanking. In Proceed-
ings of COLING 2014, pages 257?268.
Alan Ritter, Oren Etzioni, et al. 2010. A latent dirich-
let allocation method for selectional preferences. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 424?
434. Association for Computational Linguistics.
Yuen-Hsien Tseng, Lung-Hao Lee, Shu-Yen Lin, Bo-
Shun Liao, Mei-Jun Liu, Hsin-Hsi Chen, Oren Et-
zioni, and Anthony Fader. 2014. Chinese open rela-
tion extraction for knowledge acquisition. In EACL
2014, pages 12?16.
Fei Wu and Daniel S Weld. 2010. Open information
extraction using wikipedia. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 118?127. Association for
Computational Linguistics.
Ying Xu, Mi-Young Kim, Kevin Quinn, Randy Goebel,
and Denilson Barbosa. 2013. Open information
extraction with tree kernels. In Proceedings of
NAACL-HLT, pages 868?877.
Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn Chinese treebank: Phrase
structure annotation of a large corpus. Natural lan-
guage engineering, 11(2):207?238.
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Automatic acquisition
of domain knowledge for information extraction. In
Proceedings of the 18th conference on Computation-
al linguistics-Volume 2, pages 940?946. Association
for Computational Linguistics.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105?151.
Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.
2006. A composite kernel to extract relations be-
tween entities with both flat and structured features.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 825?832. Association for Computa-
tional Linguistics.
GuoDong Zhou, Su Jian, Zhang Jie, and Zhang Min.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of the 43rd annual meeting
on association for computational linguistics, pages
427?434. Association for Computational Linguistic-
s.
1880
