Evaluation challenges in large-scale document summarization
Dragomir R. Radev
U. of Michigan
radev@umich.edu
Wai Lam
Chinese U. of Hong Kong
wlam@se.cuhk.edu.hk
Arda C?elebi
USC/ISI
ardax@isi.edu
Simone Teufel
U. of Cambridge
simone.teufel@cl.cam.ac.uk
John Blitzer
U. of Pennsylvania
blitzer@seas.upenn.edu
Danyu Liu
U. of Alabama
liudy@cis.uab.edu
Horacio Saggion
U. of Sheffield
h.saggion@dcs.shef.ac.uk
Hong Qi
U. of Michigan
hqi@umich.edu
Elliott Drabek
Johns Hopkins U.
edrabek@cs.jhu.edu
Abstract
We present a large-scale meta evaluation
of eight evaluation measures for both
single-document and multi-document
summarizers. To this end we built a
corpus consisting of (a) 100 Million auto-
matic summaries using six summarizers
and baselines at ten summary lengths in
both English and Chinese, (b) more than
10,000 manual abstracts and extracts, and
(c) 200 Million automatic document and
summary retrievals using 20 queries. We
present both qualitative and quantitative
results showing the strengths and draw-
backs of all evaluation methods and how
they rank the different summarizers.
1 Introduction
Automatic document summarization is a field that
has seen increasing attention from the NLP commu-
nity in recent years. In part, this is because sum-
marization incorporates many important aspects of
both natural language understanding and natural lan-
guage generation. In part it is because effective auto-
matic summarization would be useful in a variety of
areas. Unfortunately, evaluating automatic summa-
rization in a standard and inexpensive way is a diffi-
cult task (Mani et al, 2001). Traditional large-scale
evaluations are either too simplistic (using measures
like precision, recall, and percent agreement which
(1) don?t take chance agreement into account and (2)
don?t account for the fact that human judges don?t
agree which sentences should be in a summary) or
too expensive (an approach using manual judge-
ments can scale up to a few hundred summaries but
not to tens or hundreds of thousands).
In this paper, we present a comparison of six
summarizers as well as a meta-evaluation including
eight measures: Precision/Recall, Percent Agree-
ment, Kappa, Relative Utility, Relevance Correla-
tion, and three types of Content-Based measures
(cosine, longest common subsequence, and word
overlap). We found that while all measures tend
to rank summarizers in different orders, measures
like Kappa, Relative Utility, Relevance Correlation
and Content-Based each offer significant advantages
over the more simplistic methods.
2 Data, Annotation, and Experimental
Design
We performed our experiments on the Hong Kong
News corpus provided by the Hong Kong SAR of
the People?s Republic of China (LDC catalog num-
ber LDC2000T46). It contains 18,146 pairs of par-
allel documents in English and Chinese. The texts
are not typical news articles. The Hong Kong News-
paper mainly publishes announcements of the local
administration and descriptions of municipal events,
such as an anniversary of the fire department, or sea-
sonal festivals. We tokenized the corpus to iden-
tify headlines and sentence boundaries. For the En-
glish text, we used a lemmatizer for nouns and verbs.
We also segmented the Chinese documents using the
tool provided at http://www.mandarintools.com.
Several steps of the meta evaluation that we per-
formed involved human annotator support. First, we
Cluster 2 Meetings with foreign leaders
Cluster 46 Improving Employment Opportunities
Cluster 54 Illegal immigrants
Cluster 60 Customs staff doing good job.
Cluster 61 Permits for charitable fund raising
Cluster 62 Y2K readiness
Cluster 112 Autumn and sports carnivals
Cluster 125 Narcotics Rehabilitation
Cluster 199 Intellectual Property Rights
Cluster 241 Fire safety, building management concerns
Cluster 323 Battle against disc piracy
Cluster 398 Flu results in Health Controls
Cluster 447 Housing (Amendment) Bill Brings Assorted Improvements
Cluster 551 Natural disaster victims aided
Cluster 827 Health education for youngsters
Cluster 885 Customs combats contraband/dutiable cigarette operations
Cluster 883 Public health concerns cause food-business closings
Cluster 1014 Traffic Safety Enforcement
Cluster 1018 Flower shows
Cluster 1197 Museums: exhibits/hours
Figure 1: Twenty queries created by the LDC for
this experiment.
asked LDC to build a set of queries (Figure 1). Each
of these queries produced a cluster of relevant doc-
uments. Twenty of these clusters were used in the
experiments in this paper.
Additionally, we needed manual summaries or ex-
tracts for reference. The LDC annotators produced
summaries for each document in all clusters. In or-
der to produce human extracts, our judges also la-
beled sentences with ?relevance judgements?, which
indicate the relevance of sentence to the topic of the
document. The relevance judgements for sentences
range from 0 (irrelevant) to 10 (essential). As in
(Radev et al, 2000), in order to create an extract of
a certain length, we simply extract the top scoring
sentences that add up to that length.
For each target summary length, we produce an
extract using a summarizer or baseline. Then we
compare the output of the summarizer or baseline
with the extract produced from the human relevance
judgements. Both the summarizers and the evalua-
tion measures are described in greater detail in the
next two sections.
2.1 Summarizers and baselines
This section briefly describes the summarizers we
used in the evaluation. All summarizers take as input
a target length (n%) and a document (or cluster) split
into sentences. Their output is an n% extract of the
document (or cluster).
? MEAD (Radev et al, 2000): MEAD is
a centroid-based extractive summarizer that
scores sentences based on sentence-level and
inter-sentence features which indicate the qual-
ity of the sentence as a summary sentence. It
then chooses the top-ranked sentences for in-
clusion in the output summary. MEAD runs on
both English documents and on BIG5-encoded
Chinese. We tested the summarizer in both lan-
guages.
? WEBS (Websumm (Mani and Bloedorn,
2000)): can be used to produce generic and
query-based summaries. Websumm uses a
graph-connectivity model and operates under
the assumption that nodes which are connected
to many other nodes are likely to carry salient
information.
? SUMM (Summarist (Hovy and Lin, 1999)):
an extractive summarizer based on topic signa-
tures.
? ALGN (alignment-based): We ran a sentence
alignment algorithm (Gale and Church, 1993)
for each pair of English and Chinese stories.
We used it to automatically generate Chinese
?manual? extracts from the English manual ex-
tracts we received from LDC.
? LEAD (lead-based): n% sentences are chosen
from the beginning of the text.
? RAND (random): n% sentences are chosen at
random.
The six summarizers were run at ten different tar-
get lengths to produce more than 100 million sum-
maries (Figure 2). For the purpose of this paper, we
only focus on a small portion of the possible experi-
ments that our corpus can facilitate.
3 Summary Evaluation Techniques
We used three general types of evaluation measures:
co-selection, content-based similarity, and relevance
correlation. Co-selection measures include preci-
sion and recall of co-selected sentences, relative util-
ity (Radev et al, 2000), and Kappa (Siegel and
Castellan, 1988; Carletta, 1996). Co-selection meth-
ods have some restrictions: they only work for ex-
tractive summarizers. Two manual summaries of the
same input do not in general share many identical
sentences. We address this weakness of co-selection
Lengths #dj
05W 05S 10W 10S 20W 20S 30W 30S 40W 40S FD
E-FD - - - - - - - - - - x 40
E-LD X X X X x x X X X X - 440
E-RA X X X X x x X X X X - 440
E-MO x x X x x x X x X x - 540
E-M2 - - - - - X - - - - - 20
E-M3 - - - - - X - - - - - 8
E-S2 - - - - - X - - - - - 8
E-WS - X - X x x - X - X - 160
E-WQ - - - - - X - - - - - 10
E-LC - - - - - - x - - - - 40
E-CY - X - X - x - X - X - 120
E-AL X X X X X X X X X X - 200
E-AR X X X X X X X X X X - 200
E-AM X X X X X X X X X X - 200
C-FD - - - - - - - - - - x 40
C-LD X X X X x x X X X X - 240
C-RA X X X X x x X X X X - 240
C-MO X x X x x x X x X x - 320
C-M2 - - - - - X - - - - - 20
C-CY - X - X - x - X - X - 120
C-AL X X X X X X X X X X - 180
C-AR X X X X X X X X X X - 200
C-AM - X X X X X X X X - 120
X-FD - - - - - - - - - - x 40
X-LD X X X X x x X X X X - 240
X-RA X X X X x x X X X X - 240
X-MO X x X x x x X x X x - 320
X-M2 - - - - - X - - - - - 20
X-CY - X - X - x - X - X - 120
X-AL X X X X X X X X X X - 140
X-AR X X X X X X X X X X - 160
X-AM - X X X X X X X - X - 120
Figure 2: All runs performed (X = 20 clusters, x = 10 clusters). Language: E = English, C = Chinese,
X = cross-lingual; Summarizer: LD=LEAD, RA=RAND, WS=WEBS, WQ=WEBS-query based, etc.; S =
sentence-based, W = word-based; #dj = number of ?docjudges? (ranked lists of documents and summaries).
Target lengths above 50% are not shown in this table for lack of space. Each run is available using two
different retrieval schemes. We report results using the cross-lingual retrievals in a separate paper.
measures with several content-based similarity mea-
sures. The similarity measures we use are word
overlap, longest common subsequence, and cosine.
One advantage of similarity measures is that they
can compare manual and automatic extracts with
manual abstracts. To our knowledge, no system-
atic experiments about agreement on the task of
summary writing have been performed before. We
use similarity measures to measure interjudge agree-
ment among three judges per topic. We also ap-
ply the measures between human extracts and sum-
maries, which answers the question if human ex-
tracts are more similar to automatic extracts or to
human summaries.
The third group of evaluation measures includes
relevance correlation. It shows the relative perfor-
mance of a summary: how much the performance
of document retrieval decreases when indexing sum-
maries rather than full texts.
Task-based evaluations (e.g., SUMMAC (Mani
et al, 2001), DUC (Harman and Marcu, 2001), or
(Tombros et al, 1998) measure human performance
using the summaries for a certain task (after the
summaries are created). Although they can be a
very effective way of measuring summary quality,
task-based evaluations are prohibitively expensive at
large scales. In this project, we didn?t perform any
task-based evaluations as they would not be appro-
priate at the scale of millions of summaries.
3.1 Evaluation by sentence co-selection
For each document and target length we produce
three extracts from the three different judges, which
we label throughout as J1, J2, and J3.
We used the rates 5%, 10%, 20%, 30%, 40% for
most experiments. For some experiments, we also
consider summaries of 50%, 60%, 70%, 80% and
90% of the original length of the documents. Figure
3 shows some abbreviations for co-selection that we
will use throughout this section.
3.1.1 Precision and Recall
Precision and recall are defined as:
PJ2 (J1) =
A
A+ C
,RJ2 (J1) =
A
A+ B
J2
Sentence in
Extract
Sentence not
in Extract
Sentence in
Extract
A B A+ B
J1 Sentence not
in Extract
C D C +D
A+ C B +D N = A +
B+C+D
Figure 3: Contingency table comparing sentences
extracted by the system and the judges.
In our case, each set of documents which is com-
pared has the same number of sentences and also
the same number of sentences are extracted; thus
P = R.
The average precision Pavg(SY STEM) and re-
call Ravg(SY STEM) are calculated by summing
over individual judges and normalizing. The aver-
age interjudge precision and recall is computed by
averaging over all judge pairs.
However, precision and recall do not take chance
agreement into account. The amount of agreement
one would expect two judges to reach by chance de-
pends on the number and relative proportions of the
categories used by the coders. The next section on
Kappa shows that chance agreement is very high in
extractive summarization.
3.1.2 Kappa
Kappa (Siegel and Castellan, 1988) is an evalua-
tion measure which is increasingly used in NLP an-
notation work (Krippendorff, 1980; Carletta, 1996).
Kappa has the following advantages over P and R:
? It factors out random agreement. Random
agreement is defined as the level of agreement
which would be reached by random annotation
using the same distribution of categories as the
real annotators.
? It allows for comparisons between arbitrary
numbers of annotators and items.
? It treats less frequent categories as more im-
portant (in our case: selected sentences), simi-
larly to precision and recall but it also consid-
ers (with a smaller weight) more frequent cate-
gories as well.
The Kappa coefficient controls agreement P (A)
by taking into account agreement by chance P (E) :
K =
P (A)? P (E)
1? P (E)
No matter how many items or annotators, or how
the categories are distributed, K = 0 when there is
no agreement other than what would be expected by
chance, and K = 1 when agreement is perfect. If
two annotators agree less than expected by chance,
Kappa can also be negative.
We report Kappa between three annotators in the
case of human agreement, and between three hu-
mans and a system (i.e. four judges) in the next sec-
tion.
3.1.3 Relative Utility
Relative Utility (RU) (Radev et al, 2000) is tested
on a large corpus for the first time in this project.
RU takes into account chance agreement as a lower
bound and interjudge agreement as an upper bound
of performance. RU allows judges and summarizers
to pick different sentences with similar content in
their summaries without penalizing them for doing
so. Each judge is asked to indicate the importance
of each sentence in a cluster on a scale from 0 to
10. Judges also specify which sentences subsume or
paraphrase each other. In relative utility, the score
of an automatic summary increases with the impor-
tance of the sentences that it includes but goes down
with the inclusion of redundant sentences.
3.2 Content-based Similarity measures
Content-based similarity measures compute the sim-
ilarity between two summaries at a more fine-
grained level than just sentences. For each automatic
extract S and similarity measure M we compute the
following number:
sim(M,S, {J1, J2, J3}) =
M(S, J1) +M(S, J2) +M(S, J3)
3
We used several content-based similarity mea-
sures that take into account different properties of
the text:
Cosine similarity is computed using the follow-
ing formula (Salton, 1988):
cos(X,Y ) =
?
xi ? yi
??
(xi)2 ?
??
(yi)2
where X and Y are text representations based on
the vector space model.
Longest Common Subsequence is computed as
follows:
lcs(X,Y ) = (length(X) + length(Y )? d(X,Y ))/2
where X and Y are representations based on
sequences and where lcs(X,Y ) is the length of
the longest common subsequence between X and
Y , length(X) is the length of the string X , and
d(X,Y ) is the minimum number of deletion and in-
sertions needed to transform X into Y (Crochemore
and Rytter, 1994).
3.3 Relevance Correlation
Relevance correlation (RC) is a new measure for as-
sessing the relative decrease in retrieval performance
when indexing summaries instead of full documents.
The idea behind it is similar to (Sparck-Jones and
Sakai, 2001). In that experiment, Sparck-Jones and
Sakai determine that short summaries are good sub-
stitutes for full documents at the high precision end.
With RC we attempt to rank all documents given a
query.
Suppose that given a queryQ and a corpus of doc-
uments Di, a search engine ranks all documents in
Di according to their relevance to the query Q. If
instead of the corpus Di, the respective summaries
of all documents are substituted for the full docu-
ments and the resulting corpus of summaries Si is
ranked by the same retrieval engine for relevance to
the query, a different ranking will be obtained. If
the summaries are good surrogates for the full docu-
ments, then it can be expected that rankings will be
similar.
There exist several methods for measuring the
similarity of rankings. One such method is Kendall?s
tau and another is Spearman?s rank correlation. Both
methods are quite appropriate for the task that we
want to perform; however, since search engines pro-
duce relevance scores in addition to rankings, we
can use a stronger similarity test, linear correlation
between retrieval scores. When two identical rank-
ings are compared, their correlation is 1. Two com-
pletely independent rankings result in a score of 0
while two rankings that are reverse versions of one
another have a score of -1. Although rank correla-
tion seems to be another valid measure, given the
large number of irrelevant documents per query re-
sulting in a large number of tied ranks, we opted for
linear correlation. Interestingly enough, linear cor-
relation and rank correlation agreed with each other.
Relevance correlation r is defined as the linear
correlation of the relevance scores (x and y) as-
signed by two different IR algorithms on the same
set of documents or by the same IR algorithm on
different data sets:
r =
?
i
(xi ? x)(yi ? y)
??
i
(xi ? x)2
??
i
(yi ? y)2
Here x and y are the means of the relevance scores
for the document sequence.
We preprocess the documents and use Smart to
index and retrieve them. After the retrieval process,
each summary is associated with a score indicating
the relevance of the summary to the query. The
relevance score is actually calculated as the inner
product of the summary vector and the query vec-
tor. Based on the relevance score, we can produce a
full ranking of all the summaries in the corpus.
In contrast to (Brandow et al, 1995) who run 12
Boolean queries on a corpus of 21,000 documents
and compare three types of documents (full docu-
ments, lead extracts, and ANES extracts), we mea-
sure retrieval performance under more than 300 con-
ditions (by language, summary length, retrieval pol-
icy for 8 summarizers or baselines).
4 Results
This section reports results for the summarizers and
baselines described above. We relied directly on the
relevance judgements to create ?manual extracts? to
use as gold standards for evaluating the English sys-
tems. To evaluate Chinese, we made use of a ta-
ble of automatically produced alignments. While
the accuracy of the alignments is quite high, we
have not thoroughly measured the errors produced
when mapping target English summaries into Chi-
nese. This will be done in future work.
4.1 Co-selection results
Co-selection agreement (Section 3.1) is reported in
Figures 4, and 5). The tables assume human perfor-
mance is the upper bound, the next rows compare
the different summarizers.
Figure 4 shows results for precision and recall.
We observe the effect of a dependence of the nu-
merical results on the length of the summary, which
is a well-known fact from information retrieval eval-
uations.
Websumm has an advantage over MEAD for
longer summaries but not for 20% or less. Lead
summaries perform better than all the automatic
summarizers, and better than the human judges.
This result usually occurs when the judges choose
different, but early sentences. Human judgements
overtake the lead baseline for summaries of length
50% or more.
5% 10% 20% 30% 40%
Humans .187 .246 .379 .467 .579
MEAD .160 .231 .351 .420 .519
WEBS .310 .305 .358 .439 .543
LEAD .354 .387 .447 .483 .583
RAND .094 .113 .224 .357 .432
Figure 4: Results in precision=recall (averaged over
20 clusters).
Figure 5 shows results using Kappa. Random
agreement is 0 by definition between a random pro-
cess and a non-random process.
While the results are overall rather low, the num-
bers still show the following trends:
? MEAD outperforms Websumm for all but the
5% target length.
? Lead summaries perform best below 20%,
whereas human agreement is higher after that.
? There is a rather large difference between the
two summarizers and the humans (except for
the 5% case for Websumm). This numerical
difference is relatively higher than for any other
co-selection measure treated here.
? Random is overall the worst performer.
? Agreement improves with summary length.
Figures 6 and 7 summarize the results obtained
through Relative Utility. As the figures indicate,
random performance is quite high although all non-
random methods outperform it significantly. Fur-
ther, and in contrast with other co-selection evalua-
tion criteria, in both the single- and multi-document
5% 10% 20% 30% 40%
Humans .127 .157 .194 .225 .274
MEAD .109 .136 .168 .192 .230
WEBS .138 .128 .146 .159 .192
LEAD .180 .198 .213 .220 .261
RAND .064 .081 .097 .116 .137
Figure 5: Results in kappa, averaged over 20 clus-
ters.
case MEAD outperforms LEAD for shorter sum-
maries (5-30%). The lower bound (R) represents the
average performance of all extracts at the given sum-
mary length while the upper bound (J) is the inter-
judge agreement among the three judges.
5% 10% 20% 30% 40%
R 0.66 0.68 0.71 0.74 0.76
RAND 0.67 0.67 0.71 0.75 0.77
WEBS 0.72 0.73 0.76 0.79 0.82
LEAD 0.72 0.73 0.77 0.80 0.83
MEAD 0.78 0.79 0.79 0.81 0.83
J 0.80 0.81 0.83 0.85 0.87
Figure 6: RU per summarizer and summary length
(Single-document).
5% 10% 20% 30% 40%
R 0.64 0.66 0.69 0.72 0.74
RAND 0.63 0.65 0.71 0.72 0.74
LEAD 0.71 0.71 0.76 0.79 0.82
MEAD 0.73 0.75 0.78 0.79 0.81
J 0.76 0.78 0.81 0.83 0.85
Figure 7: RU per summarizer and summary length
(Multi-document).
4.2 Content-based results
The results obtained for a subset of target lengths
using content-based evaluation can be seen in Fig-
ures 8 and 9. In all our experiments with tf ? idf -
weighted cosine, the lead-based summarizer ob-
tained results close to the judges in most of the target
lengths while MEAD is ranked in second position.
In all our experiments using longest common sub-
sequence, no system obtained better results in the
majority of the cases.
10% 20% 30% 40%
LEAD 0.55 0.65 0.70 0.79
MEAD 0.46 0.61 0.70 0.78
RAND 0.31 0.47 0.60 0.69
WEBS 0.52 0.60 0.68 0.77
Figure 8: Cosine (tf?idf ). Average over 10 clusters.
10% 20% 30% 40%
LEAD 0.47 0.55 0.60 0.70
MEAD 0.37 0.52 0.61 0.70
RAND 0.25 0.38 0.50 0.58
WEBS 0.39 0.45 0.53 0.64
Figure 9: Longest Common Subsequence. Average
over 10 clusters.
The numbers obtained in the evaluation of Chi-
nese summaries for cosine and longest common sub-
sequence can be seen in Figures 10 and 11. Both
measures identify MEAD as the summarizer that
produced results closer to the ideal summaries (these
results also were observed across measures and text
representations).
10% 20% 30% 40%
SUMM 0.44 0.65 0.71 0.78
LEAD 0.54 0.63 0.68 0.77
MEAD 0.49 0.65 0.74 0.82
RAND 0.31 0.50 0.65 0.71
Figure 10: Chinese Summaries. Cosine (tf ? idf ).
Average over 10 clusters. Vector space of Words as
Text Representation.
10% 20% 30% 40%
SUMM 0.32 0.53 0.57 0.65
LEAD 0.42 0.49 0.54 0.64
MEAD 0.35 0.50 0.60 0.70
RAND 0.21 0.35 0.49 0.54
Figure 11: Chinese Summaries. Longest Common
Subsequence. Average over 10 clusters. Chinese
Words as Text Representation.
We have based this evaluation on target sum-
maries produced by LDC assessors, although other
alternatives exist. Content-based similarity mea-
sures do not require the target summary to be a sub-
set of sentences from the source document, thus,
content evaluation based on similarity measures
can be done using summaries published with the
source documents which are in many cases available
(Teufel and Moens, 1997; Saggion, 2000).
4.3 Relevance Correlation results
We present several results using Relevance Correla-
tion. Figures 12 and 13 show how RC changes de-
pending on the summarizer and the language used.
RC is as high as 1.0 when full documents (FD) are
compared to themselves. One can notice that even
random extracts get a relatively high RC score. It is
also worth observing that Chinese summaries score
lower than their corresponding English summaries.
Figure 14 shows the effects of summary length and
summarizers on RC. As one might expect, longer
summaries carry more of the content of the full doc-
ument than shorter ones. At the same time, the rel-
ative performance of the different summarizers re-
mains the same across compression rates.
C112 C125 C241 C323 C551 AVG10
FD 1.00 1.00 1.00 1.00 1.00 1.000
MEAD 0.91 0.92 0.93 0.92 0.90 0.903
WEBS 0.88 0.82 0.89 0.91 0.88 0.843
LEAD 0.80 0.80 0.84 0.85 0.81 0.802
RAND 0.80 0.78 0.87 0.85 0.79 0.800
SUMM 0.77 0.79 0.85 0.88 0.81 0.775
Figure 12: RC per summarizer (English 20%).
C112 C125 C241 C323 C551 AVG10
FD 1.00 1.00 1.00 1.00 1.00 1.000
MEAD 0.78 0.87 0.93 0.66 0.91 0.850
SUMM 0.76 0.75 0.85 0.84 0.75 0.755
RAND 0.71 0.75 0.85 0.60 0.74 0.744
ALGN 0.74 0.72 0.83 0.95 0.72 0.738
LEAD 0.72 0.71 0.83 0.58 0.75 0.733
Figure 13: RC per summarizer (Chinese, 20%).
5% 10% 20% 30% 40%
FD 1.000 1.000 1.000 1.000 1.000
MEAD 0.724 0.834 0.916 0.946 0.962
WEBS 0.730 0.804 0.876 0.912 0.936
LEAD 0.660 0.730 0.820 0.880 0.906
SUMM 0.622 0.710 0.820 0.848 0.862
RAND 0.554 0.708 0.818 0.884 0.922
Figure 14: RC per summary length and summarizer.
5 Conclusion
This paper describes several contributions to text
summarization:
First, we observed that different measures rank
summaries differently, although most of them
showed that ?intelligent? summarizers outperform
lead-based summaries which is encouraging given
that previous results had cast doubt on the ability of
summarizers to do better than simple baselines.
Second, we found that measures like Kappa, Rel-
ative Utility, Relevance Correlation and Content-
Based, each offer significant advantages over more
simplistic methods like Precision, Recall, and Per-
cent Agreement with respect to scalability, applica-
bility to multidocument summaries, and ability to
include human and chance agreement. Figure 15
Property Prec, recall Kappa Normalized RU Word overlap, cosine, LCS Relevance Correlation
Intrinsic (I)/extrinsic (E) I I I I E
Agreement between human extracts X X X X X
Agreement human extracts and automatic extracts X X X X X
Agreement human abstracts and human extracts X
Non-binary decisions X X
Takes random agreement into account by design X X
Full documents vs. extracts X X
Systems with different sentence segmentation X X
Multidocument extracts X X X X
Full corpus coverage X X
Figure 15: Properties of evaluation measures used in this project.
presents a short comparison of all these evaluation
measures.
Third, we performed extensive experiments using
a new evaluation measure, Relevance Correlation,
which measures how well a summary can be used
to replace a document for retrieval purposes.
Finally, we have packaged the code used for this
project into a summarization evaluation toolkit and
produced what we believe is the largest and most
complete annotated corpus for further research in
text summarization. The corpus and related software
is slated for release by the LDC in mid 2003.
References
Ron Brandow, Karl Mitze, and Lisa F. Rau. 1995. Auto-
matic Condensation of Electronic Publications by Sen-
tence Selection. Information Processing and Manage-
ment, 31(5):675?685.
Jean Carletta. 1996. Assessing Agreement on Classifica-
tion Tasks: The Kappa Statistic. CL, 22(2):249?254.
Maxime Crochemore and Wojciech Rytter. 1994. Text
Algorithms. Oxford University Press.
William A. Gale and Kenneth W. Church. 1993. A
program for aligning sentences in bilingual corpora.
Computational Linguistics, 19(1):75?102.
Donna Harman and Daniel Marcu, editors. 2001. Pro-
ceedings of the 1st Document Understanding Confer-
ence. New Orleans, LA, September.
Eduard Hovy and Chin Yew Lin. 1999. Automated Text
Summarization in SUMMARIST. In Inderjeet Mani
and Mark T. Maybury, editors, Advances in Automatic
Text Summarization, pages 81?94. The MIT Press.
Klaus Krippendorff. 1980. Content Analysis: An Intro-
duction to its Methodology. Sage Publications, Bev-
erly Hills, CA.
Inderjeet Mani and Eric Bloedorn. 2000. Summariz-
ing Similarities and Differences Among Related Doc-
uments. Information Retrieval, 1(1).
Inderjeet Mani, The?re`se Firmin, David House, Gary
Klein, Beth Sundheim, and Lynette Hirschman. 2001.
The TIPSTER SUMMAC Text Summarization Evalu-
ation. In Natural Language Engineering.
Dragomir R. Radev, Hongyan Jing, and Malgorzata
Budzikowska. 2000. Centroid-Based Summarization
of Multiple Documents: Sentence Extraction, Utility-
Based Evaluation, and User Studies. In Proceedings
of the Workshop on Automatic Summarization at the
6th Applied Natural Language Processing Conference
and the 1st Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Seattle, WA, April.
Horacio Saggion. 2000. Ge?ne?ration automatique
de re?sume?s par analyse se?lective. Ph.D. the-
sis, De?partement d?informatique et de recherche
ope?rationnelle. Faculte? des arts et des sciences. Uni-
versite? de Montre?al, August.
Gerard Salton. 1988. Automatic Text Processing.
Addison-Wesley Publishing Company.
Sidney Siegel and N. John Jr. Castellan. 1988. Non-
parametric Statistics for the Behavioral Sciences.
McGraw-Hill, Berkeley, CA, 2nd edition.
Karen Sparck-Jones and Tetsuya Sakai. 2001. Generic
Summaries for Indexing in IR. In Proceedings of the
24th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 190?198, New Orleans, LA, September.
Simone Teufel and Marc Moens. 1997. Sentence Ex-
traction as a Classification Task. In Proceedings of the
Workshop on Intelligent Scalable Text Summarization
at the 35th Meeting of the Association for Computa-
tional Linguistics, and the 8th Conference of the Eu-
ropean Chapter of the Assocation for Computational
Linguistics, Madrid, Spain.
Anastasios Tombros, Mark Sanderson, and Phil Gray.
1998. Advantages of Query Biased Summaries in In-
formation Retrieval. In Eduard Hovy and Dragomir R.
Radev, editors, Proceedings of the AAAI Symposium
on Intelligent Text Summarization, pages 34?43, Stan-
ford, California, USA, March 23?25,. The AAAI
Press.
Proceedings of NAACL-HLT 2013, pages 727?732,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Semi-Supervised Discriminative Language Modeling
with Out-of-Domain Text Data
Arda C?elebi1 and Murat Sarac?lar2
1Department of Computer Engineering
2Department of Electrical and Electronics Engineering
Bog?azic?i University, Istanbul, Turkey
{arda.celebi, murat.saraclar}@boun.edu.tr
Abstract
One way to improve the accuracy of auto-
matic speech recognition (ASR) is to use dis-
criminative language modeling (DLM), which
enhances discrimination by learning where
the ASR hypotheses deviate from the uttered
sentences. However, DLM requires large
amounts of ASR output to train. Instead,
we can simulate the output of an ASR sys-
tem, in which case the training becomes semi-
supervised. The advantage of using simu-
lated hypotheses is that we can generate as
many hypotheses as we want provided that we
have enough text material. In typical scenar-
ios, transcribed in-domain data is limited but
large amounts of out-of-domain (OOD) data
is available. In this study, we investigate how
semi-supervised training performs with OOD
data. We find out that OOD data can yield im-
provements comparable to in-domain data.
1 Introduction
Discriminative language modeling (DLM) helps
ASR systems to discriminate between acoustically
similar word sequences in the process of choos-
ing the most accurate transcription of an utterance.
DLM characterizes and learns from ASR errors by
comparing the reference transcription of the utter-
ance and the candidate hypotheses generated by the
ASR system. Although previous studies based on
this supervised setting have been successful (Roark
et al, 2007; Ar?soy et al, 2009; Ar?soy et al, 2012;
Sak et al, 2012), they require large amounts of tran-
scribed speech data and a well-trained in-domain
ASR system, both of which are hard to obtain. To
overcome this difficulty, instead of training with the
real ASR output, we can use simulated output, in
which case the training becomes semi-supervised.
Semi-supervised training for discriminative lan-
guage modeling has been shown to achieve as good
word error rate (WER) reduction as the training done
with real ASR output (Sagae et al, 2012; C?elebi et
al., 2012). In this approach, first a confusion model
(CM) is estimated from supervised data. This CM
contains all seen confusions and their occurrence
probabilities in hypotheses generated by an ASR
system. Then, the CM is used to generate a num-
ber of alternative-but-incorrect hypotheses, or simu-
lated hypotheses, for a given sentence. Since the CM
characterizes the errors that the ASR system makes,
simulated hypotheses carry these characteristics. At
the end, the DLM is trained on the reference sen-
tences and their simulated hypotheses. Although be-
ing able to simulate the output of the ASR system
allows us to generate as much output as we need
for the DLM training, there is not always enough
text data that is in the same domain as the ASR sys-
tem. Yet, it is easier to find large amounts of out-of-
domain (OOD) text data. In this study, we extend the
previous studies where in-domain text data was used
for hypothesis simulation. Instead of using limited
in-domain data, we experiment with larger amounts
of OOD data for hypothesis simulation.
The rest of the paper is organized as follows. In
Section 2, we summarize the related work. In Sec-
tion 3, we explain the methods to simulate the hy-
potheses and to train the DLM. We give the exper-
imental results in Section 4 before concluding with
Section 5.
727
2 Related Work
The earliest work on hypothesis simulation for DLM
was done by Kurata et al (2009; 2012). They gen-
erate the probable n-best lists that an ASR system
may output for a hypothetical input utterance given
a word sequence. In another study, Tan et al (2010)
propose a system for channel modeling of ASR
for simulating the ASR corruption using a phrase-
based machine translation system trained between
the reference and output phoneme sequences from
a phoneme recognizer. Jyothi and Fosler-Lussier
(2010) also model the phonetic confusions using a
confusion matrix that takes into account word-based
phone confusion log likelihoods and distances be-
tween the phonetic acoustic models. This model
is then used to generate confusable word graphs
for training a DLM using the perceptron algorithm.
Xu et al (2009) propose the concept of cohorts
and report significant WER improvement for self-
supervised DLM. Similarly, Sagae et al (2012) use
phrasal cohorts to simulate ASR output and the per-
ceptron algorithm for training. They observe half of
the WER reduction that the fully supervised meth-
ods achieve. In another parallel study, C?elebi et al
(2012) work on a Turkish ASR system and consider
various confusion models at four different granular-
ities (word, morph, syllable, and phone) and differ-
ent sampling methods to choose from a large list of
simulated hypotheses. They observe that the strat-
egy that matches the word error (WE) distribution
of the simulated hypotheses to the WE distribution
of the ASR outputs yields the best WER reduction.
While the previous studies use in-domain data
sets for simulation, it is quite common to collect
large amounts of OOD text data from the web. How-
ever, given the nature of web data, some kind of se-
lection mechanism is needed to ensure quality. Bu-
lyko et al (2007) use perplexity-based filtering to
select a relevant subset from vast amounts of web
data in order to increase the training data of the gen-
erative LM used by the ASR system. There are also
studies that use a relative-entropy based selection
mechanism in order to match the n-gram distribution
of the selected data against the in-domain data by
Sethy et al (2006; 2009). In this study, we consider
the perplexity-based selection method for a start.
3 Method
3.1 Sentence Selection from OOD Data
In order to select sentences from the OOD data,
we use three methods in addition to random selec-
tion. We calculate the perplexity of each sentence
with SRILM toolkit, which gives normalized scores
with respect to the length of the sentence. Then,
we order sentences based on their perplexity scores
in increasing order. Perplexity is calculated by a
LM trained on in-domain data. After ordering, the
top of the list contains those sentences that resem-
ble the in-domain data the most whereas the sen-
tences at the bottom resemble the in-domain data the
least. We apply the three methods on this ordered
list of sentences. The first two methods, TOP-N and
BOTTOM-N , simply get the top and bottom N sen-
tences, respectively. The third method, RC-NxM ,
picks uniformly separated N clusters of M consec-
utive sentences, while making sure that top and bot-
tom M sentences are among the selected ones.
3.2 Hypothesis Simulation
Semi-supervised DLM training uses artificially gen-
erated hypotheses which mimic the ASR system
output. To generate the hypotheses, we follow
the three-step finite state transducer based pipeline
given in C?elebi et al (2012) and summarized by the
following composition sequence:
sample(N -best(prune(W?LW?CM)?LM
?1?GM))
In the first step of the pipeline, we use the confusion
model transducer (CM) to generate all possible con-
fusions that the ASR system can make for a given
reference sentenceW . We consider syllable, morph
and word based confusion models, and convert W
to these units using the lexicon LW . The generated
alternatives are pruned for efficiency reasons.
As the output of the first step may include many
implausible sequences, the second step converts
them to morphs using LM?1 and reweights them
with a morph-based language model GM to favor
the meaningful sequences. For this, we use three ap-
proaches. The first approach is to use the LM that is
used by the ASR system, called GEN-LM. The sec-
ond LM called ASR-LM is trained from the output
of the ASR system, whereas the third approach is
not to use any language model, denoted by NO-LM,
728
in which case we just use the scores coming from
the confusion model in the first step. A large list of
of N -best (N = 1000) hypotheses are produced at
this stage.
The third step, called sampling, involves picking a
subset of the hypotheses from a larger set with broad
variety. This step is done in order to pick samples so
as to make sure that they include error variety in-
stead of just high scoring hypotheses. As done by
C?elebi et al (2012), we use four sampling meth-
ods to pick 50 hypotheses out of the highest scoring
1000 hypotheses. The simplest of them is Top50,
where we select the highest scoring 50 hypotheses.
Another method is Uniform Sampling (US) which
selects instances from the WER-ordered list in uni-
form intervals. Third method, called RC5x10, forms
5 clusters separated uniformly, each containing 10
hypotheses. Lastly, ASRdist-50 selects 50 hypothe-
ses in such a way that the WE distribution of selected
hypotheses resembles the WE distribution of the real
ASR output as much as it can. We accomplish this
by filling the WE bins with the hypotheses having
required number of WEs.
3.3 DLM Estimation
The training of the DLM involves representing the
training data as feature vectors and processing via
a discriminative learning algorithm. We represent
the simulated N -best lists using unigram features as
described by Dikici et al (2012). As the learning
algorithm, we apply the WER-sensitive perceptron
algorithm proposed by Sak et al (2011b), which has
been shown to perform better for reranking ASR hy-
potheses as it minimizes an objective function based
on the WER rather than the number of misclassifi-
cations.
4 Experiments
4.1 Experimental Setup
We employ DLM on a Turkish broadcast news tran-
scription data set (Ar?soy et al, 2009), which com-
prises disjoint training (105356 sentences), held-out
(1947 sentences) and test (1784 sentences) subsets
consisting of ASR outputs represented as N -best
lists. We use Morfessor (Creutz and Lagus, 2005)
to obtain the morph level word segmentations from
which we build the LMs. For semi-supervised ex-
periments, we use the first half of the training sub-
set (t1: 53992 sentences, 965K morphs) to learn
the confusion models, and the reference transcrip-
tions of the second half (t2: 51364 sentences, 935K
morphs) to generate in-domain simulated n-best lists
to be compared against OOD simulated ones. For
this setup, the generative baseline WER and oracle
WER on the held-out set are 22.9% and 14.2% and
on the test set are 22.4% and 13.9%, respectively.
When we use ASR 50-best from t1 for DLM train-
ing, WERs drop to 22.2% and 21.8% on the held-out
and the test sets, respectively.
For OOD data, we use a data set of 10.8M
sentences (140M morphs) from newspaper articles
downloaded from the Internet (Sak et al, 2011a).
To calculate the perplexity of OOD sentences for se-
lection, we use a language model trained over the
reference transcripts and 50-best lists of t1 and t2.
4.2 Results on Out-of-Domain Data
We start our experiments with 500K randomly se-
lected OOD sentences, or RAND-500K. We run
the simulation pipeline with four sampling methods,
three confusion and three language models, giving
36 experiments in total. We choose among the pro-
posed sampling approaches and confusion models
using a rank-based comparison as done by Dikici et
al. (2012).
We look at which sampling method performs the
best by first dividing experiments into 9 groups, each
having 4 results from all sampling methods. Within
each group, we rank the sampling methods based on
the WER they achieve in increasing order and take
the average of assigned ranks. ASRdist-50 gets the
lowest average rank of 1.8, while RC5x10, US-50,
and TOP-50 come after with the averages of 2.1, 2.4,
and 3.4, respectively. This shows that ASRdist-50
gives the best WER reduction on OOD data, which
is also true for in-domain data (C?elebi et al, 2012).
Doing the same rank-based comparison for the
CMs this time, we observe that the syllable and
morph-based models have the same average rank of
1.5, whereas the word-based model has 2.8. How-
ever, a closer look reveals that the syllable-based
CM paired with NO-LM is an outlier because NO-
LM approach allows variety at the output but when
the unit of the confusion model is as small as syl-
lables, it produces too much variety that deterio-
729
rates the discriminative model. If we don?t consider
the ranks coming from NO-LM, the average rank of
syllable- and morph-based models become 1.1 and
1.8, respectively. Thus, we use syllable-based mod-
els over the others for the rest of the experiments.
Knowing that the ASRdist-50 sampling method
and syllable-based CM together give the best re-
sults for RAND-500K, we experiment with three
more sentence selection methods described in Sec-
tion 3.1. Table 1 shows all the results obtained from
four 500K OOD data sets.
OOD Data sets GEN-LM ASR-LM NO-LM
TOP-500K 22.6 22.6 22.6
BOTTOM-500K 22.4 22.2 22.5
RAND-500K 22.2 22.5 22.6
RC-5x100K 22.4 22.6 22.5
Table 1: WER (%) on held-out set obtained with syllable-
based CMs and ASRdist-50 sampling method
According to Table 1, the highest WER reduc-
tion is achieved with BOTTOM-500K+ASR-LM
and RAND-500K+GEN-LM combinations. While
ASR-LM exceeds the other two LMs only in the
case of BOTTOM-500K, for other three OOD data
sets GEN-LM gives the best results. More interest-
ingly, using OOD sentences resembling in-domain
data (or TOP-500K) is outperformed in all cases,
especially by BOTTOM-500K. To understand this,
we look at the number of morphs in each data set
given in Table 2. Even though each OOD data set
has 500K sentences, BOTTOM-500K has the high-
est number of morphs (?6.5M) and TOP-500K had
the lowest (?3.5M), while the other two have around
5.5M morphs. We also look at the morph unigram
distribution (M) of all four data sets and calculat-
ing the KL divergence KL(M || U)1 of each M to
uniform distribution (U). We observe that the uni-
gram morph distribution of the TOP-500K data set
is the least uniform with KL distance of 6.6, whereas
BOTTOM-500K has KL distance of 2.7 and the
other two have KL distances of around 4.3. In
other words, this shows that TOP-500K has the low-
est content variation, especially when compared to
BOTTOM-500K. Note also the slightly high value
of KL distance for t2, which can be attributed to the
1KL(M || U) =
?
i pilog(
pi
1/V ) = log(V ) ? H(p), where
V = 61294 and H(p) is the entropy of p.
relatively low number of unique morphs (types).
Data set KLD Types Tokens
t2 (50K) 4.65 22,107 935,137
TOP-500K 6.63 20,689 3,519,012
BOTTOM-500K 2.71 54,458 6,474,385
RAND-500K 4.36 50,422 5,559,763
RC-5x100K 4.35 50,561 5,343,342
Table 2: KL distance, KL(M || U), between uniform dis-
tribution (U) and unigram morph distribution (M); num-
ber of unique morphs and tokens.
4.3 Out-of-Domain vs In-Domain Data
In this section, we compare the results for in-domain
data with the results for four OOD data sets in
Table 3. In order to see how the size of OOD
data set affects the WER reduction, we start with
50K sentences and increase the size gradually up
to 500K. The first row of Table 3 shows the WER
obtained with the in-domain data t2, containing ap-
proximately 50K sentences.
Data 50K 100K 200K 500K
t2 22.4 - - -
TOP 22.8 22.7 22.7 22.6
BOTTOM 22.6 22.4 22.3 22.2
RAND 22.5 22.3 22.3 22.2
RC-5 22.5 22.5 22.3 22.4
Table 3: WER (%) on held-out set for in-domain
(Syllable+ASR-LM+ASRdist-50) and four OOD data
sets in increasing sizes
According to Table 3, even though 50K OOD sen-
tences yield worse results than the same amount of
in-domain sentences, as the size of OOD data set
increases, the amount of WER reduction increases
and surpasses the level obtained by using in-domain
data. What is more interesting is that RAND outper-
forms in-domain data starting from 100K, whereas
BOTTOM starts at a higher WER but drops rela-
tively fast, leveling with RAND starting at 200K.
Note that the best WER achieved with the simulated
data matches the supervised DLM performance us-
ing ASR 50-best from t1, reported in Section 4.1.
Then we go one step further and expand the BOT-
TOM data set to 1M sentences and we observe WER
of 22.1% on the held-out set. This further supports
730
the observation that the more OOD data we use, the
lower WER we can achieve.
As a side observation, when we calculate the
WER of five 100K-blocks from the RAND-500K
set, we find that the standard deviation of WER is
0.06%, which gives and idea about the significance
level of the WER differences.
4.4 Merging Real and Simulated Hypotheses
We also evaluate whether merging simulated hy-
potheses with real ASR hypotheses yields further
WER reductions. The result of merging the real hy-
potheses from t1 with the simulated ones from in-
domain and OOD data are shown in Table 4. The
first row shows the WER of the combination with
the simulated hypotheses from in-domain data t2.
Real Simulated WER (%)
t1 t2 (50K) 22.0
t1 TOP-500K 22.3
t1 BOTTOM-500K 22.1
t1 RAND-500K 22.0
t1 RC-5x100K 22.1
t1 BOTTOM-1M 21.9
Table 4: WER (%) on held-out set obtained by merging
real and simulated hypotheses
When combined with the real hypotheses from t1,
RAND500K achieves the same level of WER re-
duction as the simulated hypotheses from t2 on the
heldout set. The results on the test set are also sim-
ilar. On the test set, the combination of the real
hypotheses from t1 and the simulated hypotheses
from t2 achieve 21.5% WER, whereas the WER is
21.6% when the simulated hypotheses from t2 are
replaced by those from RAND500K. This indicates
that enough OOD data can replace the in-domain
data and yield similar performance, even in combi-
nation with in-domain real data.
Moreover, we further expand the OOD data to 1M
for BOTTOM, however even though it reduces the
WER on the heldout set, it achieves slightly higher
WER on the test set (21.7%).
Next, we combine the in-domain real hypotheses
from t1, simulated hypotheses from t2 and simulated
ones from the OOD data sets. However, compared
to the combination of t1 and t2, adding extra 500K
OOD hypotheses on top of those two gives similar
WERs on the held-out set while WERs on the test
set increases slightly. From another point of view,
adding in-domain simulated hypotheses from t2 on
top of real ones from t1 and 500K OOD data (rows
2-5 in Table 4) provides slight WER improvement
on the held-out set but not on the test set.
5 Conclusion
In this study, we investigate whether we can
achieve the same level of WER reduction for semi-
supervised DLM with the large amounts of OOD
data instead of in-domain data. We observe that
ASRdist-50 sampling method and syllable-based
CMs yield the best results with the OOD data. More-
over, selecting OOD sentences randomly rather than
using perplexity-based methods is enough to achieve
the best WER reduction. We also observe that sim-
ulated hypotheses from the OOD data is almost as
good as in-domain simulated hypotheses or even real
ones. As a future work, we will increase the size of
the OOD data and examine other methods like rela-
tive entropy based OOD selection.
Acknowledgments
This research is supported in part by TU?BI?TAK un-
der the project number 109E142.
References
Ebru Ar?soy, Dog?an Can, S?dd?ka Parlak, Has?im Sak,
and Murat Sarac?lar. 2009. Turkish broadcast news
transcription and retrieval. IEEE Transactions on Au-
dio, Speech, and Language Processing, 17(5):874?
883, July.
Ebru Ar?soy, Murat Sarac?lar, Brian Roark, and Izhak
Shafran. 2012. Discriminative language modeling
with linguistic and statistically derived features. IEEE
Transactions on Audio, Speech, and Language Pro-
cessing, 20(2):540?550, February.
Ivan Bulyko, Mari Ostendorf, Man-Hung Siu, Tim Ng,
Andreas Stolcke, and O?zgu?r C?etin. 2007. Web
resources for language modeling in conversational
speech recognition. ACM Transactions on Speech and
Language Processing, 5(1):1?25, December.
Arda C?elebi, Has?im Sak, Erinc? Dikici, Murat Sarac?lar,
Maider Lehr, Emily T. Prud?hommeaux, Puyang Xu,
Nathan Glenn, Damianos Karakos, Sanjeev Khudan-
pur, Brian Roark, Kenji Sagae, Izhak Shafran, Daniel
Bikel, Chris Callison-Burch, Yuan Cao, Keith Hall,
731
Eva Hasler, Philipp Koehn, Adam Lopez, Matt Post,
and Darcey Riley. 2012. Semi-supervised discrimi-
native language modeling for Turkish ASR. In Proc.
ICASSP, pages 5025?5028.
Mathias Creutz and Krista Lagus. 2005. Unsupervised
morpheme segmentation and morphology induction
from text corpora using morfessor 1.0. Technical re-
port, Helsinki University of Technology. Publications
in Computer and Information Science Report A81.
Erinc? Dikici, Arda C?elebi, and Murat Sarac?lar. 2012.
Performance comparison of training algorithms for
semi-supervised discriminative language modeling. In
Proc. Interspeech, Oregon, Portland, September.
Preethi Jyothi and Eric Fosler-Lussier. 2010. Discrimi-
native language modeling using simulated ASR errors.
In Proc. Interspeech, pages 1049?1052.
Gakuto Kurata, Nobuyasu Itoh, and Masafumi
Nishimura. 2009. Acoustically discriminative
training for language models. In Proc. ICASSP, pages
4717?4720.
Gakuto Kurata, Abhinav Sethy, Bhuvana Ramabhad-
ran, Ariya Rastrow, Nobuyasu Itoh, and Masafumi
Nishimura. 2012. Acoustically discriminative lan-
guage model training with pseudo-hypothesis. Speech
Communication, 54(2):219?228.
Brian Roark, Murat Sarac?lar, and Michael Collins. 2007.
Discriminative n-gram language modeling. Computer
Speech and Language, 21(2):373?392, April.
Kenji Sagae, Maider Lehr, Emily T. Prud?hommeaux,
Puyang Xu, Nathan Glenn, Damianos Karakos, San-
jeev Khudanpur, Brian Roark, Murat Sarac?lar, Izhak
Shafran, Daniel Bikel, Chris Callison-Burch, Yuan
Cao, Keith Hall, Eva Hasler, Philipp Koehn, Adam
Lopez, Matt Post, and Darcey Riley. 2012. Halluci-
nated n-best lists for discriminative language model-
ing. In Proc. ICASSP.
Has?im Sak, Tunga Gu?ngo?r, and Murat Sarac?lar. 2011a.
Resources for turkish morphological processing. Lan-
guage Resources and Evaluation, 45(2):249?261.
Has?im Sak, Murat Sarac?lar, and Tunga Gu?ngo?r. 2011b.
Discriminative reranking of ASR hypotheses with
morpholexical and n-best-list features. In Proc. ASRU,
pages 202?207.
Has?im Sak, Murat Sarac?lar, and Tunga Gu?ngo?r. 2012.
Morpholexical and discriminative language models for
Turkish automatic speech recognition. IEEE Trans-
actions on Audio, Speech, and Language Processing,
20(8):2341?2351, October.
Abhinav Sethy, Panayiotis G. Georgiou, and Shrikanth
Narayanan. 2006. Text data acquisition for domain-
specific language models. In EMNLP ?06 Proceedings
of the 2006 Conference on Empirical Methods in Nat-
ural Language Processing, pages 382?389.
Abhinav Sethy, Panayiotis G. Georgiou, Bhuvana Ram-
abhadran, and Shrikanth Narayanan. 2009. An itera-
tive relative entropy minimization-based data selection
approach for n-gram model adaptation. IEEE Trans-
actions on Audio, Speech and Language Processing,
17(1):13?23, January.
Qun Feng Tan, Kartik Audhkhasi, Panayiotis G. Geor-
giou, Emil Ettelaie, and Shrikanth Narayanan. 2010.
Automatic speech recognition system channel model-
ing. In Proc. Interspeech, pages 2442?2445.
Puyang Xu, Damianos Karakos, and Sanjeev Khudanpur.
2009. Self-supervised discriminative training of statis-
tical language models. In Proc. ASRU, pages 317?322.
732
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 554?561, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
BOUNCE: Sentiment Classification in Twitter using Rich Feature Sets
Nadin Ko?kciyan?, Arda C?elebi?, Arzucan O?zgu?r, Suzan U?sku?darl?
Department of Computer Engineering
Bogazici University
Istanbul, Turkey
{nadin.kokciyan,arda.celebi,arzucan.ozgur,suzan.uskudarli}@boun.edu.tr
Abstract
The widespread use of Twitter makes it very
interesting to determine the opinions and the
sentiments expressed by its users. The short-
ness of the length and the highly informal na-
ture of tweets render it very difficult to auto-
matically detect such information. This paper
reports the results to a challenge, set forth by
SemEval-2013 Task 2, to determine the posi-
tive, neutral, or negative sentiments of tweets.
Two systems are explained: System A for de-
termining the sentiment of a phrase within a
tweet and System B for determining the senti-
ment of a tweet. Both approaches rely on rich
feature sets, which are explained in detail.
1 Introduction
Twitter consists of a massive number of posts on a
wide range of subjects, making it very interesting to
extract information and sentiments from them. For
example, answering questions like ?What do Twitter
users feel about the brand X?? are quite interesting.
The constrained length and highly informal nature
of tweets presents a serious challenge for the auto-
mated extraction of such sentiments.
Twitter supports special tokens (i.e. mentions and
hashtags), which have been utilized to determine the
sentiment of tweets. In (Go et al, 2009), emoticons
are used to label tweets. In (Davidov et al, 2010),
Twitter emoticons as well as hashtags are used to la-
bel tweets. O?Connor et al (2010) demonstrated
a correlation between sentiments identified in pub-
lic opinion polls and those in tweets. A subjectivity
? These authors contributed equally to this work
lexicon was used to identify the positive and nega-
tive words in a tweet. In (Barbosa and Feng, 2010),
subjective tweets are used for sentiment classifica-
tion. They propose the use of word specific (e.g.
POS tags) and tweet specific (e.g. presence of a link)
features. Most of these studies use their own anno-
tated data sets for evaluation, which makes it diffi-
cult to compare the performances of their proposed
approaches.
Sentiment Analysis in Twitter 2013 (SemEval
2013 Task 2) (Wilson et al, 2013) presented a chal-
lenge for exploring different approaches examin-
ing sentiments conveyed in tweets: interval-level
(phrase-level) sentiment classification (TaskA) and
message-level sentiment classification (TaskB). Sen-
timent are considered as positive, negative, or neu-
tral. For TaskA, the goal is to determine the sen-
timent of an interval (consecutive word sequence)
within a tweet. For TaskB, the goal is to determine
sentiment of an entire tweet. For example, let?s con-
sider a tweet like ?Can?t wait until the DLC for ME3
comes out tomorrow. :-)?. For TaskA, the interval
0-1 (Can?t wait) is ?positive? and the interval 10-10
(:-)) is ?positive?. For TaskB, this tweet is ?positive?.
In this paper, we present two systems, one for
TaskA and one for TaskB. In both cases machine
learning methods were utilized with rich feature sets
based on the characteristics of tweets. Our results
suggest that our approach is promising for sentiment
classification in Twitter.
2 Approach
The task of detecting the sentiments of a tweet or
an interval therein, is treated as a classification of
554
  TaskA        
 Tweets
   with
Intervals
Positive
Classifier
Tweet
Classifier
TaskA
Interval Classifier
TaskB
Multiple Binary Classifier
  TaskB
 Tweets
+/-/0
Classified 
Tweets
Classified 
Tweet Intervals
+/-/0
Preprocessor
+
Feature
Generator
Negative
Classifier
Lexicons
Figure 1: The Overview of BOUNCE System
tweets into positive, negative, or neutral sets. Fig-
ure 1 gives the overview of our approach. The Pre-
processor module tokenizes the tweets that are used
by the Feature Generator. At this stage, the tweets
are represented as feature vectors. For TaskA, the
feature vectors are used by the Interval Classifier
that predicts the labels of the tweet intervals. For
TaskB, the feature vectors are used by the Positive
Classifier and the Negative Classifier which report
on the positivity and negativity of the tweets. The
Tweet Classifier determines the tweet labels using a
rule-based method. Each step is described in detail
in the following subsections.
2.1 Lexicons
The core of our approach to sentiment analysis relies
on word lists that are used to determine the positive
and negative words or phrases. Several acquired lists
are used in addition to one that we curated. AFINN
(Nielsen, 2011) is the main sentiment word list in-
cluding 2477 words rated between -5 to 5 for va-
lence. SentiWordNet (Baccianella et al, 2010), de-
rived from the Princeton English WordNet (Miller,
1995), assigns positive, negative, or objective scores
to each synset in WordNet. We considered the av-
erage of a word?s synsets as its SentiWordNet score.
Thus, synsets are disregarded and no disambiguation
of the sense of a word in a given context is done.
The SentiWordNet score of a word is not used if it
has objective synsets, since it indicates that the word
might have been used in an objective sense. We use
a list of emotion words and categories that is created
by DeRose1. Furthermore, a slang dictionary down-
1http://derose.net/steve/resources/emotionwords/ewords.html
loaded from the Urban Dictionary2 containing over
16,000 phrases (with no sentiment) is used. Finally,
we curated a sentiment word list initiated with a list
of positive and negative words obtained from Gen-
eral Inquirer (Stone et al, 1966), and refined by sen-
timent emitting words from a frequency-based or-
dered word list generated from the training data set
of SemEval-2013 Task A. Naturally, this list is more
specialized to the Twitter domain.
2.2 Preprocessing
Prior to feature generation, tweets were prepro-
cessed to yield text with more common wording.
For this, CMU?s Ark Tokenizer and Part-of-Speech
(POS) Tagger (Gimpel et al, 2011), which has been
specifically trained for tweets, was used. Tweets are
tokenized and POS tagged.
2.3 Feature Sets
In addition to the lexical or syntactic characteristics,
the manner in which tweets are written may reveal
sentiment. Orthogonal shapes of words (esp. fully
or partially capitalized words), expressions of a sin-
gle word or a phrase in the form of a hashtag, posi-
tions of certain tokens in a tweet are prominent char-
acteristics of tweets. In addition to these, tweets may
convey multiple sentiments. This leads to sequence-
based features, where we append features for each
sentiment emitted by a word or a phrase in a tweet.
Moreover, since TaskA asks for sentiment of inter-
vals in a tweet, we also engineer features to catch
clues from the surrounding context of the interval,
2http://www.urbandictionary.com
555
such as the sentiments and lengths of the neighbor-
ing intervals. For TaskB, the usage of hashtags and
last words in tweets were occasionally sentimental,
thus we considered them as features as well. We ex-
plain all features in detail in Section 3.
2.4 Classification
Maximum entropy models (Berger et al, 1996) have
been used in sentiment analysis (Fei et al, 2010).
They model all given data and treat the remainder as
uniform as possible making no assumptions about
what is not provided. For this, TaskA system uses
the MaxEnt tool (Zhang, 2011).
Naive Bayes is a simple probabilistic model based
on Bayes? Theorem that assumes independence be-
tween features. It has performed well in sentiment
classification of Twitter data (Go et al, 2009; Bifet
and Frank, 2010). TaskB data was not evenly dis-
tributed. There were very few negative tweets com-
pared to positive tweets. Using a single classifier
to distinguish the classes from each other resulted
in poor performance in identifying negative tweets.
Therefore, TaskB system utilizes multiple binary
classifiers that use the one-vs-all strategy. Maximum
Entropy and Naive Bayes models were considered
and the model that performed best on the develop-
ment set was chosen for each classifier. As a result,
the positive classifier (Bpos) is based on the Max-
imum Entropy model, whereas the negative classi-
fier (Bneg) is based on Naive Bayes. TaskB system
uses the Natural Language Toolkit (Loper and Bird,
2002).
3 Systems
In this section, TaskA and TaskB systems are ex-
plained in detail. All features used in the final ex-
periments for both tasks are shown in Table 1.
3.1 TaskA System
TaskA is a classification task where we classify a
given interval as having positive, negative or neutral
sentiment. TaskA feature sets are shown in Table 1.
lexical features: These features use directly
words (or tokens) from tweets as features. single-
word feature uses the word of the single-word inter-
vals, whereas slang features are created for match-
ing uni-grams and bi-grams from our slang dictio-
nary. We also use emoticons as features, as well as
the words or phrases that emit emotion according to
the lexicons described in Section 2.1.
score-based features: These features use the
scores obtained from the AFINN and SentiWordNet
(SWN) lexicons. We use separate scores for the pos-
itive and negative sentiments, since one interval may
contain multiple words with opposite sentiment. In
case of multiple positive or negative occurances, we
take the arithmetic mean of those.
shape-based features: These features capture the
length of an interval, whether it contains a capital-
ized word or all words are capitalized, whether it
contains a URL, or ends with an exclamation mark.
tag-based features: In addition to numeric val-
ues of sentiments, we use the tokens ?positive? and
?negative? to express the type of sentiment. When
multiple words emit a sentiment in a given interval,
their corresponding tokens are appended to create a
single feature out of it, sequences. Moreover, we
have another set of features which also contains the
POS tags of these sentiment words.
indicator features: These features are used in or-
der to expose how many sentiment emitting words
from our currated large lexicon exist in a given inter-
val. hasNegation indicates the presence of a nega-
tion word like not or can?t in the interval, whereas
numOfPosIndicators and numOfNegIndicators gives
the number of tokens that convey positive and nega-
tive sentiment, respectively.
context features: In addition to the features gen-
erated from the given interval, these features capture
the context information from the neighboring inter-
vals. Feature surroundings combines the length of
the interval along with the lengths of the intervals on
both sides, whereas surrounding-shape and extra-
surrounding-shape features use number of positive
and negative sentiment indicators for the intervals.
We also use their normalized forms (those starting
with norm-) where we divide the number of indi-
cators by the length of the interval. Features with
-extra- use two adjacent intervals from both sides.
Intervals that are not available are represented with
NA.
3.2 TaskB System
TaskB is a classification task where we determine
the sentiment (positive, negative, or neutral) of a
tweet. TaskB system uses a rule-based method to
556
Feature Set Feature Example Feature Instance used by
lexical-based
single-word-* single-word-worst A, B
slang-* slang-shit A, Bpos
emoticons-* emoticons-:) A
emitted-emotions-* emitted-emotions-angry A, B
score-based
afinn-positive:#, afinn-negative:# afinn-positive:4, afinn-negative:-2 A, B
swn-positive:#, swn-negative:# swn-positive:2, swn-negative:-3 A
shape-based
length-# length-10 A
hasAllCap-T/F hasAllCap-T A
fullCap-T/F fullCap-T A
hasURL-T/F hasURL-F A, B
endsWExlamation-T/F endsWExlamation-T A, Bneg
tag-based
our-seq-* our-seq-positive-positive A, B
our-tag-seq-*, swn-seq-*, swn-tag-seq-* afinn-seq-positive-a-positive-n A
afinn-seq-*, afinn-tag-seq-* afinn-seq-positive-a-negative-n A
indicators
hasNegation-T/F hasNegation-F A
numOfPosIndicators-# numOfPosIndicators-2 A
numOfNegIndicators-# numOfNegIndicators-0 A
context
surroundings-#-#-# surroundings-1-2-NA A
surr-shape-#-#-# surrounding-shape-NA-2-1 A
extra-surr-shape-#-#-#-#-# extra-surr-shape-NA-2-1-0-1 A
norm-surr-shape-#-#-# norm-surr-shape-0.5-0.2-0.0 A
norm-extra-surr-shape-#-#-#-#-# norm-extra-surr-shape-NA-0.5-0.2-0.0-0.2 A
left-sentiment-*, right-sentiment-* left-sentiment-positive A
twitter-tags
hasEmoticon-T/F hasEmoticon-T B
hasMention-T/F hasMention-T B
hasHashtag-T/F hasHashtag-F B
[emoticon|mention|hash]-count-# mention-count-3 B
repetition
unigram-*n unigram-[no+] B
$character-count-# o-count-7 B
lastword
lastword-*n lastword-[OMG+] B
lastwordshape-* lastwordshape-XXXX B
chat chatword-* for word ?gz?: chatword-congratulations B
interjection interjection-*n interjection-[lo+l] B
negation
negword-*n negword-never Bneg
negword-count-# negword-count-3 Bneg
negcapword-count-# negcapword-count-1 Bneg
hash
hashword-* hashword-good B
hashtag-#* hashtag-#good B
hash-sentiment-[positive|negative] hash-sentiment-positive B
lingemotion [noun|verb|adverb|adjective]-$emotion noun-fear B
oursent
for tweet: a nice morning.. I hate work.. damn!
oursent-* oursent-nice, oursent-hate, oursent-damn B
oursent-longseq-* oursent-longseq-pnn B
oursent-shortseq-* oursent-shortseq-pn B
oursent-first-last-* oursent-first-last-pn B
afinn-phrases
phrase-firstsense-[positive|negative] phrase-firstsense-positive B
phrase-lastsense-[positive|negative] phrase-lastsense-negative B
afinnword-* afinnword-nice, afinnword-hate, afinnword-damn B
afinn-firstsense-[positive|negative] afinn-firstsense-positive B
afinn-lastsense-[positive|negative] afinn-lastsense-positive B
emo emo-pattern-* for =) : emo-pattern-HAPPY B
Table 1: Feature sets used in TaskA and TaskB
557
Dataset Type Positive Negative Neutral+Objective Tot. No. of Instances
TaskA
Training 5290 (5865) 2771(3120) 16118 (17943) 24179 (26928)
Development 589 (648) 392 (430) 1993 (2202) 2974 (3280)
Test 2734 1541 160 4435
TaskB
Training 3274 (3640) 1291 (1458) 4155 (4586) 8720 (9684)
Development 523 (575) 309 (340) 674 (739) 1506 (1654)
Test 1572 601 1640 3813
Table 2: Number of instances used in TaskA and TaskB
decide on the sentiment label of a tweet. For each
tweet, the probabilities of belonging to the posi-
tive class (Probpos) and negative class (Probneg)
are computed by the Bpos and Bneg classifiers, re-
spectively. If Probpos is greater than Probneg, and
greater than a predefined threshold, then the tweet
is classified as ?positive?, otherwise it is classified
as ?neutral?. On the other hand, if Probneg is
greater than Probpos, and greater than the prede-
fined threshold, then the tweet is classified as ?neg-
ative?, otherwise it is classified as ?neutral?. The
threshold is set to 0.45, since it gives the optimal F-
score on the development set. TaskB features along
with examples are shown in Table 1.
twitter-tags: hasEmoticon, hasMention, ha-
sURL, and hasHashtag indicate whether the corre-
sponding term (e.g. mention) exists in the tweet.
repetition: Words with repeating letters are
added as a feature ?n. ?n represents the normalized
version (i.e., no repeating letters) of a word. For ex-
ample, ?nooooooo? is shortened to [no+]. We also
keep the count of the repeated character.
wordshape: Shape of each word in a tweet is con-
sidered. For example, the shape of ?NOoOo!!? is
?XXxXx!!?.
lastword: The normalized form and the shape of
the last word are used as features. For example, if
the lastword is ?OMGG?, then lastword ?[OMG+]?
and lastwordshape ?XXXX? are used as features.
chat: A list of chat abbreviations that express sen-
timent is manually created. Each abbreviation is re-
placed by its corresponding word.
interjection: An interjection is a word that ex-
presses an emotion or sentiment (e.g. hurraah,
loool). Interjection wordn is used as a feature.
negation: We manually created a negation list ex-
tended by word clusters from (Owoputi et al, 2013).
A negation word is represented by spellings such
as not, n0t, and naht. Each negation wordn (e.g
neve[r+]) is considered. We keep the count of nega-
tion words and all capitalized negation words.
hash: If the hashtag is ?#good? then #good and
good become hash features. If the hashtag is a sen-
timent expressing word according to our sentiment
word list, then we keep the sentiment information.
lingemotion: Nodebox Linguistics3 package
gives emotional values of words for expressions of
emotions such as fear and sadness. POS augmented
expression information is used as a feature.
oursent: Each word in a tweet that exists in our
sentiment word list is considered. When multiple
sentiment expressing words are found, a sentiment
sequence feature is used. oursent-longseq keeps
the long sequence, whereas oursent-shortseq keeps
same sequence without repetitive sentiments. We
also consider the first and last sentiments emitted by
a tweet.
afinn: We consider each word that exists in
AFINN. If a negation exists before this word, the
opposite sentiment is considered. For example, if a
tweet contains the bigram ?not good?, then the senti-
ment of the bigram is set to ?negative?. The AFINN
scores of the positive and negative words, as well as
the first and last sentiments emitted by the tweet are
considered.
phrases: Each n-gram (n > 1) of a tweet that
exists in our sentiment phrase list is considered.
afinn-phrases: Phrases are retrieved using the
phrases feature. Each sentiment that appears in
a phrase is kept, hence we obtain a sentiment se-
quence. The first and last sentiments of this se-
quence are also considered. Then, the phrases are
removed from the tweet text and the afinn feature is
applied.
emo: We manually created an emoticon list where
3http://nodebox.net/code/index.php/Linguistics
558
each term is associated with an emotion pattern such
as HAPPY. These emotion patterns are used as a fea-
ture.
others: Bpos uses the slang feature from the lexi-
cal feature set, and Bneg uses endsWExlamation fea-
ture from the indicators feature set.
4 Experiments and Results
4.1 Data
The data set provided by the task organizers was an-
notated by using Amazon Mechanical Turk4. The
annotations of the tweets in the training and devel-
opment sets were provided to the task participants.
However, the tweets had to be downloaded from
Twitter by using the script made available by the or-
ganizers. We were unable to download all the tweets
in the training and development sets, since some
tweets were deleted and others were not publicly
accessible due to their updated authorization status.
The number of actual tweets (numbers in parenthe-
ses) and the number of collected tweets are shown in
Table 2. Almost 10% of the data for both tasks are
missing. For the test data, however, the tweets were
directly provided to the participants.
4.2 Results on TaskA
We start our experiments with features generated
from lexicons and emoticons. Called our baseline,
it achieved an f-score of 47.8 on the devset in Ta-
ble 3. As we add other features at each step, we
reach an average f-score of 81.6 on the devset at
the end. Among those features, the most contribut-
ing ones are lexical feature single-word, indicator
feature hasNegation, and especially shape feature
length. The success of the length feature is mostly
due to the nature of intervals, where the long ones
tend to be neutral, and the rest are mostly positive
or negative. Another noteworthy result is that our
curated word list contributed more compared to the
others. When the final model is used on the test set,
we get the results in Table 5. Having low neutral f-
score might be due to the fact that there were only a
few neutral intervals in the test set, which might in-
dicate that their characteristics may not be the same
as the ones in the devset.
4https://www.mturk.com/mturk/
Added Features Avg. F-Score
afinn-positive, afinn-negetive
47.8swn-positive, swn-negative,
emoticons, emitted-emotions
+ hasAllCap, fullCap, hasURL,
50.1
endsWExclamation
+ slang 51.5
+ single-word 56.8
+ afinn-seq, swn-seq, afinn-tag-seq,
57.7
swn-tag-seq
+ our-seq, our-tag-seq 60.2
+ hasNegation 64.8
+ numOfPosIndicators,
65.3
numOfNegIndicators
+ length 75.2
+ left-sentiment, right-sentiment 76.5
+ surroundings, surrounding-shape 78.9
+ extra-surrounding-shape 80.6
+ norm-surrounding-shape,
81.6
norm-extra-surrounding-shape
Table 3: Macro-averaged F-Score on the TaskA dev. set
Added Features
Average
F-Score
oursent (baseline) 58.59
+ afinn-phrases 64.64
+ tags + hash 65.43
+ interjection + chat 65.53
+ emo + lingemotion 65.92
+ repetition + lastword 66.01
+ negation + others 66.32
Table 4: Macro-averaged F-Score on the TaskB dev. set
4.3 Results on TaskB
The baseline model is considered to include oursent
feature that gives an average f-score of 58.59. Next,
we added the afinn-phrases feature which increased
the average f-score to 64.64. This increase can be
explained by the sentiment scores and sequence pat-
terns that afinn-phrases is based on. Following that
model, the other added features slightly increased
the average f-score to 66.32 as shown in Table 4.
The final model is used over the test set of TaskB,
where we obtained an f-score of 63.53 as shown in
Table 5.
559
Class Precision Recall F-Score
TestA
positive 89.7 88.3 89.0
negative 86.6 82.7 84.6
neutral 10.7 18.1 13.4
average(pos+neg) 88.15 85.5 86.8
TestB
positive 82.3 55.6 66.4
negative 48.7 80.2 60.6
neutral 68.2 73.3 70.7
average(pos+neg) 65.56 67.93 63.53
Table 5: Results on the test sets for both tasks
5 Conclusion
We presented two systems one for TaskA (a Maxi-
mum Entropy model) and one for TaskB (Maximum
Entropy + Naive Bayes models) based on using rich
feature sets. For Task A, we started with a baseline
system that just uses ordinary features like sentiment
scores of words. As we added new features, we ob-
served that lexical features and shape-based features
are the ones that contribute most to the performance
of the system. Including the context features and the
indicator feature for negations led to considerable
improvement in performance as well. For TaskB,
we first created a baseline model that uses sentiment
words and phrases from the AFINN lexicon as fea-
tures. Each feature that we added to the system re-
sulted in improvement in performance. The nega-
tion and endsWExclamation features only improved
the performance of the negative classifier, whereas
the slang feature only improved the performance of
the positive classifier.
Our results show that using rich feature sets with
machine learning algorithms is a promising ap-
proach for sentiment classification in Twitter. Our
TaskA system ranked 3rd among 23 systems and
TaskB system ranked 4th among 35 systems partici-
pating in SemEval 2013 Task 2.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An Enhanced Lex-
ical Resource for Sentiment Analysis and Opinion
Mining. In Proceedings of the Seventh Conference
on International Language Resources and Evaluation
(LREC?10), Valletta, Malta, May. European Language
Resources Association (ELRA).
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy data.
In Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, COLING ?10,
pages 36?44, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22:39?71.
Albert Bifet and Eibe Frank. 2010. Sentiment knowl-
edge discovery in twitter streaming data. In Proceed-
ings of the 13th international conference on Discov-
ery science, DS?10, pages 1?15, Berlin, Heidelberg.
Springer-Verlag.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ?10, pages 241?249, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Xiaoxu Fei, Huizhen Wang, and Jingbo Zhu. 2010. Sen-
timent word identification using the maximum entropy
model. In International Conference on Natural Lan-
guage Processing and Knowledge Engineering (NLP-
KE), pages 1?4.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
twitter: annotation, features, and experiments. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers - Volume 2, HLT
?11, pages 42?47. Association for Computational Lin-
guistics.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
Technical report, Stanford University.
Edward Loper and Steven Bird. 2002. Nltk: the natural
language toolkit. In Proceedings of the ACL-02 Work-
shop on Effective tools and methodologies for teach-
ing natural language processing and computational
linguistics - Volume 1, ETMTNLP ?02, pages 63?70,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
George A. Miller. 1995. Wordnet: A lexical database for
english. Communications of the ACM, 38:39?41.
Finn A?. Nielsen. 2011. A new ANEW: Evaluation of
a word list for sentiment analysis in microblogs. In
Proceedings of the ESWC2011 Workshop on ?Making
Sense of Microposts?: Big things come in small pack-
ages.
560
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From Tweets to Polls: Linking Text Sentiment to
Public Opinion Time Series. In Proceedings of the
International AAAI Conference on Weblogs and Social
Media.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conver-
sational text with word clusters. In Proceedings of
NAACL.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General Inquirer:
A Computer Approach to Content Analysis. MIT
Press.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara
Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013.
SemEval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the International Workshop on Se-
mantic Evaluation, SemEval ?13, June.
Le Zhang. 2011. Maximum entropy modeling toolkit for
python and c++. http://homepages.inf.ed.
ac.uk/lzhang10/maxent_toolkit.html.
Accessed: 2013-04-13.
561
