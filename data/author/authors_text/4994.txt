Detection of Language (Model) Errors 
K.Y. Hung, R.W.P. Luk, D. Yeung, K.F.L. Chung and W. Shu 
Department ofComputing 
Hong Kong Polytechnic University 
Hong Kong 
E-mail: {cskyhung, csrluk, csdaniel, cskchung, cswshu}@comp.polvu.edu.hk 
Abstract 
The bigram language models are popular, in 
much language processing applications, in 
both Indo-European and Asian languages. 
However, when the language model for 
Chinese is applied in a novel domain, the 
accuracy is reduced significantly, from 96% to 
78% in our evaluation. We apply pattern 
recognition techniques (i.e. Bayesian, decision 
tree and neural network classifiers) to 
discover language model errors. We have 
examined 2 general types of features: model- 
based and language-specific eatures. In our 
evaluation, Bayesian classifiers produce the 
best recall performance of 80% but the 
precision is low (60%). Neural network 
produced good recall (75%) and precision 
(80%) but both Bayesian and Neural network 
have low skip ratio (65%). The decision tree 
classifier produced the best precision (81%) 
and skip ratio (76%) but its recall is the lowest 
(73%). 
Introduction 
Language models are important post-processing 
modules to improve recognition accuracy of a 
wide variety of input, namely speech recognition 
(Balh et al, 1983), handwritten recognition 
(Elliman and Lancaster, 1990) and printed 
character ecognition (Sun, 1991), for many 
human languages. They can also be used for text 
correction (Ron et al, 1994) and part-of-speech 
tagging. 
For Indo-European languages, the word-bigram 
language model is used in speech recognition 
(Jelinek, 1989) and handwriting recognition 
(Nathan et al, 1995). Various ways to improve 
language models were reported. First, the model 
has been extended with longer dependencies ( .g. 
trigram) (Jelinek, 1991) and using non-contiguous 
dependencies, like trigger pairs (Rosenfeid, 1994) 
or long distance n-gram language models (Huang 
et al, 1993). For better probability estimation, the 
model was extended to work with (hidden) word 
classes (Brown et al, 1992, Ward and Issar, 1996). 
A more error-driven approach is the use of hybrid 
language models, in which some detection 
mechanism (e.g. perplexity measures \[Keene and 
O'Kane, 1996\] or topic detection \[Mahajan et al, 
1999\]) selects or combines with a more 
appropriate language model. 
For Asian languages (e.g. Chinese, Japanese and 
Korean) represented by ideographic haracters, 
language models are widely used in computer 
entry because these Asian languages have a large 
set of characters (in thousands) that the 
conventional keyboard is not designed for. Apart 
from using speech and handwriting recognition 
for computer entry, language models for Asian 
languages can be used for sentence-based 
keyboard input (e.g. Lochovsky and Chung, 1997), 
as well as detecting improper writing (e.g. dialect- 
specific words or expressions). 
Unlike Indo-European languages, words in these 
Asian languages are not delimited by space and 
conventional approximate string matching 
techniques (Wagner and Fisher, 1974; Oommen 
and Zhang, 1974) in handwriting recognition are 
seldom used in Asian language models. Instead, a
widely used and reported Asian language model is 
the character-bigram language model (Jin et al, 
1995; Xia et al, 1996) because it (1) achieved 
high recognition accuracy (around 90-96%) (2) is 
easy to estimate model parameters (3) can be 
processed quickly and (4) is relatively easy to 
implement. 
Improvement of these language models for Indo- 
European languages can be applied for the Asian 
languages but words need to be identified. For 
Asian languages, the model was integrated with 
syntactic rules (Chien, Chen and Lee, 1993). 
Class based language model (Lee and Tung, 1995) 
was also examined but the classes are based on 
semantically related words. A-new approach 
(Yang et al, 1998) is reported using segments 
87 
expressed by prefix and suffix trees but the 
comparison is based on perplexity measures, 
which may not correlate well with recognition 
improvement (Iyer et al, 1997). 
While attempts to improve the; (bigram) language 
models were (quite) successful, the high 
recognition accuracy (about 96%) is not adequate 
for professional data entry services, which 
typically require an error rate lower than 1 in 
1,000. As part of the quality control exercises, 
these services estimate their error rate by 
sampling, and they identify and correct he errors 
manually to achieve the required quality. Faced 
with a large volume of text, the ability to 
automatically identify where the errors are is 
perhaps more important than automatically 
correcting errors, in post-editing because (1) 
manual correction is more reliable than automatic 
correction, (2) manual error sampling can be 
carried out and (3) more manual efforts are 
required in error identification than correction due 
to the large volume of text. For example, if the 
identification of errors is 97% and there are no 
errors in error correction, then the accuracy of the 
language model is improved from 96% to 99.9% 
after error correction. 
In typical applications, the accuracy of the bigram 
language model may not be as high as those 
reported in the literature because the data may be 
in a different genre than that of the training data. 
For evaluation, we tested a bigram language 
model with text from a novel domain and its 
accuracy dropped significantly from 96% to 78%, 
which is similar to English (Mahajan et al, 1999). 
Improvement in the robustness of the bigram 
language model across different genre is 
necessary and several approaches are available, 
based on detecting errors of the language model. 
One (adaptive) approach is to automatically 
identify the errors and manually correcting them. 
The information about the correction of errors is 
used to improve the bigram language model. For 
example, the bigram probabilities of the language 
model may be estimated and updated with the 
corrected ata. In this way, future occurrences of 
these rrors are reduced. 
Another (hybrid) approach uses another language 
model to correct the identified errors. This 
language model can be computationally more 
expensive than the bigram language model 
because it is applied only to the identified errors. 
Also, topic detection (Mahajan et al, 1999) and 
language model selection (Keene and O'Kane, 
1996) can be applied to those area to find a more 
appropriate language model because usually 
topic-dependent words are those causing errors. 
Another (integrative) approach improves the 
language model accuracy using more 
sophisticated recognizers, instead of a 
complementary language model. The more 
sophisticated recognizer may give a set of 
different results that the bigram language model 
can re-apply on or this recognizer simply gives 
the recognized character. This integrates well with 
the coarse-fine recognition architecture proposed 
by Nagy (1988) back in the 1960s. Coarse 
recognition provides the candidates for the 
language model to select. Fine, expensive 
recognition is carried out only where the language 
models failed. Finally, it is possible to combine all 
the different approaches (i.e. adaptive, hybrid and 
integrative). 
Given the significance in detecting errors of 
language models, there is little work in this area. 
Perhaps, it was considered that these errors were 
random and therefore hard to detect. However, 
users can detect errors quickly. We suspect hat 
some of these errors may be systematic due to the 
properties of the language model used or due to 
language specific properties. 
We adopt a pattern recognition app~'~z, ch to 
detecting errors of the bigram language rnoaei for 
the Chinese language. Each output is assigned to 
either the class of correct output or the class of 
errors. The assignment of a class to an output is 
based on a set of features. We explore a number 
of features to detect errors, which are classified 
into model-based features and language-specific 
features. 
The proposed approach can work with Indo- 
European languages at the word-bigram level. 
However, language-specific eatures have to be 
discovered for the particular language. In addition, 
this approach can be adopted for n-gram language 
models. In principal, the model-based features can 
be found or evaluated similar to the bigram 
language model. For example, if the trigram 
probability (instead of bigram probability) is low, 
then the likelihood of a language model error is 
high. 
This paper is organized as follows. Section 1 
discusses various features and some preliminary 
evaluation of their suitability for error 
88 
identification. Section 2 describes 3 types of 
classifiers used. In section 3, our evaluation is 
reported. Finally, we conclude. 
1. Features 
We evaluate individual features for error detection 
because they are important to the success of 
detection. Articles from Yazhou Zhoukan (YZZK) 
magazine (4+ Mbytes)/PH corpus (Guo and Liu, 
1992) (7+ Mbytes) are used for evaluation. We 
use the recall and precision measurements for 
evaluation. The recall is the number of errors 
identified by a particular feature divided by the 
total number of errors. The precision is the 
number of errors identified by a particular feature 
divided by the total number of times the feature 
indicate that there are errors. In the first 
subsection, we describe some model-based 
features. Next, we describe the language-based 
features. In the last subsection, we discuss the 
combined use of both types of features. 
1.1 Model-based features 
The bigram language model selects the most 
likely path Pm~ out of a set S. The probability of a 
path s in S is simply the product of the conditional 
probabilities of one character c, after the other c~.l 
where s = Co.C+..c:s:, after making the Markov 
assumption. Formally, 
Pm~ = arg max {p(s)} 
s~S 
= arg~ax{p(Co)I~IP(cilC,_l)\[coC,...c,,== s} 
The set s is generated by the set of candidate 
characters for each recognition output. The 
recognizer may supply the set of candidate 
characters. Alternatively, a coarse recogniser may 
simply identify the best-matched group or class of 
characters. Then, members of this class are the 
candidate characters. Formally, we use a function 
h(.), that maps the recognition position to a set of 
candidate characters, i.e. h(i) = {ciJ. We can also 
define the set of sentences in terms of h(.), i.e. S = 
{s I s = cocz...c,, ~ ,  c, ~ h(i)}. 
1.1.1 Features based on zero probabilities (Fl,t) 
One feature to detect errors is to count the number 
of conditional probabilities p(cilc~.l) that are zero, 
between 2 consecutive positions. Zero conditional 
probabilities may be due to insufficient raining 
data or may be because they represent the 
language properties. Figure 1 shows the likelihood 
of an error occurring against he percentage of the 
conditional probabilities that are zero. 
60% 
50% 
40% - -  
30% 
20% 
10% 
0% 
50% 60% 70% 80% 90% 100% 110% 
Figure 1: The language model output errors against 
percentages ofzero conditional probabilities. 
1.1.2 Features based on low probability (Fl,z) 
When there are insufficient data, the conditional 
probabilities that are small are not reliable. IfPm~ 
have selected some conditional probabilities that 
are low, then probably there are no other choices 
from the candidate sets. Hence, the insufficient 
data problem may occur in that particular Pm~. 
In Figure 2, we plot the likelihood of errors 
identified against the different logarithmic 
conditional probability values. When the recall 
increases, unfortunately, the precision drops. 
120% \[ 
100% 
\[ : ~ prec i s ion  .~ ,~ ' -  ! 
80% l : ~  reca l l  . . . . . . .  / | Accuracy 
L 6?% \[ , ,  - . . . .  
t . . . . . . . . . . . . . . . . . . . .  - 
! I 
20% . . . .  - \  . . . . . . . . . . . . . . . .  . . . . . . . . .  - 
I \] I I I  \[ 
0% I 
~ .  . .  . . .  . . . .  . 
Figure 2: The precision, recall and accuracy (i.e. recall 
x precision) of detecting language model errors by 
examining the logarithm conditional probabilities on 
the maximum likelihood path. 
1.2 Language-specific Features 
The language-specific features are based on 
applying the word segmentation algorithm (Kit et 
al., 1989) to the maximum likelihood path. The 
ROCLING (Chen and Huang, 1993) word list 
used for segmentation has 78,000+ entries. 
89 
1.2.1 Features based on word length (F2,O 
If the matched word in the maximum likelihood 
path is long, then we expect he likelihood of an 
error is low because long words are specific. 
Figure 3 shows the precision of  detecting the 
matched word is correct and the recall of errors in 
multi-character words. In general, the longer the 
matched words, the more likely that they are 
correct and the likelihood of missing undetected 
long words is small. 
120% 
lOO% 
00% 
60% 
40% 
20% 
o; 
? precision .. 
1 2 3 4 5 6 7 a 
Figure 3: The precision of correct matched words 
against word lengths. 
1.2.2 Features based on single-character 
sequences (F2,z) 
In word segmentation, when there are no entries 
in  the dictionary that can match, the input is 
segmented into single characters. Thus, Lin et al
(1993) noted that single-character sequences after 
word segmentation might indicate segmentation 
problems. Here, we apply the same technique for 
the detection of errors. If we count on the per 
character basis, the recall of error is 80% and the 
precision in error identification is 35%. If we 
count multi-character words and a sequence of 
single-characters a blocks, then the recall of 
errors is 79% and the precision in finding one or 
more errors in the block is increased to 51%. 
120% . . . . . . . . . . . .  
20% . . . . . . . .  : 
0% . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
1 2 3 4 5 6 7 8 
Figure 4: The precision and recall of single-character 
sequences of different lengths. 
Similar to matched words in the maximum 
likelihood path, the error detection performance of
single-character sequences may depend on their 
length. Therefore, we plotted the recall and 
precision of detecting errors against he length of 
the single-character sequences. According to 
Figure 4, as the length of the single-character 
sequence is large, the likelihood of an error is 
larger. The recall of errors is particularly low for 
single-character sequences that have 2 characters. 
The other single-character sequences (i.e. its 
length is not equals to 2) have almost 100% recall. 
One possible reason why 2 single-character 
sequences achieved low precision is that there are 
many spurious bigrams and therefore false match. 
1.3 Combined use of  Features 
We carried out a preliminary study using the 
features mentioned in subsection 1.1 and 1.2. Our 
Bayesian classifier (Section 2.1) achieved 83% 
recall but 35% precision, which can be achieved 
using language specific features only (Fz2). 
Therefore, we try to combine the use of these 
features in a more carefiJl manner. We divided the 
detection into 3 scenarios: (1) single character 
(feature F2,2); (2) single-character sequence of 
length 2 (feature F2,2) and (3) 2 character words 
(feature Fzl). Each case is assigned a classifier to 
detect errors. Single-character sequences longer 
than 2 are considered as having errors (Figure 4). 
Words of length longer than 2 are considered 
correct (Figure 3). 
1.3.1 Single characters 
After word segmentation, single characters are 
those cases when there are no multi-character 
words in the dictionary that can match with it and 
its following substring. The single characters have 
different part-of-speech tags. 
L~ l .......................................................................................................................................................................... i 
| i . . . . . . . . . . . . . . . . . . . . . .  - -~  
B--ti 
Pa~cfgm:h 
Figure 5: The single characters and their corresponding 
language model output accuracy for different part-of- 
speech tags. 
Figure 5 shows that the accuracy of the language 
model for these single characters with part-of- 
90 
speech tags related to exclamations are low. For 
error detection, a feature is assigned to each part- 
of-speech tag. 
The language model accuracy for single 
characters may depend on the availability of the 
left and right context to form high probability 
bigrams. Therefore, we expect that language 
model accuracy of single characters at the 
beginning (70%) and end (70%) of a sentence is 
lower than those in the middle (85%) of the 
sentence. The worst case occurs when the 
sentence has only a single character, where the 
measured accuracy is only 8.75% (i.e. no bigram 
context). 
1.3.2 Two-single-characters sequence 
Figure 6 shows that language model output 
accuracy increases as the bigram probability of 
single-character sequences of length 2 increases. 
Hence, the bigram probabilities can be used as a 
feature for detection. 
1.2 "~ 
................... . /<  il 
. ~ / \ !  
Figure 6: The bigram (logarithm) probability of the 
single-character s quence of length 2. 
Similar to single characters, the language model 
accuracy for 2-single-characters sequences at the 
start, middle and end of a sentence are 48%, 47% 
and 30%, respectively. The accuracy is 33% if the 
sentence is the 2-single-characters sequence. 
I~ -  . . . . . . . . . . .  ) - -  
Q4 F . . . . . .  ~ . . . . . . . . . . . . . . . . . . . . . . . . . .  . /~" - - - - - - *  . . . . . . .  
ace  ) " ,~ ._~, )_____ , ) . _~_ .___  
0) 
0 1 2 3 4 5 6 7 8 
nmi:er ofhiddm,,u:rds 
Figure 7: Language model accuracy against different 
number of hidden words (see text). 
Another feature for 2-single-characters sequences 
is to examine whether the characters in the two 
candidate sets can form words that match with the 
dictionary. These matched words are called 
hidden words. Figure 7 shows that if there are 
hidden words, the language model accuracy 
dropped from 60% to 25%. Since there are not 
many cases with 6-8 hidden words, the accuracy 
for these cases are not reliable. 
1.3.3 Two-character words 
For 2 character words, the bigram probability 
(Figure 8) can be used as a feature similar to the 
single-character sequences. The position of these 
2 character words in the sentence does not relate 
to the language model accuracy. Our measured 
accuracy is 91%, 89% and 91% for the beginning, 
the end and the middle of the sentence, 
respectively. Even sentences with a single 2- 
character word achieved 90% accuracy. Hence, 
there is no need to assign features for the position 
of the 2 character words in a sentence. Similar to 
2-single-characters sequences, the language model 
accuracy (Figure 9) decreases as the humber of 
hidden words increase in the corresponding 2 sets 
of candidate characters. 
1.7 : 
o.4~- ::, !i 
O.2 ~ i ~ !~ 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~ ~+*~,-.+~.~.~.~- : .~.~.~+~..+~.~+,+~*~.~+~ 
Figure 8: The language model accuracy of 2 character 
words against the bigram probability. 
= 
1 . . . . . . . . . . . . .  
o.7 ~ . . . . . . . . . . . . .  
0.6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ~ . - - -2 . .  : : - '= -* - - .  
0,5 . . . . . . . . . . . . .  
0,4 
0,3 m 
0 .2 -  
0,1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
1 2 3 4 5 6 7 8 9 
number of  hidden words 
Figure 9: The language model accuracy against 
different number of hidden words. 
91 
2 Classifiers 
One of the problems witlh using individual 
features is that the recall and precision are not 
very high, except he language-specific features. It
is also difficult to set the threshold for detection 
because of the precision-recall trade-off. In 
addition, there may be some improvement in 
detection performance if features are combined 
for detection. Therefore, we adopt a pattern 
recognition approach to detect errors. 
Several classifiers are used to decide for error 
identification because we do not know whether 
particular features work well with particular 
classifiers, which make different assumptions 
about classification. Three types of classifers will 
be examined: Bayesian, decision tree and neural 
network. 
2.1 Bayesian Classif ier 
The Bayesian classifier is simple to implement 
and is compatible with the model-based features. 
Given the feature vector x, the Bayesian detection 
scheme assigns the correct class wc and the error 
class we, using the following rule: 
g~(x) > ge(X) assign wc 
.Otherwise assign we 
where go(.) and ge(.) are: 
gc (x) = -(x -/~c)r y.c-. (x _/~,)- log l , I +2 log p(w c ) 
g, (x)= - (x - lee)  r Z,-~(x-/~,) - log\] Z~ \] +2log p(w,) 
Pc and ,ue are the mean vectors of  the class wc and 
we, respectively, ~ and ~ are the covariance 
matrices of the class wc and we, respectively, and 
1-I is the determinant. 
2.2 Decision Tree 
Originally, we tried to use the support vector 
machine (SVM) (Vanpik, 1995) but it could not 
converge. Instead, we used the decision tree 
algorithm C4.5 by Quinlan (1993). Decision trees 
are known to produce good classification if 
clusters can be bounded by some hyper-rectilinear 
regions. We trained C4.5 with a set of feature 
vectors, described in Section 1.3. 
2.3 Neural  Network  
We use the multi-layer perceptron (MLP) because 
it can perform non-linear classification. The MLP 
has 3 layers of nodes: input, hidden and output. 
Nodes in the input layer are fully connected with 
those in the hidden layer. Likewise nodes in the 
hidden layer are fully connected to the output 
layer. For our application, one input node 
corresponds to a feature in section 1.3. The value 
of the feature is the input value of the node. Two 
output nodes indicate whether the current 
character is correct or erroneous. The number of 
hidden nodes is 2-4, calculated according to 
(Fujita, 1998). 
The output of each node in the MLP is the 
weighted sum of its input, which is transformed 
by a sigmoidal function. Initially, the weights are 
assigned with small random numbers, which are 
adjusted by the gradient descend method with 
learning rate 0.05 and momentum 0.1. 
3 Evaluation 
In the evaluation, the training data is the PH 
corpus and the test data is the YZZK magazine 
articles (4+ Mbytes), downloaded from the 
Internet. In handwritten character recognition, the 
optimal size of the number of candidates is 6 
(Wong and Chan, 1995). For robustness, each 
recognized character in our evaluation is selected 
from 10 candidates. 
We measured the performance in terms of recall, 
precision and the manual effort reduction in 
scanning the text for errors. The recall is the 
number of identified errors over the total number 
of errors. The precision is the number of identified 
errors over the total number of  cases classified as 
errors. The amount of saving in manual scanning 
for errors is called the skip ratio, which is the 
number of blocks classified as correct over the 
total number of blocks. The recall and the skip 
ratio are more important than the precision 
because post error correction (manual or 
automatic) can improve the recognition accuracy. 
It is possible to combine the recall and precision 
into one, using the F measures (Van Rijsbergen, 
1979) but the value for rating the relative 
importance is subjective. 
Table 1 shows the classification performance of 
the Bayesian classifier. The recall of errors by the 
Bayesian classifier has reduced slightly from 83% 
using a single classifier to 79% using 3 classifiers 
but the precision improved from 51% to 60%. 
Also, the skip ratio is 65%, which is much higher 
than the skip ratio of 0.1% if we did not use the 
classifier. Although the MLP has a higher 
precision (80%), its recall is slightly lower than 
92 
the Bayesian classifier. The skip ratio of the both 
Bayesian and MLP classifiers are about he same. 
Cases Measure 
Single Recall 
character 
Precision 
2 single Recall 
characters 
Precision 
2-character Recall 
words 
Precision 
Overall Recall 
Precision 
Skip Ratio 
Bayes 
71% 
40% 
60% 
88% 
60% 
29% 
79% 
60% 
65% 
Table 1: The performances 
C4.5 MLP 
56% 28% 
75% 71% 
84% 83% 
82% 80% 
17% 9% 
60% 62% 
73% 75% 
81% 80% 
76% : 66% 
of the 3 types of 
classifiers in detecting language model errors. 
4 Summary and Future Work 
We have evaluated both model-based and 
language-specific eatures for detecting language 
model errors. Individual model-based features did 
not yield good detection accuracy, suffering from 
the precision-recall trade-off. The language- 
specific features detect errors better. In particular, 
matched multi-character words are usually correct. 
If the model-based and language-specific features 
are aggregated asa single feature vector, the recall 
and precision of errors are 83% and 35%, 
respectively, which are the same if we just use 
language-specific features. Hence, instead of a 
single classifier, we separated 3 situations 
identified by the language-specific eatures and 3 
classifiers are used to detect these errors 
individually. The Bayesian classifier (simpliest) 
achieved an overall 79% recall, 60% precision 
and 65% skip ratio and the MLP achieved an 
overall 75% recall, 80% precision and a 66% skip 
ratio. Similar recall and precision performances 
are achieved using decision trees, which are 
preferred since their skip ratio is higher (i.e. 76%). 
Although the precision (so far) is not high (60% - 
80%), it is not the most important result because 
(1) this only represents a minor waste of checking 
effort, compared with scanning the entire text, and 
(2) the identified errors will be checked further or 
corrected either manually or automatically. 
Acknowledgement 
This work is supported by the (Hong Kong) 
University Grants Council under project PolyU 
5109/97E and the PolyU Central Research Grant 
G-$603. We are grateful to Guo and Liu for 
providing the PH corpus and ROCLING for 
providing their word list. 
References 
Bahl, L.R., F. Jelinek and R.L. Mercer (1983) "A 
maximum likelihood approach to continuous 
speech recognition", 1EEE Trans. PAM1, 5:2, pp. 
179-190. 
Brown, P.F., V.J. Della Pietra, P.V. deSouza nd 
R.L. Mercer (1992) "Class-based n-gram models 
of natural anguage", Computational Linguistics, 
4, pp.467-479. 
Chen, K.-C. and C.R. Huang (eds.) (1993) 
"Chinese word class analysis", Technical Report 
93-05, Chinese Knowledge Information 
Processing Group, Institute of Information 
Science, Academia Sinica, Taiwan. 
Chien, L.F., Chen, K.J., Lee, L.S. (1993) "A best- 
first language processing model integrating the 
unification grammar and Markov language model 
for speech recognition applications", 1EEE Trans. 
Speech and Audio Processing, 1:2, Page(s): 221 - 
240. 
Elliman, D.G. and I.T. Lancaster (1990) "A 
review of segmentation and contextual analysis 
techniques for text recognition", Pattern 
Recognition, 23:3/4, pp.337-346. 
Fujita, O. (1998) "Statistical estimation of the 
number of hidden units for feedforward neural 
networks", Neural Networks, 11, 851-859. 
Guo, J. and H.C. Liu, "PH - a Chinese corpus for 
pinyin-hanzi transcription", 1SS Technical Report, 
TR93-112-0, Institute of Systems Science, 
National University of Singapore, 1992. 
Huang, X., F. Alleva, H. Hon, M. Hwang,, K. Lee 
and R. Rosenfeld (1993) "The SPHINX-II speech 
recognition system: an overview", Computer 
Speech and Lanaguage, 2 137-148. 
lyer, R., M. Ostendorf and M~-Meteer (1997) 
"Analyzing and predicting language model 
93 
performance", Proc. IEEE Workshop Automatic 
Speech Recognition and Understanding, pp. 254- 
261. 
Jelinek, F. (1989) "Self-organized language 
modeling for speech recognition", in Readings in 
Speech Recognition, Morgan Kayfmann. 
Jelinek, F. (1991) "Up from trigrams", Proc. 
Eurospeech 91, pp. 181 -184. 
Jin, Y., Y. Xia and X. Chang (1995) "Using 
contextual information to guide Chinese text 
recognition", Proc. 1CCPOL '95, pp. 134-139. 
Kenne, P.E. and M. O'Kane (1996) "Hybrid 
language models and spontaneous legal 
discourse", Proc. ICSLP, Vol. 2, pp. 717-720. 
Kit, C., Y. Liu and N. Liang (1989) "On methods 
of Chinese automatic word segmentation", 
Journal of Chinese Information Processing, 3:1, 
13-20. 
Law, H.H-C. and C. Chan (1996) "N-th order 
ergodic multigram HMM for modeling of 
languages without marked word boundaries", 
Proc. COLING 96, pp. 2043-209. 
Lee, H-J. and C-H Tang (1995) "A language 
model based on semantically clustered words in a 
Chinese character recognition system", Proc. 3 rd 
lnt Conf. on Document Analysis and Recognition, 
Vol. 1., pp. 450-453. 
Lin, M-Y., T-H. Chiang and K-Y. Su (1993) "A 
preliminary study on unknown word problem in 
Chinese word segmentation", Proc. ROCLING V1, 
pp.119-141. 
Lochovsky, A.F. and K-H. Chung (1997) 
"Homonym resolution for Chinese phonetic input", 
Communications ofCOLIPS, 7:1, 5-15. 
Mahajan, M., D. Beeferman and X.D. Huang 
(1999) "Improving topic-dependent modeling 
using information retrieval techniques", Proc. 
1EEE ICASSP 99, Vol. 1, pp.541-544. 
Nagy, G. (1988), "Chinese character recognition: 
twenty-five-year retrospective", in Proc. 9th lnt. 
Conf. on Pattern Recognition, Vol. 1, pp. 163-167. 
Nathan, K.S., H.S.M. Beigi, J. Subrahmonia, G.J. 
Clary and H. Maruyama (1995) "Real-time on- 
line unconstrained handwriting recognition using 
statistical methods", 
Oommen, B.J. and K. Zhang (1996) "The 
normalized string editing problem revisited", 
1EEE Trans. on PAM1, 18:6, pp. 669-672. 
Quinlan, J.R. (1993) "C4.5 programs for machine 
learning", Morgan Kaufmann, CA. 
Ron, D., Y. Singer and N. Tishby (1994) "The 
power of Amnesia: learning probabilistic 
automata with variable memory length", to 
appear in Machine Learning 
Rosenfeld, R. (1994) "Adaptive statistical 
language modeling" a maximum entropy 
approach", Ph.D. Thesis, School of Computer 
Science, Carnegie Mellon University, Pittsburgh. 
Sun, S.W. (1991), "A Contextual Postprocessing 
for Optical Chinese Character Recognition", in 
Proc.lnt. Sym. on Circuits and Systems, pp. 2641- 
2644. 
Vapnik, V. (1995) The Nature of Statistical 
Learning Theory, Springer-Verlag, New York. 
Van Rijsbergen (1979) Information Retrieval, 
Butterworths, London. 
Wagner, R.A. and M.J. Fisher (1974) "The string 
to string correction problem", J. ACM, 21:1, pp. 
168-173. 
Ward, W. and S. lssar (1996) "A class based 
language model for speech recognition", Proc. 
1EEE 1CASSP 96, Vol. 1, pp.416-418. 
Wong, P-K. and C. Chan (1999) "Postprocessing 
statistical language models for handwritten 
Chinese character recognizer", IEEE Trans. SMC, 
Part B, 29:2, 286-291. 
Xia, Y., S. Ma, M. Sun, X. Zhu, Y. Jin and X. 
Chang (1996) "Automatic post-processing of off- 
line handwritten Chinese text recognition", Proc. 
ICCC, pp. 413-416. 
Yang, K-C., T-H. Ho, L-F. Chien and L-S. Lee 
(1998) "Statistics-based segment pattern lexicon -
a new direction for Chinese language modeling", 
Proc. 1EEE ICASSP 98, Vol. 1., pp. 169-172. 
94 
 	
 Some Considerations on Guidelines for  
Bilingual Alignment and Terminology Extraction 
 
Lawrence Cheung, Tom Lai, Robert Luk?, Oi Yee Kwong, King Kui Sin, Benjamin K. Tsou 
 
Language Information Sciences Research Centre 
City University of Hong Kong 
Tat Chee Avenue, Kowloon, Hong Kong  
{rlylc, cttomlai, rlolivia, ctsinkk, rlbtsou}@cityu.edu.hk  
?Department of Computing 
Hong Kong Polytechnic University  
Hung Hom, Kowloon, Hong Kong 
csrluk@comp.polyu.edu.hk 
 
Abstract  
Despite progress in the development of 
computational means, human input is still 
critical in the production of consistent and 
useable aligned corpora and term banks. This 
is especially true for specialized corpora and 
term banks whose end-users are often 
professionals with very stringent 
requirements for accuracy, consistency and 
coverage. In the compilation of a high quality 
Chinese-English legal glossary for ELDoS 
project, we have identified a number of issues 
that make the role human input critical for 
term alignment and extraction. They include 
the identification of low frequency terms, 
paraphrastic expressions, discontinuous units, 
and maintaining consistent term granularity, 
etc. Although manual intervention can more 
satisfactorily address these issues, steps must 
also be taken to address intra- and 
inter-annotator inconsistency.  
 
Keyword: legal terminology, bilingual 
terminology, bilingual alignment, 
corpus-based linguistics 
1. Introduction 
Multilingual terminology is an important 
language resource for a range of natural language 
processing tasks such as machine translation and 
cross-lingual information retrieval. The 
compilation of multilingual terminology is often 
time-consuming and involves much manual 
labour to be of practical use. Aligning texts of 
typologically different languages such as Chinese 
and English is even more challenging because of 
the significant differences in lexicon, syntax, 
semantics and styles. The discussion in the paper 
is based on issues arising from the extraction of 
bilingual legal terms from aligned 
Chinese-English legal corpus in the 
implementation of a bilingual a text retrieval 
system for the Judiciary of the Hong Kong Special 
Administrative Region (HKSAR) Government.  
 Much attention in computational 
terminology has been directed to the development 
of algorithms for extraction from parallel texts. 
For example, Chinese-English (Wu and Xia 1995), 
Swedish-English-Polish (Borin 2000), and 
Chinese-Korean (Huang and Choi 2000). Despite 
considerable progress, bilingual terminology so 
generated is often not ready for immediate and 
practical use. Machine extraction is often the first 
step of terminology extraction and must be used in 
conjunction with rigorous and well-managed 
manual efforts which are critical for the 
production of consistent and useable multilingual 
terminology. However, there has been relatively 
little discussion on the significance of human 
intervention. The process is far from being 
straightforward because of the different purposes 
of alignment, the requirements of target users and 
the corpus type. Indeed, there remain many 
problematical issues that will not be easy to be 
resolved satisfactorily by computational means in 
the near future, especially when typologically 
different languages are involved, and must require 
considerable manual intervention. Unfortunately, 
such critical manual input has often been treated as 
an obscure process. As with other human cognitive 
process (T?sou et al 1998), manual terminology 
markup is not a straightforward task and many 
issues deserve closer investigation. 
 In this paper, we will present some 
significant issues for Chinese-English alignment 
 and term extraction for the construction of a 
bilingual legal glossary. Section 2 describes the 
background of the associated bilingual alignment 
project. Section 3 discusses the necessity of 
manual input in bilingual alignment, and some 
principles adopted in the project to address these 
issues. Section 4 provides an outline for further 
works to improve terminology management, 
followed by a conclusion in Section 5. 
2. High Quality Terminology Alignment 
and Extraction 
2.1 Bilingual Legal Terminology in Hong 
Kong 
The implementation of a bilingual legal system in 
Hong Kong as a result of the return of 
sovereignty to China in 1997 has given rise to a 
need for the creation and standardization of 
Chinese legal terminology of the Common Law 
on par with the English one. The standardization 
of legal terminology will not only facilitate the 
mandated wider use of Chinese among legal 
professionals in various legal practices such as 
trials and production of legal documentation 
involving bilingual laws and judgments, but also 
promote greater consistency of semantic 
reference of terminology to minimize ambiguity 
and to avoid confusion of interpretation in legal 
argumentation.  
 In the early 90?s, Hong Kong law drafters 
and legal translation experts undertook the 
unprecedented task of translating Hong Kong 
Laws, which are based on the Common Law 
system, from English into Chinese. In the 
process, many new Chinese legal terms for the 
Common Law were introduced. On this basis, an 
English-Chinese Glossary of legal terms and a 
Chinese-English Glossary were published in 1995 
and 1999 respectively. The legal terminology was 
vetted by the high level Bilingual Laws Advisory 
Committee (BLAC) of Hong Kong. The 
glossaries which contain about 30,000 basic 
entries have become an important reference for 
Chinese legal terms in Hong Kong. The Bilingual 
Legal Information System (BLIS) developed by 
the Department of Justice, HKSAR provides 
simple keyword search for the glossaries and 
laws that are available in both Chinese and 
English. Nevertheless, the glossaries are far from 
being adequate for many different types of legal 
documentation, e.g. contracts, court judgments, 
etc. One major limitation of the BLIS glossary is 
its restricted coverage of legal terminology in the 
Laws of Hong Kong, within a basically 
prescriptive context as when the laws were studied 
at the time of its promulgation. There are other 
important bilingual references (Li and Poon 1998, 
Yiu and Au-Yeung 1992, Yiu and Cheung 1996) 
which focus more on the translation of Common 
Law concepts. These are almost exclusively 
nominal expressions. 
 In 2000, the City University of Hong 
Kong, in cooperation with the Judiciary, HKSAR, 
initiated a research project to develop a bilingual 
text retrieval system, Electronic Legal 
Documentation/Corpus System (ELDoS), which is 
supported by a bilingually aligned corpus of 
judgments. The purpose of the on-going project is 
twofold. First, the aligned legal corpus enables the 
retrieval of legal terms used in authentic contexts 
where the essence and spirit of the laws are tested 
(and contested) in reality, explicated and 
elaborated on, as an integral part of the evolving 
and defining body of important precedent cases 
unique to the Common Law tradition. Second, the 
corpus covers judgment texts involving 
interpretation of different language styles and 
vocabulary from Hong Kong laws. The alignment 
markup also serves as the basis for the compilation 
of a high-quality bilingual legal term bank. To 
complete the task within the tight timeframe, a 
team of annotators highly trained in law and 
language are involved in alignment markup and 
related editing. 
2.2 Need for Human Input 
The legal professionals which are the target users 
of ELDoS have very stringent demands on 
terminology in terms of accuracy, coverage and 
consistency. Aligned texts and extracted terms 
must therefore be carefully and thoroughly 
verified manually to minimize errors. 
Furthermore, many studies on terminology 
alignment and extraction deal predominantly with 
nominal expressions. Since the project aims to 
provide comprehensive information on the 
manifestations of legal vocabulary in Chinese and 
English texts, the retrieval system should not 
restrict users to nominal expressions but should 
also provide reference to many other phenomena 
such as alternation of part-of-speech (POS) (e.g. 
noun-verb alternation) inherent in bilingual texts, 
as will be seen in Section 3.  
 The availability of bilingual corpora has 
made it possible to construct representative term 
 banks. Nonetheless, current alignment and term 
extraction technology are still considered 
insufficient to meet the requirements for high 
quality terminology extraction. In ELDoS project, 
many issues are difficult to be handled 
satisfactorily by the computer in the foreseeable 
future. Although human input is essential for high 
quality term bank construction, the practice of 
manual intervention is not straightforward. 
Indeed, the manual efforts to correct the errors 
can be substantial, and the associated cost should 
not be underestimated. The annotator must first 
go through the entire texts to spot the errors and 
terms left out by the machines. In this process, 
both the source and target materials have to be 
consulted. The annotator must also ensure the 
consistency of the output. As a result, guidelines 
should be set up to streamline the process. 
3. Aspects of Terminology Alignment 
The approach adopted for the manual annotation 
of alignment markup and the maintenance of term 
bank in the ELDoS project will be described. 
Additional caution has been taken in the 
coordination of a team of annotators.  
3.1 Term Frequency 
An important reason for manual intervention in 
bilingual term alignment is the relatively poor 
recall rate for low frequency terms. Many 
extraction algorithms make use of statistical 
techniques to identify multi-word strings that 
frequently co-occur (Wu and Xia 1995; Kwong 
and Tsou 2001). These methods are less effective 
for locating low frequency terms. Of the 16,000 
terms extracted from ELDoS bilingual corpora, 
about 62% occur only once in about 80 
judgments. For high quality alignment and 
extraction, failure to include these low frequency 
terms would be totally unacceptable.  
3.2 Correspondence of Aligned Units 
Because of the different grammatical requirement 
and language style, a term in the source language 
often differs in different ways from the 
corresponding manifestations in the target 
language. These differences could be alternation 
of POS and the use of paraphrastic expressions. 
Although many term banks avoid such variations 
and focus primarily on equivalent nominals or 
verbs, the correspondence of terms between two 
typologically different languages is often more 
complicated. For example, the English nominal 
(?fulfilment?) is more naturally translated into 
Chinese as a verb (?l??, ?????, ????). 
More examples can be found in Table 1. 
 
Alternation of POS  
English  Chinese POS alternation 
The accused 
o+ 
det + adj ~ noun 
hold 
*? 
verb ~ noun 
fulfillment  
?
 
noun ~ verb 
administration  
?D 
noun ~ verb 
repudiation  
l? 
noun ~ neg + verb 
Table 1. Alternation of POS  
 
In some cases, there are simply no equivalent 
words in the target language. Paraphrasing or 
circumlocution may be necessary. Such 
correspondence is far less consistent and obvious 
to be identified by the computer.  
 
Paraphrasing/Circumlocution 
English Chinese 
The judge entered judgment in 
favour of the respondents in 
respect of their claim for arrears 
of wages, and severance payment. 
t?o31
?2??KDI
??9? 
In our view,? z??a?
 
?evidenced by the Defendant's 
letter ? 
?7:+3?}
???1$Ym
??S??? 
Table 2.  Examples of paraphrasing 
 
Because of language differences, legal terms can 
be contextually realized as anaphors in the target 
language. Examples of such correspondence 
would be useful for legal drafting and translation. 
Again, such anaphoric relations are more 
accurately handled by humans. 
 
Anaphoric Relation 
English Chinese 
He was subsequently 
charged?  
9?3??s? 
Liu JA dealt with that 
application on 14 March 
1996 and dismissed it. 
B9?t?W?
?1996?3?14?A?
 ?-??9?? 
Enforcement of a 
Convention award may 
also be refused if the 
award is in respect of a 
matter which is not capable 
of settlement by arbitration.
???*????1
??"lh?X*
?????M???
N+