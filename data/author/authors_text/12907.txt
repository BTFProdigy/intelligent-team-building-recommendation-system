Towards Robust Animacy Classification Using Morphosyntactic
Distributional Features
Lilja ?vrelid
NLP-unit, Dept. of Swedish
Go?teborg University
SE-40530 Go?teborg, Sweden
lilja.ovrelid@svenska.gu.se
Abstract
This paper presents results from ex-
periments in automatic classification of
animacy for Norwegian nouns using
decision-tree classifiers. The method
makes use of relative frequency measures
for linguistically motivated morphosyn-
tactic features extracted from an automati-
cally annotated corpus of Norwegian. The
classifiers are evaluated using leave-one-
out training and testing and the initial re-
sults are promising (approaching 90% ac-
curacy) for high frequency nouns, however
deteriorate gradually as lower frequency
nouns are classified. Experiments at-
tempting to empirically locate a frequency
threshold for the classification method in-
dicate that a subset of the chosen mor-
phosyntactic features exhibit a notable re-
silience to data sparseness. Results will be
presented which show that the classifica-
tion accuracy obtained for high frequency
nouns (with absolute frequencies >1000)
can be maintained for nouns with consid-
erably lower frequencies (?50) by back-
ing off to a smaller set of features at clas-
sification.
1 Introduction
Animacy is a an inherent property of the referents
of nouns which has been claimed to figure as an
influencing factor in a range of different gram-
matical phenomena in various languages and it
is correlated with central linguistic concepts such
as agentivity and discourse salience. Knowledge
about the animacy of a noun is therefore rele-
vant for several different kinds of NLP problems
ranging from coreference resolution to parsing and
generation.
In recent years a range of linguistic studies have
examined the influence of argument animacy in
grammatical phenomena such as differential ob-
ject marking (Aissen, 2003), the passive construc-
tion (Dingare, 2001), the dative alternation (Bres-
nan et al, 2005), etc. A variety of languages are
sensitive to the dimension of animacy in the ex-
pression and interpretation of core syntactic argu-
ments (Lee, 2002; ?vrelid, 2004). A key general-
isation or tendency observed there is that promi-
nent grammatical features tend to attract other
prominent features;1 subjects, for instance, will
tend to be animate and agentive, whereas objects
prototypically are inanimate and themes/patients.
Exceptions to this generalisation express a more
marked structure, a property which has conse-
quences, for instance, for the distributional prop-
erties of the structure in question.
Even though knowledge about the animacy of
a noun clearly has some interesting implications,
little work has been done within the field of lex-
ical acquisition in order to automatically acquire
such knowledge. Ora?san and Evans (2001) make
use of hyponym-relations taken from theWord Net
resource (Fellbaum, 1998) in order to classify ani-
mate referents. However, such a method is clearly
restricted to languages for which large scale lexi-
cal resources, such as the Word Net, are available.
Merlo and Stevenson (2001) present a method for
verb classification which relies only on distribu-
tional statistics taken from corpora in order to train
a decision tree classifier to distinguish between
three groups of intransitive verbs.
1The notion of prominence has been linked to several
properties such as most likely as topic, agent, most available
referent, etc.
47
This paper presents experiments in automatic
classification of the animacy of unseen Norwe-
gian common nouns, inspired by the method for
verb classification presented in Merlo and Steven-
son (2001). The learning task is, for a given com-
mon noun, to classify it as either belonging to the
class animate or inanimate. Based on correlations
between animacy and other linguistic dimensions,
a set of morphosyntactic features is presented and
shown to differentiate common nouns along the
binary dimension of animacy with promising re-
sults. The method relies on aggregated relative fre-
quencies for common noun lemmas, hence might
be expected to seriously suffer from data sparse-
ness. Experiments attempting to empirically lo-
cate a frequency threshold for the classification
method will therefore be presented. It turns out
that a subset of the chosen morphosyntactic ap-
proximators of animacy show a resilience to data
sparseness which can be exploited in classifica-
tion. By backing off to this smaller set of features,
we show that we can maintain the same classifica-
tion accuracy also for lower frequency nouns.
The rest of the paper is structured as follows.
Section 2 identifies and motivates the set of chosen
features for the classification task and describes
how these features are approximated through fea-
ture extraction from an automatically annotated
corpus of Norwegian. In section 3, a group of ex-
periments testing the viability of the method and
chosen features is presented. Section 4 goes on to
investigate the effect of sparse data on the clas-
sification performance and present experiments
which address possible remedies for the sparse
data problem. Section 5 sums up the main find-
ings of the previous sections and outlines a few
suggestions for further research.
2 Features of animacy
As mentioned above, animacy is highly correlated
with a number of other linguistic concepts, such
as transitivity, agentivity, topicality and discourse
salience. The expectation is that marked configu-
rations along these dimensions, e.g. animate ob-
jects or inanimate agents, are less frequent in the
data. However, these are complex notions to trans-
late into extractable features from a corpus. In
the following we will present some morphological
and syntactic features which, in different ways, ap-
proximate the multi-faceted property of animacy:
Transitive subject and (direct) object As men-
tioned earlier, a prototypical transitive rela-
tion involves an animate subject and an inan-
imate object. In fact, a corpus study of an-
imacy distribution in simple transitive sen-
tences in Norwegian revealed that approxi-
mately 70% of the subjects of these types
of sentences were animate, whereas as many
as 90% of the objects were inanimate (?vre-
lid, 2004). Although this corpus study in-
volved all types of nominal arguments, in-
cluding pronouns and proper nouns, it still
seems that the frequency with which a cer-
tain noun occurs as a subject or an object of
a transitive verb might be an indicator of its
animacy.
Demoted agent in passive Agentivity is another
related notion to that of animacy, animate be-
ings are usually inherently sentient, capable
of acting volitionally and causing an event to
take place - all properties of the prototypi-
cal agent (Dowty, 1991). The passive con-
struction, or rather the property of being ex-
pressed as the demoted agent in a passive
construction, is a possible approximator of
agentivity. It is well known that transitive
constructions tend to passivize better (hence
more frequently) if the demoted subject bears
a prominent thematic role, preferably agent.
Anaphoric reference by personal pronoun
Anaphoric reference is a phenomenon where
the animacy of a referent is clearly expressed.
The Norwegian personal pronouns distin-
guish their antecedents along the animacy
dimension - animate han/hun ?he/she? vs.
inanimate den/det ?it-MASC/NEUT?.
Anaphoric reference by reflexive pronoun
Reflexive pronouns represent another form
of anaphoric reference, and, may, in contrast
to the personal pronouns locate their an-
tecedent locally, i.e. within the same clause.
In the prototypical reflexive construction
the subject and the reflexive object are
coreferent and it describes an action directed
at oneself. Although the reflexive pronoun in
Norwegian does not distinguish for animacy,
the agentive semantics of the construction
might still favour an animate subject.
Genitive -s There is no extensive case system for
common nouns in Norwegian and the only
48
distinction that is explicitly marked on the
noun is the genitive case by addition of -s.
The genitive construction typically describes
possession, a relation which often involves an
animate possessor.
2.1 Feature extraction
In order to train a classifier to distinguish between
animate and inanimate nouns, training data con-
sisting of distributional statistics on the above fea-
tures were extracted from a corpus. For this end,
a 15 million word version of the Oslo Corpus, a
corpus of Norwegian texts of approximately 18.5
million words, was employed.2 The corpus is mor-
phosyntactically annotated and assigns an under-
specified dependency-style analysis to each sen-
tence.3
For each noun, relative frequencies for the dif-
ferent morphosyntactic features described above
were computed from the corpus, i.e. the frequency
of the feature relative to this noun is divided by
the total frequency of the noun. For transitive sub-
jects (SUBJ), we extracted the number of instances
where the noun in question was unambiguously
tagged as subject, followed by a finite verb and an
unambiguously tagged object.4 The frequency of
direct objects (OBJ) for a given noun was approx-
imated to the number of instances where the noun
in question was unambiguously tagged as object.
We here assume that an unambiguously tagged
object implies an unambiguously tagged subject.
However, by not explicitly demanding that the ob-
ject is preceded by a subject, we also capture ob-
jects with a ?missing? subject, such as objects oc-
curring in relative clauses and infinitival clauses.
As mentioned earlier, another context where an-
imate nouns might be predominant is in the by-
phrase expressing the demoted agent of a passive
verb (PASS). Norwegian has two ways of express-
ing the passive, a morphological passive (verb +
s) and a periphrastic passive (bli + past participle).
The counts for passive by-phrases allow for both
types of passives to precede the by-phrase contain-
ing the noun in question.
2The corpus is freely available for research purposes, see
http://www.hf.uio.no/tekstlab for more information.
3The actual framework is that of Constraint Grammar
(Karlsson et al, 1995), and the analysis is underspecified
as the nodes are labelled only with their dependency func-
tion, e.g. subject or prepositional object, and their immediate
heads are not uniquely determined.
4The tagger works in an eliminative fashion, so tokens
may bear two or more tags when they have not been fully
disambiguated.
With regard to the property of anaphoric ref-
erence by personal pronouns, the extraction was
bound to be a bit more difficult. The anaphoric
personal pronoun is never in the same clause as
the antecedent, and often not even in the same sen-
tence. Coreference resolution is a complex prob-
lem, and certainly not one that we shall attempt to
solve in the present context. However, we might
attempt to come up with a metric that approxi-
mates the coreference relation in a manner ade-
quate for our purposes, that is, which captures the
different coreference relation for animate as op-
posed to inanimate nouns. To this end, we make
use of the common assumption that a personal pro-
noun usually refers to a discourse salient element
which is fairly recent in the discourse. Now, if
a sentence only contains one core argument (i.e.
an intransitive subject) and it is followed by a sen-
tence initiated by a personal pronoun, it seems rea-
sonable to assume that these are coreferent (Hale
and Charniak, 1998). For each of the nouns then,
we count the number of times it occurs as a sub-
ject with no subsequent object and an immediately
following sentence initiated by (i) an animate per-
sonal pronoun (ANAAN) and (ii) an inanimate per-
sonal pronouns (ANAIN).
The feature of reflexive coreference is easier
to approximate, as this coreference takes place
within the same clause. For each noun, the num-
ber of occurrences as a subject followed by a
verb and the 3.person reflexive pronoun seg ?him-
/her-/itself? are counted and its relative frequency
recorded. The genitive feature (GEN) simply con-
tains relative frequencies of the occurrence of each
noun with genitive case marking, i.e. the suffix -s.
3 Method viability
In order to test the viability of the classification
method for this task, and in particular, the chosen
features, a set of forty highly frequent nouns were
selected - twenty animate and twenty inanimate
nouns. A frequency threshold of minimum one
thousand occurrences ensured sufficient data for
all the features, as shown in table 1, which reports
the mean values along with the standard deviation
for each class and feature. The total data points
for each feature following the data collection are
as follows: SUBJ: 16813, OBJ: 24128, GEN:
7830, PASS: 577, ANAANIM: 989, ANAINAN:
944, REFL: 558. As we can see, quite a few of
the features express morphosyntactic cues that are
49
SUBJ OBJ GEN PASS ANAAN ANAIN REFL
Class Mean SD Mean SD Mean SD Mean SD Mean SD Mean SD Mean SD
A 0.14 0.05 0.11 0.03 0.04 0.02 0.006 0.005 0.009 0.006 0.003 0.003 0.005 0.0008
I 0.07 0.03 0.23 0.10 0.02 0.03 0.002 0.002 0.003 0.002 0.006 0.003 0.001 0.0008
Table 1: Mean relative frequencies and standard deviation for each class (A(nimate) vs. I(nanimate))
from feature extraction (SUBJ=Transitive Subject, OBJ=Object, GEN=Genitive -s, PASS=Passive by-
phrase, ANAAN=Anaphoric reference by animate pronoun, ANAIN=Anaphoric reference by inanimate
pronoun, REFL=Anaphoric reference by reflexive pronoun).
Feature % Accuracy
SUBJ 85.0
OBJ 72.5
GEN 72.5
PASS 62.5
ANAAN 67.5
ANAIN 50.0
REFL 82.5
Table 2: Accuracy for the in-
dividual features using leave-
one-out training and testing
Features used Feature Not Used % Accuracy
1. SUBJ OBJ GEN PASS ANAAN ANAIN REFL 87.5
2. OBJ GEN PASS ANAAN ANAIN REFL SUBJ 85.0
3. SUBJ GEN PASS ANAAN ANAIN REFL OBJ 87.5
4. SUBJ OBJ PASS ANAAN ANAIN REFL GEN 85.0
5. SUBJ OBJ GEN ANAAN ANAIN REFL PASS 82.5
6. SUBJ OBJ GEN PASS ANAIN REFL ANAAN 82.5
7. SUBJ OBJ GEN PASS ANAAN REFL ANAIN 87.5
8. SUBJ OBJ GEN PASS ANAAN ANAIN REFL 75.0
9. OBJ PASS ANAAN ANAIN SUBJ GEN REFL 77.5
Table 3: Accuracy for all features and ?all minus one? using leave-one-out
training and testing
rather rare. This is in particular true for the passive
feature and the anaphoric features ANAAN, ANAIN
and REFL. There is also quite a bit of variation in
the data (represented by the standard deviation for
each class-feature combination), a property which
is to be expected as all the features represent ap-
proximations of animacy, gathered from an auto-
matically annotated, possibly quite noisy, corpus.
Even so, the features all express a difference be-
tween the two classes in terms of distributional
properties; the difference between the mean fea-
ture values for the two classes range from double
to five times the lowest class value.
3.1 Experiment 1
Based on the data collected on seven different fea-
tures for our 40 nouns, a set of feature vectors are
constructed for each noun. They contain the rel-
ative frequencies for each feature along with the
name of the noun and its class (animate or inan-
imate). Note that the vectors do not contain the
mean values presented in Table 1 above, but rather
the individual relative frequencies for each noun.
The experimental methodology chosen for the
classification experiments is similar to the one de-
scribed in Merlo and Stevenson (2001) for verb
classification. We also make use of leave-one-
out training and testing of the classifiers and the
same software package for decision tree learning,
C5.0 (Quinlan, 1998), is employed. In addition, all
our classifiers employ the boosting option for con-
structing classifiers (Quinlan, 1993). For calcula-
tion of the statistical significance of differences in
the performance of classifiers tested on the same
data set, McNemar?s test is employed.
Table 2 shows the performance of each individ-
ual feature in the classification of animacy. As
we can see, the performance of the features dif-
fer quite a bit, ranging from mere baseline per-
formance (ANAIN) to a 70% improvement of the
baseline (SUBJ). The first line of Table 3 shows the
performance using all the seven features collec-
tively where we achieve an accuracy of 87.5%, a
75% improvement of the baseline. The SUBJ, GEN
and REFL features employed individually are the
best performing individual features and their clas-
sification performance do not differ significantly
from the performance of the combined classifier,
whereas the rest of the individual features do (at
the p<.05 level).
The subsequent lines (2-8) of Table 3 show the
accuracy results for classification using all fea-
tures except one at a time. This provides an in-
dication of the contribution of each feature to the
classification task. In general, the removal of a
feature causes a 0% - 12.5% deterioration of re-
sults, however, only the difference in performance
caused by the removal of the REFL feature is sig-
nificant (at the p<0.05 level). Since this feature is
one of the best performing features individually, it
is not surprising that its removal causes a notable
difference in performance. The removal of the
50
ANAIN feature, on the other hand, does not have
any effect on accuracy whatsoever. This feature
was the poorest performing feature with a base-
line, or mere chance, performance. We also see,
however, that the behaviour of the features in com-
bination is not strictly predictable from their indi-
vidual performance, as presented in table 2. The
SUBJ, GEN and REFL features were the strongest
features individually with a performance that did
not differ significantly from that of the combined
classifier. However, as line 9 in Table 3 shows, the
classifier as a whole is not solely reliant on these
three features. When they are removed from the
feature pool, the performance (77.5% accuracy)
does not differ significantly (p<.05) from that of
the classifier employing all features collectively.
4 Data sparseness and back-off
The classification experiments reported above im-
pose a frequency constraint (absolute frequencies
>1000) on the nouns used for training and test-
ing, in order to study the interaction of the differ-
ent features without the effects of sparse data. In
the light of the rather promising results from these
experiments, however, it might be interesting to
further test the performance of our features in clas-
sification as the frequency constraint is gradually
relaxed.
To this end, three sets of common nouns each
counting 40 nouns (20 animate and 20 inanimate
nouns) were randomly selected from groups of
nouns with approximately the same frequency in
the corpus. The first set included nouns with an
absolute frequency of 100 +/-20 (?100), the sec-
ond of 50+/-5 (?50) and the third of 10+/-2 (?10).
Feature extraction followed the same procedure as
in experiment 1, relative frequencies for all seven
features were computed and assembled into fea-
ture vectors, one for each noun.
4.1 Experiment 2: Effect of sparse data on
classification
In order to establish how much of the generaliz-
ing power of the old classifier is lost when the fre-
quency of the nouns is lowered, an experiment was
conducted which tested the performance of the old
classifier, i.e. a classifier trained on all the more
frequent nouns, on the three groups of less fre-
quent nouns. As we can see from the first col-
umn in Table 4, this resulted in a clear deteriora-
tion of results, from our earlier accuracy of 87.5%
to new accuracies ranging from 70% to 52.5%,
barely above the baseline. Not surprisingly, the
results decline steadily as the absolute frequency
of the classified noun is lowered.
Accuracy results provide an indication that the
classification is problematic. However, it does not
indicate what the damage is to each class as such.
A confusion matrix is in this respect more infor-
mative. Confusion matrices for the classification
of the three groups of nouns, ?100, ?50 and?10,
are provided in table 5. These clearly indicate that
it is the animate class which suffers when data be-
comes more sparse. The percentage of misclas-
sified animate nouns drop drastically from 50%
at ?100 to 80% at ?50 and finally 95% at ?10.
The classification of the inanimate class remains
pretty stable throughout. The fact that a major-
ity of our features (SUBJ, GEN, PASS, ANAAN and
REFL) target animacy, in the sense that a higher
proportion of animate than inanimate nouns ex-
hibit the feature, gives a possible explanation for
this. As data gets more limited, this differentia-
tion becomes harder to make, and the animate fea-
ture profiles come to resemble the inanimate more
and more. Because the inanimate nouns are ex-
pected to have low proportions (compared to the
animate) for all these features, the data sparseness
is not as damaging. In order to examine the effect
on each individual feature of the lowering of the
frequency threshold, we also ran classifiers trained
on the high frequency nouns with only individual
features on the three groups of new nouns. These
results are depicted in Table 4. In our earlier exper-
iment, the performance of a majority of the indi-
vidual features (OBJ, PASS, ANAAN, ANAIN) was
significantly worse (at the p<0.05 level) than the
performance of the classifier including all the fea-
tures. Three of the individual features (SUBJ, GEN,
REFL) had a performance which did not differ sig-
nificantly from that of the classifier employing all
the features in combination.
As the frequency threshold is lowered, how-
ever, the performance of the classifiers employ-
ing all features and those trained only on individ-
ual features become more similar. For the ?100
nouns, only the two anaphoric features ANAAN
and the reflexive feature REFL, have a performance
that differs significantly (p<0.05) from the clas-
sifier employing all features. For the ?50 and
?10 nouns, there are no significant differences
between the classifiers employing individual fea-
51
Freq All SUBJ OBJ GEN PASS ANAAN ANAIN REFL
?100 70.0 75.0 80.0 72.5 65.0 52.5 50.0 60.0
?50 57.5 75.0 62.5 77.5 62.5 57.5 50.0 55.0
?10 52.5 52.5 65.0 50.0 57.5 50.0 50.0 50.0
Table 4: Accuracy obtained when employing the old classifier on new lower-frequency nouns with leave-
one-out training and testing: all and individual features
?100 nouns
(a) (b) ? classified as
10 10 (a) class animate
2 18 (b) class inanimate
?50 nouns
(a) (b) ? classified as
4 16 (a) class animate
1 19 (b) class inanimate
?10 nouns
(a) (b) ? classified as
1 19 (a) class animate
20 (b) class inanimate
Table 5: Confusion matrices for classification of lower frequency nouns with old classifier
tures only and the classifiers trained on the feature
set as a whole. This indicates that the combined
classifiers no longer exhibit properties that are not
predictable from the individual features alone and
they do not generalize over the data based on the
combinations of features.
In terms of accuracy, a few of the individual fea-
tures even outperform the collective result. On av-
erage, the three most frequent features, the SUBJ,
OBJ and GEN features, improve the performance
by 9.5% for the ?100 nouns and 24.6% for the
?50 nouns. For the lowest frequency nouns (?10)
we see that the object feature alone improves the
result by almost 24%, from 52.5% to 65 % accu-
racy. In fact, the object feature seems to be the
most stable feature of all the features. When ex-
amining the means of the results extracted for the
different features, the object feature is the feature
which maintains the largest difference between the
two classes as the frequency threshold is lowered.
The second most stable feature in this respect is
the subject feature.
The group of experiments reported above shows
that the lowering of the frequency threshold for the
classified nouns causes a clear deterioration of re-
sults in general, and most gravely when all the fea-
tures are employed together.
4.2 Experiment 3: Back-off features
The three most frequent features, the SUBJ, OBJ
and GEN features, were the most stable in the
two experiments reported above and had a perfor-
mance which did not differ significantly from the
combined classifiers throughout. In light of this
we ran some experiments where all possible com-
binations of these more frequent features were em-
ployed. The results for each of the three groups of
nouns is presented in Table 6. The exclusion of the
less frequent features has a clear positive effect on
the accuracy results, as we can see in table 6. For
the?100 and?50 nouns, the performance has im-
proved compared to the classifier trained both on
all the features and on the individual features. The
classification performance for these nouns is now
identical or only slightly worse than the perfor-
mance for the high-frequency nouns in experiment
1. For the ?10 group of nouns, the performance
is, at best, the same as for all the features and at
worse fluctuating around baseline.
In general, the best performing feature com-
binations are SUBJ&OBJ&GEN and SUBJ&OBJ .
These two differ significantly (at the p<.05 level)
from the results obtained by employing all the fea-
tures collectively for both the ?100 and the ?50
nouns, hence indicate a clear improvement. The
feature combinations both contain the two most
stable features - one feature which targets the an-
imate class (SUBJ) and another which target the
inanimate class (OBJ), a property which facilitates
differentiation even as the marginals between the
two decrease.
It seems, then, that backing off to the most
frequent features might constitute a partial rem-
edy for the problems induced by data sparse-
ness in the classification. The feature combina-
tions SUBJ&OBJ&GEN and SUBJ&OBJ both sig-
nificantly improve the classification performance
and actually enable us to maintain the same accu-
racy for both the ?100 and ?50 nouns as for the
higher frequency nouns, as reported in experiment
1.
52
Freq SUBJ&OBJ&GEN SUBJ&OBJ SUBJ&GEN OBJ&GEN
?100 87.5 87.5 77.5 85.0
?50 82.5 90.0 70.0 77.5
?10 57.5 50.0 50.0 47.5
Table 6: Accuracy obtained when employing the old classifier on new lower-frequency nouns: combina-
tions of the most frequent features
4.3 Experiment 4: Back-off classifiers
Another option, besides a back-off to more fre-
quent features in classification, is to back off to
another classifier, i.e. a classifier trained on nouns
with a similar frequency. An approach of this kind
will attempt to exploit any group similarities that
these nouns may have in contrast to the mores fre-
quent ones, hopefully resulting in a better classifi-
cation.
In this experiment classifiers were trained and
tested using leave-one-out cross-validation on the
three groups of lower frequency nouns and em-
ploying individual, as well as various other fea-
ture combinations. The results for all features as
well as individual features are summarized in Ta-
ble 7. As we can see, the result for the classifier
employing all the features has improved somewhat
compared to the corresponding classifiers in ex-
periment 3 (as reported above in Table 4) for all
our three groups of nouns. This indicates that there
is a certain group similarity for the nouns of sim-
ilar frequency that is captured in the combination
of the seven features. However, backing off to a
classifier trained on nouns that are more similar
frequency-wise does not cause an improvement in
classification accuracy. Apart from the SUBJ fea-
ture for the ?100 nouns, none of the other clas-
sifiers trained on individual or all features for the
three different groups differ significantly (p<.05)
from their counterparts in experiment 3.
As before, combinations of the most frequent
features were employed in the new classifiers
trained and tested on each of the three frequency-
ordered groups of nouns. In the terminology em-
ployed above, this amounts to a backing off both
classifier- and feature-wise. The accuracy mea-
sures obtained for these experiments are summa-
rized in table 8. For these classifiers, the backed
off feature combinations do not differ significantly
(at the p<.05 level) from their counterparts in ex-
periment 3, where the classifiers were trained on
the more frequent nouns with feature back-off.
5 Conclusion
The above experiments have shown that the classi-
fication of animacy for Norwegian common nouns
is achievable using distributional data from a mor-
phosyntactically annotated corpus. The chosen
morphosyntactic features of animacy have proven
to differentiate well between the two classes. As
we have seen, the transitive subject, direct object
and morphological genitive provide stable features
for animacy even when the data is sparse(r). Four
groups of experiments have been reported above
which indicate that a reasonable remedy for sparse
data in animacy classification consists of back-
ing off to a smaller feature set in classification.
These experiments indicate that a classifier trained
on highly frequent nouns (experiment 1) backed
off to the most frequent features (experiment 3)
sufficiently capture generalizations which pertain
to nouns with absolute frequencies down to ap-
proximately fifty occurrences and enables an un-
changed performance approaching 90% accuracy.
Even so, there are certainly still possibilities for
improvement. As is well-known, singleton occur-
rences of nouns abound and the above classifica-
tion method is based on data for lemmas, rather
than individual instances or tokens. One possibil-
ity to be explored is token-based classification of
animacy, possibly in combination with a lemma-
based approach like the one outlined above.
Such an approach might also include a finer
subdivision of the nouns. We have chosen to clas-
sify along a binary dimension, however, it might
be argued that this is an artificial dichotomy. (Za-
enen et al, 2004) describe an encoding scheme
for the manual encoding of animacy informa-
tion in part of the English Switchboard corpus.
They make a three-way distinction between hu-
man, other animates, and inanimates, where the
?other animates? category describes a rather het-
erogeneous group of entities: organisations, an-
imals, intelligent machines and vehicles. How-
ever, what these seem to have in common is that
they may all be construed linguistically as ani-
53
Freq All SUBJ OBJ GEN PASS ANAAN ANAIN REFL
?100 85.0 52.5 87.5 65.0 70.0 50.0 57.5 50.0
?50 77.5 77.5 75.0 75.0 50.0 50.0 50.0 50.0
?10 52.5 50.0 62.5 50.0 50.0 50.0 50.0 50.0
Table 7: Accuracy obtained when employing a new classifier on new lower-frequency nouns: all and
individual features
Freq SUBJ&OBJ&GEN SUBJ&OBJ SUBJ&GEN OBJ&GEN
?100 85.0 85.0 67.5 82.5
?50 75.0 80.0 75.0 70.0
?10 62.5 62.5 50.0 62.5
Table 8: Accuracy obtained when employing a new classifier on new lower-frequency nouns: combina-
tions of the most frequent features
mate beings, even though they, in the real world,
are not. Interestingly, the two misclassified inani-
mate nouns in experiment 1, were bil ?car? and fly
?air plane?, both vehicles. A token-based approach
to classification might better capture the context-
dependent and dual nature of these types of nouns.
Automatic acquisition of animacy in itself is not
necessarily the primary goal. By testing the use of
acquired animacy information in various NLP ap-
plications such as parsing, generation or corefer-
ence resolution, we might obtain an extrinsic eval-
uation measure for the usefulness of animacy in-
formation. Since very frequent nouns are usually
well described in other lexical resources, it is im-
portant that a method for animacy classification is
fairly robust to data sparseness. This paper sug-
gests that a method based on seven morphosyntac-
tic features, in combination with feature back-off,
can contribute towards such a classification.
References
Judith Aissen. 2003. Differential Object Marking:
Iconicity vs. Economy. Natural Language and Lin-
guistic Theory, 21:435?483.
Joan Bresnan, Anna Cueni, Tatiana Nikitina and Har-
ald Baayen. 2005. Predicting the Dative Alterna-
tion. To appear in Royal Netherlands Academy of
Science Workshop on Foundations of Interpretation
proceedings.
Shipra Dingare. 2001. The effect of feature hierarchies
on frequencies of passivization in English. M.A.
Thesis, Stanford University.
David Dowty. 1991. Thematic Proto-Roles and Argu-
ment Selection. Language, 67(3):547?619.
John Hale and Eugene Charniak. 1998. Getting Useful
Gender Statistics from English Text. Technical Re-
port, Comp. Sci. Dept. at Brown University, Provi-
dence, Rhode Island.
Christiane Fellbaum, editor. 1998. WordNet, an elec-
tronic lexical database. MIT Press.
Fred Karlsson and Atro Voutilainen and Juha Heikkila?
and Atro Anttila. 1995. Constraint Grammar:
A language-independent system for parsing unre-
stricted text. Mouton de Gruyer.
Hanjung Lee. 2002. Prominence Mismatch and
Markedness Reduction in Word Order. Natural Lan-
guage and Linguistic Theory, 21(3):617?680.
Paola Merlo and Suzanne Stevenson. 2001. Auto-
matic Verb Classification Based on Statistical Distri-
butions of Argument Structure. Computational Lin-
guistics, 27(3):373?408.
Constantin Ora?san and Richard Evans. 2001. Learning
to Identify Animate References. in Proceedings of
the Workshop on Computational Natural Language
Learning, ACL-2001.
Lilja ?vrelid. 2004. Disambiguation of syntactic func-
tions in Norwegian: modeling variation in word or-
der interpretations conditioned by animacy and def-
initeness. in Fred Karlsson (ed.): Proceedings of
the 20th Scandinavian Conference of Linguistics,
Helsinki.
J. Ross Quinlan. 1998. C5.0: An Informal Tutorial.
http://www.rulequest.com/see5-unix.html.
J. Ross Quinlan. 1993. C4.5: Programs for machine
learning. Morgan Kaufmann Publishers, Series in
Machine Learning.
Annie Zaenen, Jean Carletta, Gregory Garretson,
Joan Bresnan, Andrew Koontz-Garboden, Tatiana
Nikitina, M. Catherine O?Connor and Tom Wasow.
2004. Animacy encoding in English: why and how.
in D. Byron and B. Webber (eds.): Proceedings of
ACL Workshop on Discourse Annotation, Barcelona.
54
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 630?638,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Empirical evaluations of animacy annotation
Lilja ?vrelid
Department of Linguistics
University of Potsdam
Germany
lilja@ling.uni-potsdam.de
Abstract
This article presents empirical evaluations
of aspects of annotation for the linguis-
tic property of animacy in Swedish, rang-
ing from manual human annotation, auto-
matic classification and, finally, an exter-
nal evaluation in the task of syntactic pars-
ing. We show that a treatment of animacy
as a lexical semantic property of noun
types enables generalization over distri-
butional properties of these nouns which
proves beneficial in automatic classifica-
tion and furthermore gives significant im-
provements in terms of parsing accuracy
for Swedish, compared to a state-of-the-
art baseline parser with gold standard ani-
macy information.
1 Introduction
The property of animacy influences linguistic phe-
nomena in a range of different languages, such
as case marking (Aissen, 2003) and argument re-
alization (Bresnan et al, 2005; de Swart et al,
2008), and has been shown to constitute an im-
portant factor in the production and comprehen-
sion of syntactic structure (Branigan et al, 2008;
Weckerly and Kutas, 1999).1 In computational
linguistic work, animacy has been shown to pro-
vide important information in anaphora resolution
(Ora?san and Evans, 2007), argument disambigua-
tion (Dell?Orletta et al, 2005) and syntactic pars-
ing in general (?vrelid and Nivre, 2007).
The dimension of animacy roughly distin-
guishes between entities which are alive and en-
tities which are not, however, other distinctions
1Parts of the research reported in this paper has been sup-
ported by the Deutsche Forschungsgemeinschaft (DFG, Son-
derforschungsbereich 632, project D4).
are also relevant and the animacy dimension is of-
ten viewed as a continuum ranging from humans
to inanimate objects. Following Silverstein (1976)
several animacy hierarchies have been proposed in
typological studies, focusing on the linguistic cat-
egory of animacy, i.e., the distinctions which are
relevant for linguistic phenomena. An example of
an animacy hierarchy, taken from (Aissen, 2003),
is provided in (1):
(1) Human > Animate > Inanimate
Clearly, non-human animates, like animals, are
not less animate than humans in a biological sense,
however, humans and animals show differing lin-
guistic behaviour.
Empirical studies of animacy require human an-
notation efforts, and, in particular, a well-defined
annotation task. However, annotation studies of
animacy differ distinctly in their treatment of ani-
macy as a type or token-level phenomenon, as well
as in terms of granularity of categories. The use
of the annotated data as a computational resource
furthermore poses requirements on the annotation
which do not necessarily agree with more theo-
retical considerations. Methods for the induction
of animacy information for use in practical appli-
cations require the resolution of issues of level of
representation, as well as granularity.
This article addresses these issues through em-
pirical and experimental evaluation. We present
an in-depth study of a manually annotated data
set which indicates that animacy may be treated
as a lexical semantic property at the type level.
We then evaluate this proposal through supervised
machine learning of animacy information and fo-
cus on an in-depth error analysis of the resulting
classifier, addressing issues of granularity of the
animacy dimension. Finally, the automatically an-
630
notated data set is employed in order to train a syn-
tactic parser and we investigate the effect of the an-
imacy information and contrast the automatically
acquired features with gold standard ones.
The rest of the article is structured as follows. In
section 2, we briefly discuss annotation schemes
for animacy, the annotation strategies and cate-
gories proposed there. We go on to describe anno-
tation for the binary distinction of ?human refer-
ence? found in a Swedish dependency treebank in
section 3 and we perform an evaluation of the con-
sistency of the human annotation in terms of lin-
guistic level. In section 4, we present experiments
in lexical acquisition of animacy based on mor-
phosyntactic features extracted from a consider-
ably larger corpus. Section 5 presents experiments
with the acquired animacy information applied in
the data-driven dependency parsing of Swedish.
Finally, section 6 concludes the article and pro-
vides some suggestions for future research.
2 Animacy annotation
Annotation for animacy is not a common compo-
nent of corpora or treebanks. However, following
from the theoretical interest in the property of an-
imacy, there have been some initiatives directed at
animacy annotation of corpus data.
Corpus studies of animacy (Yamamoto, 1999;
Dahl and Fraurud, 1996) have made use of an-
notated data, however they differ in the extent to
which the annotation has been explicitly formu-
lated as an annotation scheme. The annotation
study presented in Zaenen et. al. (2004) makes use
of a coding manual designed for a project study-
ing genitive modification (Garretson et al, 2004)
and presents an explicit annotation scheme for an-
imacy, illustrated by figure 1. The main class dis-
tinction for animacy is three-way, distinguishing
Human, Other animate and Inanimate, with sub-
classes under two of the main classes. The ?Other
animate? class further distinguishes Organizations
and Animals. Within the group of inanimates, fur-
ther distinctions are made between concrete and
non-concrete inanimate, as well as time and place
nominals.2
The annotation scheme described in Zaenen et.
al. (2004) annotates the markables according to
2The fact that the study focuses on genitival modification
has clearly influenced the categories distinguished, as these
are all distinctions which have been claimed to influence the
choice of genitive construction. For instance, temporal nouns
are frequent in genitive constructions, unlike the other inani-
mate nouns.
the animacy of their referent in the particular con-
text. Animacy is thus treated as a token level
property, however, has also been proposed as a
lexical semantic property of nouns (Yamamoto,
1999). The indirect encoding of animacy in lex-
ical resources, such as WordNet (Fellbaum, 1998)
can also be seen as treating animacy as a type-
level property. We may thus distinguish between a
purely type level annotation strategy and a purely
token level one. Type level properties hold for lex-
emes and are context-independent, i.e., indepen-
dent of the particular linguistic context, whereas
token-level properties are determined in context
and hold for referring expressions, rather than lex-
emes.
3 Human reference in Swedish
Talbanken05 is a Swedish treebank which was
created in the 1970?s and which has recently
been converted to dependency format (Nivre et
al., 2006b) and made freely available. The writ-
ten sections of the treebank consist of profes-
sional prose and student essays and amount to
197,123 running tokens, spread over 11,431 sen-
tences. Figure 2 shows the labeled dependency
graph of example (2), taken from Talbanken05.
(2) Samma
same
erfarenhet
experience
gjorde
made
engelsma?nnen
englishmen-DEF
?The same experience, the Englishmen had?
_
_
_
Samma
PO
KP
erfarenhet
NN
_
gjorde
VV
PT
engelsmannen
NN
DD|HH
ROOTDT OO SS
Figure 2: Dependency representation of example
(2) from Talbanken05.
In addition to information on part-of-speech, de-
pendency head and relation, and various mor-
phosyntactic properties such as definiteness, the
annotation expresses a distinction for nominal el-
ements between reference to human and non-
human. The annotation manual (Teleman, 1974)
states that a markable should be tagged as human
(HH) if it may be replaced by the interrogative pro-
noun vem ?who? and be referred to by the personal
pronouns han ?he? or hon ?she?.
There are clear similarities between the anno-
tation for human reference found in Talbanken05
and the annotation scheme for animacy discussed
631
ANIM
CONC NCONC TIME PLACE
ORG
HUM InanimateOtheranimate
Figure 1: Animacy classification scheme (Zaenen et al, 2004).
above. The human/non-human contrast forms the
central distinction in the animacy dimension and,
in this respect, the annotation schemes do not con-
flict. If we compare the annotation found in Tal-
banken05 with the annotation proposed in Zaenen
et. al. (2004), we find that the schemes differ pri-
marily in the granularity of classes distinguished.
The main source of variation in class distinctions
consists in the annotation of collective nouns, in-
cluding organizations, as well as animals.
3.1 Level of annotation
We distinguished above between type and token
level annotation strategies, where a type level an-
notation strategy entails that an element consis-
tently be assigned to only one class. A token level
strategy, in contrast, does not impose this restric-
tion on the annotation and class assignment may
vary depending on the specific context. Garretson
et. al (2004) propose a token level annotation strat-
egy and state that ?when coding for animacy [. . . ]
we are not considering the nominal per se (e.g., the
word ?church?), but rather the entity that is the ref-
erent of that nominal (e.g. some particular thing in
the real world)?. This indicates that for all possible
markables, a referent should be determinable.
The brief instruction with respect to annotation
for human reference in the annotation manual for
Talbanken05 (Teleman, 1974, 223) gives leeway
for interpretation in the annotation and does not
clearly state that it should be based on token level
reference in context. It may thus be interesting
to examine the extent to which this manual an-
notation is consistent across lexemes or whether
we observe variation. We manually examine the
intersection of the two classes of noun lemmas
in the written sections of Talbanken, i.e., the set
of nouns which have been assigned both classes
by the annotators. It contains 82 noun lemmas,
which corresponds to only 1.1% of the total num-
ber of noun lemmas in the treebank (7554 lem-
mas all together). After a manual inspection of
the intersective elements along with their linguis-
tic contexts, we may group the nouns which were
assigned to both classes, into the following cate-
gories:that ?HH? is the tag for
Abstract nouns Nouns with underspecified or
vague type level properties with respect to ani-
macy, such as quantifying nouns, e.g. ha?lft ?half?,
miljon ?million?, as well as nouns which may be
employed with varying animacy, e.g. element ?el-
ement?, part ?party?, as in (3) and (4):
(3) . . . ocksa?
. . . also
den
the
andra
other
partenHH
party-DEF
sta?r
stands
utanfo?r
outside
?. . . also the other party is left outside?
(4) I
in
ett
a
fo?rha?llande
relationship
a?r
are
aldrig
never
ba?gge
both
parter
parties
lika
same
starka
strong
?In a relationship, both parties are never equally
strong?
We also find that nouns which denote abstract con-
cepts regarding humans show variable annotation,
e.g. individ ?individual?, adressat ?addressee?,
medlem ?member?, kandidat ?candidate?, repre-
sentant ?representative?, auktoritet ?authority?
Reference shifting contexts These are nouns
whose type level animacy is clear but which are
employed in a specific context which shifts their
reference. Examples include metonymic usage of
nouns, as in (5) and nouns occurring in derefer-
encing constructions, such as predicative construc-
tions (6), titles (7) and idioms (8):
(5) . . . daghemmensHH
. . . kindergarten-DEF.GEN
otillra?ckliga
inadequate
resurser
resources
?. . . the kindergarten?s inadequate resources?
(6) . . . fo?r
. . . for
att
to
bli
become
en
a
bra
good
soldat
soldier
?. . . in order to become a good soldier?
(7) . . . menar
. . . thinks
biskop
bishop
Hellsten
Hellsten
?thinks bishop Hellsten?
(8) ta
take
studenten
student-DEF
?graduate from highschool (lit. take the student)?
632
It is interesting to note that the main variation in
annotation stems precisely from difficulties in de-
termining reference, either due to bleak type level
properties such as for the abstract nouns, or due to
properties of the context, as in the reference shift-
ing constructions. The small amount of variation
in the human annotation for animacy clearly sup-
ports a type-level approach to animacy, however,
underline the influence of the linguistic context on
the conception of animacy, as noted in the litera-
ture (Zaenen et al, 2004; Rosenbach, 2008).
4 Lexical acquisition of animacy
Even though knowledge about the animacy of a
noun clearly has some interesting implications, lit-
tle work has been done within the field of lexical
acquisition in order to automatically acquire ani-
macy information. Ora?san and Evans (2007) make
use of hyponym-relations taken from the Word-
Net resource in order to classify animate referents.
However, such a method is clearly restricted to
languages for which large scale lexical resources,
such as the WordNet, are available. The task of
animacy classification bears some resemblance to
the task of named entity recognition (NER) which
usually makes reference to a ?person? class. How-
ever, whereas most NER systems make extensive
use of orthographic, morphological or contextual
clues (titles, suffixes) and gazetteers, animacy for
nouns is not signaled overtly in the same way.
Following a strategy in line with work on
verb classification (Merlo and Stevenson, 2001;
Stevenson and Joanis, 2003), we set out to clas-
sify common noun lemmas based on their mor-
phosyntactic distribution in a considerably larger
corpus. This is thus equivalent to treatment of
animacy as a lexical semantic property and the
classification strategy is based on generalization
of morphosyntactic behaviour of common nouns
over large quantities of data. Due to the small size
of the Talbanken05 treebank and the small amount
of variation, this strategy was pursued for the ac-
quisition of animacy information.
In the animacy classification of common nouns
we exploit well-documented correlations between
morphosyntactic realization and semantic proper-
ties of nouns. For instance, animate nouns tend to
be realized as agentive subjects, inanimate nouns
do not (Dahl and Fraurud, 1996). Animate nouns
make good ?possessors?, whereas inanimate nouns
are more likely ?possessees? (Rosenbach, 2008).
Table 1 presents an overview of the animacy data
Class Types Tokens covered
Animate 644 6010
Inanimate 6910 34822
Total 7554 40832
Table 1: The animacy data set from Talbanken05;
number of noun lemmas (Types) and tokens in
each class.
for common nouns in Talbanken05. It is clear that
the data is highly skewed towards the non-human
class, which accounts for 91.5% of the type in-
stances. For classification we organize the data
into accumulated frequency bins, which include
all nouns with frequencies above a certain thresh-
old. We here approximate the class of ?animate?
to ?human? and the class of ?inanimate? to ?non-
human?. Intersective elements, see section 3.1, are
assigned to their majority class.3
4.1 Features for animacy classification
We define a feature space, which makes use of
distributional data regarding the general syntactic
properties of a noun, as well as various morpho-
logical properties. It is clear that in order for a
syntactic environment to be relevant for animacy
classification it must be, at least potentially, nom-
inal. We define the nominal potential of a depen-
dency relation as the frequency with which it is
realized by a nominal element (noun or pronoun)
and determine empirically a threshold of .10. The
syntactic and morphological features in the feature
space are presented below:
Syntactic features A feature for each depen-
dency relation with nominal potential: (tran-
sitive) subject (SUBJ), object (OBJ), preposi-
tional complement (PA), root (ROOT)4, ap-
position (APP), conjunct (CC), determiner
(DET), predicative (PRD), complement of
comparative subjunction (UK). We also in-
clude a feature for the head of a genitive mod-
ifier, the so-called ?possessee?, (GENHD).
Morphological features A feature for each mor-
phological distinction relevant for a noun
3When there is no majority class, i.e. in the case of ties,
the noun is removed from the data set. 12 lemmas were con-
sequently removed.
4Nominal elements may be assigned the root relation of
the dependency graph in sentence fragments which do not
contain a finite verb.
633
in Swedish: gender (NEU/UTR), num-
ber (SIN/PLU), definiteness (DEF/IND), case
(NOM/GEN). Also, the part-of-speech tags
distinguish dates (DAT) and quantifying
nouns (SET), e.g. del, rad ?part, row?, so
these are also included as features.
For extraction of distributional data for the set of
Swedish nouns we make use of the Swedish Pa-
role corpus of 21.5M tokens.5 To facilitate feature
extraction, we part-of-speech tag the corpus and
parse it with MaltParser6, which assigns a depen-
dency analysis.7
4.2 Experimental methodology
For machine learning, we make use of the Tilburg
Memory-Based Learner (TiMBL) (Daelemans et
al., 2004).8 Memory-based learning is a super-
vised machine learning method characterized by
a lazy learning algorithm which postpones learn-
ing until classification time, using the k-nearest
neighbor algorithm for the classification of unseen
instances. For animacy classification, the TiMBL
parameters are optimized on a subset of the full
data set.9
For training and testing of the classifiers, we
make use of leave-one-out cross-validation. The
baseline represents assignment of the majority
class (inanimate) to all nouns in the data set. Due
to the skewed distribution of classes, as noted
above, the baseline accuracy is very high, usu-
ally around 90%.Clearly, however, the class-based
measures of precision and recall, as well as the
combined F-score measure are more informative
for these results. The baseline F-score for the ani-
mate class is thus 0, and a main goal is to improve
on the rate of true positives for animates, while
limiting the trade-off in terms of performance for
5Parole is freely available at http://spraakbanken.gu.se
6http://www.maltparser.org
7For part-of-speech tagging, we employ the MaltTagger
? a HMM part-of-speech tagger for Swedish (Hall, 2003).
For parsing, we employ MaltParser (Nivre et al, 2006a),
a language-independent system for data-driven dependency
parsing , with the pretrained model for Swedish, which has
been trained on the tags output by the tagger.
8http://ilk.uvt.nl/software.html
9For parameter optimization we employ the
paramsearch tool, supplied with TiMBL, see
http://ilk.uvt.nl/software.html. Paramsearch implements
a hill climbing search for the optimal settings on iteratively
larger parts of the supplied data. We performed parameter
optimization on 20% of the total data set, where we balanced
the data with respect to frequency. The resulting settings are
k = 11, GainRatio feature weighting and Inverse Linear (IL)
class voting weights.
Bin Instances Baseline MBL SVM
>1000 291 89.3 97.3 95.2
>500 597 88.9 97.3 97.1
>100 1668 90.5 96.8 96.9
>50 2278 90.6 96.1 96.0
>10 3786 90.8 95.4 95.1
>0 5481 91.3 93.9 93.7
Table 2: Accuracy for MBL and SVM classifiers
on Talbanken05 nouns in accumulated frequency
bins by Parole frequency.
the majority class of inanimates, which start out
with F-scores approaching 100. For calculation of
the statistical significance of differences in the per-
formance of classifiers tested on the same data set,
McNemar?s test (Dietterich, 1998) is employed.
4.3 Results
Column four (MBL) in table 2 shows the accu-
racy obtained with all features in the general fea-
ture space. We observe a clear improvement on
all data sets (p<.0001), compared to the respec-
tive baselines. As we recall, the data sets are suc-
cessively larger, hence it seems fair to conclude
that the size of the data set partially counteracts
the lower frequency of the test nouns. It is not
surprising, however, that a method based on dis-
tributional features suffers when the absolute fre-
quencies approach 1. We obtain results for ani-
macy classification, ranging from 97.3% accuracy
to 93.9% depending on the sparsity of the data.
With an absolute frequency threshold of 10, we
obtain an accuracy of 95.4%, which constitutes a
50% reduction of error rate.
Table 3 presents the experimental results rela-
tive to class. We find that classification of the inan-
imate class is quite stable throughout the experi-
ments, whereas the classification of the minority
class of animate nouns suffers from sparse data. It
is an important point, however, that it is largely re-
call for the animate class which goes down with
increased sparseness, whereas precision remains
quite stable. All of these properties are clearly ad-
vantageous in the application to realistic data sets,
where a more conservative classifier is to be pre-
ferred.
4.4 Error analysis
The human reference annotation of the Tal-
banken05 nouns distinguishes only the classes cor-
responding to ?human? and ?inanimate? along the
634
Animate Inanimate
Precision Recall Fscore Precision Recall Fscore
>1000 89.7 83.9 86.7 98.1 98.8 98.5
>500 89.1 86.4 87.7 98.3 98.7 98.5
>100 87.7 76.6 81.8 97.6 98.9 98.2
>50 85.8 70.2 77.2 97.0 98.9 97.9
>10 81.9 64.0 71.8 96.4 98.6 97.5
>0 75.7 44.9 56.4 94.9 98.6 96.7
Table 3: Precision, recall and F-scores for the two classes in MBL-experiments with a general feature
space.
>10 nouns
(a) (b) ? classified as
222 125 (a) class animate
49 3390 (b) class inanimate
Table 4: Confusion matrix for the MBL-classifier
with a general feature space on the >10 data set
on Talbanken05 nouns.
animacy dimension. An interesting question is
whether the errors show evidence of the gradi-
ence in categories discussed earlier and explic-
itly expressed in the annotation scheme by Zaenen
et.al. (2004) in figure 1. If so, we would expect
erroneously classified inanimate nouns to contain
nouns of intermediate animacy, such as animals
and organizations.
The error analysis examines the performance of
the MBL-classifier employing all features on the
> 10 data set in order to abstract away from the
most serious effects of data sparseness. Table 4
shows a confusion matrix for the classification of
the nouns. If we examine the errors for the inan-
imate class we indeed find evidence of gradience
within this category. The errors contain a group
of nouns referring to animals and other living be-
ings (bacteria, algae), as listed in (9), as well as
one noun referring to an ?intelligent machine?, in-
cluded in the intermediate animacy category in Za-
enen et al (2004). Collective nouns with human
reference and organizations are also found among
the errors, listed in (11). We also find some nouns
among the errors with human denotation, listed in
(12). These are nouns which typically occur in
dereferencing contexts, such as titles, e.g. herr
?mister?, biskop ?bishop? and which were anno-
tated as non-human referring by the human an-
notators.10 Finally, a group of abstract, human-
10In fact, both of these showed variable annotation in the
treebank and were assigned their majority class ? inanimate
denoting nouns are also found among the errors, as
listed in (13). In summary, we find that nouns with
gradient animacy properties account for 53.1% of
the errors for the inanimate class.
(9) Animals/living beings:
alg ?algae?, apa ?monkey?, bakterie ?bacteria?, bjo?rn
?bear?, djur ?animal?, fa?gel ?bird?, fladdermo?ss ?bat?,
myra ?ant?, ma?s ?seagull?, parasit ?parasite?
(10) Intelligent machines:
robot ?robot?
(11) Collective nouns, organizations:
myndighet ?authority?, nation ?nation?, fo?retagsledning
?corporate-board?, personal ?personell?, stiftelse
?foundation?, idrottsklubb ?sport-club?
(12) Human-denoting nouns:
biskop ?bishop?, herr ?mister?, nationalist
?nationalist?, tolk ?interpreter?
(13) Abstract, human nouns:
fo?rlorare ?loser?, huvudpart ?main-party?, konkurrent
?competitor?, majoritet ?majority?, va?rd ?host?
It is interesting to note that both the hu-
man and automatic annotation showed difficul-
ties in ascertaining class for a group of ab-
stract, human-denoting nouns, like individ ?indi-
vidual?, motsta?ndare ?opponent?, kandidat ?candi-
date?, representant ?representative?. These were
all assigned to the animate majority class dur-
ing extraction, but were misclassified as inanimate
during classification.
4.5 SVM classifiers
In order to evaluate whether the classification
method generalizes to a different machine learn-
ing algorithm, we design an identical set of experi-
ments to the ones presented above, but where clas-
sification is performed with Support Vector Ma-
chines (SVMs) instead of MBL. We use the LIB-
SVM package (Chang and Lin, 2001) with a RBF
kernel (C = 8.0, ? = 0.5).11
? in the extraction of training data.
11As in the MBL-experiment, parameter optimization, i.e.,
choice of kernel function, C and ? values, is performed on
20% of the total data set with the easy.py tool, supplied
with LIBSVM.
635
As column 5 (SVM) in table 2 shows, the clas-
sification results are very similar to the results ob-
tained with MBL.12 We furthermore find a very
similar set of errors, and in particular, we find that
51.0 % of the errors for the inanimate class are
nouns with the gradient animacy properties pre-
sented in (9)-(13) above.
5 Parsing with animacy information
As an external evaluation of our animacy classi-
fier, we apply the induced information to the task
of syntactic parsing. Seeing that we have a tree-
bank with gold standard syntactic information and
gold standard as well as induced animacy informa-
tion, it should be possible to study the direct effect
of the added animacy information in the assign-
ment of syntactic structure.
5.1 Experimental methodology
We use the freely available MaltParser system,
which is a language-independent system for data-
driven dependency parsing (Nivre, 2006; Nivre et
al., 2006c). A set of parsers are trained on Tal-
banken05, both with and without additional an-
imacy information, the origin of which is either
the manual annotation described in section 3 or
the automatic animacy classifier described in sec-
tion 4.2- 4.4 (MBL). The common nouns in the
treebank are classified for animacy using leave-
one-out training and testing. This ensures that
the training and test instances are disjoint at all
times. Moreover, the fact that the distributional
data is taken from a separate data set ensures non-
circularity since we are not basing the classifica-
tion on gold standard parses.
All parsing experiments are performed using
10-fold cross-validation for training and testing on
the entire written part of Talbanken05. Overall
parsing accuracy will be reported using the stan-
dard metrics of labeled attachment score (LAS)
and unlabeled attachment score (UAS).13 Statis-
tical significance is checked using Dan Bikel?s
randomized parsing evaluation comparator.14 As
our baseline, we use the settings optimized for
Swedish in the CoNLL-X shared task (Buchholz
12The SVM-classifiers generally show slightly lower re-
sults, however, only performance on the >1000 data set is
significantly lower (p<.05).
13LAS and UAS report the percentage of tokens that are as-
signed the correct head with (labeled) or without (unlabeled)
the correct dependency label.
14http://www.cis.upenn.edu/?dbikel/software.html
Gold standard Automatic
UAS LAS UAS LAS
Baseline 89.87 84.92 89.87 84.92
Anim 89.81 84.94 89.87 84.99
Table 5: Overall results in experiments with au-
tomatic features compared to gold standard fea-
tures, expressed as unlabeled and labeled attach-
ment scores.
and Marsi, 2006), where this parser was the best
performing parser for Swedish.
5.2 Results
The addition of automatically assigned animacy
information for common nouns (Anim) causes a
small, but significant improvement in overall re-
sults (p<.04) compared to the baseline, as well
as the corresponding gold standard experiment
(p<.04). In the gold standard experiment, the re-
sults are not significantly better than the baseline
and the main, overall, improvement from the gold
standard animacy information reported in ?vrelid
and Nivre (2007) and ?vrelid (2008) stems largely
from the animacy annotation of pronouns.15 This
indicates that the animacy information for com-
mon nouns, which has been automatically ac-
quired from a considerably larger corpus, captures
distributional distinctions which are important for
the general effect of animacy and furthermore that
the differences from the gold standard annotation
prove beneficial for the results.
We see from Table 5, that the improvement in
overall parse results is mainly in terms of depen-
dency labeling, reflected in the LAS score. A
closer error analysis shows that the performance
of the two parsers employing gold and automatic
animacy information is very similar with respect
to dependency relations and we observe an im-
proved analysis for subjects, (direct and indirect)
objects and subject predicatives with only minor
variations. This in itself is remarkable, since the
covered set of animate instances is notably smaller
in the automatically annotated data set. We fur-
thermore find that the main difference between the
gold standard and automatic Anim-experiments
15Recall that the Talbanken05 treebank contains animacy
information for all nominal elements ? pronouns, proper and
common nouns. When the totality of this information is
added the overall parse results are significantly improved
(p<.0002) (?vrelid and Nivre, 2007; ?vrelid, 2008).
636
does not reside in the analysis of syntactic argu-
ments, but rather of non-arguments. One rela-
tion for which performance deteriorates with the
added information in the gold Anim-experiment
is the nominal postmodifier relation (ET) which
is employed for relative clauses and nominal PP-
attachment. With the automatically assigned fea-
ture, in contrast, we observe an improvement in
the performance for the ET relation, compared to
the gold standard experiment, from a F-score in
the latter of 76.14 to 76.40 in the former. Since
this is a quite common relation, with a frequency
of 5% in the treebank as a whole, the improvement
has a clear effect on the results.
The parser?s analysis of postnominal modifica-
tion is influenced by the differences in the added
animacy annotation for the nominal head, as well
as the internal dependent. If we examine the cor-
rected errors in the automatic experiment, com-
pared to the gold standard experiment, we find ele-
ments with differing annotation. Preferences with
respect to the animacy of prepositional comple-
ments vary. In (14), the automatic annotation of
the noun djur ?animal? as animate results in cor-
rect assignment of the ET relation to the prepo-
sition hos ?among?, as well as correct nominal,
as opposed to verbal, attachment. This preposi-
tion is one of the few with a preference for an-
imate complements in the treebank. In contrast,
the example in (15) illustrates an error where the
automatic classification of barn ?children? as inan-
imate causes a correct analysis of the head prepo-
sition om ?about?.16
(14) . . . samha?llsbildningar
. . . societies
hos
among
olika
different
djur
animals
?. . . social organizations among different animals?
(15) Fo?ra?ldrar
parents
har
have
va?rdnaden
custody-DEF
om
of
sina
their
barn
children
?Parents have the custody of their children?
A more thorough analysis of the different factors
involved in PP-attachment is a complex task which
is clearly beyond the scope of the present study.
We may note, however, that the distinctions in-
duced by the animacy classifier based purely on
linguistic evidence proves useful for the analysis
of both arguments and non-arguments.
16Recall that the classification is based purely on linguistic
evidence and in this respect children largely pattern with the
inanimate nouns. A child is probably more like a physical
object in the sense that it is something one possesses and oth-
erwise reacts to, rather than being an agent that acts upon its
surroundings.
6 Conclusion
This article has dealt with an empirical evaluation
of animacy annotation in Swedish, where the main
focus has been on the use of such annotation for
computational purposes.
We have seen that human annotation for ani-
macy shows little variation at the type-level for
a binary animacy distinction. Following from
this observation, we have shown how a type-
level induction strategy based on morphosyntac-
tic distributional features enables automatic ani-
macy classification for noun lemmas which fur-
thermore generalizes to different machine learning
algorithms (MBL, SVM). We obtain results for an-
imacy classification, ranging from 97.3% accuracy
to 93.9% depending on the sparsity of the data.
With an absolute frequency threshold of 10, we
obtain an accuracy of 95.4%, which constitutes a
50% reduction of error rate. A detailed error anal-
ysis revealed some interesting results and we saw
that more than half of the errors performed by the
animacy classifier for the large class of inanimate
nouns actually included elements which have been
assigned an intermediate animacy status in theo-
retical work, such as animals and collective nouns.
The application of animacy annotation in the
task of syntactic parsing provided a test bed for
the applicability of the annotation, where we could
contrast the manually assigned classes with the
automatically acquired ones. The results showed
that the automatically acquired information gives
a slight, but significant improvement of overall
parse results where the gold standard annotation
does not, despite a considerably lower coverage.
This is a suprising result which highlights impor-
tant properties of the annotation. First of all, the
automatic annotation is completely consistent at
the type level. Second, the automatic animacy
classifier captures important distributional proper-
ties of the nouns, exemplified by the case of nom-
inal postmodifiers in PP-attachment. The auto-
matic annotation thus captures a purely linguistic
notion of animacy and abstracts over contextual
influence in particular instances.
Animacy has been shown to be an important
property in a range of languages, hence animacy
classification of other languages constitutes an in-
teresting line of work for the future, where empir-
ical evaluations may point to similarities and dif-
ferences in the linguistic expression of animacy.
637
References
Judith Aissen. 2003. Differential Object Marking: Iconicity
vs. economy. Natural Language and Linguistic Theory,
21(3):435?483.
Holly P. Branigan, Martin J. Pickering, and Mikihiro Tanaka.
2008. Contributions of animacy to grammatical func-
tion assignment and word order production. Lingua,
118(2):172?189.
Joan Bresnan, Anna Cueni, Tatiana Nikitina, and Harald
Baayen. 2005. Predicting the dative alternation. In Gosse
Bouma, Irene Kraemer, and Joost Zwarts, editors, Cog-
nitive foundations of interpretation, pages 69?94. Royal
Netherlands Academy of Science, Amsterdam.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proceedings
of the Tenth Conference on Computational Natural Lan-
guage Learning (CoNLL-X), pages 149?164.
Chih-Chung Chang and Chih-Jen Lin. 2001. LIBSVM: A
library for support vector machines. Software available at
http://www.csie.ntu.edu.tw/?cjlin/libsvm.
Walter Daelemans, Jakub Zavrel, Ko Van der Sloot, and An-
tal Van den Bosch. 2004. TiMBL: Tilburg Memory Based
Learner, version 5.1, Reference Guide. Technical report,
ILK Technical Report Series 04-02.
?Osten Dahl and Kari Fraurud. 1996. Animacy in gram-
mar and discourse. In Thorstein Fretheim and Jeanette K.
Gundel, editors, Reference and referent accessibility,
pages 47?65. John Benjamins, Amsterdam.
Peter de Swart, Monique Lamers, and Sander Lestrade. 2008.
Animacy, argument structure and argument encoding: In-
troduction to the special issue on animacy. Lingua,
118(2):131?140.
Felice Dell?Orletta, Alessandro Lenci, Simonetta Monte-
magni, and Vito Pirrelli. 2005. Climbing the path to
grammar: A maximum entropy model of subject/object
learning. In Proceedings of the 2nd Workshop on Psy-
chocomputational Models of Human Language Acquisi-
tion, pages 72?81.
Thomas G. Dietterich. 1998. Approximate statistical test for
comparing supervised classification learning algorithms.
Neural Computation, 10(7):1895?1923.
Christiane Fellbaum, editor. 1998. WordNet: an electronic
lexical database. MIT Press, Cambridge, MA.
Gregory Garretson, M. Catherine O?Connor, Barbora
Skarabela, and Marjorie Hogan, 2004. Optimal Typol-
ogy of Determiner Phrases Coding Manual. Boston
University, version 3.2 edition. Downloaded from
http://people.bu.edu/depot/coding manual.html on
02/15/2006.
Johan Hall. 2003. A probabilistic part-of-speech tagger
with suffix probabilities. Master?s thesis, Va?xjo? Univer-
sity, Sweden.
Paola Merlo and Suzanne Stevenson. 2001. Automatic verb
classification based on statistical distributions of argument
structure. Computational Linguistics, 27(3):373?408.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006a. Malt-
parser: A data-driven parser-generator for dependency
parsing. In Proceedings of the Fifth International Con-
ference on Language Resources and Evaluation (LREC),
pages 2216?2219.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006b. Tal-
banken05: A Swedish treebank with phrase structure and
dependency annotation. In Proceedings of the fifth Inter-
national Conference on Language Resources and Evalua-
tion (LREC), pages 1392?1395.
Joakim Nivre, Jens Nilsson, Johan Hall, Gu?ls?en Eryig?it, and
Svetoslav Marinov. 2006c. Labeled pseudo-projective
dependency parsing with Support Vector Machines. In
Proceedings of the Conference on Computational Natural
Language Learning (CoNLL).
Joakim Nivre. 2006. Inductive Dependency Parsing.
Springer, Dordrecht.
Constantin Ora?san and Richard Evans. 2007. NP animacy
resolution for anaphora resolution. Journal of Artificial
Intelligence Research, 29:79?103.
Lilja ?vrelid and Joakim Nivre. 2007. When word order and
part-of-speech tags are not enough ? Swedish dependency
parsing with rich linguistic features. In Proceedings of the
International Conference on Recent Advances in Natural
Language Processing (RANLP), pages 447?451.
Lilja ?vrelid. 2008. Linguistic features in data-driven
dependency parsing. In Proceedings of the Conference
on Computational Natural Language Learning (CoNLL
2008).
Anette Rosenbach. 2008. Animacy and grammatical vari-
ation - findings from English genitive variation. Lingua,
118(2):151?171.
Michael Silverstein. 1976. Hierarchy of features and erga-
tivity. In Robert M.W. Dixon, editor, Grammatical cat-
egories in Australian Languages, pages 112?171. Aus-
tralian Institute of Aboriginal Studies, Canberra.
Suzanne Stevenson and Eric Joanis. 2003. Semi-supervised
verb class discovery using noisy features. In Proceedings
of the Conference on Computational Natural Language
Learning (CoNLL), pages 71?78.
Ulf Teleman. 1974. Manual fo?r grammatisk beskrivning av
talad och skriven svenska. Studentlitteratur, Lund.
J. Weckerly and M. Kutas. 1999. An electrophysiological
analysis of animacy effects in the processing of object rel-
ative sentences. Psychophysiology, 36:559?570.
Mutsumi Yamamoto. 1999. Animacy and Reference: A cog-
nitive approach to corpus linguistics. John Benjamins,
Amsterdam.
Annie Zaenen, Jean Carletta, Gregory Garretson, Joan
Bresnan, Andrew Koontz-Garboden, Tatiana Nikitina,
M. Catherine O?Connor, and Tom Wasow. 2004. Ani-
macy encoding in English: why and how. In Donna By-
ron and Bonnie Webber, editors, Proceedings of the ACL
Workshop on Discourse Annotation.
638
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 37?40,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Improving data-driven dependency parsing
using large-scale LFG grammars
Lilja ?vrelid, Jonas Kuhn and Kathrin Spreyer
Department of Linguistics
University of Potsdam
{lilja,kuhn,spreyer}@ling.uni-potsdam.de
Abstract
This paper presents experiments which
combine a grammar-driven and a data-
driven parser. We show how the con-
version of LFG output to dependency
representation allows for a technique of
parser stacking, whereby the output of the
grammar-driven parser supplies features
for a data-driven dependency parser. We
evaluate on English and German and show
significant improvements stemming from
the proposed dependency structure as well
as various other, deep linguistic features
derived from the respective grammars.
1 Introduction
The divide between grammar-driven and data-
driven approaches to parsing has become less pro-
nounced in recent years due to extensive work on
robustness and efficiency for the grammar-driven
approaches (Riezler et al, 2002; Cahill et al,
2008b). The linguistic generalizations captured in
such knowledge-based resources are thus increas-
ingly available for use in practical applications.
The NLP-community has in recent years wit-
nessed a surge of interest in dependency-based
approaches to syntactic parsing, spurred by the
CoNLL shared tasks of dependency parsing
(Buchholz and Marsi, 2006; Nivre et al, 2007).
Nivre and McDonald (2008) show how two differ-
ent approaches to dependency parsing, the graph-
based and transition-based approaches, may be
combined and subsequently learn to complement
each other to achieve improved parse results for a
range of different languages.
In this paper, we show how a data-driven depen-
dency parser may straightforwardly be modified to
learn directly from a grammar-driven parser. We
evaluate on English and German and show signifi-
cant improvements for both languages. Like Nivre
and McDonald (2008), we supply a data-driven
dependency parser with features from a different
parser to guide parsing. The additional parser em-
ployed in this work, is not however, a data-driven
parser trained on the same data set, but a grammar-
driven parser outputing a deep LFG analysis. We
furthermore show how a range of other features ?
morphological, structural and semantic ? from the
grammar-driven analysis may be employed dur-
ing data-driven parsing and lead to significant im-
provements.
2 Grammar-driven LFG-parsing
The XLE system (Crouch et al, 2007) performs
unification-based parsing using hand-crafted LFG
grammars. It processes raw text and assigns to it
both a phrase-structural (?c-structure?) and a fea-
ture structural, functional (?f-structure?).
In the work described in this paper, we employ
the XLE platform using the grammars available
for English and German from the ParGram project
(Butt et al, 2002). In order to increase the cover-
age of the grammars, we employ the robustness
techniques of fragment parsing and ?skimming?
available in XLE (Riezler et al, 2002).
3 Dependency conversion and feature
extraction
In extracting information from the output of the
deep grammars we wish to capture as much of the
precise, linguistic generalizations embodied in the
grammars as possible, whilst keeping with the re-
quirements posed by the dependency parser. The
process is illustrated in Figure 1.
3.1 Data
The English data set consists of the Wall Street
Journal sections 2-24 of the Penn treebank (Mar-
cus et al, 1993), converted to dependency format.
The treebank data used for German is the Tiger
37
f1
?
?
?
?
?
?
?
?
?
?
?
?
PRED ?halte?. . .??
VTYPE predicative
SUBJ ?pro?
OBJ
f2
?
?
PRED ?Verhalten?
CASE acc
SPEC f3?das?
ADJUNCT
{
f4?damalige?
}
?
?
XCOMP-PRED
?
?
PRED ?fu?r?. . .??
PTYPE nosem
OBJ
[
PRED ?richtig?
SUBJ
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
SUBJ
converted:
SPEC
XCOMP-PRED
ADJCT
SUBJ-OBJ
OBJ
Ich halte das damalige Verhalten fu?r richtig.
1sg pred. acc nosem
g
SB
old:
NK
OA
NK
MO
NK
Figure 1: Treebank enrichment with LFG output; German example: I consider the past behaviour cor-
rect.
treebank (Brants et al, 2004), where we employ
the version released with the CoNLL-X shared
task on dependency parsing (Buchholz and Marsi,
2006).
3.2 LFG to dependency structure
We start out by converting the XLE output to a
dependency representation. This is quite straight-
forward since the f-structures produced by LFG
parsers can be interpreted as dependency struc-
tures. The conversion is performed by a set of
rewrite rules which are executed by XLE?s built-
in extraction engine. We employ two strategies for
the extraction of dependency structures from out-
put containing multiple heads. We attach the de-
pendent to the closest head and, i) label it with the
corresponding label (Single), ii) label it with the
complex label corresponding to the concatenation
of the labels from the multiple head attachments
(Complex). The converted dependency analysis in
Figure 1 shows the f-structure and the correspond-
ing converted dependency output of a German ex-
ample sentence, where a raised object Verhalten
receives the complex SUBJ-OBJ label. Following
the XLE-parsing of the treebanks and the ensu-
ing dependency conversion, we have a grammar-
based analysis for 95.2% of the English sentence,
45238 sentences altogether, and 96.5% of the Ger-
man sentences, 38189 sentences altogether.
3.3 Deep linguistic features
The LFG grammars capture linguistic generaliza-
tions which may not be reduced to a dependency
representation. For instance, the grammars con-
tain information on morphosyntactic properties
such as case, gender and tense, as well as more se-
mantic properties detailing various types of adver-
bials, specifying semantic conceptual categories
such as human, time and location etc., see Fig-
ure 1. Table 1 presents the features extracted for
use during parsing from the German and English
XLE-parses.
4 Data-driven dependency parsing
MaltParser (Nivre et al, 2006a) is a language-
independent system for data-driven dependency
parsing which is freely available.1 MaltParser is
based on a deterministic parsing strategy in com-
bination with treebank-induced classifiers for pre-
dicting parse transitions. MaltParser constructs
parsing as a set of transitions between parse con-
figurations. A parse configuration is a triple
?S, I,G?, where S represents the parse stack, I is
the queue of remaining input tokens, and G repre-
sents the dependency graph defined thus far.
The feature model in MaltParser defines the rel-
evant attributes of tokens in a parse configuration.
Parse configurations are represented by a set of
features, which focus on attributes of the top of the
stack, the next input token and neighboring tokens
in the stack, input queue and dependency graph
under construction. Table 2 shows an example of
a feature model.2
For the training of baseline parsers we employ
feature models which make use of the word form
(FORM), part-of-speech (POS) and the dependency
relation (DEP) of a given token, exemplified in
Table 2. For the baseline parsers and all subse-
quent parsers we employ the arg-eager algorithm
in combination with SVM learners with a polyno-
mial kernel.3
1http://maltparser.org
2Note that the feature model in Table 2 is an example fea-
ture model and not the actual model employed in the parse
experiments. The details or references for the English and
German models are provided below.
3For training of the baseline parsers we also em-
ploy some language-specific settings. For English we
use learner and parser settings, as well as feature model
from the English pretrained MaltParser-model available from
http://maltparser.org. For German, we use the learner and
parser settings from the parser employed in the CoNLL-X
38
POS XFeats
Verb CLAUSETYPE, GOVPREP, MOOD, PASSIVE, PERF,
TENSE, VTYPE
Noun CASE, COMMON, GOVPREP, LOCATIONTYPE, NUM,
NTYPE, PERS, PROPERTYPE
Pronoun CASE, GOVPREP, NUM, NTYPE, PERS
Prep PSEM, PTYPE
Conj COORD, COORD-FORM, COORD-LEVEL
Adv ADJUNCTTYPE, ADVTYPE
Adj ATYPE, DEGREE
English DEVERBAL, PROG, SUBCAT, GENDSEM, HUMAN,
TIME
German AUXSELECT, AUXFLIP, COHERENT, FUT, DEF, GEND,
GENITIVE, COUNT
Table 1: Features from XLE output, common for
both languages and language-speciffic
FORM POS DEP XFEATS XDEP
S:top + + + + +
I:next + + + +
I:next?1 + +
G:head of top + +
G:leftmost dependent of top + +
InputArc(XHEAD)
Table 2: Example feature model; S: stack, I: input,
G: graph; ?n = n positions to the left(?) or right
(+).
5 Parser stacking
The procedure to enable the data-driven parser to
learn from the grammar-driven parser is quite sim-
ple. We parse a treebank with the XLE platform.
We then convert the LFG output to dependency
structures, so that we have two parallel versions
of the treebank ? one gold standard and one with
LFG-annotation. We extend the gold standard
treebank with additional information from the cor-
responding LFG analysis, as illustrated by Figure
1 and train the data-driven dependency parser on
the enhanced data set.
We extend the feature model of the baseline
parsers in the same way as Nivre and McDon-
ald (2008). The example feature model in Table
2 shows how we add the proposed dependency
relation (XDEP) top and next as features for the
parser. We furthermore add a feature which looks
at whether there is an arc between these two tokens
in the dependency structure (InputArc(XHEAD)),
with three possible values: Left, Right, None. In
order to incorporate further information supplied
by the LFG grammars we extend the feature mod-
els with an additional, static attribute, XFEATS.
This is employed for the range of deep linguistic
features, detailed in section 3.3 above.
5.1 Experimental setup
All parse experiments are performed using 10-fold
cross-validation for training and testing. Overall
parsing accuracy will be reported using the stan-
dard metrics of labeled attachment score (LAS)
and unlabeled attachment score (UAS).Statistical
significance is checked using Dan Bikel?s random-
ized parsing evaluation comparator.4
shared task (Nivre et al, 2006b). For both languages, we em-
ploy so-called ?relaxed? root handling.
4http://www.cis.upenn.edu/?dbikel/software.html
6 Results
We experiment with the addition of two types of
features: i) the dependency structure proposed by
XLE for a given sentence ii) other morphosyntac-
tic, structural or lexical semantic features provided
by the XLE grammar. The results are presented in
Table 3.
For English, we find that the addition of pro-
posed dependency structure from the grammar-
driven parser causes a small, but significant im-
provement of results (p<.0001). In terms of la-
beled accuracy the results improve with 0.15 per-
centage points, from 89.64 to 89.79. The introduc-
tion of complex dependency labels to account for
multiple heads in the LFG output causes a smaller
improvement of results than the single labeling
scheme. The corresponding results for German are
presented in Table 3. We find that the addition of
grammar-driven dependency structures with sin-
gle labels (Single) improves the parse results sig-
nificantly (p<.0001), both in terms of unlabeled
and labeled accuracy. For labeled accuracy we ob-
serve an improvement of 1.45 percentage points,
from 85.97 to 87.42. For the German data, we
find that the addition of dependency structure with
complex labels (Complex) gives a further small,
but significant (p<.03) improvement over the ex-
periment with single labels.
The results following the addition of the
grammar-extracted features in Table 1 (Feats) are
presented in Table 3.5 We observe significant im-
provements of overall parse results for both lan-
guages (p<.0001).
5We experimented with several feature models for the in-
clusion of the additional information, however, found no sig-
nificant differences when performing a forward feature selec-
tion. The simple feature model simply adds the XFEATS of
the top and next tokens of the parse configuration.
39
English German
UAS LAS UAS LAS
Baseline 92.48 89.64 88.68 85.97
Single 92.61 89.79 89.72 87.42
Complex 92.58 89.74 89.76 87.46
Feats 92.55 89.77 89.63 87.30
Single+Feats 92.52 89.69 90.01 87.77
Complex+Feats 92.53 89.70 90.02 87.78
Table 3: Overall results in experiments expressed as unlabeled and labeled attachment scores.
We also investigated combinations of the dif-
ferent sources of information ? dependency struc-
tures and deep features. These results are pre-
sented in the final lines of Table 3. We find
that for the English parser, the combination of
the features do not cause a further improve-
ment of results, compared to the individual ex-
periments. The combined experiments (Sin-
gle+Feats, Complex+Feats) for German, on the
other hand, differ significantly from the base-
line experiment, as well as the individual ex-
periments (Single,Complex,Feats) reported above
(p<.0001). By combination of the grammar-
derived features we improve on the baseline by
1.81 percentage points.
A comparison with the German results obtained
using MaltParser with graph-based dependency
structures supplied by MSTParser (Nivre and Mc-
Donald, 2008) shows that our results using a
grammar-driven parser largely corroborate the ten-
dencies observed there. Our best results for Ger-
man, combining dependency structures and addi-
tional features, slightly improve on those reported
for MaltParser (by 0.11 percentage points).6
7 Conclusions and future work
This paper has presented experiments in the com-
bination of a grammar-driven LFG-parser and a
data-driven dependency parser. We have shown
how the use of converted dependency structures
in the training of a data-driven dependency parser,
MaltParser, causes significant improvements in
overall parse results for English and German. We
have furthermore presented a set of additional,
deep features which may straightforwardly be ex-
tracted from the grammar-based output and cause
individual improvements for both languages and a
combined effect for German.
In terms of future work, a more extensive er-
ror analysis will be performed to locate the pre-
6English was not among the languages investigated in-
Nivre and McDonald (2008).
cise benefits of the parser combination. We will
also investigate the application of the method di-
rectly to raw text and application to a task which
may benefit specifically from the combined anal-
yses, such as semantic role labeling or semantic
verb classification.
It has recently been shown that automatically
acquired LFG grammars may actually outperform
hand-crafted grammars in parsing (Cahill et al,
2008a). These results add further to the relevance
of the results shown in this paper, bypassing the
bottleneck of grammar hand-crafting as a prereq-
uisite for the applicability of our results.
References
Sabine Brants, Stefanie Dipper, Peter Eisenberg, Silvia Hansen-Schirra, Esther
Knig, Wolfgang Lezius, Christian Rohrer, George Smith, and Hans Uszko-
reit. 2004. Tiger: Linguistic interpretation of a German corpus. Research
on Language and Computation, 2:597?620.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilin-
gual dependency parsing. In Proceedings of CoNLL-X).
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hiroshi Masuichi, and
Christian Rohrer. 2002. The Parallel Grammar Project. In Proceedings
of COLING-2002 Workshop on Grammar Engineering and Evaluation.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Stefan Riezler, Josef van Gen-
abith, and Andy Way. 2008a. Wide-coverage deep statistical parsing using
automatic dependency structure annotation. Computational Linguistics.
Aoife Cahill, John T. Maxwell, Paul Meurer, Christian Rohrer, and Victoria
Rosen. 2008b. Speeding up LFG parsing using c-structure pruning. In
Proceedings of the Workshop on Grammar Engineering Across Frame-
works.
D. Crouch, M. Dalrymple, R. Kaplan, T. King, J. Maxwell, and P. Newman,
2007. XLE Documentation. http://www2.parc.com/isl/.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large
annotated corpus for English: The Penn treebank. Computational Linguis-
tics, 19(2):313?330.
Joakim Nivre and Ryan McDonald. 2008. Integrating graph-based and
transition-based dependency parsers. In Proceedings of ACL-HLT 2008.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006a. Maltparser: A data-driven
parser-generator for dependency parsing. In Proceedings of LREC.
Joakim Nivre, Jens Nilsson, Johan Hall, Gu?ls?en Eryig?it, and Svetoslav Mari-
nov. 2006b. Labeled pseudo-projective dependency parsing with Support
Vector Machines. In Proceedings of CoNLL.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDonald, Jens Nilsson, Se-
bastian Riedel, and Deniz Yuret. 2007. CoNLL 2007 Shared Task on
Dependency Parsing. In Proceedings of the CoNLL Shared Task Session
of EMNLP-CoNLL 2007, pages 915?932.
Stefan Riezler, Tracy King, Ronald Kaplan, Richard Crouch, John T. Maxwell,
and Mark Johnson. 2002. Parsing the Wall Street journal using a lexical-
functional grammar and discriminative estimation techniques. In Proceed-
ings of ACL.
40
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1379?1387,
Beijing, August 2010
Syntactic Scope Resolution in Uncertainty Analysis
Lilja ?vrelid?? and Erik Velldal? and Stephan Oepen?
? University of Oslo, Department of Informatics
?Universit?t Potsdam, Institut f?r Linguistik
ovrelid@uni-potsdam.de and erikve@ifi.uio.no and oe@ifi.uio.no
Abstract
We show how the use of syntactic struc-
ture enables the resolution of hedge scope
in a hybrid, two-stage approach to un-
certainty analysis. In the first stage, a
Maximum Entropy classifier, combining
surface-oriented and syntactic features,
identifies cue words. With a small set of
hand-crafted rules operating over depen-
dency representations in stage two, we at-
tain the best overall result (in terms of
both combined ranks and average F1) in
the 2010 CoNLL Shared Task.
1 Background?Motivation
Recent years have witnessed an increased interest
in the analysis of various aspects of sentiment in
natural language (Pang & Lee, 2008). The sub-
task of hedge resolution deals with the analysis of
uncertainty as expressed in natural language, and
the linguistic means (so-called hedges) by which
speculation or uncertainty are expressed. Infor-
mation of this kind is of importance for various
mining tasks which aim at extracting factual data.
Example (1), taken from the BioScope corpus
(Vincze, Szarvas, Farkas, M?ra, & Csirik, 2008),
shows a sentence where uncertainty is signaled by
the modal verb may.1
(1) {The unknown amino acid ?may? be used by these
species}.
The topic of the Shared Task at the 2010 Con-
ference for Natural Language Learning (CoNLL)
is hedge detection in biomedical literature?in a
sense ?zooming in? on one particular aspect of the
broader BioNLP Shared Task in 2009 (Kim, Ohta,
Pyysalo, Kano, & Tsujii, 2009). It involves two
subtasks: Task 1 is described as learning to detect
1In examples throughout this paper, angle brackets high-
light hedge cues, and curly braces indicate the scope of a
given cue, as annotated in BioScope.
sentences containing uncertainty; the objective of
Task 2 is learning to resolve the in-sentence scope
of hedge cues (Farkas, Vincze, Mora, Csirik, &
Szarvas, 2010). The organizers further suggest:
This task falls within the scope of semantic analy-
sis of sentences exploiting syntactic patterns [...].
The utility of syntactic information within var-
ious approaches to sentiment analysis in natu-
ral language has been an issue of some debate
(Wilson, Wiebe, & Hwa, 2006; Ng, Dasgupta,
& Arifin, 2006), and the potential contribution of
syntax clearly varies with the specifics of the task.
Previous work in the hedging realm has largely
been concerned with cue detection, i.e. identify-
ing uncertainty cues such as may in (1), which
are predominantly individual tokens (Medlock &
Briscoe, 2007; Kilicoglu & Bergler, 2008). There
has been little previous work aimed at actually
resolving the scope of such hedge cues, which
presumably constitutes a somewhat different and
likely more difficult problem. Morante and Daele-
mans (2009) present a machine-learning approach
to this task, using token-level, lexical informa-
tion only. To this end, CoNLL 2010 enters largely
uncharted territory, and it remains to be seen (a)
whether syntactic analysis indeed is a necessary
component in approaching this task and, more
generally, (b) to what degree the specific task
setup can inform us about the strong and weak
points in current approaches and technology.
In this article, we investigate the contribution
of syntax to hedge resolution, by reflecting on our
experience in the CoNLL 2010 task.2 Our CoNLL
system submission ranked fourth (of 24) on Task 1
and third (of 15) on Task 2, for an overall best av-
erage result (there appears to be very limited over-
lap among top performers for the two subtasks).
2It turns out, in fact, that all the top-performing systems
in Task 2 of the CoNLLShared Task rely on syntactic informa-
tion provided by parsers, either in features for machine learn-
ing or as input to manually crafted rules (Morante, Asch, &
Daelemans, 2010; Rei & Briscoe, 2010).
1379
Sentences Hedged Cues Multi-Word Tokens Cue Tokens
Sentences Cues
Abstracts 11871 2101 2659 364 309634 3056
Articles 2670 519 668 84 68579 782
Total 14541 2620 3327 448 378213 3838
Table 1: Summary statistics for the Shared Task training data.
This article transcends our CoNLL system descrip-
tion (Velldal, ?vrelid, & Oepen, 2010) in several
respects, presenting updated and improved cue de-
tection results (? 3 and ? 4), focusing on the role
of syntactic information rather than on machine
learning specifics (? 5 and ? 6), providing an anal-
ysis and discussion of Task 2 errors (? 7), and gen-
erally aiming to gauge the value of available anno-
tated data and processing tools (? 8). We present
a hybrid, two-level approach for hedge resolution,
where a statistical classifier detects cue words, and
a small set of manually crafted rules operating
over syntactic structures resolve scope. We show
how syntactic information?produced by a data-
driven dependency parser complemented with in-
formation from a ?deep?, hand-crafted grammar?
contributes to the resolution of in-sentence scope
of hedge cues, discussing various types of syn-
tactic constructions and associated scope detec-
tion rules in considerable detail. We furthermore
present a manual error analysis, which reveals re-
maining challenges in our scope resolution rules
as well as several relevant idiosyncrasies of the
preexisting BioScope annotation.
2 Task, Data, and System Basics
Task Definition and Evaluation Metrics
Task 1 is a binary sentence classification task:
identifying utterances as being certain or uncer-
tain. Following common practice, this subtask
is evaluated in terms of precision, recall, and
F1 for the ?positive? class, i.e. uncertain. In
our work, we approach Task 1 as a byproduct
of the full hedge resolution problem, labeling a
sentence as uncertain if it contains at least one
token classified as a hedge cue. In addition to
the sentence-level evaluation for Task 1, we also
present precision, recall, and F1 for the cue-level.
Task 2 comprises two subtasks: cue detection
and scope resolution. The official CoNLL eval-
uation does not tease apart these two aspects of
the problem, however: Only an exact match of
both the cue and scope bracketing (in terms of
substring positions) will be counted as a success,
again quantified in terms of precision, recall, and
F1. Discussing our results below, we report cue
detection and scope resolution performance sepa-
rately, and further put scope results into perspec-
tive against an upper bound based on the gold-
standard cue annotation.
Besides the primary biomedical domain data,
some annotated Wikipedia data was provided
for Task 1, and participating systems are classi-
fied as in-domain (using exclusively the domain-
specific data), cross-domain (combining both
types of training data), or open (utilizing addi-
tional uncertainty-related resources). In our work,
we focus on the interplay of syntax and the more
challenging Task 2; we ignored the Wikipedia
track in Task 1. Despite our using general NLP
tools (see below), our system falls into the most
restrictive, in-domain category.
Training and Evaluation Data The training
data for the CoNLL 2010 Shared Task is taken from
the BioScope corpus (Vincze et al, 2008) and
consists of 14,541 ?sentences? (or other root-level
utterances) from biomedical abstracts and articles
(see Table 1).3 The BioScope corpus provides
annotation for hedge cues as well as their scope.
According to the annotation guidelines (Vincze et
al., 2008), the annotation adheres to a principle
of minimalism when it comes to hedge cues, i.e.
the minimal unit expressing hedging is annotated.
The inverse is true of scope annotations, which ad-
here to a principle of maximal scope?meaning
that scope should be set to the largest syntactic
3As it was known beforehand that evaluation would draw
on full articles only, we put more emphasis on the article
subset of the training data, for example in cross validation
testing and manual diagnosis of errors.
1380
ID FORM LEMMA POS FEATS HEAD DEPREL XHEAD XDEP
1 The the DT _ 4 NMOD 4 SPECDET
2 unknown unknown JJ degree:attributive 4 NMOD 4 ADJUNCT
3 amino amino JJ degree:attributive 4 NMOD 4 ADJUNCT
4 acid acid NN pers:3|case:nom|num:sg|ntype:common 5 SBJ 3 SUBJ
5 may may MD mood:ind|subcat:MODAL|tense:pres|clauseType:decl 0 ROOT 0 ROOT
6 be be VB _ 5 VC 7 PHI
7 used use VBN subcat:V-SUBJ-OBJ|vtype:main|passive:+ 6 VC 5 XCOMP
8 by by IN _ 7 LGS 9 PHI
9 these these DT deixis:proximal 10 NMOD 10 SPECDET
10 species specie NNS num:pl|pers:3|case:obl|common:count|ntype:common 8 PMOD 7 OBL-AG
11 . . . _ 5 P 0 PUNC
Table 2: Stacked dependency representation of example (1), with MaltParser and XLE annotations.
unit possible.
For evaluation purposes, the task organizers
provided newly annotated biomedical articles, fol-
lowing the same general BioScope principles. The
CoNLL 2010 evaluation data comprises 5,003 ad-
ditional utterances (138,276 tokens), of which 790
are annotated as hedged. The data contains a to-
tal of 1033 cues, of which 87 are so-called multi-
word cues (i.e. cues spanning multiple tokens),
comprising 1148 cue tokens altogether.
Stacked Dependency Parsing For syntactic
analysis we employ the open-source MaltParser
(Nivre, Hall, & Nilsson, 2006), a platform for
data-driven dependency parsing. For improved
accuracy and portability across domains and gen-
res, we make our parser incorporate the pre-
dictions of a large-scale, general-purpose LFG
parser?following the work of ?vrelid, Kuhn, and
Spreyer (2009). A technique dubbed parser stack-
ing enables the data-driven parser to learn, not
only from gold standard treebank annotations, but
from the output of another parser (Nivre & Mc-
Donald, 2008). This technique has been shown to
provide significant improvements in accuracy for
both English and German (?vrelid et al, 2009),
and a similar setup employing an HPSG gram-
mar has been shown to increase domain indepen-
dence in data-driven dependency parsing (Zhang
& Wang, 2009). The stacked parser combines
two quite different approaches?data-driven de-
pendency parsing and ?deep? parsing with a hand-
crafted grammar?and thus provides us with a
broad range of different types of linguistic infor-
mation for the hedge resolution task.
MaltParser is based on a deterministic pars-
ing strategy in combination with treebank-induced
classifiers for predicting parse transitions. It sup-
ports a rich feature representation of the parse his-
tory in order to guide parsing and may easily be
extended to take additional features into account.
The procedure to enable the data-driven parser
to learn from the grammar-driven parser is quite
simple. We parse a treebank with the XLE plat-
form (Crouch et al, 2008) and the English gram-
mar developed within the ParGram project (Butt,
Dyvik, King, Masuichi, & Rohrer, 2002). We
then convert the LFG output to dependency struc-
tures, so that we have two parallel versions of the
treebank?one gold standard and one with LFG
annotation. We extend the gold standard treebank
with additional information from the correspond-
ing LFG analysis and train MaltParser on the en-
hanced data set.
Table 2 shows the enhanced dependency rep-
resentation of example (1) above, taken from the
training data. For each token, the parsed data con-
tains information on the word form, lemma, and
part of speech (PoS), as well as on the head and
dependency relation in columns 6 and 7. The
added XLE information resides in the FEATS col-
umn, and in the XLE-specific head and depen-
dency columns 8 and 9. Parser outputs, which in
turn form the basis for our scope resolution rules
discussed in Section 5, also take this same form.
The parser employed in this work is trained on
the Wall Street Journal sections 2 ? 24 of the Penn
Treebank (PTB), converted to dependency format
(Johansson & Nugues, 2007) and extended with
XLE features, as described above. Parsing uses the
arc-eager mode of MaltParser and an SVM with
a polynomial kernel. When tested using 10-fold
cross validation on the enhanced PTB, the parser
achieves a labeled accuracy score of 89.8.
PoS Tagging and Domain Variation Our
parser is trained on financial news, and although
stacking with a general-purpose LFG parser is ex-
1381
pected to aid domain portability, substantial dif-
ferences in domain and genre are bound to neg-
atively affect syntactic analysis (Gildea, 2001).
MaltParser presupposes that inputs have been PoS
tagged, leaving room for variation in preprocess-
ing. On the one hand, we aim to make parser
inputs maximally similar to its training data (i.e.
the conventions established in the PTB); on the
other hand we wish to benefit from specialized re-
sources for the biomedical domain.
The GENIA tagger (Tsuruoka et al, 2005) is
particularly relevant in this respect (as could be
the GENIA Treebank proper4). However, we
found that GENIA tokenization does not match the
PTB conventions in about one out of five sen-
tences (for example wrongly splitting tokens like
?390,926? or ?Ca(2+)?); also in tagging proper
nouns, GENIA systematically deviates from the
PTB. Hence, we adapted an in-house tokenizer
(using cascaded finite-state rules) to the CoNLL
task, run two PoS taggers in parallel, and eclec-
tically combine annotations across the various
preprocessing components?predominantly giv-
ing precedence to GENIA lemmatization and PoS
hypotheses.
To assess the impact of improved, domain-
adapted inputs on our hedge resolution system,
we contrast two configurations: first, running the
parser in the exact same manner as ?vrelid, Kuhn,
and Spreyer (2010), we use TreeTagger (Schmid,
1994) and its standard model for English (trained
on the PTB) for preprocessing; second, we give as
inputs to the parser our refined tokenization and
merged PoS tags, as described above. When eval-
uating the two modes of preprocessing on the ar-
ticles subset of the training data, and using gold-
standard cues, our system for resolving cue scopes
(presented in ? 5) achieves an F1 of 66.31 with
TreeTagger inputs, and 72.30 using our refined to-
kenization and tagger combination. These results
underline the importance of domain adaptation for
accurate syntactic analysis, and in the following
we assume our hybrid in-house setup.
4Although the GENIA Treebank provides syntactic anno-
tation in a form inspired by the PTB, it does not provide func-
tion labels. Therefore, our procedure for converting from
constituency to dependency requires non-trivial adaptation
before we can investigate the effects of retraining the parser
against GENIA.
3 Stage 1: Identifying Hedge Cues
For the task of identifying hedge cues, we devel-
oped a binary maximum entropy (MaxEnt) clas-
sifier. The identification of cue words is used
for (a) classifying sentences as certain/uncertain
(Task 1), and (b) providing input to the syntac-
tic rules that we later apply for resolving the in-
sentence scope of the cues (Task 2). We also re-
port evaluation scores for the sub-task of cue de-
tection in isolation.
As annotated in the training data, it is possible
for a hedge cue to span multiple tokens, e.g. as in
whether or not. The majority of the multi-word
cues in the training data are very infrequent, how-
ever, most occurring only once, and the classifier
itself is not sensitive to the notion of multi-word
cues. Instead, the task of determining whether a
cue word forms part of a larger multi-word cue, is
performed in a separate post-processing step (ap-
plying a heuristic rule targeted at only the most
frequently occurring patterns of multi-word cues
in the training data).
During development, we trained cue classifiers
using a wide variety of feature types, both syn-
tactic and surface-oriented. In the end, however,
we found n-gram-based lexical features to have
the greatest contribution to classifier performance.
Our best-performing classifier so far (see ?Final?
in Table 3) includes the following feature types:
n-grams over forms (up to 2 tokens to the right),
n-grams over base forms (up to 3 tokens left
and right), PoS (from GENIA), subcategorization
frames (from XLE), and phrase-structural coordi-
nation level (from XLE). Our CoNLL system de-
scription includes more details of the various other
feature types that we experimented with (Velldal
et al, 2010).
4 Cue Detection Evaluation
Table 3 summarizes the performance of our Max-
Ent hedge cue classifier in terms of precision, re-
call and F1, computed using the official Shared
Task scorer script. The sentence-level scores cor-
respond to Task 1 of the Shared Task, and the cue-
level scores are based on the exact-match counts
for full hedge cues (possibly spanning multiple to-
kens).
1382
Sentence Level Cue Level
Configuration Prec Rec F1 Prec Rec F1
Baseline, Development 79.25 79.45 79.20 77.37 71.70 74.43
Final, Development 91.39 86.78 89.00 90.18 79.47 84.49
Final, Held-Out 85.61 85.06 85.33 81.97 76.41 79.10
Table 3: Isolated evaluation of the hedge cue classifier.
As the CoNLL test data was known beforehand
to consist of articles only, in 10-fold cross vali-
dation for classifier development we tested exclu-
sively against the articles segment, while always
including all sentences from the abstracts in the
training set. This corresponds to the development
results in Table 3, while the held-out results are
for the official Shared Task evaluation data (train-
ing on all the available training data). A model
using only unigram features serves as a baseline.
5 Stage 2: Resolving Scope
Hedge scope may vary quite a lot depending on
linguistic properties of the cue in question. In our
approach to scope resolution we rely heavily on
syntactic information, taken from the dependency
structures proposed by both MaltParser and XLE,
as well as on various additional features relating
to specific syntactic constructions.
We constructed a small set of heuristic rules
which define the scope for each cue detected in
Stage 1. In developing these rules, we made use
of the information provided by the guidelines for
scope annotation in the BioScope corpus (Vincze
et al, 2008), combined with manual inspection of
the training data in order to further generalize over
the phenomena discussed by Vincze et al (2008)
and work out interactions of constructions for var-
ious types of cues.
The rules take as input a parsed sentence which
has been further tagged with hedge cues. They
operate over the dependency structures and ad-
ditional features provided by the parser. Default
scope is set to start at the cue word and span to
the end of the sentence (modulo punctuation), and
this scope also provides the baseline for the eval-
uation of our rules. In the following, we discuss
broad classes of rules, organized by categories of
hedge cues. As there is no explicit representa-
tion of phrase or clause boundaries in our depen-
dency universe, we assume a set of functions over
dependency graphs, for example finding the left-
or rightmost (direct) dependent of a given node,
or transitively selecting left- or rightmost descen-
dants.
Coordination The dependency analysis of co-
ordination provided by our parser makes the first
conjunct the head of the coordination. For cues
that are coordinating conjunctions (PoS tag CC),
such as or, we define the scope as spanning the
whole coordinate structure, i.e. start scope is set
to the leftmost dependent of the head of the coor-
dination, e.g., roX in (2), and end scope is set to
its rightmost dependent (conjunct), e.g., RNAs in
(2). This analysis provides us with coordinations
at various syntactic levels, such as NP and N (2),
AP and AdvP, or VP (3):
(2) [...] the {roX genes ?or? RNAs} recruit the entire set
of MSL proteins [...]
(3) [...] the binding interfaces are more often {kept ?or?
even reused} rather than lost in the course of
evolution.
Adjectives We distinguish between adjectives
(JJ) in attributive (NMOD) function and adjectives
in predicative (PRD) function. Attributive adjec-
tives take scope over their (nominal) head, with all
its dependents, as in (4) and (5):
(4) The {?possible? selenocysteine residues} are shown
in red, [...]
(5) Extensive analysis of the flanks failed to show any
hallmarks of {?putative? transposons that might be
associated with this RAG1-like protein}, [...]
For adjectives in a predicative function the scope
includes the subject argument of the head verb
(the copula), as well as a (possible) clausal argu-
ment, as in (6). The scope does not, however, in-
clude expletive subjects, as in (7).
1383
(6) Therefore, {the unknown amino acid, if it is encoded
by a stop codon, is ?unlikely? to exist in the current
databases of microbial genomes}.
(7) For example, it is quite {?likely? that there exists an
extremely long sequence that is entirely unique to U}.
Verbs The scope of verbal cues is a bit more
complex and depends on several factors. In our
rules, we distinguish passive usages from active
usages, raising verbs from non-raising verbs, and
the presence or absence of a subject-control em-
bedding context. The scopes of both passive and
raising verbs include the subject argument of their
head verb, as in (8) and (9), unless it is an exple-
tive pronoun, as in (10).
(8) {Interactions determined by high-throughput methods
are generally ?considered? to be less reliable than
those obtained by low-throughput studies} 1314 and
as a consequence [...]
(9) {Genomes of plants and vertebrates ?seem? to be free
of any recognizable Transib transposons} (Figure 1).
(10) It has been {?suggested? that unstructured regions of
proteins are often involved in binding interactions,
particularly in the case of transient interactions} 77.
In the case of subject control involving a hedge
cue, specifically modals, subject arguments are in-
cluded in scopes where the controller heads a pas-
sive construction or a raising verb, as in exam-
ple (1) above, repeated here for convenience:
(11) {The unknown amino acid ?may? be used by these
species}.
In general, the end scope of verbs should ex-
tend over the minimal clause that contains the verb
in question. In terms of dependency structures,
we define the clause boundary as comprising the
chain of descendants of a verb which is not inter-
vened by a token with a higher attachment in the
graph than the verb in question. In example (8)
for instance, the sentence-level conjunction and
marks the end of the clause following the cue con-
sidered.
Prepositions and Adverbs Cues that are tagged
as prepositions (including some complementizers)
take scope over their argument, with all its de-
scendants, (12). Adverbs take scope over their
head with all its (non-subject) syntactic descen-
dants (13).
Configuration F1
BS
P
Default, Gold Cues 45.21
Rules, Gold Cues 72.31
Rules, System Cues 64.77
BS
E Rules, Gold Cues 66.73
Rules, System Cues 55.75
Table 4: Evaluation of scope resolution rules.
(12) {?Whether? the codon aligned to the inframe stop
codon is a nonsense codon or not} was neglected at
this stage.
(13) These effects are {?probably? mediated through the
1,25(OH)2D3 receptor}.
Multi-Word Cues In the case of multi-word
cues, such as indicate that or either ... or, we need
to determine the head of the multi-word unit. We
then set the scope of the whole unit to the scope
of the head token.
As an illustration of rule processing, consider
our running example (11), with its syntactic anal-
ysis as shown in Table 2 above. This example
invokes a variety of syntactic properties, includ-
ing parts of speech, argumenthood, voice etc. Ini-
tially, the scope of the hedge cue is set to default
scope. Then the subject control rule is applied,
which checks the properties of the verbal argu-
ment used, going through a chain of verbal depen-
dents from the modal verb. Since it is marked as
passive in the LFG analysis, the start scope is set to
include the subject of the cue word (the leftmost
descendant in its SBJ dependent).
6 Rule Evaluation
Table 4 summarizes scope resolution performance
(viewed as a an isolated subtask) for various con-
figurations, both against the articles section of the
CoNLL training data (dubbed BSP) and against the
held-out evaluation data (BSE). First of all, we note
that the ?default scope? baseline is quite strong:
unconditionally extending the scope of a cue to
the end of the sentence yields an F1 of 45.21.
Given gold standard cue information, our scope
rules improve on the baseline by 27 points on the
articles section of the data set, for an F1 of 72.31;
with system-assigned hedge cues, our rules still
1384
achieve an F1 of 64.77. Note that scope resolu-
tion scores based on classified cues also yield the
end-to-end system evaluation for Task 2.
The bottom rows of Table 4 show the evaluation
of scope rules on the CoNLL held-out test data. Us-
ing system cues, scope resolution on the held-out
data scores at 55.75 F1. Comparing to the result
on the (articles portion of the) training data, we
observe a substantial drop in performance (of six
points with gold-standard cues, nine points with
system cues). There are several possible explana-
tions for this effect. First of all, there may well
be a certain degree of overfitting of our rules to
the training data. The held-out data may contain
hedging constructions that are not covered by our
current set of scope rules, or annotation of parallel
constructions may in some cases differ in subtle
ways (see ? 7 below). Moreover, scope resolution
performance is of course influenced by cue detec-
tion (see Table 3). The cue-level F1 of our sys-
tem on the held-out data set is 79.10, compared to
84.49 (using cross validation) on the training data.
This drop in cue-level performance appears to af-
fect classification precision far more than recall.
Of course, given that our heuristics for identifying
multi-word cues were based on patterns extracted
from the training data, some loss in the cue-level
score was expected.
7 Error Analysis
To start shedding some light on the significance
of our results, we performed a manual error anal-
ysis on the article portion of the training material
(BSP), with two of the authors (trained linguists)
working in tandem. Using gold-standard cues,
our scope resolution rules fail to exactly replicate
the target annotation in 185 (of 668) cases, corre-
sponding to 72.31 F1 in Table 4 above. Our eval-
uators reviewed and discussed these 185 cases,
classifying 156 (84%) as genuine system errors,
22 (12%) as likely5 annotation errors, and a re-
5In some cases, there is no doubt that annotation is er-
roneous, i.e. in violation of the available annotation guide-
lines (Vincze et al, 2008) or in conflict with otherwise un-
ambiguous patterns. In other cases, however, judgments are
necessarily based on generalizations made by the evaluators,
i.e. assumptions about the underlying system and syntactic
analyses implicit in the BioScope annotations. Furthermore,
selecting items for manual analysis that do not align with the
maining seven cases as involving controversial or
seemingly arbitrary decisions.
The two most frequent classes of system er-
rors pertain (a) to the recognition of phrase and
clause boundaries and (b) to not dealing success-
fully with relatively superficial properties of the
text. Examples (14) and (15) illustrate the first
class of errors, where in addition to the gold-
standard annotation we use vertical bars (?|?) to
indicate scope predictions of our system.
(14) [...] {the reverse complement |mR of m will be
?considered? to be [...]|}
(15) This |{?might? affect the results} if there is a
systematic bias on the composition of a protein
interaction set|.
In our syntax-driven approach to scope resolution,
system errors will almost always correspond to a
failure in determining constituent boundaries, in a
very general sense. However, specifically exam-
ple (15) is indicative of a key challenge in this
task, where adverbials of condition, reason, or
contrast frequently attach within the dependency
domain of a hedge cue, yet are rarely included in
the scope annotation.
Example (16) demonstrates our second fre-
quent class of system errors. One in six items
in the BSP training data contains a sentence-final
parenthesized element or trailing number, as for
example (2), (9), or (10) above; most of these are
bibliographic or other in-text references, which
are never included in scope annotation. Hence,
our system includes a rule to ?back out? from trail-
ing parentheticals; in examples like (16), how-
ever, syntax does not make explicit the contrast
between an in-text reference vs. another type of
parenthetical.
(16) More specifically, {|the bristle and leg phenotypes are
?likely? to result from reduced signaling by Dl| (and
not by Ser)}.
Moving on to apparent annotation errors, the
rules for inclusion (or not) of the subject in
the scope of verbal hedge cues and decisions
on boundaries (or internal structure) of nominals
predictions made by our scope resolution rules is likely to
bias our sample, such that our estimated proportion of 12%
annotation errors cannot be used to project an overall error
rate.
1385
seem problematic?as illustrated in examples (17)
to (22).6
(17) [...] and |this is also {?thought? to be true for the full
protein interaction networks we are modeling}|.
(18) [...] {Neur |?can? promote Ser signaling|}.
(19) |Some of the domain pairs {?seem? to mediate a large
number of protein interactions, thus acting as reusable
connectors}|.
(20) One {|?possible? explanation| is functional
redundancy with the mouse Neur2 gene}.
(21) [...] |redefinition of {one of them is ?feasible?}|.
(22) |The {Bcl-2 family ?appears? to function [...]}|.
Finally, the difficult corner cases invoke non-
constituent coordination, ellipsis, or NP-initial fo-
cus adverbs?and of course interactions of the
phenomena discussed above. Without making the
syntactic structures assumed explicit, it is often
very difficult to judge such items.
8 Reflections ? Outlook
Our combination of stacked dependency parsing
and hand-crafted scope resolution rules proved
adequate for the CoNLL 2010 competition, con-
firming the central role of syntax in this task.
With a comparatively small set of rules (imple-
mented in a few hundred lines of code), con-
structed through roughly two full weeks of ef-
fort (studying BioScope annotations and develop-
ing rules), our CoNLL system achieved an end-to-
end F1 of 55.33 on Task 2.7 The two submis-
sions with better results (at 57.32 and 55.65 F1)
represent groups who have pioneered the hedge
analysis task in previous years (Morante et al,
2010; Rei & Briscoe, 2010). Scores for other ?in-
domain? participants range from 52.24 to 2.15 F1.
6Like in the presentation of system errors, we include
scope predictions of our own rules here too, which we be-
lieve to be correct in these cases. Also in this class of errors,
we find the occasional ?uninteresting? mismatch, for exam-
ple related to punctuation marks and inconsistencies around
parentheses.
7In ? 4 and ? 6 above, we report scores for a slightly im-
proved version of our system, where (after the official CoNLL
submission date) we eliminated a bug related to the treatment
of sentence-initial whitespace in the XML annotations. At an
end-to-end F1 of 55.75, this system would outrank the sec-
ond best performer in Task 2.
Doubtless there is room for straightforward exten-
sion: for example retraining our parser on the GE-
NIA Treebank, further improving the cue classifier,
and refining scope resolution rules in the light of
the error analysis above.
At the same time, we remain mildly am-
bivalent about the long-term impact of some of
the specifics of the 2010 CoNLL task. Shared
tasks (i.e. system bake-offs) have become increas-
ingly popular in past years, and in some sub-
fields (e.g. IE, SMT, or dependency parsing) high-
visibility competitions can shape community re-
search agendas. Hence, even at this early stage, it
seems appropriate to reflect on the possible con-
clusions to be drawn from the 2010 hedge res-
olution task. First, we believe the harsh ?exact
substring match? evaluation metric underestimates
the degree to which current technology can solve
this problem; furthermore, idiosyncratic, string-
level properties (e.g. the exact treatment of punc-
tuation or parentheticals) may partly obscure the
interpretation of methods used and corresponding
system performance.
These effects are compounded by some con-
cerns about the quality of available annotation.
Even though we tried fine-tuning our cross vali-
dation testing to the nature of the evaluation data
(comprising only articles), our system performs
substantially worse on the newly annotated CoNLL
test data, in both stages.8 In our view, the anno-
tation of hedge cues and scopes ideally would be
overtly related to at least some level of syntactic
annotation?as would in principle be possible for
the segment of BioScope drawing on the abstracts
of the GENIA Treebank.
Acknowledgements
We are grateful to the organizers of the 2010
CoNLL Shared Task and creators of the BioScope
resource; first, for engaging in these kinds of com-
munity service, and second for many in-depth dis-
cussions of annotation and task details. We thank
our colleagues at the Universities of Oslo and
Potsdam for their comments and support.
8We are leaving open the possibility to further refine our
system; we have therefore abstained from an error analysis
on the evaluation data so far.
1386
References
Butt, M., Dyvik, H., King, T. H., Masuichi, H., & Rohrer,
C. (2002). The Parallel Grammar Project. In Proceed-
ings of COLING workshop on grammar engineering and
evaluation (pp. 1 ? 7). Taipei, Taiwan.
Crouch, D., Dalrymple, M., Kaplan, R., King, T., Maxwell,
J., & Newman, P. (2008). XLE documentation. Palo Alto,
CA. (Palo Alto Research Center)
Farkas, R., Vincze, V., Mora, G., Csirik, J., & Szarvas, G.
(2010). The CoNLL 2010 Shared Task: Learning to de-
tect hedges and their scope in natural language text. In
Proceedings of the 14th Conference on Natural Language
Learning. Uppsala, Sweden.
Gildea, D. (2001). Corpus variation and parser perfor-
mance. In Proceedings of the 2001 conference on Empir-
ical Methods in Natural Language Processing (pp. 167 ?
202). Pittsburgh, PA.
Johansson, R., & Nugues, P. (2007). Extended constituent-
to-dependency conversion for English. In J. Nivre, H.-
J. Kaalep, & M. Koit (Eds.), Proceedings of NODALIDA
2007 (p. 105-112). Tartu, Estonia.
Kilicoglu, H., & Bergler, S. (2008). Recognizing speculative
language in biomedical research articles: A linguistically
motivated perspective. In Proceedings of the BioNLP
2008 Workshop. Columbus, OH, USA.
Kim, J.-D., Ohta, T., Pyysalo, S., Kano, Y., & Tsujii, J.
(2009). Overview of BioNLP 2009 Shared Task on event
extraction. In Proceedings of the BioNLP 2009 workshop
companion volume for shared task (pp. 1 ? 9). Boulder,
CO: Association for Computational Linguistics.
Medlock, B., & Briscoe, T. (2007). Weakly supervised learn-
ing for hedge classification in scientific literature. In Pro-
ceedings of the 45th Meeting of the Association for Com-
putational Linguistics (pp. 992 ? 999). Prague, Czech Re-
public: Association for Computational Linguistics.
Morante, R., Asch, V. V., & Daelemans, W. (2010).
Memory-based resolution of in-sentence scope of hedge
cues. In Proceedings of the 14th Conference on Natural
Language Learning (pp. 40 ? 47). Uppsala, Sweden.
Morante, R., & Daelemans, W. (2009). Learning the scope
of hedge cues in biomedical texts. In Proceedings of
the BioNLP 2009 Workshop (pp. 28 ? 36). Boulder, Col-
orado.
Ng, V., Dasgupta, S., & Arifin, S. M. N. (2006). Examin-
ing the role of linguistic knowledge sources in the auto-
matic identification and classification of reviews. In Pro-
ceedings of the 21st International Conference on Compu-
tational Linguistics and the 44th Annual Meeting of the
Association for Computational Linguistics. Sydney, Aus-
tralia.
Nivre, J., Hall, J., & Nilsson, J. (2006). MaltParser: A data-
driven parser-generator for dependency parsing. In Pro-
ceedings of the Fifth International Conference on Lan-
guage Resources and Evaluation (p. 2216-2219). Genoa,
Italy.
Nivre, J., & McDonald, R. (2008, June). Integrating graph-
based and transition-based dependency parsers. In Pro-
ceedings of the 46th Meeting of the Association for Com-
putational Linguistics (pp. 950 ? 958). Columbus, Ohio.
?vrelid, L., Kuhn, J., & Spreyer, K. (2009). Improving data-
driven dependency parsing using large-scale LFG gram-
mars. In Proceedings of the 47th Meeting of the Associ-
ation for Computational Linguistics (pp. 37 ? 40). Singa-
pore.
?vrelid, L., Kuhn, J., & Spreyer, K. (2010). Cross-
framework parser stacking for data-driven dependency
parsing. TAL 2010 special issue on Machine Learning
for NLP, 50(3).
Pang, B., & Lee, L. (2008). Opinion mining and senti-
ment analysis. Foundations and Trends in Information
Retrieval, 2(1-2).
Rei, M., & Briscoe, T. (2010). Combining manual rules and
supervised learning for hedge cue and scope detection. In
Proceedings of the 14th Conference on Natural Language
Learning (pp. 56 ? 63). Uppsala, Sweden.
Schmid, H. (1994). Probabilistic part-of-speech tagging us-
ing decision trees. In International conference on new
methods in language processing (p. 44-49). Manchester,
England.
Tsuruoka, Y., Tateishi, Y., Kim, J.-D., Ohta, T., McNaught,
J., Ananiadou, S., et al (2005). Developing a robust
Part-of-Speech tagger for biomedical text. In Advances in
informatics (pp. 382 ? 392). Berlin, Germany: Springer.
Velldal, E., ?vrelid, L., & Oepen, S. (2010). Resolving
speculation: MaxEnt cue classification and dependency-
based scope rules. In Proceedings of the 14th Conference
on Natural Language Learning. Uppsala, Sweden.
Vincze, V., Szarvas, G., Farkas, R., M?ra, G., & Csirik, J.
(2008). The BioScope corpus: Annotation for negation,
uncertainty and their scope in biomedical texts. In Pro-
ceedings of the BioNLP 2008 Workshop. Columbus, OH,
USA.
Wilson, T., Wiebe, J., & Hwa, R. (2006). Recognizing strong
and weak opinion clauses. Computational Intelligence,
22(2), 73 ? 99.
Zhang, Y., & Wang, R. (2009). Cross-domain dependency
parsing using a deep linguistic grammar. In Proceedings
of the 47th Meeting of the Association for Computational
Linguistics. Singapore.
1387
Coling 2010: Poster Volume, pages 1122?1130,
Beijing, August 2010
Informed ways of improving data-driven
dependency parsing for German
Wolfgang Seeker
University of Stuttgart
Inst. fu?r Maschinelle Sprachverarbeitung
seeker@ims.uni-stuttgart.de
Bernd Bohnet
University of Stuttgart
Inst. fu?r Maschinelle Sprachverarbeitung
Bernd.Bohnet@ims.uni-stuttgart.de
Lilja ?vrelid
University of Potsdam
Institut fu?r Linguistik
ovrelid@uni-potsdam.de
Jonas Kuhn
University of Stuttgart
Inst. fu?r Maschinelle Sprachverarbeitung
jonas@ims.uni-stuttgart.de
Abstract
We investigate a series of targeted modifi-
cations to a data-driven dependency parser
of German and show that these can be
highly effective even for a relatively well
studied language like German if they are
made on a (linguistically and methodolog-
ically) informed basis and with a parser
implementation that allows for fast and
robust training and application. Mak-
ing relatively small changes to a range
of very different system components, we
were able to increase labeled accuracy on
a standard test set (from the CoNLL 2009
shared task), ignoring gold standard part-
of-speech tags, from 87.64% to 89.40%.
The study was conducted in less than five
weeks and as a secondary project of all
four authors. Effective modifications in-
clude the quality and combination of auto-
assigned morphosyntactic features enter-
ing machine learning, the internal feature
handling as well as the inclusion of global
constraints and a combination of different
parsing strategies.
1 Introduction
The past years have seen an enormous surge of in-
terest in dependency parsing, mainly in the data-
driven paradigm, and with a particular emphasis
on covering a whole set of languages with a single
approach. The reasons for this interest are mani-
fold; the availability of shared task data from var-
ious CoNLL conferences (among others (Buch-
holz and Marsi, 2006; Hajic? et al, 2009)), com-
prising collections of languages based on a sin-
gle representation format, has certainly been in-
strumental. But likewise, the straightforward use-
fulness of dependency representations for a num-
ber of tasks plays an important role. The rela-
tive language independence of the representations
makes dependency parsing particularly attractive
for multilingually oriented work, including ma-
chine translation.
As data-driven approaches to dependency pars-
ing have reached a certain level of maturity, it may
appear as if further improvements of parsing per-
formance have to rely on relatively advanced tun-
ing procedures, such as sophisticated automatic
feature selection procedures or combinations of
different parsing approaches with complementary
strengths. It is indeed still hard to pinpoint the
structural properties of a language (or annotation
scheme) that make the parsing task easier for a
particular approach, so it may seem best to leave
the decision to a higher-level procedure.
This paper starts from the suspicion that
while sophisticated tuning procedures are cer-
tainly helpful, one should not underestimate the
potential of relatively simple modifications of the
experimental set-up, such as a restructuring of as-
pects of the dependency format, a targeted im-
provement of the quality of automatically as-
signed features, or a simplification of the feature
space for machine learning ? the modifications
just have to be made in an informed way. This
1122
presupposes two things: (i) a thorough linguistic
understanding of the issues at hand, and (ii) a rel-
atively powerful and robust experimental machin-
ery which allows for experimentation in various
directions and which should ideally support a fast
turn-around cycle.
We report on a small pilot study exploring the
potential of relatively small, informed modifica-
tions as a way of improving parsing accuracy
even for a language that has received considerable
attention in the parsing literature, including the
dependency parsing literature, namely German.
Within a timeframe of five weeks and spending
only a few hours a day on the project (between a
group of four people), we were able to reach some
surprising improvements in parsing accuracy.
By way of example, we experimented with
modifications in a number of rather different sys-
tem areas, which we will discuss in the course
of this paper after a brief discussion of related
work and the data basis in Section 2. Based on a
second-order maximum spanning tree algorithm,
we used a hash kernel to facilitate the mapping
of the features onto their weights for a very large
number of features (Section 3); we modified the
dependency tree representation for prepositional
phrases, adding hierarchical structure that facili-
tates the picking up of generalizations (Section 4).
We take advantage of a morphological analyzer
to train an improved part-of-speech tagger (Sec-
tion 5), and we use knowledge about the structure
of morphological paradigms and the morphology-
syntax interface in the feature design for machine
learning (Section 6). As is known from other stud-
ies, the combination of different parsing strategies
is advantageous; we include a relatively simple
parser stacking procedure in our pilot study (Sec-
tion 7), and finally, we apply Integer Linear Pro-
gramming in a targeted way to add some global
constraints on possible combinations of arc labels
with a single head (Section 8). Section 9 offers a
brief conclusion.
2 Related Work and Data Basis
We quickly review the situation in data-driven de-
pendency parsing in general and on applying it to
German specifically.
The two main approaches to data-driven de-
pendency parsing are transition based dependency
parsing (Nivre, 2003; Yamada and Matsumoto,
2003; Titov and Henderson, 2007) and maximum
spanning tree based dependency parsing (Eis-
ner, 1996; Eisner, 2000; McDonald and Pereira,
2006). Transition based parsers typically have
a linear or quadratic complexity (Attardi, 2006).
Nivre (2009) introduced a transition based non-
projective parsing algorithm that has a worst case
quadratic complexity and an expected linear pars-
ing time. Titov and Henderson (2007) combined
a transition based parsing algorithm, using beam
search, with a latent variable machine learning
technique.
Maximum spanning tree based dependency
parsers decompose a dependency structure into
factors. The factors of the first order maximum
spanning tree parsing algorithm are edges consist-
ing of the head, the dependent (child) and the edge
label. This algorithm has a quadratic complexity.
The second order parsing algorithm of McDonald
and Pereira (2006) uses a separate algorithm for
edge labeling. In addition to the first order fac-
tors, this algorithm uses the edges to those chil-
dren which are closest to the dependent and has a
complexity of O(n3). The second order algorithm
of Carreras (2007) uses in addition to McDonald
and Pereira (2006) the child of the dependent oc-
curring in the sentence between the head and the
dependent as well as the edge from the dependents
to a grandchild. The edge labeling is an integral
part of the algorithm which requires an additional
loop over the labels. This algorithm therefore has
a complexity of O(n4). Johansson and Nugues
(2008) reduced the required number of loops over
the edge labels by considering only the edges that
existed in the training corpus for a distinct head
and child part-of-speech tag combination.
Predating the surge of interest in data-based
dependency parsing, there is a relatively long
tradition of dependency parsing work on Ger-
man, including for instance Menzel and Schro?der
(1998) and Duchier and Debusmann (2001). Ger-
man was included in the CoNLL shared tasks in
2006 (Multilingual Dependency Parsing, (Buch-
holz and Marsi, 2006)) and in 2009 (Syntactic and
Semantic Dependencies in Multiple Languages,
(Hajic? et al, 2009)) with data based on the TIGER
1123
corpus (Brants et al, 2002) in both cases. Since
the original TIGER treebank is in a hybrid phrase-
structural/dependency format with a relatively flat
hierarchical structure, conversion to a pure depen-
dency format involves some non-trivial steps. The
2008 ACL Workshop on Parsing German included
a specific shared task on dependency parsing of
German (Ku?bler, 2008), based on two sets of data:
again the TIGER corpus ? however with a differ-
ent conversion routine than for the CoNLL tasks ?
and the Tu?Ba-D/Z corpus (Hinrichs et al, 2004).
In the 2006 CoNLL task and in the 2008 ACL
Workshop task, the task was dependency parsing
with given gold standard part-of-speech tags from
the corpus. This is a valid way of isolating the
specific subproblem of parsing, however it is clear
that the task does not reflect the application set-
ting which includes noise from automatic part-of-
speech tagging. In the 2009 CoNLL task, both
gold standard tags and automatically assigned tags
were provided. The auto-tagged version was cre-
ated with the standard model of the TreeTagger
(Schmid, 1995) (i.e., with no domain-specific tag-
ger training).
In our experiments, we used the data set from
the 2009 CoNLL task, for which the broadest
comparison of recent parsing approaches exists.
The highest-scoring system in the shared task was
Bohnet (2009) with a labeled accuracy (LAS) of
87.48%, on auto-tagged data. The highest-scoring
(in fact the only) system in the dependency pars-
ing track of the 2008 ACL Workshop on parsing
German was Hall and Nivre (2008) with an LAS
of 90.80% on gold-tagged data, and with a data
set that is not comparable to the CoNLL data.1
3 Hash Kernel
Our parser is based on a second order maximum
spanning tree algorithm and uses MIRA (Cram-
mer et al, 2006) as learning technique in combi-
nation with a hash kernel. The hash kernel has
a higher accuracy since it can use additional fea-
tures found during the creation of the dependency
1To get an idea of how the data sets compare, we trained
the version of our parser described in Section 3 (i.e., with-
out most of the linguistically informed improvements) on
this data, achieving labeled accuracy of 92.41%, compared
to 88.06% for the 2009 CoNLL task version.
tree in addition to the features extracted from the
training examples. The modification to MIRA is
simple: we replace the feature-index mapping that
maps the features to indices of the weight vector
by a random function. Usually, the feature-index
mapping in the support vector machine has two
tasks: The mapping maps the features to an index
and it filters out features that never occurred in a
dependency tree. In our approach, we do not filter
out these features, but use them as additional fea-
tures. It turns out that this choice improves pars-
ing quality. Instead of the feature-index mapping
we use the following hash function:2
h ? |(l xor(l ? 0xffffffff00000000 >> 32))% size|
The Hash Kernel for structured data uses the hash
function h : J ? {1...n} to index ? where ?
maps the observations X to a feature space. We
define ?(x, y) as the numeric feature representa-
tion indexed by J . The learning problem is to fit
the function F so that the errors of the predicted
parse tree y are as low as possible. The scoring
function of the Hash Kernel is defined as:3
F (x, y) = ??w ? ?(x, y)
For different j, the hash function h(j) might gen-
erate the same value k. This means that the hash
function maps more than one feature to the same
weight which causes weight collisions. This pro-
cedure is similar to randomization of weights (fea-
tures), which aims to save space by sharing val-
ues in the weight vector (Blum, 2006; Rahimi
and Recht, 2008). The Hash Kernel shares values
when collisions occur that can be considered as
an approximation of the kernel function, because
a weight might be adapted due to more than one
feature. The approximation works very well with
a weight vector size of 115 million values.
With the Hash Kernel, we were able to improve
on a baseline parser that already reaches a quite
high LAS of 87.64% which is higher than the top
score for German (87.48%) in the CoNLL Shared
task 2009. The Hash Kernel improved that value
by 0.42 percentage points to 88.06%. In addition
to that, we obtain a large speed up in terms of pars-
ing time. The baseline parser spends an average of
426 milliseconds to parse a sentence of the test
2>> n shifts n bits right, and % is the modulo operation.
3??w is the weight vector and the size of ??w is n.
1124
set and the parser with Hash Kernel only takes
126 milliseconds which is an increase in speed
of 3.4 times. We get the large speed up because
the memory access to a large array causes many
CPU cache misses which we avoid by replacing
the feature-index mapping with a hash function.
As mentioned above, the speedup influences the
experimenters? opportunities for explorative de-
velopment since it reduces the turnaround time for
experimental trials.
4 Restructuring of PPs
In a first step, we applied a treebank transforma-
tion to our data set in order to ease the learning
for the parser. We concentrated on prepositional
phrases (PP) to get an idea how much this kind
of transformation can actually help a parser. PPs
are notoriously flat in the TIGER Treebank anno-
tation (from which our data are derived) and they
do not embed a noun phrase (NP) but rather attach
all parts of the noun phrase directly at PP level.
This annotation was kept in the dependency ver-
sion and it can cause problems for the parser since
there are two different ways of annotating NPs: (i)
for normal NPs where all dependents of the noun
are attached as daughters of the head noun and (ii)
for NPs in PPs where all dependents of the noun
are attached as daughters to the preposition thus
being sisters to their head noun. We changed the
annotation of PPs by identifying the head noun in
the PP and attaching all of its siblings to it. To find
the correct head, we used a heuristic in the style of
Magerman (1995). The head is chosen by taking
the rightmost daughter of the preposition that has
a category label according to the heuristic and is
labeled with NK (noun kernel element).
Table 1 shows the parser performance on the
data after PP-restructuring.4 The explanation for
the benefit of the restructuring is of course that
4Note that we are evaluating against a gold standard here
(and in the rest of the paper) which has been restructured as
well. With a different gold standard one could argue that the
absolute figures we obtain are not fully comparable with the
original CoNLL shared task. However, since we are doing
dependency parsing, the transformation does neither add nor
remove any nodes from the structure nor do we change any
labels. The only thing that is done during the transforma-
tion is the reattachment of some daughters of a PP. This is
only a small modification, and it is certainly linguistically
warranted.
now there is only one type of NP in the whole cor-
pus which eases the parser?s task to correctly learn
and identify them.
dev. set test set
LAS UAS LAS UAS
hash kernel 87.40 89.79 88.06 90.24
+restructured 87.49 89.97 88.30 90.44
Table 1: Parser performance on restructured data
Since restructuring parts of the corpus seems
beneficial, there might be other structures where
more consistent annotation could help the parser,
e. g., coordination or punctuation (like in the 2008
ACL Workshop data set, cp. Footnote 1).
5 Part-of-Speech Tagging
High quality part-of-speech (PoS) tags can greatly
improve parsing quality. Having a verb wrongly
analyzed as a noun and similar mistakes are very
likely to mislead the parser in its decision process.
A lot of the parser?s features include PoS tags and
reducing the amount of errors during PoS tagging
will therefore reduce misleading feature values as
well. Since the quality of the automatically as-
signed PoS tags in the German CoNLL ?09 data
is not state-of-the-art (see Table 2 below), we de-
cided to retag the data with our own tagger which
uses additional information from a symbolic mor-
phological analyzer to direct a statistical classifier.
For the assignment of PoS tags, we apply
a standard maximum entropy classification ap-
proach (see Ratnaparkhi (1996)). The classes of
the classifier are the PoS categories defined in the
Stuttgart-Tu?bingen Tag Set (STTS) (Schiller et al,
1999). We use standard binarized features like
the word itself, its last three letters, whether the
word is capitalized, contains a hyphen, a digit or
whether it consists of digits only. As the only non-
binary feature, word length is recorded. These
standard features are augmented by a number of
binary features that support the classification pro-
cess by providing a preselection of possible PoS
tags. Every word is analyzed by DMOR, a finite
state morphological analyzer, from whose output
analyses all different PoS tags are collected and
added to the feature set. For example, DMOR
assigns the PoS tags NN (common noun) and
ADJD (predicative adjective) to the word gegan-
1125
gen (gone). From these analyses two features are
generated, namely possible-tag:NN and possible-
tag:ADJD, which are strong indicators for the
classifier that one of these classes is very likely
to be the correct one. The main idea here is to
use the morphological analyzer as a sort of lexicon
that preselects the set of possible tags beforehand
and then use the classifier to do the disambigua-
tion (see Jurish (2003) for a more sophisticated
system based on Hidden-Markov models that uses
roughly the same idea). Since the PoS tags are in-
cluded in the feature set, the classifier is still able
to assign every class defined in STTS even if it is
not in the preselection. Where the morphological
analyzer does not know the word in question we
add features for every PoS tag representing a pro-
ductive word class in German, making the reason-
able assumption that the morphology knows about
all closed-class words and word forms. Finally,
we add word form and possible tag features for
the previous and the following word to the feature
set thus simulating a trigram tagger. We used the
method of Kazama and Tsujii (2005) which uses
inequality constraints to do a very efficient feature
selection5 to train the maximum entropy model.
We annotated the entire corpus with versions
of our own tagger, i.e., the training, development
and test data. In order to achieve a realistic be-
havior (including remaining tagging errors, which
the parser may be able to react to if they are sys-
tematic), it was important that each section was
tagged without any knowledge of the gold stan-
dard tags. For the development and test portion,
this is straightforward: we trained a model on the
gold PoS of the training portion of the data and
applied it to retag these two portions. Retagging
the training portion was a bit trickier since we
could not use a model trained on the same data,
but at the same time, we wanted to use a tagger
of similarly high quality ? i.e. one that has seen a
similar amount of training data. The training set
was therefore split into 20 different parts and for
every split, a tagging model was trained on the
other 19 parts which then was used to retag the
remaining 20th part. Table 2 shows the quality
of our tagger evaluated on the German CoNLL
5We used a width factor of 1.0.
?09 data in terms of accuracy and compares it
to the originally annotated PoS tags which have
been assigned by using the TreeTagger (Schmid,
1995) together with the German tagging model
provided from the TreeTagger website. Tagging
accuracy improves consistently by about 2 per-
centage points which equates to an error reduction
of 44.55 % to 49.0 %.
training development test
original 95.69 95.51 95.46
retagged 97.61 97.71 97.52
error red. 44.55% 49.00% 45.37%
Table 2: Tagging accuracy
Table 3 shows the parser performance when
trained on the newly tagged data. The consider-
able improvements in tagging accuracy visibly af-
fect parsing accuracy, raising both the labeled and
the unlabeled attachment score by 0.66 percentage
points (LAS) and 0.51 points (UAS) for the de-
velopment set and by 0.45 points (LAS) and 0.64
points (UAS) for the test set.
dev. set test set
LAS UAS LAS UAS
restructured 87.49 89.97 88.30 90.44
+retagged 88.15 90.48 88.75 91.08
Table 3: Parser performance on retagged data
6 Morphological Information
German, as opposed to English, exhibits a rela-
tively rich morphology. Predicate arguments and
nominal adjuncts are marked with special case
morphology which allows for a less restricted
word order in German. The German case system
comprises four different case values, namely nom-
inative, accusative, dative and genitive case. Sub-
jects and nominal predicates are usually marked
with nominative case, objects receive accusative
or dative case and genitive case is usually used
to mark possessors in possessive constructions.
There are also some temporal and spatial nominal
adjuncts which require certain case values. Since
case is used to mark the function of a noun phrase
in a clause, providing case information to a parser
might improve its performance.
The morphological information in the German
CoNLL ?09 data contains much more information
than case alone and previous models (baseline,
1126
hash kernel, retagged) have used all of it. How-
ever, since we aim to improve a syntactic parser,
we would like to exclude all morphological infor-
mation from the parsing process that is not obvi-
ously relevant to syntax, e. g. mood or tense. By
reducing the morphological annotations to those
that are syntactically relevant, we hope to reduce
the noise that is introduced by irrelevant informa-
tion. (One might expect that machine learning and
feature selection should ?filter out? irrelevant fea-
tures, but given the relative sparsity of unambigu-
ous instances of the linguistically relevant effects,
drawing the line based on just a few thousand sen-
tences of positive evidence would be extremely
hard even for a linguist.)
We annotated every case-bearing word in the
corpus with its case information using DMOR.
With case-bearing words, we mean nouns, proper
nouns, attributive adjectives, determiners and all
kinds of pronouns. Other types of morphologi-
cal information was discarded. We did not use
the manually annotated and disambiguated mor-
phological information already present in the cor-
pus for two reasons: the first one is the same as
with the PoS tagging. Since it is unrealistic to
have gold-standard annotation in a real-world ap-
plication which deals with unseen data, we want
the parser to learn from and hopefully adapt to
imperfectly annotated data. The second reason
is the German-inherent form syncretism in nom-
inal paradigms. The German noun inflection sys-
tem is with over ten different (productive and
non-productive) inflectional patterns quite com-
plicated, and to make matters worse, there are
only five different morphological markers to dis-
tinguish 16 different positions in the pronoun, de-
terminer and adjective paradigms and eight differ-
ent positions in the noun paradigms. Some po-
sitions in the paradigm will therefore always be
marked in the same way and we would like the
parser to learn that some word forms will always
be ambiguous with respect to their case value.
We also conducted experiments where we an-
notated number and gender values in addition to
case. The idea behind this is that number and gen-
der might help to further disambiguate case val-
ues. The downside of this is the increase in fea-
ture values. Combining case and number features
means a multiplication of their values creating
eight new feature values instead of four. Adding
gender annotation raises this number to 24. Be-
side the disambiguation of case, there is also an-
other reason why we might want to add num-
ber and gender: Inside a German noun phrase,
all parts have to agree on their case and number
feature in order to produce a well-formed noun
phrase. Furthermore, the head noun governs the
gender feature of the other parts. Thus, all three
features can be relevant to the construction of a
syntactic structure.6 Table 4 shows the results of
our experiments with morphological features.
dev. set test set
LAS UAS LAS UAS
retagged 88.15 90.48 88.75 91.08
no morph. 87.78 90.18 88.60 90.92
+case 88.04 90.48 88.77 91.13
+c+n 88.21 90.62 88.88 91.13
+c+n+g 87.96 90.33 88.73 90.99
Table 4: Parser performance with morph. infor-
mation (c=case, n=number, g=gender)
The no morph row in Table 4 shows, that
using no morphological information at all de-
creases parser performance. When only case val-
ues are annotated, the parser performance does
not change much in comparison to the retagged
model, so there is no benefit here. Adding num-
ber features on the other hand improves parsing
results significantly. This seems to support our in-
tuition that number helps in disambiguating case
values. However, adding gender information does
not further increase this effect but hurts parser per-
formance even more than case annotation alone.
This leaves us with a puzzle here. Annotating case
and number helps the parser, but case alone or
having case, number and gender together affects
performance negatively. A possible explanation
might be that the effect of the gender information
is masked by the increased number of feature val-
ues (24) which confuses the parsing algorithm.
7 Parser Stacking
Nivre and McDonald (2008) show how two dif-
ferent approaches to data-driven dependency pars-
6Person would be another syntactically relevant informa-
tion. However, since we are dealing with a newspaper cor-
pus, first and second person features appear very rarely.
1127
ing, the graph-based and transition-based ap-
proaches, may be combined and subsequently
learn to complement each other to achieve im-
proved parsing results for different languages.
MaltParser (Nivre et al, 2006) is a language-
independent system for data-driven dependency
parsing which is freely available.7 It is based on a
deterministic parsing strategy in combination with
treebank-induced classifiers for predicting parsing
actions. MaltParser employs a rich feature repre-
sentation in order to guide parsing. For the train-
ing of the Malt parser model that we use in the
stacking experiments, we use learner and parser
settings identical to the ones optimized for Ger-
man in the CoNLL-X shared task (Nivre et al,
2006). Furthermore, we employ the technique
of pseudo-projective parsing described in Nilsson
and Nivre (2005) and a split prediction strategy for
predicting parse transitions and arc labels (Nivre
and Hall, 2008).8 In order to obtain automatic
parses for the whole data set, we perform a 10-
fold split. For the parser stacking, we follow the
approach of Nivre and McDonald (2008), using
MaltParser as a guide for the MST parser with the
hash kernel, i.e., providing the arcs and labels as-
signed by MaltParser as features. Table 5 shows
the scores we obtain by parser stacking. Although
our version of MaltParser does not quite have the
same performance as for instance the version of
Hall and Nivre (2008), its guidance leads to a
small improvement in the overall parsing results.
dev. set test set
LAS UAS LAS UAS
MaltParser 82.47 85.78 83.84 86.8
our parser 88.21 90.62 88.88 91.13
+stacking 88.42 90.77 89.28 91.40
Table 5: Stacked parser performance with guid-
ance by MaltParser
7http://maltparser.org
8The feature models make use of information about the
lexical form (FORM), the predicted PoS (PPOS) and the de-
pendency relation constructed thus far during parsing (DEP).
In addition, we make use of the predicted values for other
morphological features (PFEATS). We employ the arc-eager
algorithm (Nivre, 2003) in combination with SVM learners,
using LIBSVM with a polynomial kernel.
8 Relabeling
In the relabeling step, we pursue the idea that
some erroneous parser decisions concerning the
distribution of certain labels might be detected and
repaired in post-processing. In German and in
most other languages, there are syntactic restric-
tions on the number of subjects and objects that
a verb might select. The parser will learn this be-
havior during training. However, since it is using a
statistical model with a limited context, it can still
happen that two or more of the same grammati-
cal functions are annotated for the same verb. But
having two subjects annotated for a single verb
makes this particular clause uninterpretable for
subsequently applied tasks. Therefore, we would
like to detect those doubly annotated grammatical
functions and correct them in a controlled way.
The detection algorithm is simple: Running
over the words of the output parse, we check for
every word whether it has two or more daughters
annotated with the same grammatical function and
if we find one, we relabel all of its daughters.9 For
the relabeling, we applied a dependency-version
of the function labeler described in Seeker et al
(2010) which uses a maximum entropy classifier
that is restrained by a number of hard constraints
implemented as an Integer Linear Program. These
constraints model the aforementioned selectional
restrictions on the number of certain types of ver-
bal arguments. Since these are hard constraints,
the labeler is not able to annotate more than one
of those grammatical functions per verb. If we
count the number of sentences that contain doubly
annotated grammatical functions in the best pars-
ing results from the previous section, we get 189
for the development set and 153 for the test set.
About two thirds of the doubly annotated func-
tions are subjects and the biggest part of the re-
maining third are accusative objects which are the
most common arguments of German verbs.
Table 6 shows the final results after relabeling
the output of the best performing parser config-
uration from the previous section. The improve-
ments on the overall scores are quite small, which
9The grammatical functions we are looking for are SB
(subject), OA (accusative object), DA (dative), OG (genitive
object), OP (prepositional object), OC (clausal object), PD
(predicate) and OA2 (second accusative object).
1128
dev. set test set
LAS UAS LAS UAS
stacking 88.42 90.77 89.28 91.40
+relabeling 88.48 90.77 89.40 91.40
Table 6: Parse quality after relabeling
is partly due to the fact that the relabeling affects
only a small subset of all labels used in the data.
Furthermore, the relabeling only takes place if a
doubly annotated function is detected; and even
if the relabeling is applied we have no guarantee
that the labeler will assign the labels correctly (al-
though we are guaranteed to not get double func-
tions). Table 7 shows the differences in precision
and recall for the grammatical functions between
the original and the relabeled test set. As one can
see, scores stay mostly the same except for SB,
OA and DA. For OA, scores improve both in recall
and precision. For DA, we trade a small decrease
in precision for a huge improvement in recall and
vice versa for SB, but on a much smaller scale.
Generally spoken, relabeling is a local repair strat-
egy that does not have so much effect on the over-
all score but can help to get some important labels
correct even if the parser made the wrong deci-
sion. Note that the relabeler can only repair incor-
rect label decisions, it cannot help with wrongly
attached words.
original relabeled
rec prec rec prec
DA 64.2 83.2 74.7 79.6
OA 88.9 85.8 90.7 88.2
OA2 0.0 NaN 0.0 NaN
OC 95.2 93.5 95.1 93.7
OG 33.3 66.7 66.7 80.0
OP 54.2 80.8 54.2 79.9
PD 77.1 76.8 77.1 76.8
SB 91.0 90.6 90.7 93.7
Table 7: Improvements on grammatical functions
in the relabeled test set
9 Conclusion
We presented a sequence of modifications to a
data-driven dependency parser of German, depart-
ing from a state-of-the-art set-up in an imple-
mentation that allows for fast and robust train-
ing and application. Our pilot study tested what
can be achieved in a few weeks if the data-driven
technique is combined with a linguistically in-
formed approach, i.e., testing hypotheses of what
should be particularly effective in a very targeted
way. Most modifications were relatively small,
addressing very different dimensions in the sys-
tem, such as the handling of features in the Ma-
chine Learning, the quality and combination of
automatically assigned features and the ability to
take into account global constraints, as well as the
combination of different parsing strategies. Over-
all, labeled accuracy on a standard test set (from
the CoNLL 2009 shared task), ignoring gold stan-
dard part-of-speech tags, increased significantly
from 87.64% (baseline parser without hash ker-
nel) to 89.40%.10 We take this to indicate that a
targeted and informed approach like the one we
tested can have surprising effects even for a lan-
guage that has received relatively intense consid-
eration in the parsing literature.
Acknowledgements
We would like to thank Sandra Ku?bler, Yannick
Versley and Yi Zhang for their support. This work
was partially supported by research grants from
the Deutsche Forschungsgemeinschaft as part of
SFB 632 ?Information Structure? at the Univer-
sity of Potsdam and SFB 732 ?Incremental Speci-
fication in Context? at the University of Stuttgart.
References
Attardi, G. 2006. Experiments with a Multilanguage Non-
Projective Dependency Parser. In Proceedings of CoNLL,
pages 166?170.
Blum, A. 2006. Random Projection, Margins, Kernels, and
Feature-Selection. In LNCS, pages 52?68. Springer.
Bohnet, B. 2009. Efficient Parsing of Syntactic and Se-
mantic Dependency Structures. In Proceedings of CoNLL
2009).
Brants, Sabine, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER treebank.
In Proceedings of the Workshop on Treebanks and Lin-
guistic Theories, Sozopol.
Buchholz, Sabine and Erwin Marsi. 2006. CoNLL-X Shared
Task on Multilingual Dependency Parsing. In In Proc. of
CoNLL, pages 149?164.
Carreras, X. 2007. Experiments with a Higher-order Projec-
tive Dependency Parser. In EMNLP/CoNLL.
10?= 0.01, measured with a tool by Dan Bikel from
www.cis.upenn.edu/? dbikel/download/compare.pl
1129
Crammer, K., O. Dekel, S. Shalev-Shwartz, and Y. Singer.
2006. Online Passive-Aggressive Algorithms. Journal of
Machine Learning Research, 7:551?585.
Duchier, Denys and Ralph Debusmann. 2001. Topologi-
cal dependency trees: a constraint-based account of linear
precedence. In Proceedings of ACL 2001, pages 180?
187, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Eisner, J. 1996. Three New Probabilistic Models for Depen-
dency Parsing: An Exploration. In Proceedings of Coling
1996, pages 340?345, Copenhaen.
Eisner, J., 2000. Bilexical Grammars and their Cubic-time
Parsing Algorithms, pages 29?62. Kluwer Academic
Publishers.
Hajic?, J., M. Ciaramita, R. Johansson, D. Kawahara,
M. Anto`nia Mart??, L. Ma`rquez, A. Meyers, J. Nivre,
S. Pado?, J. S?te?pa?nek, P. Stran?a?k, M. Surdeanu, N. Xue,
and Y. Zhang. 2009. The CoNLL-2009 Shared Task:
Syntactic and Semantic Dependencies in Multiple Lan-
guages. In Proceedings of the 13th CoNLL-2009, June
4-5, Boulder, Colorado, USA.
Hall, Johan and Joakim Nivre. 2008. A dependency-driven
parser for German dependency and constituency represen-
tations. In Proceedings of the Workshop on Parsing Ger-
man, pages 47?54, Columbus, Ohio, June. Association for
Computational Linguistics.
Hinrichs, Erhard, Sandra Ku?bler, Karin Naumann, Heike
Telljohann, and Julia Trushkina. 2004. Recent develop-
ments in linguistic annotations of the tu?ba-d/z treebank.
In Proceedings of the Third Workshop on Treebanks and
Linguistic Theories, pages 51?62, Tu?bingen, Germany.
Johansson, R. and P. Nugues. 2008. Dependency-based
Syntactic?Semantic Analysis with PropBank and Nom-
Bank. In Proceedings of the Shared Task Session of
CoNLL-2008, Manchester, UK.
Jurish, Bryan. 2003. A hybrid approach to part-of-speech
tagging. Technical report, Berlin-Brandenburgische
Akademie der Wissenschaften.
Kazama, Jun?Ichi and Jun?Ichi Tsujii. 2005. Maximum en-
tropy models with inequality constraints: A case study on
text categorization. Machine Learning, 60(1):159?194.
Ku?bler, Sandra. 2008. The PaGe 2008 shared task on pars-
ing german. In Proceedings of the Workshop on Parsing
German, pages 55?63, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
Magerman, David M. 1995. Statistical decision-tree mod-
els for parsing. In Proceedings of ACL 1995, pages 276?
283, Morristown, NJ, USA. Association for Computa-
tional Linguistics Morristown, NJ, USA.
McDonald, R. and F. Pereira. 2006. Online Learning of Ap-
proximate Dependency Parsing Algorithms. In In Proc.
of EACL, pages 81?88.
Menzel, Wolfgang and Ingo Schro?der. 1998. Decision pro-
cedures for dependency parsing using graded constraints.
In Proceedings of the COLING-ACL ?98 Workshop on
Processing of Dependency-Based Grammars, pages 78?
87.
Nilsson, Jens and Joakim Nivre. 2005. Pseudo-projective
dependency parsing. In Proceedings of ACL 2005, pages
99?106.
Nivre, Joakim and Johan Hall. 2008. A dependency-driven
parser for German dependency and constituency represen-
tations. In Proceedings of the ACL Workshop on Parsing
German.
Nivre, J. and R. McDonald. 2008. Integrating Graph-Based
and Transition-Based Dependency Parsers. In ACL-08,
pages 950?958, Columbus, Ohio.
Nivre, Joakim, Jens Nilsson, Johan Hall, Gu?ls?en Eryig?it, and
Svetoslav Marinov. 2006. Labeled pseudo-projective de-
pendency parsing with Support Vector Machines. In Pro-
ceedings of CoNLL 2006.
Nivre, J. 2003. An Efficient Algorithm for Projective De-
pendency Parsing. In 8th International Workshop on
Parsing Technologies, pages 149?160, Nancy, France.
Nivre, J. 2009. Non-Projective Dependency Parsing in Ex-
pected Linear Time. In Proceedings of the 47th Annual
Meeting of the ACL and the 4th IJCNLP of the AFNLP,
pages 351?359, Suntec, Singapore.
Rahimi, A. and B. Recht. 2008. Random Features for
Large-Scale Kernel Machines. In Platt, J.C., D. Koller,
Y. Singer, and S. Roweis, editors, Advances in Neural
Information Processing Systems, volume 20. MIT Press,
Cambridge, MA.
Ratnaparkhi, Adwait. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of EMNLP 1996,
volume 1, pages 133?142.
Schiller, Anne, Simone Teufel, and Christine Sto?ckert. 1999.
Guidelines fu?r das Tagging deutscher Textcorpora mit
STTS (Kleines und gro?es Tagset). Technical Report Au-
gust, Universita?t Stuttgart.
Schmid, Helmut. 1995. Improvements in part-of-speech tag-
ging with an application to German. In Proceedings of the
ACL SIGDAT-Workshop, volume 11.
Seeker, Wolfgang, Ines Rehbein, Jonas Kuhn, and Josef Van
Genabith. 2010. Hard Constraints for Grammatical Func-
tion Labelling. In Proceedings of ACL 2010, Uppsala.
Titov, I. and J. Henderson. 2007. A Latent Variable Model
for Generative Dependency Parsing. In Proceedings of
IWPT, pages 144?155.
Yamada, H. and Y. Matsumoto. 2003. Statistical Depen-
dency Analysis with Support Vector Machines. In Pro-
ceedings of IWPT, pages 195?206.
1130
Speculation and Negation: Rules, Rankers,
and the Role of Syntax
Erik Velldal?
University of Oslo
Lilja ?vrelid?
University of Oslo
Jonathon Read?
University of Oslo
Stephan Oepen?
University of Oslo
This article explores a combination of deep and shallow approaches to the problem of resolving
the scope of speculation and negation within a sentence, specifically in the domain of biomedical
research literature. The first part of the article focuses on speculation. After first showing how
speculation cues can be accurately identified using a very simple classifier informed only by
local lexical context, we go on to explore two different syntactic approaches to resolving the
in-sentence scopes of these cues. Whereas one uses manually crafted rules operating over depen-
dency structures, the other automatically learns a discriminative ranking function over nodes
in constituent trees. We provide an in-depth error analysis and discussion of various linguistic
properties characterizing the problem, and show that although both approaches perform well
in isolation, even better results can be obtained by combining them, yielding the best published
results to date on the CoNLL-2010 Shared Task data. The last part of the article describes how our
speculation system is ported to also resolve the scope of negation. With only modest modifications
to the initial design, the system obtains state-of-the-art results on this task also.
1. Introduction
The task of providing a principled treatment of speculation and negation is a problem
that has received increased interest within the NLP community during recent years.
This is witnessed not only by this Special Issue, but also by the themes of several recent
shared tasks and dedicated workshops. The Shared Task at the 2010 Conference on Nat-
ural Language Learning (CoNLL) has been of central importance in this respect, where
the topic was speculation detection for the domain of biomedical research literature
? University of Oslo, Department of Informatics, PB 1080 Blindern, 0316 Oslo, Norway.
E-mail: {erikve,liljao,jread,oe}@ifi.uio.no.
Submission received: 5 April 2011; revised submission received: 30 September 2011; accepted for publication:
2 December 2011.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 2
(Farkas et al 2010). This particular area has been the focus of much current research,
triggered by the release of the BioScope corpus (Vincze et al 2008)?a collection of
scientific abstracts, full papers, and clinical reports with manual annotations of words
that signal speculation or negation (so-called cues), as well as of the scopes of these cues
within the sentences. The following examples from BioScope illustrate how sentences
are annotated with respect to speculation. Cues are here shown using angle brackets,
with braces corresponding to their annotated scopes:
(1) {The specific role of the chromodomain is ?unknown?} but chromodomain
swapping experiments in Drosophila {?suggest? that they {?might? be
protein interaction modules}} [18].
(2) These data {?indicate that? IL-10 and IL-4 inhibit cytokine production by
different mechanisms}.
Negation is annotated in the same way, as shown in the following examples:
(3) Thus, positive autoregulation is {?neither? a consequence ?nor? the sole
cause of growth arrest}.
(4) Samples of the protein pair space were taken {?instead of? considering the
whole space} as this was more computationally tractable.
In this article we develop several linguistically informed approaches to automati-
cally identify cues and resolve their scope within sentences, as in the example annota-
tions. Our starting point is the system developed by Velldal, ?vrelid, and Oepen (2010)
for the CoNLL-2010 Shared Task challenge. This system implements a two-stage hybrid
approach for resolving speculation: First, a binary classifier is applied for identifying
cues, and then their in-sentence scope is resolved using a small set of manually defined
rules operating on dependency structures.
In the current article we present several important extensions to the initial system
design of Velldal, ?vrelid, and Oepen (2010): First, in Section 5, we present a simpli-
fied approach to cue classification, greatly reducing the model size and complexity
of our Support Vector Machine (SVM) classifier while at the same time giving better
accuracy. Then, after reviewing the manually defined dependency-based scope rules
(Section 6.1), we show how the scope resolution task can be handled using an alternative
approach based on learning a discriminative ranking function over subtrees of HPSG-
derived constituent trees (Section 6.2). Moreover, by combining this empirical ranking
approach with the manually defined rules (Section 6.3), we are able to obtain the best
published results so far (to the best of our knowledge) on the CoNLL-2010 Shared
Task evaluation data. Finally, in Section 7, we show how our speculation system can be
ported to also resolve the scope of negation. Only requiring modest modifications, the
system also obtains state-of-the-art results on this task. Rather than merely presenting
the implementation details of the new approaches we develop, we also provide in-depth
error analyses and discussion on the linguistic properties of the phenomena of both
speculation and negation.
Before turning to the details of our approach, however, we start by presenting the
relevant data sets and the resources used for pre-processing in Section 2, followed by
a presentation of the various evaluation measures we will use in Section 3. We also
provide a brief review of relevant previous work in Section 4.
370
Velldal et al Rules, Rankers, and the Role of Syntax
2. Data Sets and Preprocessing
Our experiments center on the biomedical abstracts, full papers, and clinical reports of
the BioScope corpus (Vincze et al 2008). This comprises 20,924 sentences (or other root-
level utterances), annotated with respect to both negation and speculation. Some basic
descriptive statistics for the data sets are provided in Table 1. We see that roughly 18% of
the sentences are annotated as uncertain, and 13% contain negations. Note that, for our
speculation experiments, we will be using only the abstracts and the papers for training,
corresponding to the official CoNLL-2010 Shared Task training data. Moreover, we will
be using the Shared Task version of this data, in which certain annotation errors had
been corrected. The Shared Task task organizers also provided a set of newly annotated
biomedical articles for evaluation purposes, constituting an additional 5,003 utterances.
This latter data set (also detailed in Table 1) will be used for held-out testing of our
speculation models. We will be using the following abbreviations when referring to the
various parts of the data: BSA (BioScope abstracts), BSP (full papers), BSE (the held-
out evaluation data), and BSR (clinical reports). Note that, when we get to the negation
task we will be using the original version of the BioScope data. Furthermore, as BSE
does not annotate negation, we instead follow the experimental set-up of Morante and
Daelemans (2009b) for the negation task, reporting 10-fold cross validation on BSA and
held-out testing on BSP and BSR.
2.1 Tokenization
The BioScope data (and other data sets in the CoNLL-2010 Shared Task), are provided
sentence-segmented only, and otherwise non-tokenized. Unsurprisingly, the GENIA
tagger (Tsuruoka et al 2005) has a central role in our pre-processing set-up. We found
that its tokenization rules are not always optimally adapted for the type of text in Bio-
Scope, however. For example, GENIA unconditionally introduces token boundaries for
some punctuation marks that can also occur token-internally, thus incorrectly splitting
tokens like 390,926, methlycobamide:CoM, or Ca(2+). Conversely, GENIA fails to isolate
some kinds of opening single quotes, because the quoting conventions assumed in
BioScope differ from those used in the GENIA Corpus, and it mis-tokenizes LATEX-
style n- and m-dashes. On average, one in five sentences in the CoNLL training data
Table 1
The top three rows summarize the components of the BioScope corpus?abstracts (BSA), full
papers (BSP), and clinical reports (BSR)?annotated for speculation and negation. The bottom
row details the held-out evaluation data (BSE) provided for the CoNLL-2010 Shared Task.
Columns indicate the total number of sentences and their average length, the number of
hedged/negated sentences, the number of cues, and the number of multiword cues. (Note that
BSE is not annotated for negation, and we do not provide speculation statistics for BSR as this
data set will only be used for the negation experiments.
Speculation Negation
Sentences Length Sentences Cues MWCs Sentences Cues MWCs
BSA 11,871 26.1 2,101 2,659 364 1,597 1,719 86
BSP 2,670 25.7 519 668 84 339 376 23
BSR 6,383 7.7 ? ? ? 865 870 8
BSE 5,003 27.6 790 1,033 87 ? ? ?
371
Computational Linguistics Volume 38, Number 2
exhibited GENIA tokenization problems. Our pre-processing approach thus deploys a
cascaded finite-state tokenizer (borrowed and adapted from the open-source English
Resource Grammar: Flickinger [2002]), which aims to implement the tokenization deci-
sions made in the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993)?much
like GENIA, in principle?but more appropriately treating corner cases like the ones
noted here.
2.2 PoS Tagging and Lemmatization
For part-of-speech (PoS) tagging and lemmatization, we combine GENIA (with its
built-in, occasionally deviant tokenizer) and TnT (Brants 2000), which operates on
pre-tokenized inputs but in its default model is trained on financial news from the
Penn Treebank. Our general goal here is to take advantage of the higher PoS accuracy
provided by GENIA in the biomedical domain, while using our improved tokenization
and producing inputs to the parsers that as much as possible resemble the conventions
used in the original training data for the (dependency) parser (the Penn Treebank, once
again).
To this effect, for the vast majority of tokens we can align the GENIA tokeniza-
tion with our own, and in these cases we typically use GENIA PoS tags and lemmas
(i.e., base-forms). For better normalization, we downcase all lemmas except for proper
nouns. GENIA does not make a PoS distinction between proper vs. common nouns
(as assumed in the Penn Treebank), however, and hence we give precedence to TnT
outputs for tokens tagged as nominal by both taggers. Finally, for the small number of
cases where we cannot establish a one-to-one correspondence between GENIA tokens
and our own tokenization, we rely on TnT annotation only.
2.3 A Methodological Caveat
Unsurprisingly, the majority of previous work on BioScope seems to incorporate infor-
mation from the GENIA tagger in one way or another, whether it regards tokenization,
lemmatization, PoS information, or named entity chunking. Using the GENIA tagger for
pre-processing introduces certain dependencies to be aware of, however, as the abstracts
in BioScope are in fact also part of the GENIA corpus (Collier et al 1999) on which the
GENIA tagger is trained. This means that the accuracy of the information provided by
the tagger on this subset of BioScope cannot be expected to be representative of the
accuracy on other texts. Moreover, this effect might of course also carry over to any
downstream components using this information.
For the experiments described in this article, GENIA supplies lemmas for the
n-gram features used by the cue classifiers, as well as PoS tags used in the input to
both the dependency parser and the Head-driven Phrase Structure Grammar (HPSG)
parser (which in turn provide the inputs to our various scope resolution components).
For the HPSG parser, a subset of the GENIA corpus was also used as part of the
training data for estimating an underlying statistical parse selection model, producing
n-best lists of ranked candidate parses (MacKinlay et al 2011). When reporting final
test results on the full papers (BSP or BSE) or the clinical reports (BSR), no such
dependencies between information sources exists. It does mean, however, that we can
reasonably expect to see some extra drop in performancewhen going fromdevelopment
results on data that includes the BioScope abstracts to the test results on these other
data sets.
372
Velldal et al Rules, Rankers, and the Role of Syntax
3. Evaluation Measures
In this section we seek to clarify the type of measures we will be using for evaluating
both the cue detection components (Section 3.1) and the scope resolution components
(Section 3.2). Essentially, we here follow the evaluation scheme established by the
CoNLL-2010 Shared Task on speculation detection, also applying this when evaluating
results for the negation task.
3.1 Evaluation Measures for Cue Identification
For the approaches presented for cue detection in this article (for both speculation and
negation), we will be reporting precision, recall, and F1 for three different levels of
evaluation; the sentence-level, the token-level, and the cue-level. The sentence-level scores
correspond to Task 1 in the CoNLL-2010 Shared Task, that is, correctly identifying
whether a sentence contains uncertainty or not. The scores at the token-level measure
the number of individual tokens within the span of a cue annotation that the classifier
has correctly labeled as a cue. Finally, the stricter cue-level scores measure how well a
classifier succeeds in identifying entire cues (which will in turn provide the input for
the downstream components that later try to resolve the scope of the speculation or
negation within the sentence). A true positive at the cue-level requires that the predicted
cue exactly matches the annotation in its entirety (full multiword cues included).
For assessing the statistical significance of any observed differences in performance,
we will be using a two-tailed sign-test applied to the token-level predictions. This
is a standard non-parametric test for paired samples, which in our setting considers
how often the predictions of two given classifiers differ. Note that we will only be
performing significance testing for the token-level evaluation (unless otherwise stated),
as this is the level that most directly corresponds to the classifier decisions. We will be
assuming a significance level of ? = 0.05, but also reporting actual p-values in cases
where differences are not found to be significant.
3.2 Evaluation Measures for Scope Resolution
When evaluating scope resolution we will be following the methodology of the CoNLL-
2010 Shared Task, also using the scoring software made available by the task organiz-
ers.1 We have modified the software trivially so that it can also be used to evaluate
negation labeling. As pointed out by Farkas et al (2010), this way of evaluating scope is
rather strict: A true positive (TP) requires an exact match for both the entire cue and the
entire scope. On the other hand, a false positive (FP) can be incurred by three different
events; (1) incorrect cue labeling with correct scope boundaries, (2) correct cue labeling
with incorrect scope boundaries, or (3) incorrectly labeled cue and scope. Moreover,
conditions (1) and (2) will give a double penalty, in the sense that they also count as false
negatives (FN) given that the gold-standard cue or scope is missed (Farkas et al 2010).
Finally, false negatives are of course also incurred by cases where the gold-standard
annotations specify a scope but the system makes no such prediction.
Of course, the evaluation scheme outlined here corresponds to an end-to-end eval-
uation of the overall system, where the cue detection performance carries over to the
1 The Java code for computing the scores can be downloaded from the CoNLL-2010 Shared Task Web site:
http://www.inf.u-szeged.hu/rgai/conll2010st/.
373
Computational Linguistics Volume 38, Number 2
scope-level performance. In order to better assess the performance of a scope resolution
component in isolation, we will also report scope results against gold-standard cues. Note
that, when using gold-standard cues, the number of false negatives and false positives
will always be identical, meaning that the scope-level figures for recall, precision, and F1
will all be identical as well, and we will therefore only be reporting the latter in this set-
up. (The reason for this is that, when assuming gold-standard cues, only error condition
(2) can occur, which will in turn always count both a false positive and a false negative,
making the two figures identical.)
Exactly how to define the paired samples that form the basis of the statistical
significance testing is less straightforward for the end-to-end scope-level predictions
than for the cue identification. It is also worth noting that the CoNLL-2010 Shared Task
organizers themselves refrained from including any significance testing when report-
ing the official results. In this article we follow a recall-centered approach: For each
cue/scope pair in the gold standard, we simply note whether it is correctly identified
or not by a given system. The sequence of boolean values that results (FP = 0, TP = 1)
can be directly paired with the corresponding sequence for a different system so that
the sign-test can be applied as above.
Note that our modified scorer for negation is available from our Web page of sup-
plemental materials,2 together with the system output (in XML following the BioScope
DTD) for all end-to-end runs with our final model configurations.
4. Related Work on Speculation Labeling
Although there exists a body of earlier work on identifying uncertainty on the sentence
level, (Light, Qiu, and Srinivasan 2004; Medlock and Briscoe 2007; Szarvas 2008), the
task of resolving the in-sentence scope of speculation cues was first pioneered byMorante
and Daelemans (2009a). In this sense, the CoNLL-2010 Shared Task (Farkas et al 2010)
entered largely uncharted territory and contributed to an increased interest for this task.
Virtually all systems for resolving speculation scope implement a two-stage archi-
tecture: First there is a component that identifies the speculation cues and then there is a
component for resolving the in-sentence scopes of these cues. In this section we provide a
brief review of previous work on this problem, putting emphasis of the best performers
from the two corresponding subtasks of the CoNLL-2010 Shared Task, cue detection
(Task 1) and scope resolution (Task 2).
4.1 Related Work on Identifying Speculation Cues
The top-ranked system for Task 1 in the official CoNLL-2010 Shared Task evaluation
approached cue identification as a sequence labeling problem (Tang et al 2010). Similarly to
the decision-tree approach of Morante and Daelemans (2009a), Tang et al (2010) set out
to label tokens according to a BIO-scheme; indicating whether they are at the Beginning,
Inside, or Outside of a speculation cue. In the ?cascaded? system architecture of Tang et
al. (2010), the predictions of both a Conditional Random Field (CRF) sequence classifier
and an SVM-based Hidden Markov Model (HMM) are both combined in a second CRF.
In terms of the overall approach, namely, viewing the problem as a sequence la-
beling task, Tang et al (2010) are actually representative of the majority of the Shared
Task participants for Task 1 (Farkas et al 2010), including the top three performers on
2 Supplemental materials; http://www.velldal.net/erik/modneg/.
374
Velldal et al Rules, Rankers, and the Role of Syntax
the official held-out data. Many participants instead approached the task as a word-by-
word token classification problem, however. Examples of this approach are the systems of
Velldal, ?vrelid, and Oepen (2010) and Vlachos and Craven (2010), sharing the fourth
rank position (out of 24 submitted systems) for Task 1.
In both the sequence- and token-classification approaches, sentences are labeled
as uncertain if they are found to contain a cue. In contrast to this, a third group of
systems instead label sentences directly, typically using bag-of-words features. Such
sentence classifiers tended to achieve a somewhat lower relative rank in the official Task 1
evaluation (Farkas et al 2010).
4.2 Related Work on Resolving Speculation Scope
As mentioned earlier, the task of resolving the scope of speculation was first introduced
inMorante andDaelemans (2009a), where a system initially designed for negation scope
resolution (Morante, Liekens, and Daelemans 2008) was ported to speculation. Their
general approach treats the scope resolution task in much the same way as the cue
identification task: as a sequence labeling task and using only token-level, lexical infor-
mation. Morante, van Asch, and Daelemans (2010) then extended on this system by also
adding syntactic features, resulting in the top performing system of the CoNLL-2010
Shared Task at the scope-level (corresponding to the second subtask). It is interesting
to note that all the top performers use various types of syntactic information in their
scope resolution systems: The output from a dependency parser (MaltParser) (Morante,
van Asch, and Daelemans 2010; Velldal, ?vrelid, and Oepen 2010), a tag sequence
grammar (RASP) (Rei and Briscoe 2010), as well as constituent analysis in combination
with dependency triplets (Stanford lexicalized parser) (Kilicoglu and Bergler 2010).
The majority of systems perform classification at the token level, using some variant
of machine learning with a BIO classification scheme and a post-processing step to
assemble the full scope (Farkas et al 2010), although several of the top performers
employ manually constructed rules (Kilicoglu and Bergler 2010; Velldal, ?vrelid, and
Oepen 2010) or even combinations of machine learning and rules (Rei and Briscoe 2010).
5. Identifying Speculation Cues
We now turn to look at the details of our own system, starting in this section with
describing a simple yet effective approach to identifying speculation cues. A cue is here
taken to mean the words or phrases that signal the attitude of uncertainty or specula-
tion. As noted by Farkas et al (2010), most hedge cues typically fall in the following cate-
gories; adjectives or adverbs (probable, likely, possible, unsure, etc.), auxiliaries (may,might,
could, etc.), conjunctions (either. . . or, etc.), or verbs of speculation (suggest, suspect, sup-
pose, seem, etc.). Judging by the examples in the Introduction, it might at first seem that
the speculation cues can be identified merely by consulting a pre-compiled list. Most, if
not all, words that can function as cues can also occur as non-cues, however. More than
85% of the cue lemmas observed in the BioScope corpus also have non-cue occurrences.
To give just one example, a hedge detection system needs to correctly discriminate
between the use of appear as a cue in Example (5), and as a non-cue in Example (6):
(5) In 5 patients the granulocytes {?appeared? polyclonal} [. . . ]
(6) The effect appeared within 30 min and returned to basal levels after 2 h.
375
Computational Linguistics Volume 38, Number 2
In the approach of Velldal, ?vrelid, and Oepen (2010), a binary token classifier was
applied in a way that labeled each and every word as cue or non-cue. We will refer to this
mode of classification as word-by-word classification (WbW). The follow-up experi-
ments described by Velldal (2011) showed that comparable results could be achieved
using a filtering approach that ignores words not occurring as cues in the training data.
This greatly reduces both the number of relevant training examples and the number
of features in the model, and in the current article we simplify this ?disambiguation
approach? even further. In terms of modeling framework, we implement our models
as linear SVM classifiers, estimated using the SVMlight toolkit (Joachims 1999). We also
include results for a very simple baseline model, however?to wit, a WbW approach
classifying each word simply based on its observed majority usage as a cue or non-cue
in the training data. Then, as for all our models, if a given sentence is found to contain
a cue, the entire sentence is subsequently labeled uncertain. Before turning to the indi-
vidual models, however, we first describe how we deal with the issue ofmultiword cues.
5.1 Multiword Cues
In the BioScope annotations, it is possible for a speculation cue to span multiple tokens
(e.g., raise an intriguing hypothesis). As seen from Table 1, about 13.5% of the cues in the
training data are such multiword cues (MWCs). The distribution of these cues is very
skewed, however. For instance, although the majority of MWCs are very infrequent
(most of them occurring only once), the pattern indicate that accounts for more than 70%
of the cases alone. Exactly which cases are treated as MWCs often seems somewhat
arbitrary and we have come across several inconsistencies in the annotations. We there-
fore choose to not let the classifiers we develop in this article be sensitive to the notion
of multiword cues. A given word token is considered a cue as long as it falls within
the span of a cue annotation. Multiword cues are instead treated in a separate post-
processing step, applying a small set of heuristic rules that aim to capture only the most
frequently occurring patterns observed in the training data. For example, if we find that
indicate is classified as a cue and it is followed by that, a rule will fire that ensures we
treat these tokens as a single cue. (Note that the rules are only applied to sentences that
have already been labeled uncertain by the classifier.) Table 2 lists the lemma patterns
currently covered by our rules.
5.2 Reformulating the Classification Problem: A Filtered Model
Before detailing our approach, we start with some general observations about the data
and the task. An error analysis of the initial WbW classifier developed by Velldal,
Table 2
Patterns covered by our rules for multiword speculation cues.
cannot {be}? exclude
either .+ or
indicate that
may,? or may not
no {evidence | proof | guarantee}
not {known | clear | evident | understood | exclude}
raise the .* {possibility | question | issue | hypothesis}
whether or not
376
Velldal et al Rules, Rankers, and the Role of Syntax
?vrelid, and Oepen (2010) revealed it was not able to generalize to new speculation
cues beyond those observed during training. On the other hand, only a rather small
fragment of the test cues are actually unseen: Using a 10-fold split for the development
data, the average ratio of test cues that also occur as cues in training is more than 90%.
Another important observation we can take into account is that although it seems
reasonable to assume that anyword occurring as a cue can also occur as a non-cue (recall
that more than 85% of the observed cues also have non-cue occurrences in the training
data), the converse is less likely. Whereas the training data contains a total of approxi-
mately 17,600 unique base forms, only 143 of these ever occur as speculation cues.
As a consequence of these observations, Velldal (2011) proposed that one might
reasonably treat the set of cue words as a near-closed class, at least for the biomedical
data considered in this study. This means reformulating the problem as follows. Instead
of approaching the task as a classification problem defined for all words, we only
consider words that have a base form observed as a speculation cue in the training
material. By restricting the classifier to only this subset of words, we can simplify the
classification problem tremendously. As we shall see, it also has the effect of leveling
out the initial imbalance between negative and positive examples in the data, acting as
a (selective rather than random) downsampling technique.
One reasonable fear here, perhaps, might be that this simplification comes at the
expense of recall, as we are giving up on generalizing our predictions to any previously
unseen cues. As noted earlier, however, the initial WbW model of Velldal, ?vrelid, and
Oepen (2010) already failed to make any such generalizations, and, as we shall see, this
reformulation comes without any loss in performance and actually leads to an increase
in recall compared to a full WbWmodel using the same feature set.
Note that although we will approach the task as a ?disambiguation problem,? it is
not feasible to train separate classifiers for each individual base form. The frequency
distribution of the cue words in the training material is rather skewed with most cues
being very rare?many occurring as a cue only once (? 40%, constituting less than
1.5% of the total number of cue word instances). (Most of these words also have many
additional occurrences in the training data as non-cues, however.) For the majority of
the cue words, then, it seems we cannot hope to gather enough reliable information to
train individual classifiers. Instead, we want to be able to draw on information from
the more frequently occurring cues also when classifying or disambiguating the less
frequent ones. Consequently, we will still train a single global classifier.
Extending on the approach of Velldal (2011), we include a final simple step to reduce
the set of relevant training examples even further. As pointed out in Section 5.1, any
token occurring within a cue annotation is initially regarded as a cue word. Many
multiword cues also include function words, punctuation, and so forth, however. In
order to filter out such spurious but high-frequency ?cues,? we compiled a small stop-
list on the basis of the MWCs in training data (containing just a dozen tokens, namely,
a, an, as, be, for, of, that, the, to, with, ?,?, and ?-?).
5.2.1 Features. After experimenting with a wide range of different features, ?vrelid,
Velldal, and Oepen (2010) concluded that syntactic features appeared unnecessary for
the cue classification task, and that simple sequence-oriented n-gram features recording
immediate lexical context based on lemmas and surface forms is what gave the best
performance.
Initially, the n-gram feature templates we use in the current article record neighbors
for up to three positions left/right of the focus word. For increased generality, we also
include non-lexicalized variants, that is, recording only the neighbors while excluding
377
Computational Linguistics Volume 38, Number 2
the focus word itself. After a grid search across the various configurations of these
features, the best performance was found for a model recording n-grams of lemmas up
to three positions left and right of the focus word, and n-grams of surface forms up to
two positions to the right.
Table 3 shows the performance of the filteringmodel when using this feature config-
uration and testing by 10-fold cross-validation on the training data (BSA and BSP), also
contrasting performance with the majority usage baseline. Achieving a sentence-level
F1 of 92.04 (compared to 89.07 for the baseline), a token-level score of 89.57 (baseline =
86.42), and a cue-level score of 89.11 (baseline = 85.57), it performs significantly better
than the baseline. Applying the sign-test as described in Section 3.1, the token-level
differences were found to be significant for p < 0.05. It is also clear, however, that the
simple baseline appears to be fairly strong.
As discussed previously, part of the motivation for introducing the filtering scheme
is to create a model that is as simple as possible without sacrificing performance. In
addition to the evaluation scores, therefore, it is also worth noting some statistics related
to the classifier and the training data itself. Before looking into the properties of the fil-
tering set-up though, let us start, for the sake of comparison, by considering some prop-
erties of a learning set-up based on full WbW classification like the model of Velldal,
?vrelid, and Oepen (2010), assuming an identical feature configuration as used for the
given filtering model. The row titled WbW in Table 3 lists the development results for
this model, and we see that they are slightly lower than for the filtering model (with the
differences being significant for ? = 0.05). Although precision is slightly higher, recall is
substantially lower. Assuming a 10-fold cross-validation scheme like this, the number of
training examples presented to the WbW learner in each fold averages roughly 340,000,
corresponding to the total number of word tokens. Among these training examples,
the ratio of positive to negative examples (cues vs. non-cues) is roughly 1:100. In other
words, the data is initially very skewed when it comes to class balance. In terms of the
size of the feature set, the average number of distinct feature types per fold, assuming
the given feature configuration, would be roughly 2,600,000 under a WbW set-up.
Turning now to the filtering model, the average number of training examples
presented to the learner in each fold is reduced from roughly 340,000 to just 10,000.
Correspondingly, the average number of distinct feature types is reduced from well
above 2,600,000 to roughly 100,000. The class balance among the tokens given to the
learner is alsomuch less skewed, with positive examples now averaging 30%, compared
to 1% for the WbW set-up. Finally, we observe that the complexity of the model in
terms of how many training examples end up as support vectors (SVs) defining the
separating hyperplane is also considerably reduced: Although the average number of
SVs in each fold corresponds to roughly 14,000 examples for the WbW model, this is
down to roughly 5,000 for the final filtered model. Note that for the SVM regularization
Table 3
Development results for detecting speculationCUES:Averaged 10-fold cross-validation results for
the cue classifiers on both the abstracts and full papers in the BioScope training data (BSA andBSP).
Sentence Level Token Level Cue Level
Model Prec Rec F1 Prec Rec F1 Prec Rec F1
Baseline 91.07 87.21 89.07 91.61 81.85 86.42 90.49 81.16 85.57
WbW 95.01 88.03 91.37 95.29 82.78 88.58 94.65 82.26 88.02
Filtering 94.52 89.72 92.04 94.88 84.86 89.57 94.13 84.60 89.11
378
Velldal et al Rules, Rankers, and the Role of Syntax
parameter C, governing the trade-off between training error and margin size, we will
always be using the default value set by SVMlight. This value is analytically determined
from the training data, and further empirical tuning has in general not led to improve-
ments on our data sets.
5.2.2 The Effect of Data Size. Given how the filtered classifier treats the set of cues as a
closed class, a reasonable concern is its sensitivity to the size of the training set. In order
to further assess this effect, we computed learning curves showing how classifier per-
formance on the development data changes as we incrementally include more training
examples (see Figure 1). For reference we also include learning curves for the word-by-
word classifier using the identical feature configuration, as well as the majority usage
baseline.
As expected, we see that classifier performance steadily improves as more training
data is included. Although additional data would no doubt be beneficial, we reassur-
ingly observe that the curve seems to start gradually flattening out somewhat. If we
instead look at the performance curve for the WbW classifier we find that, while having
roughly the same shape as that of the filtered classifier, although consistently lower, it
nonetheless appears to be more sensitive to the size of the training set. Interestingly,
we see that the baseline model seems to be the one that is least affected by data size. It
actually outperforms the standard WbWmodel for the first three increments, but at the
same time it seems unable to benefit much at all from additional data.
5.2.3 Error Analysis.When looking at the distribution of errors at the cue-level (totaling
just below 700 across the 10-fold run), we find that roughly 74% are false negatives.
Rather than being caused by legitimate cue words being filtered out during training,
however, the FNs mostly pertain to a handful of high-frequency words that are also
highly ambiguous. When sorted according to error frequency, the top four candidates
alone constitute almost half the total number of FNs: or (24% of the FNs), can (10%),
could (7%), and either (6%). Looking more closely at the distribution of these words in
Figure 1
Learning curves showing the effect on token-level F1 for speculation cues when withdrawing
some portion of the training partitions across the 10-fold cycles. The size of the training set is
shown on a logarithmic scale to better see whether improvements are constant for n-fold
increases of data.
379
Computational Linguistics Volume 38, Number 2
the training data, it is easy to see how they pose a challenge for the learner. For example,
whereas or has a total of 1,215 occurrences, only 153 of these are annotated as a cue.
Distinguishing the different usages from each other can sometimes be difficult even for
a human eye, as testified also by the many inconsistencies we observed in the gold-
standard annotation of these cases.
Turning our attention to the other end of the tail, we find that just over 40 (8%) of the
FNs involve tokens for which there is only a single occurrence as a cue in the training
data. In other words, these would first appear to be exactly the tokens that we could
never get right, given our filtering scheme. We find, however, that most of these cases
regard tokens whose one and only appearance as a cue is as part of a multiword cue,
although they typically have a high number of other non-cue occurrences as well. For
example, although number occurs a total of 320 times, its one and only occurrence as
a cue is in the multiword cue address a number of questions. Given that this and several
other equally rare patterns are not currently covered by our MWC rules in the first
place, we would not have been able to get them right even if all the individual tokens
had been classified as cues (recall that a true positive at the cue-level requires an exact
match of the entire span). In total we find that 16% of the cue-level FNs corresponds to
multiword cues.
When looking at the frequency of multiword cues among the false positives, we
find that they only make up roughly 5% of the errors. Furthermore, a manual inspection
reveals that they can all be argued to be instances of annotation errors, in that we believe
these should actually be counted as true positives. Most of them involve indicate that and
not known, as in the following examples (where the cues assigned by our system are not
annotated as cues in BioScope):
(7) In contrast, levels of the transcriptional factor AP-1, which is ?not known?
to be important in B cell Ig production, were reduced by TGF-beta.
(8) Analysis of the nuclear extracts [. . . ] ?indicated that? the composition of
NF-kappa B was similar in neonatal and adult cells.
All in all, the errors in the FP category make up 26% of the total number of errors.
Just as for the FNs, the frequency distribution of the cues involved is quite skewed,
with a handful of highly frequent and highly ambiguous cue words accounting for the
bulk of the errors: The modal could (20%), and the adjectives putative (11%), possible
(6%), potential (6%), and unknown (5%). After manually inspecting the full set of FPs,
however, we find that at least 60% of them should really be counted as true positives.
The following are just a few examples where cues predicted by our classifier are not
annotated as such in BioScope and therefore counted as FPs.
(9) IEF-1, a pancreatic beta-cell type-specific complex ?believed? to regulate
insulin expression, is demonstrated to consist of at least two distinct
species, [. . . ]
(10) We ?hypothesize? that a mutation of the hGR glucocorticoid-binding
domain is the cause [. . . ]
(11) Antioxidants have been ?proposed? to be anti-atherosclerotic agents; [. . . ]
(12) Finally, matDCC might be further stabilized by the addition of roX1 RNA,
which could interact with several of the MSLs and ?perhaps? roX2 RNA
as well.
380
Velldal et al Rules, Rankers, and the Role of Syntax
One interesting source of real FPs concerns ?anti-hedges,? which in the training data
appear with a negation and as part of a multiword cue, for example no proof. During
testing, the classifier will sometimes wrongly predict a word like proof to be a specu-
lation cue, even when it is not negated. Because we already have MWC rules for cases
like this (see Section 5.1) it would be easy to also include a check for ?negative context,?
making sure that such tokens are not classified as cues if the requiredmultiword context
is missing.
Before rounding off this section, a brief look at the BioScope inter-annotator agree-
ment rates may offer some further perspective on the results discussed here. Note that
when creating the BioScope data, the decisions of two independent annotators were
merged by a third expert linguist who resolved any differences. The F1 of each set of
annotations toward the final gold-standard cues are reported by Vincze et al (2008) to
be 83.92 / 92.05 for the abstracts and 81.49 / 90.81 for the full papers. (Recall from Table 3
that our cue-level F1 for the cross-validation runs on the abstracts and papers is 89.11.)
When instead comparing the decisions of the two annotators directly, the F1 is reported
to be 79.12 for the abstracts and 77.60 for the papers.
5.3 Held-Out Results for Identifying Speculation Cues
Table 4 presents the final evaluation of the various cue classifiers developed in this
section, as applied to the held-out BSE test data. In addition to the evaluation results for
our own classifiers, Table 4 also includes the official test results for the system described
by Tang et al (2010). The sequence classifier developed by Tang et al (2010)?combining
a CRF classifier and a large-margin HMM model?obtained the best results for the
official Shared Task evaluation for Task 1 (i.e., sentence-level uncertainty detection), as
well as the highest cue-level scores.
As seen from Table 4, although themodel of Tang et al (2010) still achieves a slightly
higher F1 (81.34) than our filtered disambiguation model for the cue-level, our model
achieves a slightly higher F1 (86.58) for the sentence-level (yielding the best-published
result for this task so far, to the best of our knowledge). The differences are not deemed
statistically significant by a two-tailed sign-test, however (p = 0.37). It is interesting to
note, however, that the two approaches appear to have somewhat different strengths
and weaknesses: Whereas our filtering classifier consistently shows stronger precision
(and theWbWmodel even more so), the model of Tang et al (2010) is stronger on recall.
The sentence-level recall of our filtered classifier is still better than any of the remaining
23 systems submitted for the Shared Task evaluation, however, and, more interestingly,
it improves substantially on the recall of the full WbW classifier.
Table 4
Held-out results for identifying speculation cues: Applying the cue classifiers to the 5,003
sentences in BSE? the biomedical papers provided for the CoNLL-2010 Shared Task evaluation.
Sentence Level Token Level Cue Level
Model Prec Rec F1 Prec Rec F1 Prec Rec F1
Baseline 77.59 81.52 79.51 77.16 72.39 74.70 75.15 72.49 73.80
WbW 89.28 83.29 86.18 87.62 73.95 80.21 86.33 74.21 79.82
Filtering 87.87 85.32 86.58 86.46 76.74 81.31 84.79 77.17 80.80
Tang et al 2010 85.03 87.72 86.36 n/a n/a n/a 81.70 80.99 81.34
381
Computational Linguistics Volume 38, Number 2
We find that, just as for the development data, the reformulation of the cue clas-
sification task as a simple disambiguation problem improves F1 across all evaluation
levels, consistently outperforming the WbW classifiers. When computing a two-tailed
signed-test for the token-level decisions (where the WbW and filtering model achieves
an F1 of 80.21 and 81.31, respectively) the differences are not found to be significant (p =
0.12). As discussed in Section 5.2, however, it is important to bear in mind that the size
and complexity of the filtered ?disambiguation? model is greatly reduced compared
to the WbW model, using a much smaller number of features and relevant training
examples.
While on the topic of model complexity, it is also worth noting that many of the
systems participating in the CoNLL-2010 Shared Task challenge used fairly complex
and resource-heavy feature types, being sensitive to properties of document structure,
grammatical relations, deep syntactic structure, and so forth (Farkas et al 2010). The fact
that comparable or better results can be obtained using a relatively simplistic approach
as developed in this section, with surface-oriented features that are only sensitive to the
immediate lexical context, is an interesting result in its own right. In fact, even the simple
majority usage baseline classifier proves to be surprisingly competitive: Comparing its
sentence-level F1 to those of the official Shared Task evaluation, it actually outranks 7 of
the 24 submitted systems.
A final point that deserves some discussion is the drop in F1 that we observe when
going from the development results to the held-out results. There are several reasons for
this drop. Section 2.3 discussed how certain overfitting effects might be expected from
the GENIA-based pre-processing. In addition to this, it is likely that there are MWC
patterns in the held-out data that were not observed in the training data, and that are
therefore not covered by our MWC rules. Another factor that may have slightly inflated
the development results is the fact that we used a sentence-level rather than a document-
level partitioning of the data for cross-validation.
6. Resolving the Scope of Speculation Cues
Once the speculation cue has been determined using the cue detection system described
here, we go on to determine the scope of the speculation within the sentence. This task
corresponds to Task 2 of the CoNLL-2010 Shared Task. Example (13), which will be
used as a running example throughout this section, shows a scope-resolved BioScope
sentence where speculation is signaled by the modal verb may.
(13) {The unknown amino acid ?may? be used by these species}.
The exact scope will vary quite a lot depending on linguistic properties of the cue
in question, and in our approaches to scope resolution we rely heavily on syntactic
information. We experiment with two different approaches to syntactic analysis; data-
driven dependency parsing and grammar-driven phrase structure parsing. Because
scope determination in BioScope makes reference to subtle and fine-grained linguistic
distinctions (e.g., passivization or subject raising), in both cases we choose parsing
systems that make available comparatively ?deep? syntactic analyses. In the following
we present three different systems; a rule-based approach using dependency structures
(Section 6.1), a data-driven approach using an SVM ranker for selecting appropriate
subtrees in constituent structures (Section 6.2), and finally a hybrid approach combining
the rules and the ranker (Section 6.3).
382
Velldal et al Rules, Rankers, and the Role of Syntax
6.1 A Rule-Based Approach Using Dependency Structures
?vrelid, Velldal, and Oepen (2010) applied a small set of heuristic rules oper-
ating over syntactic dependency structures to define the scope for each cue. In the
following we will provide a detailed description of these rules and the syntactic gen-
eralizations they provide for the scope of speculation (Section 6.1.2). We will evalu-
ate their performance using both gold-standard cues and cues predicted by our cue
classifier (Section 6.1.3), in addition to providing an in-depth manual error analysis
(Section 6.1.5). We start out, however, by presenting some specifics about the processing
of the data; introducing the stacked dependency parser that produces the input to our
rules (Section 6.1.1) and quantifying the effect of using a domain-adapted PoS tagger
(Section 6.1.4).
6.1.1 Stacked Dependency Parsing. For syntactic analysis we use the open-source Malt-
Parser (Nivre, Hall, and Nilsson 2006), a platform for data-driven dependency parsing.
For improved accuracy and portability across domains and genres, we make our parser
incorporate the predictions of a large-scale, general-purpose Lexical-Functional Gram-
mar parser. A technique dubbed parser stacking enables the data-driven parser to learn
from the output of another parser, in addition to gold-standard treebank annotations
(Martins et al 2008; Nivre and McDonald 2008). This technique has been shown to
provide significant improvements in accuracy for both English and German (?vrelid,
Kuhn, and Spreyer 2009), and a similar set-up using an HPSG grammar has been shown
to increase domain independence in data-driven dependency parsing (Zhang andWang
2009). The stacked parser used here is identical to the parser described in ?vrelid,
Kuhn, and Spreyer (2009), except for the preprocessing in terms of tokenization and
PoS tagging, which is performed as detailed in Sections 2.1?2.2. The parser combines
two quite different approaches?data-driven dependency parsing and ?deep? parsing
with a hand-crafted grammar?and thus provides us with a broad range of different
types of linguistic information to draw upon for the speculation resolution task.
MaltParser is based on a deterministic parsing strategy in combination with
treebank-induced classifiers for predicting parse transitions. It supports a rich feature
representation of the parse history in order to guide parsing andmay easily be extended
to take into account additional features. The procedure to enable the data-driven parser
to learn from the grammar-driven parser is quite simple. We parse a treebank with
the XLE platform (Crouch et al 2008) and the English grammar developed within the
ParGram project (Butt et al 2002). We then convert the LFG output to dependency
structures, so that we have two parallel versions of the treebank?one gold-standard
and one with LFG annotation. We extend the gold-standard treebank with additional
information from the corresponding LFG analysis and train MaltParser on the
enhanced data set. For a description of the parse model features and the dependency
substructures proposed by XLE for each word token, see Nivre and McDonald (2008).
For further background on the conversion and training procedures, see ?vrelid, Kuhn,
and Spreyer (2009).
Table 5 shows the enhanced dependency representation for the sentence in Ex-
ample (13). For each token, the parsed data contains information on the word form,
lemma, and PoS, as well as the head and dependency relation (last two columns). The
added XLE information resides in the Features column and in the XLE-specific head and
dependency columns (XHead and XDep). Parser outputs, which in turn form the basis
for our scope resolution rules, also take this same form. The parser used in this work is
trained on the Wall Street Journal Sections 2?24 of the Penn Treebank (PTB), converted
383
Computational Linguistics Volume 38, Number 2
Table 5
Stacked dependency representation of the sentence in Example (13), lemmatized and annotated
with GENIA PoS tags, Malt parses (Head,DepRel), and XLE parses (XHead, XDep), as well as
other morphological and lexical semantic features extracted from the XLE analysis (Features).
Id Form PoS Features XHead XDep Head DepRel
1 The DT _ 4 SPECDET 4 NMOD
2 unknown JJ degree:attributive 4 ADJUNCT 4 NMOD
3 amino JJ degree:attributive 4 ADJUNCT 4 NMOD
4 acid NN pers:3|case:nom|num:sg|ntype:common 3 SUBJ 5 SBJ
5 may MD mood:ind|subcat:MODAL|tense:pres|clauseType:decl 0 ROOT 0 ROOT
6 be VB _ 7 PHI 5 VC
7 used VBN subcat:V-SUBJ-OBJ|vtype:main|passive:+ 5 XCOMP 6 VC
8 by IN _ 9 PHI 7 LGS
9 these DT deixis:proximal 10 SPECDET 10 NMOD
10 species NNS num:pl|pers:3|case:obl|common:count|ntype:common 7 OBL-AG 8 PMOD
11 . . _ 0 PUNC 5 P
to dependency format (Johansson andNugues 2007) and extendedwith XLE features, as
described previously. Parsing uses the arc-eager mode of MaltParser and an SVMwith a
polynomial kernel. When tested using 10-fold cross validation on the enhanced PTB, the
parser achieves a labeled accuracy score of 89.8, which is lower than the current state-
of-the-art for transition-based dependency parsers (to wit, the 91.8 score of Zhang and
Nivre 2011, although not directly comparable given that they test exclusively on WSJ
Section 23), but with the advantage of providing us with the deep linguistic information
from the XLE.
6.1.2 Rule Overview. Our scope resolution rules take as input a parsed sentence that has
been further tagged with speculation cues. We assume the default scope to start at the
cue word and span to the end of the sentence (modulo punctuation), and this scope also
provides the baseline when evaluating our rules.
In developing the rules, we made use of the information provided by the guidelines
for scope annotation in the BioScope corpus (Vincze et al 2008), combined with manual
inspection of the training data in order to further generalize over the phenomena
discussed by Vincze et al (2008) and work out interactions of constructions for various
types of cues. In the following, we discuss broad classes of rules, organized by categories
of speculation cues. An overview is also provided in Table 6, detailing the source of the
syntactic information used by the rule; MaltParser (M) or XLE (X). Note that, as there is
no explicit representation of phrase or clause boundaries in our dependency universe,
we assume a set of functions over dependency graphs, for example, finding the left- or
rightmost (direct) dependent of a given node, or recursively selecting left- or rightmost
descendants.
Coordination. The dependency analysis of coordination provided by our parser makes
the first conjunct the head of the coordination. For cues that are coordinating conjunc-
tions (PoS tag CC), such as or, we define the scope as spanning the whole coordinate
structure, that is, start scope is set to the leftmost dependent of the head of the coordina-
tion, and end scope is set to its rightmost dependent (conjunct). This analysis provides
us with coordinations at various syntactic levels, such as NP and N, AP and AdvP, or
VP as in Example (14):
(14) [...] the binding interfaces are more often {kept ?or? even reused} rather
than lost in the course of evolution.
384
Velldal et al Rules, Rankers, and the Role of Syntax
Table 6
Overview of dependency-based scope rules with information source (MaltParser or XLE),
organized by the triggering PoS of the cue.
PoS Description Source
cc Coordinations scope over their conjuncts M
in Prepositions scope over their argument with its descendants M
jjattr Attributive adjectives scope over their nominal head and its descendants M
jjpred Predicative adjectives scope over referential subjects and clausal arguments, M, X
if present
md Modals inherit subject-scope from their lexical verb and scope over their M, X
descendants
rb Adverbs scope over their heads with its descendants M
vbpass Passive verbs scope over referential subjects and the verbal descendants M, X
vbrais Raising verbs scope over referential subjects and the verbal descendants M, X
* For multiword cues, the head determines scope for all elements
* Back off from final punctuation and parentheses
Adjectives.We distinguish between adjectives (JJ) in attributive (nmod) function and adjec-
tives in predicative (prd) function. Attributive adjectives take scope over their (nominal)
head, with all its dependents, as in Example (15):
(15) The {?possible? selenocysteine residues} are shown in red, [...]
For adjectives in a predicative function the scope includes the subject argument of the
head verb (the copula), as well as a (possible) clausal argument, as in Example (16). The
scope does not, however, include expletive subjects, as in Example (17).
(16) Therefore, {the unknown amino acid, if it is encoded by a stop codon, is
?unlikely? to exist in the current databases of microbial genomes}.
(17) [...] it is quite {?likely? that there exists an extremely long sequence that is
entirely unique to U}.
Verbs. The scope of verbal cues is a bit more complex and depends on several factors.
In our rules, we distinguish passive usages from active usages, raising verbs from non-
raising verbs, and the presence or absence of a subject-control embedding context. The
scopes of both passive and raising verbs include the subject argument of their head
verb, as in Example (18), unless it is an expletive pronoun, as in Example (19).
(18) {Genomes of plants and vertebrates ?seem? to be free of any recognizable
Transib transposons} (Figure 1).
(19) It has been {?suggested? that unstructured regions of proteins are often
involved in binding interactions, particularly in the case of transient
interactions} 77.
In the case of subject control involving a speculation cue, specifically modals, sub-
ject arguments are included in scopes where the controller heads a passive construction
or a raising verb, as in our running Example (13).
385
Computational Linguistics Volume 38, Number 2
In general, the end scope of verbs should extend over the minimal clause that
contains the verb in question. In terms of dependency structures, we define the clause
boundary as comprising the chain of descendants of a verb which is not intervened by
a token with a higher attachment in the graph than the verb in question.
Prepositions and Adverbs. Cues that are tagged as prepositions (including some com-
plementizers) take scope over their argument, with all its descendants, Example (20).
Adverbs take scope over their head with all its (non-subject) syntactic descendants
Example (21).
(20) {?Whether? the codon aligned to the inframe stop codon is a nonsense
codon or not} was neglected [...]
(21) These effects are {?probably? mediated through the 1,25(OH)2D3
receptor}.
Multiword Cues. In the case of multiword cues, such as indicate that or either. . . or, we set
the scope of the unit as a whole to the maximal scope encompassing the scopes of both
units.
As an illustration of processing by the rules, consider our running Example (13),
with its syntactic analysis as shown in Table 5 and the dependency graph depicted
in Figure 2. This example invokes a variety of syntactic properties, including parts of
speech, argumenthood, voice, and so on. Initially, the scope of the speculation cue is
set to default scope. Then the subject control rule is applied, it checks the properties of
the verbal argument used, going through a chain of verbal dependents (VC) from the
modal verb may (indicated in red in Figure 2). Because it is marked as passive in
the LFG analysis (+pass), the start scope is set to include the subject of the cue word
(the leftmost descendant [NMOD] of its SBJ dependent, indicated in green in Figure 2).
6.1.3 Evaluating the Rules. Table 7 summarizes scope resolution performance (viewed as
a subtask in isolation) against both the CoNLL-2010 shared task training data (BSA and
BSP) and held-out evaluation data (BSE), using gold-standard cues. First of all, we note
that the default scope baseline, that is, unconditionally extending the scope of a cue to
the end of the sentence, yields much better results for the abstracts than the full papers.
The main reason is simply that the abstracts contain almost no cases of sentence final
bracketed expressions (e.g., citations and in-text references). Our scope rules improve
on the baseline by only 3.8 percentage points on the BSA data (F1 up from 69.84 to
Figure 2
Dependency representation for Example (13), indicating rule processing of the cue word may.
386
Velldal et al Rules, Rankers, and the Role of Syntax
Table 7
Resolving the scope of gold-standard speculation cues in the development and held-out data
using the dependency rules. For Default, the scope for each cue is always taken to span
rightwards to the end of the sentence.
Data Configuration F1
B
S
A Default 69.84
Dependency Rules 73.67
B
S
P Default 45.21
Dependency Rules 72.31
B
S
E Default 46.95
Dependency Rules 66.60
73.67). For BSP, however, we find that the rules improve on the baseline by as much as
27 points (up from 45.21 to 72.31). Similarly for the papers in the held-out BSE data, the
rules improve the F1 by 19.7 points (F1 up from 46.95 to 66.60).
Comparing to the result on the training data, we observe a substantial drop in
performance on the held-out data. There are several possible explanations for this effect.
First of all, there may well be some degree of overfitting of our rules to the training
data. The held-out data may contain speculation constructions that are not covered
by our current set of scope rules, or annotation of parallel constructions may in some
cases differ in subtle ways (see Section 6.1.5). The overfitting effects caused by the data
dependencies introduced by the various GENIA-based domain adaptation steps, as
described in Section 2.3, must also be taken into account.
6.1.4 PoS Tagging and Domain Variation. As mentioned in Section 6.1.1, an advantage of
stacking with a general-purpose LFG parser is that it can be expected to aid domain
portability. Nonetheless, substantial differences in domain and genre are bound to
negatively affect syntactic analysis (Gildea 2001), and our parser is trained on financial
news. MaltParser presupposes that inputs have been PoS tagged, however, leaving
room for variation in preprocessing. In this article we have aimed, on the one hand,
to make parser inputs conform as much as possible to the conventions established in its
PTB training data, while on the other hand taking advantage of specialized resources
for the biomedical domain.
To assess the impact of improved, domain-adapted inputs on our scope resolution
rules, we contrast two configurations: Running the parser in the exact same manner
as ?vrelid, Kuhn, and Spreyer (2009)?the first configuration uses TreeTagger (Schmid
1994) and its standard model for English (trained on the PTB) for preprocessing. In
the second configuration the parser input is provided by the refined GENIA-based
preprocessing described in Section 2.2. Evaluating the two modes of preprocessing on
the BSP subset of BioScope using gold-standard speculation cues, our scope resolution
rules achieve an F1 of 66.31 when using TreeTagger parser inputs, and 72.31 (see Table 7)
using our GENIA-based tagging and tokenization combination. These results underline
the importance of domain adaptation for accurate syntactic analysis.
6.1.5 Error Analysis. In Section 5.2.3 we discussed BioScope inter-annotator agreement
rates for the cue-level. Focusing only on the cases where the annotators agree with the
final gold-standard cues (as resolved by the chief annotator), Vincze et al (2008) report
387
Computational Linguistics Volume 38, Number 2
the scope-level F1 of the two annotators toward the gold standard to be 66.72 / 89.67 for
BSP. Comparing the decisions of the two annotators directly (i.e., treating one of the
annotations as gold-standard) yields an F1 of 62.50.
Using gold-standard cues, our scope resolution rules fail to exactly replicate the
target annotation in 185 (of 668) cases in the papers portion of the training material
(BSP), corresponding to an F1 of 72.31 as seen in Table 7. Two of the authors, who
are both trained linguists, performed a manual error analysis of these 185 cases. They
classify 156 (84%) as genuine system errors, 22 (12%) as likely3 annotation errors,
and the remaining 7 cases as involving controversial or seemingly arbitrary decisions
(?vrelid, Velldal, and Oepen 2010). Out of the 156 system errors, 85 (55%) were deemed
as resulting from missing or defective rules, and 71 system errors (45%) resulted from
parse errors. The latter were annotated as parse errors even in cases where there was
also a rule error.
The two most frequent classes of system errors pertain to (a) the recognition of
phrase and clause boundaries and (b) not dealing successfully with relatively superficial
properties of the text. Examples (22) and (23) illustrate the first class of errors, where
in addition to the gold-standard annotation we use vertical bars (?|?) to indicate scope
predictions of our system.
(22) [. . . ] {the reverse complement |mR of m will be ?considered? to . . . ]|}
(23) This |{?might? affect the results} if there is a systematic bias on the
composition of a protein interaction set|.
In our syntax-driven approach to scope resolution, system errors will almost always
correspond to a failure in determining constituent boundaries, in a very general sense.
In Example (22), for instance, the parser has failed to correctly locate the head of the
subject. Example (23), however, is specifically indicative of a key challenge in this task,
where adverbials of condition, reason, or contrast frequently attach within the depen-
dency domain of a speculation cue, yet are rarely included in the scope annotation.
For these system errors, the syntactic analysis may well be correct, although additional
information is required to resolve the scope.
Example (24) demonstrates our second frequent class of system errors. One in six
items in the BSP training data contains a sentence-final parenthesized element or trailing
number (e.g., Examples [18] or [19]); most of these are bibliographic or other in-text
references, which are never included in scope annotation. Hence, our system includes a
rule to ?back out? from trailing parentheticals; in cases such as Example (24), however,
syntax does not make explicit the contrast between an in-text reference versus another
type of parenthetical.
(24) More specifically, {|the bristle and leg phenotypes are ?likely? to result
from reduced signaling by Dl| (and not by Ser)}.
3 In some cases, there is no doubt that annotation is erroneous, that is, in violation of the available
annotation guidelines (Vincze et al 2008) or in conflict with otherwise unambiguous patterns. In other
cases, however, judgments are necessarily based on our own generalizations (e.g., assumptions about
syntactic analyses implicit in the BioScope annotations). Furthermore, selecting items for manual analysis
that do not align with the predictions made by our scope resolution rules is likely to bias our sample, such
that our estimated proportion of 12% annotation errors cannot be used to project an overall error rate.
388
Velldal et al Rules, Rankers, and the Role of Syntax
Moving on to apparent annotation errors, the rules for inclusion (or not) of the
subject in the scope of verbal speculation cues and decisions on boundaries (or internal
structure) of nominals seem problematic?as illustrated in Examples (25) and (26).4
(25) [. . . ] and |this is also {?thought? to be true for the full protein interaction
networks we are modeling}|.
(26) [. . . ] |redefinition of {one of them is ?feasible?}|.
Finally, the difficult corner cases invoke non-constituent coordination, ellipsis,
or NP-initial focus adverbs?and of course interactions of the phenomena discussed
herein. Without making the syntactic structures assumed explicit, it is often very diffi-
cult to judge such items.
6.2 A Data-Driven Approach Using an SVM Constituent Ranker
The error analysis indicated that it is often difficult to use dependency paths to define
phenomena that actually correspond to syntactic constituents. Furthermore, we felt that
the factors governing scope resolution would be better expressed in terms of soft con-
straints instead of absolute rules, thus enabling the scope resolver to consider a range
of relevant (potentially competing) contextual properties. In this section we describe
experiments with a novel approach to determining the in-sentence scope of speculation
that, rather than usingmanually defined heuristics operating on dependency structures,
instead uses a data-driven approach, ranking candidate scopes on the basis of constituent
trees. More precisely, our parse trees are licensed by the LinGO English Resource Gram-
mar (ERG; Flickinger [2002]), a general-purpose, wide-coverage grammar couched in
the framework of an HPSG (Pollard and Sag 1987, 1994). The approach rests on two
main assumptions: Firstly, that the annotated scope of a speculation cue corresponds to
a syntactic constituent and secondly, that we can automatically learn a ranking function
that selects the correct constituent.
Our ranking approach to scope resolution is abstractly related to statistical parse
selection, and in particular work on discriminative parse selection for unification based
grammars, such as those by Johnson et al (1999), Riezler et al (2002), Malouf and
van Noord (2004), and Toutanova et al (2005). The overall goal is to learn a function
for ranking syntactic structures, based on training data that annotates which tree(s) are
correct and incorrect for each sentence. In our case, however, rather than discriminating
between complete analyses for a given sentence, we want to learn a ranking function
over candidate subtrees (i.e., constituents) within a parse (or possibly evenwithin several
parses). Figure 3 presents an example derivation tree that represents a complete HPSG
analysis. Starting from the cue and working through the tree bottom?up, there are three
candidate constituents to determine scope (marked in bold), each projecting onto a
substring of the full utterance, and each including at least the cue. Note that in the case
of multiword cues the intersection of each word?s candidates is selected, ensuring that
all cues appear within the scope projected by the candidate constituents.
The training data is then defined as follows. Given a parsed BioScope sentence,
the subtree that corresponds to the annotated scope for a given speculation cue will
4 As in the presentation of system errors, we include scope predictions of our own rules here too, which
we believe to be correct in these cases. Also in this class of errors, we find the occasional ?uninteresting?
mismatch, for example related to punctuation marks and inconsistencies around parentheses.
389
Computational Linguistics Volume 38, Number 2
Figure 3
An example derivation tree. Internal nodes are labeled with ERG rule identifiers; common HPSG
constructions near the top (e.g., subject-head, head-complement, adjunct-head), and lexical rules
(e.g., passivization of verbs or plural formation of nouns) closer to the leaves. The preterminals
are so-called LE types, corresponding to fine-grained parts of speech and reflecting close to a
thousand lexical distinctions.
be labeled as correct. Any other remaining constituents that also span the cue are
labeled as incorrect. We then attempt to learn a linear SVM-based scoring function that
reflects these preferences, using the implementation of ordinal ranking in the SVMlight
toolkit (Joachims 2002). Our definition of the training data, however, glosses over two
important details.
Firstly, the grammar will usually license not just one, but thousands or even hun-
dreds of thousands of different parses for a given sentence which are ranked by an
underlying parse selection model. Some parses may not necessarily contain a subtree
that aligns with the annotated scope.We therefore experiment with defining the training
data relative to n-best lists of available parses. Secondly, the rate of alignment between
annotated scopes and constituents of parsing results indicates the upper-bound per-
formance: For inputs where no constituents align with the correct scope substring,
a correct prediction will not be possible. Searching the n-best parses for alignments
enables additional instances of scope to be presented to the learner, however.
In the following, Section 6.2.1 summarizes the general parsing setup for the ERG, as
well as our rationale for the use of HPSG. Section 6.2.2 provides an empirical assessment
of the degree to which ERG analyses can be aligned with speculation scopes in BioScope
and reviews some frequent sources of alignment failures. After describing our feature
types for representing candidate constituents in Section 6.2.3, Section 6.2.4 details the
tuning of feature configurations and other ranker parameters. Finally, Section 6.2.5
provides an empirical assessment of stand-alone ranker performance, before we discuss
the integration of the dependency rules with the ranking approach in Section 6.3.
6.2.1 Basic Set-up: Parsing Biomedical Text Using the ERG. At some level of abstraction,
the approach to grammatical analysis embodied in the ERG is quite similar to the LFG
parser that was ?stacked? with our data-driven dependency parser in Section 6.1.1?
both are commonly considered comparatively ?deep? (and thus costly) approaches to
syntactic analysis. Judging from the BioScope annotation guidelines, subtle grammat-
ical distinctions are at play when determining scopes, for example, different types of
control verbs, expletives, or passivization (Vincze et al 2008). In contrast to the LFG
framework (with its distinction between so-called constituent and functional struc-
tures), the analyses provided by the ERG offer the convenience of a single syntactic
390
Velldal et al Rules, Rankers, and the Role of Syntax
representation?HPSG derivation trees, as depicted in Figure 3?where all contextual
information that we expect to be relevant for scope resolution is readily accessible.
For parsing biomedical text using the ERG, we build on the same preprocessing
pipeline as described in Section 2. A lattice of tokens annotated with parts of speech
and named entity hypotheses contributed by the GENIA tagger is input to the PET
HPSG parser (Callmeier 2002), a unification-based chart parser that first constructs a
packed forest of candidate analyses and then applies a discriminative parse ranking
model to selectively enumerate an n-best list of top-ranked candidates (Zhang, Oepen,
and Carroll 2007). To improve parse selection for this kind of data, we re-trained the
discriminative model following the approach of MacKinlay et al (2011), combining
gold-standard out-of-domain data from existing ERG treebanks with a fully automated
procedure seeking to take advantage of syntactic annotations in the GENIA Treebank.
Although we have yet to pursue domain adaptation in earnest and have not systemat-
ically optimized the parse selection component for biomedical text, model re-training
contributed about a one-point F1 improvement in stand-alone ranker performance over
the parsed subset of BSP (compare to Table 8).
As the ERG has not previously been adapted to the biomedical domain, unknown
word handling in the parser plays an important role. Here we build on a set of
somewhat underspecified ?generic? lexical entries for common open-class categories
provided by the ERG (thus complementing the 35,000-entry lexicon that comes with the
grammar), which are activated on the basis of PoS and NE annotation from preprocess-
ing. Other than these, there are no robustness measures in the parser, such that syntactic
analysis will fail in a number of cases, to wit, when the ERG is unable to derive a
complete, well-formed syntactic structure for the full input string. In this configuration,
the parser returns at least one derivation for 91.2% of all utterances in BSA, and 85.6%
and 81.4% for BSP and BSE, respectively.
6.2.2 Alignment of Constituents and Scopes. The constituent ranking approach makes ex-
plicit an assumption that is also present at the core of our dependency-based heuristics
(viz., the expectation that scope boundaries align with the boundaries of syntactically
meaningful units). This assumption is motivated by general BioScope annotation prin-
ciples, as Vincze et al (2008) suggest that the ?scope of a keyword can be determined
on the basis of syntax.? To determine the degree to which ERG analyses conform to this
expectation, we computed the ratio of alignment between scopes and constituents (over
parsed sentences) in BioScope, considering various sizes of n-best lists of parses. To
improve alignment we also apply a small number of slackening heuristics. These
rules allow (a) minor adjustments of scope boundaries around punctuation marks
Table 8
Ranker optimization on BSP: Showing ranker performance for various feature type
combinations compared with a random-choice baseline, only considering instances
where the gold-standard scope aligns to a constituent within the 1-best parse.
Features F1
Baseline 26.76
Path 78.10
Path+Surface 79.93
Path+Linguistic 83.72
Path+Surface+Linguistic 85.30
391
Computational Linguistics Volume 38, Number 2
(specifically, utterance-final punctuation is never included in BioScope annotations, yet
the ERG analyzes most punctuation marks as pseudo-affixes on lexical tokens; see Fig-
ure 3). Furthermore, the slackening rules (b) reduce the scope of a constituent to the right
when it includes a citation (see the discussion of parentheticals in Section 6.1.5); (c) re-
duce the scope to the left when the left-most terminal is an adverb and is not the cue; and
(d) ensure that the scope starts with the cue when the cue is a noun. Collectively, these
rules improve alignment (over parsed sentences) in BSP from 74.10% to 80.54%, when
only considering the syntactic analysis ranked most probable by the parse selection
model. Figure 4 further depicts the degree of alignment between speculation scopes and
constituents in the n-best derivations produced by the parser, again after application of
the slackening rules. Alignment when inspecting only the top-ranked parse is 84.37%
for BSA and 80.54% for BSP. Including the top 50-best derivations improves alignment
to 92.21% and 88.93%, respectively. Taken together with an observed parser coverage of
85.6% for BSP, these results mean that for only about 76% of all utterances in BSP can
the ranker potentially identify a constituent matching the gold-standard scope.
To shed some light on the cases where we fail to find an alignment, we manually
inspected all utterances in the BSP segment for which there were (a) syntactic analyses
available from the ERG and (b) no candidate constituents in any of the top-fifty parses
that mapped onto the gold-standard scope (after the application of the slackening rules).
The most interesting cases from this non-alignment analysis are ones judged as ?non-
syntactic? (25% of the total mismatches), which we interpret as violating the assumption
of the annotation guidelines under any possible interpretation of syntactic structure.
Following are select examples in this category:
(27) This allows us to {?address a number of questions?: what proportion of
each organism?s protein interaction network [. . . ] can be attributed to a
known domain-domain interaction}?
(28) As {?suggested? in 18, by making more such data sets available, it will be
possible to [. . . ] determine the most likely human interactions}.
Figure 4
The effect of incrementally including additional derivations from the n-best list when searching
for an alignment between a speculation scope and a constituent. Plots are shown for the BSA and
BSP subsets of the training data.
392
Velldal et al Rules, Rankers, and the Role of Syntax
(29) The {lack of specificity ?might? be attributed to a number of reasons, such
as the absence of other MSL components, the presence of other RNAs
interacting with MOF}, or worse [. . . ].
(30) [. . . ] thereby making {the use of this objective function ? and exploration
of other complex objective functions ? ?possible?}.
Example (27) is representative of a handful of similar cases, where complete sen-
tences are (implicitly or overtly) conjoined, yet the scope annotation encompasses only
part of one of the sentences. Example (28) is in a similar spirit, only in this case a
topicalized prepositional phrase (and hence an integral constituent) is only partially
included in the gold-standard scope. Although our slackening heuristics address a
number of cases of partial noun phrases (with a left scope boundary right before the
head noun or a pre-head attributive adjective), another handful of non-syntactic scopes
are of the type exemplified by Example (29), a class observed earlier already in the error
analysis of our dependency-based scope resolution rules (see Section 6.1.5). Finally,
Example (30) demonstrates one of many linguistically subtle corner cases: The causative
make in standard analyses of the resultative construction takes two arguments, namely,
an NP (the use of this objective function. . . ) and a predicative phrase (possible).
Alongside cases like these, our analysis considered 16% of mismatches owed to
divergent syntactic theories (i.e., structures that in principle can be analyzed in amanner
compatible with the BioScope gold-standard annotations, yet do not form matching
constituents in the ERG analyses). The by far largest class of mismatches was attributed
to parse ranking deficiencies: In close to 40% of cases, the ERG is capable of deriving
a constituent structure compatible with the scope annotations, but no such analysis
was available within the top 50 parses. Somewhat reassuringly, less than 6% of all
mismatches were classified as BioScope annotation errors, whereas a majority of re-
maining mismatches are owed to the recurring issue of parentheticals and bibliographic
references (see examples in Section 6.1.5).
6.2.3 Features of Candidate Scopes. We use three families of features to describe candi-
date constituents. Given our working hypothesis that scopes are aligned with syn-
tactic constituents, the most natural features to use are the location of constituents
within trees. We define these in terms of the paths from speculation cues to can-
didate constituents. For example, the correct candidate in Figure 3 has the feature
v_vp_mdl-p_le\hd-cmp_u_c\sb-hd_mc_c. We include both lexicalized and unlexical-
ized versions of this feature. As traversal from the cue to the candidate can involve
many nodes, we also include a more general version recording only the cue and the
root of the candidate constituent (rather than the full path including all intermediate
nodes). In a similar spirit we also generate bigram features for each path node and its
parent.
In addition to the given path features, we also exploit features describing the
surface properties of scope candidates. These include the enumeration of bigrams of
the preterminal lexical types, the cue position within the candidate (in tertile bins
relative to the candidate length), and the candidate size (in quartile bins relative to the
sentence length). Because punctuation may also be informative for scope resolution, we
also record whether punctuation was present at the end of the terminal preceding the
candidate or at the end of its right-most terminal.
The third family of features is concerned with specific linguistic phenomena de-
scribed in the BioScope annotation guidelines (Vincze et al 2008) or observed when
393
Computational Linguistics Volume 38, Number 2
developing the rules in Section 6.1. These include detection of passivization, subject
control verbs occurring with passivized verbs, subject raising verbs, and predicative
adjectives. Furthermore, these features are only activated when the subject of the con-
struction is not an expletive pronoun, and they are represented by appending the type
of phenomenon observed to the path features described here.
6.2.4 Ranker Optimization. We conducted several experiments designed to find an
optimal configuration of features. Table 8 lists the results of combinations of the fea-
ture families on the BSP data set when using gold-standard cues, reporting 10-fold
cross-validated F1 scores with respect to only the instances where the gold-standard
speculation scope aligns with constituents (i.e., the ?ideal circumstances? for the
ranker). The table also lists results for a random-choice baseline, calculated as the
mean ambiguity of each instance (i.e., the averaged reciprocal of the number of can-
didates). The feature optimization results indicate that each feature family is infor-
mative, and that the best result can be obtained by using all three in conjunction.
The comparatively largest improvement in ranker performance is obtained from the
?rule-like? linguistic feature family, which is noteworthy in two respects: First, our
current system includes only four such features, and second, these features parallel
some of the dependency-based rules of Section 6.1.2?suggesting that subtle syntactic
configurations are an important component also in our data-driven approach to scope
resolution.
As discussed in Section 6.2.2 and depicted in Figure 4, searching the best-ranked
parses can greatly increase the number of aligned constituents and thus improve the
upper-bound potential of the ranker. We therefore experimented with training using
the first aligned constituent in n-best derivations. At the same time we varied the
m-best derivations used during testing, using features from all m derivations. We found
that performance did not vary greatly, but that the best result was achieved when
n = 1 and m = 3 (note, however, that such optimization over n-best lists of ERG parses
will play a much greater role in the hybrid approach to scope resolution developed
in Section 6.3). As explained in Section 5.2.1, all experiments use the SVMlight de-
fault value for the regularization parameter, determined analytically from the training
data.
A cursory error analysis conducted over aligned items in BSP indicated similar
errors to those discussed in connection with the dependency rules (see Section 6.1.5).
There are a number of instances where the predicted scope is correct according to the
BioScope annotation guidelines, but the annotated scope is incorrect. We also note some
instances where the rule-like linguistic features are activated on the correct constituent,
but the ranker nevertheless selects a different candidate. In a strictly rule-based system,
these features would act as hard constraints and yield superior results in these cases.
Therefore, these instances seem a prime source of inspiration for further improvements
to the ranker in future work.
6.2.5 Evaluating the Ranker. Table 9 summarizes the performance of the constituent
ranker (coupledwith the default scope baseline in the case of unparsed items) compared
with the dependency rules, resolving the scope of both gold-standard and predicted
speculation cues. We note that the constituent ranker performs slightly superior to the
dependency rules on BSA but inferior (though well above the default scope baseline)
on BSP and BSE. Applying the sign-test (in the manner described in Section 3.2) to the
scope-level performance of the ranker and the rules on the held-out BSE data (using
gold-standard cues), the differences are found to be statistically significant.
394
Velldal et al Rules, Rankers, and the Role of Syntax
Table 9
Resolving the scope of speculation cues using the dependency rules, the constituent ranker,
and their combination. Whereas table (a) shows results for gold-standard cues, table (b) shows
end-to-end results for the cues predicted by the classifier of Section 5.2. Results are shown both
for the BioScope development data (for which both the scope ranker and the cue classifier is
applied using 10-fold cross-validation) and the CoNLL-2010 Shared Task evaluation data.
Data System F1
B
S
A
Rules 73.67
Ranker 75.48
Combined 79.56
B
S
P
Rules 72.31
Ranker 66.17
Combined 75.15
B
S
A
P Rules 73.40
Ranker 73.61
Combined 78.69
B
S
E
Rules 66.60
Ranker 58.37
Combined 69.60
(a) Resolving Gold-Standard Cues
Data System Prec Rec F1
B
S
A
Rules 72.47 66.42 69.31
Ranker 74.27 68.07 71.04
Combined 77.80 71.31 74.41
B
S
P
Rules 69.87 62.13 65.77
Ranker 62.63 55.69 58.95
Combined 72.05 64.07 67.83
B
S
A
P Rules 71.97 65.56 68.61
Ranker 71.99 65.59 68.64
Combined 76.67 69.85 73.11
B
S
E
Rules 58.95 54.21 56.48
Ranker 51.68 47.53 49.52
Combined 62.00 57.02 59.41
(b) Resolving Predicted Cues
Again we also observe a drop in performance for the results on the held-out data
comparedwith the development data.We attribute this drop partly to overfitting caused
by using GENIA abstracts to adapt the parse ranker to the biomedical domain (see
Section 2.3), but primarily to reduced parser coverage and constituent alignment in the
latter data sets. Improving these aspects should result in substantive gains in ranker
performance. Finally, note that the performance of the default baseline (which is much
better for the abstracts than the full papers of BSP and BSE) also carries over to ranker
performance for the cases where we do not have a parse.
6.3 Combining the Constituent Ranker and the Dependency Rules
Although both the constituent ranker and dependency rules perform well in isolation,
they do not necessarily perform well on the same test items. Consequently, we inves-
tigated the effects of combining their predictions. When ERG parses are available for
a given sentence, the dependency rules may be combined with the information used
by the constituent ranker. We implement this coupling by adding features that record
whether the (slackened) span of a candidate constituent matches the span of the scope
predicted by the rules (either exactly or just at one of its boundaries). When an ERG
parse is not available we simply revert to the prediction of the dependency rules.
Adding the rule prediction features may influence the effectiveness of considering
multiple parses, by compensating for the extra ambiguity. We therefore repeated our
examination of the effects of using the best-ranked parses for training and testing the
ranker. Figure 5 plots the effect on F1 for parsed sentences in BSP when including
constituents from the n-best derivations in training, and from the m-best derivations in
testing.We see that, when activating the dependency prediction features, the constituent
ranker performs best for n = 5 and m = 20.
Looking at the performance summaries of Table 9, we see that the combined ap-
proach consistently outperforms both the dependency rules and the constituent ranker
395
Computational Linguistics Volume 38, Number 2
Figure 5
Cross-validated F1 scores of the ranker combined with the dependency rules over gold cues
for parsed sentences from BSP, varying the maximum number of parse results employed for
training and testing.
in isolation, and the improvements are deemed significant with respect to both of them
(comparing results for BSE using gold-standard cues).
Comparing the combined approach to the plain ranker runs, there are two sources
for the improvements: the addition of the rule prediction features and the fact that we
fall back on using the rule predictions directly (rather than the default scope) when we
do not have an available constituent tree. To isolate the contribution of these factors,
we applied the ranker without the rule-prediction features (as in the initial ranker
set-up), but still using the rules as our fall-back strategy (as in the combined set-up).
Testing on BSP using gold-standard cues this gives an F1 of 69.61, meaning that the
8.98-percentage-point improvement of the combined model over the plain ranker owes
3.44 points to the rule-based fall-back strategy and 5.54 to the new rule-based features.
As discussed in Section 6.2.2, an important premise of the success of our ranking
approach is that scopes align with constituents. Indeed, we find that the performance
of both the ranker in isolation and the combined approach is superior on BSA, which is
the data set that exhibits the greatest proportion of aligned instances. We can therefore
expect that any improvements in our alignment procedure, as well as in the domain-
adapted ERG parse selection model, will also carry through to improve the overall
performance of our subtree ranking.
As a final evaluation of speculation resolution, Table 10 compares the end-to-end
performance of our combined approach with the best end-to-end performer in the
CoNLL-2010 Shared Task. In terms of both precision and recall, our cue classifier using
the combination of constituent ranking and dependency rules for scope resolution
achieves superior performance on BSE comparedwith the system ofMorante, van Asch,
and Daelemans (2010), improving on the overall F1 by more than 2 percentage points.
Whereas the token-level differences for cue classification are found to be significant, the
end-to-end scope-level differences are not (p = 0.39).
7. Porting the Speculation System to Negation
Dealing with negation in natural language has been a long-standing topic and there has
been work attempting to resolve the scope of negation in particular within the area of
sentiment analysis (Moilanen and Pulman 2007), where treatment of negation clearly
constitutes an important subtask and has been shown to provide improved sentiment
396
Velldal et al Rules, Rankers, and the Role of Syntax
Table 10
Final end-to-end results for scope resolution: Held-out testing on BSE, using the cue classifier
described in Section 5.2 while combining the dependency rules and the constituent ranker for
scope resolution. The results are compared to the system with the best end-to-end performance
in the CoNLL-2010 Shared Task (Morante, van Asch, and Daelemans 2010).
Cue Level Scope Level
System Configuration Prec Rec F1 Prec Rec F1
Cue classifier + Scope Rules & Ranking 84.79 77.17 80.80 62.00 57.02 59.41
Morante et al 2010 78.75 74.69 76.67 59.62 55.18 57.32
analysis (Councill, McDonald, and Velikovich 2010). The BioScope corpus (Vincze et al
2008), being annotated with negation as well as speculation, has triggered work on
negation detection in the biomedical domain as well. In this setting, there are a few
previous studies where the same system architecture has been successfully applied for
both speculation and negation. For example, whereas Morante and Daelemans (2009a)
try to resolve the scope of speculation using a system initially developed for negation
(Morante, Liekens, and Daelemans 2008), Zhu et al (2010) develop a system targeting
both tasks. In this section we investigate to what degree our speculation system can be
ported to also deal with negation, hoping that the good results obtained for speculation
will carry over to the negation task at a minimal cost in terms of adaption and modifica-
tion. We start by describing our experiments with porting the cue classifier to negation
in Section 7.1, and then present our modified set of dependency rules for resolving the
scope of the negation cues in Section 7.2. Section 7.3 presents the adaptation of the
constituent ranker, as well as the final end-to-end results when combining the ranker
and the rules, paralleling what we did for speculation. The relation to other relevant
work is discussed as we go along.
Some summarizing statistics for the negation annotations in BioScope were given
in Table 1. Note, however, that the additional evaluation data that we used for held-out
testing of the speculation system, does not contain negation annotations. For this reason,
and in order to be able to compare our results to those obtained in previous studies, we
here follow the partitioning established by Morante and Daelemans (2009b), reporting
10-fold cross-validation (for the cue classifier and the subtree ranker) on the abstracts
(BSA) and using the full papers (BSP) for held-out and cross text-type testing. Note
that for the development results using cross-validation, we partition the data on the
sentence-level, just as in Morante and Daelemans (2009b).
7.1 Identifying Negation Cues
Several previous approaches to detecting negation cues have been based on pre-
compiled lexicons, either alone (Councill, McDonald, and Velikovich 2010) or in combi-
nation with a learner (Morante and Daelemans 2009b). For the purpose of the current
article we wanted to investigate whether the ?filtered classification? approach that we
applied for detecting speculation cues would directly carry over to negation. Drawing
heavily on much of the discussion previously given for the speculation cue classifiers
in Section 5, the small modifications made to implement a classifier for negation cues
are described in Section 7.1.1. We then provide some discussion of the results in Sec-
tion 7.1.2, including comparison to previous work on negation cue detection byMorante
and Daelemans (2009b) and Zhu et al (2010) in Section 7.1.3.
397
Computational Linguistics Volume 38, Number 2
7.1.1 Classifier Description. Apart from re-tuning the feature configuration, the only
modifications that wemade with respect to the speculation classifier regard the rules for
multiword cues (as described for speculation in Section 5.1) and the corresponding stop-
list (Section 5.2). The overall approach, however, is the same:We train and apply a linear
SVM classifier that only considers words whose lemma has been observed as a negation
cue in the training data. Note that roughly 82% of the negation tokens are ambiguous in
the training data, in the sense that they have both cue and non-cue occurrences. Based
on the most frequently occurring MWC patterns observed in the abstracts we defined
post-processing rules to cover the cases shown in Table 11. Furthermore, and again
based on the MWCs, we compiled a small stop-list so that the classifier ignores certain
?spurious? tokens (namely, can, could, notable, of, than, the, with, and ?(?). Although this
of course means that the classifier will never label any such word as a cue, they will
typically be captured by the MWC rules instead.
When re-tuning the feature configuration based on the n-gram templates previously
described in Section 5.2.1, we find that the best performer for negation is the combina-
tion that records lemmas two positions to the left and the right of the target word, and
surface forms one position to the right.
7.1.2 Development Results. The performance of this model, evaluated by 10-fold cross-
validation on the BioScope abstracts, is shown in Table 12. Just as for speculation, we
also contrast the performance with a simple WbW majority usage baseline, classifying
each and every word according to its most frequent usage (cue vs. non-cue) in the
training data. Although this baseline proved to be surprisingly strong for speculation, it
is even stronger for negation: Evaluated at the token-level (though after the application
of the MWC rules) the baseline achieves an F1 of 93.60. Applying the filtering model
further improves this score to 96.00. The differences are found to be statistically signifi-
cant (according to the testing scheme described in Section 3.1), and the filtering classifier
also improves greatly with respect to the sentence-, and cue-level evaluations as well,
in particular with respect to the precision.
Recall that, when looking at the distribution of error types for the token-level
mistakes made by the speculation classifier (see Section 5.2.3), we found that almost 75%
were false negatives. The distribution of error types for the negation cue classifier is very
different: Almost 85% of the errors are false positives. After inspecting the actual cues
involved, we find the same situation as reported by Morante and Daelemans (2009b),
namely, that a very high number of the errors concern cases where not is labeled as a cue
by the classifier but not in the annotations. The same is true for the cue word absence,
and many of these cases appear to be annotation errors.
The class balance among tokens in the BioScope data is extremely skewed, with the
positive examples of negation constituting only 0.5% of the total number of examples.
Table 11
Patterns covered by our post-processing rules for multiword negation cues.
rather than
{can|could} not
no longer
instead of
with the * exception of
neither * nor
{no(t?)|neither} * nor
398
Velldal et al Rules, Rankers, and the Role of Syntax
Table 12
Results for negation cue detection, including the systems of Morante et al (2009b) and Zhu et al
(2010). Whereas the scores for BSA are obtained by 10-fold cross validation, the scores on BSP
and BSR represent held-out testing using a model trained on all the abstracts. The latter scores
thereby serves as a test of generalization performance across different text types within the
same domain.
Sentence Level Token Level Cue Level
Data Model Prec Rec F1 Prec Rec F1 Prec Rec F1
B
S
A
(1
0
-F
o
ld
) Baseline 90.34 98.81 94.37 89.28 98.40 93.60 88.92 97.78 93.14
Filtering 94.19 98.87 96.45 93.46 98.73 96.00 93.19 98.12 95.59
Morante n/a n/a n/a 84.72 98.75 91.20 94.15 90.67 92.38
Zhu n/a n/a n/a 94.35 94.99 94.67 n/a n/a n/a
B
S
P
(H
e
ld
-o
u
t) Baseline 79.48 99.41 88.34 75.96 99.00 85.96 74.55 98.41 84.84
Filtering 86.75 98.53 92.27 85.22 98.25 91.27 84.06 97.62 90.33
Morante n/a n/a n/a 87.18 95.72 91.25 85.55 78.31 81.77
Zhu n/a n/a n/a 87.47 90.48 88.95 n/a n/a n/a
B
S
R
(H
e
ld
-o
u
t) Baseline 96.64 96.42 96.53 96.12 96.01 96.06 95.87 95.98 95.93
Filtering 96.97 96.30 96.64 96.44 95.90 96.17 96.20 95.87 96.03
Morante n/a n/a n/a 97.33 98.09 97.71 96.38 91.62 93.94
Zhu n/a n/a n/a 88.54 86.81 87.67 n/a n/a n/a
In terms of the tokens actually considered by our filtering model, however, the numbers
look much healthier, with the negative examples actually being slightly outweighed
by the positives (just above 50%). Moreover, the average number of distinct n-gram
features instantiated across the 10-folds is approximately 17,500. The small size of the
feature set is of course due to the small number of training examples considered by the
learner: Whereas a WbW approach (like the majority usage baseline) would consider
every token in training data (just below 300,000 in each fold), this number is reduced
by almost 99% for the filtered disambiguation model. In effect, we can conclude that the
proposed approachmanages to combine very good results with very low computational
cost.
Figure 6 shows learning curves for both the word-by-word baseline and the fil-
tering model, plotting token-level F1 against percentages of data included in training.
Compared to the learning curves previously shown for speculation detection (Figure 1),
the curve for the filtering model seems to be somewhat flatter for negation. Looking at
the curve for the WbW unigram baseline, it again seems unable to benefit much from
any additional data after the first few increments.
7.1.3 Comparison to Related Work. To the best of our knowledge, the systems currently
achieving state-of-the-art results for detecting negation cues are those described by
Morante and Daelemans (2009b), Zhu et al (2010), and Councill, McDonald, and
Velikovich (2010). Although the latter work does not offer separate evaluation of the
cue detection scheme in isolation, Morante and Daelemans (2009b) and Zhu et al (2010)
provide cue evaluation for the data splits listed in Table 12; 10-fold cross-validation
experiments (with sentence-level partitioning) on the BioScope abstracts, and held-out
testing on the full papers and the clinical reports (with a model trained on the abstracts).
399
Computational Linguistics Volume 38, Number 2
Figure 6
Learning curves for both baseline and the filtered ?disambiguation? model showing the effect on
token-level negation cue F1 when including larger percentages (shown on a logarithmic scale)
of the training data across the 10-fold cycles on BSA.
The results5 reported by Morante and Daelemans (2009b) and Zhu et al (2010) are
token-level precision, recall, and F1. Having obtained the system output of Morante
and Daelemans (2009b), however, we also computed cue-level scores for their system.
Morante and Daelemans (2009b) identify cues using a small list of unambiguous
cue words compiled from the abstracts in combination with applying a decision tree
classifier to the remaining words. Their features record information about neighboring
word forms, PoS, and chunk information from GENIA. Zhu et al (2010) train an SVM
to classify tokens according to a BIO-scheme using surface-oriented n-gram features in
addition to various syntactic features extracted using the Berkley parser (Petrov and
Klein 2007) trained on the GENIA treebank. Looking at the results in Table 12, we see
that the performance of our cue classifier compares favorably with the systems of both
Morante and Daelemans (2009b) and Zhu et al (2010), achieving a higher cue-level F1
across all data sets (with differences in classifier decisions with respect to Morante and
Daelemans [2009b] being statistically significant for all of them).
For the 10-fold run, the biggest difference concerns token-level precision, where
both the system of Zhu et al (2010) and our own achieves a substantially higher score
than that of Morante and Daelemans (2009b). Turning to the cross-text experiment,
however, the precision of our system and that of Zhu et al (2010) suffers a large
drop, whereas the system of Morante and Daelemans (2009b) actually obtains a higher
precision than for the 10-fold run. These effects are reversed for recall, however, where
our system still maintains the higher score, also resulting in a higher F1. Looking at
the cue-level scores, we find that the precision of our system and that of Morante and
Daelemans (2009b) drops by an equal amount for the BSP cross-text testing. In terms of
recall, however, the cue-level scores of Morante and Daelemans (2009b) suffers a much
larger drop than that of our filtered classifier.
5 As the results reported by Morante and Daelemans (2009b) were inaccurate, we instead refer to values
obtained from personal communication with the authors.
400
Velldal et al Rules, Rankers, and the Role of Syntax
The drop in performance when going from cross-validation to held-out testing
can largely be attributed to the same factors discussed in relation to speculation
cues in Section 5.3 (e.g., GENIA-based pre-processing, sentence-level partitioning in
cross-validation, and unobserved MWCs). In addition, looking at the BioScope inter-
annotator agreement rates for negation cues it is not surprising that we should ob-
serve a drop in results going from BSA to BSP: Measured as the F1 of one of the
annotators with respect to the other, it is reported as 91.46 for BSA, compared with
79.42 for BSP (Vincze et al 2008). Turning to the F1-scores of each annotator with
respect to the final gold standard, the numbers are 91.71/98.05 for BSA and 86.77/91.71
for BSP.
The agreement rates for the clinical reports, on the other hand, are much closer to
those of the abstracts (Vincze et al 2008), and the held-out scores we observe on this data
set are generally also much better, not the least for the simple majority usage baseline.
In general the baseline again proves to be surprisingly competitive, most notably with
respect to recall where it actually outperforms all the other systems for both the cross-
text experiments. (Recall that the baseline scores also reflect the application of the MWC
rules, though.)
7.2 Adapting the Dependency Rules for Resolving Negation Scope
There have been several previous studies on resolving the scope of negation based on
the BioScope corpus. For example, Morante and Daelemans (2009b) present a meta-
learning approach that combines the output from three learners?a memory-based
model, an SVM classifier, and a CRF classifier?using lexical features, such as PoS and
chunk tags. Councill, McDonald, and Velikovich (2010) use a CRF learner with features
based on dependency parsing (e.g., detailing the PoS of the head and the dependency
path to the negation cue).
The annotation of speculation and negation in BioScope was performed using a
common set of principles. It therefore seems reasonable to assume our dependency-
based scope resolution rules for speculation should be general enough to allow porting
to negation with fairly limited efforts. On the other hand, negation is expressed linguis-
tically using quite different syntactic structures from speculation, so it is clear that some
modifications will be necessary as well.
As we recall, the dependency rules for speculation scope are triggered by the PoS
of the cue. Several of the same parts-of-speech (verbs, adverbs) also express negation.
As an initial experiment, therefore, we simply applied the speculation rules to negation
unmodified. As before, taking default scope to start at the cue word and spanning to
the end of the sentence provides us with a baseline system. We find that applying
our speculation scope rules directly to the task of negation scope resolution offers a
fair improvement over the baseline. For BSA and BSP, the default scope achieves F1
scores of 52.24 and 31.12, respectively, and the speculation rules applied directly without
modifications achieve 48.67 and 56.25.
In order to further improve on these results, we introduce a few new rules to
account specifically for negation. The general rule machinery is identical to the specu-
lation scope rules described in Section 6.1: The rules are triggered by the part of speech
of the cue and operate over the dependency representations output by the stacked
dependency parser described in Section 6.1.1. In developing the rules we consulted the
BioScope guidelines (Vincze et al 2008), as well as a descriptive study of negation in the
BioScope corpus (Morante 2010).
401
Computational Linguistics Volume 38, Number 2
Table 13
Additional dependency-based scope rules for negation, with information source (MaltParser or
XLE), organized by PoS of the cue.
PoS Description Source
DT Determiners scope over their head node and its descendants M
NN Nouns scope over their descendants M
NNnone none take scope over entire sentence if subject and otherwise over its descendants M
VB Verbs scope over their descendants M
RBvb Adverbs with verbal head scope over the descendants of the lexical verb M, X
RBother Adverbs scope over the descendants of the head M, X
7.2.1 Rule Overview. The added rules are presented in Table 13 and are described in more
detail subsequently, organized by the triggering PoS of the negation cue.
Determiners. Determiner cue words in BioScope are largely realized by the negative
determiner no. These take scope over their nominal head and its descendants, as seen in
Example (31):
(31) The finding that dexamethasone has {?no? effect on TPA-induced
activation of PKC} suggests [. . . ]
Nouns. Nominal cues take scope over their descendants (i.e., the members of the noun
phrase), as shown in Example (32).
(32) This unresponsiveness occurs because of a {?lack? of expression of the
beta-chain (accessory factor) of the IFN-gamma receptor}, while at the
same time [. . . ]
The negative pronoun none is tagged as a noun by our system, but deviates from regular
nouns in their negation scope: If the pronoun is a subject, it scopes over the remaining
sentence, as in Example (33), whereas in object function it simply scopes over the noun
phrase (Morante 2010). These are therefore treated specifically by our system.
(33) Similarly, {?none? of SCOPE?s component algorithms outperformed the
other ten programs on this data set by a statistically significant margin}.
Adverbs. Adverbs constitute the majority of negation cues and are largely realized by
the lexical item not. Syntactically, however, adverbs are a heterogeneous category. They
may modify a number of different head words and their scope will thus depend largely
on properties of the head. For instance, when an adverb is a nominal modifier, as in
Example (34), it has a narrow scope which includes only the head noun (34) and its
possible conjuncts.
(34) This report directly demonstrates that OTF-2 but {?not? OTF-1} regulates
the DRA gene
Verbal adverbs scope over the clause headed by the verbal head. As shown by Figure 2,
the parser?s analysis of verbal chains has the consequence that preverbal arguments and
modifiers, such as subjects and adverbs, are attached to the finite verb and postverbal
402
Velldal et al Rules, Rankers, and the Role of Syntax
arguments and modifiers are attached to the lexical verb, in cases where there is an
auxiliary. This rule thus locates the lexical verb (e.g., affect in Example [35]), in the
dependency path from the auxiliary head verb and defines scope over the descendants
of this verb. In cases where the lexical verb is passive, the subject is included in the scope
of the adverb, as in Example (36).
(35) IL-1 did {?not? affect the stability of the c-fos and c-jun transcripts}.
(36) {Levels of RNA coding for the receptor were ?not? modulated by exposure
to high levels of ligand}.
7.2.2 Evaluating the Negation Rules. The result of resolving the scope of gold-standard
negation cues using the new set of dependency rules (i.e., the speculation rules extended
with the negation specific rules of Table 13), are presented in Table 14, along with the
performance of the default scope baseline. First of all, we note that the baseline scores
provided by assigning default scope to all cues differ dramatically between the data sets,
ranging from an F1 of 52.24 for BSA, 31.12 for BSP, and 91.43 for BSR. In comparison,
the performance of the rules is fairly stable across BSA and BSP, and for both data sets
they improve substantially on the baseline (up by roughly 18.5 and 34.5 percentage
points on BSA and BSP, respectively). On BSR, however, the default scope baseline is
substantially stronger than for the other data sets, and even performs slightly better
than the rules. Recall from Table 1 that the average sentence length in the clinical reports
is substantially lower (7.7) than for the other data sets (average of 26), a property which
will make the default scope much more likely to succeed.
In order to shed more light on the performance of the rules on BSR, a manual
error analysis was performed, once again by two trained linguists working together. We
found that out of the total of 74 errors, 30 (40.5%) were parse errors, 29 (39.2%) were rule
errors, 8 (10.8%) were annotation errors, and 4 (5.4%) were undecided. Although it is
usually the case that short sentences are easier to parse, the reports contain a substantial
proportion of ungrammatical structures, such as missing subjects, dropped auxiliaries,
and bare noun phrases, as in Example (37), which clearly lead to lower parse quality,
resulting in 40% parse errors. There are also constructions, such as so-called run-on
constructions, as in Example (38), for which there is simply no correct analysis available
Table 14
Scope resolution for gold-standard negation cues across the BioScope sub-corpora.
Data Configuration F1
B
S
A
Default 52.24
Dependency Rules 70.91
Constituent Ranker 68.35
Combined 74.35
B
S
P
Default 31.12
Dependency Rules 65.69
Constituent Ranker 60.90
Combined 70.21
B
S
R
Default 91.43
Dependency Rules 90.86
Constituent Ranker 89.59
Combined 90.74
403
Computational Linguistics Volume 38, Number 2
within the dependency framework (which, for instance, requires that graphs should be
connected). In addition, the annotations of the reports data contain some idiosyncrasies
which the rules fail to reproduce. Twenty-four percent of the errors are found with the
same cue, namely, the adjective negative. The rules make attributive adjectives scope
over their nominal heads, whereas the BSR annotations define the scope to only cover
the cue word itself; see Example (37). The annotation errors were very similar to the
ones observed in the earlier error analysis of Section 6.1.5.
(37) |{?Negative?} chest radiograph|.
(38) |{?No? focal pneumonia}, normal chest radiograph|.
7.3 Adapting the Constituent Ranker for Negation
Adapting the SVM-based discriminative constituent ranker of Section 6.2 to also predict
the scope of negation is a straightforward procedure, requiring only minor modifi-
cations: Firstly, we developed a further slackening heuristic to ensure that predicted
scope does not begin with an auxiliary. Secondly, we augmented the family of linguistic
features to also record the presence of adverb cues with verbal heads (as specified by the
dependency-based scope rules in Table 13). Finally, we repeated the parameter tuning
for training with n-best and testing with m-best parses (as described in Section 6.2.4).
Performing 10-fold cross-validation on BSA using gold-standard negation cues, we
found that the optimal values for the ranker in isolation were n = 10 and m = 1.
When paralleling the combined approach developed in Section 6.3 (adding the rule-
predictions as a feature in the ranker while falling back on rule-predicted scope for
cases where we do not have an ERG parse) the optimal values were found to be n = 15
andm = 5. Examining the coverage of the parser and the alignment of constituents with
negation scope (considering the 50-best parses), we found that the upper-bound of the
constituent ranker (disregarding any fall-back strategy) on the BSA development set is
79.4% (compared to 83.6% for speculation).
Table 14 lists the performance of both the constituent ranker in isolation and the
combined approachwhen resolving the scope of gold-standard negation cues (reporting
10-fold cross-validation results for BSA, while using BSP and BSR for held-out testing).
We see that the dependency rules perform consistently better than the constituent
ranker, although the differences are not found to be statistically significant (the p-values
for BSA, BSP, and BSR are 0.06, 0.11, and 0.25, respectively). The combined approach
again outperforms the dependency rules on both BSA and BSP (and by a much larger
margin than we observed for speculation), however, with the improvements on both
data sets being significant. Just as we observed for the dependency rules in Section 7.2.2,
neither the constituent ranker nor the combined approach are effective in BSR.
7.4 End-to-End Evaluation with Comparison to Related Work
We now turn to evaluating our end-to-end negation system with SVM-based cue
classification and scope resolution using the combination of constituent ranking and
dependency-based rules. To put the evaluation in perspective we also compare our
results against the results of other state-of-the-art approaches to negation detection.
Comparison to previous work is complicated slightly by the fact that different data
splits and evaluation measures have been used across various studies. A commonly
reported measure in the literature on resolving negation scope is the percentage of
404
Velldal et al Rules, Rankers, and the Role of Syntax
correct scopes (PCS) as used by Morante and Daelemans (2009b), and Councill,
McDonald, and Velikovich (2010), among others. Councill, McDonald, and Velikovich
(2010) define PCS as the number of correct spans divided by the number of true spans. It
therefore corresponds roughly to the scope-level recall as reported in the current article.
The PCS notion of a correct scope, however, is less strict than in our set-up (Section 3.2):
Whereas we require an exact match of both the cue and the scope, Councill, McDonald,
and Velikovich (2010) do not include the cue identification in their evaluation.
Moreover, whereas the work of both Morante and Daelemans (2009b) and Councill,
McDonald, and Velikovich (2010) is based on the BioScope corpus, only Morante and
Daelemans (2009b) follow the same set-up assumed in the current article. Councill,
McDonald, and Velikovich (2010), on the other hand, evaluate by 5-fold cross-validation
on the papers alone, reporting a PCS score of 53.7%. When running our negation cue
classifier and constituent ranker (in the hybrid mode using the dependency features)
by 5-fold cross-validation on the papers we achieve a scope-level recall of 68.62 (and an
F1 of 64.50).
Table 15 shows a comparison of our negation scope resolution system with that
of Morante and Daelemans (2009b). Rather than using the PCS measure reported by
Morante and Daelemans (2009b), we have re-scored the output of their system accord-
ing to the CoNLL-2010 shared task scoring scheme, and it should therefore be kept in
mind that the system of Morante and Daelemans (2009b) originally was optimized with
respect to a slightly different metric.
For the cross-validated BSA experiments we find the results of the two systems
to be fairly similar, although the F1 achieved by our system is higher by more than
5 percentage points, mostly due to higher recall. For the cross-text experiments, the
differences are much more pronounced, with the F1 of our system being more than
22 points higher on BSP and more than 17 points higher on BSR. Again, the largest
differences are to be found for recall?even though this is the score that most closely
corresponds to the PCS metric used by Morante and Daelemans (2009b)?but as seen in
Table 15 there are substantial differences in precision as well. The scope-level differences
between the two systems are found to be statistically significant across all the three
BioScope sub-corpora.
Table 15
End-to-end results for our negation system, using the SVM cue classifier and the combination
of subtree ranking and dependency-based rules for scope resolution, comparing with Morante
et al (2009b).
Scope Level
Data Configuration Prec Rec F1
B
S
A
1
0
-F
o
ld Morante et al (2009b) 66.31 65.27 65.79
Cue classifier & Scope Rules + Ranking 69.30 72.89 71.05
B
S
P
H
e
ld
-o
u
t
Morante et al (2009b) 42.49 39.10 40.72
Cue classifier & Scope Rules + Ranking 58.58 68.09 62.98
B
S
R
H
e
ld
-o
u
t
Morante et al (2009b) 74.03 70.54 72.25
Cue classifier & Scope Rules + Ranking 89.62 89.41 89.52
405
Computational Linguistics Volume 38, Number 2
To some degree, some of the differences are to be expected, perhaps, at least with
respect to BSP. For example, the BSP evaluation represents a held-out setting for both
the cue and scope component in themachine learned system ofMorante andDaelemans
(2009b). While also true for our cue classifier and subtree ranker, it is not strictly
speaking the case for the dependency rules, and so the potential effect of any overfitting
during learningmight be less visible. The small set of manually defined rules are general
in nature, targeting the general syntactic constructions expressing negation, as shown
in Table 13. In addition to being based on the BioScope annotation guidelines, however,
both the abstracts and the full papers were consulted for patterns, and the fact that rule
development has included intermediate testing on BSP (although mostly during the
development of the initial set of speculation rules from which the negation rules are
derived) has likely made our system more tailored to the peculiarities of this data set.
When comparing the errors made by our system to those of Morante and Daelemans
(2009b), the most striking example of this is the inclusion of post-processing rules in our
system for ?backing off? from bracketed expressions (as discussed in Section 6.1). Al-
thoughmaking little difference on the abstracts, this has a huge impact when evaluating
the full papers, where bracketed expressions (citations, references to figures and tables,
etc.) are muchmore common, and the system output ofMorante and Daelemans (2009b)
seems to suffer from the lack of such robustness measures. In relation to the clinical
reports, one should bear in mind that, although our combined system outperforms that
of Morante and Daelemans (2009b) by a large margin, this result would still be rivaled
by simply using our default scope baseline, as is clear from Table 14.
The scope results of Zhu et al (2010) are unfortunately not currently directly com-
parable to ours, due to differences in evaluation methodologies. Whereas we perform
an exact match evaluation at the scope-level, as described in Section 3, Zhu et al (2010)
use a much less strict token-level evaluation even for their scopes in their end-to-end
evaluation. Nevertheless, our results appear to be highly competitive, because even
with the strict exact match criterion underlying our scope-level evaluation, our scores
are actually still higher for both the papers and the reports. (Zhu et al [2010] report an
F1 of 78.50 for the 10-fold runs on the abstracts, and 57.22 and 81.41 for held-out testing
on the papers and reports, respectively.)
8. Conclusion
This article has explored several linguistically informed approaches to the problem
of resolving the scope of speculation and negation within sentences. Our point of
departure was the system developed by Velldal, ?vrelid, and Oepen (2010) for the
CoNLL-2010 Shared Task challenge on resolving speculation in biomedical texts, where
a binary maximum entropy cue classifier was used in combination with a small set of
manually crafted scope resolution rules operating over dependency structures. In the
current article we have introduced several major extensions and improvements to this
initial system design.
First we presented a greatly simplified approach to cue identification using a linear
SVM classifier. The classifier only considers features of the immediate lexical context
of a target word, and it only aims to ?disambiguate? words that have already been
observed as speculation cues in the training data. The filtering imposed by this latter
?closed class? assumption greatly reduces the size and complexity of the model while
increasing classifier accuracy, yielding state-of-the-art performance on the CoNLL-2010
Shared Task evaluation data.
406
Velldal et al Rules, Rankers, and the Role of Syntax
We then presented a novel approach to the problem of resolving the scopes of
cues within a sentence. As an alternative to using the manually defined dependency
rules of our initial system, we showed how an SVM-based discriminative ranking
function can be learned for choosing subtrees from HPSG-based constituent structures.
An underlying assumption of the ranking approach is that annotated scopes actually
align with constituents, and we provided in-depth discussion and analysis of this issue.
Furthermore, while both the dependency rules and the constituent ranker achieve
good performance on their own, we showed how even better results can be achieved by
combining the two, as the errors they make are not always overlapping. The combined
approach uses the dependency rules for all cases where we do not have an available
HPSG parse, and for the cases where we do, the scope predicted by the rules is included
as a feature in the constituent ranker model. Together with the reformulation of our cue
classifier, this combined model for scope resolution obtains the best published results
so far on the CoNLL-2010 Shared Task evaluation data (to the best of our knowledge).
Finally, we have showed how all components of our speculation system are easily
ported to also handle the problem of resolving the scope of negation. With only modest
modifications, the system obtains state-of-the-art results also on the negation task. The
system outputs corresponding to the end-to-end experiments with our final model con-
figurations, for both speculation and negation, aremade available online (see footnote 2)
together with the relevant evaluation software.
Acknowledgments
We are grateful to the organizers of the
2010 CoNLL Shared Task and creators of
the BioScope resource; first, for engaging in
these kinds of community service, and
second for many in-depth discussions of
annotation and task details. We also want
to thank Buzhou Tang (HIT Shenzhen
Graduate School) and Roser Morante
(University of Antwerp), together with
their colleagues, for providing us with the
raw XML output of their negation and
speculation systems in order to enable
system comparisons. Andrew MacKinlay
(Melbourne University) and Dan Flickinger
(Stanford University) were of invaluable
help in adapting ERG parse selection to
the biomedical domain. We thank our
colleagues at the University of Oslo for
their comments and support during our
original participation in the 2010 CoNLL
Shared Task, as well as more recently in
preparing this manuscript. Large-scale
experimentation and engineering was
made possible though access to the TITAN
high-performance computing facilities at the
University of Oslo, and we are grateful to
the Scientific Computation staff at UiO, as
well as to the Norwegian Metacenter for
Computational Science. Last but not least,
we are indebted to the anonymous reviewers
for their careful reading and insightful
comments.
References
Brants, Thorsten. 2000. TnT. A statistical
Part-of-Speech tagger. In Proceedings of
the Sixth Conference on Applied Natural
Language Processing, pages 224?231,
Seattle, WA.
Butt, Miriam, Helge Dyvik, Tracy Holloway
King, Hiroshi Masuichi, and Christian
Rohrer. 2002. The Parallel Grammar
Project. In Proceedings of the COLING
Workshop on Grammar Engineering and
Evaluation, pages 1?7, Taipei.
Callmeier, Ulrich. 2002. Preprocessing and
encoding techniques in PET. In Stephan
Oepen, Daniel Flickinger, Jun?ichi Tsujii,
and Hans Uszkoreit, editors, Collaborative
Language Engineering. A Case Study in
Efficient Grammar-based Processing. CSLI
Publications, Stanford, CA, pages 127?143.
Collier, Nigel, Hyun S. Park, Norihiro Ogata,
Yuka Tateishi, Chikashi Nobata, Tomoko
Ohta, Tateshi Sekimizu, Hisao Imai,
Katsutoshi Ibushi, and Jun I. Tsujii. 1999.
The GENIA project: Corpus-based
knowledge acquisition and information
extraction from genome research papers.
In Proceedings of the 9th Conference of the
European Chapter of the ACL, pages 271?272,
Bergen.
Councill, Isaac G., Ryan McDonald, and
Leonid Velikovich. 2010. What?s great
and what?s not: Learning to classify the
scope of negation for improved sentiment
407
Computational Linguistics Volume 38, Number 2
analysis. In Proceedings of the Workshop
on Negation and Speculation in Natural
Language Processing, pages 51?59, Uppsala.
Crouch, Dick, Mary Dalrymple, Ron Kaplan,
Tracy King, John Maxwell, and Paula
Newman. 2008. XLE documentation. Palo
Alto Research Center, Palo Alto, CA.
Farkas, Richard, Veronika Vincze, Gyorgy
Mora, Janos Csirik, and Gy?rgy Szarvas.
2010. The CoNLL 2010 Shared Task:
Learning to detect hedges and their scope
in natural language text. In Proceedings of
the 14th Conference on Natural Language
Learning, pages 1?12, Uppsala.
Flickinger, Dan. 2002. On building a more
efficient grammar by exploiting types.
In Stephan Oepen, Dan Flickinger,
Jun?ichi Tsujii, and Hans Uszkoreit,
editors, Collaborative Language Engineering:
A Case Study in Efficient Grammar-based
Processing. CSLI Publications, Stanford,
CA, pages 1?17.
Gildea, Daniel. 2001. Corpus variation and
parser performance. In Proceedings of the
2001 Conference on Empirical Methods
in Natural Language Processing,
pages 167?202, Pittsburgh, PA.
Joachims, Thorsten. 1999. Making large-scale
SVM learning practical. In Bernhard
Sch?lkopf, Christopher J. C. Burges, and
Alexander J. Smola, editors, Advances in
Kernel Methods: Support Vector Learning.
MIT Press, Cambridge, MA, pages 41?56.
Joachims, Thorsten. 2002. Optimizing
search engines using clickthrough data.
In Proceedings of the Eighth ACM SIGKDD
International Conference on Knowledge
Discovery and Data Mining, pages 133?142,
Alberta.
Johansson, Richard and Pierre Nugues. 2007.
Extended constituent-to-dependency
conversion for English. In Proceedings of
16th Nordic Conference of Computational
Linguistics, pages 105?112, Tartu.
Johnson, Mark, Stuart Geman, Stephen
Canon, Zhiyi Chi, and Stefan Riezler.
1999. Estimators for stochastic
unification-based grammars. In Proceedings
of the 37th Meeting of the Association for
Computational Linguistics, pages 535?541,
College Park, MD.
Kilicoglu, Halil and Sabine Bergler. 2010.
A high-precision approach to detecting
hedges and their scopes. In Proceedings of
the 14th Conference on Natural Language
Learning, pages 70?77, Uppsala.
Light, Marc, Xin Ying Qiu, and Padmini
Srinivasan. 2004. The language of
bioscience: Facts, speculations, and
statements in between. In Proceedings
of the HLT-NAACL 2004 Workshop:
Biolink 2004, Linking Biological Literature,
Ontologies and Databases, pages 17?24,
Boston, MA.
MacKinlay, Andrew, Rebecca Dridan,
Dan Flickinger, Stephan Oepen, and
Timothy Baldwin. 2011. Treeblazing:
Using external treebanks to filter parse
forests for parse selection and treebanking.
In Proceedings of the 5th International Joint
Conference on Natural Language Processing,
pages 246?254, Chiang Mai.
Malouf, Robert and Gertjan van Noord. 2004.
Wide coverage parsing with stochastic
attribute value grammars. In Proceedings
of the IJCNLP Workshop Beyond Shallow
Analysis, Hainan.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English. The
Penn Treebank. Computational Linguistics,
19(2):313?330.
Martins, Andre F. T., Dipanjan Das, Noah A.
Smith, and Eric P. Xing. 2008. Stacking
dependency parsers. In Proceedings
of the 2008 Conference on Empirical
Methods in Natural Language Processing,
pages 157?166, Waikiki, HI.
Medlock, Ben and Ted Briscoe. 2007.
Weakly supervised learning for hedge
classification in scientific literature.
In Proceedings of the 45th Meeting of the
Association for Computational Linguistics,
pages 992?999, Prague.
Moilanen, Karo and Stephen Pulman. 2007.
Sentiment composition. In Proceedings
of the International Conference on Recent
Advances in Natural Language Processing,
pages 378?382, Borovets.
Morante, Roser. 2010. Descriptive analysis
of negation cues in biomedical texts.
In Proceedings of the 7th International
Conference on Language Resources and
Evaluation, pages 1429?1436, Valletta.
Morante, Roser and Walter Daelemans.
2009a. Learning the scope of hedge cues
in biomedical texts. In Proceedings of the
BioNLP 2009 Workshop, pages 28?36,
Boulder, CO.
Morante, Roser and Walter Daelemans.
2009b. A metalearning approach to
processing the scope of negation.
In Proceedings of the 13th Conference on
Natural Language Learning, pages 21?29,
Boulder, CO.
Morante, Roser, Anthony Liekens, and
Walter Daelemans. 2008. Learning the
scope of negation in biomedical texts.
408
Velldal et al Rules, Rankers, and the Role of Syntax
In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language
Processing, pages 715?724, Waikiki, HI.
Morante, Roser, Vincent van Asch, and
Walter Daelemans. 2010. Memory-based
resolution of in-sentence scope of hedge
cues. In Proceedings of the 14th Conference
on Natural Language Learning, pages 40?47,
Uppsala.
Nivre, Joakim, Johan Hall, and Jens Nilsson.
2006. MaltParser: A data-driven
parser-generator for dependency parsing.
In Proceedings of the 5th International
Conference on Language Resources and
Evaluation, pages 2216?2219, Genoa.
Nivre, Joakim and Ryan McDonald.
2008. Integrating graph-based and
transition-based dependency
parsers. In Proceedings of the 46th
Meeting of the Association for
Computational Linguistics,
pages 950?958, Columbus, OH.
?vrelid, Lilja, Jonas Kuhn, and Kathrin
Spreyer. 2009. Cross-framework
parser stacking for data-driven
dependency parsing. TAL special
issue on Machine Learning for NLP,
50(3):109?138.
?vrelid, Lilja, Erik Velldal, and Stephan
Oepen. 2010. Syntactic scope resolution
in uncertainty analysis. In Proceedings
of the 23rd International Conference on
Computational Linguistics, pages 1379?1387,
Beijing.
Petrov, Slav and Dan Klein. 2007.
Improved inference for unlexicalized
parsing. In Proceedings of Human Language
Technologies: The Annual Conference of the
North American Chapter of the Association for
Computational Linguistics, pages 404?411,
Rochester, NY.
Pollard, Carl and Ivan A. Sag. 1987.
Information-Based Syntax and Semantics.
Vol. 1: Fundamentals. CSLI Lecture
Notes # 13. CSLI Press, Stanford, CA.
Pollard, Carl and Ivan A. Sag. 1994.
Head-driven Phrase Structure Grammar.
The University of Chicago Press and
CSLI Publications, Chicago, IL.
Rei, Marek and Ted Briscoe. 2010.
Combining manual rules and supervised
learning for hedge cue and scope
detection. In Proceedings of the 14th
Conference on Natural Language Learning,
pages 56?63, Uppsala.
Riezler, Stefan, Tracy H. King, Ronald M.
Kaplan, Richard Crouch, John T. Maxwell,
and Mark Johnson. 2002. Parsing the Wall
Street Journal using a lexical-functional
grammar and discriminative estimation
techniques. In Proceedings of the
40th Meeting of the Association for
Computational Linguistics, pages 271?278,
Philadelphia, PA.
Schmid, Helmut. 1994. Probabilistic
part-of-speech tagging using decision
trees. In International Conference on
New Methods in Language Processing,
pages 44?49, Manchester.
Szarvas, Gy?rgy. 2008. Hedge classification
in biomedical texts with a weakly
supervised selection of keywords. In
Proceedings of the 46th Meeting of the
Association for Computational Linguistics,
pages 281?289, Columbus, OH.
Tang, Buzhou, Xiaolong Wang, Xuan Wang,
Bo Yuan, and Shixi Fan. 2010. A cascade
method for detecting hedges and their
scope in natural language text. In
Proceedings of the 14th Conference on
Natural Language Learning, pages 13?17,
Uppsala.
Toutanova, Kristina, Christopher D.
Manning, Dan Flickinger, and Stephan
Oepen. 2005. Stochastic HPSG parse
disambiguation using the Redwoods
corpus. Research on Language and
Computation, 3(1):83?105.
Tsuruoka, Yoshimasa, Yuka Tateishi,
Jin-Dong Kim, Tomoko Ohta, John
McNaught, Sophia Ananiadou, and
Jun?ichi Tsujii. 2005. Developing a robust
Part-of-Speech tagger for biomedical text.
In P. Bozanis and E. Houstis, editors,
Advances in Informatics. Springer, Berlin,
pages 382?392.
Velldal, Erik. 2011. Predicting speculation:
A simple disambiguation approach
to hedge detection in biomedical
literature. Journal of Biomedical Semantics,
2(Suppl 5):S7.
Velldal, Erik, Lilja ?vrelid, and Stephan
Oepen. 2010. Resolving speculation:
MaxEnt cue classification and
dependency-based scope rules.
In Proceedings of the 14th Conference on
Natural Language Learning, pages 48?55,
Uppsala.
Vincze, Veronika, Gy?rgy Szarvas, Rich?rd
Farkas, Gy?rgy M?ra, and J?nos Csirik.
2008. The BioScope corpus: Biomedical
texts annotated for uncertainty, negation
and their scopes. BMC Bioinformatics, 9
(Suppl. 11).
Vlachos, Andreas and Mark Craven. 2010.
Detecting speculative language using
syntactic dependencies and logistic
regression. In Proceedings of the 14th
409
Computational Linguistics Volume 38, Number 2
Conference on Natural Language Learning,
pages 18?25, Uppsala.
Zhang, Yi, Stephan Oepen, and John
Carroll. 2007. Efficiency in
unification-based n-best parsing. In
Proceedings of the 10th International
Conference on Parsing Technologies,
pages 48?59, Prague.
Zhang, Yi and Rui Wang. 2009.
Cross-domain dependency parsing
using a deep linguistic grammar.
In Proceedings of the 47th Meeting of the
Association for Computational Linguistics,
pages 378?386, Singapore.
Zhang, Yue and Joakim Nivre. 2011.
Transition-based dependency parsing
with rich non-local features. In Proceedings
of the 49th Meeting of the Association for
Computational Linguistics, pages 188?193,
Portland, OR.
Zhu, Qiaoming, Junhui Li, Hongling
Wang, and Guodong Zhou. 2010.
A unified framework for scope
learning via simplified shallow
semantic parsing. In Proceedings of the
2010 Conference on Empirical Methods
in Natural Language Processing,
pages 714?724, Cambridge, MA.
410
Proceedings of the ACL Student Research Workshop, pages 31?37,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Survey on parsing three dependency representations for English
Angelina Ivanova Stephan Oepen Lilja ?vrelid
University of Oslo, Department of Informatics
{angelii |oe |liljao }@ifi.uio.no
Abstract
In this paper we focus on practical is-
sues of data representation for dependency
parsing. We carry out an experimental
comparison of (a) three syntactic depen-
dency schemes; (b) three data-driven de-
pendency parsers; and (c) the influence of
two different approaches to lexical cate-
gory disambiguation (aka tagging) prior to
parsing. Comparing parsing accuracies in
various setups, we study the interactions
of these three aspects and analyze which
configurations are easier to learn for a de-
pendency parser.
1 Introduction
Dependency parsing is one of the mainstream re-
search areas in natural language processing. De-
pendency representations are useful for a number
of NLP applications, for example, machine trans-
lation (Ding and Palmer, 2005), information ex-
traction (Yakushiji et al, 2006), analysis of ty-
pologically diverse languages (Bunt et al, 2010)
and parser stacking (?vrelid et al, 2009). There
were several shared tasks organized on depen-
dency parsing (CoNLL 2006?2007) and labeled
dependencies (CoNLL 2008?2009) and there were
a number of attempts to compare various depen-
dencies intrinsically, e.g. (Miyao et al, 2007), and
extrinsically, e.g. (Wu et al, 2012).
In this paper we focus on practical issues of data
representation for dependency parsing. The cen-
tral aspects of our discussion are (a) three depen-
dency formats: two ?classic? representations for
dependency parsing, namely, Stanford Basic (SB)
and CoNLL Syntactic Dependencies (CD), and
bilexical dependencies from the HPSG English
Resource Grammar (ERG), so-called DELPH-IN
Syntactic Derivation Tree (DT), proposed recently
by Ivanova et al (2012); (b) three state-of-the art
statistical parsers: Malt (Nivre et al, 2007), MST
(McDonald et al, 2005) and the parser of Bohnet
and Nivre (2012); (c) two approaches to word-
category disambiguation, e.g. exploiting common
PTB tags and using supertags (i.e. specialized
ERG lexical types).
We parse the formats and compare accuracies
in all configurations in order to determine how
parsers, dependency representations and grammat-
ical tagging methods interact with each other in
application to automatic syntactic analysis.
SB and CD are derived automatically from
phrase structures of Penn Treebank to accommo-
date the needs of fast and accurate dependency
parsing, whereas DT is rooted in the formal gram-
mar theory HPSG and is independent from any
specific treebank. For DT we gain more expres-
sivity from the underlying linguistic theory, which
challenges parsing with statistical tools. The struc-
tural analysis of the schemes in Ivanova et al
(2012) leads to the hypothesis that CD and DT
are more similar to each other than SB to DT.
We recompute similarities on a larger treebank and
check whether parsing results reflect them.
The paper has the following structure: an
overview of related work is presented in Sec-
tion 2; treebanks, tagsets, dependency schemes
and parsers used in the experiments are introduced
in Section 3; analysis of parsing results is dis-
cussed in Section 4; conclusions and future work
are outlined in Section 5.
2 Related work
Schwartz et al (2012) investigate which depen-
dency representations of several syntactic struc-
tures are easier to parse with supervised ver-
sions of the Klein and Manning (2004) parser,
ClearParser (Choi and Nicolov, 2009), MST
Parser, Malt and the Easy First Non-directional
parser (Goldberg and Elhadad, 2010). The results
imply that all parsers consistently perform better
when (a) coordination has one of the conjuncts as
the head rather than the coordinating conjunction;
31
A , B and C A , B and C A, B and C
Figure 1: Annotation of coordination structure in SB, CD and DT (left to right) dependency formats
(b) the noun phrase is headed by the noun rather
than by determiner; (c) prepositions or subordinat-
ing conjunctions, rather than their NP or clause ar-
guments, serve as the head in prepositional phrase
or subordinated clauses. Therefore we can expect
(a) Malt and MST to have fewer errors on coor-
dination structures parsing SB and CD than pars-
ing DT, because SB and CD choose the first con-
junct as the head and DT chooses the coordinating
conjunction as the head; (b,c) no significant dif-
ferences for the errors on noun and prepositional
phrases, because all three schemes have the noun
as the head of the noun phrase and the preposition
as the head of the prepositional phrase.
Miwa et al (2010) present intristic and extris-
tic (event-extraction task) evaluation of six parsers
(GDep, Bikel, Stanford, Charniak-Johnson, C&C
and Enju parser) on three dependency formats
(Stanford Dependencies, CoNLL-X, and Enju
PAS). Intristic evaluation results show that all
parsers have the highest accuracies with the
CoNLL-X format.
3 Data and software
3.1 Treebanks
For the experiments in this paper we used the Penn
Treebank (Marcus et al, 1993) and the Deep-
Bank (Flickinger et al, 2012). The latter is com-
prised of roughly 82% of the sentences of the first
16 sections of the Penn Treebank annotated with
full HPSG analyses from the English Resource
Grammar (ERG). The DeepBank annotations are
created on top of the raw text of the PTB. Due to
imperfections of the automatic tokenization, there
are some token mismatches between DeepBank
and PTB. We had to filter out such sentences to
have consistent number of tokens in the DT, SB
and CD formats. For our experiments we had
available a training set of 22209 sentences and a
test set of 1759 sentences (from Section 15).
3.2 Parsers
In the experiments described in Section 4 we used
parsers that adopt different approaches and imple-
ment various algorithms.
Malt (Nivre et al, 2007): transition-based de-
pendency parser with local learning and greedy
search.
MST (McDonald et al, 2005): graph-based
dependency parser with global near-exhaustive
search.
Bohnet and Nivre (2012) parser: transition-
based dependency parser with joint tagger that im-
plements global learning and beam search.
3.3 Dependency schemes
In this work we extract DeepBank data in the form
of bilexical syntactic dependencies, DELPH-IN
Syntactic Derivation Tree (DT) format. We ob-
tain the exact same sentences in Stanford Basic
(SB) format from the automatic conversion of the
PTB with the Stanford parser (de Marneffe et al,
2006) and in the CoNLL Syntactic Dependencies
(CD) representation using the LTH Constituent-
to-Dependency Conversion Tool for Penn-style
Treebanks (Johansson and Nugues, 2007).
SB and CD represent the way to convert PTB
to bilexical dependencies; in contrast, DT is
grounded in linguistic theory and captures deci-
sions taken in the grammar. Figure 1 demonstrates
the differences between the formats on the coor-
dination structure. According to Schwartz et al
(2012), analysis of coordination in SB and CD is
easier for a statistical parser to learn; however, as
we will see in section 4.3, DT has more expressive
power distinguishing structural ambiguities illus-
trated by the classic example old men and women.
3.4 Part-of-speech tags
We experimented with two tag sets: PTB tags and
lexical types of the ERG grammar - supertags.
PTB tags determine the part of speech (PoS)
and some morphological features, such as num-
ber for nouns, degree of comparison for adjectives
and adverbs, tense and agreement with person and
number of subject for verbs, etc.
Supertags are composed of part-of-speech, va-
lency in the form of an ordered sequence of
complements, and annotations that encompass
category-internal subdivisions, e.g. mass vs. count
vs. proper nouns, intersective vs. scopal adverbs,
32
or referential vs. expletive pronouns. Example of
a supertag: v np is le (verb ?is? that takes noun
phrase as a complement).
There are 48 tags in the PTB tagset and 1091
supertags in the set of lexical types of the ERG.
The state-of-the-art accuracy of PoS-tagging on
in-domain test data using gold-standard tokeniza-
tion is roughly 97% for the PTB tagset and ap-
proximately 95% for the ERG supertags (Ytrest?l,
2011). Supertagging for the ERG grammar is an
ongoing research effort and an off-the-shelf su-
pertagger for the ERG is not currently available.
4 Experiments
In this section we give a detailed analysis of pars-
ing into SB, CD and DT dependencies with Malt,
MST and the Bohnet and Nivre (2012) parser.
4.1 Setup
For Malt and MST we perform the experiments
on gold PoS tags, whereas the Bohnet and Nivre
(2012) parser predicts PoS tags during testing.
Prior to each experiment with Malt, we used
MaltOptimizer to obtain settings and a feature
model; for MST we exploited default configura-
tion; for the Bohnet and Nivre (2012) parser we
set the beam parameter to 80 and otherwise em-
ployed the default setup.
With regards to evaluation metrics we use la-
belled attachment score (LAS), unlabeled attach-
ment score (UAS) and label accuracy (LACC) ex-
cluding punctuation. Our results cannot be di-
rectly compared to the state-of-the-art scores on
the Penn Treebank because we train on sections
0-13 and test on section 15 of WSJ. Also our re-
sults are not strictly inter-comparable because the
setups we are using are different.
4.2 Discussion
The results that we are going to analyze are pre-
sented in Tables 1 and 2. Statistical significance
was assessed using Dan Bikel?s parsing evaluation
comparator1 at the 0.001 significance level. We
inspect three different aspects in the interpretation
of these results: parser, dependency format and
tagset. Below we will look at these three angles
in detail.
From the parser perspective Malt and MST are
not very different in the traditional setup with gold
1http://nextens.uvt.nl/depparse-wiki/
SoftwarePage#scoring
PTB tags (Table 1, Gold PTB tags). The Bohnet
and Nivre (2012) parser outperforms Malt on CD
and DT and MST on SB, CD and DT with PTB
tags even though it does not receive gold PTB tags
during test phase but predicts them (Table 2, Pre-
dicted PTB tags). This is explained by the fact that
the Bohnet and Nivre (2012) parser implements a
novel approach to parsing: beam-search algorithm
with global structure learning.
MST ?loses? more than Malt when parsing SB
with gold supertags (Table 1, Gold supertags).
This parser exploits context features ?POS tag of
each intervening word between head and depen-
dent? (McDonald et al, 2006). Due to the far
larger size of the supertag set compared to the PTB
tagset, such features are sparse and have low fre-
quencies. This leads to the lower scores of pars-
ing accuracy for MST. For the Bohnet and Nivre
(2012) parser the complexity of supertag predic-
tion has significant negative influence on the at-
tachment and labeling accuracies (Table 2, Pre-
dicted supertags). The addition of gold PTB tags
as a feature lifts the performance of the Bohnet
and Nivre (2012) parser to the level of perfor-
mance of Malt and MST on CD with gold su-
pertags and Malt on SB with gold supertags (com-
pare Table 2, Predicted supertags + gold PTB, and
Table 1, Gold supertags).
Both Malt and MST benefit slightly from the
combination of gold PTB tags and gold supertags
(Table 1, Gold PTB tags + gold supertags). For
the Bohnet and Nivre (2012) parser we also ob-
serve small rise of accuracy when gold supertags
are provided as a feature for prediction of PTB
tags (compare Predicted PTB tags and Predicted
PTB tags + gold supertags sections of Table 2).
The parsers have different running times: it
takes minutes to run an experiment with Malt,
about 2 hours with MST and up to a day with the
Bohnet and Nivre (2012) parser.
From the point of view of the dependency for-
mat, SB has the highest LACC and CD is first-rate
on UAS for all three parsers in most of the con-
figurations (Tables 1 and 2). This means that SB
is easier to label and CD is easier to parse struc-
turally. DT appears to be a more difficult target
format because it is both hard to label and attach
in most configurations. It is not an unexpected re-
sult, since SB and CD are both derived from PTB
phrase-structure trees and are oriented to ease de-
pendency parsing task. DT is not custom-designed
33
Gold PTB tags
LAS UAS LACC
Malt MST Malt MST Malt MST
SB 89.21 88.59 90.95 90.88 93.58 92.79
CD 88.74 88.72 91.89 92.01 91.29 91.34
DT 85.97 86.36 89.22 90.01 88.73 89.22
Gold supertags
LAS UAS LACC
Malt MST Malt MST Malt MST
SB 87.76 85.25 90.63 88.56 92.38 90.29
CD 88.22 87.27 91.17 90.41 91.30 90.74
DT 89.92 89.58 90.96 90.56 92.50 92.64
Gold PTB tags + gold supertags
LAS UAS LACC
Malt MST Malt MST Malt MST
SB 90.321 89.431 91.901 91.842 94.481 93.261
CD 89.591 89.372 92.431 92.772 92.321 92.072
DT 90.691 91.192 91.831 92.332 93.101 93.692
Table 1: Parsing results of Malt and MST on
Stanford Basic (SB), CoNLL Syntactic De-
pendencies (CD) and DELPH-IN Syntactic
Derivation Tree (DT) formats. Punctuation is
excluded from the scoring. Gold PTB tags:
Malt and MST are trained and tested on gold
PTB tags. Gold supertags: Malt and MST
are trained and tested on gold supertags. Gold
PTB tags + gold supertags: Malt and MST are
trained on gold PTB tags and gold supertags.
1 denotes a feature model in which gold PTB
tags function as PoS and gold supertags act
as additional features (in CPOSTAG field); 2
stands for the feature model which exploits
gold supertags as PoS and uses gold PTB tags
as extra features (in CPOSTAG field).
Predicted PTB tags
LAS UAS LACC
Bohnet and Nivre (2012)
SB 89.56 92.36 93.30
CD 89.77 93.01 92.10
DT 88.26 91.63 90.72
Predicted supertags
LAS UAS LACC
Bohnet and Nivre (2012)
SB 85.41 89.38 90.17
CD 86.73 90.73 89.72
DT 85.76 89.50 88.56
Pred. PTB tags + gold supertags
LAS UAS LACC
Bohnet and Nivre (2012)
SB 90.32 93.01 93.85
CD 90.55 93.56 92.79
DT 91.51 92.99 93.88
Pred. supertags + gold PTB
LAS UAS LACC
Bohnet and Nivre (2012)
SB 87.20 90.07 91.81
CD 87.79 91.47 90.62
DT 86.31 89.80 89.17
Table 2: Parsing results of the Bohnet
and Nivre (2012) parser on Stanford Ba-
sic (SB), CoNLL Syntactic Dependencies
(CD) and DELPH-IN Syntactic Deriva-
tion Tree (DT) formats. Parser is trained
on gold-standard data. Punctuation is ex-
cluded from the scoring. Predicted PTB:
parser predicts PTB tags during the test
phase. Predicted supertags: parser pre-
dicts supertags during the test phase. Pre-
dicted PTB + gold supertags: parser re-
ceives gold supertags as feature and pre-
dicts PTB tags during the test phase. Pre-
dicted supertags + gold PTB: parser re-
ceives PTB tags as feature and predicts
supertags during test phase.
34
to dependency parsing and is independent from
parsing questions in this sense. Unlike SB and
CD, it is linguistically informed by the underlying,
full-fledged HPSG grammar.
The Jaccard similarity on our training set is 0.57
for SB and CD, 0.564 for CD and DT, and 0.388
for SB and DT. These similarity values show that
CD and DT are structurally closer to each other
than SB and DT. Contrary to our expectations, the
accuracy scores of parsers do not suggest that CD
and DT are particularly similar to each other in
terms of parsing.
Inspecting the aspect of tagset we conclude that
traditional PTB tags are compatible with SB and
CD but do not fit the DT scheme well, while ERG
supertags are specific to the ERG framework and
do not seem to be appropriate for SB and CD. Nei-
ther of these findings seem surprising, as PTB tags
were developed as part of the treebank from which
CD and SB are derived; whereas ERG supertags
are closely related to the HPSG syntactic struc-
tures captured in DT. PTB tags were designed to
simplify PoS-tagging whereas supertags were de-
veloped to capture information that is required to
analyze syntax of HPSG.
For each PTB tag we collected corresponding
supertags from the gold-standard training set. For
open word classes such as nouns, adjectives, ad-
verbs and verbs the relation between PTB tags
and supertags is many-to-many. Unique one-to-
many correspondence holds only for possessive
wh-pronoun and punctuation.
Thus, supertags do not provide extra level of
detalization for PTB tags, but PTB tags and su-
pertags are complementary. As discussed in sec-
tion 3.4, they contain bits of information that are
different. For this reason their combination re-
sults in slight increase of accuracy for all three
parsers on all dependency formats (Table 1, Gold
PTB tags + gold supertags, and Table 2, Predicted
PTB + gold supertags and Predicted supertags +
gold PTB). The Bohnet and Nivre (2012) parser
predicts supertags with an average accuracy of
89.73% which is significantly lower than state-of-
the-art 95% (Ytrest?l, 2011).
When we consider punctuation in the evalua-
tion, all scores raise significantly for DT and at
the same time decrease for SB and CD for all three
parsers. This is explained by the fact that punctu-
ation in DT is always attached to the nearest token
which is easy to learn for a statistical parser.
4.3 Error analysis
Using the CoNLL-07 evaluation script2 on our test
set, for each parser we obtained the error rate dis-
tribution over CPOSTAG on SB, CD and DT.
VBP, VBZ and VBG. VBP (verb, non-3rd
person singular present), VBZ (verb, 3rd per-
son singular present) and VBG (verb, gerund or
present participle) are the PTB tags that have error
rates in 10 highest error rates list for each parser
(Malt, MST and the Bohnet and Nivre (2012)
parser) with each dependency format (SB, CD
and DT) and with each PoS tag set (PTB PoS
and supertags) when PTB tags are included as
CPOSTAG feature. We automatically collected all
sentences that contain 1) attachment errors, 2) la-
bel errors, 3) attachment and label errors for VBP,
VBZ and VBG made by Malt parser on DT format
with PTB PoS. For each of these three lexical cat-
egories we manually analyzed a random sample
of sentences with errors and their corresponding
gold-standard versions.
In many cases such errors are related to the root
of the sentence when the verb is either treated as
complement or adjunct instead of having a root
status or vice versa. Errors with these groups of
verbs mostly occur in the complex sentences that
contain several verbs. Sentences with coordina-
tion are particularly difficult for the correct attach-
ment and labeling of the VBP (see Figure 2 for an
example).
Coordination. The error rate of Malt, MST and
the Bohnet and Nivre (2012) parser for the coor-
dination is not so high for SB and CD ( 1% and
2% correspondingly with MaltParser, PTB tags)
whereas for DT the error rate on the CPOSTAGS
is especially high (26% with MaltParser, PTB
tags). It means that there are many errors on
incoming dependency arcs for coordinating con-
junctions when parsing DT. On outgoing arcs
parsers also make more mistakes on DT than on
SB and CD. This is related to the difference in
choice of annotation principle (see Figure 1). As
it was shown in (Schwartz et al, 2012), it is harder
to parse coordination headed by coordinating con-
junction.
Although the approach used in DT is harder for
parser to learn, it has some advantages: using SB
and CD annotations, we cannot distinguish the two
cases illustrated with the sentences (a) and (b):
2http://nextens.uvt.nl/depparse-wiki/
SoftwarePage#scoring
35
VBP VBD VBD
The figures show that spending rose 0.1 % in the third quarter <. . .> and was up 3.8 % from a year ago .
root
SB-HD
VP-VP
HD-CMP
MRK-NH
root
SP-HD
HD-CMP Cl-CL
MRK-NH
Figure 2: The gold-standard (in green above the sentence) and the incorrect Malt?s (in red below the
sentence) analyses of the utterance from the DeepBank in DT format with PTB PoS tags
a) The fight is putting a tight squeeze on prof-
its of many, threatening to drive the small-
est ones out of business and straining rela-
tions between the national fast-food chains
and their franchisees.
b) Proceeds from the sale will be used for re-
modelling and reforbishing projects, as well
as for the planned MGM Grand hotel/casino
and theme park.
In the sentence a) ?the national fast-food? refers
only to the conjunct ?chains?, while in the sen-
tence b) ?the planned? refers to both conjuncts and
?MGM Grand? refers only to the first conjunct.
The Bohnet and Nivre (2012) parser succeeds in
finding the correct conjucts (shown in bold font)
on DT and makes mistakes on SB and CD in some
difficult cases like the following ones:
a) <. . .> investors hoard gold and help under-
pin its price <. . .>
b) Then take the expected return and subtract
one standard deviation.
CD and SB wrongly suggest ?gold? and ?help? to
be conjoined in the first sentence and ?return? and
?deviation? in the second.
5 Conclusions and future work
In this survey we gave a comparative experi-
mental overview of (i) parsing three dependency
schemes, viz., Stanford Basic (SB), CoNLL Syn-
tactic Dependencies (CD) and DELPH-IN Syn-
tactic Derivation Tree (DT), (ii) with three lead-
ing dependency parsers, viz., Malt, MST and the
Bohnet and Nivre (2012) parser (iii) exploiting
two different tagsets, viz., PTB tags and supertags.
From the parser perspective, the Bohnet and
Nivre (2012) parser performs better than Malt and
MST not only on conventional formats but also on
the new representation, although this parser solves
a harder task than Malt and MST.
From the dependency format perspective, DT
appeares to be a more difficult target dependency
representation than SB and CD. This suggests that
the expressivity that we gain from the grammar
theory (e.g. for coordination) is harder to learn
with state-of-the-art dependency parsers. CD and
DT are structurally closer to each other than SB
and DT; however, we did not observe sound evi-
dence of a correlation between structural similar-
ity of CD and DT and their parsing accuracies
Regarding the tagset aspect, it is natural that
PTB tags are good for SB and CD, whereas the
more fine-grained set of supertags fits DT bet-
ter. PTB tags and supertags are complementary,
and for all three parsers we observe slight benefits
from being supplied with both types of tags.
As future work we would like to run more ex-
periments with predicted supertags. In the absence
of a specialized supertagger, we can follow the
pipeline of (Ytrest?l, 2011) who reached the state-
of-the-art supertagging accuracy of 95%.
Another area of our interest is an extrinsic eval-
uation of SB, CD and DT, e.g. applied to semantic
role labeling and question-answering in order to
find out if the usage of the DT format grounded
in the computational grammar theory is beneficial
for such tasks.
Acknowledgments
The authors would like to thank Rebecca Dridan,
Joakim Nivre, Bernd Bohnet, Gertjan van Noord
and Jelke Bloem for interesting discussions and
the two anonymous reviewers for comments on
the work. Experimentation was made possible
through access to the high-performance comput-
ing resources at the University of Oslo.
36
References
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and
labeled non-projective dependency parsing. In
EMNLP-CoNLL, pages 1455?1465. ACL.
Harry Bunt, Paola Merlo, and Joakim Nivre, editors.
2010. Trends in Parsing Technology. Springer Ver-
lag, Stanford.
Jinho D Choi and Nicolas Nicolov. 2009. K-best, lo-
cally pruned, transition-based dependency parsing
using robust risk minimization. Recent Advances in
Natural Language Processing V, pages 205?216.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure trees. In
LREC.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05), pages 541?548, Ann Arbor,
Michigan, June. Association for Computational Lin-
guistics.
Daniel Flickinger, Yi Zhang, and Valia Kordoni. 2012.
DeepBank: a Dynamically Annotated Treebank of
the Wall Street Journal. In Proceedings of the
Eleventh International Workshop on Treebanks and
Linguistic Theories, pages 85?96. Edies Colibri.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ?10, pages 742?750, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Angelina Ivanova, Stephan Oepen, Lilja ?vrelid, and
Dan Flickinger. 2012. Who did what to whom?
a contrastive study of syntacto-semantic dependen-
cies. In Proceedings of the Sixth Linguistic Annota-
tion Workshop, pages 2?11, Jeju, Republic of Korea,
July. Association for Computational Linguistics.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
English. In Proceedings of NODALIDA 2007, pages
105?112, Tartu, Estonia, May 25-26.
Dan Klein and Christopher D. Manning. 2004.
Corpus-based induction of syntactic structure: mod-
els of dependency and constituency. In Proceedings
of the 42nd Annual Meeting on Association for Com-
putational Linguistics, ACL ?04, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn Treebank. Com-
putational Linguistics, 19(2):313?330, June.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing, HLT ?05, pages 523?530, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of the
Tenth Conference on Computational Natural Lan-
guage Learning, CoNLL-X ?06, pages 216?220,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun ichi Tsujii. 2010. Evaluating dependency repre-
sentations for event extraction. In Chu-Ren Huang
and Dan Jurafsky, editors, COLING, pages 779?787.
Tsinghua University Press.
Yusuke Miyao, Kenji Sagae, and Jun?ichi Tsujii. 2007.
Towards framework-independent evaluation of deep
linguistic parsers. In Ann Copestake, editor, Pro-
ceedings of the GEAF 2007 Workshop, CSLI Studies
in Computational Linguistics Online, page 21 pages.
CSLI Publications.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav
Marinov, and Erwin Marsi. 2007. MaltParser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Lilja ?vrelid, Jonas Kuhn, and Kathrin Spreyer. 2009.
Cross-framework parser stacking for data-driven de-
pendency parsing. TAL, 50(3):109?138.
Roy Schwartz, Omri Abend, and Ari Rappoport. 2012.
Learnability-based syntactic annotation design. In
Proc. of the 24th International Conference on Com-
putational Linguistics (Coling 2012), Mumbai, In-
dia, December. Coling 2012 Organizing Committee.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2012. A Compara-
tive Study of Target Dependency Structures for Sta-
tistical Machine Translation. In ACL (2), pages 100?
104. The Association for Computer Linguistics.
Akane Yakushiji, Yusuke Miyao, Tomoko Ohta, Yuka
Tateisi, and Jun?ichi Tsujii. 2006. Automatic con-
struction of predicate-argument structure patterns
for biomedical information extraction. In Proceed-
ings of the 2006 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?06,
pages 284?292, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Gisle Ytrest?l. 2011. Cuteforce: deep deterministic
HPSG parsing. In Proceedings of the 12th Interna-
tional Conference on Parsing Technologies, IWPT
?11, pages 186?197, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
37
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 310?318,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UiO1: Constituent-Based Discriminative Ranking for Negation Resolution
Jonathon Read Erik Velldal Lilja ?vrelid Stephan Oepen
University of Oslo, Department of Informatics
{jread,erikve,liljao,oe}@ifi.uio.no
Abstract
This paper describes the first of two systems
submitted from the University of Oslo (UiO)
to the 2012 *SEM Shared Task on resolving
negation. Our submission is an adaption of
the negation system of Velldal et al (2012),
which combines SVM cue classification with
SVM-based ranking of syntactic constituents
for scope resolution. The approach further ex-
tends our prior work in that we also identify
factual negated events. While submitted for
the closed track, the system was the top per-
former in the shared task overall.
1 Introduction
The First Joint Conference on Lexical and Compu-
tational Semantics (*SEM 2012) hosts a shared task
on resolving negation (Morante and Blanco, 2012).
This involves the subtasks of (i) identifying nega-
tion cues, (ii) identifying the in-sentence scope of
these cues, and (iii) identifying negated (and factual)
events. This paper describes a system submitted by
the Language Technology Group at the University of
Oslo (UiO). Our starting point is the negation system
developed by Velldal et al (2012) for the domain of
biomedical texts, an SVM-based system for classi-
fying cues and ranking syntactic constituents to re-
solve cue scopes. However, we extend and adapt
this system in several important respects, such as in
terms of the underlying linguistic formalisms that
are used, the textual domain, handling of morpho-
logical cues and discontinuous scopes, and in that
the current system also identifies negated events.
The data sets used for the shared task include
the following, all based on negation-annotated Co-
nan Doyle (CD) stories (Morante and Daelemans,
2012): a training set of 3644 sentences (hereafter
referred to as CDT), a development set of 787 sen-
tences (CDD), and a held-out evaluation set of 1089
sentences (CDE). We will refer to the combination
of CDT and CDD as CDTD. An example of an an-
notated sentence is shown in (1) below, where the
cue is marked in bold, the scope is underlined, and
the event marked in italics.
(1) There was no answer.
We describe two different system configurations,
both of which were submitted for the closed track
(hence we can only make use of the data provided
by the task organizers). The systems only differ
with respect to how they were optimized. In the
first configuration, (hereafter I), all components in
the pipeline had their parameters tuned by 10-fold
cross-validation across CDTD. The second config-
uration (II) is tuned against CDD using CDT for
training. The rationale for this strategy is to guard
against possible overfitting effects that could result
from either optimization scheme, given the limited
size of the data sets. For the held-out testing all mod-
els are estimated on the entire CDTD.
Unless otherwise noted, all reported scores are
generated using the evaluation script provided by the
organizers, which breaks down performance with re-
spect to cues, events, scope tokens, and two vari-
ants of scope-level exact match (one requiring exact
match of cues and the other only partial cue match).
The latter two scores are identical for our system
hence are not duplicated in this paper. Furthermore,
as we did not optimize for the scope tokens measure
this is only reported for the final evaluation.
Note also that the evaluation actually includes
two variants of the metrics mentioned above; a set
of primary measures with precision computed as
P = TP/(TP + FP ) and a set of so-called B mea-
sures that instead uses P = TP/S, where S is the
310
total number of predictions made by the system. The
reason why S is not identical with TP + FP is
that partial matches are only counted as FNs (and
not FPs) in order to avoid double penalties. We
do not report the B measures for development test-
ing as they were only introduced for the final eval-
uation and hence were not considered in our sys-
tem optimization. We note though, that the relative-
ranking of participating systems for the primary and
B measures is identical, and that the correlation be-
tween the paired lists of scores is nearly perfect
(r = 0.997).
The paper is structured according to the compo-
nents of our system. Section 2 details the process of
identifying instances of negation through the disam-
biguation of known cue words and affixes. Section 3
describes our hybrid approach to scope resolution,
which utilizes both heuristic and data-driven meth-
ods to select syntactic constituents. Section 4 dis-
cusses our event detection component, which first
applies a classifier to filter out non-factual events
and then uses a learned ranking function to select
events among in-scope tokens. End-to-end results
are presented in Section 5.
2 Cue Detection
Cue identification is based on the light-weight clas-
sification scheme presented by Velldal et al (2012).
By treating the set of cue words as a closed class,
Velldal et al (2012) showed that one could greatly
reduce the number of examples presented to the
learner, and correspondingly the number of fea-
tures, while at the same time improving perfor-
mance. This means that the classifier only attempts
to ?disambiguate? known cue words, while ignoring
any words not observed as cues in the training data.
The classifier applied in the current submission
is extended to also handle morphological or affixal
negation cues, such as the prefix cue in impatience,
the infix in carelessness, and the suffix of colourless.
The negation affixes observed in CDTD are; the pre-
fixes un, dis, ir, im, and in; the infix less (we inter-
nally treat this as the suffixes lessly and lessness);
and the suffix less. Of the total set of 1157 cues in
the training and development data, 192 are affixal.
There are, however, a total of 1127 tokens matching
one of the affix patterns above, and while we main-
tain the closed class assumption also for the affixes,
the classifier will need to consider their status as a
cue or non-cue when attaching to any such token, as
in image, recklessness, and bless.
2.1 Features
In the initial formulation of Velldal (2011), an SVM
classifier was applied using simple n-gram features
over words, both full forms and lemmas, to the
left and right of the candidate cues. In addition to
these token-level features, the classifier we apply
here includes features specifically targeting affixal
cues. The first such feature records character n-
grams from both the beginning and end of the base
that an affix attaches to (up to five positions). For
a context like impossible we would record n-grams
such as {possi, poss, . . .} and {sible, ible, . . .}, and
combine this with information about the affix itself
(im) and the token part-of-speech (?JJ?).
For the second type of affix-specific features, we
try to emulate the effect of a lexicon look-up of the
remaining substring that an affix attaches to, check-
ing its status as an independent base form and its
part-of-speech. In order to take advantage of such
information while staying within the confines of the
closed track, we automatically generate a lexicon
from the training data, counting the instances of each
PoS tagged lemma in addition to n-grams of word-
initial characters (again recording up to five posi-
tions). For a given match of an affix pattern, a fea-
ture will then record these counts for the substring it
attaches to. The rationale for this feature is that the
occurrence of a substring such as un in a token such
as underlying should be less likely as a cue given
that the first part of the remaining string (e.g., derly)
would be an unlikely way to begin a word.
It is also possible for a negation cue to span multi-
ple tokens, such as the (discontinuous) pair neither /
nor or fixed expressions like on the contrary. There
are, however, only 16 instances of such multiword
cues (MWCs) in the entire CDTD. Rather than let-
ting the classifier be sensitive to these corner cases,
we cover such MWC patterns using a small set of
simple post-processing heuristics. A small stop-list
is used for filtering out the relevant words from the
examples presented to the classifier (on, the, etc.).
Note that, in terms of training the final classifiers,
CDTD provides us with a total of 1162 positive and
311
Data set Model Prec Rec F1
CDTD
Baseline 92.25 88.50 90.34
ClassifierI 94.99 95.07 95.03
CDD
Baseline 90.68 84.39 87.42
ClassifierII 93.75 95.38 94.56
CDE
Baseline 87.10 92.05 89.51
ClassifierI 91.42 92.80 92.10
ClassifierII 89.17 93.56 91.31
Table 1: Detecting negation cues using the two clas-
sifiers and the majority-usage baseline.
1100 negative training examples, given our closed-
class treatment of cues.
Before we turn to the results, note that the dif-
ference between the two submitted versions of the
classifier (I and II) only concerns the orders of the
n-grams used for the token-level features.1
2.2 Results
Table 1 presents the results for our cue classifier. As
an informed baseline, we also tried classifying each
word based on its most frequent use as a cue or non-
cue in the training data. (Affixal cue occurrences are
counted by looking at both the affix-pattern and the
base it attaches to, basically treating the entire token
as a cue. Tokens that end up being classified as cues
are then matched against the affix patterns observed
during training in order to correctly delimit the an-
notation of the cue.) This simple majority-usage
approach actually provides a fairly strong baseline,
yielding an F1 of 90.34 on CDTD. Compare this to
the F1 of 95.03 obtained by the classifier on the same
data set. However, when applying the models to the
held-out set, with models estimated over the entire
CDTD, the classifier suffers a slight drop in perfor-
mance, leaving the baseline even more competitive:
While our best performing final cue classifier (I)
achieves F1=92.10, the baseline achieves F1=89.51,
and even outperforms four of the ten cue detection
systems submitted for the shared task (three of the
12 shared task submissions use the same classifier).
1Classifier I records the lemma and full form of the target
token, and lemmas two positions left/right. Classifier II records
the lemma, form, and PoS of the target, full forms three posi-
tions to the left and one to the right, PoS one position right/left,
and lemmas three positions to the right. The affixal-specific fea-
tures are the same for both configurations as described above.
S
NP
EX
There
VP
VBD
was
NP
DT
no
NN
answer
.
.
Figure 1: Example parse tree provided in the data,
highlighting our candidate scope constituents.
Inspecting the predictions of the classifier on
CDD, which comprises a total of 173 gold anno-
tated cues, we find that Classifier I mislabels 11
false positives (FPs) and seven false negatives (FNs).
Of the FPs, we find five so-called false negation
cues (Morante et al, 2011), including three in-
stances of the fixed expression none the less. The
others are affixal cues, of which two are clearly
wrong (underworked, universal) while others might
arguably be due to annotation errors (insuperable,
unhappily, endless, listlessly). Among the FNs, two
are due to MWCs not covered by our heuristics (e.g.,
no more), with the remainder concerning affixes.
3 Constituent-Based Scope Resolution
During the development of our scope resolution sys-
tem we have pursued both a rule-based and data-
driven approach. Both are rooted in the assumption
that the scope of negations corresponds to a syntac-
tically meaningful unit. Our starting point here will
be the syntactic analyses provided by the task or-
ganizers (see Figure 1), generated using the rerank-
ing parser of Charniak and Johnson (2005). How-
ever, as alignment between scope annotations and
syntactic units is not straightforward for all cases,
we apply several exception rules that ?slacken? the
requirements for alignment, as discussed in Sec-
tion 3.1. In Sections 3.2 and 3.3 we detail our
rule-based and data-driven approaches, respectively.
Note that the predictions of the rule-based compo-
nent will be incorporated as features in the learned
model, similarly to the set-up described by Read et
al. (2011). Section 3.4 details the post-processing
we apply to handle cases of discontinuous scope, be-
312
fore Section 3.5 finally presents development results
together with a brief error analysis.
3.1 Constituent Alignment and Slackening
In order to test our initial assumption that syntactic
units correspond to scope annotations, we quantify
the alignment of scopes with constituents in CDT,
excluding 97 negations that do not have a scope.
We find that the initial alignment is rather low at
52.42%. We therefore formulate a set of slacken-
ing heuristics, designed to improve on this alignment
by removing certain constituents at the beginning or
end of a scope. First of all, removing constituent-
initial and -final punctuation improves alignment to
72.83%. We then apply the following slackening
rules, with examples indicating the resulting scope
following slackening (not showing events):
- Remove coordination (CC) and following con-
juncts if the coordination is a rightwards sibling
of an ancestor of the cue and it is not directly
dominated by an NP.
(2) Since we have been so unfortunate as to miss him
and have no notion [. . . ]
- Remove S* to the right of cue, if delimited by
punctuation.
(3) ?There is no other claimant, I presume ??
- Remove constituent-initial SBAR.
(4) If it concerned no one but myself I would not
try to keep it from you.?
- Remove punctuation-delimited NPs.
(5) ?But I can?t forget them, Miss Stapleton,? said I.
- Remove constituent-initial RB, CC, UH,
ADVP or INTJ.
(6) And yet it was not quite the last.
The slackening rules are based on a few obser-
vations. First, scope rarely crosses coordination
boundaries (with the exception of nominal coordi-
nation). Second, scope usually does not cross clause
boundaries (indicated by S/SBAR). Furthermore, ti-
tles and other nominals of address are not included
in the scope. Finally, sentence and discourse adver-
bials are often excluded from the scope. Since these
express semantic distinctions, we approximate this
RB//VP/SBAR if SBAR\WH*
RB//VP/S
RB//S
DT/NP if NP/PP
DT//SBAR if SBAR\WHADVP
DT//S
JJ//ADJPVP/S if S\VP\VB*[@lemma="be"]
JJ/NP/NP if NP\PP
JJ//NP
UH
IN/PP
NN/NP//S/SBAR if SBAR\WHNP
NN/NP//S
CC/SINV
Figure 2: Scope resolution heuristics.
notion syntactically using parts-of-speech and con-
stituent category labels expressing adverbials (RB),
coordinations (CC), various types of interjections
(UH, INTJ) and adverbial phrases (ADVP). We may
note here that syntactic categories are not always
sufficient to express semantic distinctions. Preposi-
tional phrases, for instance, are often used to express
the same type of discourse adverbials, but can also
express a range of other distinctions (e.g., tempo-
ral or locative adverbials), which are included in the
scope. So a slackening rule removing initial PPs was
tried but not found to improve overall alignment.
After applying the above slackening rules the
alignment rate for CDT improves to 86.13%. This
also represents an upper-bound on our performance,
as we will not be able to correctly predict a scope
that does not align with a (slackened) constituent.
3.2 Heuristics Operating over Constituents
The alignment of constituents and scopes reveal con-
sistent patterns and we therefore formulate a set of
heuristic rules over constituents. These are based
on frequencies of paths from the cue to the scope-
aligned constituent for the annotations in CDT, as
well as the annotation guidelines (Morante et al,
2011). The rules are formulated as paths over con-
stituent trees and are presented in Figure 2. The
path syntax is based on LPath (Lai and Bird, 2010).
The rules are listed in order of execution, showing
how more specific rules are consulted before more
general ones. We furthermore allow for some ad-
ditional functionality in the interpretation of rules
by enabling simple constraints that are applied to
the candidate constituent. For example, the rule
RB//VP/SBAR if SBAR\WH* will be activated when
the cue is an adverb having some ancestor VP which
has a parent SBAR, where the SBAR must contain a
WH-phrase among its children.
313
In cases where no rule is activated we use a de-
fault scope prediction, which expands the scope to
both the left and the right of the cue until either the
sentence boundary or a punctuation mark is reached.
The rules are evaluated individually in Section 3.5
below and the rule predictions are furthermore em-
ployed as features for the ranker described below.
3.3 Constituent Ranking
Our data-driven approach to scope resolution in-
volves learning a ranking function over candidate
syntactic constituents. The approach has similari-
ties to discriminative parse selection, except that we
here rank subtrees rather than full parses.
When defining the training data, we begin by se-
lecting negations for which the parse tree contains
a constituent that (after slackening) aligns with the
gold scope. We then select an initial candidate by
selecting the smallest constituent that spans all the
words in the cue, and then generate subsequent can-
didates by traversing the path to the root of the
tree (see Figure 1). This results in a mean ambi-
guity of 4.9 candidate constituents per negation (in
CDTD). Candidates whose projection corresponds
to the gold scope are labeled as correct; all others are
labeled as incorrect. Experimenting with a variety of
feature types (listed in Table 2), we use the imple-
mentation of ordinal ranking in the SVMlight toolkit
(Joachims, 2002) to learn a linear scoring function
for preferring correct candidate scopes.
The most informative feature type is the LPath
from cue, which in addition to recording the full
path from the cue to the candidate constituent
(e.g., the path to the correct candidate in Fig-
ure 1 is no/DT/NP/VP/S), also includes delexicalized
(./DT/NP/VP/S), generalized (no/DT//S), and gen-
eralized delexicalized versions (./DT//S).
Note that the rule prediction feature facilitates a
hybrid approach by recording whether the candidate
matches the boundaries of the scope predicted by the
rules of Section 3.2, as well as the degree of overlap.
3.4 Handling Discontinuous Scope
10.3% of the scopes in the training data are what
(Morante et al, 2011) refer to as discontinuous. This
means that the scope contains two or more parts
which are bridged by tokens other than the cue.
Feature types I II
LPath from cue ? ?
LPath from cue bigrams and trigrams ? ?
LPath from cue to left/right boundary ?
LPath to left/right boundary ?
LPath to root ?
Punctuation to left/right ? ?
Rule prediction ?
Sibling bigrams ?
Size in tokens, relative to sentence (%) ? ?
Surface bigrams ? ?
Tree distance from cue ? ?
Table 2: Features used to describe candidate con-
stituents for scope resolution, with indications of
presence in our two system configurations.
(7) I therefore spent the day at my club and did not
return to Baker Street until evening.
(8) There was certainly no physical injury of any kind.
The sentence in (7) exemplifies a common cause
of scopal discontinuity in the data, namely ellipsis
(Morante et al, 2011). Almost all of these are cases
of coordination, as in example (7) where the cue is
found in the final conjunct (did not return [. . . ]) and
the scope excludes the preceding conjunct(s) (there-
fore spent the day at my club). There are also some
cases of adverbs that are excluded from the scope,
causing discontinuity, as in (8), where the adverb
certainly is excluded from the scope.
In order to deal with discontinuous scopes we for-
mulate two simple post-processing heuristics, which
are applied after rules/ranking: (1) If the cue is in
a conjoined phrase, remove the previous conjuncts
from the scope, and (2) remove sentential adverbs
from the scope (where a list of sentential adverbs
was compiled from the training data).
3.5 Results
Our development procedure evaluated all permuta-
tions of feature combinations, searching for opti-
mal parameters using gold-standard cues. Table 2
indicates which features are included in our two
ranker configurations, i.e., tuning by 10-fold cross-
validation on CDTD (I) vs. a train/test-split for
CDT/CDD(II).
Table 3 lists the results of our scope resolution
approaches applied to gold cues. As a baseline, all
314
Data set Model Prec Rec F1
CDTD
Baseline 98.31 33.18 49.61
Rules 100.00 71.37 83.29
RankerI 100.00 73.55 84.76
CDD
Baseline 100.00 36.31 53.28
Rules 100.00 69.64 82.10
RankerII 100.00 70.24 82.52
CDE
Baseline 96.47 32.93 49.10
Rules 98.73 62.65 76.66
RankerI 98.77 64.26 77.86
RankerII 98.75 63.45 77.26
Table 3: Scope resolution for gold cues using the
two versions of the ranker, also listing the perfor-
mance of the rule-based approach in isolation.
cases are assigned the default scope prediction of the
rule-based approach. On CDTD this results in an F1
of 49.61 (P=98.31, R=33.18); compare to the ranker
in Configuration I on the same data set (F1=84.76,
P=100.00, R=73.55). We note that our different op-
timization procedures do not appear to have made
much difference to the learned ranking functions as
both perform similarly on the held-out data, though
suffering a slight drop in performance compared to
the development results. We also evaluate the rules
and observe that this approach achieves similar held-
out results. This is particularly note-worthy given
that there are only fourteen rules plus the default
scope baseline. Note that, as the rankers performed
better than the rules in isolation on both CDTD and
CDD during development, our final system submis-
sions are based on rankers I and II from Table 3.
We performed a manual error analysis of our
scope resolution system (RankerII) on the basis of
CDD (using gold cues). First, we may note that
parse errors are a common sources of scope res-
olution errors. It is well-known that coordina-
tion presents a difficult construction for syntactic
parsers, and we often find incorrectly parsed coordi-
nate structures among the system errors. Since coor-
dination is used both in the slackening rules and the
analysis of discontinuous scopes, these errors have
clear effects on system performance. We may fur-
ther note that discourse-level adverbials, such as in
the second place in example (9) below, are often in-
cluded in the scope assigned by our system, which
they should not be according to the gold annotation.
(9) But, in the second place, why did you not come at once?
There are also quite a few errors related to the scope
of affixal cues, which the ranker often erroneously
assigns a scope that is larger than simply the base
which the affix attaches to.
4 Event Detection
Our event detection component implements two
stages: First we apply a factuality classifier, and
then we identify negated events2 for those contexts
that have been labeled as factual. We detail the two
stages in order below.
4.1 Factuality Detection
The annotation guidelines of Morante et al (2011)
specify that events should only be annotated for
negations that have a scope and that occur in fac-
tual statements. This means that we can view the
*SEM data sets to implicitly annotate factuality and
non-factuality, and take advantage of this to train an
SVM factuality classifier. We take positive exam-
ples to correspond to negations annotated with both
a scope and an event, while negative examples corre-
spond to scope negations with no event. For CDTD,
this strategy gives 738 positive and 317 negative ex-
amples, spread over a total of 930 sentences. Note
that we do not have any explicit annotation of cue
words for these examples. All we have are instances
of negation that we know to be within a factual or
non-factual context, but the indication of factuality
may typically be well outside the annotated nega-
tion scope. For our experiments here, we therefore
use the negation cue itself as a place-holder for the
abstract notion of context that we are really classi-
fying. Given the limited amount of data, we only
optimize our factuality classifier by 10-fold cross-
validation on CDTD (i.e., the same configuration is
used for submissions I and II).
The feature types we use are all variations over
bag-of-words (BoW) features. We include left- and
right-oriented BoW features centered on the nega-
tion cue, recording forms, lemmas, and PoS, and us-
ing both unigrams and bigrams. The features are ex-
2Note that the annotation guidelines use the term event
rather broadly as referring to a process, action, state, or prop-
erty (Morante et al, 2011).
315
Data set Model Prec Rec F1 Acc
CDTD
Baseline 69.95 100.00 82.32 69.95
Classifier 84.51 96.07 89.92 83.98
CDE
Baseline 69.48 100.00 81.99 69.48
Classifier 77.73 95.91 85.86 78.31
Table 4: Results for factuality detection (using gold
negation cues and scopes). Due to the limited train-
ing data for factuality, the classifier is only opti-
mized by 10-fold cross-validation on CDTD.
tracted from the sentence as a whole, as well as from
a local window of six tokens to each side of the cue.
Table 4 provides results for factuality classifica-
tion using gold-standard cues and scopes.3 We also
include results for a baseline approach that simply
considers all cases to be factual, i.e., the majority
class. In this case precision is identical to accuracy
and recall is 100%. For precision and accuracy we
see that the classifier improves substantially over the
baseline on both data sets, although there is a bit of a
drop in performance when going from the 10-fold to
held-out results. There also seem to be some signs
of overfitting, given that roughly 70% of the training
examples end up as support vectors.
4.2 Ranking Events
Having filtered out non-factual contexts, events are
identified by applying a similar approach to that of
the scope-resolving ranker described in Section 3.3.
In this case, however, we rank tokens as candidates
for events. For simplicity in this first round of de-
velopment we make the assumption that all events
are single words. Thus, the system will be unable to
correctly predict the event in the 6.94% of instances
in CDTD that are multi-word.
We select candidate words from all those marked
as being in the scope (including substrings of to-
kens with affixal cues). This gives a mean ambigu-
ity of 7.8 candidate events per negation (in CDTD).
Then, discarding multi-word training examples, we
use SVMlight to learn a ranking function for identi-
fying events among the candidates.
Table 5 shows the features employed, with in-
3As this is not singled out as a separate subtask in the shared
task itself, these are the only scores in the paper not computed
using the script provided by the organizers.
Feature type I II
Contains affixal cue ?
Following lemma ?
Lemma ? ?
LPath to scope constituent ? ?
LPath to scope constituent bigrams ? ?
Part-of-speech ? ?
Position in scope ? ?
Preceding lemma ? ?
Preceding part-of-speech ? ?
Token distance from cue ? ?
Table 5: Features used to describe candidates for
event detection, with indications of presence in our
two system configurations.
Data set Model Prec Rec F1
CDTD RankerI 91.49 90.83 91.16
CDD RankerII 92.11 91.30 91.70
CDE
RankerI 83.73 83.73 83.73
RankerII 84.94 84.95 84.94
Table 6: Event detection for gold scopes and gold
factuality information.
dications as to their presence in our two configu-
rations (after an exhaustive search of feature com-
binations). The most important feature was LPath
to scope constituent. For example, in Figure 1
the scope constituent is the S root of the tree;
the path that describes the correct candidate is
answer/NN/NP/VP/S. As discussed in Section 3.3,
we also record generalized, delexicalized and gener-
alized delexicalized paths.
Table 6 lists the results of the event ranker applied
to gold-standard cues, scopes, and factuality. For a
comparative baseline, we implemented a keyword-
based approach that simply searches in-scope words
for instances of events previously observed in the
training set, sorted according to descending fre-
quency. This baseline achieves F1=29.44 on CDD.
For comparison, the ranker (II) achieves F1=91.70
on the same data set, as seen in Table 6. We also
see that Configuration II appears to generalize best,
with over 1.2 points improvement over the F1 of I.
An analysis of the event predictions for CDD in-
dicates that the most frequent errors (41.2%) are in-
stances where the ranker correctly predicts part of
the event but our single word assumption is invalid.
Another apparent error is that the system fails to
316
Submission I Submission II
Prec Rec F1 Prec Rec F1
Cues 91.42 92.80 92.10 89.17 93.56 91.31
Scopes 87.43 61.45 72.17 83.89 60.64 70.39
Scope Tokens 81.99 88.81 85.26 75.87 90.08 82.37
Events 60.50 72.89 66.12 60.58 75.00 67.02
Full negation 83.45 43.94 57.57 79.87 45.08 57.63
Cues B 89.09 92.80 90.91 86.97 93.56 90.14
Scopes B 59.30 61.45 60.36 56.55 60.64 58.52
Events B 57.62 72.89 64.36 58.60 75.00 65.79
Full negation B 42.18 43.94 43.04 41.90 45.08 43.43
Table 7: End-to-end results on the held-out data.
predict a main verb for the event, and instead pre-
dicts nouns (17.7% of all errors), modals (17.7%) or
prepositions (11.8%).
5 Held-Out Evaluation
Table 7 presents our final results for both system
configurations on the held-out evaluation data (also
including the B measures, as discussed in the intro-
duction). Comparing submission I and II, we find
that the latter has slightly better scores end-to-end.
However, as seen throughout the paper, the picture is
less clear-cut when considering the isolated perfor-
mance of each component. When ranked according
to the Full Negation measures, our submissions were
placed first and second (out of seven submissions in
the closed track, and twelve submissions total). It
is difficult to compare system performance on sub-
tasks, however, as each component will be affected
by the performance of the previous.
6 Conclusions
This paper has presented two closed-track submis-
sions for the *SEM 2012 shared task on negation
resolution. The systems were ranked first and sec-
ond overall in the shared task end-to-end evaluation,
and the submissions only differ with respect to the
data sets used for parameter tuning. There are four
components in the pipeline: (i) An SVM classifier
for identifying negation cue words and affixes, (ii)
an SVM-based ranker that combines empirical evi-
dence and manually-crafted rules to resolve the in-
sentence scope of negation, (iii) a classifier for de-
termining whether a negation is in a factual or non-
factual context, and (iv) a ranker that determines
(factual) negated events among in-scope tokens.
For future work we would like to try training sepa-
rate classifiers for affixal and token-level cues, given
that largely separate sets of features are effective for
the two cases. The system might also benefit from
sources of information that would place it in the
open track. These include drawing information from
other parsers and formalisms, generating cue fea-
tures from an external lexicon, and using additional
training data for factuality detection, e.g., FactBank
(Saur?? and Pustejovsky, 2009).
From observations on CDTD we note that approx-
imately 14% of scopes will be unresolvable as they
are not aligned with constituents (see Section 3.1).
This can perhaps be tackled by ranking tokens as
candidates for left and right scope boundaries (sim-
ilar to the event ranker in the current work). This
would improve the upper-bound to 100% at the ex-
pense of greatly increasing the number of candi-
dates. However, the strong discriminative power of
our current approach can still be incorporated using
constituent-based features.
Acknowledgments
We thank Roser Morante and Eduardo Blanco for
their work in organizing this shared task and com-
mitment to producing quality data. We also thank
the anonymous reviewers for their feedback. Large-
scale experimentation was carried out with the TI-
TAN HPC facilities at the University of Oslo.
317
References
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the Forty-Third Annual Meeting
of the Association for Computational Linguistics, Ann
Arbor, MI.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the Eighth
ACM International Conference on Knowledge Discov-
ery and Data Mining, Alberta.
Catherine Lai and Steven Bird. 2010. Querying linguis-
tic trees. Journal of Logic, Language and Information,
19:53?73.
Roser Morante and Eduardo Blanco. 2012. *SEM 2012
shared task: Resolving the scope and focus of nega-
tion. In Proceedings of the First Joint Conference on
Lexical and Computational Semantics, Montreal.
Roser Morante and Walter Daelemans. 2012.
ConanDoyle-neg: Annotation of negation in Conan
Doyle stories. In Proceedings of the Eighth Interna-
tional Conference on Language Resources and Evalu-
ation, Istanbul.
Roser Morante, Sarah Schrauwen, and Walter Daele-
mans. 2011. Annotation of negation cues and their
scope: Guidelines v1.0. Technical report, Univer-
sity of Antwerp. CLIPS: Computational Linguistics
& Psycholinguistics technical report series.
Jonathon Read, Erik Velldal, Stephan Oepen, and Lilja
?vrelid. 2011. Resolving speculation and negation
scope in biomedical articles using a syntactic con-
stituent ranker. In Proceedings of the Fourth Inter-
national Symposium on Languages in Biology and
Medicine, Singapore.
Roser Saur?? and James Pustejovsky. 2009. Factbank:
a corpus annotated with event factuality. Language
Resources and Evaluation, 43(3):227?268.
Erik Velldal, Lilja ?vrelid, Jonathon Read, and Stephan
Oepen. 2012. Speculation and negation: Rules,
rankers and the role of syntax. Computational Lin-
guistics, 38(2).
Erik Velldal. 2011. Predicting speculation: A simple dis-
ambiguation approach to hedge detection in biomedi-
cal literature. Journal of Biomedical Semantics, 2(5).
318
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 319?327,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UiO2: Sequence-Labeling Negation Using Dependency Features
Emanuele Lapponi Erik Velldal Lilja ?vrelid Jonathon Read
University of Oslo, Department of Informatics
{emanuel,erikve,liljao,jread}@ifi.uio.no
Abstract
This paper describes the second of two sys-
tems submitted from the University of Oslo
(UiO) to the 2012 *SEM Shared Task on re-
solving negation. The system combines SVM
cue classification with CRF sequence labeling
of events and scopes. Models for scopes and
events are created using lexical and syntactic
features, together with a fine-grained set of la-
bels that capture the scopal behavior of certain
tokens. Following labeling, negated tokens are
assigned to their respective cues using simple
post-processing heuristics. The system was
ranked first in the open track and third in the
closed track, and was one of the top perform-
ers in the scope resolution sub-task overall.
1 Introduction
Negation Resolution (NR) is the task of determin-
ing, for a given sentence, which tokens are affected
by a negation cue. The data set most prominently
used for the development of systems for automatic
NR is the BioScope Corpus (Vincze et al, 2008), a
collection of clinical reports and papers in the bio-
medical domain annotated with negation and specu-
lation cues and their scopes. The data sets released
in conjunction with the 2012 shared task on NR
hosted by The First Joint Conference on Lexical and
Computational Semantics (*SEM 2012) are com-
prised of the following negation annotated stories of
Conan Doyle (CD): a training set of 3644 sentences
drawn from The Hound of the Baskervilles (CDT), a
development set of 787 sentences taken from Wis-
teria Lodge (CDD; we will refer to the combina-
tion of CDT and CDD as CDTD), and a held-out
test set of 1089 sentences from The Cardboard Box
and The Red Circle (CDE). In these sets, the con-
cept of negation scope extends on the one adopted
in the BioScope corpus in several aspects: Nega-
tion cues are not part of the scope, morphological
(affixal) cues are annotated and scopes can be dis-
continuous. Moreover, in-scope states or events are
marked as negated if they are factual and presented
as events that did not happen (Morante and Daele-
mans, 2012). Examples (1) and (2) below are exam-
ples of affixal negation and discontinuous scope re-
spectively: The cues are bold, the tokens contained
within their scopes are underlined and the negated
event is italicized.
(1) Since we have been so unfortunate as to miss him [. . . ]
(2) If he was in the hospital and yet not on the staff he could
only have been a house-surgeon or a house-physician: lit-
tle more than a senior student.
Example (2) has no negated events because the sen-
tence is non-factual.
The *SEM shared task thus comprises three sub-
tasks: cue identification, scope resolution and event
detection. It is furthermore divided into two separate
tracks: one closed track, where only the data sup-
plied by the organizers (word form, lemma, PoS-tag
and syntactic constituent for each token) may be em-
ployed, and an open track, where participants may
employ any additional tools or resources.
Pragmatically speaking, a token can be either out
of scope or assigned to one or more of the three re-
maining classes: negation cue, in scope and negated
event. Additionally, in-scope tokens and negated
events are paired to the cues they are negated by.
319
Our system achieves this by remodeling the task as a
sequence labeling task. With annotations converted
to sequences of labels, we train a Conditional Ran-
dom Field (CRF) classifier with a range of different
feature types, including features defined over depen-
dency graphs. This article presents two submissions
for the *SEM shared task, differing only with re-
spect to how these dependency graphs were derived.
For our open track submission, the dependency rep-
resentations are produced by a state-of-the-art de-
pendency parser, whereas the closed track submis-
sion employs dependencies derived from the con-
stituent analyses supplied with the shared task data
sets through a process of constituent-to-dependency
conversion. In both systems, labeling of test data is
performed in two stages. First, cues are detected us-
ing a token classifier,1 and secondly, scope and event
resolution is achieved by post-processing the output
of the sequence labeler.
The two systems described in this paper have been
developed using CDT for training and CDD for test-
ing, and differ only with regard to the source of syn-
tactic information. All reported scores are generated
using an evaluation script provided by the task or-
ganizers. In addition to providing a full end-to-end
evaluation, the script breaks down results with re-
spect to identification of cues, events, scope tokens,
and two variants of scope-level exact match; one re-
quiring exact match also of cues and another only
partial cue match. For our system these two scope-
level scores are identical and so are not duplicated
in our reporting. Additionally we chose not to opti-
mize for the scope tokens measure, and hence this is
also not reported as a development result.
Note also that the official evaluation actually in-
cludes two different variants of the metrics men-
tioned above; a set of primary measures with pre-
cision computed as P=TP/(TP+FP) and a set of B
measures where precision is rather computed as
P=TP/SYS, where SYS is the total number of pre-
dictions made by the system. The reason why SYS is
not identical with TP+FP is that partial matches are
1Note that the cue classifier applied in the current paper is
the same as that used in the other shared task submission from
the University of Oslo (Read et al, 2012), and the two system
descriptions will therefore have much overlap on this particular
point. For all other components the architectures of the two
system are completely different, however.
only counted as FNs (and not FPs) in order to avoid
double penalties. We do not report the B measures
for development testing as they were introduced for
the final evaluation and hence were not considered
in our system optimization. We note though, that the
relative-ranking of participating systems for the pri-
mary and B measures is identical, and that the cor-
relation between the paired lists of scores is nearly
perfect (r=0.997).
The rest of the paper is structured as follows.
First, the cue classifier, its features and results are
described in Section 2. Section 3 presents the sys-
tem for scope and event resolution and details differ-
ent features, the model-internal representation used
for sequence-labeling, as well as the post-processing
component. Error analyses for the cue, scope and
event components are provided in the respective sec-
tions. Section 4 and 5 provide developmental and
held-out results, respectively. Finally, we provide
conclusions and some reflections regarding future
work in Section 6.
2 Cue detection
Identification of negation cues is based on the light-
weight classification scheme presented by Velldal et
al. (2012). By treating the set of cue words as a
closed class, Velldal et al (2012) showed that one
could greatly reduce the number of examples pre-
sented to the learner, and correspondingly the num-
ber of features, while at the same time improving
performance. This means that the classifier only at-
tempts to ?disambiguate? known cue words while
ignoring any words not observed as cues in the train-
ing data.
The classifier applied in the current submission
is extended to also handle affixal negation cues,
such as the prefix cue in impatience, the infix in
carelessness, and the suffix of colourless. The types
of negation affixes observed in CDTD are; the pre-
fixes un, dis, ir, im, and in; the infix less (we inter-
nally treat this as the suffixes lessly and lessness);
and the suffix less. Of the total number of 1157 cues
in the training and development set, 192 are affixal.
There are, however, a total of 1127 tokens matching
one of the affix patterns above, and while we main-
tain the closed class assumption also for the affixes,
the classifier will need to consider its status as a cue
320
or non-cue when attaching to any such token, like
for instance image, recklessness, and bless.
2.1 Features
In the initial formulation of Velldal (2011), an SVM
classifier was trained using simple n-gram features
over words, both full forms and lemmas, to the left
and right of the candidate cues. In addition to these
token-level features, the classifier we apply here in-
cludes some features specifically targeting morpho-
logical or affixal cues. The first such feature records
character n-grams from both the beginning and end
of the base that an affix attaches to (up to five po-
sitions). For a context like impossible we would
record n-grams such {possi, poss, pos, . . .} and
{sible, ible, ble, . . .}, and combine this with infor-
mation about the affix itself (im) and the token part-
of-speech (?JJ?).
For the second feature type targeting affix cues
we try to emulate the effect of a lexicon look-up
of the remaining substring that an affix attaches to,
checking its status as an independent base form and
its part-of-speech. In order to take advantage of
such information while staying within the confines
of the closed track, we automatically generate a lex-
icon from the training data, counting the instances
of each PoS tagged lemma in addition to n-grams
of word-initial characters (again recording up to five
positions). For a given match of an affix pattern, a
feature will then record the counts from this lexicon
for the substring it attaches to. The rationale for this
feature is that the occurrence of a substring such as
un in a token such as underlying should be consid-
ered more unlikely to be a cue given that the first
part of the remaining string (e.g., derly) would be an
unlikely way to begin a word.
Note that, it is also possible for a negation cue
to span multiple tokens, such as the (discontinuous)
pair neither / nor or fixed expressions like on the
contrary. There are, however, only 16 instances of
such multiword cues (MWCs) in the entire CDTD.
Rather than letting the classifier be sensitive to these
corner cases, we cover such MWC patterns using
a small set of simple post-processing heuristics. A
small stop-list is used for filtering out the relevant
words from the examples presented to the classifier
(on, the, etc.).
Data set Model Prec Prec F1
CDD
Baseline 90.68 84.39 87.42
Classifier 93.75 95.38 94.56
CDE
Baseline 87.10 92.05 89.51
Classifier 89.17 93.56 91.31
Table 1: Cue classification results for the final classifier
and the majority-usage baseline, showing test scores for
the development set (training on CDT) and the final held-
out set (training on CDTD).
2.2 Results
Table 1 presents results for the cue classifier. While
the classifier configuration was optimized against
CDD, the model used for the final held-out testing
is trained on the entire CDTD, which (given our
closed-class treatment of cues) provides a total of
1162 positive and 1100 negative training examples.
As an informed baseline, we also tried classifying
each word based on its most frequent use as cue
or non-cue in the training data. (Affixal cue oc-
currences are counted by looking at both the affix-
pattern and the base it attaches to, basically treating
the entire token as a cue. Tokens that end up be-
ing classified as cues are then matched against the
affix patterns observed during training in order to
correctly delimit the annotation of the cue.) This
simple majority-usage approach actually provides a
fairly strong baseline, yielding an F1 of 87.42 on
CDD (P=90.68, R=84.39). Compare this to the F1 of
94.56 obtained by the classifier on the same data set
(P=93.75, R=95.38). However, when applying the
models to the held-out set, with models estimated
over the entire CDTD, the baseline seems to able
to make good use of the additional data and proves
to be even more competitive: While our final cue
classifier achieves F1=91.31, the baseline achieves
F1=89.51, almost two percentage points higher than
its score on the development data, and even outper-
forms four of the ten cue detection systems submit-
ted for the shared task (three of the 12 shared task
submissions use the same classifier).
When inspecting the predictions of our final cue
classifier on CDD, comprising a total of 173 gold
annotated cues, we find that our system mislabels
11 false positives (FPs) and 7 false negatives (FNs).
321
Of the FPs, we find five so-called false negation cues
(Morante et al, 2011), including three instances of
none in the fixed expression none the less. The
others are affixal cues, of which two are clearly
wrong (underworked, universal) while others might
arguably be due to annotation errors (insuperable,
unhappily, endless, listlessly). Among the FNs, two
are due to MWCs not covered by our heuristics (e.g.,
no more), while the remaining errors concern af-
fixes, including one in an interesting context of dou-
ble negation; not dissatisfied.
3 Scope and event resolution
In this work, we model negation scope resolution
as a special instance of the classical IOB (Inside,
Outside, Begin) sequence labeling problem, where
negation cues are labeled to be sequence starters and
scopes and events as two different kinds of chunks.
CRFs allow the computation of p(X|Y), whereX is
a sequence of labels andY is a sequence of observa-
tions, and have already been shown to be efficient in
similar, albeit less involved, tasks of negation scope
resolution (Morante and Daelemans, 2009; Councill
et al, 2010). We employ the CRF implementation in
the Wapiti toolkit, using default settings (Lavergne
et al, 2010). A number of features were used to
create the models. In addition to the information
provided for each token in the CD corpus (lemma,
part of speech and constituent), we extracted both
left and right token distance to the closest negation
cue. Features were expanded to include forward and
backward bigrams and trigrams on both token and
PoS level, as well as lexicalized PoS unigrams and
bigrams2. Table 2 presents a complete list of fea-
tures. The more intricate, dependency-based fea-
tures are presented in Section 3.1, while the labeling
of both scopes and events is detailed in Section 3.2.
3.1 Dependency-based features
For the system submitted to the closed track, the syn-
tactic representations were converted to dependency
representations using the Stanford dependency con-
verter, which comes with the Stanford parser (de
Marneffe et al, 2006).3 These dependency represen-
2By lexicalized PoS we mean an instance of a PoS-Tag in
conjunction with the sentence token.
3Note that the converter was applied directly to the phrase-
structure trees supplied with the negation data sets, and the
General features
Token
Lemma
PoS unigram
Forward token bigram and trigram
Backward token bigram and trigram
Forward PoS trigram
Backward PoS trigram
Lexicalized PoS
Forward Lexicalized PoS bigram
Backward Lexicalized PoS bigram
Constituent
Dependency relation
First order head PoS
Second order head PoS
Lexicalized dependency relation
PoS-disambiguated dependency relation
Cue-dependent features
Token distance
Directed dependency distance
Bidirectional dependency distance
Dependency path
Lexicalized dependency path
Table 2: List of features used to train the CRF models.
tations result from a conversion of Penn Treebank-
style phrase structure trees, combining ?classic? head
finding rules with rules that target specific linguistic
constructions, such as passives or attributive adjec-
tives. The so-called basic format provides a depen-
dency graph which is a directed tree, see Figure 1
for an example.
For the open track submission we used Maltparser
(Nivre et al, 2006) with its pre-trained parse model
for English.4 The parse model has been trained on a
conversion of sections 2-21 of the Wall Street Jour-
nal section of the Penn Treebank to Stanford depen-
dencies, augmented with data from Question Bank.
The parser was applied to the negation data, using
the word tokens and supplied parts-of-speech as in-
put to the parser.
The features extracted via the dependency graphs
aim at modeling the syntactic relationship between
each token and the closest negation cue. Token dis-
tance was therefore complemented with two variants
of dependency distance from each token to the lexi-
Stanford parser was not used to parse the data.
4The pre-trained model is available from maltparser.org
322
we   have  never  gone  out  without  keeping  a  sharp  watch  ,  and  no  one  could  have  escaped  our  notice  .  "
nsubj
aux
neg
conj
cc
punct
prep
part
pcomp
dobj
det
amod
dep
nsubj
aux
aux
punct
punct
dobj
poss
root
ann. 1:
ann. 2:
ann. 3:
cue
cue
cue
labels: CUE CUE CUEN N E E
N N
N N E N N N NS O S O
N
Figure 1: A sentence from the CD corpus showing a dependency graph and the annotation-to-label conversion.
cally closest cue, Directed Distance (DD) and Bidi-
rectional Distance (BD). DD is extracted by follow-
ing the reversed, directed edges from token X to the
cue. If there is no such path, the value of the feature
is -1. BD uses the Dijkstra shortest path algorithm
on an undirected representation of the graph. The
latter feature proved to be more effective than the
former when not used together; using them in con-
junction seemed to confuse the model, thus the fi-
nal model utilizes only BD. We furthermore use the
Dependency Graph Path (DGP) as a feature. This
feature was inspired by the Parse Tree Path feature
presented in Gildea and Jurafsky (2002) in the con-
text of Semantic Role Labeling. It represents the
path traversed from each token to the cue, encod-
ing both the dependency relations and the direction
of the arc that is traversed: for instance, the rela-
tion between our and no in Figure 1 is described as
 poss  dobj  nsubj  det. Like Councill et
al. (2010), we also encode the PoS of the first and
second order syntactic head of each token. For the
token no in Figure 1, for instance, we record the PoS
of one and escaped, respectively.
3.2 Model-internal representation
The token-wise annotations in the CD corpus con-
tain multiple layers of information. Tokens may or
may not be negation cues and they can be either in
or out of scope; in-scope tokens may or may not
be negated events, and are associated with each of
the cues they are negated by. Moreover, scopes may
be (partially) overlapping, as in Figure 1, where the
PoS # S PoS # MCUE PoS # CUE
punctuation 1492 JJ 268 RB 1026
CC 52 RB 28 DT 296
IN + TO 46 NN 16 NN 146
RB 38 NN 4 UH 118
PRP 32 IN 2 IN 64
rest 118 rest ? rest 38
Table 3: Frequency distribution of parts of speech over
the S, MCUE and CUE labels in CDTD.
scope of without is contained within the scope of
never. We convert this representation internally by
assigning one of six labels to each token: O, CUE,
MCUE, N, E and S, for out-of-scope, cue, mor-
phological (affixal) cue, in-scope, event and nega-
tion stop respectively. The CUE, O, N and E la-
bels parallel the IOB chunking paradigm and are
eventually translated in the final annotations by our
post-processing component. MCUE and S extend
the label set to account for the specific behavior of
the tokens they are associated with. The rationale
behind the separation of cues in two classes is the
pronounced differences between the PoS frequency
distributions of standard versus morphological cues.
Table 3 presents the frequency distribution of PoS-
tags over the different cue types in CDTD and shows
that, unsurprisingly, the majority class for morpho-
logical cues is adjectives, which typically generate
different scope patterns compared to the majority
class for standard cues. The S label, a special in-
stance of an out-of-scope token, is defined as the
323
first non-cue, out-of-scope token to the right of one
labeled with N, and targets mostly punctuation.
After some experimentation with joint labeling of
scopes and events, we opted for separation of the
two models, hence training separate models for the
two tasks of scope resolution and event detection.
In the model for scopes, all E labels are switched
to N; conversely, Ns become Os in the event model.
Given the nature of the annotations, the predictions
provided by the model for events serve a double pur-
pose: finding the negated token in a sentence and
deciding whether a sentence is factual or not. The
outputs of the two classifiers are merged during post-
processing.
3.3 Post-processing
A simple, heuristics-based algorithm was applied
to the output of the labelers in order to pair each
in-scope token to its negation cue(s) and determine
overlaps. Our algorithm works by first determining
the overlaps among negation cues. Cue A negates
cue B if the following conditions are met:
? B is to the right of A.
? There are no tokens labeled with S between A
and B.
? Token distance between A and B does not ex-
ceed 10.
In the example in Figure 1, the overlapping condi-
tion holds for never and without but not for without
and no, because of the punctuation between them.
The token distance threshold of 10 was determined
empirically on CDT. In order to assign in-scope to-
kens to their respective cue, tokens labeled with N
are treated as follows:
? Assign each token T to the closest negation cue
A with no S-labeled tokens or punctuation sep-
arating it from T.
? If A was found to be negated by cue B, assign
T to B as well.
? If T is labeled with E by the event classifier,
mark it as an event.
F1
Configuration Closed Open
(A) O, N, CUE, MCUE, E, S 64.85 66.41Dependency Features
(B) O, N, CUE, MCUE, E, S 59.35 59.35No Dependency Features
(C) O, N, CUE, E 62.69 63.24Dependency Features
(D) O, N, CUE, E 56.44 56.44No Dependency Features
Table 4: Full negation results on CDD with gold cues.
This algorithm yields the correct annotations for
the example in Figure 1; when applied to label se-
quences originating from the gold scopes in CDD,
the reported F1 is 95%. We note that this loss of in-
formation could have been avoided by presenting the
CRF with a version of a sentence for each negation
cue. Then, when labeling new sentences, the model
could be applied repeatedly (based on the number of
cues provided by the cue detection system). How-
ever, training with multiple instances of the same
sentence could result in a dilution of the evidence
needed for scope labeling; this remains to be inves-
tigated in future work.
4 Development results
To investigate the effects of the augmented set of la-
bels and that of dependency features comparatively,
we present four different configurations of our sys-
tem in Table 4, using F1 for the stricter score that
counts perfect-match negation resolution for each
negation cue. Comparing (B) and (D), we observe
that explicitly encoding significant tokens with extra
labels does improve the performance of the classi-
fier. Comparing (A) to (B) and (C) to (B) shows the
effect of the dependency features with and without
the augmented set of labels. With (A) being our top
performing system and (D) a kind of internal base-
line, we observe that the combined effects of the la-
bels and dependency features is beneficial, with a
margin of about 8 and 10 percentage points for our
closed and open track systems respectively.
Table 5 presents the results for scope resolution on
CDD with gold cues. Interestingly, the constituent
324
Closed Open
Prec Rec F1 Prec Rec F1
Scopes 100.00 70.24 82.52 100.00 66.67 80.00
Scope Tokens 94.69 82.16 87.98 90.64 81.36 85.75
Negated 82.47 72.07 76.92 83.65 77.68 80.55
Full negation 100.00 47.98 64.85 100.00 49.71 66.41
Table 5: Results for scope resolution on CDD with gold cues.
trees converted to Stanford dependencies used in the
closed track outperform the open system employing
Maltparser on scopes, while for negated events the
latter is over 5 percentage points better than the for-
mer, as shown in Table 5.
4.1 Error analysis
We performed a manual error analysis of the scope
resolution on the development data using gold cue
information. Since our system does not deal specifi-
cally with discontinuous scopes, and seeing that we
are employing a sequence classifier with a fairly lo-
cal window, we are not surprised to find that a sub-
stantial portion of the errors are caused by discon-
tinuous scopes. In fact, in our closed track system,
these errors amount to 34% of the total number of
errors. Discontinuous scopes, as in (3) below, ac-
count for 9.3% of all scopes in CDD and the closed
task system does not analyze any of these correctly,
whereas the open system correctly analyzes one dis-
continuous scope.
(3) I therefore spent the day at my club and did not
return to Baker Street until evening.
A similar analysis with respect to event detection
on gold scope information indicated that errors are
mostly due to either predicting an event for a non-
factual context (false positive) or not predicting an
event for a factual context (false negative), i.e., there
are relatively few instances of predicting the wrong
token for a factual context (which result in both a
false negative and a false positive). This suggests
that the CRF has learned what tokens should be la-
beled as an event for a negation, but has not learned
so well how to determine whether the negation is
factual or non-factual. In this respect it may be that
incorporating information from a separate and dedi-
cated component for factuality detection ? as in the
system of Read et al (2012) ? could yield improve-
ments for the CRF event model.
5 Held-out evaluation
Final results on held-out data for both closed and
open track submissions are reported in Table 6. For
the final run, we trained our systems on CDTD. We
observe a similar relative performance to our devel-
opment results, with the open track system outper-
forming the closed track one, albeit by a smaller
margin than what we saw in development. We are
also surprised to see that despite not addressing dis-
continuous scopes directly, our system obtained the
best score on scope resolution (according to the met-
ric dubbed ?Scopes (cue match)?).
6 Conclusions and future work
This paper has provided an overview of our system
submissions for the *SEM 2012 shared task on re-
solving negation. This involves the subtasks of iden-
tifying negations cues, identifying the in-sentence
scope of these cues, as well as identifying negated
(and factual) events. While a simple SVM token
classifier is applied for the cue detection task, we ap-
ply CRF sequence classifiers for predicting scopes
and events. For the CRF models we experimented
with a fine-grained set of labels and a wide range of
feature types, drawing heavily on information from
dependency structures. We have detailed two dif-
ferent system configurations ? one submitted for
the open track and another for the closed track ?
and the two configurations only differ with respect
to the source used for the dependency parses: For
the closed track submission we simply converted
the constituent structures provided in the shared task
data to Stanford dependencies, while for the open
track we apply the Maltparser. For the end-to-end
evaluation, our submission was ranked first in the
open track and third in the closed track. The system
also had the best performance for each individual
sub-task in the open track, as well as being among
325
Closed Open
Prec Rec F1 Prec Rec F1
Cues 89.17 93.56 91.31 89.17 93.56 91.31
Scopes 85.71 62.65 72.39 85.71 62.65 72.39
Scope Tokens 86.03 81.55 83.73 82.25 82.16 82.20
Negated 68.18 52.63 59.40 66.90 57.40 61.79
Full negation 78.26 40.91 53.73 78.72 42.05 54.82
Cues B 86.97 93.56 90.14 86.97 93.56 90.14
Scopes B 59.32 62.65 60.94 59.54 62.65 61.06
Negated B 67.16 52.63 59.01 63.82 57.40 60.44
Full negation B 38.03 40.91 39.42 39.08 42.05 40.51
Table 6: End-to-end results on the held-out data.
the top-performers on the scope resolution sub-task
across both tracks.
Due to time constraints we were not able to di-
rectly address discontinuous scopes in our system.
For future work we plan on looking for ways to
tackle this problem by taking advantage of syntac-
tic information, both in the classification and in the
post-processing steps. We are also interested in de-
veloping the CRF-internal label-set to include more
informative labels. We also want to test the sys-
tem design developed for this task on other corpora
annotated for negation (or other related phenom-
ena such as speculation), as well as perform extrin-
sic evaluation of our system as a sub-component to
other NLP tasks such as sentiment analysis or opin-
ion mining. Lastly, we would like to try training
separate classifiers for affixal and token-level cues,
given that largely separate sets of features are effec-
tive for the two cases.
Acknowledgements
We thank colleagues at the University of Oslo, and
in particular Johan Benum Evensberget and Arne
Skj?rholt for fruitful discussions and suggestions.
We also thank the anonymous reviewers for their
helpful feedback.
References
Isaac G. Councill, Ryan McDonald, and Leonid Ve-
likovich. 2010. What?s great and what?s not: Learn-
ing to classify the scope of negation for improved sen-
timent analysis. In Proceedings of the Workshop On
Negation and Speculation in Natural Language Pro-
cessing, pages 51?59.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of the 5th International Conference on Lan-
guage Resources and Evaluation.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistic,
28(3):245?288.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 504?513. As-
sociation for Computational Linguistics.
Roser Morante and Walter Daelemans. 2009. A met-
alearning approach to processing the scope of nega-
tion. In CoNLL ?09: Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning, pages 21?29. Association for Com-
putational Linguistics.
Roser Morante and Walter Daelemans. 2012.
ConanDoyle-neg: Annotation of negation in Conan
Doyle stories. In Proceedings of the Eighth Interna-
tional Conference on Language Resources and Evalu-
ation, Istanbul.
Roser Morante, Sarah Schrauwen, and Walter Daele-
mans. 2011. Annotation of negation cues and their
scope: Guidelines v1.0. Technical report, Univer-
sity of Antwerp. CLIPS: Computational Linguistics
& Psycholinguistics technical report series.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
Parser: A data-driven parser-generator for dependency
parsing. In Proceedings of the Fifth International
Conference on Language Resources and Evaluation,
pages 2216?2219.
Jonathon Read, Erik Velldal, Lilja ?vrelid, and Stephan
Oepen. 2012. UiO1: Constituent-based discrimina-
tive ranking for negation resolution. In Proceedings
of the First Joint Conference on Lexical and Computa-
tional Semantics, Montreal. Submission under review.
326
Erik Velldal, Lilja ?vrelid, Jonathon Read, and Stephan
Oepen. 2012. Speculation and negation: Rules,
rankers and the role of syntax. Computational Lin-
guistics, 38(2).
Erik Velldal. 2011. Predicting speculation: A simple dis-
ambiguation approach to hedge detection in biomedi-
cal literature. Journal of Biomedical Semantics, 2(5).
Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,
Gyo?rgy Mo?ra, and Ja?nos Csirik. 2008. The Bio-
Scope corpus: Biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9 (Suppl. 11).
327
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 25?32
Manchester, August 2008
Linguistic features in data-driven dependency parsing
Lilja ?vrelid
NLP-unit, Dept. of Swedish
University of Gothenburg
Sweden
lilja.ovrelid@svenska.gu.se
Abstract
This article investigates the effect of a set
of linguistically motivated features on ar-
gument disambiguation in data-driven de-
pendency parsing of Swedish. We present
results from experiments with gold stan-
dard features, such as animacy, definite-
ness and finiteness, as well as correspond-
ing experiments where these features have
been acquired automatically and show
significant improvements both in overall
parse results and in the analysis of specific
argument relations, such as subjects, ob-
jects and predicatives.
1 Introduction
Data-driven dependency parsing has recently re-
ceived extensive attention in the parsing commu-
nity and impressive results have been obtained for
a range of languages (Nivre et al, 2007). Even
with high overall parsing accuracy, however, data-
driven parsers often make errors in the assign-
ment of argument relations such as subject and
object and the exact influence of data-derived fea-
tures on the parsing accuracy for specific linguistic
constructions is still relatively poorly understood.
There are a number of studies that investigate the
influence of different features or representational
choices on overall parsing accuracy, (Bod, 1998;
Klein and Manning, 2003). There are also attempts
at a more fine-grained analysis of accuracy, target-
ing specific linguistic constructions or grammati-
cal functions (Carroll and Briscoe, 2002; Ku?bler
and Prokic?, 2006; McDonald and Nivre, 2007).
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
But there are few studies that combine the two per-
spectives and try to tease apart the influence of dif-
ferent features on the analysis of specific construc-
tions, let alne motivated by a thorough linguistic
analysis.
In this paper, we investigate the influence of a
set of linguistically motivated features on parse re-
sults for Swedish, and in particular on the analysis
of argument relations such as subjects, objects and
subject predicatives. Motivated by an error anal-
ysis of the best performing parser for Swedish in
the CoNLL-X shared task, we extend the feature
model employed by the parser with a set of lin-
guistically motivated features and go on to show
how these features may be acquired automatically.
We then present results from corresponding parse
experiments with automatic features.
The rest of the paper is structured as follows. In
section 2 we present relevant properties of Swedish
morphosyntax, as well as the treebank and parser
employed in the experiments. Section 3 presents
an error analysis of the baseline parser and we go
on to motivate a set of linguistic features in sec-
tion 4, which are employed in a set of experiments
with gold standard features, discussed in section
5. Section 6 presents the automatic acquisition of
these features, with a particular focus on animacy
classification and in section 7 we report parse ex-
periments with automatic features.
2 Parsing Swedish
Before we turn to a description of the treebank
and the parser used in the experiments, we want to
point to a few grammatical properties of Swedish
that will be important in the following:
Verb second (V2) Swedish is, like the majority of
Germanic languages a V2-language; the fi-
nite verb always resides in second position in
25
declarative main clauses.
Word order variation Pretty much any con-
stituent may occupy the sentence-initial po-
sition, but subjects are most common.
Limited case marking Nouns are only inflected
for genitive case. Personal pronouns dis-
tinguish nominative and accusative case, but
demonstratives and quantifying pronouns are
case ambiguous (like nouns).
2.1 Treebank: Talbanken05
Talbanken05 is a Swedish treebank converted to
dependency format, containing both written and
spoken language (Nivre et al, 2006a).1 For each
token, Talbanken05 contains information on word
form, part of speech, head and dependency rela-
tion, as well as various morphosyntactic and/or
lexical semantic features. The nature of this ad-
ditional information varies depending on part of
speech:
NOUN: definiteness, animacy, case (?/GEN)
PRO: animacy, case (?/ACC)
VERB: tense, voice (?/PA)
2.2 Parser: MaltParser
We use the freely available MaltParser,2 which
is a language-independent system for data-driven
dependency parsing. MaltParser is based on
a deterministic parsing strategy, first proposed
by Nivre (2003), in combination with treebank-
induced classifiers for predicting the next parsing
action. Classifiers can be trained using any ma-
chine learning approach, but the best results have
so far been obtained with support vector machines,
using LIBSVM (Chang and Lin, 2001). Malt-
Parser has a wide range of parameters that need to
be optimized when parsing a new language. As
our baseline, we use the settings optimized for
Swedish in the CoNLL-X shared task (Nivre et al,
2006b), where this parser was the best perform-
ing parser for Swedish. The only parameter that
will be varied in the later experiments is the fea-
ture model used for the prediction of the next pars-
ing action. Hence, we need to describe the feature
model in a little more detail.
MaltParser uses two main data structures, a
stack (S) and an input queue (I), and builds a de-
pendency graph (G) incrementally in a single left-
1The written sections of the treebank consist of profes-
sional prose and student essays and amount to 197,123 run-
ning tokens, spread over 11,431 sentences.
2http://w3.msi.vxu.se/users/nivre/research/MaltParser.html
FORM POS DEP FEATS
S:top + + + +
S:top+1 +
I:next + + +
I:next?1 + +
I:next+1 + + +
I:next+2 +
G: head of top + +
G: left dep of top +
G: right dep of top +
G: left dep of next + + +
G: left dep of head of top +
G: left sibling of right dep of top +
G: right sibling of left dep of top + +
G: right sibling of left dep of next + +
Table 1: Baseline and extended (FEATS) feature
model for Swedish; S: stack, I: input, G: graph;
?n = n positions to the left(?) or right (+)
to-right pass over the input. The decision that
needs to be made at any point during this deriva-
tion is (a) whether to add a dependency arc (with
some label) between the token on top of the stack
(top) and the next token in the input queue (next),
and (b) whether to pop top from the stack or push
next onto the stack. The features fed to the classi-
fier for making these decisions naturally focus on
attributes of top, next and neighbouring tokens in
S, I or G. In the baseline feature model, these at-
tributes are limited to the word form (FORM), part
of speech (POS), and dependency relation (DEP) of
a given token, but in later experiments we will add
other linguistic features (FEATS). The baseline fea-
ture model is depicted as a matrix in Table 1, where
rows denote tokens in the parser configuration (de-
fined relative to S, I and G) and columns denote
attributes. Each cell containing a + corresponds to
a feature of the model.
3 Baseline and Error Analysis
The written part of Talbanken05 was parsed em-
ploying the baseline feature model detailed above,
using 10-fold cross validation for training and test-
ing. The overall result for unlabeled and labeled
dependency accuracy is 89.87 and 84.92 respec-
tively.3
Error analysis shows that the overall most fre-
quent errors in terms of dependency relations in-
volve either various adverbial relations, due to PP-
attachment ambiguities and a large number of ad-
3Note that these results are slightly better than the official
CoNLL-X shared task scores (89.50/84.58), which were ob-
tained using a single training-test split, not cross-validation.
Note also that, in both cases, the parser input contained gold
standard part-of-speech tags.
26
Gold Sys before after Total
SS OO 103 (23.1%) 343 (76.9%) 446 (100%)
OO SS 103 (33.3%) 206 (66.7%) 309 (100%)
Table 2: Position relative to verb for confused sub-
jects and objects
verbial labels, or the argument relations, such as
subjects, direct objects, formal subjects and sub-
ject predicatives. In particular, confusion of argu-
ment relations are among the most frequent error
types with respect to dependency assignment.4
Swedish exhibits some ambiguities in word or-
der and morphology which follow from the proper-
ties discussed above. We will exemplify these fac-
tors through an analysis of the errors where sub-
jects are assigned object status (SS OO) and vice
versa (OO SS). The confusion of subjects and ob-
jects follows from lack of sufficient formal disam-
biguation, i.e., simple clues such as word order,
part-of-speech and word form do not clearly indi-
cate syntactic function.
With respect to word order, subjects and objects
may both precede or follow their verbal head. Sub-
jects, however, are more likely to occur prever-
bally (77%), whereas objects typically occupy a
postverbal position (94%). We would therefore ex-
pect postverbal subjects and preverbal objects to be
more dominant among the errors than in the tree-
bank as a whole (23% and 6% respectively). Table
2 shows a breakdown of the errors for confused
subjects and objects and their position with respect
to the verbal head. We find that postverbal subjects
(after) are in clear majority among the subjects er-
roneously assigned the object relation. Due to the
V2 property of Swedish, the subject must reside
in the position directly following the finite verb
whenever another constituent occupies the prever-
bal position, as in (1) where a direct object resides
sentence-initially:
(1) Samma
same
erfarenhet
experience
gjorde
made
engelsma?nnen
englishmen-DEF
?The same experience, the Englishmen had?
For the confused objects we find a larger propor-
tion of preverbal elements than for subjects, which
4We define argument relations as dependency relations
which obtain between a verb and a dependent which is
subcategorized for and/or thematically entailed by the verb.
Note that arguments are not distinguished structurally from
non-arguments, like adverbials, in dependency grammar, but
through dependency label.
is the mirror image of the normal distribution of
syntactic functions among preverbal elements. As
Table 2 shows, the proportion of preverbal ele-
ments among the subject-assigned objects (33.3%)
is notably higher than in the corpus as a whole,
where preverbal objects account for a miniscule
6% of all objects.
In addition to the word order variation dis-
cussed above, Swedish also has limited morpho-
logical marking of syntactic function. Nouns are
marked only for genitive case and only pronouns
are marked for accusative case. There is also syn-
cretism in the pronominal paradigm where the pro-
noun is invariant for case, e.g. det, den ?it?, in-
gen/inga ?no?, and may, in fact, also function as
a determiner. This means that, with respect to
word form, only the set of unambiguous pronouns
clearly indicate syntactic function. In the errors,
we find that nouns and functionally ambiguous
pronouns dominate the errors where subjects and
objects are confused, accounting for 84.5% of the
SS OO and 93.5% of the OO SS errors.
The initial error analysis shows that the confu-
sion of argument relations constitutes a frequent
and consistent error during parsing. Ambiguities
in word order and morphological marking consti-
tute a complicating factor and we find cases that
deviate from the most frequent word order pat-
terns and are not formally disambiguated by part-
of-speech information. It is clear that we in order
to resolve these ambiguities have to examine fea-
tures beyond syntactic category and linear word or-
der.
4 Linguistic features for argument
disambiguation
Argument relations tend to differ along several lin-
guistic dimensions. These differences are found
as statistical tendencies, rather than absolute re-
quirements on syntactic structure. The property
of animacy, a referential property of nominal el-
ements, has been argued to play a role in argument
realization in a range of languages see de Swart
et.al. (2008) for an overview. It is closely cor-
related with the semantic property of agentivity,
hence subjects will tend to be referentially animate
more often than objects. Another property which
may differentiate between the argument functions
is the property of definiteness, which can be linked
with a notion of givenness, (Weber and Mu?ller,
2004). This is reflected in the choice of refer-
ring expression for the various argument types in
27
Talbanken05 ? subjects are more often pronominal
(49.2%), whereas objects and subject predicatives
are typically realized by an indefinite noun (67.6%
and 89.6%, respectively). As mentioned in section
2, there are categorical constraints which are char-
acteristic for Swedish morphosyntax. Even if the
morphological marking of arguments in Scandina-
vian is not extensive or unambiguous, case may
distinguish arguments. Only subjects may follow
a finite verb and precede a non-finite verb and only
complements may follow a non-finite verb. Infor-
mation on tense or the related finiteness is there-
fore something that one might assume to be ben-
eficial for argument analysis. Another property of
the verb which clearly influences the assignment
of core argument functions is the voice of the verb,
i.e., whether it is passive or active.5
5 Experiments with gold standard
features
We perform a set of experiments with an extended
feature model and added, gold standard informa-
tion on animacy, definiteness, case, finiteness and
voice, where the features were employed individu-
ally as well as in combination.
5.1 Experimental methodology
All parsing experiments are performed using 10-
fold cross-validation for training and testing on
the entire written part of Talbanken05. The fea-
ture model used throughout is the extended fea-
ture model depicted in Table 1, including all four
columns.6 Hence, what is varied in the exper-
iments is only the information contained in the
FEATS features (animacy, definiteness, etc.), while
the tokens for which these features are defined re-
mains constant. Overall parsing accuracy will be
reported using the standard metrics of labeled at-
tachment score (LAS) and unlabeled attachment
score (UAS).7 Statistical significance is checked
using Dan Bikel?s randomized parsing evaluation
5We experimented with the use of tense as well as finite-
ness, a binary feature which was obtained by a mapping from
tense to finite/non-finite. Finiteness gave significantly better
results (p<.03) and was therefore employed in the following,
see (?vrelid, 2008b) for details.
6Preliminary experiments showed that it was better to tie
FEATS features to the same tokens as FORM features (rather
than POS or DEP features). Backward selection from this
model was tried for several different instantiations of FEATS
but with no significant improvement.
7LAS and UAS report the percentage of tokens that are as-
signed the correct head with (labeled) or without (unlabeled)
the correct dependency label, calculated using eval.pl with de-
fault settings (http://nextens.uvt.nl/?conll/software.html)
comparator.8 Since the main focus of this article is
on the disambiguation of grammatical functions,
we report accuracy for specific dependency rela-
tions, measured as a balanced F-score.
5.2 Results
The overall results for these experiments are pre-
sented in table 3, along with p-scores. The exper-
iments show that each feature individually causes
a significant improvement in terms of overall la-
beled accuracy as well as performance for argu-
ment relations. Error analysis comparing the base-
line parser (NoFeats) with new parsers trained with
individual features reveal the influence of these
features on argument disambiguation. We find
that animacy influences the disambiguation of sub-
jects from objects, objects from indirect objects
as well as the general distinction of arguments
from non-arguments. Definiteness has a notable
effect on the disambiguation of subjects and sub-
ject predicatives. Information on morphological
case shows a clear effect in distinguishing between
arguments and non-arguments, and in particular,
in distinguishing nominal modifiers with genitive
case. The added verbal features, finiteness and
voice, have a positive effect on the verbal depen-
dency relations, as well as an overall effect on the
assignment of the SS and OO argument relations.
Information on voice also benefits the relation ex-
pressing the demoted agent (AG) in passive con-
structions, headed by the preposition av ?by?, as in
English.
The ADCV experiment which combines infor-
mation on animacy, definiteness, case and verbal
features shows a cumulative effect of the added
features with results which differ significantly
from the baseline, as well as from each of the in-
dividual experiments (p<.0001). We observe clear
improvements for the analysis of all argument re-
lations, as shown by the third column in table 4
which presents F-scores for the various argument
relations.
6 Acquiring features
A possible objection to the general applicability
of the results presented above is that the added
information consists of gold standard annotation
from a treebank. However, the morphosyntactic
features examined here (definiteness, case, tense,
voice) represent standard output from most part-
of-speech taggers. In the following we will also
8http://www.cis.upenn.edu/?dbikel/software.html
28
UAS LAS p-value
NoFeats 89.87 84.92 ?
Anim 89.93 85.10 p<.0002
Def 89.87 85.02 p<.02
Case 89.99 85.13 p<.0001
Verb 90.24 85.38 p<.0001
ADC 90.13 85.35 p<.0001
ADCV 90.40 85.68 p<.0001
Table 3: Overall results in gold standard ex-
periments expressed as unlabeled and labeled
attachment scores.
NoFeats Gold Auto
SS subject 90.25 91.80 91.32
OO object 84.53 86.27 86.10
SP subj.pred. 84.82 85.87 85.80
AG pass. agent 73.56 81.34 81.02
ES logical subj. 71.82 73.44 72.60
FO formal obj. 56.68 65.64 65.38
VO obj. small clause 72.10 83.40 83.12
VS subj. small clause 58.75 65.56 68.75
FS formal subj. 71.31 72.10 71.31
IO indir. obj. 76.14 77.76 76.29
Table 4: F-scores for argument relations with
combined features (ADCV).
Feature Application
Definiteness POS-tagger
Case POS-tagger
Animacy - NN Animacy classifier
Animacy - PN Named Entity Tagger
Animacy - PO Majority class
Tense (finiteness), voice POS-tagger
Table 5: Overview of applications employed for
automatic feature acquisition.
show that the property of animacy can be fairly
robustly acquired for common nouns by means
of distributional features from an automatically
parsed corpus.
Table 5 shows an overview of the applications
employed for the automatic acquisition of our lin-
guistic features. For part-of-speech tagging, we
employ MaltTagger ? a HMM part-of-speech tag-
ger for Swedish (Hall, 2003). The POS-tagger dis-
tinguishes tense and voice for verbs, nominative
and accusative case for pronouns, as well as defi-
niteness and genitive case for nouns.
6.1 Animacy
The feature of animacy is clearly the most chal-
lenging feature to acquire automatically. Recall
that Talbanken05 distinguishes animacy for all
nominal constituents. In the following we describe
the automatic acquisition of animacy information
for common nouns, proper nouns and pronouns.
Common nouns Table 6 presents an overview
of the animacy data for common nouns in Tal-
banken05. It is clear that the data is highly skewed
Class Types Tokens covered
Animate 644 6010
Inanimate 6910 34822
Total 7554 40832
Table 6: The animacy data set from Talbanken05;
number of noun lemmas (Types) and tokens in
each class.
towards the non-person class, which accounts for
91.5% of the data instances. Due to the small size
of the treebank we classify common noun lem-
mas based on their morphosyntactic distribution
in a considerably larger corpus. For the animacy
classification of common nouns, we construct a
general feature space for animacy classification,
which makes use of distributional data regarding
syntactic properties of the noun, as well as various
morphological properties. The syntactic and mor-
phological features in the general feature space are
presented below:
Syntactic features A feature for each dependency
relation with nominal potential: (transitive)
subject (SUBJ), object (OBJ), prepositional
complement (PA), root (ROOT)9, apposition
(APP), conjunct (CC), determiner (DET), pred-
icative (PRD), complement of comparative
subjunction (UK). We also include a feature
for the complement of a genitive modifier, the
so-called ?possessee?, (GENHD).
Morphological features A feature for each mor-
9Nominal elements may be assigned the root relation in
sentence fragments which do not include a finite verb.
29
phological distinction relevant for a noun:
gender (NEU/UTR), number (SIN/PLU), defi-
niteness (DEF/IND), case (NOM/GEN). Also,
the part-of-speech tags distinguish dates
(DAT) and quantifying nouns (SET), e.g. del,
rad ?part, row?, so these are also included as
features.
For extraction of distributional data for the Tal-
banken05 nouns we make use of the Swedish Pa-
role corpus of 21.5M tokens.10 To facilitate feature
extraction, we part-of-speech tag the corpus and
parse it with MaltParser, which assigns a depen-
dency analysis.11 For classification, we make use
of the Tilburg Memory-Based Learner (TiMBL)
(Daelemans et al, 2004).12 and optimize the
TiMBL parameters on a subset of the full data
set.13
We obtain results for animacy classification of
noun lemmas, ranging from 97.3% accuracy to
94.0% depending on the sparsity of the data. With
an absolute frequency threshold of 10, we obtain
an accuracy of 95.4%, which constitutes a 50%
reduction of error rate over a majority baseline.
We find that classification of the inanimate class is
quite stable throughout the experiments, whereas
the classification of the minority class of animate
nouns suffers from sparse data. We obtain a F-
score of 71.8% F-score for the animate class and
97.5% for the inanimate class with a threshold of
10. The common nouns in Talbanken05 are classi-
fied for animacy following a leave-one-out training
and testing scheme where each of the n nouns in
Talbanken05 are classified with a classifier trained
on n ? 1 instances. This ensures that the training
and test instances are disjoint at all times. More-
over, the fact that the distributional data is taken
from a separate data set ensures non-circularity
10Parole is available at http://spraakbanken.gu.se
11For part-of-speech tagging, we employ the MaltTagger ?
a HMM part-of-speech tagger for Swedish (Hall, 2003). For
parsing, we employ MaltParser with a pretrained model for
Swedish, which has been trained on the tags output by the
tagger. It makes use of a smaller set of dependency relations
than those found in Talbanken05.
12TiMBL is freely available at
http://ilk.uvt.nl/software.html
13For parameter optimization we employ the
paramsearch tool, supplied with TiMBL, see
http://ilk.uvt.nl/software.html. Paramsearch implements
a hill climbing search for the optimal settings on iteratively
larger parts of the supplied data. We performed parameter
optimization on 20% of the total >0 data set, where we
balanced the data with respect to frequency. The resulting
settings are k = 11, GainRatio feature weighting and Inverse
Linear (IL) class voting weights.
since we are not basing the classification on gold
standard parses.
Proper nouns In the task of named entity recog-
nition (NER), proper nouns are classified accord-
ing to a set of semantic categories. For the annota-
tion of proper nouns, we make use of a named en-
tity tagger for Swedish (Kokkinakis, 2004), which
is a rule-based tagger based on finite-state rules,
supplied with name lists, so-called ?gazetteers?.
The tagger distinguishes the category ?Person? for
human referring proper nouns and we extract in-
formation on this category.
Pronouns A subset of the personal pronouns in
Scandinavian, as in English, clearly distinguish
their referent with regard to animacy, e.g. han,
det ?he, it?. There is, however, a quite large group
of third person plural pronouns which are ambigu-
ous with regards to the animacy of their referent,
e.g., de, dem, deras ?they, them, theirs?. Pronom-
inal reference resolution is a complex task which
we will not attempt to solve in the present context.
The pronominal part-of-speech tags from the part-
of-speech tagger distinguish number and gender
and in the animacy classification of the personal
pronouns we classify based on these tags only. We
employ a simple heuristic where the pronominal
tags which had more than 85% human instances in
the gold standard are annotated as human.14 The
pronouns which are ambiguous with respect to an-
imacy are not annotated as animate.
In table 7 we see an overview of the accuracy
of the acquired features, i.e., the percentage of
correct instances out of all instances. Note that
we adhere to the general annotation strategy in
Talbanken05, where each dimension (definiteness,
case etc.) contains a null category ?, which ex-
presses the lack of a certain property. The acqui-
sition of the morphological features (definiteness,
case, finiteness and voice) are very reliable, with
accuracies from 96.9% for voice to 98.5% for the
case feature.
It is not surprising that we observe the largest
discrepancies from the gold standard annotation
in the automatic animacy annotation. In general,
the annotation of animate nominals exhibits a de-
cent precision (95.7) and a lower recall (61.3). The
automatic classification of human common nouns
14A manual classification of the individual pronoun lem-
mas was also considered. However, the treebank has a total of
324 different pronoun forms, hence we opted for a heuristic
classification of the part-of-speech tags instead.
30
Dimension Features Instances Correct Accuracy
Definiteness DD, ? 40832 40010 98.0
Case GG, AA, ? 68313 67289 98.5
Animacy
NNPNPO
HH, ? 68313 61295 89.7
Animacy
NN
HH, ? 40832 37952 92.9
Animacy
PN
HH, ? 2078 1902 91.5
Animacy
PO
HH, ? 25403 21441 84.4
Finiteness FV, ? 30767 30035 97.6
Voice PA, ? 30767 29805 96.9
Table 7: Accuracy for automatically acquired linguistic features.
Gold Automatic
UAS LAS UAS LAS p-value
NoFeats 89.87 84.92 89.87 84.92 ?
Def 89.87 85.02 89.88 85.03 p<0.01
Case 89.99 85.13 89.95 85.11 p<.0001
Verb 90.24 85.38 90.12 85.26 p<.0001
Anim 89.93 85.10 89.86 85.01 p<.03
ADC 90.13 85.35 90.01 85.21 p<.0001
ADCV 90.40 85.68 90.27 85.54 p<.0001
Table 8: Overall results in experiments with auto-
matic features compared to gold standard features.
(Animacy
NN
) also has a quite high precision
(94.2) in combination with a lower recall (55.5).
The named-entity recognizer (Animacy
PN
) shows
more balanced results with a precision of 97.8 and
a recall of 85.2 and the heuristic classification of
the pronominal part-of-speech tags (Animacy
PO
)
gives us high precision (96.3) combined with lower
recall (62.0) for the animate class.
7 Experiments with acquired features
The experimental methodology is identical to the
one described in 5.1 above, the only difference be-
ing that the linguistic features are acquired auto-
matically, rather than being gold standard. In order
to enable a direct comparison with the results from
the earlier experiments, we employ the gold stan-
dard part-of-speech tags, as before. This means
that the set for which the various linguistic features
are defined is identical, whereas the feature values
may differ.
Table 8 presents the overall results with auto-
matic features, compared to the gold standard re-
sults and p-scores for the difference of the auto-
matic results from the NoFeats baseline. As ex-
pected, we find that the effect of the automatic fea-
tures is generally lower than their gold standard
counterparts. However, all automatic features im-
prove significantly on the NoFeats baseline. In the
error analysis we find the same tendencies in terms
of improvement for specific dependency relations.
The morphological argument features from the
POS-tagger are reliable, as we saw above, and
we observe almost identical results to the gold
standard results. The addition of information
on definiteness causes a significant improvement
(p<.01), and so does the addition of information
on case (p<.0001). The addition of the automat-
ically acquired animacy information results in a
smaller, but significant improvement of overall re-
sults even though the annotation is less reliable
(p<.03). An interesting result is that the automat-
ically acquired information on animacy for com-
mon nouns actually has a significantly better effect
than the gold standard counterparts due to captur-
ing distributional tendencies (?vrelid, 2008a). As
in the gold standard experiments, we find that the
features which have the most notable effect on per-
formance are the verbal features (p<.0001).
In parallel with the results achieved with the
combination of gold standard features, we observe
improvement of overall results compared to the
baseline (p<.0001) and each of the individual fea-
tures when we combine the features of the argu-
ments (ADC; p<.01) and the argument and ver-
bal features (ADCV; p<.0001). Column 4 in Ta-
ble 4 shows an overview of performance for the
argument relations, compared to the gold standard
experiments. We find overall somewhat lower re-
sults in the experiment with automatic features, but
find the same tendencies with the automatically ac-
quired features.
31
8 Conclusion
An error analysis of the best performing data-
driven dependency parser for Swedish revealed
consistent errors in dependency assignment,
namely the confusion of argument functions. We
established a set of features expressing distinguish-
ing semantic and structural properties of argu-
ments such as animacy, definiteness and finiteness
and performed a set of experiments with gold stan-
dard features taken from a treebank of Swedish.
The experiments showed that each feature individ-
ually caused an improvement in terms of overall la-
beled accuracy and performance for the argument
relations. We furthermore found that the results
may largely be replicated with automatic features
and a generic part-of-speech tagger. The features
were acquired automatically employing a part-of-
speech tagger, a named-entity recognizer and an
animacy classifier of common noun lemmas em-
ploying morphosyntactic distributional features. A
set of corresponding experiments with automatic
features gave significant improvement from the ad-
dition of individual features and a cumulative ef-
fect of the same features in combination. In partic-
ular, we show that the very same tendencies in im-
provement for specific argument relations such as
subjects, objects and predicatives may be obtained
using automatically acquired features.
Properties of the Scandinavian languages con-
nected with errors in argument assignment are not
isolated phenomena. A range of other languages
exhibit similar properties, for instance, Italian ex-
hibits word order variation, little case, syncretism
in agreement morphology, as well as pro-drop;
German exhibits a larger degree of word order
variation in combination with quite a bit of syn-
cretism in case morphology; Dutch has word order
variation, little case and syncretism in agreement
morphology. These are all examples of other lan-
guages for which the results described here are rel-
evant.
References
Bod, Rens. 1998. Beyond Grammar: An experience-based
theory of language. CSLI Publications, Stanford, CA.
Carroll, John and Edward Briscoe. 2002. High precision ex-
traction of grammatical relations. In Proceedings of the
19th International Conference on Computational Linguis-
tics (COLING), pages 134?140.
Chang, Chih-Chung and Chih-Jen Lin. 2001. LIBSVM: A
library for support vector machines. Software available at
http://www.csie.ntu.edu.tw/?cjlin/libsvm.
Daelemans, Walter, Jakub Zavrel, Ko Van der Sloot, and An-
tal Van den Bosch. 2004. TiMBL: Tilburg Memory Based
Learner, version 5.1, Reference Guide. Technical report,
ILK Technical Report Series 04-02.
de Swart, Peter, Monique Lamers, and Sander Lestrade.
2008. Animacy, argument structure and argument encod-
ing: Introduction to the special issue on animacy. Lingua,
118(2):131?140.
Hall, Johan. 2003. A probabilistic part-of-speech tagger
with suffix probabilities. Master?s thesis, Va?xjo? Univer-
sity, Sweden.
Klein, Dan and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguistics
(ACL), pages 423?430.
Kokkinakis, Dimitrios. 2004. Reducing the effect of name
explosion. In Proceedings of the LREC Workshop: Be-
yond Named Entity Recognition, Semantic labelling for
NLP tasks.
Ku?bler, Sandra and Jelena Prokic?. 2006. Why is German de-
pendency parsing more reliable than constituent parsing?
In Proceedings of the Fifth Workshop on Treebanks and
Linguistic Theories (TLT), pages 7?18.
McDonald, Ryan and Joakim Nivre. 2007. Characterizing
the errors of data-driven dependency parsing. In Proceed-
ings of the Eleventh Conference on Computational Natural
Language Learning (CoNLL), pages 122?131.
Nivre, Joakim, Jens Nilsson, and Johan Hall. 2006a. Tal-
banken05: A Swedish treebank with phrase structure and
dependency annotation. In Proceedings of the fifth Inter-
national Conference on Language Resources and Evalua-
tion (LREC), pages 1392?1395.
Nivre, Joakim, Jens Nilsson, Johan Hall, Gu?ls?en Eryig?it, and
Svetoslav Marinov. 2006b. Labeled pseudo-projective
dependency parsing with Support Vector Machines. In
Proceedings of the Conference on Computational Natural
Language Learning (CoNLL).
Nivre, Joakim, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. CoNLL 2007 Shared Task on Dependency Pars-
ing. In Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL 2007, pages 915?932.
?vrelid, Lilja. 2008a. Argument Differentiation. Soft con-
straints and data-driven models. Ph.D. thesis, University
of Gothenburg.
?vrelid, Lilja. 2008b. Finite matters: Verbal features in data-
driven parsing of Swedish. In Proceedings of the Interna-
tional Conference on NLP, GoTAL 2008.
Weber, Andrea and Karin Mu?ller. 2004. Word order varia-
tion in German main clauses: A corpus analysis. In Pro-
ceedings of the 20th International Conference on Compu-
tational Linguistics, pages 71?77.
32
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 48?55,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Resolving Speculation: MaxEnt Cue Classification and
Dependency-Based Scope Rules?
Erik Velldal? and Lilja ?vrelid?? and Stephan Oepen?
? University of Oslo, Department of Informatics (Norway)
?Universit?t Potsdam, Institut f?r Linguistik (Germany)
erikve@ifi.uio.no and ovrelid@uni-potsdam.de and oe@ifi.uio.no
Abstract
This paper describes a hybrid, two-level
approach for resolving hedge cues, the
problem of the CoNLL-2010 shared task.
First, a maximum entropy classifier is ap-
plied to identify cue words, using both
syntactic- and surface-oriented features.
Second, a set of manually crafted rules,
operating on dependency representations
and the output of the classifier, is applied
to resolve the scope of the hedge cues
within the sentence.
1 Introduction
The CoNLL-2010 shared task1 comprises two
sub-tasks. Task 1 is described as learning to detect
sentences containing uncertainty, while the object
of Task 2 is learning to resolve the in-sentence
scope of hedge cues (Farkas et al, 2010). Parallel-
ing this two-fold task definition, the architecture
of our system naturally decomposes into two main
steps. First, a maximum entropy (MaxEnt) classi-
fier is applied to automatically detect cue words.
For Task 1, a given sentence is labeled as uncer-
tain if it contains a word classified as a cue. For
Task 2, we then go on to determine the scope of
the identified cues using a set of manually crafted
rules operating on dependency representations.
For both Task 1 and Task 2, our system partic-
ipates in the stricter category of ?closed? or ?in-
domain? systems. This means that we do not
use any additional uncertainty-annotated material
beyond the supplied training data, consisting of
14541 sentences from biomedical abstracts and ar-
ticles (see Table 2). In the official ranking of re-
?We are grateful to our colleagues at the University of
Oslo and the University of Potsdam, for many useful discus-
sions, constructive critique, and encouragment. We specifi-
cally thank Woodley Packard for careful proof-reading.
1The CoNLL-2010 shared task website: http://www.
inf.u-szeged.hu/rgai/conll2010st/.
sults, and considering systems in all categories to-
gether (closed/open/cross-domain), our system is
ranked 4 out of 24 for Task 1 and 3 out of 15 for
Task 2, resulting in highest average rank (and F1)
overall. We detail the implementation of the cue
classifier and the syntactic rules in Sections 3 and
4, respectively. Results for the held-out testing are
provided in Section 5. First, however, the next sec-
tion describes the various resources that we used
for pre-processing the CoNLL data sets, to prepare
the input to our hedge analysis systems.
2 Architecture and Set-Up
2.1 Preprocessing
To ease integration of annotations across system
components, we converted the XML training data
to plain-text files, with stand-off annotation linked
to the raw data by virtue of character start and end
positions (dubbed characterization in the follow-
ing). Thus, hedge cues, scope boundaries, tok-
enization, Part-of-Speech (PoS) assignments, etc.
are all represented in a uniform fashion: as po-
tentially overlapping annotations on sub-strings of
the raw input.
The GENIA tagger (Tsuruoka et al, 2005) takes
an important role in our pre-processing set-up.
However, maybe somewhat surprisingly, we found
that its tokenization rules are not always opti-
mally adapted for the BioScope corpus. GENIA
unconditionally introduces token boundaries for
some punctuation marks that can also occur token-
internally. For example, it wrongly splits tokens
like ?3,926.50?, ?methlycobamide:CoM?,
or ?Ca(2+)?. Conversely, GENIA fails to isolate
some kinds of opening single quotes, because the
quoting conventions assumed in BioScope differ
from those used in the GENIA Corpus; furthermore,
it mis-tokenizes LATEX-style n- and m-dashes.
On average, one in five sentences in the CoNLL
training data exhibited GENIA tokenization prob-
48
ID FORM LEMMA POS FEATS HEAD DEPREL XHEAD XDEP
1 The the DT _ 4 NMOD 4 SPECDET
2 unknown unknown JJ degree:attributive 4 NMOD 4 ADJUNCT
3 amino amino JJ degree:attributive 4 NMOD 4 ADJUNCT
4 acid acid NN pers:3|case:nom|num:sg|ntype:common 5 SBJ 3 SUBJ
5 may may MD mood:ind|subcat:MODAL|tense:pres|clauseType:decl|passive:- 0 ROOT 0 ROOT
6 be be VB _ 5 VC 7 PHI
7 used use VBN subcat:V-SUBJ-OBJ|vtype:main|passive:+ 6 VC 5 XCOMP
8 by by IN _ 7 LGS 9 PHI
9 these these DT deixis:proximal 10 NMOD 10 SPECDET
10 species specie NNS num:pl|pers:3|case:obl|common:count|ntype:common 8 PMOD 7 OBL-AG
11 . . . _ 5 P 0 PUNC
Table 1: Enhanced dependency representation of the example sentence The unknown amino acid may
be used by these species with GENIAPoS-tags (POS), Malt parses (HEAD, DEPREL) and XLE parses
(XHEAD, XDEP).
lems. Our pre-processing approach therefore de-
ploys a home-grown, cascaded finite-state tok-
enizer (borrowed and adapted from the open-
source English Resource Grammar; Flickinger
(2000)), which aims to implement the tokeniza-
tion decisions made in the Penn Treebank (Mar-
cus et al, 1993) ? much like GENIA, in principle
? but properly treating corner cases like the ones
above. Synchronized via characterization, this to-
kenization is then enriched with the output of no
less than two PoS taggers, as detailed in the next
section.
2.2 PoS Tagging and Lemmatization
For PoS tagging and lemmatization, we combine
GENIA (with its built-in, occasionally deviant to-
kenizer) and TnT (Brants, 2000), which operates
on pre-tokenized inputs but in its default model
is trained on financial news from the Penn Tree-
bank. Our general goal here is to take advantage
of the higher PoS accuracy provided by GENIA in
the biomedical domain, while using our improved
tokenization and producing inputs to the parsing
stage (see Section 2.3 below) that as much as pos-
sible resemble the conventions used in the original
training data for the parser ? the Penn Treebank,
once again.
To this effect, for the vast majority of tokens we
can align the GENIA tokenization with our own,
and in these cases we typically use GENIA PoS
tags and lemmas (i.e. base forms). For better nor-
malization, we downcase base forms for all parts
of speech except proper nouns. However, GENIA
does not make a PoS distinction between proper
and common nouns, as in the Penn Treebank, and
hence we give precedence to TnT outputs for to-
kens tagged as nominal by both taggers. Finally,
for the small number of cases where we cannot es-
tablish a one-to-one alignment from an element in
our own tokenization to a GENIA token, we rely on
TnT annotation only. In the merging of annotations
across components, and also in downstream pro-
cessing we have found it most convenient to op-
erate predominantly in terms of characterization,
i.e. sub-strings of the raw input that need not align
perfectly with token boundaries.
2.3 Dependency Parsing with LFG Features
For syntactic parsing we employ a data-driven de-
pendency parser which incorporates the predic-
tions from a large-scale LFG grammar. A tech-
nique of parser stacking is employed, which en-
ables a data-driven parser to learn from the out-
put of another parser, in addition to gold stan-
dard treebank annotations (Nivre and McDonald,
2008). This technique has been shown to pro-
vide significant improvements in accuracy for both
English and German (?vrelid et al, 2009), and
a similar approach employing an HPSG grammar
has been shown to increase domain independence
in data-driven dependency parsing (Zhang and
Wang, 2009). For our purposes, we decide to use a
parser which incorporates analyses from two quite
different parsing approaches ? data-driven depen-
dency parsing and ?deep? parsing with a hand-
crafted grammar ? providing us with a range of
different types of linguistic features which may be
used in hedge detection.
We employ the freely available MaltParser
(Nivre et al, 2006), which is a language-
independent system for data-driven dependency
parsing.2 It is based on a deterministic pars-
ing strategy in combination with treebank-induced
classifiers for predicting parse transitions. It sup-
ports a rich feature representation of the parse his-
tory in order to guide parsing and may easily be
extended to take into account new features of the
2See http://maltparser.org.
49
Sentences Hedged Cues Multi-Word Tokens Cue Tokens
Sentences Cues
Abstracts 11871 2101 2659 364 309634 3056
Articles 2670 519 668 84 68579 782
Total 14541 2620 3327 448 378213 3838
Table 2: Some descriptive figures for the shared task training data. Token-level counts are based on the
tokenization described in Section 2.1.
parse history.
Parser stacking The procedure to enable the
data-driven parser to learn from the grammar-
driven parser is quite simple. We parse a treebank
with the XLE platform (Crouch et al, 2008) and the
English grammar developed within the ParGram
project (Butt et al, 2002). We then convert the
LFG output to dependency structures, so that we
have two parallel versions of the treebank ? one
gold standard and one with LFG-annotation. We
extend the gold standard treebank with additional
information from the corresponding LFG analysis
and train the data-driven dependency parser on the
enhanced data set. See ?vrelid et al (2010) for
details of the conversion and training of the parser.
Table 1 shows the enhanced dependency rep-
resentation of the English sentence The unknown
amino acid may be used by these species, taken
from the training data. For each token, the parsed
data contains information on the surface form,
lemma, and PoS tag, as well as on the head and de-
pendency relation in columns 6 and 7. The depen-
dency analysis suggested by XLE is contained in
columns 8 and 9, whereas additional XLE informa-
tion, such as morphosyntactic properties like num-
ber and voice, as well as more semantic properties
detailing, e.g., subcategorization frames, seman-
tic conceptual categories such as human, time and
location, etc., resides in the FEATS column. The
parser outputs, which in turn form the basis for our
scope resolution rules discussed in Section 4, also
take this same form.
The parser employed in this work is trained
on the Wall Street Journal sections 2 ? 24 of the
Penn Treebank, converted to dependency format
(Johansson and Nugues, 2007) and extended with
XLE features, as described above. Parsing is per-
formed using the arc-eager mode of MaltParser
(Nivre, 2003) and an SVM with a polynomial ker-
nel. When tested using 10-fold cross-validation on
this data set, the parser achieves a labeled accuracy
score of 89.8 (?vrelid et al, 2010).
3 Identifying Hedge Cues
For the task of identifying hedge cues, we devel-
oped a binary maximum entropy (MaxEnt) classi-
fier. The identification of cue words is used for (i)
classifying sentences as certain/uncertain (Task 1),
and (ii) providing input to the syntactic rules that
we later apply for resolving the in-sentence scope
of the cues (Task 2). We also report evaluation
scores for the sub-task of cue detection in isola-
tion.
As annotated in the training data, it is possible
for a hedge cue to span multiple tokens, e.g. as in
whether or not. The majority of the multi-word
cues in the training data are very infrequent, how-
ever, most occurring only once, and the classifier
itself is not sensitive to the notion of multi-word
cues. A given word token in the training data is
simply considered to be either a cue or a non-cue,
depending on whether it falls within the span of a
cue annotation. The task of determining whether
a cue word forms part of a larger multi-word cue,
is performed by a separate post-processing step,
further described in Section 3.2.
3.1 Maximum Entropy Classification
In the MaxEnt framework, each training exam-
ple ? in our case a paired word and label ?wi, yi?
? is represented as a feature vector f(wi, yi) =
fi ? <d. Each dimension or feature function fij
can encode arbitrary properties of the data. The
particular feature functions we are using for the
cue identification are described under Section 3.4
below. For model estimation we use the TADM3
software (Malouf, 2002). For feature extraction
and model tuning, we build on the experimen-
tation environment developed by Velldal (2008)
(in turn extending earlier work by Oepen et al
3Toolkit for Advanced Discriminative Modeling; avail-
able from http://tadm.sourceforge.net/.
50
(2004)). Among other things, its highly optimized
feature handling ? where the potentially expen-
sive feature extraction step is performed only once
and then combined with several levels of feature
caching ? make it computationally feasible to per-
form large-scale ?grid searches? over different con-
figurations of features and model parameters when
using many millions of features.
3.2 Multi-Word Cues
After applying the classifier, a separate post-
processing step aims to determine whether tokens
identified as cue words belong to a larger multi-
word cue. For example, when the classifier has
already identified one or more of the tokens in a
phrase such as raises the possibility to be part of a
hedge cue, a heuristic rule (viz. basically lemma-
level pattern-matching, targeted at only the most
frequently occurring multi-word cues in the train-
ing data) makes sure that the tokens are treated as
part of one and the same cue.
3.3 Model Development, Data Sets and
Evaluation Measures
While the training data made available for the
shared task consisted of both abstracts and full
articles from the BioScope corpus (Vincze et al,
2008), the test data were pre-announced to consist
of biomedical articles only. In order to make the
testing situation during development as similar as
possible to what could be expected for the held-out
testing, we only tested on sentences taken from the
articles part of the training data. When developing
the classifiers we performed 10-fold training and
testing over the articles, while always including all
sentences from the abstracts in the training set as
well. Table 2 provides some basic descriptive fig-
ures summarizing the training data.
As can be seen in Table 3, we will be report-
ing precision, recall and F-scores for three dif-
ferent levels of evaluation for the cue classifiers:
the sentence-level, token-level and cue-level. The
sentence-level scores correspond to Task 1 of the
shared task, i.e. correctly identifying sentences as
being certain or uncertain. A sentence is labeled
uncertain if it contains at least one token classi-
fied as a hedge cue. The token-level scores indi-
cate how well the classifiers succeed in identify-
ing individual cue words (this score does not take
into account the heuristic post-processing rules for
finding multi-word cues). Finally, the cue-level
scores are based on the exact-match counts for full
 70
 75
 80
 85
 90
 10  20  30  40  50  60  70  80  90  100
Token level F1Sentence level F1
Figure 1: Learning curves showing, for both
token- and sentence-level F-scores, the effect of
incrementally including a larger percentage of
training data into the 10-fold cycles. (As described
also for the other development results, while we
are training on both the articles and the abstracts,
we are testing only on the articles.)
hedge cues (possibly spanning multiple tokens).
These latter scores are computed using the official
shared task scorer script.
3.4 Feature Types
We trained cue classifiers using a wide vari-
ety of feature types, both syntactic and surface-
oriented. However, to better assess the contri-
bution of the different features, we first trained
two baseline models using only features defined
for non-normalized surface forms as they occur in
the training data. The most basic baseline model
(Baseline 1) included only unigram features. The
behavior of this classifier is similar to what we
would expect from simply compiling a list of cue
words from the training data, based on the major-
ity usage of each word as cue or non-cue. Base-
line 2 additionally included 2 words to the left and
3 to the right of the focus word (after first perform-
ing a search for the optimal spans of n-grams up
to 5). As shown in Table 3, this model achieved
a sentence-level F1 of 87.14 and a token-level F1
of 81.97. The corresponding scores for Baseline 1
are 79.20 and 69.59.
A general goal in our approach to hedge analy-
sis is to evaluate the contribution of syntactic in-
formation, both in cue detection and scope resolu-
tion. After applying the parser described in Sec-
tion 2.3, we extracted a range of classifier features
on the basis of the dependency structures (both as
51
proposed by the stacked MaltParser and converted
from XLE) as well as the deep grammar (XLE). Ad-
ditionally we defined various features on the basis
of base forms and PoS information provided by the
GENIA pre-processing (see Section 2.2).
For a quick overview, the feature types we ex-
perimented with include the following:
GENIA features n-gram features over the base
forms and PoS tags from the GENIA information
described in Section 2.2.
Dependency features A range of features ex-
tracted from dependency structures produced by
MaltParser and XLE (see Section 2.3), designed
to capture the syntactic properties and environ-
ment of a token: deprel ? dependency rela-
tion (Malt and XLE), deppath ? dependency
path to root, deppattern ? ordered set of co-
dependents/siblings, including focus token (Malt),
lextriple/postriple ? lexicalized and unlexicalized
dependency triplet for token (Malt), coord ? bi-
nary feature expressing coordination (XLE), co-
ordLevel ? phrase-structural level of coordination
(XLE).
Lexical parser features Other features con-
structed on the basis of the parser output: subcat
? subcategorization frame for verbs (XLE), adv-
Type ? type of adverbial, e.g. sentence, VP (XLE),
adjType ? adjectival function, e.g. attributive vs.
predicative (XLE)
When added to Baseline 2 in isolation, most of
these features resulted in a boost in classifier per-
formance. For the dependency-based features, the
contribution was more pronounced for lexicalized
versions of the features. This also points to the
fact that lexical information seems to be the key
for the task of cue identification, where the model
using only n-grams over surface forms proved a
strong baseline. As more feature types were added
to the classifier together, we also saw a clear trend
of diminishing returns, in that many of the fea-
tures seemed to contribute overlapping informa-
tion. After several rounds of grid-search over dif-
ferent feature configurations, the best-performing
classifier (as used for the shared task) used only
the following feature types: n-grams over surface
forms (including up to 2 tokens to the right), n-
grams over base forms (up to 3 tokens left and
right), PoS of the target word, ?subcat?, ?coord?,
and ?coordLevel?. The ?subcat? feature contains
information taken from XLE regarding the subcat-
egorization requirements of a verb in a specific
context, e.g., whether a verb is modal, takes an
expletive subject etc., whereas the coordination
features signal coordination (?coord?) and detail
the phrase-structural level of coordination (?co-
ordLevel?), e.g., NP, VP, etc. This defines the fea-
ture set used for the model referred to as final in
Table 3.
Recall that for Baseline 2, the F-score is 87.14
for the sentence-level evaluation and 81.97 for the
token-level. For our best and final feature config-
uration, the corresponding F-scores are 89.00 and
83.42, respectively. At both the sentence-level and
the token-level, the differences in classifier per-
formance were found to be statistically significant
at p < 0.005, using a two-tailed sign-test. Af-
ter also applying the heuristic rules for detecting
multi-word cues, the cue-level F-score for our final
model is 84.60, compared to 82.83 for Baseline 2.
3.5 The Effect of Data Size
In order to asses the effect of the size of the train-
ing set, we computed learning curves showing
how classifier performance changes as more train-
ing data is added. Starting with only 10% of the
training data included in the 10-fold cycle, Fig-
ure 1 shows the effect on both token level and
sentence-level F-scores as we incrementally in-
clude larger portions of the available training data.
Unsurprisingly, we see that the performance of
the classifier is steadily improving up to the point
where 100% of the data is included, and by extrap-
olating from the curves shown in Figure 1 it seems
reasonable to assume that this improvement would
continue if more data were available. We there-
fore tried to further increase the size of the training
set by also using the hedge-annotated clinical re-
ports that form part of the BioScope corpus. This
provided us with an additional 855 hedged sen-
tences. However, the classifiers did not seem able
to benefit from the additional training examples,
and across several feature configurations perfor-
mance was found to be consistently lower (though
not significantly so). The reason is probably that
the type of text is quite different ? the clinical re-
ports have a high ratio of fragments and also shows
other patterns of cue usage, being somewhat more
jargon-based. This seems to underpin the findings
of previous studies that hedge cue learners appear
quite sensitive to text type (Morante and Daele-
52
Sentence Level Token Level Cue Level
Model Prec Rec F1 Prec Rec F1 Prec Rec F1
Baseline 1 79.25 79.45 79.20 77.71 63.41 69.59 77.37 71.70 74.43
Baseline 2 86.83 87.54 87.14 86.86 77.69 81.97 85.34 80.21 82.69
Final 91.39 86.78 89.00 91.20 76.95 83.42 90.18 79.47 84.49
Table 3: Averaged 10-fold cross-validation results on the articles in the official shared task training data,
always including the abstracts in the training portion. The model listed as final includes features such
as n-grams over surface forms and base forms (both left and right), PoS, subcategorization frames, and
phrase-structural coordination level. The feature types are further described in Section 3.4.
PoS Description Source
CC Coordinations scope over their conjuncts M
IN Prepositions scope over their arguments with its descendants M
JJattr Attributive adjectives scope over their nominal head and its descendants M
JJpred Predicative adjectives scope over referential subjects and clausal arguments, if any M, X
MD Modals inherit subj-scope from their lexical verb and scope over their descendants M, X
RB Adverbs scope over their heads with its descendants M
VBpass Passive verbs scope over referential subjects and the verbal descendants M, X
VBrais Raising verbs scope over referential subjects and the verbal descendants M, X
* For multi-word cues, the head determines scope for all elements
* Back off from final punctuation and parentheses
Table 4: Overview of dependency-based scope rules with information source (MaltParser or XLE), orga-
nized by PoS of the cue.
mans, 2009).
4 Resolving Cue Scope
In our approach to scope resolution we rely heav-
ily on syntactic information, taken from the depen-
dency structures proposed by both MaltParser and
XLE, as well as various additional features from
the XLE parses relating to specific syntactic con-
structions.
4.1 Scope Rules
We construct a small set of heuristic rules which
define the scope for each cue detected in Stage
1. In the construction of these rules, we made use
of the information provided by the guidelines for
scope annotation in the BioScope corpus (Vincze
et al, 2008) as well as manual inspection of the
training data in order to arrive at reasonable scope
hypotheses for various types of cues.
The rules take as input a parsed sentence which
has been tagged with hedge cues and operate over
the dependency structures and additional features
provided by the parser. Default scope is set to
start at the cue word and span to the end of the
sentence (not including final puctuation), and this
scope also provides the baseline for the evaluation
of our rules. Table 4 provides an overview of the
rules employed for scope resolution.
In the case of multi-word cues, such as indicate
that, and either ... or, which share scope, we need
to determine the head of the multi-word unit. We
then set the scope of the whole unit to the scope of
the head token.
As an example, the application of the rules in
Table 4 to the sentence with the parsed output
in Table 1 correctly determine the scope of the
cue may as shown in example (1), using a variety
of syntactic cues regarding part-of-speech, argu-
menthood, voice, etc. First, the scope of the sen-
tence is set to default scope. Then the MD rule is
applied, which checks the properties of the lexical
verb used, located through a chain of verbal de-
pendents from the modal verb. Since it is passive
(passive:+), initial scope is set to include the
cue?s subject (SBJ) argument with all its descen-
dants (The unknown amino acid).
53
Task 1 Task 2 Cue Detection
Prec Rec F1 Prec Rec F1 Prec Rec F1
85.48 84.94 85.21 56.71 54.02 55.33 81.20 76.31 78.68
Table 6: Evaluation results for the official held-out testing.
Scope Prec Rec F1
Default w/gold cues 45.21 45.21 45.21
Rules w/gold cues 72.31 72.31 72.31
Rules w/classified cues 68.56 61.38 64.77
Table 5: Evaluation of the scope resolution rules
on the training articles, using both gold standard
cues and predicted cues. For the row labeled De-
fault, the scope for each cue is always taken to
span rightward to the end of the sentence. In the
rows labeled Rules, the scopes have been resolved
using the dependency-based rules.
(1) (The unknown amino acid <may> be used
by these species).
4.2 Rule Evaluation
Table 5 shows the evaluation of the set of scope
rules on the articles section of the data set, using
gold standard cues.4 This gives us an indication of
the performance of the rules, isolated from errors
in cue detection.
First of all, we may note that the baseline is
a strong one: choosing to extend the scope of a
cue to the end of the sentence provides an F-score
of 45.21. Given gold standard cue information,
the set of scope rules improves on the baseline by
27 percentage points on the articles section of the
data set, giving us an F-score of 72.31. Comparing
to the evaluation using classified cues (the bottom
row of Table 5), we find that the use of automati-
cally assigned cues causes a drop in performance
of 7.5 percentage points, to a result of 64.77.
5 Held-Out Testing
Table 6 presents the final results as obtained on
the held-out test data, which constitute the official
4This evaluation was carried out using the official scorer
script of the CoNLL shared task. When cue information is
kept constant, as in our case, the values for false positives
and false negatives will be identical, hence the precision and
recall values will always be identical as well.
results for our system in the CoNLL-2010 shared
task. The held-out test set comprises biomedical
articles with a total of 5003 sentences (790 of them
hedged).
For Task 1 we obtain an F-score of 85.21. The
corresponding result for the training data, which is
reported as ?Sentence Level? in Table 3, is 89.00.
Although we experience a slight drop in perfor-
mance (3.8 percentage points), the system seems
to generalize quite well to unseen data when it
comes to the detection of sentence-level uncer-
tainty.
For Task 2, the result on the held-out data set is
an F-score of 55.33, with quite balanced values for
precision and recall, 56.7 and 54.0, respectively. If
we compare this to the end-to-end evaluation on
the training data, provided in the bottom row of
Table 5, we find a somewhat larger drop in perfor-
mance (9.5 percentage points), from an F-score of
64.77 to the held-out 55.3. There are several pos-
sible reasons for this drop. First of all, there might
be a certain degree of overfitting of our system to
the training data. The held-out data may contain
hedging constructions that are not covered by our
set of scope rules. Moreover, the performance of
the scope rules is also influenced by the cue de-
tection, which is reported in the final columns of
Table 6. The cue-level performance of our system
on the held-out data set is 78.68, whereas the same
evaluation on the training data is 84.49. We find
that it is the precision, in particular, which suffers
in the application to the held-out data set. A pos-
sible strategy for future work is to optimize both
components of the Task 2 system, the cue detec-
tion and the scope rules, on the entire training set,
instead of just on the articles.
6 Conclusions ? Outlook
We have described a hybrid, two-level approach
for resolving hedging in biomedical text, as sub-
mitted for the stricter track of ?closed? or ?in-
domain? systems in the CoNLL-2010 shared task.
For the task of identifying hedge cues, we train
a MaxEnt classifier, which, for the held-out test
54
data, achieves an F-score of 78.68 on the cue-level
and 85.21 on the sentence-level (Task 1). For the
task of resolving the in-sentence scope of the iden-
tified cues (Task 2), we apply a set of manually
crafted rules operating on dependency representa-
tions, resulting in an end-to-end F-score of 55.33
(based on exact match of both cues and scopes). In
the official shared task ranking of results, and con-
sidering systems in all tracks together, our system
is ranked 4 out of 24 for Task 1 and 3 out of 15 for
Task 2, resulting in the highest average rank over-
all. For future work we aim to further improve the
cue detection, in particular with respect to multi-
word cues, and also continue to refine the scope
rules. Instead of defining the scopal rules only at
the level of dependency structure, one could also
have rules operating on constituent structure ? per-
haps even combining alternative resolution candi-
dates using a statistical ranker.
References
Thorsten Brants. 2000. TnT. A statistical Part-of-
Speech tagger. In Proceedings of the Sixth Con-
ference on Applied Natural Language Processing,
pages 224?231.
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The
Parallel Grammar Project. In Proceedings of COL-
ING Workshop on Grammar Engineering and Eval-
uation, pages 1?7.
Dick Crouch, Mary Dalrymple, Ron Kaplan, Tracy
King, John Maxwell, and Paula Newman. 2008.
XLE documentation. Palo Alto Research Center.
Rich?rd Farkas, Veronika Vincze, Gy?rgy M?ra, J?nos
Csirik, and Gy?rgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010): Shared
Task, pages 1?12.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering, 6 (1):15?28.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
English. In Joakim Nivre, Heiki-Jaan Kaalep,
and Mare Koit, editors, Proceedings of NODALIDA
2007, pages 105?112.
Robert Malouf. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In Pro-
ceedings of the 6th Conference on Natural Language
Learning, pages 49?55.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English. The Penn Treebank. Computa-
tional Linguistics, 19:313?330.
Roser Morante and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts. In
Proceedings of the BioNLP 2009 Workshop, pages
28?36.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proceedings of the 46th Meeting of the
Association for Computational Linguistics, pages
950?958.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
MaltParser: A data-driven parser-generator for de-
pendency parsing. In Proceedings of the Fifth In-
ternational Conference on Language Resources and
Evaluation, pages 2216?2219.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
Eighth International Workshop on Parsing Tech-
nologies, pages 149?160.
Stephan Oepen, Daniel Flickinger, Kristina Toutanova,
and Christopher D. Manning. 2004. LinGO Red-
woods. A rich and dynamic treebank for HPSG.
Journal of Research on Language and Computation,
2(4):575?596.
Lilja ?vrelid, Jonas Kuhn, and Kathrin Spreyer. 2009.
Improving data-driven dependency parsing using
large-scale LFG grammars. In Proceedings of the
47th Meeting of the Association for Computational
Linguistics, pages 37?40.
Lilja ?vrelid, Jonas Kuhn, and Kathrin Spreyer. 2010.
Cross-framework parser stacking for data-driven de-
pendency parsing. TAL 2010 special issue on Ma-
chine Learning for NLP, 50(3).
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun?ichi Tsujii. 2005. Developing a robust Part-
of-Speech tagger for biomedical text. In Advances
in Informatics, pages 382?392. Springer, Berlin,
Germany.
Erik Velldal. 2008. Empirical Realization Ranking.
Ph.D. thesis, University of Oslo, Institute of Infor-
matics, Oslo.
Veronika Vincze, Gy?rgy Szarvas, Rich?rd Farkas,
Gy?rgy M?ra, and J?nos Csirik. 2008. The Bio-
Scope corpus: Annotation for negation, uncertainty
and their scope in biomedical texts. In Proceedings
of the BioNLP 2008 Workshop.
Yi Zhang and Rui Wang. 2009. Cross-domain depen-
dency parsing using a deep linguistic grammar. In
Proceedings of the 47th Meeting of the Association
for Computational Linguistics, Singapore.
55
